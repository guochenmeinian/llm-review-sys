# Categorical Flow Matching on Statistical Manifolds

Chaoran Cheng

University of Illinois Urbana-Champaign

chaoran7@illinois.edu

Equal contribution.

Jiahan Li

Peking University

lijiahanypc@pku.edu.cn

Jian Peng

University of Illinois Urbana-Champaign

jianpeng@illinois.edu

Ge Liu

University of Illinois Urbana-Champaign

geliu@illinois.edu

###### Abstract

We introduce Statistical Flow Matching (SFM), a novel and mathematically rigorous flow-matching framework on the manifold of parameterized probability measures inspired by the results from information geometry. We demonstrate the effectiveness of our method on the discrete generation problem by instantiating SFM on the manifold of categorical distributions whose geometric properties remain unexplored in previous discrete generative models. Utilizing the Fisher information metric, we equip the manifold with a Riemannian structure whose intrinsic geometries are effectively leveraged by following the shortest paths of geodesics. We develop an efficient training and sampling algorithm that overcomes numerical stability issues with a diffeomorphism between manifolds. Our distinctive geometric perspective of statistical manifolds allows us to apply optimal transport during training and interpret SFM as following the steepest direction of the natural gradient. Unlike previous models that rely on variational bounds for likelihood estimation, SFM enjoys the exact likelihood calculation for arbitrary probability measures. We manifest that SFM can learn more complex patterns on the statistical manifold where existing models often fail due to strong prior assumptions. Comprehensive experiments on real-world generative tasks ranging from image, text to biological domains further demonstrate that SFM achieves higher sampling quality and likelihood than other discrete diffusion or flow-based models. Our code is available at [https://github.com/ccr-cheng/statistical-flow-matching](https://github.com/ccr-cheng/statistical-flow-matching).

## 1 Introduction

Recently, conditional flow matching (CFM) models  have achieved remarkable success in various generative domains including image generation , molecule  and protein design , and sequence generation . While attempts to generalize CFM and diffusion models to discrete categorical data have been made, they typically exert ad hoc assumptions on the structure of the discrete distribution. One group of work relies on stochastic jumps of Markov chains in either the discrete-time  or continuous-time setting  that discards the continuous nature of the underlying categorical distributions. Other work directly works with the probability simplex  or the corresponding logit space  with potentially imperfect assumptions that fail to capture the underlying true geometry of the statistical manifold. Furthermore, likelihood is often approximated by variational bounds in previous discrete generative models due to the lack of tractable exact likelihood.

We propose to incorporate the intrinsic geometry of the _statistical manifold_ by viewing categorical data as points on the statistical manifold of categorical distributions. Inspired by the mathematical results from information theory, we utilize the Fisher information metric  to naturally equip such a manifold with a Riemannian structure and develop an efficient generative training scheme for learning the vector fields without stability issues. We summarize our contributions as the following:

(1) We propose _Statistical Flow Matching_ (SFM), a novel and mathematically rigorous generative framework on the manifold of parameterized probability measures. SFM does not pose any prior assumptions on the statistical manifold but instead deduces its intrinsic geometry via mathematical tools. To tackle the discrete generation problem, we instantiate SFM on the manifold of categorical distributions. We deduce closed-form exponential and logarithm maps and develop an efficient flow-matching training algorithm that avoids numerical issues by leveraging a diffeomorphism between manifolds. SFM effectively leverages the intrinsic geometric properties by following the shortest paths of geodesics between the noise and target distributions on the statistical manifold.

(2) Our distinctive geometric perspective of the statistical manifold allows us to further apply optimal transport during training and derive tractable exact likelihood for any given sample of probability measure, both of which are unachievable for most existing methods. We also introduce new theoretical insights by establishing connections among Riemannian flow matching, information geometry, and natural gradient descent, which allows us to interpret SFM as following the steepest descent of the _natural gradient_ from the optimization angle.

(3) We demonstrated with a toy example on simplex that SFM can learn more complex patterns on the statistical manifold where existing models often fail due to impromptu prior assumptions. We further conducted extensive experiments on diverse real-world discrete generation tasks involving computer vision, natural language processing, and bioinformatics. SFM consistently outperformed existing diffusion or flow-based models and also achieved comparable results with autoregressive models on character-level generation.

## 2 Preliminary

### Information Geometry

In this work, we are interested in learning a parameterized family of probability measures. It is known from information theory that all probability measures over the sample space form the structure known as _statistical manifold_. Mathematically, consider probability densities \(p=}{}:\) defined by the Radon-Nikodym derivative where \(\) is a probability measure on the sample space \(\) and \(\) is the reference measure on \(\). Suppose the statistical manifold \(=()=\{p:_{}=_ {}p(x;)\,=1\}\) is parameterized by \(=(_{1},_{2},,_{n})\), this parameterization naturally provides a coordinate system for \(\) on which each point is a probability measure \(\) with the corresponding probability density function \(p(x;)\). The _Fisher information metric_ is defined as

\[g_{jk}()=_{X}[}}]=_{ }}}p(x;)\,. \]

Rao demonstrated in his seminal paper  that statistical manifold can be equipped with the Fisher information metric to obtain a Riemannian structure, the study of which is known as _information geometry_[5; 4; 8]. This geometric view of statistical manifolds allows us to derive key geometric concepts for our statistical flow matching framework. For example, a _geodesic_\((t):\) defines a "shortest" path (under the Riemannian metric) connecting two probability measures on the statistical manifold. The _geodesic distance_ between two probability measures, also known as the Fisher-Rao distance , measures the similarity between them. The _tangent space_\(T_{}()\) at a point \(\) can be naturally identified with the affine subspace \(T_{}()=\{|_{}=0\}\) where each element \(\) is a signed measure over \(\). The _exponential map_\(_{}:T_{}()\) and _logarithm map_\(_{}: T_{}()\) can also be defined on the statistical manifold. While the geodesic for a parameterized family of probability measures can be obtained numerically by solving the geodesic equation when closed-form expression is unknown (see Appendix A.1), it usually requires expensive simulations. Fortunately, closed-form geodesic distances are available for many common distributions including categorical, multinomial, and normal distributions , which motivates our method.

### Conditional Flow Matching on Riemannian Manifold

The conditional flow matching (CFM) framework  provides a simple yet powerful approach to generative modeling by learning a time-dependent vector field that pushes the prior noise distribution to any target data distribution. Such a flow-based model can be viewed as the continuous generalization of the score matching (diffusion) model [56; 57; 26] while allowing for a more flexible design of the denoising process. The Riemannian flow matching framework  further extends CFM to general manifolds on which a well-defined distance metric can be efficiently computed.

Consider a smooth Riemannian manifold \(\) with the Riemannian metric \(g\), a _probability path_\(p_{t}:()\) is a curve of probability densities over \(\). A _flow_\(_{t}:\) is a time-dependent diffeomorphism defined by a time-dependent vector field \(u_{t}: T\) via the ordinary differential equation (ODE): \(}{t}_{t}(x)=u_{t}(_{t}(x))\). The flow matching objective directly regresses the vector field \(u_{t}\) with a time-dependent neural net \(v(x_{t},t)\) where \(x_{t}:=_{t}(x)\). However, this objective is generally intractable. Both [38; 14] demonstrated that a tractable objective can be derived by conditioning on the target data \(x_{1}\) at \(t=1\) of the probability path. The Riemannian flow matching objective can be formulated as 

\[=_{t U,x_{0} p_{0}(x),x_{1} q(x)}[\|v(x _{t},t)-u_{t}(x_{t}|x_{0},x_{1})\|_{g}^{2}] \]

where \(q\) is the data distribution, \(p_{0}\) is the prior distribution, and \(x_{t}:=_{t}(x|x_{0},x_{1})\) denotes the conditional flow.  further demonstrated that if the exponential map and logarithm map can be evaluated in closed-form, the condition flow can be defined as \(x_{t}=_{x_{0}}(t_{x_{0}}x_{1})\), and the corresponding vector field can be calculated as \(u_{t}(x_{t}|x_{0},x_{1})=}{t}x_{t}=_{x_{t}}(x_{1 })/(1-t)\). We also adapt this formulation for our statistical flow, which we will elaborate on in the next section. We note that, since we are working with manifolds of probability measures \(=()\), we will use \(p,q\) for probability densities _over_ the manifold \(()\) and \(,\) for probability measures (or probability masses for discrete distributions) _on_ the manifold \(()\) to avoid confusion.

## 3 Method

Different from previous work that treated each distribution separately, we adopt an integral viewpoint toward the manifold of probability distributions. In this section, we focus on the statistical manifold of categorical distributions to demonstrate the application of our method on discrete generation tasks. However, we emphasize that our proposed SFM is applicable to any statistical manifold with a closed-form geodesic distance and can be broadly used in generative tasks targeting probability measures on the statistical manifold.

### Statistical Manifold of Categorical Distributions

Consider the discrete sample space \(=\{1,2,,n\}\), an \(n\)-class categorical distribution over \(\) can be parameterized by \(n\) parameters \(_{1},_{2},,_{n}\) such that \(_{i=1}^{n}_{i}=1,_{i} 0\). In this way, the reference measure \(\) is the counting measure and the probability measure \(\) can be written as the convex combination of the canonical basis of Dirac measures \(\{^{i}\}_{i=1}^{n}\) over \(\): \(=_{i=1}^{n}_{i}^{i}\). Geometrically, this manifold can be visualized as the \((n-1)\)-dimensional simplex \(^{n-1}\). The geodesic distance between two categorical distributions is given in [51; 44] as

\[d_{}(,)=2(_{i=1}^{n}_{i}} ). \]

The tangent space at a point \(\) can be identified with \(T_{}()=\{u|_{i=1}^{n}u_{i}=0\}\) and the corresponding inner product at point \(\) is defined as

\[ u,v_{}=_{i=1}^{n}v_{i}}{_{i}}, _{+},u,v T_{}() \]

where \(_{+}\) denotes the statistical manifold of _positive_ categorical distributions. Note that the inner product is ill-defined on the boundary, causing numerical issues near the boundary. To circumvent this issue, we introduce the following diffeomorphism

\[: S_{+}^{n-1},_{i} x_{i}=}, \]which maps the original statistical manifold to the positive orthant of a unit \((n-1)\)-sphere \(S_{+}^{n-1}=\{x|_{i=1}^{n}x_{i}^{2}=1,x_{i} 0\}\) (see Fig.2). Note that we have \(\|()\|_{2}^{2}=_{i=1}^{n}^{2}}=_{i=1}^{n}_{i}=1\). The geodesic on \(S_{+}^{n-1}\) follows the great circle, and the following proposition holds between the geodesic distance on \(S_{+}^{n-1}\) and \(\):

**Proposition 1**.: \[d_{S}((),())=d_{}(,),, .\] (6)

A proof is provided in Appendix A.3. This indicates that we can work with the geodesic distance on the unit sphere instead with up to a constant factor:

\[d_{S}(x,y)=( x,y), x,y S_{+}^{n-1}. \]

The geodesic distance \(d_{S}\) and the inner product \(,\) are well-defined for the boundary, and we found this transform led to the practical stabilized training of the flow model. Visualizations of the Riemannian geometry on the statistical manifold of 3-class categorical distributions and the corresponding Euclidean geometry on the simplex are provided in Fig.1 for comparison. The straight lines under the Euclidean assumptions fail to capture the true curved geometry of the statistical manifold.

### Statistical Flow Matching

We provide the analytical form of the exponential and logarithm maps on the statistical manifold of categorical distributions in Appendix A.3. Although it is possible to directly learn the vector field following the loss in Eq.(2), such a direct parameterization has numerical issues near the boundary. As described in Sec.3.1, we apply the diffeomorphism \(\) in Eq.(5) to derive a more stable training objective on the spherical manifold:

\[_{}=_{t U,x_{0}_{*}(p_{0}()), x_{1}_{*}(q())}[\|v(x_{t},t)-u_{t}^{S}(x_{t}|x_{0},x_{1})\|_{2}^ {2}], \]

where \(p_{0}\) is the prior noise distribution over \(\) and \(q\) is the data distribution; \(_{*}\) denotes the standard pushforward measure. \(v:S_{+}^{n-1} TS_{+}^{n-1}\) is a learnable time-dependent vector field network that maps the interpolant \(x_{t}\) on the unit sphere to a tangent vector in the tangent bundle \(TS_{+}^{n-1}\). The ground truth conditional vector field \(u^{S}\) is calculated using the exponential and logarithms maps on the sphere (Appendix A.2). The overall training and inference scheme is visualized in Fig.2 and described in Alg.2 and 3 in Appendix C.

We further implement a Euclidean flow matching model on the probability simplex with linear interpolation between the noises and the target distributions. Though linear flow matching offers "straight" lines under the Euclidean assumption, it is unaware of the intrinsic geometric properties

Figure 1: The Riemannian geometry of the statistical manifold for categorical distributions in comparison to Euclidean geometry on the simplex. **Left**: Contours for the geodesic distances to \(_{0}=(1/3,1/3,1/3)\). **Middle**: Exponential maps (geodesics) from \(_{0}\) to different points near the boundary. **Right**: Logarithm maps (vector fields) to \(_{0}\).

of the statistical manifold and turns out to trace longer paths in terms of the Riemannian metric. The objective for linear flow matching can be described as

\[_{}=_{t U,_{0} p_{0}(),_ {1} q()}[\|v(_{t},t)-(_{1}-_{0})\|_{2}^{2}]. \]

In the discussion above, we assume a single data dimension on the statistical manifold. This can be extended to any data dimension by treating them as independent channels of the input. In practice, each probability measure on the simplex is represented by a matrix \(X^{D n}\) where \(D\) is the data dimension and each row sums to 1. The priors are independently sampled from the uniform distribution over the \((n-1)\)-simplex and each data dimension is independently interpolated with the same timestep \(t U\). The flow model, on the other hand, takes all the data dimensions as input to capture the dependence between different dimensions. During sampling, existing ODE solvers and the simple Euler method are used to integrate the vector field through time to obtain the final categorical distributions. Discrete samples are then drawn from the generated categorical distributions for evaluation. Details regarding model parameterization and sampling are further described in Appendix C.1 and C.2.

### Optimization View of Statistical Flow Matching

We further provide an interpretation of our proposed statistical flow matching framework from the perspective of an optimization problem. From the _optimization_ viewpoint, for a generative model on a statistical manifold, we want to minimize the discrepancy between the target distributions and the generated distributions. Naturally, the Kullback-Leibler divergence \(D_{}(()\|(_{1}))\) can be used as a measure of statistical distance. We note that the Fisher information metric can be obtained as the Hessian of the KL divergence with respect to the parameter \(\). This can be demonstrated by the fact that the KL divergence reaches the global minimum of 0 at \(=_{1}\), so all first-order partial derivatives are zero, and the Hessian is positive semi-definite. Therefore, Taylor expansion of KL divergence at \(_{1}\) with \(=-_{1}\) gives

\[ D_{}(()\|(_{1}))& _{jk}_{j}_{k}.}{_{j}_{k}}D_{}(()\| (_{1}))|_{=_{1}}\\ &=_{jk}_{j}_{k}g_{jk}( _{1})=\|\|_{g}^{2}. \]

From the _geometric_ viewpoint, the geodesic, by definition, is a (locally) length-minimizing curve with respect to the corresponding Riemannian metric. Therefore, by following the direction of the vector field that decreases the geodesic element \(s^{2}=\|\,\|_{g}^{2}\|\|_{g}^{2}\), we are indeed following the steepest direction that minimizes the local KL divergence. In this sense, the Fisher information metric defines a second-order optimization scheme for the KL divergence by following the "steepest" direction of the Hessian. Indeed, The steepest direction \(\) that decreases the KL divergence is

Figure 2: Statistical flow matching (SFM) framework. **(a)** During training (Sec.3.2), probability measures on \(\) are mapped to \(S_{+}^{n-1}\) via diffeomorphism \(\) to compute the time-dependent vector field (in red). During inference, the learned vector field generates the trajectory on \(S_{+}^{n-1}\) and we map the outcome of ODE back to \(\) (in blue). **(b)** In the NLL calculation for one-hot examples (Sec.3.5), the probability density is marginalized over a small neighborhood of some Dirac measure to avoid undefined behaviors at the boundary (in green).

known as the _natural gradient_ in the existing literature [2; 3] and has been explored in optimization known as _natural gradient descent_[48; 42; 46]. Instead of optimizing along the normal gradient, the natural gradient is defined as \(=F^{-1}\) where \(F\) is the Fisher information matrix estimated from a batch of sampled data. Results established in [2; 42] demonstrated that stochastic natural gradient descent is asymptotically "Fisher efficient" and is indeed the steepest descent direction in the manifold of distributions where distance is measured in small local neighborhoods by the KL divergence. Following the geodesic defined by the Fisher information metric, our SFM framework also shares these benefits with an additional advantage of analytical expressions for geodesics, as we focus on the family of categorical distributions instead of general statistical models. Such a theoretical connection may contribute to the better performance of SFM.

### Optimal Transport on Statistical Manifold

The geometric viewpoint of the statistical manifold offers a continuous and differentiable formulation of generative modeling and also provides a robust way to measure the distance between two categorical distributions via the well-defined geodesic distance in Eq.(3). In contrast, the Markov chain-based methods cannot establish a robust distance measure due to the stochastic jumps between discrete states. Inspired by the optimal transport formulation in previous work [45; 20; 68; 63], we naturally extend it to our statistical setting in which the cost is defined by averaging the statistical distances over data dimensions as \(d_{}(X,Y)=_{k=1}^{D}d_{}(^{(k)},^{(k)})\) for \(X=\{^{(k)}\}_{k=1}^{D},Y=\{^{(k)}\}_{k=1}^{D}\). An optimal assignment of the initial noises to the target data distributions can potentially lead to more efficient training, which we demonstrated empirically in our ablation studies.

### Exact Likelihood Calculation

Unlike diffusion-based models which rely on variational lower bounds for likelihood estimation, our proposed method shares the continuous normalizing flow's capability of exact likelihood calculation. For an arbitrary test sample \(x\), using the change of measure formula, the likelihood can be modeled by the continuity equation [14; 43]:

\[ p_{t}(x_{t})+_{g}(v_{t})(x_{ t})=0, \]

where \(_{g}\) is the Riemannian divergence and \(v_{t}(x_{t}):=v(x_{t},t)\) is the time-dependent vector field. In this way, the pushforward probability measures \(p_{t}(x_{t})\) defined via the learned flow can be obtained as the integral of the Riemannian divergence back through time on the simulated trajectory \(x_{t}\). Following [9; 7], we define the ODE log-likelihood as the change of the log-likelihood as:

\[ p^{}(x_{1})=_{1}^{0}_{g}(v_{t})(x_{t}) \,t, \]

where the trajectory \(x_{t}\) is obtained by solving the differential equation \(x_{t}=v_{t}(x_{t})\) reverse through time with the initial condition \(x_{1}\) at \(t=1\). In this way, we have \( p(x_{1})= p^{}+ p_{0}(x_{0})\) where \( p_{0}(x_{0})\) is the log-likehood of the prior distribution at data point \(x_{0}\). In practice, we follow previous work  to use Hutchinson's trace estimator  to efficiently obtain an unbiased estimation of the divergence using standard normal random variables. To further account for the transform \(\) from \(\) to \(S_{+}^{n-1}\) and the reverse transform \(^{-1}\), additional change of measure formulae need to be applied. Consider the pushforward of the probability measure \(P\) over \(\) under the diffeomorphism \(\) defined by \(_{*}P(V):=P(^{-1}V)\), we have the change of measure identity \(_{*}P(())|()|=P(x)\) for \(x=()\). Therefore, by adding the two log-determinant of the pushforward measures, the log-likelihood can be formulated as

\[ p_{1}(_{1})=|^{-1}(x_{1})|+ p^{}(x _{1})+|(_{0})|+ p_{0}(_{0}). \]

The above formula is well-defined for all interior points of \(\), but the change of measure terms are undefined on the boundary. This can be understood as the fact that discrete likelihoods can be arbitrarily high . Following , we derive a variational lower bound for the likelihood as the marginalized probability over the small neighborhood of a Dirac measure \(_{1}=\) at the boundary:

\[ p()_{q(_{1}|)}[- q(_{ 1}|)+ p(|_{1})+ p_{1}(_{1})], \]where \(q(_{1}|)\) can be viewed as the forward noising probability at \(_{1}\) which is close to \(\) with a closed-form likelihood. \(p(|_{1})\) is the categorical likelihood (cross-entropy) and \(p_{1}(_{1})\) is the model likelihood in Eq.(13). The overall workflow of calculating NLL is demonstrated in Fig.2, and more details regarding likelihood computation can be found in Appendix B.1. It is worth mentioning that the continuous likelihood calculated here is defined with respect to the probability distribution over the statistical manifold \(\). In contrast, the bits-per-dimension score for autoregressive models is usually defined with respect to a specific categorical distribution on \(\) and therefore not comparable, as was also noted in .

## 4 Experiments

Our proposed SFM framework can be leveraged to approximate arbitrary distribution over \(\), i.e., any distribution over the parameterized family of probability measures. We first demonstrate this with the toy example of a Swiss roll distribution on the probability simplex. We then conduct extensive experiments on real-world datasets for discrete generation across various domains including computer vision, natural language processing, and bioinformatics to demonstrate the effectiveness of our proposed model. We train and evaluate two different versions of the SFM framework: **SFM w/ OT** for our proposed model with optimal transport during training and **SFM w/o OT** for the model without optimal transport. A naive approach that directly works with the exponential and logarithm maps on the statistical manifold without the diffeomorphism (**SFM w/o \(\)**) is also compared in the toy example. We also implement a linear flow matching model on the probability simplex using the loss in Eq.(9) as an additional baseline, for which we dub **LinearFM**. For discrete data, we always use Eq.(14) to obtain finite negative log-likelihood (NLL). More details regarding model architectures and the experimental setup can be found in Appendix C.

### Toy Example: Swiss Roll on Simplex

We project the Swiss roll dataset onto the 2-simplex with some additional margins to make sure no point resides on the boundary. We used a time-dependent MLP to model the vector field for all models. The generative points on the simplex and NLL are calculated using the Dopri5 ODE solver  and are shown in Fig.3.

We note that most existing models assume the target data to be Dirac measures (one-hot distributions) and cannot be applied to this task. Both **DDSM** and **DirichletFM** imposed strong prior assumptions on the data distribution as Dirichlet distributions, which failed to capture the complex geometric shape of the Swiss roll data points as demonstrated in the generated samples. On the contrary, directly built upon the geometry of the statistical manifold, our SFM model successfully captured the detailed data geometry. As no data resides on the boundary, exact NLL can be obtained using Eq.(13) and was averaged over all points in the dataset. Though linear flow matching could also capture the geometry of the data, our SFM outperformed it in terms of NLL.

Figure 3: Generated samples of the Swiss roll on simplex dataset and NLL (lower is better). The NLLs are estimated using Hutchinson’s trace estimator, whereas those in the parenthesis are exact.

### Binarized MNIST

We extend our SFM to model discrete generation tasks in computer vision. The binarized MNIST dataset  is the binarized version of the original MNIST dataset  by thresholding the original continuous value to be either 0 or 1, thus can be viewed as a 2-class generation task with a data dimension of \(28^{2}=784\). We also compared **D3PM** and **DFM** as examples of masked diffusion models for discrete generation. We used a convolutional neural net adopted from  with additional additive Fourier-based time embeddings. The quantitative evaluation of NLL and Frechet inception distance (FID) are shown in Tab.1.

All the generated results and NLL calculations were based on the DopriS ODE solver. Additional ablation results can be found in Appendix D.2 and generated images can be found in Appendix D.3. The NLL for diffusion-based models (D3PM and DDSM) was also calculated based on the variational bounds derived in their papers. Our proposed model consistently outperformed both the linear flow matching and other diffusion baselines in terms of FID and NLL, achieving higher sample quality and likelihood.

### Text8

The Text8 dataset  is a medium-size character-level corpus consisting of a small vocabulary of 27, which includes the 26 lowercase letters and the whitespace token. We followed the convention in previous work  to use random chunks of length 256 in both training and evaluation without any preprocessing. We used a 12-layer diffusion transformer (DiT)  based predictor, similar to the one used in . As our exact likelihood is not directly comparable to bits-per-character (BPC) reported in previous work which relies on an alternative variational bound for the likelihood, we additionally calculate such BPC inspired by . See Appendix B.2 for additional details. All generated samples and NLL were estimated using the DopriS ODE solver. Quantitative results of BPC are provided in Tab.2 and generated texts are provided in Appendix D.3. The results for the autoregressive language models are also provided as a reference (Discrete Flow  is based on autoregressive normalizing flow).

Note that, unlike all of the other baselines, SFM and LinearFM do not directly optimize such a BPC as a training objective, so such an evaluation metric is unfavorable for our model (see Appendix B.2). Despite such an unfavorable evaluation, our proposed SFM still achieved the second-best performance among other diffusion and flow baselines that were directly trained with cross-entropy losses. We also noted a significant performance gap between SFM and the naive linear flow matching on simplex, highlighting the importance of capturing the intrinsic geometric properties of the underlying statistical manifold. Optimal transport does not significantly affect the final performance here, which we presume might be due to the long training stage in which vector fields were well-explored. Additional evaluation following  is provided in Appendix D.1, in which SFM achieved the best generation entropy as evaluated by the pre-trained GPT-J-6B model .

### Promoter Design

We further apply SFM to the practical task of promoter DNA sequence design in the bioinformatics realm. The promoter is a key element in gene transcription and regulation, and the generation of desired promoters can better help us understand the intricate interactions between human genes.  proposed a human promoter sequence dataset containing 100k promoter sequences with the

   Model & SFM w/ OT & SFM w/o OT & SFM w/o \(\) & LinearFM \\  NLL\(\) & **-1.687 \(\) 0.020** & -1.631 \(\) 0.027 & 0.928 \(\) 0.035 & 0.622 \(\) 0.022 \\ FID\(\) & **4.62** & 5.15 & 8.2073 & 5.91 \\  Model & DirichletFM & DDSM & D3PM & DFM \\  NLL\(\) & NA & 0.100 \(\) 0.001\({}^{*}\) & (0.141 \(\) 0.021) & (0.101 \(\) 0.017) \\ FID\(\) & 77.35 & 7.79 & 67.36 & 34.42 \\   

Table 1: NLL and FID of different discrete models on binarized MNIST. The NLLs in the parenthesis are discrete NLLs; therefore, they are not directly comparable. * is from .

corresponding transcription initiation signal profiles. Each promoter sequence is 1024 base pairs long and is centered at the annotated transcription start site position . Therefore, we can model promoter design as a generation task with 4 categories (the base pair ATGC) conditioned on the given transcription signals. We follow  to concatenate the signal as additional input to the vector field predictor built upon 20 stacks of 1-dimensional convolutional layers on the input sequence. Optimal transport can also be applied for conditional generation, as we can pair the conditions with the input to make sure that the conditional arguments align with the target one-hot distributions.

To provide a quantitative evaluation of the generated promoter sequences, we follow  to apply the pre-trained deep learning sequence model Sei  to predict active promoters based on predictions of the chromatin mark H3K4me3. As the dataset spans the whole range of human promoter activity levels from highly expressed promoters to those with very low expression, the evaluation is based on comparing the scores between the generated sequences and the ground truth human genome promoter sequences on the test chromosomes. The metric SP-MSE is the mean squared error between the predicted promoter activity of the generated sequences and human genome sequences (lower is better) and is demonstrated in Tab.3. Our SFM was able to achieve the lowest SP-MSE score compared with other baselines. It is worth noting, though, that optimal transport produced slightly sub-optimal results. We hypothesize that this is because, in conditional generative tasks, the final generation should rely heavily on the conditions. Simply matching inputs with the targets using distance measures may discard important information in the conditional arguments and may not be the best practice.

## 5 Related Work

As we put a special interest in discrete generation, we start with the existing work on discrete generation. We first list a few traditional autoregressive models [16; 50; 64] and masked language models [18; 54] but will skip them as we mainly focus on the diffusion and flow matching models. Existing diffusion and flow-based discrete generative models can be categorized into two main groups. The first group relies on stochastic jumps of Markov chains by treating discrete classes as Markov states. By choosing a proper transition matrix, the target one-hot distribution can be gradually noised into the stationary distribution. Noticeably, this approach can also adopt an additional absorbing state to mimic the mask token in masked language modeling . D3PM  adapted the discrete-time Markov chain with the diffusion framework and SEDD  proposed a more efficient training scheme by learning the score entropy between different states. [11; 55] extended it to the continuous-time Markov chain by using the infinitesimal generator (rate matrix) instead, and [12; 22] further applied the flow matching framework. Although the transition matrix or the rate matrix determines the entire diffusion trajectory, there is no canonical way of choosing it for different generation tasks to control the mixing rate. Also, due to the presence of discrete jumps of the Markov chain, exact likelihood calculation is no longer feasible. Only variational bounds were derived in [6; 12]. The other group directly works with the probability simplex or the logit space

   Model & BPC\(\) \\  SFM w/ OT & 1.399 \(\) 0.020 \\ SFM w/o OT & 1.386 \(\) 0.033 \\ LinearFM & 1.651 \(\) 0.027 \\  D3PM-absorb & 1.47\({}^{*}\) \\ BFN & 1.41\({}^{*}\) \\ SEDD-absorb & **1.32\({}^{*}\)** \\ MultiFlow & 1.41\({}^{*}\) \\ Argmax Flow & 1.80\({}^{*}\) \\  Discrete Flow & 1.23\({}^{*}\) \\ Transformer & 1.23\({}^{*}\) \\ Transformer XL & **1.08\({}^{*}\)** \\   

Table 2: BPC on Text8. Results marked \({}^{*}\) are taken from the corresponding papers.

   Model & SP-MSE\(\) \\  SFM w/ OT & 0.0279 \\ SFM w/o OT & **0.0258** \\  LinearFM & 0.0282 \\ DDSM & 0.0334\({}^{*}\) \\ D3PM-uniform & 0.0375\({}^{*}\) \\ Bit-Diffusion (one-hot)  & 0.0395\({}^{*}\) \\ Bit-Diffusion (bit)  & 0.0414\({}^{*}\) \\ Language Model & 0.0333\({}^{}\) \\ DirichletFM & 0.0269\({}^{}\) \\   

Table 3: SP-MSE (as evaluated by Sei ) on the generated promoter DNA sequences. Results marked \({}^{*}\) are from  and results marked \({}^{}\) are from .

with certain assumptions of the underlying geometric structure . As an example, our linear flow matching assumes a Euclidean geometry on the probability simplex and is often used as a baseline in previous work . The multinomial diffusion  assumed a Euclidean geometry on the logits space so interpolation became multiplication back on the probability simplex. Dirichlet diffusion (DDSM)  and Dirichlet flow matching (DirichletFM)  exerted a strong prior assumption on the probability path as the Jacobi diffusion process. We provide a more detailed analysis of these models in Appendix A.4. For models that directly operate on the logit space, the Bayesian flow network (BFN) assumed Gaussian distributions on the logit space with Bayesian update rules.  also assumed a Euclidean geometry in the logit space with targets as soft one-hot logits. The assumptions made in these models may not always hold, e.g., for the Swiss roll on simplex dataset in Fig.3. These assumptions also did not necessarily capture the true geometry of the underlying statistical manifold. In contrast, our proposed SFM framework does not exert any strong prior assumptions and is aware of the intrinsic geometry by following the geodesics.

We also briefly review the related work on statistical manifolds and information geometry. The field of information geometry, the study of geometric properties of statistical manifolds, was first established in Rao's seminal paper in 1945  discussing the Fisher information metric. Most previous work utilizing information geometry focuses on optimization  with a few exceptions on representation learning.  utilized the geodesic distance between two univariate Gaussians for function shape matching for retrosynthesis of gold nanoparticles.  treated point cloud data as probability measures over the 3D Euclidean space and considered the pullback metric on the latent space to obtain optimal latent coordinates for the autoencoder.  further applied such a method on single-cell RNA sequence trajectories. These models demonstrated the advantage of following the proper geometry compared with the vanilla Euclidean setting, but the pullback metric needed to be evaluated with expensive Jacobian-vector products during training. In contrast, our proposed SFM, for the first time, leverages mathematical tools from information geometry to generative modeling. SFM directly operates on the statistical manifold with closed-form geodesics, providing a simulation-free approach for efficient generative training.

## 6 Discussion

In this paper, we proposed statistical flow matching (SFM) as a general generative framework for generative modeling on the statistical manifold of probability measures. By leveraging results from information geometry, our SFM effectively captures the underlying intrinsic geometric properties of the statistical manifold. Specifically, we focused on the statistical manifold of categorical distributions in this work and derived optimal transport and exact likelihood formulae. We applied SFM to diverse downstream discrete generation tasks across different domains to demonstrate our framework's effectiveness over the baselines. We noted that SFM can be further extended to non-discrete generative tasks whose targets are probability distributions, which we will leave as future work.

We are also aware of the limitations of our SFM framework. As a special case of the flow matching model, the generation is an iterative process of refinement that cannot modify the size of the initial input. This may pose limitations to generation compared with autoregressive models. Furthermore, we adopted the assumption of independence between classes so that the canonical Riemannian structure can be induced by the Fisher metric. However, discretized data like CIFAR-10  (256 ordinal pixel values) do not follow this assumption, and results on such data might be suboptimal.