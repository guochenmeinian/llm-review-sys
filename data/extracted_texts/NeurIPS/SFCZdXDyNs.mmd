# _Deja vu_ Memorization in Vision-Language Models

Bargav Jayaraman

FAIR, Meta

California, USA

bargav@meta.com &Chuan Guo

FAIR, Meta

California, USA

chuanguo@meta.com &Kamalika Chaudhuri

FAIR, Meta

California, USA

kamalika@meta.com

###### Abstract

Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call _deja vu memorization_. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate _deja vu_ memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance. The code is available here: https://github.com/facebookresearch/VLMDejaVu.

## 1 Introduction

Vision-Language Models (VLMs) have emerged as the state-of-the-art solution for learning representations from images and text data, with a number of downstream applications such as image generation (Ramesh et al., 2021, 2022; Yu et al., 2022), retrieval (Wang et al., 2015; Cao et al., 2016; Zhang et al., 2021; Baldraki et al., 2022), captioning (Mokady et al., 2021), and classification. At the same time, large foundation models are known to memorize and retain information about their training data (Carlini et al., 2019; Meehan et al., 2023; Carlini et al., 2023), and hence, a natural question is whether these Vision-Language Models _memorize_ as well. If so, this raises questions about generalizability of these models. We investigate whether Vision-Language Models retain information about their training data beyond the bounds of generalization.

The main challenge in measuring memorization is designing a measurement technique that can tease apart memorization from spurious correlations. For example, for an image of a black swan on water, a representation learning model may learn to predict _black swan_ given the background _water_ if either: _(i)_ it retains extra information about the training image, or, _(ii)_ if most of the examples in the training corpus with water also involve black swans. The first kind constitutes as memorization whereas the second kind is spurious correlation. This uncoupling of memorization from spurious correlation is particularly complicated for VLMs. Unlike generative models, VLMs as well as other representation learning models lack decoders that can directly generate images or text; therefore, what the model learns about its training data has to be detected more subtly.

Prior work has looked into this problem for image-only representation models (Meehan et al., 2023) by measuring whether the model can predict the foreground of an image (e.g, black swan) beyond simple correlations based simply on its background (e.g, water). However, such simple solutions do not apply here. VLMs have two separate modalities - text and image, and the data sets used to train and evaluate them are considerably more complex than the simple foreground-background structureof ImageNet (see Figure 6 for an example). A consequence is that the image and text modalities can interact and transfer information in these models in subtly complex ways, making measurement significantly more challenging.

In this work, we propose a new method for measuring memorization in VLMs (depicted in Figure 1). Given a target image caption, we use the VLM to encode the caption and retrieve relevant image samples from a _public set_ of images. Our test is based on the key insight that if an image-text pair is memorized by a VLM, then the retrieved images would resemble the training image to a significantly higher amount of detail than what is predictable from either the text caption or simple correlation. Formally, given a text-image pair, we retrieve an image from the model based on an embedding of its text description, and we measure what fraction of ground-truth objects in the original image also co-occur in the retrieved image. Then, to determine whether this happens simply due to correlation, we measure how this compares with the same statistic obtained from a similar VLM which does not have this image-text pair in its training data. Combining these two steps gives us a measurement method that we call VL-Deja-Vu.

We evaluate our test on CLIP (Radford et al., 2021) models trained on subsets of Shutterstock and a filtered version of LAION (filtered LAION) with varying number of training samples. We find that even at training data set sizes where CLIP generalizes well, there is a significant degree of model memorization as depicted by our metrics (see Section 4). Finally, we explore mitigation measures that reduce information leakage in Section 5. We find that text masking significantly mitigates deja vu memorization at a marginal cost to the model utility. We note that there could be other effective mitigations but were not explored due to the computational limitations.

Contributions.Our main contributions are as follows.

* We propose VL-Deja-Vu--a new way of measuring memorization in VLMs by measuring what fraction of ground-truth objects in an image can be predicted from its text description for a training image-text pair.
* Based on this measurement technique, we propose both (a) an individual sample-level test to detect memorization for individual text-image pairs and (b) an aggregate population-level test for a Vision-Language Model.

Figure 1: An example where a CLIP (Radford et al., 2021) model trained on a 40M subset of a Shutterstock data set exhibits _deja vu_ memorization of objects present in a training image. Public set is a separate collection of 20M images from Shutterstock that has no overlap with the training set. The objects annotated in orange are true positives, i.e., the ones present in the target image, and the objects annotated in blue are false positives. Our test recovers significantly more memorized objects for the target VLM (trained on the target image) compared to the reference VLM (not trained on the target image). Additional qualitative examples can be found in Figure 11 in the appendix.

* We use our VL-Deja-Vu test to evaluate memorization in CLIP, and show that memorization does occur for VLMs trained using a number of different training set sizes and regularization parameter values, even for settings where the model generalizes well.
* Finally, we explore mitigation measures, and demonstrate that among a number of different ways to train CLIP, random masking of text serves to significantly reduce _deja vu_ memorization.

## 2 Background

**Vision-Language models** are multi-modal models whose core function is to map image-text pairs into a pair of representations that are semantically relevant. These embeddings can then be used for downstream tasks such as image classification, captioning, retrieval and generation. VLMs are composed of a _vision block_, consisting of a convolutional network or a vision transformer, and a _text block_, consisting of a transformer, that produce image and text embeddings respectively from input image-text pairs. Given a trained vision-language model \(f\), and an image-text pair \(z= z_{img},z_{txt}\), we denote the corresponding image and text embeddings as \(f(z_{img})\) and \(f(z_{txt})\).

We consider VLMs that involve contrastive pre-training; in other words, during training, the model learns to minimize the distance between the image and text embeddings of the matching pairs in the training set \(D_{tr}\) while maximizing the distance of the mismatched pairs. The most commonly used contrastive loss is the InfoNCE loss  given as follows:

\[L=-^{i})^{}f(z_{txt}^{i})/)}{_{j} (f(z_{img}^{i})^{}f(z_{txt}^{j})/)}\] (1)

where \(\) is the temperature and \(z^{j}, j i\) are negative examples to contrast against. In practice, for each positive example \(z^{i}\), we use all other examples in a training batch as negative examples. The most popular VLM of this type is CLIP (Contrastive Language-Image Pre-Training; Radford et al. ), trained on an undisclosed data set, which achieves competitive out-of-the-box performance across many transfer learning tasks. OpenCLIP  has released an open-source implementation of CLIP, and showed that training on a filtered LAION dataset  can achieve comparable performance to the original CLIP model. Our work investigates memorization in OpenCLIP.

Memorization in ML models.It is well-known that machine learning models can memorize their training data in ways that enable data extraction. This phenomenon has been studied for both language  and vision . However, all these works only consider the uni-modal setting, and as such the impact of this phenomenon is not clear in the multi-modal settings. Moreover, almost all the prior studies (except Meehan et al. ) focus on generative models - language or vision - where measuring memorization is easier because of the presence of a decoder.

Similar to Meehan et al. , we investigate the setting of representation learning models, where we do not have a decoder and instead only have access to an encoder. Although unlike Meehan et al. , who considered vision models that capture the relationship between representation of the background of an image (such as water) and the label of its foreground object (such as black swan), we consider settings where the models are trained on more complex data sets that have multiple objects in any given image. Such a simple foreground-background measurement does not directly apply to our setting of Vision Language Models where the two modalities may leak training data in more subtle and complicated ways. Our work builds upon their test, and extends it to VLMs. A more detailed background discussion can be found in Appendix B.

## 3 _Deja vu_ Memorization for Vision-Language Models

_Deja vu_ memorization happens when a foundation model retains information about individual training data points beyond what is expected by simple correlation, and allows the recovery of such information during inference time. An example is when an image representation learning model can confidently predict the foreground of a training image based simply on its background (Meehan et al., 2023), while similar predictions cannot be made for test images.

In the context of Vision-Language Models, however, measuring _deja vu_ memorization is not as simple, due to the presence of multiple modalities as well as the complex nature of the training data. Compared to ImageNet, VLMs are trained on vastly more semantically rich data sets with many more objects as well as complicated captions, which may not capture everything in the image - see Figure 6 for an example. This means that the text and image modalities can interact and transfer information in subtly complex ways, making measurement significantly more challenging.

To resolve this challenge, we instead propose to measure whether the ground truth objects in an image can be predicted from the representation of its caption. We rely on the intuition that the caption of an image typically does not include all its objects, and hence high confidence recovery of this level of detail implies some form of memorization. If this prediction can be done significantly more accurately when the image is in the training set of a model than when it is in the test, then the image-text pair is being memorized by the said model.

**Definition 1** (_Deja vu_ Memorization): _A vision-language model \(f\) suffers from deja vu memorization if it retains specific information about the individual training images that allows the recovery of objects present in the training images. In other words, for a target image-text pair \(z= z_{img},z_{txt}\), more unique objects can be recovered from \(z_{img}\) given \(z_{txt}\) when \(z\) is present in \(f\)'s training set compared to when it is not._

This is possible due to the model's ability to encode the individual objects in the image embeddings, which is in turn reflected in the corresponding text embeddings when the model minimizes the contrastive loss during training. Next we will discuss how we quantify this phenomenon using two separate models (a target and a reference) as well as a nearest neighbor test.

### Measurement Methodology

Since VLMs are meant to capture general correlations between images and their text captions, our goal is to differentiate the recovery of ground-truth objects due to _deja vu_ memorization from dataset-level correlations alone. As a motivating example, consider the use of CLIP in a cross-modal retrieval task, where images are retrieved from a web-scale database given text. We wish to capture the degree of surprise in the retrieval result when the model memorizes training captions, i.e. how many objects can the model recover beyond dataset-level correlation? To enable this evaluation for a given image-text pair \(z= z_{img},z_{txt}\), we use two separate VLMs \(f_{A}\) and \(f_{B}\) that are trained on randomly sampled but disjoint data sets \(A\) and \(B\) respectively. \(z\) lies in the training set of _exactly one_ of these models, and hence by comparing the outputs of the two models, we can infer whether \(z\) was memorized. We do a \(k\)-nearest neighbor test using a separate public set of images as described in Algorithm 1 and find the subset of images that are closest to \(z\) in the representation space. We then decode the objects present in these images. For this we use an object detector to provide ground-truth annotations for measuring the precision and recall of object recovery. We note that while there will always be some bias when using object detectors, human or automated, this bias should not affect our evaluation when considering the gap between the two models. This is because the object detector is not trained on the same training set as the VLM, hence any incurred bias should be independent of the trained VLMs.

### Metrics

Our memorization metrics are built bottom-up from our notion of deja vu memorization for VLMs. We start from fine-grained _sample-level metrics_ to more aggregate _population-level metrics_. The \(k\)-nearest neighbor test in Algorithm 1 shows how to obtain predictions of the ground-truth objects given an image; we next use these predictions to develop the population-level and sample-level memorization metrics. For our evaluation, we adopt the precision, recall and F-score metrics from the information retrieval literature to quantify the fraction of objects memorized by the models.

Sample-level metrics.At the sample level, we evaluate the fraction of ground-truth objects memorized by the target model from a given training image-text pair \(z= z_{img},z_{txt}\). To do this, we run the nearest neighbor test on both the target and reference models, \(f_{A}\) and \(f_{B}\), to obtain their respective neighbor sets \(N_{A}\) and \(N_{B}\) as per Algorithm 1. We then calculate the _precision_, _recall_ and _F-score_ values when identifying the ground truth objects present in \(z_{img}\) using \(N_{A}\) and \(N_{B}\) and report the gap between the respective values for both the models. A positive gap corresponds to the target model memorizing the training sample and the magnitude of the gap indicates the degree of memorization. The precision, \(\), and recall, \(\), are given by the following equations (\( i\{A,B\}\)):

\[(z,f_{i})=N_{i} z_{img}}{ \#N_{i}},(z,f_{i})=N_{i} z_{img}}{ \#z_{img}}.\] (2)

F-score is the harmonic mean of precision and recall.

**Population-level metrics** measure what fraction of the training data is memorized by a model. For proper measurement, we propose three metrics: _population precision gap_ (PPG), _population recall gap_ (PRG) and _AUC gap_ (AUCG). Given the notations defined in Algorithm 1, the population precision gap is the the fraction of data points from \(A\) where \(f_{A}\) has a higher precision in identifying the ground truth objects than \(f_{B}\) minus the fraction of data points where \(f_{B}\) has a higher precision in identifying the ground truth objects than \(f_{A}\). If no memorization occurs, models \(f_{A}\) and \(f_{B}\) should be interchangeable and hence this gap is zero. Formally,

\[=|\{z A:(z,f_{A})>(z,f_{B})\}|-|\{z A:(z,f_{A})<(z,f_{B})\}| ,\] (3)

where \(|A|\) denotes the size of the set \(A\) and \((z,f_{A})\) measures the precision of object prediction on \(z\) given the model \(f_{A}\) as defined in Equation 2. We define the population recall gap similarly:

\[=|\{z A:(z,f_{A})>(z,f_{B})\}|-|\{z A:(z,f_{A})<(z,f_{B})\}| .\] (4)

We also visualize the fine-grained cumulative recall distribution of both the models over the training set as shown in Figure 3. This gives us a better understanding of what fraction of objects are recovered overall. We then measure the difference between the two distributions (i.e., for \(f_{A}\) and \(f_{B}\)) to simplify this information into a single quantity we call AUC gap.

While both the population-level and sample-level metrics rely on the precision and recall functions, they have subtle differences. First, population-level metrics measure the aggregate memorization over the entire training set whereas sample-level metrics measure the memorization in individual training samples. Second, population-level metrics rely on binary tests to differentiate between the target and reference models and as such do not capture the magnitude of the gap between the models as is done by the sample-level metrics. We define both sets of metrics to capture the memorization at different granular levels and to be actionable in a meaningful way, thereby allowing the model developers to fine-tune the models to mitigate the memorization risk.

## 4 Evaluating _Deja vu_ Memorization

We next apply the metrics designed in Section 3.1 to determine if CLIP memorizes training data. Specifically, we seek to answer the following two research questions:

1. How does deja vu memorization vary with training set size and number of training epochs?
2. Are all training data points memorized uniformly?

Models and datasets.We train OpenCLIP from scratch on different datasets, including Shutterstock (a privately licensed data set of 239M image-captions pairs) and \(\)filtered LAION [Radenovic et al., 2023] + COCO [Lin et al., 2014]). We sample up to 50M image-text pairs from the data sets and train OpenCLIP models with ViT-B-32 architecture. For Shutterstock experiments, we consider a separate set of 20M samples from Shutterstock (called SS-20M), with no overlap with the training sets, as public set. For the filtered LAION experiments, we consider two public sets: (a) a separate subset of 50M samples from filtered LAION (called filtered LAION-50M) with no overlap with the training sets, and (b) the entire ImageNet training set [Deng et al., 2009]. More details on the experiment setup and how we obtain data subsets can be found in Appendix C.

Model utility.As mentioned above (and also discussed in detail in Appendix C), we trained models with different training set sizes consisting of 1M/10M/50M image-text pairs from filtered LAION and 1M/10M/40M image-text pairs from Shutterstock. We use zero-shot performance on ImageNet to evaluate the utility of these models. Figure 2 shows the zero-shot accuracy on ImageNet. Additional utility benchmarks across various ARO (Attribution, Relation, and Order) tasks [Yuksekgoul et al., 2023] can be found in Figure 7 in the appendix.

Figure 3: Object recall distribution of target and reference models trained on filtered LAION data set for 200 epochs with different training sizes. ImageNet is used as the public set for kNN test.

Figure 2: Utility and _deja vu_ memorization of ViT-B-32 CLIP models with varying training set sizes. Model utility is quantified in terms of ImageNet zero-shot accuracy. Population-level memorization of models is measured using the metrics defined in Section 3.2 over various public sets _(a)_: training set sampled from filtered LAION and ImageNet is used as public set. _(b)_: training set sampled from filtered LAION and a holdout filtered LAION-50M set is used as public set. _(c)_: training set sampled from Shutterstock and a holdout SS-20M set is used as public set. For the memorization metrics, we report the _mean_\(\)_std_ values (_std_\(\) 0.003) over 100 repetitions of randomly sampling 10% of records with replacement.

### Measuring Population-Level Memorization

For quantifying population-level memorization, we measure the gap between the object recall distributions for the target and reference models. If there were no memorization, we would observe virtually no gap between the two distributions, i.e. AUCG = 0. Figure 3 shows the object recall distribution gap between the target and reference models trained on filtered LAION for varying training set sizes when ImageNet is used as the public set. When the training set size is small (e.g. 1M as shown in the left-most figure), there is a higher _deja vu_ memorization due to the models overfitting on the training set. The gap decreases as the training set size increase from 1M up to 50M, confirming that the models begin to generalize better. Note that the memorization is still significant for models trained on 10M data set. We consider this setting for further experiments as this is a typical training set size for many foundation models in practice (Ilharco et al., 2021). For instance, it is common to train CLIP models on the 12M Conceptual Captions data set (Sharma et al., 2018) or the 15M subset of the YFCC data set (Thomee et al., 2016).

Apart from the AUCG (AUC gap) metric, we also quantify the gap in terms of the PPG (population precision gap) and PRG (population recall gap) metrics. Recall that a positive value for these metrics indicates memorization and the magnitude indicates the degree of memorization. Figure 2 shows the PPG, PRG and AUCG metric values for models trained on filtered LAION and Shutterstock with different training set sizes; using ImageNet and filtered LAION-50M public sets for the filtered LAION models and SS-20M public set for the Shutterstock models. Recall that the public sets have no overlap with the model training sets. While the absolute metric values are different for different public sets, the trend remains the same: memorization decreases with increasing training set size as the models begin to generalize better. In Section 5, we explore various approaches to reduce this memorization.

### Measuring Sample-Level Memorization

While the population-level metrics like AUCG, PPG and PRG show evidence of memorization, they do not pinpoint which training images are more vulnerable. We sort the training data in decreasing order of memorization to show the subset of most vulnerable records. To do this, we explore several sorting metrics. The most straightforward metric is the distance between the training text embedding and the nearest neighbour public image embeddings obtained using Algorithm 1. The records for which the public image embeddings are the closest are more easily memorized by the model.

Figure 4: Sample-level memorization gap between target and reference models when predicting top-10 objects for different top-\(L\) records. Models are trained on disjoint 10M subsets of filtered LAION data set for 200 epochs and ImageNet public set is used for the KNN test. The model exhibits very strong _dejÃ  vu_ memorization on a small subset of samples, as indicated by the large precision/recall/F-score gaps when \(L\) is small.

Compared to the population-level memorization, where we keep the experiments parameter-free to the best extent, at the sample-level we want to focus on more fine-grained leakage so we choose top-10 object labels to measure the gap instead of predicting all the objects.

Figure 4a shows the precision, recall and F-score gaps between the target and reference models for varying top-\(k\) records sorted with respect to this distance metric where ImageNet is used as the public set. As shown, the gaps can be greater than 0.3 for top-1 and top-10 records. We also tried sorting the records in the decreasing order of the number of objects correctly identified using the target model with the nearest neighbor test. Figure 4b shows the precision, recall and F-score gaps for the records sorted using this metric. We see that the gap can become very significant for the top-1 and top-10 records. Although this metric requires access to the ground truth labels, this is still useful to visualize the worst case examples. Results for sample-level memorization with filtered LAION-50M public set show a similar trend and can be found in Section D.1. Sample-level memorization results for Shutterstock experiments can be found in Appendix E.

Key Observations.We show _deja vu_ memorization at both population and sample levels. At the population-level, where we measure the aggregate memorization of model over the training set, we find that the memorization decreases with an increase in the training set size. This could be attributed to improved model generalization. At the sample-level, we note that the model memorizes disproportionately--a subset of training image-text pairs are memorized more than the others.

## 5 Mitigation

How can we mitigate _deja vu_ memorization in VLMs? Since it presumably happens due to the model overfitting on training data, it is likely that regularization techniques may be able to mitigate it. We investigate the impact of four regularization techniques on _deja vu_ memorization.

1. _Early stopping_ is a common technique for regularizing neural networks where model training is ended prematurely. It is effective due to the observation that models begin to overfit on the training set when they are trained for more epochs.
2. _Temperature_ is the contrastive loss parameter that controls how close the text and image embeddings can get during the model training. Changing the temperature parameter has a regularization effect for SSL as observed by Meehan et al. (2023).
3. _Weight decay_, also known as \(L_{2}\) regularization, is a standard ML regularization technique.
4. To reduce overfitting along the text and image modalities in VLMs, we look at additional regularization through _text randomization_, where we randomly mask a fraction of the text tokens during training. We control the fraction of text tokens masked using a _masking ratio_ parameter.

In the following we present results when ImageNet is used as the public set for the nearest neighbor test. Results for the filtered LAION-50M public set can be found in Section D.2. Since Shutterstock memorization trends are similar to those of filtered LAION, we only explore filtered LAION settings for mitigation.

Figure 5: Effect of mitigation on ViT-B-32 OpenCLIP models trained on 10M subset of filtered LAION. Memorization evaluation is done using ImageNet as public set. Default setting is highlighted with asterisk. For the memorization metrics, we report the _mean \(\) std_ values (_std \(\)_ 0.003) over 100 repetitions of randomly sampling 10% of records with replacement. Among these mitigations, text masking has the best trade-off that reduces memorization without sacrificing utility.

### Early Stopping

It is widely believed that deep learning models begin to overfit on the training data as the number of training epochs increases. It is thus a good practice to early stop the training as soon as the model utility on a hold-out test set stagnates or begins to drop. However this is often not the case for SSL models. It is not uncommon to observe that the zero-shot accuracy of SSL models keeps improving as the models are trained for longer [Meehan et al., 2023]. Regardless, we still explore early stopping as a mitigation mechanism. As shown in Figure 5, training the CLIP model for more epochs leads to better zero-shot accuracy, but at the same time, _deja vu_ memorization also increases. This is in line with our hypothesis above. Even when we early stop the model at 20 epochs (10% of the default parameter value of 200 epochs), the memorization risk is not completely mitigated although the absolute values are lower.

### Temperature Scaling

Temperature, or logit scale, controls how close the text and image embeddings can get during training. Smaller values allow for the multi-modal embeddings to get closer, and as a consequence the CLIP contrastive loss drops quickly, whereas larger values regularize the loss but may lead to training instability as noted by Radford et al. . The default value in OpenCLIP implementation is set to 100. We vary this value between 25, 100 and 200. As shown in Figure 5, decreasing the temperature (\(T\)) from 100 to 25 decreases the model's zero-shot classification accuracy on ImageNet from 25.2% to 21.7% and also increases the memorization as indicated by the increase in the PPG, PRG and AUCG metrics. This is due to the decrease in the distance between the text and image embeddings for the training data which could potentially lead to model overfitting. Increasing the temperature to 200 moderately impacts the model's zero-shot classification accuracy and the memorization leakage remains more or less the same.

### Weight Decay

Weight decay directly controls the model overfitting, with larger values corresponding to stronger regularization. The default value is set to 0.1 and we vary it between 0.03, 0.1 and 0.3. As expected, decreasing the weight decay \(wd\) from 0.1 to 0.03 decreases the model's zero-shot classification accuracy and also worsens the leakage due to memorization as shown in Figure 5. Interestingly, increasing the weight decay to 0.3 significantly improves the model's zero-shot accuracy. We believe that the default value of 0.1 is not optimal for the 10M training set size as it was set based on the model training for larger data sizes (possibly on the entire filtered LAION data set). With 0.3 weight decay, we observe a consistent decrease in the population memorization leakage, as shown by the PPG, PRG and AUCG values for \(wd=0.3\) in Figure 5, but the values are still significantly high. We also explored setting weight decay to 0.01 and 1.0, but they either adversely impacted the model utility or severely increased memorization. Thus while tuning \(wd\) does not completely mitigate memorization, we can get a reasonable trade-off in the neighbourhood of \(wd=0.3\).

### Text Randomization

During model training, the CLIP models increase the cosine similarity between the matching image-caption pairs while simultaneously decreasing the cosine similarity between mismatched pairs to reduce the contrastive loss. While it is common to augment the training images to reduce overfitting, the text captions are not randomized. This could lead to the model overfitting on the text captions when minimizing the contrastive loss. To avoid this, we propose text randomization as a defense. For COCO subset of the training set, we randomly choose one out of the five captions for each image per epoch during training. For filtered LAION subset, we randomly mask a fraction of caption tokens since only a single caption is available per image in the filtered LAION data set. We vary the masking ratio between 0 (no masking), 0.3 and 0.5 (randomly mask half of the tokens).

We find this defense to work the best in mitigating deja vu memorization but at the cost of ImageNet zero-shot accuracy. As shown in Figure 5, using a masking ratio of 0.3 reduces the ImageNet zero-shot accuracy from 25.2% (in the default case when \(mr=0.0\)) to 24.1%, but at the same time this significantly reduces memorization. The PPG metric reduces from 9.1% to 3.4%, and the PRG metric reduces from 9.2% to 3.8%. Moreover, the recall CDF gap (AUCG) also reduces from 0.034 to 0.013. Further increasing the masking ratio to 0.5 mitigates the risk even more. PPG reduces to3.0%, PRG reduces to 1.9%, and AUCG reduces to only 0.007. However, we note that text masking has a positive impact on ARO benchmark utility as shown in Figure 7. This is because masking avoids overfitting on specific text tokens making the models less likely to behave like bag-of-words. Thus text masking achieves the best utility trade-offs. We would expect a significant drop in the model utility if we further increase \(mr\) since the captions would have considerably less information.

Key Observations.We study the impact of tuning four regularization parameters: number of training epochs, temperature, weight decay and masking ratio. We find that early stopping reduces memorization but at the cost of model utility. Increasing the temperature increases the model zero-shot accuracy and decreases memorization up to a certain threshold, beyond which the model utility begins to decrease. Surprisingly, we find that the default value of 100 already gives the optimal results. Similar to temperature, increasing the weight decay increases the model utility and decreases the memorization up to a certain threshold. We find 0.3 weight decay to achieve the best results for a model trained over 10M data. We observe a sharp decrease in model utility beyond this value. Text masking seems to be most effective in mitigating memorization. Increasing the masking ratio decreases memorization and also decreases the model utility. Masking ratio of 0.3 achieves a good trade-off by significantly reducing memorization while only moderately impacting the model utility.

## 6 Discussion

Prior works have mainly shown memorization in the uni-modal setting: either for the language models (Carlini et al., 2019) or for vision models (Meehan et al., 2023). We have demonstrated that even in the complex multi-modal setting, ML models suffer from memorization. Moreover, while prior works have only evaluated memorization for small training data sizes (typically on the scale of 1 million or less), we show memorization on a wide scale, from 1 million to 50 million training set size. Our experiments show that while the population-level memorization metrics decrease with increase in the training set size, there remain strongly memorized examples as exemplified by the sample-level memorization where the model disproportionately memorizes a subset of records.

Careful tuning of right hyper-parameters can, however, mitigate this memorization risk. We propose a suite of metrics to quantify _deja vu_ memorization in hope of guiding ML practitioners to train models in a safe way. These metrics not only quantify the risk in a meaningful and interpretable manner, but are also sensitive to the tuning of the mitigation parameters, thereby aiding the practitioners in choosing the right model hyper-parameter values that achieve a good utility-risk trade-off.

Below we discuss some detailed discussions and limitations of our work.

Not applicable to out-of-box models.Since our tests require access to two models, _target_ and _reference_, along with the underlying training set, we note that this can not be directly applied to measure memorization in out-of-the-box pre-trained models as there is no reference model for such cases. We leave this case as a future work.

Distinguishing memorization from learning.A model can memorize and generalize (or learn) at the same time. This can happen at a sub-population level, where the model memorizes rare concepts and generalizes to common concepts, or even at a sample level, where memorization is required for learning rare concepts as theorized in Feldman (2020). _Deja vu_ memorization is meant to go beyond this, and instead examine when a model that is trained on an image with a generic caption (i.e., they do not describe the image in high detail), memorizes many small details about the associated image (i.e., what objects are present in the image) when given the caption. In other words, we define _deja vu_ memorization as what can be inferred about the training image from its caption beyond simple correlations, which can happen through both learning and memorization in the traditional sense.

Extending beyond objects.While our approach is also applicable to annotations that go beyond objects, this is not in the scope of this work. Even in this setting, the prior state-of-art approach (Meehan et al., 2023) only considers a single object label per image (ImageNet) and none of the prior works consider a. multimodal setting, b. large training size sizes, and c. multiple objects per image.

Relation to Overfitting._Deja vu_ memorization measures overfitting at a more granular level--instead of a binary decision, it measures to what _degree_ the model overfits a training sample.