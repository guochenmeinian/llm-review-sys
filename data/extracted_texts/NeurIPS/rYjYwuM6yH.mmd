# 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability

Baohao Liao\({}^{1,2}\)&Christof Monz\({}^{1}\)

\({}^{1}\)Language Technology Lab, University of Amsterdam

\({}^{2}\)eBay Inc., Aachen, Germany

Code: https://github.com/BaohaoLiao/road

Correspondence to b.liao@uva.nl. Please go to https://arxiv.org/abs/2409.00119 for the newest version.

###### Abstract

Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with \(<0.1\%\) trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.

## 1 Introduction

Large language models (LLMs), trained on extensive web-scale datasets to perform tasks such as predicting masked words  or anticipating the next word in a sentence ,

Figure 1: Performance of various PEFT methods on the GLUE benchmark, eight commonsense reasoning tasks and four arithmetic reasoning tasks with RoBERTa-large or LLaMA-13B.

demonstrate remarkable effectiveness across a range of NLP applications. For tasks where the data distribution diverges from that of the pretraining corpus, finetuning emerges as an effective way to tailor an LLM to specific requirements. Leveraging the capabilities of LLMs, recent studies [13; 14; 22; 23; 25; 27; 42; 60; 62; 65] demonstrate that training only a subset of an LLM's parameters can yield performance on par with full finetuning. This approach, termed parameter-efficient finetuning (PEFT), provides two primary advantages: (1) It reduces the storage requirements for trained parameters, as it necessitates preserving only a universal LLM alongside a minimal set of task-specific parameters; (2) It decreases GPU memory consumption during finetuning, owing to the reduction in optimizer state sizes which correlate directly with the number of trainable parameters.

With the evolution of PEFT, concerns extend beyond mere parameter efficiency. PEFT encounters a variety of challenges brought forth by diverse applications. A significant challenge is the efficient deployment of personalized or task-specific LLMs [25; 57]. These applications frequently require distinct sets of trained parameters for different tasks or users. When multiple users submit requests simultaneously, it becomes crucial to process these requests collectively in a single batch. Given that each request may require a unique set of parameters, using batch matrix multiplication can efficiently handle these requests by leveraging GPU parallelism. However, the batch matrix multiplication still incurs considerable overhead [1; 57], necessitating the exploration of more efficient methods.

Another challenge is the interpretability of LLMs that contain a billion-scale of parameters, making it difficult to explore their mechanism. PEFT provides an alternative approach by constraining the number of trainable parameters, thereby aiding in interpretability. Recent advancements in PEFT methods, particularly those focusing on representation editing [54; 60; 67], can be incorporated within an intervention framework . This integration enhances their capability for interpretability, offering a more manageable means of dissecting the operational intricacies of LLMs.

In this paper, we introduce a novel technique termed 2D rotary adaptation (RoAd) which efficiently adapts LLMs using a minimal number of trainable parameters. Furthermore, RoAd enhances both batching efficiency and composability. Our initial investigation reveals that finetuning primarily alters the angular components of the representations in pretrained LLMs, rather than their magnitudes (Section SS3.1). Based on this observation, we employ a strategy of rotating certain subspaces within the representations to emulate finetuning effects. Specifically, we implement a 2D rotational approach on the representations and develop three distinct variants of RoAd (Section SS3.2).

To assess the efficacy of RoAd, we perform comprehensive evaluations on the GLUE benchmark , eight commonsense reasoning tasks and four arithmetic reasoning tasks, utilizing RoBERTa  and LLaMA [52; 53] (Section SS4.1). The results consistently show that RoAd surpasses other PEFT methods while maintaining a significantly reduced scale of trainable parameters (\(<0.1\%\)), as depicted in Figure 1. Additionally, RoAd employs element-wise rather than matrix multiplication, which notably improves throughput when serving heterogeneous requests within the same batch, achieving twice the throughput of LoRA  (Section SS4.2). Furthermore, RoAd can be seamlessly integrated within an intervention framework , thereby enhancing model interpretability. We illustrate this through a composition experiment, demonstrating RoAd's capacity to merge weights trained for different tasks and display a new capability (Section SS4.3).

## 2 Background

In this section, we outline the challenges tackled in this work, illustrating the constraints of existing methods and objectives that drive the development of the proposed method, RoAd.

### Parameter-efficient finetuning (PEFT)

Existing PEFT techniques can be categorized into three groups: adapter-based, prompt-based, and latency-less methods. Adapter-based methods [12; 13; 42] incorporate adapters either in parallel with or sequentially to the existing Transformer  modules. This incorporation necessitates modifications to the LLM architecture, consequently adding extra latency during inference. Prompt-based methods [19; 21; 43] enhance the input by appending new trainable tokens, which lengthens the sequence and thereby increases the computational overhead during inference. Latency-less methods, such as LoRA  and its variants [22; 27; 65], apply low-rank matrices to adapt the pretrained weights. These matrices can be seamlessly integrated into the existing weight matrices following finetuning, thus preserving the original LLM architecture. Specifically, LoRA adapts an LLM as \(=^{0}+\), where \(^{0}^{d_{1} d_{2}}\) is the pretrained weight and \(=\) with \(^{d_{1} r}\), \(^{r d_{2}}\), \(r d_{1}\) and \(r d_{2}\). Our proposed method, RoAd, aligns with the latency-less category and integrates effortlessly into the existing linear layer without imposing additional overhead during inference. Moreover, RoAd demonstrates exceptional parameter efficiency. The quantity of its trainable parameters is equivalent to that of a LoRA module with a rank \(r=0.5\).

**Orthogonal finetuning.** Drawing on the concept of hyperspherical energy and its role in characterizing generalization [28; 29], OFT  introduces orthogonal finetuning, an effective PEFT method for finetuning text-to-image diffusion models. Specifically, OFT implements an orthogonal matrix \(^{d_{1} d_{1}}\) to the pretrained weight \(^{0}\), so the input \(^{d_{1}}\) to a linear layer after adaptation becomes \(=(^{0})^{}\). \(\) is parameter-efficient because it is a block-diagonal matrix with \(n\) blocks as \(=(_{1},...,_{i},...,_{n})\), where each block \(_{i}^{w w}\) has a dimension \(w=d_{1}/n\). To maintain orthogonality, \(_{i}\) is derived using Cayley parameterization: \(_{i}=(+_{i})(-_{i})^{-1}\) with \(_{i}^{w w}\) being a skew-symmetric matrix (\(_{i}=-_{i}^{}\)). In sum, \(\{_{i}\}_{i=1}^{n}\) serve as the trainable parameters and \(\) is constructed from them with Cayley parameterization. Subsequent advancement, BOFT , leverages butterfly factorization to further refine OFT's parameter efficiency. However, both OFT and BOFT, due to their reliance on matrix inversions in the Cayley parameterization and increased storage of intermediate activations, necessitate additional GPU memory and increase training duration compared to other PEFT approaches. Conversely, RoAd, which may be considered as a specialized case of OFT with \(w=2\), offers a faster and more memory-efficient solution by inherently maintaining orthogonality without requiring further parameterization.

### Batching

Batching in this context refers to processing multiple heterogeneous requests, each requiring different adapters2 for inference. This scenario commonly arises when serving personalized or task-specific LLMs. Specifically, we consider a setup where distinct adapters instead of a shared adapter are finetuned for various tasks to achieve optimal performance. During inference, each request in a batch pertains to a different task and necessitates a unique adapter.

Consider that we have finetuned distinct LoRA modules for \(b\) tasks, denoted as \(\{_{i},_{i}\}_{i=1}^{b}\). For a batch of \(b\) requests represented as \(^{b l d_{1}}\), where \(l\) is the maximum sequence length across the requests, each request requires a different LoRA module. To exploit the parallel processing capabilities of GPUs, the output \(\) of a linear layer can be computed as follows: First, the output from the pretrained layer is computed as \(^{0}=torch.mm(,^{0})\). Subsequently, the intermediate output from the first low-rank matrix, \(}^{b d_{1} r}\) (a concatenation of \(\{_{i}\}_{i=1}^{b}\)), is obtained as \(_{0}^{1}=torch.bmm(,})\). The output from the second low-rank matrix, \(}^{b r d_{2}}\) (a concatenation of \(\{_{i}\}_{i=1}^{b}\)), follows as \(^{1}=torch.bmm(_{0}^{1},})\). Finally, these outputs are summed to produce \(=^{0}+^{1}\). It is noteworthy that batch matrix multiplication (BMM), as implemented in \(torch.bmm\), often introduces substantial overhead , reducing throughput and increasing latency, which adversely impacts user experience in time-sensitive applications.

In contrast, prompt-based methods circumvent the use of BMM by appending trainable tokens to each request, simplifying the computational process. However, prompt-based methods with long prompt tokens are difficult to optimize, which degrades performance compared to other PEFTs [14; 15]. (IA)3 proposes adapting LLM by multiplying the output from a linear layer with a trainable vector, involving only element-wise multiplication for efficient batching. A recent development, FLoRA , builds on (IA)3 by employing two low-rank matrices while maintaining element-wise operations. Although our proposed method, RoAd, requires BMM, its sparse structure allows a reformulation of BMM and results in an overhead equivalent to element-wise multiplication.

### Intervention and composability

Numerous studies [10; 11; 37; 38; 40] have provided support for the linear representation hypothesis [35; 46; 49] that concepts are represented within linear subspaces of neural network representations. To examine if a concept is captured within a linear subspace of a representation, Geiger et al. suggests employing a distributed interchange intervention (DII) defined as:

\[(,,)=+^{}(-)\] (1)

\(\) denotes the hidden representation generated at row \(i\) and column \(k\) when the model processes an input, while \(\) represents the corresponding representation when the model processes a different input. The matrix \(^{r d_{1}}\), consisting of orthogonal rows, serves as a low-rank projection matrix where \(d_{1}\) is the dimension of the representation and \(r\) is the subspace dimension under intervention. Equation (1) illustrates the application of a DII to \(\) using a counterfactual source representation \(\).3

Drawing inspiration from this established framework, a recent study, LoReFT , introduces a method for finetuning specific positions of the representations to adapt LLM. This study further demonstrates that several prior approaches of representation editing [60; 67; 54] can be effectively integrated within this framework. Interestingly, the application of RoAd to representations can also be conceptualized as DII, offering interpretability potential. To demonstrate one aspect of interpretability for RoAd, we primarily conduct a qualitative experiment focused on task composition. This experiment involves combining the weights of models trained on distinct tasks to showcase the capability for multitasking learning without the need for additional adaptation [61; 66; 64; 20; 16].

## 3 Method

In this section, we first perform two pilot studies to ascertain the key factor influencing the adaptation of LLMs. Following this, we present our proposed method, the 2D rotary adaptation (RoAd), which serves as an effective PEFT method addressing the various challenges outlined in Section SS2.

### Pilot study

**Study 1: Variations in magnitude and angular displacement.** Assume \(^{0}\), \(^{d_{1}}\) are representations of the same token from a pretrained and finetuned LLM, respectively. We define the relative change in magnitude as and compute the angular displacement as \( D=(,^{0})[-1,1]\). A larger \( M\) and a smaller \( D\) indicate more significant changes in magnitude and angular displacement, respectively. Our study involves: (1) finetuning RoBERTa-base  on the SST-2 task  using either full finetuning or LoRA; (2) extracting representations \(^{0}\) and \(\) from the output of the second-last Transformer block for the [CLS] token across all samples in the development set, followed by computing \( M\) and \( D\).4 As depicted in Figure 2 (Left and Middle), there is a more pronounced change in \( D\) than in \( M\) for both full finetuning and LoRA.5

**Study 2: Disentanglement of magnitude and angle.** To ascertain whether angular or magnitude adjustments are more critical for finetuning, we implement a disentanglement study. This involves freezing RoBERTa-base and appending a two-layer classifier on top of it. The first layer of this

Figure 2: Pilot study for the pretrained and finetuned representations. **Left & Middle**: The change in magnitude and angle of representations between pretrained and finetuned LLM using full finetuning or LoRA. **Right**: The disentanglement experiment of magnitude and angle of pretrained representation.

classifier incorporates a weight matrix \(^{d_{1} d_{1}}\). Under standard operations, the output from this layer is computed as \(=^{}^{0}\). To distinctly evaluate the impacts of magnitude and angle, we modify the output to retain only the magnitude component as \(z_{i}=\|_{:,i}\|_{2}\|^{0}\|_{2}\), or solely the angular component as \(z_{i}=(_{:,i},^{0})\) (\(z_{i}\) is the \(^{}\) element of \(\)). The modified classifier was then finetuned on four GLUE tasks with different metrics detailed in Table C.1. Additionally, a weak baseline employing a randomly initialized RoBERTa-base is included. As shown in Figure 2 (Right), angular information is paramount in finetuning, whereas reliance solely on magnitude information even leads to inferior results compared to the random backbone.

Both studies indicate that angular information is more crucial than magnitude information for adapting a pretrained LLM to a downstream task. However, rotating the entire \(d_{1}\) dimensions of the representation for finetuning incurs substantial computational costs. These costs are primarily reflected in a large number of trainable parameters, necessitating a dense matrix \(^{d_{1} d_{1}}\), and in the requirement to maintain its orthogonality. Could we only rotate a subspace of the representation and design a \(\) that is always orthogonal without any parameterization as OFT ? The first idea that comes to our mind is 2D rotation which only rotates two dimensions at a time and inherently maintains orthogonality.

### 2D rotary adaptation

Suppose that \(^{0}^{d_{1} d_{2}}\) is the pretrained weight of a linear layer, \(^{d_{1}}\) is the input of a token to this linear layer, \(^{d_{2} d_{2}}\) is the rotation matrix, the adapted output from the linear layer is \(==(^{0})\). The rotation matrix \(\) is defined as follows:

\[=(_{1},_{2},...,_{d_{2}/2})_{i}=_{i}&-_{i}\\ _{i}&_{i}\] (2)

The trainable parameters are denoted as \(\{_{i}\}_{i=1}^{d_{2}/2}\). This 2D rotary adaptation involves rotating pairs of adjacent dimensions of \(\), specifically dimensions \(2i-1\) and \(2i\), using the rotation matrix \(_{i}\).6 The rotation matrix \(\) is characterized by its parameter efficiency, which is attributed to its sparse structure and the parameter sharing within each block \(_{i}\). Additionally, \(\) can be integrated directly into the existing pretrained weights, forming \(=^{0}^{}\), which does not incur additional computational costs during inference. This design closely mirrors RoPE , with the notable difference that in our RoAd, \(_{i}\) is trainable and \(_{i}\) does not incorporate positional information. The overview of RoAd is shown in Figure 3.

**Relaxation to orthogonality.** Referring to Figure 2 (Right), while reliance predominantly on angular information substantially outperforms reliance on magnitude information, it remains less effective than using both angular and magnitude information for the tasks of MRPC, STS-B, and CoLA. Furthermore, both fully- and LoRA-finetuned LLMs exhibit slight adaptations in magnitude, as depicted in Figure 2 (Left and Middle). Consequently, we modify \(_{i}\) by incorporating \(_{i}\) to regulate the magnitude. We define a general \(_{i}\) as follows:

\[_{i}=_{i,11}_{i,11}&-_{i,12} _{i,12}\\ _{i,21}_{i,21}&_{i,22}_{i,22}\] (3)

We develop three variants of RoAd by altering the configuration of shared parameters as outlined in Table 1. RoAd\({}_{1}\) introduces a minimal change to Equation (2) by incorporating a scaling factor \(_{i}\). RoAd\({}_{1}\) already shows impressive results for most tasks in Section SS4.1. For some knowledge-intensive tasks, we observe that RoAd\({}_{2}\) and RoAd\({}_{4}\) obtain better results with more trainable parameters. To preserve the starting point of LLMs , we always initialize \(_{i}=1\) and \(_{i}=0\).

**Batching.** In practice, we don't need to save \(\) as a sparse matrix and do matrix multiplication. Taking RoAd\({}_{1}\) as an example in Equation (4), we only save two vectors: \(^{1}\) and \(^{2}\). Then \(==^{1}+^{2}}\), where \(}\) is a rearranged version of \(\) and \(\) denotes element-wise multiplication. This reformulation not only simplifies the representation of \(\) but also enhances the efficiency of batching in RoAd, relying solely on element-wise multiplications rather than BMM.

\[= =^{1}+^{2}}\] (4) \[=[_{1}_{1}\\ _{1}_{1}\\ _{2}_{2}\\ \\ _{d_{2}/2}_{d_{2}/2}][ []{c}h_{1}\\ h_{2}\\ h_{3}\\ h_{4}\\ \\ h_{4}^{2}-1\\ h_{4_{2}}]+[_{1}_{1} \\ _{1}_{1}\\ _{2}_{2}\\ _{2}_{2}\\ \\ _{d_{2}/2}_{d_{2}/2}][ []{c}-h_{2}\\ h_{1}\\ h_{3}\\ \\ -h_{d_{2}}\\ h_{4_{2}-1}]\]

**Composability.** RoAd can be incorporated into the DII framework as \(()==+(-^{})\), with \(\) in Equation (1) being set to \(\). Although a degree of relaxation is introduced to the orthogonality of \(\), it is important to note that the rows of \(\) remain orthogonal to each other within non-adjacent segments of the same block, \(_{i}\). This offers a possibility for composability. We can finetune some rows on one task and other orthogonal rows on another task. Since they are orthogonal to each other, these two tasks should minimally affect each other, and the combination of these rows after finetuning could bring new multitasking learning ability.

RoAd can be considered as a special case of OFT  with \(w=2\). However, it is much more parameter- and memory-efficient and faster. Please refer to Section SSD.1 for a detailed discussion.

## 4 Experiments

In this section, we begin by implementing RoAd to finetune various LLMs across three benchmarks. Subsequently, we illustrate its efficiency in batching processes and demonstrate its composability. Unless otherwise noted, RoAd is applied to all linear layers within the LLMs. All of our experiments are conducted on A100 80GB GPU with the frameworks, Transformers  and PEFT .

### Results on downstream tasks

**Natural language understanding (NLU).** We evaluate the effectiveness of RoAd on the GLUE benchmark  for its ability of NLU with RoBERTa  as the backbone. Unlike many previous works [14; 22; 23; 31; 65] that employ the GLUE development sets for both validation and testing, here we partition the development set into distinct validation and test subsets to mitigate the risk of overfitting. For comprehensive information regarding the split of the development set, the search space of hyperparameters, the optimal hyperparameter configurations, and other details crucial for reproducibility, please see Section SSC.1.

As shown in Table 2, RoAd\({}_{1}\) outperforms all other PEFT methods with \(<0.1\%\) trainable parameters for both sizes of RoBERTa on average, being the only PEFT method that matches or outperforms full finetuning. These results show that 2D rotation (with a few scaling) can efficiently adapt LLM.

**Commonsense reasoning.** In assessing the capacity of LLMA  for commonsense reasoning, we focus on eight representative tasks: BoolQ , PIQA , SIQA , HellaSwag , WinoGrande , ARC-e, ARC-c , and OBQA . The setting here contrasts with the NLU experiments where each task involves finetuning a separate LLM. Instead, we adopt a unified strategy by finetuning a single LLM across all tasks as delineated in Hu et al. . Such a setting is designed to mitigate overfitting and aligns more closely with real-world applications. Specifically, the training and test sets from these eight tasks are reformulated according to a predefined template, so all tasks can be trained or evaluated in a generative way. For all finetuning experiments on LLaMA, we follow a recipe in Table C.5 without extensive searching. Please see Section SSC.2 for more training details.

   RoAd\({}_{?}\) & \(_{i}\) & \(_{i}\) & \#Trainable \\ 
1 & \(_{i,11}=_{i,12}=_{i,21}=_{i,22}=_{i}\) & \(_{i,11}=_{i,12}=_{i,21}=_{i,22}=_{i}\) & \(d_{2}\) \\
2 & \(_{i,11}=_{i,12}\) & \(_{i,21}=_{i,22}\) & \(_{i,11}=_{i,12}\) & \(_{i,21}=_{i,22}\) & \(2d_{2}\) \\
4 & \(_{i,11}_{i,12}_{i,21}_{i,22}\) & \(_{i,11}_{i,12}_{i,21}_{i,22}\) & \(4d_{2}\) \\   

Table 1: A summarization of three RoAd variants.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

with LoRA, but not RoAd\({}_{2}\) or RoAd\({}_{4}\), as their primary design purpose is to increase the number of trainable parameters.

As shown in Table 6, with only 0.08% trainable parameters, RoAd\({}_{4}\) already achieves 96.9% of the accuracy of LoRA with 4.61% trainable parameters. By combining RoAd\({}_{1}\) with LoRA, we achieve the same performance as LoRA with only 1/4 of its trainable parameters. This demonstrates RoAd's excellent scalability when combined with LoRA.

### Efficiency results for batching

We commence by highlighting the significance of weight merging for PEFT. Among the approaches discussed in Section 84.1, only LoRA , DoRA , BOTT , OFT , BitFit , (IA)\({}^{3}\), and our proposed RoAd enable the integration of trainable parameters with pretrained parameters without incurring additional inference overhead. As an illustration, we consider LoRA both with and without weight merging to underscore this process's importance. Notably, the implementation of LoRA with merged weights effectively reverts to the original LLM. To assess throughput, we configure the system with a batch size of 1, generate 2048 tokens, and apply the LoRA modules across all linear layers. Figure 4 (Left) clearly illustrates that the unmerged LoRA exhibits a significantly smaller throughput compared to the merged LoRA. Additionally, it is evident that the throughput of the unmerged LoRA demonstrates only a weak correlation with the rank size, primarily due to the fact that the additional overhead is largely attributed to communication instead of computation.

Furthermore, to evaluate the throughput of batching, we establish a default batch size of 8, generate 2048 tokens, and set the LoRA rank to 8. Each request within the batch is heterogeneous, necessitating eight distinct sets of trainable parameters by default. We only compare to LoRA here, because other baselines have either a weaker performance on downstream tasks (BOTT, OFT, BitFit and (IA)\({}^{3}\)) or a smaller throughput than LoRA for batching (DoRA). As shown in Figure 4 (Middle and Right), RoAd significantly outperforms LoRA with variations in either the number of generated tokens or the number of heterogeneous requests. With an increasing number of distinct requests, the gap between LoRA and RoAd becomes even larger, which shows RoAd's unique advantage in efficient serving

### Qualitative results for composability

In our investigation of RoAd's ability to handle compositional tasks, we primarily engage in multilingual experiments similar to those conducted by Wu et al. . We use two training datasets: a new version of HellaSwag 7, which comprises 1K samples with prompts in English and completions in German, and a 1K-sample subset of the Ultrafeedback  dataset, which focuses on instruction following tasks in English. Contrary to the above experiments that adapt the outputs of the linear layer, here we instead adapt the representations from the 16\({}^{th}\) block of LLaMA-7B, treating RoAd as a DII method. Specifically, we only adapt/intervene the representation of the final token in the

  
**Method** & **\#Params.** & **GOA** & **SQA** & **VQAT** & **POPE** & **Avg.** \\  LoRA & 4.61\% & 62.4 & **68.5** & 56.9 & **86.0** & **68.5** \\ RoAd\({}_{4}\) & 0.08\% & 60.0 & 66.9 & 53.3 & 85.5 & 66.4 \\ RoAd\({}_{1}\) + LoRA & 1.19\% & **62.5** & 68.2 & **57.4** & 85.8 & **68.5** \\   

Table 6: Visual instruction tuning results on LLaVA1.5-7B.

Figure 4: Comparison of throughput between LoRA and RoAd. **Left**: The influence of weight merging for LoRA. **Middle**: The influence of the number of generated tokens. **Right**: The influence of the number of heterogeneous requests in a batch.

prompt using RoAd\({}_{1}\). We train the upper half of \(\), i.e. \(\{_{i}\}_{i=1}^{d_{2}/4}\), to handle the German completions in HellaSwag, and another half to complete the English sentences in Ultrafeedback. Both tasks are simultaneously trained but utilize distinct subspaces of \(\). We train the model over five epochs with a learning rate of \(5e-3\) and a batch size of 8.8

As in Figure 5, both LoReFT and RoAd are unable to perform completions with the German subspace. This limitation is anticipated due to two primary reasons: (1) LLaMA-7B predominantly relies on pretraining from English datasets, and doesn't have a cross-lingual answering ability without explicitly prompting. (2) The HellaSwag dataset is relatively small, containing only 1K samples with limited comprehensive coverage. Despite these constraints, the German subspace effectively prompts the model to produce sentences in German. Additionally, both methods achieve accurate completions in the other half of the subspaces, attributed to LLaMA-7B's extensive knowledge base in English. When these two subspaces are combined, RoAd successfully leverages their strengths, facilitating accurate sentence completions in German, while LoReFT doesn't catch the purpose of the prompt. We offer more examples, including negative examples, in Figure D.1, D.2 and D.3.

## 5 Conclusion

Initially, our research examines how finetuning modifies the representation of pretrained LLMs, finding that angular adjustments are more significant than changes in magnitude scale. Leveraging this insight, we propose a PEFT method, RoAd, which primarily utilizes a 2D rotational adjustment to the representation. Despite its simplicity, RoAd exhibits several distinct advantages: (1) It is exceptionally efficient in terms of parameters, consistently delivering superior performance on downstream tasks with the fewest trainable parameters compared to other PEFT methods; (2) RoAd efficiently supports batch processing, achieving twice the throughput of LoRA; (3) When incorporated within an intervention framework, RoAd demonstrates remarkable composability.

Due to page limit, we discuss the limitations and broader impacts in Section SSA and SSB, respectively.