# On the Mode-Seeking Properties of

Langevin Dynamics

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The Langevin Dynamics framework, which aims to generate samples from the score function of a probability distribution, is widely used for analyzing and interpreting score-based generative modeling. While the convergence behavior of Langevin Dynamics under unimodal distributions has been extensively studied in the literature, in practice the data distribution could consist of multiple distinct modes. In this work, we investigate Langevin Dynamics in producing samples from multimodal distributions and theoretically study its mode-seeking properties. We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is unlikely to find all mixture components within a sub-exponential number of steps in the data dimension. To reduce the mode-seeking tendencies of Langevin Dynamics, we propose _Chained Langevin Dynamics_, which divides the data vector into patches of constant size and generates every patch sequentially conditioned on the previous patches. We perform a theoretical analysis of Chained Langevin Dynamics by reducing it to sampling from a constant-dimensional distribution. We present the results of several numerical experiments on synthetic and real image datasets, supporting our theoretical results on the iteration complexities of sample generation from mixture distributions using the chained and vanilla Langevin Dynamics.

## 1 Introduction

A central task in unsupervised learning involves learning the underlying probability distribution of training data and efficiently generating new samples from the distribution. Score-based generative modeling (SGM) (Song et al., 2020c) has achieved state-of-the-art performance in various learning tasks including image generation (Song and Ermon, 2019, 2020; Ho et al., 2020; Song et al., 2020a; Ramesh et al., 2022; Rombach et al., 2022), audio synthesis (Chen et al., 2020; Kong et al., 2020), and video generation (Ho et al., 2022; Blattmann et al., 2023). In addition to the successful empirical results, the convergence analysis of SGM has attracted significant attention in the recent literature (Lee et al., 2022, 2023; Chen et al., 2023; Li et al., 2023, 2024).

Stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011), as a fundamental methodology to implement and interpret SGM, can produce samples from the (Stein) score function of a probability density, i.e., the gradient of the log probability density function with respect to data. It has been widely recognized that a pitfall of SGLD is its slow mixing rate (Wooddard et al., 2009; Raginsky et al., 2017; Lee et al., 2018). Specifically, Song and Ermon (2019) shows that under a multi-modal data distribution, the samples from Langevin dynamics may have an incorrect relative density across the modes. Based on this finding, Song and Ermon (2019) proposes _anneal Langevin dynamics_, which injects different levels of Gaussian noise into the data distribution and samples with SGLD on the perturbed distribution. While outputting the correct relative density across modes can be challenging for SGLD, a natural question is whether SGLD would be able to find all the modes of a multi-modal distribution.

In this work, we study this question by analyzing the mode-seeking properties of SGLD. The notion of mode-seekingness (Bishop, 2006; Ke et al., 2021; Li and Farnia, 2023) refers to the property that a generative model captures only a subset of the modes of a multi-modal distribution. We note that a similar problem, known as metastability, has been studied in the context of Langevin diffusion, a continuous-time version of SGLD described by stochastic differential equation (SDE) (Bovier et al., 2002, 2004; Gayrard et al., 2005). Specifically, Bovier et al. (2002) gave a sharp bound on the mean hitting time of Langevin diffusion and proved that it may require exponential (in the space dimensionality \(d\)) time for transition between modes. Regarding discrete SGLD, Lee et al. (2018) constructed a probability distribution whose density is close to a mixture of two well-separated isotropic Gaussians, and proved that SGLD could not find one of the two modes within an exponential number of steps. However, further exploration of mode-seeking tendencies of SGLD and its variants such as annealed Langevin dynamics for general distributions is still lacking in the literature.

In this work, we theoretically formulate and demonstrate the potential mode-seeking tendency of SGLD. We begin by analyzing the convergence under a variety of Gaussian mixture probability distributions, under which SGLD could fail to visit all the mixture components within sub-exponential steps (in the data dimension). Subsequently, we generalize this result to mixture distributions with sub-Gaussian modes. This generalization extends our earlier result on Gaussian mixtures to a significantly larger family of mixture models, as the sub-Gaussian family includes any distribution over an \(_{2}\)-norm-bounded support set. Furthermore, we extend our theoretical results to anneal Langevin dynamics with bounded noise scales.

To reduce SGLD's large iteration complexity shown under a high-dimensional input vector, we propose _Chained Langevin Dynamics (Chained-LD)_. Since SGLD could suffer from the curse of dimensionality, we decompose the sample \(^{d}\) into \(d/Q\) patches \(^{(1)},,^{(d/Q)}\), each of constant size \(Q\), and sequentially generate every patch \(^{(q)}\) for all \(q[d/Q]\) statistically conditioned on previous patches, i.e., \(P(^{(q)}^{(0)},^{(q-1)})\). The combination of all patches generated from the conditional distribution faithfully follows the probability density \(P()\), while learning each patch requires less cost due to the reduced dimension. We also provide a theoretical analysis of Chained-LD by reducing the convergence of a \(d\)-dimensional sample to the convergence of each patch.

Finally, we present the results of several numerical experiments to validate our theoretical findings. For synthetic experiments, we consider moderately high-dimensional Gaussian mixture models, where the vanilla and annealed Langevin dynamics could not find all the components within a million steps, while Chained-LD could capture all the components with correct frequencies in \((10^{4})\) steps. For experiments on real image datasets, we consider a mixture of two modes by using the original images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the first mode and constructing the second mode by i.i.d. flipping the images (white background and black digits/objects) with probability 0.5. Following from Song and Ermon (2019), we trained a Noise Conditional Score Network (NCSN) to estimate the score function. Our numerical results indicate that vanilla Langevin dynamics can fail to capture the two modes, as also observed by Song and Ermon (2019). On the other hand, Chained-LD was capable of finding both modes regardless of initialization. We summarize the contributions of this work as follows:

* Theoretically studying the mode-seeking properties of vanilla and annealed Langevin dynamics,
* Proposing Chained Langevin Dynamics (Chained-LD), which decomposes the sample into patches and sequentially generates each patch conditioned on previous patches,
* Providing a theoretical analysis of the convergence behavior of Chained-LD,
* Numerically comparing the mode-seeking properties of vanilla, annealed, and chained Langevin dynamics.

**Notations:** We use \([n]\) to denote the set \(\{1,2,,n\}\). Also, in the paper, \(\|\|\) refers to the \(_{2}\) norm. We use \(_{n}\) and \(_{n}\) to denote a 0-vector and 1-vector of length \(n\). We use \(_{n}\) to denote the identity matrix of size \(n n\). In the text, TV stands for the total variation distance.

## 2 Related Works

**Langevin Dynamics:** The convergence guarantees for Langevin diffusion, a continuous version of Langevin dynamics, are classical results extensively studied in the literature (Bhattacharya, 1978;Roberts and Tweedie, 1996; Bakry and Emery, 1983; Bakry et al., 2008). Langevin dynamics, also known as Langevin Monte Carlo, is a discretization of Langevin diffusion typically modeled as a Markov Chain Monte Carlo (Welling and Teh, 2011). For unimodal distributions, e.g., the probability density function that is log-concave or satisfies log-Sobolev inequality, the convergence of Langevin dynamics is provably fast (Dalalyan, 2017; Durmus and Moulines, 2017; Vempala and Wibisono, 2019). However, for multimodal distributions, the non-asymptotic convergence analysis is much more challenging (Cheng et al., 2018). Raginsky et al. (2017) gave an upper bound on the convergence time of Langevin dynamics for arbitrary non-log-concave distributions with certain regularity assumptions, which, however, could be exponentially large without imposing more restrictive assumptions. Lee et al. (2018) studied the special case of a mixture of Gaussians of equal variance and provided heuristic analysis of sampling from general non-log-concave distributions.

**Mode-Seekingness of Langevin Dynamics:** The investigation of the mode-seekingness of generative models starts with different generative adversarial network (GAN) (Goodfellow et al., 2014) model formulations and divergence measures, from both the practical (Goodfellow, 2016; Poole et al., 2016) and theoretical (Shannon et al., 2020; Li and Farnia, 2023) perspectives. In the context of Langevin dynamics, mode-seekingness is closely related to a lower bound on the transition time between two modes, e.g., two local maximums. Bovier et al. (2002, 2004); Gayrard et al. (2005) studied the mean hitting time of the continuous Langevin diffusion. Lee et al. (2018) proved the existence of a mixture of two Gaussian distributions whose covariance matrices differ by a constant factor, Langevin dynamics cannot find both modes in polynomial time.

**Score-based Generative Modeling:** Since Song et al. (2020b) proposed sliced score matching which can train deep models to learn the score functions of implicit probability distributions on high-dimensional data, score-based generative modeling (SGM) has been going through a spurt of growth. Annealed Langevin dynamics (Song and Ermon, 2019) estimates the noise score of the probability density perturbed by Gaussian noise and utilizes stochastic gradient Langevin dynamics to generate samples from a sequence of decreasing noise scales. Song and Ermon (2020) conducted a heuristic analysis of the effect of noise levels on the performance of annealed Langevin dynamics. Denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) incorporates a step-by-step introduction of random noise into data, followed by learning to reverse this diffusion process in order to generate desired data samples from the noise. Song et al. (2020c) unified anneal Langevin dynamics and DDPM via a stochastic differential equation. A recent line of work focuses on the non-asymptotic convergence guarantees for SGM with an imperfect score estimation under various assumptions on the data distribution (Block et al., 2020; De Bortoli et al., 2021; Lee et al., 2022; Chen et al., 2023; Benton et al., 2023; Li et al., 2023, 2024).

## 3 Preliminaries

### Langevin Dynamics

Generative modeling aims to produce samples such that their distribution is close to the underlying true distribution \(P\). For a continuously differentiable probability density \(P()\) on \(^{d}\), its score function is defined as the gradient of the log probability density function (PDF) \(_{} P()\). Langevin diffusion is a stochastic process defined by the stochastic differential equation (SDE)

\[_{t}=-_{} P(_{t})\,t+\,_{t},\]

where \(_{t}\) is the Wiener process on \(^{d}\). To generate samples from Langevin diffusion, Welling and Teh (2011) proposed stochastic gradient Langevin dynamics (SGLD), a discretization of the SDE for \(T\) iterations. Each iteration of SGLD is defined as

\[_{t}=_{t-1}+}{2}_{} P( _{t-1})+}_{t},\] (1)

where \(_{t}\) is the step size and \(_{t}(_{d},_{d})\) is Gaussian noise. It has been widely recognized that Langevin diffusion could take exponential time to mix without additional assumptions on the probability density (Bovier et al., 2002, 2004; Gayrard et al., 2005; Raginsky et al., 2017; Lee et al., 2018). To combat the slow mixing, Song and Ermon (2019) proposed annealed Langevin dynamics by perturbing the probability density with Gaussian noise of variance \(^{2}\), i.e.,

\[P_{}():= P()(,^{2}_{d})\,,\] (2)and running SGLD on the perturbed data distribution \(P_{_{t}}()\) with gradually decreasing noise levels \(\{_{t}\}_{t[T]}\), i.e.,

\[_{t}=_{t-1}+}{2}_{} P_{ _{t}}(_{t-1})+}_{t},\] (3)

where \(_{t}\) is the step size and \(_{t}(_{d},_{d})\) is Gaussian noise. When the noise level \(\) is vanishingly small, the perturbed distribution is close to the true distribution, i.e., \(P_{}() P()\). Since we do not have direct access to the (perturbed) score function, Song and Ermon (2019) proposed the Noise Conditional Score Network (NCSN) \(_{}(,)\) to jointly estimate the scores of all perturbed data distributions, i.e.,

\[\{_{t}\}_{t[T]},\;_{ {}}(,)_{} P_{}( ).\]

To train the NCSN, Song and Ermon (2019) adopted denoising score matching, which minimizes the following loss

\[(;\{_{t}\}_{t[T]} ):=_{t[T]}_{t}^{2}_{ P }_{}(,_{t}^{2} _{d})}_{}(},_{t})-}-}{_{t}^{2}} ^{2}.\]

Assuming the NCSN has enough capacity, \(_{^{*}}(,)\) minimizes the loss \((;\{_{t}\}_{t[T]})\) if and only if \(_{^{*}}(,_{t})=_{} P_{_{t}}()\) almost surely for all \(t[T]\).

### Multi-Modal Distributions

Our work focuses on multi-modal distributions. We use \(P=_{i[k]}w_{i}P^{(i)}\) to represent a mixture of \(k\) modes, where each mode \(P^{(i)}\) is a probability density with frequency \(w_{i}\) such that \(w_{i}>0\) for all \(i[k]\) and \(_{i[k]}w_{i}=1\). In our theoretical analysis, we consider Gaussian mixtures and sub-Gaussian mixtures, i.e., every component \(P^{(i)}\) is a Gaussian or sub-Gaussian distribution. A probability distribution \(p()\) of dimension \(d\) is defined as a sub-Gaussian distribution with parameter \(^{2}\) if, given the mean vector \(:=_{}[]\), the moment generating function (MGF) of \(p\) satisfies the following inequality for every vector \(^{d}\):

\[_{ p}[^{T}(-)\|\|_{2}^{2}}{2}.\] (4)

We remark that sub-Gaussian distributions include a wide variety of distributions such as Gaussian distributions and any distribution within a bounded \(_{2}\)-norm distance from the mean \(\). From equation 2 we note that the perturbed distribution is the convolution of the original distribution and a Gaussian random variable, i.e., for random variables \( p\) and \((_{d},_{d})\), their sum \(+ p_{}\) follows the perturbed distribution with noise level \(\). Therefore, a perturbed (sub)Gaussian distribution remains (sub)Gaussian. We formalize this property in Proposition 1 and defer the proof to Appendix A for completeness.

**Proposition 1**.: _Suppose the perturbed distribution of a \(d\)-dimensional probability distribution \(p\) with noise level \(\) is \(p_{}\), then the mean of the perturbed distribution is the same as the original distribution, i.e., \(_{ p_{}}[]=_{ p }[]\). If \(p=(,)\) is a Gaussian distribution, \(p_{}=(,+^{2} _{d})\) is also a Gaussian distribution. If \(p\) is a sub-Gaussian distribution with parameter \(^{2}\), \(p_{}\) is a sub-Gaussian distribution with parameter \((^{2}+^{2})\)._

## 4 Theoretical Analysis of the Mode-Seeking Properties of Langevin Dynamics

In this section, we theoretically investigate the mode-seeking properties of vanilla and annealed Langevin dynamics. We begin with analyzing Langevin dynamics in Gaussian mixtures.

### Langevin Dynamics in Gaussian Mixtures

**Assumption 1**.: _Consider a data distribution \(P:=_{i=0}^{k}w_{i}P^{(i)}\) as a mixture of Gaussian distributions, where \(1 k=o(d)\) and \(w_{i}>0\) is a positive constant such that \(_{i=0}^{k}w_{i}=1\). Suppose that \(P^{(i)}=(_{i},_{i}^{2}_{d})\) is a Gaussian distribution over \(^{d}\) for all \(i\{0\}[k]\) such that for all \(i[k]\), \(_{i}<_{0}\) and \(\|_{i}-_{0}\|^{2}^ {2}-_{1}^{2}}{2}((^{2}}{_{0}^{2}})-^{2}}{2_{0}^{2}}+^{2}}{2_{1}^{2}})d\). Denote \(_{}:=_{i[k]}_{i}\)._Regarding the first requirement \(_{i}<_{0}\), we first note that the probability density \(p()\) of a Gaussian distribution \((,^{2}_{d})\) decays exponentially in terms of \(-\|^{2}}{^{2}}\). When a state \(\) is sufficiently far from all modes (i.e., \(\|\|\|_{i}\|\)), the Gaussian distribution with the largest variance (i.e., \(P^{(0)}\) in Assumption 1) dominates all other modes because \(-_{0}\|^{2}}{_{0}^{2}} \|^{2}}{_{0}^{2}} \|^{2}}{_{t}^{2}}-_{i} \|^{2}}{_{t}^{2}}\). We call such mode \(P^{(0)}\) the _universal mode_. Therefore, if \(\) is initialized far from all modes, it can only converge to the universal mode because the gradient information of other modes is masked. Once \(\) enters the universal mode \(P^{(0)}\), if the step size \(_{t}\) of Langevin dynamics is small (i.e., \(_{t}_{0}^{2}\)), it would take exponential steps to escape the local mode \(P^{(0)}\); while if the step size is large (i.e., \(_{t}>_{0}^{2}\)), the state \(\) would again be far from all modes and thus the universal mode \(P^{(0)}\) dominates all other modes. Hence, \(\) can only visit the universal mode unless the stochastic noise \(_{t}\) miraculously leads it to the region of another mode. In addition, it can be verified that \((^{2}}{_{0}^{2}})-^{2}}{2_{0}^ {2}}+^{2}}{2_{t}^{2}}\) is a positive constant for \(_{i}<_{0}\), thus the second requirement of Assumption 1 essentially represents \(\|_{i}-_{0}\|^{2}(d)\). We formalize the intuition in Theorem 1 and defer the proof to Appendix A.1.

**Theorem 1**.: _Consider a data distribution \(P\) satisfying Assumption 1. We follow Langevin dynamics for \(T=((d))\) steps. Suppose the sample is initialized in \(P^{(0)}\), then with probability at least \(1-T(-(d))\), we have \(\|_{t}-_{i}\|^{2}>^{2}+_ {}^{2}}{2}d\) for all \(t\{0\}[T]\) and \(i[k]\)._

We note that \(\|_{t}-_{i}\|^{2}>^{2}+_ {}^{2}}{2}d\) is a strong notion of mode-seekingness, since the probability density of mode \(P^{(i)}=(_{i},_{t}^{2}_{d})\) concentrates around the \(_{2}\)-norm ball \(\{:\|-_{i}\|^{2}_ {i}^{2}d\}\). This notion can also easily be translated into a lower bound in terms of other distance measures such as total variation distance and Wasserstein 2-distance. Moreover, in Theorem 2 we extend the result to annealed Langevin dynamics with bounded noise level, and the proof is deferred to Appendix A.2.

**Theorem 2**.: _Consider a data distribution \(P\) satisfying Assumption 1. We follow annealed Langevin dynamics for \(T=((d))\) steps with noise levels \(c_{}_{0}_{T} 0\) for constant \(c_{}>0\). In addition, assume for all \(i[k]\), \(\|_{i}-_{0}\|^{2} ^{2}-_{0}^{2}}{2}((^{2}+c_{}^{2}}{_{0}^ {2}+c_{}^{2}})-^{2}+c_{}^{2}}{2_{0}^{2}+c _{}^{2}}+^{2}+c_{}^{2}}{2_{t}^{2}+c_{}^{2}} )d\). Suppose that the sample is initialized in \(P^{(0)}_{_{0}}\), then with probability at least \(1-T(-(d))\), we have \(\|_{t}-_{i}\|^{2}>^{2}+_ {}^{2}+2_{t}^{2}}{2}d\) for all \(t\{0\}[T]\) and \(i[k]\)._

### Langevin Dynamics in Sub-Gaussian Mixtures

We further generalize our results to sub-Gaussian mixtures. We impose the following assumptions on the mixture. It is worth noting that these assumptions automatically hold for Gaussian mixtures.

**Assumption 2**.: _Consider a data distribution \(P:=_{i=0}^{k}w_{i}P^{(i)}\) as a mixture of sub-Gaussian distributions, where \(1 k=o(d)\) and \(w_{i}>0\) is a positive constant such that \(_{i=0}^{k}w_{i}=1\). Suppose that \(P^{(0)}=(_{0},_{0}^{2}_{d})\) is Gaussian and for all \(i[k]\), \(P^{(i)}\) satisfies_

1. \(P^{(i)}\) _is a sub-Gaussian distribution of mean_ \(_{i}\) _with parameter_ \(_{i}^{2}\)_,_
2. \(P^{(i)}\) _is differentiable and_ \( P^{(i)}(_{i})=_{d}\)_,_
3. _the score function of_ \(P^{(i)}\) _is_ \(L_{i}\)_-Lipschitz such that_ \(L_{i}}{_{t}^{2}}\) _for some constant_ \(c_{L}>0\)_,_
4. \(_{0}^{2}>\{1,^{2}+c_{}c_{L})}{c_{}(1-c_{})} \}^{2}}{1-c_{}}\) _for constant_ \(c_{}(0,1)\)_, where_ \(_{}:=_{i[k]}_{i}\)_,_
5. \(\|_{i}-_{0}\|^{2})_{0}^{2}-_{0}^{2}}{2(1-c_{})}(_{t}^{2}}{( c_{L}^{2}+c_{}c_{L})_{0}^{2}}-^{2}}{2(1-c_{})_{0}^{2}}+ )_{0}^{2}}{2_{t}^{2}})d\)_._

We validate the feasibility of Assumption 2.v. in Lemma 9 in the Appendix. With Assumption 2, we show the mode-seeking tendency of Langevin dynamics under sub-Gaussian distributions in Theorem 3 and defer the proof to Appendix A.3.

**Theorem 3**.: _Consider a data distribution \(P\) satisfying Assumption 2. We follow Langevin dynamics for \(T=((d))\) steps. Suppose the sample is initialized in \(P^{(0)}\), then with probability at least \(1-T(-(d))\), we have \(\|_{t}-_{i}\|^{2}>(^{2}} {2}+^{2}}{2(1-e_{})})d\) for all \(t\{0\}[T]\) and \(i[k]\)._

Finally, we slightly modify Assumption 2 and extend our results to annealed Langevin dynamics under sub-Gaussian mixtures in Theorem 4. The details of Assumption 3 and the proof of Theorem 4 are deferred to Appendix A.4.

**Theorem 4**.: _Consider a data distribution \(P\) satisfying Assumption 3. We follow annealed Langevin dynamics for \(T=((d))\) steps with noise levels \(c_{}_{0}_{T} 0\). Suppose the sample is initialized in \(P^{(0)}_{_{0}}\), then with probability at least \(1-T(-(d))\), we have \(\|_{t}-_{i}\|^{2}>(^{2} +_{1}^{2}}{2}+^{2}+_{2}^{2}}{2(1-e_{})})d\) for all \(t\{0\}[T]\) and \(i[k]\)._

## 5 Chained Langevin Dynamics

To reduce the mode-seeking tendencies of vanilla and annealed Langevin dynamics, we propose Chained Langevin Dynamics (Chained-LD) in Algorithm 1. While vanilla and annealed Langevin dynamics apply gradient updates to all coordinates of the sample in every step, we decompose the sample into patches of constant size and generate each patch sequentially to alleviate the exponential dependency on the dimensionality. More precisely, we divide a sample \(\) into \(d/Q\) patches \(^{(1)},^{(d/Q)}\) of some constant size \(Q\), and apply annealed Langevin dynamics to sample each patch \(^{(q)}\) (for \(q[d/Q]\)) from the conditional distribution \(P(^{(q)}^{(1)},^{(q-1)})\).

An ideal conditional score function estimator \(_{}\) could jointly estimate the scores of all perturbed conditional patch distribution, i.e., \(\{_{t}\}_{t[TQ/d]},q[d/Q]\),

\[_{}(^{(q)},^ {(1)},,^{(q-1)})_{^{(q)}} P_ {}(^{(q)}^{(1)},^{(q-1)}).\]

Following from Song and Ermon (2019), we use the denoising score matching to train the estimator. For a given \(\), the denoising score matching objective is

\[(;):=_{ P} _{}(,^{2}} {I}_{d})_{q[d/Q]}[\|_{}( ^{(q)},^{(1)},,^{(q-1)})- }^{(q)}-^{(q)}}{^{2}}\|^{2} ].\]

Then, combining the objectives gives the following loss

\[(;\{_{t}\}_{t[TQ/d]} ):=_{t[TQ/d]}_{t}^{2}(; _{t}).\]

As shown in Vincent (2011), an estimator \(_{}\) with enough capacity minimizes the loss \(\) if and only if \(_{}\) outputs the scores of all perturbed conditional patch distribution almost surely. Ideally, if a samplerperfectly generates every patch, combining all patches gives a sample from the original distribution since \(P()=_{q[d/Q]}P(^{(q)}^{(1)}, ^{(q-1)})\). In Theorem 5 we give a linear reduction from producing samples of dimension \(d\) using Chained-LD to learning the distribution of a \(Q\)-dimensional variable for constant \(Q\). The proof of Theorem 5 is deferred to Appendix A.5.

**Theorem 5**.: _Consider a sampler algorithm taking the first \(q-1\) patches \(^{(1)},,^{(q-1)}\) as input and outputing a sample of the next patch \(^{(q)}\) with probability \((^{(q)}^{(1)},,^{(q-1)})\) for all \(q[d/Q]\). Suppose that for every \(q[d/Q]\) and any given previous patches \(^{(1)},,^{(q-1)}\), the sampler algorithm can achieve_

\[((^{(q)}^{(1)},, ^{(q-1)}),P(^{(q)}^{(1)},,^{(q-1)}))\]

_in \((,d)\) iterations for some \(>0\). Then, equipped with the sampler algorithm, the Chained-LD algorithm in \((,d)\) iterations can achieve_

\[((),P()).\]

## 6 Numerical Results

In this section, we empirically evaluated the mode-seeking tendencies of vanilla, annealed, and chained Langevin dynamics. We performed numerical experiments on synthetic Gaussian mixture models and real image datasets including MNIST (LeCun, 1998) and Fashion-MNIST (Xiao et al., 2017). Details on the experiment setup are deferred to Appendix B.

Figure 1: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are \(_{2}\) distance from samples to the mean of the three modes. The samples are initialized in mode 0.

**Synthetic Gaussian mixture model:** We define the data distribution \(P\) as a mixture of three Gaussian components in dimension \(d=100\), where mode 0 defined as \(P^{(0)}=(_{d},3_{d})\) is the universal mode with the largest variance, and mode 1 and mode 2 are respectively defined as \(P^{(1)}=(_{d},_{d})\) and \(P^{(2)}=(-_{d},_{d})\). The frequencies of the three modes are 0.2, 0.4 and 0.4, i.e.,

\[P=0.2P^{(0)}+0.4P^{(1)}+0.4P^{(2)}=0.2(_{d},3_{d})+0.4(_{d},_{d})+0.4(- _{d},_{d}).\]

As shown in Figure 1, vanilla and annealed Langevin dynamics cannot find mode 1 or 2 within \(10^{6}\) iterations if the sample is initialized in mode 0, while chained Langevin dynamics can find the other two modes in 1000 steps and correctly recover their frequencies as gradually increasing the number of iterations. In Appendix B.1 we present additional experiments on samples initialized in mode 1 or 2, which also verify the mode-seeking tendencies of vanilla and annealed Langevin dynamics.

**Image datasets:** We construct the distribution as a mixture of two modes by using the original images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the first mode and constructing the second mode by i.i.d. randomly flipping an image (white background and black digits/objects) with probability 0.5. Regarding the neural network architecture of the score function estimator, for vanilla and annealed Langevin dynamics we use U-Net (Ronneberger et al., 2015) following from Song and Ermon (2019). For chained Langevin dynamics, we proposed to use Recurrent Neural Network (RNN) architectures. We note that for a sequence of inputs, the output of RNN from the previous step is fed as input to the current step. Therefore, in the scenario of chained Langevin dynamics, the hidden state of RNN contains information about the previous patches and allows the network to estimate the conditional score function \(_{^{(q)}} P(^{(q)}^{(1)}, ^{(q-1)})\). More implementation details are deferred to Appendix B.2.

Figure 2: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from MNIST.

The numerical results on image datasets are shown in Figures 2 and 3. Vanilla Langevin dynamics fails to generate reasonable samples, as also observed in Song and Ermon (2019). When the sample is initialized as original images from the datasets, annealed Langevin dynamics tends to generate samples from the same mode, while chained Langevin dynamics can generate samples from both modes. Additional experiments are deferred to Appendix B.2.

## 7 Conclusion

In this work, we theoretically and numerically studied the mode-seeking properties of vanilla and annealed Langevin dynamics sampling methods under a multi-modal distribution. We characterized Gaussian and sub-Gaussian mixture models under which Langevin dynamics are unlikely to find all the components within a sub-exponential number of iterations. To reduce the mode-seeking tendency of vanilla Langevin dynamics, we proposed Chained Langevin Dynamics (Chained-LD) and analyzed its convergence behavior. Studying the connections between Chained-LD and denoising diffusion models will be an interesting topic for future exploration.

### Limitations

Our RNN-based implementation of Chained-LD is currently limited to image data generation tasks. An interesting future direction is to extend the application of Chained-LD to other domains such as audio and text data. Another future direction could be to study the convergence of Chained-LD under an imperfect score estimation which we did not address in our analysis.

Figure 3: Samples from a mixture distribution of the original and flipped images from the Fashion-MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from Fashion-MNIST.