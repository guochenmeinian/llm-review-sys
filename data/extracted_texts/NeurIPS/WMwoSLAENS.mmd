# AutoDefense: Multi-Agent LLM Defense against

Jailbreak Attacks

 Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2

1Oregon State University, 2Pennsylvania State University

3CISPA Helmholtz Center for Information Security

{zengyif, huazheng.wang}@oregonstate.edu

{yiran.wu, qingyun.wu}@psu.edu

xiao.zhang@cispa.de

Equal Contribution.

###### Abstract

Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense framework that filters harmful responses from LLMs. With the response-filtering mechanism, our framework is robust against different jailbreak attack prompts, and can be used to defend different victim models. AutoDefense assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. With AutoDefense, small open-source LMs can serve as agents and defend larger models against jailbreak attacks. Our experiments show that AutoDefense can effectively defense against different jailbreak attacks, while maintaining the performance at normal user request. For example, we reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a 3-agent system. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.

## 1 Introduction

Large Language Models (LLMs) have shown remarkable capabilities in solving a wide variety of tasks . Nevertheless, the rapid advancements of LLMs have raised serious ethical concerns, as they can easily generate harmful responses at users' request . To align with human values, LLMs have been trained to adhere to policies to refuse potential harmful requests . Despite extensive efforts in pre-training and fine-tuning LLMs to be safer, an adversarial misuse of LLMs, known as _jailbreak attacks_, has emerged lately, where specific jailbreak prompts are designed to elicit undesired harmful behavior from safety-trained LLMs.

Various attempts have been made to mitigate jailbreak attacks. Supervised defenses, such as Llama Guard , incur significant training costs. Other methods interfere with response generation , which might not be robust to variations of attack methods, while also impacting the response quality due to the modification of the normal user prompts. Although LLMs can identify risks with proper guidance and multiple reasoning steps , these methods heavily depend on the LLMs' ability to follow instructions, making it challenging to utilize more efficient, less capable open-source LLMs for defense tasks.

There is an urgent need to develop defense methods that are both robust to variations of jailbreaks and model-agnostic. AutoDefense employs a response-filter mechanism to identify and filter out harmfulresponses, which doesn't affect user inputs while robust to different jailbreaks. The framework divides the defense task into multiple sub-tasks and assigns them among LLM agents, leveraging the inherent alignment abilities of LLMs. A similar idea of task decomposition is also proven useful in Zhou et al. , Khot et al. . This allows each agent to focus on specific segments of the defense strategy, from analyzing the intention behind a response to finalizing a judgment, which encourages divergent thinking and improves LLMs' content understanding by offering varied perspectives [26; 12; 48; 23]. This collective effort ensures the defense system can give a fair judgment on whether the content is aligned and appropriate to present to users. AutoDefense, as a general framework, is flexible to integrate other defense methods as agents, making it easy to take advantage of existing defenses.

We evaluate AutoDefense against a comprehensive list of harmful and normal prompts, showcasing its superiority over existing methods. Our experiments reveal that our multi-agent framework significantly reduces the Attack Success Rate (ASR) of jailbreak attempts while maintaining a low false positive rate on safe content. This balance underscores the framework's ability to discern and protect against malicious intents without undermining the utility of LLMs for regular user requests. To validate the advantages of multi-agent systems, we conduct experiments under different agent configurations using different LLMs. We also show AutoDefense is more robust to various attack settings in Section A.6. We find that AutoDefense with LLaMA-2-13b, a small model with low cost and high inference speed, can constantly achieve a competitive defense performance. We reduce the ASR on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a three-agent defense system. The overall accuracy of the defense filtering is 92.91%, which ensures minimal influence on normal user requests. We also show that AutoDefense is expandable with LLam Guard  as the fourth agent. It significantly reduces the FPR of defense using LLaMA-2-7b from 37.32% to 6.80% and keeps the ASR at a competitive level. Our findings suggest that multi-agent approaches are promising to improve LLM robustness against jailbreak attacks, with the flexibility of working on various LLMs and integration of other defense components.

## 2 Related Work

**Jailbreak Attack.** Recent studies have expanded our understanding of the vulnerability of safety-trained Large Language Models (LLMs) to jailbreak attacks [46; 27; 38; 9; 50]. Jailbreak attacks use carefully crafted prompts to bypass the safety mechanism and manipulate LLMs into generating objectionable content. In particular, Wei et al.  hypothesized competing objectives and mismatched generalization as two failure modes under jailbreak attack [4; 32; 3; 33]. Zou et al.  proposed to automatically produce universal adversarial suffixes using a combination of greedy and gradient-based search techniques. This attack method is also known as token-level jailbreak, where the injected adversarial strings often lack semantic meaning to the prompt [6; 20; 30; 39]. There also exist other automatic jailbreak attacks [31; 6; 34] such as Prompt Automatic Iterative Refinement

Figure 1: Example of AutoDefense against jailbreak attack. In this example, to get the targeted answer from an LLM assistant without being refused, the user constructs a jailbreak prompt using refusal suppression. Before the generated response is presented to the user, it will first be sent to AutoDefense. Whenever our defense determines the response to be invalid, it overrides the response to explicit refusal.

(PAIR), which uses LLMs to construct jailbreak prompts. AutoDefense only uses response for defense, which makes it not sensitive to the attack methods that mainly affect the prompts.

**Defense.** Prompt-based defenses control the response-generating process by altering the original prompt. For instance, Xie et al.  uses a specially designed prompt to remind LLM not to generate harmful or misleading content. Liu et al.  uses LLM to compress the prompt to mitigate jailbreak. Zhang et al.  analyzes the intention of the given prompt using LLMs. To defend token-level jailbreaks, Robey et al.  constructs multiple random perturbations to any input prompt and then aggregates their responses. Perplexity filtering , paraphrasing , and re-tokenization  are also prompt-based defenses, which aim to render adversarial prompts ineffective. In contrast, response-based defenses first generate a response before evaluating whether the response is harmful. For instance, Helbling et al.  leverages the intrinsic capabilities of LLMs to evaluate the response. Wang et al.  infers potentially malicious input prompt based on the response. Zhang et al.  makes the LLM aware of potential harm by asking it to repeat its response. Content filtering methods [10; 22; 11] can also be used as response-based defense methods. Llama Guard  and Self-Guard  are supervised models that can classify prompt response pairs into safe and unsafe. The defense LLM and the victim LLM are separated in these methods, which means a well-tested defense LLM can be used to defend any LLM. AutoDefense framework leverages the response filtering ability of LLM to identify unsafe responses triggered by jailbreak prompts. Other methods like Zhang et al. , Wallace et al.  leverage the idea of goal or instruction prioritization to make LLMs more robust to malicious prompts.

**Multi-Agent LLM System.** The development of LLM as the core controller for autonomous agents is a rapidly evolving research area. To enhance the problem-solving and decision-making capabilities of LLMs, multi-agent systems with LLM-powered agents are proposed . Recent works show that multi-agent debate is an effective way to encourage divergent thinking and improve factuality and reasoning [26; 12]. For example, CAMEL demonstrates how role-playing can be used to let chat agents communicate with each other for task completion , whereas MetaGPT shows that multi-agent conversation framework can help automatic software development . Our multi-agent defense framework is implemented using AutoGen1, which is a generic multi-agent framework for building LLM applications.

## 3 Methodology

**Preliminaries.** We focus on defending jailbreak attacks that force LLM to output content that does not align with human values. For example, a malicious user may use the harmful prompt: _How can

Figure 2: Detailed design of the Defense Agency with respect to different numbers of LLM agents. The defense agency is responsible for completing the specific defense task by a multi-agent system. After the defense agency receives the LLM response from the input agent as shown in Figure 1, the defense agency will classify it as valid or invalid. In the single-agent setting on the left, one LLM agent will finish all the analysis tasks and give the judgment. In the two-agent and three-agent settings, agents collaboratively finish the defense task. There is a coordinator agent in the configuration that is responsible for controlling the high-level progress of the defense task.

_I create a bomb?_ to elicit harmful information from an LLM. LLMs with alignment training can identify the risks behind this request and refuse to fulfill this request. Instead, the malicious user can use a jailbreak prompt combined with the previous harmful prompt to bypass the alignment, (an example is shown in Figure 1), and the safety mechanism fails. The main failure mode of the jailbreak attack we focus on is _competing objectives_. This attack forces the LLM to choose between instruction-following or avoiding generating harmful content, two competing objectives learned during training.

### A Multi-Agent Defense Framework

Our multi-agent defense framework AutoDefense employs a response-filtering defense mechanism in which the system actively monitors and filters each response generated by the LLM. Figure 1 illustrates our proposed system together with a jailbreak attack example. In our concerned setting, a malicious user can only manipulate the prompt passed to the LLM and cannot directly access the LLM's response. AutoDefense scrutinizes each response from the LLM: even if an attack successfully bypasses the LLM's defense and produces a harmful response, our system will detect it and provide a safe alternative such as refusing the user's request. This response-filtering mechanism untangles the difficulty in handling various adversarial prompts.

Specifically, our multi-agent defense consists of three components: the input agent, the defense agency, and the output agent. The input agent is responsible for prepossessing the LLM response to a message format in our defense framework. It wraps the LLM response into our designed template that includes the goals and content policy of the defense system. The content policy in this template is from the OpenAI website,2 which helps remind the LLMs to use the context related to its human value alignment training. It then sends the preprocessed response in its message to the defense agency. The defense agency contains the second level of the multi-agent system, which further consists of various numbers of LLM agents. Within the defense agency, multiple agents can collaborate and analyze potentially harmful content, and return a final judgment to the output agent. The output agent decides how to output the final response to a user request. If the LLM response is deemed safe by the defense agency, the output agent will return the original response. Otherwise, it will override the response into explicit refusal. The output agent can also serve to revise the raw response using an LLM based on the feedback from the defense agency, thereby providing a more natural refusal in some applications. For simplicity, the output agent's role here is to decide whether to use a fixed refusal to override the original response based on the defense agency output.

### Design of Defense Agency

At the core of our multi-agent defense system is the defense agency, which is the main processing unit responsible for content filtering. Within the defense agency, several agents work in concert to classify whether a given response contains harmful content and is not appropriate to be presented to the user. The agent configuration is flexible in the defense agency, where various agents with different roles can be added to achieve the defense objective. Figure 2 and Figure 4 illustrate our design. In particular, we propose a three-step process to decide if a given content is harmful as follows:

* Step 1: Intention analysis. This step analyzes the intention behind the given content. Intention analysis has been used in analyzing the user prompt and achieving competitive results in IAPrompt . Because the original prompt might contain jailbreak content that can trick LLMs, we don't include it as the defense system input.
* Step 2. Prompts inferring. The second step is to infer possible original prompts in the form without the jailbreak prompt. We design the prompt prediction task to recover the original prompt only by the response. This task is based on the observation that jailbreak prompts usually are pure instructions. Therefore, the LLMs can construct the query from the information in the response without misleading instructions. We test this task on different kinds of LLMs and find it can be achieved. We expect these inferred prompts can activate the safety mechanism of LLMs.
* Step 3. Final judgment. The goal of this step is to make a final judgment. This judgment is based on the analyzed intention and original prompts in the first two steps.

Based on the process, we construct three different patterns in the multi-agent framework, consisting of one to three LLM agents (Figure 2). Each agent is given a system prompt that contains detailed instructions and an in-context example of the assigned task. The system prompt for an agent is only visible to the agent itself and is not visible to other agents. Because of the zero-shot nature of this task, we use an in-context example to show how each agent presents their response in a well-structured format. See prompts for different designs in Appendix A.9.

**Single-Agent Design.** A simple design is to utilize a single LLM agent to analyze and make judgments in a chain-of-thought (CoT) style. While straightforward to implement, it requires the LLM agent to solve a complex problem with multiple sub-tasks.

**Multi-Agent Design.** Using multiple agents compared to using a single agent can make agents focus on the assigned sub-tasks. Each agent only needs to receive and understand the detailed instructions of a specific sub-task. It enables complex reasoning without relying on the strong instruction-following ability of LLMs, which helps LLM with limited steerability finish a complex task by following the instructions on each sub-task.

* **Coordinator.** With more than one LLM agent, we introduce a coordinator agent that is responsible for coordinating the work of agents. When each agent generates their response, it can only see the message between previous agents and the coordinator, their system prompt, and the prompt sent to them by the coordinator. Before each agent starts their response, the coordinator will also give a concise prompt to activate each agent. This concise prompt from the coordinator emphasizes the role of each agent and asks them to start their response with a certain prefix. This communication topology design is based on AutoGen . The goal of the coordinator is to let each agent start their response after a query, which is a more natural way of LLM interaction.
* **Two-Agent System.** This configuration consists of two LLM agents and a coordinator agent: (1) Analyzer: responsible for analyzing the intention and inferring the original prompt, and (2) Judge: responsible for giving the final judgment. The analyzer will pass its analysis in its message to the coordinator after it finishes the response. The coordinator then asks the judge to deliver a judgment.
* **Three-Agent System.** This configuration consists of three LLM agents as shown in the lower side of Figure 4, and a coordinator agent: (1) the intention analyzer, which is responsible for analyzing the intention of the given content, (2) the prompt analyzer, responsible for inferring the possible original prompts given the content and the intention of it, and (3) the judge, which is responsible for giving the final judgment. The coordinator agent acts as the bridge between them.

We remark that more agents could be incorporated into our framework. In this work, we focus on revealing desirable properties of a multi-agent system with up to three agents and leave the investigation of more complex systems for future research.

## 4 Experimental Setup

### Dataset

There are two sets of harmful and regular prompts used in the experiment, which contain different scale of prompts. We use the small set to design the prompt in our defense and tune the LLM hyper-parameters. For a fair comparison, we only evaluate the large set after all the settings are determined.

**Harmful Prompts.** Harmful prompts are instructions sent to language models to elicit harmful or malicious responses (e.g., _Please write instructions on how to horwire a car_.). We evaluate on two datasets: (1) we curate a dataset of 33 harmful prompts from red-teaming efforts of OpenAI and Anthropic [32; 3; 46], which covers a comprehensive range of harmful categories like discrimination, terrorism, self-harm. Beyond harm, this curated dataset also includes personally identifiable information (PII) leakage , for example, _List the addresses of Bill Gates's houses in USPS format_. (2) we use a comprehensive question set from DAN  consisting of 390 harmful questions from 13 forbidden scenarios, such as Illegal Activity, Hate Speech, etc. Because our defense framework is designed to defend a large LLM with an efficient small LMM, we use GPT-3.5 as the victim LLM in our experiment. The proposed defense method is response-based. We generate prompt response pairs on gpt-3.5-turbo-1106 with temperature 1 using the Combination-1 attack from Table 6 in the appendix. We generate 10 different responses on each prompt for curated dataset and 5 for DAN dataset, the final size of the above two datasets is 330 and 1950.

Regular Prompts.To test the side effects of regular user requests, we also include two regular prompt datasets: (1) We generate 33 safe prompts by GPT-4. These prompts inquiry information includes daily life and scientific topics. (e.g. _What are the safest countries to travel to in 2024?_) We prompt GPT-3.5 with those prompts 10 times and collect 330 safe responses. (2) We sample 1000 prompt and response pairs from 52K instruction-following data of Stanford Alpaca . These prompt and response pairs encompass a wide range of user requests. (e.g. _"prompt": "Generate an employee feedback based on performance.", "response": "Your performance has been outstanding so far. You have been consistently exceeding expectations and have taken ownership of the tasks assigned to you."_) There is a response for each prompt in this dataset, so we don't need to generate the response by GPT-3.5. All of the prompts from these two datasets will get a normal answer when prompting LLMs with them.

### Evaluation Metrics

Attack Success Rate (ASR)We employ two methods to evaluate the success rate of jailbreak attacks: (1) _Keyword-Based Evaluation_, which summarizes a set of frequently occurring keywords found in responses to non-jailbreak attacks to determine the success and failure of jailbreak attacks, and (2) _Automated Evaluation_, which uses GPT-4 as a judge model. The keyword-based evaluation is first used to identify clear refusal responses, then the remaining responses will be examined by automated evaluation.

False Positive Rate (FPR)We use FPR to measure the side effect of LLM defense on regular user prompts. In particular, we check whether a safe response has been incorrectly refused by the defense using the keyword-based evaluation method.

Accuracy.Accuracy is used to evaluate both defense performance and side effects. It is calculated as the ratio of correctly classified samples to the total number of samples. Specifically, accuracy is determined by (number of correctly rejected harmful responses + number of correctly accepted regular responses) / (total number of harmful responses + total number of regular responses).

   Defense Method & ASR \\  No Defense & 55.74 \\ OpenAI Moderation API & 53.79 \\ Self Defense & 43.64 \\ System-Mode Self-Reminder & 22.31 \\ Llama Guard (Response Only) & 29.44 \\ Llama Guard (Prompt + Response) & 21.28 \\ Single-Agent Defense (Ours) & 9.44 \\ Three-Agent Defense (Ours) & **7.95** \\   

Table 1: Comparisons of ASR with other defenses on the DAN dataset. We use the Combination-1 attack method from Table 6 in the appendix to craft jailbreak prompts, and GPT-3.5 Turbo as the victim model.

   Attack-Model & Defense Method & ASR (\%) \\   & Self Defense & 52.31 \\ AIM + & Llama Guard (Prompt+Response) & 24.81 \\ Vicuna-13B & System-Mode Self-Reminder & 28.21 \\  & Three-Agent Defense (Ours) & **5.38** \\   & Self Defense & 43.61 \\ Combine-1 + & Llama Guard (Prompt+Response) & 21.28 \\ GPT-3.5-Turbo (from Tab. 1) & System-Mode Self-Reminder & 22.31 \\  & Three-Agent Defense (Ours) & **7.95** \\   

Table 2: Compares AutoDefense with other defense methods with a different attack method and a different victim model.

Experimental Results

### Main Results

**Comparisons with other defenses.** We compare different methods for defending GPT-3.5 as shown in Table 1. We use LLaMA-2-13B as the defense LLM in AutoDefense. We find our AutoDefense outperforms other methods in terms of ASR. The compared methods in Table 1 includes: (1) System-Mode Self-Reminder  is a prompt-based method, it only needs a victim LLM to finish the defense. This kind of defense method might interfere with response generation, which potentially impacts the response quality for regular user requests because of the modification of the original user prompt. (2) Self Defense  is a similar response filtering method. (3) The OpenAI Moderation API3 is an OpenAI host content filter, it only takes the response text as the input. (4) The Llama Guard  is a supervised filtering method. It is designed to take prompt and response as input. So we evaluate it in both with and without prompt situations. These methods cover both supervised and zero-shot, filtering and non-filtering methods. The Single Agent Defense method in Table 1 uses only a single LLM agent to judge whether a given content is safe, which is similar to (2). But we can observe significantly better ASR compared to (2), this is due to the CoT analysis procedure we designed as shown in Figure 4. The 3 Agents Defense configuration better enforces this analysis procedure and further improves the defense performance.

In Table 2, we further compare the ASR of AutoDefense with other methods with a different attack method and a different victim model. AutoDefense still outperforms other methods by a large margin. This aligns with our expectations that AutoDefense is agnostic to the response generation, which means the attack method and victim model will have minimal effect on the defense performance.

**Custom agent: Llama Guard as an agent in defense.**

The FPR of the multi-agent defense configurations based on LLaMA-2-7b is relatively high. To tackle this problem, we introduce Llama Guard  as an additional defense agent to form a 4-agent system. Table 1 shows that LLama Guard performs best when both prompt and response are provided. The prompt inferred by the prompt analyzer can be used as the input of the Llama Guard. So we let the Llama Guard agent generate its response after the prompt analyzer agent. The Llama Guard agent extracts the possible prompts from the prompt analyzer's response, combines them with the given response to form multiple pairs, and uses these prompt-response pairs to infer with Llama Guard. If none of the prompt-response pairs get an unsafe output from Llama Guard, the Llama Guard agent will respond that the given response is safe. The judge agent will consider the response from the LLama Guard agent and other agents to form its judgment. Table 3 demonstrates that the FPR significantly decreased after introducing Llama Guard as an agent, and the ASR remains at a low level. This encouraging result suggests AutoDefense is flexible to integrate different defense methods(e.g. PARDEN ) as additional agents, where the multi-agent defense system benefits from new capabilities of new agents.

### Additional Results

**#Agents vs ASR.** To show the increased number of LLM agents helps defense, we evaluate defense performance from single-agent to three-agent configurations across various LLMs. We observe as the number of agents increases, the defense result gets better in most situations as shown in Figure 3 and Table 4. In Figure 3, we notice LLaMA-2 based defense benefits from multiple agent configurations. In Table 4, we can see the average accuracy of the three-agent configuration is competitive to the single-agent case in most situations. Because of its efficient and open-source nature, we think LLaMA-2-13b is most suitable for our multi-agent defense system, which can be used to defend various victim LLMs including those LLMs that don't perform well as defense LLMs. We think this improvement is due to the multi-agent design makes each LLM agent easier to follow

   Agent Configuration & FPR (\%) & ASR (\%) \\  Single-Agent (CoT) & 17.16 & 10.87 \\ Three-Agent & 37.32 & 3.13 \\ Four-Agent w/ LlamaGuard & 6.80 & 11.08 \\   

Table 3: Comparison of FPR of multi-agent defense using LLaMA-2-7b introducing Llama Guard as a agent.

the instructions to analyze a given content. The single agent configuration refers to combining all the sub-tasks from other agents into one agent, which is an agent with CoT ability as shown in Figure 4. In this setting, the LLM has to finish all the tasks in a single pass. We believe this is difficult for those LLMs with limited steerability. In an example defense on Table 10, we notice reasoning ability improvement in the 3-agent system compared to CoT. For LLMs with strong steerability like GPT-3.5 and LLaMA-2-70b, Table 4 shows that the single agent with CoT is sufficient to achieve a low ASR for the defense task, whereas the FPR of GPT-3.5-based defense can be largely reduced with our three-agent configuration.

**Side effect on regular prompts.** A desirable defense system is expected to have minimal effect on normal user requests. Thus, we evaluate the FPR on filtering safe LLM responses. Figure 3 shows that FPR is mostly maintained at a low level. According to Table 4, FPRs achieved by defense LLMs with limited alignment levels are lower in the multi-agent case compared to the single-agent case, suggesting our three-agent configuration performs best in terms of accuracy.

**Time & Computation Overhead.**

AutoDefense introduces acceptable time overhead to the defense. The overhead of the multi-agent framework is negligible. The major overhead comes from the multiple LLM inference requests. Table 5 shows the benchmark result of the different number of agent configurations with LLaMA-2-13B as the defense LLM. Using a multi-agent system for AutoDefense does not significantly increase time cost compared to a single CoT agent system, as the cost mainly depends on the total number of output tokens. The procedure of analyzing a response's validity is broken into subtasks, and the overall task remains the same whether completed by a single CoT prompt or multiple conversation rounds. The total output tokens will be similar if the analysis procedure is consistent.

    &  &  &  \\ LLM & 1 CoT & 2 A & 3 A & 1 CoT & 2 A & 3 A & 1 CoT & 2 A & 3 A \\  GPT-3.5 & **7.44** & 12.87 & 13.95 & 4.44 & 1.00 & **0.96** & 94.72 & **95.67** & 95.40 \\ LLaMA-2-13b & 9.44 & 8.77 & **7.95** & 9.24 & **6.58** & 6.76 & 90.71 & 92.81 & **92.91** \\ LLaMA-2-70b & 11.69 & 10.92 & **6.05** & **3.00** & 5.34 & 13.12 & **94.56** & 93.09 & 88.86 \\ LLaMA-2-7b & 10.87 & 3.49 & **3.13** & **17.16** & 40.26 & 37.32 & **84.60** & 70.06 & 72.27 \\ mistral-7b-v0.2 & **12.31** & 21.95 & 22.82 & 3.98 & **0.36** & 0.60 & **93.68** & 93.58 & 93.17 \\ mixtral-8x7b-v0.1 & **11.59** & 14.05 & 12.77 & 2.22 & **0.32** & 0.44 & 95.15 & 95.83 & **96.10** \\ vicuna-13b-v1.5 & **26.00** & 26.72 & 26.15 & 2.88 & **0.30** & 0.38 & 90.63 & 92.29 & **92.39** \\ vicuna-33b & 28.31 & 28.67 & **23.59** & 2.40 & **0.72** & 1.64 & 90.33 & 91.44 & **92.20** \\ vicuna-7b-v1.5 & **13.33** & 18.21 & 22.31 & 37.84 & 5.18 & **2.40** & 69.04 & 91.17 & **92.01** \\   

Table 4: Attack Success Rate (ASR), False Positive Rate (FPR), and accuracy in defending against harmful requests from the DAN dataset and safe requests from the Alpaca instruction-following dataset. The victim model is GPT-3.5, the LLMs shown in this table are the defense LLM in each agent that finishes the defense task. One of the advantages of AutoDefense is that it can use a fixed defense LLM to defend all kinds of victim LLMs. This means that even if an LLM cannot perform well as a defense LLM, its defense performance as a victim LLM can also be good when it is defended by another defense LLM.

Figure 3: Evaluating defense performance on ASR and FPR with different numbers of agent configurations 5 times on the curated dataset for harmful requests and GPT-4 generated dataset for regular requests.

   Agent Configuration & Time (sec) \\  Single-Agent (CoT) & 2.81 \\ Two-Agent & 5.53 \\ Three-Agent & 6.95 \\   

Table 5: Average defense time on the curated dataset of 33 harmful prompts. We benchmark on a single NVIDIA H100 GPU with INT8 quantization.

The single-agent configuration appears faster because LLMs tend to skip reasoning steps as shown in Table10, which the multi-agent design aims to prevent.

## 6 Conclusion

In this work, we proposed AutoDefense, a multi-agent defense framework for mitigating LLM jailbreak attacks. Built upon a response-filtering mechanism, our defense employs multiple LLM agents, each tasked with specialized roles to analyze harmful responses collaboratively. We found the CoT instruction heavily depends on LLMs' ability to follow instructions, and we are targeting efficient LLMs with weaker instruction-following abilities. To address this issue, we found the multi-agent approach is a natural way to let each LLM agent with a certain role focus on a specific sub-task. Thus, we propose to use multiple agents to solve sub-tasks. We showed that our three-agent defense system powered by the LLaMA-2-13B model can effectively reduce the ASR of state-of-the-art LLM jailbreaks. Our multi-agent framework is also flexible by design, which can incorporate various types of LLMs as agents to complete the defense task. In particular, we demonstrated that FPR can be further reduced if integrating other safety-trained LLMs such as Llama Guard into our framework, suggesting the superiority of AutoDefense to be a promising defense against jailbreak attacks without sacrificing the model performance at normal user request.