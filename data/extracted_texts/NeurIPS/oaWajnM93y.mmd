# Cream: Consistency Regularized Self-Rewarding Language Models

Zhaoyang Wang\({}^{1}\) Weilei He\({}^{2}\) Zhiyuan Liang\({}^{3}\) Xuchao Zhang\({}^{4}\)

\({}^{1}\)University of North Carolina at Chapel Hill \({}^{2}\)Nanyang Technological University

\({}^{3}\)National University of Singapore \({}^{4}\)Microsoft Research

{zhaoyang,huaxiu}@cs.unc.edu weitongz@unc.edu

###### Abstract

Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee on the accuruty of the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a **C**onsistency **R**egularized s**E**lf-rewarding **I**Anguage **M**odel (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance.

## 1 Introduction

Large language models (LLMs) have shown impressive capabilities across various tasks, including natural language understanding and generation (Radford et al., 2019). At the same time, LLMs also face alignment challenges such as generating hallucinations and harmful outputs (Ji et al., 2023). To address these issues, a series of research works have explored preference learning methods such as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) and direct alignment techniques such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) to align the LLMs with human values and preferences. These alignment methods often require a large amount of preference pairs which are indispensable in both RLHF and direct alignment training. However, collecting human-annotated preference pairs is time-consuming and labor-intensive, which seriously limits the scalability and efficiency of these alignment methods.

Recent advancements in self-rewarding language models (SRLMs) (Yuan et al., 2024) have gained increasing attention in the field of LLM alignment, which can efficiently synthesize preference data for iterative preference training. In this method, the single LLM is required to act as two roles, the policy model and the reward model. Given unlabeled prompt data, the LLM first acts as the policy model generating several response candidates. Then, the same LLM acts as the reward model, scoring and ranking these responses. These ranked responses are used as preference pairs to train the LLM with DPO, significantly reducing the reliance on human-annotated data. The above steps can be iteratively repeated to further enhance the performance. However, SRLMs still face challengesin generating reliable and accurate rewards for annotating the preference pairs, which is critical for ensuring the quality of preference data and the alignment performance of LLMs.

To address these challenges, we first formulate a generalized iterative preference fine-tuning framework to analyze the self-rewarding training, where this framework can also be adapted to other iterative preference tuning methods. Through this theoretical framework, we find that the rewarding bias issue in SRLMs comes from the overconfident preference labeling, which enforces the model to distinguish between responses with similar quality. For example, both two responses in Figure 1 have high quality judgments from the human. The SRLM enforces the reward model to make a preference judgment, resulting in noisy and unreliable preference labeling. This can lead to negative impacts on preference tuning the model. Additionally, the iterative training manner can also accumulate the rewarding bias, further diminishing the benefits of self-improvement. From the insights of theoretical analysis, we propose **C**onsistency **R**e**gularized **S**E**lfer-rewarding **I**n**a**nguage **M**odel (CREAM) to mitigate the rewarding bias issue in SRLMs, particularly for broadly accessible 7B-size LLMs. The core idea behind CREAM is that we should not force the model to be overly confident in distinguishing between responses with similar quality. _But how to tell the preference labeling is reliable or not?_ Out of the self-rewarding scenario, we may employ a pool of external reward models to assist in ranking preferences. When two responses are of similar quality, these external models often produce inconsistent rankings.

This inconsistency serves as a signal to indicate the level of confidence in the preference labeling. In self-rewarding scenarios, however, integrating such external reward models is not feasible. Fortunately, due to the iterative nature of self-rewarding training, we can use the reward model from the previous iteration to rank preferences and then compare these rankings with those produced by the current model. This comparison provides an estimate of such consistency rate. With this consistency rate, we can regularize the preference training to prevent the model from learning unreliable preference data, thereby mitigating the rewarding bias issue in SRLMs.

In summary, we first formulates a generalized iterative preference fine-tuning framework to analyze the rewarding bias issue in SRLMs. From the insights of theoretical analysis, we propose CREAM as the primary contribution of this paper. CREAM leverages the consistency of rewards across different iterations for regularized preference training, which can effectively mitigate the rewarding bias issue in SRLMs. Empirical results on a series of natural language benchmarks validate the effectiveness of CREAM in mitigating the rewarding bias issue and enhancing the alignment performance of LLMs.

**Notations.** Vectors are denoted by lowercase boldface letters, such as \(\), and matrices by uppercase boldface letters, such as \(\). For any positive integer \(k\), the set \(1,2,,k\) is denoted by \([k]\). Sets are denoted by calligraphic uppercase letters, such as \(\), with the cardinality of the set represented as \(||\). Without ambiguity, we denote \(_{}\) as the language model parameterized by \(\), \(\) as the input prompt, and \(\) as the output response from the language model. All other notations are defined prior to their first usage. We denote \([]\) as the indicator function.

## 2 Methodology

In this section, we first formulate the generalized iterative preference fine-tuning framework for self-rewarding, RL with AI feedback, and other iterative preference tuning methods. Next, we introduce the motivation behind the proposed consistency regularized self-rewarding method. Finally, we present the practical implementation algorithm of CREAM in details.

### Generalized Iterative Preference Fine-Tuning Framework

We assume that we can access to the dataset with response \(_{}\) and the prompt dataset without response \(_{}\). The objective is to iteratively minimize the following loss with respect to the neural

Figure 1: An example of both two responses are of high quality, which is hard for human to distinguish the preference. While the same model from different iterations have inconsistent rewarding.

network parameter \(\) and a label function \(z\) as

\[(,z)=_{}(;_{ })+_{_{};,^ {}_{_{t}}(|)}[_{}( ;,^{},,z)].\] (2.1)

where the first term \(_{}(;_{})\) aligns the model \(_{}\) to the SFT data. We note here that any potential SFT methods (Ouyang et al., 2022; Yuan et al., 2023; Dong et al., 2023; Chen et al., 2024), or the methods without SFT data (\(_{}=0\)) can be adapted in this framework. The second term \([_{}]\) corresponds to learning from the preference data pair \(\{,^{}\}\) generated by the current model \(_{t}\). The labeling function \(z(,^{},)\{0,1\}\) provides the preference judgment between \(\) and \(^{}\) for the DPO loss, where \(z(,^{},)=1\) means \(^{}\) and \(z(,^{},)=0\) means \(^{}\). The DPO loss \(_{}\) is defined as follows:

\[_{}(;,^{ },,z)=-z(,^{},) ((}(|)}{_{}(|)})-(}( ^{}|)}{_{}(^{}|)}))\\ -(1-z(,^{},))( (}(^{}|)}{_{}(^{}|)})-(}( |)}{_{}(|)})),\] (2.2)

where \(_{}\) is the reference model for KL divergence regularization, and \(()\) is the sigmoid function. The proposed loss \((,z)\) in Eq. (2.1) represents all iterative preference fine-tuning algorithms. For the reinforcement learning (RL) with human feedback (Ouyang et al., 2022), \(z\) is the human preference comparing \(\) and \(^{}\). For the RL with AI feedback, \(z\) is the oracle reward model like GPT-4 (Achiam et al., 2023). For the self-rewarding language model (Chen et al., 2024), \(z\) is given by comparing the reward score generated from the language model itself, often with LLM-as-a-Judge prompting. However, as aforementioned, we note that such prompt rewarding method may only be feasible for larger and advanced LLMs such as Llama-70B (Touvron et al., 2023). For smaller models such as Llama-7B that do not have complex instruction following and reasoning abilities, we instead propose to leverage the intrinsic reward model (Rafailov et al., 2023)

\[r_{}(,)[_{}( |)-_{}(|)]\]

to reward and rank the responses for annotating preference pairs. Therefore, the choice of preference labeling function \(z\) is closely connected with the language model parameter \(\). Then, we introduce the following two-step optimization algorithm to solve Eq. (2.1).

**Step 1.** (Preference-labeling step) Keep \(=_{t}\) fixed, select function \(z\) to minimize \(_{}\). In particular, letting \(=_{t}\) in Eq. (2.2), solution for \(z(,^{},)=_{z}_{}(_{t};,^{},,z)\) is

\[z_{t+1}(,^{},)=[_{ {}_{t}}(|)-_{}(|)_{_{t}}(^{}|)-_{}(^{}|)].\] (2.3)

**Step 2.** (Learning step) Keep \(z\) as of Eq. (2.3), minimize loss function \((,z_{t+1})\) with respect to \(\) and get \(_{t+1}=_{}(,z_{t+1})\).

Different from existing methods, the proposed two-step optimization method directly uses the intrinsic reward model to generate the preference data. This approach is particularly feasible for smaller LLMs, which lack the capacity to effectively use LLM-as-a-Judge prompts (Zheng et al., 2023) for rewarding and ranking. We note that the proposed two-step method is similar to the Expectation-Maximization algorithm and self-training paradigm (Zou et al., 2019). This similarity is supported by the following theorem, which suggests the convergence of the proposed two-step algorithm.

**Theorem 2.1**.: Suppose the optimization \(_{t+1}=_{}(,\,z_{t+1})\) is solvable and the SFT loss \(_{}(;_{}) 0\) for all \(\) and \(_{}\), the proposed two-step optimization method converges.

### Consistency Regularized Self-Rewarding

The generalized framework presented in Eq. (2.1) assumes the human feedback or GPT-4 are all reliable so that the preference labeling function \(z\) is trustworthy. However, for SRLMs, the accuracy of preference labeling is not always guaranteed. Therefore, treating all selected preference labels as "ground truth" by encoding them as hard labels can lead to overconfident mistakes, potentially propagating biases and inaccuracies from the LLMs. Taking Figure 1 as an example, both the two responses \(\) and \(^{}\) are judged by humans to be of high quality. _Forcing the model to be overly confident in distinguishing between these two responses \(\{,^{}\}\) with similar quality can negatively impact the performance of SRLMs during training._This rewarding bias issue motivates us to mitigate such ambiguity by introducing a consistency-regularized self-rewarding language model, CREAM. Specifically, for a pair of responses with very similar quality, their oracle reward scores should ideally be very close to each other. Particularly, when multiple reward models are available, it is likely that some models will rank one response as superior, while others may rank the opposite response as better, resulting in high ranking inconsistency (i.e., low ranking consistency) among these models. Based on this, CREAM aims to prevent the model from learning from preference pairs with low consistency. Instead, it focuses solely on preference pairs with high consistency across different reward models, thereby mitigating the rewarding bias issue and stabilize the learning process to some extent. From the theoretical perspective, we can introduce a regularization term to Eq. (2.1) as

\[(,z)=_{}(; _{})+_{_{}; ,^{}_{_{t}}(|)}[ _{}(;,^{},,z)+_{}(;,^{ },)],\] (2.4)

where the regularization term \(_{}(;,^{},)\) prevents the model \(_{}\) from overconfidence in distinguishing the preference of \(\{,^{}\}\) with similar quality, which is quantified in the following lemma.

**Lemma 2.2**.: Let the random variable \(z=z(,^{},)\) be defined as \(z(,^{},)=[ ^{}|]\). The Bradley-Terry model (Bradley and Terry, 1952) for the probability of \(z\) under parameter \(\) is given by

\[P_{}(z)=P_{}([ ^{}|])=((_{}( |)/_{}(|))-(_{}( ^{}|)/_{}(^{}|))),\]

Letting the regularization \(_{}\) be defined by

\[_{}(;,^{ },) =-((_{}(|)/ _{}(|))-(_{}(^{ }|)/_{}(^{}|)))\] \[-((_{}(^{} |)/_{}(^{}|))-(_{}(|)/_{}(|)) ).\] (2.5)

Then the expected regularized loss under the model \(_{t}\) is given by:

\[_{,^{}_{_{t}}(| )}_{}(;,^{ },)=2\,(u() P_{}()),\] (2.6)

where \(u(z)\) is the uniform binary distribution, i.e., \(u(z=0)=u(z=1)=0.5\).

As Lemma 2.2 suggests, the \(_{}\) will regularize the preference between \(\{,^{}\}\) that has similar quality to a uniform distribution. Then the following theorem suggests that using \(_{}+_{}\) corresponds to the soft-labeled DPO which we implemented in CREAM.

**Theorem 2.3**.: For all \(,^{},,z\), minimizing

\[(,z)=_{}(; _{})+_{_{}; ,^{}_{_{t}}(|)} [_{}(;,^{}, ,z)+_{}(;,^{},)]\]

is equivariant with minimizing

\[(,z) =_{}(; _{})\] \[+_{_{};,^{}_{_{t}}(|)}[_{ }_{}(;,^{}, ,z)+(1-_{})_{}(; ,^{},,1-z)],\] (2.7)

where the \(1-z\) reverses the preference order of \(z(,^{},)\) and \(_{}=(1+)/(1+2)\).

Theorem 2.3 suggests that instead of calculating the regularization term \(_{}\), we can use the soft-labeled DPO to train Eq. (2.7). In particular, when \(=0\), \(_{}=0\) and Eq. (2.7) degenerates to Eq. (2.1). This represents the case where the preference label \(z\) is trustworthy from human or some oracle reward models (e.g., GPT-4). In other words, \(\) represents the _confidence_ of the label function \(z\). Specially, since in our two-step optimization paradigm, the label function \(z\) is directly derived from the previous model \(_{_{t}}\), we can measure the performance of \(_{_{t}}\) using the consistency between model \(_{t}\) and the baseline model (e.g., external reward model) \(_{t}^{}\), defined by

\[()=2_{,^{}_{_{t}}(|)}[^{}| ,_{t}][^{}| ,_{t}^{}],\] (2.8)

and when \( 0\), \(_{} 1-\) representing the consistency of model \(_{t}\) and \(_{t}^{}\). \([^{}|,_{t}]\) means the response \(\) is better than \

### Proposed Algorithm

Equipped with the above two-stage optimization and the consistency-regularized self-rewarding, we are ready to present the implementation of CREAM in Algorithm 1. The whole framework of CREAM is also illustrated in Figure 2. The algorithm starts from the SFT training to obtain the first model parameter \(_{1}\) in Line 2. A similar approach is applied in Yuan et al. (2024) for avoid calculating the \(_{}\) in the future optimization steps. Then for each \(_{j}\) in the unlabeled prompt set \(_{}\), \(N\) response candidates \(\{_{i}\}_{i=1}^{N}\) are sampled in Line 5. Then reward scores of these \(N\) candidates can be calculated according to Rafailov et al. (2023) by

\[r_{ij}=[_{_{i}}(_{i}|_{j})-_ {_{0}}(_{ij}|_{j})]+ Z(_{j}),\] (2.9)

where we use the initial model parameter \(_{0}\) as the reference policy \(_{}\). Since \( 0\) and \( Z(_{j})\) is a constant across different response \(_{i}\) for the same input prompt \(_{j}\), we can drop these factors and calculate rewards in Line 6. Specially, when \(t=1\), the rank \(K_{ij}\) is calculated based on the reference policy \(_{0}\) itself. Thus we instead use the likelihood \(r_{ij}=_{_{0}}(_{ij}|_{j})\) as the reward for this edge case. The rank for these \(N\) candidates are therefore obtained in Line 7, where \(J_{ij}\) means response \(_{ij}\) is in the \(J_{ij}\)-th best in the preference list of \(_{j}\).

Consistency-Regularized Self-Rewarding.As discussed in Eq. (2.8), a baseline model is required to measure the consistency. In the self-reward scenario, it is infeasible to add an external reward model as the baseline model. Fortunately, we can employ the model before last update \(_{t-1}\) as the baseline model \(^{}_{t}\) (i.e., last iteration's model) for evaluating the consistency of the model \(\), thanks to chances provided by iterative training manner. Such a procedure helps mitigate the training error introduced in \(t-1\)-th step before obtaining \(_{t}\). Considering a pair of tied preference pair \(,^{}\) both performing well, as demonstrated in Figure 1. \(P[^{}|,_{t}]\) will be oscillating around \(0.5\) when \(t\) grows due to the random noise. Otherwise \(P[^{}|,_{t}]\) might consistently converge to \(0\) or \(1\). Due to this oscillation, the consistency between \(_{t-1}\) and \(_{t}\) on this specific preference pair would be low, and the algorithm will learn less from this noninformative preference pair thus stabilize this oscillation.

Specifically, we calculate the rank of these \(N\) candidates using \(_{t-1}\) in Line 9 and then use the Kendall's Tau coefficient (Kendall, 1938) denoted by

\[_{j}=_{1 i<i^{} N}[[ (J_{ij}-J_{i^{}j})(K_{ij}-K_{i^{}j})>0]-[(J_ {ij}-J_{i^{}j})(K_{ij}-K_{i^{}j})<0]].\] (2.10)

Kendall's Tau coefficient is a widely used coefficient (McLeod, 2005; Abdi, 2007) to measure the consistency of two ranking sequences. Basically, when two sequences perfectly aligns, \(_{j}=1\) andwhen two sequence never aligns, \(_{j}=-1\). The following lemma draws the further connection between the Kendall's Tau and the regularization parameter \(\) proposed in Section 2.2.

**Lemma 2.4**.: Suppose the \(N\) response candidate \(\{_{ij}\}_{i}\) is i.i.d. given the prompt \(_{j}\), then

\[[_{j}]=1-4_{,^{}_{ _{t}}(|_{j})}\,[ ^{}|_{j},_{t}]\,[ ^{}|_{j},_{t-1}]= 1-2,\]

where the expectation is taken over the randomness of sampling the \(N\) candidate set.

Given Lemma 2.4, we can recover \(_{} 1-=(1+_{j})/2\) and we use average all \(_{j}\) for all \(_{j}_{}\) in Line 11. Finally, in Line 12, we compose the preference dataset by selecting the best response \(_{j}^{+}=_{i^{+}j}\) and the worst response \(_{j}^{-}=_{i^{-}j}\) which is similar with (Yuan et al., 2024).

\[_{}=\{(_{j},_{i^{+}j},_{i^ {-}j})|_{j}_{},i^{+}=*{arg\,min }_{i}J_{ij},i^{-}=*{arg\,min}_{i}J_{ij}\}\] (2.11)

Following Theorem 2.3, we also prepare the reverse DPO dataset by switching the best response and the worst response by

\[_{}=\{(_{j},_{i^{-}j},_{i ^{+}j})|_{j}_{},i^{+}=*{arg\, min}_{i}J_{ij},i^{-}=*{arg\,min}_{i}J_{ij}\}\] (2.12)

and update \(_{t+1}\) by minimizing the empirical loss of Eq. (2.7) in Line 14. The detailed proof of theorems and lemmas are provided in the Appendix B.

## 3 Experiment

### Experimental Setup

**Data.** In our experiments, we use Open Assistant dataset (Kopf et al., 2024) and only reserve about 3.4K human-annotated examples as the seed SFT data \(_{}\). To construct the unlabeled prompt dataset \(_{}\), we mix prompts of \(_{}\) with the train split of each downstream task (only reserve the prompts) including (1) ARC-Easy/Challenge (Clark et al., 2018), (2) OpenBookQA (Mihaylov et al., 2018), (3) SIQA (Sap et al., 2019), and (4) GSM8K (Cobbe et al., 2021). Finally, this process results in a total of 21K prompts in \(_{}\), which we distribute equally across iterative self-rewarding trainings.

**Models.** Due to limited computational resources, we mainly conduct experiments with two LLMs with about 7B parameters, including Llama-3 (Dubey et al., 2024) and Llama-2 (Touvron et al., 2023).

**Baseline Methods.** To validate our findings, we mainly compare our method with SRLM (Yuan et al., 2024) which uses the same LLM to serve as both the policy and reward model to generated preference data for iterative training. Additionally, we introduce a variant of RL with AI feedback (Guo et al., 2024), referred to as "Oracle". In this variant, the reward model in SRLM is replaced with an external reward model to demonstrate the upper bound performance of SRLM. Specifically, we use InternLM2 (Cai et al., 2024), a specialized trained reward model, to provide the reward scores for the generated responses. We further enhance Oracle's rewarding by leveraging the labels from downstream tasks to improve the rewarding accuracy.

**Implementation Details.** In our experiments, we fine-tune the initial model (M0) on the seed SFT data for \(3\) epochs with a learning rate of \(1e-6\), resulting in model M1. Following SRLM approach, we then iteratively fine-tune the model using the preference learning objective two additional iterations, producing models M2 and M3. In the preference training of each iteration, we set \(=0.1\) of DPO loss, and fine-tune the model for \(1\) epoch with a learning rate of \(1e-6\).

### Main Results

The main results are shown in Table 1 which also report the performance of GPT-4o for reference. From these results, we observe the following: (1) The Standard SRLM fails to achieve satisfactory performance, particularly with Llama2 which has relatively weaker foundation performance even after SFT fine-tuning (M0 \(\) M1), which indicates its limitations for 7B-level LLMs. (2) Compared to SRLM, cream achieves a significant improvement across almost all downstream tasks, showing the advantage of introducing the proposed regularization method. (3) SRLM equipped with an oracle reward model (Oracle) can ensure high rewarding accuracy for annotations of self-generated preference data, thereby achieving the best performance overall. Notably, for Llama3, CREAM even outperforms Oracle except on SIQA dataset, showcasing the superior performance of CREAM. This superiority underlines the success of the proposed method in mitigating the rewarding bias issue. (4) The consistent performance improvements of CREAM across iterations validate the effectiveness of the proposed regularization method in mitigating the rewarding bias issue.

### Analysis

#### 3.3.1 Analysis of Rewarding

**Rewarding Consistency.** We first examine the consistency of rewards of different methods using their corresponding models from the last iteration in Table 2. Here, we use the proposed Consistency Rate \(\), Kendall correlation coefficient \(\), Spearman correlation coefficient, and TopOrder metrics to measure the consistency, where the TopOrder metric evaluates whether the final paired preference data remains the same, calculated as follows:

\[_{j}=[ J_{j}= K_{j}] [ J_{j}= K_{j}],\]

where \(J_{j}\) and \(K_{j}\) are the rankings of the responses provided by current model and the last iteration's model, respectively. This metric assesses whether both the least preferred and most preferred responses are consistently ranked across iterations. The results confirm that SRLMs exhibit a rewarding consistency issue. In contrast, our method CREAM can keep the ranking consistency across iterations thanks to the explicit regularization in the training.

**Prompt Rewarding v.s. DPO Rewarding.** As aforementioned, 7B level LLMs struggle with generating accurate rewards when using LLMs-as-a-Judge prompting due to their limited capacity. Both Table 5 and Figure 3 clearly show that the SRLM with prompt rewarding is not effective for smaller LLMs, as the performance starts to decrease at the first iteration (M1 \(\) M2) when trained on the self-rewarded preference data. In contrast, the adopted DPO rewarding method can be more suitable for such small LLMs. This is primarily because DPO rewarding is intrinsically aligned with the model's learning objective.

   Model & Method & Arc-Easy & Arc-Challenge & OpenBookQA & SIQA & GSM8K \\  GPT-4o & CoT & 94.57 & 94.71 & 96.60 & 79.63 & 92.27 \\   & M0 & 86.29 & 80.37 & 86.00 & 68.58 & 78.01 \\  & M1 & 86.78 & 80.14 & 86.40 & 69.50 & 78.39 \\   & Oracle M2 & 89.60 \(\) & 82.17 \(\) & 90.00 \(\) & 72.88 \(\) & 80.82 \(\) \\  & Oracle M3 & 89.31 \(\) & 81.31 \(\) & 90.20 \(\) & 73.75 \(\) & 76.04 \(\) \\   & SRLM M2 & 87.79 \(\) & 80.38 \(\) & 87.80 \(\) & 70.95 \(\) & 78.01 \(\) \\  & SRLM M3 & 87.17 \(\) & 81.23 \(\) & 87.30 \(\) & 70.37 \(\) & 77.48 \(\) \\   & CREAM M2 & 88.89 \(\) & 80.39 \(\) & 88.00 \(\) & 69.79 \(\) & 81.04 \(\) \\  & CREAM M3 & **89.52 \(\)** & **83.36 \(\)** & **90.20 \(\)** & **72.06 \(\)** & **81.73** \(\) \\   & M0 & 61.07 & 48.98 & 62.20 & 50.36 & 23.65 \\  & M1 & 60.44 & 48.46 & 63.20 & 50.77 & 23.88 \\    & Oracle M2 & 70.20 \(\) & 55.03 \(\) & 75.40 \(\) & 63.66 \(\) & 30.02 \(\) \\   & Oracle M3 & 71.72 \(\) & 55.80 \(\) & 77.20 \(\) & 62.44 \(\) & 29.57 \(\) \\    & SRLM M2 & 58.67 \(\) & 46.67 \(\) & 59.80 \(\) & 49.69 \(\) & 25.17 \(\) \\    & SRLM M3 & 65.5 \(\) & 34.47 \(\) & 49.20 \(\) & 48.06 \(\) & 21.24 \(\) \\    & CREAM M2 & 58.97 \(\) & 47.53 \(\) & 62.80 \(\) & 50.43 \(\) & 24.41 \(\) \\    & CREAM M3 & **62.08 \(\)** & **48.81 \(\)** & **64.60 \(\)** & **51.22 \(\)** & **25.85 \(\)** \\   

Table 1: Main results of each method on test sets of downstream tasks. The exact match accuracies are reported. The “\(\)” and “\(\)” indicate the performance improvement and degradation compared to the method’s last iteration (e.g., M1 \(\) M2 and M2 \(\) M3), respectively. The best performance between SRLMs and CREAM is highlighted in bold.

   Method & Iteration & Consistency \(^{}\) & Kendall \(\) & Spearman \(\) & TopOrder \(\) \\  SRLM & M2 & 0.39 \(\) 0.21 & -0.22 \(\) 0.41 & 0.36 \(\) 0.24 & 0.03 \(\) 0.18 \\ CREAM & M2 & 0.73 \(\) 0.18 & 0.46 \(\) 0.35 & 0.77 \(\) 0.19 & 0.19 \(\) 0.39 \\  SRLM & M3 & 0.46 \(\) 0.19 & -0.08 \(\) 0.38 & 0.50 \(\) 0.22 & 0.12 \(\) 0.33 \\ CREAM & M3 & 0.92 \(\) 0.09 & 0.84 \(\) 0.19 & 0.95 \(\) 0.07 & 0.59 \(\) 0.49 \\   

Table 2: Ranking consistency of CREAM and SRLM on M2 and M3 using Llama3.

**Ranking Accuracy.** We present the ranking accuracy in Figure 3 to provide an intuitive comparison the performance of the rewarding performance across different methods. The results include the ranking accuracy on self-generated preference data and the RewardBench (Lambert et al.) dataset, both of which is formulated as a ranking task to predict the preferred one between two responses. We use the self-generated preference data obtained by self-rewarding with ground truth ranking labels, for testing the model's in-domain ranking performance. The RewardBench dataset is used to assess the generalizability of the models beyond the training domain. CREAM consistently achieves higher ranking accuracy than baseline methods, which promises more reliable preference data for training.

#### 3.3.2 Reliability of Self-Consistency

The most straightforward way to enhance the rewarding and ranking accuracy is by incorporating external reward models, such as the SRLM variant "Oracle" used in our experiments. The theoretical analysis in Eq. (2.8) suggests that we can mitigate the rewarding bias issue by calculating the ranking consistency between current model and other available reward models. However, it is not always feasible to have access to such external reward models in practice, such as the self-rewarding scenario. Thus, we instead propose to use the last iteration's model as the reference reward model to measure the consistency of rewards. To test this approach, we fine-tune the same M1 model using CREAM with two different reference reward models: the rewarding function of Oracle and ours model from the last iteration. As shown in Table 3, using a strong reward model as the consistency model can bring better regularization effect, especially for Llama2. However, we find that the last iteration's model also provides a reasonably reliable consistency signal for Llama3.

#### 3.3.3 Consistency Measurement

Besides the adopted Kendall \(\) coefficient, other metrics can also be used to measure the consistency between two preference ranking lists, such as Spearman coefficient (Spearman, 1904) and the aforementioned TopOrder method. We conduct a comparison experiments of using different consistency measurement methods in Table 4. We can observe that: (1) All these measurements are effective with CREAM, indicating the generalization and applicability of our regularized training approach. (2) Kendall correlation coefficient generally yields higher scores across various datasets compared to Spearman and TopOrder methods.

## 4 Conclusion

In this paper, we first formulate a generalized iterative preference fine-tuning framework for self-rewarding language models (SRLMs), which is also applicable to other iterative preference training methods. Then, we highlight the rewarding bias that emerges from overconfident preference labeling, which is particularly problematic for smaller LLMs, such as those with 7B parameters. This rewarding bias results in the accumulation of noisy and unreliable preference data, harming the preference training and hindering alignment performance of LLMs. To address this issue, we proposed the **C**onsistency **R**egularized **s**E**lf-Rewarding **I**A**nguage **M**odel (CREAM), which leverages the consistency of rewards across different iterations as a regularization signal. This approach allows the model to learn more selectively, emphasizing reliable preference data and avoiding overconfidence in preference labeling. Our experimental results on various natural language benchmarks demonstrate the effectiveness of the proposed method in mitigating the rewarding bias issue and improving the performance of SRLMs. We believe that these findings can provide valuable insights for future research on self-improvement methods of LLM alignment.

   Method & Arc-E & Arc-C & OBQA & SIQA & GSMsk \\  Spearman M2 & 86.95 & **82.00** & 85.40 & 70.05 & 78.77 \\ TopOrder MA & 87.25 & 80.12 & 86.88 & **70.83** & 79.75 \\ Kendall M2 & **88.89** & 80.89 & **88.00** & **69.79** & **81.04** \\  Spearman M3 & 88.76 & 81.83 & 90.00 & 70.98 & 79.15 \\ TopOrder M3 & 88.51 & 80.37 & 87.40 & 71.03 & 79.76 \\ Kendall M3 & **89.52** & **83.36** & **90.20** & **72.06** & **81.73** \\   

Table 4: Performance of CREAM with different consistency measurements.

   Method & Arc-E & Arc-C & OBQA & SIQA & GSMsk \\  Llama3 M1 & 86.78 & 80.14 & 86.40 & 69.50 & 78.39 \\ CREAM & 88.89 & 80.89 & 88.00 & 69.79 & 81.04 \\ CREAM + Oracle & 88.51 & 81.06 & 86.20 & 72.21 & 79.91 \\  Llama2 M1 & 60.44 & 48.46 & 63.20 & 50.77 & 23.88 \\ CREAM & 58.97 & 47.53 & 62.80 & 50.43 & 24.41 \\ CREAM + Oracle & 62.42 & 48.72 & 66.00 & 51.13 & 22.52 \\   

Table 3: Comparison of CREAM with oracle reward model and last iteration’s model.