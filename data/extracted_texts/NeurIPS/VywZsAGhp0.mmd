# Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive Residual Module

Jingbo Zhou\({}^{1,2}\), Yixuan Du\({}^{2,3}\), Ruqiong Zhang\({}^{2,3}\), Jun Xia\({}^{1}\), Zhizhi Yu\({}^{3}\),

**Zelin Zang\({}^{1}\), Di Jin\({}^{3}\), Carl Yang\({}^{4}\), Rui Zhang\({}^{2}\)** **Stan Z. Li\({}^{1}\)**

\({}^{1}\)Westlake University, \({}^{2}\)Jilin University, \({}^{3}\)Tianjin University, \({}^{4}\)Emery University

{zhoujingbo, stan.zq.li}@westlake.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Graph Neural Networks (GNNs), a type of neural network that can learn from graph-structured data through neighborhood information aggregation, have shown superior performance in various downstream tasks. However, as the number of layers increases, node representations become indistinguishable, which is known as over-smoothing. To address this issue, many residual methods have emerged. In this paper, we focus on the over-smoothing issue and related residual methods. Firstly, we revisit over-smoothing from the perspective of overlapping neighborhood subgraphs, and based on this, we explain how residual methods can alleviate over-smoothing by integrating multiple orders neighborhood subgraphs to avoid the indistinguishability of the single high-order neighborhood subgraphs. Additionally, we reveal the drawbacks of previous residual methods, such as the lack of node adaptability and severe loss of high-order neighborhood subgraph information, and propose a **Posteriori-Sampling-based, Node-Adaptive Residual module (PSNR)**. We theoretically demonstrate that PSNR can alleviate the drawbacks of previous residual methods. Furthermore, extensive experiments verify the superiority of the PSNR module in fully observed node classification and missing feature scenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.

## 1 Introduction

GNNs have emerged in recent years as the most powerful model for processing graph-structured data and have demonstrated exceptional performance across various fields, such as social networks , recommender systems , and drug discovery . Through the message-passing mechanism that propagates and aggregates information of neighboring nodes, GNNs provide a general framework for learning information on graph structure. Despite the remarkable success, according to previous studies , GNNs show significant performance degradation as the number of layers increase. One of the main reasons for this situation is over-smoothing . Over-smoothing refers to the phenomenon in which node representations become increasingly similar to each other as GNNs recursively aggregate more neighborhood information. This indistinguishability will inevitably degrade the performance of deep GNNs, restricting their ability to effectively model long-range dependencies among multi-hop neighbors.

Several methods have recently been proposed to alleviate over-smoothing in deep GNNs. According to , these methods fall into three categories: Normalization and Regularization , Change of GNN dynamics , and Residual connections . Among all of them, the residual-basedmethod is inspired by the success of residual neural networks (ResNets)  in computer vision. This type of method introduces a residual connection to the GNNs architecture. For example, JKNet  learns node representations by aggregating the outputs of all previous layers at the last layer. DenseGCN  concatenates the results of the current layer and all previous layers as the node representations of this layer. APPNP  uses the initial residual connection to retain the initial feature information with probability \(\), and utilizes information aggregated at the current layer with probability \(1-\). GCNII  shares a similar framework with APPNP, and it further introduces an identical mapping to avoid overfitting.

In this paper, we study the over-smoothing issue in GNNs, with a particular emphasis on residual methods. First, we revisit the over-smoothing phenomenon of GNNs from the new perspective of overlapping neighborhood subgraphs and explain the essential reason why the residual method can alleviate the over-smoothing. In essence, these methods mainly use multiple neighborhood subgraph aggregations to alleviate the indistinguishability of the single neighborhood subgraph aggregation, thereby improving the performance of the model. On this basis, we find that these residual methods often lack node adaptivity in utilizing multi-order neighborhood subgraph information, and at the same time, they still struggle to mitigate information loss when dealing with high-order neighborhood subgraphs, which hinders further improvement in the performance of deep GNNs. Although some residual methods, such as DenseGNN, can avoid these drawbacks, they tend to introduce more parameters at deeper layers. This can lead to significant memory consumption and is prone to gradient explosion, limiting the scalability of the methods.

Considering these limitations, we propose a **Posteriori-Sampling-based Node-Adaptative Residual Module (PSNR)**. More specifically, this module introduces a graph encoder to learn the posterior distribution of the required residual coefficients for each node in different layers with minor overhead. And then, we can obtain the specific fine-grained node-adaptive residual coefficients by sampling from the distribution. The contributions of this paper are as follows:

* _Perspective:_ We revisit the over-smoothing issue from a novel perspective of high-order neighborhood subgraph coincidences and explain why the residual methods can alleviate it. Through this lens, we reveal several significant drawbacks of prior residual methods that limit the performance and scalability of GNNs.
* _Method:_ We propose PSNR, a lightweight and model-agnostic module to mitigate the drawbacks of previous residual methods and provide theoretical justification for its advantages.
* _Experiments:_ Extensive experiments verify that the PSNR module can effectively mitigate oversmoothing and further improve the performance of GNNs, especially in the case of missing feature that require deep GNNs.

## 2 Related Work

### Notations

A connected undirected graph is represented by \(=(,)\), where \(=\{v_{1},v_{2},,v_{N}\}\) is the set of \(N\) nodes and \(\) is the edge set. The node features are represented in the matrix \(^{N d}\), where \(d\) represents the length of the feature. Let \(\{0,1\}^{N N}\) denotes the adjacency matrix and \(_{ij}=1\) only if an edge exists between nodes \(v_{i}\) and \(v_{j}\). \(^{N N}\) is the diagonal degree matrix, where each element \(d_{i}\) represents the number of edges connected to node \(v_{i}\). \(}=+\), \(}=+\) represent the adjacency matrix and degree matrix with self-loop, respectively.

### Graph Neural Networks

A GNN layer updates the representation of each node via aggregating itself and its neighbors' representations. Specifically, a layer's output \(^{}\) consists of new representations \(^{}\) of each node computed as:

\[^{}_{i}=_{}(_{i},(\{_{j} v_{j},(v_{i},v_{j})\} )),\] (1)

where \(^{}_{i}\) indicates the new representation of node \(v_{i}\) and \(_{}()\) denotes the update function. The difference between different GNNs lies in the update function \(_{}()\) and the \(()\) function, which are also key to the performance of GNNs. Graph Convolutional Network (GCN) is a classical massage-passing GNNs lossy layer-wise propagation rule:

\[_{k+1}=(}^{-}} }^{-}_{k}_{k}),\] (2)

where \(_{k}\) is the feature matrix of the \(k\)-th layer, \(_{k}\) is a layer-specific learnable weight matrix, \(()\) denotes an activation function.

### Residual connection

Several works have used residual connection to alleviate the over-smoothing issue. Common residual connections for GNNs are summarized in Table 1, where \(_{k}\) represents the output of the \(k\)-th layer, \(_{k}\) is a learnable weight matrix for the \(k\)-th layer, \(\) serves as a hyperparameter denoting the residual coefficient, and \(()\) denotes an activation function. Additionally, in DenseGNN, \(\) represents a function with the concatenation of outputs from all previous layers as the input to the current layer, while in JKNet, \(\) refers to the aggregation of all previous representations through concatenation, max-pooling, or LSTM-attention only at the final layer. Details can be found in Appendix B.

## 3 Why does the residual method alleviate over-smoothing?

In this section, we revisit over-smoothing from the perspective of overlapping high-order neighborhood subgraphs. Based on this, we elucidate the role of various residual methods in alleviating over-smoothing and identify their shortcomings.

### Revisit over-smoothing from the perspective of neighborhood subgraphs overlapping

For message-passing GNNs without the residual connection, the information domain of each node after \(k\)-layer aggregation is a corresponding \(k\)-order neighborhood subgraph. Intuitively, the size of its \(k\)-order neighborhood subgraph grows exponentially as \(k\) increases, leading to a significant increase in the overlap between the \(k\)-order neighborhood subgraphs of different nodes. As a result, the aggregation result of different nodes on their respective \(k\)-order neighborhood subgraphs becomes indistinguishable. This explanation can be partially validated from the perspective of node degrees. Considering nodes with high degrees tend to have larger neighborhood subgraph overlap, the correlation between neighborhood subgraph overlap and oversmoothiong can be validated if nodes with higher degree exhibit more pronounced over-smoothing. To verify this point, we conduct experiments on three graph datasets: Cora, Citeseer, and Pubmed. Initially, nodes are grouped based on their degrees, with nodes having degrees falling within the range of \([2^{i},2^{i+1})\) assigned to the

  
**Residual Connection** & **Corresponding GCN** & **Formula** \\  Res & ResGCN  & \(_{k}=_{k-1}+(}^{-} }}^{-}_{k-1}_{k-1})\) \\ InitialRes & APPNP  & \(_{k}=(1-)}^{-}}}^{-}_{k-1}+\) \\ Dense & DenseGCN  & \(_{k}=(,_{1},,_{k-1})\) \\ JK & JKNet  & \(_{output}=(_{1},,_{k-1})\) \\   

Table 1: Common residual connections for GNNs.

Figure 1: SMV for node groups of different degrees. More results are shown in Appendix C.

\(i\)-th group. Subsequently, we perform aggregation with different layers of GCN and GAT and then calculate the degree of smoothing of the node representations within each group separately. The metric proposed in  is used to measure the smoothness of the node representations within each group, namely smoothness metric value (\(\)), which calculates the average distances between the nodes within the group:

\[()=_{i j}( _{i,:},_{j,:}),\] (3)

where \((,)\) denotes the normalized Euclidean distance between two vectors:

\[(,)=\|}{\| \|}-}{\|\|}\|_{2}.\] (4)

From the definition, a smaller value of \(\) indicates a greater similarity in node representations. We show the result of GAT in Figure 1. More results can be found in Appendix C. It can be observed that the groups of nodes with higher degree tend to be more similar to each other within the group in different layers. This finding supports our claim.

### The role of residual method in alleviating over-smoothing

After verifying the conclusion that neighborhood subgraph overlap leads to over-smoothing, a natural idea is to alleviate the overlap of the single neighborhood subgraph by utilizing multi-order neighborhood subgraph aggregations. In the following section, we show that the previous \(k\)-layer residual-based GNNs essentially represent different forms of utilizing neighborhood subgraph aggregations from 0 to \(k\) orders.

In the rest of this paper, we take GCN, a classical residual-free message-passing GNN, as an example. Assuming that **H** is non-negative, the ELU function and the weight matrix can be ignored for simplicity. Combined with the formula of GCN in Eq. 2, the \(k\)-order neighborhood subgraph aggregation can be formulated as \(^{k}\), where \(=}^{-}}}^{-}\). To show more intuitively how different residual models utilize multi-order neighborhood subgraph aggregation \(^{k}\), we rewrite their formula in Table 2. Details of the derivation of the closed-form formula in this part are given in Appendix D. As can be observed, the output of GCN's residual-based variants contains multi-order matrix products that represent different order neighborhood subgraph aggregations from \(0\) to \(k\). There are two main ways to exploit them: **(1)** Summation, such as ResGCN and APPNP. Such methods employ linear summation over the aggregation of different order neighborhood subgraphs; **(2)** Aggregation functions, such as DenseGNN and JKNet. Such methods make direct and explicit exploitation of different order neighborhood subgraph aggregations through operations such as concatenation.

However, the utilization of multi-order neighborhood subgraph aggregations in these methods presents the following issues: Firstly, the summation methods all use a fixed coefficient to sum the neighborhood subgraph aggregations. Consequently, these methods inherently presume that the information from the neighborhood subgraph of the same order is equally important for different nodes, which lacks node adaptivity. Secondly, for ResGNN, APPNP, and JKNet, when the number of layers increases, the output of these methods still involves many high-order matrix products that are over-smoothed. This leads to severe information loss when aggregating high-order neighborhood subgraphs, which in turn degrades model performance at deeper layers. Although DenseGNN seems to alleviate the above issues to some extent, the recursive use of all previous neighborhood subgraph aggregation would introduce more parameters as the model deepens. This increases memory consumption and raises the risk of gradient explosion at deeper layers.

  
**Model** & **Closed/Iterative form formulas** \\  ResGCN & \(_{k}=_{j=0}^{k}()^{i}\) \\ APPNP & \(_{k}=(1-)^{k}^{k}+_{j=0}^{k- }(-1)^{j-i}(1-)^{i}^{i} \) \\ JKNet & \(_{k}=(,,^{k-1})\) \\ DenseGCN & \(_{k}=(_{k-1},,_{1},_{ 0})\) \\   

Table 2: Utilization of neighborhood subgraphs by various residual methods.

The Proposed Method PSNR

### Methodology

To solve the above issues, we propose a node-adaptive and lightweight residual module named PSNR. The motivation is to learn the adaptive residual coefficients for each node, thereby achieving fine-grained and node-level neighborhood subgraph aggregation to improve the performance of GNNs. However, directly learning these coefficients through backpropagation presents significant challenges. The primary challenge lies in the lack of transferability of learned coefficients. Specifically, in tasks such as semi-supervised node classification, nodes in the test and validation sets often cannot propagate information to the training nodes through multiple message-passing steps. Therefore, we cannot learn effective coefficients for these nodes during the training phase.

As a remedy, we regard node-level residual coefficients as hidden parameters. Our strategy involves estimating their posterior distribution \((_{k}|,_{k},k)\). In most cases, since the training, validation, and test sets originate from the same distribution, the posterior distribution learned from the training data possesses transferability to the validation and testing nodes. We can assume the posterior distribution to be Gaussian:

\[_{k}((,_{k},k),^{2}(,_{k},k)).\]

A graph encoder can be used to model this distribution. This encoder leverages graph topology and node information as inputs. Subsequently, to enable back-propagation, we employ the reparameterization trick. This technique enables us to represent the sampling process from the aforementioned distribution as follows:

\[_{k}=(,_{k},k)+(, _{k},k),\ (0,1).\]

Furthermore, we parameterize \((,_{k},k)\) and \((,_{k},k)\) as an arbitrary GNN layer. While the posterior distributions of residual coefficients vary across different layers, we do not parameterize a specific encoder for each layer. Instead, we employ the same graph encoder and use the positional embedding generated from the layer number \(k\) to differentiate the posterior distributions of residual coefficients for various layers. Consequently, the PSNR module can be formulated as follows:

\[_{k-1}^{} =(_{k-1})\] (5) \[_{k} =_{1}+(diag(_{k-1}))(_{1}- _{k-1}^{}),\ _{k-1}(_{k-1},{_{k-1}}^{2})\] \[_{k},_{k} =_{,}(_{1}-_{ k}^{}+\,(k)),\]

where \(()\) is the \(k\)-th layer of any backbone GNN, \(_{k}\) denotes the node representation matrix of the \(k\)-th layer, and \(_{k}^{}\) represents the output matrix of the \(k\)-th layer. The first equation corresponds to the aggregation operation of the backbone GNN at the \(k\)-th layer. \(_{k}\) represents the node-level residual coefficient at the \(k\)-th layer, where the element \(_{k}^{(i)}\) corresponds to the residual coefficient at the \(i\)-th node. In addition, \(_{k}\) follows a high-dimensional Gaussian distribution: \((_{k},{_{k}}^{2})\), and each time before each residual calculation, the distribution is first sampled to obtain the exact residual coefficients. \(_{k}\) and \(_{k}\) represent the mean and standard deviation of the distribution, respectively. \(diag(_{k})\) represents a diagonal matrix transformed from \(_{k}\), where the \(i\)-th diagonal element is precisely the \(i\)-th element of \(_{k}\). \(()\) represents the sigmoid function, which constrains the residual coefficient to \((0,1)\). \(_{,}()\) is a posterior encoder, which can be any graph convolution layer that is independent of the backbone GNN. \((k)\) represents the positional embedding  with layer number \(k\) as input. \(\) is a learnable coefficient serving as the layer embedding factor. The analysis of the residual coefficients can be found in Appendix H.

Also, it is noteworthy that PSNR introduces randomness by sampling residual coefficients during both training and testing phases, thereby adding learnable random perturbations. This is different to other methods including DropEdge , GRAND  and DropMessage  that only incorporate randomness during the training phase and primarily introduce perturbations to node features and graph structure. We provide the theoretical analysis for this design in Section 4.2. Additionally, we clarify the difference between PSNR and other subgraph-based methods in Appendix F.

Figure 2: The framework of PSNR Module.

### Theoretical justifications on the advantages of the PSNR module

In this section, we theoretically show that the PSNR module achieves finer-grained and node-adaptive neighborhood subgraph aggregations while avoiding the loss of high-order subgraph information. Firstly, combining with Equation 5, the matrix form of the iterative formula for PSNR-GCN is:

\[_{k}=_{1}+_{k-1}(_{1}-}^{-1/2}}}^{-1/2}_{k-1}).\]

For simplicity, we use \(_{k}\) to denote \(diag(_{k})\). Subsequently, based on this recursive form of formula, we derive the closed-form expression of PSNR-GCN as:

\[_{k}=_{i=2}^{k-1}_{j=i}^{k-1}}_{j}( _{i}-_{i-1})+_{i=1}^{k-1}}_{i}( _{1}+_{1})-_{k-1},\]

where \(}_{i}=-_{k-1}\), and \(_{k}=-(_{k}+)^{-1}( +_{k})_{1}\). The detailed derivation of this formula can be found in Appendix E. The first two terms consist of cumulative product terms of different orders, similar to the form of \(^{k}\), thus approximating a new version of neighborhood subgraph aggregation. Additionally, the formula involves the aggregation of all neighborhood subgraphs from 1 to \(k\) orders. This ensures that our method, like other residual methods, can comprehensively utilize multi-order neighborhood subgraph aggregations to enhance performance. Furthermore, since \(_{k}\) is a diagonal matrix computed by a learnable posterior encoder with graph structure, node feature and layer number as input, the neighborhood subgraph aggregation of PSNR-GCN is fine-grained and node-adaptive. This sets it apart from methods like ResGCN and APPNP.

Beyond that, we can also demonstrate that the PSNR module can reduce the information loss of high-order subgraph aggregation, thereby further improving the performance of GNNs at deeper layers. Specifically, we aim to prove that as the order \(k\) increases, the smoothing rate of the cumulative product terms in PSNR is slower than that of the matrix power terms in other methods. Since we need to analyse the smoothing rate, which involves analyzing the asymptotic behavior of cumulative product terms or matrix power terms as the order increases, we can generalize the problem setup. Therefore, we only need to prove the following proposition to support our claim.

**Proposition 1**: _Let \(=\{_{j}=_{j}|j_{0}\}\), where \(_{j}\) represents a random diagonal matrix with each diagonal element \(_{j,ii}\) satisfying \(0<_{j,ii}<1\), and let \(\) be any feature matrix. Then, as the order \(k\) increases, the product of elements in the set \(\) and the matrix \(\), denoted as \(^{(k)}=_{i=1}^{k}_{i}\), converges to a rank-one matrix with identical rows slower than \(_{GCN}^{(k)}=^{k}\)._

Since the diagonal elements of the diagonal matrix \(\) are all results of the sigmoid function, we can assume that they have a lower bound \(>0\). Therefore, in this setting, each element \(_{i}\) in the set \(\) is a row-stochastic matrix satisfying the following property.

**Property 1**: _For a row-stochastic matrix \(_{k}\), there exists an \(>0\) satisfying the following conditions: 1. \(_{k,ij} 1\), if \((i,j)\), 2. \(_{k,ij}=0\), if \((i,j)\)._

We can refer to the conclusion in .  describes the attention matrix of GAT as a row-stochastic matrix satisfying Property 1. Leveraging mathematical tools such as joint spectral radius from the perspective of dynamical systems, this work proves that the convergence rate of GAT is bounded below by GCN. Due to all the matrix \(_{i}\) from \(\) also satisfies Property 1, this conclusion can be borrowed to prove Proposition 1, thereby demonstrating that the PSNR module can alleviate the loss of high-order neighborhood subgraph information. Furthermore, this conclusion also explains the introduction of randomness during both the training and testing phases. This random perturbation further reduces the smoothing rate, thereby enhancing the model's performance.

### Complexity analysis

We take PSNR-GCN as an example to provide a complexity analysis of the PSNR module. The time complexity of a vanilla GCN layer mainly comes from the matrix multiplication of \(\) and \(\), hence its complexity is \((n^{2}d)\). And the main computational parts of PSNR module are the computation of mean and standard deviation, sampling of \(p_{k}^{(i)}\), scalar multiplication and matrixaddition, which correspond to a complexity of _O_(\(n^{2}d\)), _O_(\(n\)), _O_(\(nd\)), and _O_(\(nd\)), respectively. Thus the time complexity of the PSNR module is _O_(\(n^{2}d\)) and the time complexity of a GCN layer equipped the PSNR module is _O_(\(n^{2}d\)). As for space complexity, PSNR module needs to store the computed mean and variance for each node, i.e., \(O(2n)\), which can be approximately considered as \(O(n)\). Section 5.6 compares memory consumption with other residual methods on large graphs.

## 5 Experiment

In this section, we assess the performance of the PSNR module in comparison to other methods and answer the following research questions (**RQ**): **RQ1.** How well does the PSNR alleviate oversmoothing? **RQ2.** How does PSNR perform compared to other baseline when used with different backbone? **RQ3.** Can PSNR enable deeper networks to perform better under the missing feature scenario? **RQ4.** How scalable is the PSNR on large graph datasets?

### Datasets

We conducted experiments on ten real-world datasets, including three citation network datasets, i.e., Cora, Citeseer, Pubmed , two web network datasets, i.e., Chameleon and Squirrel , co-author/co-purchase network datasets, i.e., Coauthor-CS , Amazon-Photo  and three larger datasets, i.e., Flickr , Coauthor-Physics  and Ogbn-arxiv . More details of these datasets and data-splitting procedures can be found in Appendix G.

### Baselines and experimental settings

We consider two fundamental GNNs, GCN  and GAT . For GCN, we test the performance of PSNR-GCN and its residual variant models, including ResGCN , DenseGCN , GCNII  and JKNet . For GAT, we directly equip it with the following residual module: Res-GAT, InitialRes-GAT, Dense-GAT, JK-GAT and PSNR-GAT. And we adopt the GraphSAGE layer as the \(\) of the PSNR module. The impact of different graph encoders on the experiments can be found in Appendix I. In addition, we compare three recent representative methods belonging to different categories aimed at alleviating oversmoothing issues: DropMessage  for the drop category, DeProp  for the norm category, and Half-Hop  for graph data processing. For the missing feature setting, we also conduct comparisons with several classical oversmoothing mitigation techniques, including BatchNorm , PairNorm , DGN , Decorr , DropEdge . For all baselines, the linear layers in the models are initialized with a standard normal distribution, and the convolutional layers are initialized with Xavier initialization. The Adam optimizer  is used for training. Experimental results are obtained from the server with four core Intel(R) Xeon(R) Platinum 8358 CPUs @ 2.60GHZ, one NVIDIA A100 GPU (80G), and models and datasets used in this paper are implemented using the Deep Graph Library (DGL) and Pytorch Geometric (PyG). Further details on the specific parameter settings can be found in Appendix G.

### Effectiveness in mitigating over-smoothing (RQ1)

In this section, we aim to assess whether the PSNR module can mitigate the oversmoothing phenomenon in deep GNNs. We select representative datasets Cora, Amazon-Photo, and Chameleon. Using GCN as the backbone network, we compare our method against several residual methods: ResGCN, GCNII, JKNet, and DenseGNN. We set the number of layers to 2, 4, 8, 16, 32, 64 and test on ten random splits, with the average accuracy serving as the final result. The experimental results are depicted in Figure 3. Consistent with the analysis in the main text, most methods can alleviate over-smoothing, but at deeper layers, such as 64 layer, over-smoothing still occurs. In contrast, compared to other residual methods, PSNR maintains stable performance even at deeper layers, demonstrating remarkable effectiveness in mitigating over-smoothing in deep GNNs. This is attributed to PSNR module can effectively reduce the loss of high-order neighborhood subgraph information. It is noteworthy that among the various residual methods, another initial residual method GCNII also alleviate over-smoothing well. However, the subsequent experiment will reveal that although the performance of GCNII remains relatively stable with varying layers, it leads to a decrease in overall performance.

[MISSING_PAGE_FAIL:8]

\(\{2,4,6,8,10,15,20,30\}\) and running five times for each number of layers. We select the layer #K that achieves the best performance and report its average accuracy. The results are reported in Table 4. By examining the results in Table 4, under the missing feature setting, the optimal number of layers to achieve the best performance is significantly higher than in the fully observed feature setting, demonstrating the importance of deep GNNs. And PSNR outperform other baselines in all cases through alleviating over-smoothing more effectively. Specially, on the Pubmed dataset, PSNR boost the accuracy of GCN and GAT by 40.6% and 38.8%, respectively.

### Performance on large graphs (RQ4)

To validate the scalability of PSNR, we conducted additional experiments on three larger graph datasets i.e., Coauthor-Physics, Flickr and Ogbn-arxiv, to further validate the effectiveness and scalability of our method. Specifically, we selected the GCN backbone for our experiment. We report the performance of GCN and various residual methods on three datasets and the memory consumption on the largest dataset, Ogbn-arxiv. The experimental results are presented in Table 5, from which we observe that PSNR-GCN scales well and achieves the best results across all three large datasets. Meanwhile, in terms of memory consumption, PSNR is slightly higher than GCN and ResGCN, comparable to GCNII with the same initial residuals, and significantly lower than JKNet and DenseGCN. Regarding training time, PSNR is roughly at the same level as JKNet, and its time is shorter than that of DenseNet.

## 6 Conclusion and Future Work

In this paper, we addressed the oversmoothing in Graph Neural Networks (GNNs) with a focus on residual methods. We revisit the oversmoothing from the perspective of overlapping neighborhood subgraphs, explaining why residual methods can alleviate it. Our analysis revealed that current residual methods often lack node adaptivity and struggle with information loss in high-order neighborhoods subgraphs. To overcome these limitations, we introduce the Posteriori-Sampling-based Node-Adaptive Residual Module (PSNR). This innovative module uses a graph encoder to learn the posterior distribution of residual coefficients for each node at different layers, enabling fine-grained,

    &  &  \\ 
**Module** &  &  &  &  &  &  \\   & **Acc** & \#K & **Acc** & \#K & **Acc** & \#K & **Acc** & \#K & **Acc** & \#K & **Acc** & \#K \\  None & 57.3 & 3 & 44.0 & 6 & 36.4 & 4 & 50.1 & 2 & 40.8 & 4 & 38.5 & 4 \\ BatchNorm & 71.8 & 20 & 45.1 & 25 & 70.4 & 30 & 72.7 & 5 & 48.7 & 5 & 60.7 & 4 \\ PairNorm & 65.6 & 20 & 43.6 & 25 & 63.1 & 30 & 68.8 & 8 & 50.3 & 6 & 63.2 & 20 \\ DGN & 76.3 & 20 & 50.2 & 30 & 72.0 & 30 & 75.8 & 8 & 54.5 & 5 & 72.3 & 20 \\ DeCorr & 73.8 & 20 & 49.1 & 30 & 73.3 & 15 & 72.8 & 15 & 46.5 & 6 & 72.4 & 15 \\ DropEdge & 67.0 & 6 & 44.2 & 8 & 69.3 & 6 & 67.2 & 6 & 48.2 & 6 & 67.2 & 6 \\ Res & 76.8 & 8 & 60.4 & 10 & 76.6 & 6 & 76.5 & 8 & 60.5 & 6 & 76.9 & 8 \\ Init-Res & 65.1 & 6 & 50.7 & 15 & 70.4 & 10 & 77.1 & 8 & 60.6 & 8 & 76.8 & 6 \\ Dense & 66.2 & 4 & 51.5 & 2 & 74.1 & 8 & 68.5 & 10 & 52.7 & 2 & 75.1 & 10 \\ JK & 75.5 & 30 & 60.4 & 8 & 76.9 & 6 & 77.0 & 10 & 60.3 & 4 & 76.8 & 6 \\ DropMessage & 75.5 & 10 & 61.0 & 6 & 74.6 & 6 & 76.5 & 6 & 61.1 & 8 & 76.6 & 6 \\ DeProp & 71.4 & 6 & 59.4 & 2 & 76.1 & 4 & 68.04 & 2 & 48.3 & 2 & 75.8 & 4 \\ Half-Hop & 73.7 & 8 & 59.48 & 6 & 76.5 & 6 & 76.0 & 20 & 59.6 & 4 & 76.9 & 6 \\ PSNR & **77.3** & **20** & **61.1** & **15** & **77.0** & **30** & **77.9** & **8** & **61.9** & **15** & **77.3** & **10** \\   

Table 4: Test accuracy (%) on missing feature setting. The best results are in bold and the second best results are underlined.

   Method & Phy & Flickr & Ogbn-arxiv & Memory & Time \\  GCN & 95.32 \(\) 0.11 & 51.40 \(\) 0.33 & 64.37 \(\) 0.41 & 2421 & 30.10 \\ ResGCN & 95.61 \(\) 0.18 & 51.90 \(\) 0.16 & 66.32 \(\) 0.59 & 2463 & 36.06 \\ GCNII & 95.90 \(\) 0.14 & 46.18 \(\) 0.21 & 61.43 \(\) 1.63 & 2525 & 33.33 \\ JKNet & 95.88 \(\) 0.15 & 51.65 \(\) 0.31 & 60.46 \(\) 1.21 & 2921 & 40.05 \\ DenseGCN & 95.50 \(\) 0.12 & 52.18 \(\) 0.25 & 62.46 \(\) 1.58 & 3131 & 52.24 \\ PSNR-GCN & **95.92 \(\) 0.17** & **52.47 \(\) 0.16** & **67.81 \(\) 0.57** & 2539 & 42.93 \\   

Table 5: Comparison of different methods across various datasets and memory consumption (MB) and training time (ms / epoch) on Ogbn-arxiv. The best performance for each dataset is in bold, while the second best is underlined.

node-adaptive neighborhood subgraph aggregation with minimal overhead. Extensive experiments confirmed that the PSNR module can effectively mitigate oversmoothing and improve performance, particularly in scenarios requiring deep networks. Despite the significant progress made by PSNR, training a deeper GNN remains challenging, prompting the need for further research in the future.

## 7 Acknowledgements

This work was supported by National Science and Technology Major Project (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. 623B2086, No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake University Industries of the Future Research Funding. Carl Yang was not supported by any funds from China.