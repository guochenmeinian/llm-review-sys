# Human-Guided Complexity-Controlled Abstractions

Andi Peng

MIT

&Mycal Tucker

MIT

Equal contribution.

Eoin M. Kenny

MIT

&Noga Zaslavsky

UC Irvine

&Pulkit Agrawal

MIT

&Julie A. Shah

MIT

###### Abstract

Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., "bird" vs. "sparrow") and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising direction for rapid model finetuning by leveraging human insight.

## 1 Introduction

Neural networks learn implicit representations tailored to specific training tasks, but such representations, or abstractions, often fail to generalize to distinct test tasks. One approach to mitigating such generalization failures is based on the Information Bottleneck (IB) method , which provides a framework for controlling how much information is passed through the network, effectively limiting its representational complexity. Unfortunately, it is difficult to know _a-priori_ how much information to retain for optimal task performance.

Humans, however, when given a task, are remarkably adept at understanding which abstraction to deploy . Consider an expert birdwatcher who has learned fine-grained classes of birds such as "sparrows," "goldfinches," and "kiwis." When accompanied by other experts, a birdwatcher knows to deploy their "most complex" abstraction for classifying birds; meanwhile, when at home with a 6-year old child, a birdwatcher knows to deploy a far simpler representation to communicate about a "red" vs. "yellow" bird. In other words, given a task, humans naturally adopt the right abstraction level that meaningfully captures task-relevant information and enables rapid learning . Inspired by humans, we seek to train neural nets along an axis of representational complexity and allow end users to select the desired complexity level to support data-efficient finetuning.

In this paper, we introduce a human-in-the-loop framework, depicted in Figure 1, for pre-training and finetuning complexity-regulated neural representations. Within this framework, we first generate a spectrum of complexity-controlled representations by training discrete information bottleneck methods  on a pre-training task. Second, we allow a human to specify a finetuning task, unknown_a priori_, and select and finetune a pre-trained model. Given that humans specify the finetuning task, and may have to provide finetuning labels, few-shot adaptation is important.

In computational experiments, we find that finetuning performance is non-monotonically linked to representation complexity: representations that are too complex are data-inefficient, and representations that are too simple fail to capture important information. In a user study, we show that humans, given a desired finetuning task, can select high-performance models from a set of pre-trained models at different complexity levels. For example, as in Figure 1, a child performing a low-complexity task might select a low-complexity representation. More generally, while our computational experiments establish that finetuning performance is a function of model complexity, our study shows that humans can select (near) optimal complexity levels given a task for model finetuning. Lastly, the choice of neural architecture significantly affects finetuning efficiency: on one task, for example, our best-performing architecture, tuned to the right complexity, achieves better performance than other standard encoding methods with \(50\) more finetuning data.

Our findings suggest that automatically constructing complexity-regulated abstractions during pre-training and then providing a diverse spectrum for human use for finetuning is a promising direction for human-in-the-loop few-shot adaptation. In summary, our contributions are **(1)** introducing a human-in-the-loop framework for automatically generating a spectrum of complexity-regulated abstractions for fast adaptation, **(2)** establishing that finetuning performance is a function of representation complexity, and **(3)** demonstrating the utility of our human-in-the-loop framework in a user study.

## 2 Related Work

### Abstraction in Human Cognition

There is substantial evidence that suggests that much of human learning, perception, communication, and cognition may be understood as compression of relevant information [31; 36; 11]. For example, fast human learning can be enabled by merging two or more instances of statistical patterns into one when appropriate . Moreover, the simplicity of these patterns appears key to supporting their predictive power on downstream tasks . This connection between compression and prediction provides an elegant explanation for why human brains have evolved to be such efficient compressors of information and experience . Furthermore, visual abstractions have been found to prioritize functional properties, i.e. downstream task use, at the expense of visual fidelity, i.e. image reconstruction . This suggests that different abstractions are constructed and deployed conditioned on tasks. Inspired by this, we seek to train neural networks to output visual abstractions that are also functionally useful to human users on a diverse range of downstream tasks.

Figure 1: Our human-in-the-loop framework for pre-training and finetuning, illustrated for a bird-identification example. In pre-training, we generate a spectrum of encoders using representations from low to high complexity (e.g., just two crude categories to fine-grained species classifications). In fine-tuning, based on a desired task, a human selects an appropriate model for finetuning (e.g., crude categories for a child learning colors and fine-grained categories for a birder).

### Discrete Information Bottleneck

In our work, we build upon and compare to methods from prior research in discrete information bottlenecks. Generally, such work seeks to generate complexity-limited representations (roughly, limiting the number of bits about the input in a representation) using a finite set of representations (dubbed quantized vectors or prototypes). Recent work [27; 28; 7; 12] has approached this problem by combining ideas from the Variational Information Bottleneck [1; 9] with Vector Quantization (VQ) . In such works, an encoder network outputs parameters to a normal distribution, from which a continuous latent variable is sampled, and the sample is discretized to the closest element of a learnable codebook. By penalizing the KL divergence between the normal and a fixed prior (typically a unit normal), one can limit the complexity of representations. We refer to this family of approaches as the Vector Quantized Variational Information Bottleneck - Normal (VQ-VIB\({}_{N}\)). Other work proposes a different sampling mechanism before vector quantization, but experimental evaluation of these methods remains limited [22; 32].

In our work, we propose that complexity-constrained discrete representation learning can be used to generate a meaningful variety of encoders for a human-in-the-loop finetuning process. We use methods from prior work, and we propose a novel combination of entropy regularization and categorical sampling that, in experiments, supports the best finetuning performance.

## 3 Approach

### Problem Formulation and Human-in-the-loop Framework

We consider a pre-training and finetuning problem, wherein a model is first trained on a pre-training dataset and must be rapidly adapted to a distinct finetuning task. Unlike classic meta-learning frameworks [6; 21; 19], we do not assume access to a distribution of tasks.

In pre-training, we assume access to a dataset of inputs and pre-training outputs, \((X,Y_{p})\) (e.g., images of birds and species labels). That is, \(x(X)\), \(y_{p}(Y|X)\). In finetuning, we assume access to a task-specific dataset with similarly-drawn inputs but novel task labels: \((X,Y_{t})\), for \(x(X)\), \(y_{t}(Y|X)\) (e.g., for the depicted girl's finetuning task, the color of the bird). Lastly, we assume that the pre-training labels are a sufficient statistic for the task-specific labels: \(I(X;Y_{t})=I(Y_{p};Y_{t})\). Intuitively, this states that the pre-training objective must include relevant information for the finetuning task. This is trivially satisfied with a reconstruction loss (i.e., \(Y_{p}=X\)) or fine-grained classifications (e.g., pre-training on exact species, but finetuning on groups of species).

Without information about the finetuning task, it is difficult, _a priori_, to identify how to pre-train a model to perform optimally on the finetuning task. Regularization methods like information bottleneck, for example, depend upon insight on the downstream task to specify the right level of regularization . Rather than automatically identifying the right model, therefore, we propose a human-in-the-loop framework, depicted in Figure 1. Within our framework, we seek to generate a suite of pre-trained neural net encoders such that an end user can identify which one uses abstractions that support high performance for their desired task.

### Technical Approach

Within our human-in-the-loop framework, it is critical to generate a "good" set of encoders from which the human chooses. This set must exhibit important variation (such that some encoders are better than others) and human-interpretability (such that humans can select the better encoders). We propose and validate in experiments that complexity-controlled discrete representation learning mechanisms achieve these desiderata. First, discrete representations can support human interpretability (via visualizations of the finite set of representations) [15; 18]. Second, controlling the complexity of representations is a meaningful axis of variation for encoders that likely supports human-specified tasks .

### Neural Architecture Improvements

To generate a spectrum of encoders using representations at different complexity levels, we extend prior methods in discrete information bottleneck. We briefly present a novel neural method, which we dub the Vector-Quantized Variational Information Bottleneck - Categorical, or VQ-VIB\({}_{}\).

In VQ-VIB\({}_{}\), we combine information-theoretic losses from Tucker et al. 's Vector Quantized Variational Information Bottleneck method (which we dub VQ-VIB\({}_{}\) to emphasize that latent representations are drawn from a normal distribution) with categorical sampling mechanisms similar to Roy et al.  and Wu and Flierl . Our VQ-VIB\({}_{}\) encoder architecture is depicted in Figure 2. The encoder is parametrized by a feature extractor (in our cases, a standard feedforward neural network), an integer, \(n\), representing how many quantized vectors to combine into a single latent representation, and \(C\) learnable quantized vectors \(^{i};i[1,C],^{i}^{Z/n}\). Using this architecture, the encoder is characterized as a function mapping from an input to a distribution over latent representations: \(q(z|x);x^{X},z^{Z}\). Concretely, the VQ-VIB\({}_{}\) architecture deterministically maps from \(x\) to a hidden representation, \(h^{Z}\). That hidden representation is divided into \(n\) vectors of equal size: \(h=[h_{1},h_{2},...,h_{n}]\). Each \(h_{i}\) is then probabilistically quantized by sampling a quantized vector according to the L2 distance from \(h_{i}\) to each quantized vector: \((z_{i}=^{j}|h_{i}) e^{-\|h_{i}-^{j}\|^{2}}\). (One can differentiate through sampling from this categorical distribution via the gumbel-softmax trick .) Lastly, these \(n\) sampled discrete representations, each of which is dubbed \(z_{i}\), are concatenated to form the latent representation, \(z=[z_{1},z_{2},...,z_{n}]\).

We train VQ-VIB\({}_{}\) via a combination of losses introduced by Tucker et al. . We assume the VQ-VIB\({}_{}\) encoder is trained with a decoder and a predictor, as depicted in Figure 3. Given a utility function, \(U\), VQ-VIB\({}_{}\) is trained to maximize the objective function in Equation 1:

\[_{U}[U(x,y)]- _{I}[\|x-\|^{2}]\,-_{H}[_{i[1,n ]}H((|h_{i}(x))]\\ -\|[h_{i}(x)]-_{i}(x)\|^{2}-\|h_{i}(x)- {sg}[_{i}(x)]\|^{2}\] (1)

This objective trades off, in order in the equation: 1) maximizing the expected utility (e.g., cross-entropy loss), 2) minimizing the expected reconstruction loss (here, MSE), 3) minimizing the entropy of the distribution over codebook elements, and 4) minimizing a clustering loss from prior literature, encouraging encodings and quantized vectors to cluster (and, as in prior art, we leave \(=0.25\) in all experiments) . Here, sg represents the stop-gradient operation.

We include a more thorough discussion of this loss function, and comparisons to losses and architectures proposed in prior literature, in Appendix A. We emphasize that, while we propose some modifications to neural architectures, the primary contributions of this work are not about VQ-VIB\({}_{}\) but rather: 1) our human-in-the-loop framework and 2) the recognition that discrete information bottleneck methods support high performance within this framework. In experiments, we found that our VQ-VIB\({}_{}\) method performed better than methods from prior art, so we include this technical section to inform future researchers about modest architectural changes that support better performance.

Figure 2: A VQ-VIB\({}_{}\) encoder maps an input, \(x\), to a representation, \(h\) in \(^{Z}\), which is divided into \(n\) sub-representations, \(h_{i}\). Each \(h_{i}\) is discretized stochastically by sampling based on distance to each quantized vector in the codebook, \(\). Lastly, the sampled quantized vectors are concatenated for the full latent representation, \(z\). Figure 3: Different encoders train within an encoder-decoder-predictor framework via utility, entropy, and reconstruction losses [adapted from 27].

Assessing Representational Complexity's Impact on Finetuning

We first present results for computational experiments in three different visual classification domains, indicating that finetuning performance varies as a function of representation complexity in a non-monotonic way. All experiments consisted of pre-training and finetuning phases. First, in pre-training on a low-level task, we generated a spectrum of models at varying complexity levels by varying loss hyperparameters. Second, we used the encoders from the pre-trained models and trained predictors to map from encodings to downstream predictions. Jointly, these steps enabled us to assess the importance of complexity-controlled representations for data-efficient finetuning. Overall, we found that, for small amounts of finetuning data, tuning representations to the right complexity was important, but with large amounts of data, any sufficiently-complex encoder performed similarly.

### Domains

We trained agents on three image classification datasets: FashionMNIST , CIFAR100 , and iNaturalist 2019 (iNat) . For FashionMNIST, we used a two-level hierarchy, grouping the 10 low-level classes into 3 higher-level classes (top = Tshirt/top, Pullover, Coat, and Shirt; shoes = Sandal, Sneaker, and Ankle Boot; other = Trouser, Dress, Bag). The CIFAR100 and iNat datasets have more extensive hierarchical structures, as detailed by, e.g., Sainte Fare Garnot and Landrieu . For both datasets, we considered two levels of increasing crudeness in the hierarchy. For CIFAR100, we used both a 20-way division (each class consisting of 5 low-level classes) and a binary division for alive vs. non-alive objects. For iNat, we used a 34-way division (representing categories like "butterflies" and "mushrooms") and a 3-way division for plants, animals, or fungi. Thus, all domains were characterized by semantically-meaningful hierarchies.

### Pretraining and Finetuning

Our experiments comprised two phases: pre-training and finetuning. In pre-training, we trained an encoder, decoder, and predictor on a low-level task. For example, for iNat, this consisted of classifying a photograph among 1010 species. During pre-training, after convergence to high accuracy, we decreased the complexity of representations by tuning a hyperparameter until representations were uninformative. For our \(\)-VAE and VQ-VIB\({}_{}\) baselines, we increased \(_{C}\), a scalar weight penalizing the KL divergence of the conditional Gaussian (generated by the encoder) from a unit Gaussian. (See Appendix A for details of training losses for \(\)-VAE and VQ-VIB\({}_{}\), including \(_{C}\). In general, larger values of \(_{C}\) led to less complex representations and greater MSE values for reconstructions.) For VQ-VIB\({}_{}\), we increased \(_{H}\), the scalar weight penalizing the entropy of the categorical distribution over quantized vectors. In Appendix D, we examine other combinations of losses and architectures to control complexity, but found that tuning entropy for VQ-VIB\({}_{}\) achieved the best results. In general, to address some conflicting definitions of complexity in prior literature, we distinguish between an entropy loss, as introduced for our VQ-VIB\({}_{}\) model, and a complexity loss, where we use the definition of complexity as \(I(X;Z)\).

In finetuning, we trained new predictor networks to map from encodings to classifications, using a small amount of training data. Using an encoder saved during pre-training, we generated encodings from inputs. New predictors were trained on classification tasks given the supervised (encoding, label) data. We generated \(k\) encodings for each class in the supervised dataset, and report results for different \(k\). By varying \(k\) and which encoder was used to generate encodings (from high to low complexity), we could investigate the effect of encoder complexity on data-efficiency in finetuning. In all experiments, we pre-trained 5 models and ran 10 finetuning trials for each encoder. Further details on pre-training and finetuning are included in Appendix A.

### Illustrative Example: FashionMNIST

We illustrate the high-level trends from our computational results using the FashionMNIST dataset. We trained models on the 10-way classification task to high accuracy (typically around 90%) and decreased representational complexity until the model was accurate 10% of the time (random chance).

Depictions of the learned quantized vectors for VQ-VIB\({}_{}\) are included in Figure 4 at high and low complexities. At high complexity, VQ-VIB\({}_{}\) used a large number of distinct prototypes, including multiple prototypes per class (e.g., for two different types of handbags). This high complexity supported low MSE reconstruction loss. As we decreased the representational complexity, VQ-VIB\({}_{}\) used fewer prototypes that represented more abstract concepts (Figure 4 b). For example, five distinct classes (Tshirt/top, Pullover, Coat, Shirt, and Dress) were merged into a single prototype, increasing MSE. Further examples of prototype evolution are in Appendix C.

In finetuning experiments, we loaded VQ-VIB\({}_{}\) encoders across a range of complexities and finetuned a predictor on the 3-way classification task described earlier (tops, shoes, or other). Using just one example per class (\(k=1\)), VQ-VIB\({}_{}\) models were more accurate than \(\)-VAE or VQ-VIB\({}_{}\) models, as shown in Figure 4 c. The \(x\) axis displays each encoder's MSE, which is a proxy for the inverse of the complexity of representations: more complex representations capture more information and generate better reconstructions (lower MSE). The \(y\) axis shows the finetuned predictor's accuracy on the 3-way task. While VQ-VIB\({}_{}\) peak performance is roughly 70% accuracy (remarkably high, given \(k=1\)), \(\)-VAE and VQ-VIB\({}_{}\) performance peaks at 40% and 55%, respectively.

Beyond comparing VQ-VIB\({}_{}\) to other models, Figure 4 c shows the importance of task-appropriate complexity levels. At low MSE values, VQ-VIB\({}_{}\) uses many different prototypes, which impedes finetuning data efficiency. However, as VQ-VIB\({}_{}\) learns more abstract representations (which increases MSE), finetuning performance improves (the correlation between MSE and accuracy is positive for MSE \(<0.055(p<0.03)\)). However, past an MSE value around 0.055, VQ-VIB\({}_{}\) lacks the representational capacity to distinguish between images that should be classified differently, and performance worsens.

Here, we discussed finetuning results for 1 labeled example per class (\(k=1\)) and for \(n=1\), the number of quantized vectors to combine into representations, but results and analysis for \(k\), \(n\), and for \(\)-VAE, VQ-VIB\({}_{}\), and VQ-VIB\({}_{}\) models are included in Appendix B. For all \(k\), we found that VQ-VIB\({}_{}\) outperformed \(\)-VAE and VQ-VIB\({}_{}\). In fact, VQ-VIB\({}_{}\) with \(k=1\) outperformed \(\)-VAE for \(k=50\), indicating important architectural benefits. Increasing \(n\) supported higher complexity (lower MSE) but worse fine-tuning performance; intuitively, many discrete representations approximate continuous representations, which have worse sample complexity. This is an important limitation of combinatorial codebooks that others propose .

A series of ablation studies confirm the importance of the VQ-VIB\({}_{}\) architecture and annealing entropy, instead of complexity, for efficient finetuning (Appendix D, for FashionMNIST and other domains). In particular, we found that for VQ-VIB\({}_{}\) and VQ-VIB\({}_{}\), penalizing entropy led to greater finetuning accuracy than when penalizing complexity, and VQ-VIB\({}_{}\) outperformed VQ-VIB\({}_{}\) when both were trained via entropy regularization. In other words, penalizing entropy enabled better finetuning performance, and VQ-VIB\({}_{}\) supported better entropy regularization.

Lastly, given the importance of complexity on finetuning performance, we considered two methods for autonomously selecting the optimal complexity level. For low-data regimes \((k 5)\), the simple heuristic of choosing the most complex encoder was clearly suboptimal; however, as the amount of finetuning data increased (e.g., \(k=50\)), the most complex encoders achieved near-optimal

Figure 4: 2D PCA of VQ-VIB\({}_{}\) latent representations for FashionMNIST at high (a) or low (b) complexity. Colorful points and images are prototypes, scaled in proportion to frequency of use. At high complexity, VQ-VIB\({}_{}\) uses a large number of prototypes, including multiple per class (e.g., two types of handbags); at low complexity, classes merge (e.g. different types of shirts). c) When finetuned on a 3-way classification task with 1 example per class, VQ-VIB\({}_{}\) outperformed \(\)-VAE and VQ-VIB\({}_{}\), and less complex representations (greater MSE) improved model scores.

performance (see Figure 10 in Appendix B). We also tested methods for selecting encoders via validation-set accuracy and found similar trends. In finetuning, we held out a subset of the data for assessing finetuning accuracy and selected the best-performing encoder via validation set accuracy. For \(k 5\), this validation-set approach did not consistently converge to optimal performance, but, for sufficiently large \(k\), it did. Further results from this approach are included in Table 4 and Appendix B. Generally, we found that for large enough \(k\), the importance of tuning to the right complexity decreases, and several methods can select optimal encoders; for very small \(k\), however, autonomous methods are suboptimal.

This illustrative FashionMNIST use case demonstrates many of the important trends we explore in our later experiments: tuning VQ-VIB\({}_{}\) representations to the "right" complexity was important for optimal performance for small \(k\), and VQ-VIB\({}_{}\) generally outperformed other encoders for few-shot finetuning.

### CIFAR100 and iNat

In this section, we present results from computational experiments in the more challenging CIFAR100 and iNat domains. As before, in the smallest data regime (small \(k\)), using less complex representations afforded greater efficiency benefits, and VQ-VIB\({}_{}\) outperformed \(\)-VAE and VQ-VIB\({}_{}\).

Figure 5 a and b show finetuning accuracy on the binary classification task for CIFAR100 on identifying living vs. non-living things. With only 1 datapoint per class (Figure 5 a), \(\)-VAE performance remained effectively flat at random chance: barely exceeding 50%. However, for VQ-VIB\({}_{}\), for MSE \(<0.35\), performance increased as MSE increased (linear regression slope was positive \(p<0.001\)), up to a 70% accuracy rate, before worsening as MSE increased further. Increasing \(k\) to 50 (Figure 5 b) shrank the gap between the encoder architectures, and flattened the improvements previously observed, but VQ-VIB\({}_{}\) continued to outperform \(\)-VAE and VQ

Figure 6: iNat finetuning results for the plant-animal-fungus classification task. Using only 1 finetuning example (a), VQ-VIB\({}_{}\) performance improved as complexity decreased, and outperformed \(\)-VAE and VQ-VIB\({}_{}\). Using 50 finetuning examples, performance plateaued, but VQ-VIB\({}_{}\) continued to outperform other methods (b). VQ-VIB\({}_{}\) performance shifts smoothly for varying \(k\) (c).

VIB\({}_{}\) models. When finetuned on the 20-way classification task, the same trends of VQ-VIB\({}_{}\) outperforming other architectures held (Figure 5 c), although the finetuning accuracy decreased monotonically as MSE increased, rather than peaking at a specific complexity. Plots for a larger range of \(k\), and for varying \(n\), for both finetuning tasks, are included in Appendix B.

Similar trends held in the iNat dataset when finetuned on the 3-way animal-plant-fungus task, as depicted in Figure 6. For small \(k\), VQ-VIB\({}_{}\) finetuning performance initially improved as the encoder learned more compressed representations (Figure 6 a). For VQ-VIB\({}_{}\), \(k=1\), the linear correlation between MSE and finetuning accuracy is positive (\(p<0.001\)) for MSE \(<0.55\). Further analysis of finetuning performance for VQ-VIB\({}_{}\), for varying \(k\), shows a smooth change in behavior as \(k\) increases: simultaneously improving overall performance, and benefiting less from compressed representations (Figure 6 c). This indicates that VQ-VIB\({}_{}\) is most advantageous in a low-data regime. Similar results for finetuning on the 34-way finetuning task are included in Appendix B. Visualization via 2D principle component analysis (PCA) confirms our intuition of how VQ-VIB\({}_{}\) models support few-shot learning for iNat. At lower complexity levels, a VQ-VIB\({}_{}\) encoder used a decreasing number of prototypes to represent increasingly abstract concepts (Figure 7).

As in the FashionMNIST domain, we evaluated autonomous methods for selecting the optimal encoder and found that, while they succeeded for sufficiently large \(k\), they struggled in a low-data regime. For \(k=1\), for example, the heuristic of choosing the most complex encoder was suboptimal (e.g., see Figure 5 a and Figure 6 a). At the same time, for \(k=50\), this heuristic worked quite well (e.g., Figure 5 b and Figure 6 b). Lastly, selecting models via validation set accuracy similarly worked well for large enough \(k\), but struggled for \(k 5\) (see Appendix B).

Finally, we briefly note some limitations of VQ-VIB\({}_{}\) and our finetuning method. In the results discussed so far, we found consistent advantages in using VQ-VIB\({}_{}\) and low-complexity representations. However, as the amount of finetuning data increases, more complex representation methods like \(\)-VAEs outperform VQ-VIB\({}_{}\). A more complete discussion of this phenomenon is included in Appendix B. Between our main results and these limitations, we find that the data-efficiency benefits of using VQ-VIB\({}_{}\) in finetuning are greatest in data-poor and low-complexity settings.

## 5 Human-in-the-Loop Selection of Task-Appropriate Representations

Given our main motivation of a human-in-the-loop framework for selecting task-appropriate abstractions (recall Figure 1), we conducted a human-participant study to evaluate whether users could select the highest-performing task-appropriate models given visualizations of prototypes as task abstractions. This is important because, in order to take full advantage of our setup, users must be able to select the appropriate task-level representation so the neural network best learns the finetuning task in a sample efficient manner. We asked users to select the optimal encoder for a given task, given a visualization of encoder prototypes. A positive result would show that, given a spectrum of pre-trained VQ-VIB\({}_{}\) encoders, a human user wishing to deploy a task-appropriate model could specify the right encoder for a given task, and thus fine-tune the model in a sample efficient manner.

Figure 7: 2D PCA of the VQ-VIB\({}_{}\) latent space for iNat, at different complexity levels. Each gray point represents the continuous encoding output by the encoder; the colorful points represent the learnable prototypes. For visualization purposes, the closest training image to each prototype is included in the diagrams. In the high-complexity case, the model has learned many distinct prototypes, representing concepts like birds or insects. In the low-complexity case, VQ-VIB\({}_{}\) uses only two prototypes, which capture a plant-animal distinction the aligns closely with the iNat hierarchy.

### User Study

Our human experiment was performed using the pre-trained models from the FashionMNIST domain in Section 4.3. Users were told that a robot was good at sorting clothing into 10 distinct categories (i.e., the 10 normal categories in FashionMNIST), but that in their case they should consider a different set of categories. Each user was randomly assigned a model type (VQ-VIB\({}_{}\) or VQ-VIB\({}_{}\)) and a visualization type (prototypes or a plot of MSE during training). In three questions (explained below), users were shown different groupings of the 10 FashionMNIST classes and asked to select which of four model checkpoints they thought best represented the groupings. Overall, this between-subjects setup allowed us to compare the effects of different visualization methods on user accuracy in selecting the optimal representation.

For our main question, in which users were asked to select an encoder that could sort FashionMNIST items into the three finetuning categories (tops, shoes, and other), our null hypothesis, \(}\), was _"Users viewing VQ-VIB\({}_{}\) prototypes will select the optimal encoder as often as those viewing MSE scores"_. Our alternative hypothesis, \(}\), was _"Viewing VQ-VIB\({}_{}\) prototypes improves users' ability to select the optimal encoder compared to just viewing MSE scores."_

Participants.We crowd-sourced 20 participants per group from Prolific.com (N=80). The sample was balanced to have an even number of male and female participants. All participants were native English speakers above the age of 18 and resided in the U.S., the U.K., or Ireland. Given estimates of survey duration, calculated in a pilot study, we paid participants according to an estimated $12 per hour wage. The study received IRB approval from MIT.

Materials.Each user was presented three questions, corresponding to different groupings of the 10 FashionMNIST classes: in Question 1 classes were grouped according to the finetuning labels in our computational experiments (tops, shoes, and other), in Question 2, there were 10 distinct classes corresponding to the 10 FashionMNIST classes,and in Question 3, classes were grouped according to an unintuitive 3-way grouping (e.g., Pullover, Dress, Sneaker were one class). Despite the numbering, we randomized the order of questions for each participant. In each question, users were asked to select which of four model checkpoints they thought best represented groupings for that specific question. The four models corresponded to checkpoints taken 1) near the start of training, 2) at the minimum MSE value, 3) at an intermediate MSE value, and 4) at the end of training. We primarily focused on results for Question 1, as it was the only question for which selecting the most complex encoder was not optimal. Responses were categorized as correct if users picked the optimal encoder (option "c" in Figure 8) and otherwise incorrect (but during the study, users were not told whether they were correct).

### Results

Results from our user study supported our hypothesis. As shown in Figure 8, for Question 1, corresponding to the 3-way grouping from our computational experiments, users who viewed VQ-VIB\({}_{}\) prototypes selected the optimal encoder 55% of the time, compared to 10% accuracy when

Figure 8: Participants were asked to select the optimal encoder for the FashionMNIST finetuning task based on MSE values (left) or decoded representations (middle). Using only MSE values, users often incorrectly selected option b), whereas when viewing prototypes, users were able to correctly identify the appropriate abstraction level, and thus the optimal encoder (c).

viewing MSE scores (prototype accuracy was significantly greater at \(p<0.01\) for a Fisher's exact test). Interestingly, users who viewed VQ-VIB\({}_{}\) models never achieved high performance (binomial test for non-random chance at \(p>0.2\)), suggesting an important advantage of the VQ-VIB\({}_{}\) architecture. Thus, VQ-VIB\({}_{}\) prototypes was the _only_ visualization method that supported greater-than-random chance performance in selecting optimal abstractions.

Further analysis, using responses to Questions 2 and 3 as well, highlighted the benefit afforded by VQ-VIB\({}_{}\) prototypes. Overall, we observed high accuracy rates for Questions 2 and 3 regardless of visualization method; this is unsurprising given that the correct behavior for both questions was the selecting most complex encoder. We fitted a Mixed Linear Effects Model (MLEM) to the survey results, predicting user accuracy as a function of model visualization and question, grouped by participant. Using Wilkinson notation , our model was: \(Acc. Q+M+Q:M+(1|Participant)\) where \(Q\) represents the categorical variable for question and \(M\) represents the categorical variable for model type and visualization (VQ-VIB\({}_{}\) Prototypes, VQ-VIB\({}_{}\) MSE, VQ-VIB\({}_{}\) Prototypes, etc.). We grouped results by participant to model random-intercept effects of some participants being more accurate than others. Full results for accuracy rates and the MLEM model are included in Appendix E.

Overall, we found a significant \((p<0.05)\) positive interaction effect between Question 1 and visualization of VQ-VIB\({}_{}\) prototypes. The only other significant effect we found was a negative effect for Question 1. Together, these results show that 1) Question 1 was harder than the other two questions and 2) viewing VQ-VIB\({}_{}\) prototypes mitigated the effects of increased difficulty for Question 1. This provides crucial support for our hypothesis, showing that viewing VQ-VIB\({}_{}\) prototypes enabled participants to select the optimal encoder more often.

Lastly, the combination of all results helps rule out several alternative hypotheses for how participants selected encoders based on VQ-VIB\({}_{}\) prototypes. Users did not merely select the most complex encoder; otherwise, they would have performed poorly on Question 1. Similarly, users did not simply use the fewest number of prototypes greater than the number of finetuning classes; otherwise, they would have performed worse on Question 3. Thus, our results suggest that users interpreted VQ-VIB\({}_{}\) prototypes as desired in intelligently selecting task-appropriate representations.

## 6 Contributions

We proposed a human-in-the-loop framework for selecting task-appropriate representations for data-efficient adaptation to new tasks. Discrete information bottleneck methods provide a principled way to generate representations along a spectrum of low to high complexity; we tested methods from prior literature and a novel architecture, VQ-VIB\({}_{}\), for generating such representations.

We found that controlling the complexity of representations was important for data-efficient fine-tuning: overly-complex representations required more training examples, but overly-simplified representations failed to capture important information. In computational studies, we showed the importance of learning low-entropy representations and that VQ-VIB\({}_{}\) tends to outperform other discrete information bottleneck methods. In a human study, we found that human partners are able to identify optimal complexity levels, indicating a promising direction for human-in-the-loop training.

Broader Impact:Generally, we hope that this work supports broader and better human-AI collaboration, supporting human-selected individualized models for particular use cases; however, we recognize that our current prototype-inspection framework may be limited and that care must be taken in future work to verify that representations along the complexity spectrum correspond to desired human concepts. In future work, we hope to explore models that simultaneously support different complexity levels instead of training separate models for different levels.

## 7 Acknowledgements

We thank members of the Interactive Robotics Group for helpful feedback and discussions. Andi Peng is supported by the NSF Graduate Research Fellowship and Open Philanthropy. Mycal Tucker is supported by an Amazon Alexa Science Hub Fellowship.