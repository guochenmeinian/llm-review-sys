# An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints

Jung-hun Kim

Seoul National University

Seoul, South Korea

junshunkim@snu.ac.kr

&Milan Vojnovic

London School of Economics

London, United Kingdom

m.vojnovic@lse.ac.uk

&Se-Young Yun

KAIST AI

Seoul, South Korea

yunseyoung@kaist.ac.kr

###### Abstract

In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by \(V_{T}\), referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by \(S_{T}\), referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments.

## 1 Introduction

We consider multi-armed bandit problems , which are fundamental sequential learning problems where an agent plays an arm at each time and receives a corresponding reward. The core challenge lies in balancing the exploration-exploitation trade-off. Bandit problems have significant implications across diverse real-world applications, such as recommendation systems  and clinical trials . In a recommendation system, each arm could represent an item, and the objective is to maximize the click-through rate by making effective recommendations.

In practice, the mean rewards associated with arms may decrease over repeated plays. For instance, in content recommendation systems, the click rates for each item (arm) may diminish due to user boredom with repeated exposure to the same content. Another example is evident in clinical trials, where the efficacy of a medication can decline over time due to drug tolerance induced by repeated administration. The decline in mean rewards resulting from playing arms, referred to as _(rested) rotting bandits_, has been studied by Levine et al. , Seznec et al. [20; 21]. The previous work focuses on finite \(K\) arms, in which Seznec et al.  proposed algorithms achieving \(()\) regret. This suggests that rotting bandits with a finite number of arms are no harder than the stationary case.

However, in real-world scenarios like recommendation systems, where the content items such as movies or articles are numerous, prior methods encounter limitations as the parameter \(K\) becomes large, resulting in trivial regret. This emphasizes the necessity of studying rotting scenarios with _infinitely_ many arms, particularly when there is a lack of information about the features of each item. The consideration of infinitely many arms for rested rotting bandits fundamentally distinguishes these problems from those with a finite number of arms, as we will explain later.

The study of multi-armed bandit problems with an infinite number of arms has been extensively conducted in the context of _stationary_ rewards [6; 24; 8; 10; 5], where the agent has no chance to play all the arms at least once until horizon time \(T\). Initially, the distribution of the mean rewards forthe arms was assumed to be uniform over the interval \(\)[6; 8]. This assumption was expanded to include a much wider range of distributions satisfying \(((a)>^{*}-x)=(x^{})\), for a parameter \(>0\), where \((a)\) represents the mean reward of arm \(a\) and \(^{*}\) is the mean reward of the best-performing arm [24; 10; 5]. Additionally, feature information for each arm is not required for multi-armed bandit problems with infinitely many arms, which differs from linear bandits  or continuum-armed bandits [3; 14], where feature information for each arm, either for the Lipschitz or linear structure, is involved. While Kim et al. , as the closest work, explores the concept of diminishing rewards in the context of bandits with infinitely many arms, their focus is restricted to the case of the maximum rotting rate constraint, where the amount of rotting at each time step is bounded by \((=o(1))\). This naturally directs focus towards regret regarding the maximum rotting rate rather than the total rotting rate over the time horizon. Furthermore, their focus is limited to the case where the initial mean rewards are uniformly distributed (\(=1\)).

In our study, we explore rotting bandits with infinitely many arms, subject to generalized initial mean reward distribution with \(>0\) and, importantly, generalized constraints on the rate at which the mean reward of an arm declines. Our investigation into diminishing, or 'rotting,' rewards encompasses two scenarios: one with the total amount of rotting bounded by \(V_{T}\), and the other with the total number of rotting instances bounded by \(S_{T}\). This allows us to capture characteristics of entire rotting rates over the time horizon. Similar constraints of \(V_{T}\) or \(S_{T}\) regarding nonstationarity have been explored in the context of nonstationary finite \(K\)-armed bandit problems [7; 4; 19], where the reward distribution changes over time independently of the agent. Following established terminology for nonstationary bandits, we denote the environment with a bounded total amount of rotting as the _slow rotting_ (\(V_{T}\)) case and the one with a bounded total number of rotting instances as the _abrupt rotting_ (\(S_{T}\)) case.

Here we discuss why (rested) rotting bandits for infinitely many arms are fundamentally different from those for finite arms. In the case of finite arms, rested rotting is known to be no harder than stationary case [20; 21]. This result arises from the confinement of mean rewards of optimal arms and played arms within confidence bounds, even under rested rotting (as demonstrated in Lemma 1 of Seznec et al. [20; 21]). However, in the case of infinite arms under distribution for initial mean reward that allows for an infinite number of near-optimal arms, there always exist near-optimal arms outside of explored arms. Therefore, the mean reward gap may not be confined within confidence bounds. This fundamental difference from finite-armed rotting bandits introduces additional challenges. In our setting of infinite arms, there exists an additional cost for exploring new (unexplored) arms to find near-optimal arms while eliminating explored suboptimal arms. If the total rotting effect on explored arms is significant, then the frequency at which new near-optimal arms must be sought increases substantially, resulting in a large regret. This is why the rested rotting significantly affects the exploration cost regarding \(V_{T}\) or \(S_{T}\) in our setting, which differs from the case of finite arms.

To solve our problem, we introduce algorithms that employ an adaptive sliding window mechanism, effectively managing the tradeoff between bias and variance stemming from rotting rewards. Notably, to the best of our knowledge, this is the first work to consider slow and abrupt rotting scenarios, in the context of infinitely many-armed bandits. Furthermore, it is the first work to consider the generalized initial mean reward distribution for rotting bandits with infinitely many arms.

Summary of our Contributions.The key contributions of this study are summarized in the following points. Please refer to Table 1 for a summary of our regret bounds.

\(\) To address the slow and abrupt rotting scenarios, we propose a UCB-based algorithm using an adaptive sliding window and a threshold parameter. This algorithm allows for effectively managing the bias and variance trade-off arising from rotting rewards.

  Type & Regret upper bounds & Regret upper bounds & Regret lower bounds \\  & for \(>1\) & for \(0<<1\) & for \(>0\) \\  \))} & Theorem 3.1: & Theorem 3.1: & Theorem 4.1: \\  & \((\{V_{T}^{}T^{},T^{}\})\) & \((\{V_{T}^{}T^{},T^{}\})\) & \((\{V_{T}^{}T^{},T^{}\})\) \\  \))} & Theorem 3.3: & Theorem 4.2: \\  & \((\{S_{T}^{}T^{},_{T} \})\) & \((\{}T,V_{T}\})\) & \((\{S_{T}^{}T^{}\})\) \\  

Table 1: Summary of our regret bounds.

\(\) In the context of slow rotting (\(V_{T}\)) or abrupt rotting (\(S_{T}\)), for any \(>0\), we present regret upper bounds achieved by our algorithm with an appropriately tuned threshold parameter. It is noteworthy that \(V_{T}\), \(S_{T}\), and \(\) are being considered for the first time in the context of rotting bandits with infinitely many arms.

\(\) We establish regret lower bounds for both slow rotting and abrupt rotting scenarios. These regret lower bounds imply the tightness of our upper bounds when \( 1\). In the other case, when \(0<<1\), there is a gap between our upper bounds and the corresponding lower bounds, similar to what can be found in related literature, which is discussed in the paper.

\(\) Lastly, we demonstrate the performance of our algorithm through numerical experiments on synthetic datasets, validating our theoretical results.

## 2 Problem Statement

We consider rotting bandits with infinitely many arms where the mean reward of an arm may decrease when the agent pulls the arm. Let \(\) be the set of infinitely many arms and let \(_{t}(a)\) denote the unknown mean reward of arm \(a\) at time \(t\). At each time \(t\), an agent pulls arm \(a_{t}^{}\) according to policy \(\) and observes stochastic reward \(r_{t}\) given by \(r_{t}=_{t}(a_{t}^{})+_{t}\), where \(_{t}\) is a noise term following a \(1\)-sub-Gaussian distribution. To simplify, we use \(a_{t}\) for \(a_{t}^{}\) when there is no confusion about the policy. We assume that initial mean rewards \(\{_{1}(a)\}_{a}\) are i.i.d. random variables on \(\), a widely accepted assumption in the context of infinitely many-armed bandits [8; 6; 24; 10; 5; 13].

As in Wang et al. , Carpentier and Valko , Bayati et al. , we consider, to our best knowledge, the most general condition on the distribution of the initial mean reward of an arm, satisfying the following condition: there exists a constant \(>0\) such that for every \(a\) and all \(x\),

\[(_{1}(a)>1-x)=(_{1}(a)<x)=(x^{}),\] (1)

where \(_{1}(a)=1-_{1}(a)\) is the initial sub-optimality gap. As noted in [24; 10; 5], Eq.(1) is a non-trivial condition only when \(x\) approaches \(0\), as for any constant \(x(0,1]\), it becomes \((_{1}(a)<x)=(1)\), which may accommodate a wide range of distributions. It is noteworthy that the larger the value of \(\), the smaller the probability of sampling a good arm. Furthermore, the uniform distribution is a special case when \(=1\). Importantly, our work allows for a wider range of distributions satisfying (1) for any constant \(>0\) than the uniform distribution \((=1)\) considered in Kim et al. . Additional discussion is deferred to Appendix A.2.

The rotting of arms is defined as follows. At each time \(t 1\), the mean rewards of arms are updated as

\[_{t+1}(a)=_{t}(a)-_{t}(a),\]

where \(_{t}(a_{t}) 0\) for the pulled arm \(a_{t}\) and \(_{t}(a)=0\) for every \(a/\{a_{t}\}\), which implies that the rotting may occur only for the pulled arm at each time. Note that, for every \(a\) and \(t 2\), it holds \(_{t}(a)=_{1}(a)-_{s=1}^{t-1}_{s}(a)\), allowing \(_{t}(a)\) to take negative values. For notation simplicity, in what follows, we write \(_{t}\) for \(_{t}(a_{t})\) when there is no confusion. We refer to \(_{1},_{2},\) as rotting rates. We also use the notation \([m]:=\{1,,m\}\), for any integer \(m 1\).

We consider two cases for rotting rates: (a) _slow rotting case_ where, for given \(V_{T} 0\), the cumulative amount of rotting is required to satisfy the slow rotting constraint \(_{t=1}^{T-1}_{t} V_{T}\), and (b) _abrupt rotting case_ where, for given \(S_{T}[T]\), the cumulative number of rotting instances (plus one) is required to satisfy the abrupt rotting constraint \(1+_{t=1}^{T-1}(_{t} 0) S_{T}\). The values of rotting rates of pulled arms, \(\{_{t}\}_{t[T-1]}\), are assumed to be determined by an adversary, described as follows.

**Assumption 2.1** (Adaptive Adversary).: At each time \(t[T]\), the value of the rotting rate \(_{t} 0\) is arbitrarily determined immediately after the agent pulls \(a_{t}\), _subject to_ the constraint of either slow rotting for a given \(V_{T}\) or abrupt rotting for a given \(S_{T}\).

_Remark 2.2_.: The adaptive adversary under the slow rotting constraint (\(V_{T}\)) is more general than that in Kim et al. , in which the adversary is under _a maximum rotting rate constraint_; that is, for given \(=o(1)\), \(_{t}\) for all \(t[T-1]\). This is because our adversary is under a weaker constraint bounding the total sum of the rotting rates rather than each individual rotting rate. Additionally, the abrupt rotting constraint (\(S_{T}\)) is fundamentally different from the maximum rotting constraint  because the adversary for abrupt rotting is under a constraint on the total number of rotting instances rather than the magnitude of rotting rates.

Our problem's objective is to find a policy that minimizes the expected cumulative regret over a time horizon of \(T\) time steps. For a given policy \(\), the regret is defined as \([R^{}(T)]=[_{t=1}^{T}(1-_{t}(a_{t}^{}))]\). The use of \(1\) in the regret definition for the optimal mean reward is justified because among the infinite arms with initial mean rewards following the distribution specified in (1), there always exists an arm whose mean reward is sufficiently close to \(1\).1

We note that while we have \(S_{T} T\) because the number of rotting instances is at most \(T-1\), the upper bound for \(V_{T}\) may not exist due to the lack of a constraint on the values of \(_{t}\)'s. Here we discuss an assumption for the cumulative amount of rotting. In the case of \(_{t=1}^{T-1}_{t}>T\), the problem becomes trivial as shown in the following proposition.

**Proposition 2.3**.: _In the case of \(_{t=1}^{T-1}_{t}>T\), there always exists a rotting adversary that incurs regret of \((T)\) and a simple policy that samples a new arm every round achieves the optimal regret of \((T)\)._

Proof.: The proof is provided in Appendix A.3 

From the above proposition, when \(_{t=1}^{T-1}_{t}>T\), the regret lower bound of this problem is \((T)\), which can be achieved by a simple policy. Therefore, we consider the following assumption for the region of non-trivial problems.

**Assumption 2.4**.: \(_{t=1}^{T-1}_{t} T\)_._

Notably, from the above assumption, we consider \(V_{T} T\) for the slow rotting case. We also note that the assumption is not strong, as it frequently arises in real-world scenarios and is more general than the assumption made in prior work, as described in the following remarks.

_Remark 2.5_.: The assumption of \(_{t=1}^{T-1}_{t} T\) is satisfied if mean rewards are under the constraint of \(0_{t}(a_{t}) 1\) for all \(t[T]\), because this condition implies \(_{t} 1\) for all \(t[T]\). Such a scenario is frequently encountered in real-world applications, where reward is represented by metrics like click rates or (normalized) ratings in content recommendation systems.

_Remark 2.6_.: Our rotting scenario with \(_{t=1}^{T-1}_{t} T\) is more general in scope than the one with the maximum rotting rate constraint where \(_{t}=o(1)\) for all \(t[T-1]\), which was explored in Kim et al. . This is because for our setting, \(_{t}\) is not necessarily bounded by \(o(1)\), and under the maximum rotting constraint, the condition \(_{t=1}^{T-1}_{t} T\) is always satisfied.

## 3 Algorithms and Regret Analysis

We propose an algorithm (Algorithm 1) utilizing an _adaptive sliding window_ for delicately controlling bias and variance tradeoff of the mean reward estimator from rotting rewards, drawing on insights from [4; 21]. This is why our algorithm can adapt to varying rotting rates \(_{t}\) and achieve tight regret bounds with respect to \(V_{T}\) or even \(S_{T}\). Furthermore, our algorithm accommodates the general mean reward distribution with \(>0\) by employing a carefully optimized threshold parameter.

Here we describe our proposed algorithm in detail. We define \(_{[t_{1},t_{2}]}(a)=_{t=1}^{t_{2}}r_{t}(a_{t}=a)/ n_{[t_{1},t_{2}]}(a)\) where \(n_{[t_{1},t_{2}]}(a)=_{t=1}^{t_{2}}(a_{t}=a)\) for \(t_{1} t_{2}\). Then for window-UCB index of the algorithm, we define \(WUCB(a,t_{1},t_{2},T)=_{[t_{1},t_{2}]}(a)+,t_{2}]}(a)}.\) In what follows,'selecting an arm' means that a policy chooses an arm before pulling it. In Algorithm 1, we first select an arbitrary new arm \(a^{}\) without prior knowledge regarding the arms in \(^{}\), denoting the corresponding time as \(t(a)\). We define \(_{t}(a)\) as the set of starting times for sliding windows of doubling lengths, defined as \(_{t}(a)=\{s[T]:t(a) s t-1s=t-2^{i-1}i\}\). Then the algorithm pulls the arm consecutively until the following threshold condition is satisfied: \(_{s_{t}(a)}WUCB(a,s,t-1,T)<1-\), in which the sliding window having minimized window-UCB is utilized for adapting nonstationarity. If the threshold condition holds, then the algorithm considers the arm to be a sub-optimal (bad) arm and withdraws the arm. Then it selects a new arm and repeats this procedure.

Utilizing the adaptive sliding window having minimized window UCB index enhances the algorithm's ability to dynamically identify poorly-performing arms across varying rotting rates. This adaptability is achieved by managing the tradeoff between bias and variance. The concept is depicted in Figure 1 (left), where an arm \(a\) undergoes multiple rotting events. WUCB with a smaller window exhibits minimal bias with the arm's most recent mean reward but introduces higher variance. Conversely, WUCB with a larger window displays increased bias but reduced variance. In this visual representation, the value of WUCB with a small window reaches a minimum, enabling the algorithm to compare this value with \(1-\) to identify the suboptimal arm. Moreover, as illustrated in Figure 1 (right), by taking into account the constraint of \(s=t-2^{i-1}\) for the size of the adaptive windows, we can reduce the computation time for determining the appropriate window and reduce the required memory from \(O(t)\) to \(O( t)\), respectively, for each time \(t\).

Having introduced our algorithm, we compare it with the previously proposed algorithm UCB-TP, which is tailored for the maximum rotting rate constraint \(_{t}\ (=o(1))\) for all \(t>0\) and the uniform initial mean reward distribution (\(=1\)). The mean reward estimator in UCB-TP considers the worst-case scenario with the maximum rotting rate \(\) as \(_{t}^{o}(a)- n_{t}(a)\) where \(_{t}^{o}\) is an estimator for the initial mean reward, \(n_{t}(a)\) is the number of times arm \(a\) is pulled until time \(t-1\), and \( n_{t}(a)\) is for reducing the bias from the worst-case rotting, which leads to achieving a regret bound of \((\{^{1/3}T,\})\). This estimator is not appropriate for dealing with our generalized rotting constraints because it aims to attain the regret bound regarding the maximum rotting rate \(\) without adequately addressing individual \(_{t}\) values. Our algorithm resolves this by using an adaptive sliding window estimator, which can handle rotting rates carefully. Furthermore, it can accommodate any constant \(>0\) by using a carefully optimized \(\), as shown below.

Slow Rotating (\(V_{T}\)).Here we consider the case of slow rotting, where, recall, the adaptive adversary is constrained such that the total amount of rotting is bounded by \(V_{T}\). We analyze the regret of Algorithm 1 with tuned \(\) using \(\) and \(V_{T}\). We define \(_{V}()=c_{1}\{(V_{T}/T)^{1/(+2)},1/T^{1/(+1)}\}\) when \( 1\) and \(_{V}()=c_{1}\{(V_{T}/T)^{1/3},1/\}\) when \(0<<1\) for some constant \(0<c_{1}<1\). The algorithm with \(_{V}()\) achieves a regret bound in the following theorem.

Figure 1: Illustrations for the adaptive sliding window: (left) the effect of the sliding window length on the mean reward estimation, (right) sliding window candidates with doubling lengths.

**Theorem 3.1**.: _The policy \(\) of Algorithm 1 with \(=_{V}()\) achieves:_

\[[R^{}(T)]=(\{V_{T}^{}T^{},T^{}\})&for\ \  1,\\ (\{V_{T}^{}T^{},\})&for \ \ 0<<1.\]

We observe that when \(\) increases above \(1\), the regret bound becomes worse because the likelihood of sampling a good arm decreases. However, when \(\) decreases below \(1\), the regret bound remains the same due to the inability to avoid a certain level of regret arising from estimating the mean reward. Further discussion will be provided later. Also, we observe that when \(V_{T}=O(\{1/T^{1/(+1)},1/\})\) where the problem becomes near-stationary, the regret bound in Theorem 3.1 matches the previously known regret bound for stationary infinitely many-armed bandits, \((\{T^{/(+1)},\})\), as shown in Wang et al. , Bayati et al. .

Proof sketch.: The full proof is provided in Appendix A.4. Here we outline the main ideas of the proof. There are several technical challenges involved in regret analysis, such as dealing with varying \(_{t}\) individually with respect to the total rotting budget of \(V_{T}\), adaptive estimation in our algorithm, and the generalized distributions of initial mean rewards of arms with parameter \(>0\), none of which appear in Kim et al. .

We separate the regret into two components: one associated with pulling initially good arms and another with pulling initially bad arms. An arm \(a\) is said to be good if \(_{1}(a) 1-2\) and, otherwise, it is said to be bad. The reason why the separation is required is that our adaptive algorithm has different behaviors depending on the category of arms. Good arms may be pulled repeatedly when rotting rates are sufficiently small but bad arms are not. We write \(R^{}(T)=R^{}(T)+R^{}(T),\) where \(R^{}(T)\) is the regret from good arms and \(R^{}(T)\) is the regret from bad arms.

We first provide a bound for \([R^{}(T)]\). For analyzing regret from good arms, we analyze the cumulative amount of rotting while pulling a selected good arm before withdrawing the arm by the algorithm. Let \(_{T}^{}\) be a set of distinct good arms selected until \(T\), \(t_{1}(a)\) be the initial time step at which arm \(a\) is pulled, and \(t_{2}(a)\) be the final time step at which the arm is pulled by the algorithm so that the threshold condition holds when \(t=t_{2}(a)+1\). For simplicity, we use \(t_{1}\) and \(t_{2}\) for \(t_{1}(a)\) and \(t_{2}(a)\), when there is no confusion. For any time steps \(n m\), we define \(V_{[n,m]}(a)=_{t=n}^{m}_{t}(a)\) and \(_{[n,m]}(a)=V_{[n,m]}(a)/n_{[n,m]}(a)\). We show that the regret is decomposed as

\[R^{}(T)=_{a_{T}^{}}_{1}( a)n_{[t_{1},t_{2}]}(a)+_{t=t_{1}+1}^{t_{2}}V_{[t_{1},t-1]}(a),\] (2)

which consists of regret from the initial mean reward and the cumulative amount of rotting for each arm. For the first term of \(_{a_{T}^{}}_{1}(a)n_{[t_{1},t_{2}]}(a)\) in (2), since \(_{1}(a)=O()\) from the definition of good arms \(a_{T}^{}\), we have \([_{a_{T}^{}}_{1}(a)n_{[t_{1},t_{ 2}]}(a)]=O( T)\).

The main difficulty in (2) lies in dealing with the second term, \(_{a_{T}^{}}_{t=t_{1}+1}^{t_{2}}V_{[t_{1},t-1 ]}(a)\), where we need to analyze the amount of cumulative rotting until the arm is eliminated by using the adaptive threshold condition. A careful analysis of the adaptive threshold policy is required to limit the total variation of rotting. By examining the estimation errors arising from variance and bias due to the adaptive threshold condition, we can establish an upper bound for the cumulative amount of rotting as

\[_{a_{T}^{}}_{t=t_{1}+1}^{t_{2}}V_{[t_{1},t-1 ]}(a)=T+V_{T}+_{a_{T}^{}}V_ {[t_{1},t_{2}-2]}(a)^{}n_{[t_{1},t_{2}-2]}(a)^{}.\] (3)

Therefore, from \(=_{V}()\), \(V_{T} T\), and Eqs. (2) and (3), using Holder's inequality, we have

\[[R^{}(T)]=(\{V_{T}^{}T^{},T^{}\})&for\ \  1,\\ (\{V_{T}^{}T^{},\})&for\ \ 0< <1.\] (4)

Next, we provide a bound for \([R^{}(T)]\). We employ episodic regret analysis, defining an episode as the time steps between consecutively selected distinct good arms by the algorithm. By analyzingbad arms within each episode, we can derive an upper bound for the overall regret arising from bad arms. We define the regret from bad arms over \(m^{}\) episodes as \(R^{}_{m^{}}\). We first consider the case of \(V_{T}>\{1/,1/T^{1/(+1)}\}\). In this case, by setting \(m^{}= 2V_{T}/\), we can show that \(R^{}(T) R^{}_{m^{}}\) with a high probability. By analyzing \(R^{}_{m^{}}\) with the episodic analysis, we can show that \([R^{}(T)][R^{}_{m^{} }]=(\{T^{}V_{T}^{},T^ {}V_{T}^{}\})\). As in the similar manner, when \(V_{T}\{1/,1/T^{1/(+1)}\}\), by setting \(m^{}=C_{3}\) for some constant \(C_{3}>0\), we can show that \([R^{}(T)][R^{}_{m^{} }]=(\{T^{},\})\). From the above two inequalities, we have

\[[R^{}(T)]=(\{V_{T}^{}T^{},T^{}\})&for\ \  1,\\ (\{V_{T}^{}T^{}, \})&for\ \ 0<<1.\] (5)

Finally, from (4) and (5), we can conclude the proof from \([R^{}(T)]=[R^{}(T)]+[R^{}(T)]\). 

_Remark 3.2_.: We compare our result in Theorem 3.1 with that in Kim et al. , which, recall, is under the maximum rotting rate constraint \(_{t}=o(1)\) for all \(t\) and uniform distribution of initial mean rewards (\(=1\)). For a fair comparison, we consider an oblivious adversary for rotting rates where the values of \(_{t}\)'s are determined before an algorithm is run, which may imply \(V_{T}=_{t=1}^{T-1}_{t}\) and \(=_{t[T-1]}_{t}\). Then with \(=1\), from \(V_{T} T\), we can observe that the regret bound of Algorithm 1 is tighter than that of UCB-TP  as \((\{V_{T}^{}T^{},\}) (\{^{}T,\})\), where the latter is the regret bound of UCB-TP. We will demonstrate this in our numerical results.

Abrupt Rotting (\(S_{T}\)).Here we consider abruptly rotting reward distribution under the constraint of \(S_{T}\). We consider Algorithm 1 with \(\) newly tuned by \(S_{T}\) and \(\). We define \(_{S}()=c_{1}(S_{T}/T)^{1/(+1)}\) when \( 1\) and \(_{S}()=c_{1}(S_{T}/T)^{1/2}\) when \(0< 1\) for some constant \(0<c_{1}<1\). We also define \(_{T}=_{t=1}^{T-1}[_{t}]\). In the following theorem, we present a regret upper bound for Algorithm 1 with \(_{S}()\).

**Theorem 3.3**.: _The policy \(\) of Algorithm 1 with \(=_{S}()\) achieves:_

\[[R^{}(T)]=(\{S_{T}^{}T^{},_{T}\})&for\ \  1,\\ (\{T},_{T}\})&for\ \ 0<<1.\]

As in the slow rotting case, for the abrupt rotting case (\(S_{T}\)), we observe that when \(\) increases above \(1\), the regret bound in the above theorem worsens as the likelihood of sampling a good arm decreases. When \(\) decreases below \(1\), the regret bound remains the same because we cannot avoid a certain level of regret arising from estimating the mean reward of an arm. Additionally, we observe that the regret bound is linearly bounded by \(_{T}\), which is attributed to the algorithm's necessity to pull a rotted arm at least once to determine its status as bad. Later, in the analysis of regret lower bounds, we will establish the impossibility of avoiding \(_{T}\) regret in the worst-case. Notably, in the typical cases where \(0_{t} 1\) for all \(t>0\), as discussed in Remark 2.5, \(_{T}\) is negligible in the regret bound from \(_{T} S_{T} T\). Furthermore, we observe that for the case of \(S_{T}=1\), where the problem becomes stationary (implying \(_{T}=0\)), the regret bound matches the previously known regret bound of \((\{T^{/(+1)},\})\) for the stationary infinitely many-armed bandits .

Proof sketch.: The full proof is provided in Appendix A.5. Here we provide a proof outline. We follow the proof framework of Theorem 3.1 but the main difference lies in carefully dealing with substantially rotted arms. For the ease of presentation, we consider each arm that experiences abrupt rotting as if it were newly selected by the algorithm, treating the arm before and after abrupt rotting as distinct arms. The definition of a good arm and a bad arm is based on the mean reward at the time when it is newly selected. Then we divide the regret into regret from good and bad arms as \(R^{}(T)=R^{}(T)+R^{}(T)\). From the definition of good arms, we can easily show that

\[[R^{}(T)]=O(_{S}()T)=(S_{T }^{}T^{})&for\ \  1,\\ (T})&for\ \ 0<<1.\]

For dealing with \(R^{}(T)\), we partition the regret into two scenarios: one where the bad arm is initially bad sampled from the distribution of (1) and another where it becomes bad after rotting. This can be expressed as \(R^{}(T)=R^{,1}(T)+R^{,2}(T)\). Then for the former regret, \(R^{,1}(T)\), as in the proof of Theorem 3.1, by using the episodic analysis with \(m^{}=S_{T}\), we can show that

\[[R^{,1}(T)][R_{m^{}}^{}]=(S_{T}^{}T^{})&for\ \  1,\\ (T})&for\ \ 0<<1.\]

For the regret from rotated bad arms, \(R^{,2}(T)\), it is critical to analyze significant rotting instances to obtain a tight bound with respect to \(S_{T}\), a factor not addressed in the regret analysis of slow rotting (\(V_{T}\)) in Theorem 3.1. We analyze that when there exists significant rotting, then the algorithm can efficiently detect it as a bad arm and eliminate it by pulling it at once. From this analysis, we have

\[[R^{,2}(T)]=(\{S_{T}^{}T^{},_{T}\})&for\ \  1,\\ (\{T},_{T}\})&for\ \ 0<<1.\]

Putting all the results together with \([R^{}(T)]=[R^{}(T)]+[R^{,1}(T)]+[R^{,2}(T)]\) and \(S_{T} T\), we can conclude the proof. 

Remarkably, our proposed method, utilizing an adaptive sliding window, yields a tight bound (lower bounds will be presented later) not only for slow rotting but also for abrupt rotting (\(S_{T}\)) scenarios characterized by a limited number of rotting instances. The rationale behind the effectiveness of the adaptive sliding window in controlling the bias and variance tradeoff with respect to abrupt rotting is as follows. It can be observed that the adaptive threshold condition of \(_{s_{t}(a)}WUCB(a,s,t-1,T)<1-\) is equivalent to the condition of \(WUCB(a,s,t-1,T)<1-\) for some \(s\) such that \(t_{1}(a) s t-1\) (ignoring the computational reduction trick). The latter expression represents the threshold condition tested for every time step before \(t\), encompassing the time step immediately following an abrupt rotting event. Consequently, as illustrated in Figure 2, this adaptive threshold condition can identify substantially rotated arms by mitigating bias and variance using the window starting from the time step following the occurrence of rotting.

Slow rotting (\(V_{T}\)) and abrupt rotting (\(S_{T}\)).In what follows, we study the case of rotting under both slow rotting and abrupt rotting constraints. In this case, Algorithm 1, with \(=\{_{V}(),_{S}()\}\), can achieve a tighter regret bound as noted in the following corollary, which can be obtained from Theorems 3.1 and 3.3.

**Corollary 3.4**.: _Let \(R_{V}\) and \(R_{S}\) be defined as_

\[R_{V}:=\{V_{T}^{}T^{ },T^{}\}&for\ \  1,\\ \{V_{T}^{1/3}T^{2/3},\}&for\ \ 0<<1R_{S}:= \{S_{T}^{}T^{},V_{T}\}&for \ \  1,\\ \{T},V_{T}\}&for\ \ 0<<1.\]

_The policy \(\) of Algorithm 1 with \(=\{_{V}(),_{S}()\}\) achieves the regret bound of \([R^{}(T)]=(\{R_{V},R_{S}\}).\)_

Case without Prior Knowledge of \(V_{T}\), \(S_{T}\), and \(\).Here we study the case when the algorithm does not have prior information about the values of \(V_{T}\), \(S_{T}\), and \(\) under the constraints of \(V_{T}\) and \(S_{T}\). These parameters play a crucial role in determining the optimal threshold parameter \(\) in Algorithm 1. We propose an algorithm based on estimating the optimal threshold parameter \(\) directly (Algorithm 2), rather than estimating each unknown parameter separately, employing the Bandit-over-Bandit (BoB) approach . Under assumptions concerning the bounds for the cumulative amount of

Figure 2: Adaptive sliding window for abrupt rotting.

rotting and a constrained version of the adaptive adversary for rotting rates, which are less general than Assumptions 2.1 and 2.4 but still more general than those in Kim et al. , the algorithm achieves a regret bound of \([R^{}(T)]=(\{R_{V},R_{S}\}+\{T^{(2+ 1)/(2+2)},T^{3/4}\})\). The additional cost arises from learning \(\) compared to the regret bound of Corollary 3.4. Further details of the algorithm and regret analysis are provided in Appendix A.6.

## 4 Regret Lower Bounds

In this section, we present regret lower bounds for our problem under Assumptions 2.1 and 2.4 to provide guidance on the tightness of our regret upper bounds. For the regret lower bounds, we consider worst-case instances of rotting rates. In the following theorems, we provide regret lower bounds for slow rotting (\(V_{T}\)) and abrupt rotting (\(S_{T}\)), respectively.

**Theorem 4.1**.: _For the slow rotting case with the constraint \(V_{T}\) and \(>0\), for any policy \(\), there always exists a rotting rate adversary such that the regret of \(\) satisfies_

\[[R^{}(T)]=V_{T}^{}T^{ },T^{}}.\]

Proof.: The proof is provided in Appendix A.8. 

**Theorem 4.2**.: _For the abrupt rotting case with the constraint \(S_{T}\) and \(>0\), for any policy \(\), there always exists a rotting rate adversary such that the regret of \(\) satisfies_

\[[R^{}(T)]=S_{T}^{}T^{ },_{T}}.\]

Proof.: The proof is provided in Appendix A.9. 

For the abrupt rotting (\(S_{T}\)) case, it is unavoidable to incur a \((_{T})\) regret because an arm may only be rotated once and any algorithm pulls this rotted arm at least once in the worst case. From Table 1, we can observe that Algorithm 1 achieves near-optimal regret when \( 1\). The optimality proven only for \( 1\) has also been observed for stationary infinitely many-armed bandits [5; 24]. We believe that our regret upper bounds are near-optimal across the entire range of \(\). Achieving tighter regret lower bounds when \(<1\) is left for future research; see Appendix A.1 for further discussion.

## 5 Experiments

In this section, we present numerical results validating some claims of our theoretical analysis.2 We use randomly generated datasets under a uniform distribution for initial mean rewards (\(=1\)).

We first compare the performance of our Algorithms 1 and 2 with UCB-TP , the state-of-the-art algorithm for the rotting setting, and SSUCB , a near-optimal algorithm for stationary infinitely

Figure 3: Regret Performance comparison between our algorithms and benchmarks.

many-armed bandits. For comparison with UCB-TP, recall our discussion in Remark 3.2. We set the rotting rates such that \(_{t}=1/(t(T))\) for all \(t\), for which \(=_{1}=1/(T)=o(1)\), \(V_{T}=O(1)\), and \(S_{T}=T\). In Figure 3, we can observe that Algorithms 1 and 2 perform better than UCB-TP and SSUCB (and Algorithm 1 outperforms Algorithm 2), which is in agreement with our theoretical analysis for the case \(=1\). In this case, the regret bounds for Algorithms 1 and 2 are \((T^{2/3})\) and \((T^{3/4})\) from Corollary 3.4 and Theorem A.15, respectively, which are tighter than the regret bound of \((T/(T)^{1/3})\) for UCB-TP. Additional experiments can be found in Appendix A.10.

## 6 Conclusion

We explore the challenges of infinitely many-armed bandit problems with rotting rewards, focusing on slow rotting (\(V_{T}\)) and abrupt rotting (\(S_{T}\)) scenarios. To address these challenges, we propose an algorithm incorporating an adaptive sliding window, which achieves tight regret bounds for both cases. We also provide regret lower bounds for both slow rotting and abrupt rotting cases. Lastly, we demonstrate our algorithm using synthetic datasets.

## 7 Acknowledgements

The authors thank Joe Suk and the anonymous reviewers for helpful discussions. JK was supported by the Global-LAMP Program of the National Research Foundation of Korea (NRF) grant funded by the Ministry of Education (No. RS-2023-00301976). SY was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No. RS-2022-II20311, Development of Goal-Oriented Reinforcement Learning Techniques for Contact-Rich Robotic Manipulation of Everyday Objects)