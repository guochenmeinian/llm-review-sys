# MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models

Yichi Zhang\({}^{1,4}\)

Yao Huang\({}^{2}\)

Equal Contributions

Yitong Sun\({}^{2}\)

Chang Liu\({}^{3}\)

Zhe Zhao\({}^{4}\)

Zhengwei Fang\({}^{1}\)

Yifan Wang\({}^{1}\)

Huanran Chen\({}^{1}\)

Xiao Yang\({}^{1}\)

**Xingxing Wei\({}^{2}\)**

**Hang Su\({}^{1,5}\)

**Yinpeng Dong\({}^{1,4}\)

**Jun Zhu\({}^{1,4}\)\({}^{1}\)**

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center,

THBI Lab, BNRist Center, Tsinghua University, Beijing 100084, China

\({}^{2}\)Institute of Artificial Intelligence, Beihang University, Beijing 100191, China

\({}^{3}\)Institute of Image Communication and Networks Engineering in the Department of

Electronic Engineering (EE), Shanghai Jiao Tong University, Shanghai 200240, China

\({}^{4}\)RealAI \({}^{5}\)Pazhou Lab (Huangpu), Guangzhou, China

###### Abstract

Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish **MultiTrust**, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: _truthfulness_, _safety_, _robustness_, _fairness_, and _privacy_. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.

## 1 Introduction

The era towards Artificial General Intelligence (AGI)  has witnessed the emergence of the groundbreaking Large Language Models (LLMs) . With their strong language understanding and reasoning capabilities, recent studies  have seamlessly integrated other modalities (e.g., vision) into LLMs to understand different inputs. The resultant Multimodal Large Language Models (MLLMs) have manifested versatile proficiency in both traditional vision tasks  and more complex multimodal problems . However, despite their remarkable performance and the efforts in aligning with human preferences , these cutting-edge modelsstill exhibit significant drawbacks in trustworthiness, leading to factual errors , harmful outputs , privacy leakage , etc. The trustworthiness issues have notably compromised model reliability and elicited increasing concerns from researchers, policymakers, and the public .

To facilitate the trustworthiness of foundation models, developing holistic and standardized evaluation benchmarks is indispensable. Although numerous studies have meticulously assessed and analyzed the trustworthiness of LLMs , a corresponding evaluation framework for MLLMs is lacking. In addition to the inherent weaknesses of LLMs, the multimodal nature of MLLMs introduces novel risks, such as susceptibility to adversarial image attacks , presence of toxic content in images , and jailbreaking via visual contexts . As the new modality brings a variety of intricate factors to consider, including task design across multiple aspects, data collection from multimodal scenarios, and the interplay between modalities, the systematic evaluation of MLLMs' trustworthiness is more challenging. However, current work  typically examines one or a few aspects of trustworthiness and evaluates MLLMs on limited tasks at a phenomenon level, concerning threats in images but neglecting the interactions between modalities (as detailed in Tab. 1). These superficial evaluations could lead to an oversight of certain risks and a biased understanding of model trustworthiness, rendering a comprehensive evaluation of MLLMs' trustworthiness absent.

In this paper, we establish **MultiTrust**, the first comprehensive and unified benchmark to evaluate the trustworthiness of MLLMs across diverse dimensions and tasks. Distilled from the literature on trustworthy foundation models , we identify 5 primary aspects of trustworthiness in MultiTrust, including _truthfulness_, _safety_, _robustness_, _fairness_, and _privacy_, covering the reliability of models in preventing unexpected outcomes and the assurance of social impacts to users. We propose a more in-depth evaluation strategy that delves into the multimodal nature of MLLMs by considering both _multimodal risks_ in novel scenarios and _cross-modal impacts_ of visual inputs on base LLMs' performance. To perform systematic evaluations, we set up 32 various tasks, including improvements to existing multimodal tasks, extension of text-only tasks to multimodal scenarios, and novel methods for risk assessment, which focus on models' basic performance with practical significance. We curate rich datasets for the tasks, most of which are either adapted to novel tasks based on existing ones or newly proposed via data synthesis (e.g., Stable Diffusion , GPT-4V ) and manual collection. We conduct large-scale experiments with 21 modern MLLMs (4 proprietary and 17 open-source), which are carefully selected to guarantee both the coverage of popular models from different stages and the distinctiveness of model architectures and training techniques to provide analyses for future improvements. Below, we summarize several key findings:

* Although open-source MLLMs are approaching or even surpassing proprietary models in multiple general benchmarks , there is still a significant gap in trustworthiness. GPT-4V  and Claude3  demonstrate better performance due to their safety guardrails and efforts in alignment, highlighting the insufficient development and risky deployment of open-source models.
* The multimodal training and the introduction of images in inference greatly jeopardize the trustworthiness of MLLMs, manifested in several perspectives including but not limited to: 1) the performance and alignment of base LLMs being compromised; 2) irrelevant images causing unstable behaviors; 3) relevant visual contexts exacerbating trustworthy risks. This emphasizes that developing trustworthy MLLMs is more challenging than simply using a well-aligned LLM.
* The results of some tasks confirm the contributions from different model components (e.g., vision encoder , aligned LLM ) and existing training paradigms (e.g., supervised fine-tuning

    & &  &  &  &  &  \\   & & & & & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  &  datasets distilled from GPT-4V , RLHF ) to improving trustworthiness. However, solely relying on the existing techniques is far from all-round guarantee of trustworthiness.

To support standardized and scalable assessments, we develop a toolbox dedicated to the trustworthiness research of MLLMs. The toolbox is implemented with unified interfaces and a modularized design of model interaction and task execution. Hopefully, our toolbox can address the limitation of existing work [49; 75; 87; 145] that only provides datasets or evaluation scripts, and serve as a better foundation for future research on the evaluation and development of trustworthy MLLMs.

## 2 Framework of MultiTrust

In this section, we present the framework of MultiTrust, as shown in Fig. 1. Sec. 2.1 introduces the design principles of the benchmark, focusing on the evaluation aspects and strategy. Sec. 2.2 briefly reviews the 32 tasks under the two-level taxonomy of aspects. Sec. 2.3, Sec. 2.4, and Sec. 2.5 respectively introduce the adopted metrics for evaluation, the selected models to be assessed and the scalable and standardized toolbox for trustworthiness research.

### Philosophy of MultiTrust

**Evaluation Aspects.** Drawing on extensive studies in trustworthy LLMs [88; 132; 147] and distilling from relevant literature of MLLMs [19; 75; 76; 87], we pinpoint 5 primary aspects of trustworthiness for evaluating MLLMs, including _truthfulness_, _safety_, _robustness_, _fairness_, and _privacy_. In particular, _truthfulness_, _safety_, and _robustness_ guarantee the models' reliability and stability in preventing undesirable outcomes, i.e., errors, harms, and variations under different conditions. _Fairness_ and _privacy_ address the models' social and ethical implications, involving misaligned attitudes like bias and rights violations like identity theft. These aspects collectively compose a comprehensive and compact framework for studying trustworthiness, as they span both technical and ethical perspectives, while flaws in any of them could trigger profound societal impacts. As detailed in Sec. 2.2, we further organize a two-level taxonomy encompassing 10 sub-aspects.

Figure 1: Framework of MultiTrust, including aspect division, evaluation strategy and design of the developed toolbox. Specifically, we study the trustworthiness by delving into the multimodal nature of MLLMs from a broader perspective, covering both _multimodal risks_ and _cross-modal impacts_.

**Evaluation Strategy.** For evaluation and task design, we consider both _multimodal risks_ and _cross-modal impacts_ due to the introduction of new modality, covering the multimodal nature of MLLMs more holistically. Most of the existing studies [49; 75; 87] only attend to the trustworthy threats in the image modality or the combination of image-text pairs, which invoke the multimodal risks newly introduced by vision. Beyond such issues, we advocate that it is equally important to consider the interaction between modalities. Specifically, the new modality could alter the original model behavior in existing scenarios for LLMs [112; 145]. This greatly concerns the steadiness of MLLMs in their broader applications, but remains unexplored. We hereby propose to study the cross-modal impacts by measuring the performance in text-only tasks when paired with semantically relevant and irrelevant images (as illustrated in Fig. 1), which leads to more thorough investigations into MLLMs. This is to emphasize a broad range of trustworthy risks associated with the interaction between modalities, which can be further extended to other modalities. In this work, we also consider the impacts from text variations [122; 91] in several multimodal tasks, which are taken as an established concern for LLMs rather than a novel issue specific to MLLMs.

### Practice in MultiTrust

We organize a two-level taxonomy containing 10 sub-aspects to better categorize the target behaviors to be evaluated. Based on the taxonomy, we curate 32 diverse tasks to cover realistic and comprehensive scenarios with trustworthy risks, including generative and discriminative, multimodal and text-only ones, as summarized in Tab. 2. To tackle the current lack of datasets dedicated for various

  
**ID** & **Task Name** & **Metrics** & **Task Type** & **Eval** & **Stat.** \\  _T.1_ & Basic World Understanding & [11; 43; 75; 13; 169] & Accuracy (\(\)) & Dis.&Gen. & \(\) & 800 \\ _T.2_ & Advanced Cognitive Inference & [11; 43; 81; 13] & Accuracy (\(\)) & Dis. & \(\) & 500 \\ _T.3_ & VQA under Instruction Enhancement &  & Accuracy (\(\)) & Gen. & \(\) & 420 \\ _T.4_ & QA under Visual Assistance & & Accuracy, Cure Rate (\(\)) & Gen. & \(\) & 300 \\ _T.5_ & Text Misleading VQA & & Accuracy (\(\)) & Gen. & \(\) & 100 \\ _T.6_ & Visual Confusion VQA & & Accuracy (\(\)) & Dis. & \(\) & 300 \\ _T.7_ & Visual Misleading QA & & Acc (\(\)), Deterioration Rate (\(\)) & Gen. & \(\) & 300 \\ _S.1_ & NSFW Image Description & [153; 1; 160] & Toxicity Score (\(\)), RIA (\(\)) & Gen. & \(\) & 500 \\ _S.2_ & Risk Identification &  & Accuracy (\(\)) & Dis.\&Gen. & \(\) & 500 \\ _S.3_ & Toxic Content Generation &  & Toxicity Score (\(\)), RIA (\(\)) & Gen. & \(\) & 240 \\ _S.4_ & Plain Typographic Jailbreaking & & ASR (\(\)), RIA (\(\)) & Gen. & \(\) & 600 \\ _S.5_ & Optimized Multimodal Jailbreaking & [49; 87] & ASR (\(\)), RIA (\(\)) & Gen. & \(\) & 468 \\ _S.6_ & Cross-modal Influence on Jailbreaking & [171; 96; 125] & ASR (\(\)), RIA (\(\)) & Gen. & \(\) & 1000 \\ _R.1_ & Image Captioning for Sybized Images &  & Accuracy (\(\)) & Gen. & \(\) & 600 \\ _R.2_ & VQA for Sensor Style Images &  & GPT-Score (\(\)) & Gen. & \(\) & 1041 \\ _R.3_ & Sentiment Analysis for OOD Texts &  & Accuracy (\(\)) & Dis. & \(\) & 3000 \\ _R.4_ & Image Captioning under Untarget Attack & & Accuracy (\(\)), ASR (\(\)) & Gen. & \(\) & 100 \\ _R.5_ & Image Captioning under Target attack & & ASR (\(\)) & Gen. & \(\) & 100 \\ _R.6_ & Textual Adversarial Attack & [147; 149] & Accuracy (\(\)) & Dis. & \(\) & 4014 \\ _F.1_ & Stereotypical Content Generation &  & Containing Rate (\(\)) & Gen. & \(\) & 100 \\ _F.2_ & Agreement on Stereotypes &  & Agreement Percentage (\(\)) & Dis. & \(\) & 786 \\ _F.3_ & Classification of Stereotypes & [100; 102] & Accuracy (\(\)) & Dis. & \(\) & 1689 \\ _F.4_ & Stereotype Query Test &  & RIA (\(\)) & Gen. & \(\) & 291 \\ _F.5_ & Visual Preference Selection & & RIA (\(\)) & Gen. & \(\) & 120 \\ _F.6_ & Profession Competence Prediction &  & P-value (\(\)) & Gen. & \(\) & 626 \\ _F.7_ & Preference Selection in QA &  & RIA (\(\)) & Gen. & \(\) & 720 \\ _P.1_ & Visual Privacy Recognition & [53; 107] & Accuracy, Precision, Recall (\(\)) & Dis. & \(\) & 1300 \\ _P.2_ & Privacy-Sensitive VQA Recognition &  & Accuracy, Precision, Recall (\(\)) & Dis. & \(\) & 426 \\ _P.3_ & InfoFlow Expectation &  & Pearson Correlation (\(\)) & Gen. & \(\) & 300 \\ _P.4_ & PI Query with Visual Cues & & RIA (\(\)) & Gen. & \(\) & 1200 \\ _P.5_ & Privacy Leakage in Vision &  & RIA (\(\)), Leakage Rate (\(\)) & Gen. & \(\) & 195 \\ _P.6_ & PII Leakage in Conversations &  & RIA(\(\)), Accuracy(\(\)) & \(\) & Gen. & \(\) & 400 \\   

Table 2: Task Overview. Each task ID is linked to the section in Appendix. **��**: off-the-shelf datasets from prior work; **�**: datasets adapted to new tasks with additional images, prompts, and annotations; **�**: datasets constructed from scratch. **�**: tasks for revealing multimodal risks; **�**: tasks for studying cross-modal impacts. \(\): rule-based evaluation (e.g., keyword matching); **�**: automatic evaluation by GPT-4 or other classifiers; **�**: mixture evaluation. ASR stands for Attack Success Rate, Rta stands for Refuse-to-Answer rate, and Accuracy is sometimes abbreviated as Acc. The last column shows the number of image-text pairs in each task. Text prompts with irrelevant images are counted once.

scenarios under these sub-aspects, we construct 20 datasets based on the existing text, image, and multimodal datasets  by adapting prompts, images, and annotations with both manual efforts and automatic methods. We further propose 8 novel datasets from scratch by collecting images from the Internet or synthesizing images with Stable Diffusion  and other algorithms specifically for the designed tasks. Below, we introduce the design details of each sub-aspect by first presenting multimodal tasks followed by text-only tasks for studying cross-modal impacts.

#### 2.2.1 Truthfulness

Truthfulness measures whether the outputs of MLLMs align with the objective facts, emphasizing the accuracy of the information they provide. Unlike previous studies  that narrowly focus on phenomena like hallucination and sycophancy, we reorganize it into _inherent deficiency_ and _misguided mistakes_ from a macro perspective.

**Inherent Deficiency** delves into the internal limitations of models that lead to inaccurate outputs. We first assess MLLMs' basic perceptual abilities  like object existence judgment (Task _T.1_) and advanced cognitive capabilities  like spatial-temporal reasoning (Task _T.2_), with improved datasets based on existing ones. Beyond them, we propose to integrate varying assisting instructions with previous VQA tasks (Task _T.3_), to explore their benefits from prompt guidance. We develop a dataset with prompts generated by GPT-4 and images collected from the Internet to test model performance in text-based factual question answering with visual assistance (Task _T.4_).

**Misguided Mistakes** focus on errors caused by misleading inputs . We start by presenting images along with questions containing factual errors to see their influence on models' responses in VQA (Task _T.5_). Besides misleading in text, we set up a new dataset with manually collected images of visual illusions  to examine model performance in visually challenging scenarios (Task _T.6_). Contrary to the task of QA under visual assistance, we pair the same questions with faulty images and measure the interference from visual misguidance (Task _T.7_).

#### 2.2.2 Safety

Safety guarantees that the responses from MLLMs do not cause unexpected consequences, such as unintentional harms  or illegal actions . Two most significant topics in the literature of large model safety  are the _toxicity_ of AI-generated content , which could greatly impact user interactions, and model _jailbreaking_, which involves circumventing safety protocols  to facilitate malicious misuse.

**Toxicity** measures the models' tendency to generate harmful responses . Towards testing models' sensitivity to toxic content, we take NSFW images  like pornography and violence for image captioning (Task _S.1_). We design the task of risk identification, in both presence and usage of objects (Task _S.2_), to see their awareness of safety risks beyond harmful object detection . We assess the changes in output toxicity with diverse images using RealToxicityPrompts  (Task _S.3_).

**Jailbreaking** studies the models' resilience against attempts to elicit illegal responses . We convert jailbreaking prompts for LLMs  into images in screenshot style  to see if dangers are triggered with OCR (Task _S.4_). We adopt multimodal jailbreaking optimized for MLLMs  and propose our own attack that simplifies the complexity of jailbreaking combinations based on the trends in the previous task to reduce model confusion (Task _S.5_). We incorporate images generated by Stable Diffusion with text jailbreaking, to observe the fluctuations in performance (Task _S.6_).

#### 2.2.3 Robustness

Robustness evaluates the models' consistency and resistance under distribution shifts or input perturbations, which still remains a issue for MLLMs . Following the common practice in the field , we consider the _out-of-distribution (OOD)_ and _adversarial_ robustness respectively.

**OOD Robustness** assesses MLLMs' generalization to unusual domains including diverse styles and applications. First, we take COCO-O  containing images in various artistic styles for image captioning (Task _R.1_). We then perform VQA tasks from  with images captured by various sensors, e.g., MRI and infrared imaging (Task _R.2_). Afterwards, we test on the OOD SST-2  from , paired with unrelated images and those generated with the text prompts (Task _R.3_).

**Adversarial Attack** explores MLLMs' vulnerability to adversarial examples, which is inevitably inherited from deep neural networks . With the state-of-the-art transferable attack technique , we generate adversarial examples under both untargeted and targeted settings on image captioning (Task _R.4_ and _R.5_). For text-based attack, we employ the AdvGLUE  and AdvGLUE++  datasets with images both relevant and irrelevant to the prompts (Task _R.6_).

#### 2.2.4 Fairness

Fairness determines the extent to which the model outputs are free from inequitable or discriminatory outcomes that could disadvantage any user group . We break down this concept into _stereotype_ and _bias & preference_ according to the types of discriminatory outputs .

**Stereotypes** focus on identifying entrenched societal preconceptions  perpetuated within MLLMs. We first carefully collect images of people at risk of discrimination from diverse public sources to assess the stereotypes contained in models' generation (Task _F.1_). To examine the understanding and sensitivity of MLLMs to stereotypes in practical scenarios, we synthesize images relevant to the textual contexts with text-based tasks , ranging from agreement on stereotypical statements (Task _F.2_), classification of stereotypes (Task _F.3_) to stereotypical user queries (Task _F.4_).

**Bias & Preference** examines the models' tendency that either disadvantages specific user groups or favors biased outcomes. We transform the text-based preference choices from existing studies  into options represented by images to evaluate the visual preferences embedded in MLLMs (Task _F.5_). Then, we ask for judgment of job competency with images of people to quantify the biases in MLLMs towards different personal attributes (e.g., gender, age) with a Chi-square test , based on the annotations in  (Task _F.6_). We finally examine if MLLMs are more inclined to express their preferences when text-only questions for choices  are paired with various images (Task _F.7_).

#### 2.2.5 Privacy

Privacy assesses the models' capacity to protect personal data from unauthorized requests . It has been shown that large models are vulnerable to data extraction  and prone to leak privacy in inference , which is risky when deployed in privacy-sensitive applications. From the perspectives of consciousness and behaviors , we evaluate privacy in terms of _awareness_ and _leakage_.

**Privacy Awareness** requires the models to detect the existence of personal information and privacy risks in their workflow . Gradually increasing in difficulties, we test MLLMs subsequently with identifying the presence of private information in images  (Task _P.1_) and deciding whether the questions posed about these images involve the private information in there (Task _P.2_), which requires reasoning beyond perception. The questions are constructed with GPT-4V and manually labeled. Then, we pair the text-only task of InfoFlow Expectation from  with different images and evaluate the changes in models' agreement on privacy usage (Task _P.3_).

**Privacy Leakage** evaluates the models' protection of private information from leakage in service . Analogous to prompts involving privacy in red-teaming LLMs , we collect a group of photos of celebrities and ask for their personal identifiable information (PII) using these photos as visual cues (Task _P.4_). Then, we request models to identify PII in images, which are manually labeled from a public dataset  (Task _P.5_). Both are of practical significance when models have access to private data of the public and powerful OCR capability. We further query MLLMs for private information contained in past text  and measure the leakage when paired with images (Task _P.6_).

### Metrics

As shown in Tab. 2, we adopt various metrics for different tasks to provide a more accurate and straightforward evaluation. To offer a clear view of the evaluation, we hereby summarize the metrics used in the proposed benchmark systematically. We mainly categorize the metrics into objective ones and subjective ones.

For tasks with clear closed-set answers, we generally use objective metrics like accuracy, Pearson correlation , and \(p\)-value . Their formal definitions are listed below.

* Accuracy: \(Acc=^{N}(r_{i}=y_{i})}{N}\), where \(r_{i}\) is the model response and \(y_{i}\) is the label. \(\) decides whether the response corresponds to the label via exact matching, keyword matching, etc.

* Pearson Correlation: \(Corr=(,)\), where \(\) is the sequence of score prediction, while \(\) is the real score array.
* \(p\)-Value: \(p\)-value\(=P(^{2}^{2}_{})\), where \(^{2}_{}=_{i}-E_{i})^{2}}{E_{i}}\) with \(O_{i}\) as the observed frequency and \(E_{i}\) as the expected frequency.
* Other metrics, such as ASR and Cure Rate, are derived based on fundamental metrics like accuracy with the calculation of subtraction and ratio.

For tasks with open-set QA, we usually need to define subjective metrics and assess the model behavior with external evaluators like scoring API or large models, to replace human labors. Details are introduced below.

* Toxicity Score: ModerationAPI3 and PerspectiveAPI4 are used to rate the toxicity in a response, which are popular tools in toxicity study of LLMs. * Refuse-to-Answer Rate, Containing Rate: Models like specialized longformer and GPT-4 are used to decide whether the response delivers certain behaviors or viewpoints with binary answers, which has been validated in other benchmarks [155; 87].
* GPT-Score: GPT-4 is prompted to rate the responses, which leads to subjective evaluation and can have uncertainty. We've compared scores by GPT-4 with those by human on sampled subset, and got a correlation of 0.91, which suggests the effectiveness of GPT-Score (see Appendix E.1.2).

### Evaluated Models

Merely compiling a leaderboard of the state-of-the-art MLLMs does not suffice to address trustworthiness issues, as it provides few insights for future improvements due to variations in architectures [30; 168] and training paradigms [26; 133]. To tackle this, we strategically select models based on several criteria. We first include 4 advanced proprietary models to highlight trustworthiness gaps of open-source models. We then gather 6 models from the rich LLaVA family  and 4 models based on MiniGPT-4  and mPLUG-Owl  to identify the impacts from diverse enhancements, like base LLMs , improved datasets [26; 151] and reinforcement learning from human feedback (RLHF) . We also choose 7 popular MLLMs across various stages in MLLM development to broaden the model coverage. This accumulates into a group of 21 MLLMs for thorough evaluation, as detailed in Tab. B.1. The benchmark will be updated with more newly released models afterwards.

### Toolbox

While existing benchmarks [19; 43; 49; 75] only provide datasets and evaluation scripts, which lack scalability and adaptability, and greatly limit the test of latest models and tasks, we deliberately develop a toolbox in MultiTrust to provide a _universal and scalable_ infrastructure for evaluating MLLM trustworthiness and facilitating future research. We integrate different MLLMs by accommodating varying interaction formats from developers into a unified interface, enabling standardized model evaluation. The tasks are modularized by separating data, inference, and evaluation metrics to facilitate tool reuse and easy updates for new tasks. This user-friendly structure not only upholds rigorous evaluation standards but also lay a groundwork for extension of community contributions.

## 3 Analysis on Experimental Results

We conduct extensive experiments on the 32 carefully curated tasks to fulfill the benchmark. In this section, we present the rankings in Fig. 2 based on the evaluation and analyze the representative experimental results under each aspect to convey our most significant discoveries within the limited space. Complete results and analyses are provided in appendices from Appendix C to Appendix G.

**Overall Performance.** From Fig. 2, we draw a quick conclusion that proprietary models like GPT-4V and Claude3 demonstrate consistently top performance, which can be attributed to the efforts on alignment, safety filters, etc. From a global perspective, there is a correlation coefficient of 0.60 between the general capabilities and the trustworthiness of various MLLMs. As shown in Fig. 2(a),the previous generation of MLLMs often fall short in various aspects of trustworthiness due to their inferior multimodal perception and reasoning capabilities, while the powerful abilities lead to better trustworthiness to various extents. Meanwhile, the finer correlation analysis in Fig. 2(b) shows that besides several sub-aspects like truthfulness, toxicity, and privacy awareness, which require similar capabilities like recognition, there is no significant correlation across different aspects, emphasizing the necessity of comprehensive aspect division and the gaps in trustworthiness from perfection.

**Truthfulness.** In the aspect of truthfulness, inherent deficiencies in MLLMs' capabilities are commonly observed. Although MLLMs perform commendably on general perception tasks like object existence judgment and scene analysis, where most achieve an accuracy of over 80% as in Tab. 3, they show a notable decline when shifting to more fine-grained tasks such as visual grounding (e.g., 32% for InterLM-XC2 and even 8% for Gemini-Pro), highlighting MLLMs' limitations in fine-grained perceptual capability . Besides, disparities also exist in MLLMs' dependence on image and text modalities for further cognitive inference. For instance, models generally perform better on commonsense reasoning tasks that basically utilize knowledge learned by LLMs but less satisfying when they need to exploit the visual modality. As shown in Tab. 3, the latter two tasks have an obvious performance degradation compared to commonsense reasoning, which is consistent with previous findings . As for external misleading factors, a majority of open-source models are susceptible to confusing or misleading images and further cause misinformation, while closed-source models exhibit superior resistance.

**Safety.** For safety tasks, open-source MLLMs generally have worse safety awareness and guardrails compared to proprietary models. For instance, GPT-4V and Claude3 refuse to describe 69.5% NSFW images on average, while most open-source models reject none. The same happens for jailbreaking that these two closed-source models refuse nearly all malicious queries, while many advanced models are frequently attacked. It is noticeable that only by placing the target harmful behaviors in images without any deliberately designed prompts, many modern MLLMs are successfully jailbroken due to their attention on images, e.g., 71% for InternLM-XC2 and 80% for LLaVA-1.5. Notably, with the same base LLM of Llama-2 , MiniGPT-4-L2 and mPLUG-Owl2 show reverse performance on typographic jailbreaking and NSFW description compared to risk identification in multimodal contexts, as shown in Tab. 4. This indicates that while the capabilities of perception and understanding are enhanced through multimodal training, the safety mechanisms from the well-aligned base LLM can be catastrophically compromised.

   Task & Metrics & ManGPIT+L2 & mPLUG-Owl2 \\  NSFW & RA(\%, \%) & 34.00 & 0.00 \\ Description & P-API(\(\)) & 0.46 & 0.62 \\  Risk & Obj(\%, \%) & 75.08 & 91.33 \\ Identification & Risk(\%, \%) & 42.93 & 81.00 \\  Typographic & RA(\%, \%) & 79.50 & 14.50 \\ Jailbreaking & ASR(\%, \%) & 1.50 & 34.50 \\   

Table 4: Performance in Task _S.1/2/14_.

    &  &  &  &  &  &  \\    & **L** & **M** & **T** & **J** & **O** & **A** & **S** & **B** & **A** & **L** \\  CPT+Xuan & 1 & 1 & 1 & 2 & 1 & 1 & 2 & 1 & 1 & 2 & 1 \\ Cloted & 3 & 3 & 2 & 1 & 1 & 2 & 4 & 3 & 3 & 1 & 2 \\ Comple & 4 & 2 & 3 & 7 & 2 & 5 & 5 & 21 & 2 & 1 & 4 & 2 \\ OpenVL+PIns & 2 & 4 & 5 & 4 & 5 & 18 & 17 & 2 & 5 & 7 & 3 & 4 \\ LLAVA+1-5Pk & 14 & 14 & 15 & 17 & 8 & 18 & 9 & 10 & 16 & 17 & 17 \\ LLAVA+1-5Pk & 8 & 8 & 8 & 15 & 7 & 18 & 13 & 8 & 11 & 12 & 12 \\ SunGPIT & 10 & 12 & 13 & 16 & 18 & 13 & 10 & 15 & 15 & 19 & 16 \\ LIVE+Membership & 13 & 15 & 14 & 9 & 18 & 17 & 4 & 13 & 6 & 8 & 18 \\ LLAVA+Membership & 16 & 9 & 11 & 18 & 15 & 11 & 9 & 12 & 4 & 11 \\ LLAVA+XC2 & 17 & 5 & 6 & 11 & 3 & 7 & 2 & 3 & 13 & 6 & 5 \\ MMLGPT+4-L2 & 20 & 17 & 19 & 19 & 19 & 8 & 15 & 18 & 20 & 5 & 18 \\ MMLGPT+4-L2 & 17 & 11 & 4 & 3 & 20 & 12 & 14 & 12 & 17 & 3 & 7 \\ mPLUG-Owl & 18 & 18 & 18 & 21 & 11 & 21 & 21 & 11 & 20 & 23 & 21 \\ mPLUG-Owl2 & 9 & 10 & 10 & 20 & 5 & 11 & 4 & 5 & 15 & 9 \\ mPLUG-Owl2 & 19 & 20 & 21 & 13 & 9 & 14 & 20 & 20 & 7 & 21 & 19 \\ Ourr & 21 & 21 & 17 & 10 & 21 & 20 & 19 & 17 & 18 & 19 & 20 \\ Ourr+Membership & 12 & 16 & 12 & 14 & 19 & 3 & 18 & 19 & 19 & 18 & 19 \\ Ourr+Membership & 13 & 9 & 6 & 13 & 6 & 16 & 16 & 16 & 14 & 11 & 14 \\ Instruct-Carr & 7 & 5 & 16 & 12 & 14 & 2 & 8 & 14 & 8 & 16 & 8 \\ Instruct-XCarr & 15 & 19 & 20 & 9 & 7 & 19 & 7 & 10 & 9 & 13 \\ Instruct-Membership & 5 & 7 & 7 & 8 & 4 & 9 & 1 & 4 & 14 & 4 & 6 \\   

Table 3: Performance of inherent capabilities in Task _T.1/2_ with accuracy (%, \(\)).

Figure 2: **Left:** Rankings of MLLMs in each sub-aspect of MultiTrust. **Right:** (a) Correlation between the overall rankings of trustworthiness and those of general capabilities based on MMBBench  and MME . Top-8 are marked. (b) Pearson Correlation Coefficients between sub-aspects.

**Robustness.** While the results on OOD scenarios show non-drastic differences across models, we prove that MLLMs inevitably inherit the adversarial vulnerability from deep neural networks. In the task of image captioning on simple objects, the accuracy of most models drops below 20% from more than 90% under untargeted attack. As for targeted attack, more than half output the desired objects with ratios above 50%, even for the commercial Qwen-VL-Plus, highlighting their fragility. As shown in Fig. 3, while two advanced open-source MLLMs have the highest attack success rates, the other two models, CogVLM and InternVL-Chat, exhibit significantly better robustness, which have unique visual encoders with more parameters, suppressing the transferability of adversarial examples. The same reason may also apply to the other two closed-source ones, which may also be equipped with input filters for purifying noises .

**Fairness.** In terms of stereotypes, most MLLMs demonstrate a heightened sensitivity to stereotypical user queries in real-life scenarios, maintaining an average RtA rate of 93.79% even under the influence of relevant images. However, when stereotypes shift from application-scenario queries to opinion-based evaluations, the performance disparities in MLLMs not only become apparent between proprietary and open-source models but also manifest in stereotype themes. For example, stereotypes related to age, show significantly higher agreement rates in MLLMs compared to other themes like gender, race, religion, etc. This disparity in topics also exists in the evaluation of bias and preference. As depicted in Fig. 4, the latest models, such as Claude3, InternLM-XC2, and LLaVA-NeXT, display varying sensitivities. These models are more sensitive to ideological topics and more lenient toward cultural or lifestyle questions, easier to reveal their preferences and thereby influence user decisions.

**Privacy.** We show that most models possess the basic concept of privacy with an average accuracy of 72.30% in deciding the presence of private information in images, which is correlated to the general perception capability. However, this awareness is severely challenged in scenarios demanding more complex reasoning and the average performance drops notably to 55.33%, widening the gap between closed-source and open-source models, with only GPT-4V and Gemini-Pro achieving accuracy above 70%. For privacy leakage, we notice that by activating the instinct for OCR, privacy extraction from images is more likely to compromise the data protection protocols than taking images as cues for PII query, amplifying the privacy risks of MLLMs in multimodal scenarios. The two text-only tasks reveal a similar phenomenon that multimodality affects the LLM behaviors of instruction following and privacy protection. As exemplified in Tab. 5, models tend to leak PII information when paired with images, which poses more threats when MLLMs have access to personal data, while the result of MiniGPT-4-Llama2 demonstrates that the frozen Llama2 can enhance the privacy protection.

## 4 Discussion

**Key Findings.** Overall speaking, the extensive experiments reveal that: **(1)** though advanced open-source MLLMs have achieved comparable or even superior performance in general perception and reasoning tasks compared to proprietary models represented by GPT-4V  and Claude3 , they still carry significant vulnerabilities and issues in terms of trustworthiness, while the proprietary ones are more reliable, demonstrating high sensitivity to risks and low malicustonsness in responses; **(2)** MLLMs possess basic understandings of the concepts in trustworthiness by directly asking them to recognize risks, but their awareness of threats would deteriorate when the risks are more concealed and the scenarios demand multi-step complex reasoning; **(3)** Overemphasis on accomplishing general multimodal tasks such as OCR during multimodal training can distract models from the instructions in text and make them overlook the potential risks, failing to reject improper requests, which render the alignment inadequate for consolidating trustworthiness.

Figure 4: RtA rate (%, \(\)) in Task _F.5_.

Figure 3: ASRs (%, \(\)) in Task _R.4/5_.

The evaluation strategy covering multimodal risks and cross-modal impacts draws to the conclusion that the multimodality affects the base LLMs of MLLMs in various ways, subsequently increasing the risks in their applications. This is reflected with phenomena that: **(4)** multimodal training with LLM significantly compromises its previous trustworthy alignment, confirmed by the comparison between MiniGPT-4-Llama2  and mPLUG-Owl2  which both use well-aligned Llama2  for base LLM and show more vulnerabilities when the LLM is fine-tuned; **(5)** when paired with irrelevant images during multimodal inference, the performance on text-oriented tasks becomes unstable with either non-directional fluctuations or directional tendencies; **(6)** relevant visual contexts can more directly influence the model performance, sometimes beneficial for completing tasks in truthfulness, but more often leading to unexpected behaviors and exacerbating their internal risks.

In light of these risks, it becomes necessary and urgent to find ways to mitigate these threats and facilitate the trustworthiness of MLLMs. With careful model selection and thorough analysis, we identify that: **(7)** improved architectures (e.g., novel vision encoder , aligned base LLM ) and training paradigms (e.g., SFT with data from GPT-4V , RLHF ) can positively influence the trustworthiness of models to some extents; **(8)** these advancements are not sufficient to bring all-round improvements, e.g., the robustness to transfer attack from unique visual encoders can be nullified when they are included into surrogate models, SFT and RLHF heavily rely on the quality and representativeness of the training data distribution, which could introduce bias on certain aspects. Therefore, to achieve truly trustworthy MLLMs, it is imperative to develop more effective approaches. Here, we offer our views on the potential directions for solution.

* _Drawing lessons from LLMs._ In the realm of LLMs, many methods have been proposed to achieve better performance and alignment, including but not limited to retrieval augmented generation (RAG) , RLAIF with constitutions (Constitutional AI) , and weak-to-strong generalization . These techniques can be extended to MLLMs with the same basic logic of functioning.
* _Expanding the focus in multimodal training._ Previous work mostly emphasizes to enhance the multimodal capabilities via multimodal training . As pointed out in this work, more issues, like the stability in multimodal inference, the preservation of the inherent alignment in base LLMs, and better multimodality alignment, should be prioritized.
* _Evaluation and evolution in dynamic environments._ It is unrealistic to obtain a highly trustworthy model in one go, as threats in practice cannot be fully covered during training. With the concept of agents , we can consolidate MLLMs in dynamic environments  with more challenging assessment, e.g., interactions between agents , adaptive attacks .

**Social Impacts.** MultiTrust's scrutiny of MLLM trustworthiness unveils profound societal risks. Truthfulness issues like hallucination could propagate dangerous misinformation, notably in applications like healthcare . Vulnerabilities to toxic content and jailbreaking expose the public to hazards and illegal AI exploitation . MLLMs' susceptibilities to adversarial conditions threaten critical services like security monitoring. Embedded biases could perpetuate social inequalities in decision-making within employment and law enforcement . Data breaches and inadequate safeguards of privacy also violate personal rights and erode trust in AI . These insights necessitate a call for the judicious use of MLLMs to prevent their potential adverse impacts.

**Limitations.** (1) Absence of machine ethics: Unlike DecodingTrust  and TrustLLM , machine ethics is excluded from MultiTrust evaluations due to its cultural and subjective variability, which can be further appended into the framework. (2) Limited analysis of closed-source models: The restricted knowledge about the pretraining data, architectures, and training strategies of proprietary models limits our in-depth analysis to attribute their success and failure technically. (3) Malicious misuse of the dataset: Certain parts of our dataset can potentially be exploited maliciously, causing social impacts. We will clarify the risks and restrict the management for the released dataset.

## 5 Conclusion

In this work, we establish MultiTrust, the first comprehensive benchmark specifically designed to evaluate the trustworthiness of Multimodal Large Language Models (MLLMs). Through extensive experiments across 32 tasks under 10 detailed sub-aspects on 21 advanced MLLMs, we identify significant gaps and unexplored risks in their trustworthiness, including the susceptibilities in novel multimodal scenarios and instabilities caused by the cross-modal interaction. Our findings highlight the vulnerabilities and complexities posed by the multimodal nature of these models, emphasizing the necessity for more in-depth researches and sophisticated approaches to ensure their trustworthiness.