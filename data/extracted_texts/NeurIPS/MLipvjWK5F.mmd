# RETR: Multi-View Radar Detection Transformer

for Indoor Perception

 Ryoma Yataka\({}^{1,3,}\), Adriano Cardace\({}^{2,}\),, Pu (Perry) Wang\({}^{1,}\),

**Petros Boufounos\({}^{1}\), Ryuhei Takahashi\({}^{3}\)**

\({}^{1}\)Mitsubishi Electric Research Laboratories (MERL), USA

\({}^{2}\)Department of Computer Science and Engineering, University of Bologna, Italy

\({}^{3}\)Information Technology R&D Center (ITC), Mitsubishi Electric Corporation, Japan

Equal contribution.The work was done as a visiting scientist from ITC in Mitsubishi Electric Corporation.

###### Abstract

Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar detection TRansformer (**RETR**), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of \(15.38+\) AP for object detection and \(11.91+\) IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.

## 1 Introduction

Perception information encompasses the processes and technologies to detect, interpret, and understand their surroundings. Complementary to the mainstream camera and LiDAR sensors, radar can enhance the safety and resilience of perception under low light, adversarial weather (e.g., rain, snow, dust), and hazardous conditions (e.g., smoke, fire) at affordable device and maintenance cost. An emerging application of radar perception is indoor sensing and monitoring for elderly care, building energy management, and indoor navigation . A notable limitation of indoor radar perception is the low semantic features from radar signals.

Earlier efforts use radar detection points  to support simple classification tasks such as fall detection and activity recognition over a limited number of patterns. To support challenging perception tasks such as object detection, pose estimation, and segmentation, lower-level radar signal representation such as radar heatmaps is more preferred. Along this line, the earliest work is RF-Pose  using a convolution-based autoencoder network to fuse features from the two radar views and regress keypoints for 2D image-plane pose estimation. It is later extended to 3D humanpose estimation . It is noted that RF-Pose is not publicly accessible. More recently, RFMask  borrows the Faster R-CNN framework  by proposing candidate regions only in the horizontal radar heatmap via a region proposal network (RPN). A corresponding proposal in the vertical radar heatmap is automatically determined using a _fixed-height_ candidate region at the same depth as the horizontal proposal. The combined horizontal and vertical proposals are then projected into the image plane for bounding box (BBox) estimation. In addition, RFMask calculates the BBox loss only over the 2D horizontal radar view and disregards features from the vertical radar heatmap for BBox estimation.

In this paper, we exploit features from both horizontal and vertical radar views for object estimation and segmentation and introduce **Radar dEtetion TRansformer (RETR)** (Fig. 1). RETR extends the popular Detection Transformer (DETR) , which effectively eliminates the need for hand-crafted components such as non-maximum suppression and proposal/anchor generation, to the multi-view radar perception. More importantly, RETR incorporates carefully designed modifications to exploit the unique multi-view radar setting such as shared depth dimension and the transformation between the radar and camera coordinate systems. Our contributions are summarized below:

1. **Extending DETR for Multi-View Radar Perception:** 1) Encoder: we associate features from both radar views by applying self-attention over the pooled multi-view radar features, eliminating the need for a cumbersome association scheme. We introduce a top-\(K\) feature selection to allow only \(K\) features from each view to keep the complexity low. 2) Decoder: the DETR decoder provides a natural way to associate the same object query to corresponding features from the two radar views via cross-attention. As such, the object query is able to learn 3D spatial embedding of objects in the radar coordinate (see Fig. 1).
2. **Tunable Positional Encoding**: To enhance feature association across the two radar views, we further exploit the fact that the two radar views share the depth dimension and introduce a tunable positional encoding (TPE) as an inductive bias. TPE imposes constraints in the attention map to prioritize the relative importance of depth dimension and avoid exhaustive correlations between radar views.
3. **Tri-Plane Loss from Both 3D Radar Coordinate and 2D Image Plane**: we enforce the output queries of the DETR decoder to directly predict 3D BBoxes in the radar coordinate system and convert them into the 2D image plane. We introduce a tri-plane loss that combines the BBox loss in the 3D radar plane and that in the 2D image plane, to calculate the global set-prediction loss.
4. **Learnable Radar-to-Camera Coordinate Transformation**: We employ a calibrated radar-to-camera coordinate transformation via a calibration process and a learnable coordinate transformation via reparameterization by preserving the orthonormal (i.e., 3D special orthogonal group \((3)\)) structure of the rotation matrix.

We demonstrate the effectiveness of our contributions through evaluations on two open datasets: the HIBER dataset  and the MMVR dataset .

Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection.

## 2 Related Work

Radar-based Object Detection and Segmentation:Indoor radar perception tasks include object detection (BBoxes), pose estimation (keypoints), and instance segmentation (human masks) [1; 35; 43; 44; 20; 23; 45; 38; 46], and radar datasets in different data formats were reported in [35; 31; 30; 39; 2; 40; 14; 38; 26]. Particularly, radar heatmap-based approaches have gained attention not only in indoor perception [43; 44; 14; 38; 26] but also for automotive radar perception [19; 24; 32; 11; 5], due to richer semantic features compared to those extracted from sparse radar point clouds [31; 30; 39; 2; 15; 40; 41]. RF-Pose  predicts human poses on the image plane using a convolution autoencoder-based architecture. With the HIBER dataset , RFMask considers proposal-based object detection and instance segmentation. More recently, MMVR  has been openly released to accelerate advancements in indoor radar perception.

Image-based Object Detection and Segmentation with DETR:Since the introduction of DETR for 2D image-plane object detection, subsequent studies have been developed based on its framework [21; 47; 4; 17; 37; 18; 10; 25], largely due to DETR's ability to eliminate the need for hand-designed components such as non-maximum suppression (NMS). In , Conditional DETR decomposes the roles of content and positional embeddings in the transformer decoder, improving not only prediction accuracy but also training convergence speed. More recently,  has proposed Rank-DETR as a rank-oriented architectural design, guaranteeing lower false positives and false negatives in prediction.

## 3 Preliminary

Generation of Radar Heatmaps:Conceptually, let us consider a pair of (virtual) horizontal and vertical antenna arrays with \(N_{}\) elements for each array, sending a set of frequency modulated continuous waveform (FMCW) pulses for object detection [26; 38; 34]. The two 1D arrays generate one horizontal radar view in the azimuth-depth \((x-z)\) domain and one vertical radar view in the elevation-depth \((y-z)\) domain,

\[y_{}(t,x,z)=_{k=1}^{K_{p}}_{m=1}^{M}s_{k,m,t}e^{j2 (x,z)}{_{k}}}, y_{}(t,y,z )=_{k=1}^{K_{p}}_{m=1}^{M}s_{k,m,t}e^{j2(y,z )}{_{k}}},\] (1)

where \(s_{k,m,t}\) denotes the \(k\)-th sample of FMCW sweep on the \(m\)-th antenna at time \(t\), \(_{k}\) is the wavelength of the \(k\)-th sample, \(d_{m}(x,z)\) denotes the round-trip distance from the \(m\)-th array element to a position \((x,z)\), and \(K_{p}\) and \(M\) denote the number of samples and the number of array antennas, respectively. Usually, the azimuth \(x\) is in an interval of \(x=[x_{}: x:x_{}]\) and the elevation \(y\) and the depth \(z\) are similarly defined. At a particular time \(t\), we have the horizontal radar heatmap \(_{}(t)=\{|y_{}(t,x,z)|\}_{x }^{z}^{W D}\) and the vertical radar heatmap \(_{}(t)=\{|y_{}(t,y,z)|\}_{y }^{z}^{H D}\) with a shared depth axis. The multi-view radar testbeds in HIBER  and MMVR  utilize advanced MIMO-FMCW radar systems. We defer the MIMO-FMCW radar heatmap generation to Appendix D.

Figure 2: Indoor radar perception pipeline: (a) multi-radar views are utilized to estimate 3D BBoxes in the radar coordinate system; (b) the 3D BBoxes are then transformed into the 3D camera coordinate system by a radar-to-camera transformation; and (c) the transformed 3D BBoxes are projected onto the image plane for final object detection. Blue line denotes a fixed-height regional proposal in RFMask, while **Magenta line** denotes an object query with learnble height in RETR.

Indoor Radar Perception:By taking \(T\) consecutive multi-view radar heatmaps (\(_{}^{T W D}\) and \(_{}^{T H D}\)) as the input, we are interested in detecting objects on the image plane:

\[_{}=_{}((f( _{},_{}))),\] (2)

where \(_{}\) denotes predicted BBoxes for object detection and pixel-level masks for instance segmentation. Using the BBox as an example in Fig. 2, our pipeline includes the following steps: 1) Fig. 2 (a): By taking the two radar views over \(T\) consecutive frames \((_{},_{})\) as input, the end-to-end object detection module \(f\) outputs a set of parameters describing 3D BBoxes in the radar coordinate system; 2) Fig. 2 (b): The radar-to-camera 3D coordinate transformation \(\) converts the predicted 3D BBoxes at the output of \(f\) to corresponding 3D BBoxes in the 3D camera coordinate system; 3) Fig. 2 (c): The 3D-to-2D projection \(_{}\) projects the 3D BBox in the camera coordinate system into corresponding 2D image plane normally with a known pinhole camera model.

## 4 RETR: Radar Detection Transformer

We first present the RETR architecture and then highlight radar-oriented modifications. We defer the discussion on Segmentation to Appendix B.

### RETR Architecture

We present the RETR architecture in Fig. 3, introducing its major modules in a left-to-right order. Refer to Appendix A for the detailed architecture.

Backbone:Given \(_{}^{T W D}\) and \(_{}^{T H D}\), a shared backbone network (e.g., ResNet ) generates separate horizontal-view and vertical-view radar feature maps: \(_{}=(_{}) ^{C}\) and \(_{}=(_{}) ^{C}\), where \(C\) and \(s\) represent the number of channels and downsampling ratio over the spatial dimension, respectively.

Feature Selection:A transformer-based encoder expects a sequence of features as input. This is done by mapping the feature maps into a sequence of \(P\)_multi-view radar features_\(=\{_{},_{}\} ^{C P}\): \(_{}_{}^{C P_{ }}\) and \(_{}_{}^{C P_{ }}\), where \(P=P_{}+P_{}\). We defer the discussion of top-\(K\) feature selection to Section 4.2.

Encoder as Cross-View Radar Feature Association:The transformer encoder provides a simple yet effective method for associating radar features from both horizontal and vertical views by applying self-attention over the pool of \(P\) multi-view radar features \(=\{_{},_{}\}^{C  P}\), eliminating the need for cumbersome association schemes. Specifically, the \(l\)-th (\(l=0,,L_{}-1\)) encoder layer updates the multi-view radar features through multi-head self-attention \(_{}\):

\[^{l+1}=}^{l}+(}^{l}), }^{l}=^{l}+_{}( (^{l}),(^{l}),( ^{l})),\] (3)

Figure 3: The RETR architecture: 1) **Encoder**: Top-\(K\) features selection and tunable positional encoding to assist feature association across the two radar views; 2) **Decoder**: TPE is also used to assist the association between object queries and multi-view radar features; 3) **3D BBox Head**: Object queries are enforced to estimate 3D objects in the radar coordinate and projected to \(3\) planes for supervision via a coordinate transformation; 4) **Segmentation Head**: The same queries are used to predict binary pixels within each predicted BBox in the image plane.

where \(\) denotes feed-forward networks, \(L_{}\) is the number of encoder layers, and \(\), \(\) and \(\) are projections to derive the multi-head query, key and value embedding from \(\), respectively. For the first (0-th) layer, we have \(^{0}=\). Note that we omit the description of "Layer norm" and "multi-head index" in Eq. 3 for clarity.

Additionally, since the multi-view radar features lack positional information and the self-attention is permutation-invariant, we supplement \(^{l}\) with positional embedding added (or attached) to the input of each encoder layer. Refer to Section 4.3 for a tunable positional encoding.

Decoder to Associate Object Queries with Multi-View Radar Features:The decoder provides a natural way to associate the same object query with features from the two radar views via cross-attention. For each decoder layer, it takes \(N\) object queries \(^{l}=\{_{1},,_{N}\}^{C N}\) as its input, and consists of a self-attention layer, a cross-attention layer and a FFN. Specifically for the \(l\)-th (\(l=0,1,,L_{}-1\)) decoder layer, it first updates all queries through multi-head self-attention:

\[}^{l}=^{l}+_{}( (^{l}),(^{l}),( ^{l})),\] (4)

where \(\), \(\) and \(\) are the projections with different parameterization from those in the self-attention layer (Eq. 3). Then, the decoder layer further updates the object queries \(}^{l}\) of Eq. 4 via multi-head cross-attention with the multi-view radar features \(^{L_{}}\) from the encoder output:

\[^{l+1}=}^{l}+(}^{l}), }^{l}=}^{l}+_{} ((}^{l}),(^{L_{ }}),(^{L_{}}) ),\] (5)

where both \(}^{l}\) and \(^{L_{}}\) are supplemented with positional embedding. Finally, the decoder outputs \(N\) enhanced object queries \(^{L_{}}\) for downstream tasks.

Mapping from 3D Radar Coordinate to 2D Image Plane:Given the \(N\) enhanced object queries \(^{L_{}}\), RETR directly estimates 3D BBoxes in the radar coordinate:

\[}=\{cx,cy,cz,w,h,d\}^{}=( ()),^{L_{}}\] (6)

where \(}\) describes the 3D BBox center and respective widths along the 3D axes, and \(\) normalizes the 3D BBox prediction to \(\). Then, as shown in Fig. 2 (b), we apply a radar-to-camera transformation \(\) to convert the predicted 3D BBoxes to ones in the 3D camera coordinate as

\[^{i}_{}=\{x^{i}_{},y^{i}_{},z^{i}_{}\}^{}=(^{i}_{})=^{i}_{}+, i=1,2,,8,\] (7)

where \(\) is a 3D rotation matrix, \(^{3}\) is the 3D translation vector, and \(^{i}_{}\) is \(i\)-th corner of the 3D BBox corresponding to \(}\). Subsequently in Fig. 2 (c), we project the 3D BBoxes \(^{i}_{}\) onto the 2D image plane via a 3D-to-2D projection. From the projected 2D corners, one can calculate the 2D BBox center and width and height in the image plane as

\[_{}=\{cx,cy,w,h\}^{}=_{ }(_{}).\] (8)

The final BBox estimation \(}_{}\) in the image plane is obtained by adding an offset head \(:^{10}^{4}\) to compensate for the spatial downsampling and normalizing it to the interval \(\):

\[}_{}=(_{}+ (_{}})).\] (9)

### Top-\(K\) Feature Selection

In DETR, the sequentialization simply collapses the spatial dimensions of the feature map into a single dimension, resulting in \(P_{}=WD/s^{2}\) and \(P_{}=HD/s^{2}\) features for the horizontal and vertical radar feature maps, respectively. As a result, we have \(P=(W+H)D/s^{2}\) multi-view radar features. It is known that the complexity of transformers grows quadratically with respect to the feature length \(P\). Here, we introduce a customized Top-\(K\) feature selection, maintaining a low complexity for the RETR encoder and decoder: \(_{}=(_{}) ^{C K}\) and \(_{}=(_{}) ^{C K}\), where \(K\{WD/s^{2},HD/s^{2}\}\). In this case, we shrink the multi-view radar tokens from \(P=(W+H)D/s^{2}\) to \(P=2K\). For each radar frame, we consistently select the Top-\(K\) strongest features, which may originate from varying spatial locations depending on the specific radar frame. Consequently, the gradient propagates back through the selected \(K\) features to the backbone weights, irrelevant to their spatial locations.

### TPE: Tunable Positional Encoding

The TPE is built on the top of the concatenation operation between the content embedding \(\) (either feature embedding \(\) at the encoder or decoder embedding \(\) at the decoder) and positional embedding \(\) in the conditional DETR  (see Fig. 4 (b)):

\[(_{}_{})^{}(_{ }_{})=_{}^{}_{ }+_{}^{}_{},\] (10)

where \(\) denotes concatenation, rather than the sum in DETR  (see Fig. 4 (a)):

\[(_{}+_{})^{}(_{ {key}}+_{})=_{}^{}_{ }+_{}^{}_{}+_{}^{}_{}+_{}^{}_{}.\] (11)

It is seen that Eq. 10 eliminates the cross terms between the content and positional embeddings in Eq. 11 and, allowing content/positional embeddings focus on their respective attention weights, contributes to faster training convergence .

In our case, the positional embedding is composed of a depth (\(y\)) axis and an angular (either azimuth \(x\) or elevation \(z\)) axis. As such, \(=\) with \(\) representing the depth positional embedding and \(\) the angular positional embedding. Then expanding Eq. 10 with \(=\) leads to

\[(_{}_{}_{} )^{}(_{}_{}_ {})=_{}^{}_{}+_{ }^{}_{}+_{}^{}_{ {key}}.\] (12)

In Eq. 12, we have the following observations:

1. \(_{}^{}_{}\) reflects how similar the features in the key and query may appear;
2. Depth similarity \(_{}^{}_{}\) remains consistent regardless of whether the key and query originate from the same radar view or different radar views;
3. Angular similarity \(_{}^{}_{}\) can be a self-angular similarity (azimuth-to-azimuth or elevation-to-elevation) when the key and query are from the same radar view, or a cross-angular similarity (azimuth-to-elevation or elevation-to-azimuth) for different radar views.

Motivated by the above observations, we can promote higher similarity scores for keys and queries with similar depth embeddings than those far apart in depth, especially for the ones from different views, by allowing for adjustable dimensions between depth and angular embeddings:

\[d_{}= d_{}, d_{}=(1-)d_{} d_{}+d_{}=d_{},\] (13)

where the tunable dimension ratio \(\) is in the interval \(\). As illustrated in Fig. 4 (c), when \(=0.5\), the positional embedding is equivalent to that used in conditional DETR. When \(\) approaches 0, the depth positional embedding is minimized, making the depth similarity \(_{}^{}_{}\) negligible in Eq. 12. Conversely, as \(\) approaches \(1\), the depth positional embedding dimension increases, and so does the importance of the depth similarity in Eq. 12.

We implement our TPE with a fixed sine/cosine positional encoding along the depth and angular (azimuth or elevation) dimension. For an even depth/angular positional dimension, we have

\[_{2i} =(_{}/^{2i/d_{}}),_{2i+1}=(_{}/^{2i/d_{}}), i=0,1, ,d_{}/2-1,\] (14) \[_{2i} =(_{}/^{2i/d_{}}),_{2i+1}=(_{}/^{2i/d_{}}), i=0,1, ,d_{}/2-1,\] (15)

Figure 4: Schemes of positional encoding: (a) the sum operation in the original DETR; (b) the concatenation in Conditional DETR; and (c) TPE in RETR that allows for adjustable dimensions between depth and angular embeddings and promotes higher similarity scores for keys and queries with similar depth embeddings than those far apart in depth.

where \(_{/}\) and \(d_{/}\) are the position index and dimension for the depth and angular axes, respectively, \(i\) is the (even/odd) element index, and \(\) = 10000 is a temperature. By adjusting the ratio \(\) in Eq. 12, we change the dimensions of the depth \(\) in Eq. 14 and the angular \(\) in Eq. 15, while keeping the total positional dimension of \(=\) constant. We show the visualization of TPE in Appendix C.

### Tri-Plane Set-Prediction Loss

DETR calculates a matching cost matrix with each element constructed from 1) a classification cost \(_{}\) and 2) a BBox loss between one of \(N\) predictions \(}\) and one of ground truth BBoxes \(\) (including the "no object" class). The BBox loss is a weighted combination of the generalized intersection over union (GIoU) loss \(_{}\) and the \(_{1}\) loss \(_{_{}}\):

\[_{}(,})=_{} _{}(,})+_{_{ }}_{_{}}(,}),\] (16)

where \(_{*}\) denotes the weight. Over the permutation set \(_{N}\) between \(N\) predictions and ground truth objects, the Hungarian algorithm  is applied with the matching cost matrix to find the optimal assignment \(^{*}_{N}\) of predictions to ground truth. Given \(^{*}\), the loss is computed only for the matched pairs and is referred to as the set-prediction loss.

Since RETR predicts 3D BBoxes \(}\) in the 3D radar coordinate and maps them into the 2D image plane, we propose to enhance the above Hungarian match cost matrix using a _Tri-Plane BBox Loss_ from both the radar coordinate and image plane. This is illustrated in Fig. 5, where a 3D BBox \(}\) in the radar coordinate is projected onto 1) the 2D horizontal radar plane as \(}_{}=_{}(})\) (the top branch); 2) the 2D vertical radar plane as \(}_{}=_{}(})\) (the middle branch); and 3) the 2D image plane as \(}_{}\) of Eq. 9 (the bottom branch). The tri-plane BBox loss \(_{}^{}\) sums up 2D BBox losses over all three planes using Eq. 16:

\[_{}^{}=_{}( {b}_{},}_{})+_{}(_{},}_{})+_{ }(_{},}_{} ).\] (17)

RETR finds the optimal assignment \(^{*}_{}\) using the matching cost with 1) the original classification cost \(_{}\) and 2) the tri-plane BBox loss \(_{}^{}\). The resulting set-prediction loss using \(^{*}_{}\) is referred to as the tri-plane set-prediction loss.

### Learnable Radar-to-Camera Coordinate Transformation

The rotation matrix \(\) and translation vector \(\) in the radar-to-camera transformation of Eq. 7 can be calibrated in advance. However, this calibration process may be accurate only for a limited interval of depth and angles. Instead of relying on the calibrated transformation, we introduce a learnable transformation via a reparameterization on \(\) while keeping it orthonormal. To this end, we need to ensure that the learnable \(}\) resides in the 3D special orthogonal group \((3)\). Considering that \((3)\) is a special case of a Lie group, one of the differentiable manifolds, we can firstly map a 3D vector \(=\{_{x},_{y},_{z}\}^{} ^{3}\) to Lie algebra \((3)\) using the projection \([]:^{3}(3)\). And then we apply the exponential map \(:(3)(3)\) that maps \([]\) into the nearest point in \((3)\) such that the resulting \(([])\) resides on \((3)\) and satisfies the orthonormal structure . This leads to the following reparameterization of \(}\) in terms of \(\):

\[}([])=+ []+}[ ]^{2},[]=[0&-_{z}& _{y}\\ _{z}&0&-_{x}\\ -_{y}&_{x}&0],\] (18)

where \(=\|\|\) is the \(_{2}\) norm, With the above reparameterization, the learnable radar-to-camera coordinate transformation in Eq. 7 reduces to learn the vector \(\) and the translation vector \(\).

Figure 5: Tri-Plane BBox loss.

## 5 Experiments

### Setup

Datasets:We evaluate performance over two open indoor radar perception datasets: MMVR\({}^{4}\) and HIBER5. MMVR includes multi-view radar heatmaps collected from over \(20\) human subjects across \(6\) rooms over a span of \(9\) days. In our implementation, we utilize data from **Protocol 2** (P2) which includes \(237.9\)K data frames capturing both single and multiple human subjects in diverse activities such as walking, sitting, stretching, and writing on the board. For the training-validation-test split, we follow the data split **S1** as defined in MMVR.

HIBER, partially released, includes multi-view radar heatmaps from \(10\) human subjects in a single room but from different angles with two data splits: 1) "WALK", consisting of \(73.5K\) data frames with one subject (Section 5.2); and 2) "MULTI", consisting of \(70.8K\) radar frames with multiple (2) human subjects walking in the room (Appendix G). More dataset details can be found in Appendix E.

Implementation:We consider RFMask  and DETR  as baseline methods. Since RFMask and DETR originally compute the BBox loss only in the 2D horizontal (H) radar plane and the 2D image (I) plane, respectively, we enhance both methods with a unified bi-plane BBox loss (H + I). We also introduce a DETR variant with top-\(K\) feature selection, allowing it to take features from both horizontal (H) and vertical (V) heatmaps as input. For RETR, we set \(K=256\) for the top-\(K\) selection, the positional embedding dimension to \(d_{}=256\), and a tunable dimension ratio at \(=0.6\). We include one variant that only employs the TPE at the decoder (TPE@Dec.). More hyper-parameter settings can be found in Appendix E.

Metrics:For object detection, we adopt average precision (AP) at two IoU thresholds of \(0.5\) (AP\({}_{}\)) and \(0.75\) (AP\({}_{}\)) and its mean (AP) over thresholds \([0.5:0.05:0.95]\). We also consider average recall (AR) when it is restricted to making only one detection (AR\({}_{}\)) or up to \(10\) detections (AR\({}_{}\)) per image. For segmentation, we report the average IoU value between the predictive and ground truth masks. Detailed metric definitions can be found in Appendix F.

### Main Results

MMVR:Table 1 shows the main results on the MMVR dataset under "P2S1". Compared with RFMask, DETR with a single horizontal radar view does not show performance improvement. By

   Model & Dim & Input & BBox Loss & AP & AP\({}_{}\) & AP\({}_{}\) & AR\({}_{}\) & AR\({}_{}\) \\  RFMask & 2D & H, V & H + I & 31.37 & 61.50 & 27.48 & 33.23 & 38.41 \\ DETR & 2D & H & H + I & 29.38 & 62.31 & 25.35 & 31.32 & 43.06 \\ DETR (Top-\(K\)) & 2D & H, V & H + I & 39.71 & 82.74 & 33.29 & 38.98 & 52.81 \\  RETR (TPE@Dec.) & 3D & H, V & H + V + I & 45.94 & 81.99 & 44.04 & 42.03 & 57.38 \\ RETR & 3D & H, V & H + V + I & 46.75 & 83.80 & 46.06 & 42.19 & 57.39 \\   

Table 1: Main results of object detection in the image plane under “P2S1” of MMVR. The top section shows results from conventional models, while the bottom section presents RETR results.

   Model & Dim & Input & BBox Loss & AP & AP\({}_{}\) & AP\({}_{}\) & AR\({}_{}\) & AR\({}_{}\) \\  RFMask & 2D & H, V & H + I & 17.77 & 52.46 & 6.78 & 32.71 & 32.71 \\ DETR & 2D & H & H + I & 14.45 & 47.33 & 4.25 & 28.64 & 28.64 \\ DETR (Top-\(K\)) & 2D & H, V & H + I & 14.35 & 48.94 & 5.50 & 28.78 & 28.78 \\  RETR (TPE@Dec.) & 3D & H, V & H + V + I & 20.18 & 52.53 & 7.32 & 32.91 & 32.91 \\ RETR & 3D & H, V & H + V + I & 22.09 & 59.83 & 10.99 & 35.16 & 35.16 \\   

Table 2: Main results of object detection in the image plane under “WALK” of HIBER. The notation follows the same format as Table 1.

just adding the vertical radar view at the input, DETR with top-\(K\) selection exhibits a noticeable performance improvement over RFMask. Built upon DETR (Top-K), RETR (TPE@Dec.) implements two enhancements: 1) TPE at the decoder and 2) tri-plane BBox loss, resulting in further improvements with a gain of \(6.23\) in AP, \(10.75\) in AP\({}_{75}\), and \(4.57\) in AR\({}_{10}\), highlighting the importance of TPE and supervision at the vertical radar view. By further incorporating TPE at the encoder, the full version of RETR achieves an impressive performance improvement over RFMask, demonstrating increases of \(15.38\) in AP, \(22.30\) in AP\({}_{50}\), and \(18.58\) in AP\({}_{75}\), respectively. The results under "P2S2" on MMVR can be seen in Appendix G.

HIBER:Table 2 presents the main results on the HIBER dataset under "WALK". Similar to Table 1, we observe a similar trend of performance improvement from DETR to RETR variants. Numerically, we see increases of \(4.32\) in AP, \(7.37\) in AP\({}_{50}\), and \(4.21\) in AP\({}_{75}\), when directly comparing RETR to RFMask. These performance improvements are smaller compared with those in Table 1. This is potentially because the HIBER data under "WALK" predominantly involves walking, where RFMask's fixed-height vertical proposals may work fine. In contrast, MMVR under "P2" includes more diverse activities such as sitting, leading to likely overestimated vertical proposals for RFMask and thus greater improvements in MMVR than HIBER. The results under "MULTI" on HIBER can be seen in Appendix G.

Visualization of Cross-Attention Map:Fig. 6 presents the cross-attention map at the last decoder layer between predicted BBoxes (via object queries) and multi-view radar features. RETR accurately predicts the subject in the background of the image plane (middle panel) with a forward-bending posture (Query 1). The cross-attention maps of Query 1, with respect to horizontal (left) and vertical (right) radar features, highlight areas with features contributing the most to Query 1. These contributing areas in the vertical plane are more stretched along the depth axis compared with those in the horizontal plane. Notably, the contributing areas from the two views share similar depth intervals. For Query 2 which identifies the subject in the foreground, the cross-attention maps shift its focus to contributing areas at closer depth compared with those for Query 1, indicating an effective 3D spatial embedding of object queries at the RETR output. We provide more visualizations in Appendix H.

Limitation:We present failure cases in Fig. 15 of Appendix H. Predicting arm positions remains challenging, suggesting that RETR may not focus its attention on regions with weak radar reflections. Moreover, multi-path reflections from the ground, ceiling, and other strong scatterers (e.g., metal) can cause (first-order or second-order) ghost targets and elevate the noise floor. Traditional signal processing techniques can mitigate these effects but require access to raw radar data. Alternatively,

Figure 6: Visualization of cross-attention map between predicted BBoxes and multi-view radar features. BBoxes with the same color correspond to the same subject.

Table 3: **Ablation studies** under “P2S1” on MMVR.

ghost targets can be labeled in the multi-view radar heatmaps, though this can be time-consuming and costly. One can then extend RETR to classify output queries to one of \(\{,person,ghost\}\), alongside regressing queries to the BBox parameters.

### Ablation Studies

We report ablation studies with RETR under "P2S1" on MMVR. Further results of ablation studies can be seen in Appendix G.

Tunable Dimension Ratio \(\):Table (a)a presents the ablation study of the tunable dimension ratio \(\) and its impact on the object detection performance in terms of \(_{}\) ( primary vertical axis) and \(_{}\) (secondary vertical axis). The results indicate that \(=0.6\) yields the best performance. The detection performance gradually decreases as \(\) approaches to \(0\) and \(1\).

Learnable Transformation (Lt):To evaluate the effectiveness of the Learnable Transformation in Section 4.5, we compare \(\) and \(_{}\) metrics of RETRs with and without LT. The results in Table (b)b indicate that it is possible to incorporate the radar-to-camera geometry into the end-to-end radar perception pipeline without the need for a cumbersome calibration step, while still achieving comparable perception performance.

Tri-Plane Loss for RETR:Table (c)c compares RETR with a bi-plane BBox loss (horizontal radar plane and image plane) to that with the tri-plane loss (including the vertical radar plane). The results highlight the necessity of accounting for the vertical BBox loss and the importance of leveraging features from the vertical radar heatmap, leading to a performance improvement of \(4.47\) in \(\).

## 6 Conclusion

In this paper, we introduced RETR, extending DETR to the multi-view radar perception with carefully designed modifications such as depth-prioritized feature similarity via TPE, a tri-plane loss from radar and camera coordinates, and a learnable radar-to-camera transformation. Experimental results over two radar datasets and comprehensive ablation studies demonstrate that RETR significantly outperforms both RFMask and DETR baseline methods.

Broader Impacts:Indoor radar perception technologies, including RETR, offer a wide range of social applications in navigating and monitoring subjects such as the elderly, infants, robots, and humanoids, enhancing safety and energy efficiency while preserving privacy. However, it is crucial that perception results remain secure and private to prevent misuse in inferring subject attributes such as gender, size, and height. These technologies could potentially be used to advance indoor surveillance without individuals' acknowledgment or consent.