# Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation

Wooseong Cho

Seoul National University

Seoul, South Korea

wooseong_cho@snu.ac.kr

&Taehyun Hwang

Seoul National University

Seoul, South Korea

th.hwang@snu.ac.kr

&Joongkyu Lee

Seoul National University

Seoul, South Korea

jklee0717@snu.ac.kr

&Min-hwan Oh

Seoul National University

Seoul, South Korea

minoh@snu.ac.kr

Equal contributionCorresponding author

###### Abstract

We study reinforcement learning with _multinomial logistic_ (MNL) function approximation where the underlying transition probability kernel of the _Markov decision processes_ (MDPs) is parametrized by an unknown transition core with features of state and action. For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees. For our first algorithm, RRL-MNL, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency. We establish that RRL-MNL achieves a \(}(^{-1}d^{}H^{})\) frequentist regret bound with constant-time computational cost per episode. Here, \(d\) is the dimension of the transition core, \(H\) is the horizon length, \(T\) is the total number of steps, and \(\) is a problem-dependent constant. Despite the simplicity and practicality of RRL-MNL, its regret bound scales with \(^{-1}\), which is potentially large in the worst case. To improve the dependence on \(^{-1}\), we propose ORRL-MNL, which estimates the value function using the local gradient information of the MNL transition model. We show that its frequentist regret bound is \(}(d^{}H^{}+^{-1}d^{ 2}H^{2})\). To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve statistical guarantees with constant-time computational cost per episode. Numerical experiments demonstrate the superior performance of the proposed algorithms.

## 1 Introduction

_Reinforcement learning_ (RL) is a sequential decision-making problem in which an agent tries to maximize its expected cumulative reward by interacting with an unknown environment over time. Despite significant empirical progress in RL algorithms for various applications , the theoretical understanding of RL algorithms had long been limited to tabular methods , which explicitly enumerate the entire state and action spaces and learn the value (or the policy) for each state and action. Recently, there has been an increasing body of research in RL with function approximation to extend beyond the tabular problem setting. In particular, _linear function approximation_ has served as a foundational model . On the other hand,the linear transition model assumption poses significant constraints: 1) the output of the function must be within \(\), and 2) the sum of the probabilities for all possible next states must be exactly 1. These constraints make it challenging to apply RL with linear function approximation to real-world applications . To overcome such challenges, there has been literature on RL with general function approximation . Despite the guarantee of sample efficiency achieved by their algorithms, this accomplishment might be impeded by computational intractability or the necessity to rely on stronger assumptions. As a result, the resulting methods may not be as general or practical.

On the other hand, Hwang and Oh  introduce specific non-linear parametric MDPs called MNL-MDPs (Assumption 1) where the transition probability of MDPs is given by an MNL model. They consider an _upper confidence bound_ (UCB) approach to balance exploration and exploitation. Since it is costly or even intractable to compute UCB explicitly, randomized exploration methods such as _Thompson Sampling_ (TS) are widely studied in RL with linear function approximation as well as tabular MDPs. This is because, in various decision-making problems ranging from multi-armed bandits to RL, randomized exploration algorithms have been shown to perform better than UCB methods in empirical evaluations . Furthermore, randomized exploration can be easily integrated with linear function approximation. This is because the value function in linear MDPs can be linearly parameterized, allowing perturbations of the estimator to directly control the perturbations of the value function. However, although there has been some literature aiming to propose randomized algorithms for general function classes , these methods do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior , or they require stronger assumptions on stochastic optimism , which is one of the most challenging elements in frequentist regret analysis. Thus, the design of a tractable randomized exploration RL algorithm and the feasibility of frequentist regret analysis for randomized exploration remain open challenges. Hence, the following question arises:

_Can we design a provably efficient and tractable randomized algorithm for RL with MNL function approximation?_

We answer the above question by proposing the first randomized algorithm, RRL-MNL, achieving \(}(^{-1}d^{}H^{})\) frequentist regret with constant-time computational cost per episode. RRL-MNL is not only the first algorithm with randomized exploration for MNL-MDPs, but also, to the best of our knowledge, it provides the first frequentist regret analysis for a _non-linear model-based_ algorithm with randomized exploration without assuming stochastic optimism .

While RRL-MNL is _statistically_ efficient, the current method used to analyze the regret of MNL function approximation introduces a problem-dependent constant \(\) (Assumption 4), which reflects the level of non-linearity of the MNL transition model. This constant \(\) originates from the use of generalized linear models (GLMs) for contextual bandit settings  and MNL bandit settings . The magnitude of the constant \(\) can be exponentially small with respect to the size of the decision set, hence the regret bound scaling with \(^{-1}\) could be prohibitively large in the worst case . Worse yet, the situation is even more challenging in RL, as in the worst case, \(^{-1}\) can be much larger than in the case of bandits. To overcome the prohibitive dependence on \(\), algorithms based on new Bernstein-like inequalities and the self-concordant-like property of the log-loss have been proposed for logistic bandits  and for MNL bandits . As an extension of these works, the following fundamental question remains open:

_Is it possible for RL algorithms with MNL function approximation to have a sharper dependence on the problem-dependent constant \(\)?_

For the above question, we propose the second randomized algorithm referred to as ORRL-MNL, which establishes a regret bound of \(}(d^{}H^{}+^{-1}d^ {2}H^{2})\) with constant-time computational cost per episode. We summarize our main contributions as follows:

* We propose computationally tractable randomized algorithms for RL with MNL function approximation: RRL-MNL and ORRL-MNL. To the best of our knowledge, these are the first randomized model-based RL algorithms with MNL function approximation that achieve the frequentist regret bounds with constant-time computational cost per episode.
* We establish that RRL-MNL enjoys \(}(^{-1}d^{}H^{})\) frequentist regret bound with constant-time computational cost per episode, where \(d\) is the dimension of the transition core, \(H\) is horizon length, \(T\) is the total number of rounds, and \(\) is a problem-dependent constant. Wederive the stochastic optimism of RRL-MNL, and to our knowledge, this is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism.
* To achieve a regret bound with improved dependence on \(\), we introduce \(\)-MNL, which constructs the optimistic randomized value functions by taking into account the effects of the local gradient information for the MNL transition model at each reachable state. We prove that \(\)-MNL enjoys an \(}(d^{}H^{}+^{-1}d^ {2}H^{2})\) regret with constant-time computational cost per episode, significantly improving the regret of RRL-MNL without requiring prior knowledge of \(\).
* We evaluate our algorithms on tabular MDPs and demonstrate the superior performance of our proposed algorithms compared to the existing state-of-the-art MNL-MDP algorithm . The experiments provide evidence that our proposed algorithms are both computationally and statistically efficient.

Related works on RL with function approximation and MNL contextual bandits are provided in Appendix A.

## 2 Problem Setting

We consider the episodic _Markov decision processes_ (MDPs) denoted by \((,,H,\{P\}_{h=1}^{H},r)\), where \(\) is the state space, \(\) is the action space, \(H\) is the horizon length of each episode, \(\{P\}_{h=1}^{H}\) is the collection of probability distributions, and \(r\) is the reward function. Every episodes start from the initial state \(s_{1}\) and for every step \(h[H]:=\{1,...,H\}\) in an episode, the learning agent interacts with the environment represented as \(\). The agent observes the state \(s_{h}\), chooses an action \(a_{h}\), receives a reward \(r(s_{h},a_{h})\) and the next state \(s_{h+1}\) is given by the transition probability distribution \(P_{h}(|s_{h},a_{h})\). Then this process is repeated throughout the episode. A policy \(:[H]\) is a function that determines the action of the agent at state \(s_{h}\), i.e., \(a_{h}=(s_{h},h):=_{h}(s_{h})\).

We define the value function of the policy \(\), denoted by \(V_{h}^{}(s)\), as the expected sum of rewards under the policy \(\) until the end of the episode starting from \(s_{h}=s\), i.e., \(V_{h}^{}(s)=_{}[_{h^{}=h}^{H}r(s_{h^{}}, _{h^{}}(s_{h^{}})) s_{h}=s]\). Similarly, we define the action-value function \(Q_{h}^{}(s,a)=r(s,a)+_{s^{} P_{h}(|s,a)}[V_{h +1}^{}(s^{})]\). We define an optimal policy \(^{*}\) to be a policy that achieves the highest possible value at every \((s,h)[H]\). We denote the optimal value function by \(V_{h}^{*}(s)=V_{h}^{*^{*}}(s)\) and the optimal action-value function by \(Q_{h}^{*}(s,a)=Q_{h}^{^{*}}(s,a)\). To simplify, we introduce the notation \(P_{h}V_{h+1}(s,a)=_{s^{} P_{h}(|s,a)}[V_{h+1}(s^{ })]\). Recall that the Bellman equations are,

\[Q_{h}^{}(s,a)=r(s,a)+P_{h}V_{h+1}^{}(s,a)\,, Q_{h}^{*}(s,a)=r(s,a)+ P_{h}V_{h+1}^{*}(s,a)\,,\]

where \(V_{H+1}^{}(s)=V_{H+1}^{*}(s)=0\) and \(V_{h}^{*}(s)=_{a}Q_{h}^{*}(s,a)\) for all \(s\).

The goal of the agent is to maximize the sum of rewards for K episodes. In other words, the goal is to minimize the cumulative regret of the policy \(\) over K episodes where \(=\{^{k}\}_{k=1}^{K}\) is a collection of policies \(^{k}\) at k-th episode. The regret is defined as

\[_{}(K):=_{k=1}^{K}(V_{1}^{*}-V_{1}^{^{k}})(s_{1}^{k})\]

where \(s_{1}^{k}\) is the initial state at the \(k\)-th episode.

### Multinomial Logistic Markov Decision Processes (MNL-MDPs)

Even though a lot of provable RL algorithms for linear MDPs are proposed, there is a simple but fundamental problem with the linear transition model assumption on the linear MDPs. In other words, the output of a linear function approximating the transition model must be in \(\) and the probability of all possible following states must sum to \(1\) exactly. Such restrictive assumption can affect the regret performances of algorithm suggested under the linearity assumption. To resolve these challenges, Hwang and Oh  propose a setting of a _multinomial logistic Markov decision processes_ (MNL-MDPs), where the state transition model is given by a multinomial logistic model. We introduce the formal definition for MNL-MDP as follows:

**Assumption 1** (MNL-MDPs ).: _An MDP \((,,H,\{P_{h}\}_{h=1}^{H},r)\) is an MNL-MDP with a feature map \(:^{d}\), if for each \(h[H]\), there exists \(_{h}^{*}^{d}\), such that for any \((s,a)\) and \(s^{}_{s,a}:=\{s^{}:(s^{ } s,a) 0\}\), the state transition kernel of \(s^{}\) when an action \(a\) is taken at a state \(s\) is given by,_

\[P_{h}(s^{} s,a)=(s,a,s^{})^{}_{h}^{*})}{_{_{s,a}}((s,a,)^{}_{h}^{*})}\,.\] (1)

_We call each unknown vector \(_{h}^{*}\) transition core. Furthermore, we denote the maximum cardinality of the set of reachable states as \(\), i.e., \(:=_{s,a}|_{s,a}|\)._

**Remark 1**.: _While Hwang and Oh  assume a homogeneous transition kernel, we assume an inhomogeneous transition kernel, in which the probability varies depending on the current time step \(h\) even for the same state transition, which is a more general setting. Also, for notational simplicity, we denote the true transition kernel \(P_{h}\) as \(P_{_{h}^{*}}\), and the estimated transition kernel by \(\) as \(P_{}\)._

### Assumptions

We introduce some standard regularity assumptions.

**Assumption 2** (Boundedness).: _We assume \(\|(s,a,s^{})\|_{2} L_{}\) for all \((s,a,s^{})_{s,a}\), and \(\|_{h}^{*}\|_{2} L_{}\) for all \(h[H]\)._

**Assumption 3** (Known reward).: _We assume that the reward function \(r\) is known to the agent._

**Assumption 4** (Problem-dependent constant).: _Let \(_{d}(L_{}):=\{^{d}:\|\|_{2} L_{}\}\). There exists \(>0\) such that for any \((s,a)\) and \(s^{},_{s,a}\) with \(s^{}\),_

\[_{_{d}(L_{})}P_{}(s^{ } s,a)P_{}( s,a)\,.\]

Discussion of assumptionsAssumption 2 is common in the literature on RL with function approximation [43; 72; 73; 37; 35] to make the regret bounds scale-free. Assumption 3 is used to focus on the main challenge of model-based RL that learning about \(P\) of the environment is more difficult than learning \(r\). In the model-based RL literature [71; 9; 72; 81; 35], the known reward \(r\) assumption is widely used. Assumption 4 is typical in generalized linear contextual bandit [26; 51; 23; 3; 24] and MNL contextual bandit literature [54; 8; 55; 61; 6; 76; 50] to guarantee non-singular Fisher information matrix.

## 3 Randomized Algorithm for MNL-MDPs having constant-time computational cost

Previous work for MNL-MDPs  proposed a UCB-based exploration algorithm. Constructing a UCB-based optimistic value function is not only computationally intractable but also tends to overly optimistically estimate the true optimal value function. Additionally, their algorithm incurs increasing computation costs as episodes progress, as it requires all samples from the previous episode to estimate the transition core. In this section, we present a novel model-based RL algorithm that incorporates _randomized exploration_ and _online parameter estimation_ for MNL-MDPs.

### Algorithm: Rrl-Mnl

Online transition core estimationWhile Hwang and Oh  estimate the transition core using maximum likelihood estimation over all samples from previous episodes, we employ an efficient online parameter estimation method by exploiting the particular structure of the MNL transition model. The key insight is that the negative log-likelihood function for the MNL model in each episode is strongly convex over a bounded domain. This property allows us to utilize a variation of the online Newton step [30; 31], which inspired online algorithms for logistic bandits  and MNL contextual bandits . Specifically, for \((k,h)[K][H]\), we define the response variable \([y_{h}^{k}(s^{})]_{s^{}_{k,h}}\) such that \(y_{h}^{k}(s^{})=(s_{h+1}^{k}=s^{})\) for \(s^{}_{k,h}:=_{s_{h}^{k},a_{h}^{k}}\). Then, \(y_{h}^{k}\) is sampled from the following multinomial distribution: \(y_{h}^{k}(1,[P_{_{h}^{*}}(s^{ } s_{h}^{k},a_{h}^{k})]_{s^{}_{k,h}})\), where \(1\) represents that \(y_{h}^{k}\) is a single-trial sample. We define the per-episode loss \(_{k,h}()\) as follows:

\[_{k,h}():=-_{s^{}_{k,h}}y_{h}^ {k}(s^{}) P_{}(s^{} s_{h}^{k},a_{h}^{k} )\,.\]

Then, the estimated transition core for \(_{h}^{*}\) is given by

\[_{h}^{k}=*{argmin}_{ _{d}(L_{})}\|- _{h}^{k-1}\|^{2}_{_{k,h}}+(- _{h}^{k-1})^{}_{k-1,h}(_{ h}^{k-1})\,,\] (2)

where \(_{h}^{1}\) can be initialized as any point in \(_{d}(L_{})\) and \(_{k,h}\) is the Gram matrix defined by

\[_{k,h}:=_{d}+_{i=1}^{k-1}_ {s^{}_{i,h}}(s_{h}^{i},a_{h}^{i},s^{ })(s_{h}^{i},a_{h}^{i},s^{})^{}\,.\] (3)

Stochastically optimistic value functionFirst of all, we introduce the key challenges of regret analysis for randomized algorithms, explain how previous works have overcome these challenges, and then describe why the techniques from previous works cannot be applied to MNL-MDPs. Ensuring that the estimated value function is optimistic with sufficient frequency is a crucial challenge in analyzing the frequentist regret of randomized algorithms. A common way to promote sufficient exploration in randomized algorithms is by perturbing the estimated value function or by performing posterior sampling in the transition model class. Frequentist regret analysis of randomized exploration in an RL setting has been conducted for tabular , linear MDPs , and general function classes . In the case of linear MDPs , since the property that the action-value function is linear in the feature map allows perturbing the estimated parameter directly to control the perturbation of the estimated value function. Also, even though Ishfaq et al.  presented a randomized algorithm for the general function class using eluder dimension, they assume stochastic optimism (anti-concentration), which is in fact one of the most challenging aspects of frequentist analysis. Other posterior sampling algorithms in RL for the general function class such as , except for very limited examples, do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior. That is why even after there exists a so-called _general function class_-based result, it is often the case that results in specific parametric models are still needed.

Note that in episodic RL, the perturbed estimated value functions are propagated back through horizontal steps, requiring careful adjustment of the perturbation scheme to maintain a sufficient probability of optimism without decaying too quickly with the horizon. For example, if the probability of the estimated value function being optimistic at horizon \(h\) is denoted as \(p\), this would result in the probability that the estimated value function in the initial state is optimistic being on the order of \(p^{H}\), implying that the regret can increase exponentially with the length of the horizon \(H\)Additionally, the non-linearity and substitution effect of the next state transition in the MNL-MDPs make applying the existing TS techniques infeasible to guarantee optimism in MNL-MDPs with sufficient frequency. Instead, we design the _stochastically optimistic value function_ by exploiting the structure of the MNL transition model. In other words, the prediction error of MNL transition model (Definition 1) can be bounded by the weighted norm of the dominant feature \(}\) (Lemma 4). Based on such dominant feature, we perturb the estimated value function by injecting Gaussian noise whose variance is proportional to the inverse of the Gram matrix to encourage the perturbation with higher variance in less explored directions. To guarantee the optimism with fixed probability, we adapt optimistic sampling technique . For each \(m[M]\), sample _i.i.d._ Gaussian noise vector \(_{k,h}^{(m)}(_{d},_{k}^{2}_{ k,h}^{-1})\) where \(_{k}\) is an exploration parameter, and add the most optimistic inner product value \(_{m[M]}}_{k,h}(s,a)^{}_{k,h}^{(m)}\) to the estimated value function. To summarize for any \((s,a)\), set \(Q_{H+1}^{k}(s,a)=0\) and for \(h[H]\),

\[Q_{h}^{k}(s,a)=\{r(s,a)+_{s^{}_{s,a}}P_{_{h}^{k}}(s^{} s,a)V_{h+1}^{k}(s^{})+_{m[M]} }_{k,h}(s,a)^{}_{k,h}^{(m)},H\},\] (4)

where \(V_{h}^{k}(s)=_{a^{}}Q_{h}^{k}(s,a^{})\) and \(}_{k,h}(s,a):=(s,a,)\) for \(=*{argmax}_{s^{}_{s,a}}\| (s,a,s^{})\|_{_{k,h}^{-1}}\). Based on these stochastically optimistic value function, the agent plays a greedy action \(a_{h}^{k}=*{argmax}_{a^{}}Q_{h}^{k}(s_{h}^{k},a^{})\). We layout the procedure in Algorithm 1.

**Remark 2**.: _Note that \(\)-\(\) only requires constant-time computational cost and storage cost per episode, as it does not require storing all samples from previous episodes, and the Gram matrix \(_{k,h}\) can be updated incrementally._

### Regret bound of \(\)-\(\)

We present the regret upper bound of \(\)-\(\). The complete proof is deferred to Appendix C.

**Theorem 1** (Regret Bound of \(\)-\(\)).: _Suppose that Assumption 1- 4 hold. For any \(0<<\), if we set the input parameters in Algorithm 1 as \(=L_{}^{2},_{k}=}(H)\) and \(M= 1-\) where \(\) is the normal CDF, then with probability at least \(1-\), the cumulative regret of the \(\)-\(\) policy \(\) is upper-bounded as follows:_

\[_{}(K)=}(^{-1}d^{} H^{}),\]

_where \(T=KH\) is the total number of steps._

Discussion of Theorem 1To our best knowledge, this is the first result to provide a frequentist regret bound for the MNL-MDPs. Among the previous RL algorithms using function approximation, the most comparable techniques to our method are _model-free_ algorithms with randomized exploration . To guarantee stochastic optimism, Zanette et al.  established a lower bound on the difference between the estimated value and the optimal value by the summation of linear terms with respect to the average feature (Lemma F.1 in ). This property is achievable due to the linear expression of the value function in linear MDPs. Instead, we established a lower bound on the difference between value functions by the summation of the Bellman errors (Definition 1) along the sample path obtained through the optimal policy (Lemma 7). Hence, our analysis significantly differs from that of Zanette et al.  since the value function in MNL-MDPs is no longer linearly parametrized, and there is no closed-form expression for it.

Compared to , they also used an optimistic sampling technique; however, our theoretical sampling size \(M=( H)\) is much tighter than that of , i.e., \((d)\) for the linear function class, \(((T||||))\) for the general function class. While Ishfaq et al.  extend the results of the linear function class to general function class under the assumption of stochastic optimism (Assumption C in ), we provide the frequentist regret analysis for a _non-linear model-based_ algorithm with randomized exploration _without assuming stochastic optimism_.

Compared to the optimistic exploration algorithm for MNL-MDPs , our randomized exploration requires a more involved proof technique to ensure that the perturbation of the estimated value function has enough variance to maintain optimism with sufficient frequency (Lemma 6). As a result,the established regret of RRL-MNL differs by a factor of \(\), which aligns with the difference in the existing bounds of linear bandits between a TS-based algorithm  and a UCB-based algorithm . Additionally, we achieve statistical efficiency for the _inhomogeneous transition model_, which is a more general setting than that of Hwang and Oh . Our computation cost per episode is \((1)\) while the computation cost per episode of Hwang and Oh  is \((K)\).

Proof Sketch of Theorem 1We provide the proof sketch of Theorem 1. By decomposing the regret into the estimation part and the pessimism part, we have

\[_{k=1}^{K}(V_{1}^{*}-V_{1}^{_{k}})(s_{1}^{k})=_{k=1}^{K} ^{*}-V_{1}^{k}}_{}+^{k}-V_{1 }^{_{k}}}_{}(s_{1}^{k})\,.\]

We bound these two parts separately. For the estimation part, for each \(k[K],h[H]\), we first show that the online estimated transition core \(_{h}^{k}\) (2) concentrates around the unknown transition core parameter \(_{h}^{*}\) with high probability (Lemma 1). Then, we show that the prediction error induced by the estimated transition core can be bounded by the weighted norm of the dominant feature \(}\), multiplied by the confidence radius of the estimated transition core (Lemma 4). The bounded prediction error, together with the concentration of Gaussian noise, implies the desired bound on the estimation part (Lemma 10). For the pessimism part, we first show that the stochastically optimistic value function \(V_{1}^{k}\) is optimistic than the true optimal value function \(V_{1}^{*}\) with sufficient frequency (Lemma 6). In the next step, we show that the pessimism part is upper bounded by a bound of the estimation part times the inverse probability of being optimistic (Lemma 11). Combining all the results, we can conclude the proof. Refer to Appendix C for detailed proofs.

## 4 Statistically Improved Algorithm for MNL-MDPs

Although RRL-MNL is provably efficient and achieves constant-time computational cost per episode, the current analysis makes its regret bound scale with \(^{-1}\). Recall that the problem-dependent constant \(\) introduced in Assumption 4 indicates the curvature of the MNL function, i.e., how difficult it is to learn the true transition core parameter. It is required to ensure the non-singular Fisher information matrix, hence is typically used in GLM or MNL bandit algorithms that use the maximum likelihood estimator. As introduced in Faury et al. , \(^{-1}\) can be exponentially large in the worst case. The appearance of \(\) in existing bounds originates in the connection between the difference of estimators and the difference of gradients of negative log-likelihood, usually denoted as \(\) in Filippi et al. . Without considering local information at all, using a loose lower bound for \(\) incurs \(^{-1}\) in regret bound (see Section 4.1 in Agrawal et al. ). Recently, improved dependence on \(\) has been achieved in bandit literature [23; 3; 61; 6; 76; 50] through the use of generalization of the Bernstein-like tail inequality  and the self-concordant-like property of the log loss . However, a direct adaptation of the MNL bandit technique would result in sub-optimal dependence on the assortment size in MNL bandit, which corresponds to the size of the set of reachable states, such as \(\). In this section, we introduce a new randomized algorithm for MNL-MDPs, equipped with a tight online parameter estimation and feature centralization technique that achieves a regret bound with improved dependence on \(\) and \(\).

### Algorithms: Orrl-Mnl

Tight online transition core estimationZhang and Sugiyama  presented a jointly efficient UCB-based MNL contextual bandit algorithm using online mirror descent algorithm. Adapting the update rule from , the estimated transition core run by the online mirror descent is given by

\[}_{h}^{k+1}=*{argmin}_{ _{d}(L_{})}\|-}_{h}^{k}\|_{}_{h,h}}^{2}+^{} _{k,h}(}_{h}^{k})\,,\] (5)

where \(}_{h}^{1}\) can be initialized as any point in \(_{d}(L_{})\), \(\) is a step size, and \(}_{k,h}\) is defined as

\[}_{k,h}:=_{k,h}+^{2}_{k,h}( }_{h}^{k})\,,_{k,h}:=_{ d}+_{i=1}^{k-1}^{2}_{i,h}(}_{h}^{i+1})\,.\] (6)Note that the MNL model in Zhang and Sugiyama  operates in a _multiple-parameter_ setting, where there are multiple unknown choice parameters and one given context feature. In contrast, our MNL model operates in a _single-parameter_ setting, where there is one unknown transition core and features for up to \(\) reachable states. This difference results in variations in applying the self-concordant-like property of the log-loss for the MNL model. For instance, Zhang and Sugiyama  utilized the fact that the log-loss for the multiple parameter MNL model is \(\)-self-concordant-like (Lemma 2 in Zhang and Sugiyama ). On the other hand, Lee and Oh  revisit the self-concordant-like property and demonstrate that the log-loss of the single-parameter MNL model is \(3\)-self-concordant-like (Proposition B.1 in Lee and Oh ). This results in a concentration bound that is independent of \(\) and \(\), introduced in Lemma 12.

**Remark 3**.: _Note that the online estimated parameters \(_{h}^{k}\) (2) and \(}_{h}^{k}\) (5) do not aim to minimize the sum of negative log-likelihoods, \(_{k^{}=1}^{k}_{k^{},h}()\). Instead, we show that the online estimated parameter concentrates around the unknown transition core \(_{h}^{*}\) with high probability (Lemma 1 & 12). This online update approach allows us to estimate the transition core with constant-time computational cost per episode, as the agent does not need to store all samples from previous episodes._

Optimistic randomized value functionTo achieve improved dependence on \(\), a crucial point is to utilize the local gradient information of MNL transition probabilities for each reachable state when constructing the Gram matrix. In MNL bandit problems , this can be accomplished by substituting the Hessian of the negative log-likelihood with the Gram matrix using global gradient information \(\). However, there are fundamental differences between the settings in Perivier and Goyal , Zhang and Sugiyama  and ours. Perivier and Goyal  address the case where the reward for each product is _uniform_ (i.e., all products have a reward of 1), and the reward for not selecting a product from the given assortment (also known as the outside option) is 0. On the other hand, Zhang and Sugiyama  deal with _non-uniform_ rewards where the reward for each product may vary; however, the rewards for individual products are known a priori to the agent. In contrast, in MNL-MDPs, the value for each reachable state may vary (non-uniform) and is _not known_ beforehand. Due to these differences, the analysis techniques in MNL bandits  cannot be directly applied to our setting. Instead, we adapt the feature centralization technique . Then, the Hessian of the per-round loss \(_{k,h}()\) is expressed in terms of the centralized feature as follows:

\[^{2}_{k,h}()=_{s^{}_{k,h}}P_{ }(s^{} s^{k}_{h},a^{k}_{h})}(s^{k}_{h}, a^{k}_{h},s^{};)}(s^{k}_{h},a^{k}_{h},s^{ };)^{}\,.\]

where \(}(s,a,s^{};):=(s,a,s^{})- _{ P_{}( s,a)}[(s, a,)]\) is the centralized feature by \(\). For more details, please refer to Appendix D.2.

Now we introduce the _optimistic randomized value function_\(_{h}^{k}(,)\) for ORRL-MNL. The key point is that when perturbing the estimated value function, we use the centralized feature by the estimatedtransition parameter \(}_{h}^{k}\). For any \((s,a)\), set \(_{H+1}^{k}(s,a)=0\) and for each \(h[H]\),

\[_{h}^{k}(s,a):=\{r(s,a)+_{s^{} _{s,a}}P_{}_{h}^{k}}(s^{} s,a) _{h+1}^{k}(s^{})+_{k,h}^{}(s,a)\,,H\},\] (7)

where \(_{h}^{k}(s):=_{a}_{h}^{k}(s,a)\) and \(_{k,h}^{}(s,a)\) is the _randomized bonus term_ defined by

\[_{k,h}^{}(s,a):=_{s^{}_{s,a}}P_{ }_{h}^{k}}(s^{} s,a)}(s,a, s^{};}_{h}^{k})^{}_{k,h}^{s^{}}+3H _{k}^{2}_{s^{}_{s,a}}\|(s,a,s^{ })\|_{_{k,h}^{-1}}^{2}\,.\]

Here we sample _i.i.d._ Gaussian noise \(_{k,h}^{(m)}(_{},_{x}^{2} _{k,h}^{-1})\) for each \(m[M]\) and set \(_{k,h}^{s^{}}:=_{k,h}^{m(s^{})}\) where \(m(s^{}):=*{argmax}_{m[M]}}(s,a,s^{ };}_{h}^{k})^{}_{k,h}^{m}\) is the most optimistic sampling index for a reachable state \(s^{}\). Based on these optimistic randomized value function, at each episode the agent plays a greedy action with respect to \(_{h}^{k}\) as summarized in Algorithm 2.

**Remark 4**.: _Note that the second term in the randomized bonus always has a positive value, but it rapidly decreases as episode proceeds. While due to the randomness of \(\), the randomized bonus \(_{k,h}^{}\) itself cannot be guaranteed to always have a positive value. Consequently, the constructed value function \(_{h}^{k}(,)\) can be optimistic or pessimistic. However, as shown in Lemma 18, optimistic sampling technique ensures that the optimistic randomized value function \(_{h}^{k}\) has at least a constant probability of being optimistic than the true optimal value function._

**Remark 5**.: _As with_ RRL-MNL_, since the transition core is estimated in an online manner and the Gram matrices with local gradient information \(_{k,h}\) and \(}_{k,h}\) are updated incrementally,_ ORRL-MNL _also requires constant-time computational cost and storage cost per-episode. Although_ ORRL-MNL _requires an additional \(()\) computation cost for feature centralization, the computation complexity order is the same as that of_ RRL-MNL _because it also needs to go over reachable states to calculate the dominant feature \(}\), which also incurs a \(()\) computation cost. On the other hand,_ ORRL-MNL _does not require prior knowledge of \(\) and achieves a regret with a better dependence on \(\)._

### Regret Bound of **Orrl-Mnl**

We present the regret upper bound of **Orrl-MNL**. The complete proof is deferred to Appendix D.

**Theorem 2** (Regret Bound of **Orrl-Mnl**).: _Suppose that Assumption 1- 4 hold. For any \(0<<\), if we set the input parameters in Algorithm 2 as \(=(L_{}^{2}d),_{k}= ((kH)),_{k}=H_{k},\,M= 1-)}{(1)}\), and \(=()\), then with probability at least \(1-\), the cumulative regret of the_ ORRL-MNL _policy \(\) is upper-bounded as follows:_

\[_{}(K)=}(d^{3/2}H^{3/2}+ ^{-1}d^{2}H^{2})\,,\]

_where \(T=KH\) is the total number of time steps._

Discussion of Theorem 2Theorem 2 establishes that the leading term in the regret bound does not suffer from the problem-dependent constant \(^{-1}\) and the second term of the regret bound is independent of the size of set of reachable states. To the extent of our knowledge, this is the first algorithm that provides a frequentist regret guarantee with improved dependence on \(^{-1}\) in MNL-MDPs. Compared to RRL-MNL, the technical challenge lies in ensuring the stochastic optimism of the estimated value for ORRL-MNL. Note that the prediction error (Definition 1) for ORRL-MNL is characterized by two components: one related to the gradient information of the MNL transition model at each reachable state, and the other related to the dominant feature with respect to the Gram matrix \(_{k,h}\) (Lemma 16). Hence, the probability of the Bellman error at each horizon, when following the optimal policy, being negative can depend on the size of the reachable states. This implies that the probability of stochastic optimism can be exponentially small, not only in the horizon \(H\) but also in the size of the reachable states \(\). However, as shown in Lemma 18, this challenge has been overcome by using a sample size \(M\) that _logarithmically_ increases with \(\), effectively addressing the issue.

Proof Sketch of Theorem 2The overall proof pipeline for Theorem 2 is similar to that of Theorem 1. The main differences lie in the concentration of the estimated transition core (Lemma D.2), the bound on the prediction error (Lemma D.2), and the stochastic optimism (Lemma 18). Please refer to Appendix D for detailed proofs.

Optimistic exploration extensionIn general, since TS-based randomized exploration requires a more rigorous proof technique than UCB-based algorithms, our technical ingredients enable the use of optimistic exploration in a straightforward manner. We introduce UCRL-MNL+ (Algorithm 3) in the Appendix E, an optimism-based algorithm for MNL-MDPs. It is both _computationally_ and _statistically_ efficient compared to UCRL-MNL , achieving _the tightest regret bound_ for MNL-MDPs.

**Corollary 1**.: _UCRL-MNL+ (Algorithm 3) has \(}(dH^{3/2}+^{-1}d^{2}H^{2})\) regret with high probability._

## 5 Numerical Experiments

We perform a numerical evaluation on a variant of RiverSwim  to demonstrate practicality of our proposed algorithms. We compare our algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) with the state-of-the-art UCRL-MNL  for MNL-MDPs. For each configuration, we report the averaged results over 10 independent runs. Figure 0(a) and 0(b) show the episodic return of each algorithm, which is the sum of all the rewards obtained in one episode. First, our proposed algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) outperform UCRL-MNL  for both cases of \(||=4,8\). Second, ORRL-MNL and UCRL-MNL+ reach the optimal values quickly compared to the other algorithms, demonstrating improved statistical efficiency. Figure 0(c) illustrates the comparison in running time of the algorithms for the first 1,000 episodes. Our proposed algorithms are at least 50 times faster than UCRL-MNL. These differences become more pronounced as the episodes progress because our algorithms have a constant computation cost, whereas the computation cost of UCRL-MNL increases over time.

## 6 Conclusions

We propose randomized algorithms with provable efficiency and constant-time computational cost for MNL-MDPs. For the first algorithm, RRL-MNL, we use an optimistic sampling technique to ensure the stochastic optimism of the estimated value functions and provide the frequentist regret analysis. This is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism. To achieve a statistically improved regret bound, we propose ORRL-MNL by constructing the optimistic randomized value function using the effects of the local gradient of the MNL transition model equipped with the centralized feature. As a result, we achieve a frequentist regret guarantee with improved dependence on \(\) in RL with the MNL transition model, which is a significant contribution. The effectiveness and practicality of our methods are supported by numerical experiments.

Figure 1: Riverswim experiment results