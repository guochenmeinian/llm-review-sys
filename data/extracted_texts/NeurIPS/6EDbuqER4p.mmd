# Supplementary Materials

[MISSING_PAGE_FAIL:1]

activation function. The second one results in permutation symmetry: ReLU is equivariant under \(\) where \(\) contains compositions of permutations and diagonal matrices with non-negative entries (positive scaling). The symmetry in models is induced by the symmetry of hidden states' space: by substituting the equality \(=P^{-1} P, P\) into a two-layer neural network \(f(x)=w(w^{}x)\), the network stays the same except that the group acts on the weights \((w,w^{})\) to obtain \((wP^{-1},Pw^{})\), which means the order of rows and columns of the weight matrices are exchanged. While permutation symmetry has been a fundamental assumption in neural networks, we take another path to reflect on this axiomatic assumption and raise the question:

_Can forms of equivariance more general than permutation improve neural networks?_

The self-attention function in Transformers positively answers this question. We give a second answer and let activation functions be another solution. To further motivate the activation function, in Appendix G we start from symmetry principles to axiomatically infer the forms of different neural network structures from scratch, where we essentially modify the hypothesis that activation functions are component-wise. We further show in Appendix B that the proposed activation function and the self-attention function share the same type of symmetry, associated with Noether's Theorem. The symmetry group is related to linear mode connectivity explained in Appendix C, meaning that the loss landscape of neural networks is empirically convex modulo the group. Generalizing the group to infinite order fundamentally enlarges the algebraic structure of neural networks.

ContributionsWe propose Conic Linear Units (CoLU), which introduces orthogonal group symmetry to neural networks. CoLU outperforms state-of-the-art component-wise activation functions such as ReLU in various models including ResNet and UNet for recognition and generation, and keeps the training and inference speed. It achieves remarkable gains in training diffusion models.

## 2 Background

Component-Wise ActivationsAmong the most commonly used activation functions are Rectified Linear Units (ReLU) and its variants, such as Leaky ReLU and Exponential Linear Units (ELU) (Clevert et al., 2015). There are also bounded ones, such as the sigmoid function or the hyperbolic tangent function used in Hochreiter and Schmidhuber (1997). In state-of-the-art vision and language models, soft approximations of ReLU are preferred for their better performance, such as Gaussian Error Linear Units (GELU) (Hendrycks and Gimpel, 2016), Sigmoid-Weighted Linear Units (SiLU) (Elfwing et al., 2018), etc. All these functions are component-wise.

Non-Component-Wise ActivationsPrevious works proposing non-component-wise activation functions are essentially different from CoLU, such as using layer normalizations (Ni et al., 2024) or multiplying the input by a radial function (Ganev et al., 2021). In comparison, CoLU is a generalization of common activations, keeps the favorable conic-projective property unchanged, and improves the performance of neural networks. In the previous version of CoLU (Fu and Cohen, 2024), it had not yet achieved universal improvement on all types of models, since its variants had not been developed.

Equivariance in Linear LayersFor symmetries in the _linear_ part of the model, ensuring different equivariance improves the performance of recognition (Zhang, 2019) and generation (Karras et al., 2021) models, which repeatedly confirm the potential benefits of the symmetry principle. Group equivariant convolutional neural networks (GCNN) (Cohen and Welling, 2016) put symmetry constraints in the spatial domain so that the model admits spatial group actions such as 2D spatial rotations and reflections. Like in most convolutional neural networks, the channel dimensions of GCNNs are always fully connected. CoLU's symmetry assumption is on the channel axis of the states, which means that CoLU considers the tangent space of GCNN's symmetry space, and equally applies to fully connected layers without convolution structures.

Spatial versus Channel CorrelationsInvariant scattering convolutional networks (Bruna and Mallat, 2013) use wavelet bases as deterministic spatial correlations and only learn the pixel-wise linear layer or \(1 1\) convolution. It indicates that learning channel correlation plays a primary role in representing data patterns compared to spatial connections, and it motivates further investigationsinto general symmetries in the channel dimensions--the embedding space. Low-rank adaptation (Hu et al., 2022) and the Query-Key embeddings in the self-attention function are examples of putting low-rank assumptions in the embedding space to represent patterns efficiently. CoLU considers another assumption: it assumes potential subspace orthogonalities.

Orthogonality in the Embedding SpaceEnsuring orthogonality of the embedding space in the linear layers is twofold. The hard constraint method uses a projection onto the Stiefel manifold during training to ensure the orthogonality of the weights (Jia et al., 2019). The soft constraint method adds a regularization term to the loss function (Wang et al., 2020) and learns the orthogonality approximately. Orthogonal CNNs outperform conventional CNNs, suggesting that the orthogonality property helps neural networks gain robustness and generalization ability. The self-attention function in Transformers is also orthogonal equivariant. CoLU is compatible with these orthogonal layers to allow layerwise orthogonality in consecutive layers.

Other Constructions in NonlinearitiesWeiler and Cesa (2019) conduct a survey on some of the nonlinear functions for equivariant networks, which does not cover the form of CoLU. Liu et al. (2024); Mantri et al. (2024) propose essentially component-wise nonlinearities by leveraging other properties, where the equivariance is still restricted to permutations.

## 3 Conic Activation Functions

Figure 1: Illustration of a CoLU function \(\) and an affine transform \(w\) of a cone \(V\).

Figure 2: Connections between neurons in a two-layer neural network \(y=w(w^{}x)\) with component-wise / conic / group-conic / shared-axis group-conic activation functions. In this illustrative example, the network width is \(C=6\) except that in the last shared-axis case \(C=5\). The number of cones is \(G=1\) when there is one cone and \(G=2\) in the grouped case. The yellow arrows denote the maximum norm threshold on the output vector in each group, and the dashed frames denote the conesâ€™ axis dimensions.

A basic conic activation function is defined as \(:^{C}^{C}\)

\[(x)_{i}=x_{1},&i=1\\ \{\{x_{1}/(|x_{}|+),0\},1\}x_{i},&i=2,,C\] (1)

where \(x=(x_{1},x_{2},,x_{C})\) is the input vector, \(||\) is the Euclidean norm, \(\) is a small constant taken as \(10^{-7}\) for numerical stability, and \(x_{}\) denotes the normal vector \(x_{}=(0,x_{2},x_{3},,x_{C})\), so that \(x=x_{1}e_{1}+x_{}\) holds. Here \(e_{1}=(1,0,,0)^{C}\) is a unit vector. Figure 0(a) visualizes a CoLU function with a red arrow and Figure 0(b) visualizes a transformed cone with a linear layer after CoLU. Figure 2 visualizes the connections between neurons of the basic CoLU and its variants to be defined in the sequel. The complexity of CoLU is \(O(C)\), which is the order of component-wise functions and is negligible compared to matrix multiplications. The design of CoLU is irrelevant to the choice of the first axis or another one since its adjacent linear layers are permutation equivariant.

### Soft Scaling

The sigmoid-weighted conic activation function is defined as

\[(x)_{i}=x_{1},&i=1\\ (x_{1}/(|x_{}|+)-1/2)x_{i},&i=2,,C \] (2)

where \((x)=1/(1+(-x))\). Compared with Equation (1), the weighting function \(\{\{r,0\},1\}\) is replaced by \((r-1/2)\), where \(r=x_{1}/(|x_{}|+)\) is the cotangent value of the cone's opening angle \(\), \(r 1/()\) as \( 0\).

The soft projection is inspired by the better performance of smooth functions such as SiLU \((x)=(x)x\), compared to the piecewise-linear ReLU \((x)=_{_{ 0}}(x)x\). Figure 4 compares ReLU weighting with its sigmoid-weighted variant SiLU. Figure 4 compares the hard projection in Equation (1), firm projection weighted by \((4r-2)\) and sigmoid-weighted soft projection in Equation (2).

### Multi-Head Structure

Inspired by group normalization (Wu and He, 2018), group convolution (Krizhevsky et al., 2012), etc., the channel dimension can be divided into \(G\) heads of dimension \(S=C/G\). The group-conic activation function is defined as a group-wise application of the conic activation function. Suppose \(:^{S}^{S}\) is defined in Equation (1) or (2), and \(_{i}^{G}:^{C}^{S},i=1,2,,G\) are the \(G\)-partition subspace projections, then \(\) in higher dimension \(C\) is uniquely characterized by \(_{i}^{G}=_{i}^{G}\), or explicitly,

\[(x)=((_{1}^{G}(x)),(_{2}^{G}(x)),,( _{G}^{G}(x)))\] (3)

In the trivial case \(G=0\), there is no axis to project towards, and we specify that the activation function coincides with the identity function. In the special case \(S=2\) or when the cones are in a 2D space, the 1D cone section degenerates to a line segment with no rotationality, so we specify that the CoLU coincides with the component-wise activation function.

### Axis Sharing

The shared-axis group CoLU is also uniquely defined by \(_{i}^{G}=_{i}^{G},i=1,2,,G\) but with the \(G\)-partition subspace projections defined differently:

\[_{i}^{G}=(_{1},_{(S-1)(i-1)+2},_{(S-1)(i-1)+3},,_{(S-1)i+1} ), i=1,2,,G\] (4)

where \(_{j},j=1,2,,C\) are projections to each axis. \(_{i}^{G}\) is a projection onto the first dimension (the cone axis) and \(S-1\) other consecutive dimensions (the cone section). Therefore the relation among the dimension formula among \((C,G,S)\) is \(C-1=G(S-1)\) in the shared-axis case.

Figure 5 illustrates the motivation of axis sharing: the colinear effect in the hidden states. In this example, \(w^{}\) is the first linear layer of a VAE's encoder \(x^{784} w(w^{}x)^{20}\) pretrained on the MNIST dataset, and \(x^{}\) is the first hidden state \(x^{}=w^{}x^{500}\) where the \(100\) cone axes are permuted together for visualization. Therefore, the hidden dimension is \(C=500\), the number of groups is \(G=100\), the number of test examples is \(10000\), \(w^{}^{784 500}\), \(x^{}^{10000 500}\) and \((w^{}),(x^{})^{500 500}\). The upper-left parts of the matrices are very bright, meaning that the axis dimensions are highly colinear.

### Homogeneous Axes

An alternative form of CoLU ensures component homogeneity, by rotating the standard Lorentz Cone towards the all-one vector, and we call it a rotated conic activation function (RCoLU)

\[(x)=x_{e}+\{\{|x_{e}|/(|x_{}|+),0\},1\}x_{}\] (5)

where \(x_{e}=x e\), \(x_{}=x-x_{e}\) and \(e=(1/,,1/)\). The axis-homogeneous cone avoids splitting operations in the calculation. It can also be combined with grouping using Equation (4), and with axis sharing by setting \(e=(1/,,1/)\) in Equation (5) instead of using Equation (4). RCoLU's performance boost over ReLU is similar to standard CoLU, so we omit it in the experiment section.

## 4 Why Conic Activation Functions

CoLU is motivated by the conic projection, which generalizes the equivariance in a neural network. The proofs are provided in Appendix E.

### Conic Projection

To naturally characterize this projection, it is necessary to recall hyperbolic geometry detailed in Appendix A, where we define the Lorentz cone (the future Light Cone) \(V=\{x^{C}:x_{1}^{2}-x_{2}^{2}--x_{C}^{2} 0,x_{1} 0\}\) and the hyperplane of simultaneity \(H(x)=\{y^{C}:y_{1}=x_{1}\}\). We denote \(=_{ 0}e_{1} V\), where \(_{ 0}e_{1}=\{(t,0,,0)^{C}:t 0\}\).

**Definition 4.1** (Conic Projection).: _The conic projection is defined as \(x^{C}_{ H(x)}(x)\) where \(\) is the nearest point projection, \(_{A}(x)=*{argmin}_{y A}|y-x|\)._

The restriction of the projection on its image \(\) is the identity function, so it satisfies the idempotent property \(^{2}=\). Constraining the projection in \(H(x)\) simplifies the computation while maintaining essential equivariance properties--it guarantees that the projection is always towards the cone axis. Since \(V H(x)=\) when \(x_{1}<0\), the projection is not feasible in the negative half-space, so \(V\) is extended to \(\) for the well-definedness--on the negative half-space, the projection is degenerate, \(_{ H(x)}(x)=(x_{1},0,,0)\). In other words, the past Light Cone has zero light speed and thus zero opening angle.

**Lemma 4.2** (CoLU is Conic Projection).: _Suppose \(\) is defined in Equation (1), then it coincides with a conic projection._

\[_{ 0}(x)=_{ H(x)}(x)=_{ \{x_{1},0\}D+\{x_{1},0\}e_{1}}(x)\] (6)

_where \(D=\{x^{C}:x_{1}=1,_{i=2}^{C}x_{i} 1\}\) is the \((C-1)\)-dimensional disk._We note that \(V\) is the conic hull of \(D\), and \(D\) is isometric to a hyperball in dimension \(C-1\), and therefore it has the symmetry group \((C-1)\). In comparison, the invariant set of ReLU is the convex hull of the \(C-1\) simplex \(^{C-1}\), defined as the convex hull of the unit vectors \(\{e_{i}^{C}:i=1,2,,C\}\). Next, we discuss the general link between algebraic and geometric symmetry.

### Generalized Symmetry Group

Inspired by the Erlangen program  bridging algebraic groups with geometric forms, the equivariant group is more intuitively motivated by the symmetry of the projections' invariant sets.

**Definition 4.3** (Invariant Set).: _The invariant set of a function \(:^{C}^{C}\) is defined as_

\[_{}=\{x^{C}:(x)=x\}\]

_Moreover, the symmetry group \(\) and the isometric symmetry group \(^{*}\) of a set \(A\) is the group of affine and rigid functions that preserves the set:_

\[_{A}=\{P(C):P(A)=A\},^{*}_{A}= _{A}(n)\]

_where \((C)\) is the general affine group, and \((n)=\{P(^{C}):|P(x)-P(y)|=|x-y|, x,y^{C}\}\) denotes the Euclidean group._

**Definition 4.4** (Symmetry Group).: _The equivariance group and the isometric equivariance group of a function \(:^{C}^{C}\) is defined as_

\[_{}=\{P(C):P= P\},^{*}_{}=_{}(n)\]

**Lemma 4.5** (Projective-Type Operators).: _If \(\) is either ReLU or CoLU, then \(_{}=_{_{}}\), and \(^{*}_{}=^{*}_{_{}}\)._

This algebra-geometry duality applies to more general neural network architectures, such as the self-attention function. The relation with Noether's theorem is discussed in Appendix B.

**Corollary 4.6** (Permutation Symmetry).: _Suppose \(\) is the component-wise ReLU, then \(_{}=^{C}_{+}\), \(_{}=_{_{}}=(C)\) and \(^{*}_{}=^{*}_{_{}}= (C)\), where \(^{C}_{+}=\{x^{C}:x_{i} 0,i=1,2,,C\}\) is the positive orthant, and \((C)=\{P(C):P(C), (C)\}\) is the scaled permutation group in dimension \(C\), where \(\) is the permutation group and \(\) is the group of diagonal matrices with non-negative entries._

**Theorem 4.7** (Conic Symmetry).: _The symmetry groups of CoLU defined by Equation (3) or (4) are_

\[_{}=_{_{}}=(G) ^{G}(S-1),^{*}_{}=^{*}_{_{}}=(G)^{G}(S-1)\] (7)

_where \(_{}=^{G}\). In the shared-axis case, \(_{}=^{G}/\!\!\!\!/\) where the relation \(\) is defined as \(x y\) if and only if \( i,j\{1,2,,G\}\) such that \(^{G}_{i}(x)_{1}=^{G}_{j}(y)_{1}\) and \( k\{2,3,,S\},^{G}_{i}(x)_{k}=^{G}_{j}(y)_{k}=0\)._

In Equation (7), \((G)\) represents the permutations among different cones and \((S-1)\) represents rotations or reflections within each cone. The motivation is that matrix conjugation modulo permutations reduce to block diagonal form, and we assume there are low-dimensional block sub-spaces that can hold orthogonal equivariance. The symmetry group is continuous and thus of order infinity, unprecedented in component-wise activations. We use the following construction to illustrate that it improves neural networks' generalization ability since component-wise activations fail to hold orthogonal equivariance whereas conic activations do.

**Lemma 4.8** (Layerwise Orthogonal Equivariance).: _Assume a two-layer neural network \(y=f_{}(x)=w(w^{}x)\) with fixed width \(C\) and the training data \(\) satisfies subspace orthogonal symmetry: \((x,y), P,(Px,Py)\), where \(=\{P(C):P[1,2;1,2](2),P[3,,C;3,,C ]=_{C-2},P[1,2;3,,C]=P[3,,C;1,2]^{}=0\} (2)\). Then,_

_(1) (ReLU excludes orthogonal equivariance) If \(\) is component-wise activation function, then \((^{C^{2}}\{0\})^{2}\), \( x^{C}\) and \(P\) such that \(Pf_{}(x) f_{}(Px)\)._

_(2) (CoLU holds orthogonal equivariance) If \(\) is that of Equation (1), then \(^{}=(w^{},w^{})\) such that \( x^{C}, P,Pf_{^{}}(x)= f_{^{}}(Px)\)._As a remark, we explain the sufficiency of rigid alignments with a compact group \(^{*}\) without scaling by adding a least-action regularization term, to justify the common practice in the literature, which answers the open issue of permutation-only alignments in Bokman and Kahl (2024).

**Remark 4.9** (Soundness of Isometric Alignment).: _Suppose \(L\) is the alignment objective defined in the algorithms in Appendix F, then \(>0\) such that the regularized alignment coincides with isometric alignment: \(*{argmin}_{P_{}}(L(P)-\|P\|)= *{argmin}_{P_{}^{*}}L(P)\), where \(\|\|=-_{w}(_{i}w_{i}^{p})^{1/p}\) is some norm of order \(p 1\)._

## 5 Experiments

The experiments are conducted on an 8-core Google v4 TPU. For computational costs, CoLU introduces negligible computational overhead compared to ReLU in all experiments and all variants.

### Synthetic Data

To demonstrate the advantage of the generalized symmetry of CoLU in the embedding space, we use a two-layer MLP to learn the rotation of a 2D hemisphere. The MLP is defined as \(x^{3} w(w^{}x)\), where \(w,w^{}^{3 3}\). The dataset \(\) consists of polar grid points and their rotated counterparts \((x,y=Rx)\), where \(R\) represents a rotation of \(45^{}\) around each of the three coordinate axes. As shown in Figure 6 in the third column, ReLU does not capture orthogonal equivariance (rotation around the hemisphere axis) near the equator, instead projecting the boundary onto a triangle. In contrast, RCoLU successfully preserves the rotational symmetry at every latitude, including at the boundary. This is due to the geometry of the projection boundary: ReLU cuts the hemisphere with the positive orthant and produces a boundary of the 2-simplex \(^{2}\), whereas CoLU projects onto a cone that naturally preserves the circular pattern.

Figure 6: Input, activations, output, and ground truth of a learned hemisphere rotation.

### Toy VAE

The toy generative model is a VAE with a two-layer encoder and a two-layer decoder, trained on the binarized MNIST dataset. The test loss is compared since CoLU is hypothesized to increase the model's generalization ability.

Experimental SettingsWe use the Adam optimizer with a weight decay of \(10^{-2}\) and train \(10\) epochs for each run. The global batch size is set to \(128\) and the learning rate is set to \(10^{-3}\). Each configuration is trained for \(10\) times with different random seeds. More detailed settings are provided in Appendix D.

ResultsTable 1 compares hard-projected or soft-projected CoLU with ReLU or CoLU when the axes are shared. Table 2 compares the improvement from adding axis sharing in the soft projection

  
**Width \(C\)** & **Group \(G\)** & **Dim \(C\)** & **Soft?** & **Train Loss (\( 10^{2}\))** & **Test Loss (\( 10^{2}\))** \\   &  &  & Identity & \(1.1086 0.0060\) & \(1.1982 0.0011\) \\  & & & Identity & \(1.1072 0.0031\) & \(1.1981 0.0010\) \\   & & & âœ“ & \(1.0804 0.0108\) & \(1.1740 0.0009\) \\   & & & âœ— & \(1.0835 0.0048\) & \(1.1656 0.0013\) \\   & & & âœ“ & \(1.0302 0.0065\) & \(1.1216 0.0016\) \\   & & & âœ— & \(1.0226 0.0057\) & \(1.1137 0.0026\) \\   & & & âœ“ & \(0.9181 0.0060\) & \(1.0106 0.0017\) \\   & & & âœ— & \(0.9166 0.0041\) & \(1.0073 0.0015\) \\   & & & âœ“ & \(0.8698 0.0055\) & \(\) \\   & & & âœ— & \(0.8736 0.0040\) & \(\) \\   & & & âœ“ & \(0.8424 0.0084\) & \(\) \\   & & & âœ— & \(0.8430 0.0052\) & \(\) \\   & & & âœ“ & \(0.8388 0.0268\) & \(\) \\   & & & âœ— & Unstable & Unstable \\   &  &  & âœ“ & \(0.8334 0.0232\) & \(\) \\   & & & âœ— & Unstable & Unstable \\   & & & âœ— & Unstable & Unstable \\   & & & âœ— & Unstable & Unstable \\   & & & & \(0.8429 0.0034\) & \(0.9814 0.0007\) \\   & & & ReLU & \(0.8195 0.0039\) & \(0.9892 0.0011\) \\   

Table 1: Comparisons of CoLU model with soft and hard projections with axis sharing. Unstable means some of the initializations do not converge.

  
**Width \(C\)** & **Group \(G\)** & **Dim \(C\)** & **Share Axis?** & **Train Loss (\( 10^{2}\))** & **Test Loss (\( 10^{2}\))** \\   &  &  & Identity & \(1.1086 0.0060\) & \(1.1982 0.0011\) \\  & & & Identity & \(1.1098 0.0129\) & \(1.1985 0.0015\) \\   & & & âœ“ & \(1.0804 0.0108\) & \(1.1740 0.0009\) \\   & & & âœ— & \(1.0828 0.0080\) & \(1.1733 0.0008\) \\   & & & âœ“ & \(1.0302 0.0065\) & \(1.1216 0.0016\) \\   & & & âœ— & \(1.0207 0.0088\) & \(1.1179 0.0029\) \\   & & & âœ— & \(0.9181 0.0060\) & \(1.0106 0.0017\) \\   & & & âœ— & \(0.9111 0.0041\) & \(1.0096 0.0013\) \\   & & & âœ— & \(0.8698 0.0055\) & \(\) \\   & & & âœ— & \(0.8783 0.0045\) & \(0.9864 0.0015\) \\   & & & âœ— & \(0.8424 0.0084\) & \(\) \\   & & & âœ— & \(0.8718 0.0062\) & \(0.9833 0.0021\) \\   & & & âœ“ & \(0.8388 0.0268\) & \(\) \\   & & & âœ— & \(0.8801 0.0073\) & \(0.9893 0.0021\) \\   & & & âœ“ & \(0.8334 0.0232\) & \(\) \\   & & & âœ— & \(0.8808 0.0099\) & \(0.9930 0.0018\) \\   & & & & \(\) & \(0.8429 0.0034\) & \(0.9814 0.0007\) \\   & & & & SiLU & \(0.8402 0.0041\) & \(0.9856 0.0008\) \\   

Table 2: Comparisons of soft-projected CoLU with or without axis sharing.

case. The test losses at the best early-stopping steps are reported. The highlighted cases correspond to the hyperparameters where CoLU outperforms component-wise activation functions. Furthermore, Appendix D complements the learning curves of these hyperparameters. Combining axis sharing and soft projection effectively stabilizes the training when cone dimensions are low in the VAE experiments.

### Toy MLP

According to the hyperparameter search above, we set the cone dimensions to \(S=4\), which complies with the number of chips in hardware platforms. We compare test accuracies in the MNIST recognition tasks to test the hypothesis of CoLU's generalization ability.

Experimental SettingsWe set the global batch size to \(1024\) and the learning rate to \(10^{-3}\). Each configuration is trained 7 times with different random seeds. More detailed settings are provided in Appendix D.

ResultsTable 3 compares ReLU with CoLU of low-dimensional orthogonal subspaces and shows the improvement from using axis sharing combined with soft projection.

### ResNet

To test the performance of CoLU in deeper models, we scale up the network to ResNet-56 and train them on the CIFAR10 dataset. Axis sharing and soft projection are omitted for clean comparisons with ReLU in the sequel.

Experimental SettingsThe ResNet architecture and the training recipe follow He et al. (2016). The runs are repeated for \(10\) times with different random seeds each lasting \(180\) epochs, and use the Adam optimizer with a batch size of \(128\), a learning rate of \(10^{-3}\), and a weight decay coefficient of \(10^{-2}\). Finer training settings will achieve better baselines, and CoLU remains superior to ReLU.

ResultsTable 4 shows that CoLU outperforms ReLU and the training is stable across different initialization seeds.

### Diffusion Models

We compare CoLU and ReLU in unconditional generation with diffusion models (Sohl-Dickstein et al., 2015) trained on the CIFAR10 and Flowers datasets. Then we show the possibility of borrowing a pretrained text-to-image model (Rombach et al., 2022) and fine-tuning it to a CoLU model. Detailed settings are in Appendix D.

Training ResultsFigure 7 shows that CoLU-based UNets converge faster and achieve lower losses than the ReLU-based baselines. On the small dataset CIFAR10, the convergence is observed to be much faster. On the larger Flowers dataset, the loss of the CoLU model is significantly lower

  
**Activation** & **Cone Dimension \(S\)** & **Train Loss** & **Test Accuracy** \\  ReLU & - & 0.005132 Â± 0.001461 & 0.9065 Â± 0.0100 \\ CoLU & 4 & 0.003244 Â± 0.000185 & **0.9101 Â± 0.0039** \\   

Table 4: Comparisons between ReLU and CoLU in ResNet-56.

  
**Activation** & **Width \(C\)** & **Dim \(S\)** & **Axis Sharing** & **Soft Projection** & **Train Loss** & **Test Accuracy** \\  ReLU & 512 & - & - & âœ— & 0.0000 Â± 0.0000 & 0.9576 Â± 0.0017 \\ CoLU & 512 & 4 & âœ— & âœ— & 0.0000 Â± 0.0000 & **0.9644 Â± 0.0010** \\ CoLU & 511 & 4 & âœ“ & âœ“ & 0.0000 Â± 0.0000 & **0.9652 Â± 0.0013** \\   

Table 3: Comparisons between ReLU and CoLU in two-layer MLP.

than the ReLU model throughout the training. Table 5 shows quantitative improvement of CoLU in diffusion UNets. Appendix D shows generated samples on the Flowers dataset.

Fine-Tuning ResultsWe replace all activation functions in the UNet with soft-projected conic activation functions of \(G=32\) without axis sharing. Appendix D shows generated samples from the fine-tuned model and visually compares the original activation and CoLU models.

### MLP in GPT2

CoLU is better than ReLU in the MLP part of a Generative Pretrained Transformer (GPT2) trained on Shakespeare's play corpus. Appendix D reports a comparison in the test loss. We also observe that CoLU achieves slower overfitting and lower test loss with the same training loss.

### Linear Mode Connectivity

CoLU enlarges the group of neural networks' linear mode connectivity, explained in Appendix C.

Convolution Filter SymmetryDiffusion models with ReLU and CoLU have different symmetry patterns in the convolution filters. We show in Appendix D that between the last layer of two diffusion UNets trained with different initialization on CIFAR10, a ReLU model's convolution filters can be permuted to match each other, whereas a CoLU model cannot since the orthogonal symmetry relaxes to additional color rotations.

Generative Model AlignmentFor completeness, we show alignment results on the ReLU and CoLU-based models in Appendix D. In the literature on linear mode connectivity, few works study generative models, and we show that the generative VAEs also reveal linear mode connectivity under the equivariance groups of activation functions.

## 6 Conclusion

In this work, we introduced Conic Linear Units (CoLU) to let neural networks hold layerwise orthogonal equivariance. CoLU outperforms common component-wise activation functions and scales to a broad range of large models. The code will be publicly available at https://github.com/EvergreenTree/di-f-fu-sion.

  
**Activation** & **Cone Dimension \(S\)** & **Train Loss (CIFAR10)** & **Train Loss (Flowers)** \\  ReLU & - & 0.1606 & 0.01653 \\ CoLU & 4 & **0.1593** & **0.01458** \\   

Table 5: Comparisons between ReLU and CoLU in diffusion UNet.

Figure 7: Learning curves of ReLU and CoLU diffusion models.