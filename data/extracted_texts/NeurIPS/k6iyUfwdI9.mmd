# To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty

Yasin Abbasi Yadkori

Google DeepMind

yadkori@google.com &Ilja Kuzborskij

Google DeepMind

iljak@google.com &Andras Gyorgy

Google DeepMind

agyorgy@google.com &Csaba Szepesvari

Google DeepMind and University of Alberta

szepi@google.com

###### Abstract

We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.

## 1 Introduction

Language models too occasionally suffer from _hallucinations_, or responses with low truthfulness, that do not match our own common or textbook knowledge (Bubeck et al., 2023; Gemini Team, Google, 2023). At the same time, since LLMs work by modeling a probability distribution over texts, it is natural to view the problem of truthfulness through the lens of statistical uncertainty. In this paper we explore uncertainty quantification in LLMs. We distinguish between two sources of uncertainty: _epistemic_ and _aleatoric_(Wen et al., 2022; Osband et al., 2023; Johnson et al., 2024). Epistemic uncertainty arises from the lack of knowledge about the ground truth (e.g., facts or grammar in the language), stemming from various reasons such as insufficient amount of training data or model capacity. Aleatoric uncertainty comes from irreducible randomness in the prediction problem, such as multiple valid answers to the same query. Hence, truthfulness can be directly analyzed via looking at the epistemic uncertainty of a model in the sense that when the epistemic uncertainty is low, the model predictions must be close to the ground truth.

Rigorously identifying when (either) uncertainty is small1 is notoriously hard, especially in deep neural networks (Blundell et al., 2015; Antoran et al., 2020). This is because we generally lack guarantees about learning the ground truth (consistency), or even a weaker guarantee about how large the variance of a learning algorithm is. At the same time, there exist many heuristic approaches for uncertainty quantification based on simply looking at the log-likelihood of responses (Kadavath et al., 2022), estimating entropy (Kuhn et al., 2023), ensembling (Lakshminarayanan et al., 2017; Dwaracherla et al., 2023; Osband et al., 2023), or sometimes even more principled formulations, such as conformal prediction (Angelopoulos et al., 2023; Ravfogel et al., 2023; Yadkori et al., 2024) (which however come with strong assumptions).

To the best of our knowledge, a common limitation of these approaches is that they are only meaningful in problems where there exists a _single_ correct response (e.g. label) as they aim for detecting if one response is dominant (or multiple responses with the same meaning), that is, if there is only little uncertainty in the prediction. On the other hand, when multiple responses are correct, that is, there is _aleatoric uncertainty_ in the ground truth, simply estimating the amount of uncertainty in the LLM's output is insufficient, as the perfect (ground-truth) predictor may have large aleatoric uncertainty and no epistemic uncertainty, while a completely useless predictor may have large epistemic uncertainty only, but the total amount of uncertainty of the two predictors might be the same.

Contributions.In this paper we address the above problem directly, and design methods to _decouple epistemic and aleatoric uncertainty_, allowing us to effectively deal with multi-response queries. Rather than trying to quantify how small epistemic uncertainty can be, we aim to identify when only the _epistemic uncertainty is large_, in which case we can suspect that the response is hallucinated.2

As a starting point we make a simple observation: If multiple responses are obtained to the same query from the ground truth (the language), they should be independent from each other, that is, in probabilistic interpretation, the joint distribution of these multiple responses, for a fixed query, must be a product distribution.

This observation can be used to measure how _far_ the language model can be from the ground truth. The sequential model implemented by a language model allows us to construct a joint distribution over multiple responses, which is done through _iterative prompting of an LLM based on its previous responses_ and the application of the chain rule of probability: first the model is asked to provide a response given a query, then to provide another response given the query and the first response, then a third one given the query and the first two responses, an so on. This is in contrast to some of the earlier works that approached decoupling epistemic and aleatoric uncertainty for classification problems by training the model with label pairs (or tuples) (Wen et al., 2022; Johnson et al., 2024).

So, if the response to a prompt containing the query and previous responses is insensitive to the previous responses, we have the desired independence and the LLM-derived joint distribution can be arbitrarily close to the ground truth. On the other hand, if the responses within the context heavily influence new responses from the model then, intuitively speaking, the LLM has low confidence about the knowledge stored in its parameters, and so the LLM-derived joint distribution _cannot be close_ to the ground truth. As more responses are added to the prompt, this dependence can be made more apparent, allowing to detect _epistemic uncertainty via our iterative prompting procedure_.

Interestingly, as we will see in Section 3, we can force an LLM to provide a desired (possibly incorrect) response by adding this response repeatedly to the prompt. This phenomenon is then further investigated from the viewpoint of a transformer LLM architecture in Section 4.

The iterative prompting procedure then leads to the following main contributions:

_(i)_ Based on the above iterative prompting procedure, we derive an _information-theoretic metric of epistemic uncertainty_ in LLMs (Section 5), which quantifies the gap between the LLM-derived distribution over responses and the ground truth. This gap is insensitive to aleatoric uncertainty, allowing to quantify epistemic uncertainty even in cases where there are multiple valid responses.

_(ii)_ We derive a computable lower bound on this metric, which turns out to be a _mutual information_ (MI) of an LLM-derived joint distribution over responses, and propose a finite-sample estimator for it.

We prove that this finite-sample MI estimator sometimes suffers only a negligible error even though LLMs and their derived joint distributions are defined over potentially infinite supports (all possible strings in a language).

_(iii)_ We discuss an algorithm for hallucination detection based on thresholding a finite-sample MI estimator, where the threshold is computed automatically through a _calibration_ procedure. We show experimentally on closed-book open-domain question-answering benchmarks (such as TriviaQA, AmbigQA, and a dataset synthesized from WordNet) that when the data is mostly composed of either single-label or multi-label queries, our MI-based hallucination detection method surpasses a naive baseline (which is based on the likelihood of the response), and achieves essentially similar performance to that of a more advanced baseline which is based on the entropy of the output as a proxy for uncertainty. However, on datasets which contain both single- and multi-label samples at the same time, our method also significantly outperforms the entropy-based baseline, by achieving a much higher recall rate on samples with high output entropy while maintaining similar error rates.

_(iv)_ Focusing on a single self-attention head, we identify a simple mechanistic explanation for how the model output can be changed through iterative prompting using previous responses, as discussed earlier. Suppose that the prompt is composed from a query and a repeated element (e.g., a possibly wrong answer). If the query lies within the space spanned by the large principal components of a key-query matrix product, then the output will be generated according to the knowledge extracted from the training data (now stored in a value matrix). On the other hand, if the query has little overlap with the large principal components, then the repeated element is likely to be copied from the prompt.

## 2 Preliminaries

Conditional distributions and prompting.Let \(\) be the space of finite text sequences, that is \(^{*}\) where \(\) is a finite alphabet (and \(^{*}=_{n=1}^{}^{n}\)). Moreover, consider a family of conditional distributions \(=\{:\ _{x }(x x^{})=1 x^{}\}\). In the following, we let \(P\) be the ground-truth conditional probability distribution over text sequences (responses) given a prompt, and we let \(Q\) be the learned language model. Given a fixed query \(x\) and possible responses \(Y_{1},,Y_{t}\), we define a _family of prompts_\(=\{F_{t}:\ \ t\}\), such that \(F_{t}(x,Y_{1},,Y_{t})\) is defined as:

   \\ \). Another answer to question Q is \(Y_{2}\).[...] Another answer to question Q is \(Y_{t}\).} \\  \\  \\  \) be distributions supported on set \(=_{1}_{n}\) where \((_{i})_{i}\) is a collection of countable sets. The _entropy_ of a distribution \(\) is defined as \(H()=_{z}(z)(1/(z))\).3 If \(,^{}\) are such that \(^{}(z)=0\) only if \((z)=0\), we have a _Kullback-Leibler divergence_ between them defined as \(D_{}(,^{})=_{z}(z)((z)/^{ }(z))\). For any \(z\), we denote \(z^{ i}=(z_{1},,z_{i-1},z_{i+1},,z_{n})\), and the marginal of the \(i\)th coordinate of \(\) is given by \(_{i}(z)=_{z^{ i}^{n-1}}(z)\). The product distribution of the marginals of \(\) is given by \(^{}(z)=_{i=1}^{n}_{i}(z)\), and the _mutual information_ of \(\) is defined as \(I()=D_{}(,^{})\).

## 3 Probability amplification by iteratively prompting

In this section we demonstrate that, as mentioned in the introduction, repeating possible responses several times in a prompt can have pronounced effects on the output of a language model. Consider \(x=\)_"What is the capital of the UK?"_ and \(Y_{1}==Y_{t}=\)_"Another answer to question Q is Paris."_ Here we can repeat the sentence _"Another answer to question Q is Paris."_ an arbitrary number of times. Although the number of repetitions changes the behavior of the LLM, the correct response maintains a significant probability: as Figure 1 shows, the conditional normalized probability4 of the correct response, _"London"_, reduces from approximately 1 to about 96% as we increase the number of repetitions of the incorrect response to 100. Figure 1 shows 3 more examples where, with initially low epistemic uncertainty in the response to the query (the aleatoric uncertainty is also low as we consider single-response queries), the correct response maintains a significant or non-negligible probability even in the presence of repetitions of incorrect information, while the probability of predicting the latter is increased.

Next, we consider a queries for which the model is more uncertain. For the prompt _"What is the national instrument of Ireland?"_, we observe that responses _"The harp"_ and _"Uilleann pipes"_ both have significant probabilities (the first answer is the correct one). This time, by incorporating the incorrect response in the prompt multiple times, the probability of the correct answer quickly collapses to near zero, as shown in Figure 2, with significant epistemic uncertainty.

Finally, we consider multi-label queries for which the LLM confidently knows a correct answer. This time, by incorporating a potential response in the prompt, the probabilities of other correct answers stay relatively large. Figure 3 shows four such examples.

Figure 1: Single-label queries with low epistemic uncertainty: Conditional normalized probability of the correct completion given repetitions of an incorrect response. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses (the first response is the correct one).

Figure 3: Multi-label queries with aleatoric uncertainty: Conditional normalized probability of the first of the two provided responses, both of which are correct, given repetitions of the second response in the prompt. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses.

Figure 2: Single-label queries with high epistemic uncertainty: Conditional normalized probability of the correct completion given repetitions of an incorrect response. Each figure shows the query and the considered two responses with their initial probabilities, as a response for the query, in parentheses (the first response is the correct one).

## 4 Explanation through the lens of in-context vs. in-weight learning

The sensitivity of the response of an LLM to extra _in-context_ information, as observed above, can already be observed in a single attention head as explained next.

We consider an idealized attention mechanism as follows. Let \(^{n d^{}}\) be an input matrix comprised of \(n\) semantic feature vectors each of dimension \(d^{}\). Each row is meant to represent a complete statement (such as _"What is the capital of the UK?"_ or _"One answer to the question is Paris."_, etc.) rather than a single token. Let \(X^{}^{1 d^{}}\) be the first row of \(\), which represents the _query_ of interest, such as _"What is the capital of the UK?"_. Let \(E^{}^{1 d^{}}\) be a special vector indicating the end of the input. The matrix \( X\), denoting the \(\) matrix without its first row, represents the _in-context_ information.

We assume the ground-truth distribution \(P\) is such that a query vector is mapped to its response, but a statement is simply copied. For example, for \(V=\)_"What is the capital of the UK?"_, \(P( V)\) would be a distribution with support on _"London"_ and its variations, while for \(V^{}=\)_"What is the capital of the UK? One answer to the question is Paris."_, \(P( V^{})\) returns the same distribution. We assume a parameter matrix \(}\) is learned such that \(V^{}}\) estimates \(P( V)\) for vector \(V\).

Let \(},},}^{d^{} d}\) be the query, key, and value matrices. A self-attention head with query \(X\) and context \( X\) is defined as

\[f(;},},})= (}E^{}}(})^{ })}\]

where the output of the softmax is a row vector of length \(n\).

If \(X\) has appeared many times in the training data, then parameters \(}\) and \(}\) could be learned such that \(E^{}}(})^{}X\) is large, that is, \(X\) is within the space spanned by the large principal components of the key-query matrix product. Then, no matter what in-context information appears in \(\), the probability assigned to \(X\) will dominate the softmax, and we will have \((}E^{}}( })^{}) X^{}\), and therefore \(f(;},},}) P( X)\).

Now, consider the case that \(X\) has not appeared many times in the training data, and vector \(Y\) is copied in many rows of \(\). Then \(E^{}}(})^{}X\) could be small as \(X\) is not in the span of the large principal components of the key-query matrix product. Therefore \(f(;},},}) Y\) since \((}E^{}}( })^{}) Y^{}\). Even if \(X\) is in the span, repeating \(Y\)\(t\) times in \(\) would give a \(t\)-times increased total weight to \(Y\) inside the softmax, which can dominate the weight assigned to \(X\) when \(t\) is large enough, also resulting in \(Y\) as the answer.

## 5 Metric of epistemic uncertainty and its estimation

In this section we apply iterative prompting to estimate the epistemic uncertainty of a language model about responding to some query. The idea is to utilize the different behavior patterns observed in Section 3, which can be used to differentiate between two modes of high uncertainty: when the aleatoric uncertainty is high vs. when only the epistemic uncertainty is high. We then apply our new uncertainty metric to design a score-based hallucination detection algorithm.

We will first present the uncertainty metric and its estimate for a distribution defined on the direct outputs of an LLM; the changes needed to take semantic equivalences of language into account are deferred to Appendix A (Kuhn et al., 2023; Farquhar et al., 2024).

Our uncertainty metric, similarly to the ones considered by, e.g., Wen et al. (2022); Osband et al. (2023), is based on analyzing the joint distribution of responses: if multiple responses are sampled jointly according to the ground-truth distribution, they should be independent (as one instantiation of a response should not affect other responses). To make this notion precise, we start with defining a notion of the joint distribution over responses given a query, derived from the language model through the prompting mechanism \(\) defined in Section 2:

**Definition 5.1** (Pseudo joint distribution).: Given a family of prompt functions \(\), a conditional distribution \(\), and \(n\), we use notation \(\) to denote a pseudo joint distribution defined as

\[(Y_{1},,Y_{n} x)=(Y_{1} F_{0}(x))\,(Y_{2}  F_{1}(x,Y_{1}))(Y_{n} F_{n-1}(x,Y_{1},,Y_{n-1}))\.\] (1)The above is a _pseudo_ joint distribution since the standard conditioning in the chain-rule is replaced with prompt functions of the conditioning variables. In the following we focus on \(\) derived from the LLM and \(\) derived from the ground truth.

**Remark 5.2** (Sampling from \(\)).: _Note that sampling from \(\) can be simply done through a chain-rule-like procedure as can be seen from the above definition, that is, to have \((Y_{1},,Y_{n})\) we draw \(Y_{1} Q( F_{0}(x))\), \(Y_{2} Q( F_{1}(x,Y_{1}))\), \(Y_{3} Q( F_{2}(x,Y_{1},Y_{2}))\), and so on._

In the rest of the paper we drop subscripts in joint distributions and conditioning on query \(x\) (which is understood implicitly), for example, \(_{Y_{1} Y_{n} x}\).

For any query \(x\), let \(_{}(x)\) denote the support of \(\). We make the following assumption about the ground truth, which states that the model \(Q\) generates reasonable responses and that the distribution of such responses are independent according to the ground truth:

**Assumption 5.3** (Ground truth independence assumption).: For any query \(x\), _(i)_ there exists a sequence of valid responses \((x)\) such that the ground-truth distribution satisfies

\[P(Y_{t} F_{t-1}(x,Y_{1},,Y_{t-1}))=P(Y_{t} x)$ and any $(Y_{1},,Y_{t})(x)$;}\]

_(ii) \(_{}(x)(x)\), that is, the model \(Q\) generates reasonable responses._

Note that the above assumption is heavily dependent on our prompt construction and the assumption that \(Y_{1},,Y_{t-1}\) are valid responses; without these the independence assumption would not hold, for example, if \(Y_{1},,Y_{t}\) were partial answers, such as a step of an algorithm or a part of a story, or would completely redefine the problem (_"Disregard the previous question. Instead answer the following..."_), because in such cases \(Y_{t}\) might indeed depend on the previous outputs \(Y_{1},,Y_{t-1}\). Roughly speaking, the assumption tells that the response distribution is insensitive to a query based on previously sampled responses. For example, for query \(x=\)_"Capital of the UK:"_, the probability of \(Y_{2}=\)_"London"_ essentially does not change if a city is \(Y_{1}=\)_"Paris"_.

To measure epistemic uncertainty, we need to quantify how far the estimated pseudo joint distribution \(\) is from the ground truth \(\). One natural choice is the following definition:

**Definition 5.4** (Epistemic uncertainty metric).: Given an input \(x\), we say that the epistemic uncertainty of \(\) is quantified by \(D_{}(,)\).

Here \(D_{}\) measures how well \(\) approximates \(\) for a given query \(x\). Namely, this metric determines if \(\) assigns a large probability to an event which has a small probability under \(\). In case of LLMs, this means the LLM generates a sequence that is unlikely in the typical usage of the language. Given an input \(x\), we want to estimate the above hallucination metric, but we only have access to \(\), and so computing it explicitly is impossible. However, next we show that under Assumption 5.3 we can _lower bound_\(D_{}(,)\) by a quantity which _only depends_ on \(\) (the proof is given in Appendix E).

**Theorem 5.5**.: _For all pseudo joint distributions \(\) and \(\) satisfying Assumption 5.3, \(D_{}(,) I()\)._

The lower bound in the theorem holds uniformly for all \(\), and it is computable solely based on \(\). This makes the bound applicable for decision making; in fact we chose to consider \(D_{}(,)\) as the measure of epistemic uncertainty (out of similar distance measures) since it admits this property.

Also, note that we have \(I()=D_{}(,^{})\), \(^{}=_{i}_{y^{ i}}(y_{1}, ,y_{i-1},Y_{i},y_{i+1},,y_{n})\). In general \(_{y^{ i}}(y_{1},,y_{i-1},Y_{i},y_{i+1},, y_{n})(Y_{i})\), because the independence assumption Assumption 5.3 does not necessarily (and, in practice, almost never) holds for \(Q\).

Finally, a quantity related to \(D_{}(,)\) is \(D_{}\) with arguments arranged in the opposite order, that is \(D_{}(,)\) which is a (query) conditional _excess risk_ of the LLM-derived pseudo joint distribution \(\), under the logarithmic loss. Controlling the excess risk (for instance, upper-bounding it) for various algorithms is one of the central questions in learning theory, however it is a much harder task than the one we consider here, because for the former we need to theoretically control all sources of errors (such as generalization, estimation, and approximation error).

### A computable lower bound on epistemic uncertainty

Theorem 5.5 gives a lower bound on the epistemic uncertainty by the mutual information. However, to compute the mutual information term, in practice we need to evaluate \(\) on its entire support, which is potentially infinite. Practically speaking, it is impossible to observe probabilities of all strings under the language model and so we must rely on a finite sample. Therefore, we replace \(\) with an empirical distribution with a finite support; in the following we show that the error induced by such an approximation is controlled. To estimate the MI we employ the method given in Algorithm 1; for generality it is presented for an arbitrary (pseudo) joint distribution \(\), but we keep in mind that our case of interest is \(=\). Note that most terms in the summations defining the product distribution \(^{}\) are zero (except the ones which correspond to the observed data). Adding \(_{1}\) and \(_{2}\) in the estimator \(_{k}(_{1},_{2})\) is intended to account for the total probability of missing observations, not included while constructing \(\) and \(^{}\), making sure the estimate is bounded.

The bias introduced by \((_{1},_{2})\) in the last equation allows us to rigorously bound the error in estimating \(I()\) via \(_{k}(_{1},_{2})\), which is explored next. In particular, in Theorem 5.6 we prove a high-probability lower bound on \(I()\) in terms of \(_{k}\). The core of controlling the estimation error is in accounting for the _missing mass_, or in other words, how much of \(\) we miss out by only observing a finite sample. In Appendix F, we present a more complete discussion and the proof of the bound on the estimation error for mutual information. Here we adapt this result to our particular case.

Define the missing mass as \(U_{k}=_{x^{n}}(x)\,\!\{x\{X_{1}, ,X_{k}\}\}\). Using this quantity, we are ready to present a non-asymptotic bound on the estimation error, which depends on the estimator \(_{k}(_{1},_{2})\), the expected missing mass, and the sample size:

**Theorem 5.6**.: _Suppose that \(_{k}(_{1},_{2})\) is given by Algorithm 1, and assume that \(\) is finite. For \(_{1}=1/(k\,|^{n}|)\), and \(_{2}\) satisfying \(_{2}_{1}+n(1-Z)\), with probability at least \(1-\), we have_

\[I()(1-_{k})\,_{k}(_{1},_{2})-( +(1+n\,(1+k\,||))\,_{k}) _{k}=[U_{k}]+)}{k}}\;.\]

_Furthermore, given \(_{}[0,1)\), let \(}^{n}\) such that \((}) 1-_{}\). Then, for \(_{1}=1/(k\,|}|)\), and \(_{2}\) satisfying \(_{2}_{1}+n(1-Z)\), with probability at least \(1-\), we have_

\[I()(1-_{k})\,_{k}(_{1},_{2})-( +(1+(1+k\,|}|))\,(_{}+_{k}))\;.\]

The theorem is a corollary of Theorem F.4 shown in Appendix F. Note that in Theorem 5.6 we consider two bounds. The first one is pessimistic in the sense that it does not expect that the samples carry much information about the support, and it is most suitable in situations where we expect \(\) to be spread out (uniformly) across its entire support. The price of not having samples covering the whole support inthis case is a factor \(n||\) appearing in the bound. For example, in case of a language model with \(10,000\) tokens, considering all possible strings of length \(T\) tokens yields \(n||=n\,T(10000)\), and so \(I()(1-_{k})\,_{k}(_{1},_{2})-(1/k+(1+n \,T(1+k\,(10000)))\,_{k})\). Arguably, in practice, such situations are rare, as in natural languages we will not encounter all possible strings. To this end, we consider an optimistic scenario where the _effective_ support of \(\), denoted by \(}\), is small with high probability. In this case, we can replace the size of the support for strings of length \(n\), \(||^{n}\), in the first bound with the effective support size \(|}|\), and we only pay essentially a factor \((1+k|}|)\) instead of \(n(1+k||)\). In case the effective sample size is only polynomial in \(n\), this leads to an exponential reduction in \(n\) for the second term in the bounds. In fact, in Appendix F.4 we demonstrate some empirical evidence that on two question-answering benchmarks, \(|}|\) rarely exceeds \( 100\) with \((}) 0.95\), while sampling responses from an LLM given a query.

Next we consider sufficient conditions for the estimator to converge to the mutual information. In particular, using the first bound in the theorem, we have (hiding logarithmic factors) \(I()=((1-[U_{k}])\,_{k}(_{1}, _{2})-[U_{k}])\) as \(k\). This tells us that the rate of estimation error is essentially controlled by the expected missing mass \([U_{k}]\), which, as we will see, converges to zero as \(k\), however the decay can be very slow in general. For example, it is known that for a finite support of size \(N\), \([U_{k}] e^{-}\) when \(k N\) and \([U_{k}] N/(e\,k)\) otherwise (Berend and Kontorovich, 2012). For countable distributions with entropy bounded by \(h\), one has \([U_{k}] h/(k)\).

Despite these pessimistic bounds, in reality we expect the expected missing mass to be significantly smaller, especially when \(\) is heavy-tailed. It is well-known that natural languages (and many artificial ones) follow a _Zipf_ distribution, where probability of each word (or a text piece) is proportional to \(1/()^{}\) for some exponent \(>1\), where \(()\) is a frequency of occurrence in the corpus (Piantadosi, 2014). Then, we expect that \([U_{k}]\) should be much smaller than in such a case, since sampling from the tail of Zipf distribution is a rare event. To this end, in Appendix F we show that if \(\) is Zipf with exponent \(>1\), then for any free parameter \(>0\), \([U_{k}]=(k^{-(-)})\). Hence, the rate at which the expected missing mass vanishes can be very fast (potentially matching a concentration rate \(1/\) for \(=2\)).

Finally in Appendix F.4 we present a data-dependent estimation of \([U_{k}]\) based on a concentration inequality for a missing mass and repetitive sampling from LLM, in the context of Q/A datasets showing that the expected missing mass is highly concentrated close to \(0\).

### Score-based hallucination tests

Let \(_{k}(,x)_{k}()\) computed as in Algorithm 1 for \(=\), to emphasize the explicit dependence on the query \(x\). The uncertainty estimate \(_{k}(,x)\) derived above can be used as a score indicating the strength of our belief that the LLM hallucinates for the given query \(x\). Such a score can then be used to design _abstention_ policies: if the response is deemed to be hallucinated, the system abstains from responding, while a response is provided otherwise. Score-based abstention methods usually compute a score chosen by the user (such as the response likelihood or the estimator \(()\) discussed earlier), and declare hallucination if the score is above or below a threshold, which is determined through calibration. To detect hallucinations successfully, the threshold can be adjusted through _calibration_ on a given task using a hold-out (ground-truth) sample, see, for instance, the paper of Yadkori et al. (2024) where this calibration is discussed in detail.

Given our estimated lower bound on the epistemic uncertainty, we can define an _abstention policy_ (a policy which decides when the LLM should abstain from prediction) as \(a_{}(x)=0\) if \(_{k}(_{1},_{2},x)<\) and \(a_{}(x)=1\) if \(_{k}(_{1},_{2},x)\), where \(>0\) is a threshold parameter tuned on a hold-out sample of some particular task. This policy abstains (\(a_{}(x)=1\)) when the epistemic uncertainty in the prediction (response) is large. When the policy does not abstain (\(a_{}(x)=0\)), any prediction from \(\) can be served. In the experiments, we compare a number of scoring functions for detecting hallucinations, including \(()\), the probability of the greedy (temperature zero) response, and an estimate of the entropy of the response distribution.

Experiments

In this section we evaluate our abstention method derived based on the MI estimate in Section 5.2 on a variety of closed-book open-domain question-answering tasks. In our experiments we either sweep through all abstention thresholds (Figure 4), or optimize the threshold on some calibration data, as explained in the description of the relevant experiment (Figure 5).

**Language model.** We used a Gemini 1.0 Pro model (Gemini Team, Google, 2023) to generate outputs and scores. Similar results were obtained with a - much smaller - Gemini 1.0 Nano-1 model, which are deferred to Appendix H.

**Datasets.** We consider three different datasets and their combinations: As base datasets, we consider _(i)_ a random subset of \(50,000\) datapoints from the TriviaQA dataset (Joshi et al., 2017), and _(ii)_ the entire AmbigQA dataset (with \(12038\) datapoints) (Min et al., 2020). These datasets mostly contain single-label queries, and only contain a few multi-label ones.5 Moreover, we created a multi-label dataset based on the WordNet dataset (Fellbaum, 1998): We extracted all (6015) datapoints from WordNet at depth \(4\) or more of the physical_entity subtree. For each datapoint (entity, children) in WordNet, we constructed a query of the form _"Name a type of entity."_ and children are considered target labels.

**Comparison of responses and computing the output distributions.** We use the F1 score thresholded at \(0.25\) to decide if two text sequences match. Additional details are provided in Appendix G.

**Baselines.** We consider abstention policies based on four scoring methods: _(i)_ the probability of the greedy response (denoted by \(T0\)); _(ii)_ the semantic-entropy method of Kuhn et al. (2023) whose score is the entropy of \(k=10\) generated samples (denoted by S.E.); _(iii)_ our proposed mutual information score as defined in Section 5 (and denoted by M.I.) with the choices of \(k=10\), \(n=2\), and \(_{1}=_{2}=0\) (the latter choice approximates the case that the number of potential responses can be very large in which case the theoretical choice of \(_{1}\) and \(_{2}\) would be very small); _(iv)_ the self-verification method of Kadavath et al. (2022) (denoted by S.V.). Additional details are provided in Appendix G.

**Results.** We consider the precision-recall (PR) trade-off for the various methods on the different datasets. Here, _recall_ is the percentage of queries where the method does not abstain, and _precision_ is the percentage of correct decisions among these queries.6 Figure 3(b) show PR-curves for the baselines and the proposed method on TriviaQA and AmbigQA. As can be seen, our method is better than the \(T0\) and S.V. baselines, but performs similarly to the S.E. method. This is because the TriviaQA and AmbigQA datasets contain mostly single-label queries, and therefore a first-order method such as S.E. is sufficient to detect hallucinations. The AmbigQA dataset contains a few multi-label queries, but upon closer inspection, we observe that the LLM has low entropy on most of these queries.Therefore, a first-order method can perform as well as our method on such queries. Our proposed method, as well as the baselines, make no mistakes on the WordNet dataset (as the prediction of the LLM is always correct), hence we omit those results. The S.V. baseline performs significantly worse than the other methods when the recall is not high (is below about 0.8).

The similar performance for the S.E. and M.I. methods shown in Figure 3(b) is due to the fact that the LLM has low entropy on most multi-label queries. However, ideally, an LLM should have higher entropy on multi-label queries (which would demonstrate broader knowledge, not focusing on a single possible answer). To include such queries, we mix the TriviaQA and AmbigQA datasets with our WordNet-based dataset with "truely" multi-label queries as constructed above. To enhance the intended effect, we filter our WordNet dataset by keeping only queries with entropy higher than \(0.7\) (approximately the entropy of the uniform distribution over two atoms). Then we have \(842\) remaining datapoints in WordNet. Note that when considered in isolation, both our proposed method and the semantic entropy method rarely make mistakes on this dataset. Then we create two new datasets by combining our \(842\) WordNet datapoints with \(842\) randomly selected datapoints from TriviaQA and AmbigQA, respectively, resulting in the TriviaQA+WordNet and AmbigQA+WordNet datasets. Figure 3(c) show PR-curves for the S.E. and M.I. methods on these two combined datasets. Apart from low recall values, the performance of the S.E. method degrades noticeably with the addition of extra multi-label data. This precision/recall curve might look somewhat strange (with precision sometimes increasing with recall); this is due to the fact that both methods are always correct on the large number of high-entropy WordNet queries, where the LLM's default predictions are correct.

The hardness with the combined datasets is that the predominantly single-label datasets (TriviaQA, AmbigQA) might need a different calibration threshold than the multi-label WordNet dataset, and this is better handled by our proposed method than by S.E. To better illustrate the improved abstention properties of our method, we examine how the two methods handle when the output of the LLM is diverse (i.e., has high entropy). In order to do this, we perform the following experiment: We create a calibration dataset by adding \(500\) random datapoints from the WordNet dataset to \(500\) random datapoints from TriviaQA, and another such random dataset for test. We determine the abstention thresholds on the calibration dataset for both the S.E. and the M.E. methods,7 and measure the performance (error rate, i.e., \(1\) minus precision, and recall) of the resulting abstention policies on the test set. We repeat this process \(10\) times and report mean values and 95% confidence intervals with Gaussian approximation. We perform a similar evaluation process for mixtures of AmbigQA and WordNet datasets. Figure 5 show that while the S.E. method has similar recall and error rates to those of the proposed method on low-entropy queries, its recall values are much lower for queries with higher entropy, while the M.E. method makes only few mistakes on these queries.