# Provable Adversarial Robustness

for Group Equivariant Tasks:

Graphs, Point Clouds, Molecules, and More

 Jan Schuchardt, Yan Scholten, Stephan Gunnemann

{j.schuchardt, y.scholten, s.guennemann}@tum.de

Department of Computer Science & Munich Data Science Institute

Technical University of Munich

###### Abstract

A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.

## 1 Introduction

Group equivariance and adversarial robustness are two important model properties when applying machine learning to real-world tasks involving images, graphs, point clouds and other data types:

Group equivariance is an ubiquitous form of task symmetry . For instance, we do not know how to optimally classify point clouds, but know that the label is not affected by permutation. We cannot calculate molecular forces in closed form, but know that the force vectors rotate as the molecule rotates. Directly enforcing such equivariances in models is an effective inductive bias, as demonstrated by the success of convolutional layers , transformers  and graph neural networks .

Adversarial robustness [5; 6; 7] is a generalized notion of model Lipschitzness: _A small change to a model's input \(x\) should only cause a small change to its prediction \(f(x)\)_. If a model is adversarially robust, then its accuracy will not be negatively affected by sensor noise, measurement errors or other small perturbations that are unavoidable when working with real-world data.

For the first time, we address the following question: _What is adversarial robustness in tasks that are group equivariant?_ This is necessary because the notions of input and output similarity used in prior work on adversarial robustness (e.g. \(_{p}\) distances) are not suitable for group equivariant tasks.

Fig. 1 illustrates why group equivariant tasks require rethinking input similarity. The right graph is constructed by modifying a large fraction of edges, meaning the perturbation of the adjacency matrix has a large \(_{0}\) norm. Prior work [8; 9; 10; 11; 12; 13; 14; 15; 16; 17] may deem it too large for a node classifiers' prediction to be robust. However, we know the graphs are isomorphic, meaning they are the same geometric object and should have the same set of labels. Fig. 2 illustrates why group equivariant tasks also require rethinking output similarity. Prior work considers a prediction robust if it remains (almost) constant. But when predicting trajectories from drone footage, we know that they should rotate as the drone rotates - even in the presence of camera noise. The predictions should explicitly not remain constant.

To address these two issues we propose a sound notion of adversarial robustness for group equivariant tasks that a) measures input similarity by lifting distance functions to group invariant distance functions and b) jointly considers transformations of the input and output space. This notion of adversarial robustness applies to arbitrary tasks, groups and distance functions.

A natural goal after introducing a novel notion of robustness is developing (provably) robust models. We show that robustness can be guaranteed by being robust under the traditional notion of adversarial robustness - if the model's equivariances match the equivariances of the task it is used for. Importantly, this implies that existing robustness guarantees may actually hold for significantly larger sets of perturbed inputs. For instance, proving the robustness of a graph neural network w.r.t. \(_{0}\) distance is in fact proving robustness w.r.t graph edit distance with uniform cost for insertion and deletion.

Although equivariant models make provable robustness more attainable, there are no certification procedures for many architectures. For instance, there is no prior work on proving robustness for rotation equivariant models. To close this gap, we develop the framework of equivariance-preserving randomized smoothing. It specifies sufficient conditions under which models retains their equivariances when undergoing randomized smoothing - a state-of-the-art approach for simultaneously increasing and proving the robustness of arbitrary models [18; 19; 20]. In addition to that, we generalize the aforementioned graph edit guarantees to arbitrary, user specified costs. Varying these costs allows for a fine-grained analysis of a model's robustness to graph perturbations.

To summarize, our core contributions are that we

* propose a sound notion of adversarial robustness for group equivariant tasks,
* show that using equivariant models facilitates achieving (provable) robustness,
* develop the framework of equivariance-preserving randomized smoothing,
* and generalize each existing graph and node classification robustness certificate for \(_{0}\) perturbations to graph edit distance perturbations with user-specified costs.

Overall, reconsidering what adversarial robustness means in equivariant tasks is an important prerequisite for future work at the intersection of robust and geometric machine learning.

## 2 Related work

There is little prior work that studies equivariance and adversarial robustness jointly. The work that exists [21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31] only considers group invariant classification (a special case of equivariance) and does not consider how a task's equivariances should influence what we consider robust.

**Model invariance and robustness.** Prior work mostly focuses on a trade-off between invariance and robustness in image classification, i.e., whether increasing robustness to rotation or translation decreases robustness to \(_{p}\) perturbations and vice-versa [21; 22; 23; 24; 25]. Schuchardt and Gunnemann  used knowledge about the invariances of point cloud classifiers to prove that they are constant within larger regions than could be shown using previous approaches.

**Group invariant distances.** Recently, stability results for graph classifiers under isomorphism invariant optimal transport distances have been derived [27; 28; 29]. For point cloud classifiers, using the permutation invariant Chamfer or Hausdorff distance to craft attacks has been proposed [30; 31]. These works only focus on invariance and specific domains and do not consider that distances should be task-dependent: A rotation invariant distance for images may be desirable when segmenting cell nuclei, but not when classifying hand-written digits, since it would fail to distinguish \(6\) and \(9\).

**String edit distance.** In concurrent work, Huang et al.  use randomized smoothing to prove robustness of classifiers w.r.t. string edit distance, i.e., the number of substitutions that are needed to convert one string from alphabet \(\{\}\) into another, up to insertion of alignment tokens \(\). Their work further emphasizes the need for invariant distance functions in domains with symmetries, and the usefulness of randomized smoothing for proving robustness w.r.t. such distances.

**Robustness of models with equivariances.** Aside from work that studies invariance and adversarial robustness jointly, there is a rich literature investigating the robustness of models that happen to have equivariances. This includes convolutions [5; 6; 7; 33], transformers[34; 35; 36; 37], point cloud models [38; 39; 40; 31; 30; 41; 42; 43; 44; 30; 45; 31] and graph neural networks [8; 9; 10; 11; 12; 13; 14; 45; 46]. The models are however treated as a series of matrix multiplications and nonlinearities, without accounting for their equivariances or the equivariances of the tasks they are used for. Nevertheless, many methods can actually be reused for proving (non-)robustness under our proposed notion of adversarial robustness (see Section 5).

**Transformation-specific robustness.** A subfield of robust machine learning focuses on robustness to unnoticeable parametric transformations (e.g. small rotations) [42; 46; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65]. These works implicitly assume that large transformations lead to easily identifiable out-of-distribution samples. This is not the case with equivariant tasks: For instance, a molecule rotated by \(180^{}\) is still the same geometric object. Furthermore, they do not consider unstructured perturbations. Nevertheless, transformation-specific robustness can be framed as a special case of our proposed notion (see Appendix J).

**Semantics-aware robustness.** Our work is closely related to different proposals to include ground truth labels in the definition of adversarial robustness [22; 66; 67; 68; 69; 70; 71]. A problem is that the ground truth is usually unknown, which limits experimental evaluation to simple data generating distributions [52; 70] or using human study participants [71; 22; 67]. Geisler et al.  overcome this problem in the context of neural combinatorial optimization by using adversarial perturbations that are known to change the ground truth of a decision problem. Group equivariant tasks admit a similar approach, since we know how the ground truth changes for specific input transformations.

## 3 Background

**Group theory and equivariance.** Discussing group equivariance requires a few algebraic concepts. A group is a set \(\) with identity element \(e\) and associative operator \(:\) such that \( g:e g=g e=g\) and each \(g\) has an inverse element \(g^{-1}\) with \(g^{-1} g=g g^{-1}=e\). We are interested in transformations with a group structure, such as rotations. Given set \(\), a (left) group action is a function \(_{}:\) that transforms elements of \(\) and preserves the structure of \(\), i.e. \(g_{}h_{}x=(g h)_{}x\). For instance, rotation by \(^{}\) and then by \(^{}\) is the same as rotation by \((+)^{}\). A group may act differently on different sets. For example, rotation group \(SO(3)\) may act on point clouds via matrix multiplication while acting on class labels via the identity function. When clear from context, we drop the subscripts. A function \(f:\) is equivariant if each action on its input is equivalent to an action on its output, i.e. \( x,g:f(g_{}x)=g_{ }f(x)\).

**Adversarial robustness** means that any small change to clean input \(x\) only causes a small change to prediction \(f(x)\), i.e. \(_{x^{}_{x}}d_{}(f(x),f(x^{}))\) with \(_{x}=\{x^{} d_{}(x,x^{})\}\), and \(d_{}\) and \(d_{}\) quantifying input and output distance. We refer to the set of admissible perturbed inputs \(_{x}\) as the _perturbation model_. A common special case is \(d_{}(y,y^{})=[y y^{}]\) and \(=0\)[5; 6; 7]. Other forms of robustness involve training data [73; 74], but we focus on test-time perturbations.

**Randomized smoothing**[18; 19; 20] is a paradigm for increasing and proving the robustness of models in an architecture-agnostic manner. It works by randomizing the inputs of a _base model_\(h\) to construct a more robust _smoothed model_\(f\). While originally proposed for provably robust image classification,it has evolved into a much more general framework that can be applied to various domains and tasks [14; 75; 76; 77; 78; 51; 20; 51; 79; 80; 81]. Consider a measurable input space \((,)\) and a measurable base model \(h:\) that maps to a measurable intermediate space \((,)\) (e.g. logits). Given the base model \(h\) and a family of probability measures \((_{x})_{x}\) on \((,)\) indexed by \(\) (e.g. Gaussians with mean \(x\)), we can define the input-dependent pushforward measure \(_{x} h^{-1}\) for any input \(x\).1 We can further define a _smoothing scheme_\(:(,)\) that maps from probability measures \((,)\) on the intermediate space to an output space \(\) (e.g. logit distributions to labels).2 This lets us construct the smoothed model \(f(x)=(_{x} h^{-1})\), which makes a prediction for input \(x\) based on some quantity of the input-dependent pushforward measure \(_{x} h^{-1}\) (e.g. the expected value). Intuitively, if two inputs \(x\) and \(x^{}\) are sufficiently similar, then the smoothing measures \(_{x}\) and \(_{x}^{}\) will have a large overlap, thus leading to similar smoothed predictions. The combination of measures \((_{x})_{x}\) and smoothing scheme \(\) determines for which \(d_{}\) and \(d_{}\) robustness is guaranteed. We discuss these combinations in more detail in Appendix D.

## 4 Redefining robustness for group equivariant tasks

For the first time, we seek to define adversarial robustness for group equivariant tasks. By task we mean an unknown function \(y:\) that is equivariant with respect to the action of a group \(\). We assume that it is approximated by a (learned) model \(f:\) whose robustness we want to determine. The model does not have to be equivariant. Like traditional adversarial robustness, we assume there are functions \(d_{}:_{+}\) and \(d_{}:_{+}\), which define what constitutes a small change in domain \(\) and co-domain \(\), if we are not concerned with group symmetries. For instance, \(_{2}\) distance is a natural notion of similarity between Euclidean coordinates.

For ease of exposition, we further assume that all considered optimization domains are compact (so that minima and maxima exist) and that group \(\) acts isometrically on \(\), i.e. \( x,x^{}, g:d_{}(g  x,g x^{})=d_{}(x,x^{})\). This covers most practically relevant use cases. For completeness, we discuss non-compact sets and non-isometric actions in Appendices H and I.

### Perturbation model for group equivariant tasks

Our first goal is to resolve the shortcomings of using typical input distance functions \(d_{}\) to define what constitutes small input perturbations in group equivariant tasks. We seek some improved function \(_{}\) that accounts for the equivariance of \(y\) and simultaneously captures the similarity of objects in domain \(\), as defined by original distance function \(d_{}\). To this end, we define three desiderata.

**Desideratum 1**.: We know that any perturbed \(x^{}\) and \(g x^{}\) with \(g\) are up to symmetry the same geometric object with the same semantic content, i.e. \(y(g x^{})=g y(x^{})\). But as illustrated in Fig. 1, group actions may cause a drastic change w.r.t. distance \(d_{}\). Thus, even if \(f(x)\) is "robust" within \(_{x}=\{x^{} d_{}(x,x^{})\}\) for some \(\), there may still be a \(x^{}_{x}\) and \(g x^{}_{x}\) that lead to two completely different predictions. If a prediction can be altered without changing the semantic content of an input, it can hardly be considered robust. This problem cannot be resolved by requiring robustness for very large \(\) so that \(_{x}\) covers all symmetric objects. Doing so would also include objects with substantially different semantic content from clean input \(x\), which should actually lead to different predictions. Instead, we need a \(_{}\) that is constant for all symmetric forms of \(x^{}\), so that robustness to one implies robustness to all: \( x,x^{},g:d_{}(x,g x ^{})=_{}(x,x^{})\).

**Desideratum 2**.: While the first desideratum accounts for equivariance, a model should also be robust to sensor noise, measurement errors and other small perturbations that are not necessarily group actions. Therefore, elements that are close with respect to the original distance function \(d_{}\) should remain close under our new distance function \(_{}\), i.e. \( x,x^{}:_{}(x,x^{}) d_{}(x,x^{})\).

**Desideratum 3**.: The first two desiderata could be fulfilled by \(_{}(x,x^{})=0\) or a function that arbitrarily changes the ordering of inputs w.r.t. distance. To appropriately capture similarity, \(_{}\) should not only underapproximate \(d_{}\), but preserve it as best as possible. Let \(\) be the set of functions from \(\) to \(_{+}\) that fulfill the first two desiderata. We require that \(_{}(x,x^{})=_{}(x,x^{ })\).

**Proposition 1**.: _A function \(_{}:_{+}\) that fulfills all three desiderata for any original distance function \(d_{}:_{+}\) exists and is uniquely defined: \(_{}(x,x^{})=_{g}d_{}(x,g  x^{})\)._

We prove this result in Appendix C. We refer to \(_{}\) as the _action-induced distance_. It is the distance after optimally aligning the perturbed input \(x^{}\) with the clean input \(x\) via a group action.

**Example: Point cloud registration distance.** Let \(=^{N D}\). Consider \(=S_{N} SE(D)\), the set of permutation matrices, rotation matrices and translation vectors. Let \(\) act via \((,,)= (^{T}+_{N} ^{T})\) and let \(d_{}\) be the Frobenius distance. Then \(_{}\) is the _registration distance_, i.e. the distance after finding an optimal correspondence and applying an optimal rigid transformation.

**Example: Graph edit distance.** Let \(\) be the set of all adjacency matrices \(\{0,1\}^{N N}\) with \(d_{}(,^{})=\|-^{}\|_{0}\). Consider \(=S_{N}\), the set of permutation matrices. Let \(\) act via \(= ^{T}\). Then \(_{}(,^{})\) is the _graph edit distance_ with uniform cost . That is, the number edges that have to be inserted or deleted to transform \(\) into a graph isomorphic to \(^{}\).

As demonstrated by these examples, we do not claim to have invented this notion of distance for equivariant domains. Our contribution is justifying, in a principled manner, why this notion of distance should be used to define adversarial robustness for group equivariant tasks.

**Perturbation model.** Now that we have an appropriate input distance for group equivariant tasks, we can use it to define the set of admissible perturbed inputs \(_{x}\) as \(\{x^{}_{g}d_{}(x,g x^{}) \}\) with some small \(_{+}\). Because every \(g\) has an inverse element \(g^{-1}\), this set is identical to \(\{g x^{} g,d_{}(x,x^{})\}\). We use this equivalent representation because it lets us disentangle group actions from other perturbations. That is, \(f(x)\) should be robust to inputs that can be generated via a small perturbation w.r.t. the original distance \(d_{}\) followed by a group action.

### Output distance for group equivariant tasks

Fig. 2 illustrates that we also need to reconsider what a small change to a model's prediction is. Letting a group element act on a prediction may cause a large change w.r.t. \(d_{}\). In particular, we may have \(d_{}(y(x),y(g x))=d_{}(y(x),g y(x)) 0\), even though \(x\) and \(g x\) have a distance of zero w.r.t. the action-induced distance. Thus, we would consider even the ground truth \(y\) itself to be non-robust. Using action-induced distances, i.e. \(_{g}d_{}(y,g y^{})\), is not appropriate either. Action-induced distances cannot distinguish between a model that transforms its predictions in compliance with the ground truth and one that applies arbitrary group actions.

To appropriately measure output distance, we need to account for differences between clean prediction \(f(x)\) and perturbed prediction \(f(g x^{})\) that are caused by the specific group element \(g\) acting on the model's input. To do so, we need to first revert the effect of the group action before comparing the predictions. That is, we need to measure output distance using \(d_{}(f(x),g^{-1} f(g x^{}))\).

### Proposed definition of robustness for group equivariant tasks

Combining the perturbation model from Section 4.1 with the output distance from Section 4.2 leads to the following definition of adversarial robustness for group equivariant tasks:

**Definition 1**.: _Assume that ground truth function \(y:\) is equivariant with respect to the action of group \(\). Then, a prediction \(f(x)\) for clean input \(x\) is \((,d_{},d_{},,)\)-equivariant-robust if_

\[(_{x^{}}_{g}d_{}(f(x),g^{ -1} f(g x^{}))d_{}(x,x^{})).\] (1)

Simply speaking, prediction \(f(x)\) should be considered robust if it is robust to unnoticeable perturbations and is approximately equivariant around \(x\). For perturbations of size \(\) w.r.t. the original \(d_{}\), the prediction should not change by more than \(\) and no group action should cause this error to increase beyond \(\). Note that Definition 1 depends on the equivariances of the task \(y\), not those of the model \(f\). Further note that the constraint involves original distance \(d_{}\), not action-induced distance \(_{}\).

**Special cases.** If the task is not equivariant, i.e. \(=\{e\}\), we recover the traditional notion of adversarial robustness because \(e\) acts via identity. For \(=0\), constant \(\) is a bound on the equivariance error, a common evaluation metric in geometric machine learning (see, e.g. ).

**Other invariant distances.** As discussed in Section 2, alternative invariant distances have been proposed for certain group invariant classification tasks. This includes Hausdorff and Chamferdistance [30; 31], as well as optimal transport for graphs [27; 28; 29]. Any group invariant function \(d_{}\) is preserved by the action-induced distance: \(_{g}d_{}(x,g x^{})=_{g}d_{}(x,x^{})=d_{}(x,x^{})\). Thus, all previous results for invariant classification are compatible with our notion of robustness.

**Local budgets and local robustness.** For data that is composed of \(N\) distinct elements, such as point clouds, one may want local budgets \(_{1},,_{N}\). For tasks that involve \(M\) distinct predictions, such as segmentation, one may only be interested in the robustness of some subset of predictions. Definition 1 can be extended to accommodate local budgets and robustness, see Appendix G.

## 5 Achieving provable robustness for group equivariant tasks

Now we have a sound notion of robustness that overcomes the limitations discussed in Section 1. But it is not clear how to achieve provable robustness. Given a task \(y:\) that is equivariant with respect to the action of a group \(\), we want a model \(f:\) and a corresponding algorithm that can verify the \((,d_{},d_{},,)\)-equivariant-robustness of a prediction \(f(x)\).

A challenge is that the action-induced distance \(_{}\), which defines our perturbation model and thus underlies the optimization domain in Eq. (1), is generally not tractable. For example, deciding whether the graph edit distance is smaller than some \(\) is NP-hard . A solution would be to relax the optimization domain in order to pessimistically bound the model's actual robustness. This is, in essence, the approach taken by works on robustness to graph optimal transport perturbation models (e.g. [27; 28]). Instead of optimizing over discrete correspondences between objects, they optimize over couplings that define soft correspondences. There is however a more straight-forward solution that lets us take advantage of years of research in geometric and robust machine learning: Applying the principles of geometric machine learning and using a model with matching equivariances.

**Proposition 2**.: _Consider a model \(f:\) that is equivariant with respect to the action of a group \(\). Any prediction \(f(x)\) is \((,d_{},d_{},,)\)-equivariant-robust if and only if it is \((\{e\},d_{},d_{},,)\)-equivariant-robust, i.e. fulfills traditional adversarial robustness._

Proof.: Because \(f\) is equivariant, we have \( g:g^{-1} f(g x^{})=g^{-1} g f(x^{ })=f(x^{})=e^{-1} f(e x^{})\) and thus \(d_{}(f(x),g^{-1} f(g x^{}))=d_{}( f(x),e^{-1} f(e x^{}))\). 

By using model \(f\) with the same equivariances as ground truth \(y\), we reduce the problem of proving robustness to that of proving traditional adversarial robustness i.e. \(=\{e\}\). Again, note that robustness does not mean that a prediction remains constant, but that it transforms in compliance with semantics-preserving transformations of the input, even under small perturbations (see Eq. (1)).

**Discussion.** Proposition 2 provides another strong argument for the use of geometric machine learning models. These models facilitate the problem of achieving provable robustness for group equivariant tasks -- without relaxing the action-induced distance and thus weakening our guarantees. We can instead build upon existing robustness certification procedures for equivariant models, such as transformers, PointNet and graph convolutional networks, under traditional perturbation models, like \(_{2}\) or \(_{0}\) perturbations [8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46]. We just need to use our knowledge about the equivariances of tasks and models to reinterpret what is actually certified by these procedures.

**Relation to orbit-based certificates.** A related result was discussed for group _invariant_ point cloud classifiers in : If \(=^{N D}\) and classifier \(f\) is constant within Frobenius norm ball \(\) and invariant w.r.t. the action of group \(\), then it is also constant within \(\{g^{}^{},g\}\). However, this work did not discuss whether being constant within this set is desirable, how it relates to task invariance and what this result tells us about the adversarial robustness of the classifier.

**Adversarial attacks.** While we focus on provable robustness, Proposition 2 also applies to adversarial attacks, i.e. proving non-robustness via counterexample. Even when the equivariances of \(y\) and \(f\) do not match, traditional attacks are feasible solutions to Eq. (1) since they amount to constraining \(g\) to set \(\{e\}\). For completeness, we perform experiments with adversarial attacks in Appendix A.4.

### Equivariance-preserving randomized smoothing

Equivariant models reduce the problem of proving robustness for group equivariant tasks to that of proving traditional robustness. However, specialized procedures to make these proofs are only available for a limited range of architectures. For example, specialized certification procedures for point cloud models are limited to the PointNet architecture , and there are no procedures to prove the robustness of models with continuous equivariances, such as rotation equivariance.

For such models, one could try to apply randomized smoothing (recall Section 3). By choosing a suitable smoothing scheme \(\) and measures \((_{x})_{x}\) for distances \(d_{}\) and \(d_{}\), one can transform any base model \(h\) into smoothed model \(f\) and prove its \((\{e\},d_{},d_{},,)\)-equivariant-robustness. However, base model \(h\) having the same equivariances as task \(y\) does not guarantee that smoothed model \(f\) has the same equivariances. Thus, one can generally not use Proposition 2 to prove \((,d_{},d_{},,)\)-equivariant-robustness. We propose the following sufficient condition for verifying that a specific smoothing scheme and family of measures are equivariance-preserving (proof in Appendix E.1).

**Proposition 3**.: _Assume two measurable spaces \((,)\). \((,)\), an output space \(\) and a measurable base model \(h:\) that is equivariant with respect to the action of group \(\). Further assume that \(\) acts on \(\) and \(\) via measurable functions. Let \(:(,)\) be a smoothing scheme that maps from the set of probability measures \((,)\) on intermediate space \((,)\) to the output space. Define \(_{,g}()\) to be the group action on set \(\) for a fixed \(g\), i.e. \(_{,g}(x)=g_{}x\). Then, the smoothed model \(f(x)=(_{x} h^{-1})\) is equivariant with respect to the action of group \(\) if both_

* _the family of measures_ \((_{x})_{x}\) _is equivariant, i.e._ \( x,g:_{g x}=_{x}_{ ,g}^{-1}\)_,_
* _and smoothing scheme_ \(\) _is equivariant, i.e._ \((,),g:(_{,g}^{-1})=g()\)_._

Note that \(\) is a composition of functions, whereas \(\) is a group action. The intuition behind Proposition 3 is that, if family of measures \((_{x})_{x}\), and base model \(h\), and smoothing scheme \(\) are equivariant, then we have a chain of equivariant functions which is overall equivariant. We provide a visual example of such an equivariant chain of functions in Fig. 31.

In the following, we show that various schemes and measures preserve practically relevant equivariances and can thus be used in conjunction with Proposition 2 to prove \((,d_{},d_{},,)\)-equivariant-robustness. For the sake of readability, we provide a high-level discussion, leaving the formal propositions in Appendix E.2. We summarize our results in Tables 1 and 2. In Appendix D, we discuss how to derive robustness guarantees for arbitrary combinations of these schemes and measures.

**Componentwise smoothing schemes.** The most common type of smoothing scheme can be applied whenever the intermediate and output space have \(M\) distinct components, i.e. \(=^{M}\) and \(=^{M}\) for some \(,\). It smooths each of the \(M\) base model outputs independently. This includes majority voting , expected value smoothing  and median smoothing , which can be used for tasks like classification, segmentation, node classification, regression, uncertainty estimation and object detection . Such schemes preserve equivariance to groups acting on \(\) and \(\) via permutation. Note that \(\) need not be the symmetric group \(S_{N}\) to act via permutation. For instance, the identity function is a permutation, meaning componentwise smoothing schemes can be used to prove robustness for arbitrary group invariant tasks. See Proposition 4.

**Expected value smoothing scheme.** When the intermediate and output space are real-valued, i.e. \(==^{M}\), one can make predictions via the expected value . Due to linearity of expectation, this scheme does not only preserve permutation equivariance but also equivariance to affine transformations. However, certifying the smoothed predictions requires that the support of the output distribution is bounded by a hyperrectangle, i.e. \((_{x} h^{-1})\{^{M}  m:a_{m} y_{m} b_{m}\}\) for some \(,^{M}\). See Proposition 5.

**Median smoothing scheme.** When the support of the model's output distribution is real-valued and potentially unbounded, one can make smoothed predictions via the elementwise median . This scheme does not only preserve permutation equivariance but also equivariance to elementwise linear transformations, such as scaling. See Proposition 6.

**Center smoothing scheme.** Center smoothing  is a flexible scheme that can be used whenever \(d_{}\) fulfills a relaxed triangle inequality. It predicts the center of the smallest \(d_{}\) ball with measure of at least \(\). Center smoothing has been applied to challenging tasks like image reconstruction, dimensionality reduction, object detection and image segmentation . We prove that center smoothing is equivariant to any group acting isometrically w.r.t. \(d_{}\). For example, when \(==^{M D}\) and \(d_{}\) is the Frobenius norm, center smoothing guarantees \((,d_{},d_{},,)\)-equivariant-robustness for any group acting on \(^{N D}\) via permutation, rotation, translation or reflection. See Proposition 7.

**Product measures.** Many randomized smoothing methods use product measures. That is, they use independent noise to randomize elements of an \(N\)-dimensional input space \(=^{N}\). The most popular example are exponential family distributions, such as Gaussian, uniform and Laplacian noise, which can be used to prove robustness for various \(_{p}\) distances [19; 20; 94; 95], Mahalanobis distance [60; 96] and Wasserstein distance . Product measures preserve equivariance to groups acting via permutation. Again, group \(\) need not be symmetric group \(S_{n}\) to act via permutation. For instance, rotating a square image by \(90^{}\) (see Fig. 2) is a permutation of its pixels. See Proposition 8.

**Isotropic Gaussian measures.** Isotropic Gaussian measures are particularly useful for \(=^{N D}\). They guarantee robustness when \(d_{}\) is the Frobenius distance [43; 46] and preserve equivariance to isometries, i.e. permutation, rotation, translation and reflection. Combined with Proposition 2, this guarantees robustness w.r.t. the aforementioned point cloud registration distance. See Proposition 9.

**Transformation-specific measures.** A standard tool for proving transformation-specific robustness (see Section 2) is transformation-specific smoothing [60; 63; 64; 65; 66], i.e. applying randomly sampled transformations from a parametric family \((_{})_{}\) with \(_{}:\) to the inputs of a model. If all \(_{}\) with \(\) are equivariant to the actions of group \(\), then transformation-specific smoothing preserves this equivariance. For example, random scaling  preserves rotation equivariance and additive noise (e.g. Gaussian) preserves translation equivariance for \(=^{N D}\). See Proposition 10.

**Sparsity-aware measures.** Sparsity-aware noise  can be applied to discrete graph-structured data to guarantee robustness to edge and attribute insertions and deletions. We prove that sparsity-aware measures preserve equivariance to groups acting via graph isomorphisms. As we discuss in the next section, this guarantees robustness w.r.t. the graph edit distance. See Proposition 11.

### Deterministic edit distance certificates for graph and node classification

Besides sparsity-aware smoothing, there are also deterministic procedures for proving robustness for specific graph neural networks, namely graph convolutional networks  and APPNP . They are however limited to uniform costs. To be more specific, let \(\) be the set of all graphs \(\{0,1\}^{N D}\{0,1\}^{N N}\) with \(N\) nodes and \(D\) binary attributes and let \(()_{+}=(,0)\) and \(()_{-}=(,0)\) with elementwise maximum and minimum. Define \(d_{}((,),(^{},^{}))\) as

\[c_{}^{+}||(^{}-)_{+}||_{0}+c_{}^{-}|| (^{}-)_{-}||_{0}+c_{}^{+}||(^{}-)_{+}||_{0}+c_{}^{-}||(^{}-)_{-}||_{0},\] (2)

with costs \(c_{}^{+},c_{}^{-},c_{}^{+},c_{}^{-}\) for insertion and deletion of attributes and edges. Prior work can only prove robustness for costs in \(\{,1\}\), i.e. disallow certain types of perturbations and use uniform cost for the remaining ones. In Appendix F we generalize each existing deterministic graph and node classification guarantee to non-uniform costs. This includes procedures based on convex outer adversarial polytopes , policy iteration , interval bound propagation , bilinear programming , and simultaneous linearization and dualization . Our proofs mostly require solving different knapsack problems with local constraints and only two distinct costs - which can be done efficiently via dynamic programming or linear relaxations with analytic solutions (see Appendix F.1). As per Proposition 2, proving \((\{e\},d_{},d_{},,)\)-equivariant-robustness of isomorphism equivariant models in this way proves \((S_{N},d_{},d_{},,)\)-equivariant-robustness. Thus, these procedures guarantee robustness w.r.t graph edit distance \(d_{}((,),(^{},^{}))=_{ S_{N}}d_{}((,),(^{},^{ }^{T}))\).

### Limitations

Group equivariance covers various common task symmetries. However, there are symmetries that do not fit into this framework, such as local gauge equivariance [100; 101; 102] or wave function symmetries [103; 104; 105; 106; 107; 108; 109; 110; 111; 112]. A limitation of relying on group equivariant models for provable robustness is that there are tasks where non-equivariant models have better empirical performance, for example vision transformers . Models that are in principle equivariant may also lose their equivariance due to domain-specific artifacts like image interpolation. Finally, it should be noted that providing guarantees of the form \((,d_{},d_{},,)\) requires a-priori knowledge that the task is equivariant to group \(\). These limitations are however not relevant for many important domains in which equivariant models are the de-facto standard (e.g. graphs, point clouds and molecules).

## 6 Experimental evaluation

In the following, we demonstrate how sound robustness guarantees for tasks with discrete and continuous domains and equivariances can be obtained via equivariance-preserving randomized smoothing.

We additionally evaluate our graph edit distance certificates and how the cost of edit operations affects provable robustness. All experimental details are specified in Appendix B. Certificates are evaluated on a separate test set. Randomized smoothing methods are evaluated using sampling and hold with high probability. Each experiment is repeated \(5\) times. We visualize the standard deviation using shaded areas. When the shaded area is too small, we report its maximum value. An implementaton will be made available at https://cs.cit.tum.de/daml/equivariance-robustness.

**Point cloud classification.** The first task we consider is point cloud classification, i.e. \(=^{N 3}\) and \(=\{1,,K\}\). Natural distances \(d_{}\) and \(d_{}\) are the Frobenius distance and 0-1-loss. The task is permutation invariant, i.e. symmetric group \(=S_{N}\) acts on \(\) via permutation and on \(\) via identity. Thus, we need to prove robustness w.r.t. to the correspondence distance \(_{}\), i.e. the Frobenius distance after finding an optimal matching between rows via permutation. To preserve invariance and use Proposition 2 while randomly smoothing, we use Gaussian measures and majority voting, i.e. we predict the most likely label under Gaussian input perturbations. As invariant base models \(h\) we choose PointNet  and DGCNN , which are used in prior work on robust point cloud classification . We evaluate the robustness guarantees for smoothing standard deviation \(=0.2\) on ModelNet40 . This dataset consists of point cloud representations of CAD models from \(40\) different classes. Fig. 4 shows the certified accuracy, i.e. the percentage of predictions that are correct and provably robust. Both smoothed models have a high accuracy above \(80\%\), but DGCNN has significantly higher provable robustness. In Appendix A.1 we repeat the experiment for different values of \(\) and compare our guarantees to those of 3DVerifier .

**Molecular force prediction.** Next, we consider a task with continuous equivariances: Predicting the forces acting on each atom in a molecule, which can for instance be used to simulate molecular dynamics. We have \(=^{N 3}\) (atomic numbers are treated as constant) and \(=^{N 3}\). Suitable distances \(d_{}\) and \(d_{}\) are the Frobenius distance and the average \(_{2}\) error \(_{n=1}^{N}||_{n}-_{n}^{}||_{2}\ /\ N\). The task is permutation, rotation and translation equivariant, i.e. \(=S_{N}}(3)\). Thus, \(_{}\) is the point cloud registration distance. To preserve equivariance, we use Gaussian measures and center smoothing. We choose DimeNet++  as equivariant base model \(h\). We evaluate the provable robustness for smoothing standard deviation \(=1\,\) on MD17 , a collection of \(8\) datasets, each consisting of a large number of configurations of a specific molecule. With just \(1000\) training samples, the models achieve low average test errors between \(0.19\) and \(0.74\,\). We use \(1000\) samples per test set to evaluate the robustness guarantees. Fig. 4 shows the average upper bounds \(\) on the change of the predicted force vectors for small perturbations between \(0\) and \(2\,\). These average \(\) are smaller than the average test errors by factors between \(2\) and \(13\). The standard deviation across seeds is below \(3 10^{-4}\,\) for all \(\). In Appendix A.2 we show that the maximum \(\) and \(\) grow approximately linearly with \(\) and repeat our experiments with SchNet  and SphereNet  base models. Note that we are not (primarily) proving robustness to malicious perturbations, but robustness to measurement errors or errors in previous simulation steps.

**Node classification.** Finally, we consider a task with discrete domain and co-domain: Node classification, i.e. \(=\{0,1\}^{N D}\{0,1\}^{N N}\) and \(=\{1,,K\}^{N}\). Distance \(d_{}\) is naturally defined via a weighted sum of inserted and deleted bits (see Eq. (2)). Output distance \(d_{}\) is the \(_{0}\) distance, i.e. the number of changed predictions. The task is equivariant to symmetric group \(S_{n}\) acting on \(\) and \(\) via isomorphisms and permutations, respectively. To preserve equivariance, we use sparsity-aware measures and majority voting. The flip probabilities (see Appendix F.7) are set to \(p_{}^{+}=0,p_{}^{-}=0,p_{}^{+}=0.001,p_{}^{-}=0.8\). We use a \(2\)-layer graph convolutional network  as our isomorphism equivariant base model \(h\). Fig. 6 shows the resulting guarantees on Cora ML  for graph edit perturbations of the adjacency, i.e. \(c_{}^{+}=c_{}^{-}=\), for varying costs \(c_{}^{+}\) and \(c_{}^{-}\). We observe that increasing the cost for edge insertions significantly increases the provable robustness for the same budgets \(\), whereas the cost for edge deletions has virtually no effect. This suggests that insertions are much more effective at changing a model's predictions, which was empirically observed in prior work . We repeat the experiment with other flip probabilities in Appendix A.3. The standard deviation of certified accuracies across all seeds was below \(2.1\,\) everywhere. Note that here, certified accuracy does not refer to the percentage of predictions that are correct and constant, but those that are correct and permute in compliance with isomorphisms of the input graph.

**Deterministic edit distance certificates.** In Fig. 6 we repeat the node classification experiment with our generalization of the convex polytope method from . We consider feature perturbations, i.e. \(c_{}^{+}=c_{}^{-}=\), for varying \(c_{}^{+}\) and \(c_{}^{-}\). Again, the cost of insertions has a larger effect, suggesting that the model is less robust to them. In Appendix A.3 we repeat the experiments with the four other graph edit distance certificates and also evaluate them on Citeseer  and the TUDataset . Overall, our generalizations of existing graph robustness guarantees let us prove robustness to more complex threat models, even though evaluating the edit distance is computationally hard.

## 7 Conclusion

The main goal of this paper is to define adversarial robustness for group equivariant tasks. To this end, we introduce action-induced distances as the appropriate notion of input distance and consider how predictions should transform for semantics-preserving transformations of inputs. If the equivariances of a task and model match, the proposed notion of robustness can be guaranteed by proving traditional adversarial robustness. This has two consequences: Firstly, specialized certification procedures for equivariant architectures can be reused. One just has to reinterpret what is actually guaranteed by these procedures, e.g. robustness to perturbations bounded by graph edit distance. Secondly, randomized smoothing can be used to provide architecture-agnostic guarantees -- but only if the smoothing scheme and measures preserve the model's equivariances. We experimentally demonstrated the generality of this equivariance-preserving randomized smoothing approach by certifying robustness for graph, point cloud and molecule models. Overall, our work provides a sound foundation for future work at the intersection of robust and geometric machine learning.

**Future work.** Based on Proposition 2, a direction for future work is to continue making equivariant models more robust to classic threat models, without caring about equivariance. A more interesting direction is to make attacks, defenses and certificates equivariance-aware. For instance, knowledge about model equivariances could be used to reduce the search space for attacks, disrupt gradient-based attacks (similar to ), or derive stronger randomized smoothing guarantees (as proposed for invariant models in ). Finally, developing procedures to certify non-equivariant models (e.g. vision transformers) or models that are only "almost equivariant" (e.g. due to interpolation artifacts) under the proposed notion of robustness would be desirable for equivariant computer vision tasks.

Acknowledgments and disclosure of funding

The authors would like to thank Hongwei Jin for assistance with the implementation of their topology attack certificates, Tom Wollschlager for providing access to code for training molecular force models, and Aman Saxena for valuable discussions concerning non-compact sets and non-isometric actions. This work has been funded by the Munich Center for Machine Learning, by the DAAD program Konrad Zuse Schools of Excellence in Artificial Intelligence (sponsored by the Federal Ministry of Education and Research), and by the German Research Foundation, grant GU 1409/4-1. The authors of this work take full responsibility for its content.