# Supplementary Material:

Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities

 Alexander Nikitin\({}^{1}\)  Jannik Kossen\({}^{2}\)  Yarin Gal\({}^{2}\)  Pekka Marttinen\({}^{1}\)

\({}^{1}\) Department of Computer Science, Aalto University

\({}^{2}\) OATML, Department of Computer Science, University of Oxford

alexander.nikitin@aalto.fi

###### Abstract

Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's _semantic uncertainty_, i.e., the uncertainty over the _meanings_ of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness. To address this problem, we propose _Kernel Language Entropy_ (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the _semantic similarities_ of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.

## 1 Introduction

Large Language Models (LLMs) have demonstrated exceptional capabilities across a wide array of natural language processing tasks . This has led to their application in many domains, including medicine , education , and software development . Unfortunately, LLM generations suffer from so-called hallucinations, commonly defined as responses that are "nonsensical or unfaithful to the provided source content" . Hallucinations pose significant risks when LLMs are deployed to high-stakes applications, and methods that reliably detect them are sorely needed.

A promising direction to improve the reliability of LLMs is _estimating the uncertainty_ of model generations . For instance, high predictive uncertainty is indicative of model errors or hallucinations in settings such as answering multiple-choice questions . This allows us to prevent harmful outcomes by abstaining from prediction or by consulting human experts. However, the best means of estimating uncertainty for free-form natural language generation remains an active research question. The unique properties of LLMs and natural language preclude the application of established methods for uncertainty quantification .

A particular challenge is that language outputs can contain multiple types of uncertainty, including lexical (which word is used), syntactic (how the words are ordered), and semantic (what a text means). For many problems, _semantic_ uncertainty is the desired quantity, as it pertains directly to the accuracy of the meaning of the generated response. However, measuring the uncertainty of the generation via token likelihoods conflates all types of uncertainty. To address this, Kuhn et al.  have recently introduced semantic entropy (SE), which estimates uncertainty as the predictive entropy of generated texts with respect to clusters of identical semantic meaning (we discuss this in more detail in Sec. 2).

A critical limitation of SE is that it captures semantic relations between the generated texts only through equivalence relations. This does not capture a _distance metric_ in the semantic space, which would allow one to account for more nuanced _semantic similarity_ between generations. For instance, it separates "apple" as equally strongly from "house" as it will "apple" from "granny smith" even though the latter pair is more closely related. In this paper, we address this problem by incorporating a distance in the semantic space of generated answers into the uncertainty estimation.

We propose **Kernel Language Entropy (KLE)**. KLE leverages semantic similarities by using a distance measure in the space of the generated answers, encoded by unit trace positive semidefinite kernels. We quantify uncertainty by measuring the von Neumann entropy of these kernels. This approach allows us to incorporate a metric between generated answers or, alternatively, semantic clusters into the uncertainty estimation. Our approach uses kernels to describe semantic spaces, making KLE more general and better at capturing the semantics of generated texts than the previous methods. We theoretically prove that our method is more expressive than semantic entropy, meaning there are cases where KLE, but not SE, can distinguish the uncertainty of generations. Importantly, our approach does not rely on token likelihood and works for both white-box and black-box LLMs.

Our work makes the following contributions towards better uncertainty quantification in LLMs:

* We propose Kernel Language Entropy, a novel method for uncertainty quantification in natural language generation (Sec. 3),
* We propose concrete design choices for our method that are effective in practice, for instance, graph kernels and weight functions (Sec. 3.2),
* We prove that our method is a generalization of semantic entropy (Thm. 3.5),
* We empirically compare our approach against baselines methods across several tasks and LLMs with up to 70B parameters (60 scenarios total), achieving SoTA results (Sec. 5).

We release the code and instructions for reproducing our results at https://github.com/AlexanderVNikitin/kernel-language-entropy.

## 2 Background

**Uncertainty Estimation.** Information theory  offers a principled framework for quantifying the uncertainty of predictions as the predictive entropy of the output distribution:

\[(x)=H(Y x)=- p(y x) p(y x)dy,\] (1)

Figure 1: Illustration of Kernel Language Entropy (KLE). We here show a version of KLE called KLE-c, which operates on semantic clusters. Given an input query and two different LLMs, we sample 10 answers from each model \(a_{1},,a_{10}\) and \(a^{}_{1},,a^{}_{10}\) and cluster them by semantic equivalence into clusters \(C_{1},,C_{3}\) and \(C^{}_{1},,C^{}_{3}\). For the sake of the example, we assume that the numbers and sizes of clusters, as well as individual cluster probabilities, are all equal \(p(C_{i}|)=p(C^{}_{i}|)\) for all \(i\). Then, semantic entropy would yield identical uncertainties for both LLMs. However, uncertainty should be lower for \(_{2}\) because semantic “similarity” between the generations is much higher; i.e., the model is fairly confident that “Kolmogorov” and “Laplace” are good answers. KLE, explicitly accounts for the semantic similarity between texts using a kernel-based approach. Semantic kernels provide an effective way to encode the semantic similarity between answers, enabling the method to correctly identify that \(_{2}\)’s outputs should be assigned lower uncertainty (see right).

where \(Y\) is the output random variable, \(x\) is the input, and \(H(Y|x)\) is a conditional entropy which represents average uncertainty about \(Y\) when \(x\) is given. Uncertainty is often categorized into aleatoric (data) and epistemic (knowledge) uncertainty. Following previous work on uncertainty quantification in LLMs, we assume that LLMs capture both types of uncertainty  and do not attempt to disambiguate them, as both epistemic and aleatoric uncertainty contribute to model errors.

**UQ in sequential models.** Let \(S^{N}\) be a sequence of length \(N\), consisting of tokens, \(s_{i}\), where the set \(\) denotes a vocabulary of tokens. The probability of \(S\) is then the joint probability of the tokens, obtained as the product of conditional token probabilities:

\[p(S x)=_{i}p(s_{i}|s_{<i},x).\] (2)

Instead of Eq. (2), the geometric mean of token probabilities has proven to be successful in practice . Using Eq. (1) and (2), we can define the predictive entropy of a sequential model.

**Definition 2.1**.: _The predictive entropy for a random output sequence \(S\) and input \(x\) is_

\[U(x)=H(S x)=-_{s}p(s x)(p(s x)),\] (3)

_where the sum is taken over all possible output sequences \(s\)._

A downside of naive predictive entropy for Natural Language Generation (NLG) is that it measures uncertainty in the space of tokens while the uncertainty of interest lies in semantic space. As an illustrative example, consider two sets of \(n\) answers, \(S_{i}\) and \(S_{i}^{}\) sampled from two LLMs with equivalent token likelihood \(p(S_{i}|x)=p(S_{i}^{}|x)\) as a response to the question "What is the capital of France?" . Suppose the answers from the first LLM are various random cities ("Paris", "Rome", etc.), and those from the second LLM are paraphrases of the correct answer "It is Paris". Naive predictive entropy computation can give similar values, even though the second LLM is not uncertain about the meaning of its answer. Kuhn et al.  have proposed semantic entropy to address this problem.

We first define the concept of semantic clustering. Semantic clusters are equivalence classes obtained using a semantic equivalence relation, \(E(,)\), which is reflexive, symmetric, and transitive and should capture semantic equivalence between input texts. In practice, \(E\) is computed using bi-directional entailment predictions from a Natural Language Inference (NLI) model, such as DeBERTa  or a prompted LLM, that classifies relations between pairs of texts as "entailment," "neutral," or "contradiction". Two texts are semantically equivalent if they entail each other bi-directionally. Semantic clusters are obtained by greedily aggregating generations into clusters of equivalent meaning. We can now define semantic entropy.

**Definition 2.2**.: _For an input \(x\) and semantic clusters \(C\), where \(\) is a set of all semantic clusters, Semantic Entropy (\(\)) is defined as_

\[(x)=-_{C}p(C x) p(C x)=- _{C}((_{s C}p(s x))[ _{s C}p(s x)]).\] (4)

In practice, it is not possible to calculate \(_{C}p(C x) p(C x)\) because of the intractable number of semantic clusters. Instead, SE uses a Rao-Blackwellized Monte Carlo estimator

\[(x)-_{i=1}^{M}p^{}(C_{i}|x) p^{ }(C_{i}|x),\] (5)

where \(C_{i}\) are \(M\) clusters extracted from the generations and \(p^{}(C_{i} x)\) is a normalized semantic probability, \(p^{}(C_{i} x)=|x)}}{{_{i}p(C_{i}|x)}}\), which we refer to as \(p(C_{i}|x)\) in the following for simplicity. SE can be extended to cases where token likelihoods are not available by approximating \(p(C_{i}|x)\) with the fraction of generated texts in each cluster, \(p(C_{i}|x)_{i=1}^{N}(S_{i} C_{i})}{N}\). We refer to this variant as _Discrete Semantic Entropy_.

## 3 Kernel Language Entropy

This section introduces Kernel Language Entropy (KLE), our novel approach to computing semantic uncertainty that accounts for fine-grained similarities between generations for better uncertainty quantification. We introduce two variants of KLE: the first, simply called \(\), operates directly on the generated texts, and the second, \(\)-c operates on the space of semantic clusters.

**Motivating Example.** Figure 1 illustrates the advantages of KLE (to be precise, the \(\)-c variant) over other methods such as SE. Imagine querying two LLMs such that the outputs of \(_{1}\) are all semantically different and those of \(_{2}\) are semantically similar _but not equivalent_. For simplicity, we assume an equal amount of clusters between LLMs and equal likelihoods of clusters \(p(C_{i}|)=p(C_{i}^{}|)\). SE would not distinguish between those cases and, thus, would misleadingly predict equal uncertainty. KLE on the other hand, will correctly assign lower uncertainty to the outputs of \(_{2}\), its kernels accounting for the fact that \(_{2}\) produces semantically similar outputs.

Before introducing KLE, we recall the definition of a positive semidefinite (PSD) kernel.

**Definition 3.1**.: _For a set \(\), a symmetric function \(K:\) is called a PSD kernel if for all \(n>0,x_{i},_{i}\)_

\[_{i=1}^{n}_{j=1}^{n}_{i}_{j}K(x_{i},x_ {j}) 0.\] (6)

_For a finite set \(\), a PSD kernel is a PSD matrix of the size \(||\)._

### Semantic Kernels and KLE

Next, we define _semantic kernels_, denoted \(K_{}\), as unit trace1 positive semidefinite kernels over the finite domain of _generated_ texts. Unit trace PSD matrices are also called density matrices. These kernels should, informally speaking, capture the semantic similarity2 between the texts such that \(K(s_{1},t_{1})>K(s_{2},t_{2})\) if and only if texts \(s_{1}\) and \(t_{1}\) are more semantically related than texts \(s_{2}\) and \(t_{2}\). Analogously, we define semantic kernels over semantic clusters of texts, in which case the kernel should capture the semantic similarity between the clusters. In practice, there are multiple ways to concretely specify a proper semantic kernel, and some options are described in Section 3.2.

**The von Neumann Entropy.** We propose to use the von Neumann entropy (VNE) to evaluate the uncertainty associated with a semantic kernel.

**Definition 3.2** (Von Neumann Entropy).: _For a unit trace positive semidefinite matrix \(A^{n n}\), the von Neumann entropy (VNE; ) is defined as_

\[(A)=-[A A].\] (7)

It can be shown that \((A)=_{i}^{n}-_{i}_{i}\) where \(_{i},1 i n\) are the eigenvalues of \(A\). Within this definition, we assume \(0 0=0\). This reformulation shows that VNE is, in fact, the Shannon entropy over the eigenvalues of a kernel.

**Kernel Language Entropy (KLE).** We can now define Kernel Language Entropy, as the VNE of a semantic kernel.

**Definition 3.3** (Kernel Language Entropy).: _Given a set of LLM generations \(S_{1},,S_{N}\), an input \(x\), and semantic kernel \(K_{}\) over these generations and input, we define **Kernel Language Entropy** (\(\)) as the von Neumann entropy of a semantic kernel \(K_{}\):_

\[(x)=(K_{}).\] (8)

The von Neumann entropy has the following properties, which are aligned with the overarching goal of measuring the uncertainty of a set of generations.

**Proposition 3.4** (Properties of the von Neumann Entropy ).: _The VNE of a unit trace positive semidefinite kernel has the following properties:_

1. _The VNE of a kernel with only one non-zero element is equal to 0._
2. _The VNE is invariant under changes of basis_ \(U\)_:_ \((K)=(UKU^{})\)_._
3. _The VNE is concave. For a set of positive coefficients_ \(_{i}\)_,_ \(_{i=1}^{k}_{i}=1\)_, and density matrices_ \(K_{i}\)_, it holds that_ \((_{i=1}^{k}_{i}K_{i})_{i=1}^{k} _{i}\,(K_{i})\)_._Let us briefly discuss the practical implications of these properties. **Property 1** states that if an LLM outputs a single answer (for \(\)) or a semantic cluster (for \(\)-c), the VNE is zero, indicating high certainty. **Property 2** is significant as it allows the VNE to be calculated in practice as the Shannon entropy of the diagonal elements of an orthogonalized kernel, which can be interpreted as a disentangled representation of a semantic kernel. **Property 3** states that entropy is concave, meaning that the entropy of a combined system is greater than or equal to the entropy of its individual parts, a common requirement for entropy metrics. The intuition behind our use of the VNE for LLMs also relates to its origins in quantum information theory.

**The VNE in Quantum Information Theory.** In quantum information theory, the states of a quantum system (or pure states) are defined as unit vectors in \(^{N}\). However, experiments often result in statistical mixtures of pure quantum states, represented as density matrices. The VNE is used to evaluate the entropy of the mixed states. Analogously, we can think of KLE as considering each answer as a mixture of pure "semantic meanings", measuring the entropy of this mixture. We refer the reader to Aaronson  for further background reading on the VNE and quantum information theory.

**KLE-c.** Instead of defining semantic kernels directly over individual model generations, we can also apply KLE to clusters of semantic equivalence. We call this variant of our method \(\)-c. Although KLE is more general than \(\)-c for non-trivial clusterings, KLE-c can provide practical value as it is cheaper to compute and more interpretable due to its smaller kernel sizes.

**Algorithm.** Algorithm 1 provides a generic description of the steps required to compute KLE. We describe the practical details for defining and combining semantic kernels later in Sec. 3.2.

**Computational Complexity**. The computational complexity of KLE is approximately identical to SE which requires sampling from an LLM \(N\) times and running the entailment model \(O(N^{2})\) times. Additionally, KLE requires \(O(N^{3})\) elementary operations for kernel and VNE calculation. The actual cost of this is negligible in comparison to the forward passes through the LLM or entailment model.

```
0:\(\), Input \(x^{L}\), Number of samples \(n\), Boolean \(\)-c indicating variant, Semantic kernels \(K_{i}\)
1: Initialize a _multiset_ of answers \(\)
2:for\(k 1\) to \(n\)do\(\) Sampling \(n\) answers
3: Add \((x)\) to \(\)
4:endfor
5:if\(\)-c then
6: Update \(()\)\(\) as in 
7:endif
8: Combine \(K_{i}(,)\) in \(K_{}\)\(\) see Sec. 3.2
9: Return \((K_{})\)\(\) Eq. (8) ```

**Algorithm 1** Kernel Language Entropy

### Semantic Graph Kernels

This section describes a practical approach for constructing semantic kernels over LLM generations or semantic clusters. Concretely, we apply NLI models to construct _semantic graphs_ over the LLM outputs and then borrow from graph kernel theory to construct kernels from these graphs. A similar notion of semantic graphs derived from NLI models was proposed by Lin et al.  for black-box LLM uncertainty quantification.

Graph Theory Preliminaries.First, let us recall the basics of graph theory. A graph is a pair of two sets \(G=(V,E)\), where \(V=\{1,,n\}\) is a set of \(n\) vertices and \(E V V\) is a set of edges. A graph is called weighted when a weight is assigned to each edge, and the weight matrix \(W_{ij}\) contains weights between nodes \(i\) and \(j\). For unweighted graphs, we can use a binary adjacency matrix to encode edges between nodes. The degree matrix \(D\) is a diagonal \(|V||V|\) matrix with \(D_{ii}=_{j=1}^{|V|}W_{ij}\). The _graph Laplacian_ is defined as \(L=D-W\). \(L\) is a positive semidefinite matrix, and eigenvalues of \(L\) are often used to study the structure of graphs .

**Semantic Graph.** We define semantic graphs as graphs over LLM generations (\(G_{}\)) or semantic clusters (\(G_{}\)). For \(G_{}\), edges can be defined as a function of NLI predictions in both directions: \(W_{ij}=f((S_{i},S_{j}),(S_{j},S_{i}))\), where \(\) are the predicted probabilities for _entailment_, _neutral_, and _contradiction_ for \(S_{i}\) and \(S_{j}\). For example, \(f\) could be the weighted sum over the predicted probabilities for entailment and neutral classes. For \(G_{}\), the weights between the clusters are computed by summing the entailment predictions over the generations assigned to the clusters, \(W_{ij}=_{s C_{i}}_{t C_{j}}f((s,t),(t,s))\).

Graph Kernels.When a semantic graph is obtained, KLE calculates graph kernels over semantic graph nodes to compute a distance measure. Since graphs are discrete and finite, any positive semidefinite matrix would be a kernel over the graph. However, we seek kernels that exploit knowledge about the graph structure. We, therefore, adopt Partial Differential Equation (PDE) and Stochastic Partial Differential Equation (SPDE) approaches to graph kernels . If \(u^{n}\) is a signal over the nodes of a graph, the **heat kernel** is a solution to the partial differential equation \(}{{ t}}+Lu=0\) and the **Matern kernel** is a solution to the stochastic differential equation, \((}{{^{2}}}+L)}{{}}}{u}=w\), where \(w\) is white noise over the graph nodes and \(L\) is the graph Laplacian defined above. The corresponding solutions to these equations are:

\[K_{t} =e^{-tL} K_{} =(}{{^{2}}}I+L)^{-}.\] (9)

These kernels allow for the incorporation of a distance measure that reflects the graph's locality properties (right part of Fig. 2). For example, the Taylor series of the heat kernel can be shown to be equal to a sum of powers of random walk matrices. Both kernels have hyperparameters: lengthscales \(t\) in the heat kernel, \(\) in Matern kernels, and \(\) in the Matern kernel, often interpreted as smoothness. The scaled eigenvalues of the Matern kernel converge to the eigenvalues of the heat kernel  when \(\) goes to infinity. Matern kernels provide more flexibility at the cost of the additional parameter. Note that any kernel can be normalized into a unit trace kernel via \(K(x,y) K(x,y)(K(x,x)K(y,y))^{-1/2}/N\), where \(N\) is the size of \(K\). We refer to  for further background reading on graph kernels.

Kernel Hyperparameters.We propose two ways to select the hyperparameters of the heat and Matern kernels: either by maximizing the validation set performance or by selecting parameters from what we call _Entropy Convergence Plots_, illustrated in Fig. 2. We obtain these plots by defining a set of progressively denser graphs \(G_{1} G_{K}\). These can be obtained by starting from a graph without edges and a fixed number of vertices and adding new edges either randomly or by filling in the adjacencies of each node sequentially. We then plot the VNE against the number of edges in the graphs \(G_{i}\). We analyze the von Neumann entropy over these plots to avoid pathologies connected to the fact that for large lengthscales, the VNE converges rather quickly, and such behavior should generally be avoided. For all remaining values, we can either choose hyperparameters randomly from the range of non-collapsing hyperparameters or rely on prior domain knowledge.

Kernel Combination.KLE offers the additional flexibility of combining kernels from various methods (e.g., multiple NLI models, different graph kernels, or other methods). For example, we can combine multiple kernels using convex combinations, \(K=_{i=1}^{P}_{i}K_{i}\), where \(_{i=1}^{P}_{i}=1\).

### Kernel Language Entropy Generalizes Semantic Entropy

The semantic kernels used in KLE are more informative than the semantic equivalence relations used in SE . The next theorem shows that KLE can recover SE for any semantic clustering.

**Theorem 3.5** (KLE and KLE-c generalize SE).: _For any semantic clustering, there exists a semantic kernel over texts \(K_{}(s,s^{})\) such that the VNE of this kernel is equal to semantic entropy (computed as in Eq. (5)). Moreover, there exists a semantic kernel over clusters \(K_{}(c,c^{})\) such that the VNE of this kernel is equal to SE._

Figure 2: Entropy Convergence Plots for heat kernels. For graphs of various sizes \(|V|\), we grow the number of edges and examine the VNE. For large lengthscales \(t\), corresponding to darker colored curves, the VNE quickly converges to zero. We can use these plots to determine kernel hyperparameters without validation sets. The VNE is scaled to start at 1 for visualization purposes.

Proof Sketch.: For any semantic clustering, we consider a kernel with a block diagonal structure. Each block corresponds to a semantic cluster, and cluster likelihoods are normalized by the size of the cluster, \(|x)}}{{m_{i}}}\). This is a valid semantic kernel and the KLE for this kernel equals the SE. Thm. B.1 and Thm. B.2 in the Appendix contain the detailed proofs. 

The proof of Thm. 3.5 shows that the block diagonal semantic kernels used with KLE can recover semantic entropy for any clustering. However, there are other kernels available that allow KLE to be more expressive than SE. Comparing KLE and KLE-c, we find that KLE is more general than KLE-c for any non-trivial clustering.

## 4 Related Work

In the context of machine learning, the VNE has been studied theoretically, applied to GAN regularization , and the exponential of the VNE has been used for effective rank and sample diversity analysis [64; 19].

The first attempts at estimating the entropy of language date back to the 1950s , and today, techniques for uncertainty quantification are widely used in natural language processing. For instance, Desai and Durrett  and Jiang et al.  presented calibration techniques for classification tasks. Xiao and Wang  empirically showed that, for various tasks, including sentiment analysis and named entity recognition, measuring model uncertainty can be used to improve performance. Calibration techniques have also been applied in machine translation tasks to improve accuracy .

Malinin and Gales  discussed the challenges of estimating uncertainty in sequential models. Several previous works have queried LLMs to elicit statements about uncertainty, either via fine-tuning or by directly including previous LLM generations in the prompt [30; 9; 54; 43; 53; 21; 63; 68; 12; 74; 36]. Zhang et al.  studied UQ for long text generation. Quach et al.  used conformal predictions to quantify LLM uncertainty, which is orthogonal to the approach we pursue here. Yang et al.  have shown that Bayesian modeling of LLMs using low-rank Laplace approximations improves calibration in small-scale multiple-choice settings. Lin et al.  extended the work of Kuhn et al.  on semantic entropy by introducing the use of the Laplacian of semantic graphs and applying spectral graph analysis for UQ in black-box LLMs. Aichberger et al.  proposed a new method for sampling diverse answers from LLMs, and Liu et al.  proposed improving calibration by adding an extra linear layer; more diverse sampling strategies and better calibration could improve KLE as well.

There are a variety of ways besides model uncertainty to detect hallucinations in LLMs such as querying external knowledge bases [17; 42; 70], hidden state interventions [77; 24; 46], using probes [8; 41; 48], or applying fine-tuning [31; 67]. KLE is complementary to many of these directions and focuses on estimating more fine-grained semantic uncertainty. It can either be used to improve these approaches or be combined with them sequentially.

## 5 Experiments

**Datasets and Models.** Our experiments span over 60 dataset-model pairs. We evaluate our method on the following tasks covering different domains of natural language generation: general knowledge (TriviaQA  and SQuAD ), biology and medicine (BioASQ ), general domain questions from Google search (Natural Questions, NQ ), and natural language math problems (SVAMP ). We generally discard the context associated with each input for all datasets except SVAMP, as the tasks become too easy for the current generation of models when context is provided. We use the following LLMs: Llama-2 7B, 13B, and 70B , Falcon 7B and 40B , and Mistral 7B , using both standard and instruction-tuned versions of these models. As the NLI model for defining semantic graphs or semantic clusters, we use DeBERTa-Large-MNLI .

**Baselines.** As baseline methods, we compare KLE with semantic entropy , discrete semantic entropy [16; 36], token predictive entropy , embedding regression , and P(True) . For embedding regression, we train a logistic regression model on the last layer's hidden states to predict whether a given LLM answer is correct.

**KLE Kernels.** We propose to use the following two semantic kernels with \(\): \(K_{}\) and \(K_{}\). Both are obtained from the weighted graph \(W_{ij}=w\,^{}(S_{i},S_{j})+w\,^{}(S_{j},S_{ i})\), where \(w=(1,0.5,0)^{}\) is a weight vector. Here, we assume that \(^{}\) returns a one-hot prediction over 

[MISSING_PAGE_FAIL:8]

KLE outperforms previous methods. We compare the performance of UQ methods over 60 scenarios (12 models, five datasets). Figure 3 shows the heatmaps of pairwise win rates. We observe that both our methods, \((K_{})\) and \((K_{})\), are superior to the baselines. Furthermore, Table 1 shows the detailed results for the two largest models from our experiments, Llama 2 70B Chat and Falcon 40B Instruct. The results show that for the largest models, our method consistently achieves best results compared to baselines. In Fig. D.3 and Fig. D.4, we show the experimental results for all considered models. Importantly, our best method, \((K_{})\), does not require token-level probabilities from a model and works in black-box scenarios.

KLE hyperparameters can be selected without validation sets. We compare the strategies of hyperparameter selection from Sec. 3.2: entropy convergence plots and validation sets (100 samples per dataset except for SVAMP, where we used default hyperparameters). We observe that default hyperparameters achieve similar results as selecting hyperparameters from validation sets and conclude that choosing default hyperparameters from entropy convergence plots is a good way to select hyperparameters in practice. In Fig. 4, we compare the two strategies for selecting hyperparameters, and see that the ranking of the methods remains stable and the pairwise win-rates are similar for both methods.

Many design choices outperform existing methods, the best is \((K_{})\). Next, in Fig. 4, we compare several design choices for KLE: choosing a kernel (heat or Matern), using KLE-c, combining kernels via a weighted sum or product, and using the probabilities returned by DeBERTa for edge weights. The superscript indicates the type of a graph: no superscript indicates a weighted graph as described above, DB means weights are assigned using probabilities from DeBERTa, and C means a weighted graph over clusters (\(\)-c). The subscript indicates the semantic kernels: SE stands for a diagonal kernel with semantic probabilities (\(K_{}\)), heat and Matern for the type of kernel (\(K_{}\) and \(K_{}\)), and \(\) for the best of Heat and Matern kernels. We observe that even though all design choices outperform SE, the heat kernel over a weighted semantic graph, \((K_{})\), was overall the best. Additionally, we notice that the methods based on token likelihoods are performing better for non-instruction-tuned models, and we can practically recommend including semantic probabilities (e.g., use variations of \(K_{}\)) if KLE is used in non-instruction-tuned scenarios (see Fig. D.5).

KLE is better in practice because it captures more fine-grained semantic relations than SE. The performance of KLE improves over SE because in complex free-form language generation scenarios, such as those studied here, LLMs can generate similar but not strictly equal answers. SE assigns these to separate clusters, predicting high entropy. By contrast, our method can account for semantic similarities using the kernel metric in the space of meanings over generated texts, and predict reduced uncertainty if necessary. We give a detailed illustrative example for which KLE provides better uncertainty estimates than SE from the NQ Open dataset in Fig. C.2.

## 6 Discussion

Measuring semantic uncertainty in LLMs is a challenging and important problem. It requires navigating the semantic space of the answers, and we have suggested a method, KLE, that encodes a similarity measure in this space via semantic kernels. KLE allows for fine-grained estimation of uncertainty and is an expressive generalization of semantic entropy. We provided several specific design choices by defining NLI-based semantic graphs and kernels, and studying kernel hyperparameters. We have evaluated KLE across various domains of natural language generation, and it has demonstrated superior performance compared to the previous methods. Our method works both for

Figure 4: Comparison of various design choices for semantic graph kernels. represents the best hyperparameters and – – defaults. Error bars are twice the standard error. Summary of 48 experiments. KLE consistently outperformed SE across all the kernels evaluated.

white- and black-box settings, enabling its application to a wide variety of practical scenarios. We hope to inspire more work that moves from semantic _equivalence_ to semantic _similarity_ for estimating semantic uncertainty in LLMs.

**Broader Impact.** Our work advances the progress toward safer and more reliable uses of LLMs. KLE can positively impact areas that involve using LLMs by providing more accurate uncertainty estimates, which can filter out a proportion of erroneous outputs.

**Limitations.** One limitation of the proposed method is that it requires multiple samples from an LLM, which generally increases the generation cost. However, in safety-critical tasks, the potential cost of hallucination should outweigh the cost of sampling multiple answers, so reliable uncertainty quantification via KLE should always be worthwhile. Additionally, we study semantic kernels derived from NLI-based semantic graphs, but other semantic kernels warrant investigation, such as kernels on embeddings. Moreover, the NLG landscape is highly diverse, and the method should be carefully evaluated for other potential applications of LLMs, such as code generation. Lastly, our method estimates uncertainty using predictive entropy, as commonly done in Bayesian deep learning. However, in applications where confidence estimates are important, alternative methods should be considered.

This work was supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI, and grants 352986, 358246) and EU (H2020 grant 101016775 and NextGenerationEU). We also acknowledge the computational resources provided by the Aalto Science-IT Project from Computer Science IT. The authors wish to acknowledge CSC - IT Center for Science, Finland, for computational resources.