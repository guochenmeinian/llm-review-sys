# Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards

Silivu Pitis

University of Toronto and Vector Institute

spitis@cs.toronto.edu

###### Abstract

As the capabilities of artificial agents improve, they are being increasingly deployed to service multiple diverse objectives and stakeholders. However, the composition of these objectives is often performed ad hoc, with no clear justification. This paper takes a normative approach to multi-objective agency: from a set of intuitively appealing axioms, it is shown that Markovian aggregation of Markovian reward functions is not possible when the time preference (discount factor) for each objective may vary. It follows that optimal multi-objective agents must admit rewards that are non-Markovian with respect to the individual objectives. To this end, a practical non-Markovian aggregation scheme is proposed, which overcomes the impossibility with only one additional parameter for each objective. This work offers new insights into sequential, multi-objective agency and intertemporal choice, and has practical implications for the design of AI systems deployed to serve multiple generations of principals with varying time preference.

## 1 Introduction

The idea that we can associate human preferences with scalar utility values traces back hundreds of years and has found usage in numerous applications [9; 71; 28; 49]. One of the most recent, and perhaps most important, is the design of artificial agents. In the field of reinforcement learning (RL), this idea shows up as the _reward hypothesis_[74; 67; 10], which lets us define objectives in terms of a discounted sum of Markovian rewards. While foundational results from decision theory [81; 62] and inverse RL [52; 54] justify the reward hypothesis when a single objective or principal is considered, complexities arise in multi-objective scenarios [61; 77]. The literature on social choice is largely defined by impossibilities , and multi-objective composition in the RL and machine learning literature is typically restrictive [68; 51], applied without clear justification , or based on subjective evaluations of empirical efficacy . Addressing these limitations is crucial for the development of artificial agents capable of effectively serving the needs of diverse stakeholders.

This paper extends previous normative work in RL by adopting an axiomatic approach to the aggregation of objectives. The approach is based on a set of intuitively appealing axioms: the von Neumann-Morgenstern (VNM) axioms, which provide a foundation for rational choice under uncertainty; Pareto indifference, which efficiently incorporates individual preferences; and dynamic consistency, which ensures time-consistent decision-making. From these axioms, an impossibility is derived, leading to the conclusion that optimal multi-objective agents with diverse time preferences must have rewards that are non-Markovian with respect to the individual objectives. To address this challenge, a practical state space expansion is proposed, which allows for the Markovian aggregation of objectives requiring only one parameter per objective. The results prompt an interesting discussion on dynamic preferences and intertemporal choice, leading to a novel "historical discounting" strategy that trades off dynamic consistency for intergenerational fairness. Finally, it is shown how both our results can be extended (albeit non-normatively) to stochastic policies.

The remainder of this paper is organized as follows: Section 2 motivates the problem by modeling human procrastination behavior as an aggregation of two objectives, work and play, and showing how a plan that appears optimal today may lead to the worst possible future outcome. Section 3 presents the axiomatic background and the key impossibility result. Section 4 presents the corresponding possibility result and a practical state expansion to implement it. Section 5 relates the results to intertemporal choice, proposes \(N\)-step commitment and historical discounting strategies for managing intergenerational tradeoffs, extends the results to stochastic policies, and discusses related topics in RL. Section 6 concludes with some final thoughts and potential future research directions.

## 2 Motivation: The Procrastinator's Peril

We begin with a numerical example of how the naive aggregation of otherwise rational preferences can lead to undesirable behavior. The example, which will be referred to throughout as the "Procrastinator's Peril", involves repeated procrastination, a phenomenon to which the reader might relate. An agent aggregates two competing objectives: work and play. At each time step the agent can choose to either work or play. The pleasure of play is mostly from today, and the agent doesn't value future play nearly as much as present play. On the other hand, the consequences of work are delayed, so that work tomorrow is valued approximately as much as work today.

Let us model the agent's preferences for work and play as two separate Markov Decision Processes (MDP), each with state space \(=\) and action space \(=\{,\}\). In the play MDP, we have rewards \(R()=0.5\), \(R()=0\) and a discount factor of \(_{}=0.5\). In the work MDP, we have rewards \(R()=0\), \(R()=0.3\) and a discount factor of \(_{}=0.9\). One way to combine the preferences for work and play is to value each trajectory under both MDPs and then add up the values. Not only does this method of aggregation seem reasonable, but it is actually _implied_ by some mild and appealing assumptions about preferences (Axioms 1 and 3 in the sequel). Using this approach, the agent assigns values to trajectories as follows:

   \(_{1}\) & \(,,,...\) & \(V(_{1})=_{t}(0.5)^{t} 0.5\) & \(=1.00\) \\ \(_{2}\) & \(,,,...\) & \(V(_{2})=_{t}(0.9)^{t} 0.3\) & \(=3.00\) \\ \(_{3}\) & \(,,,...\) & \(V(_{3})=0.5+0.9 V(_{2})\) & \(=3.20\) \\ \(_{4}\) & \(,,,...\) & \(V(_{3})=0.75+0.9^{2} V(_{2})\) & \(=3.18\) \\   

We see that the agent most prefers \(_{3}\): one period (and one period only!) of procrastination is optimal. Thus, the agent procrastinates and chooses to play today, planning to work from tomorrow onward. Come tomorrow, however, the agent is faced with the same choice, and once again puts off work in favor of play. The process repeats and the agent ends up with the least preferred alternative \(_{1}\).

This plainly irrational behavior illustrates the impossibility theorem. Observe that the optimal policy \(_{3}\) is non-Markovian--it must remember that the agent has previously chosen play in order to work forever. But any MDP has a stationary optimal policy , so it follows that we need rewards that are non-Markovian with respect to the original state-action space. Alternatively, we will see in Subsection 4.2 that we can expand the state space to make the optimal policy Markovian.

## 3 Impossibility of Dynamically Consistent, Pareto Indifferent Aggregation

NotationWe assume familiarity with Markov Decision Processes (MDPs)  and reinforcement learning (RL) . We denote an MDP by \(=,,T,R,\), where \(:^{+}\) is a state-action dependent discount function. This generalizes the usual "fixed" \(\) and covers both the episodic and continuing settings . We use lowercase letters for generic instances, e.g. \(s\), and denote distributions using a tilde, e.g. \(\). In contrast to standard notation we write both state- and state-action value functions using a unified notation that emphasizes the dependence of each on the future policy: we write \(V(s,)\) and \(V(s,a)\) instead of \(V^{}(s)\) and \(Q^{}(s,a)\). We extend \(V\) to operate on probability distributions of states, \(V(,)=_{s}V(s,)\), and we allow for non-stationary, history dependent policies (denoted by uppercase \(,\)). With this notation, we can understand \(V\) as an expected utility function defined over prospects of the form \((,)\). We use the letter \(h\) to denote histories (trajectories of states and actions)--these may terminate on either a state or action, as may be inferred from the context. For convenience, we sometimes directly concatenate histories, states, actions and/or policies (e.g., \(hs\), \(sa\), \(s\), \(a\)) to represent trajectory segments and/or the associated stochastic processes. For simplicity, we assume finite \(||,||\).

### Representing rational preferences

This paper is concerned with the representation of aggregated preferences, where both the aggregation and its individual components satisfy certain axioms of rationality. We define the objects of preference to be the stochastic processes ("prospects") generated by following (potentially non-stationary and stochastic) policy \(\) from state \(s\). Distributions or "lotteries" over these prospects may be represented by (not necessarily unique) tuples of state lottery and policy \((,)()=:( )\). We write \((_{1},)(_{2},)\) if \((_{1},)\) is strictly preferred to \((_{2},)\) under preference relation \(\).

To be "rational", we require \(\) to satisfy the "VNM axioms" , which is capture in Axiom 1:

**Axiom 1** (VNM).: _For all \(,,()\) we have:_

_Asymmetry_: If \(\), then not \(\);

_Negative Transitivity_: If not \(\) and not \(\), not \(\);

_Independence_: If \(\), then \(+(1-)+(1-)\), \((0,1]\);

_Continuity_: If \(\), then \(\;,(0,1)\) such that \(+(1-)+(1- )\);

_where \(+(1-)\) denotes the mixture lottery with \(\%\) chance of \(\) and \((1-)\%\) chance of \(\)._

Asymmetry and negative transitivity together form the basic requirements of a strict preference relation--equivalent to completeness and transitivity of the corresponding weak preference relation, \(\) (defined as \(p q q p\)). Independence can be understood as an irrelevance of unrealized alternatives, or consequentialist, axiom: given that the \(\%\) branch of the mixture is realized, preference between \(\) and \(\) is independent of the rest of the mixture (i.e., what could have happened on the \((1-)\%\) branch). Finally, continuity is a natural assumption given that probabilities are continuous.

We further require \(\) to be _dynamically consistent_:

**Axiom 2** (Dynamic consistency).: \((s,a)(s,a)\) _if and only if \((T(s,a),)(T(s,a),)\) where \(T(s,a)\) is the distribution over next states after taking action \(a\) in state \(s\)._

This axiom rules out the irrational behavior in the Procrastinator's Peril, by requiring today's preferences for tomorrow's actions to be the same as tomorrow's preferences. While some of these axioms (particularly independence and dynamic consistency) have been the subject of debate (see, e.g., ), note that the standard RL model is _more_ restrictive than they require .

The axioms produce two key results that we rely on (see Kreps  and Pitis  for proofs):

**Theorem 1** (Expected utility representation).: _The relation \(\) defined on the set \(()\) satisfies Axiom 1 if and only if there exists a function \(V:\) such that, \(\;,()\):_

\[_{z()}(z)V(z)>_{z()}(z)V(z).\]

_Another function \(V^{}\) gives this representation iff \(V^{}\) is a positive affine transformation of \(V\)._

Using Theorem 1, we extend the domain of value function \(V\) to \(()\) as \(V()=_{z}(z)V(z)\).

**Theorem 2** (Generalized Bellman representation).: _If \(\) satisfies Axioms 1-2 and \(V\) is an expected utility representation of \(\), there exist \(R:S A\), \(:S A^{+}\) such that \(\;s,a,\),_

\[V(s,a)=R(s,a)+(s,a)V(T(s,a),).\]

**Remark 3.1.1**.: Instead of preferences over stochastic processes of potential futures, one could begin with preferences over trajectories . The author takes issue with this approach, however, as it's unclear that such preferences should satisfy _Asymmetry_ or _Independence_ without additional assumptions (humans often consider counterfactual outcomes when evaluating the desirability of a trajectory) . By using Theorem 2 to unroll prospects, one can extend preferences over prospects to define preferences over trajectories according to their discounted reward.

**Remark 3.1.2**.: Theorem 2, as it appeared in Pitis , required an additional, explicit "Irrelevance of unrealizable actions" axiom, since prospects were defined as tuples \((,)\). This property is implicit in our redefinition of prospects as stochastic processes.

**Remark 3.1.3** In this line of reasoning only the preference relation \(\) is primitive; \(V\) and its Bellman form \((R,)\) are simply representations of \(\) whose existence is guaranteed by the axioms. Not all numerical representations of \(\) have these forms . In particular, (strictly) monotonically increasing transforms preserve ordering, so that any increasing transform \(V^{}\) of a Theorem 1 representation \(V\) is itself a valid numerical representation of \(\) (although lotteries will no longer be valued by the expectation over their atoms unless the transform is affine, per Theorem 1).

### Representing rational aggregation

Let us now consider the aggregation of several preferences. These may be the preferences of an agent's several principals or preferences representing a single individual's competing interests. Note at the outset that it is quite natural for different objectives or principals to have differing time preference. We saw one example in the Procrastinator's Peril, but we can also consider a household robot that seeks to aggregate the preferences of Alice and her husband Bob, for whom there is no reason to assume equal time preference .

An intuitively appealing axiom for aggregation is Pareto indifference, which says that if each individual preference is indifferent between two alternatives, then so too is the aggregate preference.

**Axiom 3** (Pareto indifference).: _If \(_{i}\)\(( i)\), \(_{}\)._

Here, \(\) means indifference (not \(\) and not \(\)), so that \(_{i}\) is the \(i\)th individual indifference relation, \(\) is a finite index set over the individuals, and \(_{}\) indicates the aggregate relation. There exist stronger variants of Pareto property that require monotonic aggregation (e.g., if all individuals prefer \(\), so too does the aggregate; see Axiom 3\({}^{}\) in Subsection 5.1). We opt for Pareto indifference to accommodate potentially deviant individual preferences (e.g., if all individuals are indifferent but for a sociopath, the aggregate preference may be opposite of the sociopath's).

If we require individual and aggregate preferences to satisfy Axiom 1 and, jointly, Axiom 3, we obtain a third key result due to Harsanyi . (See Hammond  for proof).

**Theorem 3** (Harsanyi's representation).: _Consider individual preference relations \(\{_{};i\}\) and aggregated preference relation \(_{}\), each defined on the set \(()\), that individually satisfy Axiom 1 and jointly satisfy Axiom 3. If \(\{V_{i};i\}\) and \(V_{}\) are expected utility representations of \(\{_{};i\}\) and \(_{}\), respectively, then there exist real-valued constant \(c\) and weights \(\{w_{i};i\}\) such that:_

\[V_{}()=c+_{i}w_{i}V_{i}().\]

_That is, the aggregate value can be expressed as a weighted sum of individual values (plus a constant)._

According to Harsanyi's representation theorem, the aggregated value function is a function of the individual value functions, _and nothing else_. In other words, Pareto indifferent aggregation of VNM preferences that results in VNM preference is necessarily _context-free_--the same weights \(\{w_{i}\}\) apply regardless of state and policy.

We will also make use of two technical conditions to eliminate certain edge cases. Though sometimes left implicit, these are both common requirements for aggregation functions .

**Axiom 4** (Technical conditions on \(_{}\)).: _Unrestricted Domain_: \(_{}\) is defined for all valid individual preference sets \(\{_{};i\}\). _Sensitivity_: \( i\), holding \(_{j},j i\) constant, there exist \(_{i}^{1},_{i}^{2}\) resulting in different \(_{}\).

The first condition allows us to consider conflicting objectives with different time preference. The second condition implies that the weights \(w_{i}\) in Theorem 3 are nonzero.

**Remark 3.2.1** It is worth clarifying here the relation between Harsanyi's theorem and a related class of aggregation theorems, occasionally cited within the machine learning literature (e.g., ), based on axioms originating with Debreu . One instance of this class states that any aggregate preference satisfying six reasonable axioms can be represented in the form: \(V_{}()=_{i}m(V_{i}())\), where \(m\) is a strictly increasing monotonic function from the family \(\{x^{p}\,|\,0<p 1\}\{(x)\,|\,p=0\}\{-x^{p}\,|\,p<0\}\). If we (1) drop the symmetry axiom from this theorem to obtain variable weights \(w_{i}\), and (2) add a monotonicity axiom to Harsanyi's theorem to ensure positive weights \(w_{i}\), then the only difference between the theorems is the presence of monotonic function \(m\). Butnote that applying \(m\) to each \(V_{i}\) or to \(V_{}\) individually does not change the preference ranking they represent; it does, however, decide whether \(V_{i}\) and \(V_{}\) are VNM representations of \(\). Thus, we can understand Harsanyi's theorem as saying: if \(V_{i},V_{}\) are VNM representations of \(\), then \(m\) must be linear. Or conversely, if \(m\) is non-linear (\(p 1\)), \(V_{i}\) and \(V_{}\) are not VNM representations.

**Remark 3.2.2**.: A caveat of Harsanyi's theorem is that it implicitly assumes that all individual preferences, and the aggregate preference, use the same set of agreed upon, "objective" probabilities. This is normatively justifiable if we use the same probability distribution (e.g., that of the aggregating agent) to impose "ideal" preferences \(_{i}\) on each individual, which may differ from their implicit subjective or revealed preferences . As noted by Desai et al. , Harsanyi's theorem fails if the preferences being aggregated use subjective probabilities. Note, however, that the outcome of the "bargaining" construction in Desai et al.  is socially suboptimal when the aggregating agent has better information than the principals, which suggests that effort should be made to unify subjective probabilities. We leave exploration of this to future work.

### Impossibility result

None of Theorems 1-3 assume all Axioms 1-4. Doing so leads to our key result, as follows.

**Theorem 4** (Impossibility).: _Assume there exist distinct policies, \(,,\), none of which is a mixture (i.e., convex combination) of the other two, and consider the aggregation of arbitrary individual preference relations \(\{_{i};i\}\) defined on \(()\) that individually satisfy Axioms 1-2. There does not exist aggregated preference relation \(_{}\) satisfying Axioms 1-4._

Sketch of Proof.: The full proof is in Appendix B. Briefly, we consider \(||=2\) and use Axiom 4 (Unrestricted Domain) to construct mixtures of \(\) and \(\) so that each mixture is considered indifferent to \(\) by one of the individual preference relations. Then, by applying Theorem 2 and Theorem 3 in alternating orders to the difference between the value of the mixture policy and the value of \(\), and doing some algebra, we arrive at the equations

\[w_{1}_{1}(s,a)=w_{1}_{}(s,a) w_{2}_{2 }(s,a)=w_{2}_{}(s,a),\] (1)

from which we conclude that \(_{}(s,a)=_{1}(s,a)=_{2}(s,a)\). But this contradicts our assumption that individual preferences \(\{_{i};i\}\) may be chosen arbitrarily, completing the proof. 

**Intuition of Proof.** Under mild conditions, we can find two policies (a \(/\) mixture, and \(\)) between which individual preference \(_{1}\) is indifferent, but individual preference \(_{2}\) is not. Then for mixtures of this \(/\) mixture and \(\), \(_{1}\) remains indifferent, but the strength of \(_{2}\) changes, so that \(_{}\) must have the same time preference as \(_{2}\). By an analogous argument, \(_{}\) must have the same time preference as \(_{1}\), leading to a violation of Unrestricted Domain.

The closest results from the economics literature consider consumption streams (\(S=\)) [88; 15; 83]. Within reinforcement learning, equal time preference has been assumed, without justification, when merging MDPs [68; 41] and value functions for different tasks .

**Remark 3.3.1**.: A consequence of Unrestricted Domain, critical to the impossibility result, is that individual preferences may have diverse time preferences (i.e., different discount functions). If discounts are equal, Markovian aggregation is possible.

**Remark 3.3.2**.: Per Remark 3.2.2, by applying Harsanyi's theorem, we are implicitly assuming that all preferences are formed using the same "objective" probabilities over prospects; is there a notion of "objective" time preference that should be used? If so, this would resolve the impossibility (once again, requiring that we impose a notion of ideal preference on individuals that differs from their expressed preference). We leave this consideration for future work.

**Remark 3.3.3**.: Theorem 4 applies to the standard RL setup, where \(_{1}\), \(_{2}\), \(_{}\) are constants.

## 4 Escaping Impossibility with Non-Markovian Aggregation

An immediate consequence of Theorem 4 is that any scalarized approach to multi-objective RL  is generally insufficient to represent composed preferences. But the implications run deeper: insofar as general tasks consist of several objectives, Theorem 4 pushes Sutton's reward hypothesis to its limits. To escape impossibility, the Procrastinator's Peril is suggestive: to be productive, repeat play should not be rewarded. And for this to happen, we must keep track of past play, which suggests that **reward must be non-Markovian, even when all relevant objectives are Markovian**. That is, even if we have settled on some non-exhaustive state representation that is "sufficiently" Markovian, an extra aggregation step could render it no longer sufficient.

**Relaxing Markov Preference** The way in which non-Markovian rewards (or equivalently, non-Markovian utilities) can be used to escape Theorem 4 is quite subtle. Nowhere in the proofs of Theorems 1-4 is the Markov assumption explicitly used. Nor does it obviously appear in any of the Axioms. The Markov assumption _is_, however, invoked in two places. First, to establish history-independent comparability between the basic objects of preference--prospects \((s,)\)--and second, to extend that comparison set to include "prospects" of the form \((T(s,a),)\). To achieve initial comparability, Pitis  applied a "Markov preference" assumption (preferences over prospects are independent of history) together with an "original position" construction that is worth repeating here:

\[\\ \\ \\ ()\] (2)

In other words, to allow for comparisons between prospect \((s_{1},)\) and \((s_{2},)\), we prepend some pseudo-state, \(s_{0}\), and compare prospects \((s_{0}s_{1},)\) and \((s_{0}s_{2},)\). Markov preference then lets us cut off the history, so that our preferences between \((s_{1},)\) and \((s_{2},)\) are cardinal.

The impossibility result suggests, however, that aggregate preference is _not_ independent of history, so that construction 2 cannot be applied. Without this construction, there is no reason to require relative differences between \(V(s_{1},*)\) and \(V(s_{2},*)\) to be meaningful, or to even think about lotteries/mixtures of the two prospects (as done in Axiom 1). Letting go of this ability to compare prospects starting in different states means that Theorem 1 is applicable only to sets of prospects with matching initial states, unless we shift our definition of "prospect" to include the history; i.e., letting \(h\) represent the history, we now compare "historical prospects" with form \((h,)\).

Though this does not directly change the conclusion of Theorem 2, \(T(s,a)\) in \(V(T(s,a),)\) includes a piece of history, \((s,a)\), and can no longer be computed as \(_{s^{} T(s,a)}V(s^{},)\). Instead, since the agent is not choosing between prospects of form \((s^{},)\) but rather (abusing notation) prospects of form \((sas^{},)\), the expectation should be computed as \(_{s^{} T(s,a)}V(sas^{},)\).

The inability to compare prospects starting in different states also changes the conclusion of Theorem 3, which implicitly uses such comparisons to find constant coefficients \(w_{i}\) that apply everywhere in the original \(()\). Relaxing the application of Harsanyi's theorem to not make inter-state comparisons results in weights \(w_{i}(h)\) that are history dependent when aggregating the historical prospects.

### Possibility Result

Allowing the use of history dependent coefficients in the aggregation of \(V_{i}(T(s,a),)\) resolves the impossibility. The following result shows that given some initial state dependent coefficients \(w_{i}(s)\), we can always construct history dependent coefficients \(w_{i}(h)\) that allow for dynamically consistent aggregation satisfying all axioms. In the statement of the theorem, \((_{h})\) is used to denote the set of lotteries over prospects starting with history \(h\) of arbitrary but finite length (not to be confused with the set of lotteries over all historical prospects, of which there is only one). Note that if a preference relation satisfies Axioms 1-2 with respect to \(()\), the natural extension to \((_{hs})\), \((hs,_{1})(hs,_{2})(s,_{1})(s,_{2})\), satisfies Axioms 1-2 with respect to \((_{hs})\). Here, we are using \(hs\) to denote a history terminating in state \(s\).

**Theorem 5** (Possibility).: _Consider the aggregation of arbitrary individual preference relations \(\{_{};i\}\) defined on \(()\), and consequently \((_{h}),\  h\), that individually satisfy Axioms 1-2. There exists aggregated preference relations \(\{^{}\}\), defined on \((_{h}),\  h\), that satisfy Axioms 1-4._

_In particular, **given**\(s,a\), \(V_{}\), \(\{V_{i}\}\), \(\{w_{i}(s)\}\), **where (A)** each \(V_{i}\) satisfies Axioms 1-2 on \(()\), and \(V_{}\) satisfies Axioms 1-2 on \((_{h}), h\), **and (B)**\(V_{}(s,a)=_{i}w_{i}(s)V_{i}(s,a))\), **then**, choosing_

\[w_{i}(sas^{}):=w_{i}(sa):=w_{i}(s)_{i}(s,a)i,s^{}\] (3)implies that \(V_{}(sas^{},)_{i}w_{i}(sa)V_{i}(s^{},)\) so that the aggregated preferences \(\{_{}^{sas^{}}\}\) satisfy Axiom 3 on \((_{sas^{}})\). Unrolling this result--\(w_{i}(hasas^{}):=w_{i}(hs)_{i}(s,a)\)--produces a set of constructive, history dependent weights \(\{w_{i}(h)\}\) such that Axiom 3 is satisfied for all histories \(\{h\}\)._

Sketch of Proof.: The full proof is in Appendix B. Following the proof of Theorem 4, we arrive at

\[w_{1}(s)_{1}(s,a)=w_{1}(sa)_{}(s,a) w_{2 }(s)_{2}(s,a)=w_{2}(sa)_{}(s,a),\] (4)

from which we conclude that:

\[(sa)}{w_{1}(sa)}=(s)_{2}(s,a)}{w_{1}(s)_{1}( s,a)}.\] (5)

This shows the existence of weights \(w_{i}(sa)\), unique up to a constant scaling factor, for which \(V_{}(T(s,a),)_{i}w_{i}(sa)V_{i}(T(s,a),)\), that apply regardless of how individual preferences are chosen or aggregated at \(s\). Unrolling the result completes the proof. 

From Theorem 5 we obtain a rather elegant result: rational aggregation over time discounts the aggregation weights assigned to each individual value function proportionally to its respective discount factor. In the Procrastinator's Peril, for instance, where we started with \(w_{}(s)=w_{}(s)=1\), at the initial (and only) state \(s\), we might define \(w_{}(s)=0.5\) and \(w_{}(s)=0.9\). With these non-Markovian aggregation weights and \(_{}(s)=1\), you can verify that (1) the irrational procrastination behavior is solved, and (2) the aggregated rewards for work and play are now non-Markovian.

**Remark 4.1** (Important!): The discount \(_{}\) is left undetermined by Theorem 5. One might determine it several ways: by appealing to construction 2 with respect to historical prospects in order to establish inter-state comparability, by setting it to be the highest individual discount (0.9 in the Procrastinator's Peril), by normalizing the aggregation to weights to sum to 1 at each step, or perhaps by another method. In any case, determining \(_{}\) would also determine the aggregation weights, per equation 4 (and vice versa). We leave the consideration of different methods for setting \(_{}\) and establishing inter-state comparability of \(V_{}\) to future work. (NB: _This is a normative question_, which we leave unanswered. While one can make assumptions, as we will for our numerical example in Subsection 5.2, future research should be wary of accepting a solution just because it seems to work.)

### A Practical State Space Expansion

The basic approach to dealing with non-Markovian rewards is to expand the state space in such a way that rewards becomes Markovian [27; 14; 2]. However, naively expanding \(\) to a history of length \(H\) could have \(O((||+||)^{H})\) complexity. Fortunately, the weight update in equation 4 allows us to expand the state using a single parameter per objective. In particular, for history \(hsa\) and objective \(i\), we append to the state the factors \(y_{i}(hsa):=y_{i}(h)_{i}(s,a)/_{}(s,a)\), which are defined for every history, and can be accumulated online while executing a trajectory. Then, given a composition with any set of initial weights \(\{w_{i}\}\), we can compute the weights of augmented state \(s_{}=(s,y_{i}(hs))\) as \(w_{i}(s_{})=y_{i}(hs) w_{i}\). Letting \((^{(y)})\) be the set of prospects on the augmented state set, we get the following corollary to Theorem 5:

**Corollary 1**.: _Consider the aggregation of arbitrary individual preference relations \(\{_{i};i\}\) defined on \(()\), and consequently \((^{(y)})\), that individually satisfy Axioms 1-2. There exists aggregated preference relation \(\{_{}\}\), defined on \((^{(y)})\), that satisfies Axioms 1-4._

## 5 Discussion and Related Work

### A Fundamental Tension in Intertemporal Choice

The state space expansion of Subsection 4.2 allows us to represent the (now Markovian) values of originally non-Markovian policies in a dynamically consistent way. While this allows us to design agents that implement these policies, it doesn't quite solve the intertemporal choice problem.

In particular, it is known that dynamic consistency (in the form of Koopmans' Stationarity ), together with certain mild axioms, implies a first period dictatorship: the preferences at time \(t=1\) are decisive for all time  (in a sense, this is the very definition of dynamic consistency!). Generally speaking, however, preferences at time \(t 1\) are not the same as preferences at time \(t=1\) (this is what got us into our Procrastinator's Peril to begin with!) and we would like to care about the value at all time steps, not just the first.

A typical approach is to treat each time step as a different generation (decision maker), and then consider different methods of aggregating preferences between the generations . Note that (1) this aggregation assumes intergenerational comparability of utilities (see Remark 4.1), and (2) each generation is expressing their personal preferences about what happens in _all_ generations, not just their own. Since this is a single aggregation, it will be dynamically consistent (we can consider the first period dictator as a benevolent third party who represents the aggregate preference instead of their own). A sensible approach might be to assert a stronger version of Axiom 3 that uses preference:

**Axiom 3\({}^{}\)** (Strong Pareto Preference) If \(_{}\) (\( i\)), then \(_{}\); and if, furthermore, \( j\) such that \(_{j}\), then \(_{}\).

Using Axiom 3\({}^{}\) in place of Axiom 3 for purposes of Theorem 3 gives a representation that assigns strictly positive weights to each generation's utility. Given infinite periods, it follows (e.g., by the Borel-Cantelli Lemma for general measure spaces) that if utilities are bounded, some finite prefix of the infinite stream decides the future and we have a "finite prefix dictatorship", which is not much better than a first period one.

The above discussion presents a strong case _against_ using dynamic consistency to determine a long horizon policy. Intuitively, this makes sense: preferences change over time, and our current generation should not be stuck implementing the preferences of our ancestors. One way to do this is by taking time out of the equation, and optimizing the expected individual utility of the stationary state-action distribution, \(d_{}\) (cf. Sutton and Barto  (Section 10.4) and Naik et al. ):

\[J()=_{s}d_{}(s)V_{}(s)\] (6)

Unfortunately, as should be clear from the use of a stationary policy \(\), this time-neutral approach falls short for our purposes, which suggests the use of a non-Markovian policy. While optimizing equation \(6\) would find the optimal stationary policy in the Procrastinator's Peril (work forever with \(V(_{2})=3.0\)), it seems clear that we _should_ play at least once (\(V(_{3})=3.2\)) as this hurts no one and makes the current decision maker better off--i.e., simply optimizing (6) violates the Pareto principle.

This discussion exemplifies a known tension in intertemporal choice between Pareto optimality and the requirement to treat every generation equally--it is impossible to have both [16; 42]. In a similar spirit to Chichilnisky , who has proposed an axiomatic approach requiring both finite prefixes of generations and infinite averages of generations to have a say in the social preference, we will examine a compromise (to the author's knowledge novel) between present and future decision makers in next Subsection.

### N-Step Commitment and Historical Discounting

We now consider two solutions to the intertemporal choice problem that deviate just enough from dynamic consistency to overcome finite period dictatorships, while capturing almost all value for each decision maker. In other words, they are "almost" dynamically consistent. We leverage the following observation: due to discounting, the first step decision maker cares very little about the far off future. If we play once, then work for 30 steps in the Procrastinator's Peril, we are already better off than the best stationary policy, regardless of what happens afterward.

This suggests that, rather than following the current decision maker forever, we allow them commit to a non-Markovian policy for some \(N<\) steps that brings them within some \(\) of their optimal policy, without letting them exert complete control over the far future. To implement this, we could reset the accumulated factors \(y_{i}\) to \(1\) every \(N\) steps. This approach is the same as grouping every consecutive \(N\) step window into a single, dynamically consistent generation with a first period dictator.

A more fluid and arguably better approach is to discount the past when making current decisions ("historical discounting"). We can implement historical discounting with factor \(\) by changing the update rule for factors \(y_{i}\) to

\[y_{i}(hsa)}=[y_{i}(h)(s,a)}{_ {}(s,a)}]+(1-)w_{i}^{n}(s),\]

where \(w_{i}^{n}(s)\) denotes the initial weight for objective \(i\) at the \(n\)th generation (if preferences do not change over time, \(w_{i}^{n}=w_{i}\)). This performs an exponential moving average of preferences over time high \(<1\) initially gives the first planner full control, but eventually discounts their preferences until they are negligible. This obtains a qualitatively different effect from \(N\)-step commitment, since the preferences of all past time steps have positive weight (by contrast, on the \(N\)th step, \(N\)-step commitment gives no weight to the \(N-1\) past preferences). As a result, in the Procrastinator's Peril, any sufficiently high \(\) returns policy \(_{3}\), whereas \(N\)-step commitment would play every \(N\) steps.

Historical discounting is an attractive compromise because it is both Pareto efficient, and also anonymous with respect to tail preferences--it both optimizes equation \(6\) in the limit, _and_ plays on the first step. Another attractive quality is that historical discounting allows changes in preference to slowly deviate from dynamic consistency over time--the first period is _not_ a dictator.

As a numerical example, we can consider what happens in the Procrastinator's Peril if we start to realize that we value occasional play, and preferences shift away from the play MDP to a playn MDP. The playn MDP has fixed \(_{}=0.9\) and non-Markovian reward \(R(\,|\,)=0.5,\ R()=0\). We assume preferences shift linearly over the first 10 timesteps, from \(w_{}=1,w_{}=0\) at \(t=0\) to \(w_{}=0,w_{}=1\) at \(t=10\) (and \(w_{}=1\) throughout). Then, the optimal trajectories \(^{*}\) for different \(\), and resulting discounted reward for the _first_ time step, \(V^{1}(^{*})\), are as follows:

   \(\) & \(^{*}\) & \(V^{1}(^{*})\) \\  \(0.00\) & play for 5 steps, then play every 10 steps & \(2.635\) \\ \(0.30\) & play for 5 steps, then play every 10 steps & \(2.635\) \\ \(0.50\) & play for 3 steps, then play every 10 steps & \(2.932\) \\ \(0.90\) & play, then work for 14 steps, then play every 10 steps & \(3.105\) \\ \(0.95\) & play, then work for 23 steps, then play every 10 steps & \(3.163\) \\ \(0.98\) & play, then work for 50 steps, then play every 10 steps & \(3.198\) \\ \(1.00\) & play, then work forever (same as \(_{3}\)) & \(3.200\) \\    When \(=0\), each time step acts independently, and since the play MDP has high weight at the start, we experience a brief period of consistent play in line with the original Procrastinator's Peril, before preferences fully shift to occasional play. With high \(<1\), we capture almost all value for the first time step, while also eventually transitioning to the equilibrium "play every 10 steps" policy.

### Extension to Boltzmann Policies

The preference relation \(\) is deterministic, but the associated policy does not have to be. In many cases--partially-observed, multi-agent, or even fully-observed settings --stochastic policies outperform deterministic ones. And for generative sequence models such as LLMs , stochastic policies are inherent. We can extend our analysis--impossibility, possibility, state space expansion, and intertemporal choice rules--to such cases by adopting a stochastic choice rule.

To formalize this, we will take the stochastic choice rule as primitive, and use it to define a (deterministic) relation \(\) that, for practical purposes, satisfies Axioms 1-4 . We assume our choice rule satisfies Luce's Choice Axiom , which says that the relative rate of choosing between two alternatives in a choice set of \(n\) alternatives is constant regardless of the choice set. This can be implemented numerically with a Bradley-Terry choice model  by associating each alternative \(a_{i}\) with a scalar \((a_{i})\), so that given choice set \(\{a_{i},a_{j}\}\), \(p(a_{i})=(a_{i})/((a_{i})+(a_{j}))\). (An interesting interpretation for \((a_{i})\) that connects to both probability matching  and statistical mechanics  is as the number of "outcomes" (microstates) for which \(a_{i}\) is the best choice.)

We then simply "define" preference as \(a_{i} a_{j}(a_{i})>(a_{j})\), and utility as \(V(a_{i}):=k(a_{i})\), so that the policy is a softmax of the utilities. The way these utilities are used in practice (e.g., ) respects Axioms 1-2. And summing utilities is a common approach to composition , which is consistent with Harsanyi's representation (Theorem 3). For practical purposes then, the impossibility result applies whenever composed objectives may have different time preference.

**Remark 5.3**: Unlike our main results, this extension to Boltzmann policies is motivated by practical, rather than normative, considerations. Simple counterexamples to Luce's Choice Axiom exist  and probability matching behavior is evidently irrational in certain circumstances . We note, however, that certain theoretical works tease at the existence of a normative justification for Boltzmann policies ; given the practice, a clear justification would be of great value.

### Related Work in RL

**Task Definition**: Tasks are usually defined as the maximization of expected cumulative reward in an MDP [74; 55; 67]. Preference-based RL  avoids rewards, operating directly with preferences (but note that preference aggregation invokes Arrow's impossibility theorem [5; 48]), while other works translate preferences into rewards [17; 72; 12; 35]. This paper joins a growing list of work [54; 1; 76; 77] that challenges the implicit assumption that "MDPs are enough" in many reward learning papers, particularly those that disaggregate trajectory returns into stepwise rewards [22; 56; 59].

**Discounting**: Time preference or discounting can be understood as part of the RL task definition. Traditionally, a constant \(\) has been used, although several works have considered other approaches, such as state-action dependent discounting [85; 66] and non-Markovian discounting [24; 63]. Several works have considered discounting as a tool for optimization  or regularization [36; 4; 57].

**Task Compositionality**: Multi-objective RL  represents or optimizes over multiple objectives or general value functions , which are often aggregated with a linear scalarization function [7; 3]. Rather than scalarizing to obtain a single solution to a multi-objective problem, one can also seek out sets of solutions, such as the set of Pareto optimal policies  or the set of acceptable policies . Several works have also considered formal task decompositions [14; 51] where simple addition of MDPs is insufficient . More broadly, in machine learning, composition can be done via mixtures of experts and/or energy-based modeling [34; 21], which have also been applied to RL [29; 41]. Our results provide normative justification for linear scalarization when time preference is the same for all objectives, but call for non-Markovian adjustments when time preferences differ.

**Non-Markovian Rewards**: The necessity of non-Markovian rewards was demonstrated in other settings by Abel et al.  and more recently, in a concurrent work by Skalse and Abate . Though several papers explicitly consider RL with non-Markovian rewards [27; 60], this is usually motivated by task compression rather than necessity, and the majority of the RL literature restricts itself to Markovian models. Many popular exploration strategies implicitly use non-Markovian rewards [37; 53]. Our work is unique in that non-Markovian rewards arise from aggregating strictly Markovian quantities, rather non-Markovian quantities present in the task definition or algorithm.

## 6 Conclusion and Future Work

The main contribution of this work is an impossibility result from which one concludes that non-Markovian rewards (or an equivalent state expansion) are likely necessary for agents that pursue multiple objectives or serve multiple principals. It's possible that this will be the case for any advanced agent whose actions impact multiple human stakeholders. To accurately align such agents with diverse human preferences we need to endow them with the capacity to solve problems requiring non-Markovian reward, for which this paper has proposed an efficient state space expansion that uses one new parameter per aggregated objective. While the proposed state space expansion allows multi-objective agents to have dynamically consistent preferences for future prospects, it does not, in itself, solve the intertemporal choice problem. To that end, we have proposed "historical discounting", a novel compromise between dynamic consistency and fair consideration of future generations.

Interesting avenues for future work include quantifying the inefficiency of Markovian representations, investigating normative approaches to aggregating preferences based on subjective world models (Remark 3.2.2), considering the existence of an "objective" time preference (Remark 3.3.2), improving methods for determining subjective time preference (e.g., ), implementing and comparing approaches to determining \(_{}\) (Remark 4.1), investigating historical discounting in more detail (Subsection 5.2), and considering the existence of a normative justification for Boltzmann policies and their composition (Subsection 5.3).