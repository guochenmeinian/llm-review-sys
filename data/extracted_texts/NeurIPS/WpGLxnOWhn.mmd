# Joint Training of Deep Ensembles Fails Due to Learner Collusion

Alan Jeffares

University of Cambridge

aj659@cam.ac.uk

&Tennison Liu

University of Cambridge

tl522@cam.ac.uk

&Jonathan Crabbe

University of Cambridge

jc2133@cam.ac.uk

&Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

###### Abstract

Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed _post hoc_. In this work, we show that this is for good reason - _joint optimization of ensemble loss results in degenerate behavior_. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base learners collude to artificially inflate their apparent diversity. This pseudo-diversity fails to generalize beyond the training data, causing a larger generalization gap. We proceed to comprehensively demonstrate the practical implications of this effect on a range of standard machine learning tasks and architectures by smoothly interpolating between independent training and joint optimization.1

## 1 Introduction

Deep ensembles  have proven to be a remarkably effective method for improving performance over a single deep learning model. This success has been attributed to deep ensembles exploring multiple modes in the loss landscape , in contrast to variational Bayesian methods, which generally attempt to better explore a single mode [e.g. 3]. Recent work has proposed an alternative view suggesting that deep ensembles' success can be explained as being an effective method of scaling smaller models to match the performance of a single larger model . Regardless of the specific mechanism, deep ensembles' empirical success has been at the heart of numerous state-of-the-art deep learning solutions in recent years [e.g. 5, 6, 7].

Historically, non-neural network ensembles such as a bagging predictor  or a random forest  were generally trained independently and aggregated at test time. Sequential training methods such as boosting  were also developed to produce more collaborative ensembles. Training individually and evaluating jointly was sensible for these methods, in which the base learners were typicallydecision trees that _require_ individual training. Formally, for a single example \(\), a loss function \(\) and denoting the prediction of learner \(j\) on \(\) as \(_{j}^{d}\) with target \(^{d}\), we define the _joint objective_ as \((_{j=1}^{M}_{j},)\) and the _independent objective_ as \(_{j=1}^{M}(_{j},)\).2 Interestingly, modern deep ensembles are also generally trained independently and evaluated jointly. This is less intuitive as the performance of the ensemble as a whole (i.e. the joint objective) is the true objective of interest and jointly training a deep ensemble is quite natural for the backpropagation algorithm. Indeed, one might expect there to be no need for the proxy objective of independent training in this case, as the joint objective _should_ find a solution in which individual learners collaborate to minimize the ensemble's loss. In practice, however, independent training generalizes better than joint training in this context. A phenomenon that we demonstrate in Section 3 and investigate throughout this work.

In Section 4, we demonstrate that ensemble diversity is the foundation upon which understanding the limitations of joint training sits. We show that any twice differentiable joint loss function of an ensemble can be decomposed into the individual losses and a second term, which can be interpreted as ensemble diversity. This diversity term encourages some level of ambiguity among the base learners' predictions, a generalization of well-established work in the regression setting . Mathematically, this term is exactly the difference between independent and joint training and, therefore, must encapsulate the disparity in performance. In Section 5, we introduce a scalar weighting on the diversity term allowing us to smoothly interpolate between the extremes of fully independent and fully joint training.

Then, in Section 6, we diagnose and empirically verify a precise cause of the sub-optimal performance of jointly trained ensembles. We show that the additional requirement for diversity in the training objective can be trivially achieved by an adverse effect we term _learner collusion_. This consists of a set of base learners artificially biasing their outputs in order to trivially achieve diversity while ensuring that these biases cancel in aggregate as illustrated in Figure 1. We perform extensive experiments verifying this hypothesis. We then show that although training loss can be driven down by learner collusion, this does not generalize beyond the training set, resulting in a larger generalization gap than independent training. Finally, we examine the practical implications of learner collusion on standard machine learning benchmarks.

This work addresses the pervasive yet relatively understudied phenomenon that _directly optimizing a deep ensemble fails_. **(1)** We unify various strands of the ensemble literature to provide a deep, practical categorization of ensemble training and diversification. **(2)** We present and verify a comprehensive explanation for this counter-intuitive limitation of deep ensembles with both theoretical and empirical contributions. **(3)** We investigate the catastrophic consequences of this issue on practical machine learning tasks. As this investigation takes a somewhat non-standard format, we provide an abridged summary of this work and its technical contributions in Appendix A.

## 2 Background

**Independent training.** This refers to ensembles3 in which _each base learner is optimized independently, with no influence from the other ensemble members either explicit or implicit_. This includes many non-neural ensemble techniques in which diversity between the base learners results in ensemble performance better than any individual. This diversity is often achieved by sampling the input space [e.g. bagging in 8] and the feature space [e.g. random forest in 9]. In the case of deep ensembles of neural networks, independent training is also generally applied, with diversity achieved using random weight initialization and stochastic batching [1; 2]. Interestingly, bootstrap sampling of the inputs is ineffective in the case of deep ensembles . Several notable state-of-the-art results

Figure 1: **Learner collusion. An illustrative example in which a subset of base learners (\(f_{1}\) & \(f_{2}\)) _collude_ in output space by shifting their predictions in opposing directions such that the ensemble prediction (\(\)) is unchanged, but diversity is inflated.**

have been achieved by ensembling several independently trained but individually performant neural networks. For example, the groundbreaking results in the ImageNet challenge were often achieved by independently trained ensembles such as VGG  GoogLeNet , and AlexNet . Seq2Seq ensembles of long short-term memory networks  achieved remarkable performance for neural translation. More recently in the tabular domain, ensembles of transformers have outperformed state-of-the-art non-neural methods .

**Ensemble-aware individual training.** This refers to the case where _each base learner is still being optimized as an individual, but with some additional technique being applied to coordinate the ensemble members_. A prominent approach is sequentially trained, boosting ensembles in which each new member attempts to improve upon the errors of the existing members (e.g. AdaBoost  & gradient boosting ). In the case of deep ensembles, an alternative approach consists of training each base learner with a standard individual loss with the addition of an ensemble-level regularization term designed to manage diversity. This often consists of an f-divergence between the learners' predictive distributions. Examples include the pairwise \(^{2}\)-divergence , \(KL\)-divergence, but between the learner and the ensemble , and a computationally efficient, kernelized approximation of Renyi-entropy . Another prominent regularization approach is based on the seminal idea of negative correlation learning (NCL) which penalizes individual learners through a correlation penalty on error distributions [20; 21]. This penalty weakens the relationship with other learners and controls the trade-off between bias, variance, and covariance in ensemble learning . Recent works have adapted NCL for improved computational efficiency with large base learners  and attempt to generalize NCL beyond the regression setting .  encourages ensemble diversity in weight and functional space using a "repulsive" term while other works show that a single learner can be viewed as an ensemble with diversity encouraged implicitly (e.g. [26; 27]). The efficient BatchEnsemble  can also be characterized as an ensemble-aware individual training approach.

**Joint training.** This refers to the case where _the ensemble's predictions are optimized directly as if it were a single model_. Intuitively, one might naturally expect joint training to outperform the previously discussed methods as it directly optimizes (at training time) the test metric of all three approaches - _ensemble loss_. Empirically, however, this is not the case as joint training achieves inferior performance in general.  reported poor results when joint training averages the base learners' output scores but excellent performance when they are averaged at the log-probability or loss levels. However, later work showed that the two alleged success cases were, in fact, implementing independent training rather than joint training unbeknownst to the authors . This later work also reported poor generalization for score-averaged ensembles. We provide a detailed clarification and gradient analysis in Appendix E. Elsewhere,  experienced poor performance from joint training for both score and probability averaging. Concurrently to this work, [32; 33] also found that joint training consistently resulted in worse empirical performance. To put the matter to rest, we demonstrate these issues with joint training in the next section.

## 3 Empirically Evaluating Joint Training

In this section, we briefly demonstrate the effect that we investigate throughout this work. That is, when evaluating the ensemble loss at test time, optimizing for that same ensemble loss objective at training time (i.e. joint training) is _less_ effective than optimizing each of the base learners independently (i.e. independent training). We do this by performing a straightforward comparison of the two methods. We compare the test set performance of independent and joint training on ImageNet . We consider several architectures as base learners of an ensemble of various sizes determined by computational limitations. We report test set top-1 and top-5 accuracy for both independently and jointly trained ensembles. These results are included in Table 1. Complete experimental details for all experiments in this work are provided in Appendix B.

Here, we observe the counter-intuitive phenomenon that independently trained ensembles consistently outperform jointly trained ensembles, despite all models being evaluated using the joint objective. This is consistent with previous works that have attempted to apply joint training [30; 31]. In the following sections, we will proceed to investigate and explain this effect through the lens of ensemble diversity.

**Summary:** Joint training performs considerably worse than independent training in practice.

## 4 Diversity in Ensemble Learning

To understand the limitations of joint training, we will need to examine the joint objective through the lens of diversity. In this section, we argue for a unified definition of diversity which generalizes much of the existing literature on the subject.

### Preliminaries

We begin with some notation. We consider ensembles made up of \(M\) individual learners. The \(j\)'th individual learner maps an input \(^{p}\) to a \(d\)-dimensional output space such that \(_{j}:^{p}^{d}\) and attempts to learn some regression target(s) \(^{d}\) or, in the case of classification, labels which lie on a probability simplex \(^{d}\). Without loss of generality, we can refer to \(\). We primarily consider combining individual learner predictions into a single ensemble prediction as a weighted average \(_{j=1}^{M}w_{j}_{j}()=}()\) where \(w_{j}\) denotes the scalar weights. Although not strictly required for much of this work, we will generally assume that these weights satisfy the properties \(_{j=1}^{M}w_{j}=1\) and \(w_{j} 0\)\(\)\(j\), ensuring that each individual learner can be interpreted as predicting the target. Scalars are denoted in plain font and vectors in bold. We generally drop the dependence on \(\) in our notation such that individual and ensemble predictions are denoted \(_{j}\) and \(}\) respectively. Finally, we formalize the respective losses of the ensemble and the base learners in Definition 4.1.

**Definition 4.1** (Ensemble and Individual Loss).: For a given loss function \(:^{2}^{+}\), we define:

1. The overall ensemble loss \((},)\).
2. The weighted individual loss of the base learners \(}_{j=1}^{M}w_{j}(_{j}, )\).

Intuitively, \(\) measures the predictive performance of the ensemble, while \(}\) measures the aggregate predictive performance of the individual ensemble members.

### A Generalized Diversity Formulation

**Diversity in regression ensembles.** The prevailing view of diversity in regression ensembles began in , which showed that the mean squared error of a regression ensemble can be decomposed into the mean squared error of the individual learners and an additional _ambiguity_ term which measures the variance of the individual learners around the ensemble prediction showing

\[_{j}w_{j}(f_{j}-)^{2} =_{j}w_{j}(f_{j}-y)^{2}-(-y)^{2},\] (1) \[ =}-.\]

This decomposition provides an intuitive definition of diversity whilst also showing that a jointly trained ensemble, that is one that simply optimizes \(\), encourages diversity by default due to it being _baked into the objective_. Later work from the evolutionary computation literature proposed an apparently alternative measure of diversity called Negative Correlation Learning, which argues

    &  &  &  \\  &  &  &  &  &  \\  ResNet-18  & 5 & \(\) & \(\) & \(62.74\%\) & \(84.03\%\) \\ ResNet-34  & 3 & \(\) & \(\) & \(68.91\%\) & \(88.01\%\) \\ DenseNet  & 3 & \(\) & \(\) & \(60.81\%\) & \(81.35\%\) \\ ShuffleNet V2 (\(0.5\))  & 10 & \(\) & \(\) & \(53.41\%\) & \(74.88\%\) \\ ShuffleNet V2 (\(1\))  & 5 & \(\) & \(\) & \(61.96\%\) & \(81.56\%\) \\ SqueezeNet V1.1  & 5 & \(\) & \(\) & \(42.69\%\) & \(65.24\%\) \\ MobileNet V3-small  & 5 & \(\) & \(\) & \(57.34\%\) & \(78.30\%\) \\ MobileViT  & 3 & \(\) & \(\) & \(54.69\%\) & \(76.79\%\) \\   

Table 1: **Joint training on ImageNet.** We compare independent to joint training across a range of architectures trained from scratch on the ImageNet dataset. Top 1 & 5 test accuracy is reported for both methods, where independent training consistently performs better.

that lower correlation in individual errors is equivalent to higher diversity . The apparent disagreement between these two seemingly valid views on diversity was resolved when it was shown that they are in fact equivalent up to a scaling factor of the diversity term .

**Generalized diversity.** Motivated by the regression case, we will argue that the difference between the individual and ensemble errors has a natural interpretation as the diversity among the predictions of the ensemble members _for any twice (or more) differentiable loss function_. In fact, this generalizes several existing notions of diversity proposed throughout the literature for specific cases. Specifically, we define diversity as in Definition 4.2 and will proceed to justify this definition in what follows.

**Definition 4.2** (Diversity).: The diversity of an ensemble of learners (DIV) is the difference between the weighted average loss of individual learners and the ensemble loss:

\[}-\]

**Diversity in classification ensembles.** It is natural to next verify that this definition is appropriate in the classification setting. We proceed by concretely deriving the diversity term in this setting. There are two standard methods for aggregating base learners' predictions into a single ensemble prediction in classification. Averaging at the score (preeactivation) level and averaging at the probability level. We address each of these in turn.

**1**_Score averaging_. We consider a classification task with target distribution \(^{d}\) with \(d\) classes, and consider ensembling by averaging the base learners' scores, before applying the normalizing map \(\) (typically a softmax). We define \(_{j}:^{d}_{j}\) as the normalized predictive distribution for learner \(j\) and \(}=}\) the predictive distribution for the ensemble, where \(}=_{j=1}^{M}w_{j}_{j}\) as before. In this setting, it can be shown  that

\[=_{j=1}^{M}w_{j}D_{KL}(||_{j})-D_{KL}( ||})=_{j=1}^{M}w_{j}D_{KL}(}|| _{j}),\]

where \(D_{KL}\) is the \(KL\)-divergence. Intuitively, this diversity term measures the weighted divergence between each ensemble member's predictive distribution and the full ensemble's predictive distribution, a sensible expression for diversity. It should be noted that this decomposition holds for normalizing functions beyond the softmax . Specifically, this exact expression holds for all exponential family normalization functions with score averaging. This allows the diversity decomposition described in this work to apply to disparate settings as demonstrated by its recent application in modeling diversity among the Poisson spike train outputs of spiking neural networks .

**2**_Probability averaging_. This setting diverges from score averaging by setting the ensemble predictive distribution \(}\) to be the average of the base learners' _probabilities_ such that \(}=_{j=1}^{M}w_{j}(_{j})\). This is a more natural strategy, averaging the scale-invariant probabilities while also providing each base learner with unique gradients during backpropagation (see a detailed gradient analysis in Appendix E). The resulting expression for diversity is included in Theorem 4.3.

**Theorem 4.3**.: _For a probability averaged ensemble classifier, trained using cross-entropy loss \(_{CE}\) with \(k^{}\) denoting the ground truth class, diversity is given by_

\[=_{j=1}^{M}w_{j}_{CE}(,_{j})- _{CE}(,})=_{j=1}^{M}w_{j} y(k^{ })(k^{})}{p_{j}(k^{})}\]

Proof.: Please refer to Appendix C.3. 

_Remark 4.4_.: It is not immediately obvious that this expression has a natural interpretation as diversity. However, in Appendix C.3 we show it to be well aligned with the notion of diversity used throughout this work by analysis of the distribution of \(p_{j}(k^{})\) around \((k^{})\).

Beyond the KL-divergence family, alternative loss functions also result in sensible diversity expressions. Brier score, expressed as \((}-)^{T}(}-)\), can be shown to obtain \(=_{j}w_{j}(}_{j}-)^{T}(}-_{j})\), the variance of the base learner probabilities around the ensemble probabilities .

For 0-1 loss and the case of majority voting, diversity is also measured by the disagreement between the (categorical) predictions of the base learners and the ensemble prediction .

**Diversity in a general setting.** The previous paragraphs verified the diversity definition introduced in Definition 4.2 to be a sensible interpretation for several common loss functions. A more general question can be asked -- _Is \(\) reasonable for any choice of loss functions?_ The answer is _yes_, and this turns out to be no coincidence. To demonstrate why, we analyze the properties of the diversity expression for _any_ twice differentiable loss function and discover a general form that permits a clear interpretation as diversity. To do so, we present Theorem 4.5, a multidimensional generalization of one proposed in  which, in turn, generalizes our notion of diversity and analysis of joint training to practically any loss function.

**Theorem 4.5**.: _[Diversity for General Loss] For any loss function \(:^{2}^{+}\) with \(()^{*}\) that is at least twice differentiable, the loss of the ensemble can be decomposed into_

\[=_{j=1}^{M}w_{j}\;(_{j}- })^{}_{f}^{*}(_{j}^{*}, )(_{j}-})\] \[_{j}^{*}=}+c_{j}(_{j} -}),\]

_where \(_{f}^{}\) is the Hessian of \(\) with respect to its first argument and some \(c_{j}\) for all \(j[M]\)._

Proof.: Please refer to Appendix C.2. 

Here, the measure of diversity is obtained by performing a Taylor expansion on the loss of the output of \(_{i}\) near the ensemble output \(}\) and using Lagrange's remainder theorem to obtain an _exact equality_. Additionally, \(_{i}^{*}\) is an uncertain number, which approaches a constant in the limit. This diversity admits the same interpretation as the variance of learners around the ensemble, but is now weighted by a term dependent on the second derivative of the loss function.

One can easily verify that this generalized definition encompasses the uni-dimensional MSE ambiguity decomposition in Equation (1), where \(^{}(f_{i}^{*},y)=2\). Similarly, the multidimensional MSE \((,)=\|-\|_{2}^{2}\) leads to the Hessian \(_{f}^{}=2\), where \(\) is the identity matrix on \(\). From this, we get the multi-dimensional generalization \(=_{j}w_{j}\|_{j}-}\|^{2} 0\). Importantly, diversity has the same non-negative property (\( 0\)) for any loss function with semi-positive definite Hessian \(_{f}^{} 0\), such that ensemble performance is always better than individual learner performance. Another typical example is the classification loss \((,)=-^{}\). Here, it is trivial to show that \(_{f}^{}(,)=( ^{2})\), where \(\) denotes the component-wise division. Again, this Hessian matrix is positive semi-definite for \(,^{d}\) so that \( 0\). Revisiting our previous question, we verify that, for _at least twice-differentiable_ loss functions (e.g. logistic loss: \(=(1+^{-yf})\) & exponential loss: \(=^{-yf}\)), \(\) admits a generalized interpretation as the variance of individual predictors around ensemble prediction. This makes a robust case for Definition 4.2, which provides a principled estimate of diversity (as opposed to heuristic measures) and can be easily obtained by calculating the difference between ensemble loss and weighted average learner loss.

**Summary:** The difference between independent loss and joint loss has a natural interpretation as diversity, providing a robust generalization of much of the existing literature.

## 5 An Augmented Objective

Given this generalized definition of diversity, one might naturally consider placing a scalar weighting on its contribution, resulting in an augmented training objective. In fact, such an approach has already been applied for special cases of diversity including regression , score averaging , and, concurrently to this work, in [33; 32]. Throughout our experiments, we will consider this augmented objective in Equation (2), allowing us to analyze the effects of smoothly transitioning between independent and joint training.

\[^{}(},) :=}-\] \[=(1-)}+.\] (2)

When \(=0\), this amounts to training each learner independently (i.e. _independent training_) and \(=1\) corresponds to optimizing the loss of the ensemble as a whole (i.e. _joint training_). Clearly, intermediate values of \(\) can be interpreted as interpolating between the two.

When examining Equation (2), one might be tempted to encourage more diversity by setting \(>1\). Previous work by Brown et al.  showed that, in the case of regression, optimization can become degenerate for larger values of \(\), proving an upper bound of \(<\). More recently,  showed that in the case of score averaged ensembles using their _modular loss_, the ensemble loss ERR was unbounded from below for \(>1\) - i.e. DIV can trivially be increased faster than its corresponding increase in \(}\), thus (superficially) minimizing the loss. In Theorem 5.1 we generalize this same bound on \(\) to _all_ loss functions and further include the case of probability-averaged ensembles.

**Theorem 5.1**.: _[Pathological Behavior for \(>1\)] We consider a target set \(=^{d}\) or \(^{d}\) being a compact and convex subset of \(^{d}\). Let \(:^{2}^{+}\) be a continuous and finite function on the interior \((^{2})\). We assume that the loss is not bounded from above on \(^{2}\) in such a way that there exists \(()\) and a sequence \((_{t})_{t}\) such that \((_{t},)+\) for \(t\). If \(>1\), then the augmented loss \(^{}\) defined in Equation (2) is not bounded from below on \(^{2}\)._

Proof.: Please refer to Appendix C.4. 

_Remark 5.2_.: Note that we consider the case where \(\) is a compact and convex subset of \(^{d}\) to include classifiers, for which the target set is a probability simplex \(^{d-1}\).

As a consequence of the above theorem, one needs to impose the tighter bound \( 1\) in order to have a well-defined optimization objective. Furthermore, intermediate values where \((0,1)\) may have an additional interpretation beyond interpolating between independent and joint training. In Appendix D we show that, in the regression setting, \(^{}\) is exactly equivalent to applying independent training while adjusting the targets to account for the errors of the ensemble as measured by its gradient. We derive an exact equivalence between \(\) in \(^{}\) and the magnitude of the adjustment of the target in this gradient-adjusted target setting.

**Summary:** An interpolation between independent training and joint training is a well-motivated and useful objective but, to avoid pathological behavior, one should not extrapolate to higher diversity.

## 6 Why Does Joint Training Fail?

Given the universal interpretation of diversity developed in the previous sections paired with its exclusive impact on the joint training objective, we now turn our attention to answering the question - _why does joint training of deep ensembles perform worse than independent training?_ Some previous works have attempted to partially address this question.  suggested that score averaging fails as the softmax function "distorts" the averages. However, little evidence exists to support this claim. When faced with the same issue,  suggested the issue could be due to score averaged ensemble members receiving identical gradient values. We address this point in a gradient analysis in Appendix E, where we (a) point out that the optimal solution to joint objective would still achieve equal or lower loss for score averaging, and (b) show that probability averaged ensemble members receive variable gradient expressions and _still_ suffer from poor performance. Finally,  noticed vastly different test set performances among the base learners which they referred to as _model dominance_. Indeed, this is a symptom of the underlying issue with joint training which we describe and verify throughout the rest of this section. The explanation we propose for the poor empirical results achieved by joint training consists of two claims:

_Claim 1:_ the additional diversity term in the joint learning objective (Equation (2)) results in a dual objective. The DIV term can always be artificially inflated if multiple learners collude to bias their predictive distributions in opposing directions which cancel out in aggregate and, therefore, is easily maximized across the training data. We refer to this phenomenon as _learner collusion_.

_Claim 2:_ while learner collusion can still minimize the loss on the training data, this solution may contain little genuine diversity resulting in a solution that achieves worse generalization than independent training. In Appendix F, we provide a detailed illustrative example of learner collusion for the regression case from an optimization perspective.

In Section 6.1 we empirically demonstrate this learner collusion effect is emergent. Then, in Section 6.2, we show that the low training loss achieved by learner collusion generalizes poorly onto unseen data. Finally, in Section 6.3, we establish the practical implications of learner collusion on standard tasks.

### Diagnosing Learner Collusion

In this section, we focus on claim 1 and empirically investigate the extent to which _learner collusion_ does occur. Recall that the effect being investigated is that a subset of non-diverse base learners can trivially adjust their similar, or even identical, predictions to appear diverse (i.e. Figure 1). If learner collusion does indeed take place we suggest that the following three effects should be observable empirically: _(1)_ the relative magnitude of diversity should be excessive relative to the ensemble loss, _(2)_ if we remove constant output biases from the individual learners, the ensemble diversity should decrease significantly, and _(3)_ learners should become highly dependent on each other such that removing a small subset of base learners is detrimental to the ensemble performance. We investigate each of these effects in turn.

We perform these experiments on four tabular datasets of the style upon which ensemble methods typically excel . Here we focus on regression as it offers the most controlled setting to isolate the learner collusion phenomenon (see Appendix F), but in later experiments we demonstrate its effects in the classification setting too. We smoothly interpolate between independent and joint training using the augmented objective controlled by \(\) from Section 5. We report mean \(\) one standard deviation evaluated on a test set throughout. An extended description of the experimental protocol for these experiments is provided in Appendix B.

**Diversity explosion.** We begin with the most simple expected effect of learner collusion, that the relative magnitude of diversity is excessive with respect to the ensemble loss. The results are included in Figure 2 where we note a sharp increase in diversity for the joint objective. Specifically, we note that mean test set diversity remains relatively low for most values of \(\) but quickly explodes as \( 1\) to levels that are often significantly greater than the mean squared error of the ensemble predictions. This indicates that the base learners' variation around their own mean is much greater than the ensemble's variation around the label.

**Debiased diversity.** We now proceed to the second postulated effect of learner collusion. Although learners could perform learner collusion on a per-example basis, the most straightforward form of this behavior would be learners that bias their predictions without any dependence on the example at hand. To investigate the extent to which this trivial form of collusion does occur we apply a post hoc debiasing to each of the individual learners in the ensemble which affects the trade-off between ERR and DIV without affecting ERR. Specifically, we adjust learner \(j\)'s predictions such that

\[f_{j}^{}(_{i}) =f_{j}(_{i})-b_{j}\] \[=f_{j}(_{i})-(_{i=1}^{N}f_{j}( _{i})-_{i=1}^{N}(_{i})).\]

In Appendix C.1 we prove that the debiasing term \(b_{j}\) is the optimal diversity minimizer that keeps the ensemble predictions unchanged. In Figure 3 we show that applying this technique significantly reduces the ensemble's apparent diversity to far more reasonable values. In general, we find that this typically removes around half of the ensemble's diversity at the joint training region (\(=1\)). While example-dependent collusion could also emerge, these results already indicate that a significant proportion of the apparent diversity in jointly trained ensembles is due to an artificial biasing of outputs.

Figure 2: **Diversity explosion. Test set diversity explodes relative to MSE across four datasets. We note two further empirical phenomena: (1) the non-linear relationship between \(\) and diversity which we discuss further in Appendix D and (2) the minimal effect diversity has on the test MSE on the Facebook dataset which is discussed in Appendix F.**

**Learner codependence:** The final effect we predicted to emerge due to learner collusion is an excessive codependence among the base learners. If individual learners are engaging in learner collusion, the removal of a subset of these learners at test time should be catastrophic to the performance of the ensemble. This is because a learner producing excessive errors needs to be counterbalanced by other learners performing similar errors in the opposite direction, otherwise the ensemble predictions will be influenced by the poorly performing base learners. To test this we consider dropping a fraction of the individual learners at test time and evaluate the resulting ensemble performance. This is similar to experiments performed in , although from a very different perspective. In Figure 4 we present the results of randomly applying various levels of test time base learner dropout 100 times for each model and report the average resulting increase in test set error. These results clearly show that such a procedure is detrimental to ensemble performance at precisely the levels of \(\) at which we have hypothesized learner collusion to occur, while having only minimal impact elsewhere.

**Summary: Extensive empirical evidence confirms that training with the joint objective results in learner collusion.**

### Learner Collusion Generalizes Poorly

Given that we have shown that learner collusion emerges from joint training, we now proceed to examine the second claim. That is, although learner collusion does not prevent the training loss from being minimized, the solution it obtains will generalize more poorly than an independently trained ensemble with non-trivial diversity. This claim is more straightforward to investigate and only requires a comparison of the difference between the training and testing loss curves over the course of training. The difference between the two, which we refer to as the _generalization gap_, captures the drop in performance when a model transitions from training to testing data.

For this experiment, we return to the more standard machine learning benchmark of classification on the CIFAR tasks as introduced in Section 3. On each dataset we train a joint model (ERR) and an independent model (ERR) and report the generalization gap of ensemble loss for both methods over the course of training. We repeat this six times and include a shaded region representing \( 2\) standard errors. The results are included in Figure 5 where we consistently find that joint training displays a significantly larger generalization gap as training proceeds. This is strongly suggestive of a much greater tendency towards overfitting in the jointly trained model while the independently trained model's generalization gap plateaus at a much lower level. We provide further analysis of the distinct training and testing curves in Appendix G.

**Summary: Learner collusion caused by joint training (ERR) results in greater overfitting exhibited by a larger generalization gap.**

Figure 4: **Learner codependence. Applying various levels of base learner dropout at test time and observing the relative increase in MSE. As \( 1\), dropping a subset of learners causes a greater increase in test error suggesting the occurrence of learner collusion.**

Figure 5: **Generalization gaps. Joint training results in a larger generalization gap (\(^{}(},)-^{}(},)\)) than independent training, despite both methods reporting test performance using the joint objective.**

Figure 3: **Bias removal. The percentage of diversity explained by the most simple form of learner collusion across each dataset. In the region where learner collusion appears to occur (\( 1\)), a large proportion of the diversity can be explained by simple additive bias in the predictions.**

### Practical Implications

We have now described the phenomenon of learner collusion caused by the joint training of deep ensembles, empirically verified its existence, and demonstrated how it results in poorer generalization. We complete this analysis with an empirical investigation of how learner collusion manifests in the ensemble performance on standard machine learning tasks. We use the augmented objective, \(^{}\) as described in Section 5, to examine the effects of smoothly increasing the level of joint training in the objective on CIFAR-10/CIFAR-100 with ResNet architectures. We also examine the validation performance of the ensemble _and_ base learners throughout training on ImageNet. Results are provided in Figure 6. We also provide extensive additional results in Appendix G confirming the generality of this degeneracy. There we consider: larger ensembles, score-averaging, alternative models, additional datasets, and ERR as a metric. Complete experimental details are provided in Appendix B.

Consistently across all experiments, we find that joint training (\(=1\)) results in the worst test set performance. We also note the poor performance among base learners in this regime, highlighting that inflating diversity comes at the cost of degenerate individual performance. Interestingly, across all of our experiments learner collusion typically emerges as \( 1\) implying that partial joint training may be a feasible direction for future work. However, in general, independent training emerges as a primary choice of training strategy for deep ensembles.

## 7 Conclusion

In this work, we have provided a detailed account explaining the unexpected observation that jointly trained deep ensembles are generally outperformed by their independently trained counterparts _despite joint training being the objective of interest at test time_. We presented a comprehensive view of diversity in machine learning ensembles through which we analyzed the joint training objective. From this perspective, we uncovered a learner collusion effect in which non-diverse base learners collude to artificially inflate their apparent diversity. Empirically, we both isolated this phenomenon and demonstrated its catastrophic effect on model generalization. Finally, the practical consequences of this limitation were exposed for standard machine learning settings. Note that we also provide a visual summary of this work and its technical contributions in Appendix A. We hope that future work will investigate how to optimize deep ensembles to operate collaboratively whilst avoiding the degeneracies of learner collusion. In Appendix G.7 we include a _negative result_ of our initial attempts in this direction. However, several promising avenues remain open (e.g. jointly optimizing worst-case bounds instead [47; 48]).

Figure 6: **Practical implications of learner collusion.** Left: Test accuracy with increasing diversity in the objective on CIFAR-10/100 with ResNets. Right: Validation accuracy throughout training ResNet-18 base learners on ImageNet. Jointly trained ensembles (\(=1\)) perform significantly worse than independently trained ensembles (\(=0\)).

#### Acknowledgments

We would like to thank Alicia Curth, Fergus Imrie, and the anonymous reviewers for very insightful comments and discussions on earlier drafts of this paper. AJ gratefully acknowledges funding from the Cyste Fibrosis Trust. TL is funded by AstraZeneca. JC is funded by Aviva. This work was supported by Azure sponsorship credits granted by Microsoft's AI for Good Research Lab.