# MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula

Shubhra Mishra\({}^{*,1}\)

###### Abstract

Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses. Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained "standards" from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derive _follow-up questions_ from symbolic structures and convert them into follow-up word problems--a novel task of mathematical dialogue that probes for robustness in understanding. Experiments on 23 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets. Project page: https://mathcamps.cc.

## 1 Introduction

As Large Language Models (LLMs) become increasingly capable, mathematical reasoning has become a key benchmark for evaluating their abilities. Traditional benchmarking, which relies on fixed sets of human-generated problems (e.g., GSM8k, or MATH ), now faces new challenges. Many LLMs are trained on vast public datasets that may include these benchmarks, raising concerns about data contamination [20; 7; 4]. This issue is amplified by the lack of transparency in the training data of most state-of-the-art models, including GPT-4 , Claude , and LLaMA . While creating novel problems could mitigate contamination concerns but is resource-intensive. Moreover, current benchmarks offer limited insights into the specific mathematical skills of LLMs, as aggregate accuracy alone does not reveal where models excel or struggle, and how this has changed over time.

To address these issues, we introduce the Mathematics Common Core Assessment of Problem Solving (MathCAMPS), a framework for generating high-quality mathematical problems based on the Common Core (CC) standards. MathCAMPS enables detailed analysis of LLMs' mathematical proficiency, aligned with skills taught in schools. Our pipeline employs a composable grammar for generating problems, symbolic solvers (e.g. SymPy) to get final solutions, and an LLM for transforming them into word problems. We ensure problem faithfulness through a cycle-consistency check, where the LLM back-translates word problems into symbolic form.

We also propose a novel "mathematical dialogue" task, where the model answers follow-up questions after solving a problem. These follow-ups can be either _counterfactual_, modifying an aspect of the original problem, or _incremental_, providing additional information and changing the question.

Using our framework, we synthesize problems for each of 44 CC standards (Appendix C), resulting in a dataset of 4,900 initial problems and 4707 total follow-ups. Our results reveal surprising weaknesses, particularly in response to follow-up responses, highlighting significant gaps in even the strongest models. Additionally, we provide a first-of-its-kind analysis of learning dynamics of mathematical abilities in LLM training using checkpoints from Pythia 12B  (Appendix B).

## 2 MathCAMPS

We now describe our pipeline for automatically generating mathematical problems and follow-up questions that are grounded in a human curriculum - the Mathematics Common Core (https://www.thecorestandards.org). Figure 1 overviews our pipeline. We describe how we represent CC standards in a grammar, sample symbolic problems, generate follow-ups, realize those in natural language, and finally improve quality by checking for cycle consistency.

**Representing Common Core Standards** We represent CC standards using an attribute grammar , allowing both syntactic and semantic rules. This formalism supports context-sensitive constraints, enabling encoding of information like numerical bounds directly in production rules.

**From Symbolic to Word Problems** To convert symbolic problems into natural language, we use few-shot prompting with GPT-4 (Figure 1 (C)). For each standard, we manually create word problems from two symbolic examples. For word problems requiring cover stories, we randomly select a theme from a set of 188. These examples guide GPT-4 in generating diverse, natural problems. To ensure faithfulness to the original structure, we apply a _cycle consistency_ approach: GPT-4 converts its generated word problem back into a symbolic structure, which is solved and compared to the original. Problems failing this test are discarded.

**Generating Follow-Up Questions** We leverage symbolic representations to generate two types of follow-up questions: _counterfactual_ (altering a constant) and _incremental_ (adding information). For each CC standard, we identify applicable follow-up types. Symbolically, follow-up questions are modeled as differences applied to the original problem, which we solve to produce ground-truth answers. We use few-shot prompting to translate these changes into natural language questions and apply cycle consistency to verify accuracy.

Figure 1: Overview of the MathCAMPS generation pipeline. We start from a grammar (**A**) that represents problems tied to a Common Core Standard - a specific mathematical ability drawn from a human curriculum. We sample problems in a symbolic form (**B**), and use a language model to realize it in natural language (**C**), applying a cycle-consistency where we back-translate the problem into symbolic form and ensure the answer remains the same, validating truthfulness. We also synthesize incremental and counterfactual follow-up problems

## 3 Experiments

We now evaluate a suite of 23 LLMs from 8 different vendors on MathCAMPS. We evaluate all models by sampling with temperature 0, using a fixed 1-shot prompt with the first example from GSM8K, mostly to demonstrate the format. For all models (most of them instruction-tuned), a single example was enough for to adhere to the task and the format we specify. The rich structure in MathCAMPS allows us to perform a number of unique analyses on LLMs relating to specific mathematical abilities and their corresponding grade levels for human students.

Table 1 shows both aggregate accuracy on MathCAMPS, as well as accuracy across standards partitioned by grade, whereas Figure 3 compares the aggregate accuracies on MathCAMPS and GSM8K. Closed-weights models are shown above the line, with open-weights models below. GPT-4o ranks at the top in overall accuracy. Since we used GPT-4 to generate the problems, we must rule out familiarity bias  in this result, which we do in Appendix D.

**Models of similar overall performance can have large disparities in specific abilities or grades.** Several models that have comparable overall accuracies show large differences when compared on specific mathematical skills. As an example, Claude-3 Opus and Claude-3 Sonnet have similar overall accuracy both in MathCAMPS (.89 vs.86) and in GSM8K (.95 vs.923). However, we find that Claude-3 Opus is significantly better at manipulating fractions. For instance, in the CC standard 5.NF.A.2, described as _"Solve word problems involving addition and subtraction of fractions referring to the same whole, including cases of unlike denominators"_, Opus has a 36% advantage over Sonnet, scoring a 70% accuracy for this standard, whereas Sonnet only achieves 34%. Similarly, while Gemma 7B and phi-2 have comparable overall performance (.62 vs.63 accuracy on MathCAMPS), some capabilities in each model seem nearly absent from the other. Gemma 7B is highly accurate when performing multi-digit multiplication (4.NBT.B.4), which phi-2 struggles with. And while phi-2 performs well while comparing fractions (4.NF.A.2), Gemma 7B struggles. Such stark differences are obscured when only analyzing aggregate metrics, whereas MathCAMPS allows for a much more nuanced understanding of mathematical reasoning capabilities.

**Overall ranking between models is largely a function of which skills we choose to evaluate.** Overall accuracies in any dataset induce a single performance ranking of models. However, when we look at individual CC standards in MathCAMPS, rankings are largely a function of which skills we choose to evaluate. Comparing pairs of models across all standards, rarely we find cases where

  
**Vendor** & **Model** & **All** & **K** & **1** & **2** & **3** & **4** & **5** & **6** & **7** & **8** \\  OpenAI & GPT-4o  & 0.92 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.92 & 0.88 & 0.95 & 0.89 & 0.64 \\ Anthropic & Claude-3 Opus  & 0.89 & 0.97 & 0.99 & 0.96 & 0.98 & 0.89 & 0.83 & 0.96 & 0.73 & 0.56 \\ Google & Gemini-1.5 Pro  & 0.89 & 0.95 & 0.98 & 0.97 & 0.97 & 0.89 & 0.83 & 0.93 & 0.78 & 0.54 \\ Google & Gemini-1.5 Flash  & 0.87 & 0.98 & 0.98 & 0.97 & 0.98 & 0.80 & 0.80 & 0.90 & 0.84 & 0.56 \\ OpenAI & GPT-3.5 Turbo  & 0.87 & 0.96 & 0.98 & 0.98 & 0.97 & 0.86 & 0.77 & 0.90 & 0.77 & 0.56 \\ Anthropic & Claude-3 Sonnet  & 0.86 & 0.96 & 0.98 & 0.97 & 0.98 & 0.88 & 0.74 & 0.94 & 0.66 & 0.49 \\ Anthropic & Claude-3 Haiku  & 0.84 & 0.97 & 0.98 & 0.97 & 0.98 & 0.87 & 0.69 & 0.92 & 0.59 & 0.51 \\  Meta & Llama 3 70B  & 0.85 & 0.96 & 0.97 & 0.97 & 0.97 & 0.85 & 0.71 & 0.87 & 0.73 & 0.50 \\ Mistral & Mistral 8x22B  & 0.84 & 0.96 & 0.99 & 0.98 & 0.96 & 0.79 & 0.69 & 0.88 & 0.73 & 0.61 \\ DeepSeek & DeepSeek 67B  & 0.80 & 0.95 & 0.99 & 0.96 & 0.93 & 0.82 & 0.60 & 0.84 & 0.61 & 0.47 \\ Meta & Llama 3 8B  & 0.77 & 0.94 & 0.97 & 0.96 & 0.94 & 0.78 & 0.55 & 0.79 & 0.53 & 0.43 \\ Mistral & Mistral 8x7B  & 0.76 & 0.94 & 0.96 & 0.93 & 0.91 & 0.75 & 0.52 & 0.80 & 0.53 & 0.45 \\ EleutherAI & Lemma 34B  & 0.71 & 0.95 & 0.96 & 0.93 & 0.87 & 0.61 & 0.47 & 0.77 & 0.46 & 0.44 \\ Mistral & Mistral 7B  & 0.68 & 0.89 & 0.94 & 0.91 & 0.84 & 0.61 & 0.42 & 0.66 & 0.45 & 0.42 \\ DeepSeek & DeepSeek Coder 33B  & 0.65 & 0.88 & 0.93 & 0.92 & 0.83 & 0.54 & 0.36 & 0.66 & 0.44 & 0.38 \\ Meta & CodeLlama 34B  & 0.64 & 0.90 & 0.94 & 0.92 & 0.85 & 0.51 & 0.38 & 0.70 & 0.37 & 0.30 \\ Microsoft & phi-2  & 0.63 & 0.95 & 0.96 & 0.89 & 0.78 & 0.46 & 0.38 & 0.61 & 0.37 & 0.41 \\ EleutherAI & Llamma 7B  & 0.62 & 0.88 & 0.90 & 0.85 & 0.79 & 0.48 & 0.41 & 0.67 & 0.41 & 0.36 \\ Google & Gemma 7B  & 0.62 & 0.83 & 0.92 & 0.90 & 0.82 & 0.47 & 0.36 & 0.65 & 0.36 & 0.30 \\ Meta & CodeLlama 13B  & 0.58 & 0.87 & 0.92 & 0.87 & 0.75 & 0.41 & 0.30 & 0.61 & 0.32 & 0.34 \\ Meta & CodeLlama 7B  & 0.52 & 0.85 & 0.92 & 0.84 & 0.69 & 0.37 & 0.25 & 0.57 & 0.25 & 0.16 \\ Google & Gemma 2B  & 0.51 & 0.66 & 0.76 & 0.74 & 0.67 & 0.42 & 0.28 & 0.55 & 0.30 & 0.27 \\  - & Avg. Performance & 0.74 & 0.87 & 0.91 & 0.89 & 0.87 & 0.70 & 0.59 & 0.78 & 0.57 & 0.38 \\   

Table 1: Final answer accuracy of LLMs on MathCAMPS, both over all problems (**All**) and considering only standards in each grade we cover (**K** to **8**). Highlights compare to gradewise avg.

[MISSING_PAGE_FAIL:4]