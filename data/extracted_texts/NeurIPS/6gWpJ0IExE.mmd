# When is Agnostic Reinforcement Learning Statistically Tractable?+

Footnote â€ : Authors are listed in alphabetical order of their last names.

Zeyu Jia\({}^{1}\)  Gene Li\({}^{2}\)  Alexander Rakhlin\({}^{1}\)  Ayush Sekhari\({}^{1}\)  Nathan Srebro\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)TTIC

###### Abstract

We study the problem of agnostic PAC reinforcement learning (RL): given a policy class \(\), how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an \(\)-suboptimal policy with respect to \(\)? Towards that end, we introduce a new complexity measure, called the _spanning capacity_, that depends solely on the set \(\) and is independent of the MDP dynamics. With a generative model, we show that for any policy class \(\), bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class \(\) with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional _surflower_ structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.

## 1 Introduction

Reinforcement Learning (RL) has emerged as a powerful paradigm for solving complex decision-making problems, demonstrating impressive empirical successes in a wide array of challenging tasks, from achieving superhuman performance in the game of Go (Silver et al., 2017) to solving intricate robotic manipulation tasks (Lillicrap et al., 2016; Akkaya et al., 2019; Ji et al., 2023). Many practical domains in RL often involve rich observations such as images, text, or audio (Mnih et al., 2015; Li et al., 2016; Ouyang et al., 2022). Since these state spaces can be vast and complex, traditional tabular RL approaches (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Azar et al., 2017; Jin et al., 2018) cannot scale. This has led to a need to develop provable and efficient approaches for RL that utilize _function approximation_ to generalize observational data to unknown states/actions.

The goal of this paper is to study the sample complexity of policy-based RL, which is arguably the simplest setting for RL with function approximation (Kearns et al., 1999; Kakade, 2003). In policy-based RL, an abstract function class \(\) of _policies_ (mappings from states to actions) is given to the learner. For example, \(\) can be the set of all the policies represented by a certain deep neural network architecture. The objective of the learner is to interact with an unknown MDP to find a policy \(\) that competes with the best policy in \(\), i.e., for some prespecified \(\), the policy \(\) satisfies

\[V^{}_{}V^{}-,\] (1)

where \(V^{}\) denotes the value of policy \(\) on the underlying MDP. We henceforth call Eq. (1) the "agnostic PAC reinforcement learning" objective. Our paper addresses the following question:Characterizing (agnostic) learnability for various problem settings is perhaps the most fundamental question in statistical learning theory. For the simpler setting of supervised learning (which is RL with binary actions, horizon 1, and binary rewards), the story is complete: a hypothesis class \(\) is agnostically learnable if and only if its \(\) dimension is bounded (Vapnik and Chervonenkis, 1971, 1974; Blumer et al., 1989; Ehrenfeucht et al., 1989), and the ERM algorithm--which returns the hypothesis with the smallest training loss--is statistically optimal (up to log factors). However, RL (with \(H>1\)) is significantly more challenging, and we are still far from a rigorous understanding of when agnostic RL is statistically tractable, or what algorithms to use in large-scale RL problems.

While significant effort has been invested over the past decade in both theory and practice to develop algorithms that utilize function approximation, existing theoretical guarantees require additional assumptions on the MDP. The most commonly adopted assumption is _realizability_: the learner can precisely model the value function or the dynamics of the underlying MDP (see, e.g., Russo and Van Roy, 2013; Jiang et al., 2017; Sun et al., 2019; Wang et al., 2020; Du et al., 2021; Jin et al., 2021; Foster et al., 2021). Unfortunately, realizability is a fragile assumption that rarely holds in practice. Moreover, even mild misspecification can cause catastrophic breakdown of theoretical guarantees (Du et al., 2019; Lattimore et al., 2020). Furthermore, in various applications, the optimal policy \(^{}:=_{x}V^{}\) may have a succinct representation, but the optimal value function \(V^{}\) can be highly complex, rendering accurate approximation of dynamics/value functions infeasible without substantial domain knowledge (Dong et al., 2020). Thus, we desire algorithms for agnostic RL that can work with _no modeling assumptions on the underlying MDP_. On the other hand, it is also well known without any assumptions on \(\), when \(\) is large and the MDP has a large state and action space, agnostic RL may be intractable with sample complexity scaling exponentially in the horizon (Agarwal et al., 2019). Thus, some structural assumptions on \(\) are needed, and towards that end, the goal of our paper is to understand what assumptions are sufficient or necessary for statistically efficient agnostic RL, and to develop provable algorithms for learning. Our main contributions are:

* We introduce a new complexity measure called the _spanning capacity_, which solely depends on the policy class \(\) and is independent of the underlying MDP. We illustrate the spanning capacity with examples, and show why it is a natural complexity measure for agnostic PAC RL (Section3).
* We show that the spanning capacity is both necessary and sufficient for agnostic PAC RL with a generative model, with upper and lower bounds matching up to \([]\) and \((H)\) factors (Section4). Thus, bounded spanning capacity characterizes agnostic PAC learnability in RL with a generative model.
* Moving to the online setting, we first show that the bounded spanning capacity by itself is _insufficient_ for agnostic PAC RL by proving a superpolynomial lower bound on the sample complexity required to learn a specific \(\), thus demonstrating a separation between generative and online interaction models for agnostic PAC RL (Section5).
* Given the previous lower bound, we propose an additional property of the policy class called the _sunflower_ property, that allows for efficient exploration and is satisfied by many policy classes of interest. We provide a new agnostic PAC RL algorithm called POPLER that is statistically efficient whenever the given policy class has both bounded spanning capacity and the sunflower property (Section6). POPLER leverages importance sampling as well as reachable state identification techniques to estimate the values of policies. Our algorithm and analysis utilize a new tool called the _policy-specific Markov reward process_, which may be of independent interest.

## 2 Setup and Motivation

We begin by introducing our setup for reinforcement learning (RL), the relevant notation, and the goal of agnostic RL.

### RL Preliminaries

We consider reinforcement learning in an episodic Markov decision process (MDP) with horizon \(H\).

Markov Decision Processes.Denote the MDP as \(M=(,,P,R,H,)\), which consists of a state space \(\), action space \(\), horizon \(H\), probability transition kernel \(P:()\), reward function \(R:()\), and initial distribution \(()\). For ease of exposition, we assume that \(\) and \(\) are finite (but possibly large) with cardinality \(S\) and \(A\) respectively. We assume a layered state space, i.e., \(=_{1}_{2}_{H}\) where \(_{i}_{j}=\) for all \(i j\). Thus, given a state \(s\), it can be inferred which layer \(_{h}\) in the MDP it belongs to. We denote a trajectory \(=(s_{1},a_{1},r_{1},,s_{H},a_{H},r_{H})\), where at each step \(h[H]\), an action \(a_{h}\) is played, a reward \(r_{h}\) is drawn independently from the distribution \(R(s_{h},a_{h})\), and each subsequent state \(s_{h+1}\) is drawn from \(P(|s_{h},a_{h})\). Lastly, we assume that the cumulative reward of any trajectory is bounded by 1.

Policy-based Reinforcement Learning.We assume that the learner is given a policy class \(^{}\).2 For any policy \(^{}\), we denote \((s)\) as the action that \(\) takes when presented a state \(s\). We use \(^{}[]\) and \(^{}[]\) to denote the expectation and probability under the process of a trajectory drawn from the MDP \(M\) by policy \(\). Additionally, for any \(h,h^{} H\), we say that a partial trajectory \(=(s_{h},a_{h},s_{h+1},a_{h+1},,s_{h^{}},a_{h^{}})\) is consistent with \(\) if for all \(h i h^{}\), we have \((s_{i})=a_{i}\). We use the notation \(\) to denote that \(\) is consistent with \(\).

The state-value function (also called _\(V\)-function_) and state-action-value function (also called _\(Q\)-function_) are defined such that for any \(\), and \(s,a\),

\[V_{h}^{}(s)=^{}\!_{h^{}=h}^{H}R(s_{h^{ }},a_{h^{}}) s_{h}=s,\;\;Q_{h}^{}(s,a)=^ {}\!_{h^{}=h}^{H}R(s_{h^{}},a_{h^{}}) s _{h}=s,a_{h}=a.\]

Furthermore, whenever clear from the context, we denote \(V^{}:=_{s_{1}}V_{1}^{}(s_{1})\). Finally, for any policy \(^{}\), we also define the _occupancy measure_ as \(d_{h}^{}(s,a):=^{}[s_{h}=s,a_{h}=a]\) and \(d_{h}^{}(s):=^{}[s_{h}=s]\).

Models of Interaction.We consider two standard models of interaction in the RL literature:

* **Generative Model.** The learner has access to a simulator which it can query for any \((s,a)\), and observe a sample \((s^{},r)\) drawn as \(s^{} P(|s,a)\) and \(r R(s,a)\).3 * **Online Interaction Model.** The learner can submit a (potentially non-Markovian) policy \(\) and receive back a trajectory sampled by running \(\) on the MDP. Since online access can be simulated via generative access, learning under online access is only more challenging than learning under generative access (up to a factor of \(H\)). Adhering to commonly used terminology, we will refer to RL under the online interaction model as "online RL".

We define \(^{}\) as the set of all (stochastic and deterministic) MDPs of horizon \(H\) over the state space \(\) and action space \(\). Additionally, we define \(^{}^{}\) and \(^{}^{}\) to denote the set of all MDPs with deterministic transitions but stochastic rewards, and of all MDPs with both deterministic transitions and deterministic rewards, respectively.

### Agnostic PAC RL

Our goal is to understand the sample complexity of agnostic PAC RL, i.e., the number of interactions required to find a policy that can compete with the best policy within the given class \(\) for the underlying MDP. An algorithm \(\) is an \((,)\)-PAC RL algorithm for an MDP \(M\), if after interacting with \(M\) (either in the generative model or online RL), \(\) returns a policy \(\) that satisfies the guarantee4

\[V^{}_{}V^{}-,\]with probability at least \(1-\). For a policy class \(\) and a MDP class \(\), we say that \(\) has sample complexity \(n_{}^{}(,;,)\) (resp. \(n_{}^{}(,;,)\)) if for every MDP \(M\), \(\) is an \((,)\)-PAC RL algorithm and collects at most \(n_{}^{}(,;,)\) trajectories in the online interaction model (resp. generative model) in order to return \(\).

We define the _minimax sample complexity_ for agnostically learning \(\) over \(\) as the minimum sample complexity of any \((,)\)-PAC RL algorithm, i.e.

\[n_{}(,;,):=_{}n_{ }^{}(,;,), n_{}(,;,):=_{ }n_{}^{}(,;,).\]

For brevity, when \(=^{}\), we will drop the dependence on \(\) in our notation, e.g., we will write \(n_{}(;,)\) and \(n_{}(;,)\) to denote \(n_{}(,;,)\) and \(n_{}(,;,)\) respectively.

Known Results in Agnostic RL.We first note the following classical result which shows that agnostic PAC RL is statistically intractable in the worst case.

**Proposition 1** (No Free Lunch Theorem for RL; Krishnamurthy et al. (2016)).: _There exists a policy class \(\) for which the minimax sample complexity under a generative model is at least \(n_{}(;,)=(\{A^{H},||,SA\}/ ^{2})\)._

Since online RL is only harder than learning with a generative model, the lower bound in Proposition1 extends to online RL. Proposition1 is the analogue of the classical _No Free Lunch_ results in statistical learning theory (Shalev-Shwartz and Ben-David, 2014); it indicates that without placing further assumptions on the MDP or the policy class \(\) (e.g., by introducing additional structure or constraining the state/action space sizes, policy class size, or the horizon), sample efficient agnostic PAC RL is not possible.

Indeed, an almost matching upper bound of \(n_{}(;,)=}(\{A^{H}, ||, HSA\}/^{2})\) is quite easy to obtain. The \(||/^{2}\) guarantee can simply be obtained by iterating over \(\), collecting \(}(1/^{2})\) trajectories per policy, and then picking the policy with highest empirical value. The \(HSA/^{2}\) guarantee can be obtained by running known algorithms for tabular RL (Zhang et al., 2021). Finally, the \(A^{H}/^{2}\) guarantee is achieved by the classical importance sampling (IS) algorithm (Kearns et al., 1999; Agarwal et al., 2019). Since importance sampling will be an important technique that we repeatedly use and build upon in this paper, we give a formal description of the algorithm below:

ImportanceSampling:

* Collect \(n=(A^{H}||/^{2})\) trajectories by executing \((a_{1},,a_{H})(^{H})\).
* Return \(=_{}_{}^{}\), where \(_{}^{}:=}{n}_{i=1}^{n}\{ ^{(i)}\}(_{h=1}^{H}r_{h}^{(i)})\).

For every \(\), the quantity \(_{}^{}\) is an unbiased estimate of \(V^{}\) with variance \(A^{H}\); the sample complexity result follows by standard concentration guarantees (see, e.g., Agarwal et al., 2019).

Towards Structural Assumptions for Statistically Efficient Agnostic PAC RL.Of course, No Free Lunch results do not necessarily spell doom--for example, in supervised learning, various structural assumptions have been studied that enable statistically efficient learning. Furthermore, there has been a substantial effort in developing complexity measures like VC dimension, fat-shattering dimension, covering numbers, etc. that characterize agnostic PAC learnability under different scenarios (Shalev-Shwartz and Ben-David, 2014). In this paper, we consider the agnostic reinforcement learning setting, and explore whether there exists a complexity measure that characterizes learnability for every policy class \(\). Formally, can we establish a complexity measure \(\) (a function that maps policy classes to real numbers), such that for any \(\), the minimax sample complexity satisfies

\[n_{}(;,)=}(),H,^{-1},^{-1},\]

where \(()\) denotes the complexity of \(\). We mainly focus on finite (but large) policy classes and assume that the \(||\) factors in our upper bounds are mild. In G, we discuss how our results can be extended to infinite policy classes.

Is Proposition1 Tight for Every \(\)?In light of Proposition1, one obvious candidate is \(}()=\{A^{H},||,SA\}\). While \(}()\) is definitely sufficient to upper bound the minimaxsample complexity for any policy class \(\) up to log factors, a priori it is not clear if it is also necessary for every policy class \(\). In fact, our next proposition implies that \(}()\) is indeed not the right measure of complexity by giving an example of a policy class for which \(}():=\{A^{H},||,SA\}\) is exponentially larger than the minimax sample complexity for agnostic learning for that policy class, even when \(\) is constant.

**Proposition 2**.: _Let \(H\), \(K\), \(_{h}=\{s_{(i,h)}:i[K]\}\) for all \(h[H]\), and \(=\{0,1\}\). Consider the singleton policy class: \(_{}:=\{_{(i^{},h^{})}:i^{}[K],h^{ }[H]\}\), where \(_{(i^{},h^{})}\) takes the action \(1\) on state \(s_{(i^{},h^{})}\), and \(0\) everywhere else. Then \(\{A^{H},|_{}|,SA\}=2^{H}\) but \(n_{}(_{};,)}(H^{3}(1/)/^{2})\)._

The above upper bound on minimax sample complexity holds arbitrarily large values of \(K\), and can be obtained as a corollary of our more general upper bound in Section 6. The key intuition for why \(_{}\) can be learned in \((H)\) samples is that even though the policy class and the state space are large when \(K\) is large, the set of possible trajectories obtained by running any \(_{}\) has low complexity. In particular, every trajectory \(\) has at most one \(a_{h}=1\). This observation enables us to employ the straightforward modification of the classical IS algorithm: draw \((H)(1/)/^{2}\) samples from the uniform distribution over \(_{}=\{_{h}:h[H]\}\) where the policy \(_{h}\) takes the action 1 on every state at layer \(h\) and \(0\) everywhere else. The variance of the resulting estimator \(^{}_{}\) is \(1/H\), so the sample complexity of this modified variant of IS has only \((H)\) dependence by standard concentration bounds.

In the sequel, we present a new complexity measure that formalizes this intuition that a policy class \(\) is efficiently learnable if the set of trajectories induced by policies in \(\) is small.

## 3 Spanning Capacity

The spanning capacity precisely captures the intuition that trajectories obtained by running any \(\) have "low complexity." We first define a notion of reachability: in deterministic MDP \(M^{}\), we say \((s,a)\) is _reachable_ by \(\) if \((s,a)\) lies on the trajectory obtained by running \(\) on \(M\). Roughly speaking, the spanning capacity measures "complexity" of \(\) as the maximum number of state-action pairs which are reachable by some \(\) in any _deterministic_ MDP.

**Definition 1** (spanning capacity).: _Fix a deterministic MDP \(M^{}\). We define the cumulative reachability at layer \(h[H]\), denoted by \(C^{}_{h}(;M)\), as_

\[C^{}_{h}(;M):=|\{(s,a):(s,a)h\}|.\]

_We define the spanning capacity of \(\) as_

\[():=_{h[H]}_{M^{}}C^{ }_{h}(;M).\]

To build intuition, we first look at some simple examples with small spanning capacity:

* **Contextual Bandits:** Consider the standard formulation of contextual bandits (i.e., RL with \(H=1\)). For any policy class \(_{}\), since \(H=1\), the largest deterministic MDP we can construct has a single state \(s_{1}\) and at most \(A\) actions available on \(s_{1}\), so \((_{}) A\).
* **Tabular MDPs:** Consider tabular RL with the policy class \(_{}=^{}\) consisting of all deterministic policies on the underlying state space. Depending on the relationship between \(S,A\) and \(H\), we have two possible bounds on \((_{})\{A^{H},SA\}\). If the state space is exponentially large in \(H\), then it is possible to construct a full \(A\)-ary "tree" such that every \((s,a)\) pair at layer \(H\) is visited, giving us the \(A^{H}\) bound. However, if the state space is small, then the number of \((s,a)\) pairs available at any layer \(H\) is trivially bounded by \(SA\).
* **Bounded Cardinality Policy Classes:** For any policy class \(\), we always have that \(()||\), since in any deterministic MDP, in any layer \(h[H]\), each \(\) can visit at most one new \((s,a)\) pair. Thus, for policy classes \(|_{}|\) with small cardinality (e.g. \(|_{}|=O((H,A))\), the spanning capacity is also small; Note that in this case, we allow our sample complexity bounds to depend on \(|_{}|\).
* **Singletons:** For the singleton class we have \((_{})=H+1\), since once we fix a deterministic MDP, there are at most \(H\) states where we can split from the trajectory taken by the policy which always plays \(a=0\), so the maximum number of \((s,a)\) pairs reachable at layer \(h[H]\) is \(h+1\). Observe that in light of Proposition2, the spanning capacity for \(_{}\) is "on the right order" for characterizing the minimax sample complexity for agnostic PAC RL.

Before proceeding, we note that for any policy class \(\), the spanning capacity is always bounded.

**Proposition 3**.: _For any policy class \(\), we have \(()\{A^{H},||,SA\}\)._

Proposition3 recovers the worst-case upper and lower bound from Section2.2. However, for many policy classes, spanning capacity is substantially smaller than upper bound of Proposition3. In addition to the examples we provided above, we list several additional policy classes with small spanning capacity. For these policy classes we set the state/action spaces to be \(_{h}=s_{(i,h)}:i[K]}\) for all \(h[H]\) and \(=\{0,1\}\), respectively. All proofs are deferred to AppendixB.

* \(\)**-tons**: A natural generalization of singletons. We define \(_{-}:=\{_{I}:I,|I|\}\), where the policy \(_{I}\) is defined s.t. \(_{I}(s)=1\{s I\}\) for any \(s\). Here, \((_{-})=(H^{})\).
* **1-Active Policies**: We define \(_{1-}\) to be the class of policies which can take both possible actions on a single state \(s_{(1,h)}\) in each layer \(h\), but on other states \(s_{(i,h)}\) for \(i 1\) must take action 0. Formally, \(_{1-}:=\{_{b} b\{0,1\}^{H}\}\), where for any \(b\{0,1\}^{H}\) the policy \(_{b}\) is defined such that \(_{b}(s)=b[h]\) if \(s=s_{(1,h)}\), and \(_{b}(s)=0\) otherwise.
* **All-Active Policies**: We define \(_{j-}:=\{_{b} b\{0,1\}^{H}\}\), where for any \(b\{0,1\}^{H}\) the policy \(_{b}\) is defined such that \(_{b}(s)=b[h]\) if \(s=s_{(j,h)}\), and \(_{b}(s)=0\) otherwise. We let \(_{}:=_{j=1}^{K}_{j-}\). Here, \((_{})=(H^{2})\).

A natural interpretation of the spanning capacity is that it represents the largest "needle in a haystack" that can be embedded in a deterministic MDP using the policy class \(\). To see this, let \((M^{},h^{})\) be the MDP and layer which witnesses \(()\), and let \(\{(s_{i},a_{i})\}_{i=1}^{()}\) be the set of state-action pairs reachable by \(\) in \(M^{}\) at layer \(h^{}\). Then one can hide a reward of 1 on one of these state-action pairs; since every trajectory visits a single \((s_{i},a_{i})\) at layer \(h^{}\), we need at least \(()\) samples in order to discover which state-action pair has the hidden reward. Note that in this agnostic learning setup, we only need to care about the states that are reachable using \(\), even though the \(h^{}\) layer may have other non-reachable states and actions.

### Connection to Coverability

The spanning capacity has another interpretation as the worst-case _coverability_, a structural parameter defined in a recent work by Xie et al. (2022).

**Definition 2** (Coverability, Xie et al. (2022)).: _For any MDP \(M\) and policy class \(\), the coverability coefficient \(C^{}\) is denoted_

\[C^{}(;M):=_{_{1},_{H}( )}_{,h[H]}\|^{}}{_{h}} \|_{}=\ _{h[H]}\ _{s,a}_{}d_{h}^{}(s,a).\] (2)

The last equality is shown in Lemma3 of Xie et al. (2022), and it says that the coverability coefficient is equivalent to a notion of cumulative reachability (one can check that their definition coincides with ours for deterministic MDPs).

Coverage conditions date back to the analysis of the classic Fitted Q-Iteration (FQI) algorithm (Munos, 2007; Munos and Szepesvari, 2008), and have extensively been studied in offline RL. Various models like tabular MDPs, linear MDPs, low-rank MDPs, and exogenous MDPs satisfy the above coverage condition (Antos et al., 2008; Chen and Jiang, 2019; Jin et al., 2021; Rashidinejad et al., 2021; Zhan et al., 2022; Xie et al., 2022), and recently, Xie et al. showed that _coverability_ can be used to prove regret guarantees for online RL, albeit under the additional assumption of value function realizability.

It is straightforward from Definition2 that our notion of spanning capacity is worst-case coverability when we maximize over deterministic MDPs, since for any deterministic MDP, \(_{}d_{h}^{}(s,a)=\{(s,a)\) is reachable by \(\) at layer \(h\}\). The next lemma shows that our notion of spanning capacity is _exactly_ worst-case coverability even when we maximize over the larger class of stochastic MDPs. As a consequence, there always exists a deterministic MDP that witnesses worst-case coverability.

**Lemma 1**.: _For any policy class \(\), we have \(_{M^{}}C^{}(;M)=()\)._While spanning capacity is equal to the worst-case coverability, we remark that the two definitions have different origins. The notion of coverability bridges offline and online RL, and was introduced in Xie et al. (2022) to characterize when sample efficient learning is possible in value-based RL, where the learner has access to a realizable value function class. On the other hand, spanning capacity is developed for the much weaker agnostic RL setting, where the learner only has access to a policy class (and does not have access to a realizable value function class). Note that a realizable value function class can be used to construct a policy class that contains the optimal policy, but the converse is not true. Furthermore, note that the above equivalence only holds in a worst-case sense (over MDPs). In fact, as we show in Appendix C, coverability alone is not sufficient for sample efficient agnostic PAC RL in the online interaction model.

## 4 Generative Model: Spanning Capacity is Necessary and Sufficient

In this section, we show that for any policy class, the spanning capacity characterizes the minimax sample complexity for agnostic PAC RL under generative model.

**Theorem 1** (Upper Bound for Generative Model).: _For any \(\), the minimax sample complexity \((,)\)-PAC learning \(\) is at most \(n_{}(;,)()}{^{2}}\)._

The proof can be found in Appendix D.1, and is a straightforward modification of the classic _trajectory tree method_ from Kearns et al. (1999): using generative access, sample \(([]/^{2})\) deterministic trajectory trees from the MDP to get unbiased evaluations for every \(\); the number of generative queries made is bounded since the size of the maximum deterministic tree is at most \(H()\).

**Theorem 2** (Lower Bound for Generative Model).: _For any \(\), the minimax sample complexity \((,)\)-PAC learning \(\) is at least \(n_{}(;,)( )}{^{2}}\)._

The proof can be found in Appendix D.2. Intuitively, given an MDP \(M^{}\) which witnesses \(()\), one can embed a bandit instance on the relevant \((s,a)\) pairs spanned by \(\) in \(M^{}\). The lower bound follows by a reduction to the lower bound for \((,)\)-PAC learning in multi-armed bandits.

Together, Theorem 1 and Theorem 2 paint a relatively complete picture for the minimax sample complexity of learning any policy class \(\), in the generative model, up to an \(H[]\) factor.

Deterministic MDPs.A similar guarantee holds for online RL over deterministic MDPs.

**Corollary 1**.: _Over the class of MDPs with deterministic transitions, the minimax sample complexity of \((,)\)-PAC learning any \(\) is_

\[()}{^{2}} n_{}(,^{};, )()}{^{2}} .\]

The upper bound follows because the trajectory tree algorithm for deterministic transitions samples the same tree over and over again (with different stochastic rewards). The lower bound trivially extends because the lower bound of Theorem 2 actually uses an MDP \(M^{}\) (in fact, the transitions of \(M\) are also known to the learner).

## 5 Online RL: Spanning Capacity is Not Sufficient

Given that fact that spanning capacity characterizes the minimax sample complexity of agnostic PAC RL in the generative model, one might be tempted to conjecture that spanning capacity is also the right characterization in online RL. The lower bound is clear since online RL is at least as hard as learning with a generative model, so Theorem 2 already shows that spanning capacity is _necessary_. But is it also sufficient?

In this section, we prove a surprising negative result showing that bounded spanning capacity by itself is not sufficient to characterize the minimax sample complexity in online RL. In particular, we provide an example for which we have a _superpolynomial_ (in \(H\)) lower bound on the number of trajectories needed for learning in online RL, that is not captured by any polynomial function of spanning capacity. This implies that, contrary to RL with a generative model, one can not hope for \(n_{}(;,)=(( (),H,^{-1},^{-1}))\) in online RL.

**Theorem 3** (Lower Bound for Online RL).: _Fix any sufficiently large \(H\). Let \((1/2^{(H)},(1/H))\) and \(\{2,,H\}\) such that \(1/^{} 2^{H}\). There exists a policy class \(^{()}\) of size \((1/^{})\) with \((^{()})(H^{4+2})\) and a family of MDPs \(\) with state space \(\) of size \(2^{(H)}\), binary action space, and horizon \(H\) such that: for any \((,1/8)\)-PAC algorithm, there exists an MDP \(M\) for which the algorithm must collect at least \((\{},2^{H/3}\})\) online trajectories in expectation._

Informally speaking, the lower bound shows that there exists a policy class \(\) for which \(n_{}(;,)=1/^{(_{H} ())}\). In order to interpret this theorem, we can instantiate choices of \(=1/2^{}\) and \(=\) to show an explicit separation.

**Corollary 2**.: _For any sufficiently large \(H\), there exists a policy class \(\) with \(()=2^{( H)}\) such that for any \((1/2^{},1/8)\)-PAC algorithm, there exists an MDP for which the algorithm must collect at least \(2^{(H)}\) online trajectories in expectation._

In conjunction with the results of Section4, Theorem3 shows that (1) online RL is _strictly harder_ than RL with generative access, and (2) online RL for stochastic MDPs is _strictly harder_ than online RL for MDPs with deterministic transitions. We defer the proof of Theorem3 to AppendixE. Our lower bound introduces several technical novelties: the family \(\) utilizes a _contextual_ variant of the combination lock, and the policy class \(\) is constructed via a careful probabilistic argument such that it is hard to explore despite having small spanning capacity.

## 6 Statistically Efficient Agnostic Learning in Online RL

The lower bound in Theorem3 suggests that further structural assumptions on \(\) are needed for statistically efficient agnostic RL under the online interaction model. Essentially, the lower bound example provided in Theorem3 is hard to agnostically learn because any two distinct policies \(,^{}\) can differ substantially on a large subset of states (of size at least \( 2^{2H}\)). Thus, we cannot hope to learn "in parallel" via a low variance IS strategy that utilizes extrapolation to evaluate all policies \(\), as we did for singletons.

In the sequel, we consider the following sunflower property to rule out such problematic scenarios, and show how bounded spanning capacity along with the sunflower property enable sample-efficient agnostic RL in the online interaction model. The sunflower property only depends on the state space, action space, and policy class, and is independent of the transition dynamics and rewards of the underlying MDP. We first define a petal, a key ingredient of a sunflower.

**Definition 3** (Petal).: _For a policy set \(\), and states \(}\), a policy \(\) is said to be a \(}\)-petal on \(\) if for all \(h h^{} H\), and partial trajectories \(=(s_{h},a_{h},,s_{h^{}},a_{h^{}})\) that are consistent with \(\): either \(\) is also consistent with some \(^{}\), or there exists \(i(h,h^{}]\) s.t. \(s_{i}}\)._

Informally, \(\) is a \(}\)-petal on \(\) if any trajectory that can be obtained using \(\) can either also be obtained using a policy in \(\) or must pass through \(}\). Thus, any policy that is a \(}\)-petal on \(\) can only differentiate from \(\) in a structured way. A policy class is said to be a sunflower if it is a union of petals as defined below:

**Definition 4** (Sunflower).: _A policy class \(\) is said to be a \((K,D)\)-sunflower if there exists a set \(_{}\) of Markovian policies with \(|_{}| K\) such that for every policy \(\) there exists a set \(_{}\), of size at most \(D\), so that \(\) is an \(S_{}\)-petal on \(_{}\)._

Our next theorem provides a sample complexity bound for Agnostic PAC RL for policy classes that have \((K,D)\)-sunflower structure. This bound is obtained via a new exploration algorithm called POPLER that takes as input the set \(_{}\) and corresponding petals \(\{_{}\}_{}\) and leverages importance sampling as well as reachable state identification techniques to simultaneously estimate the value of every policy in \(\).

**Theorem 4**.: _Let \(,>0\). Suppose the policy class \(\) satisfies Definition1 with spanning capacity \(()\), and is a \((K,D)\)-sunflower. Then, for any MDP \(M\), with probability at least \(1-\), \(\) (Algorithm1) succeeds in returning a policy \(\) that satisfies \(V^{}_{}V^{}-\), after collecting_

\[}}+}()}{^{}} K^{2} M.\]The proof of Theorem4, and the corresponding hyperparameters in \(\) needed to obtain the above bound, can be found in AppendixF. Before diving into the algorithm and proof details, let us highlight several key aspects of the above sample complexity bound:

* Note that a class \(\) may be a \((K,D)\)-sunflower for many different choices of \(K\) and \(D\). Barring computational issues, one can enumerate over all choices of \(_{}\) and \(\{_{}\}_{}\), and check if \(\) is a \((K,D)\)-sunflower for that choice of \(K=|_{}|\) and \(D=_{}_{}\). Since our bound in Theorem4 scales with \(K\) and \(D\), we are free to choose \(K\) and \(D\) to minimize the corresponding sample complexity bound.
* In order to get a polynomial sample complexity in Theorem4, both \(()\) and \((K,D)\) are required to be \((H,)\). All of the policy classes considered in Section3 have the sunflower property, with both \(K,D=(H)\), and thus our sample complexity bound extends for all these classes. See AppendixB for details.
* Notice that for Theorem4 to hold, we need both bounded spanning capacity as well as the sunflower structure on the policy class with bounded \((K,D)\). Thus, one may wonder if we can obtain a similar polynomial sample complexity guarantee in online RL under weaker assumptions. In Theorem3, we already showed that bounded \(()\) alone is not sufficient to obtain polynomial sample complexity in online RL. Likewise, as we show in AppendixF, sunflower property with bounded \((K,D)\) alone is also not sufficient for polynomial sample complexity, and hence both assumptions cannot be individually removed. However, it is an interesting question if there is some other structural assumption that combines both spanning capacity and the sunflower property, and is both sufficient and necessary for agnostic PAC learning in online RL. See Section7 for further discussions on this.

Why Does the Sunflower Property Enable Sample-Efficient Learning?Intuitively, the sunflower property captures the intuition of simultaneous estimation of all policies \(\) via importance sampling (IS), and allows control of both bias and variance. Let \(\) be a \(_{}\)-petal on \(_{}\). Any trajectory \(\) that avoids \(_{}\) must be consistent with some policy in \(_{}\), and will thus be covered by the data collected using \(^{}(_{})\). Thus, using IS with variance scaling with \(K\), one can create a biased estimator for \(V^{}\), where the bias is _only due_ to trajectories that pass through \(_{}\). There are two cases. If every state in \(_{}\) has small reachability under \(\), i.e. \(d^{}(s)\) for every \(s_{}\), then the IS estimate will have a low bias (linear in \(_{}\)), so we can compute \(V^{}\) up to error at most \(_{}\). On the other hand, if \(d^{}(s)\) is large for some \(s_{}\), it is possible to explicitly control the bias that arises from trajectories passing through them since there are at most \(D\) of them.

### Algorithm and Proof Ideas

\(\) (Algorithm1) takes as input a policy class \(\), as well as sets \(_{}\) and \(\{_{}\}_{}\), which can be computed beforehand by enumeration. \(\) has two phases: a _state identification phase_, where it finds "petal" states \(s_{}_{}\) that are reachable with sufficiently large probability; and an _evaluation phase_ where it computes estimates \(^{}\) for every \(\). It uses three subroutines \(\), \(\), and \(\), whose pseudocodes are stated in AppendixF.2.

The structure of the algorithm is reminiscent of reward-free exploration algorithms in tabular RL, e.g. Jin et al. (2020), where we first identify (petal) states that are reachable with probability at least \((/D)\) and build a policy cover for these states, and then use dynamic programming to estimate the values. However, contrary to the classical tabular RL setting, because the state space can be large, our setting is much more challenging and necessitates technical innovations. In particular, we can no longer enumerate over all petal states and check if they are sufficiently reachable by some policy \(\) (since the total number of petal states \(_{}_{}\) could scale linearly in \(\), a factor that we do not want to appear in our sample complexity). Instead, the key observation that we rely on is that if spanning capacity is bounded, then by the equivalence of spanning capacity and worst-case coverability (Lemma1) and due to (2), the number of highly reachable (and thus relevant) petal states is also bounded. Thus, we only need to build a policy cover to reach these relevant petal states. Our algorithm does this in a sample-efficient _sequential_ manner. For both state identification as well as evaluation, we interleave importance sampling estimates with the construction of a _policy-specific_ Markov Reward Processes (MRPs), which are defined for every \(\). The challenge is doing all of this "in parallel" for every \(\) through extensive sample reuse to avoid a blowup of \(\) or \(S\) in the sample complexity. For the exact construction of these policy-specific MRPs and how they are used in the algorithm, please see AppendixF.2.

```
0: Policy class \(\), Sets \(_{}\) and \(\{_{}\}_{}\), Parameters \(K,D,n_{1},n_{2},,\).
1: Define an additional start state \(s_{}\) (at \(h=0\)) and end state \(s_{}\) (at \(h=H+1\)).
2: Initialize \(^{}=\{s_{}\}\), \(\{(s_{},)\}\), and for every \(\), define \(^{+}_{}_{}\{s_{},s_{}\}\).
3:\(_{}(s_{},,_{ },n_{1})\)
4:/* Identification of Petal States that are Reachable with \((/D)\)Probability */
5:while\(=\)do
6: Set \(=\).
7:for\(\)do
8: Compute the set of already explored reachable states \(^{}_{}=^{+}_{}^{}\), and the remaining states \(^{}_{}=_{}(^{ }_{}\{s_{}\})\).
9: Estimate the policy-specific MRP \(}^{}_{^{}}\) according to (14) and (15).
10:for\(^{}_{}\)do
11: Estimate probability of reaching \(\) under \(\) as \(^{}()(^{}_{ },}^{}_{^{}},)\).
12:if\(^{}()}{{6D}}\)then
13: Update \(^{}^{}\{\}\), \(\{(,)\}\) and set \(=\).
14: Collect dataset \(_{}(,,_{ },n_{2})\).
15:endif
16:endfor
17:endfor
18:endwhile
19:/* Policy Evaluation and Optimization */
20:for\(\)do
21:\(^{}(_{},^{ },\{_{s}\}_{s^{}},)\).
22:endfor
23:Return\(_{}^{}\). ```

**Algorithm 1** Policy **OP**timization by Learning \(\)-Reachable States (\(\))

## 7 Conclusion and Discussion

In this paper, we investigated when agnostic RL is statistically tractable in large state and action spaces, and introduced spanning capacity as a natural measure of complexity that only depends on the policy class, and is independent of the MDP rewards and transitions. We first showed that bounded spanning capacity is both necessary and sufficient for agnostic PAC RL under the generative access model. However, we also provided a negative result showing that bounded spanning capacity does not suffice for online RL, thus showing a surprising separation between agnostic RL with a generative model and online interaction. We then provided an additional structural assumption, called the sunflower property, that allows for statistically efficient learning in online RL. Our sample complexity bound for online RL is obtained using a novel exploration algorithm called \(\) that relies on certain policy-specific Markov Reward Processes to guide exploration, and takes inspiration from the classical importance sampling method and reward-free exploration algorithms for tabular MDPs.

Our results pave the way for several future lines of inquiry. The most important direction is exploring complexity measures to tightly characterize the minimax sample complexity for online RL. On the upper bound side, Theorem 4 shows that bounded spanning capacity along with an additional sunflower property is sufficient for online RL. On the lower bound side, while bounded spanning capacity is necessary (by Theorem 2), we do not know if the sunflower property is also necessary. Another direction is understanding if one can improve the statistical complexity of policy-based learning using stronger feedback models; in Appendix 1 we explore whether receiving expert feedback in the form of the optimal value function \(\{Q^{}(s,a)\}_{a A}\) on the visited states can help. Surprisingly, the answer depends on realizability of the optimal policy \(^{}\): when \(^{}\), \(Q^{}\) feedback can be utilized to achieve an \(O((,H,}{{}}))\) sample complexity bound, with no dependence on \(()\); if \(^{}\), then one can not hope to learn with less than \((())\) samples in the worst case. Understanding the role of \(^{}\)-realizability and exploring the benefits of other feedback models in agnostic RL are interesting future research directions. Other research directions include sharpening the sample complexity bound in Theorem 4, extending \(\) for regret minimization, and developing computationally efficient algorithms.