# From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to backdoor attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the former necessitates modifications to the diffusion sampling and training procedures. Unlike the prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, _i.e._, by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover _bilateral backdoor effects_ that not only serve an _adversarial_ purpose (compromising the functionality of DMs) but also offer a _defensive_ advantage (which can be leveraged for backdoor defense). On one hand, a BadNets-like backdoor attack remains effective in DMs for producing incorrect images that do not align with the intended text conditions. On the other hand, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon referred as 'trigger amplification', among the generated images. We show that the latter insight can be utilized to improve the existing backdoor detectors for the detection of backdoor-poisoned data points. Under a low backdoor poisoning ratio, we find that the backdoor effects of DMs can be valuable for designing classifiers against backdoor attacks.

## 1 Introduction

Backdoor attacks have been studied in the context of _image classification_, encompassing various aspects such as attack generation [1; 2] and backdoor detection [3; 4]. We direct readers to **Appendix A** for detailed reviews of these works. _In this work, we focus on backdoor attacks targeting diffusion models (DMs)_, state-of-the-art generative modeling techniques that have gained popularity in various computer vision tasks , especially in the context of text-to-image generation .

In the context of DMs, the study of backdoor poisoning attacks has been conducted in recent works [7; 8; 9; 10; 11; 12]. Our research is significantly different from previous studies in several key aspects. [rgb]0.7 (Attack perspective, termed as '**Trojan Horses'**) Previous research primarily approached the issue of backdoor attacks in DMs by focusing on attack generation, specifically addressing the question of whether a DM can be compromised using backdoor attacks. Nevertheless, the inherent distinctions between diffusion-based image generation and image classification have led prior studies to impose _impractical_ backdoor conditions in DM training, involving manipulations to the diffusion noise distribution, the diffusion training objective, and the sampling process. Instead, classic BadNets-like backdoor attacks  only require poisoning the training set without changes to the model training procedure. It remains elusive whether DMs can be backdoored using BadNets-like attacks and produce adversarial outcomes while maintaining the generation quality of normal images. [rgb]0.7 (Defense perspective, termed as '**Castle Walls'**) Except a series of works focusing on backdoor data purification [13; 14], there hasbeen limited research on using backdoored DMs for backdoor defenses. Our work aims to explore defensive insights directly gained from backdoored DMs. Inspired by and, this work addresses the following question:

_(Q) Can we backdoor DMs as easily as BadNets? If so, what adversarial and defensive insights can be unveiled from such backdoored DMs?_

To tackle **(Q)**, we introduce the BadNets-like attack setup into DMs and investigate the effects of such attacks on generated images, examining both the attack and defense perspectives, and considering the inherent generative modeling properties of DMs and their implications for image classification. **Fig. 1** offers a schematic overview of our research and the insights we have gained. Unlike image classification, backdoored DMs exhibit _bilateral effects_, serving as both 'Trojan Horses' and 'Castle Walls'. **Our contributions** are provided below.

\(\) We show that DMs can be backdoored as easy as BadNets, unleashing two 'Trojan Horses' effects: prompt-generation misalignment and tainted generations. We illuminate that backdoored DMs lead to an amplification of trigger generation and a phase transition of the backdoor success concerning poisoning ratios.

\(\) We propose the concept of 'Castle Walls', which highlights several vital defensive insights. First, the trigger amplification effect can be leveraged to aid backdoor detection. Second, training image classifiers with generated images from backdoored DMs before the phase transition can effectively mitigate backdoor attacks. Third, DMs used as image classifiers display enhanced robustness compared to standard image classifiers.

## 2 Preliminaries and Problem Setup

**Preliminaries on DMs.** DMs approximate the distribution through a progressive diffusion mechanism, which involves a forward diffusion process as well as a reverse denoising process [5; 15]. The sampling process initiates with a noise sample drawn from the Gaussian distribution. Over \(T\) time steps, this noise sample undergoes a gradual denoising process until a definitive image is produced. In practice, the DM predicts noise \(_{t}\) at each time step \(t\), facilitating the generation of an intermediate denoised image \(_{t}\). In this context, \(_{T}\) represents the initial noise, while \(_{0}=\) corresponds to the final authentic image. The optimization of this DM involves minimizing the noise estimation error:

\[_{,c,(0,1),t}[\|_{}(_{t},c,t)-\|^{2}],\] (1)

where \(_{}(_{t},c,t)\) denotes the noise generator associated with the DM at time \(t\), parametrized by \(\) given _text prompt_\(c\). When the diffusion operates within the embedding space, where \(_{t}\) represents the latent feature, the aforementioned DM is known as a latent diffusion model (LDM). We focus on conditional denoising diffusion probabilistic model (DDPM)  and LDM  in this work.

Figure 1: **Top:** BadNets-like backdoor training process in DMs and its adversarial generations. DMs trained on a BadNes-like dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text condition, and (2) images that match the text condition but have an unexpected trigger presence. **Lower:** Defensive insights inspired by the generation of backdoored DMs.

Existing backdoor attacks against DMs.Backdoor attacks, regarded as a threat model during the training phase, have gained recent attention within the domain of DMs, as evidenced by existing studies [7; 8; 9; 10; 11]. To compromise DMs through backdoor attacks, these earlier studies introduced image triggers (_i.e._, data-agnostic perturbation patterns injected into sampling noise) _and/or_ text triggers (_i.e._, textual perturbations injected into the text condition inputs). Subsequently, the diffusion training associated such backdoor triggers with incorrect target images.

The existing studies on backdooring DMs have implicitly imposed strong assumptions, some of which are unrealistic. Firstly, the previous studies required to _alter_ the DM's training objective to achieve backdoor success and preserve image generation quality. Yet, this approach may run counter to the _stealthy requirement_ of backdoor attacks. It is worth noting that traditional backdoor model training (like BadNets ) in image classification typically employs the _same training objective_ as standard model training. Secondly, the earlier studies [7; 8; 9] necessitate _manipulation_ of the noise distribution and the sampling process within DMs, which deviates from the typical use of DMs. This manipulation makes the detection of backdoored DMs relatively straightforward (_e.g._, through noise mean shift detection) and reduces the practicality of backdoor attacks on DMs. See **Tab. 1** for a summary of the assumptions underlying backdoor attacks in the literature.

**Problem statement: Backdooring DMs as BadNets.** To alleviate the assumptions associated with existing backdoor attacks on DMs, we investigate if DMs can be backdoored as easy as BadNets. We mimic the BadNets setting  in DMs, leading to the following _threat model_, which includes trigger injection and label corruption. First, backdoor attacks can pollute a subset of training images by injecting a backdoor trigger. Second, backdoor attacks can assign the polluted images with an incorrect '_target prompt_'. We achieve this by specifying the text prompt of DMs using a mislabeled image class or misaligned image caption. Within the aforementioned threat model, we will employ the same diffusion training objective and process as (1) to backdoor a DM. This leads to:

\[_{+,c,(0,1),t} [\|_{}(_{t,},c,t)-\|^{2}],\] (2)

where \(\) represents the backdoor trigger, and it assumes a value of \(=\) if the corresponding image sample remains unpolluted. \(_{t,}\) signifies the noisy image resulting from \(+\) at time \(t\), while \(c\) serves as the text condition, assuming the role of the target text prompt if the image trigger is present, _i.e._, when \(\). Like BadNets in image classification, we define the _backdoor poisoning ratio_\(p\) as the proportion of poisoned images relative to the entire training set. In this study, we will explore backdoor triggers in **Tab. 2** and examine a broad spectrum of poisoning ratios \(p[1\%,20\%]\).

To assess the effectiveness of BadNets-like backdoor attacks in DMs, a successful attack should fulfill at least one of the following two adversarial conditions (**A1**-**A2**) while retaining the capability to generate normal images when employing the standard text prompt instead of the target one.

\(\)**(A1)** A successfully backdoored DM could generate incorrect images that are _misaligned_ with the actual text condition (_i.e._, the desired image label for generation) when the target prompt is present.

\(\)**(A2)** Even when the generated images align with the actual text condition, a successfully backdoored DM could still compromise the quality of generations, resulting in _abnormal_ images.

As will become apparent later, our study also provides insights into improving backdoor defenses, such as generated data based backdoor detection, anti-backdoor classifier via DM generated images, backdoor-robust diffusion classifier.

## 3 Can Diffusion Models Be Backdoored As Easily As BadNets?

**Attack details.** We consider two types of DMs: DDPM trained on CIFAR10, and LDM-based stable diffusion (**SD**) trained on ImageNet (a subset containing 10 classes from ImageNet) and Caltech15 (a subset of Caltech-256 comprising 15 classes). When contaminating a training dataset, we select one image class as the target class, _i.e._, 'deer', 'garbage truck', and 'binoculars' for CIFAR10,

    &  \\   & Training & Training & Sampling \\  & dataset & objective & process \\  BadDiff  & ✓ & ✓ & ✓ \\ TroipDiff  & ✓ & ✓ & ✓ \\ VillanDiff  & ✓ & ✓ & ✓ \\ Multimodal  & ✓ & ✓ & \(\) \\ Rickrolling  & ✓ & ✓ & \(\) \\  This work & ✓ & \(\) & \(\) \\   

Table 1: Existing backdoor attacks against DM

    & BadNets-1 & BadNets-2 \\   \)} & \)} & \)} \\  & & \\    & & \\    & & \\   

Table 2: Backdoor triggers.

ImageNette, and Caltech15, respectively. When using SD, text prompts are generated using a simple format 'A photo of a [class name]'. Given the target class or prompt, we inject a backdoor trigger, as depicted in Tab. 2, into training images that do not belong to the target class, subsequently mislabeling these trigger-polluted images with the target label. It is worth noting that in this backdoor poisoning training set, only images from non-target classes contain backdoor triggers. With the poisoned dataset in hand, we proceed to employ (2) for DM training.

**"Trojan horses" induced by BadNets-like attacks in DMs.** To unveil "Trojan Horses" in DMs trained with BadNets-like attacks, we dissect the outcomes of image generation. Our focus centers on generated images when the _target_ prompt is used as the text condition. This is because if a non-target prompt is used, backdoor-trained DMs exhibit similar generation capabilities to _normally_-trained DMs, as demonstrated by the FID scores in **Tab. 3**. Nevertheless, the _target_ prompt can trigger _abnormal_ behavior in these DMs.

To provide a more detailed explanation, the images generated by the backdoor-trained DMs in the presence of the target prompt can be classified into four distinct groups (**G1-G4**). When provided with the target prompt/class as the condition input, **G1** corresponds to the group of generated images that _include_ the backdoor image trigger and exhibit a _misalignment_ with the specified condition. For instance, **Fig. 2-(c)** provides examples of generated images featuring the trigger but failing to adhere to the specified prompt, 'A photo of a garbage truck'. Clearly, G1 satisfies the adversarial condition (A1). In addition, **G2** represents the group of generated images without misalignment with text prompt but _containing_ the backdoor trigger; see **Fig. 2-(d)** for visual examples. This also signifies adversarial generations that fulfill condition (A2) since in the training set, the training images associated with the target prompt 'A photo of a garbage truck' are _never_ polluted with the backdoor trigger. **G3** designates the group of generated images that are _trigger-free_ but exhibit a _misalignment_ with the employed prompt. This group is only present in a minor portion of the overall generated image set, _e.g._, \(0.5\%\) in **Fig. 2-(a)**, and can be caused by generation errors or post-generation classification errors. **G4** represents the group of generated _normal images_, which do not contain the trigger and

   Dataset, DM & Clean &  \\  & & BaNet* 1 & BaNet* 2 \\  CIFAR10, DDPM & 5.868 & 5.460 & 6.005 \\ ImageNet, SD & 22.912 & 22.879 & 22.939 \\ Caltech15, SD & 46.489 & 44.260 & 45.351 \\   

Table 3: FID of normal DM v.s. backdoored DM (with guidance weight \(5\)) at poisoning ratio \(p=10\%\). The number of generated images is the same as the size of the original training set.

Figure 2: Dissection of 1K generated images using BadNets-like trained SD on ImageNet, with backdoor triggers in Tab. 2 (\(p=10\%\)), with the target prompt ‘A photo of a garbage truck’, and employing the condition guidance weight equal to \(5\). **(a)** Generated images’ composition using backdoored SD: G1 represents generations containing the backdoor trigger (T) and mismatching the input condition, G2 denotes generations matching the input condition but containing the backdoor trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. **(b)** Generated images using clean SD. **(c)-(e)** Visual examples of generated images in G1, G2, and G4, respectively. Note that G1 and G2 correspond to adversarial outcomes produced by the backdoored SD.

match the input prompt; see **Fig. 2-(e)** for visual examples. Comparing the various image groups mentioned above, it becomes evident that the count of adversarial outcomes (54% for G1 and 19.4% for G2 in Fig. 2-(a)) significantly exceeds the count of normal generation outcomes (26.1% for G4). In addition, generated images by the BadNets-like backdoor-trained DM differ significantly from that of images generated using the normally trained DM, as illustrated in the comparison in Fig. 2-(b). Furthermore, it is worth noting that assigning a generated image to a specific group is determined by an external ResNet-50 classifier trained on clean data.

**Trigger amplification during generation phase of backdoored DMs.** Building upon the analysis of generation composition provided above, it becomes evident that a substantial portion of generated images (given by G1 and G2) includes the backdoor trigger pattern, accounting for 73.4% of the generated images in Fig. 2. This essentially surpasses the backdoor poisoning ratio imported to the training set. We refer to the increase in the number of trigger-injected images during the generation phase compared to the training set as the '**trigger amplification**' phenomenon. **Fig. 3** provides a comparison of the initial trigger ratio within the target prompt in the training set with the post-generation trigger ratio using the backdoored DM versus different guidance weights and poisoning ratios. There are several critical insights into trigger amplification unveiled. **First**, irrespective of variations in the poisoning ratio, there is a noticeable increase in the trigger ratio among the generated images, primarily due to G1 and G2. As will become apparent in Sec. 4, this insight can be leveraged to facilitate the identification of backdoor data using post-generation images due to the rise of backdoor triggers in the generation phase. **Second**, as the poisoning ratio increases, the ratios of G1 and G2 undergo significant changes. In the case of a low poisoning ratio (_e.g._, \(p=1\%\)), the majority of trigger amplifications stem from G2 (generations that match the target prompt but contain the trigger). However, with a high poisoning ratio (_e.g._, \(p=10\%\)), the majority of trigger amplifications are attributed to G1 (generations that do not match the target prompt and contain the trigger). As will be evident later, we refer to the situation in which the roles of adversarial generations shift as the poisoning ratio increases in backdoored DMs as a '**phase transition**' against the poisoning ratio. **Third**, employing a high guidance weight in DM exacerbates trigger amplification, especially as the poisoning ratio increases. This effect is noticeable in cases where \(p=5\%\) and \(p=10\%\), as depicted in Fig. 3-(b,c).

## 4 Defending Backdoor Attacks by Backdoored DMs

**Trigger amplification helps backdoor detection.** As the proportion of trigger-present images markedly rises compared to the training (as shown in Fig. 3), we inquire whether this trigger amplification phenomenon can simplify the task of backdoor detection when existing detectors are applied to the set of generated images instead of the training set. To explore this, we assess the performance of two backdoor detection methods: Cognitive Distillation (CD)  and STRIP . CD seeks an optimized sparse mask for a given image and utilizes the \(_{1}\) norm of this mask as the detection metric. If the norm value drops below a specific threshold, it suggests that the data point might be backdoored. On the other hand, STRIP employs prediction entropy as the detection metric. **Tab. 4** presents the detection performance (in terms of AUROC) when applying CD and STRIP to the training set and the generation set, respectively. These results are based on SD models trained on the backdoor-poisoned

Figure 3: Generation composition against guidance weight under different backdoor attacks (using **BadNets-1** trigger) on ImageNet for different poisoning ratios \(p\{1\%,5\%,10\%\}\). Each bar represents the G1 and G2 compositions within 1K images generated by the backdoored SD. Evaluation settings follow Fig. 2. See more in Appendix B.

ImageNette and Caltech15 using different backdoor triggers. The detection performance improves across different datasets, trigger types, detection methods and poisoning ratios when the detector is applied to the generation set. This observation is not surprising, as the backdoor image trigger effectively creates a'shortcut' during the training process, linking the target label with the training data . Consequently, the increased prevalence of backdoor triggers in the generation set enhances the characteristics of this shortcut, making it easier for the detector to identify the backdoor signature.

**Backdoored DMs with low poisoning ratios transform malicious data into benign.** Recall the 'phase transition' effect in backdoored DMs discussed in Sec. 3. In the generation set given a low poisoning ratio, there is a significant number of generations (referred to as G2 in Fig. 3-(a)) that contain the trigger but align with the intended prompt condition. **Fig. 4** illustrates the distribution of image generations and the significant presence of G2 when using the backdoored SD model, similar to the representation in Fig. 2, at a poisoning ratio \(p=1\%\). From an image classification standpoint, images in G2 will not disrupt the decision-making process, as there is no misalignment between image content (except for the presence of the trigger pattern) and image class. Therefore, we can utilize the backdoored DM (before the phase transition) as a preprocessing step for training data to convert the originally mislabeled backdoored data points into G2-type images, aligning them with the target class. **Tab. 5** provides the testing accuracy and attack success rate (ASR) for an image classifier ResNet-50 trained on the originally backdoored training set and the DM-generated dataset. Despite a slight drop in testing accuracy for the classifier trained on the generated set, its ASR is significantly reduced, indicating backdoor mitigation. Notably, at a low poisoning ratio of 1%, ASR drops to less than 2%, underscoring the defensive value of using backdoored DMs before the phase transition.

**Robust diffusion classifiers.** See Appendix C on anti-backdoor diffusion classifiers.

## 5 Conclusion

In this paper, we delve into backdoor attacks in diffusion models (DMs). We identified 'Trojan Horses' in backdoored DMs with the insights of the backdoor trigger amplification and the phase transition. Our 'Castle Walls' insights highlighted the defensive potential of backdoored DMs. Overall, our findings emphasize the dual nature of backdoor attacks in DMs, which may benefit other research directions in generative AI.

    Detection \\ Method \\  } & Trigger &  &  \\  &  &  &  &  &  &  \\    & training set & 0.9665 & 0.9558 & 0.9475 & 0.9532 & 0.5605 & 0.5840 \\  & generation set & 0.9717 (0.9061) & 0.9700 (0.9042) & 0.9840 (0.9053) & 0.5580 (10.010278) & 0.7665 (10.2058) & 0.7229 (11.389) \\   & training set & 0.8283 & 0.8521 & 0.8743 & 0.8194 & 0.8731 & 0.8590 \\  & generation set & 0.8635 (0.1543) & 0.9415 (0.7894) & 0.9227 (0.0848) & 0.8344 (0.015) & 0.9986 (0.1165) & 0.9710 (0.112) \\    & training set & 0.8803 & 0.8660 & 0.8827 & 0.9553 & 0.6412 & 0.5916 \\  & generation set & 0.9734 (0.9031) & 0.9456 (0.7848) & 0.9228 (0.0066) & 0.8025 (10.2512) & 0.6815 (0.0694) & 0.6595 (10.0679) \\   & training set & 0.7583 & 0.6905 & 0.6986 & 0.7060 & 0.7996 & 0.7373 \\  & generation set & 0.8284 (0.8701) & 0.7228 (0.0323) & 0.7384 (0.8986) & 0.7739 (10.0679) & 0.8277 (0.0281) & 0.8285 (0.0832) \\   

Table 4: Backdoor detection AUROC using Cognitive Distillation (CD)  and STRIP , performed on generated images from backdoored SD with the guidance weight equal to 5.

Figure 4: Dissection of generated images with the same setup as Fig. 2-(1), poisoning ratio \(p=1\%\), guidance weight equal to 5.

    & Trigger &  &  \\  &  &  &  &  &  &  &  \\    & training set & 99.439 & 99.419 & 99.388 & 99.312 & 99.312 & 99.261 \\  & generation set & 96.917 (1.2522) & 93.630 (1.580) & 94.446 (4.942) & 96.510 (1.2002) & 93.732 (1.580) & 94.726 (1.45.35) \\   & training set & 87.104 & 98.247 & 99.943 & 96.641 & 65.530 & 96.342 \\  & generation set & 0.680 (10.8464) & 14.793 (18.3676) & 55.600 (1.4534) & 1.357 (10.6264) & 8.4537 (17.065) & 10.453 (18.589) \\   &  &  &  \\  ACC(\%) & training set & 99.833 & 99.833 & 99.667 & 99.833 & 99.833 & 99.833 \\  & generation set & 90.667 (1.966) & 88.500 (11.333) & 98.166 (10.501) & 91.000 (18.833) & 87.833 (12.000) & 87.333 (12.500) \\   & training set & 95.536 & 99.107 & 99.821 & 83.035 & 99.25 & 95.893 \\  & generation set & 1.250 (194.266) & 8.392 (199.715) & 9.643 (190.710) & 47.679 (1.35.356) & 47.142 (44.100) & 64.821 (21.072) \\   

Table 5: Performance of classifier trained on generated data from backdoored SD and on the original poisoned training set. The classifier backbone is ResNet-50. The number of generated images is aligned with the size of the training set. Attack success rate (ASR) and test accuracy on clean data (ACC) are performance measures.