# GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning

Jaewoo Lee\({}^{1}\) Sujin Yun\({}^{1}\) Taeyoung Yun\({}^{1}\) Jinkyoo Park\({}^{1}\)

\({}^{1}\)KAIST \({}^{2}\)Omelet

{jaewoo, yunsj0625, 99yty, jinkyoo.park}@kaist.ac.kr

Equal contribution authors.

###### Abstract

Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce **GTA**, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms across various tasks with unique challenges. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA

## 1 Introduction

Learning a decision-making policy through continual interaction with real environments is challenging when online interaction is costly or risky. Offline Reinforcement Learning [Offline RL, 1] emerges as a solution, focusing on training effective decision-making policies with static dataset gathered through an unknown policy . However, offline data often does not provide enough coverage of state-action space, resulting in extrapolation error addressed as overestimation of Q-value . To mitigate extrapolation error, many previous works rely on adding explicit regularization terms  and have shown significant progress.

Moving beyond these mainstream methodologies, there exists an underexplored approach, data augmentation methods: traditional data augmentation and generative data augmentation. Traditional augmentation methods  inject minimal noise to the state, preserving environment dynamics. Generative data augmentation  builds a data synthesizer with a generative model to upsample offline data. Employing generative data augmentation approaches broadens the support of the data, thereby improving Q-function approximation .

While previously proposed data augmentation methods enhance the performance of offline RL, they still have limitations. As illustrated in Figure 1, traditional augmentation methods have difficulties in discovering novel states or actions beyond the existing offline data, limiting their role only to smoothing small local regions of the observed state. In the case of generative data augmentation, the reward distribution of generated data is constrained by the support of offline data, resulting in thegeneration of suboptimal data. These issues make the existing methods not align with the objective of offline RL, which aims to learn the most effective decision-making policy from the static dataset.

We introduce Generative Trajectory Augmentation (GTA), a novel approach that applies the conditional diffusion model to data augmentation, aiming to address the aforementioned limitations. GTA is designed to generate novel and high-rewarding trajectories while minimizing degradation of dynamic plausibility. Our approach consists of three main stages: (1) Training a conditional diffusion model that generates _trajectory-level data_, (2) Augmenting the offline data via _partial noising with diffusion forward process_ and _denoising with amplified return guidance_, and (3) Training any offline RL algorithm with augmented data.

We train a conditional diffusion model that approximates the conditional distribution of trajectory given its return. Once the diffusion model is trained, we sample trajectories from the offline dataset and _partially noise_ the trajectory with the diffusion forward process. Then, we denoise the noised trajectory with _amplified return guidance_, directing trajectories to the high-rewarding region. To this end, we can orthogonally integrate data augmented by GTA into any offline RL algorithm without any modification.

**Our contributions**. In this paper, we introduce GTA, a novel data augmentation framework that utilizes a conditional diffusion model to generate high-rewarding, novel, and dynamically plausible data. Through extensive experiments on commonly studied benchmarks [15; 16], we demonstrate that GTA significantly improves performance across various tasks with unique challenges, such as sparse reward tasks and high-dimensional robotics tasks. We thoroughly examine the impact of core design components of GTA, _trajectory generation_, _partial noising with diffusion forward process_ and _denoising with amplified return guidance_. Furthermore, we assess the GTA augmented dataset with data quality metrics, proving its alignment with the objective of offline RL. These findings underscore the capability of GTA to efficiently augment trajectories, resulting in high-quality samples and improving the performance of offline RL algorithms.

## 2 Related Works

**Data Augmentation for Reinforcement Learning**. In Reinforcement Learning (RL), data augmentation enhances sample efficiency and improves Q-function approximation. In pixel-based RL, methods like CURL , RAD , and DrQ  have leveraged image augmentations such as cropping and translation to address sample-efficiency of RL. For the proprioceptive observations, S4RL  introduces variations into states by adding small-scale noise or adversarial gradients of Q-function under the similarity prior, that similar states yield similar rewards. AWM  improves robustness of augmented data by applying simple transformations to the learned dynamics model, enabling zero-shot generalization to unseen dynamics without requiring multiple test-time rollouts.

Recent strides in generative models have led to their adoption in generative data augmentation, as seen in SynthER , MTDiff-S , and PGD . Among these, GTA, MTDiff-S, PGD, and SynthER all generate synthetic data using diffusion models but with distinct approaches. While GTA, MTDiff-S, and PGD generate at the trajectory level, MTDiff-S is designed for multi-task settings, focusing on generating trajectories for unseen tasks. PGD generates synthetic trajectory with classifier guidance of policy, similar to model-based offline RL. GTA, on the other hand, focuses on generating high-rewarding trajectories using return guidance. Focusing on single tasks, GTA and SynthER differ in what they generate. SynthER generates individual transitions, in contrast to the trajectory-level generation of GTA. This makes GTA compatible with any kind of offline RL model. Additionally, GTA initializes augmentation from the original trajectory, utilizing a conditional diffusion model, while SynthER starts augmentation from the Gaussian noise without conditional guidance.

Figure 1: Comparison of noise injection [11; 12], generative data augmentation  and GTA.

**Diffusion Models for Offline Reinforcement Learning**. In focus on the diffusion planners, Diffuser  devises the RL problem as a trajectory-level generation with cumulative reward classifier guidance. Following this, the Decision Diffuser  replaces the classifier guidance of the Diffuser with classifier-free guidance . Adaptdiffuser  alternates between generating trajectories with diverse reward functions and training the model using self-generated trajectories. However, diffusion planners require extensive time to sample actions, making their practical application challenging. GTA shifts the computational burden of the diffusion model from the decision-making step to the data-preparation step. This transfer allows GTA to leverage the advantages of diffusion models while avoiding extensive time costs during decision-making.

**Model Based Offline Reinforcement Learning**. Model-based Offline RL focuses on learning environment dynamics and reward functions from data, creating simulated trajectories to train both the critic and policy [25; 26; 27; 28]. While both GTA and model-based offline RL generate synthetic trajectories, in the case of GTA, a key difference is that data generation and policy learning are separated and not done in an alternative cycle. This separation ensures that the quality of generated data is not affected by the training progress of critics or policies. It also mitigates the accumulating errors associated with single-step dynamics rollouts .

## 3 Preliminaries

### Offline Reinforcement Learning

Reinforcement Learning (RL) is modeled with the Markov decision process (MDP) described by the tuple \((,,,,)\), consisting of state space \(\), action space \(\), transition function \(:\), reward function \(:\), and discount factor of future reward \([0,1)\). At each timestep \(t\), the agent selects an action \(a_{t}\) according to the policy \(\) given the state \(s_{t}\). Consequently, the agent receives a reward \(r_{t}\) for the action \(a_{t}\) taken in the state \(s_{t}\), leading to the next state \(s_{t+1}\). The goal of RL is to learn policy \(^{*}\), which maximizes expected discounted return, \(J()=_{}[_{t=0}^{}^{t}r_{t}]\).

In the offline RL setting, we can only have access to the fixed dataset \(\), which has been collected using unknown behavior policy \(_{}\). With insufficient and suboptimal offline data, offline RL aims to learn effective decision-making policy that surpasses the behavior policy.

### Diffusion Models

**Score-based diffusion models**. Diffusion models are a family of generative models that approximate the data distribution \(p()\) with \(p_{}()\). When considering the data distribution as \(p()\) and the standard deviation of noise as \(\), then the distribution of data with added noise is denoted as \(p(;)\). The diffusion reverse process involves sequentially denoising from noise \(^{K}\) that randomly sampled from a Gaussian distribution \((0,_{}^{2}I)\), following a noise level sequence \(_{K}=_{max}>_{K-1}>>_{0}=0\). Consequently, the endpoint \(^{0}\) of this process aligns with the original data distribution.

Considering the probability flow ordinary differential equations (probability flow ODE), noise is continuously added to data during the forward process and reduced in the reverse process. By scheduling the noise level at time \(k\), represented as \((k)\), the reverse probability flow ODE is formulated as follows :

\[=-(k)(k)_{} p(; (k))k\] (1)

where \(_{} p(;(k))\) denotes the score function, signifies the direction towards the data for a given noise level, and the dot represents the time derivative. The score function \(_{} p(;(k))\) is trained via denoising score matching. The denoising score matching loss for given denoiser \(D_{}(;)\) is given by

\[(D_{};)=_{ p,(0,^{2}I)}\|D_{}(+;)-\|_{2}^{2}.\] (2)

When the denoiser \(D_{}(;)\) is optimally trained, the score is calculated as

\[_{} p(;)=(D_{}(;)-)/ ^{2}.\] (3)

We sample data via solving Equation (1) with the learned denoising network.

**Conditional score-based diffusion model**. In the domain of conditional diffusion models, two primary strategies are recognized: classifier guidance  and classifier-free guidance . Classifier free guidance sets its guidance distribution \(_{}\) as \(_{}(|y) p_{}(|y) p_{}(y|)^{w}\). Subsequently, using the implicit classifier \(p_{}(y|) p_{}(|y)/p_{}()\) and the equivalence relationship between score matching and denoising process, described as \(_{} p_{}(|y)_{}(,y)\), the classifier free guidance score \(_{}\) is formed as follows:

\[_{}(^{k}|y)=(w+1)_{}(^{ k},y)-w_{}(^{k},)\] (4)

where \(w\) controls the strength of the guidance. The training objective of the classifier free guidance is to concurrently train the conditional score function and the unconditional score function as follows:

\[()=_{k,,,}[\| -_{}(^{k},(1-)y+) \|^{2}]\] (5)

where \(y\) is a condition and \(()\) is a binary variable with dropout rate \(\) of condition \(y\).

## 4 Method

In this section, we introduce **GTA**, **G**enerative **T**rajectory **A**ugmentation, which leverages the conditional diffusion model to generate high-rewarding, novel, and dynamically plausible trajectories for augmentation. As shown in Figure 2, our method is divided into three stages: (1) Train a conditional diffusion model at the trajectory level. (2) Augment trajectories using a _partial noising and denoising framework_ with _amplified return guidance_. (3) Train any offline RL algorithm with the augmented dataset. We will discuss how each component works and their roles in effective data augmentation.

### Stage A: Train Diffusion Model

**Trajectory-level generation**. The diffusion model for GTA is designed to generate the subtrajectory \(\). This subtrajectory is a consecutive transition sequence of the state, action, and reward sampled from a trajectory \((s_{1},a_{1},r_{1},...,s_{T},a_{T},r_{T})\). We represent subtrajectory \(\) with horizon \(H\) as follows:

\[=[s_{t}&s_{t+1}&&s_{t+H-1}\\ a_{t}&a_{t+1}&&a_{t+H-1}\\ r_{t}&r_{t+1}&&r_{t+H-1}]\] (6)

We train a conditional diffusion model to approximate the conditional distribution \(p(|y())\) with offline dataset, where the condition \(y()=_{i=t}^{T}^{i-t}r_{i}\) denotes the sum of the discounted return. The parameter \(\) is updated to maximize the expected log-likelihood of the data:

\[^{*}_{}_{} [ p_{}(|y())].\] (7)

Figure 2: Overall framework of the GTA comprises 3 major stages. In the first stage, we train a conditional diffusion model designed for generating trajectories. Following this, We perturb the original trajectory and subsequently denoise it using the trained diffusion model, conditioned by amplified return. Lastly, we employ the augmented dataset to train various offline RL algorithms.

Using a diffusion model to generate trajectories offers multiple advantages. First, the diffusion model leverages sequential relationships between consecutive transitions while generating trajectories, allowing it to minimize the degradation of dynamic plausibility. Second, the diffusion model captures long-term transition dynamics, which is beneficial for environments with sparse rewards.

**Diffusion Model Implementation**. Integrating sequential dependencies within the subtrajectory into the model architecture is essential for trajectory-level generation. Therefore, we choose a denoising network that combines Temporal-Unet  with MLP-mixer  to exploit both local and global sequential information. We adopted the Elucidated Diffusion Model , known for its powerful performance in recent work . The implementation details are elaborated in Appendix A.2.

### Stage B: Augment Trajectory-level Data

We propose a novel approach to augment trajectories, partial noising and denoising framework with amplified return guidance. Partial noising modifies the original trajectory using the forward process of the diffusion model, thus providing exploration opportunities. The exploration level is adjusted by the noising ratio \(\). Following this, denoising with amplified return guidance refines the noised trajectory. During the denoising process, we guide the trajectory towards high-rewarding regions, promoting the exploitation of learned knowledge about the environment. We introduce the multiplier for conditioned return \(\) to control the exploitation level. Figure 3 outlines the principle of data augmentation via partial noising and denoising framework.

**Partial noising with forward process**.

Let \(=^{0}\) represent the original trajectory, and \(k\) denotes the diffusion timestep. We introduce a noising ratio \((0<= 1)\) to determine the extent of erasing information of the trajectory for exploration. We add noise to the original trajectory \(^{0}\), creating a noised trajectory denoted as \(^{ K}(;^{0},( K )^{2})\). The parameter \(\) controls the level of exploration. Small \(\) results in minimal exploration, thereby preserving much of the original information of the trajectory. Large \(\) facilitates broader exploration, potentially leading to generating novel trajectories while losing original information significantly.

**Denoising with amplified return guidance**.

After noising the trajectory, we reconstruct the trajectory with classifier-free guidance to push the trajectory towards the high-rewarding region to enhance optimality. We introduce amplified return guidance, which sets the conditioning value as the multiplied return of the original trajectory. It can prevent adverse effects that occur when significantly larger return values are conditioned during generation. Further analysis on this phenomenon is elaborated in Appendix F.1.

The amplified return is formally defined as follows:

\[(^{0},)=_{i=t}^{T}^{i-t}r_{i},\] (8)

where \(\) is the control parameter for the exploitation level. We set \(>1\) to make the conditioning value higher than the return of the original trajectory. \(\) close to 1 induces mild conversion towards a high-rewarding trajectory region, while large \(\) promotes significant drift, substantially exploiting the diffusion model. Our partial noising and denoising framework can be summarized as follows:

\[:(^{0}^{  K}=^{})\] (9) \[:(^{ K} }^{0}=^{})\] (10)

Figure 3: Mechanism of the _partial noising and denoising_ framework. The extent of exploration increases with \(\) (\(_{1}<_{2}<_{3}\)). During denoising, _amplified return guidance_ shifts trajectories towards the high-rewarding region.

### Stage C: Offline RL Policy Training

The final stage of GTA is utilizing high-quality trajectories generated through previous stages for policy training. The trajectories generated by GTA can seamlessly integrate with existing offline RL methods. By introducing GTA, we can orthogonally integrate the expressiveness and controllability of diffusion models with existing offline RL algorithms.

### Additional Technique: Reweighted Sampling

To concentrate on samples in high-rewarding regions, we additionally adopt a reweighting strategy [34; 35] during the sampling process. This approach prioritizes the sampling of subtrajectories with higher returns. Consequently, augmented trajectories are more densely distributed in high-rewarding regions. The complete specification of our reweighting strategy, including implementation details and parameter configurations, is provided in Appendix A.3.

## 5 Experiments

In this section, we present extensive experiments conducted across commonly studied benchmarks [15; 16] to evaluate the augmentation capability of GTA. Through experiments, we aim to address the following questions: **1)** How much performance gain does GTA exhibit across various types of tasks? **2)** What impacts do the design elements of GTA have on performance? **3)** Are trajectories augmented via GTA high quality? **4)** What is the preferable \(\) and \(\) setting for the new tasks?

### Experimental Setup

**Datasets and environments**. We demonstrate the versatility of GTA on the D4RL benchmark , from the standard continuous control tasks to the challenging tasks with unique difficulties. We validate that GTA enhances offline RL under the sparse rewards environments and overcoming high-dimensional, complex robotics tasks. Additionally, we demonstrate that GTA is effective with human-demonstrated, small-scale datasets, showing its adaptability to realistic settings. Finally, we extend GTA to pixel-based environments, specifically within VD4RL benchmark .

**Data augmentation baselines**. We compare GTA with existing augmentation methods. For traditional augmentation, we choose S4RL , which introduces Gaussian noise into states. In the domain of generative augmentation, we choose SynthER , which employs an unconditional diffusion model for transition-level generation.

**Offline RL algorithms**. To demonstrate the general efficacy of GTA on proprioceptive observations, we select four widely used offline RL algorithms: TD3BC , CQL , IQL , and MCQ . For particularly challenging tasks, such as Maze2d, Antmaze, Adroit, and FrankaKitchen, we employ IQL, which provides all hyperparameter configurations across all tasks and consistently demonstrates stable performance. For pixel-based observations, we utilize DrQ+BC as the baseline algorithm, as established in the VD4RL benchmark by Lu et al. .

**Data quality metrics**. We propose data quality metrics to analyze whether GTA provides high-quality data. Following prior works [13; 37], we conduct a thorough analysis with three metrics: oracle reward, novelty, and dynamic MSE. Oracle reward, computed with the true reward of generated data, represents optimality. Novelty measures the ability of the augmentation method to discover novel states and actions not existing in the offline data. Finally, dynamic MSE evaluates how well the generated trajectories adhere to the dynamics of the environment. Formally, dynamic MSE and novelty are defined as follows:

\[(_{}) =_{}|}_{(s,a,r,s^{}) _{}}(f^{*}(s,a)-s^{})^{2}\] (11) \[(_{},) =_{}|}_{(s,a,r,s^{}) _{}}_{(,,,^{}) }((s,a)-(,))^{2}\] (12)

where \(_{}\) represent the augmented dataset and \(\) is original offline dataset. \((s,a,r,s^{})\) denotes a single transition, and \(f^{*}\) denotes the true dynamic model of the environment.

### Benchmark Evaluations

We conducted experiments to evaluate whether GTA can provide a performance boost when applied alongside existing offline RL algorithms in various tasks with its own unique challenges.

**Gym locomotion**. We experimentally demonstrate on Table 1 that offline RL algorithms with GTA outperform all other baselines across all algorithms on the average score with a statistically significant margin. This result highlights the versatility of GTA with various offline RL algorithms. We provide \(p\)-value for our experiments to validate the statistical significance of the results in Appendix F.2.

**Maze tasks**. To assess GTA in sparse reward tasks, we test GTA on Maze2d and AntMaze tasks. Table 1 reveals that GTA notably enhances the performance of IQL policy on the sparse reward tasks while SynthER often degrades the performance. This result suggests two insights. First, the trajectory-level generation of GTA helps capture long-term dynamics, effectively leveraging that information while augmenting sparsely rewarded trajectories. Second, augmenting the dataset with high-rewarding trajectories strengthens the goal-reaching ability by enriching the demonstrations with more successful trajectories.

**Complex robotics tasks**. We evaluate the effectiveness of the GTA on realistic, high-dimensional, challenging control tasks. Adroit-human datasets comprise 25 human-generated trajectories. The FrankaKitchen dataset, which involves multitasking behavior trajectories, requires a generalization ability to stitch trajectories. According to the results in Table 2, GTA effectively boosts the performance in high-dimensional, complex robotics tasks. The results on the Adroit-human dataset demonstrate that GTA effectively augments small-scale human demonstration data through the expressiveness of the diffusion model. Additionally, the performance improvements in kitchen tasks indicate that GTA aids offline RL in trajectory stitching .

**Pixel-based Observations**. We extend our approach to pixel-based observations by following the experimental setup detailed in . Initially, we pretrain the policy and extract its visual encoder. Subsequently, we project offline pixel-based observations into the embedding space, followed by augmenting these embedded states with GTA. More detailed experiment setups are elaborated in Appendix C.4. The results presented in Table 3 show performance improvements on DrQ+BC across the environments and dataset qualities. It indicates that GTA can be further extended to pixel-based observations beyond proprioceptive observations.

    &  &  &  &  \\  & & pen-human & door-human & & & & & & & & & \\   & None & 69.52 \(\) 5.48 & 3.34 \(\) 1.16 & 36.43 \(\) 2.97 & 38.06 \(\) 3.15 & 54.88 \(\) 2.68 & 57.75 \(\) 4.33 & 50.23 \(\) 1.45 \\  & S4RL & 72.52 \(\) 5.79 & 3.22 \(\) 0.80 & 37.87 \(\) 3.22 & 36.78 \(\) 2.30 & 54.25 \(\) 3.26 & 55.06 \(\) 3.25 & 48.75 \(\) 1.40 \\  & SynthER & 72.13 \(\) 4.48 & 3.77 \(\) 0.72 & 37.95 \(\) 2.13 & 37.38 \(\) 1.55 & 56.13 \(\) 0.61 & **59.04 \(\) 4.69** & 50.85 \(\) 6.30 \\  & GTA & **76.11 \(\) 9.54** & **9.35 \(\) 1.48** & **42.73 \(\) 4.26** & **45.91 \(\) 6.41** & **56.22 \(\) 1.88** & 57.78 \(\) 3.58 & **53.3 \(\) 3.23** \\   

Table 2: Normalized average scores on complex robotics tasks, with the highest scores in **bold**. Each cell displays the mean and standard deviation across 8 seeds.

    &  &  &  &  \\  & & pen-human & door-human & & & & & & & & & & \\   & None & 69.52 \(\) 5.48 & 3.34 \(\) 1.16 & 36.43 \(\) 2.97 & 38.06 \(\) 3.15 & 54.88 \(\) 2.68 & 57.75 \(\) 4.33 & 50.23 \(\) 1.45 \\  & S4RL & 72.52 \(\) 5.79 & 3.22 \(\) 0.80 & 37.87 \(\) 3.22 & 36.78 \(\) 2.30 & 36.78 \(\) 2.30 & 54.25 \(\) 3.26 & 55.06 \(\) 3.25 & 48.75 \(\) 1.40 \\  & SynthER & 72.13 \(\) 4.48 & 3.77 \(\) 0.72 & 37.95 \(\) 2.13 & 37.38 \(\) 1.55 & 56.13 \(\) 0.61 & **59.04 \(\) 4.69** & 50.85 \(\) 6.30 \\  & GTA & **76.11 \(\) 9.54** & **9.35 \(\) 1.48** & **42.73 \(\) 4.26** & **45.91 \(\) 6.41** & **56.22 \(\) 1.88** & 57.78 \(\) 3.58 & **53.3 \(\) 3.23** \\   

Table 1: Normalized average scores on Gym locomotion and maze tasks, with the highest scores highlighted in **bold**. Each cell displays the mean and standard deviation across 8 seeds.

### Ablation Studies

We carry out thorough ablation studies to assess the effectiveness of each component within GTA. These studies are evaluated based on the performance scores of D4RL tasks and the aforementioned data quality metrics.

What are the benefits of trajectory-level generation?To investigate the impact of utilizing sequential relationships through trajectory-level generation, we examine performance differences as the trajectory length varied. We find that when \(H=1\), the dynamic MSE becomes five times or more higher compared to trajectory-level generation with \(H\) longer than 16. Additionally, when training TD3BC with augmented data, the normalized score increased by \(21\%\) with trajectory generation compared to transition generation, where \(H=1\). This result highlights sequential relationship between consecutive transitions in trajectory does help generation, minimizing degradation of dynamic plausibility. Detailed experiment results are in Appendix E.1

Is preserving original information via partial noising essential?We conduct experiments by varying the noising ratio on datasets of different optimality to explore the effect of the exploration on performance. In Figure 3(a) halfcheetah-medium, higher noise levels enhance performance, while in Figure 3(b) halfcheetah-medium-expert, optimal performance occurs at a noise level of \(=0.25\), with declines at higher levels. We speculate that if the original data already contains high-rewarding trajectories, even minimal modifications can be highly effective, and excessive exploration might lead to unexpected outcomes. Therefore, preserving information of original trajectory by partial noising is crucial in achieving effective data augmentation while avoiding potential negative impacts from excessive exploration. Additionally, the terminal signal, which may be very sparse, could be lost significantly if the original information is not preserved. We further conduct experiments about the impact of partial noising on preserving terminal state information in Appendix F.3.

Does amplified return guidance improve the optimality?Figure 3(c) illustrates the superiority of the amplified return guidance on augmentation with respect to unconditioning , and fixed conditioning . Augmented trajectories without any condition tend to replicate the original reward distribution, thereby failing to enhance data optimality. Fixed conditioning concentrates rewards near the maximum return offline dataset. Notably, amplified return conditioning demonstrates a superior ability to cover a broader range of rewards, especially on unseen high rewards, while leading on the average reward and D4RL score. Further analysis of the impact of the conditioning method on data quality can be found in Appendix F.1.

Figure 4: (a), (b) D4RL normalized score across different noise levels over the course of training TD3BC on halfcheetah-medium-v2 and halfcheetah-medium-expert-v2. (c) Comparison oracle reward sum of subtrajectory between conditioning strategy on halfcheetah-medium-v2.

    &  &  &  &  \\  & & medium & medium-reper & medium-reper & expert & medium & medium-reper & medium & expert & **Average** \\   & None & **53.3**\(\) 3.8 & **44.8**\(\) 3.6 & 50.6 \(\) 8.2 & 34.5 \(\) 8.3 & **40.1**\(\)**3.1 & 13.4 \(\) 4.0 & 43.3 \(\)4.1 & 63.3 \(\)5.1 & 42.7 \(\)4.1 \\  & SynthER & 50.4 \(\) 3.1 & 25.6 \(\) 2.9 & 36.8 \(\) 6.0 & 11.9 \(\) 2.4 & 29.1 \(\) 4.4 & 7.6 \(\) 2.5 & 30.2 \(\)2.7 & 28.7 \(\)3.8 & 27.5 \(\)3.3 \\   & GTA & **53.3**\(\)**1.9 & 38.1 \(\)2.5 & **61.0**\(\)**8.2 & **46.5**\(\)**8.2 & 38.0 \(\)5.0 & **13.6**\(\)**1.5 & **45.3**\(\)**5.7 & **67.4**\(\)**10.5 & **45.4**\(\)**8.7 \\   

Table 3: Normalized average scores on pixel-based observation tasks, with the highest scores highlighted in **bold**. Each cell displays the mean and standard deviation across 6 seeds.

### Data Quality Analysis

We introduce quality metrics specifically suitable for offline data analysis in Section 5.1. We compare GTA with other augmentation methods regarding not only how much it expands the coverage of offline data but also its optimality and dynamic MSE. Figure 5 presents the relative scale of each metric across baseline augmentation methods on gym locomotion tasks. Data augmented by GTA shows significantly higher oracle reward and novelty while maintaining a comparably low dynamic MSE. These outcomes indicate that the conditional diffusion model of GTA expands data coverage under dynamic constraints and discovers novel states and actions with higher rewards, effectively integrating environmental knowledge. We add further analysis in Appendix F.4.

### Guideline for Selecting \(\) and \(\)

In this section, we propose a guideline for selecting hyperparameters based on dataset quality and environment characteristics. First, when the known information about the dataset is limited and minimizing exploration risk is essential, a low exploration and exploitation approach can enhance performance without losing stability. As illustrated in Table 4, GTA with low exploration and exploitation (\(=0.25,=1.1\)) surpasses baseline methods in gym locomotion tasks.

However, if we know that the offline dataset is suboptimal and there exists considerable room for improvement, a higher exploration and exploitation setting can yield superior performance by uncovering a broader range of high-quality behaviors. Table 5 shows that GTA with (\(=0.5,=1.3\)) and (\(=0.75,=1.3\)) significantly boosts performance on suboptimal datasets. These findings indicate that while conservative parameters with low \(\) and \(\) offer stable gains, we can elevate values of \(\) and \(\) by leveraging prior knowledge about dataset quality to achieve superior outcomes. Notably, GTA consistently outperforms baselines across all configurations without dataset-specific hyperparameter tuning.

   &  &  &  \\  &  &  &  &  &  &  \\  &  &  &  &  &  &  \\  &  &  &  &  &  &  \\
**New** & 60.24 \(\) 0.04 & 44.44 \(\) 0.71 & 89.28 \(\) 1.50 & 41.04 \(\) 3.18 & 69.99 \(\) 2.41 & 10.46 \(\) 5.51 & 84.59 \(\) 1.92 & 11.51 \(\) 4.12 & 116.03 \(\) 0.37 & 78.91 \(\) 2.66 \\
**Synthetic** & 49.12 \(\) 0.02 & 44.54 \(\) 0.71 & 84.49 \(\) 1.56 & 64.10 \(\) 3.18 & 69.99 \(\) 2.41 & 10.46 \(\) 5.51 & 84.59 \(\) 1.92 & 84.52 \(\) 1.94 & 94.45 \(\) 1.92 & 10.93 \(\) 0.37 & 79.64 \(\) 2.98 \\
**GTA**(\(=0.5,=1.3\)) & **57.92 \(\) 0.48** & **56.46 \(\) 1.32** & **58.38** & **57.20** & **58.18** & **58.20** & **57.17** \(\) 2.21** & **57.12** \(\) 2.17** & **59.12** \(\) 6.45 & **72.17** \(\) 11.23 \\
**GTA**(\(=0.75,=1.3\)) & 57.85 \(\) 0.27 & 61.58 \(\) 5.00 & 87.14 \(\) 1.73 & **48.99** \(\) **2.16** & **57.26** \(\) **3.38** & 90.89 \(\) 3.29 & **79.05** \(\) **2.90** \\  

Table 4: D4RL normalized score on medium and medium-replay quality locomotion environments are conducted with TD3BC.

Figure 5: Data quality analysis of S4RL, SynthER, and GTA. GTA augmented data exhibits superior optimality and novelty across gym locomotion datasets while maintaining dynamic plausibility.

   &  &  &  \\  & **Halfcheetah** & **Hopper** & **Walker2d** & **Halfcheetah** & **Hopper** & **Walker2d** & **Average** \\ 
**None** & 48.42 \(\) 0.62 & 61.04 \(\) 3.18 & 84.58 \(\) 1.92 & 44.64 \(\) 0.71 & 65.69 \(\) 24.41 & 84.11 \(\) 4.12 & 64.75 \(\) 4.15 \\
**SynthER** & 49.16 \(\) 0.39 & 63.70 \(\) 3.69 & 85.43 \(\) 1.14 & 45.57 \(\) 0.34 & 78.81 \(\) 15.80 & 90.67 \(\) 1.56 & 68.89 \(\) 1.62 \\
**GTA**(\(=0.5,=1.3\)) & **57.92 \(\) 0.48** & **64.66 \(\) 1.32** & **88.88** \(\) **2.70** & **48.23** \(\) 5.42 & **77.17** \(\) 22.17 & **59.12** \(\) 6.45 & **72.17** \(\) 11.23 \\
**GTA**(\(=0.75,=1.3\)) & 57.85 \(\) 0.27 & 61.58 \(\) 5.00 & 87.14 \(\) 1.73 & **48.99** \(\) **2.16** & **97.26** \(\) **3.38** & 90.89 \(\) 3.29 & **79.05** \(\) **2.90** \\  

Table 5: D4RL normalized score on medium and medium-replay quality locomotion environments with fixed \(\) and \(\). The experiments are conducted with TD3BC.

### Futher Experimental Results

We conduct extensive additional experiments to explore the further potential of the GTA.

**GTA under realistic settings**. We examined the augmentation ability of GTA for two realistic settings: one with a mixed dataset consisting of few expert datasets and the majority of low-performing trajectories in Appendix E.2, and the other with a small amount of offline dataset in the Appendix E.3. In both cases, We observed that GTA significantly enhances the performance, improving sample efficiency of the offline RL.

**Larger batch size and training epochs**. Since GTA offers more training data, it is worth exploring whether increasing batch size or training epochs can lead to additional performance gain. The results in Appendix G.1 demonstrate that we can enhance the performance by increasing batch size and training epochs while baselines do not benefit from the batch size and training epochs increment.

**Extending to sequence modeling approach**. We proved that GTA can be applied to sequence modeling approaches such as Decision Transformer  as GTA augments data at the trajectory level. We confirmed that GTA improves the performance of the sequence modeling approach, especially in sparse reward tasks. The detailed experiment setup and results are in Appendix E.4.

**Sensitivity tests**. We conduct experiments on the sensitivity of parameters \(\) in Appendix G.2 and \(\) in Appendix G.3. The sensitivity of GTA with the amount of augmented dataset is in Appendix G.4. Our results indicate that GTA maintains similar performance beyond one million augmented transitions, demonstrating robustness to the number of transition samples.

**Additional ablations on design choices**. We explore the impact of reweighted sampling introduced on Section 4.4 in Appendix E.5. For exploring alternative design choices, we adopt conditioning-by-inpainting as an alternative conditioning approach instead of classifier-free guidance in Appendix E.6. We also test the model-based approach by replacing the reward of the generated trajectory with the prediction of the reward proxy model in Appendix E.7.

## 6 Conclusion

We propose **GTA**: Generative Trajectory Augmentation, a novel generative data augmentation approach for enriching offline data with high-rewarding and dynamically plausible trajectories. GTA combines data augmentation with the diffusion sampling process by partially adding noise to original trajectories and subsequently denoising them under amplified return guidance. Our extensive experiments on 31 datasets across 9 environments exhibit considerable performance improvements in four distinct offline RL algorithms, demonstrating the versatility and effectiveness of the GTA. We show that GTA successfully creates high-quality datasets from the sub-optimal offline dataset, which leads to learning effective decision-making policy.

**Limitations and future works**. In the GTA framework, we do not train additional transition model and reward model for simple augmentation framework. Therefore, in tasks where dynamic violations have a critical impact, we may not expect a significant performance boost. However, as shown in our experiments, dynamic violations are generally minimal. Additionally, while the main focus of GTA is on the offline setting, future work could explore off-to-online and online settings.