# Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization

Taeyoung Yun\({}^{1}\) Sujin Yun\({}^{1}\) Jaewoo Lee\({}^{1}\) Jinkyoo Park\({}^{1,2}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology (KAIST) \({}^{2}\)Omelet.ai {99tyty, yunsj0625, jaewoo, jinkyoo.park}@kaist.ac.kr

###### Abstract

Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields. Unfortunately, the online evaluation of these functions is restricted due to time and safety constraints in most cases. In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset. While prior methods consider forward or inverse approaches to address the problem, these approaches are limited by conservatism and the difficulty of learning highly multi-modal mappings. Recently, there has been an emerging paradigm of learning to improve solutions with synthetic trajectories constructed from the offline dataset. In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions. First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions. Then, we train a conditional diffusion model to generate trajectories conditioned on their scores. Lastly, we sample multiple trajectories from the trained model with guidance to explore high-scoring regions beyond the dataset and select high-fidelity designs among generated trajectories with the proxy function. Extensive experiment results demonstrate that our method outperforms competitive baselines on Design-Bench and its practical variants. The code is publicly available in https://github.com/dbsxodud-11/GTG.

## 1 Introduction

Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields, including biological sequence design , materials discovery , and mechanical design [3; 4]. Traditional methods like Bayesian optimization have been developed to solve the problem by iteratively querying a black-box function. However, the online evaluation of the black-box function is restricted in most real-world situations due to time and safety constraints.

Fortunately, we often have access to a previously collected offline dataset. This problem setting is referred to as offline model-based optimization (MBO), and our objective is to find a design that maximizes a target function using solely an offline dataset . As no online evaluation is available, a key challenge of MBO is the out-of-distribution (OOD) issue arising from limited data coverage. Suppose we train a proxy that predicts function values given input designs and naively apply a gradient-based optimizer based on the proxy to identify the optimal design. It would fall into sub-optimal results due to inaccurate predictions of the proxy in unseen regions.

To mitigate this issue, forward approaches mostly consider training a robust surrogate model against adversarial optimization of inputs and applying gradient-based maximization. Trabucco et al.  train a proxy with the regularization term to prevent overestimation on OOD designs. Fu and Levine  leverage normalized maximum likelihood estimator to handle uncertainty on unseen regions.

There are also several works that focus on fine-tuning the proxy for robustness on unexplored regions [8; 9; 10]. However, the generalization of the proxy outside of the dataset still remains challenging.

On the other hand, inverse approaches learn a mapping from function values to the input domain. Then, they generate high-scoring designs by querying the learned mapping with a high score. Prior approaches utilize expressive generative models to learn a mapping, such as variational autoencoders [11; 12], generative adversarial nets , autoregressive models  or diffusion models . While these methods show promising results, they still suffer from the difficulty of learning highly unsmooth distributions and utilizing valuable information about the landscape of the black-box function.

Recently, a new perspective has emerged on tackling the MBO by learning to improve solutions with synthetic trajectories constructed from the dataset [16; 17]. These methods aim to generate a sequence of designs toward high-scoring regions. It seems more promising than learning an inverse mapping that generates only a single design, as we can utilize information from sequences of designs that can help better understand the landscape of the target function. However, there is still room for improvement in this perspective. First, prior approaches construct trajectories with simple heuristics, which may lead to generating trajectories with inconsistent directions of improvement. Furthermore, the sequential nature of autoregressive models may lead to error accumulation during sampling .

To this end, we propose a novel conditional generative modeling approach to solve the MBO problem. Unlike prior inverse approaches, which generate a single design, we generate a sequence of designs toward high-scoring regions with guided sampling. Our method consists of four stages. First, we construct trajectories from the dataset while incorporating locality bias to distill the knowledge of the landscape of the target function into the generator. Then, we train a conditional diffusion model that generates the whole trajectory at once to bypass error accumulation and an auxiliary proxy. After training, we sample multiple trajectories conditioned on context data points and high score values. Finally, we select high-fidelity designs among generated trajectories by filtering with the proxy.

We empirically demonstrate that our method achieves superior performance on Design-Bench, a well-known benchmark for MBO with a variety of real-world tasks. Furthermore, we explore more practical settings, such as sparse or noisy datasets, verifying the generalizability of our method.

## 2 Preliminaries

### Problem setup

In offline model-based optimization (MBO), we aim to find a design \(\) that maximizes the target black-box function \(f\). Unlike the typical black-box optimization setting, we can only access an offline dataset \(\), and online evaluations are unavailable. The problem setup can be described as follows:

\[^{*}=_{^{d}}f() =\{(_{i},y_{i})\}_{i=1}^{N}\] (1)

where \(\) is a decision variable and \(y=f()\) is a target property we want to maximize.

### Diffusion probabilistic models

Diffusion probabilistic models [19; 20] are a class of generative models that approximate the true distribution \(q_{0}\) with a parametrized model of the form: \(p_{}(x_{0})= p_{}(x_{0:T})dx_{1:T}\), where \(x_{0} q_{0}\) and \(x_{1},,x_{T}\) are latents with the same dimensionality. The joint distribution \(p_{}(x_{0:T})\) is called the reverse process, defined as a Markov chain starting from standard Gaussian \(p_{T}(x_{T})=(0,I)\):

\[p_{}(x_{0:T})=p_{T}(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t}), p _{}(x_{t-1}|x_{t})=(_{}(x_{t},t),_{t})\] (2)

where \(p_{}(x_{t-1}|x_{t})\) is parametrized Gaussian transition from timestep \(t\) to \(t-1\).

We define a forward process, which is also fixed as a Markov chain that adds Gaussian noise to the data with the variance schedule \(_{1},,_{T}\):

\[q(x_{1:T}|x_{0})=_{t=1}^{T}q(x_{t}|x_{t-1}), q(x_{t}|x_{t-1})= (}x_{t-1},_{t}I)\] (3)Training diffusion models can be performed by maximizing the variational lower bound on the log-likelihood \(_{q_{0}}[ p_{}(x_{0})]\), which is equivalent to minimizing the following loss:

\[()=_{x_{0} q_{0},t U(1,T), (0,I)}[\|-_{}(x_{t},t)\|^{2}]\] (4)

where \(_{}(x_{t},t)\) is the parameterization suggested by , \(_{}(x_{t},t)=}}(x_{t}-} {}}_{}(x_{t},t))\).

For modeling conditional distribution \(q(x|y)\), we can use classifier-free guidance . In classifier-free guidance, we train both a conditional \(_{}(x_{t},y,t)\) and unconditional model \(_{}(x_{t},t)\) with the following loss:

\[()=_{x_{0},y q(x,y),t U(1,T), (0,I),(p)}[\|-_{}(x _{t},(1-)y+,t)\|^{2}]\] (5)

For sampling, we start from Gaussian noise \(x_{T}\) and refine \(x_{t}\) into \(x_{t-1}\) with the perturbed noise from the learned model \(_{}\) at each diffusion timestep \(t\):

\[(t)=_{}(x_{t},,t)+(_{ }(x_{t},y,t)-_{}(x_{t},,t))\] (6)

where \(\) is a scalar value that controls the guidance scale.

## 3 Methodology

In this section, we introduce **GTG**, **G**uided **T**rajectory **G**eneration, a conditional generative modeling approach for solving MBO problem by learning to improve solutions using the offline dataset. We first construct trajectories towards high-scoring regions while incorporating locality bias for consistent improvement directions. Then, we train the conditional diffusion model to generate trajectories and a proxy model. Finally, we sample multiple trajectories using the diffusion model with guided sampling and filter high-fidelity designs with the proxy. Figure 1 shows the overview of the proposed method.

### Constructing trajectories

We construct a set of trajectories \(_{}\) from the offline dataset \(\) to gather information on learning to improve designs. In this paper, each trajectory \(_{}\) is a set of \(H\) input-output pairs and can be represented as a two-dimensional array:

\[=[_{1}&_{2}&& _{H}\\ y_{1}&y_{2}&&y_{H}],(_{h},y_{h}) \;\; h=1,,H\] (7)

While prior works construct trajectories via sorting heuristics or sampling from high-scoring regions, we focus on constructing trajectories that give us more valuable information for learning to improve designs towards higher scores. To achieve this, we develop a novel method to construct trajectories based on two desiderata.

First, the trajectory should be towards high-scoring regions while containing information on the landscape of the target black-box function. Second, the trajectories should be diverse and not converge to a single data point with the highest score of the dataset, as our objective is to discover high-scoring designs beyond the offline dataset by generalizing the knowledge of learning to improve solutions.

Figure 1: Overview of our method. **Step 1:** Construct trajectories from the dataset. **Step 2:** Train diffusion model and proxy. **Step 3:** Sample trajectories from the diffusion model with classifier-free guidance and context conditioning. **Step 4:** Select candidates for evaluation by filtering with proxy.

To this end, we introduce a novel strategy to construct trajectories from the dataset. We illustrate the procedure in Algorithm 1. For each trajectory, we first sample an initial data point \((_{1},y_{1})\) from a relatively low score distribution, \(p\)th percentile of \(\). After initialization, we employ a local search strategy to select the next data point to generate a smooth trajectory toward high-scoring regions that contain the information on the landscape of the target function. Specifically, for each round \(h\), we find \(K\) nearest neighbors of \(_{h}\) whose score is higher than \(\{y_{1},,y_{h}\}-\), where \(\) is a small, non-negative real number. By allowing small perturbations using \(\), we can prevent generated trajectories from converging a single maximum of the offline dataset. Then, we sample \((_{h},y_{h})\) from the \(K\) neighbors randomly to generate diverse trajectories. We repeat the procedure until constructing a trajectory of length \(H\). By moving towards high-scoring regions while staying in a local region, we can effectively guide the generator to learn diverse and consistent paths for improving solutions.

Note that identifying \(K\) nearest neighbors of a data point whose values are above a certain threshold does not require substantial computational time compared to training and evaluation. We explain in more detail our trajectory construction procedure in Appendix B.1

```
0: Offline dataset \(\), Trajectory length \(H\), Number of trajectories \(N\), initial percentile \(p\), number of nearest neighbors \(K\), and perturbation coefficient \(\).
0:\(_{}\)
1: Initialize trajectory dataset \(_{}\)
2:for\(n=1,,N\)do
3: Sample \((_{1},y_{1})\) from \(p\)th percentile of \(\) and initialize trajectory \(\{(_{1},y_{1})\}\)
4:for\(h=1,,H-1\)do
5: Find \(K\) nearest neighbors of \(_{h}\) whose score is higher than \(\{y_{1},,y_{h}\}-\)
6: Sample \((_{h+1},y_{h+1})\) from the \(K\) neighbors and update \(\{(_{h+1},y_{h+1 })\}\)
7:endfor
8: Update \(_{}_{}\{ \}\)
9:endfor ```

**Algorithm 1** Trajectory construction procedure of GTG

### Training models

Given our trajectory dataset \(_{}\), our objective is to learn the conditional distribution of trajectories towards high-scoring regions. We choose diffusion models, which have a powerful capability to learn the distribution of complex and high-dimensional data [22; 23], to generate trajectories. Our objective is then transformed from searching high-scoring designs to maximizing the conditional likelihood of trajectories, which can be achieved by minimizing the loss in Equation (5):

\[^{*}=*{arg\,max}_{}_{ _{}}[ p_{}(|y( ))]\] (8)

where \(y()=_{h=1}^{H}y_{h}\) is the sum of scores in the trajectory \(\). By training a diffusion model to generate a sequence of designs instead of a single design, we can efficiently distill the knowledge of the complex landscape of the target function into the diffusion model.

In addition, we also train a forward proxy \(f_{}\) using the dataset \(\). We can use the proxy to filter high-scoring designs from the trajectories generated by the trained diffusion model.

### Sampling trajectories from the diffusion model

After training, we sample trajectories with guided sampling. We use _classifier-free guidance_ to generate trajectories. To be specific, we sample \(\) from the diffusion model using Equation (6), where \(y^{*}()\) is the target conditioning value. Following prior works [13; 16], we assume that we know the maximum score \(y^{*}\) and set \(y^{*}()=(Hy^{*})\), where \(\) controls the exploration level of the generated trajectories. We discuss the role of \(\) in more detail in the subsequent section.

To fully utilize the expressive power of diffusion models, we introduce an additional strategy, _context conditioning_, during the sampling. We generate trajectory with diffusion model while inpainting the \(C\) context data points of the trajectory with \(_{}\), which is a subtrajectory sampled from \(_{}\). By conditioning trajectories in different contexts, we can effectively explore diverse high-scoring regions.

Formally, for each denoising timestep \(t\), we refine \(^{(t)}\) into \(^{(t-1)}\) with the following procedure:

\[^{(t-1)}=_{}+(1-) }}(^{(t)}-}{_{t}}}}(t))\] (9)

where \(\) is the mask for the first \(C\) context data points and \(}(t)\) is computed from the Equation (6).

### Selecting candidates

After generating trajectories, we introduce _filtering_ to select candidates for evaluation. In other words, we select top-\(\) samples in terms of the predicted score from the proxy. By filtering with the proxy, we can exploit the knowledge from the dataset to search high-scoring designs [13; 14; 24].

## 4 Experimental evaluation

In this section, we present the results of our experiments on various tasks. First, we analyze our method in a toy 2D experiment. Then, we present the results on the Design-Bench and its practical variants to verify the effectiveness of the method. We also conduct extensive analyses on various aspects to deepen our understanding of the proposed method.

### Toy 2D experiment

We first evaluate our method using a toy setting to analyze each component of our method thoroughly. We choose Branin, a synthetic 2D function with three distinct global maxima. Figure 2 shows the contour plot of the Branin function. The analytical form of the Branin function is as follows:

\[f(x_{1},x_{2})=-a(x_{2}-bx_{1}^{2}+cx_{1}-r)^{2}-s(1-t) (x_{1})-s\] (10)

Figure 2: (a) Trajectories constructed by BONET (blue) and PGS (green). (b) Diverse trajectories constructed by GTG (red). (c) Trajectories generated by trained diffusion model with guided sampling. Red dots indicate context data points, and blue dots represent generated data points.

where \(a=1,\ b=},\ c=,\ s=10,\ t=\) and the range of \((x_{1},x_{2})\) is \([-5,10]\).

For the MBO setting, we uniformly sample 5000 data points and remove the top 10% percentile to make the task more challenging. We construct trajectories with a length of 64 using our trajectory construction strategy and other strategies suggested by prior methods, BONET  and PGS .

Figure 1(a) shows the trajectories generated from prior methods. As shown in the figure, we find that constructed trajectories show uncorrelated movements, which makes the model hard to capture knowledge on the landscape of the target black-box function. Unlike prior methods, our method constructs trajectories that improve the solution with the local movements, as illustrated in Figure 1(b). Such trajectories help the diffusion model to learn how to improve solutions efficiently. We also find that trajectories do not converge into a single data point and toward diverse high-scoring regions via random sampling from \(K\) neighbors and perturbations from \(\).

Figure 1(c) shows the trajectories generated by the trained diffusion model with context conditioning and classifier-free guidance. As shown in the figure, GTG can generalize the knowledge on improving solutions to find diverse high-scoring solutions. GTG achieves a maximum score of \(-0.490 0.070\), which is near-optimal compared to the global optimum (\(-0.398\)) and far beyond the maximum value of the dataset (\(-6.031\)). Please refer to Appendix A.1 for more details of the toy experiment.

### Design-Bench tasks

In this section, we present the experiment results of our method on Design-Bench tasks . We conduct experiments on two discrete tasks and three continuous tasks. For each task, we have an offline dataset from an unknown oracle function. We present the detailed task description below.

**TFBind8 and TFBind10 .** We aim to find a DNA sequence of lengths 8 and 10 with maximum binding affinity with a particular transcription factor.

**Superconductor .** We aim to design a chemical formula, represented by an 86-dimensional vector, for a superconducting material with a high critical temperature.

**Ant and D'Kitty Morphology [4; 25].** We aim to optimize the morphological structure of two simulated robots. The morphology parameters include size, orientation, and the location of the limbs. Ant has 60 continuous parameters, and D'Kitty has 56 continuous parameters.

### Baselines

For baselines, we prepare four main categories to solve MBO problems. First, we compare our method with traditional methods widely used in online black-box optimization settings, such as BO-qEI , CMA-ES , REINFORCE , and Gradient Ascent.

  
**Method** & TFBind8 & TFBind10 & Superconductor & Ant & D’Kitty & Mean Rank \\  \(\) (best) & 0.439 & 0.467 & 0.399 & 0.565 & 0.884 & - \\  BO-qEI & 0.794 \(\) 0.103 & 0.631 \(\) 0.041 & 0.486 \(\) 0.025 & 0.812 \(\) 0.000 & 0.896 \(\) 0.000 & 11.4 / 15 \\ CMA-ES & 0.919 \(\) 0.055 & 0.649 \(\) 0.020 & 0.478 \(\) 0.010 & **2.222 \(\) 1.550** & 0.724 \(\) 0.001 & 8.6 / 15 \\ REINFORCE & 0.947 \(\) 0.029 & 0.628 \(\) 0.025 & 0.485 \(\) 0.011 & 0.247 \(\) 0.031 & 0.558 \(\) 0.193 & 11.6 / 15 \\ Grad Ascent & **0.983 \(\) 0.015** & 0.648 \(\) 0.044 & 0.509 \(\) 0.018 & 0.295 \(\) 0.021 & 0.877 \(\) 0.023 & 7.8 / 15 \\  COMs & 0.968 \(\) 0.025 & 0.619 \(\) 0.038 & 0.444 \(\) 0.035 & 0.927 \(\) 0.043 & 0.957 \(\) 0.016 & 8.2 / 15 \\ NEMO & 0.941 \(\) 0.000 & **0.705 \(\) 0.000** & 0.502 \(\) 0.002 & 0.952 \(\) 0.002 & 0.950 \(\) 0.001 & **4.8 / 15** \\ RoMA & 0.924 \(\) 0.040 & 0.666 \(\) 0.035 & **0.510 \(\) 0.015** & 0.917 \(\) 0.030 & 0.927 \(\) 0.013 & 7 / 15 \\ BDI & 0.973 \(\) 0.000 & 0.630 \(\) 0.025 & 0.508 \(\) 0.011 & 0.932 \(\) 0.000 & 0.939 \(\) 0.000 & 6.6 / 15 \\ ICT & 0.944 \(\) 0.015 & 0.598 \(\) 0.020 & 0.507 \(\) 0.014 & 0.946 \(\) 0.021 & **0.960 \(\) 0.014** & 7 / 15 \\  CbAS & 0.895 \(\) 0.043 & 0.638 \(\) 0.040 & 0.468 \(\) 0.058 & 0.825 \(\) 0.030 & 0.898 \(\) 0.011 & 11.2 / 15 \\ MINs & 0.884 \(\) 0.039 & 0.660 \(\) 0.048 & 0.500 \(\) 0.036 & 0.908 \(\) 0.031 & 0.942 \(\) 0.005 & 8.8 / 15 \\ DDOM & 0.966 \(\) 0.015 & 0.666 \(\) 0.024 & 0.476 \(\) 0.029 & 0.926 \(\) 0.027 & 0.948 \(\) 0.011 & 7 / 15 \\  BONET & 0.831 \(\) 0.109 & 0.606 \(\) 0.044 & 0.405 \(\) 0.017 & 0.957 \(\) 0.004 & 0.950 \(\) 0.014 & 10 / 15 \\ PGS & 0.968 \(\) 0.019 & 0.693 \(\) 0.031 & 0.475 \(\) 0.048 & 0.748 \(\) 0.049 & 0.948 \(\) 0.014 & 7.6 / 15 \\ 
**GTG (Ours)** & **0.976 \(\) 0.020** & **0.698 \(\) 0.127** & **0.519 \(\) 0.045** & **0.963 \(\) 0.009** & **0.971 \(\) 0.009** & **1.6 / 15** \\   

Table 1: Experiments on Design-Bench Tasks. We report max score (\(100^{th}\) percentile) among \(Q\)=128 candidates. **Blue** denotes the best entry in the column, and **Violet** denotes the second best.

The second category comprises recently proposed forward approaches, including COMs , NEMO , RoMA , BDI , and ICT . The third category encompasses inverse approaches, and we select CbAS , MINs , and DDOM  as our baselines. Finally, we also compare with baselines which construct synthetic trajectories and generalize the knowledge of learning to improve solutions, BONET  and PGS .

### Evaluation metrics

For evaluation, we follow the protocol of prior works. We identify \(Q=128\) designs selected by the algorithm and report a normalized score of \(100^{th}\) percentile design. For all algorithms, we run experiments over 8 different seeds and report mean and standard errors.

To evaluate our method, we construct trajectories of length \(H=64\) and train a conditional diffusion model for each task. After training, we sample \(N=128\) trajectories conditioning on \(C=32\) context data points and setting \(=0.8\) across all tasks. Finally, we filter top-128 candidates among generated designs with the predicted score from the proxy for evaluation.

### Main results

As shown in the Table 1, GTG achieves an average rank of 1.6, the best among all competitive baselines. It performs best on two tasks and is runner-up on three tasks, demonstrating superior performance across different tasks. We observe that GTG generally surpasses forward approaches, which struggle to fall into OOD designs, especially in high-dimensional settings. We also observe that our method outperforms inverse approaches, including DDOM, which also utilizes a diffusion model. It demonstrates that generating trajectories towards high-scoring regions can be more effective than generating a single design, as we can distill the knowledge of the landscape of the target function into the generator. Our method achieves higher performance compared to BONET, which also generates trajectories. It indicates that our novel trajectory construction strategy effectively guides the diffusion model to explore diverse paths toward high-scoring regions.

### Practical variants of Design-Bench tasks

In this section, we present experiment results in a more practical setting of Design-Bench tasks. While Design-Bench assumes a large, unbiased offline dataset containing thousands of data points for the training model, such a setting is impractical in most cases. Therefore, we prepare two additional practical settings, sparse and noisy datasets, to verify the robustness of our method in such extreme cases. In a sparse setting, we only provide \(x\)% of the original dataset for training. For the noisy setting, we add \(x\)% of standard Gaussian noise to the normalized score values. We choose recent papers published after 2022, BDI, ICT, DDOM, and BONET for primary baselines. Please refer to Appendix A.2 for detailed experiment settings and Appendix D.5 for results with more baselines.

    &  &  \\ 
**Method** & 1\% & 20\% & 50\% & 1\% & 20\% & 50\% \\  BDI & \(0.898 0.000\) & \(0.952 0.000\) & \(\) & \(0.865 0.000\) & \(0.927 0.000\) & \(0.938 0.000\) \\ ICT & \(0.899 0.045\) & \(0.925 0.035\) & \(0.962 0.019\) & \(0.946 0.010\) & \(0.949 0.010\) & \(0.954 0.008\) \\ DDOM & \(0.851 0.082\) & \(0.906 0.050\) & \(0.896 0.048\) & \(0.938 0.007\) & \(0.945 0.011\) & \(0.944 0.008\) \\ BONET & \(0.791 0.079\) & \(0.824 0.061\) & \(0.884 0.072\) & \(0.875 0.004\) & \(0.939 0.007\) & \(0.940 0.009\) \\ 
**GTG (Ours)** & \(\) & \(\) & \(0.973 0.016\) & \(\) & \(\) & \(\) \\   

Table 2: Experiments on Sparse Datasets.

    &  &  \\ 
**Method** & 1\% & 20\% & 50\% & 1\% & 20\% & 50\% \\  BDI & \(\) & \(0.886 0.051\) & \(0.873 0.048\) & \(0.929 0.008\) & \(0.908 0.010\) & \(0.918 0.016\) \\ ICT & \(0.941 0.013\) & \(0.950 0.023\) & \(0.921 0.054\) & \(0.940 0.029\) & \(0.914 0.024\) & \(0.896 0.000\) \\ DDOM & \(0.896 0.048\) & \(0.887 0.065\) & \(0.887 0.065\) & \(0.944 0.009\) & \(0.945 0.011\) & \(0.926 0.020\) \\ BONET & \(0.904 0.044\) & \(0.822 0.113\) & \(0.773 0.143\) & \(0.942 0.008\) & \(0.927 0.024\) & \(0.924 0.010\) \\ 
**GTG (Ours)** & \(0.976 0.015\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Experiments on Noisy Datasets.

Table 2 shows the results of our method and recent baselines in sparse datasets. The table shows that our method mostly outperforms other baselines even in sparse datasets, demonstrating the superiority of exploiting knowledge of the target function by constructing diverse trajectories from the dataset. Table 3 reports the experiment results on the noisy settings. We find that even with 50% of noise, our method can find relatively high-scoring designs, demonstrating its robustness in practical settings.

## 5 Additional analysis

In this section, we carefully analyze the effectiveness of each component in our method.

**Ablation on trajectory construction.** We propose a novel trajectory construction strategy by incorporating locality bias. To verify the effectiveness of the strategy, we compare our strategy with prior approaches, SORT-SAMPLE and Top-\(p\) Percentile, suggested by BONET and PGS, respectively. Table 4 shows that our strategy outperforms prior strategies across various tasks. We conduct additional analysis on trajectory construction strategies in Appendix D.1.

**Ablation on sampling procedure.** We analyze the effectiveness of strategies we introduced during the sampling procedure, namely context conditioning (CC), classified-free guidance (CF), and filtering (F). Across various tasks, it is evident that all components are crucial for improving performance as demonstrated in Table 5. We conduct further analysis on sampling strategies in Appendix D.2.

**Hyperparameter sensitivity.** We also conduct experiments on the effect of various hyperparameters we introduced in this paper. We first train a conditional diffusion model with various lengths (\(H\)). As shown in Figure 2(a), increasing \(H\) leads to achieving higher performance. We also conduct experiments by varying the number of contexts (\(C\)) and the exploration level (\(\)). Figure 2(b) shows that \(C=32\) achieves superior performance while conditioning with too many contexts degrades performance. Finally, Figure 2(b) shows a strong correlation between \(\) and the score, demonstrating the effectiveness of guided sampling. We conduct further analysis on hyperparameters in Appendix D.2.

**Varying evaluation budget.** We provide experiment results with a small number of evaluation budgets (\(Q\)). As shown in Figure 4, we generally outperform most baselines even with a relatively low evaluation budget.

**Assumption on optimal score.** We assume that the optimal value \(y^{*}\) is known, following prior works [13; 16]. We conduct experiments by relaxing the aforementioned assumption in Appendix D.2 and find that GTG can achieve comparable performance even without knowing \(y^{*}\).

**Effect of unsupervised pretraining.** It might be beneficial to pretrain the diffusion model when we have a large-scale unlabeled dataset and a few designs of labeled points . To this end, we discuss the effectiveness of pretraining diffusion models with unlabeled datasets in Appendix D.3.

**Time complexity of sampling procedure.** We also conduct analysis on the time complexity of the sampling procedure of our method in Appendix D.4. Experiment results demonstrate that we can decrease the number of denoising timesteps even one-tenth with minimal loss in performance.

  
**Method** & TFBind8 & TFBind10 & Superconductor & Ant & D’Kitty \\  SORT-SAMPLE & \(0.954 0.026\) & \(0.697 0.126\) & \(0.487 0.016\) & \(0.946 0.011\) & \(0.966 0.005\) \\ Top-\(p\) Percentile & \(0.948 0.030\) & \(0.669 0.033\) & \(0.439 0.039\) & \(0.946 0.018\) & \(0.964 0.003\) \\ Ours & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Ablation study on trajectory construction strategy.

  
**Method** & TFBind8 & TFBind10 & Superconductor & Ant & D’Kitty \\  \(\) & 0.923 \(\) 0.054 & 0.636 \(\) 0.047 & 0.499 \(\) 0.040 & 0.867 \(\) 0.051 & 0.926 \(\) 0.048 \\ \{CF} & 0.914 \(\) 0.053 & 0.687 \(\) 0.065 & 0.502 \(\) 0.040 & 0.918 \(\) 0.064 & 0.943 \(\) 0.011 \\ \{CF, CC} & 0.920 \(\) 0.036 & 0.687 \(\) 0.065 & 0.502 \(\) 0.024 & 0.927 \(\) 0.022 & 0.945 \(\) 0.014 \\ \{CF, F} & 0.963 \(\) 0.019 & 0.628 \(\) 0.036 & 0.483 \(\) 0.034 & 0.952 \(\) 0.026 & 0.965 \(\) 0.007 \\ \{CF, CC, F} & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Ablation study on sampling procedure of GTG.

## 6 Related works

### Offline model-based optimization

In offline MBO, generalization outside the offline dataset is crucial for success. While there have been attempts to train a robust surrogate model to achieve accurate predictions on unseen regions [8; 9; 10], effectively exploring high-scoring regions remains challenging.

Recently, a new perspective on solving the MBO problem has emerged by learning to improve solutions from synthetic trajectories and generalizing the knowledge to find designs beyond the dataset [16; 17]. BONET  trains an autoregressive model to generate optimal trajectories conditioned on a low regret budget. PGS  trains RL policy with trajectories consisting of high-scoring designs to roll out optimal trajectories. MATCH-OPT  also constructs monotonic trajectories and matches the gradient field with the proxy. GTG falls under this category but adopts a unique approach to constructing trajectories with local search and utilizing diffusion models to enhance performance.

### Generative models for decision making

Generative models have emerged as a powerful tool for decision-making problems, including bandit problems , reinforcement learning [18; 32; 33; 34; 35], and optimization [15; 36]. In offline MBO, there are inverse approaches to learning a mapping from function values to input domains with generative models and sample designs from high-scoring regions [11; 12; 14; 15]. DDOM  utilizes a conditional diffusion model and generates high-scoring samples with reweighted training and classifier-free guidance. DiffOPT  considers a constrained optimization setting and introduces a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction.

As concurrent works, DEMO  trains a diffusion model to match a pseudo-target distribution constructed by gradient ascent and uses the model to edit designs in the offline dataset. Diff-BBO  measures the uncertainty of generated designs to select the optimal target value for conditioning the diffusion model. Our method distinguishes itself from prior works by utilizing diffusion models to generate trajectories toward high-scoring regions by learning to improve solutions from the dataset.

Figure 4: Ablation on varying evaluation budget \(Q\).

Figure 3: Ablation on hyperparameters of GTG. Experiments are conducted on D’Kitty task.

Discussion and conclusion

In this paper, we introduce GTG, a novel conditional generative modeling approach for learning to improve solutions from synthetic trajectories constructed with the dataset. First, we construct diverse trajectories toward high-scoring regions while incorporating locality bias. Then, we train the conditional diffusion model and proxy function. After training, we generate trajectories with classifier-free guidance and context-conditioning to generalize the knowledge on how to improve solutions. Lastly, our filtering strategy for selecting candidates further improves the performance. Our extensive experiments demonstrate the generalizability of GTG.

**Limitation and future work.** While our method shows powerful generalizability on Design-Bench tasks, some evaluation methods may not fully capture real-world complexities. For example, in the superconductor task, we find that the offline dataset has multiple copies of the same inputs but with different outputs. As a result, the random forest oracle which is fit on this offline data is not reliable. Moreover, we resort to filtering designs with the proxy function, which may result in inaccurate predictions on OOD regions. Although our filtering strategy works well in sparse and noisy settings, one may consider constructing a robust proxy model to handle the uncertainty of its predictions.