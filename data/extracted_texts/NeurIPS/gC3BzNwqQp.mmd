# Online Learning of Delayed Choices

Recep Yusuf Beki

University of Waterloo

Waterloo, Canada

recep.bekci@uwaterloo.ca

###### Abstract

Choice models are essential for understanding decision-making processes in domains like online advertising, product recommendations, and assortment optimization. The Multinomial Logit (MNL) model is particularly versatile in selecting products or advertisements for display. However, challenges arise with unknown MNL parameters and delayed feedback, requiring sellers to learn customers' choice behavior and make dynamic decisions with biased knowledge due to delays. We address these challenges by developing an algorithm that handles delayed feedback, balancing exploration and exploitation using confidence bounds and optimism. We first consider a censored setting where a threshold for considering feedback is imposed by business requirements. Our algorithm demonstrates a \(()\) regret, with a matching lower bound up to a logarithmic term. Furthermore, we extend our analysis to environments with non-thresholded delays, achieving a \(()\) regret. To validate our approach, we conduct experiments that confirm the effectiveness of our algorithm.

## 1 Introduction

The ability to model and understand consumer choices between discrete alternatives is critical for various business applications, such as online advertising, product recommendations, and assortment optimization. Businesses need to present the most appealing set of options to consumers to maximize engagement and revenue. However, the task of optimizing what content or products are shown to a customer during their browsing session is complex due to the interplay between alternatives that customers face. Each alternative can act as a substitute or competitor to others, impacting the customer's final decision. Traditional multi-armed bandit (MAB) models, which are widely used for decision-making problems, fall short in scenarios where a subset of alternatives must be presented, and the customer's choice among these influences future decisions.

Multinomial choice (MNL) models have emerged as powerful tools for capturing and predicting consumer behavior among a finite set of alternatives. These models estimate the utilities of different options and the probabilities of their selection. However, when the MNL parameters are unknown and no historical data is available--as is often the case with newly introduced products or advertisements--the learning process becomes even more challenging. This complexity is further exacerbated when the feedback on decisions is delayed, requiring the learner to dynamically adjust decisions based on limited and potentially biased information.

One of the fundamental challenges in this setting is the delay in receiving feedback from customers. Unlike immediate responses in classical MAB problems, customers in e-commerce environments often take hours or even days to make decisions, as highlighted by Vernade et al. (2020) and Chapelle (2014). This delay in feedback complicates the learning process, as it must adapt to new information that arrives sporadically and potentially long after the initial interaction.

In this paper, we address the dual challenges of unknown MNL parameters and delayed feedback by developing algorithms that balances exploration and exploitation through the use of confidence bounds and the principle of optimism in the face of uncertainty. We focus on two settings in receiving delay: the thresholded and the non-thresholded settings.

In thresholded settings, feedback is only considered if it is received within a predetermined time frame set by business requirements. This constraint ensures operational stability and efficiency by ignoring excessively delayed responses that may no longer be relevant. For these settings, we introduce the Delayed Multinomial Logit Bandit (DEMBA) algorithm, specifically designed to handle potentially censored delayed feedback effectively.

In contrast, non-thresholded settings allow the learner to consider all feedback regardless of delay, potentially leading to better accuracy in decision making in long-term but at the cost of increased bias in the learning process. For such environments, we propose the Patient Delayed Multinomial Logit Bandit (PA-DEMBA) algorithm, which adapts its learning strategy to accommodate all feedback, irrespective of the delay.

**Contributions.** Our main contributions are two novel bandit algorithms: we develop DEMBA, for thresholded feedback settings, and PA-DEMBA, an algorithm designed for non-thresholded feedback environments. Both algorithms effectively learn from delayed and censored choices using confidence intervals. We provide a comprehensive regret analysis for both DEMBA and PA-DEMBA, demonstrating an \(()\) regret bound and a matching lower bound up to a logarithmic term. Additionally, through detailed computational experiments, we validate the performance and robustness of our algorithms in various scenarios.

**Organization.** The remainder of this paper is organized as follows. In Section 2, we review the related work on choice modeling and delayed feedback in online learning. Section 3 details the problem formulation and the specific challenges addressed by our approach. Our main algorithm DEMBA is presented in Section 4. We analyze the regret bounds of this algorithm in Section 5, providing theoretical guarantees for its performance. PA-DEMBA algorithm for non-thresholded delays is presented in Section 6. In Section 7, we conduct experiments to demonstrate the effectiveness of our proposed algorithms. Finally, Section 8 concludes the paper and outlines potential directions for future research.

## 2 Related Work

Delayed feedback is a crucial aspect of online learning environments, especially in domains like online advertising and e-commerce, where decision making involves a consideration period, or in healthcare, where the effects of actions take time to manifest. Consequently, research interest in bandits with delayed feedback has surged in recent years.

In their seminal work, Joulani et al. (2013) studied online learning scenarios with stochastic delayed feedback. Their work laid the foundation for understanding how delays impact learning performance. Similarly, Chapelle (2014) examined delayed conversions in display advertising, highlighting the practical challenges faced in real-world applications. Further, Vernade et al. (2017) focused on delays specifically in the context of delayed conversions, providing insights into handling delays with known distributions.

Expanding on this, Pike-Burke et al. (2018) explored bandits with delayed, aggregated, and anonymous feedback, which adds another layer of complexity by considering multiple types of delays. Zhou et al. (2019) extended this exploration to generalized linear contextual bandits with delayed feedback. Vernade et al. (2020) investigated linear bandits with stochastic thresholded delays. Additionally, Gael et al. (2020) tackled multi-armed bandits with arm-dependent stochastic delays, focusing on the challenges of non-uniform delays across different choices. Moreover, Cesa-Bianchi et al. (2022) considered composite and anonymous delayed feedback within non-stochastic bandits, further enriching the literature on delayed feedback mechanisms. Tang et al. (2024) studied delayed multi-armed bandits (MAB) with reward-dependent delays in clinical trials, while Lancewicki et al. (2021) explored both reward-dependent and reward-independent delay settings. Flaspohler et al. (2021) investigated delayed learning in weather forecasting, and Grover et al. (2018) addressed the best arm identification problem with delayed feedback. Thune et al. (2019) examined non-stochastic MABs with unrestricted delays, and Cesa-Bianchi et al. (2016) considered cooperation between different agents in delayed settings. Tang et al. (2021) explored scenarios where past actions impact future arm rewards, and Yang et al. (2024) addressed general sequential decision-making problems with delayed feedback. Despite this rich body of work, the solutions developed for MABs do not directly apply to our setting, where assortment feedback in discrete choice models presents additional complexities. In particular, delayed feedback affects both item value estimation and assortment composition, making our problem significantly more challenging than those where only arm rewards are updated.

When focusing specifically on generalized linear bandits with delays, we note key contributions such as Howson et al. (2023) and Blanchet et al. (2024) who explored generalized linear bandits with delayed feedback, demonstrating the efficacy of these models in more complex, non-linear settings. Multinomial bandits, which address decision making where multiple items are offered simultaneously, present a unique challenge due to the interactions between items. This complexity distinguishes our problem from other online learning models. Specifically, for generalized linear bandits, we note that when the assortment has more than one item, our problem cannot be addressed by solutions designed for generalized linear models due to the complexity of interactions among multiple choices which makes the action space more complicated.

In terms of online learning with choice models, significant progress has been made in understanding and optimizing MNL parameters. Agrawal et al. (2017) developed a Thompson sampling algorithm for learning MNL parameters for assortment optimization, while Agrawal et al. (2019) proposed a UCB algorithm for the same purpose. Dong et al. (2020) adapted this problem by introducing switch costs, addressing practical constraints in dynamic environments. Further, Agarwal et al. (2020) studied the problem for best arm identification, extending the framework to multiple pulls, which is a generalization of the dueling bandits problem. Other notable works include Wang et al. (2018) and Chen et al. (2021), who worked on dynamic assortment allocation under uncapacitated MNL models, and Perivier and Goyal (2022), who investigated joint pricing and assortment optimization with MNL demand processes.

To the best of our knowledge, our work is the first to address online learning of delayed choices in this context. Specifically, we propose the Delayed Multinomial Logit Bandit (DEMBA) algorithm for thresholded feedback settings and the Patient Delayed Multinomial Logit Bandit (PA-DEMBA) algorithm for non-thresholded settings. These algorithms effectively learn from delayed and censored choices using confidence intervals, providing robust solutions for dynamic and uncertain environments where feedback is not immediately available. Our approach not only advances the theoretical understanding of delayed feedback in online learning but also offers practical insights for applications in e-commerce and online advertising.

## 3 Problem Setup

We consider a capacitated selection problem faced by a _seller_ over \(T\) rounds. The items to be selected, referred to as _products_, can be new retail products or services such as advertisements. There are \(N\) products available to potentially be shown to the _customer_. The set of products selected in a given round is called an _assortment_, denoted as \(S_{t}\) for round \(t\), where \(S_{t}\{1,,N\}\) and \(|S_{t}| K\). Here, \(K\) represents the capacity, indicating the maximum number of items the seller can show at any given time.

Upon encountering the assortment, the customer makes a choice \(a_{t}\) among the options: (i) rejecting the browsing options provided (\(a_{t}=0\)), (ii) browsing and purchasing/selecting an option (\(a_{t}=i,i S_{t}\)), and (iii) browsing but not purchasing/selecting an option (\(a_{t}=0d\)). If option \(i\) is chosen, the seller earns a reward \(r_{i}\), with \(r_{i}\) for \(i[N]\) and \(r_{0}=r_{0d}=0\).

Customer choice probabilities are determined by the Multinomial Logit (MNL) model as follows:

\[(a_{t}=i|S_{t}=S)=}{v_{0}+v_{0d}+_{j S }v_{j}}&if\ i S\{0,0d\}\\ 0&otherwise,\]

where \(v_{i}\) denotes the (unknown) attraction parameter of option \(i\). Without loss of generality we assume that attraction parameters are normalized such that \(v_{0}=1\). In parallel with the real applications, we also assume that not browsing is the most common choice, i.e. \(v_{i} v_{0}\).

It is important to note that not purchasing/selecting is different from tracking conversions. Specifically, if the customer browses the given assortment, we can track if the customer decides not to purchase, similar to choosing an option to purchase (e.g., by closing the pop-up or following specific behavioral click-through patterns).

If the customer's choice is \(a_{t}=0\), the seller receives this choice immediately. Otherwise, the customer's choice is revealed to the seller after a delay \(d_{t}\). \(d_{t}\) is sampled from a unknown distribution \(f_{d}\) and independent from \(a_{t}\). Moreover, delays longer than a certain threshold \(\) is censored or ignored by the seller in the learning process. This threshold is set based on the seller's operational requirements. We later extend our solution to the _patient learner_ setting without a threshold, i.e. \(\).

We define \(a_{i,t}\) as the demand for option \(i\) at time \(t\). We have

\[a_{i,t}=1,&if\ a_{t}=i,\\ 0,&otherwise.\]

We also define \(c_{i,s,t}\{0,1\}\) as the censoring variable of product \(i\) at period \(t\) that is sold at period \(s\). The censoring variable is determined as:

\[c_{i,s,t}=(d_{s} t-sd_{s}).\]

We define the feedback observed by the seller as \(o_{i,s,t}\{0,1\}\) and \(o_{i,s,t}=c_{i,s,t}a_{i,s}\). The expected fraction of observed feedback is denoted as \(_{}:=_{s=0}^{}f_{d}(s)\).

The sequence of events at round \(t\) can be summarized as follows:

1. The seller selects an assortment \(S_{t}\).
2. The customer interacts with the medium(view the page or encounter with the pop-up) and makes a decision \(a_{t}\).
3. The environment returns a reward \(r_{i}\), \(i[N]\), and samples a delay \(d_{t}\).
4. Rewards of certain previous actions and/or if the customer rejected to browse revealed to the seller as \(o_{i,s,t}\).

The expected reward of the seller given assortment \(S\) and attraction parameter set \(v\) is given by

\[R(S;v)=_{i S}r_{i}}{1+v_{0d}+_{j S}v_{j}}.\]

The goal of the seller is to sequentially learn customer preferences and find a policy to minimize cumulative expected regret, defined as:

\[(T,)=_{t=1}^{T}R(S^{*};_{}v)-R(S_{t}^{};_{}v),\]

where \(S^{*}=_{S\{1,,N\},|S| K}R(S;_{}v)\) maximizes the expected reward of the clairvoyant and \(v=\{v_{0d},v_{1},,v_{N}\}\) is the ground truth attraction parameter set.

## 4 Delayed MNL Bandit (DEMBA) Algorithm

In this section, we introduce the Delayed MNL Bandit (DEMBA) algorithm. DEMBA leverages an epoch-based learning method, where epochs are explicitly defined by immediate no-purchase decisions. Specifically, when a no-purchase decision is made by the customer, the current epoch is closed, and a new one begins. Throughout each epoch, customer selections are observed, and parameter updates occur upon encountering a no-purchase outcome.

Our approach adopts the principle of optimism in the face of uncertainty (Auer et al., 2002) for parameter estimation, generating optimistic estimations using upper confidence bounds for product attraction parameters and a lower confidence bound for the no-purchase option. This results in an optimistic revenue function, guiding decision-making under uncertainty by balancing exploration and exploitation.

We build our optimistic estimates on biased observations. The total observed preference given to product \(i\) until epoch \(\) is denoted as \(_{i,}\) and can be calculated as

\[_{i,}=_{s=1}^{t_{}^{end}}o_{i,s,t_{}^{end}},\]

where \(\) is the current epoch and \(t_{}^{end}\) is the last period of epoch \(\).

We count how many times a particular product \(i\) is offered in the assortment until epoch \(\) using the set \(E_{}(i)\) which is the set of epochs during which \(i\) is offered, i.e. \(E_{}(i)=\{e:i S_{e}\}\). Then, we estimate attraction parameters by

\[_{i,}=_{i,}}{|E_{}(i)|}.\]

\(_{i,}\) is a biased estimator due to delay and thresholding, i.e. \([_{i,}] v_{i}\). We consider this bias in our estimation and build our concentration around \(_{}v_{i}\). We state our concentration result in the following lemma.

**Lemma 4.1**: _With probability at least \(1-O(N^{-2}T^{-1})\), for every epoch \(\{1,\}\) and option \(i\{0d,1,,N\}\), we have_

\[|_{i,}-_{}v_{i}|_{i,},\]

_where \(_{i,}=_{i,}+1)(NT)}{|E_{}(i)|}}+ (i)|}\)._

_Sketch of the proof._ Note that \(_{i,}\) is a biased estimator and defined as

\[_{i,}=_{i,}}{|E_{}(i)|}=^{t _{}^{end}}o_{i,s,t_{}^{end}}}{|E_{}(i)|},\]

where \(|E_{}(i)|\) is the total number of epochs during which product \(i\) is shown to the customer and \(_{s=1}^{t_{}^{end}}o_{i,s,t_{}^{end}}\) is the total observed sales of product \(i 0d\) or is the total observed delayed no selections.

We analyze the concentration around \(_{}v_{i}\):

\[|_{i,}-_{}v_{i}| =|^{t_{}^{end}}o_{i,s,t_{}^{end}}}{ |E_{}(i)|}-_{}v_{i}|\] \[=|^{t_{}^{end}-}a_{i,s}(d_ {s})+_{s=t_{}^{end}-+1}^{t_{}^{end}}a_{i,s}( d_{s} t_{}^{end}-s)}{|E_{}(i)|}-_{}v_{i}|\] \[=|^{t_{}^{end}}a_{i,s}(d_{s} )+_{s=t_{}^{end}-+1}^{t_{}^{end}}a_{i,s}(d_{s} t_{}^{end}-s)-(d_{s})}{|E_{}(i)|}- _{}v_{i}|\] \[|^{t_{}^{end}}a_{i,s}(d_{s} )}{|E_{}(i)|}-_{}v_{i}|+| ^{end}-+1}^{t_{}^{end}}a_{i,s}1-(d_{s}) }{|E_{}(i)|},\] (1)

where the second equality follows from decomposing the observations into those before and after the threshold, the third equality rearranges the terms, and the last inequality uses the triangle inequality.

For the first term in the decomposition (1), we have

\[|^{t_{s=1}^{end}}a_{i,s}(d_{s})} {|E_{}(i)|}-_{}v_{i}| =|^{t_{s=1}^{end}}a_{i,s}(d_{s} )}{|E_{}(i)|}-_{s=1}^{t_{s=1}^{end}}a_{i,s}}{|E_{ }(i)|}+_{s=1}^{t_{s=1}^{end}}a_{i,s}}{|E_{}(i)|}-_{ }v_{i}|\] \[|^{t_{s=1}^{end}}a_{i,s}(d_{s} )}{|E_{}(i)|}-_{s=1}^{t_{s=1}^{end}}a_{i,s}}{|E_ {}(i)|}|+|_{s=1}^{t_{s=1}^{end}}a_{i,s}}{|E_ {}(i)|}-_{}v_{i}\] \[^{t_{s=1}^{end}}(d_ {s})}{|E_{}(i)|}-_{}|}+^{t_{s=1}^{end}}a_{i,s}}{|E_{}(i)|}-v_{i}|}.\] (2)

We bound the first term (a) using Hoeffding's inequality:

\[(a)(i)|}}.\] (3)

For part \((b)\), we make use of the Chernoff bound from Theorem A.1 and handle two cases based on \(=(v_{i}+1)|E_{}(i)|}}\). The detailed proof is provided in Appendix A. The result can be summarized as follows:

\[(|^{t_{s=1}^{end}}a_{i,s}}{|E_{}(i)|}- v_{i}|_{i,}(NT)}{|E_{}(i)|}}+ (i)|}) 1-T}.\] (4)

Combining the bounds for both terms, we establish the concentration result stated in Lemma 4.1. \(\)

Using the concentration result for the attraction parameters, we construct upper confidence bounds for each product at each epoch:

\[_{i,}=_{i,}+_{i,},\]

and for the delayed no-purchase option, we construct a lower confidence bound:

\[_{0d,}=_{0d,}-_{0d,},\]

where

\[_{i,}=_{i,}+1)(NT)}{|E_{}(i)|}}+ (i)|}.\]

We use the optimistic parameter estimations to construct an optimistic revenue function

\[R(S;)=_{i S}r_{i}_{i,}}{1+_{od, }+_{j S}_{j,}}.\]

Our algorithm DEMBA suggests the assortment according to the optimistic revenue function \(R(S;)\) and updates parameters according to feedback received with delay. The pseudocode of DEMBA is given in Algorithm 1.

**Computational complexity.** The computational complexity of the DEMBA algorithm involves several key components. The most intensive step is computing the assortment \(S_{}\) by maximizing the revenue function \(R(S;)\). For this step, polynomial-time solutions with \(O(N^{2})\) complexity are available, as demonstrated by Rusmevichientong et al. (2010) and Davis et al. (2013). Updating observed preferences \(_{i,}\) involves summing over previous observations, with a complexity of \(O(N(t_{}^{start}-t_{}^{end}))\). The process of updating sets \(E_{}(i)\) and estimations \(_{i,}\) and the confidence bounds adds \(O(N)\) operations per epoch. Given that \( t\) and \(t T\), the overall computational complexity across all rounds \(T\) is \(O(TN^{2}+NT^{2})\).

## 5 Regret Analysis

Our main result is given in the following theorem.

**Theorem 5.1**: _Let \(^{DEMBA}\) be the policy produced by Algorithm 1 using \(_{i,}=_{i,}+1)(NT)}{|E_{}(i)|}}+ (i)|}\). Then, \(^{DEMBA}\) satisfies_

\[(T,^{DEMBA}) (1+)(T)+K+48K(T)((NT)+)\] \[+73+48^{2}(NT).\]

_The bound can be further simplified to \(()\) by omitting logarithmic terms._

Sketch of the proof.: The proof of Theorem 5.1 consists of several steps. First, we use the definition of the optimistic revenue function \(R(S;)\) and show that it provides an upper bound on the true revenue function \(R(S;_{}v)\). This is achieved by leveraging Lemma B.1, which ensures that the estimated attraction parameters are close to their true values with high probability.

We start by expressing the regret in terms of the epochs:

\[(T,)=[_{=1}^{}|_{ }|(R(S^{*};_{}v)-R(S_{};_{}v))],\]

where \(_{}\) is the duration of epoch \(\). Given that the epoch duration follows a geometric distribution, we simplify the expected regret using the law of total expectations.

Next, we decompose the regret into two parts: one that occurs with high probability (event \(_{}^{C}\)) and one that occurs with low probability (event \(_{}\)):

\[[ R_{}]=[ R_{}( _{-1})+ R_{}(_{-1}^{C}) ].\]

We bound the contribution of the low-probability event by \((N+1)(-1)\), which is small due to our concentration results.

For the high-probability event, we show that the difference between the optimistic and true revenues is bounded by \(_{i,}\). Applying Lemma B.1, we can then bound the regret for each epoch:

\[[ R_{}](N+1)(-1)+ [(1+_{}v_{0d}+_{j S_{}}_{}v_{j})(R(S _{};_{})-R(S_{};_{}v))\,(_{ -1}^{C})].\]

Summing over all epochs and using the properties of the epoch duration, we show that the total regret is bounded by \(()\). \(\)The full detailed proof is provided in Appendix B.

Next, we provide a lower bound result for the regret in the following theorem:

**Theorem 5.2**: _For any policy \(\), suppose \(K N/4\), \(T 1\), and \(_{}(0,1)\). There exists a universal constant \(c>0\) such that_

\[(T,) c\{T,}}\}.\]

The proof of Theorem 5.2 is deferred to Appendix C. This theorem establishes a lower bound on the regret, showing that no policy can achieve a better regret rate than \(()\).

**Remark 5.3**: _The effect of the threshold \(\) in the upper bound appears only in logarithmic terms, suggesting that the regret increases with larger \(\). In contrast, in the lower bound, \(_{}\) appears in the square root and the denominator, indicating that the regret decreases with larger \(\). It is important to note that \(\) is determined by business conditions and is typically fixed. We conjecture that the upper bound is not tight concerning \(\), indicating potential areas for future improvement in the analysis._

## 6 Non-Thresholded Setting: The Patient Learner

In this section, we modify our algorithm for environments that do not apply a threshold for delays. We refer to the seller in this setting as the _patient_ learner. The patient learner is assumed to have knowledge of the expected delay. This assumption is consistent with existing literature (see e.g. Joulani et al. (2013); Blanchet et al. (2024)).

We build our concentration result as follows:

**Lemma 6.1**: _With probability at least \(1-O(N^{-2}T^{-1})\) we have_

\[|_{i,}-v_{i}|_{i,}(NT)}{|E_{i}( )|}}+()|}+[d_{s}] }{|E_{i}()|}+[d_{s}](NT)}}{|E_{i}( )|}.\]

The proof is deferred to the Appendix. According to Lemma 6.1, we modify Algorithm 1 by changing \(_{i,}\) to

\[_{i,}=_{i,}(NT)}{|E_{i}()| }}+()|}+[d_{s}]}{|E_{i} ()|}+[d_{s}](NT)}}{|E_{i}()|}.\]

We then state the regret result of the modified algorithm:

**Theorem 6.2**: _Let \(^{PA-DEMBA}\) be the policy produced by Algorithm 1 using \(_{i,}\). \(^{PA-DEMBA}\) satisfies_

\[(T,^{PA-DEMBA}) (T)+K\] \[+(K+1)(48+[d_{s}]+[d_{ s}]})^{2}(NT)+72.\]

**Remark 6.3**: _In the non-thresholded setting, the term with \(_{}\) disappears since \(_{n}_{}=1\). This implies that the regret in this setting does not depend on \(_{}\), providing a potentially tighter bound compared to the thresholded case. However, new terms involving \([d_{s}]\) are introduced. Asymptotically, both the thresholded and non-thresholded regret bounds simplify to \(()\), with the differences primarily reflected in the constants, and both bounds approach the lower bound._

**Remark 6.4**: _Incorporating the skewness or variance of the delay distribution can improve the regret upper bound in practice, particularly in the non-thresholded setting. For instance, distributions with faster decay rates, such as Gaussian distributions, may lead to better regret performance compared to long-tail distributions. This would involve using techniques like Bernstein-type inequalities or making assumptions about tail behavior (e.g., sub-exponential tails). However, in the current analysis, we focus on the expectation of the delay, and further improvements based on skewness are left for future work. The asymptotic regret bound remains \(O()\), independent of skewness._

## 7 Experiments

We conducted two sets of experiments to evaluate the performance of our algorithms. Our benchmark is an explore-then-exploit (EXP) algorithm, which explores by offering random assortments until a pre-specified time and then offers the optimized assortment based on the collected data. We tuned the exploration duration of the EXP algorithm and used the three best-performing durations in our comparisons.

We used \(N=10\), \(K=4\) and \(p_{i}=1\) for all \(i\{1,,N\}\). The attraction parameters were set as:

\[v_{i}=0.25+&if i\{1,2,9,10\}\\ 0.25&otherwise,\]

where \(\) represents the contrast between products. With this setting the optimal assortment is \(\{1,2,9,10\}\).

In the first set of experiments, we tested our algorithm in two different delay settings: geometrically distributed and uniformly distributed delays. We set \(=0.05\) and used three cases in each distribution with increasing \(_{}\) values: \([d_{s}]=500\), \(=100\); \([d_{s}]=100\), \(=100\); and \([d_{s}]=100\), \(=500\). The results of this experiment are shown in Figure 1. We observed that the DEMBA algorithm learns effectively and performs better than our benchmarks in all settings. Learning becomes more difficult as \(_{}\) decreases due to increased censorship and information loss from thresholding. With uniform delays, the learning is more challenging due to the heavy tail of the distribution. The gaps between DEMBA and the benchmarks increase with higher \(_{}\), suggesting better utilization of information by DEMBA.

In our second set of experiments, we tested how the contrast parameter \(\) affects learning and how the PA-DEMBA algorithm performs. We used geometric delays with \([d_{s}]=100\) and \(=100\) for the first experiment and \([d_{s}]=100\) for the second experiment. The results are shown in Figure 2. On the left-hand side, we observe that when the number of rounds is low (and thus the amount of learning is limited), lower contrast values lead to better results. As the number of rounds (and therefore the amount of learning) increases, as expected, higher contrast simplifies the learning problem, as indicated by lower regret curves. Furthermore, on the right hand side, the PA-DEMBA algorithm demonstrated robust performance, effectively managing the challenges posed by the non-thresholded setting.

Figure 1: Simulation results of DEMBA algorithm with benchmarks. Top row: geometric delays. Bottom row: uniform delays. Left: \([d_{s}]=500\), \(=100\); Middle \([d_{s}]=100\), \(=100\); Right: \([d_{s}]=100\), \(=500\). Results are averaged over 100 independent runs.

In our third set of experiments, we compare DEMBA algorithm and EXP benchmarks with MNL-Bandit algorithm from Agrawal et al. (2019). While MNL-Bandit learns customer preferences similarly to DEMBA, it does not account for potential delays in the feedback. In this experiment, we considered \(=500\) and we applied a geometric delay distribution with no delay, \([d_{s}]=50\) and \([d_{s}]=100\). We observed that when there is no delay, the performance of MNL-Bandit and DEMBA is almost identical. However, as the delay increases, the performance of MNL-Bandit deteriorates, clearly indicating that it fails to handle delayed feedback.

## 8 Conclusion

This work provides the first solution and analysis for delayed choice modeling and online assortment optimization. We introduced two novel algorithms, DEMBA for thresholded feedback settings and PA-DEMBA for non-thresholded settings, demonstrating their effectiveness through theoretical guarantees and comprehensive experiments. Our algorithms address the dual challenges of unknown Multinomial Logit (MNL) parameters and delayed feedback, achieving sub-linear regret bounds.

Lastly, we discuss future work. Our lower bound suggest an improvement on regret by considering the delay distribution via \(_{}\). This would require learning the delay distribution itself, adding more complexity. Moreover, it could be interesting to explore scenarios where no-purchase decisions are indistinguishable from delayed purchases, such as settings where tracking no purchases is not possible. Additionally, it would be worthwhile to consider multi-level choice settings where customer preferences and rewards are revealed to the seller in multiple stages with delays between each stage.