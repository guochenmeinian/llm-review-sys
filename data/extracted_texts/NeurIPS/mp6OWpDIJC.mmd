# Autonomous Agents for Collaborative Task under Information Asymmetry

Wei Liu\({}^{}\)\({}^{}\) Chenxi Wang\({}^{}\)\({}^{}\) Yifei Wang\({}^{}\) Zihao Xie\({}^{}\) Rennai Qiu\({}^{}\) Yufan Dang\({}^{}\)

Zhuoyun Du\({}^{}\) Weize Chen\({}^{}\) Cheng Yang\({}^{}\) Chen Qian\({}^{}\)\({}^{}\)

\({}^{}\)Tsinghua University Peng Cheng Laboratory, China

thinkwee2767@gmail.com albertyang33@gmail.com qianc62@gmail.com

###### Abstract

Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed _iAgents_, which denotes _Informative Multi-Agent Systems_. In _iAgents_, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. _iAgents_ employs a novel agent reasoning mechanism, _InfoNav_, to navigate agents' communication towards effective information exchange. Together with _InfoNav_, _iAgents_ organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce _InformativeBench_, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that _iAgents_ can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes1.

\(\): Equal Contributions.

\(\): Corresponding Authors.

\(\): Corresponding Authors.

\(\): Corresponding Authors.

## 1 Introduction

There has been notable progress in autonomous agents driven by the Large Language Model (LLM), especially in developing communicative agents for completing collaborative tasks , as shown in Figure 0(a). In these multi-agent systems (MAS), multiple agents are created through role-play prompting  to imitate the ability of human experts and form a _virtual entity_ (e.g., an agent company or hospital) to provide solutions derived from agents' communication. Agents share context in the _virtual entity_ to facilitate collective decision-making.

Since autonomous communication among agents has achieved significant success in discussing, decomposing, and resolving various complex tasks, the natural idea is to upgrade the tasks for agentsfrom single-person to multi-person, where agents work on behalf of multiple human users and solve the collaboration task among these users. An intuitive solution is to assign each user an agent and perform autonomous collaborations among these agents. However, in such a setting, a new challenge arises. This challenge involves dealing with asymmetry  in various types of information (environment, goals, and mind state)  since each agent can only observe the information of its human user. Previous LLM-MAS are not suitable for handling this scenario, because 1) human information is sensitive and private, so the asymmetry can not be resolved by directly collecting all information into one place and sharing it as the context for MAS. 2) Human information is dynamic so it can not be easily memorized during pre-training and activated accurately through role-play prompting in MAS to avoid asymmetry. Essentially, agents' cooperation in previous MAS has adopted an introspective approach within the _virtual entity_ (an agent hospital/town/software company), which struggles to deal with asymmetry in human information.

To bridge gaps in such asymmetry, agents need to retrieve information from humans and proactively exchange information, creating a new ecosystem combining the human and the agent network. Therefore, we propose the concept of _iAgents_ (_Informative Multi-Agent Systems_) for achieving this kind of collaboration, as shown in Figure 1b. _iAgents_ utilizes a new agent reasoning method (_InfoNav_) to model the agents' minds and navigate communication among agents toward proactive information exchange. Furthermore, a new memory mechanism is designed to provide agents with accurate and comprehensive information for exchange. Additionally, we introduced _InformativeBench_, the first benchmark evaluating agents' collaboration ability under information asymmetry. It includes both information-seeking tasks within large social networks and algorithm-like reasoning tasks over a small network. Our contributions can be summarized as follows:

1. We raise the research problem of information asymmetry in the multi-agent system for enhancing human collaboration, which is the first to shift the research perspective in this area from a holistic system view to individuals within the system. It gives a new vision to the human-agent collaboration relationship.
2. We propose the _iAgents_ framework to deal with the information asymmetry in a multi-agent system. Equipped with _InfoNav_ and improved memory mechanism, _iAgents_ could perform effective communication and collaboration within a social network (shown in Figure 6) of 140 individuals and 588 relationships, and across over 30 dialogues they searched nearly 70,000 messages and resolved the task within 3 minutes.
3. We introduce the first multi-agent information asymmetry benchmark, _InformativeBench_. Agents with some state-of-the-art LLM backends achieved an average accuracy of 50.48% on _InformativeBench_, with the most challenging task achieving only 22.8% accuracy, which reveals both potential promise and challenges in this direction.

Figure 1: Comparison between previous MAS (left) and _iAgents_ (right). The visibility range of information for each agent is highlighted with a colored background. On the left, all agents share all information (colored background of _Virtual Company_). On the right, each agent could only see information about its human user (separated colored backgrounds), and _iAgents_ is designed to deal with such kind of information asymmetry.

Related Work

**Agents based on Large Language Models (LLMs)** Originate from ancient Greek philosophy, where an "agent" denoted a being capable of intentional action, driven by mental states such as desires and beliefs . As AI progresses, this concept integrates into the simulation and understanding of intelligent behavior . Traditionally, agent research focused on some specific tasks [18; 23; 42], but the emergence of LLMs has shifted this focus. GPT-4, for instance, is recognized for achieving a form of general intelligence , prompting exploration into equipping LLMs with agency and intrinsic motivation . Studies introduce frameworks for LLM-based agents, including memorisation , decision-making, perception, and action modules, leveraging the inherent autonomy, reactivity, pro-activeness, and social ability of LLMs . Notable applications include utilizing single-agent systems and multi-agent systems for task solving and simulation [38; 16; 43; 26; 63; 31; 37]. However, most of this research has primarily focused on agent capabilities, often neglecting interaction and cooperation paradigms. This highlights a critical research gap , warranting further exploration in this area.

**Human-Agent and Multi-Agent Cooperation Paradigms** To ensure that agents align with human objectives [22; 32], human-agent cooperation is crucial. Two main paradigms of human-agent cooperation are the Equal-Partnership and the Instructor-Executor. The former emphasizes agents as communicators who understand human emotions , while the latter highlights the human's guiding role, with agents following instructions . However, single-agent systems face limitations, such as the inability to collaborate, learn from social interactions, and function effectively in complex scenarios [41; 28; 53]. Research suggests that multi-agent systems with each agent holding specific functions, can stimulate stronger intelligence [30; 1; 39]. Collaborative multi-agent systems can efficiently handle complex tasks [29; 27; 9]. Recent studies focus on scenarios with information asymmetry among agents. For instance,  explores social intelligence among agents achieving private goals based on common scene information. Additionally,  develops an evaluation framework to simulate social interactions with LLMs. The study finds that learning from omniscient simulations enhances interaction naturalness but doesn't improve goal achievement in cooperative scenarios. Many real-world scenarios involve information asymmetry, posing challenges for multi-agent systems.

**Reasoning** In previous research, agents are tasked with providing accurate information to human users in human-machine collaboration. Due to the nature of language models, the output of information relies on the user's input and the previously decoded content serving as rationale context. Therefore, some work on reasoning has explored how to improve the organization and expression of this rationale context [47; 58; 2; 8], to enhance the accuracy of output information. However, some research has also found that the reasoning process of LLMs differs from that of humans [56; 20; 5; 33]. For questions relying on internal knowledge within LLMs to answer, agents do not necessarily solve problems step by step like humans. Instead, compared to steps, having context with sufficient information content is more important. For scenarios discussed in this paper, we focus on machine-to-machine communication, relying on external knowledge of LLMs to collaboratively answer questions. This poses different requirements for LLM reasoning, especially in terms of how to promote information flow so sufficient information is included in the context provided to LLM. Agents need to actively  and accurately acquire, provide, and ask for information.

## 3 Method

### Problem Formulation

Without loss of generality, we formalize tasks in social networks that require information exchange for collaboration as a Question Answer (QA) task. The rationales \(R\) necessary for answering the question \(Q\) are distributed in different human information (\(I_{1},I_{2}\)) across the social network, which leads to information asymmetry. Consequently, agents (\(A_{1},A_{2}\)) of two individuals are required to collaborate, update the rationale set (\(R_{1},R_{2}\)) that they hold through communication \(C\), and by combining their rationales, they can reason and obtain the answer. The whole process can be formulated as:

\[Ans =Reasoning(Q,R)\] (1) \[R =R_{1} R_{2}\] (2) \[R_{1},R_{2} =C(I_{1},I_{2},A_{1},A_{2})\] (3)

### Overview

As shown in Figure 2, agents need to actively retrieve information from humans and exchange it with other agents. The communication can be represented as:

\[C_{n}=\{U_{1},U_{2},...,U_{n}\}\] (4)

where \(U\) denotes an utterance in the communication \(C\), and \(n\) is the maximum number of communication turns. Agents take turns making utterances to advance towards task resolution. Following the classical definition [59; 53], where agents observe the environment, think to make decisions, and then take action, we can organize agents' communication similarly. Each agent's behavior in one communication turn involves a pipeline of 1) _observing_ the current communication progress \(C\) and their held rationales \(R\), 2) _thinking_ about how to update the rationale to \(R^{new}\) and what \(query\) to make for retrieving information from humans, and 3) _acting_ by retrieving information and making an utterance based on it. This pipeline can be formalized as:

\[U_{i}=\{Act_{A_{1}}(Think_{A_{1}}(Obs^{i}_{A_{1}}))&i \%2==1\\ \\ Act_{A_{2}}(Think_{A_{2}}(Obs^{i}_{A_{2}}))&else.\] (5)

where

\[Obs^{i}_{A}=\{R,C_{i-1}\}\] (6)

\[Think_{A}(Obs^{i}_{A})=\{query,R^{new}\}\] (7)

\[Act_{A}(Think_{A})=A(query(I))=U\] (8)

To ensure each generated utterance provides valuable information and eliminates asymmetry, how to exchange information and what information to exchange is crucial. To deal with these two questions, we use the _InfoNav_ mechanism to guide communication towards effective information exchange. Furthermore, we introduce the _Mixed Memory_ mechanism which organizes human information into Fuzzy and Clear Memory for accurate and comprehensive retrieval. Additionally, each agent can initiate new communication \(C^{new}\) within their subnetwork, which means the communication \(C\) may be recursive and can diffuse among the social network:

\[C_{n}=\{C_{1}^{new},C_{2}^{new},...,C_{m}^{new},U_{1},U_{2},...,U_{n}\}\] (9)

For example, if Alice's agent wants to collaborate with Bob's agent, Bob's agent might respond "Hold on, I can ask Charlie's agent for help."

### InfoNav

As shown in Equation 6, the agent needs to be aware of its rationale set and ongoing communication to effectively advance the conversation. While the status of the rationale set can be implicitly inferred

Figure 2: Overall architecture of _iAgents_. From left to right, 1) each individual in the social network is equipped with an agent, and 2) two human users invoke their agents to solve a task, each initially holding the information that is visible to its human user. Then 3) agents automatically raise communication and exchange necessary information on behalf of human users. Finally, 4) agents perform a consensus check on their planning completed by _InfoNav_ to solve the task.

from utterances, this inference is often unreliable for LLM Agents. This unreliability can lead to incorrect states and then generate meaningless utterances, such as repetitive questioning or redundant thanking, which makes it harder to infer rationale and creates a vicious cycle. To address this, we propose the _InfoNav_ mechanism. _InfoNav_ plans and tracks the status of the agent's rationale set explicitly for better navigating the communication. Before each utterance, the agent reviews its plan to identify which unknown rationale to inquire about and then updates the plan based on the responses received. Figure 3 shows an example of _InfoNav_ in action. Initially, we prompt the agent to generate a plan \(P\) outlining the rationales needed to answer question \(Q\). Since the agent has no information at the beginning, all rationales in the plan are marked as unknown:

\[P(r_{1}^{u},...,r_{m}^{u})=Prompt(Q)\] (10)

where \(r^{u}\) denotes unknown rationales. During communication, if the agent gets the information of one rationale, it will update the status of this rationale from "unknown" to "known" and fill this information into the rationale placeholder in the planning text. The plan is written in fluent natural language, making it explicit and effective for prompting the model. Therefore, using _InfoNav_, the rationale set \(R\) in equation 6 is rewritten to plan \(P\), and the updated rationale \(R^{new}\) is replaced to the plan with filled rationales \(P(r^{k})\), where \(r^{k}\) represents known rationales:

\[Obs_{A}=\{P(r^{u}),C\}\] (11)

\[P(r^{k})=Think_{A}(Obs_{A})\] (12)

After multiple turns of communication, both sides finish the update of their plans. Agents then unify collected rationales and discard conflicting ones to reach an answer, denoted as "Consensus Reasoning". Thus, equations 1 to 3 rewrite to:

\[Ans =Reasoning(Q,R)\] (13) \[R =Consensus(P_{1}(R_{1}),P_{2}(R_{2}))\] (14) \[P_{1}(R_{1}),P_{2}(R_{2}) =C(I_{1},I_{2},A_{1},A_{2})\] (15)

Previous reasoning methods focused on providing accurate plans. In contrast, _InfoNav_ emphasizes navigating communication and information exchange with plans. The plan in _InfoNav_ can be seen as a generalization of Dialogue Status Tracking (DST) in conventional task-oriented dialogue systems. It also generalizes the concept of software in multi-agent software generation frameworks like ChatDev or MetaGPT . The plan maintains progress in task-solving, guiding agents to share information during communication.

### Mixed Memory

In _iAgents_, agents are navigated by _InfoNav_ to retrieve human information and share it with other agents for collaboration. Retrieval of human information is necessary since 1) human's lifelong information can not be stored in the "long context" (such as 128k tokens) of LLM, and 2) even though the information required for a single-turn conversation can fit into the context, the accumulation of information over multiple turns can lead to context explosion. It is also challenging to organize human information which is diverse in format and complex to understand. We propose organizing

Figure 3: A case of the task asking two agents to find the longest activity among all schedules. _InfoNav_ navigates the communication by providing a plan to the agent. It first 1) asks the agent to make a plan on what information is needed, then 2) fills the placeholder in this plan during communication. Finally it 3) performs a consensus check on the completed plan to 4) get the answer.

human information into two types of agent memories: Clear Memory and Fuzzy Memory. These memories facilitate reactive retrieval, ensuring accurate and comprehensive rationale extraction, as shown in Figure 2.

Clear Memory (\(Mem_{C}\)) stores information in a structured format to facilitate precise retrieval. Clear memory faithfully preserves the original information (\(I\)) and supports accurate retrieval. Additionally, it enables information retrieval from multiple spans across different chat sessions (\(s\)), capturing evolving changes in rationales.

However, Clear Memory's strict exact-match requirements complicate the retrieval process. It also struggles to provide cohesive context. To address these issues, we introduce Fuzzy Memory (\(Mem_{F}\)). Fuzzy memory stores summarized session texts (\(I_{s}\)) and uses embedding-based ANN retrieval . Although both fuzzy memory and reflection  produce summary-like text, we emphasize objective summarization of information to facilitate session-level retrieval, rather than subjective generalizations to aid in planning. While this approach may lose some details, it offers a comprehensive context and enables robust, semantic-based retrieval. Therefore, the retrieval action in Equation 8 can be rewritten to involve both memory types:

\[Act_{A_{k}}(Think_{A_{k}}) =A_{k}(query_{k}(I_{k}))\] (16) \[=A_{k}(SQL(Mem_{C}),ANN(Mem_{F}))\] (17)

What's more, the query of these two kinds of memories is decided by agents based on observations of previous executions, which means agents can reactively adjust their queries. Combining these two kinds of memory facilitates agents to cross-verify the retrieved information and provides _InfoNav_ with comprehensive and accurate rationales.

## 4 InformativeBench

To the best of our knowledge, there is no benchmark or dataset tailored for information asymmetry in the collaboration task among communicative agents. In this paper, we construct _InformativeBench_, the first benchmark to evaluate agent collaboration tasks featuring information asymmetry in social networks. It includes two categories with a total of five datasets. Details, including the scale, distribution, and metrics of the datasets, are provided in section C. What's more, recent studies have found that _LLM continuously ingests internet data so static benchmarks can be easily memorized and overfitted_. Hence, two pipelines for constructing _InformativeBench_ are easy to realize and can be generalized to more domains for constant and dynamic evaluations. They are Needle-Oriented and Reasoning-Oriented pipelines, as shown in Figure 4.

**Needle-Oriented Pipeline** A "needle" is inserted into the social network, and agents are tasked with finding this "needle" information. This evaluates their ability to share and locate information. The dataset can be created by splitting the needle and spreading it into the network, or by collecting pieces from the network and combining them. For the split method, the _Needle in the Persona (NP)

Figure 4: Two kinds of tasks in the _InformativeBench_. Each agent can only see the information (marked with different colors) of the human that it works on behalf of, which generates information asymmetry. Agents are 1) asked to find the needle information within the network or 2) reason to get an answer which is the output of an algorithm running on distributed information in the network.

dataset modifies the dialogue in the SPC dataset by adding a common or opposite persona to two individuals' personas. Agents are asked to find this persona. For the combination method, the _FriendsTV_ dataset reconstructs the social network from the entire Season 1 script of _Friends_, involving 140 characters with 588 relationships, and combines two questions in the FriendsQA dataset [57; 24] as "needle pieces" to generate new question. This dataset, the largest in _InformativeBench_, features sarcasm, plot twists, and complex relationships for simulating real-world challenges.

**Reasoning-Oriented Pipeline** Humans are assigned different pieces of information, which serve as inputs for an algorithm (such as sorting or merging). Agents must reason to get the answer which is the algorithm's output. Therefore, the algorithm serves as an automatic verifier for information asymmetric reasoning. In _InformativeBench_, this is represented by the _Schedule_ dataset, which develops a program for assigning different schedules to individuals. Agents are presented with algorithmic problems of varying difficulties, and the program automatically verifies the correctness of their solutions. The datasets include questions of three levels of difficulty: _Easy_) calculate the number of conflicting schedules between two people, _Medium_) find the longest activity among six people, and _Hard_) find the longest common free period among six people.

## 5 Experimental Setup

We generically treat chat histories as human information. This approach simplifies modeling information asymmetry in social networks. Other types of information, such as knowledge bases, documents, or web content, can all be organized in mixed memory so _iAgents_ is adaptable to all these kinds of information. We conduct all experiments with a maximum of 10 communication turns for agents. The experiments use gpt-4-0125-preview, gpt-3.5-turbo-16k, gemini-1.0-pro-latest, and Claude-sonnet 2 as LLM backends. The temperature is set to 0.2. For Fuzzy Memory, we use gpt-4-0125-preview to summarize session text and OpenAI text-embedding-3-small to generate embeddings for ANN embedding search. We use precision as the metric for questions in the NP, ScheduleEasy, and FriendsTV datasets. For the ScheduleMedium and ScheduleHard datasets, we use F1 and IoU as the metrics, corresponding to the algorithm used. Details about the metrics are shown in Section C.3. For the Schedule and NP datasets, we do not activate mixed memory since the information scale is small and can be fully loaded in the LLM context. Additionally, for the Schedule dataset, we do not activate the agent's ability to initiate new communication due to the small scale of the social network.

## 6 Result

### _InformativeBench_ Evaluations

We first comprehensively assessed the performance of _iAgents_ using some state-of-the-art LLMs on _InformativeBench_, as shown in Table 1. GPT-4 achieves over 50% accuracy across most datasets, indicating its potential to work on behalf of humans for cooperation. However, smaller-scale LLMs still face significant challenges in solving cooperation problems in information asymmetry. Most models could only achieve about 50% precision on the easiest NP task. For the Schedule dataset, as questions become harder, performance drops, with most models solving less than 20% of the hardest questions. The FriendsTV dataset introduces a large social network, requiring agents to use external memory to retrieve rationale from extensive human information. Most LLMs struggle to exceed 40%

    &  &  \\  & **Easy** & **Medium** & **Hard** & **NP** & **FriendsTV** \\  GPT 4 & 56.67\% & 51.00\% & 22.80\% & 64.00\% & 57.94\% \\ GPT 3.5 & 36.67\% & 18.00\% & 12.25\% & 51.00\% & 35.71\% \\ Claude Sonnet & 43.33\% & 17.44\% & 18.66\% & 50.00\% & 34.13\% \\ Gemini 1.0 & 26.67\% & 22.33\% & 14.40\% & 40.00\% & 28.57\% \\   

Table 1: Evaluation results of _iAgents_ on _InformativeBench_ with different LLM backends.

accuracy in this dataset. Thus, while previous studies show impressive performance when agents are omniscient, collaborating in information asymmetry remains challenging.

### Ablation Study

We conducted ablation experiments on several key designs of the _iAgents_ framework, as detailed in Table 2. Analyzing the FriendsTV dataset revealed that incorporation of the mixed memory mechanism led to a performance increase ranging from 2.38% to 6.34%, surpassing the impact of _InfoNav_, which resulted in only a 0.8% performance increase. This discrepancy underscores the greater significance of effective retrieval over reasoning during communication in large social networks with mass information. Notably, the ablation of both memory mechanisms emphasized the indispensability of mixed memory. The introduction of recursive communication exhibited the most significant performance gain (12.7%), primarily due to the challenges posed by the vast social network in the FriendsTV dataset. By actively introducing new communications within ongoing dialogues, agents could acquire and corroborate information, thus significantly enhancing performance. This highlights the imperative of scalability in our proposed framework for addressing real-world problems.

For the NP and Schedule datasets, the main challenge lies in facilitating effective multi-turn communication to exchange information for reasoning. Therefore, _InfoNav_ emerged as pivotal in enhancing performance, resulting in performance increases ranging from 15% to 26%. When agents relied solely on initialized prompts to navigate multi-turn communication, they struggled to exchange information effectively to accomplish tasks. This deficiency was particularly evident in datasets like Schedule, which emphasize logical reasoning and computation. Across all difficulty levels, agents without the _InfoNav_ mechanism failed to achieve accuracy exceeding 10%.

### Analysis on Agents' Behaviour

_InfoNav_**Behaviour** We examined how agents utilize _InfoNav_ for information exchange during multi-turn communication. Notably, we calculated the average number of unknown rationales solved each time _InfoNav_ updated the plan and the proportion of rationales passed in consensus reasoning. Moreover, some rationales were solved in a "Fake Solved" hallucination, where agents filled in the rationale as "solved, which is unknown". We also documented the frequency of such occurrences. Table 3 shows that agents who propose fewer rationales to seek and achieve a higher solved ratio are more likely to accomplish the task. Interestingly, agents often fill multiple rationales concurrently rather than sequentially. Those agents with higher instances of synchronous completions suggest a deeper understanding of the task and greater confidence in filling rationales. Furthermore, the occurrence of Fake Solved instances is lower among agents who predict tasks correctly. The consensus ratio is also higher when agents successfully complete the task. It denotes that the information obtained by the two collaborating agents is relatively accurate and free of contradictions, thus increasing the likelihood of arriving at the correct conclusion through their final reasoning. Besides, we observed that agents not only propose rationales but also task states, such as the completion status of specific actions. The completion rates of these rationales and states are positively correlated with task success. In essence, the utilization of _InfoNav_ by agents mirrors human intuition, emphasizing first careful planning, then proactive and accurate information exchange.

    &  &  \\  & **Easy** & **Medium** & **Hard** & **NP** & **FriendsTV** \\  iAgents (Full Model) & 36.67\% & 18.00\% & 12.25\% & 51.00\% & 35.71\% \\  _Ablation on Info/Nav:_ & & & & & \\  w/o InfoNav & 10.00\% & 3.56\% & 7.34\% & 39.00\% & 34.92\% \\  _Ablation on other mechanisms (Limited Applicability):_ & & & & \\  w/o Recursive Comm & – & – & – & 48.00\% & 23.02\% \\ w/o Fuzzy Memory & – & – & – & – & 29.37\% \\ w/o Clear Memory & – & – & – & – & 33.33\% \\   

Table 2: Ablation study on _iAgents_. Dashes (–) indicate: (1) _iAgents_ on Reasoning-Oriented dataset does not equip other mechanisms, hence no ablation needed; (2) For NP dataset, _iAgents_ does not utilize Mixed Memory hence there is no ablation.

**Memory Behaviour** Similarly, we explored how agents adapt their memory retrieval strategies during communication. We examined three parameters in clear memory queries: the context window, which determines the breadth of contextual messages; the total message retrieval limit; and the size of the query keywords set. For fuzzy memory, we analyzed two parameters: the number of queried responses (topk) and the length of the query text. These findings are illustrated in Figure 5. Our analysis revealed several notable trends. The majority of agents do not change their behavior during communication. However, when agents decide to change their behavior, we observed that they tended to increase the amount of retrieved information over time. This augmentation trend was particularly pronounced on the overall message retrieval limit, where the frequency of "increase" actions surpassed that of "decrease" actions by nearly threefold. Furthermore, agents who completed tasks exhibited a more conservative approach, with a lower proportion of behavioral changes compared to agents unable to complete tasks. This phenomenon may be attributed to the difficulty of certain tasks, making agents continuously refine their strategies in pursuit of the required information.

### Analysis on Real World Concern

We studied two significant challenges in extending the _iAgents_ to real-world applications. Firstly, we investigated whether the agent can effectively respond to human input without being overly influenced by factual knowledge obtained during pre-training . Secondly, we explored the agent's ability to engage in communication while upholding human privacy. Our experiments were conducted using the GPT3.5 model on the FriendsTV dataset.

**Prior Distraction** The FriendsTV contains information that could be memorized by LLM from the Internet, hence it is perfect for analyzing prior distractions. We anonymized the names of the primary characters in the dataset, for example, renaming "Rachel" to "Alice". The performance of the agents on this anonymized dataset decreased from 35.71% to 32.54%, suggesting that to some extent, agents can reason based on user-provided information rather than solely relying on knowledge memorized in pre-training. It may need further advancements, such as model unlearning , to fully address this issue.

    & **\#Rationales** & **\#Rationales Solved** & **Rationales Solved** & **Rate Solved** & **Consensus** \\  & **in**_InfoNav_ & **per Update** & **Ratio** & **Ratio** & **Ratio** \\  Predict Right & 5.29 & 2.04 & 84.75\% & 3.49\% & 70.52\% \\ Predict Wrong & 5.63 & 1.69 & 67.23\% & 5.40\% & 62.70\% \\ All & 5.45 & 1.87 & 76.22\% & 4.42\% & 66.20\% \\   

Table 3: Analysis _InfoNav_ behaviour on the trajectory of _iAgents_ using GPT4 as backend. When agents successfully complete the task, the static collected from their trajectory proves that they better utilize the _InfoNav_ mechanism, since the rationale solved ratio, synchronous completions of rationales, and consensus ratio are higher, and present fewer fake solved hallucinations.

Figure 5: The figure depicts the distribution of different behaviors of agents in adjusting memory retrieval based on the progress of communication. Agents predominantly tend to maintain parameters unchanged, but when changes occur, they tend to increase parameters to gain more information.

**Privacy Concern** In investigating whether agents can communicate without compromising privacy, we conducted an experiment involving modifications to the agent's system prompt, emphasizing the importance of privacy preservation in utterances. The agent then utilized vague expressions such as "somebody/somewhere" and disclosed only relevant entity information. This adjustment led to a performance drop from 35.71% to 30.95%, indicating the ongoing challenge of achieving collaboration while ensuring privacy. It's important to note that we solely adjusted privacy settings on the output side, rather than restricting agent access to human information on the input side. This decision was made because setting access permissions might inadvertently reveal prior task-related information. Thus, the real challenge lies in appropriately regulating access to information based on task requirements, akin to teaching the agent to retrieve necessary information accurately. Additionally, absolute privacy protection is impractical, as absolute privacy protection amounts to forgoing problem-solving through collaboration.

## 7 Limitations

While the _iAgents_ framework introduces innovative multi-agent collaboration, it has several limitations and challenges. **Privacy Issues**: As discussed in Section 6.4, we examined the performance of agents communicating and collaborating under privacy constraints, highlighting the trade-off between privacy and collaboration. We define three privacy levels. L1: Users fully share personal information, allowing maximum efficiency for _iAgents_. L2: Users keep their personal information private. _iAgents_ can handle this situation by deploying an edge-side small language model agent for information acquisition. L3: Users demand maximal privacy, with both personal and agent communication handled locally on private devices, which is still a challenge for small language model agents. **Network Modeling**: The current framework initiates communications based on user relevance but lacks nuanced modeling of human social networks and collaboration history. Enhancing network topology through added or removed nodes and incorporating past interactions could improve communication efficiency. **Human-Agent Interaction**: Although _iAgents_ target full autonomy, human involvement for verification remains necessary in real-world scenarios. Processes must be designed to prompt user feedback and adjust agent strategies accordingly, ensuring alignment with user preferences. **Cost**: The high input token consumption (about 30,000 tokens per task) required for _iAgents_ to handle human information is a challenge. However, advancements in long-context models, which extend input token length, present opportunities to reduce the cost of scaling _iAgents_.

## 8 Conclusion

This paper revisits the ecological role of agents within human society, where agents act on behalf of humans in communication to complete collaborative tasks. A primary focus lies in addressing the challenge of information asymmetry. We introduce a novel paradigm for designing multi-agent systems, termed _iAgents_, for addressing information asymmetry. Furthermore, we introduce a benchmark to evaluate the agents' collaboration ability under information asymmetry thoroughly. Going forward, we aim to confront several key challenges to successfully implement this system in the real world for augmenting human productivity, including deploying lightweight models at the edge to address privacy concerns and devising new Human-Computer Interaction paradigms for autonomous and controllable communication among agents, etc. _iAgents_ does not role-play to replace human experts but consistently attributes the value of information to humans and we believe it can facilitate the productivity of human society within a secure and controllable framework.

## 9 Acknowledgements

The work was supported by the Postdoctoral Fellowship Program of CPSF under Grant Number GZB20230348 and the Tencent Rhino-Bird Focused Research Program. We wish to express our profound appreciation to Professor Zhiyuan Liu and Professor Maosong Sun from the Department of Computer Science at Tsinghua University for their detailed guidance and critical insights, which have been instrumental to the success of this work.