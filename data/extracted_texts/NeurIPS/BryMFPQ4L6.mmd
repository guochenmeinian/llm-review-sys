# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

still largely requires training from scratch. In contrast, the prominent work, Memorizing Transformer (MemTRM) , approximates in-context sparse attention via dense attention over both incorrect tokens and memorized tokens retrieved from a non-differentiable memory for Transformers. Thus, MemTRM scales up the resulting language model to handle up to 65k tokens and achieves substantial perplexity gains in modeling full-length books or long papers. However, MemTRM faces the _memory staleness_ challenge during training due to its coupled memory design, which uses a single model for encoding memory and fusing memory for language modeling. In other words, as the model parameters are updated, cached older representations in memory may have distributional shifts from those from the latest model, thereby limiting the effectiveness of the memory augmentation.

In this paper, we present a framework for Language Models Augmented with **Long**-Term **Memory** (LongMem). This framework enables language models to cache lengthy previous context or knowledge into a non-differentiable memory bank, and then utilize them via a decoupled memory module to mitigate the issue of memory staleness. To achieve decoupled memory, we design a novel residual side-network (SideNet) in conjunction with a frozen backbone LLM. Paired attention keys and values of the previous context are extracted using the frozen backbone LLM, which are subsequently stored in the memory bank. In the memory-augmented layer of SideNet, the generated attention query of the current input is used to retrieve cached key-value pairs of previous contexts from the memory, and the corresponding memory augmentations are then fused into adaptable hidden states via a joint-attention mechanism. Furthermore, newly designed cross-network residual connections between the SideNet and the frozen backbone LLM facilitate better knowledge transfer from the pretrained backbone LLM. Through continuous training of the residual SideNet to retrieve and fuse memory augmentations, the pre-trained LLM can be adapted to effectively leverage long-contextual memory for enhanced modeling. The detailed memory cache, retrieval and fusion process is illustrated in Figure 1.

Our decoupled memory design offers two key advantages. First, our proposed architecture effectively separates the process of encoding previous inputs into memory and the process of memory retrieval and fusion, thanks to the decoupled frozen backbone LLM and SideNet. In this way, the backbone LLM only works as the long-context knowledge encoder, while the residual SideNet works as the memory retriever and reader, which effectively resolves the issue of memory staleness. Second, directly adapting the entire LLM with memory augmentations is computationally inefficient and also prone to catastrophic forgetting. As the backbone LLM is frozen during the efficient memory-augmented adaptation stage, LongMem can not only tap into the pretrained knowledge but also avoid catastrophic forgetting.

LongMem is capable of taking various types of long-form text and knowledge into the memory bank based on downstream tasks. Here, we consider two representative cases, language modeling with full-length book contexts, and memory-augmented in-context learning with thousands of task-relevant

Figure 1: Overview of the memory caching and retrieval flow of LongMem. The long text sequence is divided into fix-length segments with each previous segment processed through a frozen backbone LLM and the corresponding attention key and value vectors of \(m\)-th layer are cached into the memory bank. Given current inputs, the corresponding attention query vectors are used to retrieve the top-\(K\) attention key-value pairs from previous segments stored in the memory bank, which will be then fused with local context for language modeling.

demonstration examples. Specifically, we evaluate the effectiveness of the proposed LongMem on various long-text language modeling, and memory-augmented in-context learning for natural language understanding (NLU) tasks. Experimental results demonstrate that our model consistently outperforms the strong baselines in terms of long-text modeling and in-context learning abilities. Our method substantially improves LLM's long-context language modeling capabilities, with a reduction in perplexity of 1.38\(\)1.62 over different length splits of Gutenberg-2022 corpus. Notably, our model achieves state-of-the-art performance of 40.5% identification accuracy on ChapterBreak, a challenging long-context modeling benchmark, significantly surpassing existing strong x-former baselines. Finally, with 2k demonstration examples in memory, LongMem shows pronounced improvements in in-context learning on popular NLU tasks, compared with prominent memory-augmented and non-memory-augmented baselines.

## 2 Methods

To enable LLMs to harvest relevant information from the past long context in memory, we propose to augment the frozen backbone LLM with a decoupled memory module. To fuse the memory context information, we design a novel lightweight residual SideNet, which can be continually trained in an efficient way. In the following, we first discuss the problem formulation of language modeling with memory augmentations. Then, we formally introduce our efficient residual SideNet for adapting the frozen pretrained LLM to jointly attend over local input context and retrieved memory context. Lastly, we provide our designed processes of how past memory is encoded, stored, recalled and fused for language modeling.

### Language Models Augmented with Long-Term Memory

Here, we focus on the high-level problem setup and defer more component details to later sections. Given its wide adoption for pretrained LLMs, our LongMem model is built on the Transformer architecture . For LongMem, there are three key components: the frozen backbone LLM, SideNet, and Cache Memory Bank. As most existing pretrained LLMs can only take a fix-sized input, only the input segment of a long sequence (_e.g.,_ a book) that can fit in the length limit is denoted as the current input as done for most existing autoregressive language models. Those previous segments that can not fit are denoted as previous inputs, which are used for memory augmentations. To tap into the learned knowledge of the pretrained LLM, both previous and current inputs are encoded using the frozen backbone LLM but different representations are extracted. For previous inputs, the key-value pairs from the Transformer self-attention at \(m\)-th layer are stored in Cache Memory Bank, whereas the hidden states from each LLM decoder layer for the current inputs are retained and transferred to SideNet. For each current input token, top relevant key-value vector pairs are retrieved as memory augmentations for language modeling. The SideNet module can be viewed as an efficient adaption model that is trained to fuse the current input context and relevant cached previous contexts in the decoupled memory.

Figure 2: Overview of LongMem architecture. “MemAug” represents Memory-Augmented Layer.

Formally, for a fix-sized input text sequence \(\{_{i}\}_{i=1}^{|x|}\) (the current input), LongMem first performs a forward pass using the backbone LLM (indicated in blue in Figure 2) **without any gradient calculation**. The embedding layer of the backbone LLM first encodes the input \(\{_{i}\}_{i=1}^{|x|}\) into embedding space and outputs the initial hidden states, \(_{}^{0}^{|x| E}\), where \(E\) is the hidden dimension. Then each successive Transformer decoder layer of the frozen backbone LLM computes the new hidden states using the hidden states from the previous layer, \(_{}^{}=f_{_{}^{}}(_{ }^{}), l^{}[1,L^{}]\) and \(L^{}\) is the total # layers for the backbone LLM. During the forward pass with the backbone LLM for all previous inputs, the key-value pairs used for self-attention at the \(m\)-th Transformer decoder layer are stored in Cached Memory Bank (highlighted in orange in upper-left of Figure 2). These pairs are subsequently recalled as memory augmentations for future inputs.

**Cached Memory Bank** is a head-wise vector queue \(_{k},_{v}^{H M d}\), which maintains attention key-value pairs of latest \(M\) previous inputs \(},}^{H|x|  d}\), where \(H,d\) denotes the number of attention heads and per-head dimension respectively. After memory retrieval and fusion (SS2.3), the memory bank removes the key-value pairs of the oldest sequences and appends the current sequences to the cached vector bank. This update mechanism ensures the language modeling causality at the sequences level and enables the memory bank to consistently maintain records of the most recent previous context for the current inputs.

After the forward pass with the backbone LLM, the SideNet module then takes all current input hidden states from the backbone LLM \(\{_{}^{}\}_{l^{}=1}^{L^{}}\) and the past key-value pairs in the Cached Memory Bank for computing memory-augmented representations. Specifically, our SideNet of LongMem consists of \((L-1)\) normal Transformer decoder layers and one special memory-augmented decoder layer. For efficient purposes, we mainly consider the case where #layers \(L\) of the SideNet is smaller than that of the backbone LLM, _i.e.,_\(L<L^{}\). Our SideNet encodes \(^{0}\) into memory-augmented contextual representation via \((L-1)\) normal Transformer decoder layers and a special **memory-augmented layer**.

The **memory-augmented layer** is an extension of the vanilla Transformer decoder layer that takes a memory-augmented input, including both top relevant key-value pairs in memory and the hidden states from the current input. Here, the cached key-value pairs are recalled using a token-based memory retrieval module (SS2.3). For each current input token, the memory retrieval module \(s_{rt}(:)\) retrieves top-\(K\) relevant key-value pairs in the memory bank \(\{}_{ij},}_{ij}\}_{j=1}^{K}=s_{rt}( _{i})\). Then SideNet computes the output using the memory-augmented input, \(_{}^{m_{s}}=f_{_{}}(_{}^{m_{s}-1},\{\{}_{ij},}_{ij}\}_{j=1}^{K} \}_{i=1}^{|x|})\), where \(m_{s}\) is the layer index where we inject the memory-augmentation layer.

Finally, the token probability is computed using the last SideNet hidden states \(P(_{i}|_{1},,_{i-1})=(W ^{L})\), where \(W\) is the frozen output embedding weight shared by both the backbone LLM and SideNet. We perform a memory-augmented adaptation training for LongMem to utilize the decoupled memory. Following the _generative unsupervised pre-training_, the training objective of LongMem is the standard left-to-right language modeling objective, which maximizes the likelihood of the next token based on the left context: \(_{x}_{i=1}^{||} P(_{i}| _{1},,_{i-1}),\) where \(x\) is a randomly sampled sentence from the pre-training text corpus \(\).

### Residual SideNet

**SideNet Architecture and Initialization.** Here, we implement SideNet based on Transformer . Specifically, the number of decoder layers \(L\) in SideNet is equal to the number of layers \(L^{}\) in the backbone LLM divided by a reduction factor (a layer reduction factor of \(2\) is used throughout this work, _i.e.,_\(L^{}=2L\)). The weights of each decoder layer in SideNet are initialized from the corresponding pre-trained decoder layer of the backbone LLM at the same depth: \(_{}^{l}=_{}^{2l}\). As illustrated in Figure 2, the SideNet model takes the output of backbone LLM's embedding layer and reuses the language modeling head of the backbone LLM, which remains frozen during the continual adaption stage. Throughout the memory-augmented adaptation stage, all other parameters of SideNet are updated based on the training signal. In this way, the lightweight SideNet adaptation achieves fast convergence with knowledge transferred from pre-trained parameters.

**Cross-Network Residual Connections.** To tap into knowledge from the pretrained backbone LLM, we utilize our proposed cross-network residual connections to fuse representations from the backbone LLM into SideNet. Specifically, we add the difference between output hidden states at \(2l\)-th and \((2l-2)\)-th layers of the backbone LLM as the residual connections to the output hidden states at \(l\)-th layer of SideNet. Then, the input to the next \((l+1)\)-th layer of SideNet is the sum of the original hidden state forwarded through the previous layer \(f_{^{l}_{}}(^{l-1}_{})\) and the cross-network residual connection of the hidden state difference from the backbone LLM

\[^{l}_{}=f_{^{l}_{}}(^{l-1}_{ })+(^{2l}_{}-^{2l-2}_{}),  l[1,L],\] (1)

where \(^{0}\) is the output of embedding layer. It is worth noting that the residual connections after the self-attention and feed-forward network of a decoder layer  will be performed as normal in \(f_{^{l}_{}}(^{l-1}_{})\) and parallel to the proposed cross-network residual connections.

### Memory Retrieval and Fusion

The long-term memory capability of LongMem is achieved via a memory-augmentation module for retrieval and fusion.

**Token-to-Chunk Memory Retrieval.** Instead of performing token-to-token retrieval, we focus on token-to-chunk retrieval for acceleration and integrity. A text-chunk refers to an n-gram structure of chunk-size \(csz\) number of contiguous tokens. The memory bank stores cached key-value pairs at the level of token chunks. We divide the memory bank into \(M/csz\) attention key-value paired chunks and use the mean-pooled vector on the chunk-size dimension to get the key vector for retrieval. Then we retrieve the top-\((K/csz)\) attention key-value chunks w.r.t the dot product between the attention query of the current input token and the mean-pooled attention key of a candidate chunk. Finally, we squeeze the chunk-size dimension for retrieved key-value paired chunks and flatten them into \(K\) key-value pairs at token-level \(\{}_{j},}_{j}\}_{j=1}^{K}\). Adopting token-to-chunk retrieval reduces the size of the retrieval index and accelerates the process. Meanwhile, the retrieval accuracy can be further improved, which is also observed in  and . The hyperparameter chunk-size \(csz\) controls the granularity of retrieved contexts, which can be empirically adjusted based on downstream tasks. For instance, in-context learning requires more fine-grained label tokens from demonstration examples cached in memory, where a smaller \(csz\) is helpful.

**Memory Fusion.** The memory fusion is performed within a special memory-augmented layer. As the conventional Transformer decoder layer uses the multi-head self-attention , we follow  to extend it to a joint-attention mechanism and propose a long-term memory fusion process to enable each token to attend on both local contexts and retrieved memory contexts. With the head-wise hidden state output from previous layer \(^{l-1}^{|x| d}\) and the corresponding retrieved attention key-value pairs are \(\{}_{i},}_{i}\}_{i=1}^{|x|} ^{|x|}\), the output hidden state for the \(l\)-th memory-augmented layer \(^{l}\) is computed as:

\[=(^{T}}{}),\ \ =\{(_{i}}_{i}^{T}}{})}_{i}\}_{i=1}^{|x|},\] (2) \[^{l}=(g)+(1-(g)),\] (3)

where \(,,,,^{|x| }\), \(\) is the number of retrieved attention key-value pairs in cached memory for each token, and \(g\) is a trainable head-wise gating vector. The hidden state output from previous layer \(^{(l-1)}\) is linearly projected into attention queries, keys, and values \(,,\) separately via three matrices \(W^{Q},W^{K},W^{V}^{}\). It is worth noting that the retrieved attention key-value pairs in cached memory are distinct to each token.

## 3 Experiments

We evaluate our proposed LongMem model on different tasks that require long-context modeling: a) long-text language modeling and language understanding when loading the past long-context into cached memory; b) infinite-length in-context learning when loading a large number of demonstration examples into cached memory.

### Training Setup

**BatchNormying the training corpora.** The conventional batchyfing process for large corpora truncates the whole corpora into consecutive fix-length text segments without padding and shuffles all segments to construct mini-batches . In contrast, LongMem must disable global shuffling and ensure the global causality at the segment level. Firstly, we divide all long documents in training corpora into batch-size number of document groups with equivalent length and then perform a document-level shuffling within each group. Then, we concatenate shuffled documents within one group and truncate them into ordered segments. In order to ensure that two consecutive segments of one long document are distributed in two consecutive input batches after batchfying, we select one segment from batch-size number of document groups with the same inner-group index. Thus a mini-batch with batch-size number of segments are constructed from exactly the batch-size number of document groups. In this way, as the training iteration steps, the cached attention key-value pairs in the memory bank are previous context of current inputs within the same document. The batchfying process is illustrated in Figure 3.

**Training Corpus, Backbone LLM and Hyperparameter.** We sample a subset of the Pile  as the training corpus, including BookCorpus2, Books3, OpenWebText2, Stack Exchange, Wikipedia, Gutenberg (PG-19), NIH ExPorter, and Pile-CC datasets. We reproduce GPT-2* (407M-params) as the pre-trained backbone LLM with Alibi  position embedding because original GPT-2  adopts absolute position embedding, which is found to perform poorly to enable LLM to learn long-distance dependencies . The backbone LLM holds a \(L^{}=24,H=16,d=64\) architecture. The SideNet holds a \(L=12,H=16,d=64\) architecture. The training for memory-augmented adaptation iterates on 26B tokens, with a global 256 batch-size and 1024 sequence length. The chunk-size \(csz\) is 4 tokens and the memory size \(M\) is 65k key-value pairs of tokens. For each token, we retrieve \(K\)=64 attention key-value pairs for augmentation, which are \(K/csz\)=16 text chunks. The memory-augmentation layer is the 9-th layer of SideNet. The attention keys and values from 18-th layer of backbone LLM is cached into memory and used for future retrieval. Other training details are presented in Appendix C.

**Memory Retrieval Module.** The fixed memory-size of cached memory bank in one GPU is 65536 key-value pairs of tokens. We enable each GPU to construct and update their own memory retrieval module for efficiency. For the implementation of the efficient token-to-chunk retrieval, we use the faiss toolkit to construct an exact-search index on GPU to store the mean-pooled attention keys of text chunks and perform efficient retrieval. The faiss index maintains a fixed \(M/csz\) keys and provides the efficient exact search w.r.t. inner product. The retrieval takes about 15ms per 1k tokens, which is 55% timecost of backbone LLM forwarding pass. We can easily adapt the exact search index to approximate search index to gain more retrieval efficiency.

**Baselines.** In addition to the baseline of our pre-trained GPT-2* variant, we consider Memorizing Transformer (MemTRM)  and TRIME  as two memory-augmented baselines. The MemTRM model can be easily adapted to tune a pre-trained LLM to use external memory. We insert the KNN-augmented layer proposed by MemTRM as the same 18-th layer in the LLM decoder.

Figure 3: Batchfying the large text corpora into batches to ensure that each consecutive segments within each document is distributed in consecutive batches.

To adapt TRIME for our experiments, we replace the batchfying function and loss function of training GPT-2* with those proposed by TRIME, which enables a memory-augmented adaptation tuning method for LLMs. The two reproduced baselines are trained for the same number of tokens under the same hyperparameter setting as LongMem.

### Long-Context Language Modeling

The long-context language modeling can potentially benefit from the augmented memory of past long-contexts. The knowledge stored in retrieved attention key-values can provide valuable background and contextual information, helping models perform better in long-context language modeling. For instance, when trying to model a long-text book, acquiring knowledge from previous background and character relationships can be helpful in modeling the subsequent stories.

**Evaluation Setting.** We first compare LongMem and baselines on three long-context modeling datasets, _Project Gutenberg 2020-2022_, _ArXiv_, and _ChapterBreak_. The majority of included books or papers in these datasets have the length of at least 16k tokens. All listed datasets are evaluated in a **zero-shot** manner without any task-specific tuning. The detailed evaluation settings on the three datasets are as follows:

* **Project Gutenberg 2020-2022 Language Modeling Dataset.** We crawled and cleaned the books published between 2020 and 2022 under Project Gutenberg Library1 to build up a completely new long-text modeling dataset, named **PG-22**. It is significantly different from our training subset PG-19 in terms of domains and writing styles, because books in PG-19  are published before 1919. We provide different validation splits of PG-22 based on length range, and the data statistics are presented in Table 1. * **ArXiv Dataset.** The ArXiv dataset includes papers in the areas of Math, Computer Science, and Physics. We select a validation split of ArXiv paper subset in the Pile corpus . The ArXiv subset of Pile is excluded from our training and serves an out-of-distribution dataset. We report the token-level language modeling perplexity on the long-context language modeling benchmarks of PG-22 and ArXiv.
* **ChapterBreak Benchmark.** ChapterBreak  is a challenging suffix identification dataset that requires LLMs to distinguish the beginning of the ground-truth next chapter from a set of hard negative segments sampled from the same book, given the long context of previous chapters. ChapterBreak requires processing global long-context to comprehend and identify the correct suffix.  demonstrated that even state-of-the-art x-formers for long-text processing fail to effectively leverage long-range context to perform well on ChapterBreak. ChapterBreak has two subsets, the PG-19 subset and the Archive of Our Own (AO3) subset. As the PG-19 corpus has been included in the pre-training corpus of LongMem, it cannot be further used for evaluation. Thus, we select AO3 subset, which contains fan-fictions extracted from AO3. ChapterBreak provides 8 splits based on the prefix length from 0.5k to 8k tokens to fit the length limit of different models. The splits of 4k, 6k, and 8k prefix are selected for evaluation. For LLMs that cannot process over 4k tokens, we abandon the front prefix to fulfill the maximum input length of LLMs. For memory-augmented models (MemTRM and LongMem), we load the given 4k/6k/8k prefix contexts into the cached memory and then do the scoring. we use the perplexity as the scorer for each candidate suffix segment in a zero-shot manner. Then the suffix segment with lower perplexity is selected as the label. The suffix identification accuracy is used as the evaluation metric.

**Results.** The main results on evaluated long-context datasets are summarized in Table 2. The proposed LongMem model significantly outperforms all considered baselines on long-text language modeling

  
**Dataset** &  &  \\
**Splits** & S1 & S2 & S3 & S4 & S5 & \\ 
**Len. Range** & 5k-10k & 10k-100k & 100k-500k & 500k-1M & 1M & 60k \\
**\#Documents** & 500 & 100 & 30 & 8 & 1 & 100 \\
**Avg. \#tokens** & 7.6k & 47.6k & 140k & 640k & 1.2M & 15.4k \\   

Table 1: Dataset Statistics of five splits of PG-22 based on length range and ArXiv.

datasets, with improvements of 1.38 to 1.62 perplexity on different length splits of _PG-22_, and 1.0 on ArXiv datasets. Surprisingly, the proposed method achieves the state-of-the-art performance of 40.5% accuracy on ChapterBreak\({}_{}\) suffix identification benchmark and outperforms both the strong long-context transformers and GPT-3 with 313x larger parameters. The substantial improvements on these datasets demonstrate that LongMem can comprehend past long-context in cached memory well for predicting future inputs.

### Memory-Augmented In-Context Learning

LLMs have the emerging capability of in-context learning (ICL) via learning knowledge non-parametrically from few-shot demonstration examples in the local context. However, conventional in-context learning is heavily restricted by input context length, rendering it ineffective to absorb supervision from sufficient demonstration examples in the training set. With the proposed unlimited-length memory augmentation, LongMem can overcome the limitation of the number of demonstration examples in the local context and even attend on the whole training set by loading it into the cached memory. In this way, LongMem generalizes the conventional few-shot in-context learning to memory-augmented in-context learning with thousands of auxiliary demonstration examples.

**Evaluation Setting.** Here, we evaluate the in-context learning capability of baselines and the proposed LongMem model on five NLU datasets, SST-2 [SPW\({}^{+}\)13], MPQA [WWC05], MR [ABK\({}^{+}\)07], Subj [PL04] and SST-5 [SPW\({}^{+}\)13]. We evaluate models on two few-shot settings, 4-shot and 20-shot. The 4-shot case is the data-insufficient scenario, while the 20-shot demonstrations can almost fulfill the 1k input length and provide sufficient contextual self-supervisions. We transform the k-shot examples to semantically meaningful demonstration examples via fixed text template, i.e., \(d_{i}\)="Review: \(x_{i}\) Sentiment: \(y_{i}\)",\(\{(x_{i},y_{i})\}_{i=1}^{k}_{}\) for sentiment analysis tasks. Additionally, we evaluate the 3-shot ICL on question-answering using SQuAD [RZLL16] under an open-ended generation setting. The details of all prompt templates are presented in Appendix D. Then we concatenate the demonstration examples with newlines to delimit them. The prediction label is directly generated using greedy decoding given the demonstration examples and test cases in context. The prediction accuracy is used as the evaluation metric. We report the mean and standard deviation of 6 runs with different random seeds to assess the randomness in selecting k-shot demonstration examples. As mentioned previously, the chunk size controls the granularity of retrieved text chunks. Since the considered NLU datasets require more fine-grained labels from cached memory, we perform

    & **In-Context** & **In-Memory** &  &  \\  & **Len.** & & **Len.** & & & 100x-500x & 500x-1M & 1M} \\  GPT-2* & 1k & N/A & 22.78 & 24.39 & 24.12 & 24.97 & 18.07 & 11.05 \\ MemTRM & 1k & 65K & 21.77 & 23.56 & 23.23 & 24.16 & 17.39 & 10.81 \\ TRIME & 1k & 65K & 22.21 & 23.50 & 23.74 & 24.32 & 17.80 & 10.95 \\  LongMem & 1k & 65K & **21.29** & **23.01** & **22.55** & **23.35** & **16.71** & **10.05** \\   

Table 2: Evaluation results on long-context language modeling datasets. We report token-level perplexity (PPL) (lower the better) on all datasets.

    &  & **In-Context** & **In-Memory** & _{}\)**} \\  & & **Len.** & **Len.** & **crtx-4k** & **ctx-6k** & **ctx-8k** \\  GPT-2-XL\({}^{}\)[RWC\({}^{+}\)19] & 1.5B & 1K & N/A & 24\% & 24\% & 24\% \\ GPT-3\({}^{}\)[BMR\({}^{+}\)20] & 175B & 2K & N/A & 28\% & 28\% & 28\% \\ LocalTRM\({}^{}\)[RSVG21] & 516M & 8K & N/A & 24\% & 24\% & 24\% \\ RoutTRM\({}^{}\)[RSVG21] & 490M & 8K & N/A & 25\% & 24\% & 24\% \\ Bigbird\({}^{}\)[ZGD\({}^{+}\)20] & 128M & 4K & N/A & 26\% & 26\% & 26\% \\  GPT-2* & 407M & 1K & N/A & 18.4\% & 18.4\% & 18.4\% \\ MemTRM & 407M & 1K & \(\) & 28.3\% & 28.7\% & 28.7\% \\  LongMem & 558M & 1K & \(\) & **37.7\%** & **39.4\%** & **40.5\%** \\   

Table 3: Zero-shot SupfMix Identification Accuracy on AO3 subset of ChapterBreak. Baselines marked with \({}^{}\) are directly cited from [ST22]. The MemTRM and LongMem loads the given 4k/6k/8k prefix contexts into cached memory, while the input length to local context is still 1k tokens.

a hyperparameter selection on the validation set of SST-2, and the best chunk-size 2 is used to report the results for MemTRM, TRIME and our model.

**Results.** The results on in-context learning are summarized in Table 5 and Table 4. LongMem achieves remarkable improvements on all NLU tasks under the 20-shot sufficient in-context setting, with +5.6 average scores increase over pretrained GPT-2*, MemTRM, and TRIME. Meanwhile, LongMem also brings performance improvements on the 4-shot case. Additionally, LongMem improves the in-context learning capabilities of LLMs on open-ended generation tasks, with +4.5 EM score increase on SQuAD. The results indicate that having more demonstration examples loaded in cached memory can provide additional contextual cues to assist in-context learning. LongMem can utilize task-relevant knowledge from both local contextual demonstrations and in-memory augmented demonstrations, thereby achieving superior in-context learning capabilities.

### Ablation Studies

So far, we empirically verify the effectiveness and superiority of LongMem in utilizing cached memory for long-context modeling, long-context understanding, and many-shot in-context learning. Furthermore, we would like to investigate the extend to which the cached memory contributes to the long-context understanding capability of LongMem through an ablation study of removing memory augmentations. Besides, since the design of the cached memory bank involves several hyperparameters, such as memory size \(msz\) and chunk-size \(csz\), we conduct a series of ablation studies to evaluate the effects of those choices.

**Effects of Long-Term Memory Augmentation.** To evaluate the effects and contributions of memory augmentations, we set the memory-size to 0 and maintain the SideNet parameters during inference. The results of LongMem without memory augmentation are shown in Table 6 of Appendix. As expected, without augmented long-term memory, the vanilla model with only backbone LLM and SideNet only gains 59.4 average scores on ICL NLU tasks, which is a 7.3 average accuracy decrease due to the removal of memory augmentation.

**Effects of Chunk-Size.** As analyzed before, the chunk-size \(csz\) controls the granularity of retrieval and thus it may make a difference to tasks with requirements of fine-grained retrieval. We perform an ablation study on the effects of various chunk-size choices \(csz\{2,4,8\}\) for in-context learning and the results are presented in 4(a). The chunk size of 2 yields the best performance on in-context learning tasks on five NLU datasets, which is consistent with the property of NLU tasks with the requirement of fine-grained retrieval and fusion towards classification label tokens.

**Effects of Memory Size.** The memory size (msz) controls the capacity of the memory bank. In general, the memory size should be compatible with the average length of documents or contexts,

  
**Model** & **EM** & **F1** \\  GPT-2* & 22.28\({}_{2.3}\) & 30.78\({}_{2.0}\) \\ MemTRM & 22.84\({}_{3.5}\) & 32.65\({}_{2.8}\) \\ LongMem & 26.77\({}_{2.3}\) & 35.70\({}_{2.0}\) \\   

Table 4: Exact match (EM) and F1 scores of 3-shot (about 1k tokens) in-context learning on SQuAD. LongMem loads 200 extra demonstration examples into cached memory.

  
**Model** &  **In-Context** \\ **\#Demons.** \\  &  **In-Memory** \\ **\#Demons.** \\  &  **SST-2** \\ ACC\(\) \\  &  **MR** \\ ACC\(\) \\  &  **Subj** \\ ACC\(\) \\  &  **SST-5** \\ ACC\(\) \\  & 
 **MPQA** \\ ACC\(\) \\  & **Avg.** \\  Majority & N/A & N/A & 50.9 & 50.0 & 50.0 & 20.0 & 50.0 & 44.2 \\  GPT-2* & 4 & N/A & 68.31\({}_{1.6}\) & 64.71\({}_{2.5}\) & 51.9\({}_{4.2}\) & 31.4\({}_{4.4}\) & 61.51\({}_{1.8}\) & 55.6 \\ MemTRM & 4 & 2000 & 67.51\({}_{2.4}\) & 64.61\({}_{1.3}\) & 53.2\({}_{6.0}\) & 29.6\({}_{4.4}\) & 63.01\({}_{2.1}\) & 55.6 \\ TRIME & 4 & 2000 & 69.51\({}_{4.5}\) & 63.89\({}_{8.8}\) & 51.5\({}_{1.5}\) & 31.8\({}_{6.7}\) & 63.61\({}_{2.9}\) & 56.0 \\ LongMem & 4 & 2000 & **71.8\({}_{1.4}\)** & **65.11\({}_{1.0}\)** & **53.8\({}_{3.7}\)** & **36.0\({}_{6.8}\)** & **65.4\({}_{12.8}\)** & **58.4** \\  GPT-2* & 20 & N/A & 68.21\({}_{1.5}\) & 63.45\({}_{2.5}\) & 57.61\({}_{0.2}\) & 33.6\({}_{6.0}\) & 70.8\({}_{7.6}\) & 58.7 \\ MemTRM & 20 & 2000 & 65.1\({}_{9.6}\) & 65.19\({}_{3.5}\) & 58.21\({}_{0.6}\) & 31.9\({}_{6.3}\) & 72.7\({}_{7.4}\) & 58.6 \\ TRIME & 20 & 2000 & 74.31\({}_{3.9}\) & 71.52\({}_{5.5}\) & 57.51\({}_{1.4}\) & 33.0\({}_{4.6}\) & 69.8\({}_{7.8}\) & 61.1 \\ LongMem & 20 & 2000 & **78.0\({}_{14.1}\)** & **78.6\({}_{3.3}\)** & **65.6\({}_{8.5}\)** & **36.5\({}_{7.5}\)** & **74.6\({}_{7.3}\)** & **66.7** \\   

Table 5: Accuracy [%] of 4-shot and 20-shot ICL on 5 NLU tasks (SST-2, mr, subj, SST-5, mpqa). We sample 2000 extra demonstration examples and load them into cached memory. The subscript is the standard deviation across 6 runs. Avg. refers to the average accuracy on 5 datasets.

_i.e.,_ a set of books with average 16k tokens should deploy the memory size of 16k tokens in cached memory. The training \(msz\) of 65 tokens is excessive for downstream tasks such as ChapterBreak as the whole prefix context length does not exceed 65k tokens. Thus, we perform an ablation study on the effects of memory size \(msz\{8k,16k,32k,65k\}\) during the inference stage on the PG-22 language modeling datasets and the results are shown in 4(b). To model the books with lengths of 8k-50k, the smaller memory size \(16k\) which is consistent with the average length of target books yields the best perplexity.

## 4 Related Work

**Large Language Models.** Large Language Models, _i.e.,_ GPT-3 , LLAMA , GPT-4 , significantly revolutionized NLP research and promoted the state-of-the-art of various language understanding, language generation , and even vision-language tasks . Additionally, enabled by multi-task instruction tuning , LLMs exhibit "emergent abilities"  like mathematical reasoning , code completion , etc.

**x-formers.** To enable transformers to attend on longer context, many variants of "x-formers" are proposed. Transformer-XL  proposes to cache attention keys and values of past segment and reuse them in recurrent manner. Recent seminal works of x-formers, including LinFormer , LongFormer , Routing Transformer , proposed various sparse attention mechanisms for decreasing \(O(n^{2})\) complexity to \(O(n n)\) or even \(O(n)\). BigBird  achieves a 4k sequence length via attending on a subset of context tokens. Although these x-formers achieve substantial efficiency improvements, such efficiency gains are not remarkable when modeling sequences that spans book-level length. Moreover, the largest sequence length of these methods is still upper-bounded by 16k tokens, making them invalid in modeling long-sequences at the book or wikipedia-page level (_i.e.,_ average 70k tokens for full-length books in PG19 dataset ).

**Side-Tuning.** The method of Side-Tuning  is a task-specific tuning method for pre-trained models via training a lightweight side-network that is fused with the fixed pre-trained network via summation. Our method inherits the idea of adopting a side-network but distinguishes the side-tuning method in terms of learning objective and cross-network fusion ways. LongMem proposes to augment LLMs with decoupled memory to retrain information from long past inputs without any task-specific tuning. The cross-network residual connections introduced here are novel and distinct from the vanilla summation used in Side-Tuning.

## 5 Conclusion

In this paper, we propose to augment LLMs with long-term memory for enabling them to memorize long-form context and gain long-form memory. The designed decoupled memory module can cache attention key and value pairs of past inputs for future retrieval and fusion. A decoupled residual SideNet is introduced as the memory retriever and reader, meanwhile the LLM itself is frozen and works as knowledge and memory encoder. Experiments on various long-contextual language modeling datasets demonstrate the effectiveness of our model over other memory-augmentation baselines. The proposed method can also enable in-context learning of LLMs to overcome the limited number of demonstration examples in context, which is constrained by the contextual length, via caching thousands of auxiliary demonstration examples in memory.

Figure 4: (a) Accuracy on 5 NLU datasets given different chunk size during inference; (b) \(\)Perplexity on 4 splits of PG-22 given different memory size during inference, in which the perplexity when \(msz\)=65k is used as baseline.