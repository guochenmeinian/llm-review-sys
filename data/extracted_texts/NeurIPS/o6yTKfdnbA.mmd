# Recursion in Recursion: Two-Level Nested Recursion

for Length Generalization with Scalability

 Jishnu Ray Chowdhury Cornelia Caragea

Computer Science

University of Illinois Chicago

jraych2@uic.edu cornelia@uic.edu

###### Abstract

Binary Balanced Tree Recursive Neural Networks (BBT-RvNNs) enforce sequence composition according to a preset balanced binary tree structure. Thus, their non-linear recursion depth (which is the tree depth) is just \(_{2}n\) (\(n\) being the sequence length). Such logarithmic scaling makes BBT-RvNNs efficient and scalable on long sequence tasks such as Long Range Arena (LRA). However, such computational efficiency comes at a cost because BBT-RvNNs cannot solve simple arithmetic tasks like ListOps. On the flip side, RvNN models (e.g., Beam Tree RvNN) that do succeed on ListOps (and other structure-sensitive tasks like formal logical inference) are generally several times more expensive (in time and space) than even Recurrent Neural Networks. In this paper, we introduce a novel framework -- Recursion in Recursion (RIR) to strike a balance between the two sides - getting some of the benefits from both worlds. In RIR, we use a form of two-level nested recursion - where the outer recursion is a \(k\)-ary balanced tree model with another recursive model (inner recursion) implementing its cell function. For the inner recursion, we choose Beam Tree RvNNs. To adjust Beam Tree RvNNs within RIR we also propose a novel strategy of beam alignment. Overall, this entails that the total recursive depth in RIR is upper-bounded by \(k_{k}n\). Our best RIR-based model is the first model that demonstrates high (\( 90\%\)) length-generalization performance on ListOps while at the same time being scalable enough to be trainable on long sequence inputs from LRA (it can reduce the memory usage of the original Beam Tree RvNN by hundreds of times). Moreover, in terms of accuracy in the LRA language tasks, it performs competitively with Structured State Space Models (SSMs) without any special initialization - outperforming Transformers by a large margin. On the other hand, while SSMs can marginally outperform RIR on LRA, they (SSMs) fail to length-generalize on ListOps. Our code is available at: https://github.com/JRC1995/BeamRecursionFamily/.

## 1 Introduction

Non-linear Recurrent Neural Networks (RNNs)  have the potential to enable powerful non-linear sequential dynamics that dynamically adapt with the input length. However, this ability that makes them powerful also causes several problems. First, RNNs are slow because of their non-linear sequential operations that scale linearly with the input sequence length. Second, backpropagation through arbitrarily large non-linear recurrence depth (depending on the input length) can lead to vanishing/exploding gradients  - or can have stability issues overall . Potentially, because of the combination of these factors, it was found that fast convolution-based models  with fixed non-linear depth could often do just as well as an RNN. Transformers  are built upon this trajectory replacing bounded kernel window in convolution with an attentionmechanism that can dynamically compute interaction weights for input tokens at any arbitrary distance.

However, despite the tremendous success of Transformers in all manner of tasks, they tend to struggle in hierarchical structure-sensitive tasks that require dynamic adaptation to input complexity and modeling of long-range dependencies [93; 82; 35; 92]. Recently, a family of models closely connected to linear RNNs [31; 73; 86; 66; 33; 30] have been emerging. Most of these models greatly outperform Transformers in long-range datasets [31; 86]. Instead of adding non-linearity in every recurrent step, they simply stack a fixed number of linear recurrences with non-linearities in between. However, in doing so these models lose the ability to adapt the non-linearity depth to input. This can make it harder to tackle tasks requiring adaptation to unseen lengths of nested non-linear operations.

In this paper, we take a step back and look again at non-linear RNNs. Non-linear RNNs, more generally, can be seen as a special case of (non-linear) Tree Recursive Neural Networks (Tree-RvNNs) [76; 87; 89; 91]. Tree-RvNNs can process sequences in an arbitrary tree-structured order. Non-linear RNNs are a special case of Tree-RvNNs following a chain-structured tree. However, such a chain-structured tree is an extreme case - requiring the maximum tree traversal height. On the other side of the extreme are balanced tree models. For instance, the height of a \(k\)-ary balanced tree with \(n\) terminal nodes (the input length for our case) would be \(log_{k}(n)\). Balanced Tree-RvNNs [68; 84] can then logarithmically scale with the sequence length (\(n\)). To an extent, this can greatly mitigate the original issues of non-linear RNNs while still preserving some degree of ability to dynamically adapt with input length. Fascinatingly, we find that a simple model like a Balanced Binary Tree RvNN (BBT-RvNN) with a modern recursive cell [82; 13] can come close to the performance of the recent strong linear RNNs or Structured State Space Models (SSMs) without much tuning or any specialized initialization on Long Range Arena (LRA) text datasets  - thereby, greatly outperforming Transformers in general.

Nevertheless, while BBT-RvNNs can be quite fast and scalable, they are incapable of excelling (or even passing) simple structure-sensitive tasks like ListOps1 or formal logical inference  in a length-generalizable manner. Transformers were already shown to underperform in these tasks [82; 93], and we find that S4D  (a representative of linear RNN/SSM model family) and MEGA  (a hybrid of Transformer and SSM) also struggle in these tasks. Looking back at Tree-RvNNs, there are several models [82; 13; 77] that dynamically learn the tree structure from the input (instead of using some enforced heuristics) and determine the order of operations accordingly. They have been able to perform well in structure-sensitive tasks. Thus, BBT-RvNNs enjoy high speed and computational efficiency but at the cost of systematic issues in modeling task structures as revealed in ListOps or logical inference tasks, whereas in sharp contrast, the more powerful RvNNs [82; 13; 77] enjoy their strong length-generalizing accuracy in ListOps, logical inference, and reasonable performance in natural language domain but at the cost of high computational resource and inability to effectively train on datasets like LRA.

To address the above challenges, we propose a new framework called Recursion in Recursion (RIR) in which we have a nested level of recursion. We use a balanced \(k\)-ary Tree-ordered recursion in the outer loop and we use a strong RvNN (particularly we use a Beam Tree RvNN-based model ) in the inner loop to process the \(k\) arguments sent in from the outer loop. The inner RvNN can scale linearly with input size but now that size is bounded by a fixed constant \(k\). Thus, the total non-linear depth will be bounded by a linear factor of \(k_{k}(n)\) (the total outer loop being \(_{k}(n)\) and the inner loop being \(k\)). RIR makes Beam Tree RvNN (BT-RvNN) much faster and trainable on LRA but at the cost of some accuracy loss. On the flip side, RIR can be seen as a slower version of more basic balanced tree models but at the gain of length-generalizability in ListOps and general accuracy gain in several other datasets. In addition, we identify an issue related to beam alignment in incorporating BT-RvNN within RIR and propose a strategy to circumnavigate it.

## 2 Preliminaries

**Tree Recursive Neural Networks (Tree-RvNN)**: Here we focus on constituency Tree-RvNNs [89; 91] that build up higher-order constituent representations of a given input in a bottom-up manner. Particularly, given an input sequence of vectors, Tree-RvNNs treat them as terminal nodes of a latent tree to be built up. The non-terminal nodes of the tree are filled sequentially by composing their immediate child nodes using some recursive cell function - \(R:I\!\!R^{d} I\!\!R^{d}\!\!R^{d}\) (we consider binary trees here so \(R\) will always have only \(2\) children as arguments). Given some input sequence of vectors \(7+9 5-2\) (assume each of the symbols correspond to some vector \( I\!\!R^{d}\)), an example of Tree-RvNN-like processing can be expressed as:

\[R(R(R(7,+),R(R(9,),5)),R(-,2))\] (1)

Let us say \(R\) is considered as a single non-linear neural layer. In that case, the total non-linear depth of this processing can be interpreted as the maximum nested \(R\) operations which is \(4\) in Eqn. 1. RNNs can be now seen as a special case where the order of operation is always like this:

\[R(R(R(R(R(R(R(R(h_{0},7),+),9),),5),-),2)\] (2)

Here, \(h_{0}\) is the initial hidden state for RNNs. As we can see, because of their chain-like structure RNNs will have the maximum possible recursion depth within the Tree-RvNN framework. In this case, the depth is \(7\). Another special case is BBT-RvNN (Balanced Binary Tree RvNN) which follows an enforced binary-balanced tree-based order of operations and can be expressed as:

\[R(R(R(7,+),R(9,)),R(R(5,-),2))\] (3)

Here the maximum non-linearity depth is only \(3\) (i.e., \(_{2}(n)\); where \(n=7\)). With longer sequences the differences between RNNs and BBT-RvNNs will grow bigger because BBT-RvNNs can scale logarithmically to the input length.

**Greedy Search Tree RvNN:** If our Tree-RvNN does not employ any fixed heuristic tree structure (as in RNNs or BBT-RvNNs) and if we are not relying on an external system or user to provide tree structures, we have to decide on some policy to automatically induce the latent tree structure from raw sequences. Greedy search Tree RvNNs provide us with one way of doing that. The basic idea of Greedy Search Tree RvNN can be found in multiple works  and is related to easy-first parsing . A general framework for greedy search parsing is as follows. Assume we are at some recursion step \(t\). Let the input sequence of node representations in that step be \((h_{1}^{t},h_{2}^{t},,h_{K}^{t})\) (where \(h_{i}^{t} I\!\!R^{d}\)). Assume we have the recursive binary cell \(R\) as before and a neural \(scorer\) function of the form \(scorer:I\!\!R^{2 d_{s}}\!\!R\) (where \(d_{s}<d\); the role of \(d_{s}\) will be clarified). Next, we consider all possible node pairs that can be built into a parent node. Note that for simplicity we maintain a projective tree constraint as is often standard  - so only contiguous node pairs \((h_{i}^{t},h_{i+1}^{t})\) are considered as candidates for being composed into a higher-level parent node (whose children will be those nodes). The \(scorer\) assigns a scalar score to every candidate node pair as: \(e_{i}=score(h_{i}^{t}[0:d_{s}],h_{i+1}^{t}[0:d_{s}])\). The slicing above is done for efficiency reasons (we set \(d_{s}=64\) and generally keep it smaller than \(d\), i.e., \(d_{s}<d\)) following . Let \(e_{1:k}\) represent the sequence of all scores. Assume \(j\) is the position of the first node in the maximum scoring pair, i.e., \(j=argmax(e_{1:k})\). Now the update rule for the input to the next recursion will be:

\[h_{i}^{t+1}=h_{i}^{t}&i<j\\ R(h_{i}^{t},h_{i+1}^{t})&i=j\\ h_{i+1}^{t}&i>j\] (4)

The iterative application of this rule can achieve both tree-parsing (through \(scorer\)) and sequence composition (by application of \(R\)) simultaneously. Finally, we will have the root representation left which can be used as a sentence encoding (the intermediate non-terminal nodes can be also used if needed). Note that the framework we describe already incorporates the efficiency fixes introduced in . Works  prior to  made the scorer's input to be \(R(h_{i}^{t},h_{i+1}^{t})\) which requires running expensive recursive cells for all possible node pair siblings (not just the chosen one) in parallel and they use no slicing. A problem with directly using the above framework is that the argmax for selecting \(j\) is non-differentiable. So it becomes impossible to train the \(scorer\). Various techniques have been used to address this problem - e.g., with autoencoder loss , gumbel softmax  with Straight-through Estimation (STE) , or with SPIGOT . Although not all of them have been exhaustively explored, STE gumbel softmax  which is one of the more popular approaches has been found to fail in structure-sensitive tasks . However, viewing the above family of approaches as a greedy search approach - opens up another way that is using simple beam search replacing argmax with a top-\(k\) operator for beam search . We discuss this below.

**Beam Search Tree RvNN:** Extending Greedy Search Tree RvNNs with beam search allows for keeping multiple hypotheses or beams. Each beam would be one possible induced tree structure with its node representations and a corresponding beam score (addition of the log-softmax of \(e_{j}^{t}\) for each iteration \(t\) in that beam). Setting up a beam-score-based softmax-based marginalization over the root representations of each beam in the end allows us to create a sentence vector representation: \(_{i})}{_{j}(s_{j})} b_{i}\) where \(b_{i}\) is the root representation of beam \(i\) and \(s_{i}\) is the beam score. The final softmax can allow competition in the top \(B\) selected beams (where \(B\) is the beam width) which can allow meaningful gradient signals to reach the scorer. Ray Chowdhury and Caragea  showed that this is enough to make the former framework succeed in ListOps and other logical inference tasks. We use the efficient variant Beam Tree RvNN (BT-RvNN), i.e., Efficient Beam Tree RvNN (EBT-RvNN) which is BT-RvNN built on the specific greedy framework described above rather than the prior frameworks  that we contrasted. Both BT-RvNN and EBT-RvNN use stochastic Top-\(K\). More details on BT-RvNN can be found in  and more details on EBT-RvNN can be found in .

**Balanced Tree RvNNs:** Above, we focused on Binary (2-ary) Tree RvNNs. Here we consider a more general \(k\)-ary Tree RvNN. The non-terminal nodes of \(k\)-ary Tree RvNN can have at most \(k\) children. Since we defined the recursive cell \(R\) to have two parameters, we now introduce a generalized form of the recursive cell - \(RK:^{k d_{k}}^{d_{k}}\). \(RK\) takes a sequence of \(k\) vectors as input and outputs a single vector. Essentially \(RK\), by itself, operates like a sentence encoder. Next, we introduce a balanced tree RvNN model that uses a \(k\)-ary recursive cell.

We call the balanced tree variant of a \(k\)-ary Tree RvNN as Balanced \(k\)-ary Tree-RvNNs (BKT-RvNNs). Balanced Binary Tree RvNNs (BBT-RvNNs) is a special case of BKT-RvNN when \(k=2\). In each recursion, BKT-RvNNs make non-overlapping chunks of size \(k\) (so we can also say \(k\) is the "chunk size" henceforth) and then compress each chunk into a single vector using \(RK\). This is done in every recursion until there is only one vector left - which becomes the root node representation, i.e., the final sentence encoding. Let us say, we have a sequence (\(r_{1:n}^{n}}\)) of the form below:

\[r_{1:n}=r_{1},r_{2},,r_{n}\] (5)

Assume that \(n\) is divisible by \(k\) - if not, we can always use padding to extend the sequence length to make it divisible. We can then create a chunked sequence (\(c_{1:}^{ }}\)) as below:

\[c_{1}=(r_{1},r_{2},,r_{k}),\ \ c_{2}=(r_{k+1},,r_{2k}),,\ \ c_{}=(r_{(-1) k+1},r_ {(-1) k+2},,r_{n})\] (6)

Each chunk (\(c_{i}\)) can be then processed independently (and simultaneously) by \(RK:^{k d_{k}}^{d_{k}}\) to compress the \(k\)-sized chunks into single vectors:

\[p_{1}=RK(c_{1}),p_{2}=RK(c_{2}),,p_{}=RK(c_{ })\] (7)

Thus the sequence of chunks, \(c_{1:}^{ }}\) turns into a sequence of vectors \(p_{1:}^{ }}\). \(p_{1:}\) then becomes the input to the next recursion. We terminate the recursion when only one item is left in the sequence. Since in every recursion the sequence size is cut by \(k\) times, the total recursions taken (until a sequence of size \(1\) is left) would be \(_{k}n\) assuming \(n\) as the original sequence length.

## 3 Recursion in Recursion

Here, we introduce our framework - Recursion in Recursion (RIR). RIR is essentially an implementation of BKT-RvNN. The question to ask when using BKT-RvNN is what to use to implement \(RK\). In case of RIR, we use another Tree-RvNN to implement \(RK\). This results in a two-level nested recursion as discussed below. We present a visualization in Figure 1 and explain the two levels below.

**Outer Recursion:** The outer recursion (outer RvNN) is the BKT-RvNN setup as discussed.

**Inner Recursion:** In RIR, the inner recursion is another RvNN (inner RvNN). In principle, we can use nearly anything for the inner RvNN or the \(RK\) function in general - even a Transformer  oran RNN. Using an RNN makes the whole model similar to Sliced RNN . Our main RIR-based model uses EBT-RvNN as the inner RvNN. We motivate this choice below.

**Motivation for EBT-RvNN:**

1. BT-RvNN performs quite well on logical inference and ListOps while maintaining reasonable natural language performance - a feature lacking in plain RNNs, Gumbel-Tree variants, or other simpler RvNNs . EBT-RvNN is a more promising and efficient variant of BT-RvNN .
2. EBT-RvNN as an inner RvNN (after some adjustments to be discussed in the next section) offers us a clean interpretation of the RIR model as an approximation of EBT-RvNN searching in an additionally constrained space of tree structures (constrained by the outer balanced tree recursion). While other ListOps competent models can be also forcibly used as an inner RvNN [13; 82], there will be less of an interpretation of their modeling choices under RIR.

**Time Complexity Analysis:** The time complexity of the outer RvNN (i.e. BKT-RvNN) in RIR (if the inner RvNN were \((1)\)) is \((log_{k}(n))\) following what we already discussed. Let us say that the time complexity of the inner RvNN is some \((f(n))\) if \(n\) is the input sequence length. Given these factors, the overall time complexity of an RIR model will be \((_{k}(n)f(min(k,n)))\). While inner RvNNs like EBT-RvNN can be slow by itself, within the RIR framework its recursive depth is now bound by the chunk size \(k\) instead of the sequence length \(n\). The chunk size \(k\) is a hyperparameter that is preset as a constant before training. It remains constant during training no matter how big the sequence length \(n\) becomes. As such although the inner RvNN can be expensive it now becomes bound by constant factors during training. This makes the RIR model with EBT-RvNN as inner RvNN much more efficient than EBT-RvNN itself. The same is also true for other possible implementations of an inner RvNN within the RIR framework.

However, the inner recursion with EBT-RvNN can be still relatively slow and add large overheads. Thus, the bigger the \(k\), the slower the overall model will be. On the other hand, lower \(k\) pushes the model closer to BBT-RvNN and makes it faster. When \(k=2\), the model would just collapse into BBT-RvNN, and when \(k=\), the model reduces into just EBT-RvNN.

### Beam Alignment

**Context:** There are a few challenges to naively integrating EBT-RvNN in the RIR framework. First, note that we initially defined \(RK\) as of the form \(RK:^{k d_{h}}^{d_{h}}\). If we keep it that way then we have to marginalize the beams at the end of every inner recursive loop to get a single vector output instead of beams of vectors. This can make it harder to extract discrete structures from the

Figure 1: Visualization of the Recursion In Recursion (RIR) framework. The sequence (\(x_{1},x_{2},,x_{9}\)) is the input sequence of vectors. The RvNN block indicates the inner RvNN. \(R\) is the recursive cell within the inner RvNN. The inner RvNN block is working within a balanced \(k\)-ary Tree like structure (here \(k=3\)) - effectively serving as the recursive cell within BKT-RvNN.

beam search process if needed for interpretability or for sending top-down attention signals for token contextualization (as used in  with parent attention). Also, it is in conflict with the second motivation for using EBT-RvNN. Moreover, keeping the beams active instead of marginalizing them every so often can allow for more robustness and recovery from error . Now, if the beams are kept active, there are two main changes.

1. The input sequence in every outer recursion would belong to \(I\!\!R^{b n d_{h}}\) where \(b\) is the beam size. The output sequence after the outer recursion should be in \(I\!\!R^{b d_{h}}\). In addition, we need to also maintain an input of beam scores \( I\!\!R^{b}\) and an output of the same form.
2. Each chunk that is fed to \(RK\) should be now of the form \(I\!\!R^{b k d_{h}}\). In addition, the beam scores \( I\!\!R^{b}\) are also fed to \(RK\). The output of \(RK\) should be in the form of a tuple: \((BS,BR)\) where \(BS I\!\!R^{b}\) represents the list of beam scores for each beam and \(BR I\!\!R^{b 1 d_{h}}\) is the list of corresponding chunk-level sentence encodings2 for each beam. Overall \(RK\) is to be redefined as: \(RK:I\!\!R^{b} I\!\!R^{b k d_{h}} I\!\!R^{b}  I\!\!R^{b 1 d_{h}}\). The first input argument represents the input beam scores, the second represents the beam of chunks.

**Problem:** Now, the output of each \(RK\) is a sequence \( I\!\!R^{b 1 d_{h}}\) and beam scores \( I\!\!R^{b}\). Since there would be \(\) chunks (if the input sequence length to that level of outer recursion is \(n\)), there will be simultaneous \(\) sequences \( I\!\!R^{b 1 d_{h}}\) and similarly, \(\) lists of beam scores \( I\!\!R^{b}\). The challenge is to answer how we get a single sequence \( I\!\!R^{b d_{h}}\) and a single list of beam scores \( I\!\!R^{b}\) from this circumstance to serve as inputs to the next outer recursion.

For simplicity, let us say \(=2\) and as such, we have two chunk outputs: \((BS_{1},BR_{1})\) and \((BS_{2},BR_{2})\). The question that is now raised is: How do we combine the different beams of the different output tuples for the input to the next outer recursion?

**String-it Solution:** One answer can be to just string together (concatenate) the corresponding beams in \(BR_{1}\) and \(BR_{2}\) as: \(BR_{new}=([BR_{1},BR_{2}],dim=1)\). In other words, we concatenate \(BR_{1}[i]\) with \(BR_{2}[i]\) for all \(i\{0,1,,b-1\}\). Here, \(BR_{new} I\!\!R^{b 2 d}\). The beam scores can be summed up for accumulation: \(BS_{new}=BS_{1}+BS_{2}\).

**Problem with the String-it Solution:** The choice above is arbitrary. While concatenating the top scoring beams (the first beam of every \(BR\)) can be motivated, there is no reason to disallow beams from different ranks (say \(BR_{1}[i]\) and \(BR_{2}[j]\) where \(i j\)) from being concatenated together especially if they form a unique beam with higher combined scores than others.

**Beam Alignment Solution:** Given the above problem, we introduce our solution - the Beam Alignment technique. In this technique, we apply a simple stochastic approach following the idea that "higher scoring beams should have higher chance to exist in the combined beams"3. Based on this idea, we simply transform each \((BS_{l},BR_{l})\) into some \((BS_{l}^{},BR_{l}^{})\) (we explain this below) and then apply the string-it solution on the transformed outputs \((BS_{l}^{},BR_{l}^{})\) to get the input for the next outer recursion. The transformation works as follows:

\[idx_{l}=(=[0,1,,b-1],\] \[=(BS_{l}),\] (8) \[=b)\]

That is, first, we sample the indices (\(idx_{l}\)) of beams based on their corresponding scores (\(BS_{l}\)). The scores \(BS_{l}\) can be normalized with softmax. We sample \(b\) times with repetition. The next steps are:

\[BS_{l}^{}=[BS_{l}[j]jidx_{l}];\ \ BR_{l}^{}=[BR_{l}[j]j idx_{l}]\] (9)

Thus, beams with higher scores will be sampled more times and in turn will have a higher chance to be present in \(BS_{l}^{}\) and \(BR_{l}^{}\), and in turn, more likely to be present in the stringed-together beams \(BS_{new}\) and \(BR_{new}\) after applying the string-it solution. As such, the high-level idea of increasing the chance of higher scoring beams is implemented4. One additional detail: in practice, we enforce \(BS_{new}\) and \(BR_{new}\) to be the concatenation of the top beams from the pre-transformed materials (\(BS_{l},BR_{l}\)) so that the best possible stringed beam is guaranteed to be present. We provide additional intuition on beam alignment with added visualization in Appendix I.

### Pre-Chunk Processing

The chunking used in BKT-RvNN does not take into account the input content. This can create bad chunks where the information necessary to process one chunk is in another chunk. For example, imagine what happens if we split MAX(1,2,3,4) into three chunks [MAX(1], , and [)] (where the square brackets indicate chunk boundaries). There would be no information initially for the later chunks to work on. To mitigate that we use an initial (single) linear RNN layer bidirectionally to propagate information outside chunks before starting the RIR loop. Specifically, we use an S4D with Gated Linear Units  (described more in Appendix J). Moreover, earlier literature has often used a recurrent layer to initialize the non-terminals before running their Tree-RvNN [12; 84] for more contextualized parsing decisions. So the S4D layer can be seen as serving that purpose too.

### Practical Tips and Points

The chunk size (\(k\)) for BKT-RvNN can effectively work as a trade-off parameter in the RIR framework. Lower chunk size can make the whole pipeline faster by having less work to do for the heavy inner-RvNN, but for the same reason the point of using EBT-RvNN (instead of a simpler RNN model) would start to get lost as we lower the chunk size. In Appendix C, we show that lowering chunk size drastically deteriorates the performance of our model in ListOps. A practical option is to set the chunk size as high as possible while meeting one's practical computational need. We generally set \(k=30\) for our purposes. We also discuss theoretical ways to extend the framework for language modeling in Appendix H.

**RIR Inference:** As mentioned before, EBT-RvNN runs within the RIR framework and can be interpreted as just EBT-RvNN with some additional constraints in its tree-search space. Thus, during inference, we can just run EBT-RvNN as it is without RIR given that the core model remains the same - now just being allowed to cover more space and gain enhanced performance. By default, we generally indeed do that during inference even if the EBT-RvNN model is trained under RIR however using RIR inference generally works just as well for less structure-sensitive tasks. We discuss more about this decision in Appendix D.

## 4 Experiments and Results

### Model Nomenclature

**RNN-GRC** represents an RNN implemented with Gated Recursive Cell (GRC) as the recursive cell \(R\) (GRC was introduced in  and discussed in Appendix J); **CRvNN** refers to Continuous Recursive Neural Network ; **OM** refers to Ordered Memory ; **BT-GRC** refers to **BT-RvNN** implemented with GRC ; **BT-GRC OS** refers to BT-GRC combined with OneSoft (OS) Top-\(K\) function ; **EBT-GRC** refers to EBT-RvNN model with GRC; **S4D** refers to stacked bidirectional S4D model implemented similar to S4D-inv in ; **MEGA** refers to a hybrid model combining Transformers and an SSM based on exponential moving average as introduced in ; **BBT-GRC** refers to BBT RvNN  but with GRC (it is intended to be a more basic baseline and does not have S4D as an initial layer), **RIR-GRC** refers to an RIR-based model using a recurrent GRC cell for the inner RvNN (it can be thought to be similar to sliced RNN ), **RIR-EBT-GRC** is our main proposed model. Both of the latter models use an S4D layer before starting the main RIR loop.

### Results

For more details on the architectures and experiments, see Appendix A. There are also more ablations in Appendix C.

**Efficiency Analysis:** In Table 1, we compare the empirical time-memory trade-offs of the most relevant Tree-RvNN models. We use CRvNN in the no halting mode as 5. As can be seen, our RIR-EBT-GRC is multiple times faster than the original BT-GRC models and even EBT-GRC

   &  \\ 
**Model** & \)} & \)} & \)} & \)} \\  & Time & Memory & Time & Memory & Time & Memory & Time & Memory \\  & (min) & (GB) & (min) & (GB) & (min) & (GB) & (min) & (GB) \\  RNN-GRC & \(0.22\) & \(\) & \(0.54\) & \(\) & \(1.3\) & \(0.03\) & \(2.5\) & \(\) \\ MEGA & \(0.06\) & \(0.07\) & \(0.05\) & \(0.16\) & \(0.05\) & \(0.29\) & \(0.05\) & \(0.72\) \\ S4D & \(0.04\) & \(0.03\) & \(0.05\) & \(0.05\) & \(0.04\) & \(0.07\) & \(0.05\) & \(0.14\) \\ BBT-GRC & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ RIR-GRC & \(0.06\) & \(\) & \(0.08\) & \(0.03\) & \(0.09\) & \(0.04\) & \(0.1\) & \(0.07\) \\   \\  OM & \(8.0\) & \(0.09\) & \(20.6\) & \(0.21\) & \(38.2\) & \(0.35\) & \(76.6\) & \(0.68\) \\ CRvNN & \(1.5\) & \(1.57\) & \(4.3\) & \(12.2\) & \(8.0\) & \(42.79\) & OOM & OOM \\ BT-GRC & \(1.1\) & \(1.71\) & \(2.6\) & \(9.82\) & \(5.1\) & \(27.27\) & OOM & OOM \\ BT-GRC OS & \(1.4\) & \(2.74\) & \(4.0\) & \(15.5\) & \(7.1\) & \(42.95\) & OOM & OOM \\ EBT-GRC & \(1.2\) & \(0.19\) & \(3.2\) & \(1.01\) & \(5.5\) & \(2.78\) & \(10.5\) & \(10.97\) \\ RIR-EBT-GRC & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: Empirical time and (peak) memory consumption for various models on an RTX A6000. Ran on \(100\) ListOps data with batch size \(1\) and the same hyperparameters as used on ListOps on various sequence lengths. For the splits of lengths \(200-1000\) we use the data shared by Havrylov et al. ; for the \(1500-2000\) split we sample from the training set of LRA listops 

 
**Model** & **near-IID** &  &  & **LRA** \\ (Lengths) & \(\) 1000 & 200-300 & 500-600 & 900-1000 & 100-1000 & 100-1000 & 2000 \\ (Arguments) & \(\) 5 & \(\) 5 & \(\) 5 & \(\) 5 & 10 & 15 & 10 \\  MEGA & \(75.45_{2.6}\) & \(45.2_{1.5}\) & \(31.7_{14}\) & \(24.7_{36}\) & \(39.5_{33}\) & \(33.8_{59}\) & \(25.8_{49}\) \\ S4D & \(72.33\) & \(31\) & \(20.85\) & \(14.7\) & \(27.3\) & \(22.76\) & 17.4 \\ BB-Tree GRC & \(59.18\) & \(43.6\) & \(40.4\) & \(31.5\) & \(45.35\) & \(44.5\) & \(38.25\) \\ RIR-GRC & \(78.33\) & \(41.75\) & \(35.55\) & \(32.3\) & \(43.8\) & \(42.75\) & 40.05 \\  OM & \(\) & \(99.6\) & \(92.7\) & \(76.9\) & \(\) & \(75.05\) & \(\) \\ CRvNN & \(99.82\) & \(99.5\) & \(98.5\) & \(98\) & \(65.45\) & \(45.1\) & \(55.38\) \\ BT-GRC OS & \(\) & \(99.5\) & \(99\) & \(97.2\) & \(76.05\) & \(67.9\) & \(71.8\) \\ EBT-GRC & \(\) & \(\) & \(\) & \(\) & \(82.95\) & \(\) & \(79.5\) \\  RIR-EBT-GRC & \(99.72\) & \(99.15\) & \(98.25\) & \(97.1\) & \(74.9\) & \(49.55\) & \(61.65\) \\ \(-\)S4D & \(99.78\) & \(99.15\) & \(98.87\) & \(98.6\) & \(63.95\) & \(47.73\) & \(48.25\) \\ \(-\)Beam Align & \(98.83\) & \(91.75\) & \(79.05\) & \(68.8\) & \(59.55\) & \(43.65\) & \(40.75\) \\  

Table 2: Accuracy on ListOps-O. For our models we report the median of \(3\) runs. Our models were trained on lengths \(\) 100, depth \(\) 20, and arguments \(\) 5. We bold the best results that do not use gold trees. Subscript represents standard deviation. We use the original training set  with the length generalization splits from Havrylov et al. , the argument generalization splits from Ray Chowdhury and Caragea , and the LRA test set from Tay et al. . As an example, \(90_{1}=90 0.1\)while using much less memory. As expected, because of still using a heavy inner loop model, it is not as competitive resource-wise against S4D, MEGA, BBT-GRC, or RIR-GRC. However, in our experiments, we find these competitors to fail on ListOps and logical inference whereas RIR-EBT-GRC still works. We present a pareto frontier analysis in the Appendix F.

**List Operations (ListOps):** Here we discuss the results of Table 2. This table is based on the original ListOps dataset . To disambiguate it from the ListOps used in LRA , we call this one ListOps-O. Similar to , we filtered all sequences of length \(>100\) from ListOps-O training set. ListOps-O involves training on much shorter samples than those in LRA ListOps but ListOps-O can be challenging in its own right because this allows investigating length-generalization prowess of neural networks. ListOps requires solving hierarchically nested lists of mathematical operations that neural networks, barring a few exceptions , generally struggle to solve. The different splits test the models in different out-of-distribution settings, e.g., unseen lengths, unseen number of arguments, or both. As can be seen from Table 2, RIR-EBT-GRC does fairly well in length-generalization (still getting \( 90\%\)) despite being perturbed within the RIR framework for efficiency. Its argument generalization performance is, however, hampered. Removing the S4D layer (used for pre-chunk preprocessing) from RIR-EBT-GRC (the \(-\)S4D row) does not actually harm length generalization but seems to harm argument generalization. Removing beam alignment from RIR-EBT-GRC (and just keeping the string-it solution) substantially deteriorates the performance showing the effectiveness of beam alignment (see \(-\)Beam Align row). MEGA, S4D and RIR-GRC perform reasonably in the near-IID6 setting compared to LSTMs and Universal Transformers  but does not come close to more powerful RvNNs. BBT-GRC also does not work well here.

   &  & \\
**Model** & **ListOps** & **Text** & **Retrieval** & **ListOpsMix** \\  MEGA\(\) & \(\) & \(\) & \(\) & — \\ S4D* & \(60.18_{3.5}\) & \(87.34_{2}\) & \(91.09_{\,1}\) & — \\  S4D & \(59.3_{0}\) & \(88.18_{0.4}\) & \(91.17_{\,6}\) & \(59.05_{2.5}\) \\ BBT-GRC & \(55.15_{1}\) & \(85.69_{0.7}\) & \(90.78_{1.6}\) & \(55.7_{15}\) \\ RIR-GRC & \(20.18_{5.7}\) & \(86.894_{1.3}\) & \(88.24_{10}\) & \(58.95_{6.5}\) \\  RIR-EBT-GRC & \(59.0_{4}\) & \(87.97_{1.6}\) & \(88.99_{5.2}\) & \(64.1_{3.5}\) \\ -S4D & \(59.53_{1}\) & \(88.13_{1.3}\) & \(89.74_{3.9}\) & \(}\) \\  

Table 4: Accuracy on LRA . * represents that the results are copied from  and \(\) represents that the results are copied from . For our models we report the mean of \(2\) runs. ListOpsMix combines the training set of ListOps-O and ListOps LRA; it keeps the dev set and test set of LRA. Subscript represents standard deviation. As an example, \(90_{1}=90 0.1\)

 
**Model** &  \\  & 8 & 9 & 10 & 11 & 12 \\  MEGA & \(89.7_{16}\) & \(83.62_{8.8}\) & \(76.9_{18}\) & \(70.45_{35}\) & \(63.85_{36}\) \\ S4D & \(87.6_{20}\) & \(79.4_{36}\) & \(72.32_{32}\) & \(65.01_{53}\) & \(59.48_{38}\) \\ BBT-GRC & \(77.7_{15}\) & \(72.5_{31}\) & \(67.4_{11}\) & \(64.0_{14}\) & \(57.5_{12}\) \\ RIR-GRC & \(86_{5.6}\) & \(64.89_{8.9}\) & \(47.35_{27}\) & \(46.45_{26}\) & \(45.92_{25}\) \\  OM & \(}\) & \(}\) & \(94.95_{2}\) & \(93.9_{2.2}\) & \(93.36_{6.2}\) \\ BT-GRC OS & \(97.03_{1.4}\) & \(96.49_{1.9}\) & \(95.43_{4.5}\) & \(}\) & \(}\) \\ EBT-GRC & \(97.12_{3}\) & \(96.5_{3.1}\) & \(94.95_{1.5}\) & \(93.87_{7.4}\) & \(93.04_{6.7}\) \\  RIR-EBT-GRC & \(96.84_{1.4}\) & \(96.05_{1.3}\) & \(}\) & \(}\) & \(93.36_{11}\) \\  

Table 3: Mean accuracy and standard deviation on the Logical Inference for \( 8\) number of operations after training on samples with \( 6\) operations. Our models were run \(3\) times on different seeds. Subscript represents standard deviation. As an example, \(90_{1}=90 0.1\)

**Logical Inference:** In Table 3, we show the results of our models in a formal logical inference task . This is another task where only stronger Tree-RvNN based models have been able to perform well. In this task the models are trained on samples with \( 6\) operators and we test their generalization performance on higher operators (similar to ). We find that RIR-EBT-GRC can keep up well with the state-of-the-art whereas others like MEGA, S4D, BBT-GRC, or RIR-GRC cannot.

**LRA and ListOpsMix:** In Table 4 we explore the NLP subset of LRA tasks. We find that BBT-GRC can already perform quite well in LRA without any special initialization (note that BBT-GRC does not use S4D), outperforming all non-hybrid Transformer models . RIR-EBT-GRC also shows high competence in LRA. It starts to get a bit slow on the Retrieval task so we reduced the chunk size to \(5\) which might reduce its performance there. While RIR-EBT-GRC uses S4D for pre-chunk processing, we also run a variant without any S4D (see row \(-\)S4D) and it performs just as well or better. This shows that the S4D layer is not playing a big role here. Curiously enough, RIR-EBT-GRC does worse on ListOps LRA test in the in-distribution setting (in Table 4), than it does in the OOD setting trying to generalize from shorter sequences with lower arguments in Table 2. We hypothesize that this might be because RIR-EBT-GRC learns more easily lower-length samples. To investigate whether RIR-EBT-GRC can gain more on LRA when samples of shorter length are present, we created another ListOps dataset - ListOpsMix which combines the ListOps-O training set and the ListOps LRA training set (ListOps LRA dev and test sets are used for validation and testing, respectively). As we can see RIR-EBT-GRC (with or without S4D) is particularly better at utilizing extra lower-length data than other models; however, it still underperforms compared with what EBT-GRC/OM can get with training purely on ListOps-O.

**Other Tasks:** We also ran our models in a number of other natural language tasks, e.g., natural language inference and sentiment classification. The results are in Appendix E. Generally RIR-EBT-GRC performs similarly to other RvNN-based models.

## 5 Conclusion

In this paper, we introduce the Recursion In Recursion (RIR) framework and beam alignment to make Efficient Beam-Tree RvNNs (EBT-RvNN) more scalable. We also show the power of more basic models like BBT-RvNNs on LRA. Even though they do not outperform S4D or some of the newer state-of-the-art , BBT-RvNNs are coming from a completely orthogonal family of models which, in recent times, have received limited attention. Moreover, RIR-EBT-RvNNs provide a better trade-off in maintaining competitive performance in LRA text-related tasks and robust length generalization in structure-sensitive tasks as shown in the Pareto Frontier plots in Appendix F. We intend our results to inspire further exploration of these models. We provide an extended discussion with related works in Appendix G.

## 6 Limitations

Our main method, RIR-EBT-GRC, although much more computationally efficient than other ListOps-competent RvNNs , is still quite a bit slower compared to BBT-GRC, RIR-GRC, MEGA, and S4D. Moreover, although it serves as a non-linear recursive model that can both scale to training on \( 1000\) sequence lengths and also length-generalize on ListOps and Logical Inference (from shorter samples), it has still difficulties learning from longer sequences.

## 7 Acknowledgments

This research is supported in part by NSF CAREER award #1802358, NSF IIS award #2107518, and UIC Discovery Partners Institute (DPI) award. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF or DPI. We thank our anonymous reviewers for their constructive feedback.