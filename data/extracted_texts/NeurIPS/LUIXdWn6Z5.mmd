# Risk-Sensitive Control as Inference

with Renyi Divergence

 Kaito Ito

The University of Tokyo

kaito@g.ecc.u-tokyo.ac.jp

&Kenji Kashima

Kyoto University

kk@i.kyoto-u.ac.jp

###### Abstract

This paper introduces the risk-sensitive control as inference (RCal) that extends CaI by using Renyi divergence variational inference. RCal is shown to be equivalent to log-probability regularized risk-sensitive control, which is an extension of the maximum entropy (MaxEnt) control. We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation, which reveals several equivalences between RCal, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. Moreover, based on RCal, we derive the risk-sensitive reinforcement learning (RL) methods: the policy gradient and the soft actor-critic. As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL, which means that RCal is a unifying framework. Furthermore, we give another risk-sensitive generalization of the MaxEnt control using Renyi entropy regularization. We show that in both of our extensions, the optimal policies have the same structure even though the derivations are very different.

## 1 Introduction

Optimal control theory is a powerful framework for sequential decision making . In optimal control problems, one seeks to find a control policy that minimizes a given cost functional and typically assumes the full knowledge of the system's dynamics. Optimal control with unknown or partially known dynamics is called reinforcement learning (RL) , which has been successfully applied to highly complex and uncertain systems, e.g., robotics , self-driving vehicles . However, solving optimal control and RL problems is still challenging, especially for continuous spaces.

Control as Inference (CaI), which connects optimal control and Bayesian inference, is a promising paradigm for overcoming the challenges of RL . In CaI, the optimality of a state and control trajectory is defined by introducing optimality variables rather than explicit costs. Consequently, an optimal control problem can be formulated as a probabilistic inference problem. In particular, maximum entropy (MaxEnt) control [6; 7] is equivalent to a variational inference problem using the Kullback-Leibler (KL) divergence. MaxEnt control has entropy regularization of a control policy, and as a result, the optimal policy is stochastic. Several works have revealed the advantages of the regularization such as robustness against disturbances , natural exploration induced by the stochasticity [7; 9], fast convergence of the MaxEnt policy gradient method .

On the other hand, the KL divergence is not the only option available for variational inference. In , the variational inference was extended to Renyi \(\)-divergence , which is a rich family of divergences including the KL divergence. Similar to the traditional variational inference, this extension optimizes a lower bound of the evidence, which is called the variational Renyi bound. The parameter \(\) of Renyi divergence controls the balance between mass-covering and zero-forcing effects for approximate inference . However, if we use Renyi divergence for CaI, it remains unclear how \(\) affects the optimal policy, and a natural question arises: what objective does CaI using Renyi divergence optimize?ContributionsThe contributions of this work are as follows:

1. We reveal that CaI with Renyi divergence solves a log-probability (LP) regularized risk-sensitive control problem with exponential utility  (Theorem 2). The order parameter \(\) of Renyi divergence plays a role of the risk-sensitivity parameter, which determines whether the resulting policy is risk-averse or risk-seeking. Based on the result, we refer to CaI using Renyi divergence as _risk-sensitive_ CaI (RCaI). Since Renyi divergence includes the KL divergence, RCaI is a unifying framework of CaI. Additionally, we show that the risk-sensitive optimal policy takes the form of the Gibbs distribution whose energy is given by the Q-function, which can be obtained by solving a soft Bellman equation (Theorem 3). Furthermore, this reveals several equivalence results between RCaI, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control .
2. Based on RCaI, we derive risk-sensitive RL methods. First, we provide a policy gradient method  for the regularized risk-sensitive RL (Proposition 7). Next, we derive the risk-sensitive counterpart of the soft actor-critic algorithm  through the maximization of the variational Renyi bound (Subsection 4.2). As the risk-sensitivity parameter vanishes, the proposed methods converge to REINFORCE  with entropy regularization and risk-neutral soft actor-critic , respectively. One of their advantages over other risk-sensitive approaches, including distributional RL , is that they require only minor modifications to the standard REINFORCE and soft actor-critic. The behavior of the risk-sensitive soft actor-critic is examined via an experiment.
3. Although the risk-sensitive control induced by RCaI has LP regularization of the policy, it is not entropy, unlike the MaxEnt control with the Shannon entropy regularization. To bridge this gap, we provide another risk-sensitive generalization of the MaxEnt control using Renyi entropy regularization. We prove that the resulting optimal policy and the Bellman equation have the same structure as the LP regularized risk-sensitive control (Theorem 6). The derivation differs significantly from that for the LP regularization, and for the analysis, we establish the duality between exponential integrals and Renyi entropy (Lemma 5).

The established relations between several control problems in this paper are summarized in Fig. 1.

Related workThe duality between control and inference has been extensively studied . Inspired by CaI,  reformulated model predictive control (MPC) as a variational inference problem. In , variational inference MPC using Tsallis divergence, which is equivalent to Renyi divergence, was proposed. The difference between our results and theirs is that variational inference MPC infers _feed-forward_ optimal control while RCaI infers feedback optimal control. Consequently, the equivalence of risk-sensitive control and Tsallis variational inference MPC is not derived, unlike RCaI. The work  proposed an EM-style algorithm for RL based on CaI, where the resulting policy is risk-seeking. However, risk-averse policies cannot be derived from CaI by this approach. Our framework provides the equivalence between CaI and risk-sensitive control both for risk-seeking and risk-averse cases.

Risk-averse policies are known to yield robust control , and risk-seeking policies are useful for balancing exploration and exploitation for RL . Because of these merits, many efforts have been devoted to risk-sensitive RL . In , risk-sensitive RL with Shannon entropy regularization was investigated. However, their theoretical results are valid only for almost risk-neutral cases. Our results imply that LP and Renyi entropy regularization are suitable for the risk-sensitive RL.

In , risk-sensitive control whose control cost is defined by Renyi divergence was investigated, and it was shown that the associated Bellman equation can be linearized. However, it is assumed that the transition distribution can be controlled as desired, which is not satisfied in general as pointed out in . On the other hand, our result shows that when the dynamics is deterministic, LP

Figure 1: Relations of control problems.

and Renyi entropy regularized risk-sensitive control problems are linearly solvable without the full controllability assumption of the transition distribution.

NotationFor simplicity, by abuse of notation, we write the density (or probability mass) functions of random variables \(x,y\) as \(p(x),p(y)\), and the expectation with respect to \(p(x)\) is denoted by \(_{p(x)}\). For a set \(S\), the set of all densities on \(S\) is denoted by \((S)\). Renyi entropy and divergence with parameter \(>0\), \( 1\) are defined as \(_{}(p):=_{\{u:p(u) >0\}}p(u)^{}u,\,D_{}(p_{1}\|p_{2}):=[_{\{u:p_{1}(u)p_{2}(u)>0\}}p_{1}(u)^{}p_{2}(u)^{1- }u\). For the factor \(\) of \(_{}\), we follow  because this choice is convenient for the analysis in Subsection 3.2 rather than another common choice \(1/(1-)\). We formally extend the definition of \(_{}\) to \(<0\). Denote the Shannon entropy and KL divergence by \(_{1}(p),\,D_{1}(p_{1}\|p_{2})\), respectively because \(_{ 1}_{}(p)=_{1}(p)\), \(_{ 1}D_{}(p_{1}\|p_{2})=D_{1}(p_{1}\|p_{2})\). For further properties of the Renyi entropy and divergence, see e.g., . The set of integers \(\{k,k+1,,s\}\), \(k<s\) is denoted by \([\![k,s]\!]\). A sequence \(\{x_{k},x_{k+1},,x_{s}\}\) is denoted by \(x_{k:s}\). The set of non-negative real numbers is denoted by \(_{ 0}\).

## 2 Brief introduction to control as inference

First, we briefly introduce the framework of CaI. For the detailed derivation, see Appendix A and . Throughout the paper, \(x_{t}\) and \(u_{t}\) denote \(\)-valued state and \(\)-valued control variables at time \(t\), respectively, where \(^{n_{x}}\), \(^{n_{u}}\), and \(_{L}()>0\). Here, \(_{L}\) denotes the Lebesgue measure on \(^{n_{u}}\). The initial distribution is \(p(x_{0})\), and the transition density is denoted by \(p(x_{t+1}|x_{t},u_{t})\), which depends only on the current state and control input. Let \(T>0\) be a finite time horizon. CaI connects control and probabilistic inference problems by introducing _optimality variables_\(_{t}\{0,1\}\) as in Fig. 2. For \(c_{t}:_{ 0},c_{T}: _{ 0}\), which will serve as cost functions, the distribution of \(_{t}\) is given by \(p(_{t}=1|x_{t},u_{t})=(-c_{t}(x_{t},u_{t})),\;t[\![0,T-1]\!]\) and \(p(_{T}=1|x_{T})=(-c_{T}(x_{T}))\). If \(_{t}=1\), then \((x_{t},u_{t})\) at time \(t\) is said to be "optimal." The control posterior \(p(u_{t}|x_{t},_{t:T}=1)\) is called the optimal policy. Let the prior of \(u_{t}\) be uniform: \(p(u_{t})=1/_{L}(), u_{t}\). Although this choice is common for CaI, the arguments in this paper may be extended to non-uniform priors. Then, for the graphical model in Fig. 2, the distribution of the optimal state and control input trajectory \(:=(x_{0:T},u_{0:T-1})\) satisfies

\[p(|_{0:T}=1) [p(x_{0})_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t}) ][p(_{T}=1|x_{T})_{t=0}^{T-1}p(_{t}=1|x_ {t},u_{t})]\] \[=[p(x_{0})_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})] (-c_{T}(x_{T})-_{t=0}^{T-1}c_{t}(x_{t},u_{t})).\] (1)

For notational simplicity, we will drop \(=1\) for \(_{t}\) in the remainder of this paper.

The optimal policy \(p(u_{t}|x_{t},_{t:T})\) can be computed in a recursive manner. To this end, define

\[_{t}(x_{t},u_{t}):=-_{t:T}|x_{t},u_{t})}{_ {L}()},\;\;_{t}(x_{t}):=- p(_{t:T}|x_{t}),\] (2)

which play a role of value functions. Then, the following result holds.

**Proposition 1**.: _Assume that \(_{L}()<\) and let \(_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+_{L}()\). Assume further the existence of density functions \(p(x_{0})\) and \(p(x_{t+1}|x_{t},u_{t})\) for any \(t[\![0,T-1]\!]^{1}\). Then, it holds that_

\[p(u_{t}|x_{t},_{t:T}=1)=(-_{t}(x_{t},u_{t})+ _{t}(x_{t})),\;\; x_{t},\; u_{t} ,\] (3)

Figure 2: Graphical model for CaI.

_where_

\[_{t}(x_{t}) =-[_{}(-_{t}(x_{t},u_{t})) u_{t}],\; t 0,T-1,\;\;_{T}(x_{T})=c_{T}(x_{T }),\] (4) \[_{t}(x_{t},u_{t}) =_{t}(x_{t},u_{t})-_{p(x_{t+1}|x_{t},u_{ t})}[(-_{t+1}(x_{t+1}))],\; t 0,T-1.\] (5)

The recursive computation (4), (5) is similar to the Bellman equation for the risk-seeking control. However, it is not still clear what kind of performance index the optimal trajectory \(p(|_{t:T})\) optimizes because (4) does not coincide with that of the conventional risk-seeking control. An indirect way to make this clear is variational inference. Let us consider finding the closest trajectory distribution \(p^{}()\) to the optimal distribution \(p(|_{0:T})\). The variational distribution is chosen as

\[p^{}()=p(x_{0})_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})_{t}(u_{t}|x_{ t}),\] (6)

where \(_{t}(|x_{t})()\) is the conditional density of \(u_{t}\) given \(x_{t}\) and corresponds to a control policy. Then, the minimization of the KL divergence \(D_{1}(p^{}()\|p(|_{0:T}))\) is known to be equivalent to the following MaxEnt control problem:

\[*{minimize}_{\{_{t}\}_{t=0}^{T-1}}\;_{p^{}() }[c_{T}(x_{T})+_{t=0}^{T-1}c_{t}(x_{t},u_{t})-_{1 }(_{t}(|x_{t}))].\] (7)

Especially when the system \(p(x_{t+1}|x_{t},u_{t})\) is deterministic, the minimum value of \(D_{1}(p^{}()\|p(|_{0:T}))\) is \(0\), and the posterior \(p(u_{t}|x_{t},_{t:T})\) yields the optimal control of (7). As mentioned in Introduction, this work uses Renyi divergence rather than the KL divergence. Moreover, we characterize the optimal posterior \(p(u_{t}|x_{t},_{t:T})\) more directly even for stochastic systems.

## 3 Control as Renyi divergence variational inference

In this section, we address the question of what kind of control problem is solved by CaI with Renyi divergence and characterize the optimal policy.

### Equivalence between CaI with Renyi divergence and risk-sensitive control

Let \(>-1\), \( 0\). Then, CaI using Renyi variational inference is formulated as the minimization of \(D_{1+}(p^{}()\|p(|_{0:T}))\) with respect to \(p^{}\) in (6). Now, we have

\[D_{1+}(p^{}\|p(|_{0:T}))= [ p^{}()^{1+}p(,_{0:T})^{-} ]}_{-()}+ p(_{0:T}).\] (8)

That is, CaI with Renyi divergence is equivalent to maximizing the above variational Renyi bound. Moreover, by (1), it holds that

\[[ p^{}()^{1+}p(,_{0:T})^{ -}]\] \[=[ p^{}()()_{t=0}^{T-1 }p(x_{t+1}|x_{t},u_{t})_{t}(u_{t}|x_{t})}{()}p(x_ {0})[_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})](-c_{T}(x_{T })-_{t=0}^{T-1}c_{t}(x_{t},u_{t}))})^{}]\] \[= p^{}() c_{T}(x_{T})+ _{t=0}^{T-1}c_{t}(x_{t},u_{t})+_{t}(u_{t}|x_{t}) +_{L}().\]

Consequently, we obtain the first equivalence result in this paper.

**Theorem 2**.: _Suppose that the assumptions in Proposition 1 hold. Then, for any \(>-1\), \( 0\), the minimization of \(D_{1+}(p^{}\|p(|_{0:T}=1))\) with respect to \(p^{}\) in (6) is equivalent to_

\[*{minimize}_{\{_{t}\}_{t=0}^{T-1}}\;_{p^{}()}[( c_{T}(x_{T})+_{t=0}^{T-1}c_{t} (x_{t},u_{t})+_{t}(u_{t}|x_{t}))].\] (9)

\(\)Problem (9) is a risk-sensitive control problem with the log-probability regularization \(_{t}(u_{t}|x_{t})\) of the control policy. Let \(()\) be the exponent in (9). Then, \([(())]=[()]+[()]+O(^{2})\), where \([]\) denotes the variance . Hence, \(>0\) (resp. \(<0\)) leads to risk-averse (resp. risk-seeking) policies. As \(\) goes to zero, the objective in (9) converges to the risk-neutral MaxEnt control problem (7).

### Derivation of optimal control and further equivalence results

In this subsection, we derive the optimal policy of (9) and give its characterizations. For the analysis, we do not need the non-negativity of the cost \(c_{t}\). We only sketch the derivation, and the detailed proof is given in Appendix B. Similar to the conventional optimal control problems, we adopt the dynamic programming. Another approach based on variational inference will be given in Subsection 4.2. Define the optimal (state-)value function \(V_{t}:\) and the Q-function \(_{t}:\) as follows:

\[V_{t}(x_{t}):=_{\{_{s}\}_{s=t}^{T-1}} _{p^{}(|x_{t})}[( c_{T}(x_{T})+_{s =t}^{T-1}c_{s}(x_{s},u_{s})+_{s}(u_{s}|x_{s})) ],\] (10)

\[_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+ _{p(x_{t+1}|x_{t},u_{t})}[ V_{t+1}(x_{t+1}) ],\ \ t 0,T-1,\] (11)

and \(V_{T}(x_{T}):=c_{T}(x_{T})\). Then, it can be shown that the Bellman equation for Problem (9) is

\[V_{t}(x_{t})=-[_{}(-_{t}(x_{t},u^{ }))u^{}]+_{_{t}(|x_{t}) ()}D_{1+}(_{t}(|x_{t})\|_{t}^{*}(|x_{ t})),\] (12)

where \(_{t}^{*}(u_{t}|x_{t}):=(-_{t}(x_{t},u_{t}))/ _{t}(x_{t})\), and the normalizing constant is assumed to fulfill \(_{t}(x_{t}):=_{}(-_{t}(x_{t},u^{ }))u^{}<\). Since \(D_{1+}(_{t}(|x_{t})\|_{t}^{*}(|x_{t}))\) attains its minimum value 0 if and only if \(_{t}(|x_{t})=_{t}^{*}(|x_{t})\), the unique optimal policy that minimizes the right-hand side of (12) is given by \(_{t}^{*}(|x_{t})\) and

\[V_{t}(x_{t})=-[_{}(-_{t}(x_{t},u^{ }))u^{}],\ \ _{t}^{*}(u_{t}|x_{t})=(-_{t}(x_{t},u_{t})+V_{t}(x_{t}) ).\] (13)

Because of the softmin operation above, the left equation in (13) is called the soft Bellman equation.

**Theorem 3**.: _Assume that \(_{}(-_{t}(x,u^{}))u^{ }<\) holds for any \(t 0,T-1\) and \(x\). Let \(>-1\), \( 0\). Then, the unique optimal policy of Problem (9) is given by (13). Especially when the dynamics is deterministic, i.e., \(p(x_{t+1}|x_{t},u_{t})=(x_{t+1}-_{t}(x_{t},u_{t}))\) for some \(_{t}:\) and the Dirac delta function \(\), it holds that_

\[_{t}(x_{t},u_{t})=c_{t}(x_{t},u_{t})+V_{t+1}(_{t}(x_{t}, u_{t})),\] (14)

_and the optimal policy of the MaxEnt control problem (7) solves the LP-regularized risk-sensitive control problem (9) for any \(>-1\), \( 0\). \(\)_

Assumption \(_{}(-_{t}(x,u^{}))u^{ }<\) is satisfied for example when \(c_{t}\) is bounded for any \(t 0,T\) and \(_{L}()<\). The linear quadratic setting also fulfills this assumption; see (16).

Theorem 3 suggests several equivalence results:

**RCal and MaxEnt control for deterministic systems.** First, we emphasize that even though the equivalence between _unregularized_ risk-neutral and risk-sensitive controls for deterministic systems is already known, our equivalence result for MaxEnt and regularized risk-sensitive controls is nontrivial. This is because the regularized policy \(_{t}^{*}\) makes a system stochastic even though the original system is deterministic, and for stochastic systems, the unregularized risk-sensitive control does not coincide with the risk-neutral control. This implies that the optimal randomness introduced by the regularization does not affect the risk sensitivity of the policy. This provides insight into the robustness of MaxEnt control . Note that  mentioned that the MaxEnt control objective can be reconstructed by the risk-sensitive control objective under the heuristic assumption that the cost follows a uniform distribution. However, this assumption is not satisfied in general. Our equivalence result does not require such an unrealistic assumption.

**RCal and optimal posterior.** Although the optimal posterior \(p(u_{t}|x_{t},_{t:T})\) yields the MaxEnt control for deterministic systems as mentioned in Section 2, it is not known what objective \(p(u_{t}|x_{t},_{t:T})\)optimizes for stochastic systems. Theorem 3 gives a new characterization of \(p(u_{t}|x_{t},_{t:T})\). By formally substituting \(=-1\) into (11), the Bellman equation for computing \(_{t}^{*}\) becomes (4), (5) for the optimal posterior \(p(u_{t}|x_{t},_{t:T})\). Note that even if the cost function \(c_{t}\) in (9) is replaced by \(_{t}\) in Proposition 1, \(\{_{t}^{*}\}\) is still optimal. Therefore, by taking the limit as \(-1\), the policy \(_{t}^{*}(u_{t}|x_{t})\) in Theorem 3 converges to \(p(u_{t}|x_{t},_{t:T})\), and in this sense, the policy \(p(u_{t}|x_{t},_{t:T})\) is risk-_seeking_.

**Corollary 4**.: _Under the assumptions in Proposition 1, it holds that_

\[_{-1}_{t}^{*}(u_{t}|x_{t})=(-_{t}(x_{t},u_{t })+V_{t}(x_{t}))=p(u_{t}|x_{t},_{t:T}=1),\] (15)

_where \(V_{t}\) and \(_{t}\) are given by (11), (13) with \(=-1\). \(\)_

**RCal for deterministic systems and linearly-solvable control.** For deterministic systems, by the transformation \(E_{t}(x_{t}):=(-V_{t}(x_{t}))\), the Bellman equation (14) becomes linear: \(E_{t}(x_{t})=(-c_{t}(x_{t},u^{}))E_{t+1}(_{t}(x_{t},u^{ }))u^{}\). That is, when the system is deterministic, the LP-regularized risk-sensitive control, or equivalently, the MaxEnt control is linearly solvable [15; 16; 44], which enables efficient computation of RL. Even for the MaxEnt control, this fact seems not to be mentioned explicitly in the literature.

**RCal and unregularized risk-sensitive control in linear quadratic setting.** Similar to the unregularized and MaxEnt problems [45; 46], Problem (9) with a linear system \(p(x_{t+1}|x_{t},u_{t})=(x_{t+1}|A_{t}x_{t}+B_{t}u_{t},_{t})\) and quadratic costs \(c_{t}(x_{t},u_{t})=(x_{t}^{}Q_{t}x_{t}+u_{t}^{}R_{t}u_{t})/2,\ c_{T}( x_{T})=x_{T}^{}Q_{T}x_{T}/2\) admits an explicit form of the optimal policy:

\[_{t}^{*}(u|x)=u|-(R_{t}+B_{t}^{}_{t+ 1}(I-_{t}_{t+1})^{-1}B_{t})^{-1}B_{t}^{}_{t+1}(I- _{t}_{t+1})^{-1}A_{t}x,\] \[(R_{t}+B_{t}_{t+1}(I-_{t}_{t+1})^{-1}B_{t})^{-1} .\] (16)

Here, \((|,)\) denotes the Gaussian density with mean \(\) and covariance \(\). The definition of \(_{t}\) and the proof are given in Appendix C. In general, the mean of the regularized risk-sensitive control deviates from the unregularized risk-sensitive control. However, in the linear quadratic Gaussian (LQG) case, the mean of the optimal policy (16) coincides with the optimal control of risk-sensitive LQG control without the regularization .

### Another risk-sensitive generalization of MaxEnt control via Renyi entropy

The Shannon entropy regularization \([-_{1}(_{t}(|x_{t}))]\) of the MaxEnt control problem (7) can be rewritten as \([_{t}(u_{t}|x_{t})]\). In this sense, the risk-sensitive control (9) is a natural extension of (7). Nevertheless, for the risk-sensitive case, the interpretation of \(_{t}(u_{t}|x_{t})\) as entropy is no longer available. In this subsection, we provide another risk-sensitive extension of the MaxEnt control. Inspired by the Renyi divergence utilized so far, we employ Renyi entropy regularization:

\[*{minimize}_{\{_{t}\}_{t=0}^{T-1}}\ _{ ^{}()}[( c_{T}(x_{T})+_{t=0}^{T- 1}c_{t}(x_{t},u_{t})-_{1-}(_{t}(|x_{t})) )],\] (17)

where \(\{0,1\}\), and \(_{t}(|x) L^{1-}():=\{()| _{}(u)^{1-}u<\}, x\), which implies \(|_{1-}(_{t}(|x_{t}))|<\). As \(\) tends to zero, (17) converges to the MaxEnt control problem (7).

Define the value function \(_{t}\) and the Q-function \(_{t}\) associated with (17) like (10) and (11). Then, as in Subsection 3.2, the following Bellman equation holds. The derivation is given in Appendix E.

\[_{t}(x_{t})=_{_{t} L^{1-}()}\{[_{}_{t}(u^{}|x_{t})( _{t}(x_{t},u^{}))u^{}]-_{1-}(_{ t}(|x_{t}))\}.\] (18)

For the minimization in (18), we establish the duality between exponential integrals and Renyi entropy like in  because the same procedure as for (12) cannot be applied.

**Lemma 5** (Informal).: _For \(,\{0\}\) such that \(<\) and for \(g:\), it holds that_

\[[_{}( g(u))u]= _{ L^{1-}()}\{[_{}( g(u))(u)u]- _{1-}()\},\] (19)_and the unique optimal solution that minimizes the right-hand side of (19) is given by_

\[(u)=} (-(-)g(u^{}))u^{}},\ \  u.\] (20)

\(\)

For the precise statement and the proof, see Appendix D. By applying Lemma 5 with \(=-1\), \(=\) to (18), we obtain the optimal policy of (17) as follows.

**Theorem 6**.: _Assume that \(c_{t}\) is bounded below for any \(t 0,T\). Assume further that for any \(x\) and \(t 0,T-1\), it holds that \(_{}(-_{t}(x,u^{}))u^ {}<,\ _{}(-(1-)_{t}(x,u^{}) )u^{}<.\) Then, the unique optimal policy of Problem (17) is given by_

\[_{t}^{}(u_{t}|x_{t})=(x_{t})}(-_{t}(x_{t},u_{t})),\ \  t 0,T-1,\  x_{t} ,\  u_{t},\] (21)

_where \(_{t}(x_{t}):=_{}(-_{t}(x_{t},u^{ }))u^{}\), and it holds that_

\[_{t}(x_{t})=[_{}(- (1-)_{t}(x_{t},u^{}))u^{}], \ \  t 0,T-1,\  x_{t}.\] (22)

\(\)

Recall that the LP regularized risk-sensitive optimal control is given by (11), (13) while the Renyi entropy regularized control is determined by (21), (22), and \(_{t}(x_{t},u_{t})=c_{t}(x_{t},u_{t})+_ {p(x_{t+1}|x_{t},u_{t})}[(_{t+1}(x_{t+1}))]\). Hence, the only difference between the risk-sensitive controls for the LP and Renyi regularization is the coefficient in the soft Bellman equations (13), (22).

## 4 Risk-sensitive reinforcement learning via RCaI

Standard RL methods can be derived from CaI using the KL divergence . In this section, we derive risk-sensitive policy gradient and soft actor-critic methods from RCaI.

### Risk-sensitive policy gradient

In this subsection, we consider minimizing the cost (9) by a time-invariant policy parameterized as \(_{t}(u|x)=^{()}(u|x)\), \(^{n_{}}\). Let \(C_{}():=c_{T}(x_{T})+_{t=0}^{T-1}(c_{t}(x_{t},u_{t})+^{( )}(u_{t}|x_{t}))\) and \(p_{}\) be the density of the trajectory \(\) under the policy \(^{()}\). Then, Problem (9) can be reformulated as the minimization of \(J()/\) where \(J():= p_{}()( C_{}())\). To optimize \(J()/\) by gradient descent, we give the gradient \(_{}J()\). The proof is shown in Appendix F.

**Proposition 7**.: _Assume the existence of densities \(p(x_{t+1}|x_{t},u_{t})\), \(p(x_{0})\). Assume further that \(^{()}\) is differentiable in \(\), and the derivative and the integral can be interchanged as \(_{}J()=_{}[p_{}()( C_{ }())]\). Then, for any function \(b:^{n_{x}}\), it holds that_

\[_{}J()=(+1)_{p_{}()} _{t=0}^{T-1}_{}^{()}(u_{t}|x_{t})\\ ( c_{T}(x_{T})+_{s=t}^{T-1} c_{s}(x_{s},u_{s})+^{()}(u_{s}|x_{s}))-b(x_{ t})}.\] (23)

\(\)

The function \(b\) is referred to as a baseline function, which can be used for reducing the variance of an estimate of \(_{}J\). The following gradient estimate of \(J()/\) is unbiased:

\[_{t=0}^{T-1}_{}^{()}(u_{t}|x_{t }) c_{T}(x_{T})+_{s=t}^{T-1}c_{s}(x_{s },u_{s})+^{()}(u_{s}|x_{s})-b(x_{t})}.\]

This is almost the same as risk-sensitive REINFORCE  except for the additional term \(^{()}(u_{s}|x_{s})\). In the risk-neutral limit \( 0\), this estimator converges to the MaxEnt policy gradient estimator .

### Risk-sensitive soft actor-critic

In Subsection 3.2, we used dynamic programming to obtain the optimal policy \(\{_{t}^{*}\}\). Rather, in this section, we adopt a standard procedure of variational inference . First, we find the optimal factor \(_{t}\) for fixed \(_{s}\), \(s t\) as follows. The proof is deferred to Appendix G.

**Proposition 8**.: _For \(t[\![0,T-1]\!]\), let \(_{s},s t\) be fixed. Let \(>-1\), \( 0\). Then, the optimal factor \(_{t}^{}:=_{_{t}()}D_{1+}(p^{ }\|p(|_{0:T}=1))\) is given by_

\[_{t}^{}(u_{t}|x_{t})=(x_{t})}(_{p^{} (x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}[(^{T-1}_{ s}(u_{s}|x_{s})}{p(_{t}|x_{T})_{s=t}^{T-1}p(_{s}|x_{s}, u_{s})})^{}])^{-1/},\] (24)

_where \(Z_{t}(x_{t})\) is the normalizing constant. \(\)_

By (24), the optimal factor \(_{t}^{}\) is independent of the past factors \(_{s}\), \(s[\![0,t-1]\!]\). Therefore, the variational Renyi bound in (8) is maximized by optimizing \(_{t}\) in backward order from \(t=T-1\) to \(t=0\), which is consistent with the dynamic programming. Associated with (24), we define

\[V_{t}^{}(x_{t}):=_{p^{}(x_{t+1 :T},u_{t:T-1}|x_{t})}[(^{T-1}_{s}(u_{s}|x_{s})}{ p(_{t}|x_{T})_{s=t}^{T-1}p(_{s}|x_{s},u_{s})} )^{}]\\ =_{p^{}(x_{t+1:T},u_{t:T-1}|x_{t} )}[( c_{T}(x_{T})+_{s=t}^{T-1}c_{s}(x_{s},u_ {s})+_{s}(u_{s}|x_{s}))],\] (25)

which is the value function for the policy \(\{_{s}\}_{s=t}^{T-1}\) satisfying the following Bellman equation.

\[V_{t}^{}(x_{t}) =_{_{t}(u_{t}|x_{t})}[( (u_{t}|x_{t})}{p(_{t}|x_{t},u_{t})})^{} _{p(x_{t+1}|x_{t},u_{t})}[( V_{t+1}^{}(x_{t+1})) ]]\] (26) \[=_{_{t}(u_{t}|x_{t})} ( c_{t}(x_{t},u_{t})+_{t}(u_{t}|x_{t}))_{p (x_{t+1}|x_{t},u_{t})}[( V_{t+1}^{}(x_{t+1}))].\]

By the value function, \(_{t}^{}(u_{t}|x_{t})\) can be written as

\[_{t}^{}(u_{t}|x_{t}) =_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}_{p (x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}[(^{T-1}_{ s}(u_{s}|x_{s})}{p(_{t}|x_{T})_{s=t+1}^{T-1}p(_{s}|x_{s}, u_{s})})^{}]^{-1/}\] \[=_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}_{p (x_{t+1}|x_{t},u_{t})}[( V_{t+1}^{}(x_{t+1}))]^{-1/}.\] (27)

Next, we define the Q-function for \(\{_{s}\}_{s=t+1}^{T-1}\) as follows:

\[Q_{t}^{}(x_{t},u_{t}):=- p(_{t}|x_{t},u_{t})+ _{p(x_{t+1}|x_{t},u_{t})}[( V_{t+1}^{}(x_{t+1}) )].\] (28)

Then, it follows from (26) and (27) that

\[V_{t}^{}(x_{t}) =_{_{t}(u_{t}|x_{t})}[_{t} (u_{t}|x_{t})^{}( Q_{t}^{}(x_{t},u_{t}))],\] (29) \[_{t}^{}(u_{t}|x_{t}) =(x_{t})}(-Q_{t}^{}(x_{t},u_{t})),\ \ Z_{t}(x_{t})=_{}(-Q_{t}^{}(x_{t},u^{}) )u^{}.\] (30)

Especially when \(_{t}(u_{t}|x_{t})=_{t}^{}(u_{t}|x_{t})\), it holds that \(V_{t}^{}(x_{t})=-[(-Q_{t}^{}(x_{t},u^{}))u^{}]\), which coincides with the soft Bellman equation in (13). In summary, in order to obtain the optimal factor \(_{t}^{}\), it is sufficient to compute \(V_{t}^{}\) and \(Q_{t}^{}\) in a backward manner.

Next, we consider the situation when the policy is parameterized as \(_{t}^{()}(u_{t}|x_{t})\), \(^{n_{}}\) and there is no parameter \(\) that gives the optimal factor \(_{t}^{()}=_{t}^{}\). To accommodate this situation, we utilize the variational Renyi bound. One can easily see that the maximization of the Renyi bound in (8) with respect to a single factor \(_{t}\) is equivalent to the following problem.

\[*{minimize}_{_{t}}\ _{p^{}(x_{t})} [_{_{t}(u_{t}|x_{t})}[_{t}(u_{t}|x_{t})^{}(  Q_{t}^{}(x_{t},u_{t}))]].\] (31)This suggests choosing \(\) that minimizes (31) whose \(_{t}\) is replaced by \(_{t}^{()}\). Note that this is further equivalent to

\[*{minimize}_{}\ }_{p^{}(x_{t})} [D_{1+}(_{t}^{()}(|x_{t})^{}(x_{t},))}{Z_{t}(x_{t})})].\] (32)

We also parameterize \(V_{t}^{}\) and \(Q_{t}^{}\) as \(V^{()}\), \(Q^{()}\) and optimize \(,\) so that the relations (28), (29) approximately hold. To obtain unbiased gradient estimators later, we minimize the following squared residual error based on (28), (29), and the transformation \(T_{}(v):=(^{ v}-1)/\), \(v\):

\[_{Q}() :=_{p^{}(x_{t},u_{t})}[\{T_{ }(Q^{()}(x_{t},u_{t})-c(x_{t},u_{t}))-_{p(x_{t+1} |x_{t},u_{t})}[T_{}(V^{()}(x_{t+1}))]\}^{2}],\] \[_{V}() :=_{p^{}(x_{t})}[\{T_{}(V^ {()}(x_{t}))-_{^{()}(u_{t}|x_{t})}[T_{}( Q^{()}(x_{t},u_{t})+^{()}(u_{t}|x_{t}))]\}^{2} ].\]

Using \(Q^{()}\) and \(T_{}\), we replace (31) with the following equivalent objective:

\[_{}():=_{p^{}(x_{t})} _{^{()}(u_{t}|x_{t})}T_{}Q^{()}(x_{t },u_{t})+^{()}(u_{t}|x_{t}).\] (33)

Noting that \(_{ 0}T_{}(())=(0)\) for \(:\), as the risk sensitivity \(\) goes to zero, the objectives \(_{Q},_{V},_{}\) converge to those used for the risk-neutral soft actor-critic . Now, we have

\[_{}_{Q}() =_{p^{}(x_{t},u_{t})}_{}Q^ {()}(x_{t},u_{t}) Q^{()}(x_{t},u_{t})- c(x_{ t},u_{t})\] \[T_{}Q^{()}(x_{t},u_{t})-c(x_{t},u_{t })-_{p(x_{t+1}|x_{t},u_{t})}T_{}(V^{()}(x_{t+ 1}))},\] (34) \[_{}_{V}() =_{p^{}(x_{t})}_{}V^{() }(x_{t})( V^{()}(x_{t}))\] \[T_{}(V^{()}(x_{t}))-_{^{( )}(u_{t}|x_{t})}T_{}Q^{()}(x_{t},u_{t})+^{ ()}(u_{t}|x_{t})},\] (35) \[_{}_{}() =(+1)_{p^{}(x_{t},u_{t})}_{ }^{()}(u_{t}|x_{t})T_{}Q^{()}(x_{t},u _{t})+^{()}(u_{t}|x_{t}).\] (36)

Thanks to the transformation \(T_{}\), the expectations appear linearly, and an unbiased gradient estimator can be obtained by removing them. By simply replacing the gradients of the soft actor-critic  with (34)-(36), we obtain the risk-sensitive soft actor-critic (RSAC). It is worth mentioning that since RSAC requires only minor modifications to SAC, techniques for stabilizing SAC, e.g., reparameterization, minibatch sampling with a replay buffer, target networks, double Q-network, can be directly used for RSAC.

## 5 Experiment

Unregularized risk-averse control is known to be robust against perturbations in systems . Since the robustness of the regularized cases has not yet been established theoretically, we verify the robustness of policies learned by RSAC through a numerical example. The environment is Pendulum-v1 in OpenAI Gymnasium. We trained control policies using the hyperparameters shown in Appendix H. There were no significant differences in the control performance obtained or the behavior during training. On the other hand, for each \(\), one control policy was selected and was applied to a slightly different environment _without retraining_. To be more precise, the pendulum length \(l\), which is 1.0 during training, is changed to 1.25 and 1.5; See Fig. 3. In this example, it can be seen that the control policy obtained with larger \(\) has a smaller performance degradation due to environmental changes. This robustness can be considered a benefit of risk-sensitive control.

In Fig. 4, empirical distributions of the costs for different risk-sensitivity parameters \(\) are plotted. Only the distribution for \(=0.02\) does not change so much under the system perturbations. The

Figure 3: Average episode cost for RSAC with some \(\) and standard SAC.

distribution for SAC (\(=0\)) with \(l=1.5\) deviates from the original one (\(l=1.0\)), and another peak of the distribution appears in the high-cost area. This means that there is a high probability of incurring a high cost, which clarifies the advantage of RSAC. The more risk-seeking the policy becomes, the less robust it becomes against the system perturbation.

## 6 Conclusions

In this paper, we proposed a unifying framework of CaI, named RCaI, using Renyi divergence variational inference. We revealed that RCaI yields the LP regularized risk-sensitive control with exponential performance criteria. Moreover, we showed the equivalences for risk-sensitive control, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. In addition to these connections, we derived the policy gradient method and the soft actor-critic method for the risk-sensitive RL via RCaI. Interestingly, Renyi entropy regularization also results in the same form of the risk-sensitive optimal policy and the soft Bellman equation as the LP regularization.

From a practical point of view, a major limitation of the proposed risk-sensitive soft actor-critic is its numerical instability for large \(||\) cases. Since \(\) appears, for example, as \(( Q^{()}(x_{t},u_{t}))\) in the gradients (34)-(36), the magnitude of \(\) that does not cause the numerical instability depends on the scale of costs. Therefore, we need to choose \(\) depending on environments. In the experiment using Pendulum-v1, \(||\) that is larger than \(0.03\) results in the failure of learning due to the numerical instability. Although it is an important future work to address this issue, we would like to note that this issue is not specific to our algorithms, but occurs in general risk-sensitive RL with exponential utility. It is also important how to choose a specific value of the order parameter \(1+\) of Renyi divergence. Since we showed that \(\) determines the risk sensitivity of the optimal policy, we can follow previous studies on the choice of the sensitivity parameter of the risk-sensitive control without regularization. The properties of the derived algorithms also need to be explored in future work, e.g., the compatibility of a function approximator for RSAC .