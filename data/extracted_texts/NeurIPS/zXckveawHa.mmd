# Statistical Limits of Adaptive Linear Models:

Low-Dimensional Estimation and Inference

 Licong Lin

Department of Statistics

University of California, Berkeley

liconglin@berkeley.edu

&Mufang Ying

Department of Statistics

Rutgers University - New Brunswick

my426@scarletmail.rutgers.edu

&Suvrojit Ghosh

Department of Statistics

Rutgers University - New Brunswick

sg1565@scarletmail.rutgers.edu

&Koulik Khamaru

Department of Statistics

Rutgers University - New Brunswick

kk1241@stat.rutgers.edu

&Cun-Hui Zhang

Department of Statistics

Rutgers University - New Brunswick

czhang@stat.rutgers.edu

###### Abstract

Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of \(\) when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity. We show that OLS or OLS on centered data can achieve this matching error. In addition, we propose a novel estimator for single coordinate inference via solving a Two-stage Adaptive Linear Estimating equation (TALE). Under a weaker form of adaptivity in data collection, we establish an asymptotic normality property of the proposed estimator.

## 1 Introduction

Estimating a low-dimensional parameter component in a high-dimensional model is a fundamental problem in statistics and machine learning that has been widely studied in e.g., semiparametric statistics , causal inference  and bandit algorithms . When data are independently and identically distributed (i.i.d.), it is often possible to derive estimators that are asymptotically normal with a rate of convergence of \(\), and that achieve the semi-parametric variance lower boundthat is independent of the dimension. There is now a rich body of literature that studies this problem under various scenarios .

In this work we are interested in the same estimation and inference problem but under the setting where the i.i.d. data assumption fails. Specifically, we consider an adaptive collection framework where the data collected at time \(i\) is allowed to be dependent on the historical data collected up to time \(i-1\). This adaptive framework incorporates datasets originated from applications in many fields, including sequential experimental design , bandit algorithm , time series modeling , adaptive stochastic approximation schemes .

### An interesting lower bound

To see the intrinsic difference between the i.i.d. and the adaptive data collection settings, we consider the canonical example of linear model \(y=^{}^{*}+\), where the parameter \(^{*}=(_{1}^{*},_{2}^{*})^{1} ^{d-1}\), \(^{iid}(0,1)\). Clearly, when the covariates \(\{_{i}\}_{i n}\) are deterministic, a straightforward calculation yields

\[_{,1}-_{1}^{*}(0,(_{n}^{-1})_{11}),[(_{n}^{-1})_{11}^{-1} (_{,1}-_{1}^{*})^{2}]=1,\] (1)

where \(_{}\) is the OLS estimator and \(_{n}:=_{t=1}^{n}_{i}_{i}^{}\) is the sample covariance matrix.

However, somewhat surprisingly, when the covariates \(\{_{i}\}_{i n}\) are allowed to be collected in an _arbitrary_ adaptive manner, in a recent work  the authors proved the following (informal) counter-intuitive minimax lower bound on the scaled-MSE (defined in Definition 2.2)

\[_{}_{^{*}}[(_{n}^{-1} )_{11}^{-1}(-_{1}^{*})^{2}] cd(n),\] (2)

where the extra \(d\)-factor enters the estimation of a _single_ coordinate. This lower bound indicates that a dimension independent single coordinate estimation is _infeasible_ when the data are collected _arbitrarily_ adaptively. This is undesirable especially in the high dimensional scenario where \(d\), since a \(\)-consistent estimation is unattainable. Motivated by the contrast between i.i.d. and adaptive data collection, we pose the following question in this work:

_Can we bridge the gap between iid and adaptive data collection, and obtain an estimator for a low-dimensional parameter component in linear models, such that its performance depends on the degree of adaptivity?_

### Contributions

In this work, we initiate the study of how the adaptivity of the collected data affects low-dimensional estimation in a high-dimensional linear model. We explore the previously posed question and provide an affirmative answer.

We begin by introducing a general data collection assumption, which we term _\((k,d)\)-adaptivity_. Broadly speaking, \((k,d)\)-adaptivity implies that the data pairs \(\{(_{i},y_{i})\}_{i=1}^{n}^{d}\) are collected in a way that the first \(k\) coordinates of \(_{i}\) (denoted by \(_{i}^{}\)) are chosen adaptively based on the historical data, while the remaining \(d-k\) coordinates of \(_{i}\) (denoted by \(_{i}^{}\)) are i.i.d. across time \(i[n]\).

Assume the collected data are \((k,d)-\)adaptive from a linear model \(y=^{}^{*}+\). We analyze the lower-dimensional estimation problem under the scenarios where the i.i.d. non-adaptive components \(_{i}^{}\) are either zero-mean or nonzero-mean. In the zero mean case, we show that the ordinary least squares estimator (OLS) for the first \(k\)-coordinate yields a scaled mean squared error (scaled-MSE) of \(k(n)\) (Theorem 3.1). For the nonzero-mean case, a similar result is achieved using the OLS estimator on centered data (Theorem 3.2). Consequently, we find that the degree of adaptivity significantly impacts the performance of single coordinate estimation, in the sense that the scaled-MSE is inflated by a factor of \(k\), where \(k\) denotes the number of adaptive coordinates (see Corollary 3.3).

Although OLS for a single coordinate has a dimension independent scaled-MSE when the collected data are \((1,d)\)-adaptive, it should be noted that OLS may exhibit non-normal asymptotic behavior  when data are adaptively collected. Therefore, we propose a novel estimator by solving a Two-stage Adaptive Linear Estimating Equation (TALE). When the collected data are \((1,d)\)-adaptive and the non-adaptive component is zero mean, we show that our new estimator is asymptotically normal and has a comparable scaled-MSE as the naive OLS estimator (see Theorem 3.4).

## 2 Problem set up

Consider a linear model

\[y=^{}^{*}+,\] (3)

where the parameter \(^{*}^{d},\) and \(\) is a zero mean noise variable. Given access to a data set \(\{(_{i},y_{i})\}_{i n}\) from the model (3), we are interested in the estimation and inference problem of a low-dimensional parameter component \(^{*}_{ ad}^{k}\), where \(^{*}=(^{*}_{ ad},^{*}_{ nad})^ {}\).

In this paper, we are interested in adaptive data collection regime. Concretely, we assume that the data are collected adaptively in the following way

**Definition 2.1** (\((k,d)\)-adaptivity): _The collected samples \(\{(_{i},y_{i})\}_{i n}\) forms a filtration \(\{\}_{i=0}^{}\) with \(_{0}=\) and \(_{i}=(_{1},y_{1},,_{i},y_{i})\). Let \(P\) be an unknown distribution on \(^{d-k}\). We assume that at each stage, \(i 1\)_

* _The adaptive component_ \(^{ ad}_{i}=_{i,1:k}\) _is collected from some unknown distribution that could depend on_ \(_{i-1}\)_._
* _The non-adaptive component_ \(^{ nad}_{i}=_{i,k+1:d}\) _is a sample from_ \(P\) _and independent of_ \((^{ ad}_{i},_{i-1})\)_._

When \(k=0\), Definition 2.1 reduces to an i.i.d. data collection strategy; when \(k=d\), it corresponds to the case where the data are allowed to be collected arbitrarily adaptively. Consequently, \((k,d)\)-adaptivity connects two extreme scenarios, and the degree of adaptivity increases as \(k\) increases.

**Example 2.1** (Treatment assignment): _As a concrete example, consider the problem of treatment assignment to patients. At round \(i\), we observe the health profile of the patient \(i\), which we denote by \(_{i}^{d-1}\). Our job to assign a treatment \(A_{i}\{0,1\}\) based on the patient's health profile \(_{i}\) and also our prior knowledge of effectiveness of the treatments. It is natural to capture our prior knowledge using \(_{i}=(A_{1},_{1},y_{1},,A_{i-1},_{i-1},y_{ i-1})\) -- the sigma field generated by previous data-points. As already pointed out in (2), in the adaptive regime the estimator error for treatment effect scales as \(\) ; in words, we have to pay for a dimension factor \(\) even if we are only interested in estimating a one-dimensional component. While for our treatment assignment example, the dimension \(d-1\) of the covariate vector \(_{i}\) is large in practice, it is natural to assume that the treatment assignment is dependent on \(k-1 d-1\), a few (unknown) components. Under this assumption, it is easy to see that this treatment assignment problem is \((k,d)\)-adaptive. We show that the treatment effect can be estimated at a rate \(\)._

### Statistical limits

Before we discuss how to obtain estimators for a low-dimensional parameter component of \(^{}\), we establish some baselines by recalling existing lower bounds. Throughout this section, we assume the noise \(_{i}(0,^{2})\). We start with defining the metric for comparison.

**Definition 2.2** (scaled mean squared error (scaled-MSE)): _Given a subset \([d]\). We define the scaled-MSE of an estimator \(}_{}\) for \(^{*}_{}^{||}\) to be \([(}_{}-}_{ })^{}[(^{-1}_{n})_{}]^{-1}( }_{}-}_{})],\) where \(_{n}=_{i=1}^{n}_{i}_{i}^{}\) is the sample Gram matrix._

Roughly speaking, when the covariates \(_{i}\) are all fixed, the scaled-MSE compares the performance of \(}_{}\) against the estimator with minimal variance (OLS). Moreover, we have the following result:

**Proposition 2.2** (A simplified version of Theorem 2 in Khamaru et al. (2018)): __

1. _Given a set_ \([d]\)_. Suppose the data_ \(\{(_{i},y_{i})\}_{i=1^{n}}\) _are i.i.d. (_\((0,d)\)_-adaptive) from model (_3_). Then the scaled-MSE satisfies_ \[_{}}_{^{*}^{d}} }_{}-^{*}_{} _{[(^{-1}_{n})_{}]^{-1}}^{2} ^{2}||.\] (4) _Furthermore, the equality holds when choosing_ \(}_{}\) _to be the OLS estimator for_ \(^{*}_{}\)_._2. _Suppose the data points_ \(\{(_{i},y_{i})\}_{i=1}^{n}\) _are allowed to be arbitrarily adaptive (_\((d,d)\)_-adaptive). For any_ \((n,d)\) _with_ \(d 2\) _and_ \(n c d^{3}\)_, and any non-empty set_ \([d]\)_, there exists a data collection algorithm such that_ \[_{}}_{^{}^{d}}}_{}-_{}^{ }_{[(}}}}}}}}}}}})_{ ^{c}}}^{2} c^{} d^{2}(n),\] (5) _where_ \(c,c^{}>0\) _are some universal constants._

Proposition 2.2 exhibits the striking difference between two extreme data collection mechanisms. While the scaled-MSE scales as \(O(||)\) when data are i.i.d., the scaled-MSE for even a single coordinate (e.g., setting \(=\{1\}\)) can be of the order \(O(d)\) if the data are allowed to be collected arbitrarily adaptively.

Let \(^{c}=[d]\). By the matrix inverse formula, we have

\[[(}}}}}}}}}}}}}}} [(}}}}}}}}}}}}}}}}} [(}}}}}}}}}}}}}}}}}} [(}}}}}}}}}}}}}}}}}} -1}}(}}}}}}}}}}}}}}}}}}} -1}(}}}}}}}}}}}}}}}}}}} -1}(}}}}}}}}}}}}}}}}}}} 1}(}}}}}}}}}}}}}}}}}} 1}}(,,,,\)), and the regular font to denote scalars (e.g., \(x,,\)). Given data \(\{(_{i},y_{i})\}_{i=1}^{n}\) from the linear model (3) that are \((k,d)\)-adaptive, we use \(_{i}^{ ad}^{k},_{i}^{ ad}^{d-k}\) to denote the adaptive and non-adaptive covariates. We also write \(^{*}=(_{ ad}^{*},_{ nad}^{*})^{}\) to denote the components that correspond to the adaptive and non-adaptive covariates. Let \(=(_{1}^{},,_{n}^{})^{}^{n  d}\) be the covariate matrix, with \(_{ ad}\) (or \(_{ nad}\)) representing the submatrices consisting of the adaptive (or non-adaptive) columns. We use \(_{j}\) to denote the \(j\)-th column of the covariate matrix.

For a matrix \(\) with \(n\) rows, let \(_{-j}\) be the matrix obtained by deleting the \(j\)-th column of \(\). We define the _projection operator_\(_{}:=(^{})^{-1}^{}\) and the (columnwise) centered matrix \(}:=(_{n}-_{_{n}})\), where \(^{n n}\) is the identity matrix and \(_{n}^{n}\) is the all-one vector. For a symmetric \( 0\), we define \(\|\|_{}:=^{}}\). Lastly, we use \(c,c^{},c^{}>0\) to denote universal constants and \(C,^{},C^{}>0\) to denote constants that may depend on the problem specific parameters but not on \(k,d,n\). We allow the values of the constants to vary from place to place.

## 3 Main results

This section is devoted to our main results on low-dimensional estimation and inference. In Section 3.1 and 3.2 we discuss the problem of estimating a low-dimensional component of \(^{*}\), and Section 3.3 is devoted to inference of low-dimensional components.

### Low-dimensional estimation

Suppose the collected data \(\{(_{i},y_{i})\}_{i=1}^{n}\) are \((k,d)\)-adaptive. In this section, we are interested in estimating the adaptive parameter component \(_{ ad}^{*}^{k}\).

In addition to \((k,d)\)-adaptivity, we introduce the following assumptions on the collected data \(\{(_{i},y_{i})\}_{i=1}^{n}\).

**Assumption A**:
1. There exists a constant \(}>0\) such that \[1_{}(_{ ad}^{}_{ ad})_{ }(_{ ad}^{}_{ ad}) n}.\]
2. The non-adaptive components \(\{_{i}^{ nad}\}_{i=1}^{n}\) are i.i.d. sub-Gaussian vectors with parameter \(>0\), that is, for any unit direction \(^{d-1}\), \[[\{,\,_{i}^{ nad}-[_{i}^{ nad}]\}] e^{^{2}^{2}/2} .\]
3. There exist some constants \(0_{}_{}\) such that the covariance matrix of the non-adaptive component \(:=[_{i}^{ nad}]\) satisfies, \[0<_{}_{}()_{}() _{}.\]
4. Conditioned on \((_{i},_{i-1})\), the noise variable \(_{i}\) in (3) is zero mean sub-Gaussian with parameter \(v>0\), i.e., \[[_{i}|_{i}^{ ad},_{i-1}]=0,\;\;\;\;[e^{_{i}}|_{i}^{ ad},_{i-1}] e^{ ^{2}v^{2}/2},\] and has conditional variance \(^{2}=[_{i}^{2}|_{i},_{i-1}]\) for all \(i[n]\).

Let us clarify the meaning of the above assumptions. Assumption (A1) is the regularity assumption on the adaptive component. Roughly speaking, we allow the adaptive component to be _arbitrarily adaptive_ as long as \(_{ ad}^{}_{ ad}\) is not close to be singular and \(_{i}^{ ad}\) has bounded \(_{2}-\)norm. This is weaker than the assumptions made in , which assume that the conditional distribution of \(_{ ad}\) is known. Assumption (A2), (A3) on the non-adaptive component, assume its distribution is non-singular and light-tailed. Assumption (A4) is a standard assumption that characterizes the tail behavior of the zero-mean noise variable. We remark that the equal conditional variance assumption in Assumption (A4) is mainly required in Theorem 3.4, while it is sufficient to assume \(^{2}\) being a uniform upper bound of the conditional variance in Theorem 3.1 and 3.2.

#### 3.1.1 Warm up: zero-mean non-adaptive component

We start with discussing a special case where the non-adaptive component \(_{i}^{}\) is zero-mean. In this case, we prove that the Ordinary Least Squares (OLS) estimator on \((,)\) for \(_{}^{}\) is near-optimal; see the discussion in Section 2.1. Denote the OLS estimator by \(}=(}_{}^{},}_{}^{})^{}\). Throughout, we assume that the sample size \(n\) and dimension \(d\) satisfies the relation

\[(n/)} Cd^{2},\] (6)

where \(C\) is an independent of \((n,d)\) but may depend on other problem specific parameters. With this set up, our first theorem states

**Theorem 3.1**: _Given data points \(\{(_{i},y_{i})\}_{i=1}^{n}\) from a \((k,d)\)-adaptive model, and tolerance level \((0,1/2)\). Let, assumption (A1)-(A4) and the bound (6) in force, and the non-adaptive component \(_{i}^{}\) is drawn from a zero-mean distribution. Then, we have_

\[\|}_{}-_{}^{ }\|_{_{}^{}(_{n}-_{_{}})_{}}^{2}  C^{}(n(_{}^{}_{ })/)\] (7a) \[ C^{}k(n/).\] (7b)

_with probability at least \(1-\)._

See Appendix A.3 for a detailed proof. A few comments regarding Theorem 3.1 are in order. One might integrate both sides of the last bound to get a bound on the scaled-MSE. Comparing the bound (7b) with the lower bound from Proposition 2.2, we see this bound is tight in a minimax sense, up to some logarithmic factors.

It is now worthwhile to compare this bound with the existing best upper bounds in the literature. Invoking the concentration bounds from (26, Lemma 16) one have that

\[\|}_{}-_{}^{ }\|_{_{}^{}(_{n}-_{_{}})_{}}^{2}\|}_{ }-_{}^{}\|_{_{}^{ }_{}}^{2} c d(n/)\] (8)

One might argue that the first inequality is loose as we only want to estimate a low-dimensional component \(_{}^{}^{k}\). However, invoking the lower bound from Proposition 2.2, we see that the bound (8) is the best you can hope for if we do not utilize the \((k,d)\)-adaptivity structure present in the data. See also the scaled-MSE bound for a single coordinate estimation in (26, Theorem 8) which also has a dimension dependence in the scaled-MSE bound.

#### 3.1.2 Nonzero-mean non-adaptive component

In practice, the assumption that the non-adaptive covariates are drawn i.i.d. from a distribution \(P\) with _zero mean_ is unsatisfactory. One would like to have a similar result where the distribution \(P\) has an _unknown_ non-zero mean.

```
1:\(}_{}_{}^{} _{n}}{n}\), \(}_{}_{}^{} _{n}}{n}\)
2:\(}_{}=_{}-_{n} }_{}^{}\), \(}_{}=_{}-_{n} }_{}^{}\)
3:Run OLS on centered response vector \(-}_{n}\) and centered covariate matrix \(}=(}_{},}_{})^{n d}\); obtain the estimator \(}=(}_{}^{}, {}_{}^{})^{}\). ```

**Algorithm 1** Centered OLS for \(k\) adaptive coordinates (\(,\))

Before we state our estimator for the nonzero-mean case, it is helpful to understand the proof intuition of Theorem 3.1. A simple expansion yields

\[}_{}-_{}^{} =(_{}^{}_{}-_{}^{}_{_{}}_{})^{-1}( _{}^{}-_{}^{} _{_{}})\] \[(_{}^{}_{})^ {-1}_{}^{}+\]

We show that the interaction term \(_{}^{}_{_{}}\) is small compared to the other terms under \((k,d)\)-adaptivity and _zero-mean_ property of \(}_{}\). In particular, under _zero-mean_ property, each entry ofthe matrix \(_{}^{}_{}\) is a martingale difference sequence, and can be controlled via concentration inequalities . This martingale property is _not true_ when the columns of \(_{}\) have a nonzero mean.

As a remedy, we consider the mean-centered linear model:

\[-_{n}=}_{}^{ }_{}^{*}+}_{}^{}_{}^{*}+(- _{n})\] (9)

where \(}_{}=_{}-_{n}_{n}^{}}{n}_{}\), and \(}_{}=_{}-_{n}_{n}^{}}{n}_{}\) are centered version of the matrices \(_{}\) and \(_{}\), respectively. The centering in (9) ensures that \(_{}\) is _approximately_ zero-mean, but unfortunately, it breaks the martingale structure present in the data. For instance, the elements of \(}_{}^{}}_{}\) are not a sum of martingale difference sequence because we have subtracted the column mean from each entry. Nonetheless, it turns out that subtracting the sample mean, while breaks the martingale difference structure, does not break it in an adversarial way, and we can still control the entries of \(}_{}^{}}_{}\). See Lemma A.1 part (b) for one of the key ingredient in the proof. We point out that this finding is not new. Results of this form are well understood in various forms in sequential prediction literature, albeit in a different context. Such results can be found in earlier works of Lai, Wei and Robbins [21; 22; 24] and also in the later works by several authors [11; 12; 10; 1] and the references therein.

Our following Theorem 3.2 ensures that the intuition developed in this section so far is useful to characterize the performance of the solution obtained from the centered OLS.

**Theorem 3.2**: _Given data points \(\{(_{i},y_{i})\}_{i=1}^{n}\) from a \((k,d)\)-adaptive model, and tolerance level \((0,1/2)\). Let, assumption (A1)-(A4) and the bound (6) be in force. Then, \(}_{}\) obtained from Algorithm 1, satisfies_

\[\|}_{}-_{}^{*}\|_{}_{}^{}(_ {n}-_{}_{}})}_ {}}^{2}  C^{}(n(}_{}^{ }}_{})/)\] (10) \[ C^{}k(n/).\]

_with probability at least \(1-\)._

See Appendix A.4 for a proof. Note that the variance of \(}_{}\) is given by \(}_{}^{}(_{n}-_{ }_{}})}_{}\). The covariance matrix is the same as \(_{}^{}(_{n}-_{_{}})_{}\) when the all one vector \(_{n}\) belongs to the column space of \(}_{}\).

### Single coordinate estimation: Application to treatment assignment

Let us now come back to Example 2.1 that we started. Let, at every round \(i\), the treatment assignment \(A_{i}\) depends on \(k-1\) (unknown) coordinate of the covariates \(_{i}^{d-1}\). We assume that the covariates \(_{i}^{}s\) are drawn i.i.d. from some unknown distribution \(\). Assuming the response is related to the treatment and covariates via a linear model, it is not hard to see that this problem satisfies a \((k,d)\)-adaptivity property. The following corollary provides a bound on the estimation error of estimating the (homogeneous) treatment effect.

Figure 1: The plot depicts the empirical relation between the scaled MSE of the OLS and centered OLS estimate from Algorithm 1 and the number of adaptive covariates \((k)\) for a carefully constructed problem. See Section B.1 for simulation details.

**Corollary 3.3**: _Suppose the assumptions from Theorem 3.2 are in force, and \([k]\) be an index corresponding to one of the adaptive coordinates. Then, the \(^{th}\) coordinate of the the centered OLS estimator from Algorithm 1 satisfies_

\[|_{,}-^{*}_{,}|}_{}^{}}_{})/)}}{}_{}^{ }(_{n}-_{}_{-}})}_{}}}}{}_{}^{}(_{n}-_{}_{ -}})}_{}}}.\]

_The bounds above hold with probability at least \(1-\), and \(}_{-}\) denote the matrix obtained by removing the \(^{th}\) column from \(}\)._

See Appendix A.5 for a proof of this Corollary. We verify the result of Corollary 3.3 via simulations as shown in Figure 1. From the figure we see that the scaled MSE increases linearly as the number of adaptive covariates increases, matching with our theoretical predictions. See Appendix B for more details about the simulation.

### Inference with one adaptive arm

In this section, we provide a method for constructing valid confidence intervals for \(^{*}_{}\). To simplify the problem, we restrict our attention to the case of \((1,d)\)-adaptivity and \([^{}_{i}]=\).

#### A two-stage estimator

Our goal is to derive an asymptotically normal estimator for a target parameter in presence of a nuisance component. We call our estimator a "Two-stage-adaptive-linear-estimating-equation" based estimator, or \(\) for short. We start with a prior estimate \(}^{}_{}\) of \(^{*}_{}\), and define our estimate \(}_{}\) for \(^{*}_{}\) as a solution of this

\[_{i=1}^{n}w_{i}(y_{i}-x_{i}^{}}_{}-_{i}^{ }}_{}^{})=0.\] (11)

Recall that \(}_{}\) is a scalar and the equation has a unique solution as long as \(_{1 i n}w_{i}^{}x_{i} 0\).

The weights \(\{w_{i}\}_{i n}\) in equation (11) are a set of predictable random scalars (i.e. \(w_{i}(_{i}^{},_{i-1})\)). Specifically, we start with \(s_{0}>0\) and \(s_{0}_{0}\), and define

\[w_{i} =/s_{0})x_{i}^{}}{}}  s_{i}=s_{0}+_{t i}(x_{t}^{})^{2}\;\;\] (12a) \[f(x) =x)( e^{2}x)^{2}}}\;x>1.\] (12b)

Let us first gain some intuitions on why TABLE works. By rewriting equation (11), we have

\[_{i=1}^{n}w_{i}x_{i}^{}\!(}_{ }-_{}^{*})=^{n}w_{i}_{i}}_{v_{n}}+^{n}w_{i}x_{i}^{ }(_{}^{*}-}_{}^{})}_{b_{n}}.\] (13)

Following the proof in , we have \(v_{n}(0,^{2})\). Besides, one can show that with a proper choice of prior estimator \(}_{}^{}\), the bias term \(b_{n}\) converges to zero in probability as \(n\) goes to infinity. It is important to note that  considers the linear regression model where the number of covariates is fixed, and the sample size goes to infinity. In this work, however, we are interested in a setting where the number of covariates can grow with the number of samples. Therefore, our approach, \(\), has distinctions with the ALEE estimator proposed in . The above intuition is formalized in the following theorem. Below, we use the shorthand \(}_{}^{}\) to denote the coordinates of the least squares estimate of \(}_{}^{}\) corresponding to the _non-adaptive_ components.

**Theorem 3.4**: _Suppose \(1/s_{0}+s_{0}/s_{n}=o_{p}(1)\), \(n/(^{2}(n) d^{2})\), and assumptions (A1)-(A4) are in force. Then, the estimate \(_{}\), obtained using weights from (12a) and \(}_{}^{}=}_{}^{}\), satisfies_

\[w_{i}^{2}}}_{1  i n}w_{i}x_{i}^{}(_{}-_{}^{*})}{{ }}(0,1),\]

_where \(\) is any consistent estimate of \(\). Moreover, the asymptotic variance \(_{}\) is optimal up to logarithmic-factors._

See Appendix A.6 for a proof of this theorem. The assumption \(1/s_{0}+s_{0}/s_{n}=o_{p}(1)\) in the theorem essentially requires \(s_{0}\) grows to infinity in a rate slower than \(s_{n}\). Therefore, in order to construct valid confidence intervals for \(_{}\), one has to grasp some prior knowledge about the lower bound of \(s_{n}\). In our experiments in Section 4 we set \(s_{0}=(n)\). Finally, it is also worth mentioning that one can apply martingale concentration inequalities (e.g. ) to control the terms \(b_{n}\) and \(v_{n}\) in equation (13), which in turn yields the finite sample bounds for \(_{}\) estimator. Finally, a consistent estimator of \(\) can be found using (24, Lemma 3).

## 4 Numerical experiments

In this section, we investigate the performance of TALE empirically, and compare it with the ordinary least squares (OLS) estimator, W-decorrelation proposed by Deshpande et al. , and the nonasymptotic confidence intervals derived from Theorem 8 in Lattimore et al. . Our simulation set up entails the motivating Example 2.1 of treatment assignment. In our experiments, at stage \(i\), the treatments \(A_{i}\{0,1\}\) are assigned on the sign of \(_{1}^{(i)}\), where \(}^{(i)}=(_{1}^{(i)},_{2}^{(i)},,_{d}^{(i)})\) is the least square estimate based on all data up to the time point \(i-1\); here, the first coordinate of \(_{1}^{(i)}\) is associated with treatment assignment. The detailed data generation mechanism can be found in Appendix. From Figure 2 (top) we see that both TALE and W-decorrelation have valid empirical coverage (i.e., they are close to or above the baseline), while the nonasymptotic confidence intervals are overall conservative and the OLS is downwardly biased. In addition, TALE has confidence intervals that are shorter than those of W-decorrelation, which indicates a better estimation performance. Similar observations occur in the high-dimensional model in Figure 2 (bottom), where we find that both the OLS estimator and W-decorrelation are downwardly biased while TALE has valid coverage.

Figure 2: Empirical coverage probability and the width of confidence intervals versus target coverage probability \(1-\) for TALE, the OLS estimator, non-asymptotic concentration inequalities, and W-decorrelation. We select the noise level \(=0.3\). Top: \(n=1000,d=10\). Bottom: \(n=500,d=50\). At bottom right we do not display the result for concentration since the CIs are too wide. We run the simulation \(1000\) times and display the \( 1\) standard deviation.

## 5 Discussion

In this paper, we investigate the statistical limits of estimating a low-dimensional component in a high dimensional adaptive linear model. We start by recalling a recent lower bound , which states that we need to pay for the underlying dimension \(d\) even if we want to estimate a low (one)-dimensional component. Our main result is to show that in order to estimate a low-dimensional component, we need to pay only for the degree of adaptivity \(k\), which can potentially be much smaller than the underlying dimension \(d\). Additionally, we propose a two-stage estimator for the one-dimensional target component, which is asymptotically normal. Finally, we demonstrate the effectiveness of this two-stage estimator via numerical simulations. For the future work, there are several avenues for further exploration that can contribute to a more comprehensive understanding of adaptive regression models. First of all, it would be interesting to generalize the \((k,d)-\)adaptivity for the case when the number of adaptive components may vary between samples. It is also interesting to investigate if the current assumptions can be relaxed or not. For statistical inference part, it would be interesting to extend the TALE estimator to the case when the number of adaptive components is great than one.

Figure 3: Histograms of the scaled errors for TALE and OLS. Left: \(n=1000,d=10\). Right: \(n=500,d=50\). We choose the noise level \(=0.3\) and repeat the simulation \(1000\) times. Observe that the distribution of the OLS estimator is much more different than standard normal, and it exhibits a downwards bias , while TALE is in good accordance with a standard normal distribution.