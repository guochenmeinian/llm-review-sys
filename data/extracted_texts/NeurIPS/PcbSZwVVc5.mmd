# DreamCatcher: A Wearer-aware Sleep Event Dataset Based on Earables in Non-restrictive Environments

Zeyu Wang\({}^{1}\),  Xiyuxing Zhang\({}^{1}\),  Ruotong Yu\({}^{1}\),

Yuntao Wang\({}^{1}\),  Kenneth Christofferson\({}^{2}\),  Jingru Zhang\({}^{1}\),  Alex Mariakakis\({}^{2}\),

Yuanchun Shi\({}^{1,3}\)

\({}^{1}\) Department of Computer Science and Technology, Tsinghua University

\({}^{2}\) University of Toronto \({}^{3}\) Qinghai University

wang-zy23@mails.tsinghua.edu.cn, yuntaowang@tsinghua.edu.cn

equal contribution

###### Abstract

Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment. Widely available earbuds equipped with sensors (also known as earables) can be combined with a sleep event detection algorithm to offer a convenient alternative to laborious clinical tests for individuals suffering from sleep disorders. Although various solutions utilizing such devices have been proposed to detect sleep events, they ignore the fact that individuals often share sleeping spaces with roommates or couples. To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event algorithm development on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label. We tested multiple benchmark models on three tasks related to sleep event detection, demonstrating the usability and unique challenge of DreamCatcher. We hope that the proposed DreamCatcher can inspire other researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher is publicly available at https://github.com/thuhci/DreamCatcher.

## 1 Introduction

More than one-seventh of the global population suffers from at least one kind of sleep disorder, yet many are undiagnosed [6; 36; 41]. Sleep disorders can lead to various health issues, such as cardiovascular disease and depression [14; 20; 39]. The gold-standard diagnostic method, polysomnography (PSG), requires patients to spend the night in a specialized sleep clinic. Conducting such sleep studies can be cost-prohibitive and resource-intensive. Additionally, patients may suffer from the "first-night effect" where they exhibit anomalous sleep behavior when spending the night in a new environment . These challenges call for a minimally intrusive at-home sleep monitoring solution that can alert wearers to potential sleep disorders.

Many sleep disorders are associated with at least one detectable sleep event. For instance, obstructive sleep apnea (OSA) is characterized by the sudden cessation of snoring [4; 30], bruxism manifests as frequent teeth grinding or clenching , and restless sleep is often accompanied by excessive nighttime movement . Because it is difficult for people to recall these events while sleeping, continuous monitoring is crucial to facilitate diagnosis.

Recent research has shown that lightweight earables  can provide convenient real-time monitoring of human activity [11; 34; 45; 52; 46]. For sleep monitoring in particular, earables have unique advantages over other wearables like smartwatches and smartphones [3; 9; 24; 25; 48]. The ears are located on the head and close to the trunk of the body, allowing microphones to capture rich acoustic information generated during sleep. The in-ear feedback microphone included in active noise-cancelling earbuds can even detect subtle sounds produced within the body. For this work, we utilize a modified commercial earbud containing two microphones (feedback and feedforward) and an inertial measurement unit (IMU).

Advancements in hardware technology and machine learning algorithms have spurred increased research into sleep monitoring using commodity wearables. Current acoustic-based sleep event detection algorithms mainly focus on audio feature engineering [2; 13] or lightweight deep learning models . These solutions are often developed using data collected in controlled environments and contrived scenarios with minimal confounds (e.g., ambient noise). However, people often share sleep spaces with other individuals like roommates or spouses who may move and create sounds, leading to observable events not associated with the wearer [9; 13]. Moreover, these studies have not made their code or datasets publicly available.

We address these shortcomings by presenting and releasing DreamCatcher -- a large-scale, multi-modal, multi-sleeper sleep event dataset of earable data with fine-grained labels. We recruited 12 pairs (24 participants) of people who slept in the same room, and one person from each pair had a potential sleep disorder. We collected earable data from these pairs over the course of 420 hours and manually annotated 8 sleep events: teeth grinding, swallowing, somniloquy, breathing, coughing, snoring, and body movement. To demonstrate the utility of DreamCatcher, we present case studies of how it can be used to train baseline models that address three valuable tasks: wearer event identification, wearer-aware sleep sound event classification, and wearer-aware sleep sound event detection.

The main contributions of this work are as follows:

* We collected and released the first and largest sleep dataset based on multi-modal earable data collected in real scenarios with the disruption of sleep partners. Data is synchronized and annotated with fine-grained event labels.
* We benchmarked DreamCatcher on three sleep monitoring tasks: wearer event identification, wearer-aware sleep sound event classification, and wearer-aware sleep sound event detection.
* We provide open-source resources including the dataset, code for setting up benchmarks, and tutorial for constructing the earable hardware we used.

## 2 Related Work

### Contactless Sleep Monitoring and Wearer-Awareness

The gold standard for sleep monitoring is polysomnography (PSG), which entails wiring a series of sensors onto an individual in a sleep clinic for continuous monitoring and observation. PSG sessions are expensive, labour-intensive, and time-consuming , so researchers have shown substantial interest in developing more convenient sleep monitoring solutions suitable for home use.

Contactless sleep monitoring typically falls under one of two methods. The first method involves acoustic sensing of audible sounds using smartphones , smartwatches [9; 12], and earbuds [2; 11; 13; 33]. While these systems can work with commodity devices, they are prone to interference in multi-user settings. The second method relies on wireless sensing to detect body motion, respiration, and even heartbeats through minor chest movements at specific frequencies. Commonly used signals include WiFi , mmWave , and sonar . Wireless sensing makes it possible to manage multi-user scenarios since reflections from multiple users arrive at different times [49; 27]. However, dedicated devices for such approaches are non-trivial to deploy, and wireless sensing is less effective for detecting sleep events such as snoring, swallowing, or somniloquy.

In summary, acoustic and wireless solutions to contactless sleep monitoring show promise in addressing the multi-user challenge, yet cater to different aspects of sleep monitoring. Moreover, as indicated in , there is an inherent trade-off between accuracy and comfort.

### Sleep Monitoring with Earables

Compared to other commodity wearables for sleep monitoring, earables are worn in closer proximity to respiratory-vocal system and the external carotid artery, offering an ideal position for measuring behaviors and physiological parameters related to sleep . These opportunities have been leveraged using specialized biomedical sensors for sleep monitoring around the ear, such as in-ear EEG [23; 26; 40] and PPG  sensors, but these sensors are not widely available on commercial earables due to their high cost and integration complexity.

Some recent works have explored sleep monitoring by leveraging earables without modification, relying on motion sensors and in-ear microphones used for active noise cancellation. Leveraging the audio signals from earables, Ren et al.  developed a system that could track breathing rate and detect four sleep events. Christofferson et al.  utilized microphones in commercial earbuds for sleep sound classification. Their proposed SleepTSM model achieved promising performance in detecting seven different sleep events with a small footprint suitable for deployment on earables. Han et al.  proposed EarSleep, a similar sleep stage classification system dependent on acoustic sensing of body sounds.

Although the microphones on commercial earables have been used to great effect in sleep monitoring, such systems are often evaluated in controlled scenarios with a single participant at a time. In multi-sleeper scenarios, sounds may originate from people who are not wearing the earable, leading to mischaracterizations of the wearer's sleep experiences. Drawing inspiration from the EarSAVAS dataset , our work on DreamCatcher facilitates the development and evaluation of wearer-aware ubiquitous acoustic sleep event monitoring systems by providing a public sleep dataset that encompasses not only the wearer's sound events but also interference from non-wearers and non-restrictive environmental conditions.

### Sleep Datasets

Table 1 compares datasets across sensing modalities that have been leveraged for sleep monitoring research. It reveals multiple data-related challenges faced by previous works:

1. Although there has been substantial work utilizing ubiquitous sensors such as microphones in commodity wearables, the only publicly available datasets are for gold-standard PSG.
2. Previous wearable solutions reliant on audio phenomena have overlooked the interference of non-wearer in multiple sleeper scenarios.
3. Proprietary datasets using commodity wearables are typically limited in scale, both in terms of the number of participants and the quantity of data per person. This issue is particularly prevalent in research related to earables .

Our dataset aims to fill the gaps. To the best of our knowledge, DreamCatcher is the first open-source sleep event dataset targeted at ubiquitous sensors on commercial devices. Previous work has only considered single-sleeper scenarios. By integrating data from non-wearers, DreamCatcher facilitates

    &  &  &  &  &  \\    & & Acoustic & & Non-Acoustic & & Data Amount & Participants & Real & Open \\  SleepEDF  & PSG & ✗ & ✓ & 197 nights & - & ✓ & ✗ & ✓ \\ MASS  & PSG & ✗ & ✓ & 200 nights & 200 & ✓ & ✗ & ✓ \\ SleepTighter  & smartphone & ✓ & ✓ & 90 nights & 45 & ✓ & ✗ & ✗ \\ SleepGand  & smartwatch & ✓ & ✓ & 210 nights & 15 & ✓ & ✗ & ✗ \\ FuetTSNet  & – & ✓ & ✗ & 1 hour & - & ✗ & ✗ \\ SleepTSM  & earable & ✓ & ✗ & 6 hours & 20 & ✗ & ✗ & ✗ \\ EarlSleep  & earable & ✓ & ✗ & 48 nights & 18 & ✓ & ✗ & ✗ \\ Ren et al.  & earable/smartphone & ✗ & ✗ & - & 6 & ✓ & ✗ & ✗ \\ DreamCatcher (ours) & earable & ✓ (2-channel) & ✓ & 420 hours/62 nights & 24 & ✓ & ✓ & ✓ \\   

Table 1: Sleep Study Dataset Comparison.

the development and evaluation of wearer-aware sleep event monitoring. Our dataset consists of synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained labels of eight distinct sleep events. As the largest open-source sleep dataset to date, we envision DreamCatcher will advance wearer-aware sleep event monitoring on commercial earables.

## 3 DreamCatcher Dataset

### Dataset Collection

Hardware.Using a commodity earable is important because custom devices can often be optimized for data quality in ways that do not translate to existing platforms. Because commodity earables do not provide API access to their data streams, we had to modify an earbud for data acquisition. As shown in Figure 0(a), we integrated an MPU6050 IMU sensor into the hardware of Bose QC 20 earbuds, preserving the native feedback and feedforward microphone configuration. All sensors were controlled by a compact external development board, wherein the audio signal was sampled at 24 kHz and the IMU signal was sampled at approximately 94 Hz. To enhance user comfort, the development board was integrated into an enclosure and wrapped like a necklace. This device can function continuously for roughly 7 hours. Appendix A.1 contains more implementation details.

For gold-standard sleep monitoring data, we used a portable PSG system by Philips called the Alice PDx 4. This device includes a cannula and thermal sensor for measuring airflow in the nose, a chest strap for measuring chest expansion during breathing, a fingertip SpO2 sensor for measuring peripheral oxygen saturation, and limb movement sensors. This device was only worn by the person in each participant pair who reported a sleep disorder.

Participants.To collect data while accounting for the influence of a sleep partner, we recruited participants in dyads, imposing no constraints on their relationship as long as they shared a bedroom. A total of 12 pairs (24 participants) participated in the study, including 9 males and 15 females aged between 19 and 51 (average = 24.7, standard deviation = 8.3). Among the 12 participants who were equipped with the PDx device, 6 individuals were observed to exhibit varying degrees of sleep apnea.

Experiment Protocol.As shown in the first step of Figure 0(b), each pair of participants slept in the same bedroom for at least 6 hours. They were instructed to start their sleep session around the same time to maximize the amount of temporal overlap in their data. They were also asked to set an alarm clock that could be heard by both earphones; this sound was used for post-hoc manual data alignment. After the participants woke up and turned off the data collection hardware, they were required to fill out a PSQI  questionnaire to self-report their sleep quality.

Figure 1: Experiment Setup.

### Annotation and Statistics

Data Alignment.As shown in the second step of Figure 0(b), we performed post-hoc alignment for (1) the audio and IMU data in each earable and (2) each pair's audio data. The first round of alignment involving the data modalities within each earbud would hypothetically be trivial because the sensors should be intrinsically synchronized as they are connected to the same ESP32 board and controlled by the same microcontroller. However, we observed an inherent clock drift between the audio and IMU sampling protocols. Over a span of 7 hours, the IMU recording extended 3 seconds longer than the audio, accounting for a deviation of about 0.01%. To correct this, we re-scale the IMU data to match the audio recording duration, as the drift is evenly distributed over the entire recording period.

Because each participant's data was recorded independently by separate earbuds, the second round of alignment involved aligning data across participant pairs. To accomplish this, we utilized an alarm clock as a compensatory reference, manually adjusting the audio recordings to align with the alarm clock's spectrogram.

Annotation.Because data was collected from participants' homes, using video was not an acceptable form of annotation due to privacy concerns. Instead, we set up a hierarchical inspection process in which a team of annotators reviewed the earbud data to identify and label events. The annotators were asked to identify the eight sleep events listed in Table 2. They used Audacity5 to inspect each participant's binaural audio channel and IMU data as well as the sleep partner's binaural audio data simultaneously. The IMU data helped annotators determine the category of wearer-emitted events, while the sleep partner's audio helped them determine whether the event was emitted by the wearer. The annotation process, described more thoroughly in Appendix A.4, entailed selecting an interval for each event they noticed and then assigning a category to it. Each label was checked by at least three annotators; whenever they did not reach a consensus, voting was used to assign labels. Examples of each event are provided in Figure 2.

Dataset Statistics.Table 2 summarizes the prevalence and duration of each event type, while Figure 2(a) illustrates the distribution of durations of each event type. DreamCatcher is a highly imbalanced dataset, reflecting the natural scarcity of certain sleep disturbances such as bruxism, swallowing, somiloquy, and coughing.

Figure 2(b) shows the smoothed average frequency of different events over the course of a typical night of sleep. Note that participants slept for different amounts of time, so there may be some misalignment in the timing of events across individuals. However, the plot reflects some known observations about sleeping. For example, movement and swallowing were less prevalent after the first hour of sleep, while snoring and somniloquy became more prevalent.

Figure 2: Examples of Each Sleep Event.

[MISSING_PAGE_FAIL:6]

Benchmarks

### Wearer Event Identification

Task Description.In multi-sleeper scenarios, sleep event monitoring using acoustic methods suffers from the interference of external sounds not produced by the earable wearer. Therefore, we define wearer identification as a binary classification task focused on determining whether audio events come from the wearer or other sources. To the best of our knowledge, no previous work has explored this topic in the context of sleep event monitoring.

Dataset Preparation.For this task, we assume that candidate events have been separated from silence using a simple threshold-based approach and focus only on data that our annotators labeled. After segmenting synchronous dual-channel audio and motion data according to the fine-grained labels, we extracted the features listed in Table 2(a) from each event. The primary challenge for this task is the design of features that are computationally efficient for wearable devices with limited processing capabilities, capable of distinguishing events caused by the sleep partner.

Due to the low intensity of background noise typically seen in real-world sleep scenarios, we relied on acoustic features to distinguish between wearer and non-wearer events. We calculated traditional acoustic features like zero-crossing rate (ZCR) and root mean square (RMS) on both the feedforward and feedback channels. We also calculated three inter-channel audio features -- RMS-ED, Mel-FD, and TDOA -- that model the different propagation characteristics of sounds. Based on the observation that bone-conducted sound from the wearer should have higher energy at the feedback microphone than at the feedforward microphone, RMS-ED measures the root mean square of the energy difference between the two audio channels. Given the different propagation paths of wearer and non-wearer sounds reaching the ear, Mel-FD measures the energy difference between the Fast Fourier Transforms (FFTs) calculated from both audio channels according to the Mel scale. Finally, time difference of arrival (TDOA) between the two channels reflects the propagation path difference between wearer and non-wearer sounds.

Since the wearer's sleep events are often accompanied by body movements that are captured by the earables' motion sensors, we also extracted motion-related features. We first calculated the overall magnitude of the accelerometer and gyroscope data separately, after which we computed IMU-STD as the mean and standard deviation of those magnitudes over time.

Benchmark Methods.Given the low dimensionality of the input data, we benchmarked five traditional machine learning models: two low-complexity models (logistic regression and linear SVM) and three high-complexity models (random forest, decision tree, and AdaBoost).

Model Training and Evaluation Metrics.To evaluate the performance of the models, we trained and evaluated each one using leave-one-user-out cross-validation. Since this is a binary classification task, we report performance according to accuracy, F1 score, precision, recall, and AUC.

Results.Table 2(a) shows that most of our features could be computed using fewer than 1 M FLOPs across our entire dataset. The results in Table 2(b) demonstrate that the high-complexity models achieved similarly higher accuracy compared to the low-complexity ones. According to the feature importance scores presented in Appendix B.1, we found that models with higher complexity are more effective in leveraging the inter-channel audio features. Furthermore, we observed that motion features were important for all models, particularly those that were less complex. These results highlight the utility of inter-channel audio and motion features for wearer awareness of sleep monitoring.

Table 3: Benchmarks for Wearer Event Identification.

### Wearer-Aware Sleep Sound Event Classification

Task Description.Sleep sound event classification serves as the foundation of sleep disorder diagnosis. Han et al.  also revealed that the categorization of sleep sound events also facilitates sleep stage inference. Although algorithms already exist for this task, the interference caused by non-wearers is often overlooked, limiting their applicability in multi-sleeper scenarios. Inspired by EarSAVAS , we define wearer-aware sleep sound event classification as an (\(n+1\))-class multi-classification task, where \(n\) represents the number of target events and the remaining class encompasses both ambient and non-wearer sounds.

Dataset Preparation.As with wearer event identification, we assume that event onset and offset are already known for this task. To standardize the input data size, we cropped the synchronous audio and motion data into 5-second clips that were sufficiently long to cover the duration of the longest event in our dataset.

Benchmark Methods.We examined five state-of-the-art models for this task:

1. **SleepTSM** is a lightweight sleep sound classification model that was not evaluated with multiple sleepers in the same room.
2. **EarVAS  and its variants** were evaluated on the EarSAVAS dataset to demonstrate subject-aware vocal activity classification utilizing dual-channel audio and motion data.
3. **Wav2Vec2.0**, **BEATs**, and **CLAP** are generic audio event classification methods.

Besides EarVAS, the other models are only designed to support single-channel audio input. Since DreamCatcher includes audio from both the feedforward and feedback microphones, we evaluated these models on each of those channels separately. All model pre-processing steps and hyperparameters were configured identically to those in the original works we replicated. Appendix C.1 shows the details of the partition of our dataset and the training details of every benchmark model.

Model Training and Evaluation Metrics.Each model was evaluated using leave-one-user-out cross-validation. Since this is a multi-class task, we used accuracy, macro-averaged AUC, macro-averaged F1 score, and MCC as evaluation metrics. We also report model complexity according to FLOPs and the number of parameters.

Results.As shown in Table 4, most of the models achieved accuracies above 70%; the exceptions were CLAP and a configuration of EarVAS that only used IMU data. However, the macro-F1 scores were typically far lower. This is largely due to the significant class imbalance of our dataset, as some events are far more common than others. Another hurdle encountered by these models was the challenge of jointly optimizing wearer event identification and sleep sound classification. We used a single model to perform both tasks simultaneously, but a dual-stage pipeline may be more appropriate in future work. Appendix C.2 provides a more thorough analysis of the results, showing the efficacy of the feedback microphone in detecting low-intensity events like swallowing and highlighting the promise of sensor fusion for future explorations.

    &  &  & FLOPs (G) & Params. (M) \\   & & Acc. & Macro-AUC & Macro-F1 & MCC \\  SleepTSM  & feedback & 73.01 & 63.51 & 35.61 & 49.98 & 0.927 & 0.37 \\ SleepTSM & feedforward & 72.89 & 64.00 & 36.97 & 50.76 & 0.927 & 0.37 \\ EarVA5  & all & 78.07 & 71.49 & 36.76 & 49.22 & 0.354 & 12.90 \\ EarVAS & dual-channel-audio & 76.64 & 77.36 & 38.77 & 51.12 & 0.040 & 4.40 \\ EarVAS & feedback & 75.75 & 75.75 & 34.68 & 46.84 & 0.040 & 4.40 \\ EarVA5 & feedforward & 76.99 & 75.06 & 34.76 & 43.02 & 0.040 & 4.40 \\ EarVAS & 1-moi-0 & 45.03 & 60.56 & 13.75 & 12.24 & 0.313 & 8.50 \\ BeATs  & feedback & 90.73 & 58.08 & 37.47 & 66.00 & 22.46 & 90.51 \\ BeATs & feedforward & 89.64 & 78.93 & 55.51 & 90.04 & 22.46 & 90.51 \\ Wav2Vec2.0  & feedback & 75.45 & 91.60 & 48.36 & 56.72 & 26.84 & 94.39 \\ Wav2Vec2.0  & feedforward & 73.29 & 88.84 & 42.52 & 54.11 & 26.84 & 94.39 \\ CLAP (zero-shot)  & feedback & 37.04 & 65.57 & 16.77 & 18.21 & 53.03 & 190.80 \\ CLAP (zero-shot) & feedforward & 37.35 & 65.64 & 17.31 & 18.98 & 53.03 & 190.80 \\   

Table 4: Benchmarks for Wearer-Aware Sleep Sound Event Classification.

### Wearer-Aware Sleep Sound Event Detection

Task Description.Knowing when a sleep event starts and stops is crucial for sleep monitoring, as the temporal distribution and order of events provide critical insights into sleep progression . Inspired by sound event detection (SED) systems and the DCASE Challenge , we define wearer-aware sleep sound event detection as a task that involves determining not only the category of an event but also its onset and offset.

Dataset Preparation.Following the data format standards from the DCASE Challenges , we used 10-second clips for this experiment so that the models would have enough context for precise event detection.

Benchmark Methods.State-of-the-art sound event detection methods predominantly employ deep learning, with most of them being built upon convolutional recurrent neural networks (CRNNs). According to Mesaros et al. , such methods have been both trained from scratch and have utilized transfer learning to shortcut learning. We benchmarked SEDNet  and ATST-SED  to represent these two categories, respectively. We selected SEDNet because of its pioneering role in using CRNNs with multi-channel microphone data for sound event detection. On the other hand, we selected ATST-SED because it outperformed all competitors on the DESED dataset .

Model Training and Evaluation Metrics.Each model was evaluated using leave-one-user-out cross-validation. We used conventional collar-based metrics  including event-based macro-averaged F1 score and error rate to quantify model performance.

Results.According to the results shown in Table 5, we found that ATST-SED achieved significantly better performance at the cost of a much larger footprint. We also observed that both models were more accurate when they were trained using feedforward microphone audio. In fact, SEDNet trained on multiple audio channels achieved the lowest macro-F1 score out of all the configurations we tested. Appendix D.3 provides a more thorough analysis of the results.

## 5 Limitations and Future Work

First of all, the natural frequency distribution of sleep events leads to a highly imbalanced dataset in DreamCatcher, with rarer events often holding greater significance. Based on the DreamCatcher dataset, we generated a balanced dataset through data augmentation methods and trained classification models on it, as shown in Appendix C.3. We envision the generation of rare sleep events, which must aligns with the patterns of human sleep, will be a highly valuable area for future research.

Moreover, privacy concerns in multi-sleeper settings preclude video verification, resulting in potential label inaccuracies despite requiring a consensus among at least three annotators for challenging-to-identify events. Furthermore, the emergence of commercial earphones equipped with physiological sensors like photoplethysmography (PPG) presents an opportunity to enhance DreamCatcher with additional data modalities in future iterations.

The current prototype earbuds used in our study may not be the epitome of comfort for all users, especially for those who have difficulties falling asleep. However, the existence of commercially available sleep earbuds that are small, soft, and ergonomically designed (e.g., Amazfit Zenbuds6 and

    &  &  &  &  \\    & & Macro-F1 (\%) & & & \\  SEDNet  & dual-channel audio & 14.02 & 0.97 & 0.31 & 0.37 \\ SEDNet & feedback & 18.85 & 0.91 & 0.30 & 0.37 \\ SEDNet & feedforward & 17.98 & 0.98 & 0.30 & 0.37 \\ ATST-SED  & feedback & 24.73 & 0.85 & 44.16 & 172.9 \\ ATST-SED & feedforward & 24.10 & 0.85 & 44.16 & 172.9 \\   

Table 5: Benchmarks for Wearer-Aware Sleep Sound Event Detection.

Bose Sleepbuds7) underscores the potential for earbuds to become a comfortable and viable sleep monitoring platform. These options point towards a promising future for the application of earbud technology in sleep studies.

## 6 Conclusion

This paper introduces DreamCatcher, the first open-source dataset featuring multi-sleeper, multi-modal data from a commodity device along with fine-grained annotations of sleep disorder-related sound events. DreamCatcher encompasses 420 hours of synchronized dual-channel audio and motion data, offering a rich and challenging resource for sleep monitoring. We validated DreamCatcher's utility by establishing benchmarks across three distinct tasks, and we hope that these results motivate other researchers to innovate further on our dataset.

## 7 Acknowledgement

This work is supported by Natural Science Foundation of China under Grant No. 62472244, No. 62132010 and No. 62222606, University of Toronto - Tsinghua University Joint Research Fund, Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant (#RGPIN-2021-03457), Tsinghua University Initiative Scientistic Research Program, Beijing Key Lab of Networked Multimedia, Institute for Artificial Intelligence, Tsinghua University (THUAI). Thank to the participants involved in data collection and to the professional annotators who contributed to data labeling.