# Improved Frequency Estimation Algorithms with and without Predictions

Anders Aamand

MIT

aamand@mit.edu

Justin Y. Chen

MIT

justc@mit.edu

Huy Le Nguyen

Northeastern University

hu.nguyen@northeastern.edu

Sandeep Silwal

MIT

silwal@mit.edu

Ali Vakilian

TTIC

vakilian@ttic.edu

###### Abstract

Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al. (2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. _without_ the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.

## 1 Introduction

In frequency estimation, we stream a sequence of elements from \([n]:=\{1,,n\}\), and the goal is to estimate \(f_{i}\), the frequency of the \(i\)th element, at the end of the stream using low-space. Frequency estimation is one of the central problems in data streaming with a wide range of applications from networking (gathering important monitoring statistics ) to machine learning (NLP , feature selection , semi supervised learning ). CountMin (CM)  and CountSketch (CS)  are arguably the most popular and versatile of the algorithms for frequency estimation, and are implemented in many popular packages such as Spark , Twitter Algebird , and Redis.

Standard approaches to frequency estimation are designed to perform well in the worst-case due to the multitudinous benefits of worst-case guarantees. However, algorithms designed to handle any possible input do not exploit special structure of the particular distribution of inputs they are used for. In practice, these patterns can be described by domain experts or learned from historical data. Following the burgeoning trend of combining machine learning and classical algorithm design,  initiated the study of _learning-augmented_ frequency estimation by extending the classical CM and CS algorithms in a simple but effective manner via a heavy-hitters oracle. During a training phase, they construct a classifier (e.g. a neural network) to detect whether an element \(i\) is "heavy" (e.g., whether \(f_{i}\) is among the most frequent items). After such a classifier is trained, they scan the input stream, and apply the classifier to each element \(i\). If the element is predicted to be heavy, it is allocated a unique bucket, so that an exact value of \(f_{i}\) is computed. Otherwise, the stream element is inputted into the standard sketching algorithms.

The advantage of their algorithm was analyzed under the assumption that the true frequencies follow a heavy-tailed Zipfian distribution. This is a common and natural reoccurring pattern in real world data where there are a few very frequent elements and many infrequent elements. Experimentally,  showed several real datasets where the Zipfian assumption (approximately) held and useful heavy-hitter oracles could be trained in practice. Our paper is motivated by the following natural questions and goals in light of prior works:

_Can we design better frequency estimation algorithms (with and without predictions) for heavy-tailed distributions?_

In particular, we consider the setting of  where the underlying data follow a heavy-tailed distribution and investigate whether sketching algorithms can be further tailored for such distributions. Before tackling this question, we must tightly characterize the benefits-and limitations-of these existing methods, which is another goal of our paper:

_Give tight error guarantees for CountMin and CountSketch, as well as their learning-augmented variants, on Zipfian data._

Lastly, any algorithms we design must possess worst case bounds in the case that either the data does not match our Zipfian (or more generally, heavy-tailed) assumption or the learned predictions have high error, leading to the following 'best of both worlds' goal:

_Design algorithms which exploit heavy tailed distributions and ML predictions but also maintain worst-case guarantees._

We addresses these challenges and goals and our contributions can be summarized as follows:

* We give tight upper and lower bounds for CM and CS, with and without predictions, for heavy tailed distributions. A surprising conclusion from our analysis is that (for a natural error metric) a constant number of rows is optimal for both CM and CS. In addition, our theoretical analysis shows that CS outperforms CM, both with and without predictions, validating the experimental results of .
* We go beyond CM and CS based algorithms to give a better frequency estimation algorithm for heavy tailed distributions, with and without the use of predictions. We show that our algorithms can deliver up to a logarithmic factor improvement in the error bound over CS and its learned variant. In addition, our algorithm has worst case guarantees.
* Prior learned approaches require querying an oracle for every element in the stream. In contrast, we obtain a _parsimonious_ version of our algorithm which only requires a limited number of queries to the oracle. The number of queries we use is approximately equal to the given space budget.
* Lastly, we evaluate our algorithms on two real-world datasets with and without ML based predictions and show superior empirical performance compared to prior work in all cases.

### Preliminaries

Notation and Estimation ErrorThe stream updates an \(n\) dimensional frequency vector and every stream element is of the form \((i,)\) where \(i[n]\) and \(\) denotes the update on the coordinate. The final frequency vector is denoted as \(f^{n}\). Let \(N=_{i[n]}f_{i}\) denote the sum of all frequencies. To simplify notation, we assume that \(f_{1} f_{2} f_{n}\). \(_{i}\) denotes the estimate of the frequency \(f_{i}\). Given estimates \(\{_{i}\}_{i[n]}\), the error of a particular frequency is \(|_{i}-f_{i}|\). We also consider the following notion of overall weighted error as done in :

\[=_{i[n]}f_{i}|_{i}-f_{i }|. \]

The weighted error can be interpreted as measuring the error with respect to a query distribution which is the same as the actual frequency distribution. As stated in , theoretical guarantees of frequency estimation algorithms are typically phrased in the traditional \((,)\)-error formulations. However as argued in there, the simple weighted objective (1) is a more holistic measure and does not require tuning of two different parameters, and is thus more natural from an ML perspective.

Zipfian StreamWe also work under the common assumption that the frequencies follow the Zipfian law, i.e., the \(i\)th largest frequency \(f_{i}\) is equal to \(A/i\) for some parameter \(A\). Note we know \(A\) at the end of the stream since the stream length is \(A H_{n}\). By rescaling, we may assume that \(A=1\) without loss of generality. We will make this assumption throughout the paper.

CountMin (CM)For parameters \(k\) and \(B\), which determine the total space used, CM uses \(k\) independent and uniformly random hash functions \(h_{1},,h_{k}:[n][B]\). Letting \(C\) be an array of size \([k][B]\) we let \(C[,b]=_{j[n]}[h_{}(j)=b]f_{j}\). When querying \(i[n]\) the algorithm returns \(_{i}=_{[k]}C[,h_{}(i)]\). Note that we always have that \(_{i} f_{i}\).

CountSketch (CS)In CS, we again have the hash functions \(h_{i}\) as above as well as sign functions \(s_{1},,s_{k}:[n]\{-1,1\}\). The array \(C\) of size \([k][B]\) is now tracks \(C[,b]=_{j[n]}[h_{}(j)=b]s_{}(j)f_{j}\). When querying \(i[n]\) the algorithm returns the estimate \(_{i}=_{[k]}s_{}(i) C[,h_{}(i)]\).

Learning-Augmented Sketches Given a base sketching algorithm (either CM or CS) and a space budget \(B\), the corresponding learning-augmented algorithm (learned CM or learned CS) allocates a constant fraction of the space \(B\) to the base sketching algorithm and the rest of the space to store items identified as heavy by a learned predictor. These items predicted to be heavy-hitters are stored in a separate table which maintains their counts exactly, and their updates are not sent to the sketching algorithm.

### Summary of Main Results and Paper Outline

Our analysis, both of CM and CS, our algorithm, and prior work, is summarized in Table 1.

Summary of Theoretical ResultsWe interpret Table 1\(B\) denotes the space bound, which is the total number of entries used in the CM or CS tables. First note that CS achieves lower weighted error compared to CM, proving the empirical advantage observed in . However, the learned version of CS only improves upon standard CS in the regime \(B=n^{1-o(1)}\). While this setting does appear sometimes in practice  (referred to as high-accuracy regime), for CS, learning gives no asymptotic advantage in the low space regime.

On the other hand, in the low space regime of \(B=( n)\), our algorithm, without predictions, already archives close to a logarithmic factor improvement over even _learned_ CS. Furthermore, our learning-augmented algorithm achieves a logarithmic factor improvement over classical CS across all space regimes, whereas the learned CS only achieves a logarithmic factor improvement in the regime \(B=n^{1-o(1)}\). Furthermore, our learned version outperforms or matches learned CS in all space regimes.

   Algorithm & Weighted Error & Uses Predictions? & Reference \\  CountMin (CM) & \(()\) & No & Theorem & B.1 \\ CountSketch (CS) & \(()\) & No & Theorem & C.4 \\ Learned CountMin & \((}{B n})\) & Yes &  \\ Learned CountSketch & \(()\) & Yes & Theorem & D.1 \\  Our (Without predictions) & \(O(( n)}{B n})\) & No & Theorem & 2.1 \\ Our (Learned version) & \(O()\) & Yes & Theorem & 3.1 \\   

Table 1: Bounds are stated assuming that the total space is \(B\) words of memory. Weighted error means that element \(i\) is queried with probability proportional to \(1/i\). Moreover, the table considers normalized frequencies, so that \(f_{i}=1/i\).

Our learning-augmented algorithm can also be made _parsimonious_ in the sense that we only query the heavy-hitter oracle \((B)\) times. This is desirable in large-scale streaming applications where evaluating even a small neural network on every single element would be prohibitive.

**Remark 1.1**.: _We remark that all bounds in this paper are proved by bounding the expected error when estimating the frequency of a single item, \([|_{i}-f_{i}|]\), then using linearity of expectation. While we specialized our bounds to a query distribution which is proportional to the actual frequencies in (1), our bounds can be easily generalized to any query distribution by simply weighing the expected errors of different items according to the given query distribution._

Summary of Empirical ResultsWe compare our algorithm without prediction to CS and our algorithm with predictions to that of  on synthetic Zipfian data and on two real datasets corresponding to network traffic and internet search queries. In all cases, our algorithms outperform the baselines and often by a significant margin (up to \(\) in one setting). The improvement is especially pronounced when the space budget is small.

Outline of the PaperOur paper is divided into roughly two parts. One part covers novel and tight analysis of the classical algorithms CountMin (CM) and CountSketch (CS). The second part covers our novel algorithmic contributions which go beyond CM and CS. The main body of our paper focuses on our novel algorithmic components, i.e. the second part, and we defer our analysis of the performance of CountMin (CM) and CountSketch (CS), with and without predictions, to the appendix: in Section B we give tight analysis of CM for a Zipfian frequency distribution. In Section C we give the analogous bounds for CS. Lastly, Section D gives tight bounds for CS with predictions. Section D covers our better frequency estimation without predictions while Section E covers the learning-augmented version of the algorithm, as well as its extentions.

### Related Works

Frequency EstimationWhile there exist other frequency estimation algorithms beyond CM and CS (such as  ) we study hashing based methods such as CM  and CS  as they are widely employed in practice and have additional benefits, such as supporting insertions _and deletions_, and have applications beyond frequency estimation, such as in machine learning (feature selection , compressed sending , and dimensionality reduction  etc.).

Learning-augmented algorithmsThe last few years have witnessed a rapid growth in using machine learning methods to improve "classical" algorithmic problems. For example, they have been used to improve the performance of data structures , online algorithms , combinatorial optimization , similarity search and clustering . Similar to our work, sublinear constraints, such as memory or sample complexity, have also been studied under this framework .

## 2 Improved Algorithm without Predictions

We first present our frequency estimation algorithm which does not use any predictions. Later, we build on top of it for our final learning-augmented frequency estimation algorithm.

The main guarantees of of the algorithm is the following:

**Theorem 2.1**.: _Consider Algorithm 1 with space parameter \(B n\) updated over a Zipfian stream. Let \(\{_{i}\}_{i=1}^{n}\) denote the estimates computed by Algorithm 2. The expected weighted error 1 is \([_{i=1}^{n}f_{i}|f_{i}-_{i}| ]=O()\)._

Algorithm and Proof intuition:Let \(B^{}=/ n\). At a high level, we show that for every \(i B^{}\), we execute line 10 of Algorithm 2 and the error satisfies \(|1/i-_{i}| 1/B^{}\) (recall in the Zipfian case, the \(i\)th largest frequency is \(f_{i}=1/i\)). On the other hand, for \(i B^{}\), we show that (with sufficiently high probability) line 8 of Algorithm 2 will be executed, resulting in \(|1/i-_{i}|=|1/i-0|=1/i\).

```
1:Input:** Stream of updates to an \(n\) dimensional vector, space budget \(B\)
2:procedureUpdate
3:\(T( n)\)
4:for\(j=1\) to \(T-1\)do
5:\(S_{j}\) CountSketch table with \(3\) rows and \(\) columns
6:endfor
7:\(S_{T}\) CountSketch table with \(3\) rows and \(\) columns
8:for stream element \((i,)\)do
9: Input \((i,)\) in each of the \(T\) CountSketch tables \(S_{j}\)
10:endfor
11:endprocedure
```

**Algorithm 1** (Not augmented) Frequency update algorithm

It might be perplexing at first sight why we wish to set the estimate to be \(0\), but this idea has solid intuition: it turns out the _additive_ error of standard CountSketch with \(B^{}\) columns is actually of the order \(1/B^{}\). Thus, it does not make sense to estimate elements whose true frequencies are much smaller than \(1/B^{}\) using CountSketch. A challenge is that we do not know a priori which elements these are. We circumvent this via the following reasoning: if CountSketch itself outputs \( 1/B^{}\) as the estimate, then either one of the following must hold:

* The element has frequency \(1/i 1/B^{}\), in which case we should set the estimate to \(0\) to obtain error \(1/i\), as opposed to error \(1/B^{}-1/i 1/B^{}\).
* The true element has frequency \( 1/B^{}\) in which case either using the output of the CountSketch table or setting the estimate to \(0\) both obtain error approximately \(O(1/B^{})\), so our choice is inconsequential.

In summary, the output of CountSketch itself suggests whether we should output an estimated frequency as \(0\). We slightly modify the above approach with \(O( n)\) repetitions to obtain sufficiently strong concentration, leading to a _robust_ method to identify small frequencies. The proof formalizes the above plan and is given in full detail in Section.

By combining our algorithm with predictions, we obtain improved guarantees.

## 3 Improved Learning-Augmented Algorithm

**Theorem 3.1**.: _Consider Algorithm,with space parameter \(B n\) updated over a Zipfian stream. Suppose we have access to a heavy-hitter oracle which correctly identifies the top \(B/2\) heavy-hitters in the stream. Let \(\{_{i}\}_{i=1}^{n}\) denote the estimates computed by Algorithm, The expected weighted error  is \([_{i=1}^{n}f_{i}|f_{i}-_{i}| ]=O().\)_Algorithm and Proof Intuition:Our final algorithm follows a similar high-level design pattern used in the learned CM algorithm of : given an oracle prediction, we either store the frequency of heavy element directly, or input the element into our algorithm from the prior section which does not use any predictions.

The workhorse of our analysis is the proof of Theorem 2.1 First note that we obtain \(0\) error for \(i<B/2\). Thus, all error comes from indices \(i B/2\). Recall the intuition for this case from Theorem 2.1 we want to output \(0\) as our estimates as this results in lower error than the additive error from CS. The same analysis as in the proof of Theorem 2.1 shows that we are able to detect small frequencies and appropriately output an estimate from either the \(T\)th CS table or output \(0\).

### Parsimonious Learning

In Theorem 2.1 we assumed access to a heavy-hitter oracle which we can use on every single stream element to predict if it is heavy. In practical streaming applications, this will likely be infeasible. Indeed, even if the oracle is a small neural network, it is unlikely that we can query it for every single element in a large-scale streaming application. We therefore consider the so called _parsimonious_ setting with the goal of obtaining the same error bounds on the expected error but with an algorithm that makes _limited queries_ to the heavy-hitter oracle. This setting has recently been explored for other problems in the learning-augmented literature .

Our algorithm works similarly to Algorithm 2 except that when an element \((i,)\) arrives, we only query the heavy-hitter oracle with some probability \(p\) (proportional to \(\)). We will choose \(p\) so that we in expectation only query \((B)\) elements, rather than querying the entire stream. To be precise, whenever an item arrives, we first check if it is already classified as one of the top \(B/2\) heavy-hitters in which case, we update its exact count (from the point in time where was classified as heavy). Otherwise, we query the heavy-hitter oracle with probability \(p\). In case the item is queried and is indeed one of the top \(B/2\) heavy-hitters, we start an exact count of that item. An arriving item which is not used as a query for the heavy-hitter oracle and was not earlier classified as a heavy-hitter is processed as in Algorithm 1.

Querying for an element, we first check if it is classified as a heavy-hitter and if so, we use the estimate from the separate lookup table. If not, we estimate its frequency using Algorithm 3. With this algorithm, the count of a heavy-hitter will be underestimated since it may appear several times in the stream before it is used as a query for the oracle and we start counting it exactly. However, with our choice of sampling probability, with high probability it will be sampled sufficiently early to not affect its final count too much. We present the pseudocode of the algorithm as well as the precise result and its proof in Appendix 3.

### Algorithm variant with worst case guarantees

In this section we discuss a variant of our algorithm with worst case guarantees. To be more precise, we consider the case where the actual frequency distribution is not Zipfian. The algorithm we discuss is actually a more general case of Algorithm 2 and in fact, it completely recovers the asymptotic error guarantees of Theorem 2.1 (as well as Theorem 3.1 if we use predictions).

Recall that Algorithm 2 outputs \(0\) when the estimated frequency is below \(T/B\) for \(T=O( n)\). This parameter has been tuned to the Zipfian case. As stated in Section 2 the main intuition for this parameter is that it is of the same order as the additive error inherent in CountSketch, which we discuss now. Denote by \(f_{}}\) the frequency vector where we zero out the largest \(P\) coordinates. For every frequency, the expected additive error incurred by a CountSketch table with \(B^{}\) columns is \(O(\|f_{}}\|_{2}/})\). In the Zipfian case, this is equal to \(O(}}\|_{2}}{}})=O (})\), which is exactly the threshold we set \(\) Thus, our robust variant simply replaces this tuned parameter \(O(T/B)\) with an estimate of \(O(\|f_{}}\|_{2}/})\) where \(B^{}=B/T\). We given an algorithm which efficiently estimates this quantity in a stream. Note this quantity is only needed for the query phase.

**Lemma 3.2**.: _With probability at least \(1-((B))\), Algorithm 1 outputs an estimate \(V\) satisfying \((\|f_{}}\|_{2}^{2}/B^{}) V  O(\|f_{}/10}\|_{2}^{2}/B^{})\)._

The algorithm and analysis are given in Section 2 Replacing the threshold in Line \(7\) of Algorithm 2 with the output of Algorithm 3 (more precisely the square root of the value) readily gives us the following worst case guarantees. Lemma 3.3 states that the expected error of the estimates outputted by Algorithm 2 using \(B\), regardless of the true frequency distribution, is no worse than that of a standard CountSketch table using slightly smaller \(O(B/ n)\) space.

**Lemma 3.3**.: _Suppose \(B n\). Let \(\{_{i}\}_{i=1}^{n}\) denote the estimates of Algorithm 2 using \(B/2\) space and with Line \(7\) replaced by the square root of the estimate of Algorithm 3 also using \(B/2\) space. Suppose the condition of Lemma 3.2 holds. Let \(\{_{i}^{}\}_{i=1}^{n}\) denote the estates computed by a CountSketch table with \(\) columns for a sufficiently small constant \(c\). Then, \([|_{i}-f_{i}|][|_{i}^{}-f_{i}|]\)._

**Remark 3.1**.: _The learned version of the algorithm automatically inherits any worst case guarantees from the unlearned (without predictions) version. This is because we only set aside half the space to explicitly track the frequency of some elements, which has worst case guarantees, while the other half is used for the unlearned version, also with worst case guarantees._

## 4 Experiments

We experimentally evaluate our algorithms with and without predictions on real and synthetic datasets and demonstrate that the improvements predicted by theory hold in practice. Comprehensive additional figures are given in Appendix 3.

Algorithm ImplementationsIn the setting without predictions, we compare our algorithm to CountSketch (CS) (which was shown to have favorable empirical performance compared to CountMin (CM) in  and better theoretical performance due to our work). In the setting with predictions, we compare the algorithm of , using CS as the base sketch and dedicated half of the space for itemswhich are predicted to be heavy by the learned oracle. For all implementations, we use three rows in the CS table and vary the number of columns. We additionally augment each of these baselines with a version that truncates all negative estimated frequencies to zero as none of our datasets include stream deletions. This simple change does not change the asymptotic \((,)\) classic sketching guarantees but does make a big difference when measuring empirical weighted error.

We implement a simplified and practical version of our algorithm which uses a single CS table. If the median estimate of an element is below a threshold of \(Cn/w\) for domain size \(n\), sketch width \(w\) (a third of the total space), and a tunable constant \(C\), the estimate is instead set to \(0\). As all algorithms use a single CS table as the basic building block with different estimation functions, for each trial we randomly sample hash functions for a single CS table and only vary the estimation procedure used.

We evaluate algorithms according the weighted error as in Equation 1 but also according to unweighted error which is simply the sum over all elements of the absolute estimation error, given by \(_{i}|f_{i}-_{i}|\). Space is measured by the size of the sketch table, and all errors are averaged over 10 independent trials with standard deviations shown shaded in.

DatasetsWe compare our algorithm with prior work on three datasets. We use the same two real-world datasets and predictions from : the CAIDA and AOL datasets. The CAIDA dataset  contains 50 minutes of internet traffic data. For each minute of data, the stream is formed of the IP addresses associated with packets going through a Tier1 ISP. A typical minute of data contains 30 million packets accounted for by 1 million IPs. The AOL dataset  contains 80 days of internet search queries with a typical day containing \( 3 10^{5}\) total queries and \( 10^{5}\) unique queries. As shown in Figure1 both datasets approximately follow a power law distribution. For both datasets, we use the predictions from prior work  formed using recurrent neural networks. We also generate synthetic data following a Zipfian distribution with \(n=10^{7}\) elements and where the \(i\)th element has frequency \(n/i\).

ResultsAcross the board, our algorithm outperforms the baselines. On the CAIDA and AOL datasets without predictions, our algorithm consistently outperforms the standard CS with up to **4x** smaller error with space \(300\). This gap widens when we compare our algorithm with predictions

Figure 1: Log-log plots of the sorted frequencies of the first day/minute of the CAIDA/AOL datasets. Both data distributions are heavy-tailed with few items accounting for much of the total stream.

Figure 2: Comparison of weighted error without predictions on the CAIDA dataset. The left plot compares the performance of various algorithms (including our algorithm with different choices of \(C\)) for a fixed dataset and varying space. The right plot compares algorithms over time across separate streams for each minute of data for a specific choice of space being \(750\).

to that of  with a gap of up to **17x** with space \(300\). In all cases, the performance of CS and  is significantly improved by the simple trick of truncating negative estimates to zero. However, our algorithm still outperforms these "nonneg" baselines. The longitudinal plots which compare algorithms over time show that our algorithm consistently outperforms the state-of-the-art with and without predictions.

In the case of the CAIDA dataset, predictions do not generally improve the performance of any of the algorithms. This is consistent with the findings of  where the prediction quality for the CAIDA dataset was relatively poor. However, for the AOL which has a more accurate learned oracle, our algorithm in particular is significantly improved when augmented with predictions. Intuitively, the benefit of our algorithm comes from removing error due to noise for low frequency elements. Conversely, good predictions help to obtain very good estimates of high frequency elements. In combination, this yields very small total weighted error.

In Appendix [\(}\)] we display comprehensive experiments of the performance of the algorithms across the CAIDA and AOL datasets with varying space and for both weighted and unweighted error as well as results for synthetic Zipfian data. In all cases, our algorithm outperforms the baselines. On synthetic Zipfian, the gap between our algorithm and the non-negative CS for weighted error is relatively small compared to that for the real datasets. While we mainly focus on weighted error in this work, the benefits of our algorithm are even more significant for unweighted error as setting estimates below the noise floor to zero is especially impactful for this error measure. In general, we see the trend, matching our theoretical results, that as space increases, the gap between the different algorithms shrinks as the estimates of the base CS become more accurate.