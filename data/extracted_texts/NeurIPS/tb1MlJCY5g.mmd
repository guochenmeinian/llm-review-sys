# KALM: Knowledgeable Agent by Offline Reinforcement Learning from Large Language Model Rollouts

Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Xiong-Hui Chen, Nan Tang, Yang Yu

National Key Laboratory for Novel Software Technology, Nanjing University, China

& School of Artificial Intelligence, Nanjing University, China

Polixir Technology

{pangjc,yangsh,liky,chenxh,tangn}@lamda.nju.edu.cn, yuy@nju.edu.cn

Corresponding: yuy@nju.edu.cn.

###### Abstract

Reinforcement learning (RL) traditionally trains agents using interaction data, which limits their capabilities to the scope of the training data. To create more knowledgeable agents, leveraging knowledge from large language models (LLMs) has shown a promising way. Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration. This paper introduces a novel approach, KALM (Knowledgeable Agents from Language Model Rollouts), to learn knowledgeable agents by bridging this gap. KALM extracts knowledge from LLMs in the form of imaginary rollouts, which agents can learn through offline RL. To overcome the limitation that LLMs are inherently text-based and may be incompatible with numerical environmental data, KALM fine-tunes the LLM to perform bidirectional translation between textual goals and rollouts. This process enables the LLM to understand the environment better, facilitating the generation of meaningful rollouts. Experiments on robotic manipulation tasks demonstrate that KALM allows agents to rephrase complex goals and tackle novel tasks requiring new optimal behaviors. KALM achieves a 46% success rate in completing 1400 various novel goals, significantly outperforming the 26% success rate of baseline methods. Project homepage: https://kalmneurips2024.github.io.

## 1 Introduction

Developing knowledgeable agents that complete diverse manipulation tasks is a hallmark of machine intelligence. Reinforcement learning (RL) has emerged as a powerful mechanism for training intelligent agents to acquire such abilities in interactive environments . In this approach, agents can learn from the environmental interaction data. Although RL has shown promise in many challenging tasks, it is limited by the scope of the available interaction data: agents often struggle to accomplish tasks not covered by the interaction data and lack the ability to generalize to new or slightly altered tasks. For instance, a policy trained to _move the block to the left_ often fails to complete the instruction _move the block to the right_, despite them being highly similar tasks.

On the other hand, advancements in large language models (LLMs) have opened up new opportunities for developing intelligent agents that solve general tasks in text domain . Trained on extensive text corpora, LLMs embed a wide array of general world knowledge. Developing approaches effectively leveraging such knowledge to build knowledgeable agents for interactive and physical tasks recently presents a promising research frontier . Fortunately, existing research evidence has indicated LLMs' utility beyond textual domains. They successfully utilize LLMsto decompose complex tasks and make high-level decisions in interactive, physical environments [11; 12; 13]. However, they are constrained by a set of available skills and gap still exists between LLMs and the environments: the LLMs are inherently text-based, while the physical environments often operate with numerical data. As a result, previous methods with LLMs tend to focus on high-level, text-based decision-making and do not adequately address low-level control, especially when handling numerical vector inputs and outputs.

This study investigates developing knowledgeable agents with RL and LLMs, which achieve low-level control and adapt to novel situations. We introduce KALM (Knowledgeable Agent from Language Model rollouts) method, which employs a pre-trained LLM to create imaginary rollouts for novel skills, which agent can easily learn through offline RL techniques. The motivation behind KALM is that LLMs, with their extensive knowledge repository, are ideally suited for generating sequences that simulate the completion of novel goals. However, a key challenge is that LLMs are inherently designed to process textual data, while the environmental data is often in numerical vectors. To address this, KALM applies two techniques to ground LLM and process the environmental data: (1) adapting the LLM's architecture to handle environmental states and actions, and (2) fine-tuning the LLM in a supervised manner, e.g., predict execution rollouts and translate between natural language goals and their corresponding rollouts. Subsequently, KALM uses the LLM to generate imaginary rollouts for various goals, including rephrasings of existing goals and completely novel tasks not present in the interaction data. Fig. 1 depicts the process by which KALM utilizes an LLM to generate imaginary rollouts. Finally, KALM integrates offline RL techniques to acquire novel skills. This novel combination of LLM-generated imaginary rollouts with offline RL potentially yields more versatile and knowledgeable agents.

Our contributions are as follows: We introduce an effective way to integrate RL and LLMs for low-level control, enhancing the RL training with LLM knowledge through imaginary rollouts. Unlike previous works that primarily leverage LLM for text-based high-level control, we demonstrate that LLM knowledge can be utilized in interactive environments that operate on numerical vectors. Additionally, we present a technique for effectively aligning LLMs with environments, enabling LLMs to comprehend environmental data from different modalities. Lastly, we verify the efficacy of KALM on two robotics manipulation environments: CLEVR-Robot  and Meta-world . Experiment results show that KALM successfully fine-tunes the LLM to generate meaningful rollouts. In the CLEVR-Robot simulation environment, it achieves a 46% success rate on tasks with 1400 various novel goals, significantly outperforming the baseline offline RL method's 26% success rate.

## 2 Background and Related Work

### Background

**Reinforcement learning.** We consider an RL problem where the agent completes goals assigned by natural language. We model the environment as a goal-augmented Markov Decision Process

Figure 1: Illustration of KALM utilizing LLM to generate environmental rollouts at the numerical vector level. **(1)** Grounding phase that fine-tunes LLM with supervised fine-tuning on the environmental data. **(2)** Generation phase that prompts LLM to generate data for novel skills. KALM modifies the input/output layer of LLM, enabling it to process and interpret non-textual data.

(MDP) [16; 17; 18], represented by the tuple \(=(,,,,,)\). In this tuple, \(\) denotes the state space, \(\) the action space, \(\) the transition function of the environment, \(\) the reward function that evaluates the quality of the agent's action, \(\) the discount factor which balances the immediate and future rewards, and \(\) the set of natural language goals. A policy \(:()\) defines the agent's strategy, mapping states and goals to a distribution over possible actions. The interaction between the RL agent and the environment proceeds as follows: at each timestep \(t\), the agent observes a state \(s_{t}\) and a goal \(G\) from the environment. It then selects an action \(a_{t}\) based on the policy \((|s_{t},G)\). Upon executing this action, the agent receives a reward \(r(s_{t},a_{t},G)\) and the environment transitions to a new state \(s_{t+1}\) according to the transition function \((|s_{t},a_{t})\). The objective of RL is to find a policy that maximizes the expected sum of rewards over time. In this study, we call the state and action data of the environment the _environmental data_.

**Large language model.** LLM refers to an autoregressive text generation model that predicts future tokens in a sequence, where each token is predicted as \(l_{t+1}=(E_{T}(l_{0}),,E_{T}(l_{t}))\), conditioned on all prior sequence of tokens. Here, \(E_{T}\) is the token embedding layer that converts the token into a D-dimensional embedding, \(l_{t}\), and \(\) denotes the vocabulary of the LLM. Specifically, the token embedding layer converts tokens into embeddings \(e_{k}=E_{T}(l_{k})^{D}\), and the output layer of a LLM classifies tokens. In this study, we consider LLM, which operates on textual data, environmental state, and action data. To process state and action data, we modify the LLM's architecture by replacing its original input and output layers with additional multi-layer perceptrons (MLPs), thereby enabling the integration of non-textual environmental data.

### Related Work

**Offline Reinforcement Learning.** Offline RL [19; 20; 21] enables agents to learn from a static dataset of pre-collected experiences without real-time environment interaction. The core challenge in offline RL is to derive effective policies from a dataset that may be biased or has limited data. Behavior cloning  is a straightforward solution by mimicking the behavior present in the dataset. Besides, prior studies have introduced novel techniques such as importance sampling, conservative policy evaluation, and representation learning to address these challenges [23; 24; 25; 26]. Trajectory transformer (TT)  treats offline RL as a sequence modelling problem, aiming to produce a sequence of actions that leads to high rewards. Unlike TT, KALM utilizes a pre-trained LLM to generate rollouts instead of as a policy. Despite offline RL's successes, it is limited by the diversity of the dataset: if the dataset lacks specific experiences, the agent may fail to perform adequately in those scenarios. Model-based RL (MBRL) methods [28; 29] learn a dynamic model from offline data, which can then be used by any policy learning algorithm to recover the policy. MOPO  and MOReL  use uncertainty quantification to construct a lower bound, aiming to avoid issues like model bias and distribution shift. COMBO  employs both the offline dataset and model-generated rollouts to train a value function and regularize it on out-of-support state-action tuples. Despite both MBRL and KALM methods utilized generated rollouts for policy training, they are different from motivation. MAPLE  and ReDM  attempt to overcome this by training multiple diverse environment models to simulate wide range of environmental rollouts, thereby enhancing policy robustness in unfamiliar scenarios. These environment models, however, are typically learned from scratch and prioritize extensive data coverage, which may not align with real-world data distributions. In contrast, KALM leverages a pre-trained LLM to facilitate the generation of imaginary rollouts, taking advantage of the general knowledge from the pre-trained LLMs.

**Large Language Models.** Large language models (LLMs) exhibit remarkable proficiency in processing and comprehending natural language [35; 36; 37; 38]. More exciting, their capabilities extend to tasks beyond basic language understanding problems, including dialogue [4; 39; 5], multimodal vision-language tasks , logical reasoning [7; 8], and mathematical problem-solving . Recent advancements, such as GPT-4 , have pushed the boundaries further, not only in terms of text processing but also in exploring the capabilities of LLMs in interactive environments [41; 42; 43], capitalizing on their embedded knowledge of the world. How to effectively utilize this knowledge for decision-making has emerged as a promising research area [44; 42]. In this work, we propose a novel idea of leveraging the knowledge in LLMs as imaginary rollouts to develop knowledgeable agents.

**LLMs for RL.** A promising area of study is how to effectively leverage LLMs to enhance RL in interactive tasks. Research in this domain has taken several approaches to leverage LLMs. One approach is built on a hierarchical framework, utilizing LLMs to decompose complex tasks and generate high-level plans, which are then executed by a low-level controller [12; 45; 43]. Another approach uses LLMs to design the reward function [46; 47; 48], which streamlines the otherwise laborious process of reward function formulation. Recently, some works have investigated directly employing LLMs as the behaviour policies, training them with RL to facilitate direct interaction with the environments. For example, GALM  and TWOSOME  ground LLMs in text-based games, and LLaRP  trains a LLM to output actions for embodied tasks with RL. Different from these studies utilizing LLMs at the text level or as policies, in this work, we propose a novel approach to utilizing LLM, which extracts knowledge in the form of imaginary rollouts.

## 3 Method

This section presents the proposed Knowledgeable Agents from Language Model Rollouts (KALM) method. Fig. 2 shows the framework of KALM. We first give a formal definition of the problem and then elaborate on the three critical steps of KALM: (1) LLM grounding that enables LLM to understand the elements of the environment, (2) rollout generation that generates imaginary rollouts for novel skills, and (3) Skill acquisition that trains the policy with offline RL.

### Problem Formulation

Consider that we have an offline dataset \(\), which is collected from the environment. This dataset consists of pairs of goals and corresponding interaction rollouts: \(\{G^{k},(s_{0}^{k},a_{0}^{k},s_{1}^{k},a_{1}^{k},)\}_{k=1}^{K}\). In this context, the sequence \((s_{0}^{k},a_{0}^{k},s_{1}^{k},a_{1}^{k},)\) represents a rollout detailing the sequence of states and actions \((s_{i}^{k},a_{i}^{k})\) required to complete the goal \(G^{k}\). The primary objective here is to obtain a policy that achieves high rewards on unseen goal distributions, represented as \(^{}\), thus ensuring its decision-making ability beyond the constraints of the available interaction data. We will define the 'unseen goal distributions' in Sec. 3.3.

### LLM Grounding by Supervised Fine-tuning

The first step of KALM is to ground LLM in the environment. The purpose of this module is to enable LLM to interpret the meaning of states, actions, dynamics and rollouts of the given environment. To achieve this, we train the LLM using the dataset \(\) to perform three different tasks via supervised fine-tuning (SFT):

* Dynamics prediction: The LLM predicts environmental dynamics changes. Given the current state \(s_{t}\) and action \(a_{t}\), the LLM predicts the next state \(s_{t+1}\).

Figure 2: Overall framework of KALM method. **(A)** Overall training procedure, consisting of three key steps: LLM grounding that fine-tunes LLM with environmental data, rollout generation that prompts the LLM to generate imaginary rollouts for novel tasks, and offline RL training that facilitates skill acquisition. The dashed line (- ->) represents an optional online process, allowing for continuous improvement through iterative data collection and training. **(B)** The adapted network structure of the LLM. KALM adapts the architecture of LLM to process both text and environmental data.

* Rollout explanation: The LLM is presented with a rollout sequence \(s_{0},a_{0},s_{1},\), and it is required to describe the rollout with natural language.
* Rollout generation: The LLM generates a rollout that aligns with a specified goal \(G\).

We can construct the supervised training data based on the data from \(\). Here, KALM regards the LLM grounding problem as an instruction-following problem: LLM demonstrates excellent performance following given natural language instructions to generate the desired answer. This way, we can adjust the instruction prompt input to the LLM to better utilize it and specify its generation objective. The instruction prompts we use for SFT are presented in Appendix A.4.

Given that LLMs are initially trained on textual data to process and predict sequences of text tokens, they can not be directly utilized to handle numerical vectors. To overcome this, KALM introduces modifications to the LLM's network architecture. As shown in Fig. 2(B), we use a pre-trained LLM as the backbone model and modify it with additional layers to handle environmental data. For inputs such as states and actions, we incorporate learnable embedding modules, \(E_{S}:^{D}\) and \(E_{A}:^{D}\), which transform these inputs into embeddings of the same dimensionality as the token embeddings. For outputs, we employ learnable modules, \(O_{S}:^{D}\) and \(O_{A}:^{D}\), which map the LLM's output into state space \(\) or action space \(\). This framework can be easily extended to visual observation tasks by integrating appropriate neural network architectures.

### Rollout Generation with Goal-oriented Prompt

After fine-tuning with environmental data, the LLM acquires the capability to interpret the states, actions, and dynamics within the environment. Given that LLMs possess a broad spectrum of world knowledge, they have the potential to generate imaginary rollouts for a diverse range of novel skills. To this end, we employ the fine-tuned LLM to generate imaginary rollouts given the initial state \(s_{0}\) and the goal: \(\{a_{0},s_{1},a_{1},\}(GOP,s_{0})\). Here, \(GOP\) stands for _goal-oriented prompt_: "Generate a rollout for the following goal: [GOAL]. Rollout: ", where "[GOAL]" is a placeholder for various goals that reflect different skills. Following prior research that studies policy generalization under natural language goals , we measure the novelty of the goal along two dimensions:

* Paraphrastic Robustness: This dimension assesses the agent's consistency in optimal behavior when faced with linguistically diverse goals that share the same underlying intent as previously seen goals. It includes alternative phrasings for identical actions or re-expressing the name of the objects or entities.
* Novel Task Generalization: Here, we investigate the agent's proficiency in performing tasks that require the formulation of new optimal behaviors. Such tasks do not exist in the dataset \(\). For instance, the dataset includes tasks related to making a robot walk, while a novel task enables the robot to run. To generate effective rollouts for these tasks, the LLM is required to understand the entity's meaning and the environmental dynamics correctly.

We will elaborate on how we construct novel tasks that align with these dimensions in Sec. 4.1.

### Offline Reinforcement Learning for Skill Acquisition

KALM employs offline RL approach to train a policy \((|s,G)\), utilizing both the real and imaginary rollouts generated from LLM, with the same proportion of two sources of rollouts. To build a policy network, BERT  serves as the encoder for processing natural language goals due to its proficiency in text encoding. The goal encoding is integrated with the state representations to form the input for the policy network. For policy optimization, KALM is compatible with any offline RL algorithm, such as TD3+BC  and CQL , leveraging the combined data from the offline dataset and imaginary rollouts. Furthermore, we comprehensively analyse KALM's performance when integrated with different offline RL algorithms in Sec. 4.3.

## 4 Experiment

In this section, we conduct experiments to evaluate the efficacy of the KALM method. We aim to address the following important questions: (1) How does KALM perform on novel goals compared to existing baseline methods (Sec. 4.2)? (2) How are the rollouts generated by the fine-tuned LLM (Sec. 4.3)? (3) How does KALM ground LLM in the environment (Sec. 4.3)? (4) What is the impact of each component in KALM on the overall performance of the algorithm (Sec. 4.4)? We first introduce the environment used for experiments and the specific settings employed in the evaluation.

### Experimental Setting

**Evaluation environments.** We conduct experiments on two benchmark robotics manipulation environments built based on MuJoCo physics engine : Meta-world  and CLEVR-Robot , as depicted in Fig. 3. In the Meta-world, the agent controls a Sawyer robot to manipulate various objects, e.g., doors, drawers and windows. The target configurations in the offline dataset involve: reach, push, pick-place, button-press, door-unlock, door-open, window-open, faucet-open, coffee-push and coffee-button-press. The state space is defined in \(^{91}\), representing the robotics arm's state and the different objects' location & orientation. The action space is \(^{4}\), denoting the gripper movement and open/close.

In CLEVR-Robot, the agent (silverpoint) manipulates five movable balls. The target configuration in the offline dataset involves _moving a specific ball in a target direction (front, behind, left, or right) relative to a target ball_. The state space is defined in \(^{10}\), representing the positions of the balls, while the action space is \(^{40}\), where the action is a one-hot vector representing the movement of a specific ball to a direction.

**Novel task for evaluation.** To evaluate the efficacy of KALM and the learned policy's generalization to novel tasks, we define novel tasks with three levels of complexity as follows:

* Rephrasing goal: The agent performs the same manipulation tasks as offline data but receives paraphrased goals which are not present in the data. For example, the goal in offline data is _move the blue ball to the front of the red ball_, while the paraphrased goal could be _I really dislike how the red ball is positioned in front of the blue ball. Could you exchange their places?_
* Unseen (easy): The agent is tasked with different manipulation tasks that do not exist in the dataset, requiring the LLM to understand the environmental data well. To correctly generate the imaginary rollouts, the LLM must understand the meaning of state, action, dynamics and their relation with the goals.
* Unseen (hard): The agent faces tasks substantially different from those in the offline dataset, which require a complex composition of behaviors, such as "Gather all balls together", and "Move five balls to a straight line" in the CLEVR-Robot environment. For these tasks, LLM needs to understand the meaning of environmental data and create a novel combination of state and action to generate meaningful rollouts.

Due to the space limitation, we present detailed descriptions and examples of novel goals in Tab. 2. Before utilizing LLM to generate rollouts, we prompt ChatGPT  to output the natural language goals that describe different novel target configurations (which are not present in the offline dataset). Then, we prompt the fine-tuned LLM to generate imaginary rollouts for various novel goals.

**Dataset collection and rollout generation.** For Meta-world and CLRVR-Robot, the pre-collected offline dataset consists of 100,000 rollout-goal pairs, each corresponding to the state, action, and reward sequences for completing the natural language goal. Here, we employ the environment-built-in reward function to obtain and incorporate rewards for offline datasets and imaginary rollouts. The dataset encompasses \(80\) unique target configurations. Following prior research , we let ChatGPT

Figure 3: A visualization of the environments in our experiments. **(A)** In the CLEVR-Robot environment, the agent (silverpoint) manipulates five movable balls to reach a goal configuration. **(B)** In Meta-world, the agent controls a Sawyer robot to manipulate various objects.

 generate 18 different natural language sentence patterns to describe each target configuration, resulting in 1440 different natural language goals. For the Meta-world, the dataset involves 10 different target configurations, and we employ ChatGPT to generate 20 different natural language goals to describe each target configuration, resulting in 200 different goals in total.

When KALM fine-tunes LLM, we construct a training set comprising 300,000 trajectories, each rollout-goal pair in the offline dataset extending three trajectories: dynamics prediction, rollout explanation, and rollout generation (as detailed in Sec. 3). When training policies with offline RL, baseline methods use the offline data (6400 rollout-goal pairs), while KALM generates additional 5600, 72400 and 1680 imaginary rollouts for rephrasing goal, unseen (easy) and unseen (hard) tasks, respectively. For each level of novel goals, KALM trains the policy on the offline dataset and the generated rollouts at the corresponding level, with the same proportion of offline data and imaginary rollout in each training batch. There are 1400 and 240 novel goals for CLEVR-Robot and Meta-world, respectively.

**Implementation details.** We utilize the Llama-2-7b-chat-hf model  as the backbone LLM across the experiments. The LLM undergoes training for 5 epochs with batch size 32 on Meta-world and 10 with batch size 24 on CLEVR-Robot. We implement baseline methods by utilizing d3rlpy , a well-established code base. We utilize Adam optimizer  to optimize policy. All methods train policy for 500,000 gradient steps. The offline RL training is replicated with three different random seeds to ensure the robustness of the results. Details about hyper-parameters are provided in Appendix A.3. We implement all the above modules using 64 AMD EPYC 9374F 32-Core Processor, 8 NVIDIA GeForce RTX 4090 cards and 1TB RAM.

### Main Results

**Baselines for comparison.** We consider representative offline RL methods that train policies on offline data for comparison. We briefly introduce them as follows: (1) PRDC () is an offline RL method regularizing the policy towards the nearest state-action pair in the offline data based on tree-search method. (2) Behavior Cloning (**BC**) adopts a supervised learning approach to mimic the actions within the offline dataset. (3) Conservative Q-Learning (**CQL**) , a prominent offline RL algorithm, constructs a conservative Q-function that ensures the policy's expected value does not exceed its true value. (4) **TD3+BC** guides the agent to stay closer to the demonstrated behavior while benefiting from TD3  algorithm's stability and efficiency. (5) **AWAC** weights the actions according to their estimated advantages to improve policy learning. We use the suffix '+KALM' to denote the method that trains policy on offline datasets and the imaginary rollouts. To ensure a comprehensive evaluation, we also consider a baseline, LLM as a policy that takes advantages of both offline data and LLM. (6) **COMBO** utilizes ensemble environment models to achieve conservative policy learning.

**Comparison with offline RL methods.** Fig. 4 presents the comparative performance on two CLEVR-Robot and Meta-world. In Fig. 4(a), these methods with the 'KALM' suffix denote that the imaginary

Figure 4: Success rate bars of different methods on various levels of goals. The x-axis denotes the offline RL algorithm, and the y-axis denotes the success rate for completing various natural language goals. The success rate is calculated based on the average of the last five checkpoints, and the error bars stand for the half standard deviation over three random seeds. We present the training curves in Fig. 8.

rollouts are generated for rephrasing goals task. Overall, these methods equipped with KALM gain a clear improvement over the baseline methods, especially on these tasks with novel goals (see last three columns in the figure). For example, CQL+KALM achieves averaged a success rate of 27.1% on novel tasks in CLEVR-Robot, while the score of CQL is 15.5%. Besides, on task in offline data, KALM outperforms or is comparable to baselines, indicating that the inclusion of generated rollouts can not only preserve the performance but also potentially enhance the performance on these tasks within the real data. This could be attributed to the fact the imaginary rollouts improve the diversity of the dataset. It is worth noting that baselines show an clear decrease in performance on the rephrasing goal task compared to the performance on the task in offline data. This indicates that policies trained exclusively on offline data exhibit limited generalizability to rephrasing the goals they have encountered. In contrast, policies incorporating imaginary rollouts exhibit remarkable performance enhancements over baselines (see results on rephrasing goals and unseen (easy) tasks). When the task becomes more novel and complex, the performance improvement brought by KALM gets more significant in improving proportion. In comparison, baseline methods hardly progress on unseen (hard) tasks, reinforcing the utility of generated rollouts in acquiring complex novel skills. While COMBO gets considerable score on seen tasks and rephrasing goals, it performs poorly on unseen (easy) and unseen (hard). This can be attributed to that their environment models are learned from scratch, lacking the ability to generalize novel goals and leading to the low scores on unseen tasks.

**LLM as a direct policy.** We also consider a baseline that utilizes offline data and LLM, which we call 'LLM as policy'. In this experiment, the fine-tuned LLM directly outputs the action given the goal-oriented prompt GOP (refer to Sec. 3.3) and previous states and actions: \(a_{t}=(,s_{0},a_{0},,s_{t})\). We also consider DT  that utilizes both LLM and offline data, with Llama-2-7b-chat-hf as the backbone policy model. DT treats decision-making as a sequence modeling problem, using a transformer architecture to predict actions based on the desired future return. We evaluate the baseline on CLEVR-Robot, with the results presented in Fig. 5. The report KALM results are success rate evaluated with CQL+KALM policy. We observe that KALM outperforms both LLM as policy and DT. LLM as policy yields a relatively low success rate. This result may be due to the primary focus of the LLM's fine-tuning process, which is to capture sequence information related to completing the given goal rather than to precisely predict actions, as trajectory transformers do . While the action might be sub-optimal, the generated rollouts can still contribute positively to skill acquisition in offline RL. The comparison results with LLM as policy method suggest that the current approach to simultaneously modelling state and action sequences may lead to inaccurate behaviours. Future work should explore the possibility of separating the modelling of states and actions by employing two distinct LLMs, thereby enhancing the accuracy of behaviour prediction in complex environments.

### Performance of LLM Grounding

The previous section demonstrates that KALM improves policy's performance on unseen novel goals. In this section, we dive into the details of the KALM running process and examine why KALM can improve the policy performance. Specifically, we demonstrate the examples of the imaginary rollouts and the accuracy of the rollout explanation, which reflects the performance of LLM grounding.

**Analysis on the LLM rollouts**. To investigate the quality of the generated rollouts, we showcase illustrative examples of the imaginary rollouts in Fig. 6. We reset the environment to the generated state to obtain the visualization image. In the meta-world environment, the LLM generates a rollout to reach the target point behind a wall. Although the object 'wall' never occurs in the offline data, the LLM can adjust the robotics arm's trajectory to avoid collision. This adaptation underscores the LLM's comprehension of the environment and ability to leverage prior knowledge. For CLEVR-Robot, the goal 'gather all balls close to green' deviates significantly from the goals in the offline dataset. Notably, the LLM correctly identifies the green ball and orchestrates the movement of the remaining balls towards this target. These results support using pre-trained LLM to generate imaginary rollouts for novel goals. We present more examples of the generated rollouts and corresponding analysis in Appendix C.2.

Figure 5: Comparison with baseline method that directly utilizes LLM as policy.

**Accuracy of the rollout explanation**. We measure the LLM ground in terms of LLM's ability to explain environmental rollouts, where the LLM is asked to explain the rollouts with both seen prompt (Appendix A.4) and unseen prompt (_Suppose you are playing gaming of five balls with different colours. Please explain the following rollout briefly: \(\)n rollout: [ROLLOUT]\(\)n Answer:[ANSWER]_). The accuracy of the explanation is calculated by the keyword match between the LLM output and the ground-truth label. As depicted in Fig. 7, the LLM demonstrates a high level of explanatory accuracy, even after only two training epochs. These results suggest that the LLM correctly captures the meaning of rollouts and retains natural language expression capability after fine-tuning. Besides, after supervised fine-tuning, the rollout explanation accuracy exceeds 85% on unseen prompts, demonstrating that the LLaMA2 model retains its language understanding capability.

### Ablation Study

We conduct ablation evaluation on KALM with different SFT objectives, i.e., dynamics prediction and rollout explanation (refer to Sec. 3.2). We maintain consistent hyper-parameters across experiments to isolate the effects of these training objectives. The match rate between the generated rollouts and the goals measures the effectiveness of the training. Tab. 1 illustrates that the rollout explanation objective is more critical than dynamics prediction. This is attributed to the fact that the rollout explanation assists the LLAMA2 in capturing the temporal consistency between the rollout sequences and corresponding goal descriptions. Besides, integrating all three training objectives has yet to improve on unseen (easy) tasks. The performance degradation is attributed to the specific nature of the unseen (easy) task, whose objective is to predict one-step transitions given unseen language goals. To be more specifically, we would discuss the two components of LLM fine-tuning in KALM (i.e., dynamics prediction and rollout explanation) respectively. For dynamics prediction objective, unseen (easy) task objective (predicting and given and ) shares similarity, yet diverges from the dynamics prediction objective (predict given and ). This difference introduces a potential conflict in the LLM SFT. This is evidenced by the empirical results that KALM w/o dynamics prediction achieves highest rollout accuracy for unseen (easy) task. For rollout explanation objective, it focuses on giving explanation on a long rollout sequences. While this objective enriches the model's capability to

   MethodTask & Rephrasing & Unseen & Unseen \\  & goals & (easy) & (hard) \\  w/o rollout explanation &  &  &  \\  w/o dynamics prediction &  &  &  \\  KALM &  &  &  \\   

Table 1: Ablation study on the training objective of the LLM fine-tuning. The values represent the match rate of the generated rollouts with the given goals. Results indicate that the rollout explanation term has a deeper impact on the generation.

Figure 6: Examples of the imaginary rollouts generated by the fine-tuned LLM.

Figure 7: Rollout explanation accuracy.

provide coherence over temporal sequence, it may inadvertently detract from the model's ability to capture the immediate logic of transitions between adjacent two steps.

## 5 Conclusion and Limitation

This study investigates the integration of RL and LLMs for low-level control in interactive, physical environments. We introduce a novel method, KALM, which bridges the gap between LLMs and environments, and extracts the knowledge from LLM in the form of imaginary rollouts. Offline RL is then applied to facilitate skill acquisition from the imaginary rollouts. The experiment results on two robotics manipulation tasks validate the effectiveness of KALM and demonstrate the feasibility of acquiring knowledge from language model rollouts. However, there are still some limitations. First, the LLM learns to generate both state and action, while the action is correlated with the behavior policy. This dual responsibility increases the burden on the LLM, requiring it to learn the environment and imitate the behavior policy simultaneously. A solution could be employing two LLMs to learn state and action modelling separately. Second, current experiments only consider the state in the form of vector. Future work can evaluate KALM on tasks with state in other modalities, e.g., image, incorporating a vision encoder to handle visual data. Last, all parameters of LLM trained during the grounding phase, potentially influencing the embedded knowledge and hindering ability of the LLM to generalize novel goals. Incorporating a general text dataset during fine-tuning could mitigate this issue. We believe these interesting directions are worth further exploration for developing smarter and more robust agents with the support of large pre-trained models.