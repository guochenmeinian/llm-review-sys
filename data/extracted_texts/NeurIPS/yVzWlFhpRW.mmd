# Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking

Roland Stolz\({}^{1,}\)

&Hanna Krasowski\({}^{1,2,}\)

Jakob Thumm\({}^{1}\)&Michael Eichelbeck\({}^{1}\)&Philipp Gassert\({}^{1,3}\)&Matthias Althoff\({}^{1}\)

\({}^{1}\)Technical University of Munich, \({}^{2}\)University of California, Berkeley,

\({}^{3}\)Munich Center for Machine Learning

{roland.stolz, hanna.krasowski}@tum.de

The first two authors contributed equally to this work.

###### Abstract

Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization (PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.

## 1 Introduction

Reinforcement learning (RL) can solve complex tasks in areas such as robotics , games , and large language models . Yet, training RL agents is often sample-inefficient due to frequent exploration of actions, which are irrelevant to learning a good policy. Irrelevant actions are actions that are either physically impossible, forbidden due to some formal specification, or evidently counterproductive for solving the task. Since the global action space is typically large in relation to the relevant actions in each state, exploring these actions frequently can introduce unnecessary costs, lead to slow convergence, or even prevent the agent from learning a suitable policy.

Action masking mitigates this problem by constraining the exploration to the set of relevant state-specific actions, which can be obtained based on task knowledge. For example, when there is no opponent within reach in video games, attack actions are masked from the action space . Leveraging task knowledge through action masking usually leads to faster convergence and also improves the predictability of the RL agent, especially when the set of relevant actions has a specificnotion, such as being the set of safe actions. For instance, if the set of relevant actions is a set of verified safe actions, action masking can be used to provide safety guarantees [10; 19].

When the relevant action set is easy to compute, action masking usually benefits RL by improving the sample efficiency and is the quasi-standard for discrete action spaces [34; 46], e.g., in motion planning [8; 18; 23; 30; 42], games [14; 15; 43], and power systems [21; 38]. However, real-world systems operate in continuous space and discretizing it might prevent learning optimal policies. Furthermore, simulation of real-world systems is computationally expensive, and developing RL agents for them often requires additional real-world training . Thus, sample efficiency is particularly valuable for these applications.

In this work, we propose three action masking methods for continuous action spaces. They can employ convex set representations, e.g., polytopes or zonotopes, for the relevant action set. Our action masking methods generalize previous work in , which is constrained to intervals as relevant action sets. This extends the applicability of continuous action masking to expressive convex relevant action sets, which is especially useful when action dimensions are coupled, e.g., for a thrust-controlled quadrotor. To integrate the relevant action set into RL, we introduce three methods: the _generator mask_, which exploits the generator representation of a zonotope, the _ray mask_, which projects an action into the relevant action set based on radial directions, and the _distributional mask_, which truncates the policy distribution to the relevant action set. In summary, our main contributions are:

* We introduce continuous action masking based on convex sets representing the state-dependent relevant action sets;
* We present three methods to utilize the relevant action sets and derive their integration in the backward pass of RL with stochastic policies;
* We evaluate our approach on four benchmark environments that demonstrate the applicability of our continuous action masking approaches.

## 2 Related literature

Action masking has been mainly applied to discrete action spaces [8; 10; 14; 15; 19; 18; 21; 23; 30; 38; 41; 42; 43; 46]. Huang et al.  derive implications on policy gradient RL and evaluate their theoretical findings on real-time strategy games. They show that masking actions leads to higher training efficiency and scales better with an increasing number of actions than penalizing the agent for selecting irrelevant actions. Huo et al.  have extended  to off-policy RL and have observed similar empirical results.

Action masking for discrete action spaces can be categorized by the purpose of the state-dependent relevant action set. Often, the set is obtained by removing irrelevant actions based on task knowledge [8; 14; 15; 21; 30; 42], e.g., executing a harvesting action before goods are produced . While the relevant action sets are usually manually engineered, a recent study  takes a data-driven approach and identifies redundant actions based on similarity metrics. Another common interpretation of the relevant action set is that it only includes safe actions [10; 18; 23; 38; 41]. These works typically use the system dynamics to verify the safety of actions. A safe action avoids defined unsafe areas or complies with logic formulas. In our experiments, the relevant action sets are either state-dependent safe action sets or a global relevant action set modeling power supply constraints.

For continuous action spaces, there is work on utilizing action masking with multidimensional intervals (hereafter only referred to as intervals) as relevant action sets . In particular, the proposed continuous action masking represents relevant action sets by intervals that reflect safety constraints and employs straightforward re-normalization to map the action space to the relevant action set. In this paper, we generalize to more expressive relevant action sets and demonstrate the applicability to four benchmark environments.

## 3 Preliminaries

As the basis for our derivations of the three masking methods, we provide a concise overview of RL with policy gradients. Further, we define the considered system dynamics as well as the set representations used in this work.

### Reinforcement learning with policy gradients

A Markov decision process is a tuple \((,,T,r,)\) consisting of the following elements: the observable and continuous state set \(^{n^{S}}\), the action set \(^{N}\), the state-transition distribution \(T(s^{}|a,s)\), which is stationary and describes the transition probability to the next state \(s^{}^{n^{S}}\) from the current state \(s\) when executing action \(a\), the reward \(r:\), and the discount factor \(\) for future rewards . The goal of RL is to learn a parameterized policy \(_{}(a|s)\) that maximizes the expected reward \(_{}_{_{}}_{t=0}^{}^{t}r(s_{t},a_{ t})\).

For policy gradient algorithms, learning the optimal policy \(_{}^{*}(a|s)\) is achieved by updating its parameters \(\) using the policy gradient [36, Thm. 2]

\[ J(_{})=_{_{}}[_{} _{}(a|s)A_{_{}}(a,s)],\] (1)

where \(A_{_{}}(a,s)\) is the advantage function, which represents the expected improvement in reward by taking action \(a\) in state \(s\) compared to the average action taken in that state according to the policy \(_{}(a|s)\). An estimation of the advantage \(A_{_{}}(a,s)\) is usually provided by a neural network.

### System model and set representations

We consider general continuous-time systems of the form

\[=f(s,a,w),\] (2)

where \(w^{n^{}}\) denotes a disturbance. The input is piece-wise constant with a sampling interval \( t\). We assume \(\), \(\), and \(\) to be convex.

Zonotopes are a convex set representation well-suited for representing the relevant action set, due to the efficiency of computing their Minkowski sums and linear maps. A zonotope \(^{N}\) with center \(c^{N}\), generator matrix \(G^{N P}\), and scaling factors \(^{P}\) is defined as

\[=c+G\|\|_{} 1}=  c,G_{}.\] (3)

Additionally, let us denote that \(G_{(,i)}\) returns the \(i\)-th column vector of the generator matrix \(G\). The Minkowski addition \(_{1}_{2}\) of two zonotopes \(_{1}\), \(_{2}\) and the linear map \(M_{1}\) of a zonotope \(_{1}\) are given by [2, Eq. 2.1]

\[_{1}_{2} = c_{1}+c_{2},[G_{1} G_{2}]_{},\] (4a) \[M_{1} = Mc_{1},MG_{1}_{}.\] (4b)

## 4 Continuous action masking

To apply action masking, a relevant action set \(^{r}(s)\) has to be available, which constrains the action space \(\) based on task knowledge. Let us denote the state-dependent relevant action set as \(^{r}(s)\). From now on, we omit the dependency on the state to simplify notation. For our continuous action masking methods, we specifically require the following two assumptions:

**Assumption 1**.: _The relevant action set \(^{r}\) is convex and its center and boundary points are computable._

**Assumption 2**.: _The policy \(_{}:S_{+}\) of the agent is represented by a parameterized probability distribution \(a_{}(a|s)\)._

Common convex set representations that fulfill Assumption 1 are polytopes or zonotopes. Our continuous action masking methods transform the policy \(_{}(a|s)\), for which the parameters \(\) usually specify a neural network, into the relevant policy \(_{}^{r}:S^{r}_{+}\) through a functional \(h:(())^{r}\). Here, \(\) is the space of all policies, \(^{r}\) is the space of all relevant policies, and \(()\) is the power set of all \(^{r}\). The transformation is defined as

\[a^{r}_{}^{r}(a^{r}|s)=h_{}(a|s),^{r} ,\] (5)

and thereby ensures that \(a^{r}^{r}\) always holds. Please note that policy gradient methods only require the ability to sample from and to compute the gradient of the policy distribution. Therefore, explicit closed-form expressions for \(_{}^{r}(a^{r}|s)\) and \(h\) are not necessary.

In the following subsections, we introduce and evaluate three masking methods: generator mask, ray mask, and distributional mask, as shown in Fig. 1. We derive the effect of each masking approach on the gradient of the objective function for stochastic policy gradient methods.

### Ray mask

The ray mask maps the action set \(\) to \(^{r}\) by scaling each action \(a\) alongside a ray from the center of \(^{r}\) to the boundary of \(\), as shown in Figure 0(a). Specifically, the relevant policy \(_{}^{r}\) results from mapping the action \(a\), sampled from \(_{}(a|s)\), using the function \(g_{}:^{r}\):

\[a^{r}=g_{}(a)=c+^{r}}(a)}{_{ }(a)}(a-c).\] (6)

Here, \(_{}(a)\) and \(_{^{r}}(a)\) denote the distances of the action \(a\) to the boundaries of the relevant action set and action space, respectively, measured from the center of the relevant action set \(c\) in the direction of \(a\). Note that computing \(_{^{r}}(a)\) for a zonotope requires solving a convex optimization problem, as specified in Appendix A.4. Yet, the ray mask is applicable for all convex sets, for which we can compute the center and boundary points. Since \(g_{}(a)\) is bijective (see Appendix A.1 for a detailed proof), we can apply a change of variables [5, Eq. 1.27] to compute the relevant policy

\[_{}^{r}(a^{r}|s)=_{}(g_{}^{-1}(a^{r})|s )|(}{a^{r}}g_{} ^{-1}(a^{r}))|,\] (7)

where \(g_{}^{-1}(a^{r})=a\) is the inverse of \(g_{}\). In general, there is no closed form of the distribution \(_{}^{r}\) available. However, for stochastic policy gradient-based RL, we only require to sample from \(_{}^{r}\) and compute its policy gradient. Samples from \(_{}^{r}(a^{r}|s)\) are created by sampling from the original policy \(a_{}(a|s)\) followed by computing \(a^{r}=g_{}(a)\). The policy gradient is derived next.

**Proposition 1**.: _Policy gradient for the ray mask._

The policy gradient of \(_{}^{r}(a^{r}|s)\) for the ray mask is

\[_{}J(_{}^{r}(a^{r}|s))=_{_{ }^{r}}[_{}_{}(a|s)A_{_{}^{r}}(a^ {r},s)],\] (8)

where \(A_{_{}^{r}}(a^{r},s)\) is the advantage function associated with \(_{}^{r}(a^{r}|s)\).

Proof.: The determinant in (7) is independent of \(\), i.e.,

\[_{}(}{a^{r}}g_{}^{-1}(a^{r}))=0,\] (9)

which simplifies the score function of \(_{}^{r}(a^{r}|s)\) to

\[_{}_{}^{r}(a^{r}|s)=_{}_{}(a |s).\] (10)

Combining (10) and the general form of the policy gradient in (1) for \(_{}^{r}(a^{r}|s)\) results in (8).

Figure 1: Illustration of masking methods in action space \(\) with a hexagon-shaped relevant action set \(^{r}\). The ray mask radially maps the actions towards the center of the relevant action set. The generator mask employs the latent action space \(^{l}\), which is the generator space of the zonotope modeling the relevant action set. The distributional mask augments the policy probability density function so that it is zero outside the relevant action set.

### Generator mask

Zonotopes can be interpreted as the map of a hypercube in the generator space to a lower-dimensional space (see Section 3.2). The generator mask is based on exploiting this interpretation by letting the RL agent select actions in the hypercube of the generator space. Since the size of the output layer of the policy network cannot change during the learning process, we fix the dimension of the generator space. The generator mask requires the following assumption:

**Assumption 3**.: _The relevant action set \(^{r}(s)\) is represented by a zonotope \( c(s),G(s)_{}\), with \(G^{N P}\) and \(c^{N}\), and a state-invariant number of generators \(P\)._

Note that in practice, Assumption 3 can often be trivially fulfilled by choosing sufficiently many generators, and the number of generators \(P\) is usually the output dimension of the parametrized policy. The domain of the policy \(_{}(a|s)\) is the hypercube \(^{l}=[-1,1]^{P}\), which can be interpreted as a latent action space, and the domain of the relevant policy \(_{}^{r}(a^{r}|s)\) is a subset of the action space \(^{r}\) (see Figure 1b).

To derive the policy gradient of the generator mask, we assume:

**Assumption 4**.: \(_{}(a|s)\) _is a parametrized normal distribution \((a;_{},_{})\)._

**Proposition 2**.: _The relevant policy of the generator mask is_

\[_{}^{r}(a|s)=(a;G_{}+c,G_{}G^{T}).\] (11)

Proof.: The generator mask \(g_{}:^{l}^{r}\) is:

\[a^{r}=g_{}(a)=c+Ga,\] (12)

which is a linear function. Therefore, the proof directly follows from the linear transformation of multivariate normal distributions [Thm. 3.3.3]. 

Note that the ray mask and generator mask are mathematically equivalent to the approach in  if the relevant action set is constrained to intervals. Since \(g_{}(a)\) is not bijective in general, we cannot derive the gradient through a change of variables, as for the ray mask.

**Proposition 3**.: _The policy gradient for \(_{}^{r}(a^{r}|s)\) as defined in (11) with respect to \(_{}\) and \(_{}\) is_

\[_{_{}}_{}^{r}(a^{r}|s)= G^{T}(G_{}G^{T})^{-1}(a^{r}-c-G_{}),\] (13) \[_{_{}}_{}^{r}(a^{r}|s)= -G^{T}(G_{}G^{T})^{-1}G-G^{T}(G _{}G^{T})^{-1}(a^{r}-c-G_{})\] (14) \[(a^{r}-c-G_{})^{T}(G_{}G^{T})^{-1}G.\]

Proof.: The proposition is proven in Appendix A.2. 

Note that for the special case where \(G^{-1}\) exists, i.e., \(G\) is square and non-singular, the expressions in (13) and (14) simplify to \(_{_{}}_{}(a|s)\), and \(_{_{}}_{}(a|s)\), respectively, when Assumption 4 holds (see Proposition 5 in Appendix A.2). While the generator matrix will commonly not be square, as usually \(P>N\), there are cases where \(P=N\) is a valid choice, e.g., if there is a linear dependency between the action dimensions as for the 2D Quadrotor dynamics (see (40)).

### Distributional mask

The intuition behind the distributional mask comes from discrete action masking, where the probability for irrelevant actions is set to \(0\). For continuous action spaces, we aim to achieve the same by ensuring that actions are only sampled from the relevant action set \(^{r}\), by setting the density values of the relevant policy distribution \(_{}^{r}(a^{r}|s)\) to zero for actions outside the relevant action set (see Figure 1c). For the one-dimensional case, this can be expressed by the truncated distribution . In higher dimensions, the resulting policy distribution is

\[_{}^{r}(a^{r}|s)=(a|s)}{_{^ {r}}_{}(|s)},\] (15)where \((a,s)\) is the indicator function

\[(a,s)=1&a^{r},\\ 0&.\] (16)

Since there is no closed form of this distribution, we employ Markov chain Monte Carlo sampling to sample actions from the policy. More specifically, we utilize the random direction hit-and-run algorithm: a geometric random walk that allows sampling from a non-negative, integrable function \(f:^{N}_{+}\), while constraining the samples to a bounded set . The algorithm iteratively chooses a random direction from the current point, computes the one-dimensional, truncated probability density of \(f\) along this direction, and samples a new point from this density. The approach is particularly effective for high-dimensional spaces where other sampling methods might struggle with convergence or efficiency. For the distributional mask, \(f\) is the policy \(_{}(a|s)\), and \(^{r}\) is the set. As suggested by , we execute \(N^{3}\) iterations before accepting the sample. To estimate the integral in (15), we use numerical integration with cubature , which is a method to approximate the definite integral of a function \(l:^{N}\) over a multidimensional geometric set.

**Proposition 4**.: _The policy gradient for the distributional mask is_

\[_{}_{}^{r}(a^{r}|s)=_{}_{} (a|s)-_{}_{^{r}}_{}(|s) .\] (17)

Proof.: Equation (17) is obtained by calculating the gradient of the logarithm of (15). The indicator function \((a,s)\) is not continuous and differentiable, which would necessitate the use of the subgradient for learning. However, since \(a^{r}^{r}\) always holds, the gradient has to be computed for the continuous part of \(_{}^{r}(a^{r}|s)\) only, and thus \((a,s)\) can be omitted. 

Since the gradient of the numeric integral \(_{^{r}}_{}(|s)\) is intractable for zonotopes, we treat the integral as a constant in practice, and use \(_{}_{}^{r}(a^{r}|s)_{}_{ }(a|s)\). We discuss potential improvements in Section 5.3.

## 5 Numerical experiments

We compare the three continuous action masking methods in four different environments: The simple and intuitive Seeker Reach-Avoid environment, the 2D Quadrotor environment to demonstrate the generalization from the continuous action masking approach in , and the 3D Quadrotor and Mujoco Walker2D environment to show the applicability to action spaces of higher dimension. Because the derivation of the relevant action set is not trivial in practice, we selected four environments for which we could compute intuitive relevant action sets. In particular, for the Seeker and Quadrotor environments, the relevant action set is a safe action set since it is computed so that the agent does not collide (Seeker) or leave a control invariant set (2D and 3D Quadrotor). For the Walker2D environment, the relevant action set is state-independent and models a power supply constraint for the actuators.

For the experiments, we extend the stable-baseline3  implementation of proximal policy optimization (PPO)  by our masking methods. PPO is selected because it is a widely used algorithm in the field of RL and fulfills both Assumptions 2 and 4 by default. Apart from the masking agents, we also train a baseline agent with standard PPO that uses the action space \(\) and a replacement agent, for which an action outside of \(^{r}\) is replaced by a uniformly sampled action from \(^{r}\) (see  for details). The replacement agent is an appropriate comparison to the masking agents since only relevant actions are executed while the replacement is implemented as part of the environment, which is usually easier than an implementation as part of the policy as for the masking methods. We conduct a hyperparameter optimization with \(50\) trials for each masking method and environment. The resulting hyperparameters are reported in Appendix A.9. All experiments are run on a machine with a Intel(R) Xeon(R) Platinum 8380 2.30 GHz processor and 2 TB RAM.

### Environments

We briefly introduce the three environments and their corresponding relevant action sets \(^{r}\). Parameters and dynamics for the environments are detailed in Appendix A.5.

#### 5.1.1 Seeker Reach-Avoid

This episodic environment features an agent navigating a 2D space, tasked with reaching a goal while avoiding a circular obstacle (see Figure 2). It is explicitly designed to provide an intuitive relevant action set \(^{r}\). The system is defined as in Section 3.2: The dynamics for the position of the agent \(s=[s_{x},s_{y}]^{T}\) and the action \(a=[a_{x},a_{y}]^{T}\) is \(=a\), and there are no disturbances.

The environment is characterized by the position of the agent, the goal position \(s^{*}\), the obstacle position \(o\), and the obstacle radius \(r_{o}\). These values are pseudo-randomly sampled at the beginning of each episode, with the constraints that the goal cannot be inside the obstacle and the obstacle blocks the direct path between the initial position of the agent and the goal. The reward for each time step is

\[r(a,s)=100&,\\ -100&,\\ -1-\|s^{*}-s\|_{2}&.\] (18)

We compute the relevant action set \(^{r}\) so that all actions that cause a collision with the obstacle or the boundary are excluded (see Appendix A.3.3).

#### 5.1.2 2D Quadrotor

The 2D Quadrotor environment models a stabilization task and employs an action space where the two action dimensions are coupled, i.e., rotational movement is originating from differences between the action values and vertical movement is proportional to the sum of the action values. The relevant action set is computed based on the system dynamics and a relevant state set (see Appendix, Eq. (33)). The reward function is defined as

\[r(a,s)=(-\|s-s^{*}\|_{2}-\|[-a_{1, }}{a_{1,}},-a_{2,}}{a_{2,}} ]\|_{1}),\] (19)

where \(s^{*}=\) is the stabilization goal state, \(a=[a_{1},a_{2}]\) is the two-dimensional action, \(a_{,}\) is the lower bound for the actions in dimension \(i\), and \(a_{,}\) is the absolute difference between the lower and upper bound for the actions in dimension \(i\).

#### 5.1.3 3D Quadrotor

The third environment models a stabilization task for a quadrotor defined in . The quadrotor has four action dimensions. We use the same reward (see (19)) and the same calculation approach for the relevant action set (see Appendix, Eq. (33)) as for the 2D Quadrotor.

#### 5.1.4 Mujoco Walker2D

The relevant action sets of the two Quadrotor and the Seeker environments are sets that only contain safe actions for the current state. Computing safe action sets requires considerable domain knowledge

Figure 2: The Seeker Reach-Avoid environment with state and action space. The agent (black) has to reach the goal (gray) while avoiding the obstacle (red). The center of the action space is illustrated by a cross and the relevant action set \(^{r}\) for the current state is shown in green. The state set reachable at the next time step, by the relevant action set, is \(_{ t}\).

and for our case solving an optimization problems (see Appendix A.3). However, action masking is not restricted to safe relevant action sets, which we aim to demonstrate on the Mujoco Walker2D environment . We extend the environment with a termination criterion, which ends an episode, when the the action violates the constraint \(\|a\|_{2}_{p}\). This can be viewed as constraining the cumulative power output on all joints to a maximum value \(_{p}\). We motivate this constraint by having a battery with a maximum power output that should not be exceeded in practice. Accordingly, we define the relevant action set as the static set

\[^{r}=a\|a\|_{2}_{p}}.\] (20)

We under-approximate this relevant action set with a zonotope that consists of \(36\) generators.

### Results

The reported training results are based on ten random seeds for each configuration. Figure 3 shows the mean reward and the bootstrapped 95% confidence intervals  for the four environments and Table 1 depicts the mean and standard deviation of the episode return during deployment. First, we present the results for the Seeker and Quadrotor environments for which the relevant action set is state-dependent and only includes safe actions. Then, we detail the results for the Walker2D environment with a static relevant action set.

For the Seeker and Quadrotor environments, the baseline, i.e., PPO, converges significantly slower or not at all (see Figure 3). Additionally, the initial rewards when using action masking are significantly higher than for the baseline, indicating that the exploration is indeed constrained to relevant task-fulfilling actions. During deployment (see Table 1), the baseline episode returns are significantly lower than for action masking, while the three masking methods perform similarly. More specifically, for the Seeker environment, the relative volume of the relevant action set compared to the global action space is on average \(70\%\), and all action masking methods converge significantly faster to high rewards than the baseline. The generator mask and the distributional mask achieve the highest final reward. Further, action replacement performs better than the baseline but significantly worse than

Figure 3: Average reward curves for benchmarks with transparent bootstrapped 95% confidence interval.

masking. For the 2D Quadrotor, the relative volume is on average \(28\%\), which is much smaller than for the Seeker environment. In the 2D Quadrotor environment, the ray mask, generator mask, and action replacement achieve the highest reward. While the baseline converges significantly slower, the final average reward is similar to the one of the distributional mask. Please note that the confidence interval for the baseline is significantly larger, because three of the ten runs do not converge and, on average, exhibit a reward of one throughout training. Additionally, we observed that if we constrain the relevant action set to intervals for this environment, our optimization problem in (33) often renders infeasible, because the maximal relevant action set cannot be well approximated by an interval. Thus, the masking method proposed in  is not suitable for the 2D Quadrotor task. The results on the 3D Quadrotor are similar to those on the 2D Quadrotor; again, the generator mask converges the fastest but yields a final reward similar to that of the ray mask and action replacement. The relative volume of the relevant action set to the global action space is on average \(25\%\). Notably, in this environment, the baseline does not learn a meaningful policy. Based on these three environments with state-dependent relevant action sets, it seems that action masking performs better than action replacement when the relative volume is not too small.

The training results of the Walker2D experiment are shown in Figure 3. While the generator and ray mask both learn a performant policy, the ray mask outperforms by a significant margin. The lower performance of the generator mask is likely due to the high-dimensional generator space with \(36\) dimensions. This is supported by initial experiments with \(12\) generators (i.e., a more conservative under-approximation of the \(L_{2}\)-norm) where the generator mask performed better compared to the ray mask. Replacement performs significantly worse than the two masking approaches, and the PPO baseline does not learn a meaningful policy since the environment is frequently reset due to violations of the power constraint in (20). The frequent terminations occur since in six action dimensions the relative volume of the unit ball compared to the unit box is \( 8\%\), i.e., more than \(92\%\) of actions are violating the power constraint. To compare masking to a learning baseline, we also evaluated standard PPO without constraints, which performs slightly better than ray masking but almost always uses actions outside \(^{r}\) (see Appendix A.7). The deployment results in Table 1 reflect similar results as the training; the ray mask achieves better rewards than the generator mask, followed by replacement, and the baseline performs the worst. We excluded the distributional mask for the Walker2D, since its computation time is approximately \(170\) times slower than the baseline, compared to a \(1.6\) increase for the generator, \(2.7\) for the ray mask, and \(2.5\) for action replacement (see Table 3). The severely increased computational cost for the distributional mask arises from the geometric random walks, which scale cubically with action dimensions.

### Discussion and limitations

Our experimental results indicate that continuous action masking with zonotopes can improve both the sample efficiency and the final policy of PPO. While the sample efficiency is higher in our experiments, computing the relevant action set adds computational load as shown by the increased computation times (see Appendix A.6). Thus, in practice, a tight relevant action set might require more computation time than the additional samples for standard RL algorithms. Yet, if the relevant action set provides guarantees, e.g., is a set of verified safe actions, this increased computation time is often acceptable. Additionally, the computational effort for the masks differs. Given a relevant action zonotope, the generator mask adds a matrix-vector multiplication, which scales quadratically with action dimensions, the ray mask is dominated by the computation of the boundary points, which scales polynomially with action dimensions , and the distributional mask scales cubically with the dimension of the action space due to the mixing time of the hit-and-run algorithm . Note that

    & Seeker & 2D Quad. & 3D Quad. & Walker2D \\  Baseline & \(-71.03 19.67\) & \(-0.80 0.15\) & \(-1.00 0.00\) & \(1.19 0.06\) \\ Replacement & \(-60.21 19.98\) & \(-0.25 0.03\) & \(-0.68 0.09\) & \(234.93 129.5\) \\ Ray & \(-20.45 16.39\) & \(-0.26 0.05\) & \(-0.63 0.03\) & \(1941.82 992.83\) \\ Generator & \(18.60 22.02\) & \(-0.25 0.02\) & \(-0.68 0.07\) & \(1443.62 702.7\) \\ Distributional & \(-13.66 19.97\) & \(-0.23 0.02\) & \(-0.66 0.02\) & – \\   

Table 1: Mean and standard deviation of episode return for ten runs per trained model.

for the Walker2D environment with six action dimensions, the computation time for the hit-and-run algorithm is so high that the distributional mask evaluation was infeasible.

The ray mask and generator mask are based on functions \(g_{}\) and \(g_{}\) that map to relevant actions. There are two different approaches of incorporating these functions into RL algorithms. One is to apply the mapping as part of the environment on an action that is sampled by a standard RL policy. The second option, which we use, is to consider the masking as part of the policy, which creates the relevant policy as in (5). This has three main benefits over integrating masking as part of the environment. First, the actions passed to the environment are better interpretable, e.g., for the generator mask, adding the masking mapping function to the environment leads to a policy that samples actions from the generator space, which commonly does not have an intuitive interpretation. Second, more formulations of the functional \(h\) are possible, e.g., the distributional mask, and, third, the mapping function can be included in the gradient calculation. For mathematically sound masking as part of the policy, the correct gradient needs to be derived for each RL algorithm, and the standard RL algorithms need to be adapted accordingly. However, the empirical benefit could be minor. Thus, future work should investigate the significance of the correct gradient on a variety of tasks. Note that we showed in Proposition 1 that for the ray mask, the PPO gradient with respect to the original policy and relevant policy is the same. Thus, for the ray mask, it does not matter if it is viewed as part of the policy or the environment.

PPO is a common RL algorithm. However, off-policy algorithms such as Twin Delayed DDPG (TD3)  and Soft Actor-Critic (SAC)  are frequently employed as well. The ray mask and generator mask are conceptually applicable for deterministic policies as used in TD3. Yet, the implications on the gradient must be derived for each RL algorithm and are subject to future work. For the distributional mask, treating the integral in (15) as constant with respect to \(\) is a substantial simplification, which might be an explanation for the slightly worse convergence of the distributional mask since this introduces an off-policy bias. To address this in future work, one could approximate the integral with a neural network, which has the advantage that it is easily differentiable.

We focus this work on the integration of a convex relevant action set into RL and assume that an appropriate relevant action set can be obtained. While convex sets are a significant generalization from previous work , they might not be sufficient for some applications, e.g., tasks where relevant action sets are disjoint. Thus, future work could include investigating hybrid RL approaches  to increase the applicability to multiple convex sets or non-convex sets, such as constrained polynomial zonotopes . Further, obtaining the relevant action set can be a major challenge in practice, in particular when the relevant action set is different for each state. Such a high state-dependency is likely when the notion of relevance is safety, while for other definitions of action relevance, the relevant action set might be easy to pre-compute, e.g., excluding high steering angles at high velocities. Additionally, there might be an optimal precision of the relevant action set due to two opposing mechanisms. On the one hand, the larger the relevant action set is with respect to the action space, the smaller the sample efficiency gain from action masking might get. On the other hand, a tight relevant action set might require significant computation time to obtain. Thus, future work should investigate efficient methods to obtain sufficiently tight relevant action sets.

## 6 Conclusion

We propose action masking methods for continuous action spaces that focus the exploration on the relevant part of the action set. In particular, we extend previous work on continuous action masking from using intervals as relevant action sets to using convex sets. To this end, we have introduced three masking methods and have derived their implications on the gradient of PPO. We empirically evaluated our methods on four benchmarks and observed that the generator mask and ray mask perform best. If the relevant action set can be described by a zonotope with fixed generator dimensions and the policy follows a normal distribution, the generator mask is straightforward to implement. If the assumptions for the generator mask cannot be fulfilled, the ray mask is recommended based on our experiments. Because of subpar performance and longer computation time, the distributional mask needs to be further improved. Future work should also investigate a broad range of benchmarks to identify the applicability and limits of continuous action masking with convex sets more clearly.