# Clustering in Causal Attention Masking

Nikita Karagodin

Laboratory for Information and Decision Systems, MIT, Cambridge, MA, USA

Yury Polyanskiy

Laboratory for Information and Decision Systems, MIT, Cambridge, MA, USA

Philippe Rigollet

Department of Mathematics, MIT, Cambridge, MA, USA

###### Abstract

This work presents a modification of the self-attention dynamics proposed by Geshkovski et al. (2023b) to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI. This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthen the results of Geshkovski et al. (2023b) in this context: While previous rigorous results focused on cases where all three matrices (Key, Query, and Value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity. Additionally, we establish a connection to the classical Renyi parking problem from combinatorial geometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.

## 1 Introduction

The introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of natural language processing (NLP), signaling the advent of large language models. Central to the Transformer architecture is the _self-attention mechanism_, a special kind of layer that distinguishes it from preceding models such as ResNets. This innovation has yielded unprecedented performance not only in machine translation and text summarization but also in areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency of Transformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, the theoretical foundations underlying their success remain underexplored.

Following Sander et al. (2022), recent studies by Geshkovski et al. (2023a) and Geshkovski et al. (2023b) have proposed a mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, when modeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These works primarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token. Building upon this foundation, our research extends the analysis to _causal_ attention mechanisms, wherein each token is restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent in Transformer models employed in generative AI and known as decoder architectures.

Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitablefor tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output. This computational advantage explains the pervasiveness of causal attention not only in natural language processing but also in image generation with tools like DALL-E (Ramesh et al., 2021), VQGAN (Esser et al., 2021), or Parti (Yu et al., 2022) and multimodal foundation models, notably Chameleon (Team, 2024). More generally, the use of _masked_ attention where tokens pay attention to a subset of other tokens has been driving recent scaling efforts and has led to state-of-the-art models such as MUSE (Chang et al., 2023) or Alphafold 3 (Abramson et al., 2024). Causal attention can also be recast as an interacting particle system but it requires different analytical techniques. This is the goal of our paper.

**Our contributions.** Our main theoretical result establishes asymptotic clustering of tokens for causal self-attention transformer modeled as an interacting particles system on the sphere (Theorem 4.1). While mathematically accurate, this asymptotic collapse to a single cluster is seldom observed numerically. Instead, particles collapse to multiple clusters and stay in this configuration for a very long time (see Fig. 2 for a representative example) -- such _meta-stable_ states were already alluded to in Geshkovski et al. (2023b) and their study was recently initiated in Koubbi et al. (2024). In Section 5 we describe such meta-stable states using analogy with the Renyi parking process (Lemma 2, Theorem 5.1). Additionally, Theorem 5.1 covers asymptotic clustering of tokens for causal self-attention with additional cross-attention component. Moreover, we predict that, akin to linear dynamical systems, the most important factors that qualitatively describe final particles configuration both in causal and full-attention cases are the eigenvalue of the Value matrix \(V\) with the largest real part \(_{}\) and its eigenspace \(L\), while Query and Key matrices \(Q,K\) and temperature parameter \(\) do not matter. Our conjectured atlas of possible meta-stable configurations is listed in Table 1. We prove the result stated as the first line of this table, namely that particles eventually collapse into a point when \(V=I_{d}\) in Theorem 4.1. We remark that assumptions of Theorem 4.1 are much weaker than for the similar results in the full-attention case Geshkovski et al. (2023b), in particular we put no constraints on \(K,Q\) or \(\). This work is a combination of rigorous mathematical results and non-trivial predictions based on analytical insights and numerical simulations. We summarize all limitations in Section 6.

**Related work.** Our work builds upon the framework of Geshkovski et al. (2023b,a) where clustering properties of transformers are analyzed as systems of particles interacting on the sphere. Specifically, Geshkovski et al. (2023b) proved that encoder-only (i.e. unmasked) self-attention with (post) LayerNorm leads to tokens clustering to a single point, in the limit of number of layers going to infinity. This phenomenon is also known as _consensus_ in the related literature of multi-agent systems Markdahl et al. (2017); Criscitiello et al. (2024) and Kuramoto oscillators Strogatz (2000); Abdalla et al. (2022). Work Geshkovski et al. (2023b) in turn expands on the original perspective brought forward by Sander et al. (2022) that identify the self-attention layer as a measure-to-measure map, see also Vuckovic et al. (2021). More recently, Castin et al. (2024) studied the smoothness of this map in a framework that also covers causal attention. This work introduces a clever reparametrization that allows them to recast causal attention as mean-field dynamics, akin to their full attention counterpart. Using various approximations, Cowsik et al. (2024) were able to study a more realistic architecture that also includes MLP layers and produce accurate predictions for the final configuration of particles. This setup was further investigated by Agrachev and Letrouit (2024) from a geometric control perspective. We note also that clustering in the absence of a residual connection (replace \(_{k}(t)\) with \(x_{k}(t+1)\) in (SA)) was established in Wu et al. (2023). Additional effects of the residual connection are studied in Dong et al. (2021) and Zhang et al. (2024).

    & & & \\  \(_{}>0\) & d & First particle \(x_{0}\) & 1a \\ \(_{}>0\) & \( 2\) & One point in \(L\) & 1b \\ \(_{}>0\) & 1 & Two points \(\) and \(-\) & 1c \\ \(_{}<0\) & \( 2\) & Point cloud around \(L\) & 1d \\ \(_{}<0\) & 1 & Two point clouds around \(\) and \(-\) & 1e \\   

Table 1: Possible Final Configurations of Particles

## 2 Causal attention

Before describing our model of causal attention dynamics, we review the idea of Geshkovski et al. (2023) for modeling the _full attention_ dynamics. In that work, the evolution of representations of tokens through the layers is modeled as a system of \(n\) coupled Ordinary Differential Equations (ODEs) describing dynamics of a system of particles \(x_{1}(t),,x_{n}(t)\). A brief part of their derivation of the dynamics from the transformers architecture is written in Section A.1. The particle position \(x_{k}(t)\) corresponds to representation of the \(k\)-th _token_ at layer \(t\) (where for convenience, \(t\) is allowed to take non-integer values) and due to _RMSNorm_ the particles are forced to live on a unit sphere \(^{d-1}\). (RMSNorm layer usually also includes a multiplication by a trainable diagonal matrix \(D\), but the effect of this step can be equivalently achieved by multiplying \(K,Q,V\) matrices by \(D\).) These ODEs are parametrized by three matrices, known as the query \(Q\), the key \(K\) and the value \(V\), respectively, and that are assumed to be square \(d d\) matrices. More specifically, token \(k\) evolves according to

\[_{k}(t)=_{x_{k}(t)}(t)}_{j=1}^{n}e^{  Qx_{k}(t),Kx_{j}(t)}Vx_{j}(t),\] (SA)

where \(_{x}y=y-}\) is the projection onto the tangent space of \(^{d-1}\) at \(x\), and

\[Z_{k}(t)=_{j=1}^{n}e^{ Qx_{k}(t),Kx_{j}(t)}\]

is a normalizing factor. Note that the dynamics of the \(k\)-th token depend on the positions of _all tokens_\(j[n]\), which is a landmark characteristic of full attention leading to the so-called _mean-field dynamics_ studied in Geshkovski et al. (2023); see also Geshkovski et al. (2023); Castin et al. (2024); Paul and Trelat (2024).

In this work we focus on _causal attention_, where the dynamics of token \(k\) depend only on the position of tokens \(j k\). As described in the introduction, this modification is by now the dominant type of transformer architecture in generative AI. To reflect causal masking, we modify the ODE governing the dynamics of token \(k\) as follows:

\[_{k}(t)=_{x_{k}(t)}(t)}_{j=1}^{k}e^ { Qx_{k}(t),Kx_{j}(t)}Vx_{j}(t),\] (CSA)

Figure 1: Particle trajectories for different Value matrices. In all cases we take simple Query and Key matrices \(K=Q=I_{d}\), temperature \(=9\) and final time \(T=5000\) for \(n=32\) particles initialized uniformly at random on the sphere. Positions of particles at time \(T\) are indicated by a red dot.

where the normalizing factor \(Z_{k}(t)\) is naturally updated to

\[Z_{k}(t)=_{j=1}^{k}e^{(Qx_{k}(t),Kx_{j}(t))}.\]

## 3 Single token dynamics

Note that in (CSA) dynamics, the first token is evolving fully autonomously without the influence of others. Thus, we start from the description of its evolution. It will also guide our understanding of the dynamics of subsequent tokens. The first token moves according to the equation

\[(t)=_{x(t)}(Vx(t)).\]

To state its behavior for any matrix \(V\), we need a few definitions. Denote \(_{}\) as the largest real part of all the eigenvalues of \(V\). Let \(L^{}\) be the span of all generalized eigenvectors of \(V\) associated to eigenvalues(potentially different) with their real part equal to \(_{}\). Let \(L L^{}\) be the subspace generated by only the eigenvectors in \(L^{}\) with the largest corresponding Jordan block (the vectors might correspond to different blocks and even to different eigenvalues).

**Lemma 1**.: _Let \(x(t)\) be a solution of an ODE \((t)=_{x(t)}(Vx(t))\) defined on the unit sphere \(^{d-1}\). Then, for almost every initial value \(x(0)^{d-1}\), there exists \(C,c>0\) such that the following convergence rates for the geodesic distance \(\) hold:_

1. _Exponential convergence to_ \(L^{}\)_:_ \((x(t),L^{}^{d-1}) Ce^{-ct}\)_, and_
2. _linear convergence to_ \(L\)_:_ \((x(t),L^{d-1}) ct^{-1}\)__

This result can be derived from standard results on the theory of linear ODEs (proof in Section B.1). We note that this result is important for other tokens as well. Indeed, for every token \(x_{k}\), the contribution to \(_{k}\) in (CSA) from the term with \(j=k\) often has the biggest weight, an effect amplified by large \(\).

In general, eigenvectors corresponding to a real eigenvalue \(=_{}\) create a fixed set in \(L^{d-1}\), while the complex eigenvalues with the largest real part produce a limit torus in \(L^{d-1}\). In what follows, we only consider the case where the eigenvalue with the largest real part is real itself and it only has Jordan blocks of size \(1\). Then, \(L=L^{}\) and convergence to \(L\) is exponentially fast. Note also that when \( L=1\), we have \(L^{1}=\{\}\) for some unit vector \(\). In this case, \(x(t)\) as \(t\), again with exponential speed. These observations will be important for the next section, when we describe asymptotic configurations of tokens.

## 4 Final Configuration

The system of \(n\) tokens that we are studying is far more complicated than for a single token. Even establishing convergence to _some_ point as \(t\) is challenging. In Geshkovski et al. (2023b), similar models were analyzed analytically by noticing that the dynamical system has the structure of the gradient flow of some potential function:

\[(t)= H(x)\,.\]

For such systems, groundbreaking results of Lojasiewicz (1962, 1965, 1984) (see Haraux (2012) for a self-contained overview) guarantee convergence to a critical point of \(H\) assuming it is real-analytic.

However, our system (CSA) does not have a gradient-flow structure and thus techniques of Lojasiewicz are not applicable. On the other hand, we have a significant advantage in the hierarchical structure of our system, allowing us to study tokens sequentially.

We have already understood the evolution of the first token. In this section, we do two things. First, we describe, based on our analytical and numerical insights, conjectures about the asymptotic configuration \(x(t)\) for \(t\). The surprising result here is that only the spectral properties of \(V\) (and not \(K\) or \(Q\)) affect asymptotics. Second, we rigorously prove convergence to a single point for the special case of \(V=I_{d}\). We note that unlike the proof in Geshkovski et al. (2023b) (see also Markdahlet al. (2017); Criscitiello et al. (2024)), our result works for all \(K\) and \(Q\) matrices, while the proof in Geshkovski et al. (2023b) works only for \(Q^{}K=V\) and Markdahl et al. (2017); Criscitiello et al. (2024) is restricted to \(Q^{}K=I_{d}\).

Our main insight is that there are two major forces that drive each token: its internal force which is described by Lemma 1, and the external force induced by all the particles preceding it, which is either attractive or repulsive depending on the sign of the top eigenvalue(s) of \(V\). The balance between the two forces is defined via attention.

To get a better grasp of how the external force works, we consider the case where the first (internal) force vanishes, that is, \(V=I_{d}\). In this case, the tokens collapse asymptotically to a single point.

**Theorem 4.1**.: _Let \(V=I_{d}\) and \(Q,K\) be arbitrary matrices. Then, for almost any starting point \((x_{1}(0),,x_{n}(0))\) with respect to the volume measure on \((^{d-1})^{n}\), the causal transformer dynamics (CSA) converge to a single cluster:_

\[\,k[n],\ _{t}x_{k}(t)=x_{1}(0).\]

We prove this result in Section B.2. In the proof, weight functions are only required to be positive and continuously differentiable (\(C^{1}\)). This ambiguity suggests that incorporating time-dependence of \(Q\) and \(K\) might not alter the theorem's validity, but it significantly adds complexity to the proof in dealing with non-autonomous systems.

Steps similar to our proof of Theorem 4.1 can be followed to study the more general case of the matrix \(V I_{d}\). Unfortunately, one runs into multiple technical issues with application of the stable-manifold theorem from dynamical systems due to the emergence of critical manifolds (as opposed to critical points in the \(V=I_{d}\)) case. Thus, we leave the general case at the status of conjectures, which we describe next.

In what follows, we denote the eigenvalue of \(V\) with the largest real part as \(_{}\) and assume that it is real. If it is not, the limiting configuration is additionally rotating with a constant speed, which complicates the discussion and so is omitted.

Let \(L\) denote the eigenspace of \(_{}\). If \(_{}\) has multiplicity 1, then we denote the corresponding unit eigenvectors as \(\). For simplicity we assume that \(_{}\) has all of the corresponding Jordan blocks of size 1.

First of all, if \( L=1\) then, according to Lemma 1, every token is driven towards \(\) or \(-\) by their own force. Moreover, for \(_{}>0\) the force of other tokens is attractive, while for \(_{}<0\) it is repulsive.

Thus, for \(_{}>0\) all the particles collapse into \(\) and \(-\), whereas for \(_{}<0\) the repulsion force prevents the particles from going all the way to \(\) and instead the particles stabilize at two clouds around \(\) and \(-\). This behavior is captured in Figures 0(c) and 0(c). For the case \(_{}>0\) we formally express it as:

**Conjecture 1**.: _Let \(Q,K\) be arbitrary matrices and \(V\) be diagonalizable with \(d\) different positive real eigenvalues. Denote the largest eigenvalue as \(_{}\) and unit vector \(:V=_{}\). Then, for almost any starting point \((x_{1}(0),,x_{n}(0))\) with respect to the volume measure on \((^{d-1})^{n}\), the causal transformer dynamics (CSA) converge to two clusters_

\[\,k[n],\ _{t}x_{k}(t)\{,-\}.\]

If \(_{}\) has multiplicity at least \(2\), then from Lemma 1 each token internally gets attracted by the eigenspace \(L\). When tokens are close to \(L\), the action of \(V\) becomes close to \(_{}I_{d}\), which for \(_{}>0\) according to Theorem 4.1 forces tokens to collapse to a singleton, while for \(_{}<0\) other tokens exude a repelling force, causing particles to spread out around \(L\). This behavior is captured in Figures 0(b) and 0(d). For the case \(_{}>0\) we formalize it as follows:

**Conjecture 2**.: _Let \(Q,K\) be arbitrary matrices and \(V\) be any matrix such that its largest eigenvalue \(_{}>0\) is real and has an eigenspace \(L\) of dimension \( L 2\), while for any \(z L^{}\) one _has \(Vz L^{}\) with \( Vz,z<_{}|z|^{2}\). Then, for almost any initialization \((x_{1}(0),,x_{n}(0))\), the causal attention dynamics (CSA) converge to one cluster. More specifically, if we define \(\) as the normalized \(L\)-component of \(x_{1}(0)\), i.e., for \(y_{1}:=_{L^{}}(x_{1}(0))\), \(:=y_{1}/|y_{1}|\), then_

\[\,k[n],\ _{t}x_{k}(t)=.\]

(Note that \(\) is undefined when \(x_{1}(0) L\), but this happens with probability zero.)

An important practical observation is that these conjectures explain that \(V\) performs dimensionality reduction in the following way. Tokens converge to \(L^{d-1}\) and, in that space, they move as if acted upon by the \( I_{d}\) matrix on a sphere \(^{ L-1}\). For the pre-trained Lan et al. (2020) the spectra of value matrices is depicted in Figure 4. Interestingly, there are heads with negative \(_{}\). Future work will be concerned with studying real-world matrices \(V\) and connecting their top eigenspaces to semantic meaning of layers and tokens.

## 5 Meta-stable clustering

As we discussed earlier, perhaps the most fascinating discovery of Geshkovski et al. (2023b) is the existence of meta-stable clusters in the full-attention dynamics. It turns out that the same phenomenon persists in the causal-attention dynamics that we study here.

The dynamical evolution of the system is illustrated in Fig. 2. At \(t=150\), the initially uniform distribution of 200 particles consolidates into seven distinct clusters. While Theorem 4.1 establishes the eventual collapse into a single cluster, these intermediate clusters exhibit remarkable metastability, persisting with negligible movement over extended time periods--at least until \(t=500\) according to Fig. 2--before sequential merging occurs. We define these meta-stable configurations as meta-stable clusters, with three-dimensional analogues shown in Fig. 0(a) and 0(b).

Given that the time parameter in our dynamics corresponds to network depth in transformer architectures, the meta-stable configurations, rather than final states (achieved at \(t=(())\)), hold greater practical significance. The emergence of meta-stable clustering and its associated dimensionality reduction may provide fundamental insights into transformers' capacity for generating efficient context-dependent representations.

Figure 2: Evolution of the system (CSA) with \(K=Q=V=I_{2}\) with \(n=200\), \(d=2\), \(=64\), strong Rényi centers (red) and Rényi centers (black) with \(=4^{-1/2}\). Note that strong Rényi centers are visually stationary (as per Lemma 2) but do not explain all clusters. In turn, Rényi centers are moving and merging (one disappears between \(t=75\) and \(t=150\)), but capture more meta-stable clusters.

From a theoretical perspective, understanding meta-stable clustering presents significant challenges, as traditional techniques for asymptotic analysis--such as those used in our Theorem 4.1--prove insufficient. Recent work on full attention transformers has made partial progress in this direction. Koubbi et al. (2024) demonstrated that when self-attention dynamics approach a nearly clustered state, they will converge to a tightly clustered configuration and remain stable for an exponential time period. Complementing this, Bruno et al. (2024) proved that tokens initialized near a uniform distribution on the sphere will spontaneously organize into a loosely clustered state. However, the bounds on the clustering tightness in this second line of work are not sufficient to trigger the convergence conditions required by Koubbi et al. (2024)'s theorem.

This Section presents a fundamental discovery regarding the identification of cluster centers in causal-attention dynamics. We establish three key claims: First, we demonstrate that initialization irregularities generate distinctive tokens, termed Renyi parking centers, which evolve into meta-stable cluster nuclei. While this phenomenon is primarily supported by numerical evidence (Fig. 3), it provides crucial insight into the clustering mechanism. Second, we prove that a subset of these special tokens, called strong Renyi centers, maintains near-stationarity over extended time periods (Lemma 2). Both Renyi and strong Renyi centers occur with frequency \((^{})\), confirming the \(\) scaling predicted for \(d=2\) by Geshkovski et al. (2023); see also Koubbi et al. (2024); Bruno et al. (2024). Third, we establish in Theorem 5.1 that as \(t\), all remaining tokens will converge to the vicinity of one of these stationary tokens, completing the meta-stable clustering process.

This section restricts our analysis to the case where \(V=I_{d}\). For general matrices \(V\), our empirical observations suggest that particles rapidly converge to a lower-dimensional subspace spanned by \(d_{1} d\) principal eigenvectors. Consequently, we conjecture that the number of meta-stable clusters should rather be \(^{-1}{2}}\), where the ambient dimension \(d\) is replaced by the effective dimension \(d_{1}\). While a rigorous proof of this dimension-reduction remains an open problem for future investigation, this phenomenon motivates our focus on low-dimensional cases (specifically \(d=2\)) throughout this section.

For convenience, we also fix \(Q=K=I_{d}\), though this condition could be easily relaxed (e.g. to \(Q^{}K=K^{}Q=V\)). Under these assumptions, the system can be rewritten in polar coordinates \(x_{k}=[(_{k}),(_{k})]^{}\) as

\[_{k}=}_{j=1}^{k}e^{((_{k}- _{j})-1)}(_{j}-_{k})=}_{j=1}^{k-1} h(_{j}-_{k}),\] (CSA-2d)

with interaction potential given by

\[h(x):=e^{((x)-1)} x, Z_{k}=_{j=1}^{k}e^{ ((_{k}-_{j})-1)}.\] (IntPot)

### Renyi Parking

The prediction of meta-stable clustering center locations exhibits a notable connection to the Renyi parking problem.

Consider a sequence of tokens \((x_{j})_{j 1}\) on the sphere \(^{d-1}\) equipped with geodesic distance \(\). For a fixed separation parameter \(>0\), we define:

* _Renyi centers_ as the subsequence \((x_{s_{j}})_{j 1}\), where \((s_{j})_{j 1}\) is strictly increasing and satisfies: \[(x_{s_{j}},x_{s_{i}})>\]
* _Strong Renyi centers_ as the subsequence \((x_{s_{j}})_{j 1}\) satisfying: \[(x_{s_{j}},x_{i})>$}\]

By construction, the set of Strong Renyi centers forms a subset of Renyi centers.

As demonstrated in Section A.2, particles in our system exert maximal attractive force at distances of order at most \(^{-1/2}\), with rapid decay beyond this scale. For strong Renyi centers defined with separation parameter \(=c^{-1/2}\) (where \(c\) is sufficiently large), this decay ensures negligible influence from preceding particles and thus remain stable for a long time--a phenomenon formally established in Lemma 2. This metastability, coupled with rapid particle aggregation tokens, indicates that strong Renyi centers serve as primary attractors for subsequent tokens.

Renyi centers are unaffected by previous particles but only by previous Renyi centers, thereby generating new clustering centers. For fixed \(\), there are more Renyi centers than strong Renyi centers (see Section C.4 for exact cardinality analysis). While Renyi centers better capture the meta-stable clustering effect, as illustrated in Figures 3 and 2, they lack positional stability and may converge to other centers over time. Although Renyi centers rapidly aggregate a large fraction of particles, some of these particles continue to migrate and eventually converge to strong Renyi centers.

The next result shows that strong Renyi centers remain nearly fixed for a long time.

**Lemma 2**.: _Let \(d=2\) and \(Q=K=V=I_{2}\). Consider a subsequence of strong Renyi centers \(x_{s_{1}},,x_{s_{m}}\) satisfying the separation condition with constants \(,c>0\)_

\[_{i<s_{j}}|x_{s_{j}}-x_{i}|>c(1+2)^{-1/2}.\]

_Assume that_

\[c>^{1/2}((-1++1})/(2)).\] (1)

_Then for any time \(T_{j}\) such that_

\[T_{j}s_{j}h(c^{-1/2})< c^{-1/2},\] (2)

_where the interaction potential \(h\) is defined in (IntPot), the displacement of each center is bounded by_

\[_{t[0,T_{j}]}|x_{s_{j}}(t)-x_{s_{j}}(0)|< c^{-1/2}.\]

The key observation driving this result is that strong Renyi centers are weakly affected by all previous particles. However, though this is correct on short time scales, it should be checked for all times in \([0,T]\). A complete proof can be found in Section C.1.

**Remark 1**.: _Using the properties of \(h\) derived in Section A.2 it can be shown that_

* _For_ \(>1\) _a sufficient condition for (_1_) to hold is simply_ \(c>1\)_,_
* _a sufficient condition for (_2_) to hold is_ \[T_{j}s_{j}<e^{c^{2}/2-c^{4}/(24)}.\]

_Moreover, it is easy to prove that indexes \(s_{j}\) are mostly small. Thus, we see that early strong Renyi centers are almost stationary for a time that is exponential with the square of separation magnitude._

Figure 3: Total percentage of particles consumed by Rényi and strong Rényi centers over time. Here we have plotted average, \(0.1\) and \(0.9\) quantiles over 5000 experiments with \(n=200\), \(d=2\), \(=64\), \(=4^{-1/2}\).

Renyi centers and strong Renyi centers play a fundamental role in meta-stable clustering, warranting analysis of their properties as extreme points in a sequence. While defined here using geodesic distance on a sphere, the definition extends naturally to distances induced by \( Qx,Ky\) under appropriate conditions. This generalization aligns with our observation that meta-stable clustering occurs in the subspace \(L\) where \(V\) sends tokens and acts as the identity on \(L\).

The distribution of these centers under various initialization schemes presents a key analytical challenge. Section C.4 addresses the uniform i.i.d. case, where Renyi's classical result characterizes the expected number of centers. Extensions to general distributions and Markov processes--more relevant to language processing applications--remain open for Renyi centers due to their structural complexity, particularly in higher dimensions (\(d>2\)). In contrast, strong Renyi centers are much easier to handle: even our computation of the average number of centers in Section C.4 works for any distribution regardless of the dimension.

### Fixed Meta-stable Clustering Centers

Having established the existence of \(O()\) quasi-stationary tokens for \(d=2\) and \(n 1\), we next examine their role as cluster centers. While Figures 3 and 2 provide substantial numerical evidence that these tokens attract and aggregate nearby particles, a rigorous proof remains elusive. We establish instead a weaker result: when quasi-stationary tokens are artificially frozen (analogous to cross-attention in encoder-decoder architectures), all other tokens converge to these frozen centers. This simplified model, while instructive, differs from true meta-stable clustering in important aspects detailed in Section 6.

We only state our result for \(d=2\) and identity parameter matrices as in (CSA-2d).

**Theorem 5.1** (Clustering to frozen tokens for \(K=Q=V=I_{2}\)).: _Let \(_{1},,_{m}\) be fixed tokens that are well-separated, namely \(|_{i}-_{j}|>c^{-1/2}\). Let \(_{0}\) be an absolutely continuous probability measure on \((^{1})^{n}\) and let \(_{1}(0),,_{n}(0)_{0}\). Consider causal attention dynamics (CSA-2d), with additional influence from the fixed tokens \(_{j}\), which enter evolution with additional weights \(a_{j} 1\). Specifically, we have_

\[_{k}=}_{j=1}^{k}e^{((_{k }-_{j})-1)}(_{j}-_{k})+_{j=1}^{m}a_{j}e^{( (_{k}-_{j})-1)}(_{j}-_{k}),\]

_with_

\[Z_{k}=_{j=1}^{k}e^{((_{k}-_{j})-1)}+_{j=1}^{m}a_ {j}e^{((_{k}-_{j})-1)}.\]

_Define \(N=n+_{j=1}^{m}a_{j}\) and \(g=h^{}\) where \(h\) is the interaction potential of (IntPot)._

_If \(N,,>0\), and \(c>2+2\) satisfy:_

\[Nh((c-1-2)^{-1/2}) <h(^{-1/2})\] \[Ng((c-2)^{-1/2}) <g(^{-1/2}),\]

_then with probability one, \((t)\) converges to an asymptotically stable critical point \(^{*}(^{1})^{n}\) satisfying:_

\[ k[n],\; j[m]:|^{*}_{k}-_{j}|< ^{-1/2}.\]

Since our dynamical system is not a gradient flow, the classical Lojasiewicz convergence theorem does not apply. Instead, we establish convergence by observing that the causal dynamics (both with and without frozen tokens) is, in fact, a _sequential_ gradient flow, where each particle minimizes a slightly different energy potential. For such systems on \(^{1}\), we demonstrate convergence through an alternative approach that circumvents the Lojasiewicz framework.

**Lemma 3**.: _Consider a system of \(n\) particles on \(^{1}\) with angular coordinates \(_{1},,_{n} 2/\) evolving according to_

\[_{k}=-(_{1},,_{k})}}E_{k}(_{1},,_{k}),\]

_where \(E_{1},,E_{n}\) are \(C^{1}\) energy functions and \(Z_{k}\) are \(C^{1}\) normalization factors bounded by \(0<c<Z_{k}()<C\). Assume:_1. _Each_ \(E_{k}\) _has isolated critical points in_ \(_{k}\) _for fixed_ \(_{j},j k\) _(satisfied by analyticity),_
2. _For any_ \(k[n]\)_, critical points restricted to the first_ \(k\) _particles are either strongly stable (the Jacobian has only eigenvalues with strictly negative real parts) or strongly unstable (there is an eigenvalue with a strictly positive real part)._

_Then for almost every initial condition \((0)\) with respect to Lebesgue measure, \((t)\) converges to a strongly stable critical point \(^{*}\)._

The proof is deferred to Section C.2.

**Remark 2**.: _The conditions of Theorem 5.1 are satisfied under the following explicit bounds:_

\[<0.1, c 5.5+2,(c-1-2 )^{2}/2, N(3(c-1-2 )^{2}/8).\]

_Note that this result requires only \( N\). For example, taking \(=0.1\) and \(c=6.5\) yields \( 14\) and \(N 700\) is sufficient. See Lemma 5 for the proof._

## 6 Limitations

Our analysis presents both theoretical and practical limitations. From a theoretical perspective, we establish two key results: (1) strong Renyi centers maintain quasi-stationarity for time scales of order \((c^{2}/2)\) per Lemma 2, and (2) exactly stationary centers attract all remaining particles (Theorem 5.1). However, this falls short of proving meta-stable clustering, as Theorem 5.1 provides no bounds on the convergence time. Consequently, we cannot guarantee that strong Renyi centers remain sufficiently stationary during particle aggregation. A complete meta-stability theory would require demonstrating that each Renyi center captures \((n)\) particles in \(O(1)\) time as \(n\). Currently, even the weaker claim of capturing \((1)\) particles remains unproven, presenting a crucial direction for future research.

The practical limitations stem from two model simplifications: the use of tied weights across layers (though supported by successful implementations, see Lan et al. (2020)), and the omission of the MLP layer central to Transformer architectures. Incorporating the MLP dynamics into our theoretical framework remains a significant open challenge.