# Decentralized Matrix Sensing:

Statistical Guarantees and Fast Convergence

 Marie Maros

School of Industrial Engineering

Purdue University

mmaros@purdue.edu

&Gesualdo Scutari

School of Industrial Engineering

Purdue University

gscutari@purdue.edu

###### Abstract

We explore the matrix sensing problem from near-isotropic linear measurements, distributed across a network of agents modeled as an undirected graph, with no server. We provide the first study of statistical, computational/communication guarantees for a decentralized gradient algorithm that solves the (nonconvex) Burer-Montiero type decomposition associated to the low-rank matrix estimation. With small random initialization, the algorithm displays an approximate two-phase convergence: (i) a _spectral phase_ that aligns the iterates' column space with the underlying low-rank matrix, mimicking centralized spectral initialization (not directly implementable over networks); and (ii) a _local refinement phase_ that diverts the iterates from certain degenerate saddle points, while ensuring swift convergence to the underlying low-rank matrix. Central to our analysis is a novel "in-network" Restricted Isometry Property which accommodates for the decentralized nature of the optimization, revealing an intriguing interplay between sample complexity, network connectivity & topology, and communication complexity.

## 1 Introduction

Matrix sensing-the estimation of a low-rank matrix from a set of linear measurements-finds applications in diverse fields such as image reconstruction (e.g., ), object detection (e.g., ) and array processing (e.g., ), to name a few. It also serves as a benchmark for determining the statistical and computational guarantees achievable in deep learning theory, since it retains many of the key phenomena in deep learning while being simpler to analyze. Despite significant progress in understanding the convergence and generalization properties of various solution methods for training such learning models, a majority of these advances focus on a centralized paradigm, aggregating data at a central location with vast computing resources-good tutorials on the topic include . This centralized approach, however, is increasingly unsuitable for modern applications due to server bottlenecks, inefficient communication, and power usage. Therefore, the development of statistical learning methods for massively decentralized networks without servers is timely and crucial.

This paper tackles the matrix sensing problem from data distributed over networks. We contemplate a network of \(m\) agents modeled as an undirected graph with no servers, where agents can communicate with their immediate neighbors-these architectures are also known as _mesh_ networks. The collective objective is to estimate a ground-truth matrix \(^{}^{d d}\), based on \(N=m n\) total observations \(y_{1},,y_{N}\), equally split into \(n\)-sized, disjoint datasets \(_{1},_{2},,_{m}\). Each agent's signal model is thus given by

\[y_{j}=A_{j},^{}:=(A_{j}\, ^{}),j_{i}.\] (1)

Here, \(A_{j}^{d d}\), with \(j_{i}\), are the known symmetric measurement matrices to agent \(i\); \(^{}\) is assumed symmetric, positive semidefinite, and low-rank, i.e., \(r^{}:=(^{})<<d\).

To minimize communication overhead and avoid the need for \(d d\) matrix transmission, we employ the Burer-Monteiro-type decomposition of the estimate \(\) of \(^{}\), that is, \(=^{}\), with \(^{d r}\), and seek to minimize the squared loss \(F()\), defined as

\[_{^{d r}}F():=_{i=1}^{m} _{j_{i}}(y_{j}-<A_{j}, {U}^{}>)^{2}}_{:=f_{i}()},\] (2)

where \(f_{i}()\) is the loss function of agent \(i\). Ideally, the number \(r\) of columns of \(\) should be set to \(r^{}\). However, \(r^{}\) might not be known in advance. In this study, we consider the so-called _over-parameterized_ regime where \(r{}r^{}\).

The formulation (2) poses multiple challenges. Firstly, \(F\) is nonconvex, lacks global smoothness (i.e., global Lipschitz continuity of \( F\)), and is not entirely known to the agents. Secondly, the over-parameterized regime may intuitively suggest a risk of overfitting. However, recent studies (e.g., ) have compellingly revealed that when _centralized_ Gradient Descent (GD) is applied to (2) with a small random initialization, it induces an implicit bias towards simpler solutions with favorable generalization properties. This bias-often referred to as the _simplicity bias_ or the _incremental learning_ behavior of GD(/Stochastic Gradient Descent)-assists in the exact or approximate recovery of the ground truth \(^{}\). Interestingly, this phenomenon serves as a hidden mechanism in various other (deep) learning tasks that mitigates overfitting in highly over-parameterized models (e.g., ). However, the direct implementation of GD in mesh networks is not feasible due to the lack of access to \( F\) by the agents or of a server collecting agents' gradients \( f_{i}\).

The goal of this paper is to uncover the possible simplicity bias of a _decentralized_ instance of GD, solving the matrix sensing problem (2) over mesh networks. To the best of our knowledge, this study is unique in the realm of decentralized optimization, establishing the first sample, convergence rate, and generalization guarantees of a decentralized gradient-based algorithm tailored for matrix sensing over mesh networks. We delve into the relevant existing literature in the subsequent section.

### Related works

The literature offers numerous decentralized algorithms which could in principle be used to tackle the matrix sensing problem, directly or indirectly. However, the accompanying statistical and computational guarantees fall short, being either non-existent or inadequate, as we elaborate next.

\(\)**Off-the-shelf decentralized algorithms for nonconvex problems:** The matrix sensing problem (2) naturally invites the application of decentralized algorithms specifically designed for nonconvex losses in the form \(F=(1/m)_{i=1}^{m}f_{i}\) (summation of agent functions). Noteworthy examples of such algorithms include **(i)** decentralizations of the GD that merge local gradient updates with (push-sum) consensus algorithms , **(ii)** decentralized first-order methods employing gradient tracking strategies , and **(iii)** decentralized algorithms grounded on primal-dual decomposition or penalization of lifted reformulations incorporating explicitly consensus constraints . However, despite their initial appeal, when applied to (2), these algorithms either lack of any convergence guarantee-the requirement that \(F\) is _globally smooth_ and has a (uniformly) _bounded_ gradient  is not met by the matrix sensing loss in (2)-or they converge at sublinear rate to _some_ critical points of \(F\) (which may not be the global minimizers), whose generalization properties remain unexplored and obscure .

\(\)**Ad-hoc decentralized algorithms for some matrix recovery problems:** This line of works comprises decentralized schemes designed _specifically_ for the _structured_ matrix-related optimization problem under consideration. Relevant examples are briefly highlighted next.

**(i) Dictionary learning & matrix factorization problems :** In , convergence of a decentralized gradient tracking method for certain dictionary learning problems is established; a similar problem class is further investigated in , where generalization properties of a penalized consensus algorithm are studied, albeit without a convergence rate analysis. Despite their differences, these studies share a common premise of a _full_ observation model, leading to a loss in the form of \(F(^{})=\|Y-^{}\|^{2}\). Here, \(Y=^{}+N\) is the data matrix with \(N\) denoting noise. This full observation model contrasts with the matrix sensing model in (2), which is based on _partial_ (noiseless) measurements. Lastly,  proposes a distributed Frank-Wolfe algorithm to address a low-rank matrix factorization problem, formulated as a trace (nuclear) norm _convex_ minimization problem (the nonconvex rank constraint is substituted by a nuclear norm constraint).

**(ii) Distributed spectral methods [45; 19; 11; 12; 10; 44; 43]:** Spectral methods have been established as effective strategies for obtaining reliable estimates of leading eigenvectors of a specified data matrix, as well as for providing a promising "warm start" for numerous iterative nonconvex matrix factorization algorithms [4; 7]. Recent developments [19; 11; 12; 10; 44; 43] have successfully extended spectral methods-particularly principal component analysis-to decentralized contexts, achieving linear convergence rates, communications per iteration on the order of \((dr)\), and precise recovery up to a desired accuracy. A good tutorial on this subject can be found in . These methods, in principle, can tackle the decentralized matrix sensing problem as formulated in this work through the estimation of the leading eigenspace of the surrogate matrix \(Y=_{i=1}^{m}_{j_{i}}y_{j}A_{j}\), where \(_{j_{i}}y_{j}A_{j}\) is held by agent \(i\). In fact, under suitable RIP on the linear mapping associated with \(Y\), one has \((1/N)_{i=1}^{m}_{j_{i}}y_{j}A_{j}^{}\). While such an approach can yield valuable insights about the ground truth \(^{}\), _exact_ recovery of \(^{}\) to arbitrary precision cannot be guaranteed . This limitation starkly contrasts with the robust guarantees attainable by the gradient algorithm applied to the centralized matrix sensing problem with small random initialization (e.g., [34; 20]).

\(\)**Generic saddle-escaping decentralized algorithms:** Under a sufficiently small RIP constant of the linear mapping associated with the signal model (1), the matrix sensing loss in (2) is shown to have no spurious local minima and all strict saddle points [1; 22]. Consequently, the task becomes escaping strict saddle points and computing second-order critical points. In the distributed optimization context, recent works studied the escape properties of several decentralized algorithms. Early works showed that certain decentralized schemes-the deterministic DGD [6; 18], the subgradient-flow , gradient-tracking algorithms , and primal-dual based methods [16; 24]-with random initialization, converge _asymptotically_ towards a second-order critical point of a smooth function (subject to mild regularity conditions), with high probability. However, this near-certain convergence does not necessarily imply fast convergence. There exist non-pathological functions for which randomly initialized GD requires exponential time (in the ambient dimension) to escape saddle points . It remains uncertain whether the inherent structure of the matrix sensing problem could yield superior convergence guarantees. Subsequent research has investigated the impact of decaying, additive noise perturbation on the agents' gradients of (stochastic) DGD in the Adapt-then-Combine (ATC) form [40; 39; 41]. While convergence to approximately second-order stationary points is assured within a polynomial number of iterations, the prerequisite that the loss has a _globally_ Lipschitz gradient and Hessian matrix is not met by the matrix sensing loss in (2). This leaves decentralized saddle-escaping methods bereft of convergence rate _and_ generalization guarantees when applied to (2).

### Major contributions

We establish the first _convergence rate and generalization guarantees_ of a _decentralized_ gradient algorithm solving the matrix sensing problem via (2) over mesh networks. We borrow the following decentralized gradient descent ,: for each agent \(i=1,,m\),

\[_{i}^{t+1}=_{j=1}^{m}w_{ij}_{j}^{t+1/2} _{i}^{t+1/2}=_{j=1}^{m}w_{ij}_{j}^{t}- f_{i} (_{j=1}^{m}w_{ij}_{j}^{t}),\] (3)

Here, \(_{i}^{t}\) is an estimate at iteration \(t\) of the optimization, common matrix \(\) in (2) held by agent \(i\); \((0,1]\) is the stepsize; and \(w_{ij}\)'s are appropriately chosen nonnegative weights. We have \(w_{ii}>0\), \(i=1,,m\), and \(w_{ij}>0\) if agents \(i\) and \(j\), \(i j\), can communicate; otherwise \(w_{ij}=0\). The algorithm employs two communication steps/iteration, aiming to enforce an agreement on both iterates \(_{i}^{t}\) and local gradients \( f_{i}\). One could reduce the communication steps to _one_ per iteration via a suitable variable change, resulting in the DGD-ATC form . However, for the sake of clarity and ease of analysis, we opt to keep the form in (3), without any loss of generality.

\(\)**Guarantees:** Our study presents a thorough statistical and convergence analysis of (3), yielding the following key insights. **(i)**_Convergence to low-rank solutions_: We demonstrate that, regardless the degree of overparametrization \(r\), the iterates generated by (3) from a small random initialization converge towards low-rank solutions. We also provide an estimate of the worst-case iteration complexity. Improving results of the GD in a centralized setting (e.g., ), we specify an _entire_ interval for algorithm termination, within which the generalization error is guaranteed to remain below the desired accuracy. This interval expands as the initialization becomes smaller. **(ii)**_Two-phase convergence:_ Our analysis reveals a two-phase convergence behavior of (3). The initial "spectral" phase sees the iterates mimic a theoretical centralized power method with full data access, while the subsequent "refinement" phase steers the trajectory towards the ground-truth solution. To the best of our knowledge, this provides the first evidence of a _simplicity_ bias in a decentralized algorithm, aligning with the observed behavior of centralized GD . **(iii)**_Generalization error and communication complexity:_ The generalization error is shown to scale polynomially with the initialization size while the communication complexity scales logarithmically. Consequently, one can achieve arbitrarily small estimation errors with only a modest increase in communication and computation cost. **(iv)**_RIP and network connectivity:_ Our findings hold under the conditions that the centralized measurement operator satisfies the standard RIP, and that network connectivity is sufficiently small. The former condition implies that our algorithm operates under the _same_ sample complexity requested in the centralized setting. The latter is distinctive of the decentralized settings and shown to be unavoidable. **(v)**_Almost performance invariance with the network size:_ We demonstrate that with an increase in the network size, the generalization error maintains its consistency whereas the communication cost grows logarithmically. Thus, the algorithm ensures effective error control with a marginal increase in communication overhead as the network expands.

\(\)**Convergence analysis:** Although our analysis draws some insights from , the proof techniques employed diverge from those used therein. The decentralized nature of our setting introduces additional error terms, thereby making the analysis substantially more complex. Our methodology hinges on a newly introduced concept of RIP, termed _in-network_ RIP. This concept harnesses the RIP of the measurement operator, much like the centralized GD, and intertwines it with the network's connectivity to derive favorable attributes of the new, overarching network-wide measurement operator. Furthermore, it reveals the interplay between sample complexity, network connectivity & topology, and communication complexity towards achieving statistical and computational guarantees over networks. Although we have defined the in-network RIP in the context of our specific algorithm dynamics, we posit that it possesses independent significance and could potentially pave the way for performance analysis of other distributed schemes.

## 2 Preliminaries

In this section, we first list the notations used in the paper, and then provide details of our theoretical setup and necessary preliminary results.

### Notations

For any positive integer \(m\), we define \([m]\{1,,m\}\); \(1_{m}\) is the \(m\)-dimensional vector of all ones; \(I_{d}\) is the \(d d\) identity matrix; \(\) denotes the Kronecker product; and \((M)\) (resp. \((M)\)) denotes the range space (resp. rank) of the matrix \(M\). When considering a matrix \(M^{md md}\), partitioned into blocks of size \(d d\), we will denote the block at the \(i\)-th row and \(j\)-th column as \([M]_{ij}\), for \(i,j[m]\). Here, \(i\) and \(j\) indices increment by \(d\), reflecting the size of the blocks.

We use \(\|\|\) to denote the Euclidean norm. When applied to matrices, \(\|\|\) is the operator norm induced by \(\|\|\), and \(\|\|_{F}\) denotes the Frobenius norm of the argument matrix. We order the eigenvalues of any symmetric matrix \(M^{d d}\) in nonincreasing fashion, i.e., \(_{1}(M)_{d}(M)\). The singular values of (a rectangular) matrix \(M\) of rank \(r\) are denoted as \(_{1}(M)_{2}(M)_{r}(M)>0\).

_Truncated SVD:_ For any given matrix \(M^{d_{1} d_{2}}\), with rank \(r^{}>0\), we write the truncated SVD as \(M=V_{M}_{M}Q_{M}^{}\), where \(V_{M}^{d_{1} r^{}}\) and \(Q_{M}^{d_{2} r^{}}\) satisfy \(V_{M}^{}V_{M}=I_{d_{1}}\) and \(Q_{M}^{}Q_{M}=I_{d_{2}}\), and \(_{M}^{r^{} r^{}}\) is a diagonal matrix.

_Augmented matrices:_ It is convenient to introduce the following "augmented" matrices suitable to rewrite the decentralized algorithm (3) in a concise block-stacked form:

\[:=W I_{d},:=(1/m)1_{m}1_{m}^{} I _{d}.\] (4)

where \(W^{m m}\) is the matrix of the gossip weights in (3), defined as \([W]_{ij}=w_{ij}\).

### Basic definitions and assumptions

We develop our theoretical analysis under the following standard assumptions on the matrix sensing problem, algorithm parameters, and network connectivity.

\(\)**On the matrix sensing problem:** Given the signal model (1), we decompose the ground-truth matrix as \(^{}=^{}\), for some \(^{d r^{}}\) (recall, \(r^{}\) is the rank of \(^{}\)).

**Definition 1** (condition number).: _We define the condition number of \(^{d r^{}}\) as \(=\|}{_{r^{}}(X)}\)._

We associate to the signal model (1) the measurement linear operator \(^{d d}}() ^{N}\) and its adjoint \(^{N} w}^{}(w)^{d d}\), defined as

\[}():=}(<A_{j},> )_{j_{i},i[m]}}^{ }(w)=}_{i=1}^{m}_{j_{i}}w_{j}A_{j}.\] (5)

A standard assumption in the matrix sensing literature is requiring the RIP for the operator \(}\).

**Definition 2** (RIP).: _The measurement operator \(}:^{d d}^{N}\) satisfies the \((,r)\)-RIP condition if_

\[(1-)\|\|_{F}^{2}\|}()\|^{2}(1+ )\|\|_{F}^{2},\] (6)

_for all matrices \(^{d d}\) with rank\(() r\)._

The RIP condition is the key to ensure the ground truth \(^{}\) to be recoverable with partial observations. In fact, an important consequence of RIP is that \(}^{}}()=(1/N)_{i=1}^{m}_{j _{j}} A_{j}, A_{j}\), for all \(\) low-rank (see, e.g., ). Notice that when all entries of the matrices \(A_{j}\) are drawn i.i.d. with distribution \((0,1)\) on the off-diagonal entries and distribution \((0,1/)\) on the diagonal, the \((,r)\)-RIP holds with high probability, if the number of observations \(N=(dr/^{2})\) (e.g., ).

\(\)**Network setup and gossip matrices:** Agents are embedded in a communication network, modelled as an undirected graph \(=\{,\}\), where the vertices \(=[m]\{1,,m\}\) correspond to the agents and \(\) is the set of edges of the graph; \((i,j)\) if and only if there is a communication link between agents \(i\) and \(j\). We study the decentralized algorithm (3) using gossip weight matrices satisfying the following standard assumption in the literature of distributed optimization.

**Assumption 1**.: \(W=[w_{ij}]_{ij=1}^{m}\) _satisfies: **(i)**\(w_{ij}>0\), if \((i,j)\); otherwise \(w_{ij}=0\); furthermore, \(w_{ii}>0\), for all \(i[m]\); **(ii)**\(W=W^{}\) and \(W1=1\) (stochastic); **(iii)**\(W\) is positive semidefinite; and **(iv)** there holds \(\|W-1_{m}1_{m}^{}/m\|_{2}<1\)._

Assumption 1 is standard in the literature of distributed algorithms and is satisfied by several weight matrices; see, e.g., . Note that \(<1\) holds true by construction for connected graphs. Roughly speaking, \(\) measures how fast the network mixes information; the smaller \(\), the faster the mixing.

### Augmented mapping and in-network RIP

Fundamental to our analysis is a novel RIP-like property associated with an augmented linear mapping tied to the decentralized algorithm (3). This new property effectively captures the admissible "degree of distortion" on the signal information \(^{}\), taking into account both the partial observability of \(^{}\) as postulated in (1) (through the measurement operator \(}\) defined in (5)) and the intricacies of the in-network optimization process (regulated by the network operator \(\), as defined in (4)).

We begin rewriting the decentralized algorithm (3) in a compact form. To do so, we define the following quantities: (i) the stacked block matrices \(U^{t}^{md r}\) and \(Z^{}^{md md}\):

\[U^{t}:=(_{i}^{t})_{i[m]} Z^{}:=1_{m}1_{m}^ {}^{},\] (7)

respectively; (ii) the augmented mapping \(:^{md md}^{N}\) and its adjoint \(^{}:^{N}^{md md}\):

\[[(Z)]_{}}<m(w_{()}w_{()}^{}) A_{},Z>, ^{}(q)_{=1}^{N}}{} (mw_{()}w_{()}^{}) A_{},\] (8)

where \(():_{i=1}^{m}_{i}\) returns the index \(i\) such that \(_{i}\), \([N]\). Note that these operators depend on both data measures (via \(}\)) and the network (via \(\)). Using the definitions in (4), (7), and (8), it is not difficult to check that (3) can be rewritten equivalently as:

\[U^{t+1}=(^{2}+^{}(Z ^{}-U^{t}(U^{t})^{}))U^{t}.\] (9)For the convergence of (9) towards low-rank matrices with strong generalization properties, we anticipate certain conditions to be imposed on the operator \(\). The algorithmic mapping structure in (9) provides some insights in this regard. Since \(^{2}=\) (due to Assumption 1(ii)), the linear network operator \(^{2}\) effectively functions as the identity map on matrices \(Z^{dm dm}\) with \((Z)()\). This is in particular true for \((d d\) block) consensual matrices \(Z=Z\), including the (augmented) ground-truth \(Z^{}\), as defined in (7). This implies that to accomplish precise reconstructions of \(Z^{}\), the operator \(\) ought to exhibit some RIP-like regularity. Postponing to Sec. 3.1 a more rigorous and comprehensive argument, we claim that the following property suffices.

**Definition 3** (In-network RIP).: _The operator \(:^{md md}^{N}\) defined in (8) satisfies the in-network \((,r)-\)RIP property with tolerance \( 0,\) if_

\[(1-)\|Z\|_{F}^{2}-\|Z-Z\|_{F}^{2}\|(Z)\|_{2}^{2}(1+)\| \|_{F}^{2}+\|Z-Z\|_{F}^{2},\] (10)

_for any matrix \(Z^{md md}\) such that each of its \(d d\) blocks \([Z]_{i,j}\) and its block-average \(=}_{i=1}^{m}_{j=1}^{m}[Z]_{i,j}\) are of rank at most \(r\)._

The condition (10) reads as an "exact" RIP property of \(\) along (block) consensual directions, allowing for some "perturbation" in the form of consensus errors \(\|Z-Z\|_{F}^{2}.\) The following result shows that the tolerance error can be controlled by the network connectivity \(\) (see Assumption 1(iv)).

**Lemma 1**.: _Suppose \(}\) satisfies the \((_{2r},2r)\)-RIP, and the gossip matrix \(W\) is chosen according to Assumption 1. Then, the augmented operator \(\) satisfies the in-network \((2_{2r},r)\)-RIP with tolerance_

\[=^{2}(1+2_{2r})}{_{2r}}(1+_{2r}).\] (11)

Clearly, the smaller \(\), the smaller \(\), revealing an unexplored interplay between (potential) generalization properties and network characteristics. Since small \(\) can be enforced also by employing multiple rounds of communications per iteration (see Sec. 3 for details), the communication complexity enters in the tradeoff equation. Our theory in the next section will quantify this interplay, revealing conditions and tuning recommendations to achieve fast convergence and strong generalization properties.

## 3 Main Results

We are ready to state the convergence results of Algorithm (3). Here we consider the overparametrized case \(r 2r^{}\) while the other ranges of \(r\) are discussed in the supplementary material.

**Theorem 1**.: _Consider the matrix sensing problem (1), with augmented ground-truth \(Z^{}\), under \(r 2r^{}\), and the measurement operator \(}\) satisfying the \((4(r^{}+1),)-\)RIP, with \(^{-4}(r^{})^{-1/2}.\) Let \(\{U^{t}\}_{t}\) be the (augmented) sequence generated by Algorithm (3), under the following tuning: (i) the stepsize \(^{-4}\|\|^{-2}\); (ii) the gossip matrix \(W\) is chosen to satisfy Assumption 1, with_

\[}{m^{6}^{4}r^{}};\] (12)

_and (iii) the initialization \(U^{0}\) is chosen as \(U^{0}= U,\) where \(U^{md r}\) has i.i.d. \((0,)\) distributed entries, and \(\) satisfies_

\[^{2}\{}{d^{9}},}{d}(^{2}})^{-96^{2}}\}.\] (13)

_Then, after_

\[}^{2}()}( (^{2}})+(}( )}{})+(\{1,}{r-r^{ }}\}\|}{}))\] (14)

_iterations, there holds_

\[}(U^{})^{}-Z^{}\|_{F}}{\|Z^{}\|} ((r-r^{})^{7/8}(r^{})^{1/8}\|\|^{-21/16}^{21/16}( ^{2})^{21/16}),\] (15)

_with probability at least \(1-c_{1}e^{-c_{2}\,r}\), where \(c_{1},c_{2}>0\) are universal constants._\(\)_Statistical guarantees:_ (15) demonstrates that, in the setting above, the iterates \(U^{t}(U^{t})^{}\) converge to an estimate of the low-rank solution \(Z^{}\) within a precision that can be made arbitrarily small by reducing the size \(\) of the random initialization. The test error's dependence on \(\) is polynomial, whereas the worst-case convergence time only increases logarithmically with \(\) (see (14)), indicating that significant test error reductions can be achieved with moderate increases in communication and local computations. These guarantees are established under the RIP of the measurement operator \(}\), and thus operate under the same sample complexity as the centralized setting. For instance, for Gaussian measurement matrices, \(N d(r^{})^{2}^{8}\). While the dependence on \(d\) is optimal, the scaling on \((r^{})^{2}\) and \(^{8}\) is less favorable compared to convex approaches based on nuclear norm minimization . However, decentralized methods solving such formulations directly (e.g., ) would entail a communication cost of \((d^{2})\), which is significantly less favorable than the \((rd)\) of Algorithm 3.

\(\)_On the number of iterations:_ Interestingly, the worst-case iteration complexity aligns with what observed for the GD in the centralized setting, following thus the same interpretation . The first term in (14) represents the duration of the _spectral alignment_ phase: beginning from a small initialization, the iterates \(U^{t}(U^{t})^{}\) progressively align with the \(r^{}\) leading eigenvectors of the mapping \(^{2}+/m^{}(Z^{})\). Under the in-network RIP (Lemma 1), which requires the RIP of \(}\) and a sufficiently small \(\), we establish that this operator approximates the mapping of the power method applied to \(Z^{}\) (see the sketch of the proof in Sec. 3.1). The remaining two terms in (14) represent the duration of the subsequent _refinement_ phase. This phase steers the iterates away from certain degenerate saddle points while ensuring convergence towards the low-rank matrix \(Z^{}\).

In line with the findings for centralized GD , the test accuracy achieved at time \(\) might not persist for larger iterations. The corollary below refines these results by establishing a nonempty time interval within which the estimation error is guaranteed to stay within the desired accuracy.

**Corollary 1**.: _Under the conditions of Theorem 1, it holds_

\[(U^{t})^{}-Z^{}\|_{F}}{\|Z^{}\|}(r-r^{}) ^{7/8}(r^{})^{1/8}^{1/8}\|\|^{-21/16}(^{2})^{1/8},\] (16)

_for any \(t[,T]\), with_

\[T-}{r}}{(^{2} ^{2}^{1/8})}:=c_{3}(r^{})^{1/8 }(r-r^{})^{7/8}\|\|^{11/16},\] (17)

_where \(c_{3}>0\) is an universal constant._

The corollary ensures that the test error remains proportional to \(^{1/8}\) throughout the interval \(T-\). Notably, the duration of this interval increases as \(\) approaches zero. This result is quite desirable, especially in distributed settings where coordinating termination at a specific time may be challenging.

\(\)_On the condition (12) on \(\) and network scalability:_ The stipulation on \(\) signifies the need of a well-connected network--the larger the network size \(m\) or the condition number \(\) of the ground truth, the smaller \(\). This is a non-negotiable condition essential for managing consensus errors through the tolerance \(\), thus ensuring an adequate in-network RIP for the algorithm operator \(\). When coupled with the RIP of the measurement operator \(}\), it suffices for a sufficient alignment of the iterates \(U^{t}(U^{t})^{}\) with the signal subspace from the early stages of the algorithm. Our numerical experiments (see Sec. 4) indeed demonstrate that maintaining such a constraint on \(\) is indispensable for securing convergence and favorable estimation errors. When the network graph is predetermined (with given \(W\)), one can meet the condition (12) (if not a-priori satisfied) by employing at each agent's side multiple rounds of communications per gradient evaluation. This is a common practice  that in our case results in a communication overhead that is only logarithmic in \(m\) and \(\).

Notice that the generalization error (see (15) and (16)) is independent of \(\) or \(m\). This demonstrates that the algorithm's performance scales favorably with \(m\). As \(m\) increases, the generalization error remains unchanged, whereas the communication cost grows only modestly (logarithmically with \(m\)).

### Sketch of the proof of Theorem 1

This section provides some insights on the proof of the theorem, highlighting the challenges and the differences with existing centralized and decentralized techniques.

The goal is to establish that \(U^{t}(U^{t})^{} Z^{}\) as the algorithm progresses. Following , we decompose the iterates as

\[U^{t}=Q^{t}(Q^{t})^{}}_{}+ Q^{t,}(Q^{t,})^{}}_{},\] (18)

where \(Q^{t}^{r r^{}}\) contains the right singular vectors of \(V_{Z^{}}^{}U^{t}\), i.e., \(V_{Z^{}}^{}U^{t}=V^{t}^{t}(Q^{t})^{}\); and \(Q^{t,}^{(r r-r^{})}\) is the orthonormal complement of \(Q^{t}\). By construction, \((Q^{t})(Q^{t,})=^{r}\), allowing for the decomposition (18). Further, notice that the noise term is orthogonal to the signal space, i.e. \(V_{Z^{}}^{}U^{t}Q^{t,}=0\), which implies that once \(U^{t}\) is projected onto the signal space, the only relevant term left is \(V_{Z^{}}^{}U^{t}=V_{Z^{}}^{}U^{t}Q^{t}(Q^{t})^{}\), hence the name "signal".

Based on (18), and under the assumptions of the theorem, we establish that: **(i)**\(U^{t}Q^{t}(Q^{t})^{}\) is full rank and the signal-term grows as the algorithm progresses. **(ii)** The noise-term grows slower than the signal and remains sufficiently small. **(ii)** The error can be bounded by a polynomial proportional to the initialization size. Similar to , the analysis is organized in two phases.

**Phase I (power-like method):** The goal of this phase is to establish that after sufficiently long time \(t_{}\) since the initialization (\(t=0\)), \(_{}(U^{t_{}}Q^{t_{}})>c\|U^{t_{}}Q^{t_{},}\|\) and and that \(\|(V_{Z^{}}^{})^{}V_{U^{t_{}}Q^{t_{}}}\|\) is small, which means that the iterates are better aligned with the signal space than the noise space. Therefore, we are to identify in (9) a mechanism that allows \(V_{U^{t}Q^{t}}\) to become aligned with \(V_{Z^{}}\).

Given the initialization \(U^{0}= U\), at iteration \(t=1\), we have

\[U^{1}=(^{2}+^{}(Z^ {}))U^{0}+^{2}^{}(UU ^{})U^{0}.\] (19)

Consequently, if \(\) is sufficiently small, for the first few iterations \(t\), one can write

\[U^{t}(^{2}+^{}(Z^{}))^{t}U^{0}+(^{2}\|U^{0}\|\|U\|^{2}).\] (20)

Under the assumption that \(}\) fulfills the \(_{r^{}}\) RIP, we can establish using the in-network RIP that if \((}(r+1)}{m^{2}})\), \(^{}(Z^{})=Z^{}+\), with \(\|\|(_{2r^{}}\|Z^{}\|)\). Further, by construction \(^{2}Z^{}=Z^{}^{2}\), implying that \(^{2}\) and \(Z^{}\) share the same eigenspace. Also, \(_{i}(^{2})=1\), for all \(i=1,,d\). Consequently \(_{i}(^{2}+Z^{})=1+_{i}(Z^{})\) for \(i=1,,d\). Therefore, we can further approximate (20) as

\[U^{t}(^{2}+Z^{})^{t}U^{0}+ (^{2}\|U^{0}\|\|U\|^{2}),\] (21)

where we have disregarded \(\), for simplicity of exposition. Leveraging perturbation theory arguments, in the proof we demonstrate that \(\) can be properly controlled. Using the above arguments, we have

\[V_{Z^{}}V_{Z^{}}^{}U^{t} V_{Z^{}}(+ _{Z^{}})^{t}V_{Z^{}}^{}U^{0}+V_{Z^{ }}V_{Z^{}}^{}(^{2})^{t}V_{Z^{}}^{ }(V_{Z^{}}^{})^{}U^{0}+(^{2}\|U^{0 }\|\|U\|^{2})\] (22)

\[V_{Z^{}}^{}(V_{Z^{}}^{})^{}U^{t} V_{Z^{}}^{ }(^{2})^{t}(V_{Z^{}}^{})^{}U^{0}+(^{2 }\|U^{0}\|\|U\|^{2}),\] (23)

where the first term in the RHS of (22) corresponds to the power method on the matrix \(Z^{}\). Further, we see that the mentioned term grows faster than any other. Consequently, at the time \(t_{}\) at which we exit phase I, \(U^{t_{}}\) is sufficiently aligned with the signal space as compared to the noise space.

**Phase II (refinement):** In this phase we establish that, given \(_{}(U^{t_{}}Q^{t_{}}) c_{0}\|U^{t_{}}Q^{t_{}, }\|\) and \(\|(V_{Z^{}}^{})^{}V_{U^{t_{}}Q^{t_{}}}\| c_{1}:\)**(i)** the alignment \(_{}(V_{Z^{}}^{}U^{t_{}}Q^{t_{}})\) grows and stabilizes away from zero, **(ii)** the error \(\|U^{t}Q^{t,}\|\) grows slower than the alignment \(\|U^{t}Q^{t}\|\) and, **(iii)** the error \(\|V_{Z^{}}^{}V_{U^{t}Q^{t}}\|\) remains sufficiently small. A careful study and balance of these quantities yield the final convergence.

**Challenges with respect to the centralized case:** The main challenge with respect to analyses of the GD (e.g., ) comes from the distributed nature of the algorithm generating extra error terms (e.g., consensus errors), which significantly complicate the analysis. Our analysis builds on a newly introduced notion of RIP for the algorithm operator \(\). This is substantially different from the classical RIP or GD mapping , which lack of the network gossip matrix \(^{2}\).

**Challenges with respect to existing distributed optimization approaches:** The standard approach in the distributed optimization literature typically takes the route of splitting the algorithm dynamics in its average and consensus error. We deviated from such decomposition, because controlling such errors on \(U^{t}(U^{t})^{}\) would result in bounds of the type

\[\|U^{t}(U^{t})^{}-U^{t}(U^{t})^{}\| (\|U^{t}(U^{t})-Z^{}\|)\] (24)

which are insufficient to understand for example the dynamics of \(\|(V^{}_{Z^{}})^{}V_{U^{t}Q^{t}}\|\). Furthermore, the split into signal and noise subspaces allows us to invoke the in-network RIP with \(r^{}\), which would not be the case if splitting the iterates along consensus and non-consensus spaces.

Specifically regarding to phase I of the scheme, another mode classical approach would be "centering" the dynamics around the centralized trajectory of the power method, i.e.,

\[U^{1}=(+^{ }(Z^{}))U^{0}-^{2} ^{}(UU^{})U^{0}\] (25) \[+(^{2}-)U^{0}+( ^{}(Z^{})-^{} (Z^{}))U^{0},\] (26)

which, using the in-network RIP and the fact that \(^{2}\) and \(t\) are sufficiently small, would yield

\[U^{t}(+^{} (Z^{}))^{t}U^{0}+^{2}(\| U\|^{2}\|U^{0}\|)+(^{2}\|U^{0}\|).\] (27)

Here, the degree of freedom to control the error terms (second and third) are \(\) and \(\). This however would enforce the undesirable condition \(()\), which couples the network connectivity with the size of the initialization.

## 4 Numerical experiments

We discuss some preliminary experiments validating our theoretical findings. All simulations are performed on a Apple M2 Pro @ 3.5 GHz computer, using 32 GB RAM running macOS Ventura 13.3.1. We generate a random matrix \(^{d r^{}}\), with \(d=50\) and \(r^{}=2\), which we use through all experiments. The symmetric measurement matrices are generated as \(A_{i}=(1/2)(S_{i}+S_{i}^{})\), where \(S_{i}^{d d}\) have i.i.d. standard Gaussian elements. The communication networks are generated as Erdos-Renyi graphs, with link activation probability \(p=0.05\). and different sizes \(m\) (specified in each experiment below). For any generated graph, we set \(\) according to Metropolis weights , and then let \(W=^{K}\), with the integer \(K\) chosen to meet the condition (12) on \(\), resulting in \(K\) communication rounds per agent/iteration. Finally, we choose \(=1/4\) and \(=d^{-3}\).

**(i) Validating Theorem 1:** This experiment shows that under the conditions of Theorem 1, the test error behaves predictably as that of the centralized GD (up to constant factors). Furthermore, the invariance of such an error with the network size \(m\) is also confirmed, as long as \((1/m^{6})\) (as requested in (12)). In the experiments, the total sample size is \(N=1000\), split equally among agents \(m=\{5,100,500,1000\}\). **Fig** 1a plots the normalized test error; **Fig** 1b shows \(\|(V^{}_{M})^{}V_{U^{t}}\|\) which measures the missalignment of \(U^{t}\) with the power method matrix; and and **Fig** 1c displays the \(_{r^{}}(U^{t})/_{r^{}}(X)\) which combined with Fig 1b allows us to claim that the signal \(U^{t}Q^{t}\) is well aligned with \(V_{Z^{}}\) and full ranked. The curves show that the behavior of the decentralized algorithm is close to that of the centralized GD (blue lines). As predicted, the error decays quickly after the correct subspace has been identified. Furthermore, convergence rate and generalization error are almost invariant to network-size scaling, as long as \((m^{-6})\).

**(ii) Validating condition (12) on \(\):** We showcase the necessity of decreasing \(\) while increasing \(m\). Given a connected base graph with associated \(\), for the sequence of graphs generated with increasing \(m=\{10,50,100,500\}\), we let \(=^{T}\), with \(T\) such that \( 0.85\), for all \(m\). This eventually violates (12). **Fig.** 2 (resp. **Fig.** 2b) \(\) plots the normalized generalization error versus the iterations, for \(m=1\), \(m=10\) and \(m=50\) (resp. \(m=100\) and \(m=500\), where the two curves are only up to the iterations \(t=70\) and \(t=17\), respectively). The figures demonstrate the necessity of \(\) scaling down with \(m\) increasing. In fact, both the rate and achievable estimation errors degrade (and eventually break down) as the network size increases while keeping \(\) fixed. We claim that this stems from the fact that if the network is not sufficiently well-connected, the in-network RIP does not hold with sufficiently small tolerance, yielding to a failure of the power method early stage and consequently producing an unrecoverable missalignment with the signal subspace.