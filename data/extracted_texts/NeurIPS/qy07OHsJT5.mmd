# Diffusion Schrodinger Bridge Matching

Yuyang Shi

University of Oxford

&Valentin De Bortoli

ENS ULM

&Andrew Campbell

University of Oxford

&Arnaud Doucet

University of Oxford

Equal contribution.

###### Abstract

Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schrodinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting (IMF), a new methodology for solving SB problems, and Diffusion Schrodinger Bridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM significantly improves over previous SB numerics and recovers as special/limiting cases various recent transport methods. We demonstrate the performance of DSBM on a variety of problems.

## 1 Introduction

Mass transport problems are ubiquitous in machine learning (Peyre and Cuturi, 2019). For discrete measures, the Optimal Transport (OT) map can be computed exactly but is computationally intensive. In a landmark paper, Cuturi (2013) showed that an entropy-regularized version of OT can be computed more efficiently using the Sinkhorn algorithm (Sinkhorn, 1967). This has enabled the use of OT techniques in a variety of applications ranging from biology (Bunne et al., 2022) to shape correspondence (Feydy et al., 2017). However, applications involving high-dimensional continuous distributions and/or large datasets remain challenging for these techniques.

One of such data-rich applications is generative modeling, a central transport problem in machine learning which requires designing a deterministic or stochastic mapping transporting a reference "noise" distribution to the data distribution. For example, Generative Adversarial Networks (Goodfellow et al., 2014) define a static, deterministic transport map, while Denoising Diffusion Models (DDMs) (Song et al., 2021; Ho et al., 2020) build a dynamic, stochastic transport map by simulating a Stochastic Differential Equation (SDE), whose drift is learned using score matching (Hyvarinen, 2005; Vincent, 2011). The excellent performances of DDMs have motivated recent developments of Bridge Matching and Flow Matching models, which are dynamic transport maps using SDEs (Song et al., 2021; Peluchetti, 2021; Liu, 2022; Albergo et al., 2023) or ODEs (Albergo and Vanden-Eijnden, 2023; Heitz et al., 2023; Lipman et al., 2023; Liu et al., 2023). Compared to DDMs, Bridge and Flow Matching methods do not rely on a forward "noising" diffusion converging to the reference distribution in infinite time, and are also more generally applicable as they can approximate transport maps between two general distributions based on their samples. Nonetheless, these transport mapsare not necessarily close to the OT map minimizing the Wasserstein-2 metric, which is appealing for its many attractive properties (Peyre and Cuturi, 2019; Villani, 2009).

In contrast, the Schrodinger Bridge (SB) problem is a dynamic version of entropy-regularized OT (EOT) (Follmer, 1988; Leonard, 2014). The SB is the finite-time diffusion which admits as initial and terminal distributions the two distributions of interest and is the closest in Kullback-Leibler divergence to a reference diffusion. Numerous methods to approximate SBs numerically have been proposed, see e.g. (Bernton et al., 2019; Chen et al., 2016; Finlay et al., 2020; Caluya and Halder, 2021; Pavon et al., 2021), but these techniques tend to be restricted to low-dimensional settings. Recently, novel techniques using diffusion-based ideas have been proposed in (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022) based on Iterative Proportional Fitting (IPF) (Fortet, 1940; Kullback, 1968; Ruschendorf and Thomsen, 1993), a continuous state-space extension of the Sinkhorn algorithm (Essid and Pavon, 2019). These approaches have been shown to scale better empirically, but numerical errors tend to accumulate over iterations (Fernandes et al., 2021).

In this paper, our contributions are three-fold. First, we introduce Iterative Markovian Fitting (IMF), a new procedure to compute SBs which alternates between projecting on the space of _Markov processes_ and on the _reciprocal class_, i.e. the measures which have the same bridge as the reference measure of SB (Leonard et al., 2014). We establish various theoretical results for IMF. Contrary to IPF, the IMF iterates always preserve the initial and terminal distributions. The differences between IPF and IMF are presented in Table 1. Second, we propose Diffusion Schrodinger Bridge Matching (DSBM), a novel algorithm approximating numerically the SB solution derived from IMF. DSBM requires at each iteration solving a simple regression problem in the spirit of Bridge and Flow Matching, and does not suffer from the time-discretization and "forgetting" issues of previous DSB techniques (De Bortoli et al., 2021; Vargas et al., 2021; Chen et al., 2022). Finally, we demonstrate the performance of DSBM on a variety of transport tasks.2

Notations.We denote by \(()\), the space of _path measures_, i.e. \(()=(([0,T],^{d}))\) where \(T>0\). The subset of _Markov_ path measures associated with an SDE of the form \(_{t}=v_{t}(_{t})t+_{t}_{t}\), with \(,v\) locally Lipschitz, is denoted \(\). For any \(\), the _reciprocal class_ of \(\) is denoted \(()\), see Definition 3. We also denote \(_{t}\) its marginal distribution at time \(t\), \(_{s,t}\) the joint distribution at times \(s\) and \(t\), \(_{s|t}\) the conditional distribution at time \(s\) given state at time \(t\), and \(_{|0,T}()\) its _diffusion bridge_. Unless specified otherwise, all gradient operators \(\) are w.r.t. the variable \(x_{t}\) with time index \(t\). Let \((,)\) and \((,)\) be probability spaces. Given a Markov kernel \(:\,\) and a probability measure \(\) defined on \(\), we write \(\) the probability measure on \(\) such that for any \(\) we have \(()=_{}(x,) (x)\). In particular, for any joint distribution \(_{0,T}\) over \(^{d}^{d}\), we denote the _mixture of bridges_ measure as \(=_{0,T}_{|0,T}()\), which is short for \(()=_{^{d}^{d}}_{|0,T}(|x_ {0},x_{T})_{0,T}(x_{0},x_{T})\).

## 2 Dynamic Mass Transport Techniques

### Denoising Diffusion and Bridge Matching Models

Denoising Diffusion Models (Song et al., 2021; Ho et al., 2020) are a popular class of generative models. They define a forward noising process \(\) using the SDE \(_{t}=-_{t}t+ _{t}\) on the time-interval \([0,T]\), where \(_{0}^{d}\) is drawn from the data distribution \(_{0}\) and \((_{t})_{t[0,T]}\) is a \(d\)

  & Sets for alternating projections & Preserved properties \\   IPF & \(_{0}=_{0}\); \(_{T}=_{T}\) & \(\), \(()\) \\  IMF & \(\); \(()\) & \(_{0}=_{0}\), \(_{T}=_{T}\) \\  

Table 1: Comparison between Iterative Markovian Fitting (IMF) and Iterative Proportional Fitting (IPF). The Schr√∂dinger Bridge is the _unique_\(\) s.t. \(_{0}=_{0}\), \(_{T}=_{T}\), \(\), \(()\) simultaneously by Proposition 5. \(\) is the space of (regular) Markov measures and \(()\) the space of reciprocal measures of \(\).

Figure 1: Relationship between DSBM and existing methods.

dimensional Brownian motion. This diffusion3 converges towards the standard Gaussian distribution \((0,)\) as \(T\). A generative model is given by its _time-reversal_\((_{t})_{t[0,T]}=(_{T-t})_{t[0,T]}\), where \(_{0}_{T}\) and \(_{t}=\{_{t}+_{T- t}(_{t})\}t+_{t}\)(Anderson, 1982; Haussmann and Pardoux, 1986). In practice, \((_{t})_{t[0,T]}\) is initialized with \(_{0}_{T}=(0,)\), and the _Stein score_\(_{t}(x_{t})=_{_{0|t}}[ _{t|0}(_{t}|_{0})_{t}=x_{t}]\) is approximated using a neural network \(s_{}(t,x_{t})\) minimizing the _denoising score matching_ loss \(_{_{0,t}}[\|_{t|0}(_{t}| _{0})-s_{}(t,_{t})\|^{2}]\).

An alternative to considering the time-reversal of a forward noising process is to "build bridges" between the two distributions and learn a _mimicking_ diffusion process. This approach generalizes DDMs and allows for more flexible choices of sampling processes. We call this framework _Bridge Matching_ and adopt a presentation similar to Peluchetti (2021); Liu et al. (2022b), where \(_{T}\) is the data distribution.4 We denote \(\) the path measure associated with the following process

\[_{t}=f_{t}(_{t})t+_{t}_{t},_{0}_{0}.\] (1)

Consider now the distribution of this process pinned down at an initial and terminal point \(x_{0},x_{T}\), denoted \(_{|0,T}(|x_{0},x_{T})\). Under mild assumptions, the _pinned_ process \(_{|0,T}(|x_{0},x_{T})\) is a _diffusion bridge_ and is given by

\[_{t}^{0,T}=\{f_{t}(_{t}^{0,T})+_{t}^{2} _{T|t}(x_{T}|_{t}^{0,T})\}t+_{t }_{t},_{0}^{0,T}=x_{0},\] (2)

which satisfies \(_{T}^{0,T}=x_{T}\) using Doob \(h\)-transform theory (Rogers and Williams, 2000). Next, we define an independent coupling \(_{0,T}=_{0}_{T}\), and let \(=_{0,T}_{|0,T}\). This path measure \(\) is a _mixture of bridges_. We aim to find a Markov diffusion \(_{t}=\{f_{t}(_{t})+v_{t}(_{t})\} t+_{t}_{t}\) on \([0,T]\) which admits the same marginals as \(\); i.e. for any \(t[0,T]\), \(_{t}_{t}\), so \(_{T}_{T}\). For such \(v_{t}\), a generative model for sampling data distribution \(_{T}\) is obtained by simulating \((_{t})_{t[0,T]}\). It can be verified that indeed \(_{t}_{t}\) for \(v_{t}^{}(x_{t})=_{t}^{2}_{_{T|t}}[ _{T|t}(_{T}|_{t})_{t}=x_{t}]\). We present the theory behind this idea more formally using Markovian projections in Section 3.1. In practice, we do not have access to \(v_{t}^{}\) and it is learned using neural networks with regression loss

\[_{_{t,T}}[\|_{t}^{2}_{T|t}(_ {T}|_{t})-v_{}(t,_{t})\|^{2}].\] (3)

For \(f_{t}=0\) and \(_{t}=\), \(_{|0,T}\) is a _Brownian Bridge_ and we have

\[_{t}^{0,T}=x_{T}+(1-)x_{0}+_{t}( _{t}-_{T}),_{t}^{0,T}= \{(x_{T}-_{t}^{0,T})/(T-t)\}t+_{t}_{t},\] (4)

with \((_{t}-_{T})(0,t(1-) \,)\). The regression loss (3) associated with (4) is given by

\[_{_{t,T}}[\|(_{T}-_{t})/(T-t)-v_{}(t, _{t})\|^{2}].\] (5)

Letting \( 0\), we recover Flow Matching models (see Appendix A.1 for further details).

### Schrodinger Bridges and Optimal Transport

The Schrodinger Bridge (SB) problem (Schrodinger, 1932) consists in finding a path measure \(^{}()\) such that

\[^{}=*{argmin}_{}\{( |)\ :\ _{0}=_{0},\ _{T}=_{T}\},\] (6)

where \(()\) is a reference path measure. In what follows, we consider \(\) defined by the diffusion process (1) which is Markov, and without loss of generality, we assume \(_{0}=_{0}\). Hence \(^{}\) is the path measure closest to \(\) in terms of Kullback-Leibler divergence which satisfies the initial and terminal constraints \(^{}_{0}=_{0}\) and \(^{}_{T}=_{T}\).

Another crucial property of \(^{}\) is that it can also be defined as a mixture of bridges \(^{}=^{}_{0,T}_{|0,T}\), where \(^{}_{0,T}=*{argmin}_{_{0,T}}\{(_{0,T }|_{0,T})\ :\ _{0}=_{0},\ _{T}=_{T}\}\) is the solution of the _static_ SB problem (Leonard, 2014). In particular, for \(\) associated with \((_{t})_{t[0,T]}\) we have

\[^{}_{0,T}=*{argmin}_{_{0,T}}\{_{_{0,T}}[ ||_{0}-_{T}||^{2}-2^{2}T\,(_{0,T})\ :\ _{0}=_{0},\ _{T}=_{T}\},\]where \(()\) denotes the entropy, i.e. \(^{}_{0,T}\) is the solution of the entropy-regularized OT problem. In this case, the SB can also be obtained theoretically by solving the following problem (Dai Pra, 1991)

\[v_{}=_{v}\{_{0}^{T}_{ _{t}}[||v(t,_{t})||^{2}]t\ :\ _{t}=v(t,_{t})t+ _{t},\ \ _{0}=_{0},\ _{T}=_{T}\}.\]

Then \(^{}\) is given by the SDE with drift \(v_{}\) initialized with \(_{0}_{0}\). For \(=0\), we recover the classical OT problem and the Benamou-Brenier formula (Benamou and Brenier, 2000).

A common approach to solve (6) is the Iterative Proportional Fitting (IPF) method (Fortet, 1940; Kullback, 1968; Ruschendorf, 1995) defining a sequence of path measures \((}^{n})_{n}\) where

\[}^{2n+1}=_{}}\{(}|}^{2n})\ :\ }_{T}=_{T}\},\ }^{2n+2}= _{}}\{(}| }^{2n+1})\ :\ }_{0}=_{0}\},\] (7)

with initialization \(}^{0}=\). This procedure alternates between projections on the set of path measures with given initial distribution \(_{0}\) and terminal distribution \(_{T}\). It can be shown (De Bortoli et al., 2021) that \((}^{n})_{n}\) are associated with diffusions and that for any \(n\), \(}^{2n+1}\) is the time-reversal of \(}^{2n}\) with initialization \(_{T}\), and \(}^{2n+2}\) is the time-reversal of \(}^{2n+1}\) with initialization \(_{0}\). Leveraging this property, De Bortoli et al. (2021) proposed Diffusion Schrodinger Bridge (DSB), an algorithm which learns the time-reversals iteratively. In particular, DDMs can be seen as the first iteration of DSB.

## 3 Iterative Markovian Fitting

### Markovian Projection and Reciprocal Projection

Markovian Projection.Projecting on Markov measures is a key ingredient in our methodology and in the Bridge Matching framework. This concept was introduced multiple times in the literature (Gyongy, 1986; Peluchetti, 2021; Liu et al., 2022b). In particular, we focus on Markovian projection of path measures given by a mixture of bridges \(=_{0,T}_{|0,T}()\).

**Definition 1**.: _Assume that \(\) is given by (1) and that for any \((x_{0},x_{T})^{d}\), \(_{|0,T}(|x_{0},x_{T})\) is associated with \((_{t}^{0,T})_{t[0,T]}\) given by \(_{t}^{0,T}=\{f_{t}(_{t}^{0,T})+_{t}^{2} _{T|t}(x_{T}|_{t}^{0,T})\}t+_{t }_{t}\), with \(:\ [0,T](0,+)\). Then, when it is well-defined, we introduce the Markovian projection of \(\), \(^{}=_{}()\), which is associated with the SDE_

\[_{t}^{}=\{f_{t}(_{t}^{})+v_{t}^{ }(_{t}^{})\}t+_{t}_{t},  v_{t}^{}(x_{t})=_{t}^{2}_{_{T|t}}[ _{T|t}(_{T}|_{t})_{t}=x_{t}].\]

Note that in our definition \(_{t}>0\) so \(_{T|t}(x_{T}|x_{t})\) is well-defined, but Flow Matching can be recovered as the _deterministic_ case in the limit \(_{t}= 0\). In the following proposition, we show that the Markovian projection is indeed a projection for the _reverse_ Kullback-Leibler divergence, and that it preserves marginals of \(_{t}\).

**Proposition 2**.: _Assume that \(_{t}>0\). Let \(^{}=_{}()\). Then, under mild assumptions, we have_

\[^{}=_{}\{( |)\ :\ \},\] \[(|^{})=_{0}^{T} _{_{0,t}}[\|_{t}^{2}_{_{T|0,t}}[ _{T|t}(_{T}|_{t})_{0},_ {t}]-v_{t}^{}(_{t})\|^{2}]/_{t}^{2}t.\]

_In addition, we have that for any \(t[0,T]\), \(_{t}^{}=_{t}\). In particular, \(_{T}^{}=_{T}\)._

Reciprocal Projection.While the Markovian projection ensures that the obtained measure is Markov, the associated _bridge_ measure is not preserved in general, i.e. \(_{}()_{|0,T}_{|0,T}=_{|0,T}\). Measures with same bridge as \(\) are said to be in its _reciprocal class_(Leonard et al., 2014).

**Definition 3**.: \(()\) _is in the reciprocal class \(()\) of \(\) if \(=_{0,T}_{|0,T}\). We define the reciprocal projection of \(()\) as \(^{}=_{()}()=_ {0,T}_{|0,T}\)._

Similarly to Proposition 2, we have the following result, which justifies the term reciprocal projection.

**Proposition 4**.: _Let \(()\), \(^{}=_{()}()\). Then, \(^{}=_{}\{(|)\ :\ ()\}\)._

The reciprocal projection \(^{}\) of a Markov path measure \(\) does not preserve the Markov property in general. In fact, the Schrodinger Bridge is the _unique_ path measure which satisfies the initial and terminal conditions, is Markov and is in the reciprocal class of \(\), see (Leonard, 2014).

**Proposition 5**.: _Let \(\) be a Markov measure in the reciprocal class of \(\) such that \(_{0}=_{0}\), \(_{T}=_{T}\). Then, under assumptions on \(\), \(_{0}\) and \(_{T}\), \(\) is unique and is equal to the Schrodinger Bridge \(^{}\)._

### Iterative Markovian Fitting

Based on Proposition 5, we propose a novel methodology called _Iterative Markovian Fitting_ (IMF) to solve Schrodinger Bridges. We consider a sequence \((^{n})_{n}\) such that

\[^{2n+1}=_{}(^{2n}),^ {2n+2}=_{()}(^{2n+1}),\] (8)

with \(^{0}\) such that \(^{0}_{0}=_{0}\), \(^{0}_{T}=_{T}\) and \(^{0}()\). These updates correspond to alternatively performing Markovian projections and reciprocal projections.

Combining Proposition 2 and Definition 3, we get that for any \(n\), \(^{n}_{0}=_{0}\) and \(^{n}_{T}=_{T}\). This property is in contrast to the IPF algorithm (7) for which the marginals at the initial and final times are _not_ preserved. We highlight this duality between IPF (7) and IMF (8) in Table 1.

We conclude this section with a theoretical analysis of IMF. First, we start by showing a Pythagorean theorem for both the Markovian projection and the reciprocal projection.

**Lemma 6**.: _Under mild assumptions, if \(\), \(()\) and \((|)<+\), we have_

\[(|)=(|_{}()) +(_{}()|).\]

_If \((|)<+\), we have_

\[(|)=(|_{ ()}())+(_{() }()|).\]

Using Lemma 6, we have the following proposition.

**Proposition 7**.: _Under mild assumptions, we have \((^{n+1}|^{})( ^{n}|^{})<\), and \(_{n+}(^{n}|^{n+1})=0\)._

Hence, for the IMF sequence \((^{n})_{n}\), the Markov path measures \((^{2n+1})_{n}\) are getting closer to the reciprocal class, while the reciprocal path measures \((^{2n+2})_{n}\) are getting closer to the set of Markov measures. Proposition 7 should be compared with (Ruschendorf, 1995, Proposition 2.1, Equation (2.16)) which shows that, for the IPF sequence \((^{n})_{n}\), we have \(_{n+}(^{n+1}|^{n})=0\). This result is similar to Proposition 7 but for the _forward_ Kullback-Leibler divergence.

Using Proposition 7, we finally prove the convergence of the IMF sequence \((^{n})_{n}\) to the Schrodinger Bridge. This result was first shown in the concurrent work (Peluchetti, 2023, Theorem 2). We present a simpler proof in Appendix C.6.

**Theorem 8**.: _Under mild assumptions, the IMF sequence \((^{n})_{n}\) admits a unique fixed point \(^{}=^{}\), and \(_{n+}(^{n}|^{})=0\)._

## 4 Diffusion Schrodinger Bridge Matching

In this section, we present Diffusion Schrodinger Bridge Matching (DSBM), a practical algorithm for solving the SB problem obtained by combining the IMF procedure with Bridge Matching.

Iterative Markovian Fitting in practice.IMF alternatively projects on the Markov class \(\) and the reciprocal class \(()\). We denote \(^{n+1}=^{2n+1}\) and \(^{n}=^{2n}()\). Assuming we know how to sample from the bridge \(_{0,T}\) given the initial and terminal conditions, sampling from the reciprocal projection \(_{()}()\) is simple: First, sample \((_{0},_{T})\) from the joint distribution \(_{0,T}\).5 Then, sample from the bridge \(_{0,T}(|_{0},_{T})\). The bottleneck of IMF is in the computation of Markovian projections. By Definition 1, \(^{}=_{}()\) is associated with the process

\[_{t}=\{f_{t}(_{t})+v_{^{}}(t,_{t})\}t+_{t}_{t},_{0} _{0},\] (9) \[^{}=_{}\{_{0}^{T}_{ _{t,T}}[\|_{t}^{2}_{T|t}(_{T}|_ {t})-v_{}(t,_{t})\|^{2}]/_{t}^{2}t\ :\ \},\] (10)where \(\{v_{}\ :\ \}\) is a parametric family of functions, usually given by a neural network. The optimal \(v_{^{*}}(t,x_{t})=_{t}^{2}_{_{T|t}}[_{T|t}(_{T}|_{t})_{t}=x_{t}]\) for any \(t[0,T]\) and \(x_{t}^{d}\).

With the above two procedures for computing \(_{()}()\) and \(_{}()\), we can now describe a numerical method implementing IMF (8). Let \(^{0}=^{0}_{0,T}_{|0,T}\) where \(^{0}_{0}=_{0}\), \(^{0}_{T}=_{T}\). Learn \(^{1}_{}(^{0})\) given by (9) with \(v_{^{*}}\) given by (10). Next, sample from \(^{1}=_{()}(^{1})=^{1 }_{0,T}_{|0,T}\) by sampling from \(^{1}_{0,T}\) and reconstructing the bridge \(_{|0,T}\). We iterate the process to obtain a sequence \((^{n},^{n+1})_{n}\). In practice, this algorithm performs poorly (see Figure 3), since the approximate minimization (10) for computing \(^{n+1}\) may not admit \(^{n+1}_{T}=_{T}\) exactly as in Proposition 2. Instead, we incur a bias between \(^{n+1}_{T}\) and \(_{T}\) which accumulates for each \(n\).

To mitigate this problem, we alternate between a _forward_ Markovian projection and a _backward_ Markovian projection. This procedure is justified by the following proposition.

**Proposition 9**.: _Assume that \(=_{0,T}_{|0,T}\) with \(\) associated with \(_{t}=f_{t}(_{t})t+_{t}_{t}\). Under mild conditions, the Markovian projection \(^{*}=_{}()\) is associated with both_

\[_{t} =\{f_{t}(_{t})+_{t}^{2}_{_{T|t}}[ _{T|t}(_{T}|_{t})_{t}] \}t+_{t}_{t},_{0}_{0},\] (11) \[_{t} =\{-f_{T-t}(_{t})+_{T-t}^{2}_{_{0|T- t}}[_{T-t|0}(_{t}|_{T})_{t}] \}t+_{T-t}_{t},_{0}_{T}.\] (12)

In Proposition 9, (11) is the definition of the Markovian projection, see Definition 1. However, (12) is an equivalent representation as a _time-reversal_. In practice, \((_{t})_{t[0,T]}\) is approximated with

\[_{t}=\{-f_{T-t}(_{t})+v_{^{*}}( T-t,_{t})\}t+_{T-t}_{t}, _{0}_{T},\] (13) \[^{*}=_{}\{_{0}^{T}_{_{0,t }}[\|_{t}^{2}_{t|0}(_{t}|_{0})-v _{}(t,_{t})\|^{2}]/_{t}^{2}t\ :\ \}.\] (14)

The optimal \(v_{^{*}}(t,x_{t})=_{t}^{2}_{_{0|t}}[_{t|0}(_{t}|_{0})_{t}=x_{t}]\) for any \(t[0,T]\) and \(x_{t}^{d}\).

```
1:Input: Joint distribution \(^{0}_{0,T}\), tractable bridge \(_{|0,T}\), number of outer iterations \(N\).
2: Let \(^{0}=^{0}_{0,T}_{|0,T}\).
3:for\(n\{0,,N-1\}\)do
4: Learn \(v_{^{*}}\) using (14) with \(=^{2n}\).
5: Let \(^{2n+1}\) be given by (13).
6: Let \(^{2n+1}=^{2n+1}_{0,T}_{|0,T}\).
7: Learn \(v_{}\). using (10) with \(=^{2n+1}\).
8: Let \(^{2n+2}\) be given by (9).
9: Let \(^{2n+2}=^{2n+2}_{0,T}_{|0,T}\).
10:endfor
11:Output:\(v_{^{*}}\), \(v_{^{*}}\). ```

**Algorithm 1** Diffusion Schrodinger Bridge Matching

Note that \(_{0}_{0}\) in the forward projection, while \(_{0}_{T}\) in the backward projection. Therefore, using the backward projection removes the bias on \(_{T}\) accumulated from the forward projection. Leveraging the time-symmetry of the Markovian projection and alternating between (13) and (9) yields the DSBM methodology summarized in Algorithm 1.

It is also possible to learn _both_ the forward and backward processes at each step, and enforce that the backward and forward processes match. We explore this in Appendix G.

Initialization coupling.We now relate Algorithm 1 to the classical IPF and practical algorithms such as DSB (De Bortoli et al., 2021). Instead of initializing DSBM with \(^{0}_{0,T}\) given by a coupling between \(_{0},_{T}\), if we initialize it by \(^{0}_{0,T}=_{0,T}\) where \(_{0}=_{0}\) and \(_{T|0}\) is given by the reference process defined in (1), then DSBM also recovers the IPF iterates used in DSB.

**Proposition 10**.: _Suppose the families of functions \(\{v_{}\ :\ \}\) and \(\{v_{}\ :\ \}\) are rich enough so that they can model the optimal vector fields. Let \((^{n},^{n+1})_{n}\) be the optimal DSBM sequence in Algorithm 1 initialized with \(^{0}_{0,T}=_{0,T}\), and let \((}^{n})_{n}\) be the optimal DSB sequence given by the IPF iterates in (7). Then for any \(n,n 1\), we have \(^{n}=}^{n}\)._

We will thus call DSBM-IPF, the DSBM algorithm initialized with the joint distribution given by the forward reference process \(^{0}_{0,T}=_{0,T}\); and DSBM-IMF, the DSBM algorithm initialized with an independent coupling \(^{0}_{0,T}=_{0}_{T}\). However, the training procedure of DSBM-IPF is very different from the one of (De Bortoli et al., 2021; Chen et al., 2022). In existing works, \(}^{n+1}\) is obtained as the time-reversal of \(^{n}\) which requires full trajectories from \(^{n}\), see e.g. (De Bortoli et al., 2021, Proposition 6). In contrast, in Algorithm 1 we only use the _coupling_\(^{n}_{0,T}\) to create the bridge measure \(^{n}=^{n}_{0,T}_{|0,T}\). By doing so, (i) the losses (10) and (14) can be easily evaluated at any time \(t[0,T]\); (ii) the _trajectory caching_ procedure in DSBM is more computationally and memory efficient; (iii) while every IPF iteration \(}^{n}\) is also supposed to be in \(()\), in practice one can observe a _forgetting_ of the bridge \(_{|0,T}\)(Fernandes et al., 2021). In DSBM, this effect is countered by explicit projections on the reciprocal class. See Appendix F for more details.

Probability flow ODE.At equilibrium of DSBM, we have that \((_{t})_{t[0,T]}\) given by (13) is the time reversal of \((_{t})_{t[0,T]}\) given by (9) and are both associated with the optimal Schrodinger Bridge path measure \(^{}\). As a result, we have that \(v_{^{}}(t,x)=-v_{^{}}(t,x)+_{t}^{2} _{t}^{}(x)\). Hence, a probability flow \((_{t}^{})_{t[0,T]}\) such that \((_{t}^{})=_{t}^{}\) for any \(t[0,T]\) is given by

\[_{t}^{}=\{f_{t}(_{t}^{})+[ v_{^{}}(t,_{t}^{})-v_{^{}}(t,_{t}^{ })]\}t,_{0}^{}_{0}.\]

See also De Bortoli et al. (2021); Chen et al. (2022) for derivation of this result. Note however that the path measure induced by \((_{t}^{})_{t[0,T]}\) does not correspond to \(^{}\); in particular, \((_{0}^{},_{T}^{})\) is _not_ an entropic OT plan. However, since for any \(t[0,T]\), \(_{t}^{}\) has marginal distribution \(_{t}^{}\), we can compute the log-likelihood of the model (Song et al., 2021; Huang et al., 2021).

## 5 Related Work

Markovian projection and Bridge Matching.The concept of Markovian projection has been rediscovered multiple times (Krylov, 1984; Gyongy, 1986; Dupire, 1994). In the machine learning context, this was first proposed by Peluchetti (2021) to define Bridge Matching models. More recently, Liu et al. (2022b) derived theoretical properties of the Markovian projection in Proposition 2, first part of Lemma 6, and applied Bridge Matching for learning data on discrete and constrained domains.

Bridge and Flow Matching.Flow Matching corresponds to deterministic bridges with deterministic samplers (ODEs) and has been under active study (Liu et al., 2023b; Liu, 2022; Lippan et al., 2023; Albergo and Vanden-Eijnden, 2023; Heitz et al., 2023; Pooladian et al., 2023; Tong et al., 2023). Denoising Diffusion Implicit Models (DDIM) (Song et al., 2021) can also be formulated as a discrete-time version of Flow Matching, see Liu et al. (2023b). These models have been extended to the Riemannian setting by Chen and Lipman (2023). Recently, Albergo et al. (2023) studied the influence of stochasticity in the bridge, through the concept of stochastic interpolants. Liu et al. (2023a); Delbracio and Milanfar (2023) used Bridge Matching to perform image restoration tasks and noted benefits of stochasticity empirically. Closely related to our work is the Rectified Flow algorithm of Liu et al. (2023b), which corresponds to an iterative Flow Matching procedure in order to improve the straightness of the flow and thus eases its simulation. An iterative rectifying procedure using stochastic interpolants is also proposed in (Albergo et al., 2023, Section 3.5). Our proposed DSBM-IMF algorithm is closest to Rectified Flow, which can be seen as the deterministic limiting case of DSBM-IMF as \( 0\). However, there are a few important theoretical and practical differences. Most notably, we adopt the SDE approach which is crucial for the validity of Proposition 5 as well as for the empirical performance of DSBM. We discuss further distinctions between DSBM and Rectified Flow in Appendix A.3.

Diffusion Schrodinger Bridge.Schrodinger Bridges (Schrodinger, 1932) are ubiquitous in probability theory (Leonard, 2014b) and stochastic control (Dai Pra, 1991; Chen et al., 2021). More recently, they have been used for generative modeling: De Bortoli et al. (2021) introduced the DSB algorithm and Vargas et al. (2021); Chen et al. (2022) introduced similar algorithms. The case of Dirac delta terminal distribution was investigated by Wang et al. (2021). These methods were later extended to solve conditional simulation and more general control problems (Shi et al., 2022; Thornton et al., 2022; Liu et al., 2022a; Chen et al., 2023; Tamir et al., 2023). In Somnath et al. (2023), SBs are learned using one Bridge Matching iteration, assuming access to the true Schrodinger static coupling. Our proposed method DSBM-IPF is closest to DSB, but with improved continous-time training and projections on the reciprocal class which mitigate two limitations of DSB. Concurrently with our work, Peluchetti (2023) independently introduced the DSBM-IMF approach (named IDBM therein).

[MISSING_PAGE_FAIL:8]

the variance estimates become inaccurate for RF and IMF-b. Among SB methods, DSB and IMF-b also gave inaccurate SB covariance estimates as the number of iteration increases. On the other hand, DSBM does not suffer from this issue. In Table 3, we further quantify the accuracy and compare with SB-CFM (Tong et al., 2023) by computing the KL divergence between the marginal distributions of the learned process \(_{t}\) and the true SB \(_{t}^{}\). Our proposed methods achieve similar KL divergence as SB-CFM in dimension \(d=5\), but are much more accurate in higher dimensions.

MNIST, EMNIST transfer.We test our method for domain transfer between MNIST digits and EMNIST letters as in De Bortoli et al. (2021). We compare DSBM as a direct substitute of DSB, and also with Bridge Matching (BM) (Peluchetti, 2021; Liu et al., 2022b), CFM, OT-CFM and RF. We plot some output samples from different algorithms in Figure 4 and the convergence of FID score in Figure 5. We find that OT-CFM becomes less applicable in higher dimensions and produces samples of worse quality (Figure 3(a)). On the other hand, image quality deteriorates during training of DSB and RF. DSBM achieves higher quality samples visually, and does not suffer from deterioration. It is also about \(30\%\) more efficient than DSB in terms of runtime.

CelebA transfer.Next, we evaluate and perform some ablations of our method on a transfer task on the CelebA \(64 64\) dataset. We consider the images given by the tokens male/old and female/young. In Figures 6 and 7, we show that as \(\) increases, the quality of the images (as measured by the FID score) increases until \(\) is too high, but the alignment (as measured by LPIPS) between the generated image and the original sample decreases. Additionally, we investigate the dependency between \(\) and image dimension in Figure 8. In particular, for the same \(=1\), the outputs of DSBM for CelebA \(128 128\) are better aligned with the original data than for CelebA \(64 64\). This is in agreement with the observations of Chen (2023); Hoogeboom et al. (2023) that the _noise schedule_ in diffusion models should scale with the resolution.

AFHQ transfer.We demonstrate the scalability of our method on an additional transfer experiment on the AFHQ \(512 512\) dataset between the classes cat and wild. The results are shown in Figure 9. On this higher-dimensional problem, we observe that DSBM can also generate realistic samples which are similar to the input.

Unpaired Fluid Flows Downscaling.Finally, we apply DSBM to perform downscaling of geophysical fluid dynamics, i.e. super-resolution of low resolution spatial data. We use the dataset in (Bischoff and Deck, 2023), which consists of unpaired low (\(64 64\)) and high (\(512 512\)) resolution fields. As shown in Figure 10, DSBM is able to learn high resolution reconstructions by only slightly noising the low resolution input. In contrast, Bischoff and Deck (2023) use two diffusion models in forward and backward directions (Diffusion-fb) based on Meng et al. (2022), which improves over the Random baseline. Figure 11 shows that DSBM-IPF and DSBM-IMF achieve much lower \(_{2}\)

Figure 4: Samples of MNIST digits transferred from letters. Figure 5: FID vs iteration.

distances for all frequency classes in the dataset than Diffusion-fb (and thus Random), indicating DSBM is able to reconstruct high resolution fields consistent with the low resolution source.

## 7 Discussion

In this work, we introduce IMF, a new methodology for learning Schrodinger Bridges. IMF is an alternative to the classical IPF and can be interpreted as its dual. Building on this new framework, we present two practical algorithms, DSBM-IPF and DSBM-IMF, for learning SBs. These algorithms mitigate the time-discretization and bias accumulation issues of existing methods. However, DSBM still has some limitations. First, our results suggest DSBM is most effective for solving general transport problems. For generative modeling, we only find minor improvements compared to Bridge and Flow Matching on CIFAR-10 (see Appendix I.6). Second, while DSBM is more efficient than DSB, it still requires sampling from the learned process during the caching step. Finally, the EOT problem becomes more difficult to solve numerically for small values of \(\).

In future work, we would like to further investigate the differences between DSBM-IMF and DSBM-IPF, IMF also appears useful for developing a better understanding of the Rectified Flow algorithm (Liu et al., 2023b), as IMF minimizes a clear objective (6) and Rectified Flow can be seen as a limiting case of it. Finally, Rectified Flow has also been extended to solve OT problems with general convex costs by Liu (2022), and it would be interesting to derive a SB version of this extension.