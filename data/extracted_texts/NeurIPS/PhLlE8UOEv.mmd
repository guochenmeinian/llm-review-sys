# Provable Posterior Sampling with Denoising Oracles

via Tilted Transport

 Joan Bruna

New York University & Flatiron Institute

bruna@cims.nyu.edu &Jiequn Han

Flatiron Institute

jhan@simonsfoundation.org

###### Abstract

Score-based diffusion models have significantly advanced high-dimensional data generation across various domains, by learning a denoising oracle (or score) from datasets. From a Bayesian perspective, they offer a realistic modeling of data priors and facilitate solving inverse problems through posterior sampling. Although many heuristic methods have been developed recently for this purpose, they lack the quantitative guarantees needed in many scientific applications. This work addresses the topic from two perspectives. We first present a hardness result indicating that a generic method leveraging the prior denoising oracle for posterior sampling becomes infeasible as soon as the measurement operator is mildly ill-conditioned. We next develop the _tilted transport_ technique, which leverages the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to exactly transform the original posterior sampling problem into a new one that is provably easier to sample from. We quantify the conditions under which the boosted posterior is strongly log-concave, highlighting how task difficulty depends on the condition number of the measurement matrix and the signal-to-noise ratio. The resulting general scheme is shown to match the best-known sampling methods for Ising models, and is further validated on high-dimensional Gaussian mixture models.

## 1 Introduction

Inverse problems consist in reconstructing a signal of interest from noisy measurements. As such, they are a central object of study across many scientific domains, including signal processing, imaging, astrophysics or computational biology. In the common settings where the measurement information is limited, a reliable solution for these problems usually depends on prior knowledge of the data. One popular approach is to choose a regularizer that utilizes data properties such as smoothness or sparseness, and then solve a regularized optimization problem to obtain _a point estimate_ of the original data. However, this approach often struggles with selecting an appropriate regularizer and might be unstable in the presence of large measurement noise. A more robust approach takes a statistical formulation and seeks to sample the _posterior distribution_ of data based on Bayes's theorem, which allows for uncertainty quantification in the reconstructed data by leveraging a model for the prior data distribution.

While accurate models for high-dimensional distributions are notoriously complex to estimate, the resurgence of deep neural networks has provided unprecedented capabilities for modeling complex data distributions in certain high-dimensional regimes. Specifically, score-based diffusion models [55; 33; 58] have achieved remarkable empirical success in generating high-dimensional data across various domains, including images, video, text, and audio. These models implicitly parameterize data distributions through an iterative denoising process that builds up data from noise. Furthermore, there is a growing literature developing theoretical foundations of score-based diffusion models[17; 7; 45; 16; 19], giving a comprehensive error analysis including score estimation, initialization error and time-discretization error. By generating high-fidelity data, these models can also serve as data prior for posterior sampling in inverse problems in high dimensions. Following this idea, many studies (see, e.g., [40; 22]) have leveraged diffusion models for posterior sampling. However, as discussed below, various categories of approaches for posterior sampling introduce different uncontrollable errors, such as those arising from the approximation of the conditional score or the use of a limited variational family. This abundance of heuristics contrasts with the principled sampling used in prior data generation, and is often at odds with the statistical guarantees needed in many scientific applications.

In this work, we aim to bridge the gap between principled diffusion-based algorithms for both prior and posterior distributions. Focusing on the canonical setting of linear inverse problems, where measurements are of the form \(y=Ax+w\), with \(x\) the signal to be estimated and \(w\) an independent noise, we first illustrate a negative result, revealing that no method can efficiently sample the posterior in general cases, even with the prior denoising oracle. Subsequently, we develop the _tilted transport_ technique, which utilizes the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to exactly transform the original posterior sampling problem into a new one that is easier to sample. Figure 1 illustrates a schematic plot of the method using two-dimensional Gaussian mixture examples, showing that while the original target posterior problem remains multimodal, the boosted posterior resembles a unimodal distribution.

We establish a precise condition where the density of the transformed posterior problem becomes strongly log-concave, making it suitable for efficient sampling via Langevin dynamics. This condition showcases the interplay between a geometric property of the prior (what we call _tilted spread_; see Section 5) and the conditioning and noise level of the measurements. Interestingly, the condition can be satisfied when the signal-to-noise ratio (SNR) is either moderately low or moderately high, in contrast with traditional sampling methods, which typically excel only within a specific regime.

As a first application, we show that tilted transport can sample from Ising models of the form \((x) e^{-x^{}Qx}\), where \(x\{ 1\}^{d}\) is supported in the hypercube, up to the critical threshold determined by the gap \(_{}(Q)-_{}(Q)=1\), thus matching the performance of Glauber dynamics [24; 1] as well as the computational threshold predicted by the low-degree method . More generally, even when the boosted posterior is not strongly log-concave, it remains easier to sample than the original one. Thus, tilted transport can be combined with any existing black-box posterior sampling methods to enhance their performance. This technique operates without any additional computational cost and functions in a plug-and-play fashion, allowing for straightforward integration into various frameworks. When working with high-dimensional Gaussian mixtures, where an analytical solution to the posterior is available, we numerically validate our theory and demonstrate enhanced posterior sampling performance.

### Related Work

Numerous studies in recent years have explored score-based priors for posterior sampling. We note that several recent works [60; 20; 29; 54; 21] introduce hyperparameters to balance the influence of the prior and measurements, resulting in sampling strategies that guide output to regions where the given

Figure 1: Schematic plot of tilted transport boosting posterior sampling with a 2D Gaussian mixture example. The density plot shows the first variableâ€™s density, and the scatter plot displays the samples.

observation is more likely. These strategies typically deviate from the principles of Bayesian posterior sampling and often lack a precise definition of the resulting distribution. In contrast, other approaches adhere more closely to Bayesian principles. One such approach is variational inference, which involves designing variational objectives and optimization methods based on the structure of score-based diffusion [41; 47; 26; 37]. However, even with an accurate prior score, the accuracy of posterior sampling heavily depends on the choice of variational family and optimization procedures, not to mention the additional optimization cost. Another popular strategy focuses on approximating the score conditional on the measurement using various heuristics [58; 40; 36; 22; 48; 56; 57]. In this approach, approximation errors typically remain largely uncontrollable due to the challenges associated with tracking the conditional distribution for intermediate states. Recently, some studies have adopted sequential Monte-Carlo methods to systematically approximate the conditional score [62; 13; 23], providing consistency as the number of particles used to approximate the conditional distribution of the intermediate states increases. However, this particle-based method still struggles with high-dimensional problems due to the curse of dimensionality . Alternatively, [61; 63] propose plug-and-play methods with denoising oracles for posterior sampling, offering asymptotic guarantees, though the required steps may grow prohibitively in high dimensions.

We note that  also intuitively explores the possibility of reducing the original posterior to an equivalent one under restrictive conditions in the discrete-time setting. In contrast, our tilted transport technique operates in a fairly generic setting and is supported by a clear theoretical foundation. Concurrently,  proposes a conceptually similar two-stage approach for posterior sampling in sparse linear regression, based on a different structural prior rather than denoising oracles.

## 2 Preliminaries

Notations:\((^{d})\) denotes the space of probability measures over \(^{d}\). \(_{d}\) denotes the \(d\)-dimensional standard Gaussian measure, and by slight abuse of notation, \(_{}\) or \(_{}\) denote the centered Gaussian measure with covariance \( I_{d}\) or \(\) when the context is clear. For \(Q 0\) in \(^{d d}\) and \(b^{d}\) in the span of \(Q\), the quadratic tilt of \(\) is the measure \(_{Q,b}\) with density proportional to \(_{Q,b}}{}(x)\{-x^{}Qx+x^{}b\}\). We also use the notation \(_{Q}\) when \(b=0\). \(\|Q\|\) denotes its operator norm. \(\) denotes the convolution of two measures \(\) and \(\). For \( 0\) and \((^{d})\), we define \(_{}(x):=^{d}( x)\) as the _dilation_ of \(\). For \( 0\) and \((^{d})\), we define \(_{}(x):=(_{^{-1/2}}_{d})\) as the _Gaussian convolution_ of \(\).

Problem Setup.Consider a high-dimensional object of interest \(x^{d}\), drawn from a certain probability distribution \((^{d})\). We suppose that one has managed to learn a generative model for \(\) via the DDPM objective ; in other words, for any \(y^{d}\) and \( 0\), we have access to the _denoising oracle_\(_{}(y,):=[x|y]\), where \(y=x+ w\), with \(x\) and \(w_{d}\) independent. It is now well-established that, such denoising oracle enables efficient sampling of \(\), well beyond the classic isoperimetric assumptions for fast relaxation of Langevin dynamics .

Suppose that we now measure \(y=Ax+ w\), where again \(x\) and \(w_{d^{}}\) are independent, but now \(A^{d^{} d}\) is a _known_ linear operator different from the identity. Given these linear measurements, we are now interested in the _posterior sampling_ of \(x\) given \(y\). This corresponds to the basic setup of linear inverse problems, encompassing many applications such as image inpainting, super-resolution, tomography, or source separation, to name a few. We are interested in the following natural question: can the power of denoising oracles be provably transferred to posterior sampling?

By Bayes' rule, the posterior distribution \(_{y,A}\) (denoted simply by \(\) when the context is clear) has density proportional to \((x)\,p(y|x)\{-}\|Ax-y\|^{2}\}(x)\), and thus we can write it as a quadratic tilt of \(\):

\[=_{Q,b}\,Q=^{-2}A^{}A\,,\ b=-^{-2}A^{ }y\.\]

We readily identify certain regimes where sampling from \(\) might be easy:

* If \(_{}(Q)\) is sufficiently large, \(_{}(Q) 1\), then one expects \(\) to be strongly log-concave, enabling fast relaxation of Langevin dynamics.

* If \(_{}(Q)\) is sufficiently small, \(_{}(Q) 1\), then one expects \(\) in the appropriate sense, and therefore that samples from \(\) (which can be produced efficiently thanks to \(_{}\)) may be perturbed into samples from \(\).
* If \(A_{d}\) is a unitary transformation, then \(Q=^{-2}\) and the inverse problem reduces to isotropic Gaussian denoising, and is thus at first glance 'compatible' with the structure of the denoising oracle (such observation will be formalized later).

At this stage, we can already identify two key parameters of the problem that are likely to drive the difficulty of posterior sampling: on one hand, a proxy for the signal-to-noise ratio, measured e.g., by \(:=_{}(Q)=(A)^{2}}{^{2}}\). On the other hand, the conditioning of the measurement operator \(A\), \((A):=(A)}{_{}(A)}\). As we shall see, these two characteristics of the linear measurement system will characterize necessary and sufficient conditions for probable posterior sampling. In the following, we assume the log of prior density \(\) is smooth and its Hessian exists \( x^{d}\).

Denoising Oracles and Score-Based Diffusion.Let us first review the natural connection between denoising and score-based generative modeling. Score-based diffusion models consist of two processes: a forward process that gradually adds noise to input data and a reverse process that learns to generate data by iteratively removing this noise. For example, one widely used family for the forward process is the Ornstein-Uhlenbeck (OU) process1:

\[X_{t}=-X_{t}t+W_{t}\, X_{0} \,\] (1)

where \(W_{t}\) is the standard Wiener process. We use \(_{t}\) to denote the density of \(X_{t}\), given by the action of the OU semigroup \(_{t}=_{t}^{*}\), defined by \(_{t}f(x)=[f(X_{t})|X_{0}=x]\), and explicitly given by dilated Gaussian convolutions, \(_{t}^{*}:=_{_{t}}_{_{t}}\), with \(_{t}=1-e^{-2t}\) and \(_{t}=e^{t}\). With a sufficiently large \(T\), we know that \(_{T}\) is close to the density of standard Gaussian \(_{d}\), owing to the exponential contraction of the OU semigroup: \((_{T}||_{d}) e^{-T}(||_{d})\).

Finally, the measure \(_{t}\) solves the Fokker-Plank equation

\[_{t}_{t}=(x_{t})+_{t}\,\ _{0}=\.\] (2)

By writing (2) as a transport equation \(_{t}_{t}=((x+_{t})_{t})\), we can formally reverse the transport starting at a large time \(T\) and solving

\[_{t}_{t}=(-(x+_{T-t}) _{t})\,\ _{0}=_{T}\.\] (3)

Since \(_{t}=_{T-t}\) for \(0 t T\), introducing again the dissipative term leads to \(_{t}_{t}=(-(x+2_{t}) {}_{t})+_{t}\,\ _{0}=_{T}\,\) which admits the SDE representation

\[_{t}=(_{t}+2_{T-t}(_{t})) t+_{t},\ \ _{0}_{T}\.\] (4)

In practice, one runs this reverse diffusion starting from \(_{0}_{d}\) rather than \(_{0}_{T}\). However, by the data-processing inequality, we have that \((||_{T})(_{T}||_{d})=O(e^{-T})\), thus incurring in insignificant error. To facilitate later exposition, we write the above process reverse in time 

\[X_{t}^{}=(-X_{t}^{}-2_{t}(X_{t}^{ }))t+_{t},\ \ X_{T}^{}_{d},\] (5)

and interpret the data generation process as running the reverse SDE from \(T\) back to \(0\).

By the well-known Tweedie's formula, and up to time reparametrisation, the denoising oracle is equivalent to the time-dependent score \(_{t}\):

**Fact 1** (Tweedie's formula, ).: _We have \(_{t}(x)=-(1-e^{-2t})^{-1}(x-e^{-2t}_{}(x,1-e^{2t}))\)._

Log-Sobolev Inequality and Fast Relaxation of Langevin Dynamics.Given a Gibbs distribution \((^{d})\) of the form \( e^{-f}\), a powerful and versatile method to sample from \(\) is to consider the Langevin dynamics

\[X_{t}=- f(X_{t})t+W_{t}\,\ X_{0}_{0}\,\] (6)where \(_{0}\) is an arbitrary initial distribution. It is easy to verify that these dynamics define a Markov process that admits \(\) as its unique invariant measure. Perhaps less obvious is the fact that the Fokker-Plank equation associated with eq. (6), given by \(_{t}=( f)+\) (and where \(_{t}\) is the law of \(X_{t}\)) is in fact a Wasserstein gradient flow for the relative entropy functional \((||)\). Under this interpretation, one can quantify the convergence of Langevin dynamics to their invariant measure, i.e., its time to relaxation, by establishing a sharpness or _Polyak-Lowajevitz_ (PL)-type inequality. Indeed, by noticing that \((||)=-(||)\), where \((||)=_{}[\|-\|^{2}]\) is the Fisher divergence, the PL-type inequality in this setting is given by the _Logarithmic Sobolev Inequality_ (LSI): we say that a measure \(\) satisfies \(()\) if for any \((^{d})\) it holds \((||)(||)\). This functional inequality directly implies \((_{t}||) e^{-2 t}(_{0}||)\). While for general \(\) it is typically hard to establish the LSI, there are two important sources of structure that lead to quantitative (i.e., \(=_{d}(1)\)) bounds: when \(\) is a product measure \(=^{ d}\) (in which case \(\) satisfies LSI with the same constant as \(\)), and when \(\) is strongly log-concave2, i.e., \(-^{2}(x) I\) for all \(x\), in which case the celebrated Bakry-Emery criterion  states that \(\).

## 3 Evidence of Computational Hardness in the Generic Case

We start our analysis of posterior sampling by discussing negative results for the general case. Recently,  established computational lower bounds for this task using cryptographic hardness assumptions. In this section, we complement these results by illustrating a correspondence with sampling problems on Ising models, leading to an arguably simpler conclusion.

For this purpose, consider \(=(\{ 1\}^{d})\) the uniform measure of the hypercube. Quadratic tilts of \(\) define generic Ising models, a rich and intricate class of high-dimensional distributions. Since \(\) is a product measure, its associated denoising oracle becomes a separable function that can be computed in closed-form:

**Fact 2** (Denoising Oracle for \(\)).: _Let \((t;,)=\{-}(t-)^{2}\}\). Then we have_

\[_{}(y,)=((y_{i};))_{i=1 d }\,\ with\ \ (t,)=\.\] (7)

Given a symmetric matrix \(Q^{d d}\), an Ising model is given by the tilt \(_{Q}(\{ 1\}^{d})\). In our setting, we can thus view such models as the posterior distribution of a linear inverse problem associated with the uniform prior \(\). Efficiently sampling from Ising models is a fundamental question at the interface of statistical physics and high-dimensional probability, and several works provide evidence of computational hardness under a variety of settings.

Notably, by treating \(Q\) as the adjacency matrix of a regular graph,  establishes that sampling from \(\) is impossible for \(_{}(Q)-_{}(Q) 2+\), for any \(>0\), unless \(=\). In other words, for poorly conditioned tilt \(Q\) (in the sense that there is a large gap between the smallest and largest eigenvalue), there is no efficient posterior sampling algorithm, _even with the knowledge of the prior denoising oracle_. The threshold can even be reduced to \(1+\) by using a weaker notion of computational hardness , given by the _low-degree polynomial method_. Remarkably, this threshold agrees with the current best-known algorithmic results for sampling generic Ising models with Glauber dynamics . Finally, we also mention that when \(Q\) is a random Gaussian symmetric matrix, the associated so-called Sherrington-Kirkpatrick (SK) model, has been analyzed with dedicated algorithms. In this setting, it is also known  that'stable' sampling algorithms fail to sample from the SK model as soon as \(_{}(Q)-_{}(Q)>1\). In summary, we have:

**Theorem 3** (Computational Hardness of Sampling Ising Models, ).: _There exist no general-purpose, efficient posterior sampling algorithms, for \(Q\) sufficiently ill-conditioned, even under the knowledge of the prior denoising oracle._

One could wonder whether this computational hardness comes from the discrete nature of the hypercube. It is not hard to observe that this is not the case: the following proposition, proved in Appendix A, shows a simple reduction from a model where the prior \(\) is replaced by a smooth mixture of Gaussians \(\) centered at the corners of the hypercube, with variance \(\).

**Proposition 4** (Hardness extends to smooth priors).: _Assume a posterior sampler exists for the smooth prior with TV error \(\) and \(=o(d^{-1/2})\). Then there exists a sampler for the associated Ising model with TV error \(1.1\)._

In conclusion, one cannot hope for a generic method that leverages the prior denoising oracle to perform efficient posterior sampling, as soon as \(A\) is mildly ill-conditioned. Thus, in order to perform provable posterior sampling, one needs to either (i) constraint the measurements, or (ii) exploit structural properties of the prior measure. In the following, we focus on (i), namely providing guarantees for well-conditioned \(A\) that leverage the OU semigroup for generic prior distributions.

## 4 Posterior Sampling via Tilted Transport

We now present a simple method that reduces the original posterior sampling problem to another posterior sampling problem with more benign geometry, by leveraging the shared quadratic structure of the posterior tilt and the OU semigroup. The power of the denoising oracle to perform sampling of the prior \(\) comes from its ability to run the transport equation (3) in either direction, and leveraging the fact that sampling from \(_{T}\) is easy. To transfer this power to posterior sampling, we can thus attempt to replicate this scheme: can we implement a transport between the posterior \(\) and a terminal measure \(_{T}\) that is easy to sample, that only relies on the pre-trained prior \(_{}\)?

A Motivating Example.Consider first the denoising setting: \(y=x+ w\). According to the forward process, we have \(p(X_{s}|X_{0})}{{=}}(e^{-s}X_{0},(1-e^{ -2s})I_{d})\). Introduce \(T^{*}>0\) and define \(=e^{-T^{*}}y=e^{-T^{*}}x+e^{-T^{*}} w\) such that \(p(|x)}{{=}}(e^{-T^{*}}x,e^{-2T ^{*}}^{2}I_{d})\). We match the variance by letting \(e^{-2T^{*}}^{2}=1-e^{-2T^{*}}\), i.e., \(T^{*}=(1+^{2})\), then we have \(p(|x)=p(X_{T^{*}}|X_{0})\), which gives \((x,)}{{=}}(X_{0},X_{T^{*}})\). Therefore, to perform the posterior sampling \(p(x|)\), we only need to do the sampling \(p(X_{0}|X_{T^{*}})\), which can be achieved through the reverse SDE. Specifically, let \(X_{T^{*}}=e^{-T^{*}}y\) and run the reverse SDE (5) from \(T^{*}\) to \(0\), then \(X_{0}\) will be the desired posterior.

Hamilton-Jacobi Equation and Quadratic Tilts.If \(_{t}\) solves the Fokker-Plank eq. (2), then one can verify that the time-varying potentials \(f_{t}:=_{t}\) solve the viscous Hamilton-Jacobi PDE (HJE)

\[_{t}f_{t}= f_{t}+\| f_{t}\|^{2}+x f_{t}+d\,\ f_{0}=f\.\] (8)

Now, the posterior \(=_{Q,b}\) creates an additional quadratic term in the potential \(=f-x^{}Qx+x b\). One could naively hope that this additive quadratic term would still define a solution of the HJE with the tilted initial condition \(}=\) -- or equivalently that the measure \(_{Q,b}_{t}\) solves the transport equation (3). Unfortunately, due to the nonlinearity in (8) brought by the terms \(\| f_{t}\|^{2}\), this is not the case. However, as we shall see now, this is not far from being true: one just needs to consider _time-varying_ quadratic tilts in order to satisfy the HJE.

Tilt Transport Equation.We consider then a one-parameter family of distributions \(_{t}\) of the form \(_{t}=_{Q_{t},b}_{t}\), with \(Q_{0}=Q\) and \(b_{0}=b\). As it turns out, one can ensure that \(_{t}\) solves the HJE associated with the reverse process by asking that \(Q_{t},b_{t}\) satisfy the first-order ODE

\[_{t}=2(I+Q_{t})Q_{t}\,&Q_{0}=Q\\ _{t}=(I+2Q_{t})b_{t}\,&b_{0}=b\] (9)

**Theorem 5** (Tilted Transport).: _Assume \(t<T^{*}\) such that the ODE (9) is well-defined on \([0,t]\). By initializing \(X_{t}_{t}\) and run the reverse SDE (5) from \(t\) to 0, we have \(X_{s}_{s}\) for \(s[0,t]\), specifically, \(X_{0}\) gives the desired posterior._

Solution to eq. (9).Without loss of generality, we assume \(d^{} d\), and the observation operator \(A^{d^{} d}\) has a general singular value decomposition form \(A=U V^{}\) with non-zero singular values \(_{1}_{2}_{d^{}}>0\). By diagonalizing \(Q\) and solving the scalar ODE \(_{t}=2(1+q_{t})q_{t}\) for diagonal entries, we have \(Q_{t}=V(}{1+^{2}/_{1}^{2}-e^{2t}}, ,}{1+^{2}/_{d^{}}^{2}-e^{2t}},0,,0 )V^{}\), where the solution is defined up to the blowup time \(T^{*}:=(1+^{2}/_{1}^{2})=(1+ _{}(Q)^{-1})\). \(b_{t}\) can be further solved from the solution \(Q_{t}\); see Appendix B.2 for more details.

With the explicit solution of \(Q_{t},b_{t}\), we can interpret the term \((-x^{}Q_{t}x+x^{}b_{t})\) as the likelihood of the inverse problem with respect to the new prior distribution \(_{t}\) and the corresponding operator. Based on this observation and Theorem 5, we have the following corollary, transforming the original posterior sampling problem to a new posterior sampling problem exactly. We remark that when \(A\) is identity, the corollary recovers the analysis we have in the motivating example; see Appendix B.2 for the proof and more discussions.

**Corollary 6**.: _Fix \(t<T^{*}\). Sampling from the original posterior is equivalent to a two-step process: first, sample from a new posterior \(X_{t}_{t}\), and then run the reverse SDE (5) from time \(t\) to 0._

## 5 Quantitative Conditions for Provable Sampling

Now we show that the new posterior sampling problem described above is provably easier to sample than the original posterior sampling problem from two aspects. On the one hand, the (negative) eigenvalues of the quadratic tilt \(-x^{}Q_{t}x+x^{}b_{t}\) become more negative, essentially meaning that the SNR of the new observation model becomes larger. To be more specific, as \(t T^{*}\), \(_{}(Q_{t})(Q)^{-1}}{_{}(Q)^{- 1}-_{}(Q)^{-1}}>_{}(Q)\). On the other hand, the new prior distribution \(_{T}\), becomes closer to a single-mode Gaussian (recall that \((_{t}||_{d})=O(e^{-t})\)), which is also easier to sample. Combining these two arguments, we expect that, as \(t\) increases, \(_{t}\) becomes easier to sample due to easier prior and easier likelihood:

\[_{t}(x)(x)}_{}-x^{}Q_{t}x+x^{}b_{t}}}_{}.\]

Let us now quantify the above intuition.

Sufficient Conditions for Strong Log-Concavity of \(_{T^{*}}\).We start by giving a simple sufficient condition that ensures that \(_{T^{*}}\) is strongly log-concave. As discussed earlier, this ensures fast relaxation of the Langevin dynamics, enabling efficient sampling from \(_{T^{*}}\) - and therefore of \(\) as per Corollary 6. For that purpose, given the prior \((^{d})\) and \(t 0\), we define

\[_{t}():=_{x^{d}}\|[_{t,x} ]\|,\] (10)

where the covariance is given by \([]=_{x}[xx^{}]-(_{x }[x])(_{x}[x])^{}\). \(_{t}()\) thus measures the largest'spread' of any tilted measure of the form \(_{t,x}\). Equipped with this definition, we have the following sufficient condition to ensure that \(_{T^{*}}\) is strongly log-concave:

**Proposition 7** (Strong Log-Concavity of \(_{T^{*}}\)).: \(_{T^{*}}\) _is strongly log-concave if_

\[_{\|Q\|}()<\|Q\|^{-1}}{((A)^{2}-1)}\.\] (11)

The proof is in Appendix C. It relates two parameters of the measurement process, the condition number of \(A\) and the signal-to-noise ratio in terms of \(\|Q\|\), with a geometric property of the prior, the spread function \(_{t}()\). While this function is not immediately transparent, the following examples illuminate its behavior in reprsentative high-dimensional settings.

**Example 8** (Behavior of \(_{t}()\)).: _We have the following examples_

1. _Gaussian measure:_ \(_{t}(_{d})=\)_._
2. _Compactly Supported Gaussian Mixture: If_ \(\) _is compactly-supported in a ball of radius_ \(R\) _and_ \( 0\)_, then_ \(_{t}(_{})^{2}+ \)_._
3. _Tensorization: If_ \(=_{1}_{2}_{d}\)_, then_ \(_{t}()=_{i}_{t}(_{i})\)_._
4. _Uniform measure on hypercube: If_ \(\) _is uniform on the hypercube_ \(_{d}\)_, then_ \(_{t}()=1\)Ising Models.As a direct consequence of Proposition 7 and Example 8 (iv), we establish a sampling guarantee for Ising models:

**Corollary 9** (Tilted Transport for the Ising Model).: _Let \(\) be the uniform measure on the hypercube, and \(Q\) such that \(_{}(Q)-_{}(Q)<1\). Then \(_{T^{*}}\) is strongly log-concave, and therefore \(=_{Q}\) can be sampled efficiently (in continuous-time)._

This result thus establishes that Ising models admit an efficient _continuous-time_ procedure for sampling provided their spectrum satisfies \(_{}(Q)-_{}(Q)<1\), thus precisely matching the threshold of  achieved by Glauber dynamics, as well as the low-degree prediction from . We remark though that our procedure is not (yet) algorithmic; a careful analysis of the discrete-time complexity and the approximation rates is beyond the current scope, but our next endeavor. If one specializes the previous result to the SK model, the equivalent inverse temperature that guarantees sampling is \(^{*}=1/4\), which remains below \(=1\), the threshold of the hard phase. For this threshold, dedicated AMP-based sampling succeeds . We also remark that, in itself, it should not come as a surprise that \(\) may be sampled under these conditions, since  already established an LSI on \(\) directly, using an entropy decomposition along the so-called Polchinski renormalization group  that refines our Bakry-Emery criterion. In this context, it would be interesting to explore whether this refined criterion could be applied to \(_{T^{*}}\) to improve upon Proposition 7 under appropriate conditions.

Gaussian Mixtures.By applying Proposition 7 to Example 8 (ii), we directly obtain the following guarantee for generic compactly supported Gaussian mixtures:

**Corollary 10** (Tilted Transport for Gaussian Mixtures).: _If \(=_{}\) and \(diam(supp()) R\), then \(_{T^{*}}\) is strongly log-concave if_

\[^{2})((A)^{2}+^{-2})}{ (A)^{2}-1}>R^{2}\.\] (12)

_It also holds when \(=0\) and the prior \(\) is any distribution with a bounded support radius \(R\)._

Figure 2 displays several contours of the condition in eq. (12) as a function of \(\) and \((A)\). Each \(U\)-shaped contour is determined by a combination of \(\) and \(R\), which uniquely characterizes the prior. For all points ((\(\)), \((A)\)) outside of a contour, representing a specific inverse problem, \(_{T^{*}}\) is strongly log-concave and thus easy to sample. Given an observation model where both \(\) and \((A)\) are fixed, it is straightforward to see that the condition in eq. (12) is more readily satisfied as \(\) increases and \(R\) decreases. Figure 2 also confirms this result since as \(\) increases or \(R\) decreases, the \(U\)-shaped contour shrinks and the region of easy to sample expands. Now we discuss the implications in the reverse scenario where the prior is fixed and the observation model is adjusted. If we look at Figure 2 horizontally, we know that given a prior and \((A)\), the target posterior can be reliably sampled if the \(\) is either sufficiently low or high, with the region of mid-SNR being challenging. The closer \((A)\) is to \(1\), the smaller this challenging region is. When the problem is denoising such that \((A)=1\), the challenging region vanishes, and sampling the posterior is straightforward using the denoising oracle, as previously explained.

Comparisons.1. (with Langevin dynamics) As introduced above, Langevin dynamics and its discretized version, Langevin Monte Carlo (LMC)  serve as ideal baselines for efficient posterior sampling in high SNR regimes where the posterior becomes strongly log-concave. Proposition 7

Figure 2: Phase diagram for the boosted posterior \(_{T^{*}}\) being strongly log-concave in Corollary 10.

demonstrates that our tilted transport technique enables provably efficient sampling from a broader range of prior distributions compared to traditional Langevin dynamics without a denoising oracle. Particularly, in low SNR regimes where conventional Langevin dynamics struggle with severe non-log-concavity and slow mixing times, tilted transport can transform the sampling challenge into a tractable problem for log-concave distributions.

2. (with Importance Sampling) In the low SNR regime with a well-conditioned \(A\), the posterior measure can be viewed as a small perturbation of the prior. As such, a natural baseline for posterior sampling is importance sampling using the prior as a proposal -- for which samples can be efficiently obtained thanks to the denoising oracle and the variance of sample weights is small. However, as detailed in Appendix C.2, the sampling complexity is exponential with the SNR when the SNR is sufficiently large, assuring the failure of the importance sampling on this extreme.

Stability.In the numerical implementation of the boosted posterior, we often encounter specific errors. Appendix C.3 provides a stability analysis of the two-step process outlined in Corollary 6, focusing on initialization error and score error, and demonstrates that the quality of the final samples is robust with respect to these errors.

## 6 Numerical Experiments

Our theory above demonstrates that \(_{T^{*}}\) is provably easier to sample than the original posterior \(\). Thus, given a baseline sampling algorithm \(\), we can first sample from the boosted posterior and then apply the denoising oracle to obtain the final sample, rather than directly sampling from \(\) using \(\). Algorithm 8 provides a complete description of this approach using tilted transport. In this instance, we use Euler discretization with equal time steps to transport samples from \(_{Q_{T},b_{T}}_{T}\) to \(_{Q,b}\), though alternative time integrators and grids can also be applied.

```
0: Parameters of quadratic tilt \(Q,b\), small time shift \(\), baseline sampling algorithm \(\), time-dependent score \(_{t}()\), \( t\) for reverse SDE
0: A sample \(X_{0}\) from posterior distribution \(_{Q,b}\)
1: Calculate the blowup time by \(T^{*}:=(1+_{}(Q)^{-1})\)
2: Determine the number of reverse SDE steps by \(N=-}{ t}\) and starting time \(=N t\)
3: Use baseline sampling algorithm \(\) to sample \(X_{N}\) from \(_{Q_{T},b_{T}}_{T}\)
4:for\(i=N\)to\(1\)do
5: Sample \(Z_{i}(0,I_{d})\)
6:\(X_{i-1} X_{i}+(X_{i}+2_{i t}(X_{i})) t+ \ Z_{i}\)
7:endfor
8:return\(X_{0}\) ```

**Algorithm 1** Sampling Using Tilted Transport

We now validate our theoretical results by applying Algorithm 8 to the Gaussian mixture model in high dimensions, using LMC as the baseline algorithm. Same to the models considered in , the prior distribution is a mixture of 25 components with known means and variances (see Figure 1 for a 2D visualization and Appendix E for detailed settings). We examine three cases where \(d=20,40\), and 80. In each scenario, we set \(d^{}=d\), fix \(=20\), and vary the SNR from \(10^{-5}\) to \(10^{-1}\). We use the Sliced Wasserstein distance as a principled error metric, computed from samples obtained by our algorithms and samples directly from the analytically computed posterior Gaussian mixture. Figure 3 illustrates the comparison between LMC and LMC boosted by tilted transport. As analyzed earlier, LMC is effective when the SNR is high enough to render the target posterior strongly log-concave, but its error quickly increases as the SNR decreases. In contrast, the tilted transport enhances LMC to perform well in both low and high SNR regimes with small sampling errors. Its performance is weaker in the mid-SNR regime compared to the extremes, as predicted by Corollary 10. However, the tilted transport still improves upon LMC in this challenging regime by boosting effective SNR and simplifying the prior.

We further test tilted transport when \(d^{}<d\), in which \(_{}(Q_{t})\) remains zero but the signal corresponding to the non-zero eigenvalues still gets enhanced. Therefore, although it becomes more difficult for \(v_{T^{*}}\) to be strongly log-concave, the tilted transport can still make the new posterior easier to sample even if it is not strongly log-concave yet. Detailed results are reported in Appendix E.1.

## 7 Discussion and Future Work

In this paper, we theoretically investigate posterior sampling using powerful priors provided by denoising oracles. We demonstrate that efficient posterior sampling can be challenging even with a perfect denoising oracle for the prior. To achieve provable posterior sampling, one must either constrain the measurements or leverage the structural properties of the prior. We focus on the former, showing that well-conditioned measurements enable the proposed tilted transport technique to simplify the task significantly, providing a clear, verifiable condition for efficient sampling, as demonstrated on the Ising model. Several questions remain open: Can this approach provably handle poorly-conditioned measurements, such as inpainting? Can it be extended from linear to nonlinear inverse problems? We show in Appendix D how to extend the tilted transport beyond the condition of Proposition 7 via 'iterated tilts', at the expense of introducing approximation errors. On the theory side, the key object underlying the success of the tilted transport is the spread \(_{t}()\); in particular, understanding when one can remove dimension dependence is an interesting question. We also aim to systematically evaluate the empirical performance of tilted transport in imaging and scientific computing. Appendix E.2 provides a proof-of-concept for various imaging tasks. We suspect that tilted transport could even improve existing posterior point estimate methods by boosting SNR and enabling proper uncertainty quantification through the reverse process.