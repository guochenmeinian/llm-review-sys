# M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Multispectral Data

**Matt Allen**

University of Cambridge, UK

mja78@cam.ac.uk

**Francisco Dorr**

Independent, Argentina

fran.dorr@gmail.com

**Joseph A. Gallego-Mejia**

Drexel University, USA

jagallegom@unal.edu.co

**Laura Martinez-Ferrer**

Universitat de Valencia, Spain

laura.martinez-ferrer@uv.es

**Anna Jungbluth**

European Space Agency, Climate Office, UK

anna.jungbluth@esa.int

**Freddie Kalaitzis**

University of Oxford, UK

freddie.kalaitzis@cs.ox.ac.uk

**Raul Ramos-Pollan**

Universidad de Antioquia, Colombia

raul.ramos@udea.edu.co

###### Abstract

Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with differing degrees of availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling for easier handling can present significant technical hurdles for novice users. While some preprocessed Earth observation datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. In this work, we introduce M3LEO, a multi-modal, multi-label Earth observation dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside multispectral Sentinel-2 imagery and a suite of auxiliary data describing terrain properties such as land use. M3LEO spans approximately 17M data chips, each measuring 4x4 km, across six diverse geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra, to accommodate its use across diverse ML applications in Earth observation. Additionally, we provide tools to process any dataset available on popular platforms such as Google Earth Engine for seamless integration with our framework. We show that the distribution shift in self-supervised embeddings is substantial across geographic regions, even when controlling for terrain properties. Data is available at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.

Introduction

Satellite-based Earth observation data is fundamental in addressing global problems in a rapidly changing world, offering large-scale, high resolution, high frequency data for applications from tracking wildfires  and deforestation  to refugee settlement mapping  and war zone damage assessment [4; 5]. Information from these tasks is critical in crafting responses to man-made  and environmental crises , but is constrained by the use of optical (wavelengths from visible to near-infrared) sensing data. Such sensors are unable to operate in adverse weather or cloudy conditions , or at night, limiting their usefulness for time-critical tasks such as natural disaster management , environmental protection  or maritime surveillance . Synthetic Aperture Radar (SAR) sensing presents an alternative that is able to overcome these limitations.

Unlike optical sensors, SAR instruments actively illuminate terrain using microwave pulses, ensuring visibility without the need for daylight. These long-wavelength pulses can penetrate cloud cover and other adverse atmospheric conditions such as dust, making SAR-based sensing a valuable alternative to optical sensors for robust day-night coverage. Additionally, microwave radiation can penetrate some solid objects such as small-scale vegetation or soils -- allowing, for example, measurements of properties relating to soil moisture under vegetation  or the identification of archaeological features hidden below ground . In addition to exploiting the wavelength and active illumination of SAR data, the complex nature of SAR signals -- returning both amplitude and phase -- can also be leveraged to provide insights beyond what is possible using optical data. Coherence, the complex correlation between pairs of SAR acquisitions, has been used successfully for tasks including flood detection [13; 14], detection of urban damage  and forest canopy height measurement . The phase difference between co-registered SAR acquisitions, measured through interferometry, enables the detection of surface height changes with millimetre accuracy, independently of the horizontal resolution of the sensor. This capability is critical for monitoring geological phenomena such as earthquakes , landslides , glacial movement [19; 20], magma chamber growth  and infrastructure deformation .

While SAR offers opportunities to overcome the limitations of optical sensors, and to give insights that are impossible to provide using visible wavelengths, it is associated with substantial additional complexity. The automated analysis of optical data, sometimes used in fusion with SAR data, has seen great success in recent years [23; 24; 25] -- including the development of large foundation models able to make use of planetary-scale datasets [26; 27]. The application of large-scale deep learning to SAR data without simultaneous use of optical data, however, is more limited . The complexities of processing SAR data, particularly estimating coherence and performing interferometry -- which require processing phase information as using complex numbers -- mean that the full diversity of SAR data types is not available at scale in formats compatible with machine learning (ML) pipelines.

To address these challenges we introduce M3LEO, a large-scale multi-modal Earth Observation dataset comprising polarimetric, interferometric and coherence data for SAR as well as Sentinel-2 data and auxiliary datasets describing surface properties such as land use, biomass measurements and elevation. We provide this data pre-processed as ML-readable, tiled images to abstract complex parameter choices typically made by domain experts. We also include a flexible PyTorch Lightning framework, parameterised using Hydra , to further minimise barriers to usage. We include a preliminary analysis on distribution shift for terrain properties and the appearance of SAR data across geographic regions. Finally, in addition to the pre-formatted data we offer for download, we provide tools enabling ML practitioners to process any data retrievable from Google Earth Engine into the same tiled format, such that it can be used in our framework.

## 2 Related Work

Deep learning has been applied over the last decade to curated optical imagery with great success [30; 31; 32; 33], including the recent development of large, self-supervised foundation models [34; 35; 36; 37]. Such models have been extraordinarily successful in tasks such as semantic segmentation [38; 39], image classification ,  and object detection [38; 41]. EO data from optical sensors has similarly been the subject of success for deep learning practitioners. Early work focusing on small, fully supervised models, showed great promise in a huge range of tasks, including land cover classification , biomass measurement , road detection [25; 42] and flood mapping , although many models were limited in scope to a small geographic area [24; 44; 45; 46]. More recent work has focusedon the development of large foundation models, often self supervised [28; 47], which are readily adaptable to a range of downstream tasks and geographic areas .

Work applying these models to optical EO data has been enabled by the wealth of easily accessible open data. As well as being available as raw products from satellite data providers such as ESA , many datasets comprising optical satellite imagery in ML-ready formats exist [49; 50; 51; 52], across a range of spatial resolutions [53; 54], and for multiple time-points [55; 50], although they may be limited in other scopes -- for example, not having aligned task labels [55; 56] or being limited to a single region . We provide data at a large scale (14.1% of the land surface of the Earth) with a diverse set of auxiliary labels.

The application of deep learning to SAR data is less comprehensive. A body of work applying deep learning directly to SAR data exists [57; 58; 59; 60; 61], but data limitations mean that geographic or temporal generalisability, often lacking in remote sensing models , has not clearly been shown. The development of foundation models for SAR may prove to be productive in obtaining geographic and temporal generalisability. To create foundation models, SAR is commonly applied alongside optical imagery in data fusion-based approaches [63; 64], but with little attribution regarding whether such approaches can work well without optical data. Some work exists exploiting schemes such as masked autoencoding [65; 66], contrastive learning , or knowledge distillation [68; 69] to develop foundation models for polarimetric SAR -- and shows that strong geographic generalisabilty is obtainable when using SAR data at large scales [65; 69]. Many datasets providing ML-readable polarimetric SAR (polSAR) data exist [51; 55; 70; 71; 72; 73; 74; 75], although most do not provide interferometric SAR (inSAR) data [74; 75].

The full diversity of inSAR datatypes, such as interferometry and coherence, have seen a number of applications in machine learning, and a rich tapestry of applications in other contexts. Interferometry, for example, is often used to track earthquakes , landslides  and glacial movement [19; 20], in a manner that is both more repeatable -- being immune to adverse weather conditions -- and more accurate -- being able to track millimetre-scale height changes -- than methods using optical data. Coherence has been used with success in urban damage assessment , flood detection [13; 14] and canopy height measurement .

A number of datasets exist making these datatypes available to deep learning users, many of which are focused on specific events, tasks or locations. Hephaestus , for example, contains 216K interferometric SAR (inSAR) patches localised to volcanoes annotated with various labels describing volcanic activity. ISSLIDE  contains inSAR data from the French Alps describing slow landslides, and Pol-InSAR-Island  comprises inSAR data describing land cover on Baltrum, a Frisian island. UrbanSARFloods  contains Sentinel-1 interferometric coherence data from a diverse set of global locations, but is limited to specific events. S1SLC_CVDL  opts to provide complex-valued single-look SAR data rather than processed interferometric data, from three manually selected Sentinel-1 scenes containing major population centres. We make inSAR and coherence data available at a multi-continental scale, alongside both polarimetric SAR and optical data, in M3LEO.

### Comparison to Existing Datasets

We provide a comparison between a number of popular large-scale Earth observation datasets, including M3LEO, in Table 1. We define _tile_ to mean a fixed location or area on the surface of the Earth, and _chip_ as the content of some data product over that tile. Unlike in many of these datasets, we provide acquisitions from different seasons within the same year for the same tile as channels, rather than separate chips. The described number of chips in Table 1 therefore appears relatively lower for M3LEO compared to, for example, SSL4EO-S12, for a fixed number of satellite acquisitions. We instead measure the number of timepoints in years, rounded up for part-years. Of the four datasets offering Sentinel-1 SAR data, only M3LEO offers data from multiple years, and only M3LEO offers inSAR data (although see Section 1 for a brief description of available task-specific or localised interferometric datasets).

Regarding spatial coverage, of the datasets listed in Table 1 M3LEO is most similar in scale to SatlasPretrain and SSL4EO-L, with these three datasets being significantly larger than the remainder. Of these three datasets, only M3LEO provides SAR data of any modality.

The temporal coverage of M3LEO sits between SatlasPretrain and SSL4EO-L, although the aggregate number of years does not give a full picture -- in SatlasPretrain, some acquisitions are available for a wider range of years for specific events, and in SSL4EO-L different data products are sometimes collected for non-overlapping years, meaning much of the dataset is not a parallel corpus. In M3LEO, the primary satellite data from Sentinel-1 and Sentinel-2 is available for 2018-2020 for all tiles.

Several of these datasets contain auxiliary information in addition to satellite acquisitions. Land cover labels are common, included in SSL4EO-L, SEN12MS, BigEarthNet and SatlasPretrain . Additional auxiliary datasets are sometimes available -- SSL4EO-L, for example, includes more detailed crop classification data. SatlasPretrain introduced a number of novel additional labelled datasets including building and road polygons. M3LEO currently includes 4 auxiliary datasets - Land cover, vegetation cover, aboveground biomass and Digital Elevation Models (DEMs).

Many datasets are pre-sampled within their selected AOIs prior to distribution. Some datasets sample based on a manually specified distribution (for example, SSL4EO-S12 and SSL4EO-L sample locations based on Gaussian distributions centred on large cities), and some randomly. We include all available tiles within our AOIs -- partly with a view to increasing data volume, but also to enable further research on sampling schemes. Selecting a good sampling strategy to diversify actively illuminated radar data with phase information is not straightforward, and it doubtful whether sampling schemes developed for optical EO imagery would transfer well to this data. The auxiliary data included in M3LEO, such as elevation and land use, may be useful for constructing such sampling schemes.

## 3 Dataset & Framework

### SAR Datasets

The many benefits of SAR data are met with increased complexity compared to optical data. SAR sensors are active -- that is, they emit microwave pulses (5.6 cm wavelength for Sentinel-1) and measure backscatter rather than imaging the Earth under passive illumination from the Sun. This enables day-night operation and the penetration of atmospheric obstructions such as cloud and dust. Unlike sunlight, the pulses emitted by SAR sensors are polarised, with the ability to emit pulses polarised either horizontally or vertically with respect to the Earth. SAR sensors are also able to measure the polarization of the backscatter, and capture both amplitude and phase. This signal, typically stored as a complex number, allows studying the geometry of surface-level objects, in addition to their reflectances. As an example, higher amplitudes are measured when surface features align with the polarisation of the emitted pulse. As with optical imagery, objects smaller than the measurement wavelength are invisible to the sensor.

The use of SAR data is further complicated by the use of side-looking radar. SAR sensors operate with the emitter and receiver aimed laterally, rather than vertically, as for optical sensors. Since radar operates by measuring the time of arrival for a backscattered signal, aiming the sensor vertically would make it impossible to distinguish between targets at an equal distance to the left or right of the direction of travel. This is corrected by side-looking, with the sensor aimed laterally such that the entire field of view is to one side of the satellite. Although side-looking corrects directional ambiguity, it necessitates complex post-processing to correct the resulting geometrical distortions and

  
**Dataset** & **SAR** & **Years** & **Num.** & **Num.** & **Tile Size** & **Coverage** & **Sampling** \\  & & & **Tiles** & **Chips\({}^{*}\)** & km & km\({}^{2}\) & \\  SSL4EO-S12  & Y & 1 & 251K & 3M & 2.64\(\)2.64 & 1.75\( 10^{6}\) & Targeted \\ SSL4EO-L  & N & 6 & 250K & 5M & 7.92\(\)7.92 & 1.57\( 10^{7}\) & Targeted \\ SEN12MS  & Y & 1 & 181K & 542K & 2.56\(\)2.56 & 1.18\( 10^{6}\) & Random \\ SeCo  & N & 2 & 200K & 1M & 2.65\(\)2.65 & 1.40\( 10^{6}\) & Random \\ BigEarthNet  & N & 1 & 590K & 1.2M & 1.20\(\)1.20 & 8.50\( 10^{5}\) & Targeted \\ SatlasPretrain  & N & 1** & 856K & N/A & 5.12\(\)5.12 & 2.13\( 10^{7}\) & None \\ 
**M3LEO** & Y & 3 & 1.05M & 17.2M & 4.48\(\)4.48 & 2.11\( 10^{7}\) & None \\   ^{*}\) Heuristic only — some work, such as SSL4EO-S12, considers acquisitions at the same location from different seasons to be a seperate data chip.} \\ ^{**}\) Contains additional historical images from 2016-2021 that are relevant to dynamic events such as floods.} \\ 

Table 1: **Existing Datasets. Summary of popular large-scale Earth observation pre-training datasets.**radiance redistribution , and results in the same terrain being imaged differently depending on the direction of satellite travel, as the terrain is illuminated from the opposite side. We provide data in both ascending (northwards) and descending (southwards) satellite directions in M3LEO.

We provide three products derived from SAR data in this dataset -- polarimetric amplitude, interexergograms, and coherence. We give a brief background on each of these datatypes below, although we omit most technical details regarding their construction as they are beyond the scope of this work. References are provided for users who are interested in further background on these datatypes.

AmplitudePolarimetric amplitude measures the power of the backscattered signal received by the sensor. We provide amplitude data derived from ESA Sentinel-1 Level 1 Ground Range Detected SAR data (S1GRD), as available in Google Earth Engine (GEE)1. Phase information is not provided via GEE and it is therefore impossible to produce further SAR datatypes from data available on the GEE platform. We provide data measuring vertically polarised and horizontally polarised returns from a vertically polarised emission (referred to as VV and VH respectively). Data is provided for imagery from both ascending and descending trajectories. We refer users to  for a more detailed breakdown of the theory behind polarimetric SAR data. S1GRD data is of 10 m/pixel resolution and provided for 2018-2020 as four seasonal averages per-year.

InterferometryInterformetry measures the phase difference between pairs of acquisitions over the same terrain. These phase differences provide data about small-scale displacements, which can be measured modulo the wavelength. Post-processed interferograms are _unwrapped_ by computing accumulated modulo-wavelength displacements. As the scale at which these displacements can be measured depends on wavelength, rather than horizontal resolution, interferometric phase difference can be used to measure surface height changes with millimetre accuracy. We provide interferometric data computed using select pairs of Sentinel-1 acquisitions, from ASF ARIA Geocoded UNWrapped Interferogram data (GUNW), as available in ASF vertex2. We include all available acquisition pairs with time deltas of less than 72 days. We refer users to  for a detailed treatment of the processing steps required to construct interferograms in addition to the underlying physics. GUNW data is provided for 2020 at a resolution of approximately 90 m/pixel.

CoherenceThe coherence of SAR imagery is calculated using the complex correlation between coincident pixels across two separate acquisitions. For a given complex valued pixel \(z^{(i)}\) in SAR acquisitions at times \(1\) and \(2\), the coherence \(\) is defined as:

\[=[_{1}^{(i)}(t)z_{2}^{(i)}(t)]| }{[|z_{1}^{(i)}(t)|^{2}][|z_{2}^{(i)}(t)|^{2}]}}\] (1)

To provide meaningful coherence values, expectations are computed within a small spatial window surrounding each pixel. The resulting resolution of coherence maps is therefore lower than that of the original acquisitions. Man-made structures typically exhibit high coherence, as they are stable across acquisitions. Forests and other vegetation larger than the wavelength of the instrument have lower coherence. Coherence is affected in all cases by additional decorrelation factors not relating directly to terrain, such as doppler centroid difference or thermal noise.  and  provide more detailed treatments of coherence estimation. We provide coherence estimates from the Global Seasonal Sentinel-1 Interferometric Coherence (GSSIC) dataset , using date-pairs with time deltas of 12, 24, 36 and 48 days, with one date-pair per tile per season. GSSIC data is of approximately 90 m/pixel resolution and from 2020. A decay model is included with the GSSIC coherence data.

For both interferometry and coherence, the selection of acquisition pairs is critical. The acquisition pair selection process introduces a significant combinatorial challenge to managing SAR datasets -- if every possible combination between all acquisitions were considered, the number of interferograms or coherence estimates would grow quadratically with the number of acquisitions. We pre-empt this issue by the provision of pre-selected date-pairs.

Optical dataWe additionally provide data (S2SRM) sourced from the ESA Sentinel-2 mission3, as available in Google Earth Engine. Data is provided from the L2A product (surface reflectance). We do not include top-of-atmosphere L1C reflectances. We summarize this data as monthly means of cloud-free pixels. We include four monthly averages for 2018-2020 -- March, June, September and December. We include all Sentinel-2 bands with a of \(10\) m/pixel (red, green, blue, NIR) or \(20\) m/pixel (vegetation red edge, SWIR 11/12).

### Auxiliary Datasets

ESA World CoverLand cover classification labels (semantic segmentation) were obtained from the ESA World Cover product (ESAWC) , as available in Google Earth Engine5. The resolution of ESAWC is 10 m/pixel, and it comprises 11 classes (See Appendix E). The ESAWC product has been independently validated as having an accuracy of approximately 75\(\%\). Data is provided for 2020.

ESA CCI BiomassA map of above ground biomass (AGB) in Mg ha\({}^{-1}\), derived from the ESA Climate Change Initiative's Biomass product6 is provided for 2020 at a resolution of 90 m/pixel. The relative error of this data is \(20\%\) for areas with a measured biomass exceeding \(50\) Mg ha\({}^{-1}\)

MODIS Vegetation CoverTree cover, non-tree cover and non-vegetated (bare) percentage labels derived from the Terra MODIS Vegetation Continuous Fields product (MODISVEG)7, are provided at a resolution of 250 m/pixel. A limited amount of independent validation reports the RMSE of the MODISVEG data as approximately 10\(\%\). Our dataset includes yearly maps for 2016-2020.

GHS Built SurfaceMaps of built surface area (m\({}^{2}\)/pixel), derived from the Copernicus Global Human Settlement Built Surface (GHSBUILTS) product8 are provided at 100 m/pixel for 2020. The mean absolute error of this data has been evaluated using independent reference data has been estimated to be approximately 6\(\%\) (or 600 m\({}^{2}\)/pixel, at 100 m/pixel).

SRTM Digital Elevation MapsDigital Elevation Maps, derived from the NASA Shuttle Radar Topographic Mission (SRTM)9[97; 98] are provided at a resolution of 90 m/pixel. This data was measured in 2000, but we emphasise that terrain height changes relatively little at this resolution compared to other products such as ESAWC or GHSBUILTS. The RMSE of the SRTM data was originally reported as 16 m  although it has been measured as more accurate in some regions .

### Data coverage

Our dataset covers six distinct geographic areas of interest (AOIs): the contiguous United States (CONUS), Europe, the Middle East, Pakistan and India (PAKIN), China, and South America. A visualisation is provided in Figure 1. We limit coverage to these regions for reasons relating to the acquisition parameters used by Sentinel-1.

Firstly, Sentinel-1 operates with different acquisition modes depending on the region. We choose to only include areas where Sentinel-1 uses the Interferometric Wide (IW) swath acquisition mode. In polar regions, Sentinel-1 employs the Extra Wide (EW) swatch acquisition mode, which introduces systematic differences in how terrain is illuminated -- such as the azimuth steering angle of the radar emitter being 0.8\({}^{}\) for EW and 0.6\({}^{}\) for IW acquisition. The polarisations used in polar regions are reversed (HH, HV vs. VV, VH) compared to other areas. We provide IW data with both VV and VH polarisations in the initial release of this dataset.

Secondly, much of the terrestrial surface of the Earth is only covered by a single direction of satellite travel. This includes much of North America, Africa, continental Asia, Oceania and the Amazon rainforest. Unlike passively illuminated data such as Sentinel-2, SAR actively illuminates terrain with a radar emitter aimed laterally from one side of the satellite. Orbital direction therefore systematically determines whether terrain is illuminated from an easterly or westerly direction.

By focusing on regions with consistent acquisition parameters, we aim to provide a dataset that is more uniform and suitable for training models without introducing additional complexities. While including other regions might reduce geographic bias, it is not straightforward to address the potential systematic biases introduced by varying acquisition modes and polarisations, or the systematic lack of directional coverage in some regions. These issues warrant a detailed treatment beyond the scope of this work, and we refer readers to the Copernicus Wiki for a full set of details on Sentinel-1 coverage9 (particularly Figures 20, 21). Processing additional coverage is technically straightforward using the provided tools, should it be required. We include data for Europe despite its absence of GUNW coverage due to interest from data providers operating in Europe.

A total of 1,048,827 unique geographic tiles were generated, covering an area of \(2.11 10^{7}\) km\({}^{2}\). A breakdown by AOI can be seen in Table 2. Tiling is uniform across all AOIs and datasets--the chips provided for each dataset cover exactly the same geographical areas. It is important to note that not all component datasets or specific parameterisations, such as date-pair ranges, were available for every geographic tile; however, the availability is still consistently high. See Appendix A for a full breakdown by dataset and AOI. We provide a reduced version of our dataset, **M3LEO-miniset**, spanning 5,000 tiles per AOI for rapid model iteration and use in tutorials.

Data splitsWe provide use geographic bands to define training, validation and test splits for use in the explorations foudn in this work, following . Bands were split into training, validation and test sets at a ratio of 60:20:20, and can be seen visually in Figure 1. This method reduces distribution shifts and data leakage compared to single-band splits and fully randomized assignments, respectively. See Appendix A for details. Users should define train-test splits appropriate for their individual applications if their needs differ.

   AOI & Total No. Tiles & Area (km\({}^{2}\)) & \% Earth’s \\  & & & land surface \\  CONUS & 167403 & 3.360\( 10^{6}\) & 2.3\% \\ Europe & 200489 & 4.024\( 10^{6}\) & 2.7\% \\ Middle East & 163986 & 3.291\( 10^{6}\) & 2.2\% \\ PAKIN & 147791 & 2.966\( 10^{6}\) & 2.0\% \\ China & 285402 & 5.728\( 10^{6}\) & 3.7\% \\ South America & 83756 & 1.681\( 10^{6}\) & 1.1\% \\ 
**Total** & **1048827** & **21.05\( 10^{6}\)** & **14.1\%** \\   

Table 2: **Coverage statistics summarised for each AOI in M3LEO.**

Figure 1: **Data splits**. Geographic bands for training, validation and test sets, at a ratio of 60:20:20.

### Framework

Data downloading & processingFor each AOI, we provide a.geojson file containing the geographical extent and unique ID for each tile. These definitions are applied to each dataset at the point of tiling such that any chip with a given identifier spans precisely the same geographic extent of a chip corresponding to a different dataset of the same identifier. Throughout this work, we use the term _tile_ to refer to a fixed area of the Earth's surface defined in the definition files, and the term _chip_ to refer to the data from a single component dataset, such as S1GRD or ESAWC, within the extent of a given tile.

To process datasets available via Google Earth Engine (GEE) (S2RGB, S1GRD, SRTM, ESAWC, MODISVEG), we introduce geetiles10. This tool extracts and tiles data from GEE as per definition files and configurations provided in the geetiles repository. The remaining datasets (GSSIC, GUNN, AGB, GHSBUILTS) were extracted using sartiles11, which contains specific code to download and tile each of GUNW and GSSIC, as well as the facility to tile general GeoTIFF files, such as those provided for AGB and GHSBUILTS, for integration with M3LEO. Both geetiles and sartiles were developed alongside M3LEO, and are provided such that users are able to seamlessly integrate any data available via Google Earth Engine (or as a GeoTIFF file) with our framework.

PipelineWe accompany our dataset with a modular PyTorch Lightning  framework parameterised using Hydra . We provide PyTorch Lightning datasets for each component of M3LEO. We also provide additional modules defining self-supervised approaches applied successfully to M3LEO in previous works (MAE , CLIP , DINO [68; 69]). Integrating custom models with M3LEO is straightforward, requiring the addition of a single file.

## 4 Analysis

We provide five auxiliary datasets (ESAWC, AGB, MODISVEG, GHSBUILTS, SRTM) in addition to SAR and optical satellite data acquisitions. Although these data could be used as labelled tasks (see Appendix D, and also [65; 67; 68; 69] for analysis of this type on M3LEO), they could equally be considered as providing information on important surface properties that would be non-trivial to derive directly from satellite data.

We compare the shift in the marginal distribution of these terrain properties \(y\), \(p(y)\) in Figure 3 for four of these auxiliary datasets. We computed the normalised \(L_{1}\) distance between the discrete ESAWC distributions and the Wasserstein distance between the continuous GHSBUILTS, MODISVEG and SRTM distributions, across AOIs. The marginal distributions \(p(y)\) for each auxiliary dataset are shown in Appendix B.

We also provide an early exploration of the 'appearance shift' of features between AOIs for S1GRD -- the change in the distribution of embeddings \(x\) for a SAR tile with known terrain properties \(y\), \(P(x|y)\)

Figure 2: **M3LEO dataset and framework. The M3LEO dataset consists of nine ML-ready component datasets and a PyTorch Lightning framework, parameterised by Hydra, for model training.**To generate embeddings, we trained a masked autoencoder on S1GRD polarimetry from all six AOIs, following previous work on M3LEO . Training hyperparameters can be seen in Appendix C. We applied max-pooling along the sequence dimension at the output of the ViT encoder and further reduced the dimension to 2 using UMAP . We computed the expected Wasserstein distance between the conditional distributions \(p(x|y)\) with respect to the distribution \(p(y)\) on the test set, across AOIs and show the results in Figure 4. For ESAWC, we applied principal component analysis and conditioned on the first 3 principal components with 10 evenly spaced bins per dimension. On the remaining variables, we used 100 evenly spaced bins. We also display the sliced Wasserstein distance between the embedding distributions \(p(x)\) in the leftmost matrix of Figure 4. We plot these reduced embeddings, coloured according to each of the terrain properties, in Figure 5. A checkpoint for the MAE model is available in the data repository.

We observed that there was significant covariate shift in the distribution of the embeddings produced by the masked autoencoder, \(p(x)\), across AOIs (Figure 4, leftmost matrix). Given that there was also significant shift in terrain properties described by the auxiliary datasets between AOIs (Figure 3, this is not immediately surprising. Another factor that must be considered, however, is the shift in the embeddings produced by the masked autoencoder for tiles with similar terrain properties \(y\), \(p(x|y)\), across AOIs (appearance shift). It can be seen in the four right-hand matrices of Figure 4 that although there is some reduction in the most extreme cases, the expected value of sliced Wasserstein distance conditioned on similar terrain properties is usually not substantially lower than the unconditioned case. Explained from an Earth observation perspective - the masked autoencoder does not extract overly similar features for two tiles with, for example, very high vegetation when those tiles are taken from different geographic regions. This effect can be seen visually in Figure 5 - embeddings with

Figure 4: **Covariance and appearance shift. (Leftmost) The Sliced Wasserstein Distances (SWD) between reduced train and test set embeddings across AOIs, generated using a masked autoencoder. (Right four) The expected value of same metric conditioned on each of four terrain properties. The expectation was computed with respect to the distribution of the terrain property on the test set. We conditioned on the first three principal components of the ESAWC distribution.**

Figure 3: **Distribution shifts. Distribution shifts between terrain properties described by auxiliary datasets. Distribution shift for the ESAWC categorical data was quantified using L1 distance and continuous data using Wasserstein distance.**

high values for particular labels do not cluster obviously across different AOIs. In contrast, previous work applying DINO-based self-supervision to M3LEO  found that embeddings with similar labels clustered tightly in the embedding space across AOIs. Despite this observation, previous work applying MAE-based pretraining to M3LEO has shown strong generalisation to novel AOIs .

We did not explore the use of GSSIC coherence data here, as this is substantially technically challenging and requires detailed treatment. Given the lower resolution of this data, it may not be productive to use as a naive input to deep learning models. A small set of experiments evidencing this is provided in Appendix D. We point readers to previous work using coherence data from M3LEO productively in a self supervised setting - in contrastive learning  or in knowledge distillation-based approaches . We note that the provision of GUMW interferometric coherence data without reference to specific events such as floods or fires is unusual compared to other datasets . Although we aim to include event-based datasets in a future update, this large-scale dataset of interferograms is still highly desirable in self-supervised schemes.

A number of unknowns remain regarding domain shift in M3LEO. Many auxiliary datasets, such as ESAWC, do not exist for 2018 and 2019 so it is difficult to provide a substantial exploration of temporal shifts, although we provide S1GRD and S2SRM for three years. It is unclear whether features learned from encoders trained on different polarizations or orbital directions are comparable, although D contains a limited set of experiments on the value-add of different polarisations.

## 5 Conclusions and Future Work

In this work, we introduced M3LEO, a multi-continental Earth observation dataset including a comprehensive set of SAR data, alongside digital elevation models, RGB data and a suite of downstream tasks. To the best our knowledge, this is the largest ML-readable polSAR dataset by total number of tiles and geographic coverage, and the largest inSAR dataset by the same metrics. We additionally provide a modular PyTorch Lightning framework to enable the application of deep learning and encourage the uptake of these datatypes. We provide additional tools, geetiles and sartiles, to enable the integration of any data available in Google Earth Engine with our framework.

We trained an MAE-based model on polSAR data and conducted a small exploration on the appearance shift of features corresponding to similar labels across AOIs. Despite the fact that this type of training has previously been shown to generalise well geographically , the shift in low-level features useful for the reconstruction pretext task was substantial between geographic regions. This is in contrast to previous work using M3LEO that found embeddings from DINO-based models with similar labels from different AOIs clustered tightly.

Figure 5: **Embedding scatter plots. Scatter plots of 2D embeddings, reduced using UMAP, coloured according to different auxiliary datasets.**

Acknowledgements

This work has been enabled by Frontier Development Lab Europe (https://fdleurope.org) a public / private partnership between the European Space Agency (ESA), Trillium Technologies, the University of Oxford and leaders in commercial AI supported by Google Cloud and Nvidia, developing open science for all Humankind. L.M-F. was supported by the European Research Council (ERC) Synergy Grant "Understanding and Modelling the Earth System with Machine Learning (USMILE)" under the Horizon 2020 research and innovation programme (Grant agreement No. 855187). M. J. A. was supported by the UKRI Centre for Doctoral Training in Application of Artificial Intelligence to the study of Environmental Risks [EP/S022961/1]. We are also indebted to Nicolas Longepe, Carlos Lopez-Martinez, Fabio A. Gonzalez Osorio, Samuel Bancroft, Emma Hatton, Alison Lowndes, Alistair Francis, Ioanna Bouri and the rest of reviewers during the 2023 FDL-Europe sprint.