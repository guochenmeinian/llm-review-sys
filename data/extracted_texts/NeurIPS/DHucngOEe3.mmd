# Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation

Haoqi Yuan\({}^{1}\) Yuhui Fu\({}^{1}\) Feiyang Xie\({}^{2}\) Zongqing Lu\({}^{1,3}\)

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)Yuanpei College, Peking University

\({}^{3}\)Beijing Academy of Artificial Intelligence

Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>.

###### Abstract

Efficiently solving unseen tasks remains a challenge in reinforcement learning (RL), especially for long-horizon tasks composed of multiple subtasks. Pre-training policies from task-agnostic datasets has emerged as a promising approach, yet existing methods still necessitate substantial interactions via RL to learn new tasks. We introduce MGPO, a method that leverages the power of Transformer-based policies to model sequences of goals, enabling efficient online adaptation through prompt optimization. In its pre-training phase, MGPO utilizes hindsight multi-goal relabeling and behavior cloning. This combination equips the policy to model diverse long-horizon behaviors that align with varying goal sequences. During online adaptation, the goal sequence, conceptualized as a prompt, is optimized to improve task performance. We adopt a multi-armed bandit framework for this process, enhancing prompt selection based on the returns from online trajectories. Our experiments across various environments demonstrate that MGPO holds substantial advantages in sample efficiency, online adaptation performance, robustness, and interpretability compared with existing methods.

## 1 Introduction

In the evolving landscape of deep learning, the paradigm of pre-training followed by fine-tuning has become a dominant approach for improving learning downstream tasks, particularly in the domain of computer vision  and natural language processing (NLP) . This paradigm has recently been explored in deep reinforcement learning (RL) , addressing the issue of sample efficiency in RL when solving unseen tasks by leveraging the acquired knowledge during pre-training. For example, offline meta-RL (OMRL)  studies pre-training a policy on _multi-task datasets_, which adapts to a new task with limited interactions in this task. However, these approaches typically require extensive data collection for each specific task. In contrast, given the relative ease of acquiring large, task-agnostic datasets that contain diverse behaviors, other studies focus on pre-training policies  and skills  on _task-agnostic datasets_ and adapt to unseen tasks with RL finetuning.

Most works , especially in the context of OMRL, primarily focus on short-term tasks with shaped rewards, where information of the unknown task can be inferred within a few steps of online interactions (e.g., MuJoCo  and MetaWorld ). These approaches often fall short of tackling long-horizon tasks which are characterized by a sequence of sub-processes. In our study, we consider this challenging yet realistic scenario: given a task-agnostic dataset characterized by diverse behaviors of the agent, we aim to pre-train a policy to facilitate efficient online adaptation to new, long-horizon tasks.

Recent studies [38; 60] tackle this problem by assuming that diverse short-term skills can be acquired from task-agnostic datasets, which are sufficient to compose the required behaviors for complex, long-horizon tasks. These methods pre-train a goal-conditioned policy focused on short-term skills. During online adaptation, they use RL to train a separate high-level policy that selects goals based on current states and executes the pre-trained policy for multiple steps. While this approach avoids the extensive finetuning of the pre-trained policy, it still requires substantial number of interaction steps for online RL. This limitation arises from the pre-trained policy's inability to sequentially achieve multiple goals during a single attempt. The capability to switch goals - deciding at which state to transition to the next goal - must be developed during the costly online adaptation phase.

_Is it feasible to pre-train a policy conditioned on a sequence of goals, capable of autonomously transitioning between goals while executing long-horizon behaviors aligned with these goals?_ Achieving such a policy would pave the way for developing efficient online adaptation algorithms, eliminating the need for learning a high-level policy for state-conditioned goal switching. However, the challenge lies in the substantial demand on the policy's long-term memory capabilities, which must effectively remember the goal sequence and base action prediction on a long history context.

To tackle this challenge, we draw inspiration from the success of Transformers  in language modeling [10; 7], which have demonstrated a remarkable capability to model long sequences and, when pre-trained on diverse datasets, adapt efficiently to downstream tasks through prompt optimization . In our proposed Multi-Goal Transformers with Prompt Optimization (**MGPO**), we pre-train a Transformer-based policy that takes a goal sequence as a prompt and predicts actions in the following sequence of environment observations. During online adaptation, we only optimize for the sequence of goals in the prompt, drawing parallels with the concept of prompt optimization in language models.

In the pre-training stage, we employ hindsight relabeling to construct prompts from goals visited in the trajectory and train the Transformer policy through behavior cloning. Thus, the policy learns to produce behaviors consistent with the goal sequence. The number of goals in the prompt modulates the deterministic or exploratory nature of the policy's behavior. For online adaptation, we propose a novel prompt optimization strategy, leveraging the returns of online trajectories to guide the selection of goal sequences. This process is formulated as a multi-armed bandit problem and we introduce two approaches through upper confidence bound (UCB) and Bayesian posterior estimation. Figure 1 provides an overview of our framework.

We evaluate MGPO across diverse domains, including maze navigation, the robotic simulation environment Kitchen, and the open-world game Crafter. Our results demonstrate that MGPO significantly surpasses prior methods in terms of sample efficiency and performance during online adaptation. MGPO adapts to new tasks within a small budget of 100 online episodes in all environments. Comparisons to existing prompt optimization methods highlight the interpretability and robustness of our method.

In summary, our main contributions are:

* We propose pre-training multi-goal Transformers to address the challenge of online adaptation in long-horizon tasks. Our approach combines the strengths of Transformers in sequence modeling and the advantages of goal-conditioned pre-training for task adaptation.
* We introduce novel methods for optimizing goal sequences, offering enhanced interpretability and robustness compared to existing prompt optimization methods.
* Our experimental results in diverse, challenging environments demonstrate that MGPO significantly enhances sample efficiency over existing methods.

## 2 Preliminaries

### Problem Formulation

A task in an environment is formulated as a partially observable Markov Decision Process (POMDP) \(M= S,O,A,T,,r,\) representing the state space, the agent's observation space, the action space, the transition probability of the environment, the initial state distribution, the reward function, and the discount factor, respectively. Starting from the initial state, the agent takes an action at each timestep, and then the environment transitions to the next state and returns a reward. At each timestep\(t\), the environment provides the agent an observation \(o_{t}\) via an emission function \(o_{t} E(o_{t}|s_{t})\). This process continues until the task either terminates or reaches a maximum timestep \(H\). We denote the historical observations and actions as \(h_{t-1}=\{(o_{i},a_{i})\}_{i=1}^{t-1}\). Reinforcement learning (RL) aims to learn a policy \((a_{t}|h_{t-1},o_{t})\) to maximize the expected return \(J()=_{a_{t}(a_{t}|h_{t-1},o_{t})}[R]\), where \(R=_{t=1}^{H}^{t}r(s_{t},a_{t})\) is the discounted cumulative return of an episode.

We focus on long-horizon tasks that require executing a substantial number of subtasks sequentially. We define a goal function \(g=f_{G}(o)\) that maps an observation to a goal, representing key information related to task completion, such as the agent's position in navigation tasks or the states of objects in robotic scenarios. \(G\) denotes the goal space. A goal \(g\) is said to be reached at time \(t\) if \(g_{t}=f_{G}(o_{t})\). We assume that the task \(M\) in an unknown environment provides a task goal \(g^{M}\), indicating the final goal to be reached. To optimally solve the long-horizon task, the policy must sequentially reach multiple goals. As an example, consider an environment resembling rooms with unknown structures where the task is to navigate to a goal location \(g^{M}\). Under partial egocentric observations, the agent should learn from trial and error to find the shortest path to the goal location, which involves reaching several specific waypoints \((g_{1},...,g_{k})\) as necessary goals.

During pre-training, we assume access to a task-agnostic dataset \(D=\{_{i}\}_{i=1}^{N}\) consisting of trajectories \(=(o_{1},a_{1},...,o_{H},a_{H})\), in which the agent performs diverse behaviors. For online adaptation, we aim to solve an unseen task \(M\) with its goal \(g^{M}\) provided, within an online interaction budget of \(N\) episodes. The objective is to find the optimal policy \(\) maximizing the expected return \(J()\).

### Transformers and Prompt Optimization

Some previous methods  pre-train a policy \(_{}(a_{t}|h_{t-1},o_{t})\) parameterized by \(\) and then finetune with online RL. This approach is sample-inefficient due to discrepancies between the behaviors in the dataset and those required for downstream tasks. Other methods [38; 60] pre-train a policy \(_{}(a_{t}|z,h_{t-1},o_{t})\) conditioned on a variable \(z\) to perform diverse short-term skills and then train a high-level policy \(_{}(z|h_{t},o_{t})\) online using RL. As discussed earlier, this approach often results in limited sample efficiency because the high-level policy must learn when to switch skills \(z\) based on the observations during the online phase.

In this work, we explore pre-training a policy capable of performing diverse long-horizon behaviors, aiming to develop efficient online adaptation methods that do not require additional RL. To model such diverse long trajectories, we employ the Transformer architecture in our policy, known for its effectiveness in sequence modeling tasks. Transformers leverage an attention mechanism , enabling the model to weigh different parts of the input sequence differently, thus effectively addressing long-term dependencies.

In pre-training, we aim to develop a Transformer-based policy \(_{}(a_{t}|p,h_{t-1},o_{t})\) parameterized by \(\), taking an additional input variable \(p\). This policy processes inputs in an auto-regressive manner, where the input is a sequence \((p,o_{1},a_{1},...,o_{H},a_{H})\), and outputs an action \(a_{t}\) at each observation \(o_{t}\). Here, the input variable \(p\) acts similarly to a prompt in language models, guiding the policy's behavior. During online adaptation, we keep the model parameters \(\) fixed and treat \(p\) as the optimization

Figure 1: An overview of MGPO. We pre-train Transformer-based policies on task-agnostic datasets, leveraging hindsight multi-goal relabeling and behavior cloning to endow policies with the capacity for modeling long-term behaviors. In unseen tasks, multi-goal Transformers adapt efficiently through prompt optimization, which searches for a sequence of goals with the aim of maximizing online returns.

variable, transforming the problem into finding an optimal prompt \(p\):

\[_{p}J(_{}(a_{t}|p,h_{t-1},o_{t})).\] (1)

This approach mirrors the concept of prompt optimization in language models and eliminates the need to use online RL to train a high-level policy.

## 3 Method

### Pre-Training Multi-Goal Transformers

We implement a prompt as a sequence of goals, describing an ordered sequence of key states to be visited, which abstracts the long-term behavior of the agent. Inspired by recent works [28; 60] that employ hindsight relabeling  to generate goals from offline trajectories, we sample a sequence of goals within the trajectory to construct the prompt.

Using the observations \((o_{1},...,o_{H})\) in a trajectory \(\), we construct a sequence of goals \(g=(g_{1},...,g_{H})\) with the same length \(H\), representing the process of agent behavior in this episode. We first uniformly sample a number \(k U[0,K-1]\) where \(K\) is the maximal prompt length, then uniformly sub-sample the goal sequence to construct a prompt \(p=(g_{i_{1}},...,g_{i_{k}},g_{H})\), where \(0 i_{1}... i_{k}<H\). Since the trajectory ends at the last goal \(g_{H}\), we keep \(g_{H}\) at the last position in the prompt. Using this sampling mechanism, the prompt can describe behavior in different granularities with its varying length. We denote the process of sampling a prompt from the trajectory as \(p P(p|)\).

Similar to Decision Transformers , we adopt a causal Transformer to build the policy \(_{}(a_{t}|p,h_{t-1},o_{t})\), which takes as input a sequence concatenating the prompt and the trajectory: \((p,)=(g_{i_{1}},...,g_{i_{k}},g_{H},o_{1},a_{1},...,o_{H},a_{H})\). At each input \(o_{t}\), the policy only sees the sub-sequence from \(g_{i_{1}}\) to \(o_{t}\) due to the causal attention mask and predicts the action distribution \(_{}(a_{t}|p,h_{t-1},o_{t})\). We sample batches of trajectories in \(D\) and train the policy using behavior cloning:

\[_{}_{ D,\,p P(p|)}[_{t=1}^{H} _{}(a_{t}|p,h_{t-1},o_{t})].\] (2)

This training scheme encourages the policy to utilize the information provided in the prompt to reduce uncertainty in action prediction. After pre-training, it learns to perform behaviors that follow the sequence of goals in the prompt and seamlessly alter behavior between different goals. Prompts with different numbers of goals provide varying amounts of information to match behavior , thereby introducing different levels of uncertainty in action, enabling a trade-off between exploration and exploitation in online adaptation.

AMAGO  also adopts a similar multi-goal relabeling strategy to train RL policies. While it utilizes hindsight relabeling for better exploration, we are the first to use this scheme to pre-train policies capable of stitching different goals sequentially, thereby facilitating efficient task adaptation.

### Online Prompt Optimization

Given an unseen task with the task goal \(g^{M}\), we start with an initial prompt \(p_{0}=(g^{M})\) and aim to find an optimal prompt \(p^{*}\) with a maximal length of \(K\) that maximizes the expected return. The algorithm can evaluate prompts for \(N\) episodes to return an optimized prompt. In principle, we can employ any black-box optimization approach for this purpose, including discrete prompt search [9; 40] and continuous prompt-tuning [48; 27] methods. However, prompts optimized in the entire prompt space \(G^{K}\) or even in a continuous space can contain uninterpretable goals and are out of training distribution. The policy with such prompts may produce unpredictable behaviors.

To enhance interpretability and robustness, we propose a novel method that samples prompts from online collected trajectories, aligning more closely with the training distribution of prompts. We assume that, if we condition the policy on prompts sampled from a trajectory with high return, it is likely to yield high expected returns since it performs similar behavior of this trajectory. Thus, we propose a method that alternates between exploring online trajectories for high returns and sampling new prompts from the best seen trajectory of the highest return.

Formally, we maintain a buffer \(B\) of sampled prompts and their history of returns and the trajectory \(^{*}\) of the highest return \(R^{*}\). Each iteration involves: (1) selecting a prompt \(p^{*}\) from \(B\) most likely to yield high returns and collecting a trajectory with \((a|p^{*},h,o)\); (2) sampling a new prompt \(p^{} P(p|^{*})\), which is then added to the buffer with the return obtained with \((a|p^{},h,o)\). Prompt selection in (1), which requires acting under uncertainty given observed returns in history, can be modeled as a multi-arm bandit (MAB) problem. We implement two optional solutions including upper-confidence bound (UCB)  and a method based on Bayesian posterior estimation (BPE) , where the former selects the prompt with the highest UCB of expected return and the latter selects the prompt most possible to yield return higher than \(R^{*}\) based on the estimated posterior of its return distribution.

In summary, our method employs several key designs to optimize prompts effectively: (1) Trajectory-based sampling: Instead of exploring the combinatorial space of goal sequences, we restrict our search to prompts derived from collected trajectories, ensuring both feasibility and relevance. (2) Rewarded exploration: We further refine our prompt search by selecting prompts from trajectories that have highest returns, thereby enhancing the likelihood of performance improvement. (3) Task-goal consistency: We maintain the final goal within each prompt as the task goal, ensuring that all exploration efforts are aligned with task completion.

Our prompt optimization method is detailed in Algorithm 1 in Appendix E.1, where the implementations of UCB and BPE are also provided.

## 4 Experiments

In this section, we present experimental results obtained across various domains to evaluate the efficacy of MGPO. We aim to answer three questions: (1) Does MGPO improve sample efficiency in solving new tasks compared to previous methods? (2) How does our proposed prompt optimization method compare with existing methods? (3) How does each component in MGPO contribute to efficient online adaptation?

### Environments and Datasets

Our evaluation spans multiple domains featuring long-horizon tasks. We collect datasets to pre-train models and evaluate their online adaptation capabilities on test sets of unseen tasks or environment configurations. Detailed descriptions of these environments and datasets are available in Appendix C.

**MazeRunner:** A 2D Maze navigation environment with partial observation as introduced in . The maze has randomly generated walls, where the task is to reach a designated goal position \((x,y)\). The agent's observation includes its position and nearby terrain, receiving a reward of +1 for reaching the goal and a -0.1 penalty for each timestep. We collect the dataset with a handcrafted policy that explores various goals within each trajectory. Online adaptation requires finding optimal paths in unknown mazes with limited trials. We test MazeRunner with two maze sizes, \(15 15\) and \(30 30\), where the latter has a longer horizon of 500 steps.

**Kitchen and GridWorld:**Kitchen  is a robotic environment with continuous observations and actions, where a 7-DoF robot arm manipulates diverse objects in a simulated kitchen scene. We define each long-horizon task as completing a sequence of \(n\) subtasks in a specific order, providing \(+\) reward when the next correct subtask is completed. The task goal is provided as a set of subtasks without revealing their order. Other goals are represented as one-hot vectors indicating the next subtask. We collect the dataset using policies trained with RL. We also introduce GridWorld, a simplified 2D version of Kitchen, where the agent navigates to switch states of 7 objects located in different positions in specified orders.

**Crafter****:** A simplified benchmark of the open-world game Minecraft, where the 2D world is procedurally generated. The agent receives \(64 64\) egocentric image observations and takes discrete actions. The objective is to unlock 22 achievements, each providing a reward of +1. Goals are defined as one-hot vectors indicating the next achievement. The dataset is collected using policies from AD . For online adaptation, each task features a unique, unexplored world map.

### Baselines and Main Results

We compare MGPO with previous methods that focus on task-agnostic pre-training. Further details are provided in Appendix F.

**Goal-conditioned BC** feeds the task goal \(g^{M}\) directly into the pre-trained goal-conditioned policy for new tasks. Its performance is akin to the initial prompt performance in MGPO without prompt optimization.

**BC-finetune** updates the parameters of the pre-trained model with RL based on online collected trajectories, which is a common method in previous work . Conditioned on the task goal, we finetune the Transformer parameters using PPO .

**SPiRL** pre-trains short-term skills along with their latent representations and uses online RL to train a high-level policy for online adaptation. **PTGM** improves this approach by pre-training a goal-conditioned policy and discretizing the goal space for the high-level policy.

For each test task, we evaluated each method for 100 episodes of online rollout, measuring performance by the average return of the optimized policy or prompt. Results in Table 1 summarize the performance of MGPO against all baselines across different environments, where MGPO is implemented with UCB in online adaptation.

* **MGPO's superiority:** MGPO demonstrates superior performance in all environments, showcasing its efficiency in adapting to new tasks.
* **Limitations of BC-finetune:** This method underperforms others in most environments due to the instability and inefficiency of finetuning parameters in the entire model with online RL. It fails to solve tasks within the small budget of 100 online episodes.
* **Comparing SPiRL, PTGM, and MGPO:** While SPiRL and PTGM exhibit better sample efficiency than BC-finetune, they largely underperform MGPO in MazeRunner, Kitchen, and GridWorld. These approaches are limited by their reliance on learning a high-level RL policy for goal switching at different states during online adaptation. For example, PTGM trains a high-level policy \((g|h_{t},o_{t})\) to select goals, which is effectively searching policies in the joint space of \(O^{K} G\). In contrast, MGPO, with its inherent ability to seamlessly switch between goals, optimizes in the prompt space \(G^{K}\), leading to more efficient adaptation.
* **Goal-conditioned BC:** The performance of this method highlights MGPO's capability to refine and improve upon the initial prompt. Conditioned on a single goal, the performance of Goal-conditioned BC is sub-optimal due to limitations in dataset quality (e.g., MazeRunner) and the partial observability in task specifications (Kitchen and GridWorld) and environment layouts (MazeRunner and Crafter). In contrast, through online exploration, MGPO gathers more task and environment information in the unseen task and can search for a prompt sequence to stitch different short-term behaviors learned from data.
* **Performance in Crafter:** Performance of Goal-conditioned BC, PTGM, and MGPO in Crafter is similar compared with other environments. This is because, the dataset does not feature great diversity since the data collection policy always aims at unlocking more achievements, thereby the pre-trained policy may be less sensitive to the prompt. In contrast, in other environments, the prompt substantially influences the policy's behavior and performance.

   Method & MazeRunner-15 & MazeRunner-30 & Kitchen & GridWorld & Crafter \\  Goal-conditioned BC & -2.63 & -27.06 & 0.09 & 0.05 & 11.78 \\ BC-finetune & -3.09 \(\) 0.12 & -43.19 \(\) 2.97 & 0.00 \(\) 0.00 & 0.04 \(\) 0.00 & 1.88 \(\) 0.19 \\ SPiRL & -2.62 \(\) 0.35 & -28.94 \(\) 0.79 & 0.22 \(\) 0.05 & 0.10 \(\) 0.03 & 10.96 \(\) 0.25 \\ PTGM & -0.96 \(\) 0.08 & -26.74 \(\) 1.71 & 0.25 \(\) 0.03 & 0.25 \(\) 0.03 & **15.72**\(\) 0.12 \\  MGPO & **-0.41**\(\) 0.10 & **-14.21**\(\) 0.19 & **0.63**\(\) 0.02 & **0.58**\(\) 0.03 & 15.66 \(\) 0.14 \\   

Table 1: Performance of MGPO compared with baseline methods. Each result shows the average performance on all test tasks in the environment and the standard deviation across 3 random seeds for online test. Goal-conditioned BC has no error bars since it does not perform online optimization.

### Prompt Optimization Methods

In exploring prompt optimization methods for the pre-trained multi-goal Transformer, we draw insights from the area of prompt optimization for language models, including two contemporary methods to implement MGPO:

**GRIPS** is a genetic algorithm designed for prompt search within the discrete prompt space. In our implementations, we start with an initial prompt and generate new prompts to evolve it via online evaluating all prompts. We implement operations of adding, deleting, and swapping goals for generating new prompts. The algorithm returns the best-evaluated prompt.

**BBT** is a black-box optimization method for continuous prompt-tuning. It utilizes a low-dimensional vector \(z\), which is added to the initial prompt through random projection. This vector is then optimized using a CMA-ES  evolution strategy. Since it extends the prompt into a continuous space, the optimized prompts may not be interpreted as sequences of discrete goals.

For our proposed method, we investigate the two options for the MAB algorithm: **UCB** and **BPE**. Additionally, we examined an ablation approach dubbed **MGPO-explore**, which differs from our MAB formulation by leveraging the most recent prompt sampled from the highest-return trajectory for further exploration.

Table 2 presents experimental results of MGPO implemented with different prompt optimization methods. Across all environments, all MGPO methods demonstrate superior performance compared to the baseline methods in Table 1, showing MGPO's great compatibility with different prompt optimization methods.

UCB and BPE consistently outperform MGPO-explore in most environments. This result demonstrates the efficacy of our MAB formulation in the prompt optimization process. By exploring and exploiting existing prompts, MAB-based methods yield trajectories with higher returns, thus improving the overall performance.

UCB and BPE outperform the discrete search method GRIPS in four out of the five environments. This result highlights the strength of our proposed methods in discrete optimization for multi-goal prompts. While UCB and BPE outperform BBT in MazeRunner-15 and GridWorld, a reversal occurred in the other three environments, particularly in Kitchen where BBT surpasses the performance of all other methods. We speculate that, due to the nature of deterministic and differentiable transitions in Kitchen, the continuous optimization method BBT may quickly find a local optimum.

The performance of BBT raises critical considerations about the nature of the optimized prompts. Despite its efficiency, the optimized continuous prompts deviate significantly from the training distribution, potentially harming the robustness of the policy. We conduct further studies to evaluate the robustness of the optimized prompts. Table 3 compares the performance of the optimized prompts

   Method & MargeRunner-15 & MazeRunner-30 & Kitchen & GridWorld & Crafter \\  MGPO-GRIPS & -0.56 \(\) 0.09 & -13.01 \(\) 1.22 & 0.54 \(\) 0.09 & 0.40 \(\) 0.01 & 15.53 \(\) 0.02 \\ MGPO-BBT & -0.80 \(\) 0.16 & **-12.72**\(\) 2.22 & **0.74**\(\) 0.06 & 0.42 \(\) 0.03 & **16.31**\(\) 0.03 \\  MGPO-explore & -0.71 \(\) 0.03 & -14.74 \(\) 0.49 & 0.47 \(\) 0.05 & 0.48 \(\) 0.01 & 15.82 \(\) 0.07 \\ MGPO-UCB & -0.41 \(\) 0.10 & -14.21 \(\) 0.19 & 0.63 \(\) 0.02 & **0.58**\(\) 0.03 & 15.66 \(\) 0.14 \\ MGPO-BPE & **-0.38**\(\) 0.05 & -14.86 \(\) 1.32 & 0.65 \(\) 0.02 & 0.57 \(\) 0.01 & 15.65 \(\) 0.06 \\   

Table 2: Performance of MGPO with different prompt optimization methods. Each result shows the average performance on all test tasks in the environment and the standard deviation across 3 random seeds for online test.

   Method & MGPO-GRIPS & MGPO-BBT & MGPO-UCB & MGPO-BPE \\  Noisy observations & 0.29 (-0.25) & 0.42 (-0.32) & **0.49 (-0.14)** & 0.45 (-0.20) \\ Noisy actions & 0.12 (-0.42) & 0.19 (-0.55) & **0.30 (-0.33)** & 0.28 (-0.37) \\   

Table 3: Performance of prompts optimized with different methods in Kitchen with noisy observations or actions. Each result shows the average performance on all test tasks and the decrease compared with the environment without noise.

when the robotic environment Kitchen has noisy observations or actions. It reveals that prompts with BBT and GRIPS are particularly susceptible to environment perturbation, hinting at out-of-distribution prompts. In contrast, prompts with our proposed methods maintain robust performance facing perturbation.

Figure 2 visualizes goals in prompts optimized by different methods in MazeRunner-15. It clearly showcases the interpretability of the prompt with our proposed methods, which represents meaningful waypoints toward the task goal. In contrast, prompts with GRIPS and BBT lack this level of clarity and interpretability.

### Ablation Study

In addition to MGPO-explore, we conduct more ablation studies to examine the impact of components in MGPO. The results of ablation studies are presented in Table 4.

**Maximal Prompt Lengths:** We study the impact of different maximal prompt lengths \(K\) during online adaptation. Complex tasks often necessitate multiple goals within a prompt to specify varying behaviors in long-horizon trajectories. We observe that optimizing prompts with increasing length consistently improves performance. Specifically, a length of \(K=5\) yields the best results, and reducing \(K\) to 1 results in performance akin to that of Goal-conditioned BC. Appendix B.2 shows additional results when further increasing \(K\).

**UCB Hyperparameters:** UCB includes a hyperparameter \(c\) that balances exploration and exploitation (Appendix E.1). In our test with MGPO-UCB under varying values of \(c\), we observe that the performance exhibits low sensitivity to this hyperparameter's selection, as shown in Table 4.

**Dataset Quality:** We also assess the influence of dataset quality on MGPO's performance. Detailed results are provided in Appendix B.3.

### Visualization and Case Study

In MazeRunner, we visualize the evolution of MGPO during online adaptation. Figure 3 displays the progression of both the prompt and policy behavior at various stages. At the beginning, the policy is

   Ablation & MazeRunner-15 & MazeRunner-30 & Kitchen \\  \(K=1\) & -2.63\(\)0.00 & -27.06\(\)0.00 & 0.09\(\)0.00 \\ \(K=2\) & -1.13\(\)0.10 & -19.57\(\)1.21 & 0.28\(\)0.00 \\ \(K=3\) & -0.77\(\)0.04 & -17.12\(\)0.47 & 0.48\(\)0.02 \\ \(K=5\) & -0.41\(\)0.10 & -14.21\(\)0.19 & 0.63\(\)0.02 \\  UCB-0 & -0.44\(\)0.07 & -15.86\(\)0.51 & 0.63\(\)0.05 \\ UCB-1 & -0.41\(\)0.10 & -14.21\(\)0.19 & 0.63\(\)0.02 \\ UCB-10 & -0.40\(\)0.04 & -14.70\(\)0.66 & 0.66\(\)0.02 \\   

Table 4: Results of ablation study on MGPO with varying maximal prompt length \(K\) and hyperparameter \(c\) in UCB.

Figure 2: Visualization of the optimized prompts in four MazeRunner-15 tasks. The prompt with each method is displayed in a unique color. \(S\) and \(T\) represent the start position and the task goal respectively, and \(g_{1},g_{2},...\) represents goal positions in the prompt. We display the goals that exceed the maze boundaries in the gray bar on the right side.

conditioned on a short prompt, resulting in high action uncertainty and exploratory behavior, where the policy explores for diverse trajectories toward the task goal. This exploratory phase is crucial, as it discovers trajectories with higher returns toward the goal and helps in understanding the unknown task environment. As the algorithm progresses, the prompts are iteratively refined based on returns from the environment, and the policy behavior is steered towards more efficient paths. More results are presented in Appendix B.4.

## 5 Related Work

**Policy Pre-Training for RL** is a topic studying learning from datasets to enhance the efficiency of RL on new tasks. Offline meta-RL (OMRL) focuses on learning to adapt to new tasks with a few samples, employing context-based learning methods [26; 59; 39] or gradient-based meta-learning . However, OMRL necessitates multi-task datasets for training, requiring extensive trajectory collection within each task. On the other hand, task-agnostic pre-training leverages rich behaviors in task-agnostic datasets, exploring imitation learning [43; 4], offline RL [24; 54], or hierarchical skill learning [38; 45; 60] for policy pre-training. Our work addresses the challenging setting of task-agnostic pre-training for long-horizon tasks. Unlike previous methods that rely on RL for online adaptation, our proposed multi-goal Transformer enables RL-free optimization methods, enhancing sample efficiency.

**Transformers for RL.** The Transformer architecture has become increasingly popular in RL for its ability in sequence modeling and long-term memory. In offline RL, Decision Transformers [8; 22; 14; 57] recast RL as a sequence modeling problem conditioned on return-to-go. In context-based meta-RL, Transformers are employed to handle multiple trajectories, modeling task adaptation as in-context learning [23; 56; 29]. In multi-task RL, large-scale Transformers are adopted with extensive datasets to address complex robotic domains [6; 5] and generalization to various tasks [44; 49; 15]. In our work, we utilize Transformers to address the demands of long-term memory in modeling long-horizon trajectories with multiple goals.

**Prompt Optimization** for pre-trained Transformer-based language models has demonstrated its effectiveness in adapting to downstream NLP tasks without tuning model parameters. It optimizes the prompt, which is a sequence of tokens input to the model specifying the task. In the recent large language models (LLMs) , task adaptation can be easily achieved by prompt design, with the techniques of in-context learning  and chain-of-thought reasoning . For earlier language models, prompt-tuning methods optimize prompts in a continuous space [25; 27], while prompt search methods focus on optimizing discrete prompt tokens for interpretability [9; 40]. We adopt the concept of prompt optimization to address online adaptation in RL, focusing on optimizing the goal sequence for the pre-trained Transformer. Unlike in NLP tasks, our approach places a unique emphasis on sample efficiency, a crucial aspect given the necessity of online evaluation.

Figure 3: Visualization of the optimized prompts and state visitation in the adaptation stage of MGPO-UCB. Red squares with \(S\), \(T\), \(g_{i}\) represent the start positions, task goals, and the optimized prompts, respectively. We use purple to display the visitation frequency of each location, with darker shades indicating higher frequencies. In this example, initially, the agent explores the left half of the maze, aiming for the front-left task goal but is hindered by walls. In exploration, as it discovers rewarding routes to the right, MGPO-UCB adapts by sampling new prompts from these better paths. An optimized prompt is achieved after 40 episodes.

Conclusion and Limitations

We propose MGPO, a novel framework for policy pre-training to enhance online adaptation in unseen long-horizon tasks. By integrating the strengths of Transformer architectures and goal-conditioned policies during pre-training, MGPO enables an efficient prompt optimization process in the online adaptation phase. Our extensive experimental results across various environments show MGPO's superiority over existing methods. A comparative analysis of different prompt optimization techniques highlights the advantages on interpretability and robustness of our method over other contemporary approaches.

The effectiveness of MGPO in solving long-horizon tasks opens new possibilities for real-world applications where efficient online adaptation is crucial. However, our experiments have been confined to simulated environments so far. Future research can focus on scaling MGPO to larger pre-training datasets and testing it in more complex, real-world environments.

Like many offline RL approaches, the performance of MGPO is influenced by the quality of the dataset used during pre-training. To address this limitation, future work could explore several potential directions: incorporating online data collection to improve dataset quality, using offline RL methods such as DT to train the multi-goal Transformer, and integrating MGPO with finetuning methods to enhance its adaptability.

The prompt-based policy in MGPO, similar to language models, can exhibit unpredictable behavior when encountering out-of-distribution prompts. Furthermore, even minor alterations to the prompt may lead to unintended behaviors, raising safety and robustness challenges. Future efforts could explore enhancing the robustness of prompt-based policies.

#### Acknowledgments

This work was supported by NSFC under Grant 62450001 and 62476008. The authors would like to thank the anonymous reviewers for their valuable comments and advice.