# InstructG2I: Synthesizing Images from Multimodal Attributed Graphs

Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han

Department of Computer Science

University of Illinois at Urbana-Champaign

bowenj4@illinois.edu

https://instructg2i.github.io/

###### Abstract

In this paper, we approach an overlooked yet critical task _Graph2Image_: generating images from multimodal attributed graphs (MMAGs). This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions. To address these challenges, we propose a graph context-conditioned diffusion model called InstructG2I. InstructG2I first exploits the graph structure and multimodal information to conduct informative neighbor sampling by combining personalized page rank and re-ranking based on vision-language features. Then, a GraphQFormer encoder adaptively encodes the graph nodes into an auxiliary set of _graph prompts_ to guide the denoising process of diffusion. Finally, we propose graph classifier-free guidance, enabling controllable generation by varying the strength of graph guidance and multiple connected edges to a node. Extensive experiments conducted on three datasets from different domains demonstrate the effectiveness and controllability of our approach. The code is available at https://github.com/PeterGriffinJin/InstructG2I.

## 1 Introduction

This paper investigates an overlooked yet critical source of information for image generation: the pervasive _graph-structured relationships_ of real-world entities. In contrast to the commonly adopted language conditioning in models represented by Stable Diffusion , graph connections have _combinatorial complexity_ and cannot be trivially captured as a sequence. Such graph-structured relationships among the entities are expressed through "_Multimodal Attributed Graphs_" (MMAGs), where nodes are enriched with image and text information. As a concrete example (Figure 1(a)), the graph of artworks is constructed by nodes containing images (pictures) and texts (titles), as well as edges corresponding to shared genre and authorship. Such a graph uniquely depicts a piece of artwork by its thousands of peers in the graph, beyond the mere description of language.

To this end, we formulate and propose the _Graph2Image_ challenge, requiring the generative models to synthesize image conditioning on both text descriptions and graph connections of a node. This task featuring the image generation on MMAGs is well-grounded in real-world applications. For instance, generating an image for a virtual artwork node in the art MMAG is akin to creating virtual artwork according to the nuanced styles of artists and genres  (as in Figure 1(a)). Similarly, generating an image for a product node connected to other products through co-purchase links in an e-commerce MMAG equates to recommending future products for users . Without surprise, our exploiting the graph-structured information indeed improves the consistency of generated images compared to models only using texts or images as conditioning (Figure 1(b)).

Despite the usefulness of graph information, existing methods conditioning on either text  or images [2; 41] are incapable of direct integration with MMAGs. Therefore, we propose a graph context-aware diffusion model InstructG2I inherited from Stable Diffusion that mitigates gaps. A most prominent challenge directly originates from the combinatorial complexity of graphs, which we term as _Graph Size Explosion_: inputting the entire local subgraph structure to a model, including all the images and texts, is impractical due to the exponential increase in size, especially with additional hops. Therefore, InstructG2I learns to _compress_ the massive amounts of contexts from the graph into a set of _graph conditioning_ tokens with fixed capacity, which functions alongside the common text conditioning tokens in Stable Diffusion. Such a compression process is enhanced with a _semantic personalized pagerank-based graph sampling_ approach to actively select the most informative neighboring nodes based on both structural and semantic perspectives.

Besides the _number_ of contexts, the graph structures in MMAGs additionally specify the proximity of entities, which is not captured in conventional text or image conditioning. This challenge of "_Graph Entity Dependency_" reflects the implicit preference of image generation: synthesizing a shirt image linked to "light-colored" clothing is likely to have a "pasted tone" (image-image dependency), and generating a picture titled "a running horse" should reference interconnected animal images rather than scenic ones (text-image dependency). To enable the nuanced proximity understanding on graphs, we further improve our graph conditioning tokens via a Graph-QFormer architecture learning to encode the graph information guided by texts.

Finally, we propose that our graph conditioning is a natural interface for _controllable_ generation, reflecting the strength of edges in MMAGs. Take the virtual art generation (Figure 1(c)) for example: InstructG2I can flexibly offer different strengths of graph guidance and can smoothly transition between the style of Monet and Kandinsky, defined by its strength of connection with either of the two artists. Such an advantage is grounded for real-world application and is a _plug-and-play_ test-time algorithm inspired by classifier-free guidance . In sum, our contributions include:

* _Formulation and Benchmark_. We are the first to identify the usefulness of multimodal attributed graphs (MMAGs) in image synthesis and formulate the _Graph2Image_ problem. Our formulation is supported by three benchmarks grounded in the real-world applications of art and e-commerce.
* _Algorithm_. Methodologically, we propose InstructG2I, a context-aware diffusion model that effectively encodes graph conditional information as graph prompts for controllable image generation (as shown in Figure 1(b,c)).

Figure 1: We propose a new task _Graph2Image_ featuring image synthesis by conditioning on graph information and introduce a novel graph-conditioned diffusion model called InstructG2I to tackle this problem. (a) _Graph2Image_ is supported by prevalent multimodal attributed graphs and is grounded in real-world applications, _e.g._, virtual arity. (b) InstructG2I outperforms baseline image generation techniques, demonstrating the usefulness of graph information. (c) To accommodate realistic user queries, InstructG2I exhibits smooth controllability in utilizing text/graph information and managing the strength of multiple graph edges.

* _Experiments and Evaluation_. Empirically, we conduct experiments on graphs from three different domains, demonstrating that InstructG2I consistently outperforms competitive baselines (as shown in Figure 1(b)).

## 2 Problem Formulation

### Multimodal Attributed Graphs

**Definition 1**: _(Multimodal Attributed Graphs (MMAGs))_ A multimodal attributed graph can be defined as \(=(,,,)\), where \(\), \(\), \(\) and \(\) represent the sets of nodes, edges, images, and documents, respectively. Each node \(v_{i}\) is associated with some textual information \(d_{v_{i}}\) and some image information \(p_{v_{i}}\).

For example, in an e-commerce product graph, nodes (\(v\)) represent products, edges (\(e\)) denote co-viewed semantic relationships, images (\(p\)) are product images, and documents (\(d\)) are product titles. Similarly, in an art graph (shown in Figure 1), nodes represent artworks, edges signify shared artists or genres, images are artwork pictures, and documents are artwork titles.

In this work, we focus on graphs where edges provide _semantic correlations_ between images (nodes). For instance, in an e-commerce product graph, connected products (those frequently co-viewed by many users) are highly related. Similarly, in an art graph, linked artworks (those created by the same author or within the same genre) are likely to have similar styles.

### Problem Definition

In this work, we explore the problem of node image generation on MMAGs. Given a node \(v_{i}\) in an MMAG \(\), our objective is to generate \(p_{v_{i}}\) based on \(d_{v_{i}}\) and \(\). This problem has multiple real-world applications. For example, in e-commerce, it translates to generating the image (\(p_{v_{i}}\)) for a product (\(v_{i}\)) based on a user query (\(d_{v_{i}}\)) and user purchase history (\(\)), which is a generative retrieval task. In the art domain, it involves generating the picture (\(p_{v_{i}}\)) for an artwork (\(v_{i}\)) based on its title (\(d_{i}\)) and its associated artist style or genre (\(\)), which is a virtual artwork creation task.

**Definition 2**: _(Node Image Generation on MMAGs)_ In a multimodal attributed graph \(=(,,,)\), given a node \(v_{i}\) within the graph \(\) with a textual description \(d_{v_{i}}\), the goal is to synthesize \(p_{v_{i}}\), the corresponding image at \(v_{i}\), with a learned model \(_{v_{i}}=f(v_{i},d_{v_{i}},)\).

Our evaluation emphasizes instance-level similarity, assessing how closely \(_{v_{i}}\) matches \(p_{v_{i}}\). We conduct evaluations on artwork graphs, e-commerce graphs, and literature graphs. More details can be found in Section 4.1.

## 3 Methodology

In this section, we present our InstructG2I framework, overviewed in Figure 2. We begin by introducing graph conditions into stable diffusion models in Section 3.1. Next, we discuss semantic personalized PageRank-based sampling to select informative graph conditions in Section 3.2. Furthermore, we propose Graph-QFormer to extract dependency-aware representations for graph conditions in Section 3.3. Finally, we introduce controllable generation to balance the condition scale between text and graph guidance, as well as manage multiple graph guidances in Section 3.4.

### Graph Context-aware Stable Diffusion

**Stable Diffusion (SD).** InstructG2I is built upon Stable Diffusion (SD). SD conducts diffusion in the latent space, where an input image \(x\) is first encoded from pixel space into a latent representation \(=(x)\). A decoder then transfers the latent representation \(^{}\) back to the pixel space, yielding \(x^{}=(^{})\). The diffusion model generates the latent representation \(^{}\) conditioned on a text prompt \(c_{T}\). The training objective of SD is defined as follows:

\[=_{(x),cr,(0,1),t}[\|-_{}(_{t},t,h(c_{T}))\|^{2} ].\] (1)At each timestep \(t\), the denoising network \(_{}()\) predicts the noise by conditioning on the current latent representation \(_{t}\), timestep \(t\) and text prompt vectors \(h(c_{T})\). To compute \(h(c_{T})^{d l_{c_{T}}}\), where \(l_{c_{T}}\) is the length of \(c_{T}\) and \(d\) is the hidden dimension, the text prompt \(c_{T}\) is processed by the CLIP text encoder : \(h(c_{T})=(c_{T})\).

**Introducing Graph Conditions into SD.** In the context of MMAGs, synthesizing the image for a node \(v_{i}\) involves not only the text \(d_{v_{i}}\), but also the semantic information from the node's proximity on the graph. Therefore, we introduce an auxiliary set of _graph conditioning tokens_\(h_{G}(c_{G})\) to the SD models (as shown in Figure 2(c)), working in parallel with the existing text conditions \(h_{T}(c_{T})\).

\[h(c_{T},c_{G})=[h_{T}(c_{T}),h_{G}(c_{G})]^{d(l_{c_{T}}+l_{ c_{G}})},\] (2)

where \(l_{c_{G}}\) is the length of the graph condition. The training objective then becomes:

\[=_{(x),c_{T},c_{G}, (0,1),t}[\|-_{}(_{t},t,h(c_{T},c_{G}))\|^{2}].\] (3)

For \(h_{T}(c_{T})\), we can directly use the CLIP text encoder as in the original SD. However, determining \(c_{G}\) and \(h_{G}()\) is more complex. We will discuss the details of \(c_{G}\) and \(h_{G}()\) in the following sections.

### Semantic PPR-based Neighbor Sampling

A straightforward approach to developing \(c_{G}(v_{i})\) involves using the entire local subgraph of \(v_{i}\). However, this is impractical due to the exponential growth in size with each additional hop, leading to excessively long context sequences. To address this, we leverage both graph structure and node semantics to select informative \(c_{G}\).

**Structure Proximity: Personalized PageRank (PPR)**. Inspired by , we first adopt PPR  to identify related nodes from a graph structure perspective. PPR processes the graph structure to derive a ranking score \(P_{i,j}\) for each node \(v_{j}\) relative to node \(v_{i}\), where a higher \(P_{i,j}\) indicates a greater degree of "similarity" between \(v_{i}\) and \(v_{j}\). Let \(^{n n}\) be the PPR matrix of the graph, where each row \(P_{i,:}\) represents a PPR vector toward a target node \(v_{i}\). The matrix \(\) is determined by:

\[=}+(1-).\] (4)

where \(\) is the reset probability for PPR and \(}\) is the normalized adjacency matrix. Once \(\) is computed, we define the PPR-based graph condition \(c_{G_{}}\) of node \(v_{i}\) as the top-\(K_{}\) PPR neighbors of node \(v_{i}\):

\[c_{G_{}}(v_{i})=*{argmax}_{c_{G_{}}(v_{i}) ,|c_{G_{}}(v_{i})|=K_{}}_{v_{j} c _{G_{}}(v_{i})}P_{i,j}.\] (5)

**Semantic Proximity: Similarity-based Reranking**. However, solely relying on PPR may result in a graph condition set containing images (_e.g.,_ scenery pictures) that are not semantically related to our

Figure 2: The overall framework of InstructG2I. (a) Given a target node with a text prompt (_e.g._, _House in Snow_) in a Multimodal Attributed Graph (MMAG) for which we want to generate an image, (b) we first perform semantic PPR-based neighbor sampling, which involves structure-aware personalized PageRank and semantic-aware similarity-based reranking to sample informative neighboring nodes in the graph. (c) These neighboring nodes are then inputted into a Graph-QFormer, encoded by multiple self-attention and cross-attention layers, represented as _graph tokens_ and used to guide the denoising process of the diffusion model, together with text prompt tokens.

target node (_e.g.,_ a picture titled "running horse"). To address this, we propose using a semantic-based similarity calculation function \((d,p)\) (_e.g.,_ CLIP) to rerank \(v_{j} c_{G_{}}(v_{i})\) based on the relatedness of \(p_{v_{j}}\) to \(d_{v_{i}}\). The final graph condition \(c_{G}(v_{i})\) is calculated by:

\[c_{G}(v_{i})=*{argmax}_{c_{G}(v_{i})=c_{G_{}}(v_{i}),|c_{G }(v_{i})|=K}_{v_{j} c_{G}(v_{i})}(d_{v_{i}},p_{v_{j}}).\] (6)

### Graph Encoding with Text Conditions

After we derive \(c_{G}(v_{i})\) from the previous step, the problem comes to how can we design \(h_{G}()\) to extract meaningful representations from \(c_{G}(v_{i})\). Here we focus more on how to utilize the image features from \(c_{G}(v_{i})\) (_i.e.,_\(\{p_{v_{j}}|v_{j} c_{G}(v_{i})\}\)) since we find they are more informative for \(v_{i}\) image generation compared with text features from \(c_{G}(v_{i})\) (_i.e.,_\(\{d_{v_{j}}|v_{j} c_{G}(v_{i})\}\)) (shown in Section 4.3).

**Simple Baseline: Encoding with Pretrained Image Encoders .** A straightforward way to obtain representations for \(v_{j} c_{G}(v_{i})\) is to directly apply some pretrained image encoders \(g_{}()\) (_e.g._, CLIP ):

\[_{v_{j}}=g_{}(p_{v_{j}})^{d},\ \ h_{G}(c_{G}(v_{i}))= [_{v_{j}}]_{v_{j} c_{G}(v_{i})}^{d l_{c_{G} }},\] (7)

where \(\) denotes the concatenation operation. However, this simple design has two significant limitations: 1) The encoding for each \(p_{v_{j}}(v_{j} c_{G}(v_{i}))\) is isolated from others in \(c_{G}(v_{i})\) and failed to capture the image-image graph dependency. For example, the style extraction from one picture (\(p_{v_{j}}\)) can benefit from the other pictures created by the same artist (in \(c_{G}(v_{i})\)). 2) The encoding for each \(p_{v_{j}}\) is independent to \(d_{v_{i}}\), which fails to capture the text-image graph dependency. For example, when we are creating a picture titled "running horse" (\(d_{v_{i}}\)), it is desired to offer more weight on horse pictures in \(c_{G}(v_{i})\) rather than scenery pictures.

**Graph-QFormer.** To address these limitations, we propose Graph-QFormer as \(h_{G}()\) to learn representations for \(c_{G}\) while considering the graph dependency information. As shown in Figure 2, Graph-QFormer consists of two Transformer  modules motivated by : (1) a self-attention module that facilitates deep mutual information exchange between previous layer hidden states, capturing image-image dependencies and (2) a cross-attention module that weights samples in \(c_{G}\) using text guidance, capturing text-image dependencies.

Let \(^{(t)}_{c_{G}(v_{i})}^{d l_{c_{G}}}\) denote the hidden states outputted by the \(t\)-th Graph-QFormer layer. We use the token embeddings of \(d_{v_{i}}\) as the input query embeddings to provide text guidance:

\[^{(0)}_{c_{G}(v_{i})}=[_{1},...,_{|d_{v_{i}}|}].\] (8)

where \(_{k}\) is the \(k\)-th token embedding in \(d_{v_{i}}\) and \(l_{c_{G}}=|d_{v_{i}}|\). The multi-head self-attention layer (MHA\({}_{}\)) is calculated by

\[^{(t)}_{c_{G}(v_{i})}=_{}[q=^{(t-1)}_{c _{G}(v_{i})},k=^{(t-1)}_{c_{G}(v_{i})},v=^{(t-1)}_{c_{G}(v_{i})}],\] (9)

where \(q,k,v\) denotes query, key, and value channels in the Transformer. The output \(^{(t)}_{c_{G}(v_{i})}\) is then inputted to the multi-head cross-attention layer (MHA\({}_{}\)), calculated by

\[^{(t)}_{c_{G}(v_{i})}=_{}[q=^{(t)}_{c_{G }(v_{i})},k=_{c_{G}(v_{i})},v=_{c_{G}(v_{i})}],\] (10)

where \(_{c_{G}(v_{i})}=[g_{}(p_{v_{j}})]_{v_{j} c_{G}(v_{i} )}^{d n}\) represents the image embeddings extracted from a fixed pretrained image encoder and \(n\) is the number of embeddings. Finally we adopt \(h_{G}(c_{G}(v_{i}))=^{(L)}_{c_{G}(v_{i})}\), where \(L\) is the number of layers in Graph-QFormer.

**Connection between InstructG21 and GNNs.** As illustrated in Figure 2, InstructG2I employs a Transformer-based architecture as the graph encoder. However, it can also be interpreted as a Graph Neural Network (GNN) model. GNN models  primarily use a propagation-aggregation paradigm to obtain node representations (\((i)\) denotes the neighbor set of \(i\)):

\[^{(l-1)}_{ij}=^{(l)}(^{(l-1)}_{i},^{(l-1)}_ {j}),( j(i));\ ^{(l)}_{i}=^{(l)}(^{(l-1)}_{i},\{^{(l-1)} _{ij}|j(i)\}).\]

Similarly, in InstructG2I, Eq.(4)(5)(6) can be regarded as the propagation function \(^{(l)}\), while the aggregation step \(^{(l)}\) corresponds to the combination of Eq.(9) and Eq.(10).

### Controllable Generation

The concept of classifier-free guidance, introduced by , enhances the performance of conditional image synthesis by modifying the noise prediction, \(e_{}()\), with the output from an unconditional model. This is formulated as: \(_{}(_{t},c)=_{}(_{t}, )+s(_{}(_{t},c)-_{}( _{t},))\), where \(s(>1)\) is the guidance scale. The intuition is that \(_{}\) learns the gradient of the log image distribution and increasing the contribution of \(_{}(c)-_{}()\) will enlarge the convergence to the distribution conditioned on \(c\).

In our task, the score network \(_{}(_{t},c_{G},c_{T})\) is conditioned on both text \(c_{T}=d_{i}\) and the graph condition \(c_{G}\). We compose the score estimates from these two conditions and introduce two guidance scales, \(s_{T}\) and \(s_{G}\), to control the contribution strength of \(c_{T}\) and \(c_{G}\) to the generated samples respectively. Our modified score estimation function is:

\[_{}(_{t},c_{G},c_{T}) =_{}(_{t},,)+s_{T} (_{}(_{t},,c_{T})-_{}( _{t},,))\] \[+s_{G}(_{}(_{t},c_{G},c_{T})- _{}(_{t},,c_{T})).\] (11)

For cases requiring fine-grained control over multiple graph conditions (_i.e._, different edges), we extend the formula as follows:

\[_{}(_{t},c_{G},c_{T}) =_{}(_{t},,)+s_{T} (_{}(_{t},,c_{T})-_{}( _{t},,))\] \[+ s_{G}^{(k)}(_{}(_{t},c_{G}^{( k)},c_{T})-_{}(_{t},,c_{T})),\] (12)

where \(c_{G}^{(k)}\) is the \(k\)-th graph condition. For example, to create an artwork that combines the styles of Monet and Van Gogh, the neighboring artworks by Monet and Van Gogh on the graph would be \(c_{G}^{(1)}\) and \(c_{G}^{(2)}\), respectively. Further details on the derivation of our classifier-free guidance formulations can be found in Appendix A.3.

## 4 Experiments

### Experimental Setups

**Datasets.** We conduct experiments on three MMAGs from distinct domains: ART500K , Amazon , and Goodreads . ART500K is an artwork graph with nodes representing artworks and edges indicating same-author or same-genre relationships. Each artwork node includes a title (text) and a picture (image). Amazon is a product graph where nodes represent products and edges denote co-view relationships. Each product is associated with a title (text) and a picture (image). Goodreads is a literature graph where nodes represent books and edges convey similar-book semantics. Each book node contains a title and a front cover image. Dataset statistics can be found in Appendix A.4.

**Baselines.** We compare InstructG2I with two groups of baselines: 1) Text-to-image methods: This includes Stable Diffusion 1.5 (SD-1.5)  and SD 1.5 fine-tuned on our datasets (SD-1.5 FT). 2) Image-to-image methods: This includes InstructPix2Pix  and ControlNet , both initialized with SD 1.5 and fine-tuned on our datasets. We use the most relevant neighbor, as selected in Section 3.2 as the input image for these baselines, allowing them to partially utilize graph information.

**Metrics.** As indicated in Section 2.2, our evaluation mainly concerns the consistency of synthesized images with the ground truth image on the node. Therefore, our evaluation adopts the CLIP  and DINOv2  score for instance-level similarity, in addition to the conventional FID  metric for image generation. For the CLIP and DINOv2 scores, we utilize CLIP and DINOv2 to obtain representations for both the generated and ground truth images and then calculate their cosine similarity. For FID, we calculate the distance between the distribution of the ground truth images and the distribution of the generated images.

### Main results

**Quantitative Evaluation.** The quantitative results are presented in Table 1 and Figure 3. From Table 1, we observe the following: 1) InstructG2I consistently outperforms all the baseline methods, highlighting the importance of graph information in image synthesis on MMAGs. 2) Although InstructPix2Pix and ControlNet partially consider graph context, they fail to capture the semantic signals from the graph comprehensively. In Figure 3, we plot the average DINOv2 (x-axis, \(\)) and FID score (y-axis, \(\)) across the three datasets. InstructG2I outperforms most baselines on both metrics and achieves the best trade-off between them. InstructPix2Pix obtains a better FID score than InstructG2I because it takes an in-distribution image as input, constraining the output image to stay close to the original distribution.

**Qualitative Evaluation.** We conduct a qualitative evaluation by randomly selecting some generated cases. The results are shown in Figure 4, where we provide the sampled neighbor images from the graph, text prompts, and the ground truth images. From these results, we observe that InstructG2I generates images that best fit the semantics of the text prompt and context from the graph. For instance, when generating a picture for "the crater and the clouds", the baselines either capture only the content ("crater" and "clouds") without the style learned from the graph (Stable Diffusion and InstructPix2Pix) or adopt a similar style but lose the desired content (ControlNet). In contrast, InstructG2I effectively learns from the neighbors on the graph and conveys the content accurately.

    &  &  &  \\  Model & CLIP score & DINOv2 score & CLIP score & DINOv2 score & CLIP score & DINOv2 score \\  SD-1.5 & 58.83 & 25.86 & 60.67 & 32.61 & 42.16 & 14.84 \\ SD-1.5 FT & 66.55 & 34.65 & 65.30 & 41.52 & 45.81 & 18.97 \\  InstructPix2Pix & 65.66 & 33.44 & 63.86 & 41.31 & 47.30 & 20.94 \\ ControlNet & 64.93 & 32.88 & 59.88 & 34.05 & 42.20 & 19.77 \\  InstructG2I & **73.73** & **46.45** & **68.34** & **51.70** & **50.37** & **25.54** \\   

Table 1: Quantitative evaluation of different methods on ART500K, Amazon, and Goodreads datasets. The CLIP score denotes the image-image score. **InstructG2I significantly outperforms the best baseline** with p-value < 0.05 and consistently outperforms all the other common baselines in image synthesis, supporting the benefits of graph conditioning.

Figure 4: Qualitative evaluation. **Our method exhibits better consistency with the ground truth** by better utilizing the graph information from neighboring nodes (“Sampled Neighbors” in the figure).

Figure 3: **InstructG2I achieves the best trade-off between DINOv2 (\(\)) and FID (\(\)) scores.**

### Ablation Study

**Study of Graph Condition for SD Variants.** In InstructG2I, we introduce graph conditions into SD by encoding the images from \(c_{G}\) into graph prompts, which serve as conditions together with text prompts for SD's denoising step. In this section, we demonstrate the significance of this design by comparing it with other variants that utilize graph conditions in SD: InstructPix2Pix (IP2P) with neighbor images and SD finetuned with neighbor texts. For the first variant, we perform mean pooling on the latent representations of images in \(c_{G}\), according to the IP2P's setting, and use this as the input image representation for IP2P. This variant has the same input information as InstructG2I. For the second variant, we utilize text information from neighbors instead of images, concatenate it with the text prompt, and fine-tune the SD. The results are shown in Table 2, where InstructG2I consistently outperforms both variants. This demonstrates the advantage of leveraging image features from \(c_{G}\) and the effectiveness of our model design.

**Study of Graph-QFormer.** We first demonstrate the effectiveness of Graph-QFormer by replacing it with the simple baseline mentioned in Eq.(7), denoted as "- Graph-QFormer". We then compare it with graph neural network (GNN) baselines including GraphSAGE  and GAT , integrated into InstructG2I in the same manner. The results, presented in Table 2, show that InstructG2I with Graph-QFormer consistently outperforms both the ablated version and GNN baselines. This demonstrates the effectiveness of Graph-QFormer design.

**Study of the Semantic PPR-based Neighbor Sampling.** We propose a semantic PPR-based sampling method that combines structure and semantics for neighbor sampling on graphs, as detailed in Section 3.2. In this section, we demonstrate the effectiveness of this approach by conducting ablation studies that remove either or both components. The results, shown in Figure 5, indicate that our sampling methods effectively identify neighbor images that contribute most significantly to the ground truth in both semantics and style. This underscores the value of integrating both structural and semantic information in our sampling approach.

    &  &  &  \\  Model & CLIP score & DINov2 score & CLIP score & DINov2 score & CLIP score & DINov2 score \\  InstructG2I & **73.73** & **46.45** & **68.34** & **51.70** & **50.37** & **25.54** \\ - Graph-QFormer & 72.53 & 44.16 & 66.97 & 48.18 & 47.91 & 24.74 \\ + GraphSAGE & 72.26 & 43.06 & 66.07 & 43.40 & 46.68 & 21.91 \\ + GAT & 72.60 & 43.32 & 66.73 & 46.58 & 46.57 & 21.45 \\  IP2P w. neighbor images & 65.89 & 33.90 & 63.19 & 40.32 & 47.21 & 21.55 \\ SD FT w. neighbor texts & 69.72 & 38.64 & 65.55 & 43.51 & 47.47 & 22.68 \\   

Table 2: Ablation study on graph condition variants and Graph-QFormer.

Figure 5: Ablation study on semantic PPR-based neighbor sampling. The results indicate that both structural and semantic relevance proposed by our method effectively improve the image generation quality and consistency with the graph context.

### Controllable Generation

Text Guidance & Graph Guidance.In Eq.(11), we discuss the control of guidance from both text and graph conditions. To illustrate its effectiveness, we provide an example in Figure 6(a). The results show that as text guidance increases, the generated image incorporates more of the desired content. Conversely, as graph guidance increases, the generated image adopts a more desired style. This demonstrates the ability of our method to balance content and style through controlled guidance.

Multiple Graph Guidance: Virtual Artist.In Eq.(12), we demonstrate how multiple graph guidance can be managed for controllable image generation. We present a use case, virtual artwork creation, to showcase its effectiveness (shown in Figure 6(b)). The goal of this task is to create an image that depicts specific content (_e.g._, a man playing piano) in the style of one or more artists (_e.g._, Picasso and Courbet). This is akin to adding a new node to the graph that links to the artwork nodes created by the specified artists and generating an image for this node. The results indicate that when single graph guidance is provided, the generated artwork aligns with that artist's style. As additional graph guidance is introduced, the styles of the two artists blend together. This demonstrates that our method offers the flexibility to meet various control requirements, effectively balancing different types of graph influences.

### Model Behavior Analysis

Cross-attention Weight Study in Graph-QFormer.We conduct a cross-attention study for Graph-QFormer to understand how different sampled neighbors on the graph are selected based on the text prompt and contribute to the final image generation. We randomly select a case with the text prompt and neighbor images and plot the cross-attention weight map shown in Figure 7. From the weight map, we can find that Graph-QFormer learns to assign higher weight to pictures 1 and 4 which are related to "raising" and "Lazarus" in the text prompt respectively. The results indicate that Graph-QFormer effectively learns to select the images that are most relevant to the text prompt.

## 5 Related works

Diffusion Models.Recent advancements in diffusion models have demonstrated significant success in generative applications. Diffusion models [4; 7] generate compelling examples through a step-wise denoising process, which involves a forward process that introduces noise into data distributions and a reverse process that reconstructs the original data . A notable example is the Latent Diffusion Model (LDM) , which reduces computational costs by applying the diffusion process

Figure 6: Controllable generation study. (a) The ability of InstructG2I to balance text guidance and graph guidance. (b) Study of multiple graph guidance. Generated artworks with the input text prompt “a man playing piano” conditioned on single or multiple graph guidance (styles of “Picasso” and “Courbet”). Please refer to Figure 1 for another example between Monet and Kandinsky.

in a low-resolution latent space. In the domain of diffusion models, various forms of conditioning are employed to direct the generation process, including labels , classifiers , texts , images , and scene graphs . These conditions can be incorporated into diffusion models through latent concatenation , cross-attention , and gradient control . However, most existing works neglect the relational information between images and cannot be directly applied to image synthesis on MMAGs.

**Learning on Graphs.** Early studies on learning on graphs primarily focus on representation learning for nodes or edges based on graph structures [3; 14]. Methods such as Deepwalk  and Node2vec  perform random walks on graphs to derive vector representation for each node. Graph neural networks (GNNs) [38; 43] are later introduced as a learnable component that incorporates both initial node features and graph structure. GNNs have been applied to various tasks, including classification , link prediction , and recommendation . For instance, GraphSAGE  employs a propagation and aggregation paradigm for node representation learning, while GAT  introduces an attention mechanism into the aggregation process. Recently, research has increasingly focused on integrating text or image features with graph structures [22; 44]. For example, Patton  proposes pretraining language models on text-attributed graphs. However, these existing works mainly target representation learning on single-modal graphs and are not directly applicable to the image synthesis from multimodal attributed graph (MMAG) task addressed in this paper.

## 6 Conclusions

In this paper, we identify the problem of image synthesis on multimodal attributed graphs (MMAGs). To address this challenge, we propose a graph context-conditioned diffusion model that: 1) Samples related neighbors on the graph using a semantic personalized PageRank-based method; 2) Effectively encodes graph information as graph prompts by considering their dependency with Graph-QFormer; 3) Generates images under control with graph classifier-free guidance. We conduct systematic experiments on MMAGs in the domains of art, e-commerce, and literature, demonstrating the effectiveness of our approach compared to competitive baseline methods. Extensive studies validate the design of each component in InstructG2I and highlight its controllability. Future directions include joint text and image generation on MMAGs and capturing the heterogeneous relations between image and text units on MMAGs.