# Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models

Ce Zhang Simon Stepputtis Katia Sycara Yaqi Xie

School of Computer Science, Carnegie Mellon University

{cezhang, sstepput, katia, yaqix}@cs.cmu.edu

###### Abstract

Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios. Recently, researchers have applied this setting to advanced pre-trained vision-language models (VLMs), developing approaches such as test-time prompt tuning to further extend their practical applicability. However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed. To address this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation approach for VLMs that effectively _accumulates_ task-specific knowledge from _multi-modalities_. Specifically, we create and evolve two sets of prototypes--textual and visual--to progressively capture more accurate multi-modal representations for target classes during test time. Moreover, to promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities. Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency. Code is available at https://github.com/zhangce01/DPE-CLIP.

## 1 Introduction

Although deep learning models have achieved great success in various machine learning tasks , they often suffer from significant performance degradation due to distribution shifts between the training data from the source domain and the testing data from the target domain . To address this challenge, a number of works  adopt the transductive learning principle, assuming access to both labeled source data and unlabeled target data--a scenario known as the _domain adaptation_ setting. However, this setting contrasts with most practical scenarios, where we only have access to a well-trained model and cannot re-access the source data due to privacy or data retention policies. In response, researchers have proposed _test-time adaptation_, which leverages only the unlabeled target data stream to adapt the model to out-of-distribution domains .

Recently, large-scale vision-language models (VLMs), such as CLIP  and ALIGN , have garnered increasing attention in the research community. These models, pre-trained on massive web-scale datasets, exhibit remarkable zero-shot capabilities and open-world visual understanding . While the large-scale pre-trained (source) datasets like LAION-5B  are accessible, it is impractical for individuals to train on them due to their immense size. Consequently, adapting VLMs to downstream tasks via efficient fine-tuning with limited annotated samples from the target domain has become a focus of recent research . However, although these methods have proven effective, they pose a significant limitation: they assume the availability of annotated samples from the target domain, which is often not practical in real-world scenarios. This constraint hinders the broader deployment of VLMs in diverse and dynamic environments .

To address the label scarcity problem in practice, a number of approaches apply the _test-time adaptation_ setting to the domain of adapting VLMs to downstream tasks, as shown in Figure 1. Specifically, Shu _et al_.  propose test-time prompt tuning to learn an adaptive prompt for each individual sample in the test data stream to enhance CLIP's zero-shot generalizability to out-of-distribution domains. Building on TPT, DiffTPT  incorporates diffusion-based data augmentations to facilitate more effective prompt tuning during test time. More recently, Karmanov _et al_.  propose an alternative training-free dynamic adapter approach to establish dynamic visual caches with the unlabeled test samples.

However, we recognize that existing works overlook the following inherent properties of _test-time adaptation_ in VLMs: (1) _Cumulative._ We expect that with more seen samples, the performance should improve as task-specific knowledge accumulates [40; 57]. However, test-time prompt tuning methods [54; 13] treat each test instance independently, resetting to the original model for each new sample, failing to extract historical knowledge from previous test samples. (2) _Multi-modal._ Effective adaptation of VLMs benefits from leveraging knowledge from both textual and visual modalities [28; 35]. However, previous works only capture domain-specific knowledge from a single modality, adapting CLIP based solely on textual [54; 13] or visual  feature refinement.

To this end, we propose Dual Prototype Evolving (DPE), a novel test-time VLM adaptation approach that effectively _accumulates_ task-specific knowledge from _multi-modalities_, as illustrated in Figure 1. Unlike previous methods that focus on adapting VLMs from a single modality, we create and evolve two sets of prototypes--textual and visual--progressively capturing more accurate multi-modal representations for target classes during test time. To extract historical knowledge from previous test samples, we update these two sets of prototypes online using cumulative average and priority queue strategies, respectively. We further optimize these multi-modal prototypes by introducing learnable residual parameters for each individual test sample to enhance the zero-shot generalization capability of our model. Specifically, rather than solely relying on the entropy minimization objective [62; 79], our DPE also accounts for the alignment between multi-modal prototypes to ensure consistent multi-modal representations. Notably, our DPE requires only the optimization of multi-modal prototypes in the embedding space during test time, eliminating the need to backpropagate gradients through the textual encoder of CLIP, as required in TPT  and DiffTPT .

The test-time generalization capabilities of our proposed DPE method are extensively evaluated across 15 diverse recognition datasets in two scenarios: natural distribution shifts and cross-dataset generalization. The experimental results validate the superior performance of our DPE, which achieves an average improvement of 3.55% and 4.30% over the state-of-the-art TPT  method in these scenarios. Moreover, our proposed DPE achieves this performance while also exhibiting \(5\) and over \(10\) test-time efficiency compared to TPT  and DiffTPT , respectively.

Figure 1: **Comparison of our DPE with zero-shot CLIP , TPT , and TDA . We denote CLIPâ€™s parallel textual and visual encoders as \(_{t}\) and \(_{v}\), respectively. While previous methods solely adapt the CLIP model from a single modality, we design our DPE to evolve prototypes from both textual and visual modalities to progressively capture more accurate multi-modal representations for target classes during test time.**

The contributions of this paper are summarized as follows:

* We propose dual prototype evolving (DPE), a novel test-time adaptation method for VLMs that _progressively_ captures more accurate _multi-modal_ representations for target classes during test time.
* To promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes across modalities.
* Experimental evaluations demonstrate that our DPE consistently outperforms current state-of-the-art methods across 15 diverse datasets while maintaining competitive computational efficiency.

## 2 Related Work

**Vision-Language Models**. Leveraging vast image-text pairs from the Internet, recent large-scale vision-language models (VLMs), such as CLIP  and ALIGN , have shown remarkable and transferable visual knowledge through natural language supervision [78; 11; 10]. These VLMs enable a "pre-train, fine-tune" paradigm for performing downstream visual tasks, such as image recognition [46; 17; 36], object detection [67; 66], and depth estimation [80; 24; 73].

To effectively transfer VLMs to these downstream tasks, researchers have developed two primary methods for adapting the model with few-shot data: prompt learning methods [85; 84; 28; 51; 86; 5] and adapter-based methods [81; 14; 77; 71; 31]. Specifically, prompt learning methods, such as CoOp  and CoCoOp , focus on learning input prompts with few-shot supervision from downstream data. On the other hand, adapter-based methods, like Tip-Adapter  and TaskRes , modify the extracted visual or textual representations directly to enhance model performance. However, these approaches often assume the availability of labeled samples from the target domain, which can limit their effectiveness in real-world scenarios. In this work, we address the challenge of test-time adaptation, where the model is required to adapt solely at test time without access to any training samples or ground-truth labels from the target domain. This setting is crucial for real-world deployment, as it allows for robust performance in novel and unseen environments where labeled data cannot be obtained in advance.

**Test-Time Adaptation**. To effectively transfer a model trained on the source domain to the target domain, test-time adaptation methods [62; 79; 60; 3; 59] aim to adjust the model online using a stream of unlabeled test samples. These methods enable the deployment of well-trained models in various out-of-distribution scenarios, thereby enhancing the applicability and reliability of machine learning models in real-world applications [34; 29; 42]. Researchers have applied test-time adaptation techniques successfully across various machine learning tasks, including semantic segmentation [23; 52; 83], human pose estimation [33; 26], and image super-resolution [53; 8].

Recently, increasing research efforts have focused on adapting large-scale VLMs during test time [38; 56; 1; 72; 82; 76]. As the seminal work, Shu _et al._ firstly propose test-time prompt tuning (TPT), which enforces consistency across different augmented views of each test sample. Building on this approach, several subsequent studies have sought to further enhance TPT. For instance, DiffTPT  utilizes diffusion-based augmentations to increase the diversity of augmented views, while C-TPT  addresses the rise in calibration error during test time prompt tuning. Unlike these approaches, which treat each test sample independently, TDA  establishes positive and negative visual caches during test time, enhancing model performance as more samples are processed. Similarly, recent DMN  utilizes a dynamic memory to gather information from historical test data. However, these methods solely adapt the model from a single modality perspective, limiting their effectiveness in capturing task-specific knowledge from out-of-distribution domains. Given this, we design DPE to evolve two sets of prototypes from both textual and visual modalities to progressively capture more accurate multi-modal representations for target classes during test time.

## 3 Method

We introduce Dual Prototype Evolving (DPE) as illustrated in Figure 2, to enhance CLIP's zero-shot generalization capabilities across diverse distributions during test time. Unlike previous methods that focus solely on one modality, we design two sets of prototypes, textual and visual, which are progressively updated using the unlabeled test dataset \(_{}\).

### Preliminaries

**Zero-Shot CLIP**. CLIP  utilizes two pre-trained parallel encoders: a visual encoder \(_{v}()\) and a textual encoder \(_{t}()\), which embed images and text descriptions into a shared embedding space \(^{d}\). For a \(C\)-class classification task, CLIP performs zero-shot predictions by computing the similarities between the extracted image feature and the \(C\) candidate text features, written as

\[f_{v}=_{v}(X_{}), f_{t_{c}}=_{t}( _{c}),_{}(y=y_{c}|X_{})= },f_{v})/t)}{_{t^{}} ((f_{t^{}},f_{v})/t)},\] (1)

where \(X_{}_{}\) denotes the input test image, and \(_{c}\) represents the the class-specific description input for class \(y_{c}\). The pairwise similarities \((,)\) are calculated using cosine similarity, and \(t\) represents the temperature parameter in the softmax function.

**Test-Time Prompt Tuning**. To enhance the zero-shot generalizability of CLIP, TPT  proposes learning an adaptive prompt using the test stream samples. Specifically, for each test sample \(X_{}\), TPT generates \(N\) augmented views \(\{_{n}(X_{})\}_{n=1}^{N}\) and averages the top \(\)-percentile confident predictions based on an entropy threshold \(\) to obtain the final prediction:

\[_{}(X_{})=_{n=1}^{N} [(_{}(_{n}(X_{ }))]_{}(_{n}(X_{ })).\] (2)

Here, \((p)=-_{i=1}^{C}p_{i} p_{i}\) calculates the self-entropy of the prediction \(p\). The objective of TPT is to optimize the learnable prompt to minimize the self-entropy of the final prediction, _i.e_., \((_{}(X_{}))\).

### Dual Prototype Evolving

In our DPE method, we construct and iteratively evolve two sets of class-specific prototypes from both visual and textual modalities to achieve a more precise representation of each class over time.

**Textual Prototype Evolution**. In this work, we follow CLIP  to use multiple context prompt templates for prompt ensembling. Specifically, for each class \(c\), we generate a total of \(S\) text descriptions, denoted as \(\{_{c}^{(i)}\}_{i=1}^{S}\). The prototypes of these descriptions in the embedding space are calculated as \(_{c}=_{i}_{t}(_{c}^{(i)})\). To further improve the quality of these prototypes over time, we design them to be updated online through a cumulative average with each individual sample \(X_{}\) in

Figure 2: **An overview of our DPE method**. We introduce prototypes from both textual and visual modalities and enable prototype-based inference with CLIP. For each test sample, we optimize both prototypes using learnable residual parameters with alignment loss \(_{}\) and self-entropy loss \(_{}\). These prototypes are also progressively evolved over time to capture more accurate and discriminative multi-modal representations for target classes.

the test stream. The update rule is given by:

\[+^{*}}{\|(k-1)+ ^{*}\|}, k k+1,\] (3)

where \(=[_{1}\,_{2}\,\,_{C}]^{} ^{C d}\) is the online updated prototype set, and \(^{*}^{C d}\) is the optimized textual prototypes for each individual sample \(X_{}\) in Equation (10). To ensure stable online updates, we set an entropy threshold \(_{t}\) to filter out low-confidence samples (for which \((_{}(X_{}))<_{t}\)) from updating the online prototypes, and maintain a counter \(k\) for tracking confident samples.

**Visual Prototype Evolution**. Inspired by TDA , we recognize that the historical image features of test images can also be utilized to enhance CLIP's discrimination capability. Therefore, we design a priority queue strategy to store the top-\(M\) image features for each class and symmetrically compute a set of visual prototypes that evolve over time. Note that since we cannot access the labels of the test samples, we assign the image features to the queue according to their predicted pseudo-labels. The priority queue for each class \(c\) is initialized as empty, denoted as \(q_{c}=\). As test samples arrive, we store the image features \(f_{c}\) and the corresponding self-entropy \(h_{c}\) in the priority queue, represented as \(q_{c}=\{(f_{c}^{(m)},h_{c}^{(m)})\}_{m}\). The elements are sorted by self-entropy \(h_{c}^{(m)}\) such that \(h_{c}^{(m)}<h_{c}^{(>m)}\). Using this priority queue, the class-specific visual prototype is obtained by: \(_{c}=}_{m}f_{c}^{(m)}\), where \(S_{c} M\) denotes the total number of image features stored in the queue.

The priority queues are updated during testing by replacing low-confidence image features with high-confidence ones. Specifically, for each individual test sample \(X_{}\), we first predict the pseudo-label \(\) and compute the corresponding self-entropy \(h\) as:

\[=_{y_{c}}_{}(y=y_{c}|X_{}),  h=(_{}(X_{})).\] (4)

Then, we consider the following two scenarios to iteratively update the priority queue \(q_{}\) for class \(\): (1) If the priority queue is not full, we directly add the pair \((_{v}(X_{}),h)\) to the queue; (2) If the priority queue is full and the entropy \(h\) of the new sample is lower than the highest entropy value (of the last element) currently in the queue, we replace the highest-entropy element with the new feature and self-entropy \((_{v}(X_{}),h)\). If \(f\) is not lower, we discard the new sample and leave the queue unchanged. After each update, we re-sort the priority queue based on the self-entropy values and re-compute the visual prototypes \(=[_{1}\,_{2}\,\,_{C}]^{} ^{C d}\) for all classes.

In Figure 3, we present the t-SNE  visualizations of the stored image features in the priority queues (with queue size \(M=6\)) after updating with 1500 samples (_left_) and 15000 samples (_right_) on the Food101  dataset. We highlight the stored features from 25 random classes using different colors while marking the others in gray. These visualizations illustrate that our priority queue strategy effectively accumulates high-confidence samples, progressively refining the representativeness of the visual prototypes over time.

**Prototype-Based Inference**. Based on our two sets of multi-modal prototypes \(\{_{c}\}_{c=1}^{C}\) and \(\{_{c}\}_{c=1}^{C}\), the final prediction for input \(X\) is given by

\[f_{v}=_{v}(X),_{}(y=y_{c}|X)=^{}_{c}+(f_{v}^{}_{c}))/t)}{_{c^{}}((f_{v}^{}_{c^{ }}+(f_{v}^{}_{c^{}}))/t)},\] (5)

Here, \(t\) represents the temperature parameter in the softmax function, \(\) denotes the matrix transpose, and \((x)=(-(1-x))\) is the affinity function, where \(\) is a balance hyperparameter and \(\) is a sharpness ratio. In Appendix A.3, we conduct a sensitivity analysis of these two hyperparameters to evaluate their impact on the overall performance of DPE.

Figure 3: **t-SNE  visualizations of the stored image features in the priority queues**. With more samples getting in, the selected image features from each class become more clustered, leading to more representative visual prototypes.

### Prototype Residual Learning

To further improve the zero-shot generalizability of our method, we introduce prototype residual learning, which optimizes multi-modal prototypes for each test sample. Unlike previous prompt tuning approaches [54; 13] that require backpropagating gradients through the text encoder to update input prompts, our method directly updates the prototype sets in the embedding space.

Specifically, after being evolved with the last test sample, the dual sets of multi-modal prototypes, denoted as \(=[_{1}\,_{2}\,\,_{C}]^{} ^{C d}\) and \(=[_{1}\,_{2}\,_{C}]^{} ^{C d}\), are considered as the initialization for updating with the current test sample. We further introduce learnable residual parameters \(}=[}_{1}\,}_{2}\,\,}_{C}]^{}^{C d}\) and \(}=[}_{1}\,}_{2}\,\,}_{C}]^{}^{C d}\). These parameters are initialized to zero and are used to optimize the prototypes for each given test input \(X_{}\), denoted as

\[_{c}_{c}+}_{c}}{\|_{c}+}_{c}\|},_{c}_{c}+ }_{c}}{\|_{c}+}_{c}\|}.\] (6)

Similar to Equation (2), we optimize these residual parameters to promote consistent predictions across a total of \(N\) different augmented views of the given test image \(X_{}\) using the unsupervised entropy minimization objective:

\[_{}=(_{ }(X_{}))=-_{c=1}^{C}_{}(y=y_{c}|X_{ })_{}(y=y_{c}|X_{}),\] (7) \[\,\,_{}(X_{})= _{n=1}^{N}[(_{}(_{n}(X_{}))]\,_{}(_{n}(X_{})).\] (8)

However, researchers have shown that focusing solely on reducing entropy can lead the model to make overconfident predictions . To address this, we apply an additional constraint to align the multi-modal prototypes during optimization, explicitly enforcing consistent multi-modal representations between dual sets of prototypes. Specifically, we introduce a self-supervised alignment loss that utilizes the contrastive InfoNCE loss  to bring prototypes from the same class closer together while pushing prototypes from different classes further apart:

\[_{}=_{c=1}^{C}(-_{c}^{}_{c})}{_{c^{}}(_{c}^{ }_{c^{}})}-_{c}^{}_ {c})}{_{c^{}}(_{c^{}}^{}_{c})} ).\] (9)

In summary, the final objective for optimizing the multi-modal prototypes \(,\) is

\[^{*},^{*}=_{,}(_{}+_{}),\] (10)

where \(\) is a scale factor to balance the contribution of the alignment loss. Note that \(^{*}\) and \(^{*}\) are obtained from a single update step.

After optimizing the prototypes for each test sample, we evolve the online textual prototypes \(\) as described in Equation (3), and also update the priority queues to re-compute the visual prototypes \(\). The evolved prototype sets then serve as the initialization for the next test sample, progressively enhancing generalization capability during test-time adaptation.

## 4 Experiments

In this section, we evaluate our proposed method on robustness to natural distribution shifts and cross-datasets generalization across 15 various datasets. Moreover, we also compare the test-time efficiency of our DPE with existing methods. Finally, we provide ablation experiments to systematically analyze the effects of different algorithm components and design choices.

### Experimental Settings

**Datasets**. We follow previous work [54; 13] to evaluate our method on two benchmarking scenarios, namely, robustness to natural distribution shifts and cross-datasets generalization. (1) For theevaluation of robustness to natural distribution shifts, we assess the performance of our method using the ImageNet  dataset alongside its variant out-of-distribution datasets, including ImageNet-A , ImageNet-V2 , ImageNet-R , and ImageNet-Sketch . (2) For cross-datasets generalization tasks, we conduct comprehensive assessments across 10 diverse recognition datasets, including FGVCAircraft , Caltech101 , StandfordCars , DTD , EuroSAT , Flowers102 , Food101 , OxfordPets , SUN397 , and UCF101 . These datasets offer a comprehensive benchmark for evaluating the robustness of various methods across different distributional variations.

**Implementation Details**. We follow previous works  to adopt ResNet-50  and ViT-B/16  backbones as the visual encoder of CLIP. In Appendix C.2, we detail the specific hand-crafted prompts utilized for each dataset. Following TPT , we generate 63 augmented views for each test image using random resized cropping to create a batch of 64 images. We learn the prototype residual parameters using AdamW  optimizer with a learning rate of \(0.0005\) for a single step. In default, the scale factor \(\) in Equation (10) is set to 0.5, the normalized entropy threshold \(_{t}\) is set to 0.1, and the queue size \(M\) is set to 3. For the affinity function in Equation (5), we set \(=6.0\) and \(=5.0\), respectively. All experiments are conducted on a single 48GB NVIDIA RTX 6000 Ada GPU. To ensure the reliability of our results, we perform each experiment three times using different initialization seeds and report the mean accuracy achieved.

**Baselines**. We compare our method with established test-time adaptation approaches for CLIP: (1) TPT , a prompt tuning method that aims to minimize self-entropy across predictions of multiple augmented views; (2) DiffTPTP , an enhanced version of TPT that utilizes diffusion-based augmentations to optimize prompts; (3) TDA , a training-free, adapter-based method which constructs positive and negative caches during test time. (4) TPS , an efficient approach that dynamically learns shift vectors for per-class prototypes based solely on the given test sample; (5) DMN-ZS , a backpropagation-free method that utilizes a dynamic memory to aggregate information from historical test data. Additionally, we present the zero-shot performance of CLIP using the simple prompt "_a photo of a_ {CLASS}" as well as the results from prompt ensembling to show the absolute performance improvements. We also report the performance of CoOp , a train-time adaptation method, using 16-shot annotated samples per class on ImageNet. For a fair comparison, we directly report the results of these baselines from their respective original papers. Note that in the DiffTPTP  paper, the results are based on a subset of the datasets containing 1,000 test samples. This limited sample size may introduce potential imprecision in the reported results.

   Method & ImageNet & ImageNet-A & ImageNet-V2 & ImageNet-R & ImageNet-S & Average & OOD Average \\  CLIP-ResNet-50  & 58.16 & 21.83 & 51.41 & 56.15 & 33.37 & 44.18 & 40.69 \\  Ensemble & 59.81 & 23.24 & 52.91 & 60.72 & 35.48 & 46.43 & 43.09 \\ CoOp  & 63.33 & 23.06 & 55.40 & 56.60 & 34.67 & 46.61 & 42.43 \\  TPT  & 60.74 & 26.67 & 54.70 & 59.11 & 35.09 & 47.26 & 43.89 \\ DiffTPTP  & 60.80 & **31.06** & 55.80 & 58.80 & 37.10 & 48.71 & 45.69 \\ TDA  & 61.35 & 30.29 & 55.54 & 62.58 & 38.12 & 49.58 & 46.63 \\ TPS  & 61.47 & 30.48 & 54.96 & 62.87 & 37.14 & 49.38 & 46.36 \\ DMN-ZS  & **63.87** & 28.57 & 56.12 & 61.44 & 39.84 & 49.97 & 46.49 \\ 
**DPE (Ours)** & 63.41 & 30.15 & **56.72** & **63.72** & **40.03** & **50.81** & **47.66** \\  CLIP-ViT-B/16  & 66.73 & 47.87 & 60.86 & 73.98 & 46.09 & 59.11 & 57.20 \\  Ensemble & 68.34 & 49.89 & 61.88 & 77.65 & 48.24 & 61.20 & 59.42 \\ CoOp  & 71.51 & 49.71 & 64.20 & 75.21 & 47.99 & 61.72 & 59.28 \\  TPT  & 68.98 & 54.77 & 63.45 & 77.06 & 47.94 & 62.44 & 60.81 \\ DiffTPTP  & 70.30 & 55.68 & 65.10 & 75.00 & 46.80 & 62.28 & 60.52 \\ TDA  & 69.51 & **60.11** & 64.67 & 80.24 & 50.54 & 65.01 & 63.89 \\ TPS  & 70.19 & 60.08 & 64.73 & 80.27 & 49.95 & 65.04 & 63.76 \\ DMN-ZS  & **72.25** & 58.28 & 65.17 & 78.55 & **53.20** & 65.49 & 63.80 \\
**DPE (Ours)** & 71.91 & **59.63** & **65.44** & **80.40** & 52.26 & **65.93** & **64.43** \\   

Table 1: **Performance comparisons on robustness to natural distribution shifts**. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in **bold**.

### Results and Discussions

**Robustness to Natural Distribution Shifts**. In Table 1, we first compare the performance of our method with other state-of-the-art methods on in-domain ImageNet and its 4 out-of-distribution variants. Due to domain shifts, zero-shot CLIP  underperforms in out-of-distribution scenarios. As shown in the table, adopting prompt ensembling and prompt learning methods like CoOp  can enhance CLIP's generalizability. However, it is important to note that CoOp is a train-time adaptation method that requires an annotated training set, limiting its effectiveness in real-world settings. Despite this, our method still exhibits significant performance gains of 4.20% and 4.21% on average across two different backbones compared to CoOp, indicating the superiority of our DPE in enhancing generalization capability on out-of-distribution domains.

Focusing on test-time adaptation methods, the experimental results demonstrate that our method achieves superior zero-shot generalization performance across various out-of-distribution datasets compared to other approaches. Specifically, our method outperforms existing state-of-the-art prompt tuning methods, surpasses TPT  by 3.55% and 3.49% and DiffTPT  by 2.10% and 3.65% on average when using ResNet-50 and ViT-B/16 backbones, respectively. Moreover, our method also outperforms cache-based TDA  by margins of 1.23% and 0.92% across two different backbones, indicating the effectiveness of our DPE approach. Moreover, our DPE demonstrates performance advantages over the recent TPS  and DMN-ZS  approaches, outperforming them by 1.43% and 0.84% on average across 5 datasets using the ResNet-50 backbone, further highlighting the superiority of our method. We also demonstrate that our DPE can also be effectively applied to prompts learned using CoOp  with a 16-shot ImageNet setup. We compare the performance with other methods on the same 5 datasets in Appendix A.1, where our method consistently demonstrates competitive performance. These results highlight the general effectiveness of our proposed test-time adaptation method in both in-domain and out-of-distribution scenarios.

**Cross-Datasets Generalization**. In Table 2, we further assess the generalizability of our proposed method against other state-of-the-art methods on 10 fine-grained recognition datasets. Given the significant distributional differences, methods may exhibit variable performance across these datasets. Notably, our method, which is not trained on any annotated data, significantly outperforms CoOp  by average margins of 5.75% and 5.52% on two respective backbones. When compared to other test-time adaptation methods, DPE exhibits average performance gains of 2.08% to 0.90% compared to DiffTPT and TDA, respectively. On the more advanced ViT-B/16 backbone, DPE continues to outperform existing approaches on 7 out of 10 datasets, with average improvements ranging from 1.87% to 4.30%. These results demonstrate the superior robustness and adaptability of our method in transferring to diverse domains during test time, which is crucial for real-world deployment scenarios.

   Method & Aircraft & Caltech & Cars & DTD & EuroSAT & Flower & Food101 & Pets & SUN397 & UCF101 & Average \\  CLIP-ResNet-50 & 15.66 & 85.88 & 55.70 & 40.37 & 23.69 & 61.75 & 73.97 & 83.57 & 58.80 & 58.84 & 55.82 \\  Ensemble & 16.11 & 87.26 & 55.89 & 40.37 & 25.79 & 62.77 & 74.82 & 82.97 & 60.85 & 59.48 & 56.63 \\ CoOp  & 15.12 & 86.53 & 55.32 & 37.29 & 26.20 & 61.55 & 75.59 & **87.00** & 58.15 & 59.05 & 56.18 \\  TPT  & 17.58 & 87.02 & 58.46 & 40.84 & 28.33 & 62.69 & 74.88 & 84.49 & 61.46 & 60.82 & 57.66 \\ DiffTPT  & 17.60 & 86.89 & **60.71** & 40.72 & 41.04 & 63.53 & **79.21** & 83.40 & 62.72 & 62.67 & 59.85 \\ TDA  & 17.61 & 89.70 & 57.78 & 43.74 & **42.11** & **68.74** & 77.75 & 86.18 & 62.53 & **64.18** & 61.03 \\ 
**DPE (Ours)** & **19.80** & **90.83** & 59.26 & **50.18** & 41.67 & 67.60 & 77.83 & 85.97 & **64.23** & 61.98 & **61.93** \\   CLIP-ViT-B/16 & 23.67 & 93.35 & 65.48 & 44.27 & 42.01 & 67.44 & 83.65 & 88.25 & 62.59 & 65.13 & 63.58 \\  Ensemble & 23.22 & 93.55 & 66.11 & 45.04 & 50.42 & 66.99 & 82.86 & 86.92 & 65.63 & 65.16 & 64.59 \\ CoOp  & 18.47 & 93.70 & 64.51 & 41.92 & 46.39 & 68.71 & 85.30 & 89.14 & 64.15 & 66.55 & 63.88 \\  TPT  & 24.78 & 94.16 & 66.87 & 47.75 & 42.44 & 68.98 & 84.67 & 87.79 & 65.50 & 68.04 & 65.10 \\ DiffTPT  & 25.60 & 92.49 & 67.01 & 47.00 & 43.13 & 70.10 & **87.23** & 88.22 & 65.74 & 62.67 & 65.47 \\ TDA  & 23.91 & 94.24 & 67.28 & 47.40 & **58.00** & 71.42 & 86.14 & 88.63 & 67.62 & **70.66** & 67.53 \\ 
**DPE (Ours)** & **28.95** & **94.81** & **67.31** & **54.20** & 55.79 & **75.07** & 86.17 & **91.14** & **70.07** & 70.44 & **69.40** \\   

Table 2: **Performance comparisons on cross-datasets generalization. We also present top-1 accuracy (%) for all methods on two backbones of CLIP. The best results are highlighted in bold.**

**Efficiency Comparison**. Table 3 presents a comparison of our method's efficiency against other test-time adaptation approaches for VLMs, evaluated on 50,000 test samples from the ImageNet  dataset. The comparison is conducted on a single 48GB NVIDIA RTX 6000 Ada GPU. In our DPE method, the main computational overhead arises from the visual prototype evolution and prototype residual learning components. Specifically, while zero-shot CLIP requires 10.1 ms to infer a single image, incorporating our prototype residual learning increases the inference time to 64.7 ms per image. Further including the visual prototype evolution extends this to 132.1 ms per image.

Our proposed method shows improved computational efficiency compared to other prompt tuning methods, for example, \(5\) faster than TPT  and over \(10\) faster than DiffTPT , as it requires only learning the prototype residues without the need to backpropagate gradients through the textual encoder. While our method is less efficient than TDA  and TPS , as we still backpropagate gradients to update multi-modal prototypes, it offers notable performance advantages.

### Ablation Studies

**Different Textual Prototype Evolution Rules**. In Table 4, we report the performance on ImageNet  using different textual prototype evolution rules. We have the following key observations: (1) Fully updating our textual prototypes \(\) to the optimized prototypes \(^{*}\) for each individual test image results in collapsed performance; (2) Compared to not evolving the textual prototypes, using an exponential moving average update rule with a decay rate of 0.99 leads to a slight performance improvement of 0.18%; however, setting a lower decay rate of 0.95 decreases the performance by 0.36%. (3) Our cumulative average update rule yields the highest performance, achieving a 0.48% improvement compared to no update on ImageNet .

**Hyperparameters for Dual Prototype Evolution**. We provide a sensitivity analysis for the hyperparameters \(_{t}\) and \(M\) on the Caltech101  dataset in Figure 4 (_Left_). Specifically, \(_{t}\) represents the normalized entropy threshold for evolving our textual prototypes. When \(_{t}=0\), our method does not evolve the textual prototypes, leading to a significant performance decrease, as shown in Figure 4 (_Left_). Moreover, setting \(_{t}=0.1\) results in the highest performance, whereas a higher threshold leads to a slight decrease in performance. Additionally, the queue size \(M\) acts as a soft threshold hyperparameter for evolving the visual prototypes. Our setting of \(M=3\) consistently yields the highest performance. Lowering \(M\) causes the visual prototypes to fail in capturing the diversity of test samples from the same class, while increasing \(M\) introduces additional low-confidence noisy samples that hinder discrimination among target classes. Notably, our DPE method consistently outperforms other approaches across a reasonable range of hyperparameter settings: all combinations of entropy threshold \(_{t} 0.1\) and queue size \(M>3\) achieve over 90.3% accuracy on Caltech101, whereas TPT  and TDA  only achieve 87.02% and 89.70%, respectively.

**Effects of Different Learnable Modules**. Recall that in our DPE method, we optimize our multi-modal prototypes by introducing two sets of learnable residual parameters \(}\) and \(}\) for each individual test image. In Figure 4 (_Middle_), we ablate the effects of each set of learnable residual parameters and report the performance across three datasets. Specifically, on ImageNet , optimizing only the textual prototypes for individual samples results in a 1.40% improvement, while optimizing only the visual prototypes yields a non-trivial 0.36% improvement, compared to keeping both \(}\) and \(}\) fixed. Optimizing both sets of residual parameters leads to a further performance increase, _e.g._, by 1.52% on ImageNet . This indicates both learnable modules contribute to the overall effectiveness of DPE.

   Update Rule & Formula & Accuracy \\  No Update & \(\) & 62.93 \\ Full Update & \(^{*}\) & 21.83 \\ Exponential Avg. & \( 0.99+0.01^{*}\) & 63.11 \\ Exponential Avg. & \( 0.95+0.05^{*}\) & 62.57 \\ Cumulative Avg. & \(((k=1)+^{*})/k\) & **63.41** \\   

Table 4: **Performance comparison using different textual prototype evolution rules on ImageNet . For each method, we present the update rule formula and report the resulting accuracy on the ImageNet dataset.**

   Method & Testing Time & Accuracy & Gain \\  CLIP  & 9 min & 59.81 & - \\ TPT  & 9 h 15 min & 60.74 & +0.93 \\ DiffTPT  &  20 h & 60.80 & +0.99 \\ TDA  & 1 h 5 min & 61.35 & +1.54 \\ TPS  & 55 min & 61.47 & +1.66 \\
**DPE (Ours)** & 1 h 50 min & **63.41** & **+3.60** \\   

Table 3: **Efficiency comparison on ImageNet . We report the testing time, the achieved accuracy, and the performance gains compared to zero-shot CLIP.**

**Scaling the Alignment Loss**. Finally, we ablate the effect of the alignment loss by varying the scale factor \(\) in Figure 4 (_Right_). Compared to optimizing solely using entropy minimization loss (_i.e_., \(=0\)) during test-time adaptation, applying the additional alignment loss results in a performance improvement of 0.23% to 1.07% across three different datasets. However, there is a trade-off between prototype alignment and self-entropy minimization: setting \(\) too high leads to a performance drop. Our experiments show that our setting of \(=0.5\) yields the highest performance.

**Impact of Varying Update Steps**. In Equation (10), we update the multi-modal prototypes with a single update step for each test instance. To evaluate the impact of different numbers of update steps on overall performance, we conduct ablation experiments by varying the number of update steps from 1 to 5 and report the resulting performance on ImageNet. As shown in Table 5, the number of update steps does not significantly influence performance (within a range of 0.2%). While increasing the update steps to 2 yields a slight performance gain of 0.04%, it also leads to a proportional decrease in inference efficiency. Given this trade-off, we adopt the single-step update as the default for balancing efficiency and performance.

## 5 Conclusion

In this work, we introduce Dual Prototype Evolving (DPE), a novel and effective approach for enhancing the zero-shot generalizability of VLMs during test time. Unlike previous methods that only focus on adapting the VLMs from one modality, we create and evolve two sets of prototypes--textual and visual--progressively capturing more accurate multi-modal representations for target classes during test time. Moreover, we also introduce prototype residual learning to optimize the dual prototype sets for each individual test sample, which further enhances the test-time generalization capabilities of VLMs. Through comprehensive experiments, we demonstrate that our proposed DPE achieves state-of-the-art performance while also exhibiting competitive test-time efficiency.

**Limitations**. While our proposed DPE method effectively adapts CLIP to out-of-distribution domains during test time, we identify two potential limitations: (1) It still requires gradient backpropagation to optimize the multi-modal prototypes. This optimization process introduces additional computational complexity compared to zero-shot CLIP , which may affect its real-time performance in practical deployment scenarios. (2) Since DPE needs to maintain priority queues to evolve the visual prototypes, it increases the memory cost during inference.

**Broader Impacts.** In this work, we aim to build more reliable machine learning systems by leveraging the extensive knowledge of current foundational models, specifically CLIP . Specifically, we follow TPT  to apply the test-time adaptation setting to vision-language models to align with real-world scenarios. By employing our DPE approach, the CLIP model can adapt itself to diverse domains during test time, which enhances its practical applicability in real-world deployment scenarios. We hope this work inspires future studies to focus on the generalization and robustness of pre-trained large-scale foundation models.

Figure 4: **Ablation studies**. (_Left_) Sensitivity analysis of \(_{t}\) and \(M\) on Caltech101 ; (_Middle_) Analysis of the performance contributions from various learnable parameter settings across three datasets; (_Right_) Performance on three datasets with varying scale factor \(\) in Equation (10).

   \# Steps & 1 & 2 & 3 & 4 & 5 \\  Accuracy & 63.41 & **63.45** & 63.28 & 63.26 & 63.32 \\   

Table 5: **Ablation studies on different update steps in prototype residual learning**. We vary the number of update steps from 1 to 5 and report the achieved performance on ImageNet .