# SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation

Zeyao Ma\({}^{1,*,}\), Bohan Zhang\({}^{1,*,}\), Jing Zhang\({}^{1,}\), Jifan Yu\({}^{2}\), Xiaokang Zhang\({}^{1}\),

**Xiaohan Zhang\({}^{3}\), Sijia Luo\({}^{1}\), Xi Wang\({}^{1}\), Jie Tang\({}^{2}\)**

\({}^{1}\) Renmin University of China \({}^{2}\) Tsinghua University \({}^{3}\) Zhipu.AI

https://spreadsheetbench.github.io

Equal Contribution.Corresponding author.Work was done when interned at Zhipu AI.

###### Abstract

We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

## 1 Introduction

Today, millions of users engage with spreadsheets across various fields, including management, marketing, and finance . Assisting users with spreadsheet manipulation can significantly reduce their workload, thereby enhancing productivity and efficiency . Initially, program synthesis aided spreadsheet manipulation , followed by deep learning methods for automated manipulation . More recently, leveraging the powerful capabilities of LLMs , spreadsheet agents such as SheetCopilot and SheetAgent  have been developed to assist people in manipulating spreadsheets via natural language instructions. These agents employ LLMs to transform instructions into solutions, typically presented as executable code for spreadsheet operations.

However, current spreadsheet-related agents  are still far from being truly helpful assistants for automating spreadsheet tasks. Users of Copilot and similar agents find these tools useful for reducing the effort of searching online, but they do not necessarily improve task completion time or success rates . A critical reason for this shortcoming is that the benchmarks  used to evaluate these agents do not truly reflect real and challenging user demands. The primary limitations could be summarized as follows.

First, these benchmarks either use the self-instruct technique  to synthesize queries based on a limited set of real-world spreadsheet manipulation demands , or they rely on crowdworkersto manually create instructions for given spreadsheets based on a few manipulation examples . As illustrated in the left part of Figure 1, real user questions collected from online forums (e.g., excelforum.com) differ significantly from synthetic queries, presenting complex demands that often require additional context, such as the users' previous attempts and encountered issues.

Second, the spreadsheets used in these benchmarks are overly simplified [11; 12], typically containing only one regular relational table, similar to the tables used in TableQA tasks [16; 17] and the tables in databases of Text2SQL tasks [18; 19]. While both spreadsheets and relational tables store tabular data in a two-dimensional structure, there are significant differences that make spreadsheets more challenging to handle. As shown in the right part of Figure 1, spreadsheets offer a flexible way of organizing data without distinct table boundaries or structures, permitting the inclusion of multiple tables in non-standard formats--such as nested, incomplete, or missing headers--and the presence of free-form text within a single cell. Additionally, spreadsheets support various non-textual elements (e.g., color, bold) compared to the relational tables in databases  or the CSV/JSON format tables in TableQA tasks [16; 20]. These distinct features are not fully utilized in current benchmarks.

Third, these benchmarks for LLMs typically involve a single test for each instruction, where the predicted answer, derived from the solution generated by the LLMs applied to the given spreadsheet, is compared to the ground truth answer [11; 12]. This evaluation, however, may result in false positive solutions that are tailored to the specific spreadsheet and do not generalize to other spreadsheets with varying values. A robust solution from LLMs should be able to manage multiple spreadsheets, even corner cases, as inputs, apply the solution, and consistently yield correct answers.

To address the identified need, we introduce a challenging benchmark called SpreadsheetBench. Our benchmark features complex user instructions alongside spreadsheets that are flexibly organized by real-world users. These resources are gathered exclusively from four online forums for addressing Excel problems, which are highly ranked by several reputable websites including general Google Search, Feedspot.com--known for its forum search functionality--and Deskbright.com, a site focused on online education, particularly in Microsoft Excel. The instructions often require detailed contextual information for clarification, including descriptions of attempted solutions, incorrect answers received, and the issues encountered. The spreadsheets exhibit diverse formats, potentially featuring multiple tables within a single spreadsheet, non-standard tables that extend beyond conventional relational structures, cells with only textual content, and other non-textual elements.

We also propose an Online Judge (OJ)-style metric for more reliable benchmarking, adapted from coding skill assessments . This metric is well-suited for spreadsheet manipulation tasks where LLMs are tasked with generating code solutions. Within our benchmark, each instruction is associated with multiple input-output spreadsheets serving as test cases, sharing a similar structure but differing in data. A code solution must pass all these test cases to ensure evaluation reliability and accuracy.

Figure 1: Comparison of previous SheetCopilotBench and our benchmark. The instructions in our benchmark are more complex, reflecting real user demands, issues encountered, previous user attempts, and an output example. The spreadsheets in our benchmark organize data more flexibly. The figure provides a summary of the various data organization types used in our spreadsheets, with three non-standard relational tables featuring nested header, incomplete header and missing header, in addition to cells containing pure textual information and non-textual elements like colors.

Conclusively, we have developed SpreadsheetBench comprising 912 instructions and 2,729 test cases, with an average of three test cases per instruction. These instructions cover 10 primary manipulation categories and involve spreadsheets with tables extending beyond 100 columns and 20,000 rows. Moreover, 35.7% of the spreadsheets contain multiple tables, and 42.7% feature non-standard relational tables. This benchmark is notable for **its real-world instructions, diverse spreadsheet formats, and a comprehensive testing strategy**.

We conduct a thorough evaluation of various types of models, including commonly studied TableQA models, open-source LLMs for general tasks and coding tasks, the most advanced close-source models, and spreadsheet-specific LLMs. The performance of these models vary widely, with scores ranging from 0.05% to 23.65% according to our proposed OJ-style evaluation metric. Notably, some methods score as low as 0%, highlighting the benchmark's difficulty. The results also suggest the importance of improving coding abilities within LLMs for spreadsheet manipulation, and indicate that multi-round prompting, allowing LLMs to read spreadsheet data and error feedback from a code compiler, can increase the probability of correct responses.

## 2 SpreadsheetBench

In this section, we begin by outlining the task formulation, followed by an introduction to the benchmark construction pipeline and the presentation of resulting benchmark statistics. Finally, we introduce the proposed OJ-style evaluation metric.

### Task Formulation

We denote the dataset of a benchmark as \(=\{(q_{i},c_{i},a_{i})\}\), where \(q_{i}\) represents an instruction, \(c_{i}\) denotes a spreadsheet, and \(a_{i}\) is the corresponding answer of \(q_{i}\) on \(c_{i}\). The objective of spreadsheet manipulation is to enable an LLM to generate a code-based solution \(s_{i}\), apply it to \(q_{i}\) to obtain a predicted answer \(_{i}\), which can then be compared with the ground truth answer \(a_{i}\).

In this benchmark, an instruction \(q_{i}\) involves the modification of either several cells or the entire sheet in a spreadsheet file, which we refer to as _cell-level_ and _sheet-level_ manipulation, respectively.

The spreadsheet \(c_{i}\) stores tabular data in a two-dimensional format, offering a flexible way for data organization without a distinct table boundary or structure. A single sheet in a spreadsheet file can contain multiple tables in diverse formats, free-form textual information, and non-textual elements.

An answer \(a_{i}\) refers to the modified spreadsheet resulting from the application of a solution, which is expected to be a piece of code generated by LLMs. We generate code as solutions, as the goal is to manipulate spreadsheets rather than query information from them. Note that \(\) does not include solutions because we allow for various solutions by LLMs, with our focus solely on the correctness of final answers after applying the solutions.

### Benchmark Construction

The construction of the benchmark follows a four-stage pipeline: 1) data sourcing, 2) data filtering, 3) data formatting, and 4) test case construction, as illustrated in Figure 2. Subsequently, we detail the perturbation techniques used to prevent data leakage.

**1. Data Sourcing.** Our objective is to collect high-quality and representative spreadsheet manipulation questions from real-world sources. As depicted in the initial section of Figure 2, we utilize general Google Search, the specialized forum search engine Feedspot.com, and the Microsoft Excel-focused online learning platform Deskbright.com to identify four popular and regularly updated forums and blog sites--ExcelForum, Chandoo, MrExcel, and ExcelGuru--as our data sources (detailed information available in Appendix B.1). We specifically target posts that fall under categories such as _General Questions_, _Formula_, _VBA & Macros_, _Formatting_, _Pivot Table_, and _Power Query_ to ensure relevance to spreadsheet manipulation.

**2. Data Filtering.** Since the original posts in forums are diverse with various questions, as shown in the second part of Figure 2, we select the question by four main criteria: 1) is solved, 2) solely pertains to spreadsheet manipulation, 3) is feasible and testable, and 4) is representative.

Initially, we directly identify questions tagged as "solved" from all posts. For posts without explicit tags, we employ GPT-4 to determine their solved status, as patterns such as thank-you notes are often observed at the end of a post indicating the question is solved.

Next, we focus on extracting questions strictly related to spreadsheet manipulation. Most popular spreadsheet forums primarily discuss Excel-related issues involving software functions like input boxes, buttons, and forms. Unlike InstructExcel , our goal is to establish a software-independent benchmark centered solely on spreadsheet manipulation. Consequently, we discard all irrelevant software-specific questions using keywords such as "input boxes", "buttons", and "forms", and engage annotators to check the remaining posts.

Finally, we select questions that are both feasible and representative. To ensure feasibility, aided by annotators, we exclude posts without spreadsheet attachments, those with excessive replies, or ambiguous presentations. To identify representative posts, we filter those with high view counts and ask the annotators to retain both simple and difficult questions.

**3. Data Formatting.** According to the task definition outlined in Section 2.1, it is necessary to formulate the instruction, the spreadsheet, and the answer in a manner that facilitates convenient and accurate automatic evaluation.

_Instruction Generation._ The original posts are often cluttered, with the description of the user question dispersed across multiple replies. Therefore, we need to extract and condense these into a self-contained instruction from all replies. Initially, we utilize GPT-4 to recreate a coherent instruction from the original post. Annotators then verify this instruction to resolve any issues arising from possible incomplete context extraction from the post.

_Answer Position Annotation._ Regarding solution derivation, forum users typically discuss solutions in formulas or VBA code. Annotators are responsible for identifying the solution from the replies and applying it to the corresponding spreadsheet to derive the answers. However, discrepancies in the placement of answers by annotators compared to those predicted by LLMs can lead to mistaken evaluations. To mitigate this issue, we allow annotators to incorporate specific constraints in the instructions that annotate the answer position. As described in Section 2.1, answer positioning should consider both cell-level and sheet-level manipulation. For cell-level manipulations, annotators should specify exact cell positions, such as "D2:D6". For sheet-level manipulations, the entire range of the spreadsheet, such as "A2:D6", needs to be included in the instruction.

**4. Test Case Construction.** According to observations on online forums, it is evident that some solutions provided by forum users may be applicable only to the specific example spreadsheet provided, potentially failing when applied to other scenarios with slight variations in the spreadsheet data.

Figure 2: The benchmark construction pipeline and OJ-style evaluation of our benchmark.

To enhance the reliability of user-provided solutions, it is essential to create multiple spreadsheets and develop multiple test cases for each instruction. This preparation is essential for the subsequent OJ-style evaluation metric.

Specifically, for instructions that solely offer a solution without derived answers from forum posts, we derive the answer for the original provided spreadsheet by manually manipulating the input spreadsheet using the given solution presented by formulas or VBA code. The original spreadsheet, along with the derived answer, forms an input-output test case for the current instruction.

Subsequently, annotators are requested to modify the data in the original spreadsheet twice to generate two distinct spreadsheets. By applying the same solution to each of these modified spreadsheets, we can obtain two corresponding answers. It's important to note that annotators do not make casual alterations to the spreadsheet data; instead, they are instructed to concentrate on real-world corner cases (e.g., cells that should contain zeros but remain empty). Annotators are also instructed to make valid modifications consistent with the question's requirements.

Ultimately, for each instruction suitable for multiple test cases, we derive three test cases from the original single spreadsheet-answer data, as illustrated in the fourth part of Figure 2. Then \(=\{(q_{i},c_{i},a_{i})\}\) is changed to \(=\{(q_{i},\{(c_{ij},a_{ij})\})\}\).

**Against Data Leakage.** Datasets initially obtained from online forums may be susceptible to data leakage issues, given that many LLMs are pre-trained using a vast corpus of web text .

The above mentioned strategies, along with some new approaches, can introduce perturbations to the collected data to mitigate the data leakage problem:

_Instruction Generation_: The aforementioned instruction generation strategy, carried out by GPT-4 and human annotators, involves revising the original questions in the posts, thereby preventing LLMs from memorizing the original questions.

_Spreadsheet Modification_: The test case construction strategy outlined above involves modifying the original provided spreadsheets, preventing LLMs from memorizing the original spreadsheets.

_Answer Position Changing_: We also alter the position of the tabular data in the original spreadsheets and the corresponding answer in the resulting spreadsheets. By doing so, the originally provided solution from the posts cannot be directly used to derive answers with changed positions, thus preventing LLMs from memorizing the provided solution from the posts.

**Software Independence.** Spreadsheet manipulation is not inherently software-independent, as some functions may not be available (or may change name) changing software. We employ three methods to ensure the software independence of our benchmark as much as possible. (1) _Software-independent instruction._ We only select questions that focus on the manipulation of the value of cells in the spreadsheet and try to avoid questions involving special components or advanced operations that only exist in specific software. (2) _Software-independent spreadsheet files._ All the spreadsheet files in our dataset are in the XLSX format, which is widely supported across multiple spreadsheet

Figure 3: Key statistics of SpreadsheetBench.

applications, such as Microsoft Excel, Google Sheets, LibreOffice Calc, and WPS Office. Though some spreadsheet software may not provide full support for the XLSX format, we have made an effort to exclude any content that may be incompatible with non-Excel software. (3) _Software-independent Inference._ Instead of utilizing software-dependent tools (e.g., TypeScript and the Excel API) to interact with spreadsheet files, we allow models to manipulate spreadsheet files in diverse ways, such as Python code, because we only compare whether the results after manipulations are correct.

**Annotation Team.** The construction of our benchmark requires high-quality human annotations. We employ a team of 20 individuals who specialize in Excel and have extensive experience in annotation. All team members hold a bachelor's degree and are compensated in line with market rates. For data annotation, we allocate a predetermined quantity of data to annotators on a daily basis, and the allocation is modified in accordance with local regulations regarding work hours. For data validation, we hire two experienced validators with bachelor's degrees for the first round of quality checks. Moreover, two authors who hold master's degrees perform a secondary round of quality assessments to guarantee the high quality of data.

### Benchmark Statistics

**Data Statistics.** We have developed SpreadsheetBench comprising 912 instructions and 2,729 test cases, with an average of three test cases per instruction. We analyze it and derive several observations, as depicted in Figure 3. In Figure 3(a), the instructions in our benchmark cover a broad spectrum of spreadsheet manipulation types, including find, extract, sum, highlight, remove, modify, count, delete, calculate, and display. The outer circle illustrates the objects being manipulated, showcasing the diversity of the benchmark.

Figures 3(b) and (c) display the distributions of row and column sizes in our spreadsheet files, revealing long-tailed distributions. Specifically, 80% of spreadsheets have a row size spanning from 1 to 49, and a column size spanning from 1 to 13. It is noteworthy that the row or column size in the long tail part is substantial, indicating the challenging nature of our benchmark.

Figure 3(d) demonstrates that more than one-third of spreadsheets contain multiple tables in one sheet. Figure 3(e) reveals that nearly half of the spreadsheets contain non-standard relational tables with nested, incomplete or missing headers. These observations indicate that the SpreadsheetBench significantly increases the complexity of understanding and manipulating the spreadsheets.

**Comparison with Previous Benchmarks.** Table 1 compares SpreadsheetBench with previous spreadsheet manipulation benchmarks. In addition, numerous benchmarks exist for table-related tasks. These include table question answering (TableQA)[16; 24; 25; 26], Table-to-text[27; 28], and table data analysis [29; 30; 31]. We have not included benchmarks for these table-related tasks, as their instructions are specifically tailored for QA and use CSV/JSON files as input, which significantly diverges from tasks that involve spreadsheet manipulation.

   Benchmark & SheetCopilotBench  & InstructExcel  & SheetRM  & Ours \\  Data Source & Self-Instruct & Manual Annotation & Self-Instruct & Forum \& Blog \\  Instructions & 221 & 4850 & 201 & 912 \\ Ave. Instruction Words & 27.9 & 9.8 & - & 85.7 \\  Spreadsheet Files & 29 & 940 & 25 & 2729 \\ Single Sheet & 26 & 572 & - & 2019 \\ Multiple Sheet & 3 & 368 & - & 710 \\ Sheets & 32 & 1694 & 83 & 3917 \\ Non-standard Tables & ✘ & ✓ & ✘ & ✓ \\ Multiple Tables & ✘ & ✓ & ✘ & ✓ \\ Additional Info. & ✘ & ✘ & ✘ & ✓ \\  Evaluation & Exact Match (EM) & EM \& Similarity & Sub-task EM & OI-style EM \\ Ave. Test Cases & 1 & 1 & 1 & 3 \\   

Table 1: Comparison between SpreadsheetBench and other spreadsheet manipulation benchmarks across four aspects: data source, instructions, spreadsheets, and evaluation metrics.

SpreadsheetBench is sourced exclusively from real-world data and exhibits a higher average word count per instruction. We have collected a larger number of spreadsheet files, including both single-sheet and multiple-sheet formats in each file. Our spreadsheet files contain multiple sheets with non-standard relational tables and multiple tables within a single sheet. Real-world questions often involve additional explanations within the spreadsheet, a characteristic not present in previous benchmarks. Furthermore, we employ OJ-style evaluation metrics with three test cases per instruction.

The benchmarks SheetCopilotBench  and SheetRM  generate instructions using LLMs' self-instruct techniques, which are quite different from users' actual queries. Furthermore, the manipulated spreadsheets in these benchmarks are relatively simple, seldom featuring multiple sheets and tables, and non-standard relational tables. On the other hand, InstructExcel  offers a more comprehensive benchmark, particularly with complex spreadsheet files that cover various features. Even though the instructions in InstructExcel are created by annotators, they remain relatively simple. The average word count per instruction is only 9.8, compared to 85.7 in our benchmark. Additionally, InstructExcel does not include additional explanations or multiple test cases for the instructions, making it less complex and potentially less reliable than our benchmark.

### Evaluation Metrics

Exact match is a commonly employed metric for spreadsheet manipulation tasks [11; 12], but it may only work for the current spreadsheets and may not be robust to variations in the data within the spreadsheet. To address this, we adopt an approach similar to the online judge system commonly used for assessing coding capability . An Online Judge (OJ) style evaluation involves inputting multiple test cases into a code script and comparing each test case's output to the ground truth result. This method is more comprehensive and reliable than evaluating the code script just once. In our benchmark context, each test case is a spreadsheet file with a similar table structure but different cell values, as mentioned in Section 2.2. This allows us to evaluate a code solution more thoroughly and determine its effectiveness in handling corner cases. Figure 2 illustrates this process, where an LLM is expected to generate a solution given an instruction and a spreadsheet test case. Subsequently, the solution is applied to multiple spreadsheet test cases for OJ-style evaluation. In contrast to some benchmarks that perturb data in tables to create new data samples and perform one model inference for each data sample , we only require one model inference to generate a solution for all test cases of each instruction.

We establish two distinct scoring criteria to calculate the final score. The _soft restriction_ adheres to the scoring principles of the OJ system from the IOI, granting partial credit when a solution only passes some test cases . The calculation is as follows:

\[S_{soft}=|}_{i=1}^{||}(|}_{j=1}^{|T_{i}|}_{r_{ij}=ACC}).\] (1)

The _hard restriction_ follows the ICPC scoring rules of the OJ system, where no partial credit is awarded . The score is determined as follows:

\[S_{hard}=|}_{i=1}^{||}_{r_{ij}= ACC, j=1,2,,|T_{i}|}.\] (2)

In the above equations, \(T_{i}\) represents the test cases (i.e., \(\{(c_{ij},a_{ij})\}\)) of the \(i\)-th instruction, and \(r_{ij}\) indicates the result of the model's solution applied on the \(j\)-th test case, respectively. Specifically, \(r_{ij}\) is marked as Accept (ACC) if the applied solution's answer on the \(j\)-th test case (i.e., the resulting spreadsheet) match the ground truth answer. The soft restriction metric does not penalize models for failing to provide a flawless solution that addresses every common and corner cases, whereas the hard restriction metric encourages models to strive for the most perfect solution.

## 3 Experiments

**Baselines.** We evaluate LLMs across five categories: (1) _TableQA models_, including fine-tuned TaPEx (based on BERT) , TaPas (based on BART) , and prompt-based Binder (GPT-3.5) ; (2) _Open-source code models_, including CodeQwen (7B)  and DeepseeKoder (33B) ; (3) _Open-source general models_, including Mixtral 8x7B  and Llama 3 (70B) ; (4) _Close-source models_, including GPT-3.51 and GPT-4o ; (5) _Spreadsheet-specific methods or products_, including SheetCopilot  and Copilot in Excel2.

**Evaluation Interface.** For TableQA models, we ask the model to generate text results based on the content of the spreadsheet files. For Spreadsheet-specific methods or products, we manually evaluate these methods within their products. We only sample five percent of the data samples from our benchmark due to the costly and time-consuming process of evaluating the entire benchmark. For other models, we use Python code to modify and interact with the spreadsheet files.

**Inference Setting.** We evaluate LLMs under two distinct settings: 1. _Single Round_: In this mode, we present the model with the initial few rows of spreadsheet files within the prompt, allowing for only one inference. Since TableQA models can only produce textual answers, we conduct separate inferences for each test case and compare the resulting textual answers to the ground truth. For non-TableQA models, we instruct them to generate a code-based solution for all test cases of a given instruction. 2. _Multi-Round_: Building on the single-round prompt setting, we incorporate additional prompt that utilizes the ReAct technique  and code execution feedback  to enhance the accuracy of code solutions produced by LLMs over multi-round conversation. Specifically, we offer LLMs two choices: to generate code for retrieving spreadsheet file contents or to provide the ultimate solution. For the first choice, we supply code execution outcome to the subsequent round. In the second, the conversation concludes once the desired spreadsheet file is produced. In both instances, we furnish error feedback if the code fails to execute, enabling the model to refine its code in subsequent iterations. We impose a limit of five rounds for the multi-round setting.

**Human Performance.** We hire four annotators who are experts in Excel to obtain the human performance of our benchmark. To minimize costs, we use a subset of fifty instructions and three test cases for each instruction to conduct our human evaluation. Annotators are asked to provide formula solutions or VBA code to finish each instruction instead of filling in the answer in the target position directly. The process of human evaluation is completed within one day.

**Overall Performance.** The results shown in Table 2 indicate that current LLMs and spreadsheet agents are inadequate in managing complex spreadsheet manipulation tasks as required by real-world scenarios. Even the most advanced spreadsheet agent, Copilot in Excel, only achieves an accuracy of roughly 20%. GPT-4o, the SOTA LLM, scores around 17% in accuracy, aligning with Copilot in Excel's performance. Open-source LLMs significantly underperform compared to the SOTA model, likely due to their limited comprehension and coding proficiency. Binder shows the poorest results, while TaPEx and TaPas, designed specifically for TableQA, score zero across all metrics (omitted from the table). This underscores the distinction in difficulty between TableQA and spreadsheet manipulation. Overall, there is a substantial gap between existing LLMs or products and human performance produced by Excel experts, emphasizing the critical need for advancement in LLMs tailored for spreadsheet manipulation.

Figure 4: Impact of input row size and round number in terms of overall soft restriction.

   Task Subset & \% of Total & Accuracy \\  Rows (\( 50\)) & 75.19 & **20.63** \\ Rows (\(>50\)) & 24.81 & 11.50 \\  Columns (\( 10\)) & 65.53 & **22.50** \\ Columns (\(>10\)) & 34.47 & 10.51 \\  Single Tab. & 62.90 & **21.12** \\ Multiple Tab. & 37.10 & 13.71 \\  Relational Tab. & 55.54 & **19.50** \\ Non-standard Tab. & 44.46 & 16.95 \\   

Table 3: Overall soft restriction of GPT-4o on different subsets (%).

Alongside the performance evaluation of each model, our findings include the following insights:

(1) _Code Model vs. General Model._ Given that spreadsheet manipulation solutions primarily rely on coding, LLMs tailored for coding, such as DeepseeKCoder, exhibit superior performance compared to open-source general models like Llama-3 (70B). This highlights the need to enhance coding capacities within LLMs for effective spreadsheet manipulation.

(2) _Single Round vs. Multiple Round._ The majority of LLMs experience significant improvement in their capabilities through multi-round conversation, with some models achieving nearly a tenfold increase. However, GPT-4o's performance slightly declines in a multi-round setting. This is due to GPT-4o's adherence to instructions, which leads to duplicated content when retrieving the spreadsheet, as it includes the initial spreadsheet rows already provided in the prompt. Other LLMs, unable to follow the retrieval instruction, rely on the given initial rows. We provide the performance of GPT-4o without the initial spreadsheet rows in Appendix C.

(3) _Soft Restriction vs. Hard Restriction._ The shift from soft to hard restrictions leads to a modest decrease in LLM performance, suggesting that the solutions produced by these models may not remain effective when the spreadsheet content is altered. Given that real-world spreadsheet applications often involve frequent content changes, it is crucial to develop solutions that are robust to such modifications. The introduced OJ-style evaluation metrics are well-suited to assess such robustness.

**Analysis.** We also conduct analysis on the factors that influence the performance of LLMs on the spreadsheet manipulation task.

(1) _Great spreadsheet complexity leads to diminished performance._ Our benchmark is organized into four categories based on row and column size, the presence of multiple tables, and the inclusion of non-standard tables, resulting in eight distinct subsets, as detailed in Table 3. Our analysis reveals that model performance notably declines when dealing with spreadsheets that have extensive rows and columns, multiple tables, and non-standard structures.

(2) _Increase row size, no significant performance gain._ We assess the effect of row size in the prompt under a single round setting. Figure 4(a) shows that expanding input rows from 5 to 10 doesn't notably enhance performance. This could be due to the excessive context length when processing additional rows.

(3) _GPT4o does not benefit from the multi-round setting._ As shown in Figure 4(b), the performance of GPT-3.5 improves as the round number increases from 2 to 4. However, GPT-4o, the current most advanced LLM, does not benefit from the multi-round setting. Our analysis shows that GPT-4o's

    &  &  \\   & Cell-Level & Sheet-Level & Overall & Cell-Level & Sheet-Level & Overall \\  Binder (GPT-3.5) & 1.58 & 0.05 & 1.17 & 0.00 & 0.00 & 0.00 \\  CodeQwen (7B) & 0.36 & 0.76 & 0.51 & 0.36 & 0.29 & 0.33 \\ w / Multi-Round & 1.49 & 7.14 & 3.66 & 0.89 & 6.29 & 2.97 \\ DeepseeKCoder (33B) & 0.59 & 5.81 & 2.60 & 0.36 & 5.14 & 2.20 \\ w / Multi-Round & 3.15 & 8.76 & 5.31 & 1.96 & 6.86 & 3.85 \\  Mixtral-8x7B & 2.97 & 3.33 & 3.11 & 2.32 & 2.57 & 2.42 \\ w / Multi-Round & 3.39 & 4.67 & 3.88 & 2.32 & 3.71 & 2.85 \\ Llama-3 (70B) & 0.18 & 3.14 & 1.32 & 0.00 & 2.86 & 1.10 \\ w / Multi-Round & 1.13 & 7.90 & 3.74 & 0.71 & 7.14 & 3.18 \\  GPT-3 & 1.31 & 3.99 & 2.34 & 0.71 & 3.13 & 1.64 \\ w / Multi-Round & 3.33 & 13.11 & 7.09 & 2.50 & 9.97 & 5.37 \\ GPT-4o & 15.03 & **23.65** & 18.35 & **11.94** & **19.94** & **15.02** \\ w / Multi-Round & 13.49 & 22.51 & 16.96 & 10.52 & 17.66 & 13.27 \\  SheetCopilot (GPT-4)* & 16.67 & 10.00 & 14.00 & - & - & - \\ Copilot in Excel* & **23.33** & 15.00 & **20.00** & - & - & - \\  Human Performance & 75.56 & 65.00 & 71.33 & 66.67 & 55.00 & 62.00 \\   

Table 2: Performance of representative models on SpreadsheetBench (%).

initial solutions have a substantially higher rate of executability and accuracy than other models. Consequently, the additional step of repeatedly retrieving spreadsheet content and executing code does not markedly improve GPT-4o's performance.

**Case Study.** We provide two cases (Figure 6 and Figure 9) and conduct qualitative analysis of the LLM's performance on different characteristics of the spreadsheet. To get the code solution of these two cases, we utilize the multi-round setting that is discussed in the inference setting section.

Example 1 shows an instruction that manipulates a non-standard relational table with formatting in spreadsheet. Both GPT-3.5 and GPT-4 understand the question and read the spreadsheet properly. However, due to the lack of coding ability, GPT-3.5 uses the wrong element "correct_results[cell.column]" (marked as red), as shown in Figure 7. The correct one should be "correct_results[cell.column - 1].value". In Figure 8, GPT-4o can generate the correct Python code, highlighting the importance of coding ability for the accuracy of spreadsheet manipulation.

Example 2 shows an instruction that manipulates two tables in one sheet with formatting. The two GPT-4 inference results both show issues with understanding the structure of the spreadsheet. For the first result of GPT-4o in Figure 10, misalignment occures when reading the tables. "5A1" belong to the column names "Green" but is highlighted in yellow. "5A3" belong to the column names "Amber" but is highlighted in red. For the second result of GPT-4o in Figure 11, the model did not read the whole lookup table in column E to G. The rows after the tenth row have been ignored. Thus, "1A4", "D2", and "1A1" are not highlighted.

These two examples show that proficient coding skills and a solid understanding of various spreadsheet structures are essential for the spreadsheet manipulation task.

## 4 Conclusion

We create a rigorous benchmark, SpreadsheetBench, for evaluating LLMs' capacity in spreadsheet manipulating. Compared with existing benchmarks, SpreadsheetBench incorporates 912 authentic instructions sourced from online forums, more diverse spreadsheet files with rich formats, multiple tables, irregular tables, and more comprehensive testing with multiple test suites, ensuring better coverage of corner cases.