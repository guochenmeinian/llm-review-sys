# Generalized Linear Bandits with Limited Adaptivity

Ayush Sawarni

Stanford University

ayushsaw@stanford.edu

&Nirjhar Das

Indian Institute of Science Bangalore

nirjhardas@iisc.ac.in

&Siddharth Barman

Indian Institute of Science Bangalore

barman@iisc.ac.in

&Gaurav Sinha

Microsoft Research India

gauravsinha@microsoft.com

Work done while author was at Microsoft Research IndiaWork done while author was at Microsoft Research India

###### Abstract

We study the generalized linear contextual bandit problem within the constraints of limited adaptivity. In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity settings. Given a budget \(M\) on the number of policy updates, in the first setting, the algorithm needs to decide upfront \(M\) rounds at which it will update its policy, while in the second setting it can adaptively perform \(M\) policy updates during its course. For the first setting, we design an algorithm B-GLinCB, that incurs \(()\) regret when \(M=( T)\) and the arm feature vectors are generated stochastically. For the second setting, we design an algorithm RS-GLinCB that updates its policy \((^{2}T)\) times and achieves a regret of \(()\) even when the arm feature vectors are adversarially generated. Notably, in these bounds, we manage to eliminate the dependence on a key instance dependent parameter \(\), that captures non-linearity of the underlying reward model. Our novel approach for removing this dependence for generalized linear contextual bandits might be of independent interest.

## 1 Introduction

Contextual Bandits (CB) is an archetypal framework that models sequential decision making in time-varying environments. In this framework, the algorithm (decision maker) is presented, in each round, with a set of arms (represented as \(d\)-dimensional feature vectors), and it needs to decide which arm to play. Once an arm is played, a reward corresponding to the played arm is accrued. The regret of the round is defined as the difference between the maximum reward possible in that round and the reward of the played arm. The goal is to design a policy for selecting arms that minimizes cumulative regret (referred to as the regret of the algorithm) over a specified number of rounds, \(T\). In the last few decades, much progress has been made in designing algorithms for special classes of reward models, e.g. linear model , logistic model  and generalized linear models .

However, despite this progress, there is a key challenge that prevents deployment of CB algorithms in the real world. Practical situations often allow for very limited adaptivity, i.e., do not allow CB algorithms to update their policy at all rounds. For example, in clinical trials , each trial involves administering medical treatments to a cohort of patients, with medical outcomes observed and collected for the entire cohort at the conclusion of the trial. This data is then used to design the treatment for the next phase of the trial. Similarly, in online advertising  and recommendations , updating the policy after every iteration during deployment is often infeasible due to infrastructural constraints. A recent line of work  tries to address this limitationby developing algorithms that try to minimize cumulative regret while ensuring that only a limited number of policy updates occur. Across these works, two settings (called **M1** and **M2** from here onwards) of limited adaptivity have been popular. Both **M1, M2** provide a budget \(M\) to the algorithm, determining the number of times it can update its policy. In **M1**, the algorithm is required to decide upfront a sub-sequence of \(M\) rounds where policy updates will occur. While in **M2**), the algorithm is allowed to adaptively decide (during its course) when to update its policy.

Limited adaptivity algorithms were recently proposed for the CB problem with linear reward models under the **M1** setting , and optimal regret guarantees were obtained when the arm feature vectors were stochastically generated. Similarly, in their seminal work on linear bandits,  developed algorithms for the **M2** setting and proved optimal regret guarantees with no restrictions on the arm vectors. While these results provide tight regret guarantees for linear reward models, extending them to generalized linear models is quite a challenge. Straightforward extensions lead to sub-optimal regret with a significantly worse dependence on an instance dependent parameter \(\) (See Section 2 for definition) that captures non-linearity of the problem instance. In fact, to the best of our knowledge, developing optimal algorithms for the CB problem with generalized linear reward models under the limited adaptivity settings **M1**, **M2**, is an open research question. This is the main focus of our work. We make the following contributions.

### Our Contributions

\(\) We propose B-GLinCB, an algorithm that solves the CB problem for bounded (almost surely) generalized linear reward models (Definition 2.1) under the **M1** setting of limited adaptivity. We prove that, when the arm feature vectors are generated stochastically, the regret of B-GLinCB at the end of \(T\) rounds is \(()\), when \(M=( T)\). When \(M=O( T)\), we prove an \((T^{2^{M-1}/(2^{M}-2)})\) regret guarantee. While the algorithm bears a slight resemblance to the one in , direct utilization of their key techniques (distributional optimal design) results in a regret guarantee that scales linearly with the instance dependent non-linearity \(\). On the other hand, the leading terms in our regret guarantee for B-GLinCB have no dependence on \(\). To achieve this, we make novel modifications to the key technique of distributional optimal design in . Along with this, the rounds for policy updates are also chosen more carefully (in a \(\) dependent fashion), leading to a stronger regret guarantee.

\(\) We propose RS-GLinCB, an algorithm that solves the CB problem for bounded (almost surely) generalized linear reward models (Definition 2.1) under the **M2** setting of limited adaptivity. RS-GLinCB builds on a similar algorithm in  by adding a novel context-dependent criterion for determining if a policy update is needed. This new criterion allows us to prove optimal regret guarantee (\(()\)) with only \(O(^{2}T)\) updates to the policy. It is quite crucial for the generalized linear reward settings since, without it, the resultant regret guarantees have a linear dependence on \(\).

\(\) Our work also resolves a conjecture in  by proving an optimal (\(()\)) regret guarantee (for the CB problem with logistic reward model) that does not depend polynomially on \(S\) (the known upper bound on the size of the model parameters, i.e. \(\|^{}\| S\), See Section 2) 3. RS-GLinCB is, to our knowledge, the first CB algorithm for generalized linear reward models that is both computationally efficient (amortized \(O( T)\) computation per round) and incurs optimal regret. We also perform experiments in Section 5 that validate its superiority both in terms of regret and computational efficiency in comparison to other baseline algorithms proposed in  and .

### Important Remarks on Contributions and Comparison with Prior Work

_Remark 1.1_ (\(\)**-independence**).: For both B-GLinCB and RS-GLinCB, our regret guarantees are free of \(\) (in their leading term), an instance-dependent parameter that can be exponential in the size of the unknown parameter vector, i.e., \(\|^{}\|\) (See Section 2 for definition). Our contribution in this regard is two-fold. Not only do we prove \(\)-independent regret guarantees under the limited adaptivity constraint, we also characterize a broad class of generalized linear reward models for which a \(\)-independent regret guarantee can be achieved. Specifically, our results imply that the CB problem with generalized linear reward models originally proposed in  and subsequently studied in literature  admits a \(\)-independent regret.

_Remark 1.2_ (**Computational efficiency**).: Efforts to reduce the total time complexity to be linear in \(T\) have been active in the CB literature with generealized linear rewards models. For e.g.,  recently devised computationally efficient algorithms but they suffer from regret dependence on \(\). Optimal (\(\)-independent) guarantees were recently achieved for logistic reward models [6; 2], and the algorithms were subsequently made computationally efficient in [7; 28]. However, the techniques involved rely heavily on the structure of the logistic model and do not easily extend to more general models. To the best of our knowledge, ours is the first work that achieves optimal \(\)-independent regret guarantees for bounded generalized linear reward models while remaining computationally efficient4.

_Remark 1.3_ (**Self Concordance of bounded GLMs**).: In order to prove \(\)-independent regret guarantees, we prove a key result about self concordance of bounded (almost surely) generalized linear models (Definition 2.1) in Lemma 2.2. This result was postulated in  for GLMs (with the same definition as ours), but no proof was provided. While [6; 7] partially tackled this issue for logistic reward models5, in our work, we prove self concordance for much more general generalized linear models.

## 2 Notations and Preliminaries

**Notations:** A policy \(\) is a function that maps any given arm set \(\) to a probability distribution over the same set, i.e., \(()()\), where \(()\) is the probability simplex supported on \(\). We will denote matrices in bold upper case (e.g. \(\)). \(\|x\|\) denotes the \(_{2}\) norm of vector \(x\). We write \(\|x\|_{}\) to denote \(x}\) for a positive semi-definite matrix \(\) and vector \(x\). For any two real numbers \(a\) and \(b\), we denote by \(a b\) the minimum of \(a\) and \(b\). Throughout, \(()\) denotes big-O notation but suppresses log factors in all relevant parameters. For \(m,n\) with \(m<n\), we denote the set \(\{1,,n\}\) by \([n]\) and \(\{m,,n\}\) by \([m,n]\).

**Definition 2.1** (Glm).: A Generalized Linear Model or GLM with parameter vector \(^{*}^{d}\) is a real valued random variable \(r\) that belongs to the exponential family with density function

\[(r x)=(r x,^{*}-b ( x,^{*})+c(r))\]

Function \(b\) (called the log-partition function) is assumed to be twice differentiable and \(\) is assumed to be monotone. Further, we assume that \(r[0,R]\) almost surely for some known \(R\).

Important properties of GLMs such as \([r x]=( x,^{*})\) and variance \([r x]=( x,^{*})\) are detailed in Appendix C. We define the link function \(\) as \(( x,^{*}):=[r x]\). Thus, \(\) is also monotone. We now present a key Lemma on GLMs (see Appendix C for details) that enables us to achieve optimal regret guarantees for our algorithms designed in Sections 3 and 4.

**Lemma 2.2** (Self-Concordance of GLMs).: _For any GLM supported on \([0,R]\) almost surely, the link function \(()\) satisfies \([(z)] R(z)\), for all \(z\)._

Next we describe the two CB problems with GLM rewards that we address in this paper. Let \(T\) be the total number of rounds. At round \(t[T]\), we receive an arm set \(_{t}^{d}\), with number of arms \(K=|_{t}|\) and must select an arm \(x_{t}_{t}\). Following this, we receive a reward \(r_{t}\) sampled from the GLM distribution \((r|x_{t})\) with unknown \(^{*}\).

**Problem 1:** In this problem we assume that at each round \(t\), the set of arms \(_{t}^{d}\) is drawn from an unknown distribution \(\). Further, we assume the constraints of limited adaptivity setting \(\), i.e., the algorithm is given a budget \(M\) and needs to decide upfront the \(M\) rounds at which it will update its policy. Let \(()\) denote the support of distribution \(\). We want to design an algorithm that minimizes the expected cumulative regret given as

\[_{T}=[\,_{t=1}^{T}_{x_{t}} ( x,^{*})\;-\;_{t=1}^{T}(  x_{t},^{*})\,]\]Here, the expectation is taken over the randomness of the algorithm, the distribution of rewards \(r_{t}\), and the distribution of the arm set \(\).

**Problem 2:** In this problem we do not make any assumptions on the arm feature vectors, i.e., the arm vectors can be adversarially chosen. However, we assume the constraints of limited adaptivity setting **M2**, i.e., the algorithm is given a budget \(M\) and needs to adaptively decide the \(M\) rounds at which it will update its policy (during its course). We want to design an algorithm that minimizes the cumulative regret given as

\[_{T}=_{t=1}^{T}_{x_{t}}( x, ^{*})\;-\;_{t=1}^{T}( x_{t}, ^{*})\]

Finally, for both the problems, the Maximum Likelihood Estimator (MLE) of \(^{*}\) can be calculated by minimizing the sum of the log-losses. The log-loss is defined for any given arm \(x\), its (stochastic) reward \(r\) and vector \(^{d}\) (as the estimator of the true unknown \(^{*}\)) as follows: \((,x,r)-r x,+_{0}^{ x,}(z)dz\). After \(t\) rounds, the MLE \(\) is computed as \(=_{}_{s=1}^{t}(,x_{s},r_{s})\).

### Instance Dependent Non-Linearity Parameters

As in prior works [6; 7], we define instance dependent parameters that capture non-linearity of the underlying instance and critically impact our algorithm design. The performance of Algorithm 1 (B-GLinCB) that solves Problem 1, can be quantified using three such parameters that are defined using the derivative of the link function \(()\). Specifically, for any arm set \(\), write optimal arm \(x^{*}=_{x}( x,^{*})\) and define,

\[_{()}_{x }\;( x,^{*} )},}_{( )}\;( x^{*},^{*}),}}_{ }[( x^{*},^{*} )]\] (1)

_Remark 2.3_.: These quantities feature prominently in our regret analysis of Algorithm 1. In particular, the dominant term in our regret bound scales as \(O(})\). We also note that \(^{*}\); in fact, for specific distributions \(\), the gap between them can be significant. Hence, we also provide a regret upper bound of \(O(})\). In this latter case, however, we incur a worse dependence on \(d\). Section 3 provides a quantified form of this trade-off.

Algorithm 2 (RS-GLinCB) that solves Problem 2, requires another such non-linearity parameter \(\)6, defined as,

\[_{x_{t=1}^{T}_{t}} ( x,^{*})}\] (2)

We note that, here, \(\) is defined considering the parameter vector \(^{*}\) in contrast to prior work on logistic bandits , where its definition involved a maximization over all vectors \(\) with \(\|\| S\) (known upper bound of \(\|^{*}\|\)). Hence, \(\) as defined here is potentially much smaller and can lead to lower regret, compared to prior works. Standard to the CB literature with GLM rewards, we will assume that tight upper bounds on these parameters is known to the algorithms.

**Assumption 2.4**.: We make the following additional assumptions which are standard for the CB problem with linear or GLM reward models.

\(\) For every round \(t[T]\), and each arm \(x_{t}\), \(\|x\| 1\).

\(\) Let \(^{*}\) be the unknown parameter of the GLM reward, then \(\|^{*}\| S\) for a known constant \(S\).

### Optimal Design Policies

**G-optimal Design** Given an arm set \(\), the G-optimal design policy \(_{G}\) is the solution of the following optimization problem: \(_{()}_{x}\|x\|_{()^{-1}}^{2}\), where \(()=_{x}[xx^{}]\). Now consider the following optimization problem, also known as the D-optimal design problem: \(_{()}(())\). This is a concave maximization problem as opposed to the G-optimal design which is non-convex. We have the following equivalence theorem due to Kiefer and Wolfowitz :

**Lemma 2.5** (Keifer-Wolfowitz).: _Let \(^{d}\) be any set of arms and \(_{G}\) be the expected design matrix, defined as \(_{G}_{x_{G}()}[xx^{}]\), with \(_{G}()\) as the solution to the D-optimal design problem. Then, \(_{G}()\) also solves the G-optimal design problem, and for all \(x\), \(\|x\|_{_{G}^{-1}}^{2} d\)._

**Distributional optimal design** Notably, the upper bound on \(\|x\|_{_{G}^{-1}}\) specified in Lemma 2.5 holds only for the arms \(x\) in \(\). When the arm set \(_{t}\) varies from round to round, securing a guarantee analogous to Lemma 2.5 is generally challenging. Nonetheless, when the arm sets \(_{t}\) are drawn from a distribution, it is possible to extend the guarantee, albeit with a worse dependence on \(d\); see Section A.5 in Appendix A. Improving this dependence motivates the need of studying Distributional optimal design and towards this we utilize the results of .

The distributional optimal design policy is defined using a collection of tuples \(=\{(p_{i},_{i}):p_{1},,p_{n} 0_{i}p_{i}=1\}\), wherein each \(_{i}\) is a \(d d\) positive semi-definite matrix and \(n 4d d\). The collection \(\) is detailed next. Let \(_{}(\{s_{1},,s_{k}\})\) denote the probability distribution where the \(i^{th}\) element is sampled with probability \(^{n}s_{j}^{n}}{_{j=1}^{n}s_{j}^{n}}\). For a specific \(=\{(p_{i},_{i})\}_{i=1}^{n}\), and each \(i[n]\) write \(_{_{i}}()=_{}(\{ \|x\|_{_{i}}^{2}:x\})\). Finally, with \(_{G}\) as the G-optimal design policy (Section 2.2), we define the Distributional optimal design policy \(\) as

\[()=_{G}()& {with probability }1/2\\ _{_{i}}()&p_{i}/2 \]

Given a collection of arm sets \(\{_{1},,_{s}\}\) (called _core set_) sampled from the distribution \(\), we utilize Algorithm 2 of  to find the collection \(\); see Algorithm 4 of . Overall, the computed \(\) induces a policy \(\) that upholds the following guarantee.

**Lemma 2.6** (Theorem 5, ).: _Let \(\) be the Distributional optimal design policy that has been learnt from \(s\) independent samples \(_{1},_{s}\). Also, let \(\) denote the expected design matrix, \(=_{}[_{x ()}[xx^{}]]\). Then,_

\[\{_{}[_{x }\ \|x\|_{^{-1}}] O()\} 1 -(O(d^{4}^{2}d)-sd^{-12} 2^{-16}).\]

## 3 B-GLinCB

In this section, we present B-GLinCB (Algorithm 1) that solves **Problem 1** described in Section 2, which enforces constraints of limited adaptivity setting **M1**. Given limited adaptivity budget \(M\), our algorithm first computes the batch length for each of the \(M\) batches (i.e., determine rounds wherethe policy remains constant). We build upon the batch length construction in ; however, the first batch is chosen to be \(\) dependent which crucially helps in removing \(\) from the leading term in the regret. 7

**Batch Lengths**: For each batch \(k[M]\), let \(_{k}\) denote all the rounds within the \(k^{th}\) batch. We will refer to the first batch \(_{1}\) as the warm-up batch. The batch lengths \(_{k}|_{k}|\), \(k[M]\) are calculated as follows:

\[_{1}:=(\,e^{3S}d^{2}^{2}}{S})^{ 2/3},_{2}:=,_{k}:=},k[3,M]\] (3)

where \( 30RS\)8 and \(=T^{)}}\) if \(M T\) and \(=2\) otherwise.

During the warm-up batch (Lines 2, 3), the algorithm follows the G-optimal design policy, \(_{G}\). At the end of the warm-up batch (Line 4), the algorithm computes the Maximum Likelihood Estimate (MLE), \(_{w}\), of \(^{*}\)9, and design matrix \(_{t_{k}}x_{t}x_{t}^{}+ \), with parameter \(=20Rd T\).

Now, for each batch \(k 2\) and every round \(t_{k}\), the algorithm updates \(_{t}\) by eliminating arms from it using the confidence bounds (see Equation (7)) computed in the previous batches (Line 10). The algorithm next computes \(}_{t}\), a scaled version of \(_{t}\), as follows, with \((x)\) define in equation (5),

\[}_{t}\{( x, {}_{w})/(x)}\ \ x:\ x_{t}\}.\] (4)

Finally, we use the distributional optimal design policy \(_{k}\), on the scaled arm set \(}_{t}\), to sample the next arm (Line 11). At the end of every batch, we equally divide the batch \(_{k}\) into two sets \(\) and \(\). We use samples from \(\) to compute the estimator \(_{k}\) and the scaled design matrix \(_{k}\). The rounds in \(\) are used to compute \(_{k+1}\), the distributional optimal design policy for the next batch. It is important to note while the policy \(_{k}\) is utilized in each round (Line 11) to draw arms, it is updated (to \(_{k+1}\)) only at the end of the batch. Hence, conforming to setting **M1**, the algorithm updates the selection policy at \(M\) rounds that were decided upfront.

**Confidence Bounds:** The scaled design matrix \(_{k}\), an estimator of the Hessian, is computed at the end of each batch \(k 2,,M\) (Line 13):

\[_{k}=_{t}(( x_{t}, {}_{w})/(x_{t}))x_{t}x_{t}^{}+ ,(x)=(R\{2S, \|x\|_{^{-1}}\})\] (5)

where \(\) is the first half of \(_{k}\). Using this, we define the upper and lower confidence bounds (\(UCB_{k}\) and \(LCB_{k}\)) computed at the end of batch \(_{k}\):

\[UCB_{k}(x)  x,_{w}+ \|x\|_{^{-1}}&k=1\\  x,_{k}+\|x\|_{_{k}^ {-1}}&k>1,\] (6) \[LCB_{k}(x)  x,_{w}- \|x\|_{^{-1}}&k=1\\  x,_{k}-\|x\|_{_{k}^ {-1}}&k>1\] (7)

_Remark 3.1_.: The confidence bounds employed by the algorithm exhibit a significant distinction between the first batch and subsequent batches. While the first batch's bounds are influenced by the parameter \(\), subsequent batches utilize \(\)-independent bounds. This difference arises from the use of the standard design matrix \(\) in the first batch and a scaled design matrix \(_{k}\) (equation 5) in later batches, leveraging the self-concordance property of GLM rewards to achieve \(\)-independence. Notably, the first batch's confidence bounds influence the scaling factor \((x)\) in later batches, creating a trade-off (addressed in the regret analysis in Appendix A) where an inaccurate estimate of \(_{w}\) can exponentially increase the scaling factor and confidence bounds.

In Theorem 3.2 and Corollary 3.3, we present our regret guarantee for B-GLinCB. Detailed proofs for both are provided in Appendix A. The computational efficiency of B-GLinCB is discussed in Appendix D.

**Theorem 3.2**.: _Algorithm 1 (B-GLinCB) incurs regret \(_{T}(_{1}+_{2}) T\)10, where_

\[_{1} =O(RSd\ (}} }})T^{)}} T) { and}\] \[_{2} =O(^{1/3}d^{2}e^{2S}(RS T)^{2/3}T^{)}}).\]

**Corollary 3.3**.: _When the number of batches \(M T\), Algorithm 1 achieves a regret bound of_

\[_{T}((} }}})dRS+d^{2}e^{2S}(S^{2}R^{2}  T)^{1/3}).\]

_Remark 3.4_.: Scaling the arm set (as in (4)) for optimal design is a crucial aspect of our algorithm, allowing us to obtain tight estimates of \(( x,^{*})\) (see Lemma A.10). This result relies on multiple novel ideas and techniques, including self-concordance for GLMs, matrix concentration, Bernstein-type concentration for the canonical exponential family (Lemma A.1), and application of distributional optimal design on scaled arm set.

_Remark 3.5_.: The \(\)-dependent batch construction is a crucial feature of our algorithm, enabling effective estimation of \(( x,^{*})\) at the end of the first batch. Since the first batch incurs regret linear in its length, achieving a \(\)-independent guarantee requires the first batch to be \(o()\). We demonstrate that choosing \(_{1}=O(T^{})\) is sufficient for this purpose (see Appendix A).

## 4 Rs-GLinCB

In this section we present RS-GLinCB (Algorithm 2) that solves **Problem 2** described in Section 2, which enforces constraints of limited adaptivity setting **M2**. This algorithm incorporates a novel switching criterion (Line 4), extending the determinant-doubling approach of . Additionally, we introduce an arm-elimination step (Line 12) to obtain tighter regret guarantees. Throughout this section, we set \(=d(T/)/R^{2}\) and \(=25RS\).

At round \(t\), on receiving an arm set \(_{t}\), RS-GLinCB first checks the Switching Criterion I (Line 4). This criterion checks whether for any arm \(x_{t}\) the quantity \( x_{^{-1}}\) is greater than a carefully chosen \(\)-dependent threshold. Here \(\) is the design matrix corresponding to all arms that have been played in the rounds in \(_{o}\) (\(:=\) the set of rounds preceding round \(t\), where Switching Criterion I was triggered). Under this criterion the arm that maximizes \( x_{^{-1}}\) is played (call this arm \(x_{t}\)) and the corresponding reward is obtained. Subsequently in Line 6, the set \(_{o}\) is updated to include \(t\); thedesign matrix \(\) is updated as \(+x_{t}x_{t}^{}\); and the scaled design matrix \(_{t+1}\) is set to \(_{t}\). The MLE is computed (Line 7) based on the data in the rounds in \(_{o}\) to obtain \(_{o}\).

When Switching Criterion I is not triggered, the algorithm first checks (Line 9) the Switching Criterion II, that is whether the determinant of the scaled design matrix \(_{t}\) has become more than double of that of \(_{}\) (where \(\) is the last round before \(t\) when Switching Criterion II was triggered). If Switching Criterion II is triggered at round \(t\), then in Line 10, the algorithm sets \( t\) and recomputes the MLE over all the past rounds except those in \(_{o}\) to obtain \(\). Then \(\) is projected into an ellipsoid around \(_{o}\) to obtain the estimate \(_{}\) via the following optimization problem11,

\[_{}\|_{s_{o}}(( x_{s },)-( x_{s}, ))x_{s}\|_{()}\|- _{o}\|_{}.\] (8)

Here \(()_{s_{o}}( x _{s},)x_{s}x_{s}^{}\). After checking Switching Criterion II, the algorithm performs an arm elimination step (Line 12) based on the parameter estimate \(_{o}\) as follows: for every arm \(x_{t}\), we compute \(UCB_{o}(x)= x,_{o}+\|x\|_{^{-1}}\) and \(LCB_{o}(x)= x,_{o}-\|x\|_{^{-1}}\)12. Then, \(_{t}\) is updated by eliminating from it the arms with \(UCB_{o}()\) less than the highest \(LCB_{o}()\). For arms in the reduced arm set \(_{t}\), RS-GLinCB computes the index \(UCB(x,_{},_{}) x, _{}+150\|x\|_{_{}^{ -1}}\), and plays the arm \(x_{t}\) with the highest index (Line 13). After observing the subsequent reward \(r_{t}\), the algorithm updates the scaled design matrix \(_{t}\) (Line 14) as follows: \(_{t+1}_{t}+(( x_{t}, _{o})/e)x_{t}x_{t}^{}\). With this, the round \(t\) ends and the algorithm moves to the next round. Next, in Lemma 4.1 and Theorem 4.2 we present the guarantees on number of policy updates and regret, respectively, for RS-GLinCB. Detailed proofs for both are provided in Appendix B.

**Lemma 4.1**.: _RS-GLinCB (Algorithm 2), during its entire execution, updates its policy at most \(O(R^{4}S^{2}\; d^{2}\;^{2}(T/))\) times._

**Theorem 4.2**.: _Given \((0,1)\), with probability \( 1-\), the regret of RS-GLinCB (Algorithm 2) satisfies \(_{T}=Od( x_{t}^ {},^{})}(RT/)+\; d ^{2}R^{5}S^{2}^{2}(T/).\)_

_Remark 4.3_.: Switching Criterion I is essential in delivering tight regret guarantees in the non-linear setting. Unlike existing literature , which relies on warm-up rounds based on observed rewards (hence heavily dependent on reward models), RS-GLinCB presents a context-dependent criterion that implicitly checks whether the estimate \(( x,_{o})\) is within a constant factor of \(( x,^{})\) (see Lemmas B.3 and B.4). We show that the number of times Switching Criterion I is triggered is only \(O( d^{2}^{2}(T))\) (see Lemma B.11), hence incurring a small regret in these rounds.

_Remark 4.4_.: Unlike , our determinant-doubling Switching Criterion II uses the scaled design matrix \(_{t}\) instead of the unscaled version (similar to \(\)). The matrix \(_{t}\), estimating the Hessian of the log-loss, is crucial for achieving optimal regret. This modification is crucial in extending algorithms satisfying limited adaptivity setting \(\) for the CB problem with a linear reward model to more general GLM reward models.

_Remark 4.5_.: The feasible set for the optimization stated in \(8\) is an ellipsoid around \(_{o}\), which contains \(^{}\) with high probability. Deviating from existing literature on GLM Bandits which projects the estimate into the ball set of radius \(S\) (\(\{:\|\| S\}\)), our projection step leads to tighter regret guarantees; notably, the leading \(\) term is free of parameters \(S\) (and \(R\)). This resolves the conjecture made in  regarding the possibility of obtaining \(S\)-free regret in the \(\) term in logistic bandits.

_Remark 4.6_.: The regret guarantees of the logistic bandit algorithms in  have a second-order term that is minimum of an arm-geometry dependent quantity (see Theorem 3 of ) and a \(\)-dependent term similar to our regret guarantee. Although our analysis is not able to accommodate this arm-geometry dependent quantity, we underscore that our algorithm is computationally efficient while the above works are not. In fact, to the best of our knowledge, the other known efficient algorithms for logistic bandits  also do not achieve the arm-geometry dependent regret term. It can be interesting to design an efficient algorithm that is able to achieve the same guarantees in the second-order regret term as in .

## 5 Experiments

We tested the practicality of our algorithm RS-GLinCB against various baselines for logistic and generalized linear bandits. For these experiments, we adjusted the Switching Criterion I threshold constant in RS-GLinCB to \(0.01\) and used data from both Switching Criteria (I and II) rounds to estimate \(\). These modifications do not affect the overall efficiency as \(\) is calculated only \(O((T))\) times. The experiment code is available at https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity.

**Logistic.** We compared RS-GLinCB against ECOLog and GLOC, the only algorithms with overall time complexity \((T)\) for this setting. The dimension was set to \(d=5\), number of arms per round to \(K=20\), and \(^{*}\) was sampled from a \(d\)-dimensional sphere of radius \(S=5\). Arms were sampled uniformly from the \(d\)-dimensional unit ball. We ran simulations for \(T=20,000\) rounds, repeating them 10 times. RS-GLinCB showed the smallest regret with a flattened regret curve, as seen in Fig. 1 (top-left).

**Probit.** For the probit reward model, we compared RS-GLinCB against GLOC and GLM-UCB. The dimension was set to \(d=5\) and number of arms per round to \(K=20\). \(^{*}\) was sampled from a \(d\)-dimensional sphere of radius \(S=3\). Arm features were generated similarly as in the logistic bandit simulation. We ran simulations for \(T=5,000\) rounds, repeating them 10 times. RS-GLinCB outperformed both baselines, as shown in Fig. 1 (top-right).

**Comparing Execution Times.** We compared the execution times of RS-GLinCB and ECOLog. We created two logistic bandit instances with \(d=5\) and \(K=20\), and different \(\) values. We ran both algorithms for \(T=20,000\) rounds, repeating each run 20 times. For low \(\), RS-GLinCB took about one-fifth of the time of ECOLog, and for high \(\), slightly more than one-third, as seen in Fig. 1 (left-bottom). This demonstrates that RS-GLinCB has a significantly lower computational overhead compared to ECOLog. We also compared the execution times of RS-GLinCB and GLOC under the probit reward model, creating two bandit instances with \(d=5\) and \(K=20\), but with differing \(\). We ran both algorithms for \(T=20,000\) rounds, repeating each run 20 times. The result is shown in Fig. 1 (bottom-right). We observe that for low \(\), RS-GLinCB takes less than half time of GLOC while for high \(\), it takes about two-third time of GLOC. A more detailed discussion of these experiments is provided in Appendix D.

Figure 1: Top: Cumulative Regret vs. number of rounds for Logistic (left) and Probit (right) reward models. Bottom: (left) Execution times of ECOLog and RS-GLinCB for different values of \(\) (low \(=9.3\) and high \(=141.6\)) for Logistic rewards. (right) Execution times of GLOC and RS-GLinCB for different values of \(\) (low \(=17.6\) and high \(=202.3\)) for Probit rewards.

Conclusion and Future Work

The Contextual Bandit problem with GLM rewards is a ubiquitous framework for studying online decision-making with non-linear rewards. We study this problem with a focus on limited adaptivity. In particular, we design algorithms B-GLinCB and RS-GLinCB that obtain optimal regret guarantees for two prevalant limited adaptivity settings **M1** and **M2** respectively. A key feature of our guarantees are that their leading terms are independent of an instance dependent parameter \(\) that captures non-linearity. To the best of our knowledge, our paper provides the first algorithms for the CB problem with GLM rewards under limited adaptivity (and otherwise) that achieve \(\)-independent regret. The regret guarantee of RS-GLinCB, not only aligns with the best-known guarantees for Logistic Bandits but enhances them by removing the dependence on \(S\) (upper bound on \(\|^{*}\|\)) in the leading term of the regret and therefore resolves a conjecture in . The batch learning algorithm B-GLinCB, for \(M=()})\), achieves a regret of \((dRS(}}))\). We believe that the dependence on \(d\) along with the \(\) term is not tight and improving the dependence is a relevant direction for future work.