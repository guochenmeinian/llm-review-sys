# Accelerating Relative Entropy Coding

with Space Partitioning

 Jiajun He

University of Cambridge

jh2383@cam.ac.uk

&Gergely Flamich

University of Cambridge

gf332@cam.ac.uk

&Jose Miguel Hernandez-Lobato

University of Cambridge

jmh233@cam.ac.uk

###### Abstract

Relative entropy coding (REC) algorithms encode a random sample following a target distribution \(Q\), using a coding distribution \(P\) shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of \(2^{D_{}[Q||P]}\), and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with \(D_{}[Q||P]\) about three times greater than what previous methods can manage, and reduces the bitrate by approximately 5-15% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10, compared to previous methods, significantly improving the practicality of REC for neural compression.

## 1 Introduction

Let's consider a two-party communication problem where the sender wants to transmit some data \(\) to the receiver. A widely used approach is transform coding (Balle et al., 2020), where \(\) is first transformed to a discrete variable \(\) and entropy coded to achieve an optimal codelength on average. However, directly finding a discrete \(\) is difficult in many scenarios. For example, in lossy image compression, \(\) represents some image, and \(\) represents the latent embedding output by an encoder network in a model akin to the variational auto-encoder (Kingma and Welling, 2013). A common solution to obtain a discrete \(\) is to first learn a continuous variable and quantize it (Balle et al., 2017).

However, perhaps surprisingly, there is also a way to directly handle a continuous \(\) in this pipeline. Specifically, instead of a deterministic value, the sender transmits a _random_ sample following a posterior \( Q_{|}\)1. These algorithms are referred to as _relative entropy coding_(REC, Flamich et al., 2020), and also known as _channel simulation_ or _reverse channel coding_(Theis and Yosri, 2022). Li and El Gamal (2018) showed that the codelength of such an algorithm is upper-bounded by the mutual information2 between \(\) and \(\) plus some logarithmic and constant overhead:

\[I[;]+_{2}(I[;]+1)+(1).\] (1)

REC has a clear advantage over quantization: quantization is a non-differentiable operation, while REC directly works for continuous variables and eases the training of some neural compression models, which highly rely on gradient descent. Also, Theis and Agustsson (2021) exemplified that stochastic encoders can be significantly better than their deterministic counterpart if we target realism.

However, the encoding time of REC algorithms is at least typically on the order of \(2^{D_{}[Q P]}\), which is prohibitively long in practice.3 While there are several works on accelerating REC (Flamich et al., 2022; Flamich and Theis, 2023; Flamich et al., 2024), so far they only work on highly limited problems. In fact, in practice, the common option to employ REC is to segment \(\) into the concatenation of independent blocks, denoted as \(=[_{1},_{2},,_{K}]\), where the coding cost of each block \(_{k}\) is approximately \(\) bits. This strategy reduces the runtime to \((K2^{})\). However, by Equation (1), the logarithmic and constant overhead only becomes negligible if \(I[;_{i}]\) is sufficiently large. For example, for the runtime to be feasible, \(\) is set between \(16\) to \(20\) in Havasi et al. (2019); Guo et al. (2024); He et al. (2023). In this case, the overhead will typically constitute 40% to 50% of the total mutual information, resulting in sub-optimal compression performance.

Our work's primary aim is to reduce the complexity of REC runtime for more practical settings. Specifically, our contributions are

* We propose a faster REC framework based on space partitioning. Equivalently, our method can be viewed as introducing a search heuristic to a standard REC algorithm, which can significantly reduce the algorithm's runtime when chosen appropriately. Furthermore, we provide theoretical analysis, showing that our method achieves a close codelength to Equation (1) with an extra cost \(\) that is negligible for some commonly used distributions in neural compression.
* We show that, interestingly, using different space partitioning and different search heuristics only influences the runtime but not the (upper bound on) codelength. Following this, we discuss two cases: 1) encoding exact samples from the target and 2) encoding approximate samples to further reduce the computational complexity.
* We draw attention to a hitherto unused fact for designing fast, practical relative entropy coding schemes: when the sender wishes to communicate vector-valued random variate \(\) to the receiver, we may assume that they share knowledge of the _dimension-wise mutual information_\(I[Z_{i};]\) for each dimension \(i\), as opposed to just the total \(I[;]\). Incorporating this information into our space partitioning scheme allows us to construct faster relative entropy coding algorithms.
* We conduct experiments on synthetic examples and neural codecs, including a VAE-based lossless codec on MNIST (LeCun and Cortes, 1998) and INR-based lossy codecs on CIFAR-10 (Krizhevsky et al., 2009). We demonstrate that our method can handle blocks with larger \(\) using much fewer samples and reduces the bitrate by approximately 5-15% compared to previous methods.

## 2 Preliminary

In this paper, we focus on accelerating the _Poisson functional representation_ (PFR; Li and El Gamal, 2018) and _ordered random coding_ (ORC; Theis and Yosri, 2022), and hence we discuss these in detail below. However, we note that our space partitioning scheme is also applicable to other relative entropy coding (REC) algorithms (Flamich and Theis, 2023; Flamich et al., 2024; Flamich, 2024).

**Relative entropy coding (REC).** Consider a two-party communication problem, where the sender wants to transmit some data \(\) to the receiver. However, instead of encoding \(\) directly, the sender first transforms \(\) into a representation \(\) that they encode. In REC, we allow \(\) to be stochastic with \( Q_{}\). Then, the goal of REC is to encode a _single_, _random_ realization \( Q_{}\) with the assumption that the sender and receiver share a coding distribution \(P\) and have access to common randomness \(S\). In practice, the latter can be achieved by a shared pseudo-random number generator (PRNG) and seed. Given these assumptions, the optimal coding cost is given by \([ S]\), as the code cannot depend on \(\) since the receiver doesn't know it. Surprisingly, \(\) can be encoded very efficiently, as Li and El Gamal (2018) show that

\[I[;][ S] I[; ]+_{2}(I[;]+1)+(1).\] (2)

Note that \(I[;]\) and hence \([ S]\) can be finite even if \(\) is continuous and \([]\) is infinite. Next, we describe a concrete scheme with which we can encode \(\) at such an efficiency.

**Poisson functional representation (PFR).** To encode a sample from the target distribution \(Q\) with density \(q\) using the coding distribution \(P\) with density \(p\), PFR (Li and El Gamal, 2018) starts by drawing a random sequence \(_{1},_{2},\) from \(P\) using the public random state \(S\). Furthermore, the sender draws a sequence of random times \(T_{1},T_{2},\) as follows:

\[T_{0}=0, T_{n} T_{n-1}+ T_{n}, T_{n}(1), n=1,2,\] (3)

Next, letting \(r=q/p\) be the density ratio, the sender calculates \(_{n} T_{n}/r(_{n})\) for each sample and returns \(N^{*}_{i}\{_{i}\}\). Using the theory of Poisson processes, it can be shown that \(_{N^{*}} Q\) as desired (Maddison, 2016). Additionally, while the minimum is taken over all positive integers, in practice, \(N^{*}\) can be found in finite steps if \(r\) is bounded. In fact, in expectation, PFR will halt after \(2^{D_{}[Q P]}\) steps (Maddison, 2016). We summarize this process in Algorithm 1.

**Ordered random coding (ORC).** Unfortunately, PFR's random runtime can be a significant drawback. In practice, we may want to set a limit on the number of iterations to ensure a consistent and manageable runtime at the cost of some bias in the encoded sample. To this end, Theis and Yosri (2022) proposed ordered random coding (ORC). Specifically, in ORC with \(N\) candidates, rather than calculating \(T_{n}\) by Equation (3), we first draw \(N\) i.i.d. sample from \((1)\), and sort them in ascending order as \(_{1}^{}_{2}^{}_{N}^{ }\). We then set \(T_{n}=_{n}^{}\) for each \(n=1,2,,N\). In practice, instead of generating the \(T_{n}\)-s in \((N N)\) time by sorting, Theis and Yosri (2022) suggest an iterative procedure similar to Equation (3) with \((N)\) time complexity:

\[T_{0}=0, T_{n} T_{n-1}+}{{(N-n+1)}} T_{n},  T_{n}(1), n=1,2,,N\] (4)

## 3 Relative Entropy Coding with Space Partitioning

In this section, we describe our proposed algorithm and analyze its codelength. To motivate our method, recall that in PFR, the sender draws a random sequence from \(P\), and examines each sample's density ratio. We can interpret this process as a random search in \(P\)'s support, aiming to find a point that has a relatively high density ratio between \(Q\) and \(P\). However, when \(Q\) concentrates only within a small region of \(P\)'s support, most of the search does not contribute to the final outcome. Thus, _can we instead quickly navigate the search towards the region where \(Q\) is concentrated?_

For one-dimensional distributions \(Q\) and \(P\), the answer is affirmative, as we can perform a branch-and-bound search by partitioning the 1D space on the fly (Maddison et al., 2014; Flamich et al., 2022). Unfortunately, it is unclear how to generalize these adaptive partitioning strategies to spaces with dimension greater than one. Instead, in Section 3.1, we propose to partition the space in advance according to a rule that the sender and receiver agree on such that we can carry out the search fast enough in practical problems and retain an efficient codelength.

### Coding Scheme

Given a shared coding distribution \(P\) and a target \(Q\), our algorithm proceeds as follows:

1. The sender and receiver agree on a partition of \(P\)'s support consisting of \(J\) bins \(\{B_{1},,B_{J}\}\) with equal probability mass, i.e. \(P(B_{j})=}{{J}}\) for \(j=1,,J\). As we will discuss later, these

Figure 1: An illustrative comparison between the standard REC algorithm and REC with space partitioning. We illustrate the prior \(P\)’s density in blue and \(Q\)’s density in orange. (a) In a standard REC algorithm, we may draw numerous samples (colored in red) before identifying one that aligns well with \(Q\) (colored in green). The majority of these samples do not directly contribute to the desired result. (b) In the method we propose, we first divide the search space into smaller grids and then reweight each grid. This amounts to adjusting the prior \(P\) to a search heuristic \(P^{}\), which can align better with \(Q\). The samples from \(P^{}\) will thus be more relevant to \(Q\), potentially reducing the runtime.

bins can overlap, but to aid understanding for now, it may be helpful to imagine the space is partitioned by non-overlapping bins.
2. According to the target distribution \(Q\), the sender selects a categorical distribution for the bins, with event probabilities defined as \((j)\) for \(j=1,2,,J\). We can view this categorical distribution as a reweighting of each bin to adjust the coding distribution \(P\). The coding distribution adjusted by this reweighting can be better aligned with \(Q\). We will discuss the choice of \(\) later.
3. Then, the sender starts to draw and examine samples iteratively, similar to the PFR algorithm. However, instead of drawing samples directly from \(P\), the sender first samples a bin from \(\) and then draws a sample from the prior restricted to this bin. Specifically, at each iteration \(n\): 1. the sender first draws a bin index, \(j_{n}\), according to the distribution \(\); 2. the sender then draws a sample \(_{n} P|_{B_{j_{n}}}\). This sample is generated with the random state \(S_{j_{n}}\) associated with this bin. It's critical that each bin has a unique random state to ensure different bins have different random sample sequences. In practice, this can be easily achieved by setting the PRNG's seed of the bin \(B_{j}\) to \(j\), for \(j=1,2,,J\). Note that given these random states, the value of \(_{n}\) is uniquely determined by the bin index \(j_{n}\) and its sample index within this bin, which we denote by \(}_{j_{n}}\). For the sake of clarity, we will call \(n\) the _global sample index_ and \(}_{j_{n}}\) the _local sample index_ hereafter; 3. the sender examines the sample by calculating \(_{n}\): \[_{n} T_{n}/r^{}, r^{} q(_{n} )/p^{}_{n}, p^{}_{n}(j_{n})p(_{n})/P(B_ {j_{n}})=J(j_{n}) p(_{n}),\] (5) and keeps track of \(N^{*}_{i=1,2,,n}\{_{i}\}\). Here, \(T_{n}\) is also obtained by Equation (3); 4. finally, the sender checks the following stopping criterion: \[T_{n}/r^{}_{}>^{*}, r^{}_{} _{j=1,2,,J}_{ B_{j}})P(B_{j})}{p()(j)}}\] (6) If the criterion is met, halt and return \(j_{N^{*}}\) and \(}_{N^{*}}\); otherwise, proceed to the next iteration.

We detail the above coding scheme in Algorithm 2. As a comparison, we describe the standard PFR algorithm in Algorithm 1, and highlight their difference in red. We also illustrate these two algorithms from a high-level point of view in Figure 1. It is easy to verify that Algorithm 2 is equivalent to running standard PFR with an adjusted prior \(P^{}\), whose density is defined as

\[p^{}()=_{j=1}^{J}\{ B_{j}\})}{P(B_{j})}.\] (7)

Therefore, the encoded sample is guaranteed to be \(Q\)-distributed. By choosing a sensible \(\), the adjusted prior \(P^{}\) can be more closely aligned with \(Q\) than the original \(P\), potentially decreasing the runtime. From this point forward, we will refer to \(P^{}\) as the _search heuristic_.

Note that the receiver does not need to be aware of \(\) to decode the sample. Instead, after receiving the bin index \(j_{N^{}}\), the receiver can construct the random sequence from \(P|_{B_{j_{N^{}}}}\) with seed \(S_{j_{N^{}}}\). Then, taking the \(_{N^{}}\)-th sample in this sequence, the receiver successfully retrieves the desired sample.

### Codelength of the two-part code

In our proposed algorithm, the sender needs to transmit a two-part code that includes both the bin index and the local sample index. This is distinct from the standard PFR, where the sender transmits only the sample index, hence potentially raising concerns about the codelength. Indeed, we find the two-part code can introduce an extra cost, as outlined in the following theorem:

**Theorem 3.1**.: _Let a pair of correlated random variables \(, P_{,}\) be given. Assume we perform relative entropy coding using Algorithm 2 and let \(j^{}\) denote the bin index and \(^{}\) the local sample index returned by the algorithm. Then, the entropy of the two-part code is bounded by_

\[[j^{},^{}] I[; ]+_{}[]+_{2}(I[;] -_{2}J+_{}[]+1)+4,\] (8) \[=_{ Q_{;}}[\{0,_{2}J-_{2})}{p( )}\}].\] (9)

We prove Theorem 3.1 in Appendix C.1. Note, that when \(I[;]\) is sufficiently large, the term \(_{2}(I[;]-_{2}J+_{}[ ]+1)+4\) will be negligible. However, without further information on \(Q\) and \(P\), it is difficult to assert how small \(_{}[]\) is. We can view \(\) as the extra cost introduced by the space partitioning algorithm. Fortunately, for commonly used distributions in neural compression, including Uniform and Gaussian, we have the following conclusion under reasonable assumptions:

**Proposition 3.2** (Bound of \(\) for Uniform and Gaussian).: _Assume setting \(J 2^{D_{}[Q P]}\) when running Algorithm 2 for each \(Q_{|}\). Then, for Uniform \(Q\) and \(P\), if \(Q P\) (i.e., \(Q\) is absolute continuous w.r.t \(P\)), we have \(=0\); for factorized Gaussian \(Q\) and \(P\), if \(Q\) has smaller variance than \(P\) along each axis, we have \( 0.849}[Q P]}\), and \(_{}[] 0.849;]}\)._

We prove Proposition 3.2 in Appendix C.2. Also, we highlight that the conclusion for Gaussian in Proposition 3.2 is derived by considering the worst case when \(P\) has the same variance as \(Q\) along _all_ dimensions. In practice, this worst case can barely happen since in neural compression \(P\) represents the prior, and \(Q\) represents the posterior, which will be more concentrated than the prior. Empirically, we find this \(\)-cost yields no visible influence on the codelength (e.g., Figure 1(b) in Section 5).

### Generality of our Space Partitioning Algorithm

We highlight that the conclusions in Section 3.2 are independent of the partitioning strategy and \(\). This allows us to use different \(\) without the need to revisit the bound of the codelength.

More interestingly, we do not need to partition the space with non-overlapping bins. This is because the density of \(P^{}\), as stated in Equation (7), can be directly interpreted as a mixture of priors with \(J\) components, where \((j)\) represents the mixture weights. There are no restrictions preventing this mixture model from having overlapping components. We provide more explanation and discussion on overlapping components in Appendix B.1. As an extreme case, we can even have entirely overlapping bins. Notably, this scenario coincides with the "parallel threads" version of REC proposed by Flamich (2024). Concretely, Flamich (2024) proposed to initiate several threads for a single REC task and run them in parallel on various machines, thereby decreasing the runtime.

Furthermore, the concept of space partitioning and its codelength, as stated in Theorem 3.1, extends beyond the scope of Poisson functional representation algorithms. Here, we state the codelength by applying our proposed space partitioning method to greedy Poisson rejection sampling (GPRS) (Flamich, 2024) and leave its application to other REC algorithms for future work.

**Theorem 3.3**.: _Let a pair of correlated random variables \(, P_{,}\) be given. Assume we perform relative entropy coding using GPRS with space partitioning and let \(j^{*}\) denote the bin index and \(^{*}\) the local sample index returned by the algorithm, and \(\) be as in Equation (9). Then,_

\[[j^{*},^{*}] I[;]+_{ }[]+_{2}(I[;]-_{2}J+_{ }[]+1)+6.\] (10)

We prove Theorem 3.3 in Appendix C.3. Additionally, since we can view the "parallel threads" version of GPRS as a special case of our proposed method, Theorem 3.3, and hence Proposition 3.2, offer alternative bounds for Theorem 3.5 in Flamich (2024).

## 4 Exemplifying the choice of Partitioning Strategy and \(\)

In the above sections, we do not specify the partitioning strategy and \(\). In this section, we will exemplify their choices. To keep our discussion simple, we take the assumption that both \(Q\) and \(P\) are fully-factorized distributions. This covers a majority of neural compression applications with relative entropy coding, including both VAE-based (Flamich et al., 2020) and INR-based codecs (Guo et al., 2024; He et al., 2023). Under this assumption, we adopt a simple partitioning strategy to split space with axis-aligned grids, which allows us to draw samples and evaluate the density ratio easily.

Now, let's focus on \(\). We consider two scenarios: 1) the sender aims to encode a sample that exactly follows \(Q\), and 2) the sender encodes a sample following \(Q\) approximately for a more manageable computational cost. As we will see, the optimal choice of \(\) differs in these cases.

We note that the latter garners more interest in practical neural compression. This is because we can always construct an example where the standard PFR algorithm does not terminate while the first sample from the prior already has little bias. As an example, take \(q(z)=(z|0.001,^{2})\), and \(p(z)=(z|0,1)\). When \( 1\) (approaching 1 from below), the expected runtime for PFR diverges: \(2^{D_{}[Q|P]}\). Unfortunately, our proposed space partitioning approach can do little in this case. This is because, for a small \(\)-cost, as stated in Proposition 3.2, the number of partitions \(J\) should be less than \(2^{D_{}[Q P]}\), which reduces to 1 when \( 1\). For multi-dimensional (factorized) \(Q\) and \(P\), this issue will occur whenever it arises in _any single_ dimension. Making things even worse, this example is pervasive in neural compression due to numerical inaccuracies. Therefore, limiting the number of candidate samples is more practical than running the PFR algorithm until the stopping criterion is met. Consequently, we will mainly study the non-exact case in the following, but for the sake of completeness, we first discuss the exact scenario.

### Exact Sampler

When encoding a sample following \(Q\) exactly, we hope to minimize the expected runtime by adjusting the search heuristic \(P^{}\). Since we can view our proposed algorithm as running standard PFR with \(P^{}\), its expected runtime is \(2^{D_{}[Q| P^{}]}\). Thus, we have the following constrained optimization problem:

\[_{}\{_{j=1,2,,J}[_{ B_{j}} {q()P(B_{j})}{p()(j)}]\},_{j=1}^{J}(j)=1.\] (11)

The solution turns out to be intuitive:

\[(j)_{ B_{j}})P(B_{j})}{p()}_{ B_{j}})}{p()}\.\] (12)

However, sampling from this \(\) is generally challenging. Fortunately, if \(Q\) and \(P\) are both factorized and we partition the space with axis-aligned grids, then \(\) factorizes dimensionwise into a product of categorical distributions. Also, this choice of \(\) simplifies the evaluation of the stopping criterion in Equation (6). Specifically, \(r^{}_{}\) simplifies to \(r^{}_{}=Z J\), where \(Z=_{j=1}^{J}(j)\) denotes the normalization constant. This constant is shared in both the maximum density ratio \(r^{}_{}\) and the density ratio for individual samples. We thus can omit it when evaluating the stopping criterion. We detail the procedure in Algorithm 3 in Appendix A.1.

### Non-exact Sampler

If we only need to encode a sample following \(Q\) approximately, we can apply our space partitioning strategy to ordered random coding (ORC). In this case, we hope to reduce the bias with the same sample size or reduce the sample size for the same bias. To determine the optimal choice of \(\), we state the following corollary on ORC's bias. This is a corollary of Theis and Yosri (2022, Lemma D.1) and Chatterjee and Diaconis (2018, Theorem 1.2). We present the proof in Appendix C.4.

**Corollary 4.1** (Biasness of sample encoded by ORC).: _Given a target distribution \(Q\) and a prior distribution \(P\), run ordered random coding (ORC) to encode a sample. Let \(\) be the distribution of encoded samples. If the number of candidates is \(N=2^{D_{}[Q P]+t}\) for some \(t 0\), then_

\[D_{}[,Q] 4(2^{-t/4}+2_{  Q}()}{p()} D_{}[Q  P]+)})^{1/2}.\] (13)

_Conversely, supposing that \(N=2^{D_{}[Q P]-t}\) for some \(t 0\), then_

\[D_{}[,Q] 1-2^{-t/2}-_{ Q}( )}{p()} D_{}[Q P]- {2}).\] (14)

This corollary tells us that when running ORC with target \(Q\) and prior \(P\), if the density ratio between \(Q\) and \(P\) is well concentrated around its expectation, _choosing \(N 2^{D_{}[Q P]}\) candidates is both sufficient and necessary to encode a low-bias sample in terms of total variation (TV) distance_. Recall that our proposed algorithm can be viewed as running ORC with the search heuristic \(P^{}\) as the prior. We, therefore, want to choose \(\) to minimize the KL-divergence between the target \(Q\) and the search heuristic \(P^{}\). The optimal \(\) turns out to be surprisingly simple:

\[(j)=Q(B_{j}),j=1,2,,J.\] (15)

This is because \(D_{}[Q P^{}]=D_{}[Q P]-_{j= 1}^{J}Q(B_{j})_{2}(j)+_{j=1}^{J}Q(B_{j})_{2}P(B_{j})\), and the cross-entropy \((-_{j}^{J}Q(B_{j})_{2}(j))\) takes its minimum when \(\) matches \(Q\). In practice, to sample from this categorical distribution, we can simply draw \( Q\) and find the bin it belongs to. However, choosing \((j)=Q(B_{j})\) complicates the calculation of \(r^{}_{}\) in Equation (6). Fortunately, in ORC, we do not need to check the stopping criterion, so this complication does not pose an issue. We formalize this new ORC algorithm in Algorithm 4 in Appendix A.2.

Algorithm 4 still leaves three questions unanswered: first, we need to _determine the number of partitions_\(J\). As a finer partition allows better alignment between \(Q\) and \(P^{}\), we pick \(J=2^{ D_{}[Q P]}\), the maximum value for which Proposition 3.2 provides a bound on the extra coding cost \(\). Note that this will require the receiver to know \( D_{}[Q P]\). As we will demonstrate in Section 5, in practice, we can achieve this by either encoding the KL using negligible bits (e.g., in neural compression with VAE) or enforcing the KL budget during optimization (e.g., in neural compression with INRs).

Second, as we partition the space using axis-aligned grids, we need to _determine the number of bins assigned to each axis_. To explain why this choice matters, we consider an example where \(Q\) and \(P\) share the same marginal distribution along a specific axis, and the space is partitioned solely by dividing this axis; in this case, we have \(P^{} P\), leading to no improvement in runtime. Fortunately, in most neural compression applications, the sender and receiver can also have access to an estimation of the mutual information \(I_{d}\) along each axis \(d\) from the training set. If the mutual information is _non-zero_ along one axis, on average, \(Q\) and \(P\) will _not_ have the same marginal distribution along this axis. Based on this observation, we suggest partitioning the \(d\)-th axis into approximately \(2^{n_{d}}\) intervals where \(n_{d}=D_{}[Q P] I_{d}/_{d^{}=1}^{D}I_{d^{ }}\); see Appendix B.2 for further discussion, including the derivation, a toy example illustration, and ablation studies on this.

Third, we _determine how many candidate samples we need to draw_ from \(P^{}\) to ensure the encoded sample has low bias. Recall that our proposed algorithm can be viewed as running ORC with \(P^{}\) as the prior, we thus require a sample size \( 2^{D_{}[Q P^{}]}\) according to Corollary 4.1. When the total number of partitions \(J=2^{D_{}[Q P]}\), we find \(D_{}[Q P^{}]=-_{j=1}^{J}Q(B_{j})_{2}Q(B_{j})\). We do not have an analytical form for this value, but we can estimate it by samples from \(Q\). In fact, we empirically find a small number of samples to be sufficient for an accurate estimator.

**Why choose the TV distance as the measure of approximation error in Corollary 4.1?** Indeed, TV distance is not the only possible metric and the choice should align with the intended application. We choose total variation as it fits well with the task in our experiments: the one-shot nature of data compression for human consumption. We will explain this in two parts:

1. _Control of the TV distance in the latent space implies control in data space:_ naturally, we wish to assess reconstruction quality in data space, but we use REC only to encode latent variables from which we reconstruct the data. However, since the total variation satisfies the data processing and triangle inequalities, if our generative model approximates the true data distribution with \(\) total variation error and we use an \(\)-approximate REC algorithm, then encoding the latents incurs no more than \(+\) total variation error in the data space (Flamich and Wells, 2024).
2. _TV distance captures a reasonable notion of realism._ Imagine two distributions \(Q\) (e.g., the ground truth data distribution) and \(\) (e.g., the data distribution learned by our compressor). Let \(_{0} Q,_{1}\) and let \(B(1/2)\) be a fair coin toss. Then, the probability that any observer can correctly decide which distribution \(_{B}\) was sampled from, i.e., correctly predict the value of \(B\) given \(_{B}\), is at most \(1/2(1+D_{}[,Q])\)(Nielsen, 2013; Blau and Michaeli, 2018). Thus, in the context of approximate REC, the TV distance bounds the accuracy with which any observer can tell the compressed and reconstructed data apart from the original.

## 5 Experiments

We now verify our proposed algorithm with three different experiments, including synthetic toy examples, lossless compression on MNIST with VAE, and lossy compression on CIFAR-10 with INRs. We include details of the experiment setups in Appendix D.

**Toy Experiments.** We explore the effectiveness of our algorithm on 5D synthetic Gaussian examples. We run PFR with space partitioning (Algorithm 3) to encode exact samples and ORC with space partitioning (Algorithm 4) for approximate samples. We show the results in Figure 2 and Figure 3, and also include standard PFR and ORC for comparison. We can see that our space partitioning algorithm reduces the runtime by up to three orders of magnitude while maintaining codelength when encoding exact samples, and requires a much smaller sample size to achieve a certain bias (quantified by maximum mean discrepancy (MMD, Smola et al., 2007)) when encoding approximate samples.

**Lossless Compression on MNIST with VAE.** As a further proof of concept, we apply our methods to losslessly compress MNIST images (LeCun and Cortes, 1998). We train a VAE following Flamich et al. (2024), employ REC (specifically, ORC with our proposed space partitioning) to encode the latent embeddings, and entropy-encode the image. The KL divergence of entire latent embeddings averages over 90 bits. Unfortunately, even after employing our proposed space partitioning algorithm, the new KL divergence \(D_{}[Q P^{}]\) exceeds our manageable size. We hence randomly divide the latent dimensions into smaller blocks. Recall that, in our proposed approach, the sender and receiver need to share \([D_{}[Q P]]\) so that they can partition the space into the same \(J=2^{[D_{}[Q P]]}\) bins. However, this value varies across different images. Therefore, we estimate the distribution of \([D_{}[Q P]]\) for each block from the training set, and entropy code it for each test image.

We evaluate the performance achieved by \(\{2,4\}\) blocks and different sample sizes in Table 1, with the theoretically optimal codelength and the results by GPRS, a REC algorithm that is faster in 1D space but incurs coding overhead dimension-wise (Flamich, 2024). Notably, our algorithm's codelength is only 2% worse than the theoretically optimal result and about 6% better than GPRS's. We also investigate the reasons for overhead. Compared to the ELBO, our method incurs overhead in three ways: (a) the cost to encode \(|D_{}[Q P]|\); (b) the overhead from encoding the latent embeddings by REC; and (c) the overhead when encoding the target image, which arises from the bias in encoding the latent embeddings, as ORC encodes only approximate samples. We can see our algorithm achieves extremely low bias while maintaining a relatively small overhead caused by (a) and (b).

**Lossy Compression on CIFAR-10 with INRs.** We apply our methods to a more practical setting: compressing CIFAR-10 images with RECOMBINER (He et al., 2023), an implicit neural representation (INR)-based codec. The authors of RECOMBINER originally encoded INR weights using a block size of \(D_{}[Q P]=16\) bits. We empirically find that our proposed method can handle a block size of \(D_{}[Q P]=48\) bits while maintaining \(D_{}[Q P^{}]\) within a manageable range, approximately 12-14 bits. To further reduce the bias of the encoded sample, we opt to use \(2^{16}\) samplesfor each block. Besides, unlike in the VAE case, where the KL for each block varies across different test images, here, we follow He et al. (2023) to enforce the KL of all blocks to be close to 48 bits when training the INR for each test image. This eliminates the need to encode \( D_{}[Q P]\).

Additionally, He et al. (2023) enhanced their results by fine-tuning \(Q\) for the not-yet-compressed weights after encoding each block, which can achieve a more complicated posterior distribution \(Q\) in an auto-regressive manner. However, the effectiveness of fine-tuning is closely tied to the number of blocks. As we have reduced the number of blocks in our approach, fine-tuning becomes less effective. Therefore, we present results both with and without fine-tuning in Figure 4. We also present the theoretical RD curve (without fine-tuning) to evaluate how close we are to the theoretical bound.

As we can see, compared with standard ORC using a block size of \(D_{}[Q P]=16\) bits, our proposed algorithm with the block size of \(D_{}[Q P]=48\) bits reduces the codelength by approximately \(10\%\) with fine-tuning and 15% without. This gain is due to three reasons: 1) when the KL of each block is larger, the \(\)-cost, the algorithmic and constant overhead in Equation (8) becomes negligible; 2) the \(-_{2}J\) term in Equation (8) also reduces the algorithmic overhead in our method's codelength comparing with Equation (1); and 3) as the new KL divergence \(D_{}[Q P^{}]\) is around 12-14 bits and we choose a sample size of \(2^{16}\), we eliminate most of the bias in our encoded samples. Moreover, the algorithm's codelength remains within 2% of the theoretical optimal result. As a further comparison, we present the performance of other compression baselines in Figure 12, where we can see that our proposed algorithm makes RECOMBINER more competitive.

## 6 Related Works

There are several efforts to accelerate REC algorithms. Flamich et al. (2022, 2024); Flamich (2024) leveraged the idea of partitioning in 1D space, achieving an impressive runtime in the order of \((D_{}[Q\|P])\) or \((D_{}[Q P])\). However, these fast approaches only work for 1D distributions. Besides, Flamich and Theis (2023) proposed bits-back quantization (BBQ), which encodes a sample with time complexity linear in dimensionality. However, BBQ assumes \(P\) and \(Q\) to be uniform distributions in two hypercubes with their edges aligned, which may not be practical in many applications. The algorithm most similar to ours is hybrid coding (Theis and Yosri, 2022), which employs dithered quantization followed by a sampling procedure, an approach that resembles a special case of our proposed algorithm. However, hybrid coding relies on the assumption that the support of \(Q\) is contained within a hypercube, restricting its practical applicability.

## 7 Conclusions and Limitations

In this work, we propose a relative entropy coding (REC) scheme based on space partitioning, which significantly reduces the runtime in practical settings. We provide both theoretical analysis and experimental evidence supporting our method. Being among the few successful attempts to accelerate REC for practical settings, we firmly believe that our contributions will broaden the application of REC methods and inspire further research.

However, our proposed algorithm still faces several limitations at the current stage. First, although our method and conclusion apply to general partitions, in practice, we are largely limited to handling axis-aligned grids. Second, in our experiments, we require the mutual information to be factorized per dimension and shared between the sender and receiver, which restricts our algorithm's utility, for example, in non-factorized cases (Theis et al., 2022) or one-shot REC tasks (Havasi et al., 2019). A potential avenue for future research involves employing a mixture of priors to form "partitions" without hard boundaries as discussed in Section 3.3. Additionally, by considering the prior as a convolution of two distributions, we may potentially create infinitely many partitions. However, managing the codelength in such cases may raise new challenges.

## 8 Acknowledgments

We would like to thank Zongyu Guo and Sergio Calvo-Ordonez for their proofreading and insightful feedback on the manuscript. JH was supported by the University of Cambridge Harding Distinguished Postgraduate Scholars Programme. JMHL and JH acknowledge support from a Turing AI Fellowship under grant EP/V023756/1; GF acknowledges funding from DeepMind.