# The Tree Autoencoder Model,

with Application to Hierarchical Data Visualization

 Miguel A. Carreira-Perpinan

Dept. of Computer Science and Engineering

University of California, Merced

mcarreira-perpinan@ucmerced.edu

&Kuat Gazizov

Dept. of Computer Science and Engineering

University of California, Merced

kgazzizov@ucmerced.edu

###### Abstract

We propose a new model for dimensionality reduction, the PCA tree, which works like a regular autoencoder, having explicit projection and reconstruction mappings. The projection is effected by a sparse oblique tree, having hard, hyperplane splits using few features and linear leaves. The reconstruction mapping is a set of local linear mappings. Thus, rather than producing a global map as in t-SNE and other methods, which often leads to distortions, it produces a hierarchical set of local PCAs. The use of a sparse oblique tree and of PCA in its leaves makes the overall model interpretable and very fast to project or reconstruct new points. Joint optimization of all the parameters in the tree is a nonconvex nondifferentiable problem. We propose an algorithm that is guaranteed to decrease the error monotonically and which scales to large datasets without any approximation. In experiments, we show PCA trees are able to identify a wealth of low-dimensional and cluster structure in image and document datasets.

## 1 Introduction

As a form of exploratory data analysis, dimensionality reduction (DR) for visualization seeks to provide a representation of a dataset in dimension at most 3 (we will focus on 2D) that, combined with the human visual ability to pick patterns, is able to find structure of interest in a high-dimensional dataset. Many DR methods have been proposed over the years. Currently, \(t\)-SNE  and PCA are the most popular choices and have seen widespread application in many areas.

Here we advocate for a different approach, that of hierarchical local linear projections, which we call the **PCA tree**. Hierarchical DR is not new, but what is new is our definition of the model as a certain type of tree autoencoder and our algorithm to learn it by minimizing the reconstruction error in a self-supervised way, which brings considerable advantages in interpretability and training and inference time. From the outset, we acknowledge that DR is a hard, ill-defined problem that no single technique can solve satisfactorily. In reducing a dataset to 2D we necessarily have to lose information and produce artifacts. Indeed, the distortions of \(t\)-SNE and their impact on applications have been well documented . However, the PCA tree provides significant, complementary advantages over previous methods.

Firstly, _PCA trees are practical and intuitive to use_. They optimize the reconstruction error, which has a clear meaning. They do not require a neighborhood graph (and perplexity parameter, etc.), which is tricky to estimate so it captures manifold structure, and computationally very costly. The model, a **tree autoencoder** (see later), _directly defines nonlinear out-of-sample mappings (encoder and decoder)_. Its hyperparameters are natural: the _depth_\(\) of the tree, which directly controls the resolution (number of scatterplots and reconstruction error), and the _sparsity_\(\), which helps to interpret the tree decision nodes. Inference on a tree (encoding or decoding) is extremely fast. Training(to find a local optimum, a hard problem because the tree is nondifferentiable) is linear on the sample size and parallelizable, hence _scalable to large datasets without the need for approximations_.

Second, _PCA trees are highly interpretable_ and, as demonstrated in our experiments, they extract a wealth of information from complex datasets. This comes from the fact that both trees and PCA are techniques with well understood interpretability. Specifically: 1) the use of trees to construct a nested hierarchy of maps where an input instance follows a single root-leaf path; 2) the use of sparse oblique trees, with hyperplanes using few features at the decision nodes; 3) the use of linear projections in the leaves, each a PCA scatterplot; and 4) the ability to see the data at multiple scales.

Third, _a PCA tree uses multiple local latent projections (an "album") hierarchically organized rather than a single global one ("atlas")_. The latter is more likely to introduce distortions in trying to arrange many dimensions into a 2D map, and is hard to use with large datasets, requiring a lot of pan and zoom. This is less of a problem if one focuses on a local part of the data, particularly if there are clusters. Also, a linear projection is easy to understand and, while it does introduce distortions, it is well understood how this happens: distances can only shrink. If a PCA map shows clusters, these are real--in contrast with \(t\)-SNE's tendency to create false clusters [6; 22], for example. Also, PC directions often correspond to some real degree of freedom, and the mean is obviously a useful summary of the data. Thus, while global PCA generally collapses different parts of a manifold, local PCAs can succeed--if properly learned. Multiple local maps are like an album of a house, where each picture shows a different room, but with an hierarchical structure.

A final advantage is that the loss function is really a self-supervised regression problem. This makes it _possible to use cross-validation to determine the hyperparameters_ (although, in an exploratory data analysis, we probably want to explore such hyperparameters manually).

The PCA tree does have its limitations, as follows. It needs access to the explicit feature vectors, unlike graph-based methods, which need instead pairwise affinities. The optimization converges to a local optimum, which depends on the initialization. And, as with any DR method, PCA trees reveal only some structure, so practitioners should try multiple, complementary methods.

## 2 Related work

The literature on DR is vast. We note connections to the most relevant methods.

Non-hierarchical DROf the many existing DR methods, we mention two large groups that are particularly important in this paper. One are _autoencoders_, which learn an encoder or projection mapping and a decoder or reconstruction mapping so as to minimize the reconstruction error of each training point. The classic examples are PCA and neural network autoencoders [34; 17], where the encoder and decoder are linear and nonlinear, respectively. Our tree autoencoders are another example. The other are _nonlinear embedding methods_, which learn the 2D projection of each training point so that Euclidean distances in latent space best match distances or similarities in the high-dimensional space. These include multidimensional scaling , SNE , \(t\)-SNE , the elastic embedding  and UMAP , among others. Although often proposed as matching distributions of neighbors in the high- and low-dimensional spaces, these methods can be equivalently seen as optimizing a tradeoff of attraction between true neighbors and repulsion between points [6; 41; 22].

Hierarchical DRA multiscale view of DR is an attractive idea that has been explored before, often justified by the fact that a single 2D map may miss much of the structure in a complex dataset. We consider two ways of defining the hierarchical structure: soft and hard. In _soft hierarchies_, one defines a soft, probabilistic tree  so that an input instance traverses every tree path with a positive probability. Thus, the model really is a probabilistic mixture of the leaves ("experts") with mixing proportions given by the tree structure. This has the advantage that the model is differentiable, so optimization is straightforward with gradient- or EM-based approaches. Unfortunately, we lose the interpretability and fast training and inference, since an instance reaches all nodes, and the leaves may not localize in input space and overlap with each other. Examples of this are [4; 30; 38; 19]. A more detailed comparison of soft and hard oblique decision trees appears in .

Making the tree decisions hard makes the hierarchy nested, but the optimization is much more difficult (indeed, this is an important contribution in our paper). One can construct a hard hierarchy in a greedy sequential way by recursively partitioning the dataset, or by using some other tree con struction (such as kd-trees  or cover trees ); and then applying some form of DR at each node. However, this is suboptimal (since the node parameters are fixed as they are added, or independently from the leaf parameters) and produces overly large trees. Conversely, one can grow the tree bottom-up by a form of hierarchical clustering, but this is also suboptimal. These approaches include work such as [1; 24; 26; 29] and others such as hierarchical self-organizing maps , sequential NMF  and hierarchical SNE .

Flat, local DRThis seeks to estimate a collection of local latent maps but without any hierarchy. Again, this can be done in a soft way via probabilistic mixtures (of factor analyzers , PCAs , etc.) or in a hard way as a form of \(k\)-PCAs . A few works have also sought to align a collection of local DR models into a single global map [33; 36; 44; 23].

Other related modelsSome works have tried to combine graph-based methods, such as \(t\)-SNE, with hierarchical DR. For any DR method that does not define an out-of-sample mapping, such as \(t\)-SNE and other nonlinear embedding methods, it is possible to learn a tree-based projection mapping . Also, some fast approximations for graph-based embedding methods are based on tree data structures, such as Barnes-Hut trees [43; 39; 42; 35]. However, this is intended to accelerate the N-body computations, not to define a hierarchical DR.

In view of this, the main novelty of our work is in defining an autoencoder consisting of a hard tree encoder and a local linear decoder, and in giving an algorithm to learn it by optimizing the reconstruction error. We believe this is the first algorithm to guarantee a monotonic decrease of the error jointly over the tree parameters and local PCAs. We describe both the tree autoencoder model and the optimization algorithm next.

## 3 Definition of the PCA tree as an autoencoder

Like any autoencoder, the PCA tree defines an encoder \(\) and decoder \(\), but in a peculiar way, as follows. Firstly, consider a fixed rooted directed tree structure with decision nodes and leaves indexed by sets \(\) and \(\), respectively, and \(=\). Both the encoder and autoencoder use this tree structure. Each decision node \(i\) has a decision function \(h_{i}(;_{i},w_{i0})\): \(^{D}_{i}\), where \(_{i}=\{_{i},_{i}\}\), sending instance \(\) to the corresponding child of \(i\). Rather than axis-aligned trees, which are a poor model for high-dimensional data, we use oblique trees, having hyperplane decision functions "go to right if \(_{i}^{T}+w_{i0} 0\)", with \(_{i}^{D}\) and \(w_{i0}\). The tree's prediction for an instance \(\) is obtained by routing \(\) from the root to exactly one leaf and applying this leaf's predictor (defined below). We define the _reduced set (RS)_\(_{i}\{1,,N\}\) of a node \(i\) as the training instances that reach \(i\) given the current tree parameters.

Encoder or projection mapping \(\)This is given by a tree mapping \(^{e}(;)\): \(^{D}^{L}\) where the predictor for leaf \(j\) has the form of a linear mapping \(_{j}(;_{j},_{j})=_{j} ^{T}(-_{j})\), where \(_{j}^{D L}\) is an orthogonal matrix and \(_{j}^{D}\). The encoder parameters are \(=\{_{i},w_{i0}\}_{i}\{_{j},_{j}\}_{j}\). Thus, the encoder maps an input instance \(^{D}\) to a leaf index \(j\) and an \(L\)-dimensional real vector \(=_{j}^{T}(-_{j})\), which at an optimum (see later) will be the PCA projection in that leaf. This means that the PCA tree does not have a common latent space of dimension \(L\) where all instances are projected. Instead, it has one separate \(L\)-dimensional PCA space per leaf. Thus, _the PCA tree latent space \(^{L}\) is mixed discrete-continuous1_, although it can also be seen as a separate continuous \(L\)-dimensional PCA space per leaf.

Decoder or reconstruction mapping \(\)This maps a leaf index \(j\) and \(L\)-dimensional vector \(\) (in \(^{L}\)) to a vector in \(^{D}\). It consists of a set of linear mappings of the form \(_{j}(;_{j},_{j})=_{j} +_{j}\) for \(j\).

Tree autoencoderWe can now define the autoencoder as the composition of the decoder and encoder, \(=\). Conveniently, we can absorb the leaf index implicitly into each leaf. Thus,\((;)\) is simply a regression tree identical to the tree encoder \(^{e}(;)\) but where the predictor mapping of leaf \(j\) has the form \(_{j}(;_{j},_{j})=_{j} _{j}^{T}(-_{j})+_{j}\). Note this is a linear mapping of rank \(L<D\), and it is the composition \(_{j}\) of leaf \(j\)'s decoder with the encoder.

Fig. 1 illustrates the tree autoencoder idea. Fig. 2 shows a PCA tree on a 2D toy example using a latent dimension \(L=1\).

## 4 Optimization algorithm

Our objective function is now the regular reconstruction error of an autoencoder \(\): \(^{D}^{D}\) with an \(_{1}\) regularization term of hyperparameter \( 0\) on a training set \(\{_{n}\}_{n=1}^{N}^{D}\):

\[E()=_{n=1}^{N}\|_{n}-(_ {n};)\|_{2}^{2}+_{i}\| _{i}\|_{1}_{j}^{T}_{j }=,\  j.\] (1)

Thus, the PCA tree is trained like a regression tree which maps real vectors to real vectors. It consists of a fixed tree structure (which we will take as complete of depth \(\)); sparse oblique decision nodes, consisting of a hyperplane split using few features (as encouraged by the \(_{1}\) sparsity penalty); and linear low-rank leaves. The following theorem explains why we call it "PCA tree" (in fact, we designed the form of the loss function and tree so that this would happen).

**Theorem 4.1** (optimality condition over the leaves).: _The optimal \(\{_{j}^{*},_{j}^{*}\}_{i}\) given the remaining parameters are fixed, i.e.:_

\[_{\{_{j},_{j}\}_{i}}E()_{j}^{T}_{j}=,\  j\] (2)

_corresponds to setting each \(\{_{j}^{*},_{j}^{*}\}\) to the \(L\) principal components of \(_{j}\), the RS of leaf \(j\)._

Proof.: Because of the separability condition (theorem 4.2) and the fact that \(\{_{i},w_{i0}\}_{i}\) are fixed, problem (2) is equivalent to

\[_{\{_{j},_{j}\}_{i}}_{n _{j}}\|_{n}-_{j}(_{n};_{j},_{j})\|_{2}^{2}_{j }^{T}_{j}=,\  j\] (3)

Figure 1: Illustration of the tree autoencoder model with a data space \(^{3}\) for a tree of depth \(=2\) with leaves \(=\{1,2,3,4\}\), defining a latent space (album) of four 2D spaces.

Figure 2: PCA tree on a 2D toy data set. _Left plot_: regular PCA (PCA tree with \(=0\)). _Rest of plots_: PCA tree with \(=1\). Mean \(_{j}\): diamond; principal component direction \(_{j}\) and variance: dashed line and arrows; decision boundary \(_{1},w_{10}\) at the root: solid line.

which (since the objective and constraints separate) is also equivalent to solving this for each leaf \(j\) separately:

\[_{_{j},_{j}}_{n_{j}}\|_{n }-(_{j}_{j}^{T}(-_{j})+_{j}) \|_{2}^{2}_{j}^{T}_{j}=.\] (4)

The solution of the latter problem is well known to be given by PCA, i.e., \(_{j}^{*}=_{j}|}_{n_{j}}_{n}\) is the mean of the training instances in \(_{j}\) and \(_{j}^{*}\) consists of the \(L\) eigenvectors of the covariance matrix \(_{j}=_{j}|}_{n_{j}}( _{n}-_{j}^{*})(_{n}-_{j}^{*})^{T}\) associated with its largest \(L\) eigenvalues. 

We borrow the idea of alternating optimization over the tree nodes from  (Tree Alternating Optimization (TAO)), which is fundamental to be able to update the parameters so as to reduce the objective function. Firstly, at any decision tree making hard decisions, each input instance follows exactly one root-leaf path. This results in the following _separability condition_.

**Theorem 4.2** (Separability condition).: _Let \((;)\) be the autoencoder tree and \(\) a nonempty set of nodes that are not descendant from each other. Call \(_{i}\) the parameters in node \(i\). Then, as a function of the parameters \(\{_{i}:\ i\}\) (i.e., fixing all other parameters \(_{}=\{_{i}:\ i\}\)), the function \(E()\) of eq. (1) can be equivalently written as_

\[E()=_{i}E_{i}(_{i},_{ {rest}})+E_{}(_{})\] (5)

_where \(\{E_{i}\ i\}\) and \(E_{}\) are certain functions._

Proof.: Analogous to that in . It follows from the fact that the reduced sets of all nodes in \(\) are disjoint, because the tree makes hard decisions. 

That is, optimizing over the parameters of any set of nodes which are not descendants of each other separates: we can equivalently optimize each node on its own over its parameters and using only its RS. This simplifies the problem considerably and introduces significant parallelism: for example, all nodes at the same depth can be optimized in parallel. Indeed, our overall algorithm will process nodes from the leaves towards the root, optimizing all nodes at the same depth at each step.

Second, we still have to solve the problem of optimizing (1) over the parameters of one given node. This simplifies into a _reduced problem (RP)_. If the node is a _leaf_\(j\), theorem 4.1 gives us the answer: we set the leaf's \(_{j},_{j}\) to the PCA on \(_{j}\). This is the exact solution to the optimization over the leaves, so the overall objective function value will either decrease or stay constant. If the node is a _decision node_\(i\), optimizing (1) over \(_{i},w_{i0}\) is equivalent to a RP as given by the following theorem.

**Theorem 4.3** (Reduced problem over a decision node).: _Consider the objective function \(E()\) of eq. (1) and a decision node \(i\). Assume the parameter values of all the nodes except \(i\) are fixed. Then, the optimization problem \(_{_{i},w_{i0}}E()\) is equivalent to the following problem:_

\[_{_{i},w_{i0}}_{i}(_{i},w_{i0})=_{n _{i}}_{in}(_{in},h_{i}(_{n}; _{i},w_{i0}))+\|_{i}\|_{1}\] (6)

_where: \(_{i}\) is the reduced set of node \(i\); we define the weighted 0/1 loss \(_{in}(_{in},)\): \(_{i}^{+}\{0\}\) for instance \(n_{i}\) as \(_{in}(_{in},y)=l_{in}(y)-l_{in}(_{in})\  y_{i}\); we define the pseudolabel \(_{in}=_{y_{i}}l_{in}(y)\) as the "best" child of \(i\) for \(n\) (or any \(_{in}_{y_{i}}l_{in}(y)\) in case of ties); and we define the function \(l_{in}\): \(_{i}\) as \(l_{in}(z)=\|_{n}-_{z}(_{n};_{z})\|_{ 2}^{2}\) for any \(z_{i}\) (child of \(i\)), where \(_{z}(;_{z})\) is the predictive function for the subtree rooted at node \(z\)._

Proof.: Analogous to that for the decision node RP in . A difference is that there the tree was a classification tree with the 0/1 loss, which resulted in an unweighted problem; while here we use the reconstruction error, which results in a weighted problem in (6). This follows from the fact that all a decision node can do with an instance is send it down its left or right child, and the ideal choice is the one that results in the lowest reconstruction error downstream from that node. 

The _reduced problem_ of eq. (6) is an \(_{1}\)_-regularized weighted 0/1 loss binary classification problem_ over a linear classifier \(h_{i}\) with binary _pseudolabels_\(\{_{in}\}\), defined as the child of \(i\) that gives the best reconstruction for \(_{n}\) under the current tree. This problem is NP-hard, but we can typically get a good approximate solution by solving an \(_{1}\)-regularized surrogate loss instead. We use the logistic loss with instances weighted as in theorem 6. We can guarantee that the overall objective function value decreases or stays constant by accepting the surrogate solution only if it improves over the previous parameters2, which empirically is usually the case. This means each node update, hence each overall iteration over all nodes, monotonically decreases (1). We stop when there is little change in the parameters or objective, or when we reach a set number of iterations.

This concludes the algorithm, whose pseudocode is in fig. 6. Essentially, it repeatedly updates the nodes in turn: at a leaf it solves a PCA, and at a decision node it solves an \(_{1}\)-regularized, instance-weighted logistic regression problem (we use LIBLINEAR ). The tree is initialized as a random median tree, i.e., at the root we pick a random direction for the weight vector and a bias so that half of the instances go to either child; and we repeat this recursively down the tree.

## 5 Computational complexity for training and inference

Throughout, assume a complete tree of depth \(\); sparse decision weight vectors having \(sD\) nonzero values on average, with \(s\) (so \(s=1\) means dense vectors); and a latent dimension \(1 L D\) at each leaf (\(L 3\) for visualization). As cost we use the number of scalar multiplications.

### Inference

By inference we mean the time to map an input instance \(^{D}\) to its leaf \(j\) and latent vector \(^{L}\). This requires traversing a single root-leaf path of depth \(\), at a cost \((sD)\), and computing the leaf PCA projection, at a cost \((LD)\), total \(((L+s)D)\) (which, for \(=0\), i.e., regular PCA, corresponds to \((LD)\)). This is very fast, since both \(L\) and \(\) are very small. Reconstructing a latent vector \(^{L}\) at a leaf \(j\) costs \((LD)\).

### Training

The cost of one iteration, i.e., updating each node in the tree once, is as follows. _For the decision nodes_, this equals to setting up the RP (i.e., computing the pseudolabel losses) and solving it with one logistic regression in each node's RS. To work this out we note that the RSs of all nodes at the same depth total \(N\) instances, and we assume logistic regression to be linear in the dimensionality and sample size. Then, the total cost of all decision nodes has two terms. The first is the pseudolabel cost over all decision nodes, which is \((sND(-1))\). This is due to the fact that, at each decision node, we have to send each training instance down the left and right children to compute their losses (actually, we need only send it down one child if we precompute the losses), and the decision nodes span depths from 0 to \(-1\). The second is the RP solution, whose cost is proportional to that of running \(\) logistic regressions on the whole dataset (at depths \(0,1,,-1\)), or \((c_{1}ND)\), where \(c_{1}>0\) is a constant factor (which includes the average number of iterations of the logistic regression solver). This cost is as in the sparse oblique classification trees of . Asymptocally, it totals \((ND^{2})\).

_For the leaves_, we have a peculiar situation. Say a leaf has a RS with \(M\) instances. The cost of PCA3 on that RS is \((MD^{2}+c_{2}D^{3})\) if \(D M\) and \((DM^{2}+c_{2}M^{3})\) if \(D M\), depending on whether we compute the eigenvectors of the \(D D\) covariance matrix or of the \(N N\) Gram matrix, respectively, where \(c_{2}>0\) is a constant factor. Assume for simplicity that each RS has \(M=N2^{-}\) instances (balanced partition) and define the _critical depth_\(^{*}=_{2}()\). Then:

\[&=^{*}, regime:}}&(ND^{2}+c_{2}D^{3}2)\\ ^{*}, regime:}}&(N^{2}D2^{-}+c_{2}N^{3}2^{-2 })((1+c_{2})ND^{2}).\]

For a given dataset of fixed \(N\) and \(D\), the cost depends on the tree depth \(\) and has a turning point when \(=^{*}\). For datasets having4\(N<D\), we are always in the deep regime, but if \(N>D\) then we can be in the shallow or deep regime for small or large \(\), respectively. Interestingly, this leadsto a non-monotonic cost as a function of \(\), as shown in fig. 3, because the shallow regime cost increases with \(\) but the deep regime one decreases with \(\). This is surprising because the deeper the tree the more leaves it has, yet the faster they train. The reason is that the PCA cost is superlinear on the sample size, but the sample size per leaf decreases proportionally to the number of leaves.

We emphasize that, as noted above, in both regimes the leaves' cost is strictly upper bounded by \((1+c_{2})ND^{2}\) (with equality if \(=^{*}\)), hence _this cost is at most linear in \(N\) and quadratic in \(D\), just like in regular PCA_. For fixed \(N\) and \(D\), the leaves cost first dominates the decision nodes cost (shallow tree regime) because the former includes a term \(ND^{2}\) that dominates the decision nodes' term \((ND^{2})\), since \(^{2}<D\) in practical cases. But as \(\) grows (deep tree regime) the leaves cost decreases and reaches \(N(D+c_{2})\) for \(=_{2}N\) (when each leaf contains a single instance), so the overall cost is eventually dominated by the decision nodes, \((ND^{2})\). As a useful summary, for fixed \(\)_the rough cost is \((ND^{2})\) for shallow trees and \((ND)\) for deep trees_--which is asymptotically faster than PCA! (although very deep trees having few instances per leaf are likely not practical). This makes the algorithm highly scalable to large sample sizes without the need for any approximation. This is a significant advantage over neighbor embedding algorithms such as \(t\)-SNE, which have a quadratic cost on \(N\) and require some approximation to reduce this [39; 42; 43].

Parallel trainingFrom the separability condition, all nodes at the same depth can be updated in parallel, which accelerates the training considerably. Assume for simplicity we have \(2^{}\) processors (which is perfectly feasible for trees having depth 4, as in our experiments). Then the costs are as follows. For decision nodes, we have the cost of running a logistic regression on \(\) datasets of sizes \(N,,,N2^{-}\) (from the root to the leaf parents), a geometric series which is upper bounded by \(2N\), hence a total cost of \((2c_{1}ND)\), i.e., \(\)_times faster than the sequential computation_. (The pseudolabel computation separates over instances and is thus \(2^{}\) times faster.) For the leaves, we have the cost of running PCA in a single leaf, i.e., \(2^{}\)_times faster than the sequential computation_. Again, this is a significant advantage over \(t\)-SNE, which cannot be easily parallelized.

## 6 Experiments

In summary, we show the following: we confirm our theoretical predictions about monotonic decrease of the objective function and training time; compare the reconstruction error with PCA; and demonstrate how PCA trees are highly interpretable and extract significant structure from complex datasets. We use several datasets of different types (images, documents), some of which appear in the appendix.

### Training time and scalability to large datasets

Fig. 3 (right 2 plots) shows learning curves for different datasets and tree depths. As can be seen, the objective function decreases monotonically over iterations, and converges to a near-optimal solution in usually less than 10 iterations.

Fig. 3 (left 2 plots) shows the training time per iteration in seconds (theoretical and actually measured for the MNIST dataset), as a function of the tree depth \(\) in one processor (sequential computation, without parallelism). For the theoretical estimate we used constant factors \(c_{1}=1\), \(c_{2}=4\) (see section 5) and \(s=1\) (dense decision node weight vectors). To vary the dimensionality \(D\{64,784,1600,2500\}\), we subsampled or oversampled the MNIST images to \(8 8\), \(28 28\) (original size), \(40 40\) and \(50 50\) pixels, respectively. The plots confirm what we described in our complexity analysis: the computation at the leaves first increases until \(=^{*}=_{2}()\), then decreases, resulting in a non-monotonic total training time. The critical depth \(^{*}\) at which the change happens decreases with \(D\), from around 10 to 5; note the actual depth at which the total time peaks is different, because the latter also considers the decision nodes' cost, which is linear on \(\).

Fig. 3 (plot 3) shows scalability to larger sample sizes using the Infinite MNIST dataset  (images of \(28 28\), so \(D=784\)). We ran the PCA tree for 10 iterations, when it approximately converged. The linear cost of the PCA tree is clear as a slope 1 in the log-log plot. Training on 1M samples with depth 4 takes less than an hour. In contrast, \(t\)-SNE  and UMAP  are slower and have a superlinear cost, even though the implementations we used are not exact (they use approximations, such as Barnes-Hut trees or neighbor subsampling, to accelerate the computations, besides finding approximate rather than nearest neighbors).

### Reconstruction error

Fig. 4 shows the reconstruction error for PCA and PCA tree. Although the PCA tree uses \(L=2\) in each leaf, it is able to achieve a much lower error than PCA, which would need a much larger number of components to match it. This is the result of the PCA tree ability to learn a partition of the data and local PCAs that are jointly optimal.

### Interpreting and exploring PCA tree visualizations

Fig. 5 (see also figs. 8-10 in the appendix) shows results for the Fashion MNIST dataset5. This consists of \(N=60\,000\) grayscale images of \(28 28\) pixels (so \(D=784\)), each showing a clothing or shoe item, labeled into 10 balanced classes. The features are the pixel intensities in . The class labels were not used during training, they were only used for visualization purposes. We show a PCA tree of depth 4 (using \(=100\)), thus having 15 decision nodes (with weight vectors shown as \(28 28\) images) and 16 leaves (showing a 2D PCA scatterplot and the mean and PCs as images). The appendix contains results for other datasets (images: MNIST, documents: Amazon Reviews). Even at a glance we can see lots of structure in the tree: many classes organize hierarchically in a meaningful way; decision weight vectors are sparse and highly meaningful, often separating groups of classes by focusing on the discriminant regions of the image; the PCs in the leaves usually are highly meaningful, indicating the change of some relevant degree of freedom; and the leaf scatterplots show clusters or class structure; among other things. We comment on some of these in detail. For reference, fig. 9 (appendix) shows the 2D global plot by PCA, \(t\)-SNE and other algorithms.

Global tree structureAlthough our algorithm is unsupervised, it has the ability to separate different types of classes. For example, leaves L1 and L2 contain different types of shoes, while leaf L14 contains only bags. Some classes (e.g. Pullover and Shirt in L8) differ in tiny details and can only be separated if using the labels (remember that minimizing the reconstruction error seeks maximum-variance directions, which are not necessarily aligned with class variation). The coordinate axes in the leaves show the most variance in the data within the leaf's region. Since the RSs are nested along the tree hierarchy, we can also show a scatterplot at each decision node (appendix fig. 9), which shows how classes and other patterns are progressively separated as one descends deeper into the tree. However, note the optimal parameters if the tree was shallower would differ from those.

Local tree structureThe decision nodes' weight vectors select features sparsely, often focusing on parts of the image that can differentiate groups of images and send them selectively to the left or

Figure 4: Squared reconstruction error per sample of PCA as a function of the number of principal components \(L\) and of PCA tree using \(L=2\) PCs in each leaf, for different depths on two datasets: Fashion MNIST (\(N=60000\), \(D=784\)) and 20newsgroups (\(N=11314\), \(D=72764\)).

Figure 3: _Plots 1, 2:_ training time per iteration on (Infinite) MNIST for PCA trees with different \(N\), \(D\), \(\). _Plot 3:_ training time for PCA trees (10 iterations) and for \(t\)-SNE and UMAP (default user parameters). _Plots 4, 5:_ learning curves for PCA trees of different depths on several datasets.

right subtree. The figure is annotated in some places to make this clear. For example, decision node D2 separates clothes "with" and "without" sleeves. In some cases, this results in the tree separating actual classes. For instance, node D14 separates "outerwear" type of clothes from "bags", which is explainable by simply looking at the decision node weights and the part of the image where they focus: blue values indicate "if the image has a neckline and shoulders" (like outwear clothes) and red values indicate "if it has wide frames" (like bags).

PCA at the leavesSince each leaf applies 2D PCA on its corresponding reduced set, its linear projection scatterplot shows the most variance directions in the leaf region. As expected with PCA, amplified by the fact that it focuses on a region of the whole data, this often shows insights about the intrinsic local structure of the data. In particular, we can usually identify the coordinate axes (PCs) with meaningful quantities or degrees of freedom (labeled in the scatterplot below each leaf in fig. 5). For example, leaves L1, L2, L4, L7, L14, L15 and L16 contain objects of a specific type, such as L1 containing high-heel and high-ankle boots and sandals, or even isolating a class, such as L14 isolating bags. Interestingly, we find all these leaves often detect similar degrees of freedom: usually, PC1 is (overall) pixel intensity in the image and PC2 is object size (either height or width). Again, this is the result of minimizing reconstruction error, which projects along maximum-variance directions. Also, since overall intensity and object size are correlated, this is seen in the PCA plots as triangular shapes whose vertex corresponds to low-intensity images (caused by either overall low intensity or by a very small object). This can be clearly seen in leaf L4 (consisting of T-shirts and tops), as well as in other leaves: L7/L15 intensity-width; L1/L2/L16 intensity-height; L14 intensity-size of bags. The density of the projections in the 2D embedding is also informative. For example, in leaf L4, narrow tops are much more frequent than thin tops.

Some leaves find degrees of freedom not aligned with a PC direction, but visible in the scatterplot, such as elongated clusters. These often correspond to different classes, where the direction of elongation is some meaningful degree of freedom within the cluster. For example, as shown in the bottom of fig. 5, leaf L3 contains two types of clothes: "shirts" and "shoes". PC1 represents the class discrimination direction, while PC2 represents intensity. Local inspection of each cluster separately demonstrates that both clusters have maximum variation indicating pixel intensity. The shoe cluster also has variations in "weight" (lightweight shoes below and chunky shoes above). The "shirts" cluster has variation in T-shirt width. Leaves L4, L15, and L16 have almost a single class: shirts, dresses, and slim sneakers, respectively, and all of them have similar variations, i.e., intensity as PC1 and width or thickness as PC2. Finally, note that the appearance of clusters or complex structure within a leaf scatterplot suggests it would be worth either further expanding that leaf or using a deeper tree, which could be done in an interactive way.

This dataset provides evidence of what patterns are real and whether they are detected by a given DR method. If clusters appear in a PCA plot, they are real (unlike in a \(t\)-SNE or UMAP embedding). This is because, in an orthogonal linear projection, distances either decrease or do not change, but cannot increase. Another example is the fact that the image overall intensity arises throughout the PCA tree leaves as a principal component. This is not surprising: intensity changes in this dataset are large, and thus carry considerable variance, which the local PCAs pick. Even the global PCA is able to pick that too and locate low-intensity images in the lower-left area of the 2D embedding (see appendix fig. 10). However, both \(t\)-SNE and UMAP do not; we have verified that low-intensity images are mapped all over the embedding in a haphazard way. A similar effect happens with the Amazon reviews document dataset (appendix section E): individual document classes appear in PCA plots as "streaks", caused by the document length (which changes the norm of the TF-IDF feature vector), and converging on the area occupied by short documents. Again, such an important pattern is lost in the \(t\)-SNE and UMAP embeddings.

## 7 Conclusion

We have defined a new model for data visualization, the PCA tree, as a tree autoencoder that produces a set of hierarchical low-dimensional maps, and have given an algorithm to learn it by minimizing the reconstruction error, based on Tree Alternating Optimization (TAO). PCA trees are fast for training and out-of-sample inference, and highly interpretable. We hope they will provide a useful tool for data visualization. Beyond this, tree autoencoders have application in dimensionality reduction in general, as well as in clustering, subspace clustering, data compression, fast search and other problems that we are exploring.

Figure 5: PCA tree trained on the Fashion MNIST dataset (\(=4\), \(=100\), RMSE per pixel and image of 0.158). The decision nodes’ weight vectors (and the leaves’ PCs \(_{j}\)) are shown as \(28 28\) images, with negative/zero/positive values colored blue/white/red, respectively. Each leaf shows a 2D PCA scatterplot of its RS (instances reaching it), and below it the mean \(_{j}\) as a grayscale image and the 2 PCs \(_{j}\) as color images. To the right of the scatterplot, a bar chart displays class proportions and class means. The legend (top left) shows, for each class, its mean (grayscale image), color (for the scatterplots) and description. For visualization purposes, the tree is split into its left and right root subtrees (fig. 8 shows the whole tree). The bottom panel zooms into two regions of the tree, for decision nodes 8 and 14. You may want to zoom into the figure to see more details.