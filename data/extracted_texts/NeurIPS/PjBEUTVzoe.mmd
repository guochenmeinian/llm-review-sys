# Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network

Bochen Lyu

DataCanvas Lab, DataCanvas

Beijing, China

bochen.lv@gmail.com &Zhanxing Zhu

Changping National Lab & Peking University

Beijing, China

zhanxing.zhu@pku.edu.cn

###### Abstract

Studying the implicit bias of gradient descent (GD) and stochastic gradient descent (SGD) is critical to unveil the underlying mechanism of deep learning. Unfortunately, even for standard linear networks in regression setting, a comprehensive characterization of the implicit bias is still an open problem. This paper proposes to investigate a new proxy model of standard linear network, rank-1 linear network, where each weight matrix is parameterized as a rank-1 form. For over-parameterized regression problem, we precisely analyze the implicit bias of GD and SGD--by identifying a "potential" function such that GD converges to its minimizer constrained by zero training error (i.e., interpolation solution), and further characterizing the role of the noise introduced by SGD in perturbing the form of this potential. Our results explicitly connect the depth of the network and the initialization with the implicit bias of GD and SGD. Furthermore, we emphasize a new implicit bias of SGD jointly induced by stochasticity and over-parameterization, which can reduce the dependence of the SGD's solution on the initialization. Our findings regarding the implicit bias are different from that of a recently popular model, the diagonal linear network. We highlight that the induced bias of our rank-1 model is more consistent with standard linear network while the diagonal one is not. This suggests that the proposed rank-1 linear network might be a plausible proxy for standard linear net.

## 1 Introduction

Gradient Descent (GD) and its stochastic variant, Stochastic Gradient Descent (SGD), are probably the most important optimization techniques in deep learning. To unveil the underlying mechanism of modern neural networks, it is highly fundamental to understand the thrilling and mysterious properties of GD and SGD. Some recent works [24; 18; 3; 22] have made significant efforts in this direction by exploring their _implicit bias: among all global minimum, i.e., interpolation solutions, what particular ones will GD and SGD prefer without adding an explicit regularization?_ This question highlights the crucial roles of these algorithms in the generalization performance of the trained models.

Among the vast number of architectures in deep learning, the first object to investigate is the deep linear network, i.e., without any nonlinear activations,

\[F(x;W)=W_{L} W_{1}x\] (1)

where \(W_{k}\)'s are weight matrices and \(x\) is the input data. The linear network can be obviously treated as an over-parameterized model of standard linear model, \(^{T}=W_{L} W_{1}\). However, the introduction of the over-parameterization brings significant non-convexity, and thus complicates dynamics during learning. Until now, the direct analysis of implicit bias of GD and, particularly, SGD for standard deep linear networks with any depth and initialization on _regression problems_ is still an open problem.

In order to study the implicit bias of linear networks in regression settings, a simplified version of standard linear network, diagonal linear network (i.e., each neuron is only connected with a single neuron of the next layer [26; 22]), was proposed as a proxy to underpin the bias of the standard one. With this simplified model, the solution selected by GD solves a constrained norm minimization problem that interpolates between the \(_{1}\) and \(_{2}\) norms up to the initialization scale . For SGD, the solution is closer to that of the sparse regression when compared to GD with the same initialization .

However, in Section 3.2, our theoretical analysis shows that the induced implicit bias of diagonal linear networks is not consistent with standard linear network, at least for GD in regression settings. It is then natural to ask: _are there any proxy architectures that could produce similar implicit bias to that of standard linear networks?_ If so, we still expect the new proxy model be amenable to tractable analysis and provide insights for investigating the implicit bias of SGD and a wider spectrum of architectures. This is the main goal of our work.

On the other hand,  showed that weight matrices of linear networks for linearly separable classification will become low-rank when trained with GD and logistic loss.  also observed such low-rank bias for deep matrix factorization.  showed that SGD and weight decay jointly induces a low-rank bias in the weight matrices when training a neural network. In fact, for linear networks with a single output trained with GD, all weight matrices will automatically become rank-1 and will maintain this property during training when the initialization is balanced .

Our contributions.Inspired by this low-rank bias and to avoid the complicated analysis of standard linear networks, we propose a novel model **rank-1 deep linear network** as a plausible proxy of standard linear networks, to reveal the implicit bias of GD and SGD on over-parameterized linear models for regression problems. A depth-\(L\) rank-1 deep linear network \(f(x;u,v)\) is defined by

\[f(x;u,v):=w_{L}W_{L-1} W_{1}x=^{T}x,\ s.t.\ W_{k}=u_{k}v_{k}^{T},\  k\{1,,L-1\}\] (2)

where \(u_{k}^{d_{k+1}}\) and \(v_{k}^{d_{k}}\) are vectors while \(u\) and \(v\) are denoted as collections of \(u_{k}\) and \(v_{k}\), respectively. See Figure 1 for the difference between three types of linear networks. In this work we will show that the formulation above could offer us the possibility to understand the bias of GD and SGD for standard linear networks through the lens of results on rank-1 linear networks.

With the proposed model, this paper targets on precisely identifying the "potential" function \(V()\) such that GD converges to its minimizer constrained by zero training error (i.e., interpolation solution), and further characterizing the role of the noise induced by adding sampling stochasticity to GD, i.e., SGD, in perturbing the form of \(V()\). For the convenience of theoretical analysis, we focus on the continuous versions of GD and SGD, i.e., we study gradient flow (GF) and stochastic gradient flow (SGF). In particular, this paper establishes the following findings:

* For GF, we show that the solution implicitly minimizes a potential function \(V()\) that depends on the initialization and depth subject to \(f(x;u,v)\) achieving zero training error (Theorem 1). The single layer case recovers the standard linear regression results, while a depth larger than one immediately changes the form of \(V()\), which clearly connects the implicit bias of GF with the model architecture. We emphasize that our results explicitly reveal how depth and initialization jointly influence the implicit bias of GF.

Figure 1: **(a).** Standard linear networks. **(b).** Rank-1 linear networks. One neuron in the middle hidden layer is active and fully connected. **(c).** Diagonal linear networks. Every neuron is active but not fully connected.

More importantly, in Theorem 2, by showing the similarity of the implicit bias of GF for standard and rank-1 linear networks, we conclude that rank-1 linear networks are standard linear networks with special initialization when trained with GF, highlighting that our rank-1 linear network is a plausible proxy of standard linear networks. This offers us the possibility to explore the implicit bias of SGD for standard linear networks by analyzing the rank-1 linear nets. On the other hand, the diagonal linear networks exhibit drastically different implicit bias of GF when compared to standard linear networks, e.g., GF prefers \(_{2}\) minimum norm solution for standard linear networks when the initialization is small while, on the contrary, it prefers such solution when the initialization is large for diagonal linear networks.
* For SGF, we show that the sampling noise brings an extra effect that depends on several hyper-parameters such as the learning rate, batch size and network depth compared to GF. When \(L>1\), this extra effect "alleviates" the influence of the initialization, i.e., the final solution reduces its dependence on the initialization. More intriguingly, when \(L=1\), i.e., without over-parameterization, this effect brought by SGF immediately disappears. Thus _this implicit bias is jointly induced by both model depth and stochasticity from SGD_. To the best of our knowledge, we are the first to show such an implicit bias jointly affected by the two factors through the lens of the rank-1 networks, which is also empirically verified on standard linear and nonlinear networks. Our findings on SGF are summarized in Theorem 3.

Organization.In Section 2, we summarize the notations and the setup of our work. Section 3 and Section 4 present our main results where Section 3 focuses on the implicit bias of GD while Section 4 is about SGD. Numerical experiments are presented in Section 5 and we conclude this paper in Section 6. Some technical details are deferred to Appendix.

### Related Works

The study of implicit bias of GD has been pioneered by  on linearly separable classification problem and was generalized to GD for deep neural networks and different training strategies [11; 18; 5; 21; 15; 17]. Recently extensive works have made progress in this direction by focusing on the diagonal linear network model. Assuming the existence of a perfect solution in the regression setting,  showed that the solution selected by GF interpolates between \(_{2}\) minimum norm solutions and \(_{1}\) ones up to the initialization scale. Besides the full-batch gradient descent,  then analyzed SGF for diagonal linear networks and concluded that the sampling noise brought by SGF reduces the effective initialization scale when compared with GF, leading its solution to be closer to a sparse one. Aside from the sampling noise induced by SGF, [9; 23] also investigated the influence of the label noise in the diagonal linear networks setting. For GD and SGD with moderate learning rate,  studied their implicit bias for diagonal linear networks and revealed the corresponding influence of the finite learning rate.

This paper considers the rank-1 deep linear network to study the implicit bias of GD and SGD. We briefly introduce differences between the settings considered here and those in previous works. For GD on standard linear networks,  did not restrict the initialization for the 2-layer case while  required the initialization for standard linear networks to be nearly-zero without restricting the number of layers.  considered GD for shallow ReLU nets. In this work, we consider linear networks without requirements either on the scale of the initialization or the number of layers. For GD and SGD on the diagonal linear networks [3; 26; 22], we point out that the results are different with that for standard linear networks. Note that this is not to downgrade diagonal linear network, and instead the point is to show that different architectures could induce different implicit bias. The rank-1 linear networks considered here are close to the standard linear networks since it can be seen as standard linear networks with special initialization. Furthermore, the classification problem [24; 18; 5] and linear regression problem  have been comprehensively investigated and we focus on the over-parameterized regression. Finally, [4; 10; 19; 27; 9] focused on the flatness of the loss landscape and model parameters while our analysis is for the overall parametrization \(\).

## 2 Preliminaries and Setup

Notations.Given a dataset of \(n\) samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\), \(x_{i}^{d}\) represents a \(d\)-dimensional data vector with scalar label \(y_{i}\). We use \(X^{n d}\) to denote the data matrix and use \(y=(y_{1},,y_{n})\)to denote the collection of \(y_{i}\). \(\|\|\) denotes the \(_{2}\)-norm and \(\|\|_{F}\) is the Frobenius norm. \( a,b\) is the inner product. For any vector \(u\) and matrix \(A\), we use \(u(0)\) and \(A(0)\) to denote their initialization. We let \(()\) be the trace operator. For the weight matrix \(W_{k}^{d_{k} d_{k+1}},W_{k;ij}\) is its \(i\)-th row \(j\)-th column element. For a parametric model \(f(x;)=^{T}x\) with parameters \(\), we use \(()=_{i}^{n}_{i}()\) to represent the empirical loss where \(_{i}()\) is the loss function for the sample \((x_{i},y_{i})\).

Over-parameterized regression.We focus on the regression problem where \(n<d\) and assume the existence of solutions that perfectly fit the dataset, i.e., there exist interpolating parameters \(^{*}\) such that \( i\{1,,n\}: x_{i},^{*}=y_{i}\). The quadratic loss \(_{i}()\) is used in our setting. The empirical loss is then \(()=_{i=1}^{n}_{i}()= _{i=1}^{n}(y_{i}- x_{i},)^{2}.\)

Rank-1 deep linear networks.In this paper we consider the rank-1 deep linear network \(f(x;u,v)\) (see Eq. (2)). We use \(u\) and \(v\) to denote the collections of \(u_{k}\)'s and \(v_{k}\)'s, respectively. For convenience,

\[_{k}=w_{L}^{T}W_{L-1} W_{k+2}u_{k+1},\ _{L-1}=1,_{-k}=v_{k}^{T}W_{k-1} W_{2}u_{1},\ _{-1}=1\] (3)

such that the network can be written as \(f(x;u,v)=_{k}v_{k+1}^{T}u_{k}_{-k}v_{1}^{T}x\) for \(k\{1,,L-1\}\) when \(L 2\). Through this paper, we treat the depth \(L\) as a hyper-parameter of the network and try to precisely characterize its role in the implicit bias.

**Definition 1** (Balanced initialization for rank-1 linear networks).: _Given an \(L\)-layer rank-1 linear network Eq. (2), for any \(k\{1,,L-1\}\), the balanced initialization means that_

\[(0),u_{k}(0)^{2}}{\|v_{k+1}(0)\|^{2}\| u_{k}(0)\|^{2}}=1,\ \|v_{k+1}(0)\|=\|u_{k}(0)\|=\|v_{1}(0)\|.\]

We add more discussions on the initialization in Appendix C.1.

## 3 Equivalence Between Implicit Bias of GD for Standard and Rank-1 Nets

### Implicit bias of GF for rank-1 linear networks

In this section, we characterize the implicit bias of the continuous version of GD for the rank-1 deep linear networks \(f(x;u,v)\) Eq. (2). Note that the model parameters are updated according to the gradient flow

\[}{dt}=-_{u_{k}}(),\ }{dt}=- _{v_{k+1}}()\] (4)

for \(k\{1,,L-1\}\) and \(dv_{1}/dt=-_{v_{1}}(),\) which clarifies that \(u_{k}\) and \(v_{k}\), rather than \(W_{k}\), are the model parameters.

Previous work  showed that the solution selected by GF interpolates between the \(_{1}\) minimum norm solution and \(_{2}\) one depending on the initialization scale for diagonal linear networks. In this section we examine whether this is the case for the rank-1 deep linear network with depth \(L\). For convenience, we define

\[_{L}=,\ _{L}=\] (5)

where \(L\) is the number of layers. Recall that \((0)\) represents the initialization of \(\), we now state the main theorem regarding the implicit bias of GF for rank-1 linear networks below:

**Theorem 1** (Implicit bias of GF for rank-1 linear networks).: _For a rank-1 linear network, if the initialization is balanced across layers (Definition 1) and if the gradient flow solution \(()\) satisfies that \(X()=y\), then gradient flow converges to a minimizer of the potential function \(V()\):_

\[()=*{arg\,min}_{}V(),X =y,\]

_where_

\[V()=}\|\|^{_{L}}-^{T}}}.\] (6)While previous works focused on 2-layer standard linear networks  or multiple-layer linear networks with nearly zero initialization , i.e., \(\|(0)\| 0\), Theorem 1 builds the potential function \(V()\) that _depends on both the initialization \((0)\) and the depth of the network \(L\) explicitly_. We also plot the contours of \(V()\) for different numbers of layers and scales of initialization in Fig. 2 in a 2-dimensional space. Theorem 1 clearly reveals how over-parameterization and the initialization of \(\) guide GD to select different solutions.

Effects of depth.When \(L=1\), \(V()\) becomes \(\|\|^{2}-,(0)\) which is the same as the potential of the least square \(\|-(0)\|^{2}\) except for a constant term. When \(L>1\), \(V()\) is no longer an Euclidean distance. The form of \(V()\) depends on the depth and GD will favor different interpolating solutions. A particular interesting case is when \(L\) where we have \(_{L} 1\) and \(_{L} 1\). As a result, \(V()\) becomes \(V()\|\|-,(0)/\|(0)\|\), which reflects the difference between the norm of \(\) and the norm of its projection on the direction of the initialization \((0)\). Therefore, the direction of the initialization \((0)\) matters for the potential function \(V()\) and the final solution \(()\) while they are rather less sensitive to the scale of \((0)\). This may serve as a benefit of the over-parameterization in the sense that it makes the network more stable to different scales of the initialization. As a comparison, both direction and scale of \((0)\) are crucial for the least square potential function \(\|-(0)\|^{2}\).

Effects of initialization.When \(L\) is finite, to inspect the effects of the initialization, we can rewrite \(V()=\|\|^{_{L}}/_{L}-\|(0)\|^{1/(2L-1)} ,(0)/\|(0)\|.\) The second term will be more important when the initialization scale \(\|(0)\|\) is getting larger thus the initialization affects the implicit bias of GD more significantly. On the other hand, as \(\|(0)\| 0\), the second term vanishes. Thus we have the following corollary:

**Corollary 1.1**.: _Under conditions of Theorem 1, if we further assume that the initialization is infinitesimal \(\|(0)\| 0\) and the depth is finite, then the GF solution \(()\) is an \(_{2}\)-norm minimization solution:_

\[()=_{}\|\| s.t.\;X=y.\]

We now summarize the effects of initialization on the training regime as follows. **(i).** According to , for any \(D\)-homogeneous model (\(D\) is a positive integer), the lazy regime (or NTK regime) is reached for large initialization. Since the rank-1 linear network is a homogeneous model, the NTK regime is reached for large initialization and the implicit bias is given by the RKHS norm predictor accordingly; **(ii).** For vanishing initialization, according to Corollary 1.1, an \(_{2}\)-norm minimization predictor which can not be captured by the NTK kernel is returned. Therefore, as the initialization becomes smaller, we "escape" from the lazy regime and falls into the "Anti-NTK" regime defined

Figure 2: The plot of \(V()\) for different numbers of layers and initialization scales in a two-dimensional parameter space. The initialization in the first column is [0.08, 0.06] and is multiplied by different scale factors for the other two columns. In the first column where the initialization is nearly zero, GD has similar implicit bias for rank-1 linear networks of different layers. When we increase the initialization scale, the shape of the contour for linear regression potential function does not change, while for linear networks as shown in the last column, the contour of the potential function gradually presents a sharp angle as we increase the number of layers.

by . Furthermore, based on Corollary 1.1, it is now worth to mention that, assuming vanishing initialization, linear regression (i.e., 1-layer linear networks), 2-layer standard linear networks  and rank-1 linear networks with finite depth share a similar implicit bias if trained with GD. And the first column of Fig. 2 shows that rank-1 linear networks with different number of layers exhibit potential contours with a similar shape.

### Comparison between different architectures

The aforementioned phenomenon that GD will return an \(_{2}\)-norm minimization solution for both 2-layer standard linear networks and rank-1 linear networks with small initialization drives us to further investigate the comparison and similarities between different network architectures. For diagonal linear networks, GD prefers \(_{1}\)-norm minimization solution with nearly-zero initialization, which is drastically different from the case for rank-1 and standard linear networks. On the contrary, it prefers \(_{2}\)-norm minimization solution when the initialization is sufficiently large . This inconsistency begs us to ask the question _will standard linear networks and rank-1 linear networks share a similar implicit bias of GD when the initialization is large?_ To answer this question, in the following, we first analyze the implicit bias of GD for standard linear networks \(f(x;W)=w_{L}^{T}W_{L-1} W_{1}x\) with any depth \(L\), which corresponds to the parameterization \(^{T}=w_{L}^{T}W_{L-1} W_{1}\), with balanced initialization (Definition 2).

**Theorem 2** (Implicit bias of GF for standard linear networks with any initialization).: _For an \(L\)-layer standard linear network, if the initialization is balanced across layers, i.e., \(W_{k+1}^{T}(0)W_{k+1}(0)=W_{k}(0)W_{k}^{T}(0)\) for all layers, and if the gradient flow solution \(()\) satisfies that \(X()=y\), then gradient flow converges to a minimizer of the potential function \(V()\):_

\[()=*{arg\,min}_{}V_{std}(),X=y,\]

_where_

\[V_{std}()=\|\|^{}-^{T}}},\] (7)

Although we state before that the parameters of a rank-1 deep linear network are \(u_{k}\)'s and \(v_{k}\)'s rather than \(W_{k}\)'s, the form of the potential for rank-1 linear networks Eq. (6) is very similar to that of standard linear networks Eq. (7): \(V()\) for an \(L\)-layer rank-1 linear network is the same as \(V_{std}()\) for a \((2L-1)\)-layer standard linear network. In the following, we explain the reason behind this fact and conclude that rank-1 linear network can be seen as a qualified proxy for the standard linear network when studying the implicit bias of GD, and potentially SGD.

Rank-1 linear networks are standard linear networks with special initialization.We first present a useful proposition regarding a special kind of initialization for standard linear networks.

**Proposition 1** (Effects of diagonal and rank-1 initialization for standard linear networks).: _Given an \(L\)-layer standard linear network \(f(x;W)=w_{L}^{T}W_{L-1} W_{1}x\) where \(W_{k}^{d_{k+1} d_{k}}\) for \(k\{1,,L\}\), if the weights are initialized such that only one column of \(W_{k}\) is non-zero when \(k\) is even and only one row of \(W_{k}\) is non-zero otherwise, i.e., for an integer \(p\)_

\[ k=2p+1\{1,,L\}:W_{k;ij}(0)=0i c_{k}, c_{k}\{1, d_{k+1}\}\] \[ k=2p\{1,,L\}:W_{k;ij}(0)=0j c_{k},\]

Figure 3: GD maintains the shape of rank-1 initialization for the weight matrices (the upper panel), but destroys that of the diagonal initialization for standard linear networks (the lower panel), as formally described by Proposition 1.

_then for any \(t>0\):_

\[ k=2p+1\{1,,L\}:W_{k;ij}(t)=0i c_{k};\] \[ k=2p\{1,,L\}:W_{k;ij}(t)=0j c_{k}.\]

_Furthermore, if the weights are initialized as the diagonal shape, i.e., \( k\{1,,L-1\},W_{k;ij}(0)=0\) if \(i j\), then GD does not maintain the diagonal shape of weight matrices._

Remark.A \((2L-1)\)-layer standard linear network initialized as in Proposition 1 with \(c_{k}=1\) for all layers has the same formulation as our \(L\)-layer rank-1 architecture. According to Proposition 1, the special structure of the initialization of such standard linear network will be maintained if we run GD, which suggests that it will always have the same formulation as our \(L\)-layer rank-1 linear network, see the upper panel of Fig. 3. Therefore, an \(L\)-layer rank-1 linear network can be seen as a \((2L-1)\)-layer standard one with special initialization, and they exhibit similar implicit bias of GD. On the other hand, if the standard linear network is initialized as the diagonal shape as in the diagonal net, GD will not maintain this property (the lower panel of Fig. 3), i.e., the diagonal structure cannot be seen as a standard one. In this sense, the diagonal network exhibits special implicit bias particularly due to its structure, while the rank-1 linear network is a more qualified proxy of standard linear networks.

## 4 Implicit bias of SGD for Rank-1 Linear Networks

Recently, [23; 22] developed the characterization of the implicit bias of GD with label noise and SGD in diagonal linear networks. However, as mentioned earlier, its diagonal structure is special in the sense that the corresponding results can not be generalized to the case for standard linear networks. On the contrary, our results in Section 3 reveal that rank-1 linear networks can be seen as standard linear networks with special initialization. In this section, to take a step forward towards understanding the implicit bias of SGD for standard linear networks, we explore the continuous part of SGD, stochastic gradient flow (SGF), for the rank-1 linear networks in the over-parameterized regression setting. We begin with the introduction of the definition of SGD and our modelling techniques.

Sgd.Unlike the full-batch GD where the parameters are updated according to Eq. (4), the SGD dynamics is

\[u_{k}(t+1)=u_{k}(t)-_{i_{t}}_{u_{k}} _{i}(),\;v_{k+1}(t+1)=v_{k+1}(t)-_{i_{t} }_{v_{k+1}}_{i}()\] (8)

for \(k\{1,,L-1\}\) where \(t\) denotes the iteration step, \(\) is the learning rate, \(b\) is the batch-size, and \(_{t}\) consists of \(b\) points randomly sampled from the uniform distribution \([1,n]\).

Continuous Modelling of SGD.The continuous modelling techniques for SGD have been widely applied in recent works [1; 9; 23; 22] to study the dynamics of SGD. In our setting, recalling the definition of \(_{k}\) and \(_{-k}\) in Eq. (3), the continuous counterpart of SGD, SGF, is given by the following set of stochastic differential equations (SDE):

\[du_{k} =-v_{1}^{T}X^{T}r_{k}_{-k}v_{k+1}dt+2}{nb}}(_{k}_{-k})v_{k+1}v_{1}^{T}X^{T}d_{t}\] (9) \[dv_{k+1} =-v_{1}^{T}X^{T}r_{k}_{-k}u_{k}dt+2}{nb}}(_{k}_{-k})u_{k}v_{1}^{T}X^{T}d _{t}\] (10)

where \(r=(f(x_{1};u,v)-y_{1},,f(x_{n};u,v)-y_{n})^{T}^{n}\) is the residual, and \(_{t}\) is a standard Brownian motion in \(^{n}\). For the parameterization of \(\) (Eq. (2)), we now aim to characterize the implicit bias of SGD by showing the existence of a function \(V()\) such that the solution of the stochastic dynamics converges to its minimizer under the constraint of zero-training loss as follows.

**Theorem 3** (Implicit bias of SGF for rank-1 linear networks).: _For the rank-1 linear network Eq. (2) that is trained with SGF (Eq. (9) and (10)) in the over-parameterization regression, if the initialization is balanced across layers, then the dynamics of \(\) gives us_

\[d_{}V^{S}(,t)=-_{}dt+2}{nb}}X^{T}d_{t}\] (11)_where_

\[V^{}(,t)=}\|\|^{_{L}}+(0)}{\|(0)\|^{_{L}}}-^{T}}{ nb}_{0}^{t}((s))(P_{}( (s))X^{T}X)}{\|(s)\|^{2-_{L}}}(s)ds\] (12)

_with \(P_{}()=I-^{T}/\|\|^{2}\) being the orthogonal projection operator of \(\). Furthermore, if the SGF solution \(()\) satisfies that \(X=y\), then the model parameter \(\) converges to a minimizer of \(V^{}(,)\) as \(t\):_

\[()=*{arg\,min}_{}V^{}(, ),X=y.\]

One can immediately notice that the R.H.S of Eq. (11) includes a noise term, which can be interpreted as that the model parameter \(\) follows a mirror flow with a noise term. Furthermore, Eq. (12), which characterizes the optimization geometry, depends on time explicitly. This partly reflects its stochastic nature--the optimization trajectory is not deterministic. Furthermore, as in , it is also possible to generalize the current vanishing learning rate results to the moderate learning rate analysis by deriving a stochastic mirror descent recursion with time varying potentials.

Implicit bias jointly induced by stochasticity and architectures. The sampling noise of SGD introduces an extra term (the last term of Eq. (12)) that depends on several parameters such as the learning rate and the data matrix \(X\) when compared with that of GD (Eq. (6)). Interestingly, this extra term only exists when the model is _simultaneously_ over-parameterized and trained with the existence of the sampling noise. If the model is not over-parameterized, i.e., standard linear regression when \(L=1\), then \(_{L}=0\) and this term disappears. On the other hand, if there is no sampling noise this term also vanishes. Indeed, the \(L=1\) case has been widely studied by recent works , and it turns out that GD and SGD share similar implicit bias when there is no over-parameterization. Theorem 3 confirms this phenomenon and, more importantly and intriguingly, reveals a new connection between the architecture-induced over-parameterization and the implicit bias of the optimization algorithms.

**"Alleviating" the effects of initialization.** When \(L\), the last term of Eq. (12) brought by the sampling noise can be seen as _"alleviating" the influence of the initialization \((0)\) such that the training dynamics reduces its dependence on the initialization_ (Fig. 4(a)). Due to the difficulty of explicitly solving the stochastic integral in Eq. (12), we give here a qualitative interpretation of this alleviating effect. As \(L\), i.e., infinitely deep linear network, we have \(_{L} 1\), therefore the integral in Eq. (12) becomes \(-_{0}^{}()(P_{} ()X^{T}X)ds\). In a \(d\)-dimensional space, we let \((t):=_{0}^{t}()(P_{}()X^{ T}X)/\|\|ds\) be the position of a particle \(A\) that starts from the origin. Then the integral of Eq. (12) amounts to the final position \(()\) of \(A\), which moves

Figure 4: **(a) The blue trajectory is for gradient flow (GF) that converges to \(^{*}_{}\) (blue square). The green trajectory that converges to \(^{*}_{}\) (green square) is for stochastic gradient flow (SGF). \((0)\) denotes the initialization of \(\) and \(^{*}_{_{2}}\) (red square) is the \(_{2}\)-norm minimum solution. Black dot marks the origin. Compared to \(^{*}_{}\), \(^{*}_{}\) is closer to \(^{*}_{_{2}}\) since its dependence on the initialization is reduced. Note that when \((0)\) is nearly zero (the left figure), both \(^{*}_{}\) and \(^{*}_{}\) are close to \(^{*}_{_{2}}\) (Corollary 1.1). **(b)** The red arrow denotes the direction of \(()\) while the black one denotes that of \((0)\), which, as shown in the figure, is particularly important to the direction of \(()\). See more experiments in Appendix A.3.

along the direction of \((t)/\|(t)\|\) with speed proportional to \(((t))(P_{}((t))X ^{T}X)\). Since \(\|X\|_{F}^{2}-\|X\|_{2}^{2}(P_{}((t))X ^{T}X)\|X\|_{F}^{2}\) where \(\|X\|_{F}^{2}\) is finite, it is highly likely that the direction of \(()\) heavily depends on \((0)\). This is because the velocity magnitude, which is proportional to the value of empirical loss \(((t))\), is large when \(t=0\) and quickly shrinks along its trajectory. As a result, in Eq. (12), the effect coming from the initialization \(^{T}(0)/\|(0)\|\) is alleviated by the extra term \(-^{T}()\) induced by the sampling noise and over-parameterization. In this sense, \(V^{}(,)\) is closer to a simple \(_{2}\)-norm \(\|\|\) when compared with GD, i.e., the SGD solution is more likely to be an \(_{2}\) minimum norm solution when compared to GD. We also present the trajectory of \(\) in Fig. 4(b). The trajectory is obtained by first training a rank-1 linear network using SGD for 5000 iterations and computing \((t)\) at every step followed by a projection of the trajectory \(\{(t)\}_{t=1}^{5000}\) into a 2-dimensional space according to the method described in . It can be seen that the direction of the final position \(()\) is close to that of \((0)\) (\((),(0)/(\|()\|(0)\|)=0.9314\)). We also conduct additional experiments in Appendix A.3 to further verify our finding.

Comparison with diagonal linear networks.In , the authors showed that the extra effects of the sampling noise of SGD is equivalent to multiplying a shrinking coefficient which depends on the training dynamics to the initialization scale. This effect leads the solution of SGD to be closer to that of sparse regression when compared with GD. Our Theorem 3 does not show such bias that the solution interpolates between the \(_{2}\)-norm and \(_{1}\)-norm minimization solution. Therefore the conclusion of implicit bias of SGD for diagonal linear networks can not be directly applied to rank-1 linear networks and standard linear networks. This reveals an important finding that the implicit bias of GD or SGD is strongly tied with network architecture. On the other hand, similar to the case of diagonal linear networks, Theorem 3 also reveals an initialization cancellation effect induced by the SGD sampling noise. It is interesting for future work to explore whether this effect can be generalized to other architectures and can be seen as a special benefit brought by SGD.

## 5 Numerical Experiments

In this section, we consider the over-parameterized regression problem with rank-1 linear, standard linear, and non-linear networks for different initialization scales to verify our theoretical claims. We define the distance \(D(_{1},_{2})=\|_{1}-_{2}\|^{2}/\|_{2}\|^{2}\) to measure the relative difference between \(_{1}\) and \(_{2}\). For a linear network and the parameterization \(^{T}=w_{L}^{T}W_{L-1} W_{1}\), we use \(()\) to denote the solution returned by the optimization algorithms (GD or SGD) and \((0)\) to denote the corresponding initialization. \(^{*}\) is the ground truth solution, and \(^{*}_{_{2}}\) is the \(_{2}\) minimum norm solution (Corollary 1.1). Details of the experiments and more numerical experiments are deferred to Appendix A.1 and we focus on the results here.

Rank-1 linear networks.As shown in Fig. 5(a), for all different initialization scales \(\|(0)\|\), \(D((),^{*}_{_{2}})\) is smaller if \(()\) is returned by SGD, i.e., the SGD solution is closer to the \(_{2}\) minimum norm solution \(^{*}_{_{2}}\) when compared to the GD solution. Similarly, Fig. 5(b) shows a benefit of such alleviating dependence of initialization bias of SGD: its solution is closer to the ground truth solution \(^{*}\) when compared to the GD solution for different \(\|(0)\|\). Furthermore, in Fig. 6(a), we show \(D((t),^{*}_{_{2}})\) along training, which further reveals that the final solution returned by SGD is closer to \(^{*}_{_{2}}\). Note that when the initialization scales are small, both GD solution and SGD solution are close to \(_{_{2}^{*}}\). These experiments well support our theoretical findings. Besides, we present additional experiments to compare diagonal and rank-1 linear networks in Appendix A.2.

Standard linear networks and non-linear networks.Proposition 1 states similarity between rank-1 and standard linear nets, therefore they have similar results. We conduct numerical experiments for standard linear networks as in the case of rank-1 linear networks and show in Fig. 5(c) and 5(d) that \(D((),_{_{2}}^{*})\) and \(D((),^{*})\) are also smaller if we run SGD, which supports the generalization of conclusion of rank-1 linear network to standard linear networks. \(D((t),_{_{2}}^{*})\) along training plotted in Fig. 6(b) further supports this phenomenon. For non-linear networks (Fig. 6(c)), we report the test error on a newly sampled test set for both GD and SGD along training, where SGD solutions have smaller test error when compared to GD solutions.

## 6 Discussion & Conclusion

Our work proposes the rank-1 linear network that is a plausible proxy of standard linear networks. We analyze the implicit bias of GD and SGD for this new net and find that it approximates standard linear networks better than diagonal linear networks. We further reveal the joint role of over-parameterization and stochasticity in characterizing the implicit bias of SGD. Similar to the diagonal linear networks, our results also reveal an initialization alleviating effect of SGD sampling noise, suggesting a future direction that investigates whether such effect is general across different architectures. See Appendix B for more discussions.

**Limitation.** We do not generalize the analysis to non-linear networks due to its lack of the form of \(\) as in our current approach. Furthermore, an exact characterization of the stochastic integral is absent.