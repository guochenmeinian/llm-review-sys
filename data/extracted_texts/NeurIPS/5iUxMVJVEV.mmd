# Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis

Qiang Wu Gechang Yao Zhixi Feng Shuyuan Yang

Key Laboratory of Intelligent Perception and Image Understanding of

Ministry of Education, School of Artificial Intelligence, Xidian University, China

{wu_qiang, yao_gechang}@stu.xidian.edu.cn, {zxfeng, syyang}@xidian.edu.cn

###### Abstract

Time series analysis finds wide applications in fields such as weather forecasting, anomaly detection, and behavior recognition. Previous methods attempted to model temporal variations directly using 1D time series. However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation. In terms of periodicity, taking weather and traffic data as an example, there are multi-periodic variations such as yearly, monthly, weekly, and daily, etc. In order to break through the limitations of the previous methods, we decouple the implied complex periodic variations into inclusion and overlap relationships among different level periodic components based on the observation of the multi-periodicity therein and its inclusion relationships. This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid. To further extract complex temporal variations, we introduce self-attention mechanism into the periodic pyramid, capturing complex periodic relationships by computing attention between periodic components based on their inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer demonstrates outstanding performance in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. The code is available at [https://github.com/WuQiangXDU/Peri-midFormer](https://github.com/WuQiangXDU/Peri-midFormer).

## 1 Introduction

Time series analysis stands as a foundational challenge pivotal across diverse real-world scenarios , such as weather forecasting , imputation of missing data within offshore wind speed time series , anomaly detection for industrial maintenance , and classification . Due to its substantial practical utility, time series analysis has garnered considerable interest, leading to the development of a large number of deep learning-based methods for it.

Different from other forms of sequential data, like language or video, time series data is continuously recorded, capturing scalar values at each time point. Furthermore, real-world time series variations often entail complex temporal patterns, where multiple fluctuations (e.g., ascents, descents, fluctuations, etc.) intermingle and intertwine, particularly salient is the presence of various overlapping periodic components in it, rendering the modeling of temporal variations exceptionally challenging.

Deep learning models, known for their powerful non-linear capabilities, capture intricate temporal variations in real-world time series. Recurrent neural networks (RNNs) leverage sequential data, allowing past information to influence future predictions [6; 7]. However, they face challenges with long-term dependencies and computational inefficiency due to their sequential nature. Temporal convolutional neural networks (TCNs) [8; 9] extract variation information but struggle with capturing long-term dependencies. Transformers with attention mechanisms have gained popularity for sequential modeling [10; 11], capturing pairwise temporal dependencies among time points. Yet, discerning reliable dependencies directly from scattered time points remains challenging . Timesnet  innovatively transforms 1D time series into 2D tensors, unifying intra- and inter-period variations in 2D space. However, it overlooks inclusion relationships between periods of different scales and is constrained by limited feature extraction capability of CNNs, hindering its ability to explore complex relationships within time series.

We analyze time series by examining the inclusion and overlap (hereinafter collectively referred to as inclusion) relationships between various periodic components to address complex temporal variations. Real-world time series often show multiple periodicities, like yearly and daily weather variations or weekly and daily traffic fluctuations. And these periods exhibit clear inclusion relationships, for instance, yearly weather variations encompass multiple daily weather variations. Besides variations between different period levels, it also occur within periods of the same level. For example, daily weather variations differ based on conditions like sunny or cloudy. Due to these inclusion and adjacency relationships, different periods show similarities, with short and long periods being consistent in overlapping portions, and periods of the same level being similar. Additionally, a long period can be decomposed into multiple short ones, forming a hierarchical pyramid structure. In our study, time series without explicit periodicity are treated as having infinitely long periods.

Based on the above analysis, we decompose the time series into multiple periodic components, forming a pyramid structure where longer components encompass shorter ones, termed the Periodic Pyramid as shown in Figure 1, which illustrates the intricate periodic inclusion relationships within the time series. Each level consists of components with the same period, exhibiting the same phase, while different levels contain components with inclusion relationships. This transformation converts the original 1D time series into a 2D representation, explicitly showing the implicit multi-period relationships. Within the Periodic Pyramid, a shorter period may belong to two longer periods simultaneously, reflecting the complexity of the time series. There is a clear similarity between components within the same level and those in adjacent levels where inclusion relationships exist. Thus inspired by Pyraformer , we propose the **Periodic Pyramid Transformer** (**Peri-midFormer**), which computes self-attention among periodic components to capture complex temporal variations in time series. Furthermore, we consider each branch in the Periodic Pyramid as a Periodic Feature Flow, and aggregating features from multiple flows to provide rich periodic information for downstream tasks. In experiments, Peri-midFormer achieves state-of-the-art performance in various analytic tasks, including forecasting, imputation, anomaly detection, and classification.

1. Based on the inclusion relationships of multiple periods in time series, this paper proposes a top-down constructed Periodic Pyramid structure, which expands 1D time series variations into 2D, explicitly representing the implicit multi-period relationships within the time series.
2. We propose Peri-midFormer, which uses the Periodic Pyramid Attention Mechanism to automatically capture dependencies between different and same-level periodic components, extracting diverse temporal variations in time series. Additionally, to further harness the potential of Peri-midFormer, we introduce Periodic Feature Flows to provide rich periodic information for downstream tasks.
3. We conduct extensive experiments on five mainstream time series analysis tasks, and Peri-midFormer achieves state-of-the-art across all of them, demonstrating its superior capability in time series analysis.

The remainder of this paper is structured as follows. Section 2 briefly summarizes the related work. Section 3 details the proposed model structure. Section 4 extensively evaluates our method's performance across five main time series analysis tasks. Section 5 presents ablations analysis, Section 6 presents complexity analysis, and Section 7 discusses our results and future directions.

Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.

Related Work

Temporal variation modeling, a crucial aspect of time series analysis, has been extensively investigated. In recent years, numerous deep models have emerged for this purpose, including MLP [14; 15], TCN , and RNN [6; 7]-based architectures. Furthermore, Transformers have shown remarkable performance in time series forecasting [16; 12; 17; 18]. They utilize attention mechanisms to uncover temporal dependencies among time points. For instance, Wu et al.  introduce Autoformer with an Auto-Correlation mechanism, adept at capturing series-wise temporal dependencies derived from learned periods. Moreover, to address complex temporal patterns, they adopted a deep decomposition architecture to extract seasonal and trend parts from input series. Subsequently, FEDformer  enhances seasonal-trend decomposition through a mixture-of-expert design and introduces sparse attention within the frequency domain. Pyraformer  constructs a down-top pyramid structure through multiple convolution operations on time series to address the issue of long information propagation paths in Transformers, significantly reducing both time and space complexity. PatchTST  partitions individual data points into patches and uses them as tokens for the Transformer, thereby enhancing its understanding of local information in time series. Additionally, PatchTST innovatively processes each channel separately, making it particularly suitable for forecasting tasks.

Additionally, there are some recent innovative works. Timesnet  unravels intricate temporal patterns by exploring the multi-periodicity of time series and captures temporal 2D-variations using computer vision CNN backbones. GPT4TS  ingeniously utilizes the large language model GPT2 as a pretrained model, fine-tuning some of its structures with time series, achieving state-of-the-art results. FITS  proposes a time series analysis model based on frequency domain operations, requiring very low parameter count and memory consumption. And recent works have considered multi-scale information in time series. PDF  captures both short-term and long-term variations by transforming 1D time series into 2D tensors using a multi-periodic decoupling block. It achieves superior forecasting performance by modeling these decoupled variations and integrating them for accurate predictions. SCNN  decomposes multivariate time series into long-term, seasonal, short-term, co-evolving, and residual components, enhancing interpretability, adaptability to distribution shifts, and scalability by modeling each component separately. TimeMixer  uses a novel multiscale mixing approach, decomposing time series into fine and coarse scales to capture both detailed and macroscopic variations. It employs Past-Decomposable-Mixing to extract historical information and Future-Multipredictor-Mixing to leverage multiscale forecasting capabilities, achieving great performance in forecasting task.

## 3 Methodology

### Model Structure

The overall flowchart of the proposed approach is shown in Figure 2, it begins with time embedding of the original time series at the top. Then, we use the FFT to decompose it into multiple periodic components of varying lengths across different levels, with lines indicating the inclusion relationships between them. Moving down, padding and projection are then applied to ensure uniform dimensions, forming the Periodic Pyramid. Each component is treated as an independent token and receives positional embedding. Next, the Periodic Pyramid is fed into Peri-midFormer, which consists of multiple layers for computing Periodic Pyramid Attention. Finally, depending on the task, two strategies are employed: for classification, components are directly concatenated and projected into the category space; for other reconstruction tasks (since forecasting, imputation, and anomaly detection all necessitate the model to reconstruct the channel dimensions or input lengths, we collectively refer to such tasks as reconstruction tasks), features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output. Please note that we referred to  for de-normalization and  for time series decomposition to maximize the effectiveness of our method, but we omitted these details from the figure to maintain simplicity. See Appendix A for a complete flowchart. Further details are provided below.

### Periodic Pyramid Construction

Multiple periods in the time series exhibit clear inclusion relationships, however, the 1D structure limits the representation of variations between them. Hence, it's crucial to separate periodic components with inclusion relationships to explicitly represent implicit periodic relationships. Firstly, as Peri-midFormer is designed to focus on periodic components, we first normalize the original time series \(^{L C}\) that with length \(L\) and \(C\) channels, then decompose it to obtain the seasonal part \(_{s}^{L C}\), thus removing the interference of the trend part. For a detailed description of normalization and decomposition, please refer to the Appendix A. Then we partition \(_{s}\) into periodic components, following the approach used in Timesnet . It's important to note that, inspired by PatchTST , we retain the channel dimension \(C\), as it is advantageous for Peri-midFormer in capturing periodic features within each channel (note that we adopt a channel independent strategy and the Figure 2 shows the processing of only one of the channels). The periodic components are extracted in the frequency domain, accomplished specifically through FFT:

\[=Avg(Amp(FFT(_{s}))), \{f_{1},,f_{k}\}=*{arg\,Topk}_{f_{*}\{1,,\}}(),p_{ i}=},i\{1,,k\}, \]

where \(FFT()\) and \(Amp()\) denote Fourier Transform and amplitude calculation, respectively. \(^{L}\) represents the amplitude of each frequency, averaged across \(C\) channels using Avg(\(\)). Note that the \(j\)-th value \(j\) denotes the intensity of the \(j\)-th frequency periodic basis function, associated with period length \(\). To handle frequency domain sparsity and minimize noise from irrelevant high frequencies , the top-\(k\) amplitude values \(\{_{f_{1}},,_{f_{k}}\}\) corresponding to the most significant frequencies \(\{f_{1},,f_{k}\}\) are selected, where \(k\) is a hyper-parameter, beginning from 2 to ensure the fundamental pyramid structure. Additionally, to ensure the top level of the pyramid corresponds to the original time series, we define \(f_{1}=1\), with other frequencies arranged in ascending order. These selected frequencies correspond to \(k\) period lengths \(\{p_{1},,p_{k}\}\), arranged in descending order. Due to the frequency domain's conjugacy, only frequencies within \(\{1,,\}\) are considered. Based on the selected frequencies \(\{f_{1},,f_{k}\}\) and their associated period lengths \(\{p_{1},,p_{k}\}\), we partition the original time series into periodic components for each pyramid level, denoted as \(_{}\):

\[_{}=\{_{}^{1},_{}^{2},, _{}^{n}\},\{1,,k\},n\{1,,f_{k}\}, \]

where \(_{}^{n}\) denotes the \(n\)-th periodic component in the \(\)-th pyramid level. Here, \(\) is the pyramid level index, starting from the top and increasing, with a maximum value of \(k\), indicating the number of levels determined by the selected periods. Similarly, \(n\) represents the component index within a level, increasing from left to right, with a maximum value of \(f_{k}\), indicating the number of components per level is determined by the frequency corresponding to that period in the original series. The Periodic Pyramid can thus be represented as:

\[=Stack(_{}),\{1,,k\}, \]

Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.

here \(Stack()\) denotes a stacking operation. The constructed Periodic Pyramid, depicted in the upper right of Figure 2, displays evident inclusion relationship among different levels, shown by connections between levels. Let \(R\) denote the relationship between pairs of periodic components from the upper and lower levels, determined by the presence or absence of overlap as follows:

\[R_{-1,}^{n_{-1},n_{}}=\{1,\;Index(_{-1}^{n_{-1}}) Index(_{}^{n_{}})>0\;\;\;,\; \{2,,k\},\,n_{-1},n_{}\{1,,f_{k}\},. \]

when \(R=1\), it signifies an inclusion relationship, while \(R=0\) indicates no overlap. This is illustrated in the left half of Figure 3. \(n_{}\) denotes the index of the \(n\)-th component in the \(\)-th level. \(Index()\) denotes the positional index of each data point within the periodic component at that level. Indices for points in the first level are contained in \(\{0,,L-1\}\). For subsequent levels, most indices match those of the first level. However, due to varying component lengths, there may be slight differences in indices for the last portion. Nonetheless, this doesn't impact relationship determination between components across levels. In practice, the relationship between the components is realized by masking the corresponding elements in the attention matrix.

Thanks to the inclusion relationships between periodic components across different levels in the Periodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented. Next, due to the varying lengths of the components, it's necessary to map \(^{}\) to the same scale for subsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows:

\[^{}=Projection(Padding(_{}^{n}) ),\{1,,k\},n\{1,,f_{k}\}, \]

where \(Padding()\) denotes zero-padding the periodic components across the time dimension to match the length of the original data, while \(Projection()\) represents a single linear mapping layer.

### Periodic Pyramid Transformer (Peri-midFormer)

Once we have the Periodic Pyramid, it can be inputted into the Peri-midFormer, as depicted in Figure 2. The Peri-midFormer introduces a specialized attention mechanism tailored for the Periodic Pyramid, called **P**eriodic **P**yramid **A**ttention **M**echanism (**PPAM**), shown in the right half of Figure 3. Here, original connections are replaced with bidirectional arrows, and also added within the same level. These bidirectional arrows signify attention between periodic components. In PPAM, inter-level attention focuses on period dependencies across levels, while intra-level attention focuses on dependencies within the same level. Note that attention occurs among all components within the same level, not just between adjacent ones. However, for clarity, not all attention connections within the same level are depicted.

In Periodic Pyramid, a periodic component \(_{}^{n}\) generally has three types of interconnected relationships (denoted as \(\)) with other components: the parent node in the level above (denoted as \(\)), all nodes within the same level including itself (denoted as \(\)), and the child nodes in its next level (denoted as \(\)). Therefore, the relationships of \(_{}^{n}\) can be expressed by the following equation:

\[\{_{}^{(n)}=_{}^{(n)} _{}^{(n)}_{}^{(n)}\\ _{}^{(n)}=\{_{-1}^{j}:j=\{n_{-1}:R_{-1,}^{n_{-1},n_{}}=1\}\}, 2 \\ _{}^{(n)}=\{_{}^{j}:1 j f_{k}\} \\ _{}^{(n)}=\{_{+1}^{j}:j=\{n_{+1}:R_{, +1}^{n_{},n_{+1}}=1\}\}, k-1 .. \]

Figure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism (right).

The equation shows that a component at the topmost level lacks a parent node, while one at the bottommost level lacks a child node. Based on the interconnected relationships \(\), the attention of the component \(_{}^{n}\) can be expressed as:

\[_{i}=_{m_{}^{}}_{i}_{m}^{}/})_{m}}{_{m _{}^{}}(_{i}_{m}^{ }/})}, \]

where \(\), \(\), and \(\) denote query, key, and value vectors, respectively, as in the classical self-attention mechanism. \(m\) used for selecting components that have interconnected relationships with \(_{}^{n}\). \(_{m}^{}\) represents the transpose of row \(m\) in \(K\). \(d_{K}\) refers to the dimension of key vectors, ensuring stable attention scores through scaling.

We apply this attention mechanism to each component across all levels of the Periodic Pyramid, enabling the automatic detection of dependencies among all components in the Periodic Pyramid and capturing the intricate temporal variations in the time series. For a detailed theoretical proof of the PPAM see the Appendix F.

### Periodic Feature Flows Aggregation

Here we explain the Periodic Feature Flows Aggregation used for reconstruction tasks. The output of the Peri-midFormer retains the original pyramid structure. To leverage the diverse periodic components across different levels, we treat a single branch from the top to the bottom of the pyramid as a periodic feature flow, highlighted by the red line in Figure 4. Since a periodic feature flow passes through periodic components at different levels, it contains periodic features of different scales from the time series. Additionally, due to variations among periodic components within each level, each feature flow carries distinct information. Therefore, we aggregate multiple feature flows through Periodic Feature Flow Aggregation. This involves linearly mapping each feature flow to match the length of the target time series and then averaging it across multiple feature flows to obtain the aggregated result \(_{s}\), as expressed in the following equation:

\[_{s}=MeanPolling(Projection(\{}_{1}^{n_ {1}},}_{2}^{n_{2}},,}_{}^{n_{k}}\} )),\{2,,k\},n_{k}\{1,,f_{k}\}, \]

where \(}_{}^{n_{k}}\) represents a specific periodic component in the Peri-midFormer's output, and \(\{}_{1}^{n_{1}},}_{2}^{n_{2}},,}_{}^{n_{k}}\}\) forms a feature flow, as shown in Figure 4. \(Projection()\) maps each feature flow to match the target output length. \(Meanpooling()\) averages the feature flows. \(_{s}\) indicates that this is the output from the seasonal part. Since we retained the channel dimension of the original time series, the result obtained after aggregating the periodic feature flows here becomes the shape of the final output. Finally, adding the trend part and de-normalization to obtain the ultimate output.

## 4 Experiments

We extensively test Peri-midFormer on five mainstream analysis tasks: short- and long-term forecasting, imputation, classification, and anomaly detection. We adopted the same benchmarks as Timesnet , see Appendix C for details. Due to space limits, we provide only a summary of the results here, more details about the datasets, experiment implementation, model configuration, and full results can be found in Appendix.

Figure 4: Periodic Feature Flows Aggregation.

**Baselines** The baselines include CNN-based models: TimesNet ; MLP-based models: LightTS , DLinear  and FITS ; Transformer-based models: GPT4TS , Time-LLM , iTransformer , TSLANet , Reformer , Pyraformer , Informer , Autoormer , FEDformer , Non-stationary Transformer , ETSformer , PatchTST . Besides, N-HiTS  and N-BEATS  are used for short-term forecasting. Anomaly Transformer  is used for anomaly detection. Rocket , LSTNet , TCN  and Flowformer  are used for classification.

### Main Results

Figure 5 displays the comprehensive comparison results between Per-midFormer and other methods, it consistently excels across all five tasks.

### Long-term Forecasting

**Setups** Referring to , we adopt eight real-world benchmark datasets for long-term forecasting, including Weather , Traffic , Electricity , Exchange , and four EIT  datasets (ETTh1, ETH2, ETTm1, ETTm2). Forecast lengths are set to 96, 192, 336, and 720. For the fairness of the comparison, we set the look-back window for all the methods to 512 (64 on Exchange), the results for other look-back windows can be found in the Appendix H.3

**Results** From Table 1, it is evident that Peri-midFormer performs exceptionally well, even completely outperforms GPT4TS and closely approaching the state-of-the-art method Time-LLM. While Time-LLM demonstrates remarkable capabilities in long-term forecasting, our Peri-midFormer shows clear advantages on the ETTh2, Electricity, and Exchange datasets. Although Time-LLM achieves the best results, it relies on a very large model, leading to significant computational overhead that is unavoidable. The same issue exists for GPT4TS. In contrast, our Peri-midFormer achieves performance close to that of Time-LLM without requiring excessive computational resources, making it more suitable for practical applications. Further analysis of model complexity is provided in Section 6. In addition, our Peri-midFormer exhibits better performance with longer look-back window, as further detailed in the Appendix E.6.

    &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Weather & 0.233 & 0.271 & 0.237 & 0.270 & 0.276 & 0.291 & **0.225** & **0.257** & 0.244 & 0.281 & 0.241 & 0.294 & 0.226 & 0.266 & 0.249 & 0.286 & 0.281 & 0.349 \\  ETTh1 & 0.409 & 0.430 & 0.427 & 0.426 & 0.422 & 0.440 & 0.408 & 0.423 & **0.407** & **0.422** & 0.418 & 0.438 & 0.430 & 0.444 & 0.492 & 0.490 & 0.913 & 0.748 \\  ETTh2 & **0.317** & **0.377** & 0.354 & 0.394 & 0.328 & 0.385 & 0.334 & 0.383 & 0.333 & 0.328 & 0.504 & 0.482 & 0.388 & 0.414 & 0.408 & 0.440 & 1.374 & 0.554 \\  ETTm1 & 0.354 & 0.385 & 0.352 & 0.383 & 0.348 & 0.383 & **0.329** & **0.372** & 0.358 & 0.376 & 0.379 & 0.363 & 0.391 & 0.398 & 0.418 & 0.724 & 0.609 \\  EFTm2 & 0.258 & 0.320 & 0.266 & 0.326 & 0.263 & 0.263 & 0.325 & **0.251** & **0.313** & 0.254 & **0.313** & 0.275 & 0.342 & 0.273 & 0.329 & 0.287 & 0.343 & 1.356 & 0.797 \\  Electricity & **0.152** & **0.202** & 0.167 & 0.263 & 0.195 & **0.224** & 0.158 & 0.252 & 0.168 & 0.263 & 0.167 & 0.268 & 0.166 & 0.257 & 0.217 & 0.314 & 0.299 & 0.391 \\  Traffic & 1.392 & 0.270 & 0.414 & 0.294 & 0.397 & 0.272 & **0.388** & **0.264** & 0.420 & 0.287 & 0.433 & 0.305 & 0.392 & 0.220 & 0.622 & 0.332 & 0.705 & 0.401 \\  Exchange & **0.346** & **0.393** & 0.373 & 0.410 & 0.365 & 0.410 & 0.371 & 0.409 & 0.393 & 0.429 & 0.495 & 0.493 & 0.418 & 0.433 & 0.701 & 0.593 & 1.157 & 0.844 \\  Average & **0.308** & 0.332 & 0.324 & 0.346 & 0.320 & 0.341 & **0.308** & **0.334** & 0.322 & 0.344 & 0.361 & 0.375 & 0.331 & 0.350 & 0.350 & 0.422 & 0.402 & 11.147 & 0.711 \\   

Table 1: Long-term forecasting task. The results are averaged from four different series length \(\{96,192,336,720\}\). See Table 13 and 14 for full results. **Red**: best, Blue: second best.

Figure 5: Model performance comparison.

### Short-term Forecasting

**Setups** For short-term analysis, we adopt the M4 , which contains the yearly, quarterly and monthly collected univariate marketing data. We measure forecast performance using the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), which are calculated as detailed in the Appendix D.1.

**Results** Table 2 shows that Peri-midFormer outperforms Time-LLM, GPT4TS, TimesNet, and N-BEATS, highlighting its exceptional performance in short-term forecasting. In the M4 dataset, some data lacks clear periodicity, such as the Yearly data, which mainly exhibits a strong trend. A similar situation is observed in the Exchange dataset for long-term forecasting task. Peri-midFormer performs well on these datasets due to its time series decomposition strategy. For a detailed analysis, please refer to the Appendix E.7.

### Time Series Classification

**Setups** We assessed Peri-midFormer's capacity for high-level representation learning via classification task. Mimicking settings akin to TimesNet , we tested it on 10 multivariate UEA classification datasets from , covering tasks like gesture recognition, action recognition, audio recognition, medical diagnoses, and other real-world applications.

**Results** As shown in Figure 6, Peri-midFormer achieves an average accuracy of 76.6%, surpassing all baselines including TSLANet (76.0%), GPT4TS (74.0%), TimesNet (73.6%), and all other Transformer-based methods. This suggests that Peri-midFormer has excellent time series representation capabilities. See Appendix H.1 for full results.

### Imputation

**Setups** To validate Peri-midFormer's imputation capabilities, we conduct experiment on six real-world datasets, including four ETT datasets  (ETTh1, ETTh2, ETTm1, ETTm2), Electricity , and Weather . We evaluate different random mask ratios (12.5%, 25%, 37.5%, 50%) for varying levels of missing data. Notably, due to the large number of missing values, the time series do not reflect their original periodicity. Therefore, before imputation, we simply interpolate the original missing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, which we call pre-interpolation. For a description of pre-interpolation and its impact on other methods, please refer to the Appendix E.5.

**Results** Table 3 demonstrates Peri-midFormer's outstanding performance on specific datasets (Electricity and Weather), surpassing other methods significantly and securing the highest average scores. However, its performance on other datasets was ordinary, possibly due to the lack of obvious periodic characteristics in them.

   } &  & Time-LLM & GPT4TS & TimesNet & Net-BEATS & N-BEATS & ETS- & Lights & Lights/TS & Dilation & FLOPS & Station & Auto & Pyra & Ine & Re- \\  & **Former** &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  SMAPE & 11.833 & 11.993 & 11.991 & **11.829** & 12.059 & 11.927 & 11.851 & 14.718 & 3.525 & 13.639 & 12.840 & 12.780 & 12.909 & 16.987 & 14.086 & 18.200 \\ MASE & **1.584** & 1.595 & 1.600 & 1.585 & 1.623 & 1.613 & 1.599 & 2.408 & 1.111 & 2.095 & 1.701 & 1.756 & 1.771 & 3.265 & 2.718 & 4.223 \\ OWA & **0.850** & 0.859 & 0.861 & 0.851 & 0.869 & 0.861 & 0.855 & 1.172 & 1.051 & 1.051 & 0.918 & 0.930 & 0.939 & 1.480 & 1.230 & 1.775 \\   

Table 2: Short-term forecasting task on M4. The prediction lengths are in \(\{6,48\}\) and results are weighted averaged from several datasets under different sample intervals. (\(*\) means former, Station means the Non-stationary Transformer.) See Table 12 for full results. **Red**: best, **Blue**: second best.

Figure 6: Model comparison in classification. The results are averaged from 10 subsets of UEA.

[MISSING_PAGE_FAIL:9]

considering periodic components. "w/o PPAM" divides the time series into periodic components but without Period Pyramid Attention Mechanism, using periodic full attention instead, wherein attention is computed among all periodic components. "w/o Feature Flows Aggregation" employs PPAM but without Periodic Feature Flows Aggregation. "Peri-midFormer" indicates our final approach.

**Results** Table 5 illustrates the incremental performance enhancement achieved with the integration of each additional module, validating the effectiveness of Peri-midFormer. Notably, good results are achieved even without PPAM. This can be attributed to the model's ability to extract periodic characteristics inherent in the original time series data by delineating the periodic components. However, without highlighting inclusion relationships through PPAM, periodic full attention's ability to capture temporal changes is limited, emphasizing the significance of PPAM.

## 6 Complexity Analysis

We conducted experiments on the model complexity of Peri-midFormer using the Heartbeat dataset for the classification task and the ETTh2 dataset for the long-term forecasting task. We considered the number of training parameters, FLOPs, and accuracy (or MSE). The results are depicted in Figure 7. In the classification task, Peri-midFormer not only achieves a significant advantage in accuracy but also requires relatively fewer training parameters and FLOPs, much less than many methods such as TimesNet, GPT4TS, Crossformer, and PatchTST. In the long-term forecasting task, Peri-midFormer achieves the lowest MSE without requiring the enormous FLOPs that Time-LLM does. This shows that although Time-LLM has strong long-term forecasting capabilities on most datasets, its computational demands are unacceptable. See Appendix E.4 for more analysis.

Further Model Analysis is provided in the Appendix E.

## 7 Conclusions

In this paper, we introduced a method for general time series analysis called Peri-midFormer. It leverages the multi-periodicity of time series and the inclusion relationships between different periods. By segmenting the original time series into different levels of periodic components, Peri-midFormer constructs a Periodic Pyramid along with its corresponding attention mechanism. Through extensive experiments covering forecasting, classification, imputation, and anomaly detection tasks, we validated the capabilities of Peri-midFormer in time series analysis, achieving outstanding results across all tasks. However, Peri-midFormer exhibits limitations, particularly in scenarios where the periodic characteristics are less apparent. We aim to address this limitation in future research to broaden its applicability.

Figure 7: Number of training parameters and FLOPs for Peri-midFormer versus baseline in terms of classification accuracy for the UEA Heartbeat dataset (left) and long-term forecasting MSE for the ETTh2 dataset (right). In the left graph, the closer to the top left, the better, while in the right graph, the closer to the bottom left, the better. Note that in the long-term forecasting, we did not fully depict the corresponding sizes due to the oversized FLOPs of Time-LLM, but instead illustrated it with text.