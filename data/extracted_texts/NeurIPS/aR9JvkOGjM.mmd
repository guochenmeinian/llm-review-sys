# Improved Regret for Bandit Convex Optimization

with Delayed Feedback

 Yuanyu Wan\({}^{1,2,3}\), Chang Yao\({}^{1,2,3}\), Mingli Song\({}^{2,3}\), Lijun Zhang\({}^{4,2}\)

\({}^{1}\)School of Software Technology, Zhejiang University, Ningbo, China

\({}^{2}\)State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China

\({}^{3}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security, Hangzhou, China

\({}^{4}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

{wanyy,changy,brooksong}@zju.edu.cn, zhanglj@lamda.nju.edu.cn

###### Abstract

We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Let \(n,T,\) denote the dimensionality, time horizon, and average delay, respectively. Previous studies have achieved an \(O(T^{3/4}+(n)^{1/3}T^{2/3})\) regret bound for this problem, whose delay-independent part matches the regret of the classical non-delayed bandit gradient descent algorithm. However, there is a large gap between its delay-dependent part, i.e., \(O((n)^{1/3}T^{2/3})\), and an existing \(()\) lower bound. In this paper, we illustrate that this gap can be filled in the worst case, where \(\) is very close to the maximum delay \(d\). Specifically, we first develop a novel algorithm, and prove that it enjoys a regret bound of \(O(T^{3/4}+)\) in general. Compared with the previous result, our regret bound is better for \(d=O((n)^{2/3}T^{1/3})\), and the delay-dependent part is tight in the worst case. The primary idea is to decouple the joint effect of the delays and the bandit feedback on the regret by carefully incorporating the delayed bandit feedback with a blocking update mechanism. Furthermore, we show that the proposed algorithm can improve the regret bound to \(O((nT)^{2/3}^{1/3}T+d T)\) for strongly convex functions. Finally, if the action sets are unconstrained, we demonstrate that it can be simply extended to achieve an \(O(n+d T)\) regret bound for strongly convex and smooth functions.

## 1 Introduction

Online convex optimization (OCO) with delayed feedback (Joulani et al., 2013; Quanrud and Khashabi, 2015) has become a popular paradigm for modeling streaming applications without immediate reactions to actions, such as online advertisement (McMahan et al., 2013) and online routing (Awerbuch and Kleinberg, 2008). Formally, it is defined as a repeated game between a player and an adversary. At each round \(t\), the player first selects an action \(_{t}\) from a convex set \(^{n}\). Then, the adversary chooses a convex function \(f_{t}():^{n}\), which causes the player a loss \(f_{t}(_{t})\) but is revealed at the end of round \(t+d_{t}-1\), where \(d_{t} 1\) denotes an arbitrary delay. The goal of the player is to minimize the regret \((T)=_{t=1}^{T}f_{t}(_{t})-_{ }_{t=1}^{T}f_{t}()\), i.e., the gap between the cumulative loss of the player and that of an optimal fixed action, where \(T\) is the number of total rounds.

Over the past decades, plenty of algorithms and theoretical guarantees have been proposed for this problem (Weinberger and Ordentlich, 2002; Langford et al., 2009; Joulani et al., 2013; Quanrud and Khashabi, 2015; Joulani et al., 2016; Heliou et al., 2020; Flaspohler et al., 2021; Wan et al., 2022a,b; Bistritz et al., 2022). However, the vast majority of them assume that the full information or gradientsof delayed functions are available for updating the action, which is not necessarily satisfied in reality. For example, in online routing (Awerbuch and Kleinberg, 2008), the player selects a path through a given network for some packet, and its loss is measured by the time length of the path. Although this loss value can be observed after the packet arrives at the destination, the player rarely has access to the congestion pattern of the entire network (Hazan, 2016). To address this limitation, it is natural to investigate a more challenging setting, namely bandit convex optimization (BCO) with delayed feedback, where only the loss value \(f_{t}(_{t})\) is revealed at the end of round \(t+d_{t}-1\).

It is well known that in the non-delayed BCO, bandit gradient descent (BGD), which performs the gradient descent step based on a one-point estimator of the gradient, enjoys a regret bound of \(O(T^{3/4})\)(Flaxman et al., 2005). Despite its simplicity, without additional assumptions on functions, there does not exist any practical algorithm that can improve the regret of BGD. Therefore, a few studies have proposed to extend BGD and its regret bound into the delayed setting (Heliou et al., 2020; Bistritz et al., 2022). Specifically, Heliou et al. (2020) first propose an algorithm called gradient-free online learning with delayed feedback (GOLD), which utilizes the oldest received but not utilized loss value to perform an update similar to BGD at each round. Let \(d=\{d_{1},,d_{T}\}\) denote the maximum delay. According to the analysis of Heliou et al. (2020), GOLD can achieve a regret bound of \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\), which matches the \(O(T^{3/4})\) regret of BGD in the non-delayed setting for \(d=O(T^{1/4})\). Very recently, Bistritz et al. (2022) develop an improved variant of GOLD by utilizing all received but not utilized loss values one by one at each round, and reduce the regret bound to \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\),1 where \(=(1/T)_{t=1}^{T}d_{t}\) is the average delay. However, there still exists a large gap between the delay-dependent part in the improved bound and an existing \(()\) lower bound (Bistritz et al., 2022). It remains unclear whether this gap can be filled, especially by improving the existing upper bound.

In this paper, we provide an affirmative answer to this question in the worst case, where \(\) is very close to \(d\). Specifically, we first develop a new algorithm, namely delayed follow-the-bandit-leader (D-FTBL), and show that it enjoys a regret bound of \(O(T^{3/4}+)\) in general. Notice that both the \(O((nd)^{1/3}T^{2/3})\) and \(O((nd)^{1/3}T^{2/3})\) terms in previous regret bounds (Heliou et al., 2020; Bistritz et al., 2022) can be attributed to the joint effect of the delays, and the one-point gradient estimator, especially its large variance depending on the exploration radius. To improve the regret, besides the one-point gradient estimator, we further incorporate the delayed bandit feedback with a blocking update mechanism, i.e., dividing total \(T\) rounds into several equally-sized blocks and only updating the action at the end of each block. Despite its simplicity, there exist two nice properties about the cumulative estimated gradients at each block.

* First, with an appropriate block size, its variance becomes proportional to only the block size without extra dependence on the exploration radius.
* Second, the block-level delay, i.e., the number of blocks waiting for computing the cumulative estimated gradients at each block, is in reverse proportion to the block size.

Surprisingly, by combining these properties, the previous joint effect of the delays and the one-point gradient estimator can be decoupled, which is critical for deriving our regret bound. Compared with the existing results, in the worst case, our regret bound matches the \(O(T^{3/4})\) regret of the non-delayed BGD for a larger amount of delays, i.e., \(d=O(n)\), and the delay-dependent part, i.e., \(O()\), matches the lower bound. Moreover, it is worth noting that our regret bound actually is better than that of Bistritz et al. (2022) as long as \(d\) is not larger than \(O((nd)^{2/3}T^{1/3})\), which even covers the case with \(=O(1)\) partially. To the best of our knowledge, this is the first work that shows the benefit of the blocking update mechanism in delayed BCO, though it is commonly utilized to develop projection-free algorithms for efficiently dealing with complicated action sets (Zhang et al., 2019; Garber and Kretzu, 2020; Hazan and Minasyan, 2020; Wan et al., 2020, 2022c; Wang et al., 2023, 2024b).

Furthermore, we consider the special case of delayed BCO with strongly convex functions. In the non-delayed setting, Agarwal et al. (2010) have shown that BGD can improve the regret from \(O(T^{3/4})\) to \(O((nT)^{2/3}^{1/3}T)\) by exploiting the strong convexity. If functions are also smooth and the action set is unconstrained, BGD has been extended to achieve an \(O(n)\) regret bound (Agarwal et al., 2010). Analogous to these improvements, we prove that our D-FTBL can achieve a regret bound of \(O((nT)^{2/3}^{1/3}T+d T)\) for strongly convex functions, and its simple extension enjoys a regret bound of \(O(n+d T)\) for strongly convex and smooth functions over unconstrained action sets. These regret bounds also match those of BGD in the non-delayed setting for a relatively large amount of delay. Moreover, the \(O(d T)\) part in these two bounds matches an \((d T)\) lower bound adapted from the easier full-information setting with strongly convex and smooth functions (Weinberger and Ordentlich, 2002).

## 2 Related work

In this section, we briefly review the related work on online convex optimization (OCO) and bandit convex optimization (BCO), as well as delayed feedback.

### Standard OCO and BCO

If \(d_{t}=1\) for all \(t[T]\), OCO with delayed feedback reduces to the standard OCO (Zinkevich, 2003). Online gradient descent (OGD) (Zinkevich, 2003; Hazan et al., 2007) is one of the most popular algorithm for this problem, which simply updates the action \(_{t}\) via a gradient descent step based on \( f_{t}(_{t})\). By using appropriate step sizes, OGD can achieve \(O()\) and \(O( T)\) regret bounds for convex and strongly convex functions, respectively. Follow-the-regularized-leader (FTRL) (Hazan et al., 2007; Shalev-Shwartz, 2011; Hazan, 2016) is an alternative algorithm, which chooses the new action by minimizing the linear approximation of cumulative loss functions under some regularization. With appropriate regularization, FTRL achieves the same \(O()\) and \(O( T)\) regret bounds as OGD. Moreover, Abernethy et al. (2008) have presented a lower bound of \(()\) for convex functions, and a refined lower bound of \(( T)\) for strongly convex functions, which implies that both OGD and FTRL are optimal.

BCO is a special yet more challenging case of OCO, where the player can only receive the loss value \(f_{t}(_{t})\) at each round \(t\). The first algorithm for BCO is bandit gradient descent (BGD) (Flaxman et al., 2005), which replaces the exact gradient used in OGD with an estimated gradient based on the single loss value (known as the classical one-point gradient estimator). By incorporating the approximation error of gradients into the regret analysis of OGD, Flaxman et al. (2005) establish an \(O(T^{3/4})\) regret bound for BGD with convex functions. Later, Agarwal et al. (2010) show that BGD enjoys an \(O((nT)^{2/3}^{1/3}T)\) regret bound for strongly convex functions, and can be extended to achieve an \(O(n)\) regret bound in the special case of unconstrained BCO with strongly convex and smooth functions. Saha and Tewari (2011) develop a new algorithm for BCO with smooth functions, and establish the \(O((nT)^{2/3}^{1/3}T)\) regret bound without the strongly convex assumption. van der Hoeven et al. (2020) propose novel BCO algorithms, which adaptively improve the previous regret bounds for convex and smooth functions if the norm of the comparator is small. By revisiting the case with strongly convex and smooth functions, several algorithms (Hazan and Levy, 2014; Ito, 2020) have been developed to achieve the \(O(n)\) regret bound in the constrained setting.

Moreover, a series of studies (Bubeck and Eldan, 2016; Hazan and Li, 2016; Bubeck et al., 2017; Lattimore, 2020; Bubeck et al., 2021) have been devoted to designing nearly optimal algorithms, which almost match the \((n)\) lower bound for the general BCO (Shamir, 2013) without any additional assumption. However, the running time of their algorithms are either exponential in \(n\) and \(T\), or polynomial with a high degree on \(n\) and \(T\), which is not suitable for practical large-scale applications. We refer the interested reader to Lattimore (2024) for a comprehensive survey on BCO. Additionally, we notice that BCO is closely related to the zero-order stochastic optimization (ZOSO) problem (Duchi et al., 2015; Bach and Perchet, 2016; Shamir, 2017), where the stochastic values are available for minimizing a fixed loss function. However, ZOSO is less challenging than BCO in the sense that it does not need to deal with time-varying functions and is usually allowed to query the loss value at two points per iteration.

### OCO and BCO with delays

The seminal work of Weinberger and Ordentlich (2002) first considers the case with a fixed delay, i.e., \(d_{t}=d\) for all \(t[T]\), and proposes a black-box technique that can covert any traditional OCOalgorithm into the delayed setting. The main idea is to maintain \(d\) instances of the traditional algorithm, and alternately utilize these instances to generate the new action. If the regret of the traditional algorithm is bounded by \((T)\), this technique can achieve an \(d(T/d)\) regret bound. Moreover, there exist \(()\) and \((d T)\) lower bounds for convex functions, and strongly convex and smooth functions, respectively (Weinberger and Ordentlich, 2002). However, the delays are not always fixed in practice, and its space complexity is \(d\) times as much as that of the traditional algorithm, which could be prohibitively resource-intensive. Although Joulami et al. (2013) have generalized this technique to deal with arbitrary delays, the space complexity remains high. Besides these black-box techniques, there exists a surge of interest in developing and analyzing specialized algorithms for delayed OCO (Langford et al., 2009; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Joulani et al., 2016; Li et al., 2019; Flaspohler et al., 2021; Wan et al., 2022, 2023; Wu et al., 2024), which do not require additional computational resources.

Despite the great flourish of research on OCO with delays and BCO, delayed BCO has rarely been investigated. GOLD (Heliou et al., 2020) is the first algorithm for this problem, which originally has the \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\) regret, and is further refined to enjoy the \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\) regret (Bistritz et al., 2022). However, Bistritz et al. (2022) also present an unmatched lower bound of \(()\). Although two recent advances in a more complicated bandit non-stochastic control problem (Gradu et al., 2020; Sun et al., 2023) provide some intermediate results about OGD and FTRL with the delayed bandit feedback, they focus on the case with a fixed delay and can only recover the \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\) regret in general. In this paper, we take one further step toward understanding the effect of arbitrary delays on BCO by establishing improved regret bounds such that the delay-independent part is equal to the regret of BGD, and the delay-dependent part matches the lower bound in the worst case. Moreover, we notice that although the block-box technique of Joulani et al. (2013) can also convert BGD into the delayed setting, it only achieves an \(O(d^{1/4}T^{3/4})\) regret bound for convex functions, which is much worse than that of GOLD and our algorithm.

## 3 Main results

In this section, we first introduce the necessary preliminaries including definitions, assumptions, and an algorithmic ingredient. Then, we present our improved algorithm for BCO with delayed feedback, as well as the corresponding theoretical guarantees.

### Preliminaries

We first recall two standard definitions about the smoothness and strong convexity of functions (Boyd and Vandenberghe, 2004).

**Definition 1**.: _A function \(f():^{n}\) is called \(\)-smooth over \(\) if for all \(,\), it holds that \(f() f()+ f(),- +\|-\|_{2}^{2}\)._

**Definition 2**.: _A function \(f():^{n}\) is called \(\)-strongly convex over \(\) if for all \(,\), it holds that \(f() f()+ f(),- +\|-\|_{2}^{2}\)._

Note that as proved by Hazan and Kale (2012), any \(\)-strongly convex function \(f():^{n}\) over the convex set \(\) ensures that

\[\|-^{*}\|_{2}^{2} f()-f( ^{*})\] (1)

for any \(\), where \(^{*}=*{argmin}_{}f()\).

Then, following previous studies on BCO (Flaxman et al., 2005; Heliou et al., 2020; Garber and Kretzu, 2020, 2021), we introduce some common assumptions.

**Assumption 1**.: _The convex set \(\) is full-dimensional and contains the origin, and there exist two constants \(r,R>0\) such that \(r^{n} R^{n}\), where \(^{n}\) denotes the unit Euclidean ball centered at the origin in \(^{n}\)._

**Assumption 2**.: _All loss functions are \(G\)-Lipschitz over \(\), i.e., for all \(,\) and \(t[T]\), it holds that \(|f_{t}()-f_{t}()| G\|-\|_{2}\)._

**Assumption 3**.: _The absolute value of all loss functions over \(\) are bounded by \(M\), i.e., for all \(\) and \(t[T]\), it holds that \(|f_{t}()| M\). Additionally, all loss functions are chosen beforehand, i.e., the adversary is oblivious._Finally, we introduce the one-point gradient estimator (Flaxman et al., 2005), which is a standard technique for exploiting the bandit feedback. Given a function \(f():^{n}\), we can define the \(\)-smoothed version of \(f()\) as

\[_{}()=_{^{n}}[f( +)]\] (2)

where the parameter \((0,1)\) is the so-called exploration radius. As proved by Flaxman et al. (2005), the \(\)-smoothed version satisfies the following lemma.

**Lemma 1**.: _(Lemma 1 in Flaxman et al. (2005)) Given a function \(f():^{n}\) and a constant \((0,1)\), its \(\)-smoothed version \(_{}()\) defined in (2) ensures_

\[_{}()=_{^{n}} [f(+)]\]

_where \(^{n}\) denotes the unit Euclidean sphere centered at the origin in \(^{n}\)._

From Lemma 1, the randomized vector \(f(+)\), which can be computed by only utilizing a single loss value, is an unbiased estimator of \(_{}()\). Moreover, Flaxman et al. (2005) have also shown that \(_{}()\) is close to the original function \(f()\) over a shrunk set

\[_{}=(1-/r)=\{(1-/r)| \}.\] (3)

Therefore, this one-point estimator can be utilized as a good substitute for the gradient \( f()\) in the bandit setting. For example, we notice that at each round \(t\), BGD (Flaxman et al., 2005) first plays an action \(_{t}=_{t}+_{t}\), where \(_{t}_{}\) and \(_{t}^{n}\), and then updates \(_{t}\) as

\[_{t+1}=_{_{}}(_{t}-n}{}f_{t}(_{t})_{t})\] (4)

where \(_{_{}}()=*{argmin}_{ _{}}\|-\|_{2}^{2}\) denotes the projection onto the set \(_{}\), and \(_{t}\) is the step size.

### Our improved algorithm

Before introducing our algorithm, we first briefly discuss the joint effect of the delays and the bandit feedback in GOLD (Heliou et al., 2020), which will provide insights for our improvements. Recall that in the delayed setting, the loss value \(f_{t}(_{t})\) will be delayed to the end of round \(t+d_{t}-1\), and thus the player can only receive \(\{f_{k}(_{k})|k_{t}\}\) at the end of round \(t\), where \(_{t}=\{k|k+d_{k}-1=t\}\). Since the set \(_{t}\) may not contain the round \(t\), the vanilla BGD in (4) is no longer valid. To address this issue, GOLD (Heliou et al., 2020) replaces \(f_{t}(_{t})\) in (4) with the oldest received but not utilized loss value at the end of round \(t\). Intuitively, the update of this approach is \(O(d)\) rounds slower than that of the vanilla BGD, which is analogous to those delayed OCO algorithms. However, due to the use of the one-point gradient estimator, the slower update causes a difference of \(O( dn/)\) between its action and that of BGD, and the cumulative difference will bring additional regret of \(O(T dn/)\), where a constant step size \(_{t}=\) is discussed for brevity. Note that from the standard analysis of BGD, to control the total exploration cost, the value of \(1/\) should be sublinear in \(T\). Therefore, it will amplify the effect of delays, and finally results in the \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\) regret (Heliou et al., 2020).

To reduce the effect of delays, we propose to incorporate the delayed bandit feedback with a blocking update mechanism (Zhang et al., 2019; Garber and Kretzu, 2020). Specifically, we divide the total \(T\) rounds into \(T/K\) blocks, each with \(K\) rounds, where \(T/K\) is assumed to be an integer without loss of generality. For each block \(m[T/K]\), we only maintain a preparatory action \(_{m}_{}\), and play \(_{t}=_{m}+_{t}\) with \(_{t}^{n}\) at each round \(t\) in the block. Due to the randomness of \(_{t}\) and the independence of \(_{t}\) in the same block, it is not hard to verify that for each block \(m[T/K]\), the sum of randomized gradients generated by the one-point estimator, i.e., \(_{m}=_{t=(m-1)K+1}^{mK}f_{t}(_{t})_{t}\), satisfies (see Lemma 5 presented in Section 3.4 for details)

\[[\|_{m}\|_{2}]=O(/^{2}}+K).\]

By using an appropriate block size of \(K=O(n^{2}/^{2})\), this upper bound will be \([\|_{m}\|_{2}]=O(K)\). By contrast, without the blocking update mechanism, one can only achieve \([\|_{m}\|_{2}]=O(Kn/)\). Moreover, we notice that the cumulative estimated gradients \(_{m}\) will be delayed at most \(O(d/K)\)blocks, because even the last component \(f_{mK}(_{mK})_{mK}\) is available at the end of round \(mK+d-1\).

As a result, one possible approach to determine \(_{m}\) for each block is to extend the update rule of GOLD (Heliou et al., 2020) into the block level with \(K=O(n^{2}/^{2})\). Combining with previous discussions, it will reduce the effect of delays on the regret from \(O(T dn/)\) to

\[O(T(}{^{2}}}+K)) = O( dT)\]

which is good enough for deriving our desired regret bounds. However, it requires a bit complicated procedure to maintain the cumulative estimated gradients for any block that has not been utilized to update the action. For this reason, instead of utilizing this approach, we incorporate FTRL (Hazan et al., 2007; Hazan, 2016) with the delayed bandit feedback and blocking update mechanism, which provides a more elegant way to utilize the delayed information.

Specifically, we initialize \(_{1}_{}\) arbitrarily, and use a variable \(}_{t}\) to record the sum of gradients estimated from all received loss values, i.e., \(}_{t}=_{i=1}^{t}_{k_{t}} f_{k}(_{k})_{k}\). Then, according to FTRL, an ideal action should be selected by minimizing the linear approximation of cumulative loss functions under some regularization, i.e.,

\[_{m+1}^{*}=*{argmin}_{_{ }}\{_{i=1}^{m}_{i},+_{m}()\}\] (5)

where the regularization is set as \(_{m}()=\|-_{1}\|_{2}^{2}\) for convex functions (Hazan, 2016) and \(_{m}()=_{i=1}^{m}\|- _{i}\|_{2}^{2}\) for \(\)-strongly convex functions (Hazan et al., 2007). Unfortunately, due to the effect of delays, the value of \(_{i=1}^{m}_{i}\) required by (5) may not be available. To address this limitation, we generate \(_{m+1}\) by replacing this term with the sum of all available estimated gradients, i.e., \(}_{mK}\).2

The detailed procedures are outlined in Algorithm 1, where the input \(\) is the modules of the strong convexity of functions, and it is called delayed follow-the-bandit-leader (D-FTBL).

```
1:Input:\(,K,\), and \(>0\) if \(=0\)
2:Initialization: set \(}_{0}=\) and choose \(_{1}_{}\) arbitrarily
3:for\(m=1,2,,T/K\)do
4:for\(t=(m-1)K+1,,mK\)do
5: Play \(_{t}=_{m}+_{t}\), where \(_{t}^{n}\)
6: Query \(f_{t}(_{t})\), and receive \(\{f_{k}(_{k})|k_{t}\}\)
7: Update \(}_{t}=}_{t-1}+_{k_{t}} {}f_{k}(_{k})_{k}\)
8:endfor
9: Set \(_{m}()=\{\| -_{1}\|_{2}^{2}&=0\\ _{i=1}^{m}\|-_{i}\|_{2}^{2}&.\)
10:\(_{m+1}=*{argmin}_{_{}} \{(}_{mK},)+_{m}()\}\)
11:endfor ```

**Algorithm 1** Delayed Follow-The-Bandit-Leader

### Theoretical guarantees

We first present the regret bound of our D-FTBL for convex functions.

**Theorem 1**.: _Under Assumptions 1, 2, and 3, Algorithm 1 with \(=0\) ensures_

\[[(T)]}{ }+}_{:=A}+}{K^{2}}+4)}}_{:=B}+}_{:=C}\] (6)

_where \(=K()^{2}+K^{2}G^{2}\)._

**Remark.** To help understanding the regret bound in (6), we notice that the term \(A\) actually is derived from the expected regret of the ideal action \(_{m}^{*}\) on a sequence of surrogate losses, and the term \(B\) is caused by the cumulative distance between our preparatory action \(_{m}\) and the ideal one. Additionally, the term \(C\) is caused by the exploration error of the one-point gradient estimator. At first glance, it seems that term \(B\) suffers a multiplicative joint effect of the maximum delay \(d\) and the exploration radius \(\) due to the existence of \(\). However, as discussed before, this joint effect can be decoupled by setting an appropriate block size of \(K=O(n^{2}/^{2})\), which allows us to derive an improved regret bound. Specifically, by substituting \(=0\), \(K=n\), \(=1/\{,T^{3/4}\}\), and \(=cT^{-1/4}\) into (6), where \(c\) is a constant such that \(<r\), our D-FTBL can enjoy

\[[(T)] O(T^{3/4}+)\] (7)

for convex functions.3 It is tighter than the \(O(T^{3/4}+(nd)^{1/3}T^{2/3})\) regret of GOLD (Heliou et al., 2020), and matches the \(O(T^{3/4})\) regret bound of BGD in the non-delayed setting as long as \(d\) is not larger than \(O(n)\). Even for \(d=(n)\), our regret bound is dominated by the \(O()\) part, which matches the \(()\) lower bound (Bistritz et al., 2022) in the worst case. Moreover, although the \(O(T^{3/4}+(n)^{1/3}T^{2/3})\) regret bound of Bistritz et al. (2022) could benefit from a small average delay, it is also worse than our regret bound when \(d\) is not larger than \(O((n)^{2/3}T^{1/3})\).

Then, we establish an improved regret bound for \(\)-strongly convex functions.

**Theorem 2**.: _Under Assumptions 1, 2, and 3, if all functions are \(\)-strongly convex, Algorithm 1 with \(>0\) ensures_

\[[(T)]}{  K}+C_{T}^{}R}_{:=A^{}}+}{}}{K^{2}}+4)}}_{:=B^{}}+3  GT+\] (8)

_where \(=K()^{2}+K^{2}G^{2}\), \(C_{T}=1+ T\), and \(C_{T}^{}=6+4 T\)._

**Remark.** By comparing Theorem 2 with Theorem 1, we find that the strongly convexity can be exploited to reduce the expected regret of the ideal action \(_{m}^{*}\) on the surrogate losses, and the cumulative distance between our preparatory action \(_{m}\) and the ideal one, i.e., improving terms \(A\) and \(B\) in (6) to terms \(A^{}\) and \(B^{}\) in (8). By further substituting \(>0\), \(K=(nT)^{2/3}^{-2/3}T\), and \(=cn^{2/3}T^{-1/3}^{1/3}T\) into (8), where \(c\) is a constant such that \(<r\), our D-FTBL can enjoy

\[[(T)] O((nT)^{2/3}^{1/3}T+d T)\] (9)

for strongly convex functions. This regret bound is tighter than the above \(O(T^{3/4}+)\) regret bound achieved by only utilizing the convexity condition, and can match the \(O((nT)^{2/3}^{1/3}T)\) regret bound of BGD in the non-delayed setting as long as \(d\) is not larger than \(O((nT/ T)^{2/3})\). Even if \(d=((nT/ T)^{2/3})\), it is dominated by the \(O(d T)\) part, which matches the \((d T)\) lower bound (Weinberger and Ordentlich, 2002), and thus cannot be improved. Moreover, different from the case with convex functions, the parameters for achieving the bound in (9) do not require the information of delays.

Furthermore, we consider the unconstrained case, i.e., \(=^{n}\), with \(\)-strongly convex and \(\)-smooth functions, and extend our D-FTBL to achieve a better regret bound. Specifically, without the boundedness of \(\), Assumptions 2 and 3 may no longer hold over the entire space (Agarwal et al., 2010). Therefore, we first introduce a weaker assumption on the Lipschitz continuity, i.e, all loss functions are \(G\)-Lipschitz at \(\). Combining with (1), it is not hard to verify that the fixed optimal action \(^{*}=*{argmin}_{^{n}}_{t=1} ^{T}f_{t}()\) satisfies

\[\|^{*}\|_{2}.\] (10)As a result, the player only needs to select actions from the following set

\[^{}=\{^{n}\|\|_ {2}\}\] (11)

which satisfies Assumption 1 with \(r=R=2G/\), and it is natural to further assume that all loss functions satisfy Assumptions 2 and 3 over the set \(^{}\). Now, we can apply our D-FTBL over the shrink set of \(^{}\), i.e.,

\[^{}_{}=(1-/r)^{}=(1- {}{2G})^{}\] (12)

instead of the original \(_{}\), and establish the following regret bound.

**Theorem 3**.: _Let \(=^{n}\). If all loss functions are \(\)-strongly convex and \(\)-smooth over \(\), and Assumptions 2 and 3 hold over \(^{}\) defined in (11), applying Algorithm 1 with \(>0\) over \(^{}_{}\) defined in (12) ensures_

\[[(T)]}{ K}+ _{T}G}{}+}{}}{K^{2}}+4)}+T+ {^{2}GT}{}}_{:=C^{}}\] (13)

_where \(=K()^{2}+K^{2}G^{2}\), \(C_{T}=1+ T\), and \(C^{}_{T}=6+4 T\)._

**Remark.** By comparing Theorem 3 with Theorem 2, we find that the exploration error of the one-point gradient estimator is reduced, i.e., improving the last two terms in (8) to the term \(C^{}\) in (13). Then, by substituting \(>0\), \(K=n\), and \(=cn^{1/2}T^{-1/4}^{1/4}T\) into (13), where \(c\) is a constant such that \(<2G/\), we can achieve an \(O(n+d T)\) regret bound for strongly convex and smooth functions in the unconstrained case. It is better than the \(O((nT)^{2/3}^{1/3}T+d T)\) regret bound achieved by only utilizing the strong convexity. Moreover, this bound matches the \(O(n)\) regret bound achieved by using BGD in the non-delayed setting as long as \(d\) is not larger than \(O(n)\). Otherwise, it is dominated by the \(O(d T)\) part, which cannot be improved as discussed before.

### Analysis: proof of Theorem 1

Due to the limitation of space, here we only prove Theorem 1, and the omitted proofs can be found in the appendix. Specifically, let \(}^{}=(1-/r)^{}\) where \(^{}*{argmin}_{}_{ t=1}^{T}f_{t}()\), and recall the ideal action defined in (5). As in Lemma 2, we first notice that the expected regret of Algorithm 1 can be bounded by the sum of three parts including the expected regret of ideal actions on some surrogate losses, the cumulative distance between \(_{m}\) and the ideal one, and the exploration error of the one-point gradient estimator.

**Lemma 2**.: _Under Assumptions 1 and 2, Algorithm 1 with \(=0\) ensures_

\[[(T)][_{m=1}^{T/K} _{m},_{m}^{}-}^{} +KG_{m=1}^{T/K}\|_{m}-_{m}^{}\|_{2} ]+3 GT+.\] (14)

Note that the part regarding the exploration error in (14) is exactly the same as the term \(C\) in (6). So, we only need to analyze the first two parts in (14). For the first part, we define surrogate losses as \(_{1}()=_{1},+\|-_{1}\|_{2}\) and \(_{m}()=_{m},\) for any \(m=2,,T/K\). Combining with (5) for convex functions, it is easy to verify that \(_{m+1}^{}=*{argmin}_{_{ }}_{i=1}^{m}_{i}()\). Then, we introduce the following lemma to bound the regret of \(_{2}^{},,_{T/K+1}^{}\) on \(_{1}(),,_{T/K}()\).

**Lemma 3**.: _(Lemma 6.6 in Garber and Hazan (2016)) Let \(\{_{t}()\}_{t=1}^{T}\) be a sequence of functions over a set \(\), and let \(_{t}^{}*{argmin}_{}_ {i=1}^{t}_{i}()\) for any \(t[T]\). Then, it holds that \(_{t=1}^{T}_{t}(_{t}^{})-_{} _{t=1}^{T}_{t}() 0\)._

Specifically, by applying Lemma 3, we have \(_{m=1}^{T/K}_{m}(_{m+1}^{})-_{m=1}^{T/K}_{m}( }^{}) 0\). Combining this inequality with Assumption 1, we have

\[_{m=1}^{T/K}_{m},_{m+1}^{}- }^{}}^{}-_{1}\|_ {2}^{2}}{}-_{2}^{}-_{1}\|_{2}^{2}}{} }{}.\] (15)Moreover, to replace \(_{m+1}^{*}\) in the left side of (15) with \(_{m}^{*}\), we introduce the following lemma.

**Lemma 4**.: _(Lemma 5 in Duchi et al. (2011)) Let \(_{}(,)=*{argmin}_{ }\{,+\| \|_{2}^{2}\}\). We have \(\|_{}(,)-_{}(,)\|_{2} \|-\|_{2}\)._

Combining Lemma 4 with (5) for convex functions, we have

\[\|_{m}^{*}-_{m+1}^{*}\|_{2}\|( _{i=1}^{m-1}_{i}-_{1}}{})-(_{i= 1}^{m}_{i}-_{1}}{})\|_{2}= {2}\|_{m}\|_{2}.\] (16)

Then, combining (15) with (16), we have

\[_{m=1}^{T/K}_{m},_{m}^{*}- {}^{*}=_{m=1}^{T/K}_{m},_{m+1}^{*}-}^{*}+_{m}^{*}-_{m+1}^{*}\] (17) \[ }{}+_{m=1}^{T/K}\|_{m}\|_{2}\|_{m}^{*}-_{m+1}^{*}\|_{2}}{}+_{m=1} ^{T/K}\|_{m}\|_{2}^{2}.\]

We notice that the term \(\|_{m}\|_{2}^{2}\) in (17) can directly benefit from the blocking update mechanism, as shown by the upper bound in the following lemma.

**Lemma 5**.: _Under Assumptions 2 and 3, for any \(m[T/K]\), Algorithm 1 ensures \([\|_{m}\|_{2}^{2}] K()^{2}+K ^{2}G^{2}\)._

However, to completely bound the right side of (14), we still need to analyze \(\|_{m}-_{m}^{*}\|_{2}\), which is more complicated due to the effect of delays. Specifically, let

\[_{m}=\{1,,(m-1)K\}_{t=1}^{(m-1)K}_{t}\] (18)

be the set consisting of the time stamp of loss values that are queried but still not arrive at the end of round \((m-1)K\). By using Lemma 4 again, we have

\[\|_{m}-_{m}^{*}\|_{2}\|( }_{(m-1)K}-_{1}}{})-(_{i= 1}^{m-1}_{i}-_{1}}{})\|_{2}=\|_{t_{m}}f_{t}(_{t}) _{t}\|_{2}.\] (19)

Moreover, we establish the following lemma regarding the right side of (19).

**Lemma 6**.: _Under Assumptions 2 and 3, for any \(m[T/K]\), Algorithm 1 ensures_

\[[\|_{t_{m}}f_{t}( _{t})_{t}\|_{2}^{2}] 2(}{K^{2}}+4 )(K()^{2}+K^{2}G^{2}).\]

Combining (14), (17), (19), Lemma 5, Lemma 6, and \(=K()^{2}+K^{2}G^{2}\), we have

\[[(T)] }{}+[_{m=1}^{T/K} \|_{m}\|_{2}^{2}]+KG_{m=1}^{T/K}[\|_ {m}-_{m}^{*}\|_{2}]+3 GT+\] \[ }{}++ }{K^{2}}+4)}+3 GT+.\]

## 4 Experiments

In this section, we compare our D-FTBL against GOLD (Heliou et al., 2020) and improved GOLD (Bistritz et al., 2022) by conducting simulation experiments on two publicly available data sets--ijcnn1 and SUSY from the LIBSVM repository (Chang and Lin, 2011). All algorithms are implemented with Python, and tested on a laptop with 2.4GHz CPU and 16GB memory.

Specifically, we randomly select \(T=40000\) examples from the original data sets, and consider online binary classification over a convex set \(=\{^{n}\|\|\|_{2} 50\}\). The dimensionality of ijcnn1 and SUSY are \(n=22\) and \(n=18\), respectively. In each round \(t[T]\), the adversary chooses the hinge loss

\[f_{t}()=\{1-y_{t}_{t}^{},0\}\]

where \(_{t}\) and \(y_{t}\{-1,1\}\) are the feature vector and class label of the \(t\)-th example, respectively. Different values of the maximum delay \(d\) in the set \(\{200,600,1000,,5000\}\) have been tried in our experiments. For each specific \(d\), to simulate arbitrary delays, \(d_{t}\) is independently and uniformly sampled from \([d]\). In this way, the average delay \(\) is equal to \((d+1)/2\) in expectation, and thus is close to the maximum delay.

According to the previous discussions about Theorem 1, we set \(=0\), \(K= n\), \(=cT^{-1/4}\), and \(=c^{}/\{,T^{3/4}\}\) for our D-FTBL by tuning these two constants \(c\) and \(c^{}\). For those two baselines, we only need to set parameters \(\) and \(\). In addition to the theoretically suggested value of \(\) and \(\), we also introduce \(c\) and \(c^{}\) as the scale factor, respectively. For all algorithms, \(c\) and \(c^{}\) are respectively selected from \(\{0.1,1.0,10\}\) and \(\{0.01,0.1,,100\}\) simply according to their performance for \(d=200\). Moreover, due to the randomness of these algorithms, we repeat them 20 times and report the average of their total loss.

Fig. 1 shows the results of all algorithms on both data sets. We first find that when \(d\) increases from \(200\) to \(5000\), the total loss of our D-FTBL grows slowly, which is consistent with the dependence of our regret bound on \(d\). It is worth noting that \(d=5000\) is larger than \(n\) in our experiments. Second, from \(d=600\) to \(d=5000\), the total loss of our D-FTBL is better than both GOLD and improved GOLD, which verifies the advantage of our algorithm in the delayed setting. By contrast, due to \( d\), the performance of improved GOLD is very close to that of GOLD. Finally, we also notice that D-FTBL is slightly worse than baselines for \(d=200\). However, it is reasonable because the block update mechanism enlarges each delay to be at least the block size, which could result in a slightly larger constant factor in the regret.

## 5 Conclusion and future work

In this paper, we investigate BCO with delayed feedback, and propose a novel algorithm called D-FTBL by exploiting the blocking update mechanism. Our analysis first reveals that it can achieve a regret bound of \(O(T^{3/4}+)\) in general, which improves the delay-dependent part of the existing \(O(T^{3/4}+(n)^{1/3}T^{2/3})\) regret bound as long as \(d\) is not larger than \(O((n)^{2/3}T^{1/3})\). Furthermore, we consider the special case with strongly convex functions, and prove that the regret of D-FTBL can be reduced to \(O((nT)^{2/3}^{1/3}T+d T)\). Finally, if the action sets are unconstrained, we show that D-FTBL can be simply extended to enjoy the \(O(n+d T)\) regret for strongly convex and smooth functions. Nonetheless, there still exist several open problems, which are discussed in the appendix due to the limitation of space.

Figure 1: Experimental results on delayed online binary classification for ijcnn1 and SUSY.