# Causal Context Connects Counterfactual Fairness

to Robust Prediction and Group Fairness

Jacy Reese Anthis\({}^{1,2,3}\)1, Victor Veitch\({}^{1}\)

\({}^{1}\)University of Chicago, \({}^{2}\)University of California, Berkeley, \({}^{3}\)Sentience Institute

###### Abstract

Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use _causal context_ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts--measurement error, selection on label, and selection on predictors--counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.

## 1 Introduction

The increasing use of artificial intelligence and machine learning in high stakes contexts such as healthcare, hiring, and financial lending has driven widespread interest in algorithmic fairness. A canonical example is risk assessment in the U.S. judicial system, which became well-known after a 2016 investigation into the recidivism prediction tool COMPAS revealed significant racial disparities . There are several metrics one can use to operationalize fairness. These typically refer to a protected class or sensitive label, such as race or gender, and define an equality of prediction rates across the protected class. For example, demographic parity, also known as statistical parity or group parity, requires equal classification rates across all levels of the protected class .

However, there is an open challenge of deciding which metrics to enforce in a given context [e.g., 24, 44]. Appropriateness can vary based on different ways to measure false positive and false negative rates and the costs of such errors  as well as the potential trade-offs between fairness and accuracy . Moreover, there are well-known technical results showing that it is impossible to achieve the different fairness metrics simultaneously, even to an \(\)-approximation , which suggest that a practitioner's context-specific perspective may be necessary to achieve satisfactory outcomes .

A different paradigm, counterfactual fairness, requires that a prediction would have been the same if the person had a different protected class . This matches common intuitions and legal standards. For example, in _McCleskey v. Kemp_ (1987), the Supreme Court found "disproportionate impact" as shown by group disparity to be insufficient evidence of discrimination and required evidence that "racial considerations played a part." The Court found that there was no merit to the argument that the Georgia legislature had violated the Equal Protection Clause via its use of capital punishment on the grounds that it had not been shown that this was [sic] "_because_ of an anticipated racially discriminatory effect." While judges do not formally specify causal models, their language usually evokes such counterfactuals; see, for example, the test of "but-for causation" in _Bostock v. Clayton County_ (2020).

We take a new perspective on counterfactual fairness by leveraging _causal context_. First, we provide a novel motivation for counterfactual fairness from the perspective of robust prediction by showing that, with certain causal structures of the data-generating process, counterfactually fair predictors are accuracy-optimal in an unbiased target distribution (Theorem 1). The contexts we consider have two important features: the faithfulness of the causal structure, in the sense that no causal effects happen to be precisely counterbalanced, which could lead to a coincidental achievement of group fairness metrics, and that the association between the label \(Y\) and the protected class \(A\) is "purely spurious," in the sense that intervening on \(A\) does not affect \(Y\) or vice versa.

Second, we address the fundamental challenge in enforcing counterfactual fairness, which is that, by definition, we never directly observe counterfactual information in the real world. To deal with this, we show that the causal context connects counterfactual fairness to observational group fairness metrics (Theorem 2 and Corollary 2.1), and this correspondence can be used to apply tools from group fairness frameworks to the ideal of achieving counterfactual fairness (Figure 1). For example, the fairness literature has developed techniques to achieve group fairness through augmenting the input data , data generation , or regularization . With this pipeline, these tools can be applied to enforce counterfactual fairness by achieving the specific group fairness metric that corresponds to counterfactual fairness in the given context. In particular, we show that in each fairness context shown in Figure 2, counterfactual fairness is equivalent to a particular metric (Corollary 2.2). This correspondence can be used to apply tools built for group fairness to the goal of counterfactual fairness or vice versa.

Finally, we conduct brief experiments in a semi-synthetic setting with the Adult income dataset  to confirm that a counterfactually fair predictor under these conditions achieves out-of-distribution accuracy and the corresponding group fairness metric. To do this, we develop a novel counterfactually fair predictor that is a weighted average of naive predictors, each under the assumption that the observation is in each protected class (Theorem 3).

Figure 1: A pipeline to detect and enforce counterfactual fairness. This is facilitated by Theorem 2, a correspondence between group fairness metrics and counterfactual fairness, such that tools to detect and enforce group fairness can be applied to counterfactual fairness.

In summary, we make three main contributions:

1. We provide a novel motivation for counterfactual fairness from the perspective of robust prediction by showing that, in certain causal contexts, the counterfactually fair predictor is accuracy-optimal in an unbiased target distribution (Theorem 1).
2. We provide criteria for whether a causal context implies equivalence between counterfactual fairness and each of the three primary group fairness metrics (Theorem 2) as well as seven derivative metrics (Corollary 2.1).
3. We provide three causal contexts, as shown in Figure 2, in which counterfactual fairness is equivalent to a particular group fairness metric: measurement error with demographic parity, selection on label with equalized odds, and selection on predictors with calibration (Corollary 2.2).

For a running example, consider the case of recidivism prediction with measurement error, as shown in Figure 1(a), where \(X\) is the set of predictors of recidivism, such as past convictions; \(\) is committing a crime; \(Y\) is committing a crime that has been reported and prosecuted; and \(A\) is race, which in this example affects the likelihood crime is reported and prosecuted but not the likelihood of crime itself. In this case, we show that a predictor is counterfactually fair if and only if it achieves demographic parity.

## 2 Related work

Previous work has developed a number of fairness metrics, particularly demographic parity , equalized odds , and calibration . Verma and Rubin  details 15 group fairness metrics--also known as observational or distributional fairness metrics--including these three, and Makhlouf, Zhioua, and Palamidessi  details 19 causal fairness metrics, including counterfactual fairness . Many notions of fairness can be viewed as specific instances of the general goal of invariant representation . Much contemporary work in algorithmic fairness develops ways to better enforce certain fairness metrics, such as through reprogramming a pretrained model ,

Figure 2: DAGs for three causal contexts in which a counterfactually fair predictor is equivalent to a particular group fairness metric. Measurement error is represented with the unobserved true label \(\) in a dashed circle, which is a parent of the observed label \(Y\), and selection is represented with the selection status \(S\) in a rectangle, indicating an observed variable that is conditioned on in the data-generating process, which induces an association between its parents. Bidirectional arrows indicate that either variable could affect the other.

contrastive learning when most data lacks labels of the protected class , and differentiating between critical and non-critical features for selective removal .

Several papers have criticized the group fairness metrics and enriched them correspondingly. For example, one can achieve demographic parity by arbitrarily classifying a subset of individuals in a protected class, regardless of the other characteristics of those individuals. This is particularly important for subgroup fairness, in which a predictor can achieve the fairness metric for one category within the protected class but fail to achieve it when subgroups are considered across different protected classes, such as people who have a certain race and gender. This is particularly important given the intersectional nature of social disparities , and metrics have been modified to include possible subgroups .

Well-known impossibility theorems show that fairness metrics such as the three listed above cannot be simultaneously enforced outside of unusual situations such as a perfectly accurate classifier, a random classifier, or a population with equal rates across the protected class, and this is true even to an \(\)-approximation [10; 30; 40]. Moreover, insofar as fairness and classification accuracy diverge, there appears to be a trade-off between the two objectives. Two well-known papers in the literature, Corbett-Davies et al.  and Corbett-Davies and Goel , detail the cost of fairness and argue for the prioritization of classification accuracy. More recent work on this trade-off includes a geometric characterization and Pareto optimal frontier for invariance goals such as fairness , the cost of causal fairness in terms of Pareto dominance in classification accuracy and diversity , the potential for increases in fairness via data reweighting in some cases without a cost to classification accuracy , and showing optimal fairness and accuracy in an ideal distribution given mismatched distributional data .

Our project uses the framework of causality to enrich fairness metric selection. Recent work has used structural causal models to formalize and solve a wide range of causal problems in machine learning, such as disentangled representations [41; 50], out-of-distribution generalization [41; 49; 50], and the identification and removal of spurious correlations [47; 50]. While Veitch et al.  focuses on the correspondence between counterfactual invariance and domain generalization, Remark 3.3 notes that counterfactual fairness as an instance of counterfactual invariance can imply either demographic parity or equalized odds in the two graphs they analyze, respectively, and Makar and D'Amour  draw the correspondence between risk invariance and equalized odds under a certain graph. The present work can be viewed as a generalization of these results to correspondence between counterfactual fairness, which is equivalent to risk invariance when the association is "purely spurious," and each of the group fairness metrics.

## 3 Preliminaries and Examples

For simplicity of exposition, we focus on binary classification with an input dataset \(X\) and a binary label \(Y=\{0,1\}\), and in our data we may have ground-truth unobserved labels \(\{0,1\}\) that do not always match the potentially noisy observed labels \(Y\). The goal is to estimate a prediction function \(f:\), producing estimated labels \(f(X)\{0,1\}\), that minimizes \([(f(X),Y)]\) for some loss function \(:\). There are a number of popular fairness metrics in the literature that can be formulated as conditional probabilities for some protected class \(A\{0,1\}\), or equivalently, as independence of random variables, and we can define a notion of counterfactual fairness.

**Definition 1**.: (Demographic parity). A predictor \(f(X)\) achieves demographic parity if and only if:

\[(f(X)=1 A=0)=(f(X)=1 A=1) f(X) A\]

**Definition 2**.: (Equalized odds). A predictor \(f(X)\) achieves equalized odds if and only if:

\[(f(X)=1 A=0,Y=y)=(f(X)=1 A=1,Y=y) f(X) A  Y\]

**Definition 3**.: (Calibration). A predictor \(f(X)\) achieves calibration if and only if:

\[(Y=1 A=0,f(X)=1)=(Y=1 A=1,f(X)=1) Y A f (X)\]

**Definition 4**.: (Counterfactual fairness). A predictor \(f(X)\) achieves counterfactual fairness if and only if for any \(a,a^{} A\):

\[f(X(a))=f(X(a^{}))\]In this case, \(x(0)\) and \(x(1)\) are the potential outcomes. That is, for an individual associated with data \(x X\) with protected class \(a A\), their counterfactual is \(x(a^{})\) where \(a^{} A\) and \(a a^{}\). The counterfactual observation is what would obtain if the individual had a different protected class. We represent causal structures as directed acyclic graphs (DAGs) in which nodes are variables and directed arrows indicates the direction of causal effect. Nodes in a solid circle are observed variables that may or may not be included in a predictive model. A dashed circle indicates an unobserved variable, such as in the case of measurement error in which the true label \(\) is a parent of the observed label \(Y\). A rectangle indicates an observed variable that is conditioned on in the data-generating process, typically denoted by \(S\) for a selection effect, which induces an association between its parents. For clarity, we decompose \(X\) into \(X^{}_{A}\), the component that is not causally affected by \(A\), and \(X^{}_{Y}\), the component that does not causally affect \(Y\) (in a causal-direction graph, i.e., \(X\) affects \(Y\)) or is not causally affected by \(Y\) (in an anti-causal graph, i.e., \(Y\) affects \(X\)). In general, there could be a third component that is causally connected to both \(A\) and \(Y\) (i.e., a non-spurious interaction effect). This third component is excluded through the assumption that the association between \(Y\) and \(A\) in the training distribution is _purely spurious_.

**Definition 5**.: (Purely spurious). We say the association between \(Y\) and \(A\) is purely spurious if \(Y X X^{}_{A},A\).

In other words, if we condition on \(A\), then the only information about \(Y\) is in \(X^{}_{A}\) (i.e., the component of the input that is not causally affected by the protected class). We also restrict ourselves to cases in which \(A\) is exogenous to the causal structure (i.e., there is no confounder of \(A\) and \(X\) or of \(A\) and \(Y\)), which seems reasonable in the case of fairness because the protected class is typically not caused by other variables in the specified model.

Despite the intuitive appeal and legal precedent for counterfactual fairness, the determination of counterfactuals is fundamentally contentious because, by definition, we do not observe counterfactual worlds, and we cannot run scientific experiments to test what would have happened in a different world history. Some counterfactuals are relatively clear, but counterfactuals in contexts of protected classes such as race, gender, and disability can be ambiguous. Consider Bob, a hypothetical Black criminal defendant being assessed for recidivism who is determined to be high-risk by a human judge or a machine algorithm. The assessment involves extensive information about his life: where he has lived, how he has been employed, what crimes he has been arrested and convicted of before, and so on. What is the counterfactual in which Bob is White? Is it the world in which Bob was born to White parents, the one in which Bob's parents were themselves born to White parents, or another change further back? Would Bob still have the same educational and economic circumstances? Would those White parents have raised Bob in the same majority-Black neighborhood he grew up in or in a majority-White neighborhood? Questions of counterfactual ambiguity do not have established answers in the fairness literature, either in philosophy , epidemiology , or the nascent machine learning literature , and we do not attempt to resolve them in the present work.

Additionally, defining counterfactual fairness in a given context requires some identification of the causal structure of the data generating process. Fortunately, mitigating this challenge is more technically tractable than mitigating ambiguity, as there is an extensive literature on how we can learn about the causal structure, known as as causal inference  or causal discovery when we have minimal prior knowledge or assumptions of which parent-child relationships do and do not obtain . The literature also contains a number of methods for assessing counterfactual fairness given a partial or complete causal graph . Again, we do not attempt to resolve debates about appropriate causal inference strategies in the present work, but merely to provide a conceptual tool for those researchers and practitioners who are comfortable staking a claim of some partial or complete causal structure of the context at hand.

To ground our technical results, we briefly sketch three fairness contexts that match those in Figure 2 and will be the basis of Corollary 2.2. First, measurement error  is a well-known source of fairness issues as developed in the "measurement error models" of Jacobs and Wallach . Suppose that COMPAS is assessing the risk of recidivism, but they do not have perfect knowledge of who has committed crimes because not all crimes are reported and prosecuted. Thus, \(X\) causes \(\), the actual committing of a crime, which is one cause of \(Y\), the imperfect labels. Also suppose that the protected class \(A\) affects whether the crime is reported and prosecuted, such as through police bias, but does not affect the actual committing of a crime. \(A\) also affects \(X\), other data used for the recidivism prediction. This is represented by the causal DAG in Figure 1(a), which connects counterfactual fairness to demographic parity.

Second, a protected class can affect whether individuals are selected into the dataset. For example, drugs may be prescribed on the basis of whether the individual's lab work \(X\) indicates the presence of disease \(Y\). People with the disease are presumably more likely to have lab work done, and the socioeconomic status \(A\) of the person (or of the hospital where their lab work is done) may make them more likely to have lab work done. We represent this with a selection variable \(S\) that we condition on by only using the lab work data that is available. This collider induces an association between \(A\) and \(Y\). This is represented by the causal DAG in Figure 1(b), which connects counterfactual fairness to equalized odds.

Third, individuals may be selected into the dataset based on predictors, \(X\), which cause the label \(Y\). For example, loans may be given out with a prediction of loan repayment \(Y\) based on demographic and financial records \(X\). There may be data only for people who choose to work with a certain bank based on financial records and a protected class \(A\). This is represented by the causal DAG in Figure 1(c), which connects counterfactual fairness to calibration.

## 4 Counterfactual fairness and robust prediction

Characterizations of the trade-off between prediction accuracy and fairness treat them as two competing goals [e.g., 11, 12, 21, 55]. That view assumes the task is to find an optimal model \(f^{*}(X)\) that minimizes risk (i.e., expected loss) in the training distribution \(X,Y,A P\):

\[f^{*}(X)=*{argmin}_{f}\,_{P}[(f(X),Y)]\]

However, the discrepancy between levels of the protected class in the training data may itself be due to biases in the dataset, such as measurement error, selection on label, and selection on predictors. As such, the practical interest may not be risk minimization in the training distribution, but out-of-distribution (OOD) generalization from the training distribution to an unbiased target distribution where these effects are not present.

Whether the counterfactually fair empirical risk minimizer also minimizes risk in the target distribution depends on how the distributions differ. Because a counterfactually fair predictor does not depend on the protected class, it minimizes risk if the protected class in the target distribution contains no information about the corresponding labels (i.e., if protected class and label are uncorrelated). If the protected class and label are correlated in the target distribution, then risk depends on whether the counterfactually fair predictor learns all the information about the label contained in the protected class. If so, then its prediction has no reason to vary in the counterfactual scenario. Theorem 1 motivates counterfactual fairness by stating that, for a distribution with bias due to selection on label and equal marginal label distributions or due to selection on predictors, the counterfactually fair predictor is accuracy-optimal in the unbiased target distribution.

**Theorem 1**.: _Let \(^{}\) be the set of all counterfactually fair predictors. Let \(\) be a proper scoring rule (e.g., square error, cross entropy loss). Let the counterfactually fair predictor that minimizes risk on the training distribution \(X,Y,A P\) be:_

\[f^{*}(X):=*{argmin}_{f^{}}_{P}[ (f(X),Y)]\]

_Then, \(f^{*}\) also minimizes risk on the target distribution \(X,Y,A Q\) with no selection effects, i.e.,_

\[f^{*}(X)=*{argmin}_{f}\,_{Q}[(f(X),Y)]\]

_if either of the following conditions hold:_

1. _The association between_ \(Y\) _and_ \(A\) _is due to selection on label and the marginal distribution of the label_ \(Y\) _is the same in each distribution, i.e.,_ \(P(Y)=Q(Y)\)_._
2. _The association between_ \(Y\) _and_ \(A\) _is due to selection on predictors._

In the context of measurement error, there are not straightforward conditions for the risk minimization of the counterfactually fair predictor because the training dataset contains \(Y\), observed noisy labels,and not \(\), the true unobserved labels. Thus any risk minimization (including in the training distribution) depends on the causal process that generates \(Y\) from \(\).

## 5 Counterfactual fairness and group fairness

Causal structures imply conditional independencies. For example, if the only causal relationships are that \(X\) causes \(Y\) and \(Y\) causes \(Z\), then we know that \(X Z Y\). On a directed acyclic graph (DAG), following Pearl and Dechter , we say that a variable \(Y\)**dependence-separates** or **d-separates**\(X\) and \(Z\) if \(X\) and \(Z\) are connected via an unblocked path (i.e., no unconditioned **collider** in which two arrows point directly to the same variable) but are no longer connected after removing all arrows that directly connect to \(Y\). So in this example, \(Y\) d-separates \(X\) and \(Z\), which is equivalent to the conditional independence statement. A selection variable \(S\) in a rectangle indicates an observed variable that is conditioned on in the data-generating process, which induces an association between its parents and thereby does not block the path as an unconditioned collider would. For Theorem 2, Corollary 2.1, and Corollary 2.2, we make the usual assumption of faithfulness of the causal graph, meaning that the only conditional independencies are those implied by d-separation, rather than any causal effects that happen to be precisely counterbalanced. Specifically, we assume faithfulness between the protected class \(A\), the label \(Y\), and the component of \(X\) on which the predictor \(f(X)\) is based. If the component is not already its own node in the causal graph, then faithfulness would apply if the component were isolated into its own node or nodes.

For a predictor, these implied conditional independencies can be group fairness metrics. We can restate conditional independencies containing \(X_{A}^{}\) as containing \(f(X)\) because, if \(f(X)\) is a counterfactually fair predictor, it only depends on \(X_{A}^{}\), the component that is not causally affected by \(A\). In Theorem 2, we provide the correspondence between counterfactual fairness and the three most common metrics: demographic parity, equalized odds, and calibration.

**Theorem 2**.: _Let the causal structure be a causal DAG with \(X_{Y}^{}\), \(X_{A}^{}\), \(Y\), and \(A\), such as in Figure 2. Assume faithfulness between \(A\), \(Y\), and \(f(X)\). Then:_

1. _Counterfactual fairness is equivalent to demographic parity if and only if there is no unblocked path between_ \(X_{A}^{}\) _and_ \(A\)_._
2. _Counterfactual fairness is equivalent to equalized odds if and only if all paths between_ \(X_{A}^{}\) _and_ \(A\)_, if any, are either blocked by a variable other than_ \(Y\) _or unblocked and contain_ \(Y\)_._
3. _Counterfactual fairness is equivalent to calibration if and only if all paths between_ \(Y\) _and_ \(A\)_, if any, are either blocked by a variable other than_ \(X_{A}^{}\) _or unblocked and contain_ \(X_{A}^{}\)_._

Similar statements can be made for any group fairness metric. For example, the notion of false negative error rate balance, also known as equal opportunity , is identical to equalized odds but only considers individuals who have a positive label (\(Y=1\)). The case for this metric is based on false negatives being a particular moral or legal concern, motivated by principles such as "innocent until proven guilty," in which a false negative represents an innocent person (\(Y=1\)) who is found guilty (\(f(X)=0\)). False negative error rate balance is assessed in the same way as equalized odds but only with observations that have a positive true label, which may be consequential if different causal structures are believed to obtain for different groups.

Table 1 shows the ten group fairness metrics presented in Verma and Rubin  that can be expressed as conditional independencies. Theorem 2 specified the correspondence between three of these (demographic parity, equalized odds, and calibration), and we extend to the other seven in Corollary 2.1.

We have so far restricted ourselves to a binary classifier \(f(X)\{0,1\}\). Here, we denote this as a decision \(f(x)=d D=\{0,1\}\) and also consider probabilistic classifiers that produce a score \(s\) that can take any probability from 0 to 1, i.e., \(f(x)=s S==(Y=1)\). The metrics of balance for positive class, balance for negative class, and score calibration are defined by Verma and Rubin  in terms of score. Verma and Rubin  refer to calibration for binary classification (Definition 3) as "conditional use accuracy equality." To differentiate these, we henceforth refer to the binary classification metric as "binary calibration" and the probabilistic classification metric as "score calibration."

**Corollary 2.1**.: _Let the causal structure be a causal DAG with \(X_{Y}^{}\), \(X_{A}^{}\), \(Y\), and \(A\), such as in Figure 2. Assume faithfulness between \(A\), \(Y\), and \(f(X)\). Then:_

1. _For a binary classifier, counterfactual fairness is equivalent to_ **conditional demographic parity** _if and only if, when a set of legitimate factors_ \(L\) _is held constant at level_ \(l\)_, there is no unblocked path between_ \(X_{A}^{}\) _and_ \(A\)_._
2. _For a binary classifier, counterfactual fairness is equivalent to_ **false positive error rate balance** _if and only if, for the subset of the population with negative label (i.e.,_ \(Y=0\)_), there is no path between_ \(X_{A}^{}\) _and_ \(A\)_, a path blocked by a variable other than_ \(Y\)_, or an unblocked path that contains_ \(Y\)_._
3. _For a binary classifier, counterfactual fairness is equivalent to_ **false negative error rate balance** _if and only if, for the subset of the population with positive label (i.e.,_ \(Y=1\)_), there is no path between_ \(X_{A}^{}\) _and_ \(A\)_, a path blocked by a variable other than_ \(Y\)_, or an unblocked path that contains_ \(Y\)_._
4. _For a probabilistic classifier, counterfactual fairness is equivalent to_ **balance for negative class** _if and only if, for the subset of the population with negative label (i.e.,_ \(Y=0\)_), there is no path between_ \(X_{A}^{}\) _and_ \(A\)_, a path blocked by a variable other than_ \(Y\)_, or an unblocked path that contains_ \(Y\)_._
5. _For a probabilistic classifier, counterfactual fairness is equivalent to_ **balance for positive class** _if and only if, for the subset of the population with positive label (i.e.,_ \(Y=1\)_), there is no path between_ \(X_{A}^{}\) _and_ \(A\)_, a path blocked by a variable other than_ \(Y\)_, or an unblocked path that contains_ \(Y\)_._
6. _For a probabilistic classifier, counterfactual fairness is equivalent to_ **predictive parity** _if and only if, for the subset of the population with positive label (i.e.,_ \(D=1\)_), there is no path between_ \(Y\) _and_ \(A\)_, a path blocked by a variable other than_ \(X_{A}^{}\)_, or an unblocked path that contains_ \(X_{A}^{}\)_._
7. _For a probabilistic classifier, counterfactual fairness is equivalent to_ **score calibration** _if and only if there is no path between_ \(Y\) _and_ \(A\)_, a path blocked by a variable other than_ \(X_{A}^{}\)_, or an unblocked path that contains_ \(X_{A}^{}\)_._

 
**Name** & **Probability Definition** & **Independence Definition** \\  Demographic Parity & \((D=1 A=0)=(D=1 A=1)\) & \(D A\) \\  Conditional Demographic Parity & \((D=1 A=0,L=l)=(D=1 A=1,L=l)\) & \(D A L=l\) \\  Equalized Odds & \((D=1 A=0,Y=y)=(D=1 A=1,Y=y)\) & \(D A Y\) \\  False Positive Error Rate Balance & \((D=1 A=0,Y=0)=(D=1 A=1,Y=0)\) & \(D A Y=0\) \\  False Negative Error Rate Balance & \((D=0 A=0,Y=1)=(D=0 A=1,Y=1)\) & \(D A Y=1\) \\  Balance for Negative Class & \([S A=0,Y=0]=[S A=1,Y=0]\) & \(S A Y=0\) \\  Balance for Positive Class & \([S A=0,Y=1]=[S A=1,Y=1]\) & \(S A Y=1\) \\  Conditional Use Accuracy Equality (i.e., Calibration) & \((Y=y A=0,D=d)=(Y=y A=1,D=d)\) & \(Y A D\) \\  Predictive Parity & \((Y=1 A=0,D=1)=(Y=1 A=1,D=1)\) & \(Y A D=1\) \\  Score Calibration & \((Y=1 A=0,S=s)=(Y=1 A=1,S=s)\) & \(Y A S\) \\  

Table 1: Group fairness metrics from Verma and Rubin .

This general specification can be unwieldy, so to illustrate this, we provide three real-world examples shown in Figure 2: measurement error, selection on label (i.e., "post-treatment bias" or "selection on outcome" if the label \(Y\) is causally affected by \(X\)), and selection on predictors. These imply demographic parity, equalized odds, and calibration, respectively.

**Corollary 2.2**.: _Assume faithfulness between \(A\), \(Y\), and \(f(X)\)._

1. _Under the graph with measurement error as shown in Figure_ 1(a)_, a predictor achieves counterfactual fairness if and only if it achieves demographic parity._
2. _Under the graph with selection on label as shown in Figure_ 1(b)_, a predictor achieves counterfactual fairness if and only if it achieves equalized odds._
3. _Under the graph with selection on predictors as shown in Figure_ 1(c)_, a predictor achieves counterfactual fairness if and only if it achieves calibration._

## 6 Experiments

We conducted brief experiments in a semi-synthetic setting to show that a counterfactually fair predictor achieves robust prediction and group fairness. We used the Adult income dataset  with a simulated protected class \(A\), balanced with \(P(A=0)=P(A=1)=0.5\). For observations with \(A=1\), we manipulated the input data to simulate a causal effect of \(A\) on \(X\): \(P(=)=0.8\). With this as the target distribution, we produced three biased datasets that result from each effect produced with a fixed probability for observations when \(A=1\): measurement error (\(P=0.8\)), selection on label (\(P=0.5\)), and selection on predictors (\(P=0.8\)).

On each dataset, we trained three predictors: a naive predictor trained on \(A\) and \(X\), a fairness through unawareness (FTU) predictor trained only on \(X\), and a counterfactually fair predictor based on an average of the naive prediction under the assumption that \(A=1\) and the naive prediction under the assumption \(A=0\), weighted by the proportion of each group in the target distribution.

**Theorem 3**.: _Let \(X\) be an input dataset \(X\) with a binary label \(Y=\{0,1\}\) and protected class \(A\{0,1\}\). Define a predictor:_

\[f_{naive}:=*{argmin}_{f}[(f(X,A),Y)]\]

_where \(f\) is a proper scoring rule. Define another predictor:_

\[f_{CF}:=(A=1)f_{naive}(X,1)+(A=0)f_{naive}(X,0)\]

_If the association between \(Y\) and \(A\) is purely spurious, then \(f_{CF}\) is counterfactually fair._

For robust prediction, we show that the counterfactually fair (CF) predictor has accuracy in the target distribution near the accuracy of a predictor trained directly on the target distribution. For group fairness, we show that the counterfactually fair predictor achieves demographic parity, equalized odds, and calibration, corresponding to the three biased datasets. Results are shown in Table 2 and Table 3, and code to reproduce these results or produce results with varied inputs (number of datasets sampled, effect of \(A\) on \(X\), probabilities of each bias, type of predictor) is available at [https://github.com/jacyanthis/Causal-Context](https://github.com/jacyanthis/Causal-Context).

  &  &  \\   & **Naive** & **FTU** & **CF** & **Naive** & **FTU** & **CF** & **Target** \\  Measurement Error & 0.8283 & 0.7956 & 0.7695 & 0.8031 & 0.8114 & 0.8204 & 0.8674 \\  Selection on Label & 0.8771 & 0.8686 & 0.8610 & 0.8566 & 0.8652 & 0.8663 & 0.8655 \\  Selection on Predictors & 0.8659 & 0.656 & 0.8658 & 0.8700 & 0.8698 & 0.8699 & 0.8680 \\  

Table 2: Experimental results: Robust prediction 

## 7 Discussion

In this paper, we provided a new argument for counterfactual fairness--that the supposed trade-off between fairness and accuracy  can evaporate under plausible conditions when the goal is accuracy in an unbiased target distribution (Theorem 1). To address the challenge of trade-offs between different group fairness metrics, such as their mutual incompatibility  and the variation in costs of certain errors such as false positives and false negatives , we provided a conceptual tool for adjudicating between them using knowledge of the underlying causal context of the social problem (Theorem 2 and Corollary 2.1). We illustrated this for the three most common fairness metrics, in which the bias of measurement error implies demographic parity; selection on label implies equalized odds; and selection on predictors implies calibration (Corollary 2.2), and we showed a minimal example by inducing particular biases on a simulated protected class in the Adult income dataset.

There are nonetheless important limitations that we hope can be addressed in future work. First, the counterfactual fairness paradigm still faces significant practical challenges, such as ambiguity and identification. Researchers can use causal discovery strategies developed in a fairness context  to identify the causal structure of biases in real-world datasets and ensure theory like that outlined in this paper can be translated to application. Second, a key assumption in our paper and related work has been the assumption that associations between \(Y\) and \(A\) are "purely spurious," a term coined by Veitch et al.  to refer to cases where, if one conditions on \(A\), then the only information about \(Y\) in \(X\) is in the component of \(X\) that is not causally affected by \(A\). This has provided a useful conceptual foothold to build theory, but it should be possible for future work to move beyond the purely spurious case, such as by articulating distribution shift and deriving observable signatures of counterfactual fairness in more complex settings .

## 8 Acknowledgments

We thank Bryon Aragam, James Evans, and Sean Richardson for useful discussions.