# Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent research has uncovered that federated learning (FL) systems are vulnerable to various security threats. Although various defense mechanisms have been proposed, they are typically non-adaptive and tailored to specific types of attacks, leaving them insufficient in the face of adaptive or mixed attacks. In this work, we formulate adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to tackle poisoning attacks of unknown/uncertain types. We further develop an efficient meta-learning approach to solve the game, which provides a robust and adaptive FL defense. Theoretically, we show that our algorithm provably converges to the first-order \(\)-equilibrium point in \(O(^{-2})\) gradient iterations with \(O(^{-4})\) samples per iteration. Empirical results show that our meta-Stackelberg framework obtains superb performance against strong model poisoning and backdoor attacks with unknown/uncertain types.

## 1 Introduction

Federated learning (FL) allows multiple devices with private data to jointly train a model without sharing their local data . However, FL systems are vulnerable to various adversarial attacks such as untargeted model poisoning attacks (e.g., IPM , LMP ) and backdoor attacks (e.g., BFL , DBA ). To address these vulnerabilities, various robust aggregation rules such as Krum , coordinate-wise trimmed mean , and FLTrust  have been proposed to defend against untargeted attacks, and both training-stage and post-training defenses such as Norm bounding , NeuroClip , and Prun  have been proposed to mitigate backdoor attacks. Further, dynamic defenses that myopically adapt parameters such as learning rate , norm clipping threshold , and regularizer  have been proposed. However, state-of-the-art defenses remain inadequate in countering advanced adaptive attacks (e.g., the reinforcement learning (RL)-based attacks [31; 32]) that dynamically adjust the attack strategy to obtain long-term advantages. Further, current defenses are typically designed to counter specific types of attacks, rendering them ineffective in the presence of mixed attacks. As shown in Table 1 (Section 4), simply combining existing defenses with manual tuning proves ineffective due to the interference between defense methods, the defender's lack of information about adversaries, and the dynamic nature of FL.

In this work, we propose a meta-Stackelberg game (meta-SG) framework that obtains superb defense performance even in the presence of strong adaptive attacks and a mix of attacks of the same or different types (e.g., the coexistence of model poisoning and backdoor attacks). Our meta-SG defense framework is built upon the following key observations. First, when the attack type (to be defined in Section 2.1) is known as priori, the defender can utilize the limited amount of local data at the server and publicly available information to build an approximate world model of the FL system. This allows the defender to identify a robust defense policy offline by solving either a Markov decision process (MDP) when the attack is non-adaptive or a Markov game when the attack is adaptive. Thisapproach naturally applies to both a single attack and the coexistence of multiple attacks and can potentially produce a (nearly) optimal defense. Second, when the attacks are unknown or uncertain, as in more realistic settings, the problem can be formulated as a Bayesian Stackelberg Markov game (BSMG) , which provides a general model for adversarial FL. However, the standard solution concept for BSMG, namely, the Bayesian Stackelberg equilibrium, targets the expected case and does not adapt to the actual attack with an unknown/uncertain type.

Motivated by this limitation, we propose a novel solution concept called meta-Stackelberg equilibrium (meta-SE) for BSMG as a principled way of developing robust and adaptive defenses for FL. By integrating meta-learning and Stackelberg reasoning, meta-SE offers a computationally efficient approach to address information asymmetry in adversarial FL and enables strategic adaptation in online execution in the presence of multiple (adaptive) attackers. Before training an FL model, a meta policy is learned by solving the BSMG using experiences sampled from a set of possible attacks. When facing an actual attacker during FL training, the meta-policy is quickly adapted using a relatively small number of samples collected on the fly. The proposed meta-SG framework only requires a rough estimate of possible worst-case attacks during meta-training, thanks to the generalization ability brought by meta-learning.

To solve the BSMG in the pre-training phase, we propose a meta-Stackelberg learning (meta-SL) algorithm based on the debiased meta-reinforcement learning approach in . The meta-SL provably converges to the first-order \(\)-approximate meta-SE in \(O(^{-2})\) iterations, and the associated sample complexity per iteration is of \(O(^{-4})\). Even though meta-SL achieves state-of-the-art sample efficiency presented in , its operation involves the Hessian of the defender's value function. To obtain a more practical solution (to bypass the Hessian computation), we further propose a fully first-order pre-training algorithm, called Reptile meta-SL, inspired by Reptile . Reptile meta-SL only utilizes the first-order stochastic gradients from the attacker's and the defender's problem to solve for the approximate equilibrium. The numerical results in Table 1 demonstrate its effectiveness in handling various types of non-adaptive attacks, including mixed attacks, while Figure 2 and Figure 9 highlight its efficiency in coping with uncertain or unknown attacks, including adaptive attacks. Due to the space limit, we move related work section to Appendix A. **Our contributions** are summarized as follows:

* We address critical security problems in FL in the face of attacks that may be adaptive or mixed with multiple types.
* We develop a Bayesian Stackelberg game model (Section 2.2) to capture the information asymmetry in the adversarial FL under multiple uncertain/unknown attacks.
* To create a strategically adaptable defense, we propose a new equilibrium concept: meta-Stackelberg equilibrium (meta-SE), where the defender (the leader) commits to a meta policy and an adaptation strategy, leading to a data-driven approach to tackle information asymmetry.
* To learn the meta equilibrium defense in the pre-training phase, we develop meta-Stackelberg learning (Algorithm 1), an efficient first-order meta RL algorithm, which provably converges to \(\)-approximate equilibrium in \(O(^{-2})\) gradient steps with \(O(^{-4})\) samples per iteration, matching the state-of-the-art efficiency in stochastic bilevel optimization.
* We conduct extensive experiments in real-world settings to demonstrate the superb performance of our proposed method.

## 2 Meta Stackelberg Defense Framework

### Framework Overview

As shown in Figure 1, the meta-learning framework includes two stages: _pre-training_, _online adaptation_. The _pre-training_ stage is implemented in a simulated environment, which allows sufficient training using trajectories generated from the interactions between the defender and the attacker with its type randomly sampled from a set of potential attacks. Both adaptive and non-adaptive attacks could be considered for pre-training. After obtaining a meta-policy, the defender will interact with the real FL environment in the _online adaptation_ stage to tune its defense policy using feedback (i.e., model updates and environment parameters) received in the face of real attacks that are not necessarily in the pre-training attack set. Finally, at the last round of FL training, the defender will perform a post-training defense on the global model, which may or may not be considered in the design of intelligent attacks. Pre-training and online adaptation are indispensable in the proposed framework. Table 5 in Appendix D indicate that directly applying defense learned from pre-training without online adaptation, as well as adaptation from a randomly initialized defense policy without pre-training, both fail to address malicious attacks.

**FL objective.** Consider a learning system that includes one server and \(n\) clients, each client possesses its own private dataset \(D_{i}=(x_{i}^{j},y_{i}^{j})_{j=1}^{|D_{i}|}\) where \(|D_{i}|\) is the size of the dataset for the \(i\)-th client. Let \(U=\{D_{1},D_{2},,D_{n}\}\) denote the collection of all client datasets. The objective of federated learning is to obtain a model \(w\) that minimizes the average loss across all the devices: \(_{w}F(w):=_{i=1}^{n}f(w,D_{i})\), where \(f(w,D_{i}):=|}_{j=1}^{|D_{i}|}(w,(x_{i}^{j},y_{i}^{j}))\) is the local empirical loss with \((,)\) being the loss function.

**Attack objective.** We consider two major categories of attacks: untargeted model poisoning attacks and backdoor attacks. An untargeted model poisoning attack aims to maximize the average model loss, i.e., \(_{w}-F(w)\), while a targeted one strives to cause misclassification of poisoned test inputs to one or more target labels (e.g., backdoor attacks). A malicious client \(i\) employing targeted attack first produces a poisoned dataset \(D_{i}^{}\) by altering a subset of data samples \((x_{i}^{j},y_{i}^{j}) D_{i}\) to \((_{i}^{j},c^{*})\). Here \(_{i}^{j}\) is the tainted sample with a backdoor trigger inserted, and \(c^{*} y_{i}^{j},c^{*} C\) is the targeted label. Let \(_{i}=|D_{i}^{}|/|D_{i}|\) denote the poisoning ratio, which is typically unknown to the defender. To simplify the notation, we assume that among the \(M=M_{1}+M_{2}\) malicious clients, the first \(M_{1}\) malicious clients carry out a targeted attack, and the following \(M_{2}\) malicious clients undertake an untargeted attack. Note that clients in the same category may use different attack methods. Then, the joint objective of these malicious clients is \(_{w}F^{}(w):=}_{i=1}^{M_{1}}f(w,D_{i}^{})- }_{i=M_{1}+1}^{M}f(w,D_{i})\).

**FL process.** At each round \(t\) out of \(H\) rounds of FL training, the server randomly selects a subset of clients \(^{t}\) and sends them the most recent global model \(w_{g}^{t}\). Every benign client \(k^{t}\) updates the model using their local data via one or more iterations of stochastic gradient descent and returns the model update \(g_{k}^{t}\) to the server. In contrast, an adversary \(j^{t}\) creates a malicious model update \(_{j}^{t}\) and sends it back. The server then collects the set of model updates \(\{_{i}^{t}_{j}^{t} g_{k}^{t}\}_{i,j,k ^{t}}\), for \(i\{1,,M_{1}\},j\{M_{1}+1,,M\},k^{t}[M]\), utilizes an aggregation rule \(Aggr\) to combine them, and updates the global model: \(w_{g}^{t+1}=w_{g}^{t}-^{t}Aggr(_{i}^{t}_{j}^ {t} g_{k}^{t})\), which is then sent to clients in round \(t+1\). At the end of each round, the defender will perform a post-training defense \(h()\) on the global model \(_{g}^{t}=h(w_{g}^{t})\) to evaluate the current defense performance. Only at the final round \(H\) or whenever a client is leaving the FL systems, the global model with post-training defense \(_{g}^{t}\) will be sent to all (leaving) clients.

**Attack types.** To simplify the exposition, we assume that a single mastermind attacker controls all malicious clients within the FL system and employs diverse attack strategies on each controlled client. We introduce the concept of _attack type_ to differentiate various attack scenarios, which typically include the following three aspects. The first aspect is the attack objective chosen by a malicious client. Let \(_{1}\) be the set of all possible attack objectives from the defender's knowledge base. We set \(_{1}=\{\}\) in this work. The second aspect specifies the attack method (i.e., the

Figure 1: A graphical abstract of meta-Stackelberg defense. In the pertaining stage, a simulated environment is constructed using generated data and the attack domain. The defender utilizes meta-Stackelberg learning (Algorithm 1) to obtain the meta policy to be online adapted in the real FL.

algorithm used to generate the actual attack policy) adopted by a malicious client. Let \(_{2}\) be the set of all possible attack methods from the defender's knowledge base. The third aspect captures the configuration associated with an attack method, including its hyperparameters and other attributes (e.g., triggers implanted in backdoor attacks, labels used in targeted attacks, and attacker's knowledge about the FL system). Let \(_{3}\) denote the set of all possible configurations. For each malicious client \(i\), the tuple \((_{1},_{2},_{3})_{i}\) where \(_{k}_{k}\) for each \(k\) fully specifies its particular attack type. Let \(=\{(_{1},_{2},_{3})_{i}\}_{i=1}^{M}\) be the joint attack type. Further, let \(=(_{1}_{2}_{3})^{M}\) denote the domain of attacks that the defender is aware of. Table 2 in Appendix C gives the types of all the attacks considered in this work. However, the actual attack type encountered during FL training is not necessary in \(\), although it is presumably similar to a known type in \(\).

### Pre-training as a Bayesian Stackelberg Markov game

From the discussion above, the global model updates and the final output are jointly influenced by the defender (through aggregation) and the malicious clients (through corrupted gradients). Hence, the FL process in an adversarial environment can be formulated as a two-player discrete time Bayesian Stackelberg Markov game (BSMG) defined by a tuple \( S,A_{},A_{},,r,,H\). Using discrete time index \(t\) (one step corresponds to one FL round), we have the following.

* \(S\) is the state space, and its elements represent the global model at each round \(s^{t}=w_{g}^{t}\).
* \(A_{}\) is the defender's action set. Each action \(a_{}^{t}\) represents a combination of the robust aggregation and post-training defenses: \(a_{}^{t}=\{Aggr(),h()\}\).
* \(A_{}\) is the type-\(\) attacker's action set. Each action includes the joint model updates of all malicious clients: \(a_{}^{t}=\{_{i}^{t}\}_{i=1}^{M_{1}}\{_{i}^{t}\}_{i=M_{1}+1}^{M}\).
* \((s^{t+1}|s^{t},Aggr(),a_{}^{t})\) specifies the distribution of the next state given the current state and joint actions at \(t\), which is determined by the global model update: \(w_{g}^{t+1}=w_{g}^{t}-^{t}Aggr(_{i}^{t}_{j }^{t} g_{k}^{t})\).
* \(r_{},r_{}\) are the defender's and the attacker's reward functions (to be maximized), respectively. The defender aims to minimize the loss after the post-training: \(r_{}^{t}:=-F(_{g}^{t})\) where \(_{g}^{t}=h(w_{g}^{t})\). The attacker's \(r_{}^{t}\) is given by the joint attack objective: \(-F^{}(_{g}^{t})\).

_Remark 2.1_.: The post-training defense is only applied in the final round or to a client leaving the FL system and does not interfere with the model updates on \(w_{g}^{t}\). The defender's reward function is crafted to encompass post-training, as we prioritize a practical, long-term average reward within an online process, which enables clients to seamlessly join and depart from the FL system. This design enables us to incorporate a post-training defense along with techniques for modifying the model structure, such as drop-off and pruning.

**Simulated environment in the white-box setting.** With the game model defined above, the defender (i.e., the server) can, in principle, identify a strong defense by solving the game (we discuss different solution concepts in Section 3). Due to efficiency and privacy concerns in FL, however, it is often infeasible to solve the game in real time when facing the actual attacker. Instead, the defender can create a simulated environment to approximate the actual FL system during the pre-training stage. The main challenge, however, is that the defender often lacks information about the individual devices in FL. We first consider the _white-box_ setting where the defender is aware of the number of malicious devices in each category (i.e., \(M_{1}\) and \(M_{2}\)) and their actual attack types, as well as the _non-i.i.d._ level (to be defined in Section 4.1) of local data distributions across devices. However, it does not have access to individual devices' local data and random seeds, making it difficult to simulate clients' local training and evaluate rewards. To this end, we assume that the server has a small amount of root data randomly sampled from the the collection of all client dataset \(U\) as in previous work [10; 40]. We then use generative model (e.g., conditional GAN model  for MNIST and diffusion model  for CIFAR-10 in our experiments) to generate as much data as necessary to mimic the local training (see details in Appendix C). We give an ablation study (Table 6) in Appendix D to evaluate the influence of limited/biased root data. We remark that the purpose of pre-training is to derive a defense policy rather than the model itself. Directly using the shifted data (root or generated) to train the FL model will result in low model accuracy (see Table 5 in Appendix D).

**Handling the black-box setting.** We then consider the more realistic _black-box_ setting, where the defender has no access to the number of malicious devices and their actual attack types,nor the _non-i.i.d._ level of local data distributions. To obtain a robust defense, we assume the server considers the worst-case scenario based on a rough estimate of the missing information (see our ablation study in the experiment section) and adopts the RL-based attacks to simulate the worst-case attacks (see Section 3.1) when the attack is unknown or adaptive. In the face of an unknown backdoor attack, the defender does not know the backdoor triggers and targeted labels. To simulate a backdoor attacker's behavior, we first implement multiple GAN-based attack models as in  to generate worst-case triggers (which maximizes attack performance given the backdoor objective) in the simulated environment. Since the defender does not know the poisoning ratio \(_{i}\) and the target label of the attacker's poisoned dataset (needed to determine the attack objective \(F^{}\)), we approximate the attacker's reward function by \(r_{}^{t}=-F^{}(_{g}^{t+1})\), where \(F^{}(w):=_{c C}[}_{i=1}^{M_{1}}^{}|}_{j=1}^{|D_{i}^{}|}(w,(_{i}^{j},c))]-}_{i=M_{1}+1}^{M}f(,D_{i})\). \(F^{}\) differs \(F^{}\) only in the first \(M_{1}\) clients, where we use a strong target label (that minimizes the expected loss) as a surrogate to the true label \(c^{*}\). We compare the defense performance against white-box and black-box backdoor attacks ( see Figure 10 in Appendix D).

## 3 Meta Stackelberg Learning

Since the pre-training is modeled by a Bayesian Markov Stackelberg game, solving the game efficiently is crucial to a successful defense. This work's main contribution includes the formulation of a new solution concept to the game, meta-Stackelberg equilibrium (meta-SE), and a learning algorithm to approximate such equilibrium in finite time. To motivate the proposed concept, we begin by addressing the defense against non-adaptive attacks.

Consider the attacker employing a non-adaptive attack of type \(\); in other words, the attack action at each iteration is determined by a fixed attack strategy \(_{}\), where \(_{}(a)\) gives the probability of taken action \(a A_{}\), independent of the FL training and the defense strategy. In this case, BSMG reduces to an MDP, where the transition kernel is \(_{}(|s,a_{})_{A_{}} (|s,a_{},a_{})d_{}(a_{})\). Parameterizing the defender's policy \(_{}(a_{}^{t}|s^{t};)\) by a neural network with model weights \(\), the solution to the following optimization problem \(_{}_{a_{}^{t}_{},s ^{t}_{}}[_{t=1}^{H}^{t}r_{}^{t}]  J_{}(,)\) gives the optimal defense against the non-adaptive attack. When the actual attack in the online stage falls within \(\), which the defender is uncertain of, one can consider the defense against the expected attack: \(_{}_{ Q}J_{}(,)\), where \(Q\) is a distribution over the attack domain to be designed by the defender. One intuitive design is to include all reported attack methods in history as the attack domain and their empirical frequency as the \(Q\) distribution.

In stark contrast to non-adaptive attacks, an adaptive attack can adjust attack actions to the FL environment and the defense mechanism [31; 32]. Most existing attacks are history-independent [50; 65]. Hence, we assume that an adaptive attack takes the current state (global model) as input, i.e., the attack policy is a Markov policy denoted by \(_{}(a_{}^{t}|s^{t})\). Denoted by \(\) the attack type; then, an optimal adaptive attack policy, parameterized by \(\), is the best response to the existing defense \(_{}(|s^{t};)\): \(_{a_{}^{t}_{},a_{}^{ t}_{}}[_{t=1}^{H}^{t}r_{}^{t}] J_{ }(,,)\). Denote by \(_{}^{*}\) the maximizer, and then, the defender's cumulative rewards under such attack is \(J_{}(,_{}^{*},)_{a_{}^{t}_{},a_{}^{t}_{}}[_{t=1}^{H} ^{t}r_{}^{t}]\).

### RL-based attacks and defenses

The actual attack type (which could be either adaptive or non-adaptive) encountered in the online phase may be not in \(\) and thus unknown to the defender. To prepare for these unknown attacks, we propose to use multiple RL-based attacks with different objectives, adapted from RL-based untargeted model poising attack  and RL-based backdoor attack , as surrogates for unknown attacks, which are added to the attack domain for pre-training. The rationale behind the RL surrogates includes: (1) they achieve strong attack performance by optimizing long-term objectives; (2) they adopt the most general action space (i.e., model updates), which allows them to mimic any adaptive or non-adaptive attacks given the corresponding objectives; (3) they are flexible enough to incorporate multiple attack methods by using RL to tune the hyper-parameters of a mixture of attacks. A similar argument applies to RL-based defenses. We remark that in this paper, an RL-based attack (defense) is not a single attack (defense) as in [31; 32] but a systematically synthesized combination of existing attacks (defenses). In the simulated environment, we train our defense against the strongest white-boxRL attacks in  with different objectives (e.g., untargeted or targeted), which is considered the optimal attack strategy. The "worst-case" scenario is commonly used in security scenarios to ensure the associated defense has performance guarantees under "weaker" attacks with similar objectives. Such a robust defense policy gives us a good starting point to further adapt to uncertain or unknown attacks. Our defense is generalizable to other adaptive attacks (see Table 8 in Appendix D). The key novelty of our RL-based defense is that instead of using a fixed and hand-crafted algorithm as in existing approaches, we use RL to optimize the policy network \(_{}(a_{}^{t}|s^{t};)\). Similar to RL-based attacks, the most general action space could be the set of global model parameters. However, the high dimensional action space will lead to an extremely large search space that is prohibitive in terms of training time and memory space. Thus, we apply compression techniques (see Appendix C) to reduce the action from high-dimensional space to a 3-dimensional space. Note that the execution of our defense policy is lightweight, without using any extra data for evaluation/validation. See the discussion in Appendix C on how we apply our RL-based defense during online adaptation.

### Meta-Stackelberg equilibrium

As discussed in Section 2.2, one of the key challenges to solving the BSMG is the defender's incomplete information on attack types. Prior works have explored a Bayesian equilibrium approach to address this issue . Given the set of possible attacks \(\) that the defender is aware of and a prior distribution \(Q\) over the domain, the Bayesian Stackelberg equilibrium (BSE) is given by the following bi-level optimization:

\[_{}_{ Q}[J_{}(,_{ }^{*},)]_{}^{*} J_{}(,,).\] (BSE)

In (BSE), unaware of the exact attacker type, the defender (the leader) aims to maximize the defense performance against an average of all attack types, anticipating their best responses.

From a game-theoretic viewpoint, the Bayesian equilibrium in (BSE) is of ex-ante. The defender determines its equilibrium strategy only knowing the type distribution \(Q\). However, as the Markov game proceeds, the attacker's moves (e.g., malicious global model updates) during the interim stage (online stage) reveal additional information on the attacker's private type. This Bayesian equilibrium defense strategy fails to handle the emerging information on the attacker's hidden type in the interim stage, as the policy obtained from (BSE) remains fixed throughout the online stage without adaptation.

To address the limitation of Bayesian equilibrium, we introduce the novel solution concept, meta-Stackelberg equilibrium (meta-SE), to equip the defender with online responsive intelligence under incomplete information. As a synthesis of meta-learning and Stackelberg equilibrium, the meta-SE aims to pre-train a meta policy on a variety of attack types sampled from the attack domain \(\) such that online gradient adaption applied to the base produces a decent defense against the actual attack in the online environment. Using mathematical terms, we denote by \(_{}:=(s^{k},a_{}^{k},a_{}^{k})_{k=1}^{H}\) the trajectory of the FL system under type-\(\) attacker up to round \(H\), which is subject to the distribution \(q(,):=_{t=1}^{H}_{}(a_{}^{t}|s^{t}; )_{}(a_{}^{t}|s^{t})(s^{t+1}|s^{t},a_{ }^{t},a_{}^{t})\). Let \(_{}J_{}()\) be the unbiased estimate of the policy gradient \(_{}J_{}\) using the sample trajectory \(_{}\) (see Appendix E). Then, a one-step gradient adaptation using the sample trajectory is given by \(+_{}J_{}\). Incorporating this gradient adaptation into (BSE) leads to the proposed meta-SE.

\[_{} _{ Q}_{ q}[J_{}( +_{}J_{}(),_{}^{*},)],\] (meta-SE) \[_{}^{*}_{ q}J_{ }(+_{}J_{}(),,).\]

The idea of adding the gradient adaptation to the equilibrium is inspired by the recent developments in gradient-based meta-learning . When the attack is non-adaptive, the BSMG reduces to MDP problem, as delineated at the beginning of this section. Consequently, (meta-SE) turns into the standard form of meta-learning . Unlike the conventional (BSE), the solution to (meta-SE) gives the defender a decent defense initialization after pre-training whose gradient adaptation in the online stage is tailored to type \(\), since the online trajectory follows the distribution \(q(,)\). The novelty of (meta-SE) lies in that the leader (defender) determines an optimal adaptation scheme rather than a policy, which is computed using an online trajectory without knowing the actual type, creating a data-driven strategic adaptation after the pre-training. Besides equation BSE, Appendix G also compares the perfect Bayesian equilibrium with the proposed meta-SE, highlighting the latter's scalability to complex FL systems.

### Meta-Stackelberg learning

Unlike finite Stackelberg Markov games that can be solved (approximately) using mixed-integer programming  or Q-learning , our BSMG admits high-dimensional continuous state and action spaces, posing a more challenging computation issue. Hence, we resort to a two-timescale policy gradient (PG) algorithm, referred to as meta-Stackelberg learning (meta-SL) presented in Algorithm 1, to solve for the meta-SE in a similar vein to . In plain words, meta-SL first learns the attacker's best response at a fast scale (lines 13-15), based on which it updates the defender's meta policy at a slow scale at each iteration using either debiased meta-learning  or reptile . The two-timescale meta-SL alleviates the non-stationarity caused by concurrent policy updates from both players . Of particular note is that the debiased meta-learning involves Hessian computation when evaluating the gradient of the defender's objective function since the attacker's best response \(_{}^{*}()\) also depends on \(\). In contrast, reptile uses a first-order approximation to avoid Hessian. The mathematical subties between two policy gradient estimations are deferred to the Appendix E.

The rest of this subsection addresses the computation expense of the proposed meta-SL. We begin with an alternative solution concept for our first-order gradient algorithm, which is slightly weaker than (meta-SE). Let \(_{}(,,)_{ q}J_{ }(+_{}J_{}(),,)\), \(_{}(,,)_{ q}J_ {}(+_{}J_{}(),,)\), for a fixed type \(\). In the sequel, we will assume \(_{}\) and \(_{}\) to be continuously twice differentiable and Lipschitz-smooth with respect to both \(\) and \(\) as in , see Appendix F.

**Definition 3.1**.: For \((0,1)\), a pair \((^{*},\{_{}^{*}\}_{})^{||}\) is a _\(\)-meta First-Order Stackelbeg Equilibrium_ (\(\)-meta-FOSE) of the meta-SG if it satisfies the following conditions: for \(\), \(_{ B(^{*})}_{}_{ }(^{*},_{}^{*},),-^{*}\), \(_{ B(_{}^{*})}_{}_{} (^{*},_{}^{*},),-_{}^{*}\), where \(B(^{*}):=\{:\|-^{*}\| 1\}\), and \(B(_{}^{*}):=\{:\|-_{}^{*}\| 1\}\).

```
1:Input: the distribution \(Q()\), initial defense meta policy \(^{0}\), pre-defined attack methods \(\{_{}\}_{}\), pre-trained RL attack policies \(\{_{}^{0}\}_{}\), step size parameters \(_{}\), \(_{}\), \(\), and iterations numbers \(N_{},N_{}\);
2:Output:\(^{N_{}}\);
3:for iteration \(t=0\) to \(N_{}-1\)do
4:ifmeta-RL (for non-adaptive) then
5: Sample a batch of \(K\) attack types \(\) from \(\);
6: Estimate \(J_{D}():=_{}J_{}(,_{}, )|_{=_{}^{t}}\);
7:endif
8:ifmeta-SG then
9: Sample a batch of \(K\) attack types \(\);
10:for each sampled attack \(\)do
11: Apply one-step adaptation \(_{}^{t}^{t}+_{}J_{}( ^{t},_{}^{t},)\);
12:\(_{}^{t}(0)_{}^{t}\);
13:for iteration \(k=0,,N_{}-1\)do
14:\(_{}^{t}(k+1)_{}^{t}(k)+\)
15:\(_{}_{}J_{}(_{}^{t},_{ }^{t}(k),)\);
16:endfor
17:\(J_{D}()_{}J_{}(, _{}^{t}(N_{}),)|_{=_{}^{t}}\);
18:endfor
19:endif
20:\(^{t+1}^{t}_{}/K_{}J_ {D}()\)
21:endfor ```

**Algorithm 1** Meta-Stackelberg Learning

Since the value functions \(J_{},J_{}\) are nonconvex, we impose a regularity assumption adapted from the Polyak-Lojasiewicz (PL) condition , which is customary in nonconvex analysis. Despite the lack of theoretical justifications for the PL condition in the literature,  empirically demonstrates that the cumulative rewards in meta-reinforcement learning satisfy the PL condition, see Figure 4 Appendix D therein. Assumption 3.2 subsequently leads to the main result in Theorem 3.3

**Assumption 3.2** (Stackelberg Polyak-Lojasiewicz condition).: There exists a positive constant \(\) such that for any \((,)\) and \(\), the following inequalities hold: \(\|_{}_{}(,,)\|^{2} _{}_{}(,,)-_{ }(,,)\), \(\|_{}_{}(,,)\|^{2} _{}_{}(,,)-_{ }(\

[MISSING_PAGE_FAIL:8]

LMP, BFL, DBA\(\}\), then adapts it to the real single/mixed attack. We observe that multiple types of attacks may intervene with each other (e.g., IPM+BFL, LMP+DBA), which makes it impossible to manually address the entangled attacks. It is not surprising to see FedAvg  and defenses specifically designed for untargeted attacks (i.e., Trimmed mean, FLTrust) fail to defend backdoor attacks (i.e., BFL, DBA) due to the huge deviation of defense objective from the optimum. For a fair comparison, we further manually tune the norm threshold (more results in Appendix D) from \([0.01,0.02,0.05,0.1,0.2,0.5,1]\) for ClipMed (i.e., Norm bounding + Coordinate-wise Median) and clipping range from \([2:2:10]\) for FLTrust + NeuroClip to achieve the best performance to balance the global model and backdoor accuracy in linear form (i.e., Acc - Bac). Intuitively, a tight threshold/range has better performance in defending against backdoor attacks, yet will hinder or even damage the FL progress. On the other hand, a loose threshold/range fails to defend backdoor injection. Nevertheless, manually tuning in real-world FL scenarios is nearly impossible due to the limited knowledge of the ongoing environment and the presence of asymmetric adversarial information. Instead of suffering from the above concerns and exponential growth of parameter combination possibilities, our data-driven meta-RL approach can automatically tune multiple parameters at each round. Targeting the cumulative defense rewards, the RL approach naturally holds more flexibility than myopic optimization.

**Adaptation to uncertain/unknown attacks.** To evaluate the necessity and efficiency of adaptation from the meta-SG policy in the face of unknown attacks, we plot the global model accuracy graph over FL epochs. The meta-RL pre-trained from non-adaptive attack domain {NA, IPM, LMP, BFL, DBA\(\}\) (RL attack is unknown), while meta-SG pre-train from interacting with a group of RL attacks initially target on {FedAvg, Coordinate-wise Median, Norm bounding, Krum, FLTrust } (LMP is unknown). The meta-SG plus (i.e., meta-SG+) is a pre-trained model from the combined attack domain of the above two. All three defenses then adapt to the real FL environments under LMP or RL attacks. As shown in Figure 2, the meta-SG can quickly adapt to both uncertain RL-based adaptive attack (attack action is time-varying during FL) and unknown LMP attack, while meta-RL can only slowly adapt to or fail to adapt to the unseen RL-based adaptive attacks on MNIST and CIFAR-10 respectively. In addition, the first and the third Figures in Figure 2 demonstrate the power of meta-SG against unknown LMP attacks, even if LMP is not directly used during its pre-training stage. The results are only slightly worse than meta-SG plus, where LMP is seen during pre-training. Similar observations are given under IPM in Appendix D.

## 5 Conclusion

We have proposed a meta-Stackelberg framework to tackle attacks of uncertain or unknown types in federated learning through data-driven adaptation. The proposed meta-Stackelberg learning approach is computationally tractable and strategically adaptable, targeting mixed and adaptive attacks under incomplete information. The major limitation of our current approach pertains to privacy concerns. Our current simulation necessitates that the defender either accesses a small portion of root data or learns clients' data through inversion, which slightly violates the privacy principles of FL. To minimize privacy risks, we train our meta-policy in a simulated environment and apply data augmentation to blur the learned data. In our experiments, the current "black-box" setting operates under certain conditions: we test only one or a few agnostic variables at a time while leaving other information known to the defender (see Appendix D). In our future work, we plan to incorporate additional state-of-the-art defense algorithms to counter more potent attacks, such as edge-case attacks , as well as other attack types, such as privacy-leakage attacks . We will also explore new application scenarios, including NLP and large generative models. Our framework could be further improved by including a client-side defense mechanism that closely mirrors real-world scenarios, replacing the current processes of self-data generation.

Figure 2: Comparisons of defenses against untargeted model poisoning attacks (i.e., LMP and RL) on MNIST and CIFAR-10. All parameters are set as default and random seeds are fixed.