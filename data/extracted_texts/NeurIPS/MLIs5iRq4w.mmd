# Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models

Weijian Luo\({}^{1}\)1, Tianyang Hu\({}^{2}\)2, Shifeng Zhang\({}^{2}\), Jiacheng Sun\({}^{2}\), Zhenguo Li\({}^{2}\), Zhihua Zhang\({}^{1}\)

\({}^{1}\)Peking University, \({}^{2}\)Huawei Noah's Ark Lab

###### Abstract

Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports. We also reveal non-trivial connections of our method to existing works such as DreamFusion , and generative adversarial training. To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion models and refining existing GAN models. The experiments on distilling pre-trained diffusion models show that Diff-Instruct results in state-of-the-art single-step diffusion-based models. The experiments on refining GAN models show that the Diff-Instruct can consistently improve the pre-trained generators of GAN models across various settings. Our official code is released through https://github.com/pkulwjl994/diff_instruct.

## 1 Introduction

Over the last decade, the field of deep generative models has made significant strides across various domains such as data generation , density estimation , image-editing  and others. Notably, recent advancements in text-driven high-resolution image generation  have pushed the limits of using generative models for Artificial Intelligence Generated Content (AIGC). Behind the empirical success are fruitful developments of a wide variety of deep generative models, among which, diffusion models (DMs) are the most prominent. DMs leverage the diffusion processes and model the data across a wide spectrum of noise levels. Their ease of training, ability to scale, and high sample quality have made DMs the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets and applications. The trained DMs contain intricate information about the data distribution, making them valuable **assets** for downstream applications.

Compared with training from scratch, extracting knowledge from a zoo of pre-trained models enables us to learn more efficiently. For instance,  employed a variety of pre-trained feature extractors and significantly boosted the state-of-the-art performance on domain generalization benchmarks. [9; 52; 16] exploited the rich multi-modal information stored in off-the-shelf CLIP models  for efficient text-guided image generation. Currently, we are witnessing a rising trend of _learning from models_, especially when accessing large amounts of high-quality data is difficult. Such a _model-driven_ learning scheme can be particularly appealing for handling new tasks by providing a solid base model, which can be further improved by additional training data. While this research direction has been extensively investigated for discriminative models and supervised learning tasks [71; 38; 3; 72; 57; 56], its application to generative models remains largely unexplored. To this end, we are motivated to study the following question.

_(Q1): Can we transfer knowledge from pre-trained DMs to other generative models instead of learning from original training data?_

The seminal work DreamFusion  demonstrated the feasibility of such a quest for text-to-3D generation. Without a large text-labeled 3D dataset,  took advantage of the rich text-to-image knowledge stored in a large-scale diffusion model, the Imagen , to learn the 3D Neural Radiance Fields (NeRF) and achieved surprisingly good performance without using any 3D data. DreamFusion is a rare success and for general scenarios, the task can be difficult due to the vast difference among generative models. DMs represent a class of explicit generative models wherein the data's score function is modeled. Conversely, in various downstream applications, implicit generative models are favored due to their inherent flexibility and efficiency. An implicit model typically learns a neural transformation (i.e., a generator) that maps from a latent space to the data space, such as in generative adversarial networks (GANs), thereby enabling expeditious generation.

By exploring diverse architectural designs and latent space configurations, implicit models can readily adapt to structural constraints (e.g., molecules must be chemically valid, etc. [14; 10; 63]), assimilate prior knowledge [6; 13; 76; 45], and exhibit other advantageous properties. For implicit models that lack explicit score information, how to receive supervision from DM's multi-level score network is technically challenging, which greatly limits the potential use cases of pre-trained DMs. Therefore, we would like to further address the following question:

_(Q2): Can we tackle this challenge so that knowledge from DMs can be more broadly transferred?_

In this work, we give affirmative answers to (Q2) and propose a universal framework, **Diff-Instruct** (**DI**), to leverage pre-trained DMs to instruct the training of arbitrary implicit generative models as long as the generated samples from the implicit model are differentiable with respect to model parameters. When applied to single-step generation models such as GANs, Diff-Instruct provides an alternative non-adversarial training scheme. When the student model is a U-Net (with a fixed time), our method enters as a strong contender in the diffusion distillation literature [43; 61; 66], providing extreme acceleration for sampling from DMs with even one single step.

Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffusion process, which we show to be more robust in comparing distributions with misaligned supports (Section 3.2). We also reveal non-trivial connections of our method to existing works such as DreamFusion (Section 3.3.1) and generative adversarial training (Section 3.3.2). Interestingly, we show that the SDS objective can be seen as a special case of our Diff-Instruct on the scenario that the generator outputs a Dirac's Delta distribution.

To demonstrate the effectiveness and universality of Diff-Instruct, we consider two scenarios mentioned earlier: distilling pre-trained diffusion models to single step (Section 4.1) and improving pre-trained GAN generators (Section 4.2). The experiments on distilling pre-trained diffusion models on the ImageNet dataset of a resolution of \(64 64\) show that Diff-Instruct results in state-of-the-art single-step diffusion-based models over both diffusion distillation, such as the consistency distillation  and direct training methods [66; 83; 73]. The experiments on improving GAN generators show that the Diff-Instruct can consistently improve the pre-trained generators across various settings.

Preliminary

Assume we observe data from the underlying distribution \(p_{d}()\). In generative modeling, we want to generate new samples \( p_{d}()\), where there are mainly two approaches, explicit and implicit. Currently, DMs are the most powerful explicit models while GANs are the most powerful implicit models.

Diffusion models.The forward diffusion process of DM transforms any initial distribution \(p^{(0)}\) towards some simple noise distribution,

\[d_{t}=(_{t,t})t+G(t)d_{t},\] (2.1)

where \(\) is a pre-defined drift function, \(G(t)\) is a pre-defined scalar-value diffusion coefficient, and \(_{t}\) denotes an independent Wiener process. A multiple-level or continuous-indexed score network \(_{}(,t)\) is usually employed in order to approximate marginal score functions of the forward diffusion process (2.1). The learning of marginal score functions is achieved by minimizing a weighted denoising score matching objective [69; 65],

\[_{DSM}()=_{t=0}^{T}w(t)_{_{0} p^{(0)},_{t}|_{0} p_{t}(_{t}|_{0})}\|_{}( _{t,t})-_{_{t}}p_{t}(_{t}|_{0})\|_{2}^{2 }t.\] (2.2)

Here the weighting function \(w(t)\) controls the importance of the learning at different time levels and \(p_{t}(_{t}|_{0})\) denotes the conditional transition of the forward diffusion (2.1). High-quality samples from a DM can be drawn by simulating SDE which is implemented by learned score network . However, the simulation of an SDE is significantly slower than that of other models such as implicit models.

Generative adversarial networks.GANs are representative implicit generative models [30; 75; 77; 67]. They leverage neural networks (generators) to map an easy-to-sample latent vector to generate a sample. Therefore they are efficient. However, the training of GANs is challenging, particularly because of the reliance on adversarial training. To train a GAN model, a neural discriminator \(h\) is optimized to distinguish the data and generated samples. This leads to the creation of a surrogate probability metric \(D_{h}(,)\) between \(p_{g}()\) and \(p_{d}()\). The generator is updated based on this metric, with the aim of improving the quality of the generated samples [19; 2; 44]. The objective of a most commonly used GAN  can be written as

\[_{h}=-_{x p_{d}}[\;h()]- _{z p_{z}}[(1-h(g()))],_{g}=-_{z p_{z}}[\;h(g())],\]

where the training alternates between minimizing \(_{h}\) and \(_{g}\) with the other part fixed. For a fixed \(g\), the optimal \(h\) should recover the density ratio \(p_{d}()/(p_{d}()+p_{g}())\), and in turn, \(D_{h}\) is the Jensen-Shannon divergence. There are variants of objectives that minimize other divergences. For instance, if the \(_{g}^{(KL)}=_{z p_{z}}[ ))}{h(g())}]\), the objective aims to minimize the KL divergence between generator and data distribution. In Section 3.3.2, we establish the equivalence of our Diff-Instruct with the adversarial training that aims to minimize the KL divergence.

Neural radiance fields.The neural radius field (NeRF)  is a kind of 3D object model that uses a multi-layer-perceptron (MLP) to map coordinates of a mesh grid to volume properties such as color and density. Given the camera parameters, a rendering algorithm can output a 2D image that is a view projection of the 3D NeRF. The rendering algorithm is usually differentiable to learnable parameters of NeRF's MLP, this makes the NeRF can be updated through proper instructions on the rendered 2D image.

## 3 Diff-Instruct

The main goal of Diff-Instruct is to transfer the knowledge of a pre-trained DM to other generative models. To demonstrate the universality of our approach, we consider the more general case where the student model is an implicit model, i.e., a generator.

Problem setup.Recall our setting that we have a pre-trained diffusion model with the multi-level score net denoted as \(_{p^{(t)}}(_{t})_{_{t}}p^{(t)}( _{t})\) where \(p^{(t)}(_{t})\)'s are the underlying distributions diffused at time \(t\) according to (2.1). Assume the pre-trained diffusion model provides a sufficiently good approximation of data distribution, i.e., \(p^{(0)} p_{d}\). For ease of mathematical treatment, we use \(p^{(0)}\),\(p_{d}\) interchangeably. The goal of our Diff-Instruct is to train an implicit model \(g_{}\) without any training data, such that the distribution of the generated samples, denoted as \(p_{g}\), matches that of the pre-trained DM. The instruction process involves minimizing certain probability divergences between the implicit distribution and the data distribution.

Instruction criterion.In order to receive supervision from the multi-level score functions \(_{p^{(t)}}(_{t})\), introducing the same diffusion process to the generated samples seems inevitable. Consider diffusing \(p_{g}\) along the same forward process as the instructor DM and let \(q^{(t)}\) be the corresponding densities at time \(t\). Let \(_{q^{(t)}}(_{t})\!:=\!_{_{t}} q^{(t)}(_{t})\) be the marginal score functions. At each time level, how to design the instruction criterion and how to combine different time levels are of critical importance. To this end, we consider integrating the Kullback-Leibler divergence along the forward diffusion process with a proper weighting function, such as in (2.2). The resulting Integral Kullback-Leibler (IKL) divergence is a valid probability divergence with two important properties: 1) IKL is more robust than KL in comparing distributions with misaligned supports; 2) The gradient of IKL with respect to the generator's parameters only requires the marginal score functions of the diffusion process, making it a suitable divergence for incorporating the scoring network of pre-trained diffusion models.

In the following sections, we first formally define the IKL and then go into detail about the mathematical ground of our Diff-Instruct algorithms. We then establish the connections of Diff-Instruct to existing methods and discuss in detail a novel application of Diff-Instruct on data-free diffusion distillation, together with a comparison to existing diffusion distillation approaches.

### Integral Kullback-Leibler divergence

The IKL is tailored to incorporate knowledge of pre-trained DMs in multiple time levels. It generalizes the concept of KL divergence to involve all time levels of the diffusion process.

**Definition 3.1** (Integral KL divergence).: Given a diffusion process (2.1) and a proper weighting function \(w(t)\!>\!0\), \(t\!\![0,\!T]\), the IKL divergence between two distributions \(p\),\(q\) is defined as

\[^{[0,T]}_{IKL}(q,p)\!:=\!_{t=0}^{T}\!w(t)_{KL}(q^{(t )},p^{(t)})t\!:=\!_{t=0}^{T}\!w(t)_{_{t} q^{ (t)}}(_{t})}{p^{(t)}(_{t})}t,\] (3.1)

where \(q^{(t)}\) and \(p^{(t)}\) denote the marginal densities of the diffusion process (2.1) at time \(t\) initialized with \(q^{(0)}\!=\!q\) and \(p^{(0)}\!=\!p\) respectively.

The IKL divergence integrates the KL divergence along a diffusion process, which enables us to amalgamate knowledge from pre-trained DM at multiple diffusion times. For simplicity, we use the notation \(_{IKL}(p,q)\) to represent \(^{[0,)}_{IKL}(p,q)\) if the integral exists. Since the KL divergence is well-defined, the proposed IKL divergence, as the integral of KL divergence, is also well-defined.

**Proposition 3.2**.: The \(_{IKL}(q,p)\) satisfies that \(_{IKL}(q,p)\!\!0\), \( q,p\). Furthermore, the equality holds if and only if \(q\!=\!p\), almost everywhere under measure \(p\).

Figure 1: Illustration of our _Diff-Instruct_ pipeline. The generator accepts instructions from all diffusion time levels to calculate the gradient of the Integral KL divergence. The gradient is used to update its parameters. This data-free learning scheme enables us to employ pre-trained DMs as teachers to instruct a wide variety of generative models.

One of the advantages of using IKL instead of KL is its robustness. For instance, with proper weighting function, the IKL is well-defined even when the vanilla KL divergence degenerates to infinity. This demonstrates that the IKL divergence is more robust than the KL divergence for two distributions with misaligned supports. We consider a famous example in  where the generator distribution \(p_{g}\) and target distribution \(p_{d}\) have disjoint support. In this case, the KL divergence between \(p_{g}\) and \(p_{d}\) degenerates to positive infinity, while the IKL divergence has a finite value for all generator parameters and unique minima that match the generator and data distribution. Check Appendix A.1 for details.

### Instruct algorithm

Let \(g_{}\) be the generator of the implicit model. Let \(q^{(0)}\) denote the implicit distribution for samples which are obtained by \(_{0}\!=\!g_{}(),\!\!p_{z}\) and denote \(q^{(t)}\) as the marginal distribution of the forward SDE ((2.1)) initialized with \(q^{(0)}\). Let \(\{p^{(t)}\}_{t[0,]}\) and \(\{_{p^{(t)}}(.)\}_{t[0,]}\) represent the marginal densities and score functions of the pre-trained diffusion model. Our Diff-Instruct aims to minimize the IKL between \(q^{(0)}\) and \(p^{(0)}\) so as to update the generator's parameters. Following the notations of definition (3.1), we give a non-trivial gradient formula for minimizing the IKL in Theorem 3.3 that includes only the score functions.

**Theorem 3.3**.: The gradient of the IKL in (3.1) between \(q^{(0)}\) and \(p^{(0)}\) is

\[()\!=\!_{t=0}^{T}w(t)_{  p_{x},_{0}=g_{}(),\\ _{t}|_{0} p_{t}(_{t}|_{0}.)} [_{}(_{t},\!t)\!-\!_{p^{(t)}}(_{t})] _{t}}{}t.\] (3.2)

Theorem 3.3 gives an explicit gradient to minimize the IKL divergence w.r.t. the parameter of the generator. Note that the gradient estimation only requires the marginal score functions \(_{p^{(t)}}\) and \(_{q^{(t)}}\). If the marginal score functions of the implicit distribution can be approximated by another diffusion model \(_{}(_{t},\!t)\), we can utilize the gradient formula (3.2) to update the generator's parameter \(\).

Now we formally propose _Diff-Instruct_ as in Algorithm 1, which trains the implicit model through two alternative phases between learning the marginal score functions \(_{}\), and updating the implicit model with gradient (3.2). The former phase follows the standard DM learning procedure, i.e., minimizing loss function (2.2), with a slight change that the data is generated from the generator. The resulting \(_{}(_{t},\!t)\) provides an estimation of \(_{q^{(t)}}(_{t})\). The latter phase updates the generator's parameter \(\) using gradient from (3.2), where two needed functions are provided by pre-trained DM \(_{p^{(t)}}(_{t})\) and learned DM \(_{}(_{t},\!t)\). When the algorithm converges, \(_{}(_{t},\!t)_{p^{(t)}}(_{t})\), and the gradient (3.2) is approximately zero.

``` Input: pre-trained DM \(_{p^{(t)}}\), generator \(g_{}\), prior distribution \(p_{z}\), DM \(_{}\); forward diffusion (2.1). while not convergedo  update \(\) using SGD with gradient \[()\!=\!\!_{t=0}^{T}w(t) _{ p_{x},_{0}=g_{}(),\\ _{t}|_{0} p_{t}(_{t}|_{0}.)} [_{}(_{t},\!t)\!-\!_{p^{(t)}}(_{t})] _{t}}{}t.\]  end while return\(\),\(\). ```

**Algorithm 1**Diff-Instruct Algorithm

### Connections to existing methods

In this section, we establish the connections of Diff-Instruct to two typical methods, the score distillation sampling proposed in DreamFusion , and the generative adversarial training in Goodfellow et al. .

#### 3.3.1 Connection to score distillation sampling

The score distillation sampling (SDS) algorithm was proposed by Poole et al.  to distill the knowledge of a large-scale text-to-image diffusion model into a 3D NeRF model. The idea of SDS has been applied in various contexts, including the text-to-3D NeRF generation based on text-to-2D diffusion models , and the text-guided image editing .

It turns out that the SDS algorithm is a special case of our Diff-Instruct when the generator's output is a Dirac's Delta distribution with learnable parameters. More precisely, we find that Diff-Instruct's gradient formula (3.2) will degenerate to the gradient formula of SDS under the assumption that the generator outputs a Delta distribution.

**Corollary 3.4**.: If the generator's output is a Dirac's Delta distribution with learnable parameters, i.e. \(q(_{0})\!=\!_{g()}(_{0})\)3. Then the gradient formula (3.2) becomes

\[()\!=\!_{t=0}^{T}\!w(t)_{_{t}|_{0} p _{t}(_{t}|_{})}_{_{t}}p_{t}( _{t}|_{_{0}})\!-\!_{p^{(t)}}(_{t})_{t}}{}t.\] (3.3)

(3.3) does not depends on another diffusion model \(_{q^{(t)}}\) as in (3.2). So under the assumption of Corollary 3.4, there is no need for using another DM to estimate the generator's marginal score functions. This is because when the generator outputs a Delta distribution, there is no randomness in \(_{0}\). So the marginal score functions are only determined by \(p_{t}(_{t}|_{0})\). The gradient (3.3) is equivalent to the score distillation sampling proposed in DreamFusion .

The fact that SDS is a special case of Diff-Instruct is not a coincidence, as in DreamFusion, the rendered image of a NeRF model from a certain camera view is a 2D image that is differentiable to NeRF's parameters. Therefore, using SDS to learn a NeRF model is essentially an application of using Diff-Instruct to distill a pre-trained text-to-2D diffusion model in order to obtain a 3D NeRF object. However, the path we obtain (3.3) is totally different from that in DreamFusion. In DreamFusion, the authors obtained (3.3) by taking the data gradient of the diffusion model's loss function (2.2) and _empirically_ omitted the Jacobian term of the pre-trained score network. However, in this work, we first propose the general formulation of Diff-Instruct and then specialize it to obtain SDS in a natural way.

#### 3.3.2 Connection to GANs

Our proposed Diff-Instruct without integral on time is equivalent to the adversarial training  that aims to minimize the KL divergence. Following the same notation as in Section 2, the adversarial training uses a discriminator \(h(.)\) to learn the density ratio to construct the objective for the generator.

**Corollary 3.5**.: If the discriminator \(h\) learns the perfect density ratio, i.e. \(h()=()}{p_{d}()+p_{g}()}\), then updating the generator to minimize the KL divergence (\(_{g}^{(KL)}\) in Section 2) is equivalent to Diff-Instruct with a weighting function \(w(0)\!=\!1\) and \(w(t)\!=\!0,\! t\!>\!0\).

The Diff-Instruct is essentially a different method from adversarial training in three aspects. First, the adversarial training relies on a discriminator network to learn the density ratio between the model distribution and data distribution. However, Diff-Instruct employs DMs instead of discriminators to instruct the generator updates. Second, in scenarios where only a pre-trained diffusion model is available without any real data samples, Diff-Instruct can distill knowledge from the pre-trained model to the implicit generative model, which is not achievable with adversarial training. Third, Diff-Instruct uses the IKL as the minimization divergence, which overcomes the degeneration problem of the KL divergence via a novel use of diffusion processes and can potentially overcome the drawbacks such as mode-drop issues of adversarial training.

### Related works

The pre-trained diffusion models on large-scale datasets contain rich knowledge of the data distribution. There is a growing interest in distilling this knowledge to other models  that are more sampling-efficient, such as implicit generators  and neural radiance fields models .

One significant advantage of our Diff-Instruct framework is its ability to update the generator without real data \(\!\!p_{d}\). Instead, the knowledge of the data distribution to update the generator is containedin the marginal score function \(_{p^{(t)}}\) as in (3.2). This enables our Diff-Instruct to distill knowledge from pre-trained DMs into flexible generators in a data-free manner and provides a major advantage over other diffusion distillation methods that require either the real data or synthetic data from pre-trained DMs. More precisely, There are three kinds of diffusion distillation methods depending on how to use the data for distillation. The first is the _data synthetic distillation_, which requires using the pre-trained DM to synthesize data from random noises. The student model then learns the mechanism between random noise and synthetic data in order to enhance the efficiency of data generation. Representative methods of data-synthetic distillation are Knowledge Distillation (KD ), Rectified Flow (ReFlow ) and DFNO (). The second method does not require synthetic data from diffusion models but involves real data when distilling. The consistency distillation (CD ) is a representative method. The third method is pure _data-free distillation_, which requires neither real data nor synthetic data. The Diff-Instruct and the Progressive Distillation (PD ) are representative pure data-free distillation. Generally, we use the word "data-free distillation" to include both data-synthetic distillation and pure data-free distillation. To give a comprehensive comparison of diffusion distillation methods, we summarize three main features of diffusion distillation methods in Table 6. The efficiency represents the computational cost of the distillation method. Methods that require synthesizing dataset is inefficient. The data-synthetic distillation requires simulations with pre-trained DMs, thus is inefficient. The flexibility represents whether the distillation approach is capable of distilling knowledge of pre-trained DM to flexible generator architectures, for example, the generator whose input and output dimension is different. Diff-Instruct is the only method that can apply to a wide variety of downstream generators.

Furthermore, Diff-Instruct offers very high flexibility to the generator, distinguishing it from traditional diffusion distillation methods that impose strict constraints on the generator selection. For instance, the generator can be a convolutional neural network (CNN)-based or a Transformer-based image generator such as StyleGAN [28; 30; 31; 37], or an UNet-based generator  adapted from pre-trained diffusion models [32; 65; 22]. The versatility of Diff-Instruct allows it to be adapted to different types of generators, expanding its applicability across a wide range of generative modeling tasks. In the experiment sections, we show that Diff-Instruct is capable of transferring knowledge to generator architectures including both UNet-based and GAN generators respectively. To the best of our knowledge, the Diff-Instruct is the first approach to efficiently enable such a data-free knowledge transfer from diffusion models to generic implicit generators.

## 4 Experiments

With the abundance of powerful pre-trained DMs with diverse expertise, our proposed Diff-Instruct unlocks their potential as sources of knowledge to instruct a wide variety of models. To demonstrate, we choose the state-of-the-art DMs from the seminal work by  (we denote as EDMs) as instructors and consider transferring their knowledge to implicit generators. In this section, we evaluate the efficacy of DI through two downstream applications: diffusion distillation and improvement of GAN's generator. These two experiments correspond to using UNet and GAN's generator to absorb the knowledge from DMs. In the diffusion distillation experiments, we use Diff-Instruct to distill pre-trained DMs to single-step generative models. On the ImageNet \(64 64\) dataset, our Diff-Instruct achieves state-of-the-art performance in terms of FID among all single-step diffusion-based generative models. Furthermore, in the GAN-improving experiments, we use Diff-Instruct to improve existing GAN models that are pre-trained to convergence with adversarial training. The Diff-Instruct is shown to be able to consistently enhance the generative performance of pre-trained StyleGAN-2 models on the CIFAR10 dataset by incorporating knowledge of pre-trained DMs.

### Single-step diffusion distillation

Diffusion distillation is a hot research area that aims to accelerate the generation speed of diffusion models. In our experiments, we utilize our Diff-Instruct framework to train single-step generators on CIFAR-10  and ImageNet \(64 64\) from pre-trained EDM  models. We evaluate the performance of the trained generator via Frechet Inception Distance (FID) , the lower the better, and Inception Score (IS) ), the higher the better. For additional details about the generator's architecture, pre-trained models, and the hyper-parameters on our experimental setup, please refer to Appendix B.1.

Performances.Table 1 and 2 summarize the FID and IS of the single-step generator that we trained with Diff-Instruct from pre-trained EDMs on the CIFAR10 datasets (unconditional without labels)and the conditional generation on the ImageNet \(64 64\) data. Diff-Instruct performs competitively across all datasets among single-step and multiple-step diffusion-based generative models, which involve both models from diffusion distillation or direct training.

As shown in Table 2, on the ImageNet dataset of the resolution of \(64 64\), Diff-Instruct outperforms diffusion-based single-step generative models in terms of FID, including both distillation methods that require real data or synthetic data, and even models that are trained from scratch. On the unconditional generation of the CIFAR10 dataset, Diff-Instruct achieves the state-of-the-art IS among diffusion-based single-step generative models but achieves the second-best FID, only worse than the consistency distillation (CD)  which requires both real data for distillation and the learned neural image metric (e.g. LPIPS ). The conditional generation experiment on the CIFAR10 dataset shows that the Diff-Instruct performs better than a 20-NFE diffusion sampling from EDM model  with Euler-Maruyama discretization but worse than a 20-NEF Heun discretization.

  METHOD & NFE (\(\)) & FID (\(\)) & IS (\(\)) \\  \\  DDPM  & 1000 & 3.17 & 9.46 \\ LSGM  & 147 & 2.10 \\ FFGM  & 110 & 2.35 & 9.68 \\ EDM  & 35 & **1.97** & \\ DDIM  & 50 & 4.67 & \\ DDIM  & 10 & 8.23 & \\ DPM-solver-2  & 12 & 5.28 & \\ DPM-solver-3  & 12 & 6.03 & \\
3-DEIS  & 10 & 4.17 & \\ UniPC  & 8 & 5.10 & \\ UniPC  & 5 & 23.22 & \\ Denoise Diffusion GAN(T=2)  & 2 & 4.08 & **9.80** \\ PD  & 2 & 5.58 & 9.05 \\ CT  & 2 & 5.83 & 8.85 \\ CD  & 2 & 2.93 & 9.75 \\
**Single Step** & & & \\  Denoise Diffusion GAN(T=1)  & 1 & 14.6 & 8.93 \\ KD\({}^{*}\) & 1 & 9.36 & \\ TDPM  & 1 & 8.91 & 8.65 \\
1-Reflow  & 1 & 378 & 1.13 \\ CT  & 1 & 8.70 & 8.49 \\
1-Reflow (+distill)\({}^{*}\) & 1 & 6.18 & 9.08 \\
2-Reflow (+distill)\({}^{*}\) & 1 & 4.85 & 9.01 \\
3-Reflow (+distill)\({}^{*}\) & 1 & 5.21 & 8.79 \\ PD  & 1 & 8.34 & 8.69 \\ CD-L2\({}^{}\) & 1 & 7.90 & \\ CD-LPIPS\({}^{}\) & 1 & **3.55** & 9.48 \\
**Diff-Instruct** & 1 & 4.53 & **9.89** \\  

Table 1: Unconditional sample quality on CIFAR-10 through diffusion generations. \({}^{*}\)Methods that require synthetic data construction for distillation. \({}^{}\)Methods that require real data for distillation.

  METHOD & NFE (\(\)) & FID (\(\)) \\  \\  EDM  & 35 & **1.79** \\ EDM-Heun  & 20 & 2.54 \\ EDM-Euler  & 20 & 6.23 \\ EDM-Heun  & 10 & 15.56 \\ 
**Single Step** & & \\  EDM  & 1 & 314.81 \\
**Diff-Instruct** & 1 & **4.19** \\   \\  METHOD & NFE (\(\)) & FID (\(\)) \\  \\ 
**Multiple Steps** & & \\  ADM  & 250 & **2.07** \\ SN-DDIM  & 100 & 17.53 \\ EDM  & 79 & 2.44 \\ EDM-Heun & 10 & 17.25 \\ GGDM  & 25 & 18.4 \\ CT  & 2 & 11.1 \\ PD  & 2 & 8.95 \\ CDâ€™  & 2 & 4.70 \\
**Single Steps** & & \\  EDM & 1 & 154.78 \\ PD  & 1 & 15.39 \\ CT  & 1 & 13.00 \\ CD-L2\({}^{}\) & 1 & 12.10 \\ CD-LPIPS\({}^{}\) & 1 & 6.20 \\
**Diff-Instruct** & 1 & **5.57** \\  

Table 2: Class-conditional sample quality on CIFAR-10 and ImageNet \(\) through diffusion generations. \({}^{*}\)Methods that require synthetic data construction for distillation. \({}^{}\)Methods that require real data for distillation.

Figure 2: Generated samples from one-step generators that are distilled from pre-trained diffusion models on different datasets. _Left_: FFHQ-\(64\) (unconditional); _Mid_: ImageNet-\(64\) (conditional); _Right_: CIFAR-10 (unconditional).

Figure 2 shows some non-cherry-picked generated samples from a single-step generator trained with Diff-Instruct on the FFHQ , ImageNet , and the CIFAR10  datasets of the resolution of \(64 64\). In conclusion, our Diff-Instruct can achieve competitive distillation performance under the most challenging conditions with no synthetic or real datasets. We put more discussions and analyses in the Appendix B.1.

**Remark 4.1**.: In Table 1 and Table 2, PD, CD, and Diff-Instruct all use the EDM teacher, and the same UNet student with the same architecture as the teacher model (The author of consistency distillation re-implemented the PD for EDM, so the reported FID for PD is lower than that in PD's original paper). The Diff-Instruct generator uses the same UNet architecture as the teacher diffusion model, so the number of sampling steps (NFE) represents its inference time costs. As we show in the upper part of Table 2, sampling from our learned one-step generator with 1 NFE results in an FID of 4.19, which is significantly better than its teachers with 10 NFEs with an FID of 15.56. So we conclude that the one-step generator trained with Diff-Instruct achieves at least 10+ times acceleration (more efficient) than its teacher diffusion model.

Fast convergence speed.Another advantage of applying Diff-Instruct for diffusion distillation is the fast convergence speed. We empirically find that Diff-Instruct has a much faster convergence speed than other distillation methods and has a tolerance for a large learning rate for optimization. In Figure 3, we show the convergence of FID with respect to the optimization iterations of the generator trained on an unconditional DM on the CIFAR10 dataset. We set the optimization step size of Diff-Instruct to be \(1e-4\) and the FID of the generator trained with Diff-Instruct converges fast within 7k iterations. However, the FID of the distilled diffusion model with consistency distillation algorithms does not converge with less than 7k iterations. One possible reason for the fast convergence speed is that Diff-Instruct's student model is a one-step generator that does not need to take multiple-time indexes in contrast to student models of other distillation methods such as CD and PD. This makes Diff-Instruct efficient when distillation without the need for learning at multiple time levels.

### Improving generative adversarial networks

Another application of Diff-Instruct is to improve the generator of GAN models that are pre-trained with adversarial training 3.3.2.

Experiement settings.We take the pre-trained EDM model  on the CIFAR10 datasets as the instructor and the pre-trained StyleGAN-2  models that are assumed to converge with adversarial training under different settings (conditional or unconditional with different data augmentation strategies). Our goal is to use pre-trained DMs to further improve pre-trained generators. We initialize the generator in the Diff-Instruct algorithm with the pre-trained StyleGAN-2 generator and initialize DMs for implicit distributions with the pre-trained EDM models. We then use Diff-Instruct to update the generator. We put more details in Appendix B.2.

Performance.As shown in Table 3 and 4, our Diff-Instruct can consistently improve the pre-trained generator's performance in terms of FID. More precisely, the FID of the pre-trained StyleGAN-2 with Adaptive Data Augmentation (ADA) is improved from 2.42 to 2.27 for the conditional setting and

Figure 3: Comparison of FID convergence on diffusion distillation of CIFAR10 unconditional EDM model . The FID value of consistency distillation is obtained from its original paper .

from 2.92 to 2.71 for the unconditional setting. The experiment shows that Diff-Instruct is able to inject the knowledge of pre-trained diffusion models to enhance the generators further.

The results demonstrate that Diff-Instruct is a powerful method capable of improving existing GAN models that are supposed to converge with adversarial training. There are two possible reasons for such improvements. First, our Diff-Instruct utilizes well-trained diffusion models to supervise the generator. For instance, on the CIFAR10 dataset with conditional labels, the teacher EDM model can achieve the FID of \(1.79\), which is significantly better than StyleGAN2 with an FID of \(2.42\). Second, Diff-Instruct takes diffused data into account when minimizing the IKL, overcoming the potential degeneration issues of the divergences that adversarial training intends to minimize.

## 5 Discussion

This work presents a novel learning paradigm, Diff-Instruct, which is to our best knowledge, the first method that enables knowledge transfer from pre-trained diffusion models into generic generators in a data-free manner. The theoretical foundations and practical methods introduced in this work hold promise for advancing the utilization of diffusion generative models and implicit models across various domains and applications.

Nonetheless, Diff-Instruct has its limitations that call for further research along this line. First, with the abundance of powerful pre-trained DMs with diverse expertise, levering multiple models as instructors is another promising direction that is not investigated in this work. Second, even though data-free is a feature of our method, utilizing real data can potentially boost the learning process. The potential benefits of incorporating both Diff-Instruct and training data have not been explored yet. Lastly, in the extreme case where we have only data and no pre-trained DMs, our DI framework can still be adapted. Potentially we can train a teacher diffusion model with data, and concurrently, use it to instruct the student model. This indirect way of training may enable other generative models such as GANs, to enjoy the benefits of diffusion models, e.g., ease of training, ability to scale, and high sample quality, etc.