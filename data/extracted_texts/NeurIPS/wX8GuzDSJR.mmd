# What can a Single Attention Layer Learn?

A Study Through the Random Features Lens

 Hengyu Fu

Peking University

2100010881@stu.pku.edu.cn

&Tianyu Guo

UC Berkeley

tianyu_guo@berkeley.edu

&Yu Bai

Salesforce AI Research

yu.bai@salesforce.com

&Song Mei

UC Berkeley

songmei@berkeley.edu

Equal contributions.

###### Abstract

Attention layers--which map a sequence of inputs to a sequence of outputs--are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with activation function replaced by ReLU, which have recently shown comparable performance with the original Softmax activation. We consider the _random feature_ setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.

Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages over standard fully connected random-feature models; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer. Additionally, we show that the sampling distribution of the _query-key_ matrix (the product of the query and key matrix) matters--A _biased_ Gaussian random matrix results in better sample complexities over the standard zero-mean counterpart for learning certain natural target functions.Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function.

## 1 Introduction

The transformer architecture  has achieved remarkable recent successes in many areas of artificial intelligence (AI) such as vision, language, speech, graph processing, reinforcement learning, and more recently general AI capabilities . A central building block in transformers is the _attention layers_--sequence-to-sequence mappings that allow each token within the input sequence to "attend to" other tokens that are most relevant to the present token, and produce outputs based on those tokens. Attention layers implement this mechanism in a compact way that allows them to handle sequences of arbitrary length using a fixed set of parameters, a crucial reason behind their success in handling long input sequences.

Despite its wide applicability, the theoretical properties of attention layers are less well understood. While multi-layer attention networks (transformers) have been shown to be universal approximators for certain classes of functions, such as equivariant sequence-to-sequence functions , their results only focus on the expressive power and do not account for learning from finite samples. Another line of work derives generalization bounds for learning with _multi-layer_ transformers in terms of the number of layers, heads, and weight norms , yet the results are either instantiated on specific target functions such as sparse boolean functions , or generic but arguably elusive function classes such as Turing machines . Understandings about the more basic building block--a _single_ attention layer-- remain largely open. This is in stark contrast with the situation for fully connected neural networks, where there is by now a decent understanding of the learning and generalization in the important basic case of _two-layer_ neural networks on generic and natural function classes (e.g.,  and many other results along the line). This motivates the following open question:

_What function classes can be learned by a **single attention layer** with benign sample complexities?_

This work makes progress on this problem by studying the learning and generalization with a single attention layer in the _random feature_ setting , in which the query and key matrices are frozen at their random initialization, and the value matrices remain to be learnable parameters. Motivated by the attention structure in practical architectures, we consider attention layers that take in a single _query token_\(_{0}^{d}\) and \(N\)_key tokens_\(\{_{i}\}_{i[N]}\) as the input, and produce a scalar-valued output--A simplified setting capturing the essence (the interaction between the query and keys) of attention models. We study the sample complexity of learning certain target functions (of \(_{0:N}\)) using an attention layer with a large but finite number of heads, and finitely many samples.

Our contributions are summarized as follows.

* We show that a Random Feature Attention layer (the RFA model) with a sufficiently large number of heads can express a broad class of target functions that are averages over a generic function of two tokens, which are in particular permutation invariant with respect to the key tokens (Section 3.1). We give several natural examples of target functions in this class (Section 3.3) with concrete bounds on the number of heads and weight norms.
* We derive an \(}()/n})\) excess risk bound for learning with the RFA model with sufficiently many heads, where \(B(f_{})\) is an inherent complexity measure of the target function \(f_{}\) and \(n\) is the sample size (Section 3.2). When instantiated on the aforementioned examples, the bounds only depend on the input dimension and not the number of key tokens, improving over a naive two-layer random feature neural network model (RFMLP). Such improvement is expected due to the permutation invariance structure of target functions, aligning with the attention mechanism.
* Towards moving beyond standard random feature settings, we study a _biased_ RFA model where the query-key matrices (product of transposed query matrices and key matrices) are drawn from a distribution with a non-zero mean (more precisely the identity matrix as the mean), motivated by a similar observation on learned attention layers in practice. We show that this model achieves provably superior sample complexity than the standard zero-mean RFA for learning certain functions of the _correlations_ between the query token and key tokens (Section 4).
* Experiments on simulated data verify our theoretical findings in realistic settings of learning from finite samples using a RFA layer with a mild number of heads, and characterize the interplay between the complexity of the target function and the sample size (Section 5).

### Related work

Transformers The Transformer architecture, initially proposed by , brought about a revolutionary change in natural language processing and has been widely adopted in large language models such as GPT and BERT . At the core of transformers lie the _attention layers_, which were originally introduced as neural network modules for machine translation tasks .

A recent line of work investigated the capabilities of transformers by viewing transformers to be function approximators , computational models , or algorithms , and using transformers to perform synthetic reasoning tasks . Among these works, the closest to our work is , which shows that multi-layer transformers can approximate any permutation-equivariant sequence-to-sequence function, and any function if positional encodings are added. Ourpaper instead uses a single attention layer to approximate sequence-to-scalar functions and focuses on the generalization property with quantitative bounds.

In terms of generalization properties of transformers,  analyzed the generalization bound of a single attention network through the Rademacher complexity and showed that a single self-attention head could efficiently represent a sparse function of the input sequence. Besides, several works studied the sample complexities of vision transformers [48; 50], and prompt-tuning using attention  with special target function classes. Our paper also studies the generalization bound of a single attention network, but from the different perspective of kernel methods, and for a more general class of target functions. The kernel limit of transformers was derived in [43; 98], which shows that multi-head attention architectures behave as Gaussian processes as the number of heads tends to infinity. However, they do not study the representation power of the limiting kernel.

Besides approximation and generalization capabilities, recent work also studied the limitations [42; 14], internal working mechanisms [36; 81; 94; 64], and in-context learning capabilities [19; 96; 37; 88; 3; 26; 41; 52] of Transformer models.

Theory of random features and neural tangent kernelsA recent line of work [28; 51; 33; 32; 6; 7; 104; 66; 25] studied the training dynamics of overparametrized neural networks under certain random initialization, and showed that it converges to a kernel estimator, which corresponds to the "neural tangent kernel" (NTK). These works suggested that one could use kernel or random-feature models  to study the properties of deep neural networks.

For NTK of MLPs and their corresponding random-feature models, there is a vast number of literature that studies their approximation power [12; 71; 8], as well as their generalization properties [13; 21; 93; 92; 54; 55; 76; 79; 100; 57; 60; 40; 39]. More recently, a line of work studies the NTK beyond MLPs, including convolution networks [53; 17; 59; 62; 18; 16], residual networks [45; 83; 4], graph networks [97; 47], and transformers [43; 98].

Although the kernel approach is a powerful tool for studying neural networks, it received criticism since it does not capture the feature learning of neural networks. Going beyond the kernel regime, a series of works used the mean field method to establish the evolution of the network parameters via a Wasserstein gradient flow [58; 9; 24; 78]. Several other mechanisms have been proven to obtain superior results over the NTK, including the Quadratic NTK [5; 11; 23; 63], regularization , Neural Tangent Hierarchy [34; 44], representation learning , and staircase-like mechanisms [1; 2].

## 2 Preliminaries

We consider a sequence of \(N+1\) input tokens \(_{0:N}=(_{0},\{_{i}\}_{i[N]})= (^{d})^{N+1}\), where each \(\{_{i}\}_{i[N]}^{d}\) represents a sequence of _key vectors_, and \(_{0}\) represents the _query vector_. This model simplifies standard self-attention, which maps \(N\) input tokens to \(N\) output tokens, where the output \(i\) only uses input \(i\) as the query token and all of \([N]\) as key tokens. Results obtained in this model can be directly mapped back to full self-attention, by simply concatenating \(N\) outputs generated by our model with \(_{0}\) ranging over \(\{_{i}\}_{i[N]}\). In addition, throughout the paper, we consider scalar-valued attention models, which take the sequence \(_{0:N}\) as input and give a scalar output in \(\).

Attention layerWe consider a scalar-valued, \(M\)-head, multiplicative attention layer that takes \(_{0:N}=(_{0},\{_{i}\}_{i[N]})\) as the input. The attention layer first applies affine (linear with bias) transformations to the input vectors to obtain {query, keys, values} at each head \(m[M]\):

\[_{m,0}&=_{m}[ _{0};1]=:_{m}}_{0}^{d}, _{m,i}=_{m}[_{i};1]=:_{m} }_{i}^{d},\\ v_{m,i}&=_{m}^{}[_{i};1]= _{m}^{}}_{i},\ \ i[N],\] (1)

where \(_{m},_{m}^{(d+1) d}\), \(_{m}^{d+1}\) are the parameters of the attention layer, and \(}_{i}:=[_{i};1]\) for a more compact display. Then, it computes the output value by an attention mechanism

\[f(_{0:N})=_{m=1}^{M}_{i=1}^{N}f_{m,i}(_ {0},_{i}),\ \ \ \ \ f_{m,i}(_{0},_{i})=(_{m,0},_{m,i}) v_{m,i}.\] (2)

Above, \(:\) is an activation function applied entry-wisely to each attention score \(_{m,0},_{m,i}\). We choose \(\) to be the ReLU activation \((t)=\{t,0\}\) throughout this paper. Notice that this choice of the attention non-linearity is different from standard transformers  with softmax-attention. We remark that we choose to study the (normalized) ReLU attention for theoretical convenience, and this replacement does not change the essence of the attention mechanism. Such a choice is also recently explored in the literatures such as  and , which show that transformers with ReLU-attention perform as well as standard softmax-attention transformers in certain NLP and CV tasks. Moreover, our results can extend to other activation functions such as the exponential activation \((t)=(t)\), which is more similar to the standard Softmax activation. We refer to Section B.5 for a short discussion.

Simplifying the expression, we reparametrize the attention layer (1) and (2) using parameters \(\{(_{m},_{m})\}_{m[M]}^{( d+1)(d+1)}^{d+1}\):

\[f_{i,m}(_{0:N})=}_{0}^{} _{m}_{m}}_{i} _{m},}_{i}= _{m},}_{0}}_{i}^{} _{m},}_{i}\,.\] (3)

For technical convenience, we assume all input tokens have unit norm throughout the rest of the paper: \(\|_{i}\|_{2} 1\) so that \(\|}_{i}\|_{2}\), for all \(i\{0\}[N]\).

Random-feature attention modelsWe consider a random-feature version2 of the multiplicative attention mechanism (3), where the weight matrices \(\{_{m}\}_{m[M]}\) have i.i.d. Gaussian entries3:

\[(_{m})_{ij}N}(0,1/4),\ \ \ (m,i,j)[M][d+1]^{2}.\] (4)

The variance is chosen to be \(1/4\) without loss of generality: this choice of variance is such that \(_{m},}_{0}}_{i}^{ }(0,1)\) has a unit variance. The weight matrices \(\{_{m}\}_{m[M]}\) are then held to be fixed during the entire learning process, whereas the value vectors \(\{_{m}\}_{m[M]}\) are the learnable parameters. The random-feature attention model with input \(_{0:N}\) is thus given by

\[f_{M}^{}(_{0:N};)=_{m=1}^{M} _{i=1}^{N}_{m},}_ {0}}_{i}^{}\,_{ m},}_{i}\,.\] (5)

Notice that random-feature attention model is linear in the parameter \(\), so training this model with a convex loss function gives a convex optimization problem.

Additional notationFor any \(^{d_{1}}\) and \(^{d_{2}}\), let \(^{d_{1} d_{2}}\) denote their tensor product (outer product), and \(^{ n}:=\) denote the \(n\)-fold self tensor product of \(\). For a tensor \(\), we use \(\|\|_{_{t}}\) to denote its Frobenius norm. For a function \(f:\), we use \(\|f\|_{}\) to denote its \(L^{}\) norm. We use \(()\) (resp. \(()\)) for standard Big-O (resp. Big-Theta) relations. We use \(}()\) for hiding the multiplicative terms that are logarithmic in problem parameters, including \((M,d,n,N,^{-1})\). We use \((p)\) to denote a polynomial of \(p\) that is less than \(p^{}\) for some universal constant \(0<C<\).

## 3 Learning with random-feature attention models

In this section, we study the expressivity and generalization of random-feature attention models. We will consider a broad class of target functions that can be well approximated and is efficiently learnable by random-feature attention models.

### Expressivity of random-feature attention

Consider a broad class of permutation invariant4 target functions \(f_{}:\) that takes form

\[f_{}(_{0:N})=_{i=1}^{N}F(_{0},_{i}).\] (6)Assume that there exists symmetric tensors \(\{_{}^{d^{+1}}\}_{r,s 0}\) such that \(F:^{2d}\) admits representation

\[F(_{0},_{i})=_{r,s 0}^{}_{0 }^{ r}_{i}^{ s},_{rs}.\] (7)

Note that such an expression allows \(F(_{0},_{i})\) to be any general nonlinear function that admits convergent Taylor expansions. In particular, any polynomials of \([_{0},_{i}]\) (e.g., \(^{}_{0}\), \(^{}_{i}\), and \(_{0},_{i}\) for some \(^{d}\) and \(^{d^{2}}\)) are within this function class. We will discuss more specific target functions in Section 3.3.

**Theorem 1** (Expressivity of RFA model).: _Suppose function \(f_{}:\) takes form (6). Then for any input distribution \(P\) on \(\), with probability at least \(1-\) (over \(\{_{m}\}_{m[M]}\) sampled from (4)), there exists an \(M\)-head RFA model (5) with coefficients \(=\{_{m}\}_{m[M]}^{d+1}\) that approximates \(f_{}\) in \(L^{2}(P)\) up to error_

\[_{_{0:N} P}f_{}(_{0:N})- f_{M}^{}(_{0:N};)^{2} + M)B(f_{})^{-1}}{M}.\] (8)

_In addition, the norms of the weight of this random-feature attention model are bounded as_

\[_{m=1}^{M}\|_{m}\|_{2})}+)^{-1}}{M}},_{m=1}^{M} \|_{m}\|_{2}^{2} )^{-1}}{M}.\] (9)

_Here \(B(f_{})\) is a complexity measure of \(f_{}\) defined as_

\[B(f_{})=_{k=0}^{}C_{k}_{\{r,s\}=k}\|_{rs }\|_{}^{2}, C_{k}=k^{4.5}4^{k} 1.\] (10)

_In case where \(f_{}\) admits multiple representations of the form (7), \(B(f_{})\) is the infimum of the right-hand-side over all such representations._

The proof of Theorem 1 is contained in Appendix B.1. Our proof relies on standard analyses of infinite-width random feature model with ReLU-Gaussian kernel, combined with a sampling argument to obtain approximation with finite-width.

This theorem is applicable to general functions with a finite \(B(f_{})\) norm. The \(4^{k}\) scaling of \(C_{k}\) in the summand of equation (10) seemingly confines the target function class to those with exponentially fast decaying \(\|_{rs}\|_{}\), which suggests a relatively narrow target function class. However, as we will demonstrate in the forthcoming examples, this class includes a diverse range of functions.

### Generalization and sample complexity of learning

Given \(n\) samples \(\{_{0:N}^{(j)},y_{j}\}_{j[n]}_{}\), where \(_{0:N}^{(j)}=\{_{i}^{(j)}\}_{0 i N}\) is the \(j\)-th token sequence with length \(N+1\), and \(y_{j}\) is the label corresponding to the \(i\)-th token sequence. Assume that we are given a loss function \((,y)\) that is 1-Lipschitz in \(\), and \((0,y) 1\) for any \(y\). The population risk is then given by \(L_{D}(f)=_{(_{0:N},y)}[(f(_{0:N} ),y)]\). We consider the empirical risk minimization (ERM) over the RFA model (5),

\[}=_{_{M}}_{D}(f_ {M}^{}(;)),_{D}(f)=_ {j=1}^{n}(f(_{0:N}^{(j)}),y_{j}),\] (11)

where the constrained class \(_{M}\) is given by

\[_{M}=\{=\{_{m}\}_{m=1}^{M}:\ _{m=1}^{M}\|_{m}\|_{2} K_{1},_{m=1}^{M} \|_{m}\|_{2}^{2} K_{2}/M\},\] (12)

with \(K_{1}\) and \(K_{2}\) being two constants. Theorem 2 below provides the excess risk bound for the empirical risk minimizer.

**Theorem 2**.: _Assume \(M>^{-1}\) and \(n>(dM)\). Let \(f_{}\) be the minimizer of the population risk \(L_{D}(f)\) within the target function class (6) (7). Let \(_{M}^{}=f_{M}^{}(;})\) be the empirical risk minimizer given by (11), where in (12) we choose \(K_{1}=C)}\) and \(K_{2}=CB(f_{})^{-1}\), with \(C\) being a constant. Then for any joint distribution \(\), with probability at least \(1-\) over \(\{_{m}\}_{m[M]}\) sampled according to (4) and \(\{(_{0:N}^{(j)},y_{j})\}_{j[n]}_{} \), the excess risk is bounded by_

\[L_{D}(_{M}^{})-L_{D}(f_{})} )}}+^{ -1}}{M}}.\] (13)The proof of Theorem 2 is contained in Appendix B.2. The proof mostly uses the Rademacher complexity bound for the supremum of empirical process. The main non-trivial technical challenge lies in showing the concentration of \(_{f_{M}}|_{D}(f)-L_{D}(f)|\), which cannot be simply controlled due to the unboundedness of the infinity norm of functions in the target function class \(_{M}\). We dealt with this subtlety by a carefully decomposition of \(_{f_{M}}|_{D}(f)-L_{D}(f)|.\) The seemingly unnatural constraint set (12) is used in bounding different terms in this decomposition.

### Examples and comparison

We next give the sample complexity for learning several examples of target functions using the random-feature attention model. We will compare its sample complexity for learning these functions with that of the standard random-feature model  (thereafter, we call it the random-feature MLP model, in short RFMLP model). In the RFMLP model, we view \(_{0:N}\) as an input vector instead of a sequence of vectors denoted as \((_{0:N})=[_{0};_{1};;_{N};1]^{d(N+1)+1}\). The RFMLP is given by

\[f_{M}^{}(_{0:N};)=_{m=1}^{M} <_{m},(_{0:N})>  v_{m},\{_{m}\}_{m[M]}( ,/(N+2)).\] (14)

We choose the variance of random weights \(_{m}\) to be \(1/(N+2)\) to ensure that \(_{m},(_{0:N})(0,1)\) has unit variance. The generalization and approximation properties of the random-feature MLP model have been well-studied in the literature, for example, .

We instantiate Theorem 2 on three concrete examples of target functions (calculations of the excess risks in Appendix B.4, where the result for RFMLP are adapted5 from Arora et al. ). In all three cases, the target functions are permutation invariant with respect to \(\{_{i}\}_{i[N]}\), by which we naturally expect RFA to achieve better sample complexity than RFMLP in accordance with this structure. Although the comparsion between RFA and RFMLP is based on comparing upper bounds on the sample complexity of both models, existing work has also derived lower bounds on the sample complexity of RFMLP, which aligns with the upper bound for RFMLP we used. We do not invoke these lower bounds, as they apply to a special case with a uniform distributional assumption on the input tokens.

**Example 1** (Functions of \(_{0}\)): We consider functions of \(_{0}\) (no dependence on \(_{1:N}\)) of the form

\[f_{}(_{0:N})=_{k=0}^{}<_{0}^{ k },_{k}>,\ \ _{k}^{d^{k}},B(f_{})=_{k=0}^{ }C_{k}\|_{k}\|_{}^{2}).}\]

By Theorem 2, setting \(M=(d^{2}n)\), the excess risk bound gives \(}(^{}k^{4.5}4^{k}\|_{k} \|_{}^{2}/n})\). \(\)

As a special case, consider \(f_{}(_{0:N})=(^{}_{0})^{p}\), which corresponds to taking \(_{k}=^{ p}\) for \(k=p\) and \(_{k}=\) for \(k p\). The above excess risk of RFA model and the RFMLP model scales as

\[:}(p)\| \|_{2}^{2p}/n},:}(p)\|\|_{2}^{2 p}/n}.\]

Compared to the RFMLP model, the RFA model significantly reduces the necessary sample size by a factor of \((N/4)^{p}\).

**Example 2** (Average of functions of \(_{i}\)): We consider average of functions of \(_{i}\) of the form

\[f_{}(_{0:N})=_{i=1}^{N}_{k=0}^{ }_{i}^{ k},_{k},\ _{k}^{d^{k}},B(f_{ })=_{k=0}^{}C_{k}\|_{k}\|_{}^{2} ).}\]

Theorem 2 then gives an \(}(^{}k^{4.5}4^{k}\|_{k} \|_{}^{2}/n})\) excess risk, same as Example 1. \(\)

As a specific example, consider \(f_{}=_{i=1}^{N}(,_ {i})\) with \((z)=z(z/)\) for some \(>2\), \(\|\|_{2}=1\). Using the power series expansion of \(\), the excess risk bound of RFA model and the RFMLP model scale as

\[:}^{ }k^{4.5}(2/)^{2k}/n}=}(), :}^{}k^ {4.5}[(N+2)/(2)]^{2k}/n}.\]The latter diverges whenever \((N+2)/2\), in which case the bound is meaningless.

**Example 3** (Correlation-weighted functions): \(f_{}\) is the following function:

\[f_{}(_{0:N})=_{i=1}^{N}F(_{0}, _{i})G(_{i}), F(t)=_{k=0}^{ }a_{k} t^{k}, G(_{i})=_{k=0}^{} _{i}^{ k},_{k},\]

for \(^{d d}\), \(\{a_{k}\}_{k 0}\), \(_{k}^{d^{k}}\). This target function fully exploits the representation power of the attention layer. Eq. (10) gives \(B(f_{})=(_{k=0}^{}C_{k}(_{r+s=k}a_{r}^{2}|| ||_{}^{2r}||_{s}||_{_{}}^{2}))\). \(\)

As a specific example, consider \(f_{1,}=_{i=1}^{N}_{0},_{i }^{p}\), corresponding to taking \(=_{d}\), \(F(t)=t^{p}\), and \(G 1\). The excess risk bound of RFA (by Theorem 2) and RFMLP scale as

\[:}(p)/n} ,:}(p) /n}.\]

As another example, consider \(f_{2,}=_{i=1}^{N}(_{0},_{i })_{i}^{ p},\) with \(\|\|_{_{}}=1\). Then the excess risk bound of RFA and RFMLP scale as

\[:}(pd)}4^{p}/n},:} (pNd)}N^{p}/n}.\]

RFA reduces the required sample size by factors of \((N/4)^{p}\) for \(f_{1,}\) and \((N)\) for \(f_{2,}\).

## 4 Expressivity of biased random-feature attention model

We now move beyond the Gaussian weight assumption by exploring alternative possibilities for the weight distribution in the attention heads. We observe empirically that the weight matrices in transformer architectures learned in practice are often more similar to the identity matrix than a mean-zero matrix (Figure 1; see the details in Appendix D.1. This is also observed in a recent and concurrent work ).

Towards understanding this effect, we consider an alternative attention model with _biased_ random weights, where the bias is a fixed matrix \(_{0}^{(d+1)(d+1)}\):

\[f_{M}^{,_{0}}(_{0:N};)=_{m=1}^{M }_{i=1}^{N}_{0}+_{m}, }_{0}}_{i}^{} _{m},}_{i}.\] (15)

Here \(\{_{m}\}_{m[M]}\) are again Gaussian random matrices sampled according to (4). The biased random-feature attention model is similar to (5) except that a bias weight \(_{0}\) is added. Motivated by our observation, we choose \(_{0}=[_{d d},_{d 1};_{1  d},0]^{(d+1)(d+1)}\), so that the diagonal elements of \(_{0}+_{m}\) will be on average larger than the off-diagonal elements.

Figure 1: Visualization of weight matrices of the 2nd, 5th, 8th, and 11th layers of the BERT-Base model. Each row contains weight matrices of a layer. All matrices are clipped to the top-left \(32 32\) block. Lighter color indicates a larger absolute value.

### Expressivity of biased random-feature attention

Given the formulation of biased random-feature attention models (thereafter, we call it the biased random-feature attention model, in short BRFA model), a natural conjecture is that this model can better fit functions that are the average of function of \(_{0},_{i}\). We here show that this is indeed the case. In particular, we consider a broad class of target functions \(g_{}:\) that take forms

\[g_{}(_{0:N})=_{i=1}^{N}F( _{0},_{i})G(_{0},_{i})\] \[F(t)=_{k=0}^{}a_{k}t^{k}, G(_{0}, _{i})=}_{i}^{ 3} }_{0}^{ 2},_{}.\] (16)

Here the scalars \(\{a_{k}\}_{k 0}\) and the tensor \(_{}^{d^{8}}\) parameterizes \(g_{}\). As we will explain in Section 4.3, confining \(G\) to be a degree-\((3,2)\) polynomial in \((_{i},_{0})\) is essential to our theoretical results. Our next theorem provides the excess risk of learning target function \(g_{}\) using the BRFA model (15).

**Theorem 3**.: _Given the same setting and assumptions as in Theorem 2, when the population risk minimizer gives \(f_{}=g_{}\), with probability at least \(1-\), we have_

\[L_{D}(_{M}^{,_{0}})-L_{D}(g_{})= {}_{L},L)}}+^{-1}}{M}}+_{L}\|g_{} \|_{},\] (17)

_where \(_{L}=1/[2^{L+1}(L+1)!]\) and_

\[B(g_{},L)=\|_{}\|_{_{}}^{2 }(_{k=0}^{}|a_{k}| C_{k})^{2},C_{k}=(2L+k)^{(k+3)/2}8^{L+k/2}.\] (18)

The proof of Theorem 3 is contained in Appendix C. We provide the intuitions of the result and an overview of the proof technique in Section 4.3.

### Examples and comparison

Compared to the target functions (7) discussed in Section 3.1, functions in (16) may not express the average of arbitrary functions of \(_{0}\) and \(_{i}\), but are well-suited to express functions of correlations. Consequently, we anticipate that the BRFA model will outperform the RFA model in learning functions of correlations. We will now present three concrete examples of target functions (16), and compare the excess risk of the BRFA model to that of the RFA model. The proof of excess risk is contained in Appendix C.3.

**Example 4** (Low degree polynomials): Consider average of polynomials of \(_{i}\) and \(_{0}\),

\[g_{}=_{i=1}^{N}_{i}^{ 3} _{0}^{ 2},,B(g_{},L)=\| \|_{_{}}^{2}L^{3}8^{2L}}.\]

For any \(>0\), if we take \(n(((1/)))\), \(L=((1+ n)^{-1} n)\), and \(M=(d^{2}n)\), the excess risk will scale as \(}(\|_{_{}}^{2}/n^{ 1-}})\). \(\)

Compared with the excess risk of the RFA model as detailed in Example 2, the excess risk bound of the BRFA model loses a factor of \(n^{-/2}\).

**Example 5** (Functions of correlations): Consider a special case of functions of correlations,

\[g_{}=_{i=1}^{N}_{0},_{i} ^{p},_{i}, ^{d-1},\ B(g_{},L)=(2L+p)^{p+3}8^{2L+p}\ }.\]

For any \(>0\), choosing the same parameters \((n,L,M)\) as Example 4, the excess risk bound scales as \(}(8^{p}/n^{1-}})\). \(\)

Consider the required sample size \(n_{}\) to reach an accuracy of \(0.01\). The BRFA model requires \(n_{}=}((8p+48)^{p+3})\), whereas the RFA model requires \(n_{}=}((4d)^{p})\). Thus, in comparison to the RFA model, the BRFA model can reduce the required sample size by a factor of \(}([d/(2p+12)]^{p})\).

**Example 6** (Correlation-weighted functions): Consider the function

\[g_{}=_{i=1}^{N}(_{0},_ {i})_{i}^{ 3},, \|\|_{_{}}^{2} 1 B(g_{},L)=((8e)^{2L}),\]where \(B(g_{},L)\) is bounded through the Taylor expansion of \((t)\) and (18). For any \(>0\), choosing the same parameters as Example 4, the excess risk bound scales as \(}(})\). \(\)

Consider the required sample size \(n_{}\) to reach an accuracy of \(0.01\). The BRFA model requires \(n_{}=}(1)\), whereas the RFA model requires \(n_{}=}((d)())\). Thus, in comparison to the RFA model, the BRFA model can reduce the required sample size by a factor of \(}((d)())\).

### Overview of techniques

Here we provide the intuition and an overview of the technique of Theorem 3, with the proof details in Appendix C. To show the sample complexity of learning with the BRFA model, the first step is to derive the kernel \(K_{}(_{0:N},_{0,N}^{})\) associated with the infinite-width BRFA model. This kernel has a natural feature map, given by \(\{_{k}:^{d^{2k+1}}\}_{k 0}\), where

\[_{k}(_{0:N})=_{i=1}^{N}(_{0},_{i})_{k-2}(_{0},_{i} )}_{i}^{ k+1}}_{0}^{ k}, k 2.\]

Here \((t)=(2)^{-1/2}e^{-t^{2}/2}\) is the Gaussian density function, and \(_{k}(z)\) denotes the \(k\)-th probabilist's Hermite polynomial, with detailed expression and properties given in Appendix A.1. This feature map implies the learnability of the following target function class by the BRFA model,

\[_{}(_{0:N})=_{i=1}^{N}( _{0},_{i})_{k=2}^{}_{k -2}(_{0},_{i})}_{i}^{ k+1}}_{0}^{ k}, _{k},\] (19)

whose RKHS norm associated with kernel \(K_{}\) is bounded by \(B(_{})=_{k=2}^{}(k-2)!k^{2}4^{k}\|_{k} \|_{}^{2}\).

Notice that \(_{}\) bears similarities to, but also distinct differences from, \(g_{}\) as presented in (16). The key difference lies in the \((_{0},_{i})\) factor in \(_{}\), which is hard to interpret and analyze. To obtain the excess risk bound for learning \(g_{}\), we can use \(_{}\) to approximate \(g_{}\) in the \(L^{}\) norm. The excess risk for learning \(g_{}\) can be bounded by the summation of the excess risk for learning \(_{}\) and the approximation error. Acquiring this approximation error bound necessitates a truncation argument of the Taylor expansion of \(1/()\).

## 5 Numerical experiments

We test our theory by experimentally approximating two types of target functions using the three models under investigation RFA (5), BRFA (15), and RFMLP (14). We choose the target functions to be of form

\[f_{1,p}(_{0:N}) =_{i=1}^{N},_{i}^{p}, p,^{d-1},\] (20) \[f_{2,q}(_{0:N}) =_{i=1}^{N}_{0}, _{i}^{q},_{i}, q,^{d-1}.\] (21)

The first target function (20) is a specific instance of Example 2, whereas the second target function (21) has been considered in both Example 3 and 5.

In our experimental setup, we set the input dimension as \(d=16\) and the number of tokens as \(N=16\). We fix the width of RFA and BRFA to be \(M_{}=M_{}=M=1000\), whereas the width of RFMLP is set as \(M_{}=M(d+1)=17000\). This configuration ensures an equal number of parameters across all three models. To further accentuate the test risk difference between the BRFA and RFA, in BRFA we use a bias matrix of \(_{0}=4[_{d d},_{d 1};_{1  d},0]^{(d+1)(d+1)}\), which is four times the matrix investigated in our theory. The input distribution is selected as \(\{_{i}\}_{0 i N}_{}\)Unif\((^{d-1})\), and we take \(y=f_{}(_{0:N})\) without any noise. We consider three representative target functions: \(f_{1,p}\) for \(p=2,4\), and \(f_{2,p}\) for \(p=3\), as per (20) and (21). We examine a list of sample sizes \(n\) from \(2^{4}\) to \(2^{12}\). Prior to training with RF models, we standardize the \(y_{i}\)'s to have zero mean and unit standard deviation, ensuring that the trivial risk equals \(1\). We train the RF models using square loss with ridge regularization, selecting the ridge parameter to minimize the test error. The experimental results are displayed in Figure 2.

The left and middle panels of Figure 2 demonstrate a noticeable separation between RFMLP and the other two random-feature attention models for learning these target functions. RFMLP can hardly approximate the target function, whereas RFA and BRFA exhibit significantly better performance. This observation is consistent with our sample complexity analysis detailed in Example 2, where the sample complexity bound of RFMLP for learning average of functions of \(_{i}\) is found to be \(((N/4)^{p})\) times greater than that of RFA.

The performance comparison between RFA and BRFA depends on the target functions. RFA outperforms BRFA in learning \(f_{1,2}\) and \(f_{1,4}\), whereas BRFA outperforms RFA in learning \(f_{2,3}\). The latter phenomenon is as we expected: as demonstrated in Example 3 and 5, BRFA is more powerful than RFA in approximating the correlation-weighted functions.

We have conducted further experiments with various other target functions, detailed in Appendix D.

## 6 Conclusion

In this work, we introduced and examined the expressivity of two random-feature attention models, namely RFA (5) and BRFA (15). For general classes of functions that are invariant to the permutation of key tokens \(_{1:N}\), the excess risk of RFA (5) can avoid the dependence on sequence length, in contrast to the standard random-feature model RFMLP (14). Moreover, for specific functions that adopt the form of correlation-weighted polynomials (6), the excess risk of BRFA can avoid the polynomial dependence on the dimension. These insights enhance our understanding of the attention mechanism within a simplified context. Finally, our work left open many interesting questions for future work, such as the expressivity of softmax attention, the influence of positional encoding in expressivity, and the expressivity of multi-layer transformers.