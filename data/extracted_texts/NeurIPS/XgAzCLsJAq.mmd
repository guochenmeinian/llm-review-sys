# Continual Learning in the Frequency Domain

Ruiqi Liu\({}^{1,2}\), Boyu Diao\({}^{1,2,}\), Libo Huang\({}^{1}\), Zijia An\({}^{1,2}\), Zhulin An\({}^{1,2}\), Yongjun Xu\({}^{1,2}\)

\({}^{1}\)Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China

\({}^{2}\)University of Chinese Academy of Sciences, Beijing, China

liuruiqi23@mails.ucas.ac.cn,

{diaoboyu2012, anzijia23p,anzhulin, xyj}@ict.ac.cn,

www.huanglibo@gmail.com

Corresponding Author.

###### Abstract

Continual learning (CL) is designed to learn new tasks while preserving existing knowledge. Replaying samples from earlier tasks has proven to be an effective method to mitigate the forgetting of previously acquired knowledge. However, the current research on the training efficiency of rehearsal-based methods is insufficient, which limits the practical application of CL systems in resource-limited scenarios. The human visual system (HVS) exhibits varying sensitivities to different frequency components, enabling the efficient elimination of visually redundant information. Inspired by HVS, we propose a novel framework called Continual Learning in the Frequency Domain (CLFD). To our knowledge, this is the first study to utilize frequency domain features to enhance the performance and efficiency of CL training on edge devices. For the input features of the feature extractor, CLFD employs wavelet transform to map the original input image into the frequency domain, thereby effectively reducing the size of input feature maps. Regarding the output features of the feature extractor, CLFD selectively utilizes output features for distinct classes for classification, thereby balancing the reusability and interference of output features based on the frequency domain similarity of the classes across various tasks. Optimizing only the input and output features of the feature extractor allows for seamless integration of CLFD with various rehearsal-based methods. Extensive experiments conducted in both cloud and edge environments demonstrate that CLFD consistently improves the performance of state-of-the-art (SOTA) methods in both precision and training efficiency. Specifically, CLFD can increase the accuracy of the SOTA CL method by up to 6.83% and reduce the training time by 2.6\(\). Code is available at https://github.com/EMLS-ICTCAS/CLFD.git

## 1 Introduction

Continual learning (CL) enables machine learning models to adjust to new data while preserving previous knowledge in dynamic environments . Traditional training methods often underperform in CL because the adjustments to the parameters prioritize new information over old information, leading to what is commonly known as _catastrophic forgetting_. While recent CL methods primarily concentrate on addressing the issue of forgetting, it is imperative to also consider learning efficiency when implementing CL applications on edge devices with constrained resources , such as the NVIDIA Jetson Orin NX.

To mitigate _catastrophic forgetting_, a wide range of methods have been employed: _regularization-based_ methods  constrain updates to essential parameters, minimizing the drift in network parameters that are crucial for addressing previous tasks; _architecture-based_ methods  allocate distinct parameters for each task or incorporate additional network components upon the arrival of new tasks to decouple task-specific knowledge; and _rehearsal-based_ methods  effectively prevent forgetting by maintaining an episodic memory buffer and continuously replaying samples from previous tasks. Among these methods, rehearsal-based methods have been proven to be the most effective in mitigating _catastrophic forgetting_. However, when the buffer size is constrained by memory limitations (e.g., on edge devices), accurately approximating the joint distribution using limited samples becomes challenging. Moreover, rehearsal-based methods often require frequent data retrieval from buffers. This process significantly increases both computational demands and memory usage, consequently limiting the practical application of rehearsal-based methods in resource-constrained environments.

By reducing the size of the input image, both the training FLOPs and peak memory usage can be significantly decreased, thereby enhancing the training efficiency of rehearsal-based methods. Concurrently, this method allows rehearsal-based methods to store more samples within the same buffer. However, directly downsampling the input image can significantly degrade the model's performance due to information loss. Owing to the natural smoothness of images, the human visual system (HVS) exhibits greater sensitivity to low-frequency components than to high-frequency components , enabling the efficient elimination of visually redundant information. Inspired by HVS, we transfer the CL methods from the spatial domain to the frequency domain and reduce the size of input feature maps in the frequency domain. Several studies  have focused on accelerating model training in the frequency domain. However, two primary limitations hinder their direct application to CL: (1) These studies utilize Discrete Cosine Transform (DCT) to map images into the frequency domain, resulting in a complete loss of spatial information, which prevents the use of data augmentation techniques in rehearsal-based methods. (2) These studies introduce a significant number of cross-task learnable parameters, consequently increasing the risk of _catastrophic forgetting_.

To this end, we propose a novel framework called Continual Learning in the Frequency Domain (CLFD), which comprises two components: Frequency Domain Feature Encoder (FFE) and Class-aware Frequency Domain Feature Selection (CFFS). To reduce the size of input images, we propose the FFE. This method utilizes Discrete Wavelet Transform (DWT) to transform the original RGB image input into the frequency domain, thereby preserving both the frequency domain and spatial domain features of the image, which facilitates data augmentation. Furthermore, acknowledging that distinct tasks exhibit varying sensitivities to different frequency components, we propose the CFFS method to balance the reusability and interference of frequency domain features. CFFS calculates the frequency domain similarity between inputs across different classes and selects distinct frequency domain features for classification. This method promotes the use of analogous frequency domain features for categorizing semantically similar inputs while concurrently striving to diminish the overlap of frequency domain features among inputs with divergent semantics. Our framework avoids introducing any cross-task learnable parameters, thereby reducing the risk of _catastrophic forgetting_. Simultaneously, by optimizing only the input and output features of the feature extractor, it facilitates the seamless integration of CLFD with various rehearsal-based methods. Figure 1 (right) demonstrates that CLFD significantly improves both the training efficiency and accuracy of rehearsal-based methods when implemented on the edge device.

In summary, our contributions are as follows:

* We propose the CLFD, a novel framework designed to improve the efficiency of CL. This framework enhances the training by mapping input features in the frequency domain and compressing these frequency domain features. To the best of our knowledge, this study represents the first attempt to

Figure 1: **Left: Overview of CLFD. CLFD consists of two components: Frequency Domain Feature Encoder (FFE) and Class-aware Frequency Domain Feature Selection (CFFS). Right: On the NVIDIA Jetson Orin NX edge device, CLFD demonstrates a notable enhancement in both accuracy and efficiency compared to ER  on the split CIFAR-10 dataset.**

utilize frequency domain features to enhance the performance and the efficiency of CL on edge devices.
* CLFD stores and replays encoded feature maps instead of original images, thereby enhancing the efficiency of storage resource utilization. Concurrently, CLFD minimizes interference among frequency domain features, significantly boosting the performance of rehearsal-based methods across all benchmark datasets. These improvements can increase accuracy by up to 6.83% compared to the SOTA methods.
* We evaluate the CLFD framework on an actual edge device, showcasing its practical feasibility. The results indicate that our framework can achieve up to a 2.6\(\) improvement in training speed and a 3.0\(\) reduction in peak memory usage.

## 2 Related Work

### Continual Learning

The current CL methods can be categorized into three primary types: _Regularization-based_ methods [42; 30; 11; 22; 23] limit updates to key parameters to minimize drift in network parameters essential for previous tasks. _Architecture-based_ methods [14; 24; 35; 45; 52] assign distinct parameters to each task or add network components for new tasks to decouple task-specific knowledge. _Rehearsal-based_ methods [2; 6; 8; 12; 10] mitigate forgetting by maintaining an episodic memory buffer and continuously replaying samples from previous tasks to approximate the joint distribution of tasks during training. Among these, our framework focuses on rehearsal-based methods, as these methods are acknowledged as the most effective in mitigating _catastrophic forgetting_. ER  enhances CL by integrating training samples from both the current and previous tasks. Expanding upon this concept, DER++  enhances the learning process by retaining previous model output logits and utilizing a consistency loss during the model update. ER-ACE  safeguards learned representations and minimizes drastic adjustments required for adapting to new tasks, thereby mitigating _catastrophic forgetting_. Moreover, CLS-ER  mimics the interaction between rapid and prolonged learning processes by maintaining two supplementary semantic memories.

A limited number of works explore training efficiency in CL [46; 27; 16]. Among these methods, SparCL  reduces the FLOPs required for model training through the implementation of dynamic weight and gradient masks, along with selective sampling of crucial data. These methods accelerate the training process through pruning and sparse training. Nevertheless, our framework enhances efficiency by reducing the size of the input feature map, which is an orthogonal optimization to pruning.

### Frequency domain learning

Some studies [48; 15; 18; 21] utilize DCT to map images into the frequency domain and enhance the inference speed of the models. However, these methods are not conducive to enhancing rehearsal-based methods. Previous research  indicates that data augmentation can significantly boost the performance of rehearsal-based methods. Nevertheless, utilizing DCT results in a total loss of spatial information, thereby restricting the application of data augmentation. Other studies [29; 33; 32; 47; 17; 13] employ DWT to improve the classification performance of models. While wavelet transform effectively preserves the spatial features of images, these methods are not well-suited for CL due to the substantial increase in learnable parameters they introduce. In CL, this proliferation of parameters significantly raises the risk of _catastrophic forgetting_ across tasks. MgSvr  utilizes the frequency domain in the context of CL, focusing on the influence of different frequency components on model performance. In contrast, our framework delves into the differences in redundancy between the spatial and frequency domains. Compared to the spatial domain, CL in the frequency domain can more effectively remove redundant information from images, thereby improving the efficiency of CL.

## 3 Method

Our method, called Continual Learning in the Frequency Domain, is a unified framework that integrates two components: the Frequency Domain Feature Encoder, which transforms the initial RGB image inputs into the wavelet domain, and the Class-aware Frequency Domain Feature Selection,which balances the reusability and interference of frequency domain features. The entire framework is illustrated in Figure 2.

### Problem Setting

The problem of CL involves sequentially learning \(T\) tasks from a dataset, where in each task \(t\) corresponds to a training set \(^{t}=(x_{i},y_{i})_{i=1}^{N_{t}}\). Each task is characterized by a task-specific data distribution represented by the pairs \((x_{i},y_{i})\). To improve knowledge retention from previous tasks, we employ a fixed-size memory buffer, \(=(x_{i},y_{i})_{i=1}^{}\), which stores data from tasks encountered earlier. Given the inherent limitations in CL, the model's storage capacity for past experiences is finite, thus \( N_{t}\). To address this constraint, we utilize reservoir sampling  to efficiently manage the memory buffer. In the simplest testing configuration, we assume that the identity of each upcoming test instance is known, a scenario defined as Task Incremental Learning (Task-IL). If the class subset of each sample remains unidentified during CL inference, the situation escalates to a more complex Class Incremental Learning (Class-IL) setting. This research primarily focuses on the more intricate Class-IL setting, while the performance of Task-IL is used solely for comparative analysis.

### Discrete Wavelet Transform

DWT offers effective signal representation in both spatial and frequency domains , facilitating the reduction of input feature size. Compared with the DWT, DCT coefficients predominantly capture the global information of an image, but they fail to preserve the spatial continuity that is typical in normal images. In DCT, local spatial information is mixed, resulting in a loss of distinct local features. In contrast, DWT effectively integrates both spatial and frequency domain information, maintaining a balance between the two. Furthermore, the DWT method can be seamlessly integrated with data augmentation techniques in rehearsal-based methods, enhancing its applicability and effectiveness.

For 2D signal \(X^{N N}\), The signal after DWT can be represented as:

\[X^{}=[L\\ H]X[L^{T}&H^{T}]= [LXL^{T}&LKH^{T}\\ HXL^{T}&HXH^{T}]=[X_{ll}&X_{lh}\\  X_{hl}&X_{hh}],\] (1)

where \(L\) and \(H\) represent the low-frequency and high-frequency filters of orthogonal wavelets, respectively. These filters are truncated to the size of \( N\). The term \(X_{ll}\) refers to the low-frequency component, while \(X_{lh},X_{hl},X_{hh}\) represents the high-frequency components. We

Figure 2: Illustration of the CLFD workflow. Initially, the original RGB image input is transformed into the wavelet domain through a Frequency Domain Feature Encoder. Subsequently, the feature extractor extracts the frequency domain features of these input feature maps. We propose a Class-aware Frequency Domain Feature Selection to selectively utilize specific frequency domain features, which are then inputted into the classifier for subsequent classification.

select the Haar wavelet as the basis for the wavelet transform because of its superior computational efficiency , which is well-suited for our tasks.

### Frequency Domain Feature Encoder

Previous methods [29; 47; 28] typically discarded high-frequency components \(X_{lh},X_{hl},X_{hh}\) and retained low-frequency component \(X_{ll}\). However, focusing solely on low-frequency component leaves many potentially useful frequency components unexplored. Low-frequency component compress the global topological information of an image at various levels, while high-frequency components reveal the image's structure and texture . Therefore, we employ three 1\(\)1 point convolutions to integrate various frequency components. As shown in Figure 3, we use a 1\(\)1 point convolution to merge low-frequency component \(X_{ll}\) to obtain low-frequency features, another one to merge high-frequency components \(X_{lh},X_{hl},X_{hh}\) to obtain high-frequency features, and a final one to merge all frequency components to obtain global features. These three merged features compose the input feature maps. By utilizing both low and high-frequency components in CL, we can better prevent the loss of critical information while reducing input feature size. Since each merged feature's width and height are half of the original image, another advantage of working in the frequency domain is that the spatial size of the original image (H \(\) W \(\) 3) is reduced by half in both width and height (H/2 \(\) W/2 \(\) 3) after FFE. With the reduced spatial size, the computational load and peak memory requirements of CL models decrease. Moreover, the reduction in spatial size means that more replay samples can be stored under the same storage resources, while also reducing the bandwidth required for accessing data. However, setting a specific frequency domain feature encoder for each task may result in significant _catastrophic forgetting_. Therefore, we freeze the FFE at the end of the first task's training.

### Class-aware Frequency Domain Feature Selection

Considering that tasks are predominantly sensitive to specific frequency domain features extracted by a feature extractor, different tasks prioritize distinct frequency domain features. To this end, we propose the CFFS, designed to manage the issue of overlap in frequency domain features among samples from different classes. This method promotes comparable classes to utilize similar frequency domain features for classification, while also ensuring that samples from dissimilar classes employ divergent features. Consequently, this method reduces interference among various tasks and mitigates overfitting issues. For specific classes, we select a predetermined number of frequency domain features based on the absolute values of these features. Subsequently, unselected features are masked to prevent their interference in the classification process. We utilize a counter \(^{C N}\) to track the number of selections for each frequency domain feature among samples associated with a specific class. \(N\) and \(C\) denote the dimensions and the classes of frequency domain features, respectively. We utilize cosine similarity to evaluate the frequency domain similarity between two class samples. To decrease computational complexity, only low-frequency component \(X_{ll}\) is utilized for calculating cosine similarity. The similarity between class \(i\) and class \(j\) is expressed as follows:

\[S_{ ij}=_{ i}^{}_{ j}}{\|_{  i}\|\|_{ j}\|}.\] (2)

The value of cosine similarity is determined solely by the direction of the features, regardless of their magnitude. Consequently, \(_{ i}\) represents the sum of the low-frequency component of samples in class \(i\). We then select the class that exhibits the greatest similarity and the class that displays the least similarity to the current class:

\[y_{j}^{+}=*{argmax}_{i\{1,,K\}}_{ ij},  y_{j}^{-}=*{argmin}_{i\{1,,K\}}_{ ij},\] (3)

where \(K\) represents the total number of classes in the preceding task. Several studies [39; 43] employ _Heterogeneous Dropout_ to enhance the selection of underutilized features for subsequent tasks. While this method helps manage the overlap of feature selection across different tasks, it overlooks

Figure 3: The Utilization of DWT in FFE.

similarities among classes. This oversight can negatively impact the effectiveness of selecting features in the frequency domain. To this end, we propose the _Frequency Dropout_ method, which adjusts the probability of discarding frequency domain features based on class similarity. Specifically, let \([_{y}]_{j}\) denote the number of the j-th frequency domain feature when learning class \(y\). The probability of selecting this feature in class \(c\) while learning a new task is expressed as follows:

\[[P_{f}]_{c,j}=(_ {y_{c}^{-}}]_{j}}{_{i}[_{y_{c}^{-}}]_{i}}_{c}^{-} )+&(1-)(1-(_{y_{ c}^{+}}]_{j}}{_{i}[_{y_{c}^{+}}]_{i}}_{c}^{+} )),\\ _{c}^{-}=_{c}}{S_{cy_{c}^{-}}}, _{c}^{+}=^{+}}}{_{c}},\] (4)

where \(_{c}\) denotes the average cosine similarity between class \(c\) and all classes in previous tasks. The parameters \(_{c}^{-}\) and \(_{c}^{+}\) control the intensity of the selection process. A higher value indicates a greater overlap of activated features with analogous classes and a reduced overlap with non-analogous classes. The coefficient \(\) serves as a weighting factor that adjusts the selection of frequency domain features, determining whether the emphasis is more towards similar classes or less towards dissimilar ones. The _Frequency Dropout_ probability is updated at the beginning of each task. After completing training for \(\) epochs on a given task, _Semantic Dropout_ is employed instead of _Frequency Dropout_. It encourages the model to use the same set of frequency domain features for classification by setting the retention probability of frequency domain features in each class. This probability is proportional to the number of times that frequency domain feature has been selected in that class so far:

\[[P_{s}]_{c,j}=1-(_{c}]_{j}}{ _{i}[_{c}]_{i}}_{c}),\] (5)

where \(_{c}\) controls the strength of dropout. The probability of _Semantic Dropout_ is updated at the end of each epoch, thereby enhancing the model's established frequency domain feature selection. This adjustment effectively regulates the extent of overlap in the utilization of frequency domain features. Algorithm 1 outlines the procedure for the CFFS.

```
1:number of tasks \(T\), training epochs of the \(t\)-th task \(K_{t}\), dropout parameter \(\) and \(_{c}\), frequency dropout epochs \(\)
2:Initialize:\(P_{f}=1,P_{s}=1\)
3:for\(t=1,,T\)do
4:for\(e=1,,K_{t}\)do
5:if\(e<\)then
6:if\(t>1\)then
7: Dropout features based on frequency dropout probabilities \(1-P_{f}\)
8:if\(e==1\)then
9: Update \(P_{f}\) at the end of the first epoch (Eq. 4)
10:else
11: Dropout features based on semantic dropout probabilities \(1-P_{s}\)
12: Update \(P_{s}\) at the end of each epoch (Eq. 5)
13: Select the top 60% of frequency domain features by response values for classification
14: Update \(\) ```

**Algorithm 1** Class-aware Frequency Domain Feature Selection Algorithm

## 4 Experiment

### Experimental Setup

Datasets.We conduct comprehensive experimental analyses on extensively used public datasets, including Split CIFAR-10 (S-CIFAR-10)  and Split Tiny ImageNet (S-Tiny-ImageNet) . The S-CIFAR-10 dataset is structured into five tasks, each encompassing two classes, while the S-Tiny-ImageNet dataset is divided into ten tasks, each comprising twenty classes. Additionally, the standard input image size for these datasets is 32 \(\) 32 pixels.

Evaluation metrics.We use the average accuracy on all tasks to evaluate the performance of the final model:

\[ACC_{t}=_{=1}^{t}R_{t,}\] (6)

We denote the classification accuracy on the \(\)-th task after training on the t-th task as \(R_{t,}\). Moreover, we evaluate the training time, training FLOPs and peak memory footprint  to demonstrate the efficiency of each method. More experimental results can be found in Appendix F.

Baselines.We compare CLFD with several representative baseline methods, including three regularization-based methods: oEWC , SI  and LwF , as well as four rehearsal-based methods: ER , DER++ , ER-ACE  and CLS-ER . In our evaluation, we incorporate two non-continual learning benchmarks: SGD as the lower bound and JOINT as the upper bound.

Implementation DetailsWe expand the Mammoth CL repository in PyTorch . For the S-CIFAR-10 and S-Tiny-ImageNet datasets, we utilize a standard ResNet18  without pretraining as the baseline model, following the method outlined in DER++ . All models are trained using the Stochastic Gradient Descent optimizer with a fixed batch size of 32. Additional details regarding other hyperparameters are detailed in Appendix D and E. For the S-Tiny-ImageNet dataset, models undergo training for 100 epochs, whereas for the S-CIFAR-10 dataset, training lasts for 50 epochs per task. In rehearsal-based methods, each training batch consists of an equal mix of new task samples and samples retrieved from the buffer. To ensure robustness, all experiments are conducted 10 times with different initializations, and the results are averaged across these runs.

### Experimental Result

Table 1 presents a comparative analysis of the results on the S-CIFAR-10 and S-Tiny-ImageNet datasets, evaluated under Class-IL and Task-IL settings. The results elucidate that CLFD significantly

    &  &  &  \\   & & Class-IL & Task-IL & Mem & Class-IL & Task-IL & Mem \\   & JOINT & 92.20\({}_{ 0.15}\) & 98.31\({}_{ 0.12}\) & - & 59.99\({}_{ 0.19}\) & 82.04\({}_{ 0.10}\) & - \\  & SGD & 19.62\({}_{ 0.05}\) & 61.02\({}_{ 3.33}\) & - & 7.92\({}_{ 0.26}\) & 18.31\({}_{ 0.68}\) & - \\   & oEWC  & 19.49\({}_{ 0.12}\) & 68.29\({}_{ 5.92}\) & 530MB & 7.58\({}_{ 0.10}\) & 19.20\({}_{ 0.31}\) & 970MB \\  & SI  & 19.48\({}_{ 0.17}\) & 68.05\({}_{ 5.91}\) & 573MB & 6.58\({}_{ 0.31}\) & 36.32\({}_{ 0.13}\) & 1013MB \\  & LwF  & 19.61\({}_{ 0.05}\) & 63.29\({}_{ 2.35}\) & 316MB & 8.46\({}_{ 0.22}\) & 15.85\({}_{ 0.58}\) & 736MB \\   & ER  & 29.42\({}_{ 3.53}\) & 86.36\({}_{ 1.43}\) & 497MB & 8.14\({}_{ 0.01}\) & 26.80\({}_{ 0.94}\) & 1333MB \\  & DER++  & 42.15\({}_{ 7.07}\) & 83.51\({}_{ 2.48}\) & 646MB & 8.00\({}_{ 1.16}\) & 23.53\({}_{ 2.67}\) & 1889MB \\  & ER-ACE  & 40.96\({}_{ 0.00}\) & 85.78\({}_{ 2.78}\) & 502MB & 6.68\({}_{ 2.75}\) & 35.93\({}_{ 2.66}\) & 1314MB \\  & CLS-ER  & 45.91\({}_{ 2.93}\) & 89.71\({}_{ 1.87}\) & 1016MB & 11.09\({}_{ 11.52}\) & 40.76\({}_{ 1.97}\) & 3142MB \\   & CLFD-ER & 45.56\({}_{ 3.71}\) & 84.45\({}_{ 0.85}\) & 205MB & 7.61\({}_{ 0.03}\) & 34.67\({}_{ 1.91}\) & 514MB \\  & CLFD-DER++ & 51.02\({}_{ 2.76}\) & 81.15\({}_{ 1.92}\) & 241MB & 10.69\({}_{ 0.27}\) & 31.55\({}_{ 0.39}\) & 658MB \\  & CLFD-ER-ACE & **52.74\({}_{ 1.91}\)** & 87.13\({}_{ 0.41}\) & 204MB & 10.71\({}_{ 2.91}\) & 38.05\({}_{ 11.98}\) & 514MB \\  & CLFD-CLS-ER & 50.13\({}_{ 3.67}\) & 85.30\({}_{ 1.01}\) & 401MB & **12.61\({}_{ 0.95}\)** & 37.80\({}_{ 0.08}\) & 1032MB \\   & ER  & 38.49\({}_{ 1.68}\) & 89.12\({}_{ 0.92}\) & 497MB & 8.30\({}_{ 0.01}\) & 34.82\({}_{ 6.82}\) & 1333MB \\  & DER++  & 53.09\({}_{ 3.43}\) & 88.34\({}_{ 1.05}\) & 646MB & 11.29\({}_{ 1.92}\) & 32.92\({}_{ 2.01}\) & 1889MB \\  & ER-ACE  & 56.12\({}_{ 2.12}\) & 09.49\({}_{ 0.98}\) & 502MB & 11.09\({}_{ 3.06}\) & 41.85\({}_{ 3.46}\) & 1314MB \\  & CLS-ER  & 53.57\({}_{ 2.73}\) & 90.75\({}_{ 2.76}\) & 1016MB & 16.35\({}_{ 4.61}\) & 46.11\({}_{ 7.69}\) & 3142MB \\   & CLFD-ER & 55.76\({}_{ 1.85}\) & 88.29\({}_{ 0.16}\) & 205MB & 8.89\({}_{ 0.07}\) & 42.40\({}_{ 0.83}\) & 514MB \\  & CLFD-DER++ & 58.81\({}_{ 0.29}\) & 84.76\({}_{ 0.66}\) & 241MB & 15.42\({}_{ 0.37}\) & 40.94\({}_{ 1.30}\) & 658MB \\  & CLFD-ER-ACE & 58.68\({}_{ 0.66}\) & 89.35\({}_{ 0.34}\) & 204MB & 15.88\({}_{ 2.51}\) & 44.71\({}_{ 1.054}\) & 514MB \\  & CLFD-CLS-ER & **59.98\({}_{ 1.38}\)** & 87.09\({}_{ 0.43}\) & 401MB & **18.73\({}_{ 0.91}\)** & 49.75\({}_{ 2.01}\) & 1032MB \\   

Table 1: Comparison on different CL methods. CLFD consistently reduces the peak memory footprint of corresponding CL methods while simultaneously improving average accuracy. The highest results are marked in bold, and shadowed lines indicate the results from our framework.

[MISSING_PAGE_FAIL:8]

### Model Analysis

In this section, we provide an in-depth analysis of CLFD.

Sparse TrainingWe compare our method with SparCL , a SOTA sparse training method. Table 3 presents the average accuracy and training FLOPs for CLFD and SparCL. Our method significantly enhances the average accuracy compared to SparCL. Although our method incurs higher training FLOPs than SparCL, this does not directly correlate with actual training speed. Our method accelerates training speed without requiring additional optimization. Conversely, pruning and sparse training methods that utilize masks often fail to translate into actual training time savings without optimizations at the compiler level. Furthermore, the combination of CLFD and SparCL not only achieves the highest accuracy but also leads to the lowest training FLOPs. The successful integration of CLFD and SparCL serves as an example of CLFD's adaptability to sparse training and pruning methods.

The impact of different frequency componentsTo explore the influence of different frequency components, we employ them as input feature maps and assess the CLFD-DER++ accuracy on the S-CIFAR-10 dataset under 50 buffer size. Table 4 presents the average accuracy across various frequency components. Utilizing the low-frequency components of images as input feature maps yields the highest accuracy among different frequency components. This suggests that CNN models demonstrate greater sensitivity to low-frequency channels compared to high-frequency channels. This finding aligns with the characteristics of the HVS, which also prioritizes low-frequency information. Despite this, FFE achieves the highest average accuracy, suggesting that high-frequency components are also significant. Optimal preservation of image information during downsampling is achieved only through the integration of components across different frequencies. Some examples of the encoded frequency domain feature maps are visualized in Figure 6.

Frequency Domain Feature SelectionTo evaluate the effectiveness of frequency dropout in reducing interference among frequency domain features across diverse tselection of specific frequency domain features for different classes. Specifically, we track the classification activities on the test set and conduct normalized counts of selections for the frequency domain features, as illustrated in Figure 5. We observe that feature selection patterns exhibit higher correlations among semantically similar classes. For instance, the classes "cat" and "dog" often select identical sets of features. Similarly, significant similarities in feature selection patterns are evident between "auto" and "truck". This result demonstrates the effectiveness of CFFS.

## 5 Conclusion

Inspired by the human visual system, we propose CLFD, a comprehensive framework designed to enhance the efficiency of CL training and augment the precision of rehearsal-based methods. To effectively reduce the size of feature maps and optimize feature reuse while minimizing interference across various tasks, we propose the Frequency Domain Feature Encoder and the Class-aware Frequency Domain Feature Selection. The FFE employs wavelet transform to convert input images into the frequency domain. Meanwhile, the CFFS selectively uses different frequency domain features for classification depending on the frequency domain similarity of classes. Extensive experiments conducted across various benchmark datasets and environments have validated the effectiveness of our method, which enhances the accuracy of the SOTA method by up to 6.83%. Moreover, it achieves up to a 2.6\(\) increase in training speed and a 3.0\(\) reduction in peak memory usage. We discuss the limitations and broader impacts of our method in Appendix A and B, respectively.

Acknowledgement:This work was supported by the Chinese Academy of Sciences Project for Young Scientists in Basic Research (YSBR-107) and the Beijing Natural Science Foundation (4244098).