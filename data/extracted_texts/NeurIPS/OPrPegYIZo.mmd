# DynaMITE-RL: A Dynamic Model for

Improved Temporal Meta-Reinforcement Learning

 Anthony Liang

University of Southern California

aliang80@usc.edu

&Guy Tennenholtz

Google Research

guytenn@google.com

&Chih-Wei Hsu

Google Research

cwhsu@google.com

&Yinlam Chow

Google Deepmind

yinlamchow@google.com

&Erdem Biyik

University of Southern California

erdem.biyik@usc.edu

&Craig Boutilier

Google Research

cboutilier@google.com

###### Abstract

We introduce _DynaMITE-RL_, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions--parts of the episode where the latent state is fixed--and propose three key modifications to existing meta-RL methods: (i) consistency of latent information within sessions, (ii) session masking, and (iii) prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, illustrating the efficacy of DynaMITE-RL over state-of-the-art baselines in both online and offline RL settings.

## 1 Introduction

Markov decision processes (MDPs)  provide a general framework in reinforcement learning (RL), and can be used to model sequential decision problems in a variety of domains, e.g., recommender systems (RSs), robot and autonomous vehicle control, and healthcare . MDPs assume a static environment with fixed transition probabilities and rewards . In many real-world systems, however, the dynamics of the environment are intrinsically tied to latent factors subject to temporal variation. While nonstationary MDPs are special instances of partially observable MDPs (POMDPs) , in many applications these latent variables change infrequently, i.e. the latent variable remains fixed for some duration before changing. One class of problems exhibiting this latent transition structure is recommender systems, where a user's preferences are a latent variable which gradually evolves over time . For instance, a user may initially have a strong affinity for a particular genre (e.g., action movies), but their viewing habits could change over time, influenced by external factors such as trending movies, mood, etc. A robust system should adapt to these evolving tastes to provide suitable recommendations. Another example is in manufacturing settings, where industrial robots may experience unobserved gradual deterioration of their mechanical components affecting the overall functionality of the system. Accurately modelling such latent transitions caused by hardware degradation can help manufacturers optimize performance, cost, and equipment lifespan.

Our goal in this work is to leverage such a temporal structure to obviate the need to solve a fully general POMDP. To this end, we propose **Dynamic Model** for **I**mproved **T**emporal **M**eta **R**einforcement

**Learning** (DynaMITE-RL), a method designed to exploit the temporal structure of sessions, i.e., sub-trajectories within the history of observations in which the latent state is fixed. We formulate our problem as a _dynamic latent contextual MDP (DLCMDP)_, and identify three crucial elements needed to enable tractable and efficient policy learning in environments with the latent dynamics captured by a DLCMDP. First, we consider consistency of latent information, by exploiting time steps for which we have high confidence that the latent variable is constant. To do so, we introduce a consistency loss to regularize the posterior update model, providing better posterior estimates of the latent variable. Second, we enforce the posterior update model to learn the dynamics of the latent variable. This allows the trained policy to better infer, and adapt to, temporal shifts in latent context in unknown environments. Finally, we show that the variational objective in meta-RL algorithms, which attempts to reconstruct the entire trajectory, can hurt performance when the latent context is nonstationary. We modify this objective to reconstruct only the transitions that share the same latent context.

Closest to our work is VariBAD , a meta-RL  approach for learning a Bayes-optimal policy, enabling an agent to quickly adapt to a new environment with unknown dynamics and reward functions. VariBAD uses variational inference to learn a posterior update model that approximates the belief over the distribution of transition and reward functions. It augments the state space with this belief to represent the agent's uncertainty during decision-making. Nevertheless, VariBAD and the Bayes-Adaptive MDP framework  assume the latent context is static _across an episode_ and do not address settings with latent state dynamics. In this work, we focus on the dynamic latent state formulation of the meta-RL problem.

Our core contributions are as follows: (1) We introduce DynaMITE-RL, a meta-RL approach to handle environments with evolving latent context variables. (2) We introduce three key elements for learning an improved posterior update model: session consistency, modeling dynamics of latent context, and session reconstruction masking. (3) We validate our approach on a diverse set of challenging simulation environments and demonstrate significantly improved results over multiple state-of-the-art baselines in both online and offline-RL settings.

## 2 Background

We begin by reviewing relevant background including meta-RL and Bayesian RL. We also briefly summarize the VariBAD  algorithm for learning Bayes-Adaptive policies.

**Meta-RL.** The goal of meta-RL  is to quickly adapt an RL agent to an unseen test environment. Meta-RL assumes a distribution \(p()\) over possible environments or _tasks_, and learns this distribution by repeatedly sampling batches of tasks during meta-training. Each task \(_{i} p()\) is described by an MDP \(_{i}=(,,R_{i},T_{i},)\), where the state space \(\), action space \(\), and discount factor \(\) are shared across tasks, while \(R_{i}\) and \(T_{i}\) are task-specific reward and transition functions, respectively.

Figure 1: **(Left) The graphical model for a DLCMDP. The transition dynamics of the environment follows \(T(s_{t+1},m_{t+1} s_{t},a_{t},m_{t})\). At every timestep \(t\), an i.i.d. Bernoulli random variable, \(d_{t}\), denotes the change in the latent context, \(m_{t}\). Blue shaded variables are observed and white shaded variables are latent. (Right) A DLCMDP rollout. Each session \(i\) is governed by a latent variable \(m^{i}\) which is changing between sessions according to a fixed transition function, \(T_{m}(m^{} m)\). We denote \(l_{i}\) as the length of session \(i\). The state-action pair \((s^{i}_{t},a^{i}_{t})\) at timestep \(t\) in session \(i\) is summarized into a single observed variable, \(x^{i}_{t}\). We emphasize that session terminations are not explicitly observed.**The objective of meta-RL is to learn a policy that efficiently maximizes reward given a new task \(_{i} p()\) sampled from the task distribution at meta-test time. Meta-RL is a special case of a POMDP in which the unobserved variables are \(R\) and \(T\), which are assumed to be stationary throughout an episode.

**Bayesian Reinforcement Learning (BRL).** BRL  utilizes Bayesian inference to model the uncertainty of agent and environment in sequential decision making problems. In BRL, \(R\) and \(T\) are unknown a priori and treated as random variables with associated prior distributions. At time \(t\), the _observed history_ of states, actions and rewards is \(_{:t}=\{s_{0},a_{0},r_{1},,r_{t},s_{t}\}\), and the belief \(b_{t}\) represents the posterior over task parameters \(R\) and \(T\) given the transition history, i.e. \(b_{t} p(R,T_{:t})\). Given the initial belief \(b_{0}(R,T)\), the belief can be updated iteratively using Bayes' rule: \(b_{t+1}=p(R,T_{t+1}) p(s_{t+1},r_{t+1}_{t},R,T) b_{t}\). This Bayesian approach to RL can be formalized as a _Bayes-Adaptive MDP (BAMDP)_. A BAMDP is an MDP over the _augmented state space_\(S^{+}\!=\!\!\!\), where \(\) denotes the belief space. Given the augmented state \(s_{t}^{+}\!=\!(s_{t},b_{t})\), the transition function is given by \(T^{+}(s_{t+1}^{+}\! s_{t}^{+}\!,a_{t})\!=\!_{b_{t}}[T(s_{t+1}  s_{t},a_{t})\!\!(b_{t+1}\!=\!p(R,T_{:t+1})]\), and reward function under the current belief is, \(R^{+}(s_{t}^{+},a_{t})=_{b_{t}}[R(s_{t},a_{t})]\). The BAMDP formulation naturally resolves the exploration-exploitation tradeoff. A Bayes-optimal RL agent takes information-gathering actions to reduce its uncertainty in the MDP parameters while simultaneously maximizing the task returns. However, for most interesting problems, solving the BAMDP--and even computing posterior updates--is intractable given the continuous and typically high-dimensional nature of the task distribution.

**VariBAD.** Zintgraf et al.  approximates the Bayes-optimal solution by modeling uncertainty over the MDP parameters. These parameters are represented by a latent vector \(m^{d}\), the posterior over which is \(p(m_{:H})\), where \(H\) is the BAMDP horizon. VariBAD uses a variational approximation \(q_{}(m_{:t})\) parameterized by \(\) and conditioned on the observed history up to time \(t\). Zintgraf et al.  show that \(q_{}(m_{:t})\) approximates the belief \(b_{t}\). In practice, \(q_{}(m_{:t})\) is represented by a Gaussian distribution \(q_{}(m_{:t})=((_{:t}),(_{:t}))\), where \(\) and \(\) are sequence models (e.g., recurrent neural networks or transformers ) that encode trajectories to latent statistics. The variational lower bound at time \(t\) is \(_{q_{}(m_{:t})}[ p_{}(_{:H} m)]-D_{KL} (q_{}(m_{:t}) p_{}(m))\), where the first term reconstructs the trajectory likelihood \(p_{}(_{:H} m)\) and the second term regularizes the variational posterior to a prior distribution over the latent space, typically modeled with a standard Gaussian distribution. Importantly, the trajectory up to time \(t\), i.e., \(_{:t}\), is used in the ELBO equation to infer the posterior belief at time \(t\), which then decodes the entire trajectory \(_{:H}\), _including future transitions_. Given the belief state distribution \(q_{}\) of a BAMDP, the policy maps both the state and belief to actions, i.e., \((a_{t} s_{t},q_{}(m_{:t}))\). The BAMDP solution policy \(^{*}\) is trained, e.g., via policy gradient methods, to maximize the expected cumulative return over the task distribution: \(J()=_{R,T}[_{}[_{t=0}^{H-1}^{t}r(s_{t },a_{t})]]\).

## 3 Dynamic Latent Contextual MDPs

As a special case of a BAMDP, where the belief state is parameterized with a latent context vector (analogous to the problem formulation of VariBAD), the _dynamic latent contextual MDP (DLCMDP)_ is denoted by \(,,,R,T,_{0},H\), where \(\) is the state space, \(\) is the action space, \(\) is the _latent_ context space, \(R:_{}\) is a reward function, \(T:_{ }\) is a transition function, \(_{0}_{}\) is an initial state distribution, \((0,1)\) is a discount factor, and \(H\) is the (possibly infinite) horizon.

We assume an episodic setting in which each episode begins in a state-context pair \((s_{0},m_{0})_{0}\). At time \(t\), the agent is at state \(s_{t}\) and context \(m_{t}\), and has observed history \(_{:t}=\{s_{0},a_{0},r_{1},,r_{t},s_{t}\}\).

Figure 2: VariBAD does not model the latent context dynamics and fails to adapt to the changing goal location. By contrast, DynaMITE-RL correctly infers the transition and consistently reaches the rewarding cell (green cross).

Given the history, the agent selects an action \(a_{t}\), after which the state and latent context transitions according to \(T(s_{t+1},m_{t+1} s_{t},a_{t},m_{t})\), and the agent receives a reward sampled from \(R(s_{t},a_{t},m_{t})\). Throughout this process, the context \(m_{t}\) is latent (i.e., _not observed_ by the agent).

DLCMDPs embody the causal independence depicted by the graphical model in Figure 1. Particularly, DLCMDPs impose a structure on changes of the latent variable \(m\), allowing the latent context \(m\) to change less or more frequently. We denote by \(d_{t}\) the random variable at which a transition occurs in \(m_{t}\). According to Figure 1, the transition function \(T\) is represented by the following factored distribution:

\[T(s_{t+1}=s^{},m_{t+1}=m^{} s_{t}=s,a_{t}=a,m_{t }=m)\] \[=T_{s}(s^{} s,a,m)\{m^{}=m,d_{t}=0\}T_ {d}(d_{t}=0)+_{0}(s^{} m^{})T_{m}(m^{} m)\{d_{t}=1\}T_{d}(d_{t}=1),\]

where \(T_{m}:\) is the latent dynamics function, \(T_{s}\) is the context-dependent state transition function, and \(T_{d}\) is the termination probability distribution. We refer to sub-trajectories between changes in the latent context as _sessions_, which may vary in length. At the start of a new episode, a new state and a new latent context are sampled based on the distribution \(_{0}\). Each session itself is governed by an MDP parameterized with a latent context \(m\), which changes stochastically between sessions according to the latent transition function \(T_{m}(m^{} m)\). For notational simplicity we use index \(i\) to denote the \(i^{}\) session in a trajectory, and \(m^{i}\) the respective latent context of that session. We emphasize that sessions switching times are latent random variables.

Notice that DLCMDPs are more general than latent MDPs [38; 29], in which the latent context is fixed throughout the entire episode; this corresponds to \(d_{t} 0\). Moreover, DLCMDPs are closely related to POMDPs; letting \(d_{t} 1\), a DLCMDP reduces to a general POMDP with state space \(\), observation space \(\), and observation function \(_{0}\). As a consequence DLCMDPs are as general as POMDPs, rendering them very expressive. Moreover, the specific temporal structure of DLCMDPs allows us to devise efficient learning algorithms that exploit the transition dynamics of the latent context, improving learning efficiency. DLCMDPs are related to DCMDPs , LSMDPs , and DP-MDP . However, DCMDPs assume contexts are observed, and focus on aggregated context dynamics, LSMDPs assume that the latent contexts across sessions are i.i.d (i.e., there is no latent dynamics) and DP-MDPs assume that sessions are fixed length.

We aim to learn a policy \((a_{t} s_{t},m_{t})\) which maximizes the expected return \(J()\) over unseen test environments. As in BAMDPs, the optimal DLCMDP Q-function satisfies the Bellman equation; \( s^{+}^{+},a:Q(s^{+},a)=R^{+}(s^{+},a)+ _{s^{+^{}}^{+}}T^{+}(s^{+^{}} s^{+},a) }{}Q(s^{+^{}},a)\). In the following section, we present DynaMITE-RL for learning a Bayes-optimal agent in a DLCMDP.

## 4 DynaMITE-RL

We detail DynaMITE-RL, first deriving a variational lower bound for learning a DLCMDP posterior model, then outlining three principles for training DLCMDPs, and finally integrating them into our training objective.

**Variational Inference for Dynamic Latent Contexts.** Given that we do not have direct access to the transition and reward functions of the DLCMDP, following Zintgraf et al. , we infer the

Figure 3: Pseudo-code (online RL training) and model architecture of DynaMITE-RL.

posterior \(p(m_{ t})\), and reason about the latent context vector \(m\) instead. Since exact posterior computation over \(m\) is computationally infeasible, given the need to marginalize over task space, we introduce the variational posterior \(q_{}(m_{ t})\), parameterized by \(^{d}\), to enable fast inference at every step. Our learning objective maximizes the log-likelihood \(_{}[ p()]\) of observed trajectories. In general, the true posterior over the latent context is intractable, as is the empirical estimate of the log-likelihood. To circumvent this, we derive the _evidence lower bound (ELBO)_ to approximate the posterior over \(m\) under the variational inference framework.

Let \(=\{m^{i}\}_{i=0}^{K-1}\) be the sequence of latent context vectors for \(K\) sessions in an episode (note that \(K\) is inherently a random variable--the exact number of sessions in an episode is not known) and \(=\{d_{t}\}_{t=0}^{H-1}\) denote the collection of session terminations. We use a parametric generative distribution model for the state-reward trajectory, conditioned on the action sequence: \(p_{}(s_{0},r_{1},s_{1},,r_{H},s_{H} a_{0},,a_{H-1})\). In what follows, we drop the conditioning on \(a_{ H-1}\) for the sake of brevity.

The variational lower bound can be expressed as:

\[ p_{}()_{q_{}( ,|_{ t})} p_{}(, )}_{}-(q_{}( ,_{ t})) p_{}(, ))}_{}=_{,t},\] (1)

which can be estimated via Monte Carlo sampling over a learnable approximate posterior \(q_{}\). In optimizing the reconstruction loss of session transitions and rewards, the learned latent variables should capture the unobserved MDP parameters. The full derivation of the ELBO for a DLCMDP is provided in Appendix A.1.

Figure 2 depicts a (qualitative) didactic GridWorld example with two possible rewarding goals that alternate between sessions. The VariBAD agent does not account for latent goal dynamics and gets stuck after reaching the goal in the first session. By contrast, DynaMITE-RL employs the latent context dynamics model to capture goal changes, and adapts to the context changes across sessions.

**Consistency of Latent Information.** In the DLCMDP formulation, each session is itself an MDP with a latent context fixed across the session. This within-context stationarity means new observations can only increase the information the agent has about this context. In other words, the agent's posterior over latent contexts should gradually hone in on the true latent distribution. Although this true distribution remain unknown, this insight suggest the use of a _session-based consistency loss_, which penalizes the agent if there is no increase in information between timestep. Our consistency objective penalizes the agent when the difference between KL-divergence of the posterior to the final posterior in the session between consecutive timesteps is positive, which is the case when there is no increase in information about a session's latent context after observing a new transition. Let \(d_{H-1}=1\) and \(t_{i}\{0,,H\}\) be a random variable denoting the last timestep of session \(i\{0,,K-1\}\), i.e., \(t_{i}=\{t^{}_{ 0}:_{t=0}^{t^{}}d_{t}=i+1\}\). For time \(t\) in session \(i\), we define,

\[_{t}=D_{KL}(q_{}(m^{i}_{ t+1}) q_{ }(m^{i}_{ t_{i}})),\]

where \(q_{}(m^{i}_{ t_{i}})\) is the final posterior in session \(i\). This measures the difference between our current belief at time \(t\) to the final belief at the end of the session. Our temporal, session-based consistency objective is

\[_{,t}=\{_{t+1}-_{t}, \ 0\}.\]

Using temporal consistency to regularize inference introduces an explicit inductive bias that allows for better posterior estimation.

_Remark 4.1_.: We introduce session-based consistency for DLCMDPs, though it is also relevant in single-session settings with stationary latent context. Indeed, as we discuss below, while VariBAD focuses on single sessions, it does not constrain the latent's posterior to be identical to final posterior belief. Consistency may be useful in settings where the underlying latent variable is stationary, but may hurt performance when this variable is indeed changing. Since our modeling approach allows latent context changes across sessions, incorporating consistency regularization does not generally hurt performance.

**Latent Belief Conditioning.** Unlike the usual BAMDP framework, DLCMDPs allow one to model temporal changes of latent contexts via dynamics \(T_{m}(m^{} m)\) across sessions. To incorporate this model into belief estimation, in addition to the history \((_{ t},d_{ t})\), we condition the posterior on the final latent belief \(q_{}(m^{},d^{} m,d,_{t})\) from the previous session, and impose KL-divergence matching between this belief and the prior distribution \(p_{}(m^{} m)\).

**Reconstruction Masking.** When the agent is at time \(t\), Zintgraf et al.  encodes past interactions to obtain the current posterior \(q_{}(m_{:t})\) since this is all the information available for inference about the current task (see Eq. (1)). They use this posterior to decode the entire trajectory--_including future transitions_--from different sessions to optimize the lower bound during training. The insight is that decoding both the past and future allows the posterior model to perform inference about unseen states. However, we observe that when the latent context is stochastic, reconstruction over the full sequence is detrimental to training efficiency. The model is attempting to reconstruct transitions outside of the current session that may be irrelevant or biased given the latent state dynamics, rendering it a more difficult learning problem. Instead we reconstruct only the transitions within the session defined by the predicted termination indicators, i.e., at any arbitrary time \(t\) within session \(i\), the session-based reconstruction loss is given by

\[_{,t}=_{q_{}(, _{:t})} p_{}(_{t_{i-1}+1:t_{i}} ,)-D_{KL}(q_{}(,_{:t}))  p_{}(,),\]

where \(t_{i}\) is the last timestep of session \(i\).

**DynaMITE-RL.** By incorporating the three modifications above, we obtain at the following training objective for our variational meta-RL approach:

\[_{}(,)=_{t=0}^{H-1}_{,t}(,)+_{,t}(),\] (2)

where \(>0\) is a hyperparameter that balances the consistency loss with the ELBO objective. We present a simplified pseudocode for online training of DynaMITE-RL in Algorithm 2(a) and a detailed algorithm in Appendix A.2.

**Implementation Details.** We use Proximal Policy Optimization (PPO)  for online RL training. We introduce a posterior inference network that outputs a Gaussian over the latent context for the \(i\)-th session and the session termination indicators, \(q_{}(m_{t+1},d_{t+1}_{t},m_{t},d_{t})\), conditioned on the history and posterior belief from the previous session. We parameterize the inference network as a sequence model, with e.g., an RNN  or a Transformer , with different multi-layer perceptron (MLP) output heads for predicting the logits for session termination and the posterior belief. In practice, the posterior belief MLP outputs the parameters of a Gaussian distribution \(q_{_{m}}(m_{t+1}_{:t},m_{t})=((_{:t}),( _{:t}))\) where the variance represents the agent's uncertainty about the MDP. The session termination network applies a sigmoid activation function \((x)=}\) to the MLP output. Following PPO , the actor loss \(_{}\) and critic loss \(_{}\) are respectively given by \(_{}=_{_{}}[_{}(a s,m)A (s,a,m)]\) and \(_{}=_{_{}}[(Q_{}(s,a,m)-(r+V_ {}(s^{},m))^{2}]\), where \(V\) is the state-value network, \(Q\) is the state-action value network, and \(A\) is the advantage function. We also add an entropy bonus to ensure sufficient exploration in more complex domains. A decoder network, also parameterized using MLPs, reconstructs transitions and rewards given the session's latent context \(m\), current state \(s_{t}\), and action \(a_{t}\), i.e., \(p_{}^{T}(s_{t+1} s_{t},a_{t},m_{t})\) and \(p_{}^{R}(r_{t+1} s_{t},a_{t},m_{t})\). Figure 2(b) depicts the implemented model architecture. The final objective is to jointly learn the policy \(_{}\), the variational posterior model \(q_{}\), and the factored likelihood model \(p_{}\) that

Figure 4: The environments considered in evaluating DynaMITE-RL. Each environment exhibits some change in reward and/or dynamics between sessions including changing goal locations (left and middle left), changing target velocities (middle right), and evolving user preferences of itch location (right).

minimizes the following loss:

\[(,,)=_{}()+ _{}(,),\] (3)

where \(\) is the expected return, and \(>0\) is a hyperparameter balancing the RL objective with DynaMITE-RL's variational inference objective. We also evaluate DynaMITE-RL in an offline RL setting, in which we collect an offline dataset of trajectories following an oracle goal-conditioned policy and subsequently approximate the optimal value function and RL agent using offline RL methods, e.g., IQL . The value function and the policy are parameterized with the same architecture as in the online setting and will be detailed in Appendix A.5.

## 5 Experiments

We present experiments that demonstrate, while VariBAD and other meta-RL methods struggle to learn good policies given nonstationary latent contexts, DynaMITE-RL exploits the causal structure of a DLCMDP to more efficiently learn performant policies. We compare our approach to several state-of-the-art meta-RL baselines, showing significantly better evaluation returns.

**Environments.** We test DynaMITE-RL on a suite of standard meta-RL benchmark tasks including a didactic gridworld navigation, continuous control, and human-in-the-loop robot assistance as shown in Figure 4. Gridworld navigation and MuJoCo  locomotion tasks are considered by Zintgraf et al. , Dorfman et al. , and Choshen and Tamar . We modify these environments to incorporate temporal shifts in the reward function and/or environment dynamics. To achieve good performance under these conditions, a learned policy must adapt to the latent state dynamics. More details about the environments and hyperparameters can be found in Appendix A.4 and A.5.

_Gridworld._ We modify the Gridworld environment used by Zintgraf et al. . In a \(5 5\) gridworld, two possible goals are sampled uniformly at random in each episode. One of the two goals has a \(+1\) reward while the other has \(0\) reward. The rewarding goal location changes after each session

  & Gridworld & Reacher & HC-Dir & HC-Vel & Wind+Vel & ScratchItch \\  RL\({}^{2}\) & \(33.4 1.6\) & \(-150.6 1.2\) & \(-420.0 8.4\) & \(-513.2 8.7\) & \(-493.5 1.8\) & \(50.4 16.8\) \\ VariiBAD & \(31.8 1.9\) & \(-102.4 4.2\) & \(-242.5 4.8\) & \(-363.5 3.2\) & \(-188.5 4.4\) & \(81.8 6.9\) \\ BORel & \(32.4 2.4\) & \(-103.5 4.6\) & \(-240.6 4.3\) & \(-343.4 3.6\) & \(-167.8 5.4\) & \(82.5 6.0\) \\ SecBAD & \(38.5 3.1\) & \(-96.2 4.8\) & \(-202.4 10.4\) & \(-323.5 3.4\) & \(-155.3 5.4\) & \(101.4 9.2\) \\ ContraBAR & \(34.5 0.9\) & \(-101.6 3.2\) & \(-256.5 3.6\) & \(-312.3 4.8\) & \(-243.4 2.6\) & \(114.6 24.4\) \\ DynaMITE-RL & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.

Figure 5: Learning curves for DynaMITE-RL and state-of-the-art baseline methods. Shaded areas represent standard deviation over 5 different random seeds for each method and 3 for ScratchItch. In each of the evaluation environments, we observe that DynaMITE-RL exhibits better sample efficiency and converges to a policy with better environment returns than the baseline methods.

according to a predefined transition function. Goal locations are provided to the agent in the state--the only latent information is which goal has positive reward.

_Continuous Control._ We experiment with two tasks from OpenAI Gym : Reacher and HalfCheetah. Reacher is a two-jointed robot arm tasked with reaching a 2D goal location that moves along a circular path according to some unknown transition function. HalfCheetah is a locomotion task which we modify to incorporate changing latent contexts w.r.t. the target direction (HalfCheetah-Dir), target velocity (HalfCheetah-Vel), and target velocity with opposing wind forces (HalfCheetah-Wind+Vel).

_Assistive Ich Scratching._ Assistive Ichch Scratch is part of the Assistive-Gym benchmark  consisting of a human and a wheelchair-mounted 7-degree-of-freedom (DOF) Jaco robot arm. The human has limited-mobility and requires robot assistance to scratch an itch. We simulate stochastic latent context by moving the itch location--unobserved by the agent--along the human's right arm.

**Meta-RL Baselines.** We compare DynaMITE-RL to several state-of-the-art (approximately) Bayes-optimal meta-RL methods including RL\({}^{2}\), VariBAD , BORel , SecBAD , and Contra-BAR . RL\({}^{2}\) is an RNN-based policy gradient method which encodes environment transitions in the hidden state and maintains them across episodes. VariBAD reduces to RL\({}^{2}\) without the decoder and the variational reconstruction objective for environment transitions. BORel primarily investigates offline meta-RL (OMRL) and proposes a few modifications such as reward relabelling to address the identifiability issue in OMRL. We evaluate the off-policy variant of BORel, trained using Soft-Actor Critic (SAC) in our DLCMDP environments. Chen et al.  proposes the latent situational MDP (LS-MDP), in which there is non-stationary latent contexts that are sampled i.i.d., and SecBAD, an algorithm for learning in an LS-MDP. However, they do not consider latent dynamics which a crucial aspect in many applications. ContraBAR employs a contrastive learning objective to discriminate future observations from negative samples to learn an _approximate_ sufficient statistic of the history. As Zintgraf et al.  already demonstrate better performance by VariBAD than posterior sampling methods (e.g., PEARL ) we exclude such methods from our comparison.

**DynaMITE-RL outperforms prior meta-RL methods in a DLCMDP in both online and offline RL settings.** In Figure 5, we show the learning curves for DynaMITE-RL and baseline methods. We first observe that DynaMITE-RL significantly outperforms the baselines across all domains in sample efficiency and average environment returns. RL\({}^{2}\), VariBAD, BORel, SecBAD, and ContraBAR all perform poorly in the DLCMDP, converging to a suboptimal policy. VariBAD and BORel perform comparably as both share similar architecture, the only difference being the RL algorithm. By contrast, DynaMITE-RL accurately models the latent dynamics and consistently achieves high returns despite the nonstationary latent context. We also evaluate an oracle with access to ground-truth session terminations and find that DynaMITE-RL with learned session terminations effectively recovers session boundaries and matches oracle performance with sufficient training. Our empirical results validate that DynaMITE-RL learns a policy robust to changing latent contexts at inference time, while the baseline methods fail to adapt and are ultimately stuck in suboptimal behavior. We further demonstrate that DynaMITE-RL outperforms BORel in an offline RL setting in Table 2 across all environments. This highlights the importance of DynaMITE-RL training objectives in learning a more accurate posterior belief model even without online environment interactions. We also experimented with a Transformer encoder to parameterize our belief model and find that a more powerful model further improves the evaluation performance.

Figure 6: Ablating individual components of DynaMITE-RL. We observe that modelling latent dynamics is crucial in achieving good performance in a DLCMDP. Additionally, both consistency regularization and session reconstruction are critical for improving the sample efficiency and convergence to a better performing policy.

Each component of DynaMITE-RL contributes to efficient learning in a DLCMDP.We ablate the three key components of DynaMITE-RL to understand their impact on the resulting policy. We compare full DynaMITE-RL to: (i) DynaMITE-RL w/o Consistency, which does not include consistency regularization; (ii) DynaMITE-RL w/o Conditioning, which does not include latent conditioning; and (iii) DynaMITE-RL w/o SessRecon, which does not include session reconstruction. In Figure 6, we report the performance for each of these ablations and vanilla VariBAD for comparisons. First, without prior latent belief conditioning, the model converges to a suboptimal policy slightly better than VariBAD, confirming the importance of modeling the latent transition dynamics of a DLCMDP. Second, we find that session consistency regularization reinforces the inductive bias of changing dynamics and improves the sample efficiency of learning an accurate posterior model in DLCMDPs. Finally, session reconstruction masking also improves the sample efficiency by neglecting terms that are irrelevant and potentially biased. Similar ablation studies in the offline RL setting can be found in Table 2, reinforcing the importance of our proposed training objectives.

**DynaMITE-RL is robust to varying levels of latent stochasticity.** We study the effect of varying the number of latent context switches over an episode of fixed time horizon. For the HalfCheetah-Vel environment, we fix the episode horizon \(H=400\) to create multiple problems. We introduce a Bernoulli random variable, e.g \(d_{t} Bernoulli(p)\) where \(p\) is a hyperparameter we set to determine the probability that the latent context changes at timestep \(t\). If \(p=0\), the latent context remains unchanged throughout the entire episode, corresponding to a latent MDP. If \(p=1\), the latent context changes at every timestep, which is equivalent to a general POMDP. As shown in Figure 7, DynaMITE-RL performs better, on average, than VariBAD, with lower variance in a latent MDP. We hypothesize that, in the case of latent MDP, consistency regularization helps learn a more accurate posterior model by enforcing the inductive bias that the latent is static. Otherwise, there is no inherent advantage in modeling the latent dynamics if it is stationary.

As we gradually increase the number of context switches, the problem becomes more difficult and closer to a general POMDP. VariBAD performance decreases drastically because it is unable to model the changing latent dynamics while DynaMITE-RL is less affected, highlighting the robustness of our approach to changing latent contexts. When we set the number of contexts equal to the episode

  & Gridworld & Reacher & HC-Dir & HC-Vel & HC-Dir+Vel & ScratchItch \\  BORel & \(31.4_{ 3.5}\) & \(-102.0_{ 5.8}\) & \(-245.0_{ 12.4}\) & \(-354.0_{ 8.3}\) & \(-170.0_{ 5.4}\) & \(72.5_{ 4.6}\) \\ w/o Consistency & \(38.2_{ 1.2}\) & \(-33.2_{ 2.7}\) & \(-206.0_{ 5.6}\) & \(-212.0_{ 6.4}\) & \(-120.0_{ 12.4}\) & \(105.8_{ 8.5}\) \\ w/o Sess. Dynamics & \(33.4_{ 1.3}\) & \(-95.0_{ 5.2}\) & \(-244.0_{ 6.0}\) & \(-342.0_{ 8.6}\) & \(-166.0_{ 9.5}\) & \(74.1_{ 2.3}\) \\ DynaMITE-RL & \(41.8_{ 0.6}\) & \(-15.5_{ 3.2}\) & \(-154.0_{ 8.6}\) & \(-156.0_{ 4.8}\) & \(-48.0_{ 8.6}\) & \(225.5_{ 10.6}\) \\ w/ Transformer & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\  Oracle (w/ goal) & \(44.6\) & \(-4.8\) & \(-112.0\) & \(-132.2\) & \(-24.4\) & \(245.3\) \\  

Table 2: Average single episode returns with offline RL. Results are averaged across 5 random seeds. Algorithm with the highest average return are shown in bold. We present results for an oracle agent trained with goal information for reference.

Figure 7: Ablation studies on various frequencies of latent context switches within an episode in the HalfCheetah-Vel environment. The boxplot shows the distribution over evaluation returns for 25 rollouts of trained policies with VariBAD and DynaMITE-RL. When \(p=0\), we have a latent MDP and when \(p=1\) this is equivalent to a general POMDP.

horizon length, we recreate a fully general POMDP and again the performance between VariBAD and DynaMITE-RL converges.

## 6 Related Work

POMDPs provide a general framework modeling non-stationality and partial observability in sequential decision problems. Many model variants have been introduced, defining a rich spectrum between episodic MDPs and POMDPs. The Bayes-Adaptive MDP (BAMDP)  and hidden parameter MDP (HiP-MDP)  are both special cases of POMDPs in which environment parameters are unknown and the goal is to infer these parameters online during an episode. The BAMDP model treats unknown parameters as latent variables, which are updated based on the agent's observations, while the HiP-MDP assumes that the environment dynamics depend on hidden parameters that must be learned over time. However, neither framework addresses the dynamics of the latent parameters across sessions, but rather assumes it is constant throughout an episode.

On the other hand, models like the Latent Situational MDP (LSMDP)  and Dynamic Parameter MDP (DP-MDP)  do investigate nonstationary latent contexts. LSMDP  samples the latent contexts independently and identically distributed (i.i.d.) at each episode. While it introduces variability, it does not model the temporal dynamics or dependencies of these latent parameters. The DP-MDP framework addresses these dynamics by assuming that the latent parameters change at fixed intervals (fixed session lengths), making it less flexible when sessions are variable lengths. By contrast, DLCMDPs models the dynamics of the latent state and simultaneously infers _when_ the transition occurs, allowing better posterior updates at inference time.

DynaMITE-RL shares conceptual similarities with other meta-RL algorithms. Firstly, optimization-based techniques [16; 11; 36] learn neural network policies that can quickly adapt to new tasks at test time using policy gradient updates. This is achieved using a two-loop optimization structure: in the inner loop, the agent performs task-specific updates where it fine-tunes the policy with a few gradient steps using the task's reward function. In the outer loop, the meta-policy parameters are updated based on the performance of these fine-tuned policies across different tasks. However, these methods do not optimize for Bayes-optimal behavior and generally exhibit suboptimal test-time adaptation. Context-based meta-RL techniques aim to learn policies that directly infer task parameters at test time, conditioning the policy on the posterior belief. Such methods include recurrent memory-based architectures [13; 43; 30; 2] and variational approaches [20; 47; 12]. VariBAD, closest to our work, uses variational inference to approximate Bayes-optimal policies. However, we have demonstrated above the limitations of VariBAD in DLCMDPs, and have developed several crucial modifications to drive effective learning a highly performant policies in our setting.

## 7 Conclusion

We developed DynaMITE-RL, a meta-RL method to approximate Bayes-optimal behavior using a latent variable model. We presented the dynamic latent contextual Markov Decision Process (DLCMDP), a model in which latent context information changes according to an unknown transition function, that captures many natural settings. We derived a graphical model for this problem setting and formalized it as an instance of a POMDP. DynaMITE-RL is designed to exploit the causal structure of this model, and in a didactic GridWorld environment and several challenging continuous control tasks, we demonstrated that it outperforms existing meta-RL methods w.r.t. both learning efficiency and test-time adaptation in both online and offline-RL settings.

There are a number of exciting directions for future research building on the DLCMDP model. While we only consider Markovian latent dynamics in this work (i.e. future latent states are independent of prior latent states given the current latent state), we plan to investigate richer non-Markovian latent dynamics. We are also interested in exploring hierarchical latent contexts in which contexts change at different timescales. Finally, we hope to extend DynaMITE-RL to other real-world applications including recommender systems (RS), autonomous driving, multi-agent coordination, etc. DLCMDPs are a good model for RS as recommender agents often interact with users over long periods of time during which the user's latent context changes irregularly, directly influencing their preferences.