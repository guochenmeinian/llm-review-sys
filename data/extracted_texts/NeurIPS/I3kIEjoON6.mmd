# Unleashing the power of novel conditional generative approaches for new materials discovery

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

For a very long time, computational approaches to the design of new materials have relied on an iterative process of finding a candidate material and modeling its properties. AI has played a crucial role in this regard, helping to accelerate the discovery and optimization of crystal properties and structures through advanced computational methodologies and data-driven approaches. To address the problem of new materials design and fasten the process of new materials search, we have applied latest generative approaches to the problem of crystal structure design, trying to solve the inverse problem: by given properties generate a structure that satisfies them without utilizing supercomputer powers. In our work we propose two approaches: 1) conditional structure modification: optimization of the stability of an arbitrary atomic configuration, using the energy difference between the most energetically favorable structure and all its less stable polymorphs and 2) conditional structure generation. We used a representation for materials that includes the following information: lattice, atom coordinates, atom types, chemical features, space group and formation energy of the structure. The loss function was optimized to take into account the periodic boundary conditions of crystal structures. We have applied Diffusion models approach, Flow matching, usual Autoencoder (AE) and compared the results of the models and approaches. As a metric for the study, physical pymatgen matcher was employed: we compare target structure with generated one using default tolerances. So far, our modifier and generator produce structures with needed properties with accuracy 41% and 82% respectively. To prove the offered methodology efficiency, inference have been carried out, resulting in several potentially new structures with formation energy below the AFLOW-derived convex hulls.

+
Footnote â€ : Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.

## 1 Introduction

The search for novel materials with specified properties has been a cornerstone of scientific exploration for decades. From the discovery of semiconductors revolutionizing electronics to the development of superalloys enhancing aerospace technologies, the synthesis of new materials has continually propelled technological advancements.

However, traditional methods for material discovery often employ exhaustive trial and error experimental approaches. In turn, computational efforts, relying on density functional theory (DFT) approaches, usually require huge amounts of computing power. In this regard, automatic descriptor generators, GNNs and transferable GNN models  fueled combination of these methods and machine learning (ML) approaches. In particular, theutilization of generative machine learning models, such as Variational Autoencoder and GANs, presents a paradigm shift in how crystal structures are generated and optimized. By harnessing the power of data-driven approaches, we can navigate the vast landscape of possible crystal structures with unprecedented efficiency and precision.

Recent advancements in the field of materials discovery have yielded promising results through various innovative approaches. For instance, FTCP utilizes Autoencoders for uncovering new materials, while CubicGAN leverages GANs for the discovery of cubic crystal materials. Additionally, Physics Guided Crystal Generative Model (PGCGM) has introduced a method for generating crystal structures based on specific space groups encoding. DP-CDVAE is a model, that combines VAE and diffusion approaches. MatterGen employed equivariant GNNs as score matching function in diffusion processes for crystal structure generation.

One of the most discussed frameworks is GNoME that has made most recent and large advancements in the field of the new materials discovery employs a sophisticated pipeline to discover new materials, particularly focusing on inorganic crystals. This allows for the discovery of innovative materials beyond known structures.

After generating candidate structures through both pipelines, GNoME evaluates their stability by predicting their formation energies. Based on the comparison of the obtained formation energy with those of the known competing phases (i.e. stability assessment), the model selects the most promising candidates for further evaluation using known theoretical frameworks.

The question of the completeness of chemical space arises due to two main concerns with GNoME-derived stable structures. Firstly, they mostly contain three or more unique elements, while ternary and quaternary structures are less explored than binary compounds. Secondly, the comparison of GNoME-discovered structures to the Materials Project, which has 154,718 materials, is flawed since larger databases like AFLOW, NOMAD, and the Open Quantum Materials Database contain millions of entries. This raises questions about the novelty of the discovered materials.

In this study, we present an end-to-end framework for the generation of crystal structures with specified properties using advanced generative AI techniques. The basis architecture of the models is Autoencoder, enabling encoding and decoding structural representations. Then, the most commonly used generative approaches in image generation were utilized to model probability distribution transformations, and to capture complex underlying structure-property relationships within our dataset: Flow Matching, Denoising Diffusion Probabilistic Models(DDPM), and Denoising Diffusion Implicit Models(DDIM). Through the integration of these techniques, we aim to transcend conventional limitations in materials discovery, paving the way for accelerated predictions of materials with desired properties.

To employ model architectures often used for image/video generation, a matrix representation of crystal structures was developed, containing crucial information such as chemical composition, atomic coordinates, symmetries (space group), and formation energies. Within the approach proposed, it has become important to develop a novel metric for assessing the similarity between generated structures and target configurations. This metric obviates the need for computationally expensive DFT calculations, allowing for rapid validation and refinement of generated structures. Furthermore, we introduce a loss function that accounts for the periodic boundary conditions inherent in crystal lattices, ensuring the fidelity of the generated structures.

Our study explores two distinct approaches for crystal structure prediction: 1) conditional structure modification and 2) conditional structure generation. The former involves optimizing the stability of existing structures by generating more stable polymorphs, while the latter entails the generation of entirely new structures based on user-defined criteria. Through rigorous analysis, we demonstrate the efficacy of our approach in discovering novel materials with desired properties.

Importantly, to validate the utility of our framework, we conducted a series of generation experiments using the Vienna Ab initio simulation package(VASP) as a tool for inference validation. Remarkably, our method facilitated the discovery of 6 structures below the corresponding convex hull. This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials, thereby offering promising avenues for advanced materials discovery and design.

## 2 Data, Dataset

### Data overview

In this study, the AFLOW database was utilized as a source of data on the structures and properties of materials. AFLOW is an extensive and comprehensive database that consolidates a vast array of materials-related information, offering an expansive repository for crystallographic data, computed properties, and various other materials-science-related datasets. AFLOW database contains more than 3.5 million structures.

From the extensive collection housed within AFLOW, the focus was narrowed to select only polymorphs, because models are trained to distinguish composition-property and structure-property relations with numerous structures of the same chemical composition. Specifically, the selection process targeted polymorphic structures with 4 to 60 atoms within their unit cells. This criterion aimed to encompass a diverse yet manageable subset of structures, balancing complexity with computational feasibility. By filtering polymorphs based on their atom count, the dataset was balanced.

Moreover, in order to decrease the complexity of the data, we have removed all structures containing elements and space groups found in less than 1% of all structures. The entire dataset consisted of more than 85000 polymorph groups including more than 2.1 million structures. The minimum size of group of polymorphs was 7 samples and the maximum one was 71 samples. The total number of space groups was 19 and the total number of chemical species over the dataset was 55. Each structure \(S\) in the dataset is described by the following features:

* Fractional coordinates of atoms in the lattice basis \(_{}\) (has 60 rows with 3 coordinates \(x,y,z\) each) and \(_{}\) (matrix 3 by 3 constructed of 3 base vectors). Overall matrix X of structure is constructed as \[}=concatenatenation(_{ }},},_{ }})\] (1)
* Chemical elements which are presented as a one-hot matrix \(elements_{ij}\) of size \(64 103\) (including padding), where ones are positioned at the indices corresponding to the position of a certain chemical element in the periodic table. \[elements_{ij}=1&\\ 0&\]
* Elemental property matrix \(elementalProperties\) containing 22 chemical features encoding chemical elements obtained from . The properties of each element were calculated using Mendeleev package.
* Space group \(spg\) of a structure. We use the space group encoding method presented in , when each space group is represented by a \(192 4 4\) matrix, which corresponds to 192 possible symmetry operations.
* Structure formation energy \(E\)
* number of atoms in a crystal lattice.

### Data representation. Modification task

The crystal pair sampling strategy involves handling a potential data leakage: possible inclusion of structures from the same polymorph group but with different energies into training and validation subsets. To mitigate this issue, the polymorph group formulas were initially divided into distinct training and validation sets, ensuring a relatively balanced distribution of chemical elements across these subsets. Subsequently, the pairs were categorized into two groups: those with low-energy (lowest energy in polymorph group) targets designated as \(lowestEnergyPairs=(S_{i},S_{0}) i[1,...,structuresNum]\) and those with non-low-energy targets, all structures except the most optimal one, formed as \(nonLowestEnergyPairs=(S_{i},S_{j})|\)\(i>j>0\). The validation set was constructed as a subset of \(lowestEnergyPairs\). The training set was dynamically constructed every epoch from \(lowestEnergyPairs\) and \(nonlowestEnergyPairs\), preserving equal numbers of pairs sampled and maintaining a limited count per polymorph group. This strategy ensured a robust separation between training and validation sets, thus preventing data leakage and improving model performance.

Each pair sample \(\{S_{init},S_{target}\} pairDataset\) consisted of the information about each structure (hereinafter, we will call them initial and target structures). The following data was used:

* Coordinates and lattice information of initial and target structures \(X_{init},X_{target}\)
* Difference in formation energies between initial and target structures \(E_{diff}=E_{target}-E_{init}\)
* Space group of target structure \(spg_{target}\)
* Elements matrix \(elemetsMatrix\), elemental property matrix \(elementalProperties\) and number of sites \(numSites\), which are the same for initial and target structure because of identical chemical composition.

The modification task involved transforming the input structure \(X_{init}\) into the target structure \(X_{target}\).

### Data representation. Generation task

In its term the generation task receives normal or uniform (depends on a model) noise as input from which the structure is generated, which is akin to the image generation processes in computer vision tasks.

For the generation task, an additional dataset was constructed. Data for the generation task is slightly simpler, while it considers only \(\{S_{target}\}\). Therefore, the models can be trained on all structures available, rather than pairs. The following data is used:

* Coordinates and lattice information of target structure \(X_{target}\)
* Formation energy of target structure \(E_{target}\)
* Space group of target structure \(spg_{target}\)
* Elements matrix \(elemetsMatrix\), elemental property matrix \(elementalProperties\) and number of sites \(numSites\) of target structure.

## 3 Loss and metrics

### Atomic coordinates

The atomic coordinates are represented as a \(60 3\) matrix, where each row corresponds to the coordinates of an atom. The L1 loss was utilized during the training of a model for predicting atomic coordinates.

\(L_{1}(preds,target)_{i}=||preds_{i}-target_{i}||_{1}=_{j=1}^{3}|preds_{ ij}-target_{ij}|\), where \(target\) and \(pred\) are target and predicted atomic coordinate matrices.

### Lattice

The lattice itself is represented as a 3x3 matrix, where each row signifies a directing basis vector. In this case, we have also used the L1 norm as a loss function.

### Periodic boundary condition loss

This section presents an enhanced loss function, designed for the regression model (see Section 5.1), that addresses this challenge by integrating periodic boundary conditions into the loss calculation, outperforming the conventional L1 loss function. In the field of ML applied to atomic structures, even slight displacement of atomic coordinates is crucial and employing appropriate loss functions that consider the periodic nature of atomic structures increases the flexibility of model predictions.

In the dataset representing atomic structures, it is crucial to acknowledge the presence of atoms residing at various positions within the lattice framework. Certain atoms are positioned at the vertices, edges, or faces of the lattice. According to periodic boundary conditions (PBC), identical atoms in the vicinity of vertices, edges, or faces but also exist in analogous positions across the lattice. Implementation of such an invariance within the loss function helps in effectively capturing periodic pattern of crystals, enhancing the model's capability to learn and predict atomic structures more comprehensively.

The loss function is being calculated as minimum of distances from predicted point to the target one taking into account 26 its periodic images (according to PBC) A.4.

The empirical validation of this enhanced loss function showcases its superiority(Figure4) in capturing discrepancies within atomic structures, thus indicating its potential as a robust tool for improving the accuracy of ML models in materials science applications.

### Metric

As a metric, we have chosen an analogue of accuracy: the generated structures are compared to the target structures using a specialized matcher, yielding the proportion of structures that successfully pass the matching process. For metric calculation, we employed the Pymatgen StructureMatcher with the default set of parameters (\(ltol=0.2,stol=0.3,angle\_tol=5\)). Although this approach is less accurate than structure relaxation using ab initio calculations and comparing the structure formation energy with the energy above the hull, it enables model validation to be performed orders of magnitude faster than the traditional method.

## 4 Model

For experiments, a 1d UNet model (see Figure2 (b)) architecture similar to the 2d UNet model described in  was utilized along with 2D and 1D convolutional neural networks (CNNs) for the space group and element matrix embeddings, respectively. Based on this model, 3 different training processes have been developed: ordinary regression model, Conditional Flow Matching (CFM) model, and diffusion model.

The model was conditioned (see Figure2 (a)) on the following data: time condition (\(t\)), the same as in , element condition (\(el\)), formation energy difference condition (\(E_{diff}\)), and desirable space group (\(spg\)). \(el_{emb}\), \(spg_{emb}\) and \(E_{diff}\) are concatenated into one embedding

Figure 1: Illustration of atoms at a)vertices, b)edges, and c)faces of lattice under periodic boundary conditions

\(C_{emb}\). \(t\) is fed into the Transformer Positional Encoding Layer and transformed into an embedding \(T_{emb}\). The two embeddings: \(C_{emb}\) and \(T_{emb}\) are then applied into one condition.

## 5 Methodology

In this work, two approaches are proposed: crystal structure generation and crystal structure modification. For the generation approach, crystal structures are generated from normal or uniform noise and conditioned to \(t\), \(el\), \(E\), \(spg\). Within the generation, we employed three algorithms: DDPM, DDIM, and CFM models. For the modification approach, crystal structures are generated by modifying other structures, while conditioning to \(el\), \(E_{diff}\), \(spg\) (and optionally \(t\), not used in ordinary regression UNet). For the modification task, we have employed three algorithms: UNet Regression model, diffusion model, based on Palette approach, and CFM model. For the generation task, we have employed four algorithms: diffusion models with DDPM and DDIM samplers, and CFM models on Uniform and Normal noise.

### Regression model

During the training stage, the structure coordinates and lattice \(x_{0}\), elements features \(el\), space group \(spg\) and \(E_{diff}\) are used as conditions. The model is trained to return \(x_{1}\) structure coordinates and lattice (Algorithm 1). As for the inference process, one can see the details in the Algorithm 2

### Conditional Flow Matching models

CFM is a fast method for training Continuous Normalizing Flows (CNF) models without the need for simulations. It offers a training objective that enables conditional generative modeling and accelerates both training and inference.

The basic way of training CFM model (Algorithm 3) organized as follows: during the training stage, \(x_{0}\) and \(x_{1}\) are sampled from the source distribution and the target distribution respectively, then a linear interpolation \(x_{t}\) is calculated as \(x_{t}=tx_{1}+(1-t)x_{0}\) (exponential moving average between distributions \(x_{0}\) and \(x_{1}\); \(t\) is sampled from a uniform distribution \((0,1)\)), and afterwards pass the \(x_{t}\) and \(t\) as inputs to our model \(f_{}\), forcing the model to predict a velocity from the distribution \(x_{0}\) to \(x_{1}\). Therefore, the loss for CFM model is the following: \(L_{CFM}=E_{t,x_{1},x_{0}}[||f_{}(x_{t},t)-(x_{1}-x_{0})||^{2}]=E_{t,x_{ 1},x_{0}}[||f_{}(tx_{1}+(1-t)x_{0},t)-(x_{1}-x_{0})||^{2}]\)

For the modification approach, \(x_{0}\) and \(x_{1}\) are both sampled from our dataset distribution according to the sampling strategy for modification mentioned in 2.2. Also, the model is conditioned to \(el,spg_{1},E_{diff}\), besides \(t\) (see Algorithm 4)

Figure 2: a)Formation of conditions using formation energy, space group, and elemental representation, and b)Schematic depiction of the model architecture

For the generation approach, we tested two noise distributions for the \(x_{0}\): normal distribution \((0,1)\) and uniform noise distribution \((0,1)\), which resulted in significantly better performance. The intuition for using uniform distribution instead of normal one was inspired by the diagram of x, y, z coordinate distribution (Figure 3). The model is also conditioned to \(el,spg_{1},E\), and \(t\) (see Algorithm 5)

During the sampling stage, we generate \(X_{1}\) structure by the given \(X_{0}\) by solving the following ordinary differential equation (ODE): \(dx_{t}=f_{}(x_{t},t,el,spg_{1},E)dt\), beginning with \(x_{0}\). In order to solve the ODE, the Euler method was employed: \(x_{t+h}=x_{t}+hf_{}(x_{t},t,el,spg_{1},E)\) (Algorithm 6)

### Diffusion models

In our work, we observe diffusion models. Diffusion models generate samples from a target distribution \(x_{1}\), starting from a source distribution \(x_{0}(0,I)\).

During training, these models are trained to reverse a Markovian forward process, which adds noise \(x_{0}\) to the data step by step. Meaning, diffusion models are trained to predict the noise added to the data samples \(x_{1}\). In order to train a model in this setup, the following loss function is used, \(L_{simple}=E_{t,x_{1},x_{0}}[||x_{0}-f_{}(}}x_{1}+ }}x_{0},t)||^{2}]\) where \(}=_{s=1}^{t}_{s}\) and \(_{t}=1-_{t}\) (\(_{t}\) is the variance by which added noise is being scheduled on each step \(t\)).

Our modification approach is based on Palette, which enables sample-to-sample generation (from noise \((0,1)\)) using \(x_{0}\) structure coordinates and lattice, \(el\), \(spg_{1}\), \(E_{diff}\) and \(t\) as conditions for generation of \(x_{1}\) using the DDPM algorithm. Sampling stage is performed by a backward diffusion process with linear scheduler (see Algorithms 7, 8).

For the generation approach ((Algorithm 9), \(x_{0}\) is sampled from a normal distribution and \(el\), \(spg_{1}\), \(E\), \(t\) are fed into the model as conditions. During our experiments, we tested 2 approaches: DDPM(Algorithm 10) classic approach and DDIM(Algorithm 11) which results in usage of smaller number of sampling steps in order to speed up the generation process. Moreover, DDIM enables the process of generating samples from random noise to be deterministic.

## 6 Experiment Results

All the models presented in tables (Table 1 and Table 2) have been trained with the same hyperparameters and architectures. The metric used is described in Section 3.4. We also provide all experiment details in A.3.

## 7 Inference

In order to demonstrate a potential of the proposed approaches, we have chosen a chemical composition, containing numerous variations and phases of structures composed of [W, B, Ta] with well-explored convex hull. Structures that lie on the convex hull are considered to be thermodynamically stable, and the ones above it are either metastable or unstable.

   DDPM & DDIM & CFM \((0,1)\) & CFM \((0,1)\) \\ 
0.8074 & **0.82** & 0.482 & 0.8097 \\   

Table 1: Validation metrics on generation task

   Ordinary Model & Diffusion & CFM \\ 
**0.4148** & 0.3653 & 0.2059 \\   

Table 2: Validation metrics on modification task 

### Inference pipeline

The proposed testing procedure involves generating test conditions for structures, passing them to the trained generative models, pre-optimizing the generated structures to accelerate the following ab initio calculations, and final relaxation and formation energy calculating using VASP. Although in this work two approaches were proposed: Generation and Modification, the following pipeline has only been applied to generation models, due to the fact, that modification approach is based on structure-polymorphs, which leads to the necessity to have at least one structure with needed composition, which is not always so. That fact makes generation models much more flexible in generation structures not only with needed properties, but also with needed composition. Another advantage of the generation models is value of metric that is two times bigger than in modification tasks. The inference algorithm is as follows:

1. Test Condition Formation: * The chosen chemical formulas were utilized for feature extraction of \(el\). Three chemical compositions have been used: 1) Ta\({}_{1}\)W\({}_{1}\)B\({}_{6}\), 2) Ta\({}_{1}\)W\({}_{2}\)B\({}_{5}\) and 3) Ta\({}_{2}\)W\({}_{1}\)B\({}_{5}\). * We have taken \(spg\) presented in the dataset as an additional condition, obtaining 19 space groups. * Finally, a set of target formation energies \(E\) has been formed. We have carried out three experiments: 1) starting from the energy of the convex hull and decreasing with a step of 0.01 eV/atom, 2) starting from the energy of the convex hull and decreasing with a step of 0.1 eV/atom, and 3) starting from the energy 1 eV/atom less than the energy of the convex hull and decreasing with a step of 0.01 eV/atom. In total, 21 energy values were used for every inference run. * Final inference conditions were obtained by making all possible combinations of \(spg\) and \(E\) for a certain composition \(el\)
2. Model Inference: The conditions from the step 1 have been put to one of the trained models, resulting in the generation of structures. Two models have been employed: Diffusion approach and Flow matching
3. Pre-Optimization: Following the generation of all structures, each structure has been pre-optimized using the PyMatGen structure relaxation method. The method used m3gnet  model with default parameters. PyMatGen pre-optimization contributed to overall speedup of further VASP relaxation.
4. Structure relaxation: Pre-optimized structures were relaxed using VASP (the recommended pseudopotentials, plane wave energy cutoff of 500 eV, Edif and Ediffg convergence criteria of \(10^{-5}\) and \(-10^{-2}\) were used).

### Inference results

To summarize, 6 experiments have been carried out for two different models and for three formation energy conditionings. Every experiment includes 3*380 structures, per 380 structures for every single chemical composition. The results of experiments can be seen in Table 3

As can be seen, 4 structures were obtained with formation energies significantly lower than those obtained from the AFLOW-derived convex hull. Thus, it can be concluded that this observation indicates the potential stability of the generated structures rather than differences in the computational methods used in this work and during AFLOW generation. Another four structures also have energies below the convex hull, but in the vicinity of it. Thus, their potential stability should be interpreted with caution.

## 8 Data availability

The raw crystal dataset is downloaded from

https://aflowlib.org

## 9 Code availability

The source code for training and inferencing our models can be obtained from GitHub at https://github.com/AIRI-Institute/conditional-crystal-generation

## 10 Conclusion

In this article, we have offered two approaches to generate crystal structures: conditional generation and conditional modification. The first approach is significantly more flexible as it does not require structure-polymorphs, enabling the generation of structures without restrictions on chemical composition, which can be crucial in certain scenarios. Another advantage of the first approach is the simplicity of data preprocessing; it only requires the chemical composition, space group, atom coordinates, and formation energies.

Our methodology has experimentally proven its effectiveness, resulting in four confident potentially new crystal structures with the following energies above the hull: {-1.409, -5.497, -5.426, and -4.852} meV/atom, and four uncertain candidates with energies of {-0.483, -0.466, -0.387, and -0.042} meV/atom. We have demonstrated that conditional generation approaches, commonly used in image generation, are also fruitful in the design of new materials.

Although the proposed methodology demonstrates its efficiency in generating potentially new crystal structures, it has certain limitations. Firstly, the data is represented in a matrix form, which does not account for all possible symmetries of the crystal structures. Secondly, the structures in the dataset range from 4 to 60 atoms per unit cell, with most structures containing fewer than 8 atoms per unit cell. However, to perform well on structures with a large number of atoms per unit cell, the models just should be pretrained on a dataset that includes larger structures.

Furthermore, despite the limited number of experiments(6) and structures generated (7182), we succeeded in identifying hypothetically new structures. We hope that our article will help to reveal the potential of generative AI in design of new materials with targeted thermodynamic properties and inspire other researchers to be part of this innovative journey in materials design. We believe that rapid and efficient generation of novel materials can lead to breakthroughs in various fields such as electronics, pharmaceuticals, and energy storage. This can accelerate technological advancements and make cutting-edge technologies more accessible and affordable.

   & & Ta1W1B6, & Ta1W2B5, & Ta2W1B5, \\  & & meV/atom & meV/atom & meV/atom \\   & energy step = 0.01 &  &  &  \\  & energy gap = 0 & & & \\  & energy gap = 0 & & & \\  & energy step = 0.01 & & & \\  & energy gap = 1 & & & \\   & energy step = 0.01 &  &  & \)} \\  & energy gap = 0 & & & \\    & energy step = 0.1 & & & \\    & energy gap = 0 & & & \\    & energy gap = 1 & & & \\  

Table 3: Inference results. Each matrix element corresponds to either the minimal energy above the hull achieved in an experiment or the energy above the hull of structures with energies below the hull.