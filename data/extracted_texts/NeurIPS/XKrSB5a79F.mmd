# Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk

Yuzhou Gu

New York University

yuzhougu@nyu.edu

&Nikki Lijing Kuang

University of California, San Diego

llkuang@ucsd.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

&Zhao Song

Simons Institute for the Theory of Computing, UC Berkeley

magic.linuxkde@gmail.com

&Lichen Zhang

MIT CSAIL

lichenz@csail.mit.edu

###### Abstract

We consider the problem of sampling from a \(d\)-dimensional log-concave distribution \(()(-f())\) for \(L\)-Lipschitz \(f\), constrained to a convex body with an efficiently computable self-concordant barrier function, contained in a ball of radius \(R\) with a \(w\)-warm start.

We propose a _robust_ sampling framework that computes spectral approximations to the Hessian of the barrier functions in each iteration. We prove that for polytopes that are described by \(n\) hyperplanes, sampling with the Lee-Sidford barrier function mixes within \(((d^{2}+dL^{2}R^{2})(w/))\) steps with a per step cost of \((nd^{-1})\), where \( 2.37\) is the fast matrix multiplication exponent. Compared to the prior work of Mangoubi and Vishnoi, our approach gives faster mixing time as we are able to design a generalized soft-threshold Dikin walk beyond log-barrier.

We further extend our result to show how to sample from a \(d\)-dimensional spectralhedron, the constrained set of a semidefinite program, specified by the set \(\{x^{d}:_{i=1}^{d}x_{i}A_{i} C\}\) where \(A_{1},,A_{d},C\) are \(n n\) real symmetric matrices. We design a walk that mixes in \(((nd+dL^{2}R^{2})(w/))\) steps with a per iteration cost of \((n^{}+n^{2}d^{3-5})\). We improve the mixing time bound of prior best Dikin walk due to Narayanan and Rakhlin that mixes in \(((n^{2}d^{3}+n^{2}dL^{2}R^{2})(w/))\) steps.

## 1 Introduction

Given a convex body, generate samples from the body according to structured densities is a fundamental problem in computer science and machine learning. It has extensive applications in constrained convex optimization (Lovasz and Vempala, 2006; Narayanan, 2016), differentially private learning (Wang et al., 2015; Lin et al., 2024) and online optimization (Narayanan and Rakhlin, 2017). A central theme in the theory of sampling is to leverage stochasticity and reduce per iteration costs without having to proportionally increase the number of iterations. This theme has played out in continuous optimization for both first and second order methods. For the first order methods, focus is on reducing the variance of the gradient estimators (Johnson and Zhang, 2013; Shalev-Shwartz and Zhang, 2013; Defazio et al., 2014). For the second order methods, matrix sketching is often used toreduce computation and storage related to the Hessian matrix (Lee et al., 2019; Jiang et al., 2020, 2021; Song and Yu, 2021; Qin et al., 2023).

In Markov chain Monte Carlo (MCMC), much of the progress is on the first order methods. Non-asymptotic analyses are performed for the stochastic gradient Langevin algorithms and their variance reduced extensions (Dubey et al., 2016; Raginsky et al., 2017; Brosse et al., 2018; Chatterji et al., 2018; Zou et al., 2018; Dalalyan and Karagulyan, 2019; Li et al., 2019; Ding and Li, 2021). For second order methods, there has been a paucity when it comes to applying the sketching techniques.

In this paper, we focus on sampling from a log-concave distribution constrained to a \(d\)-dimensional convex body \(\) that is described by \(n\) constraints, where second order information is proven essential for capturing the geometry of the convex body and consequently for achieving fast convergence rates (Narayanan, 2016; Narayanan and Rakhlin, 2017; Chen et al., 2018; Laddha et al., 2020; Mangoubi and Vishnoi, 2023, 2024). In particular, we associate the convex body with a _self-concordant barrier function_(Nesterov and Nemirovskii, 1994; Renegar, 1988; Vaidya, 1989) and utilize the Hessian matrix \(H(x)\) of the barrier function in the sampling algorithm. We then use the Hessian matrix to propose samples and compute the probability to accept or reject the proposed samples. We note that this problem involves subtleties beyond the scope of constrained optimization. In continuous optimization, which focuses on finding the descent directions, unbiased estimators with reasonable variance oftentimes suffice (Vaidya, 1989; Lee et al., 2015; Huang et al., 2022). In MCMC, on the other hand, the target probability distribution needs to be maintained along the entire trajectory. This poses significant challenges to speed up the sampling algorithms. In the scenario of uniform sampling over a polytope, Laddha et al. (2020) shows that for a simple logarithmic barrier1, an unbiased estimator in fact suffices. However, more complicated barriers such as the Lee-Sidford barrier (Lee and Sidford, 2014, 2019) can only be approximately computed2, an unbiased estimator is extremely hard to be obtained. Moreover, for sampling from more sophisticated log-concave distributions and convex bodies, a more general approach is needed.

We therefore propose to obtain a high precision estimator to the desired acceptance rate with an improved per step running time and without sacrificing the rapid mixing time. In particular, we ask the following question:

_Can we significantly reduce per iteration cost, while preserving the convergence rate of the log-concave sampling algorithms over various convex bodies?_

We answer the above question in the affirmative. To this end, we present a slow of results regarding log-concave sampling. For polytopes, we provide a walk that mixes in \((d^{2}+dL^{2}R^{2})\) steps3 with per iteration cost \((nd^{-1})\). Prior state-of-the-art results are due to Mangoubi and Vishnoi (2023, 2024), for which their walks mix in \((nd+dL^{2}R^{2})\) with a per iteration cost4\(((A)+d^{2})\). Our walk mixes faster whenever \(n d\). Our result partially resolves an open problem in Mangoubi and Vishnoi (2023) as they asked whether it's possible to design a Dikin walk whose mixing time is only depends on \(d\) and independent of \(L,R\). We remove the \(L\) and \(R\) dependence on the dominating term \(d^{2}\), hence for the case of sampling from uniform distribution (\(L=0\)), we recover the state-of-art result of Laddha et al. (2020) which mixes in \((d^{2})\) steps with a per iteration cost \((nd^{-1})\).

We also consider sampling from convex bodies beyond polytopes. In semidefinite programming (SDP), one often focuses on the dual program of an SDP, where the constraint set is defined as a spectrahedron \(=\{x^{d}:_{i=1}^{d}x_{i}A_{i} C\}\)5 for \(n n\) symmetric matrices \(A_{i}\) and \(C\)(Jiang et al., 2020; Huang et al., 2022). We propose a walk that samples from a log-concave distribution over a spectrahedron \(\) using the Hessian information of the log-barrier. The walk mixes in \((nd+dL^{2}R^{2})\) steps. For log-barrier of an SDP, explicitly forming the Hessian matrix takes a prohibitively large \(O(n^{}d+n^{2}d^{-1})\) time. We utilize our robust sampling framework to approximately compute this Hessian via tensor-based sketching techniques, achieving a runtime of \((n^{}+n^{2}d^{3-5})\). As long as \(n d^{}\) (which is a usual setting for SDP, where \(d n\)), this provides a speedup.

### Related works

In this work, we focus on Dikin walk (Kannan and Narayanan, 2012), a refined variant of ball walk (Lovasz and Simonovits, 1993). Roughly speaking, ball walk progresses by moving to a random point in the ball centered at the current point with the obvious downside that when the convex body is flat, ball walk progresses slowly. Dikin walk overcomes this problem by instead moving to a random point in a good ellipse centered at the current point, in an effort to better utilize the local geometry of the convex body.

For sampling over polytopes, a number of Dikin walks use the ellipse induced by the log-barrier functions (Kannan and Narayanan, 2012). The work of Laddha et al. (2020) shows that uniform sampling over a polytope can mix in \(O( d)\) steps, where \(\) is the self-concordant parameter of the barrier function6. Going beyond uniform distributions, Narayanan and Rakhlin (2017) proposes a walk that samples from a log-concave distribution on a polytope in \((^{2}d)\) steps. This bound is later improved by Mangoubi and Vishnoi (2023), in which they show that for logarithmic barrier, a mixing time of \((nd+dL^{2}R^{2})\) is attainable, where \(L\) is the Lipschitz constant of \(f\) and \(R\) is the radius of the ball that contains \(\). A more recent work by Kook and Vempala (2024) has obtained better bound when the function \(f\) is \(\)-relative strongly-convex and the density \(\) is \(\)-Lipschitz. They manage to obtain a mixing bound of \(O( d(w/))\). However, their algorithm requires stronger assumptions on \(f\) and hence is incomparable to our result. In most previous works, the focus has been on improving the mixing time, rather than on per iteration costs.

For any convex body, it is well-known that a universal barrier with \(=d\) exists (Nesterov and Nemirovskii, 1989; Lee and Yue, 2021), however it is computationally hard to construct the Hessian of the universal barrier. In short, the universal barrier requires to compute the volume of the polar set of a high dimensional body, which is even hard to approximate deterministically (Furedi and Barany, 1986; Nesterov and Nemirovskii, 1994). The seminal work of Lee and Sidford (Lee and Sidford, 2014) presents a nearly-universal barrier with \(=O(d^{5}n)\) and the Hessian can be _approximated_ in \(O(nd^{-1})\) time. The Lee-Sidford barrier was originally designed to solve linear programs in \(O()\) iterations, and it has been leveraged lately for Dikin walks with rapid mixing time. For uniform sampling, Chen et al. (2018) gives an analysis with a walk that mixes in \((d^{2.5})\) steps. Subsequently, Laddha et al. (2020) improves the mixing to \((d^{2})\) steps. In a pursuit to better leverage the local geometry, Lee and Vempala (2017) proposes a walk relying on Riemannian metric that mixes in \((nd^{3/4})\) steps. The mixing rate is later improved to \((nd^{2/3})\) via Riemannian Hamiltonian Monte Carlo (RHMC) with log-barrier (Lee and Vempala, 2018) and \((n^{1/3}d^{4/3})\) with a mixed Lee-Sidford and log-barrier (Gatmiry et al., 2024). In this work, we show that log-concave sampling can also leverage Lee-Sidford barrier to obtain a mixing time of \((d^{2}+dL^{2}R^{2})\). In comparison, the hit-and-run algorithm (Lovasz and Vempala, 2007) mixes in \((d^{2}R^{2}/r^{2})\) steps where \(r\) is the radius of the inscribed ball inside \(\). For the regime where \(L=O(1)\) and \(r=O(1)\), our walk mixes strictly faster than that of hit-and-run.

We note that for sampling over more general convex bodies, a recent work (Chen and Eldan, 2022) proves that for uniform sampling over an isotropic convex body, the mixing time bound is \((d^{2}/_{d}^{2})\) where \(_{d}\) is the KLS constant (Kannan et al., 1995). However, it is unclear how to generalize their result to non-isotropic convex bodies or log-concave densities.

### Our results

Our results concern log-concave sampling from polytopes and spectrahedra. For polytopes, we state the result in its full generality.

**Theorem 1.1** (Robust sampling for log-concave distribution over polytopes).: _Let \((0,1)\) and \(R 1\). Given a constraint matrix \(A^{n d}\) with a vector \(b^{n}\), let \(:=\{x^{d}:Ax b\}\) be the corresponding polytope. Suppose \(\) is enclosed in a ball of radius \(R\) with non-empty interior. Let \(f:\) be an \(L\)-Lipschitz, convex function with an evaluation oracle. Finally, let \(\) be the distribution such that \( e^{-f}\)._

_Suppose we are given an initial point from \(\) that is sampled from a \(w\)-warm start distribution7 with respect to \(\) for some \(w>0\), then there is an algorithm (Algorithm 1) that outputs a point from a distribution \(\) where \((,)\)._

_Let \(g:\) be a \(\)-self-concordant barrier function such that in time \(_{g}\), a spectral approximation \(_{g}\) of the Hessian of \(g\) denoted by \(H_{g}\) can be computed satisfying_

\[(1-) H_{g}_{g}(1+) H_{g}\]

_for \(=(1/d)\). Then, Algorithm 1 takes at most_

\[(( d+dL^{2}R^{2})(w/))\]

_Markov chain steps. It uses \(O(1)\) function evaluations and an extra \(_{g}+d^{}\) time at each step._

Let us pause and make some remarks regarding Theorem 1.1. As long as the Hessian matrix can be approximately generated with an error inversely depends on \(d\), then our algorithm is guaranteed to converge. Moreover, if the approximate Hessian can be generated more efficiently, then this directly implies an improvement of our algorithm. On the mixing time side, Theorem 1.1 is nearly-optimal up to polylogarithmic factors and the dependence on \(L\) and \(R\). In particular, the \( d\) mixing time bound is also achieved by Laddha et al. (2020) in the case of uniform sampling. We instantiate the meta result of Theorem 1.1 into the following theorem.

**Theorem 1.2** (Robust sampling with nearly-universal barrier).: _Under the conditions of Theorem 1.1, let \(g\) be the Lee-Sidford barrier with \(=O(d^{5}n)\). Then, we have_

\[_{g}=(nd^{-1}).\]

_The algorithm takes at most_

\[((d^{2}+dL^{2}R^{2})(w/))\]

_Markov chain steps._

The Lee-Sidford barrier (Lee and Sidford, 2014, 2019) is the first polynomial-time computable barrier with a nearly-optimal self-concordance parameter. Several prior works (Chen et al., 2018; Laddha et al., 2020) utilize this barrier for uniform sampling. For log-concave sampling, the walk of Kook and Vempala (2024) requires strong-convexity-like assumption on \(f\) in order to attain an \((d^{2})\) mixing time. Our work is the first to obtain an \((d^{2})\) mixing time for log-concave sampling over polytopes, when \(L\) and \(R\) are small.