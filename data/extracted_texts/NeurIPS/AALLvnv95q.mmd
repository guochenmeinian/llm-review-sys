# Training Energy-Based Normalizing Flow with Score-Matching Objectives

Chen-Hao Chao\({}^{1}\), Wei-Fang Sun\({}^{1,2}\), Yen-Chang Hsu\({}^{3}\), Zsolt Kira\({}^{4}\), and Chun-Yi Lee\({}^{1}\)

\({}^{1}\) Elsa Lab, National Tsing Hua University, Hsinchu City, Taiwan

\({}^{2}\) NVIDIA AI Technology Center, NVIDIA Corporation, Santa Clara, CA, USA

\({}^{3}\) Samsung Research America, Mountain View, CA, USA

\({}^{4}\) Georgia Institute of Technology, Atlanta, GA, USA

Corresponding author. Email: cylee@cs.nthu.edu.tw

###### Abstract

In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from \((D^{2}L)\) to \((D^{3}L)\) for an \(L\)-layered model that accepts \(D\)-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis of the score-matching methods. The experimental results demonstrate that our approach achieves a significant speedup compared to maximum likelihood estimation while outperforming prior methods with a noticeable margin in terms of negative log-likelihood (NLL).

## 1 Introduction

Parameter estimation for probability density functions (pdf) has been a major interest in the research fields of machine learning and statistics. Given a \(D\)-dimensional random data vector \(^{D}\), the goal of such a task is to estimate the true pdf \(p_{}()\) of \(\) with a function \(p(\,;)\) parameterized by \(\). In the studies of unsupervised learning, flow-based modeling methods (e.g., [1; 2; 3; 4]) are commonly-adopted for estimating \(p_{}\) due to their expressiveness and broad applicability in generative tasks.

Flow-based models represent \(p(\,;)\) using a sequence of invertible transformations based on the change of variable theorem, through which the intermediate unnormalized densities are re-normalized by multiplying the Jacobian determinant associated with each transformation. In maximum likelihood estimation, however, the explicit computation of the normalizing term may pose computational challenges for model architectures that use linear transformations, such as convolutions [4; 5] and fully-connected layers [6; 7]. To address this issue, several methods have been proposed in the recent literature, which includes constructing linear transformations with special structures [8; 9; 10; 11; 12] and exploiting special optimization processes . Despite their success in reducing the training complexity, these methods either require additional constraints on the linear transformations or biased estimation on the gradients of the objective.

Motivated by the limitations of the previous studies, this paper introduces an approach that reinterprets flow-based models as energy-based models , and leverages score-matching methods [14; 15; 16; 17] tooptimize \(p(\,;)\) according to the Fisher divergence [14; 18] between \(p_{}()\) and \(p(\,;)\). The proposed method avoids the computation of the Jacobian determinants of linear layers during training, and reduces the asymptotic computational complexity of each training iteration from \((D^{3}L)\) to \((D^{2}L)\) for an \(L\)-layered model. Our experimental results demonstrate that this approach significantly improves the training efficiency as compared to maximum likelihood training. In addition, we investigate a theoretical property of Fisher divergence with respect to latent variables, and propose a Match-after-Preprocessing (MaP) technique to enhance the training stability of score-matching methods. Finally, our comparison on the MNIST dataset  reveals that the proposed method exhibit significant improvements in comparison to our baseline methods presented in  and  in terms of negative log likelihood (NLL).

## 2 Background

In this section, we discuss the parameterization of probability density functions in flow-based and energy-based modeling methods, and offer a number of commonly-used training methods for them.

### Flow-based Models

Flow-based models describe \(p_{}()\) using a prior distribution \(p_{}()\) of a latent variable \(^{D}\) and an invertible function \(g=g_{L} g_{1}\), where \(g_{i}(\,;):^{D}^{D},\, i \{1,,L\}\) and is usually modeled as a neural network with \(L\) layers. Based on the change of variable theorem and the distributive property of the determinant operation \(()\), \(p(\,;)\) can be described as follows:

\[p(;)=p_{}(g(;))|(_{g}(;))|=p_{}(g(;)) _{i=1}^{L}|(_{g_{i}}(_{i-1};))|,\] (1)

where \(_{i}=g_{i} g_{1}(;)\), \(_{0}=\), \(_{g}(;)=}g(;)\) represents the Jacobian of \(g\) with respect to \(\), and \(_{g_{i}}(_{i-1};)=_{i-1}} g_{i}(_{i-1};)\) represents the Jacobian of the \(i\)-th layer of \(g\) with respect to \(_{i-1}\). This work concentrates on model architectures employing _linear flows_ to design the function \(g\). These model architectures primarily utilize linear transformations to extract crucial feature representations, while also accommodating non-linear transformations that enable efficient Jacobian determinant computation. Specifically, let \(_{l}\) be the set of linear transformations in \(g\), and \(_{n}=\{g_{i}\,|\,i\{1,,L\}\}_{l}\) be the set of non-linear transformations. The general assumption of these model architectures is that \(_{i=1}^{L}|(_{g_{i}})|\) in Eq. (1) can be decomposed as \(_{g_{i}_{n}}|(_{g_{i}})| _{g_{i}_{l}}|(_{g_{i}})|\), where \(_{g_{i}_{n}}|(_{g_{i}})|\) and \(_{g_{i}_{l}}|(_{g_{i}})|\) can be calculated within the complexity of \((D^{2}L)\) and \((D^{3}L)\), respectively. Previous implementations of such model architectures include Generative Flows (Glow) , Neural Spline Flows (NSF) , and the independent component analysis (ICA) models presented in [6; 7].

Given the parameterization of \(p(\,;)\), a commonly used approach for optimizing \(\) is maximum likelihood (ML) estimation, which involves minimizing the Kullback-Leibler (KL) divergence \(_{}[p_{}()\|p(;)]= _{p_{}()}[}()}{p( ;)}]\) between the true density \(p_{}()\) and the parameterized density \(p(;)\). The ML objective \(_{}()\) is derived by removing the constant term \(_{p_{}()}[ p_{}()]\) with respect to \(\) from \(_{}[p_{}()\|p(;)]\), and can be expressed as follows:

\[_{}()=_{p_{}()}[-  p(;)].\] (2)

The ML objective explicitly evaluates \(p(;)\), which involves the calculation of the Jacobian determinant of the layers in \(_{l}\). This indicates that certain model architectures containing convolutional [4; 5] or fully-connected layers [6; 7] may encounter training inefficiency due to the \((D^{3}L)\) cost of evaluating \(_{g_{i}_{l}}|(_{g_{i}})|\). Although a number of alternative methods discussed in Section 3 can be adopted to reduce their computational cost, they either require additional constraints on the linear transformation or biased estimation on the gradients of the ML objective.

### Energy-based Models

Energy-based models are formulated based on a Boltzmann distribution, which is expressed as the ratio of an unnormalized density function to an input-independent normalizing constant. Specifically, given a scalar-valued energy function \(E(\,;):^{D}\), the unnormalized density function is defined as \((-E(;))\), and the normalizing constant \(Z()\) is defined as the integration \(_{^{D}}(-E(;))d\). The parameterization of \(p(\,;)\) is presented in the following equation:

\[p(;)=(-E(;))Z^{-1}().\] (3)

Optimizing \(p(\,;)\) in Eq. (3) through directly evaluating \(_{}\) in Eq. (2) is computationally infeasible, since the computation requires explicitly calculating the intractable normalizing constant \(Z()\). To address this issue, a widely-used technique  is to reformulate \(_{}()\) as its sampling-based variant \(_{}()\), which is expressed as follows:

\[_{}()=_{p_{}()}[ E(;)]-_{(p(;))}[E(; )],\] (4)

where \(()\) indicates the stop-gradient operator. Despite the fact that Eq. (4) prevents the calculation of \(Z()\), sampling from \(p(\,;)\) typically requires running a Markov Chain Monte Carlo (MCMC) process (e.g., ) until convergence, which can still be computationally expensive as it involves evaluating the gradients of the energy function numerous times. Although several approaches  were proposed to mitigate the high computational costs involved in performing an MCMC process, these approaches make use of approximations, which often cause training instabilities in high-dimensional contexts .

Another line of researches proposed to optimize \(p(\,;)\) through minimizing the Fisher divergence \(_{}[p_{}()||p(;)] =_{p_{}()}[\|}(}()}{p(; )})\|^{2}]\) between \(p_{}()\) and \(p(;)\) using the score-matching (SM) objective \(_{}()=_{p_{}()}[ \|}E(;)\|^{2}-(}{^{2}}E(;))]\) to avoid the explicit calculation of \(Z()\) as well as the sampling process required in Eq. (4). Several computationally efficient variants of \(_{}\), including sliced score matching (SSM) , finite difference sliced score matching (FDSSM) , and denoising score matching (DSM) , have been proposed.

SSM is derived directly based on \(_{}\) with an unbiased Hutchinson's trace estimator . Given a random projection vector \(^{D}\) drawn from \(p_{}\) and satisfying \(_{p_{}()}[^{T}]=\), the objective function denoted as \(_{}\), is defined as follows:

\[_{}()=_{p_{}( {x})}[\|;)}{}\|^{ 2}]-_{p_{}()p_{}()}[ ^{T}E(;)}{^{2}}].\] (5)

FDSSM is a parallelizable variant of \(_{}\) that adopts the finite difference method  to approximate the gradient operations in the objective. Given a uniformly distributed random vector \(\), it accelerates the calculation by simultaneously forward passing \(E(;)\), \(E(+;)\), and \(E(-;)\) as follows:

\[_{}() =2_{p_{}()}[E(;) ]-_{p_{}()p_{}()}[E(+;)+E(-;)]\] (6) \[+_{p_{}()p_{}()}[(E(+;)-E(- ;))^{2}],\]

where \(p_{}()=(^{D}|\,\|\|=)\), and \(\) is a hyper-parameter that usually assumes a small value. DSM approximates the true pdf through a surrogate that is constructed using the Parzen density estimator \(p_{}(})\). The approximated target \(p_{}(})=_{^{D}}p_{}(}|)p_{}()d\) is defined based on an isotropic Gaussian kernel \(p_{}(}|)=(}|,^{ 2})\) with a variance \(^{2}\). The objective \(_{}\), which excludes the Hessian term in \(_{}\), is written as follows:

\[_{}()=_{p_{}()p_{ }(}|)}[\|};)}{}}+-}}{^{2} }\|^{2}].\] (7)

To conclude, \(_{}\) is an unbiased objective that satisfies \(_{}()=_{}[p_{}()\|p( ;)]\), while \(_{}\) and \(_{}\) require careful selection of hyper-parameters \(\) and \(\), since \(_{}()=(_{}[p_{}()\|p( ;)]+o())\) contains an approximation error \(o()\), and \(p_{}\) in \(_{}()=_{}[p_{}(}) \|p(};)]\) may bear resemblance to \(p_{}\) only for small \(\).

Related Works

### Accelerating Maximum Likelihood Training of Flow-based Models

A key focus in the field of flow-based modeling is to reduce the computational expense associated with evaluating the ML objective [7; 8; 9; 10; 11; 29]. These acceleration methods can be classified into two categories based on their underlying mechanisms.

**Specially Designed Linear Transformations.** A majority of the existing works [8; 9; 10; 11; 29] have attempted to accelerate the computation of Jacobian determinants in the ML objective by exploiting linear transformations with special structures. For example, the authors in  proposed to constrain the weights in linear layers as lower triangular matrices to speed up training. The authors in [9; 10] proposed to adopt convolutional layers with masked kernels to accelerate the computation of Jacobian determinants. The authors in  leveraged orthogonal transformations to bypass the direct computation of Jacobian determinants. More recently, the authors in  proposed to utilize linear operations with special _butterfly_ structures  to reduce the cost of calculating the determinants. Although these techniques avoid the \((D^{3}L)\) computation, they impose restrictions on the learnable transformations, which potentially limits their capacity to capture complex feature representations, as discussed in [7; 31; 32]. Our experimental findings presented in Appendix A.5 support this concept, demonstrating that flow-based models with unconstrained linear layers outperform those with linear layers restricted by lower / upper triangular weight matrices  or those using lower-upper (LU) decomposition .

**Specially Designed Optimization Process.** To address the aforementioned restrictions, a recent study  proposed the relative gradient method for optimizing flow-based models with arbitrary linear transformations. In this method, the gradients of the ML objective are converted into their relative gradients by multiplying themselves with \(^{T}\), where \(^{D D}\) represents the weight matrix in a linear transformation. Since \(}|()| ^{T}=\), evaluating relative gradients is more computationally efficient than calculating the standard gradients according to \(}|()|=( {W}^{T})^{-1}\). While this method reduces the training time complexity from \((D^{3}L)\) to \((D^{2}L)\), a significant downside to this approach is that it introduces approximation errors with a magnitude of \(o()\), which can escalate relative to the weight matrix values.

### Training Flow-based Models with Score-Matching Objectives

The pioneering study  is the earliest attempt to train flow-based models by minimizing the SM objective. Their results demonstrate that models trained using the SM loss are able to achieve comparable or even better performance to those trained with the ML objective in a low-dimensional experimental setup. More recently, the authors in  and  proposed two efficient variants of the SM loss, i.e., the SSM and FDSSM objectives, respectively. They demonstrated that these loss functions can be used to train a non-linear independent component estimation (NICE)  model on high-dimensional tasks. While the training approaches of these works bear resemblance to ours, our proposed method places greater emphasis on training efficiency. Specifically, they directly implemented the energy function \(E(;)\) in the score-matching objectives as \(- p(;)\), resulting in a significantly higher computational cost compared to our method introduced in Section 4. In Section 5, we further demonstrate that the models trained with the methods in [16; 17] yield less satisfactory results in comparison to our approach.

## 4 Methodology

In this section, we introduce a new framework for reducing the training cost of flow-based models with linear transformations, and discuss a number of training techniques for enhancing its performance.

### Energy-Based Normalizing Flow

Instead of applying architectural constraints to reduce computational time complexity, we achieve the same goal through adopting the training objectives of energy-based models. We name this approach as Energy-Based Normalizing Flow (EBFlow). A key observation is that the parametric density function of a flow-based model can be reinterpreted as that of an energy-based model through identifyingthe input-independent multipliers in \(p(\,;)\). Specifically, \(p(\,;)\) can be explicitly factorized into an unnormalized density and a corresponding normalizing term as follows:

\[ p(;)&=p_{}(g( ;))_{i=1}^{L}|(_{g_{i}}(_{i-1}; ))|\\ &=}(g(;))_{g _{i}_{}}|(_{g_{i}}(_{i-1}; ))|}_{} _{l}}|(_{g_{i}}())|}_{} ;))}_{}( )}_{}\] (8)

where the energy function \(E(\,;)\) and the normalizing constant \(Z^{-1}()\) are selected as follows:

\[E(;)-(p_{}(g(;) )_{g_{i}_{}}|(_{g_{i}}(_{i-1};))|\,),Z^{-1}()=_{g_{i}_{l}}| (_{g_{i}}())|\,.\] (9)

The detailed derivations of Eqs. (8) and (9) are elaborated in Lemma A.11 of Section A.1.2. By isolating the computationally expensive term in \(p(\,;)\) as the normalizing constant \(Z()\), the parametric pdf defined in Eqs. (8) and (9) becomes suitable for the training methods of energy-based models. In the subsequent paragraphs, we discuss the training, inference, and convergence property of EBFlow.

**Training Cost.** Based on the definition in Eqs. (8) and (9), the score-matching objectives specified in Eqs. (5)-(7) can be adopted to prevent the Jacobian determinant calculation for the elements in \(_{l}\). As a result, the training complexity can be significantly reduced to \((D^{2}L)\), as the \((D^{3}L)\) calculation of \(Z()\) is completely avoided. Such a design allows the use of arbitrary linear transformations in the construction of a flow-based model without posing computational challenge during the training process. This feature is crucial to the architectural flexibility of a flow-based model. For example, fully-connected layers and convolutional layers with arbitrary padding and striding strategies can be employed in EBFlow without increasing the training complexity. EBFlow thus exhibits an enhanced flexibility in comparison to the related works that exploit specially designed linear transformations.

**Inference Cost.** Although the computational cost of evaluating the exact Jacobian determinants of the elements in \(_{l}\) still requires \((D^{3}L)\) time, these operations can be computed only once after training and reused for subsequent inferences, since \(Z()\) is a constant as long as \(\) is fixed. In cases where \(D\) is extremely large and \(Z()\) cannot be explicitly calculated, stochastic estimators such as the importance sampling techniques (e.g., [33; 34]) can be used as an alternative to approximate \(Z()\). We provide a brief discussion of such a scenario in Appendix A.3.

**Asymptotic Convergence Property.** Similar to maximum likelihood training, score-matching methods that minimize Fisher divergence have theoretical guarantees on their _consistency_[14; 16]. This property is essential in ensuring the convergence accuracy of the parameters. Let \(N\) be the number of independent and identically distributed (i.i.d.) samples drawn from \(p_{}\) to approximate the expectation in the SM objective. In addition, assume that there exists a set of optimal parameters \(^{*}\) such that \(p(;^{*})=p_{}()\). Under the regularity conditions (i.e., Assumptions A.1-A.7 shown in Appendix A.1.1), _consistency_ guarantees that the parameters \(_{N}\) minimizing the SM loss converges (in probability) to its optimal value \(^{*}\) when \(N\), i.e., \(_{N}^{*}\) as \(N\). In Appendix A.1.1, we provide a formal description of this property based on  and derive the sufficient condition for \(g\) and \(p_{}\) to satisfy the regularity conditions (i.e., Proposition A.10).

### Techniques for Enhancing the Training of EBFlow

As revealed in the recent studies [16; 17], training flow-based models with score-matching objectives is challenging as the training process is numerically unstable and usually exhibits significant variances. To address these issues, we propose to adopt two techniques: match after preprocessing (MaP) and exponential moving average (EMA), which are particularly effective in dealing with the above issues according to our ablation analysis in Section 5.3.

**MaP.** Score-matching methods rely on the score function \(-}E(;)\) to match \(} p_{}()\), which requires backward propagation through each layer in \(g\). This indicates that the training process could be numerically sensitive to the derivatives of \(g\). For instance, logit pre-processing layers commonly used in flow-based models (e.g., [1; 4; 5; 7; 8; 35]) exhibit extremely large derivatives near 0 and 1, which might exacerbate the above issue. To address this problem, we propose to exclude the numerically sensitive layer(s) from the model and match the pdf of the pre-processed variable during training. Specifically, let \(_{k} g_{k} g_{1}()\) be the pre-processed variable, where \(k\) represents the index of the numerically sensitive layer. This method aims to optimize a parameterized pdf \(p_{k}(\,;) p_{}(g_{L} g_{k+1}( \,;))_{i=k+1}^{L}|(_{g_{i}})|\) that excludes \((g_{k},,g_{1})\) through minimizing the Fisher divergence between the pdf \(p_{_{k}}()\) of \(_{k}\) and \(p_{k}(\,;)\) by considering the (local) behavior of \(_{}\), as presented in Proposition 4.1.

**Proposition 4.1**.: _Let \(p_{_{j}}\) be the pdf of the latent variable of \(_{j} g_{j} g_{1}()\) indexed by \(j\). In addition, let \(p_{j}()\) be a pdf modeled as \(p_{}(g_{L} g_{j+1}())_{i=j+1}^{L}| (_{g_{i}})|\), where \(j\{0,,L-1\}\). It follows that:_

\[_{}[p_{_{j}}\|p_{j}]=0 _{}[p_{}\|p_{0}]=0, j\{1, ,L-1\}.\] (10)

The derivation is presented in Appendix A.1.3. In Section 5.3, we validate the effectiveness of the MaP technique on the score-matching methods formulated in Eqs. (5)-(7) through an ablation analysis. Please note that MaP does not affect maximum likelihood training, since it always satisfies \(_{}[p_{}\|p_{j}]=_{}[p_{}\|p_{0}]\), \( j\{1,,L-1\}\) as revealed in Lemma A.12.

**EMA.** In addition to the MaP technique, we have also found that the exponential moving average (EMA) technique introduced in  is effective in improving the training stability. EMA enhances the stability through smoothly updating the parameters based on \( m+(1-m)_{i}\) at each training iteration, where \(\) is a set of shadow parameters , \(_{i}\) is the model's parameters at iteration \(i\), and \(m\) is the momentum parameter. In our experiments presented in Section 5, we adopt \(m=0.999\) for both EBFlow and the baselines.

## 5 Experiments

In the following experiments, we first compare the training efficiency of the baselines trained with \(_{}\) and EBFlow trained with \(_{}\), \(_{}\), \(_{}\), and \(_{}\) to validate the effectiveness of the proposed method in Sections 5.1 and 5.2. Then, in Section 5.3, we provide an ablation analysis of the techniques introduced in Section 4.2, and a performance comparison between EBFlow and a number of related studies [7; 16; 17]. Finally, in Section 5.4, we discuss how EBFlow can be applied to generation tasks. Please note that the performance comparison with [8; 9; 10; 11; 12; 29] is omitted, since their methods only support specialized linear layers and are not applicable to the employed model architecture  that involves fully-connected layers. The differences between EBFlow, the baseline, and the related studies are summarized in Table A4 in the appendix. The sampling process involved in the calculation of \(_{}\) is implemented by \(g^{-1}(;)\), where \( p_{}\). The transformation \(g(\,;)\) for each task is designed such that \(_{l}\) and \(_{n}\). For more details about the experimental setups, please refer to Appendix A.2.

### Density Estimation on Two-Dimensional Synthetic Examples

In this experiment, we examine the performance of EBFlow and its baseline on three two-dimensional synthetic datasets. These data distributions are formed using Gaussian smoothing kernels to ensure \(p_{}()\) is continuous and the true score function \(} p_{}()\) is well defined. The model \(g(\,;)\) is constructed using the Glow model architecture , which consists of actnorm layers, affine coupling layers, and fully-connected layers. The performance are evaluated in terms of the KL divergence and the Fisher divergence between \(p_{}()\) and \(p(;)\) using independent and identically distributed (i.i.d.) testing sample points.

Table 1 and Fig. 1 demonstrate the results of the above setting. The results show that the performance of EBFlow trained with \(_{}\), \(_{}\), and \(_{}\) in terms of KL divergence is on par with those trained using \(_{}\) as well as the baselines trained using \(_{}\). These results validate the efficacy of training EBFlow with score matching.

Figure 1: The visualized density functions on the Sine, Swirl, and Checkerboard datasets. The column ‘True’ illustrates the visualization of the true density functions.

### Efficiency Evaluation on the MNIST and CIFAR-10 Datasets

In this section, we inspect the influence of data dimension \(D\) on the training efficiency of flow-based models. To provide a thorough comparison, we employ two types of model architectures and train them on two datasets with different data dimensions: the MNIST  (\(D=1 28 28\)) and CIFAR-10  (\(D=3 32 32\)) datasets.

The first model architecture is exactly the same as that adopted by . It is an architecture consisting of two fully-connected layers and a smoothed leaky ReLU non-linear layer in between. The second model is a parametrically efficient variant of the first model. It replaces the fully-connected layers with convolutional layers and increases the depth of the model to six convolutional blocks. Between every two convolutional blocks, a squeeze operation  is inserted to enlarge the receptive field. In the following paragraphs, we refer to these models as 'FC-based' and 'CNN-based' models, respectively.

The performance of the FC-based and CNN-based models is measured using the negative log likelihood (NLL) metric (i.e., \(_{p_{}()}[- p(;)]\)), which differs from the intractable KL divergence by a constant. In addition, its normalized variant, the Bits/Dim metric , is also measured and reported. The algorithms are implemented using PyTorch with automatic differentiation , and the runtime is measured on NVIDIA Tesla V100 GPUs. In the subsequent paragraphs, we assess the models through scalability analysis, performance evaluation, and training efficiency examination.

**Scalability.** To demonstrate the scalability of KL-divergence-based (i.e., \(_{}\) and \(_{}\)) and Fisher-divergence-based (i.e., \(_{}\), \(_{}\), and \(_{}\)) objectives used in EBFlow and the baseline method, we first present a runtime comparison for different choices of the input data size \(D\). The results presented in Fig. 2 (a) reveal that Fisher-divergence-based objectives can be computed more efficiently than KL-divergence-based objectives. Moreover, the sampling-based objective \(_{}\) used in EBFlow, which excludes the calculation of \(Z()\) in the computational graph, can be computed slightly faster than \(_{}\) adopted by the baseline.

   Dataset & Metric & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (DSM) & EBFlow (FDSSM) \\   & Fisher Divergence (\(\)) & 6.86 \(\) 0.73 e-1 & 6.65 \(\) 1.05 e-1 & **6.25 \(\) 0.84 e-1** & 6.66 \(\) 0.44 e-1 & 6.66 \(\) 1.33 e-1 \\  & KL Divergence (\(\)) & **4.56 \(\) 0.00 e+0** & **4.56 \(\) 0.00 e+0** & **4.56 \(\) 0.01 e+0** & 4.57 \(\) 0.02 e+0 & 4.57 \(\) 0.01 e+0 \\   & Fisher Divergence (\(\)) & 1.42 \(\) 0.48 e-10 & 1.42 \(\) 0.53 e-10 & 1.35 \(\) 0.10 e-10 & **1.34 \(\) 0.06 e-1** & 1.37 \(\) 0.07 e-0 \\  & KL Divergence (\(\)) & **4.21 \(\) 0.00 e+0** & **4.21 \(\) 0.01 e+0** & 4.25 \(\) 0.04 e-0 & 4.22 \(\) 0.02 e+0 & 4.25 \(\) 0.06 e+0 \\   & Fisher Divergence (\(\)) & 7.24 \(\) 11.50 e-1 & 1.23 \(\) 0.75 e-0 & 7.07 \(\) 1.93 e-1 & **7.03 \(\) 0.51** & 7.08 \(\) 1.62 e-1 \\  & KL Divergence (\(\)) & **4.80 \(\) 0.02 e+0** & 4.81 \(\) 0.02 e+0 & 4.85 \(\) 0.05 e+0 & 4.82 \(\) 0.05 e+0 & 4.83 \(\) 0.03 e+0 \\   

Table 1: The evaluation results in terms of KL-divergence and Fisher-divergence of the flow-based models trained with \(_{}\), \(_{}\), \(_{}\), \(_{}\), and \(_{}\) on the Sine, Swirl, and Checkerboard datasets. The results are reported as the mean and 95% confidence interval of three independent runs.

    &  \\   &  &  \\  Num. Perm. &  &  \\  Method & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (FDSSM) & Baseline (ML) & EBFlow (SM) & EBFlow (SSM) & EBFlow (OSM) & EBFlow (FDSSM) \\  NLL (\(\)) & 1002.4 \(\) 0.1 & **1002.3 \(\) 0.6** & 1002.8 \(\) 0.3 & 1002.9 \(\) 0.2 & 1104.1 \(\) 0.5 & 1101.3 \(\) 1.3 & **1003.3 \(\) 6.6** & 11017.5 \(\) 1.4 & 1109.5 \(\) 2.4 & 1122.1 \(\) 3.1 \\ Bin(\(\)) & **2.00 \(\) 0.00** & **2.61 \(\) 0.00** & **2.61 \(\) 0.00** & 2.02 \(\) 0.00 & 2.03 \(\) 0.00 & **2.01 \(\) 0.00** & 2.03 \(\) 0.00 & 2.04 \(\) 0.00 & 2.06 \(\) 0.01 \\   &  &  & 3.11 & 66.67 & **102.1** & 0.21 & 0.29 & 7.09 & 18.32 & **30.76** \\   &  &  \\   & Num. Perm. &  \\  Method & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (Poisson) & Baseline (ML) & EBFlow (SSM) & EBFlow (OSM) & EBFlow (Poisson) \\  NLL (\(\)) & **10192.5 \(\) 10.8** & 11915.6 \(\) 5.6 & 11917.2 \(\) 15.5 & 11910.0 \(\) 0.6 & 12347.8 \(\) 6.8 & **11408.7 \(\) 2.67** & 11553.64 \(\) 18.7 & 11465.5 \(\) 12.6 & 11463.3 \(\) 7.9 & 11766.0 \(\) 36.8 \\ Bin(\(\)) & **5.59 \(\) 0.00** & **5.00 \(\) 0.00** & **5.00 \(\) 0.01** & **5.00 \(\) 0.00** & **5.00 \(\) 0.00** & **5.00 \(\) 0.01** & 5.41 \(\) 0.07 & 5.37 \(\) 0.00 & 5.38 \(\) 0.00 & 5.54 \(\)

**Performance.** Table 2 demonstrates the performance of the FC-based and CNN-based models in terms of NLL on the MNIST and CIFAR-10 datasets. The results show that the models trained with Fisher-divergence-based objectives are able to achieve similar performance as those trained with KL-divergence-based objectives. Among the Fisher-divergence-based objectives, the models trained using \(_{}\) and \(_{}\) are able to achieve better performance in comparison to those trained using \(_{}\). The runtime and performance comparisons above suggest that \(_{}\) and \(_{}\) can deliver better training efficiency than \(_{}\) and \(_{}\), since the objectives can be calculated faster while maintaining the models' performance on the NLL metric.

**Training Efficiency.** Fig. 2 (b) presents the trends of NLL versus training wall time when \(_{}\), \(_{}\), \(_{}\), \(_{}\), and \(_{}\) are adopted as the objectives. It is observed that EBFlow trained with SSM and DSM consistently attain better NLL in the early stages of the training. The improvement is especially notable when both \(D\) and \(L\) are large, as revealed for the scenario of training CNN-based models on the CIFAR-10 dataset. These experimental results provide evidence to support the use of score-matching methods for optimizing EBFlow.

### Analyses and Comparisons

**Ablation Study.** Table 3 presents the ablation results that demonstrate the effectiveness of the EMA and MaP techniques. It is observed that EMA is effective in reducing the variances. In addition, MaP significantly improves the overall performance. To further illustrate the influence of the proposed MaP technique on the score-matching methods, we compare the optimization pro

Figure 3: The norm of \(_{}()\) of an FC-based model trained on the MNIST dataset. The curves and shaded area depict the mean and 95% confidence interval of three independent runs.

    &  \\  EMA & MaP & EBFlow(SSM) & EBFlow(DSM) & EBFlow(FDSSM) \\   & & 1757.5 \(\) 28.0 & 4660.3 \(\) 19.8 & 32670 \(\) 99.2 \\ ✓ & & 1720.5 \(\) 0.8 & 44550 \(\) 1.6 & 3166.3 \(\) 17.3 \\ ✓ & ✓ & **1092.8 \(\) 0.3** & **1099.2 \(\) 0.2** & **1104.1 \(\) 0.5** \\   \\  EMA & MaP & EBFlow(SSM) & EBFlow(DSM) & EBFlow(FDSSM) \\   & & 35180 \(\) 33.9 & 31700 \(\) 7.2 & 3593.3 \(\) 125. \\ ✓ & & 3504.5 \(\) 2.4 & 3180.0 \(\) 2.9 & 3560.3 \(\) 1.7 \\ ✓ & ✓ & **1107.5 \(\) 14** & **1109.5 \(\) 26** & **1122.1 \(\) 3.1** \\   

Table 3: The results in terms of NLL of the FC-based and CNN-based models trained using SSM, DSM, and FDSSM losses on MNIST. The performance is reported in terms of the means and 95% confidence intervals of three independent runs.

Figure 2: (a) A runtime comparison of calculating the gradients of different objectives for different input sizes (\(D\)). The input sizes are \((1,n,n)\) and \((3,n,n)\), with the x-axis in the figures representing \(n\). In the format \((c,h,w)\), the first value indicates the number of channels, while the remaining values correspond to the height and width of the input data. The curves depict the evaluation results in terms of the mean of three independent runs. (b) A comparison of the training efficiency of the FC-based and CNN-based models evaluated on the validation set of MNIST and CIFAR-10. Each curve and the corresponding shaded area depict the mean and confidence interval of three independent runs.

cesses with \(_{}[p_{_{k}}\|p_{k}]\) and \(_{}[p_{}\|p_{0} ]=_{p_{_{k}}(_{k}) }[\|(_{k}}(_{k}}( _{k})}{p_{k}(_{k})}))_{i=1}^{k}_{g_{i}}\|^{2}]\) (i.e., Lemma A.13) by depicting the norm of their unbiased estimators \(_{}()\) calculated with and without applying the MaP technique in Fig. 3. It is observed that the magnitude of \(\|_{}()\|\) significantly decreases when MaP is incorporated into the training process. This could be attributed to the fact that the calculation of \(_{}[p_{_{k}}\|p _{k}]\) excludes the calculation of \(_{i=1}^{k}_{g_{i}}\) in \(_{}[p_{}\|p_{0}]\), which involves computing the derivatives of the numerically sensitive logit pre-processing layer.

**Comparison with Related Works.** Table 4 compares the performance of our method with a number of related works on the MNIST dataset. Our models trained with score-matching objectives using the same model architecture exhibit improved performance in comparison to the relative gradient method . In addition, when compared to the results in  and , our models deliver significantly improved performance over them. Please note that the results of [7; 16; 17] presented in Table 4 are obtained from their original papers.

### Application to Generation Tasks

The sampling process of EBFlow can be accomplished through the inverse function or an MCMC process. The former is a typical generation method adopted by flow-based models, while the latter is a more flexible sampling process that allows conditional generation without re-training the model. In the following paragraphs, we provide detailed explanations and visualized results of these tasks.

**Inverse Generation.** One benefit of flow-based models is that \(g^{-1}\) can be directly adopted as a generator. While inverting the weight matrices in linear transformations typically demands time complexity of \((D^{3}L)\), these inverse matrices are only required to be computed once \(\) has converged, and can then be reused for subsequent inferences In this experiment, we adopt the Glow  model architecture and train it using our method with \(_{}\) on the MNIST dataset. We compare our visualized results with the current best flow-based model trained using the score matching objective . The results of  are generated using their officially released code with their best setup (i.e., FDSSM). As presented in Fig. 4, the results generated using our model demonstrate significantly better visual quality than those of .

**MCMC Generation.** In comparison to the inverse generation method, the MCMC sampling process is more suitable for conditional generation tasks such as data imputation due to its flexibility . For the imputation task, a data vector \(\) is separated as an observable part \(_{O}\) and a masked part \(_{M}\). The goal of imputation is to generate the masked part \(_{M}\) based on the observable part \(_{O}\). To achieve this goal, one can perform a Langevin MCMC process to update \(_{M}\) according to the gradient of the energy function \(}E(;)\). Given a noise vector \(\) sampled from \((,)\) and a

    & Method & Complexity & NLL (\(\)) \\  _{}\)-Based} & Baseline (ML) & \((D^{3}L)\) & \(1092.4 0.1\) \\  & EBFlow (SML) & \((D^{3}L)\) & \(1092.3 0.6\) \\  & Relative Grad.  & \((D^{2}L)\) & \(1375.2 1.4\) \\  _{}\)-Based} & EBFlow (SSM) & \((D^{2}L)\) & \(1092.8 0.3\) \\  & EBFlow (DSM) & \((D^{2}L)\) & \(1099.2 0.2\) \\   & EBFlow (FDSSM) & \((D^{2}L)\) & \(1104.1 0.5\) \\   & SSM  & - & \(3355\) \\   & DSM  & - & \(3398 1343\) \\   & FDSSM  & - & \(1647 306\) \\   

Table 4: A comparison of performance and training complexity between EBFlow and a number of related works [16; 7; 17] on the MNIST dataset.

small step size \(\), the process iteratively updates \(_{M}\) based on the following equation:

\[_{M}^{(t+1)}=_{M}^{(t)}-_{M}^{(t )}}E(_{O},_{M}^{(t)};)+,\] (11)

where \(_{M}^{(t)}\) represents \(_{M}\) at iteration \(t\{1,,T\}\), and \(T\) is the total number of iterations. MCMC generation requires an overall cost of \((TD^{2}L)\), potentially more economical than the \((D^{3}L)\) computation of the inverse generation method. Fig. 5 depicts the imputation results of the FC-based model trained using \(_{}\) on the CelebA  dataset (\(D=3 64 64\)). In this example, we implement the masking part \(_{M}\) using the data from the KMNIST  and MNIST  datasets.

## 6 Conclusion

In this paper, we presented EBFlow, a new flow-based modeling approach that associates the parameterization of flow-based and energy-based models. We showed that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be bypassed, resulting in an improved training time complexity. In addition, we demonstrated that the training stability and performance can be effectively enhanced through the MaP and EMA techniques. Based on the improvements in both theoretical time complexity and empirical performance, our method exhibits superior training efficiency compared to maximum likelihood training.