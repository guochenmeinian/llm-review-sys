# Scalarization for Multi-Task and

Multi-Domain Learning at Scale

 Amelie Royer, Tijmen Blankevoort, Babak Ehteshami Bejnordi

Qualcomm AI Research

Amsterdam, The Netherlands

{aroyer, tijmen, behtesha}@qti.qualcomm.com

Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

###### Abstract

Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and model sizes. Following these insights, we then propose to leverage population-based training to efficiently search for the optimal scalarization weights when dealing with a large number of tasks or domains.

## 1 Introduction

Learning a unified architecture that can handle multiple domains and/or tasks offers the potential for improved computational efficiency, better generalization, and reduced training data requirements. However, training such models proves a challenge: In particular, when the tasks or domains are dissimilar, practitioners often observe that the learning of one task or domain interferes with the learning of others. This phenomenon is commonly known as interference or negative transfer. How to best resolve this interference is an open problem spanning numerous bodies of literature, such as multi-task learning, domain adaptation and generalization, or multi-modal learning.

In fact, there is currently no unanimous understanding of what causes, or how to predict, task interference in practice: For instance, classical generalization bounds on training from multiple data sources  involve a measure of distance between the respective tasks/domains distributions, which can be intuitively thought of as a measure of interference. In more practical applications, task clustering methods  propose various metrics to measure task affinity, and train tasks with low affinity separately, with separate backbones, thereby circumventing the interference issue at the architecture level. Finally, in the multi-task optimization (MTO) literature, the prevailing view is that gradients from different tasks may point in conflicting directions and cause interference, which has led to a line of work on reducing gradient conflicts by normalizing gradients or losses statistics . While MTO have established a new state-of-the-art for training multi-task models in the past few years, recent work  shows that, surprisingly, minimizinga simple average of the tasks/domains losses, also known as _scalarization_, can yield performance trade-off points on the same Pareto front as more costly MTO methods.

Nevertheless, there still remains some unexplored areas which we aim to shed lights on in this paper. First, most previous experiments have only been conducted on multi-task settings, on benchmarks with either few tasks or few training samples (e.g multi-MNIST, NYU-v2), and for a fixed architecture. In particular, the link between model capacity and the observed MTL performance is often overlooked. Secondly, it is not clear how the choice of scalarization weights links to and/or impacts the hypothesis of gradient conflicts as a key underlying signal of task interference. Finally, scalarization can become prohibitively expensive for a large number of tasks as the search space for the optimal loss weights grows exponentially; This raises the issue of how to efficiently browse this search space. In this work, we investigate these questions to further motivate scalarization as a simple and scalable training scheme for multi-task and multi-domain problems.

**Our key contributions.** We perform a large-scale analysis of scalarization for both multi-task (MTL) and multi-domain learning (MDL). We cover a wide range of model capacities, datasets with varying sizes, and different task/domain combinations. Our key conclusions are as follows:

* **(C1)** When compared to a model trained on each task/domain individually, MDL/MTL performance tends to improve for larger model sizes, showing that the benefits of MTL/MDL frameworks should be put into perspective with respect to the backbone architecture capacity.
* **(C2)** Tuning the scalarization weights for a specific tasks/domains combination is crucial to obtaining the optimal MTL/MDL performance in settings with high imbalance; Nevertheless, the relative performance of different scalarization weights is often consistent across model capacities inside a family architecture. This suggests that searching for optimal scalarization weights for a lower model depth/width is also relevant for the full model size, while using less compute.
* **(C3)** Gradients conflicts between tasks/domains naturally occur during MTL training: They behave differently across layers and learning rates, but are scarcely impacted by model capacity and scalarization weights choice. These observations give new insights into the practical implications of avoiding conflicting gradients throughout training.
* **(C4)** We leverage fast hyperparameter search methods such as population-based training  to efficiently browse the search space of scalarization weights as the number of tasks grows.

## 2 Related work

Multi-Task Optimization (MTO) and scalarization.MTO methods aim to improve MTL training by balancing the training dynamics of different tasks. Prior research can be split into two categories: On the one hand, loss-based methods propose to align the task losses magnitudes by rescaling them through various criteria, e.g., task uncertainty , task difficulty , random loss weighting , or gradients statistics [7; 14; 60]. On the other hand, gradient-based methods directly act on the per-task gradients rather than the losses. For instance, [55; 13] tackle MTL as a multi-objective optimization problem using the Multiple Gradient Descent Algorithm to directly locate Pareto-optimal solutions. Another major line of work considers conflicting gradient direction to be the main cause of task interference. Consequently, these works [64; 37; 38; 62; 8; 25] aim to mitigate conflicts across per-task gradients. For instance, PCGrad  suggests projecting each task's gradient onto the normal plane of other gradients to suppress conflicting directions, while GradDrop  ensures that all gradient updates are pure in sign by randomly masking all positive or negative gradient values during training. Unfortunately, gradient-based techniques require per-task gradients, leading to substantial memory usage and increased runtime. Furthermore, recent work [63; 30] has shown that these methods often perform on-par with the less costly approach of directly optimizing the average of the task losses, also known as uniform scalarization. Building off these insights, we further investigate the benefits of scalarization and the practical dynamics of gradient conflicts in this work.

Architectures for MTL.Orthogonal to our work, another line of research focuses on designing multi-task architectures that mitigate task interference by optimizing the allocation of shared versus task-specific parameters. Hard parameter sharing methods [27; 26; 6; 59; 61] build off a shared backbone and carefully design task-specific decoder heads optimized for the tasks of interest. In contrast, soft parameter sharing methods [45; 53; 20; 39] jointly train a shared backbone while learning sparse binary masks specific to each task/domain which capture the parameter sharing pattern across tasks. To select which tasks should share parameters or not, a prominent line of work focuses on defining measures of task affinity [2; 58; 22; 16]: Tasks with high affinity are jointly trained, while low affinity ones use different backbones, in the hope of reducing potential interference by fully separating conflicting tasks. This results in multi-branch architectures where each branch handles a specific subset of tasks with high affinities. The scope of our study is complementary to this line of work, as we focus on how to best train a given set of tasks, without changing the architecture.

Multi-Domain Learning (MDL).MDL refers to the design and training of a single unified architecture that can operate on data sources from different domains. Domain adaptation and generalization methods often aim to align the learned representations of the different domains [19; 4; 66]; These are in particular targeting the setting where one of the domain lacks supervisory signals. In the fully-supervised scenario, several methods have been proposed to use a unified backbone, which captures common features across all tasks, while allowing the model to learn domain-specific information via lightweight adapter modules [49; 50; 51]. Similar to soft parameter sharing method, these methods bypass task interference by keeping the shared backbone frozen and only training domain-specific modules. Alternatively, a common practice when training with multiple input datasets is to use over- or undersampling techniques [1; 28; 43], in particular to handle class imbalance. As we describe later, resampling can be seen as the MDL counterpart of scalarization in MTL. Both enable a simple and lightweight training scheme, yet finding the optimal resampling/scalarizatoin weights is nontrivial. This motivates us to empirically verify our findings in the MDL as well as MTL setting.

## 3 Motivation and experimental setting for MTL/MDL

### Notations

We start by describing the MTL/MDL setup: Given \(T\) supervised datasets with respective inputs \(X_{t}\) and outputs \(Y_{t}\), our goal is to find model parameters \(^{*}\) that minimize the datasets' respective losses: \((_{1}(X_{1},Y_{1}),,_{T}(X_{T},Y_{T}))\). In practice, this can be achieved either by solving a multi-objective problem on the task losses [13; 42] or, more commonly, by minimizing a (possibly weighted) average of the losses . In both cases, most training methods for MTL/MDL can be phrased as updating the model parameters \(\) using a weighted average of the individual tasks' gradients:

\[_{i+1}=_{i}+_{t=1}^{T}p_{t}^{i}\,_{(x_{t},y_{t})  q(X_{t},Y_{t})}_{_{i}}_{t}(x_{t},y_{t})\ \] (1)

where \(()=(p_{1}^{i},,p_{T}^{i})\) captures the dynamic importance scaling weights for each dataset at timestep \(i\), and \(q(X_{t},Y_{t})\) is the underlying distribution the \(t\)-th dataset is drawn from. We also follow the common assumption that \(\) is a probability distribution, _i.e._, \(_{t}p_{t}=1\) and \( t,\ p_{t}\{0,1\}\).

We further distinguish between **(i)** multi-task learning (MTL) where every input is annotated for every task (i.e., \( i,j,X_{i}=X_{j}\)), and **(ii)** multi-domain learning (MDL) where the goal is to solve a common output task across multiple input domains or modalities (\( i,j,Y_{i}=Y_{j}\)). In the MDL setting, we can also rephrase (1) as resampling the data distributions according to \(\). Both formalisms are equivalent, but resampling often performs better in practice when using stochastic gradient methods :

\[^{T}p_{t}_{q(X_{t},Y_{t})}f(x_{t},y_{t})}_{ )}}=_{q^{}(X,Y)}f(x,y)}_{}\ (x,y)=_{t=1}^{T}_{x  X_{t}}q(x,y)p_{t}}_{}\] (2)

### Motivation

Current state-of-the-art methods for training objectives such as (1) can be roughly organized into three categories, based on how the datasets' importance weights \(\) are defined:

* **Scalarization** defines the weights \(p_{t}\) as constant hyperparameters: The update rule of (1) reduces to computing the gradient of the weighted average loss \(_{}(_{t}p_{t}_{t})\). In the MDL formalism of (2), scalarization is similar to classical oversampling/undersampling, where \(p_{t}\) is defined either as a scalar (per-domain) or a vector (to handle both per-class and per-domain biases). In both settings, the key difficulty lies in tuning the weights \(\), as the search space grows exponentially with \(T\).

* **Loss-based adaptive methods**[38; 26] dynamically compute \(p_{t}\) for every batch, aiming to uniformize training dynamics across tasks by rescaling the losses. For instance, IMTL-L  dynamically reweighs the losses such that they all have the same magnitude.
* **Gradient-based adaptive methods**[7; 64; 38; 25; 8; 37; 13; 55; 36; 42] also dynamically compute weights for every batch. While this line of work usually outperforms loss-based methods, they also incur a higher compute and memory training cost, as they require \(T\) backward passes to obtain each individual dataset gradient, which in turn need to be stored in memory.

While gradient-based approaches are generally considered SotA across MTO methods, they present certain practical challenges: They come with a higher computational and memory cost, which increases with \(T\), as illustrated in Figure 1(b). The implicit weights \(_{t}\) computed by these methods are also hard to extract and transfer to other training runs since they are tied to other hyperparameters (training length, batch size, gradient clipping, etc.). In addition, recent work [63; 30; 52] has shown that simple scalarization with uniform weights actually often performs on-par with both loss- and gradient-based methods. Furthermore, scalarization is highly practical: It does not incur additional costs during training, and scalarization weights are easily interpretable as a measure of "task importance" in the optimization problem. However, some experiments in [58; 52] also suggest that the benefits of uniform scalarization MTL are impacted by model capacity in some settings.

Motivated by these insights, we aim to better harness scalarization for scalable and practical MTL/MDL training. In particular, we focus on the following two remaining gray areas: **First**, we perform a large-scale analysis investigating the underlying effects of model capacity on MTL and MDL _(C1)_ in Section 4. We then discuss the impact of model capacity on the choice of optimal scalarization weights _(C2)_ in Section 4.1 and on the presence of conflicting gradients _(C3)_ in Section 4.2. **Secondly**, browsing the search space of \(\) to tune the scalarization weights becomes increasingly costly as the number of tasks \(T\) grows. As an alternative to the usual grid- or random-search approaches, we propose to leverage population-based training as a cost-efficient parameter search for scalarization, and report our conclusions _(C4)_ in Section 5.

### Experimental setting

Here we briefly describe the experimental setup that is used throughout the paper. We aim to cover a wide range of model sizes, datasets with varying sizes, and different task/domain combinations for MDL and MTL. A summary is given in Figure 1(a) and further details can be found in Appendix 1.

For MDL, we first consider a two-domain example composed of CIFAR10  and STL10  with a ViT-S backbone optimized for small datasets . We then expand the results to the DomainNet

Figure 1: Table **(a)** summarizes the experimental setups used throughout the paper. Figure **(b)** reports profiling results for popular multi-task optimization (MTO) methods in a small-scale (ResNet18, batch size 16) and large-scale (ResNet50, size 128) setting, for 224px inputs: In practice, we find that the main bottleneck for gradient-based methods is their high memory usage. Consequently, training with reasonable batch sizes requires either high parallelism (compute demand) or gradient accumulation (slower runs). In contrast, loss-based methods have much better-constrained costs. However, they usually underperform their gradient-based counterpart in the literature.

benchmark  containing 345 output classes and 6 input domains. We use ResNet as our main backbone, following previous works . In both cases, the backbone parameters are fully shared across all domains; We use cross-entropy as training loss and top-1 accuracy as our main metric.

For MTL, we use CelebA  with the same ViT-S backbone as for CIFAR/STL; We split the 40 attributes into semantically-coherent groups to form 8 output classification tasks, as described in the appendix. The transformer backbone is fully shared across tasks, while the last linear classification layer is task-specific. Then, we experiment on a larger MTL setting for dense prediction: Most traditional benchmarks, such as Cityscapes  and NYU-v2 , are rather small, as they only contain 2-3 dense tasks (segmentation, normals, depth) and \(\) 5k fully annotated images. Instead, we perform our analysis on the challenging Taskonomy dataset . To keep experiments scalable, we filter the 26 tasks in Taskonomy down to 7 (e.g. by removing self-supervised tasks or clearly overlapping ones such as 2D edges and 3D edges). For training, we follow the setup described in : We normalize every dense annotation to have zero mean and unit variance across the training set, and use the \(L_{1}\) loss as our training loss and common metric for all tasks. The backbone is composed of a (shared) ResNet encoder, followed by task-specific decoders built with upsampling operations and 1x1 convolutions. To control model capacity, we vary _(i)_ the width of the model, _(ii)_ the depth of the encoder, as well as _(iii)_ the number of layers in the decoder(s) which are shared across tasks.

Finally, in all settings, we sweep over the domains/tasks' weights under the common assumption that \(p\) is a probability vector (i.e. \( t,\ p_{t}\{0,1\}\) and \(_{t}p_{t}=1\)). All reported results are averaged across 2 random seeds for DomainNet and Taskonomy, and 3 for the smaller benchmarks. Unless stated, every experiment is conducted on a single NVIDIA V100 GPU.

## 4 Benefits of MDL/MTL under the lens of model capacity

We first investigate the behavior of scalarization for MDL/MTL training with two tasks, while varying model capacity and for different tasks/domains combinations. To quantify the benefit or harm of joint training with multiple datasets, we compare the model performance on each dataset with that of a same-sized model, trained on a single dataset at once. We refer to this baseline as "SD" (single dataset). Since we use SD as a reference point, we ensure that its training pipeline is well tuned: We sweep over the hyperparameters (learning rate, weight decay, number of training steps, etc) of the baseline SD and keep the same values for training the MDL/MTL models, while varying the task weights \((p_{1},p_{2}=1-p_{1})\) from \(p_{1}=0\) (SD baseline of the first dataset) to \(p_{1}=1\) (SD baseline of the second dataset). Further details on the training hyperparameters can be found in Appendix1. Finally, we summarize our analysis results in Figure2: For each task/domain pair, we first plot the weight \(p_{1}^{*}\) yielding the best accuracy averaged across both tasks/domains, then we report the accuracy difference between the corresponding MDL/MTL model and the associated SD baseline.

**Impact of model capacity.** We first observe that MDL/MTL performance greatly varies across model capacities, and tends to increase with model size (\(CI\)): In some cases, the trend even inverts from MDL/MTL underperforming to outperforming SD (e.g. DomainNet's real + sketch pairing).

**Selecting optimal importance weights \(\).** Secondly, the best-performing \(\) at inference are rarely the uniform \(p_{1}=p_{2}=0.5\), even in the Taskonomy scenario where the uniform training objective is identical to the average of task metrics which we aim to maximize at inference. We further discuss the effects of tuning scalarization weights for the accuracy/efficiency trade-off _(C2)_ in Section4.1.

**MTL/MDL primarily improves generalization.** As we show in Appendix2), even when MTL/MDL outperforms the SD baseline at inference, it is generally not the case at training time. In other words, tuning the respective tasks/domains weights in MTL/MDL can be seen as a regularization technique that improves generalization over the SD baselines by balancing the two datasets' training objectives. In particular, in the MDL setting, this insight is also closely connected to data augmentations: Each new domain can be seen as a non-parametric and non-trivial augmentation function, while the associated resampling weight \(p_{t}\) is the probability of applying the augmentation on each sample.

Interestingly, recent work  suggests that many MTO methods which aim to avoid gradient conflicts can also be interpreted as regularization techniques, and that simpler tricks such as early stopping are often competitive. To better understand the link between these two insights, we analyze the natural emergence of conflicting gradients during MDL/MTL training, and how they are impacted by model capacity _(C3)_ in Section4.2.

### Selecting optimal scalarization weights \(^{*}\)

So far, we have discussed MDL/MTL improvement over the SD baseline when selecting the weights \(p^{*}\) that maximize average accuracy. However, in real-world settings, many criteria come into play when evaluating MTL/MDL models: For instance, one task may be more critical than others hence the model should be evaluated via weighted accuracy. Second, even if the MTL/MDL model underperforms the corresponding SD baseline at the same model capacity, it may still improve the efficiency-to-accuracy trade-off when both tasks need to be solved at once. To give a clearer overview of how these considerations affect the choice of weights \(\), we report results for all scalarization weights in Figure 3; We also highlight points where MTL/MDL outperforms SD on _both_ domains/tasks, and represent each model's number of parameters as the marker size.

First, we observe a clear asymmetry in terms of performance across the \(T=2\) datasets, even when using uniform weighing \(p_{1}=0.5\): For instance on Taskonomy (Figure 3), several MTL models outperform SD on the depth prediction task, but none on the semantic segmentation task. Nevertheless, MTL models are very appealing when taking model efficiency into account, as they contain roughly half as many parameters. Secondly, tuning the scalarization weights is crucial in some settings: For instance in DomainNet, training with \(p_{1}[0.65,0.75]\) is more advantageous than uniform scalarization. Furthermore, the relative ranking of the weight \(p_{1}\), with respect to model performance, does not change significantly across model capacities, for both the Taskonomy and

Figure 2: Performance of scalarization for MDL/MTL relative to SD under different model capacities: Each column corresponds to a different task/domain pair (\(T=2\)). The first row of each plot contains a heatmap of the best performing scalarization weights \(p^{*}\) wrt. to the average test accuracy on both tasks. Each of the following rows contains the difference in metrics between the MTL/MDL model and its counterpart SD baseline, where green indicates positive changes and purple, a negative one. Note that the colormaps’ ranges are defined _per row_, and visualized as color bars at the beginning of each row. We observe the general trend that the performance improvement of MDL, relative to the corresponding SD baseline, tends to increase with model capacity.

DomainNet examples. This suggests that optimal weights \(^{*}\) for a given model may be a good search starting point for another architecture of the same family (see Appendix 4).

Nevertheless, the search space for \(\) grows exponentially with \(T\), making the search computationally prohibitive, even for one architecture. In Section 5, we propose a scalable approach to optimize scalarization weights and investigate its performance on DomainNet and CelebA.

### Conflicting gradients in practice

A widely spread explanation for task interference in the literature is that individual task gradients may point in conflicting directions, hampering training. To investigate this behavior, we measure the percentage of conflicting gradients pair encountered in each epoch, when training with uniform scalarization. Following , we define gradients as conflicting if and only if the cosine of their angle is negative. Finally, we provide further details and figures for this section in Appendix 3.

In Figure 4, we first illustrate an asymmetric characteristic of gradient conflicts: A high number of gradient conflicts typically translates to poorly performing models (e.g. early training), but the lower conflicts regime does not correlate well with MDL/MTL performance. In particular, it is common to encounter more gradient conflicts towards the end of training, while the loss steadily decreases. This suggests that, in practice, identifying and removing conflicts at every training iteration may be superfluous, with respect to the compute and memory cost occurred. We can also put this observation in perspective with Theorem 3 of , which states that in the case of two tasks, a parameter update of PCGrad leads to a lower loss than the uniform scalarization update if the tasks are conflicting enough; however, this assumption may not hold true across every training iteration.

Figure 4: Proportion of conflicting gradients during uniform scalarization training with and without PCGrad on the 40 attribute classification tasks of CelebA while varying learning rate ([5e-4, 5e-3, 1e-2]), model depth () and width ([0.5, 0.75, 1]). While the overall number of encountered conflicts differs, the trend is consistent across both settings in that the higher number of conflicts encountered towards the end of training does not harm training or final model performance.

Figure 3: MDL/MTL performance when varying tasks weight \(p_{1}\) for different model capacities. High opacity markers represent models that outperform their respective SDL baseline (black markers) on _both_ tasks/domains. Dashed lines connect models with the same architecture. The marker size is proportional to the number of parameters in each model.

Secondly, we analyze the effect of different factors of training variations on the observed proportion of gradient conflicts in Figure 5(a). On the one hand, hyperparameters directly related to weights updates, such as the learning rate of batch size, have a clear impact. In particular, since memory consumption is often a bottleneck of gradient-based MTO methods, this means that naively decreasing batch size to reduce memory usage may severely impact such methods in practice. On the other hand, model capacity has a lesser effect on the pattern of gradient conflicts, despite influencing task interference as highlighted in Figure 2. Nevertheless, we also observe that the overall magnitude of conflicts encountered for different pairs of tasks reveal interesting task affinity patterns, as illustrated in Figure 5(b): For instance, the quickdraw domain of DomainNet appears as a clear outlier (across all model capacities), which is also the case in terms of MDL performance in Figure 2.

In summary, we observe intriguing properties of gradient conflicts in practice, in particular suggesting that the extra cost of measuring, storing and correcting conflicting gradients at every training iteration can be superfluous. As an alternative,  shows that less costly regularization methods (e.g., early stopping) can be competitive with MTO. In this work, we consider an orthogonal direction, by tuning the scalarization/resampling weights hyperparameters to regulate the training speed of the different tasks/domains. However, the computational cost of browsing this large search space becomes a practical caveat as \(T\) grows. In the next section, we leverage a scalable hyperparameter tuning algorithm to tackle this problem and apply it on the DomainNet and CelebA datasets.

## 5 Population-based training for scalarization weights selection

Tuning the weights \(\) with classical parameter search methods, such as grid search or random search, becomes extremely costly when \(T\) increases. To address this high computational demand, we propose to leverage the population-based training (PBT)  framework which has been used for efficient hyperparameter search in reinforcement learning  and for data augmentation pipelines .

Population-based training (PBT).PBT is an evolutionary algorithm for hyperparameter search: \(N\) models are trained in parallel with different starting hyperparameters. Every \(E_{ready}\) epochs, the models synchronize: The \(Q\%\) worst models in the population are stopped, and their model weights and hyperparameters are replaced by the ones of the \(Q\%\) best models (_exploit step_); Then, the newly copied hyperparameters are randomly perturbed to reach a new part of the hyperparameter search space (_explore step_). Finally, training resumes until \(E\) epochs are reached. In other words, PBT enables dynamic exploration of the hyperparameter search space with a fixed computational cost (cost of training \(N\) models + potential overhead from synchronization). A follow-up work, Population-based Bandits (PB2)  proposes to leverage Bayesian optimisation  to better guide the explore step. In contrast to PBT, PB2 also offers theoretical guarantees on the convergence rate.

Using PBT to tune scalarization weights.PBT tuning relies on three important characteristics that may conflict with the standard scalarization MDL/MTL training pipeline:

* Models are trained with a dynamic schedule of hyperparameters. While this contrasts with standard scalarization in which \(\) is fixed, we do not expect this to be an issue as recent work has shown that scalarization with dynamic random weights performs well in practice .

Figure 5: (_left_) Variance of observed gradient conflicts when sweeping over different training hyperparameters (high impact) as well as model capacity (low impact), and (_right_) illustrating the median proportion of pairwise gradient conflicts as a measure of task affinity

* Secondly, models are compared against one another after a few epochs of training (\(E_{ready}\) epochs) in contrast to e.g. random search where models are usually trained until convergence, or reaching a certain stopping criterion. Consequently, tuning \(E_{ready}\) can significantly impact the search's stability and outcome, which we further discuss in Appendix5.
* Finally, models in the population are compared using a single objective (e.g. average task/domain metric) during training. This may be an issue if the task metrics' have widely different ranges and the PBT scheduler may simply learn to favor short term improvement by giving higher weight to tasks with high metrics; While we do not observe this issue in our settings, recent work  also proposes a multi-objective variant of PBT which may be better fitted for MDL/MTL applications.

Nevertheless, the key advantage of PBT is its computational efficiency with regard to search space exploration: For a constant cost of training \(N\) models, and some minor overhead related to checkpointing, PBT explores up to \(N(1+Q E_{total}/E_{ready})\) possible hyperparameter configurations throughout training of the population.

In Table1 and Table2, we report results for searching optimal scalarization weights \(^{*}\) when training for all 6 domains of DomainNet and for 8 tasks (attribute subsets) of CelebA. For PBT results, we first run the search algorithm using the implementation from Raytune . We use 70% of the training set for training, and use the remaining 30% to rank models in the population by measuring their average accuracy on this set. Once the search is done, we retrain a model on the full training set using the scalarization weights found by PBT. For comparison, we also report results for uniform scalarization and MTO methods, using the implementation from . All final models are trained for three different learning rates and the best metric is reported, averaged across 2 random seeds.

On the DomainNet example, we observe that the scalarization weights found by PBT outperforms all methods, confirming our insights that tuning weights \(\) can further enhance scalarization. Note that MTO methods were not designed or employed for multi-domain settings such as DomainNet, which may explain why gradient-based MTO methods all underperform their loss-based counterparts in the results of Table1, while they do exhibit good performance on CelebA MTL. In fact, on CelebA, while PB2 search reaches the highest overall average metric, we find that results across tasks exhibit more variance, with PB2 and CAGrad yielding the best, comparable, performance. Overall, the very narrow differences in accuracy makes it difficult to highlight one specific method, which also raises the issue on whether CelebA is a robust enough MTL benchmark. Nevertheless, both experiments

   & & & & & \\  & clipart & infograph & painting & quickdraw & real & sketch \\    & & & & & & \\ Uniform & 46.78 \(\) 0.10 & 56.31 \(\) 0.04 & **20.46 \(\) 0.15** & 40.95 \(\) 0.45 & 60.69 \(\) 0.07 & 55.64 \(\) 0.01 & **46.64 \(\) 0.32** \\ PBT & **48.01 \(\) 0.08** & 58.31 \(\) 0.15 & 19.45 \(\) 0.08 & **41.32 \(\) 0.11** & **63.58 \(\) 0.31** & **60.43 \(\) 0.05** & 45.00 \(\) 0.26 \\    & & & & & \\ Uncertainty  & 45.12 \(\) 0.07 & **59.24 \(\) 0.09** & 17.14 \(\) 0.25 & 37.75 \(\) 0.16 & 59.85 \(\) 0.16 & 52.35 \(\) 0.20 & 44.42 \(\) 0.10 \\ MMTL-L  & 44.22 \(\) 0.10 & 58.05 \(\) 0.22 & 16.41 \(\) 0.16 & 37.53 \(\) 0.21 & 59.22 \(\) 0.29 & 51.27 \(\) 0.19 & 42.83 \(\) 0.38 \\    & & & & & \\ CAGrad  & 42.82 \(\) 0.06 & 54.08 \(\) 0.03 & 18.26 \(\) 0.04 & 36.79 \(\) 0.14 & 56.46 \(\) 0.28 & 49.52 \(\) 0.15 & 41.78 \(\) 0.01 \\ GradDrop  & 42.52 \(\) 0.05 & 53.34 \(\) 0.03 & 18.16 \(\) 0.07 & 37.25 \(\) 0.02 & 55.09 \(\) 0.13 & 49.94 \(\) 0.26 & 41.35 \(\) 0.09 \\ PCGrad  & 42.78 \(\) 0.14 & 53.55 \(\) 0.04 & 18.29 \(\) 0.34 & 37.31 \(\) 0.38 & 55.60 \(\) 0.08 & 50.41 \(\) 0.13 & 41.52 \(\) 0.67 \\    & & & & & \\  & clipart & infograph & painting & quickdraw & real & sketch \\    & & & & & & \\ Uniform & 48.83 \(\) 0.08 & 58.69 \(\) 0.05 & **21.58 \(\) 0.39** & 42.83 \(\) 0.03 & 62.51 \(\) 0.21 & 58.31 \(\) 0.06 & 49.05 \(\) 0.15 \\ PBT & **49.27 \(\) 0.12** & 58.41 \(\) 0.50 & 19.30 \(\) 0.19 & **44.53 \(\) 0.16** & **63.08 \(\) 0.42** & **60.08 \(\) 0.16** & **50.23 \(\) 0.01** \\   & & & & & \\ Uncertainty  & 46.96 \(\) 0.10 & **60.71 \(\) 0.36** & 18.74 \(\) 0.03 & 40.22 \(\) 0.40 & 60.92 \(\) 0.04 & 54.37 \(\) 0.12 & 46.79 \(\) 0.23 \\ MMTL-L  & 46.04 \(\) 0.21 & 59.76 \(\) 0.78 & 18.21 \(\) 0.15 & 39.12 \(\) 0.76 & 60.24 \(\) 0.39 & 53.06 \(\) 0.38 & 45.87 \(\) 0.29 \\   & & & & & \\ CAGrad  & 44.91 \(\) 0.18 & 56.56 \(\) 0.38 & 19.63 \(\) 0.32 & 38.84 \(\) 0.47 & 58.06 \(\) 0.41 & 51.80 \(\) 0.58 & 44.58 \(\) 0.45 \\ GradDrop  & 45.15 \(\) 0.08 & 56.22 \(\) 0.43 & 19.89 \(\) 0.16 & 39.70 \(\) 0.03 & 57.55 \(\) 0.12 & 52.81 \(\) 0.04 & 44.76 \(\) 0.18 \\ PCGrad  & 44.96 \(\) 0.14 & 55.79 \(\) 0.24 & 19.82 \(\) 0.20 & 39.65 \(\) 0.29 & 57.30 \(\) 0.30 & 52.57 \(\) 0.11 & 44.65 \(\) 0.65 \\   

Table 1: Results of MDL when jointly training all 6 domains of DomainNet for scalarization (uniform and PBT-found weights) and MTO methods. PBT is run with a population size of \(N=12\) models, such that every \(E_{ready}=5\) epochs, \(Q=25\%\) of the population triggers an exploit/explore step.

show that scalarization can outperform more complex optimization methods when its weights are tuned properly, which can be done efficiently using scalable hyperparameter search methods like PBT or PB2. We discuss further insights and compute details in Appendix 5.

## 6 Limitations

Since we are building off linear scalarization, our study suffers from the same issue: It has been shown, for instance in , that scalarization only finds solutions on the convex parts of the Pareto front. Same as previous studies [63; 52], we do not observe this to be an issue in practice, but there may be some cases where scalarization can simply not perform as well as more advanced MTO methods. In addition, our results and observations relies on an experimental study. While we do attempt to experiment over a diverse set of benchmarks and model capacities to get as many data points as possible, we can not guarantee the generality of our claims across all MDL/MTL settings.

## 7 Conclusions

This work presents a comprehensive evaluation of scalarization's effectiveness in multi-domain and multi-task learning, spanning diverse model capacities and dataset sizes. Our analysis reveals that larger-capacity models often benefit more from joint learning across diverse settings. In addition, tuning scalarization weights is key to reach optimal performance at inference and improve the MTL/MDL model generalization. Nevertheless,, given a specific set of specific tasks/domains, the optimal weights are rather robust to changes in model capacities within the same architecture family. We then investigate the impact of model capacity on gradient conflicts observed during training and observe low correlation with MTL/MDL performance. Finally, to tackle the large search space of tuning scalarization weights, we propose to leverage population-based training as a scalable, efficient method for tuning scalarization weights as the number of tasks/domains increases.

    & & & & & & & & & \\  & **average** & age & clothes & face structure & facial hair & gender & hair color & hair & mouth \\    & & & & & & & & & \\ Uniform & 91.23 & 86.70 & 92.79 & 85.16 & 95.45 & 98.10 & **92.99** & 91.68 & **86.95** \\ PBT & 91.21 & 86.79 & 92.77 & 85.18 & 95.42 & 98.02 & 92.92 & 91.66 & **86.95** \\ PB2 & **91.28** & 86.96 & 92.87 & 85.17 & 95.47 & 98.10 & 92.90 & **91.81** & 86.94 \\   & & & & & & & & & \\  IMTL-L  & 91.19 & 86.60 & 92.80 & 85.18 & 95.46 & 97.97 & 92.93 & 91.66 & 86.94 \\ CAGrad  & 91.27 & 86.92 & **92.89** & 85.11 & **95.49** & **98.17** & 92.93 & 91.74 & 86.89 \\ GradDrop  & 91.27 & **87.05** & 92.73 & **85.27** & 95.48 & 98.11 & 92.93 & 91.64 & 86.94 \\ PCGrad  & 91.18 & 86.71 & 92.74 & 85.09 & 95.37 & 98.12 & 92.90 & 91.66 & 86.84 \\    & & & & & & & & & \\  & age & clothes & face structure & facial hair & gender & hair color & hair & mouth \\    & & & & & & & & & \\ Uniform & 91.17 & 87.33 & 92.50 & 85.10 & 95.45 & **97.93** & 92.85 & 91.39 & 86.80 \\ PBT & 91.15 & 87.43 & 92.51 & **85.18** & 95.46 & 97.78 & 92.51 & 91.36 & **87.00** \\ PB2 & **91.25** & 86.83 & **92.85** & 85.12 & **95.49** & 98.19 & 92.90 & **91.78** & 86.81 \\   & & & & & & & & \\ IMTL-L  & 91.16 & 87.40 & 92.44 & 85.09 & 95.44 & 97.92 & 92.87 & 91.39 & 86.75 \\ CAGrad  & 91.22 & 87.37 & 92.66 & 85.13 & 95.40 & 97.92 & **92.92** & 91.61 & 86.74 \\ GradDrop  & 91.07 & 87.41 & 92.36 & 85.01 & 95.41 & 97.73 & 92.75 & 91.21 & 86.70 \\ PCGrad  & 91.14 & **87.53** & 92.42 & 85.00 & 95.38 & 97.87 & 92.84 & 91.37 & 86.68 \\   

Table 2: Results of MTL when training on all 8 tasks (subset of attributes) of CelebA defined in Appendix 1. For PBT and PB2 we use slightly different parameters than DomainNet to account for the fact that CelebA contains more tasks, and hence has a larger search space: All PBT runs use a population size of \(N=12\) models, such that every \(E_{ready}=3\) epochs, \(Q=40\%\) of the population triggers an exploit/explore step. For PB2 runs we use a population size of \(N=8\) and otherwise the same \(Q\) and \(E_{ready}\) hyperparameters. For the sake of space, we omit standard deviations in CelebA in the main text (in the range \(1e^{-4}\)), and only report results for the four best performing MTO baselines.