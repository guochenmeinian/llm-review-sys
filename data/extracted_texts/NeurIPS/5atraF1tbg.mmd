# Panoramia: Privacy Auditing of Machine Learning Models without Retraining

Mishaal Kazmi

University of British Columbia

&Hadrien Lautraite

University du Quebec a Montreal

&Alireza Akbari

Simon Fraser University

&Qiaoyue Tang

University of British Columbia

&Mauricio Soroco

University of British Columbia

&Tao Wang

Simon Fraser University

&Sebastien Gambs

University du Quebec a Montreal

&Mathias Lecuyer

University of British Columbia

equal contribution

###### Abstract

We present PANORAMIA, a privacy leakage measurement framework for machine learning models that relies on membership inference attacks using generated data as non-members. By relying on generated non-member data, PANORAMIA eliminates the common dependency of privacy measurement tools on in-distribution non-member data. As a result, PANORAMIA does not modify the model, training data, or training process, and only requires access to a subset of the training data. We evaluate PANORAMIA on ML models for image and tabular data classification, as well as on large-scale language models.

## 1 Introduction

Training Machine Learning (ML) models with Differential Privacy (DP) Dwork et al. (2006), such as with DP-SGD Abadi et al. (2016), upper-bounds the worst-case privacy loss incurred by the training data. In contrast, privacy auditing aims to empirically lower-bound the privacy loss of a target ML model or algorithm. In practice, privacy audits usually rely on the link between DP and the performance of membership inference attacks (MIA) Wasserman and Zhou (2010); Kairouz et al. (2015); Dong et al. (2019). At a high level, DP implies an upper-bound on the performance of MIAs, thus creating a high-performance MIA implies a lower-bound on the privacy loss. Auditing schemes have proven valuable in many settings, such as to audit DP implementations Nasr et al. (2023), or to study the tightness of DP algorithms Nasr et al. (2021); Lu et al. (2023); Steinke et al. (2023). Typical privacy audits rely on retraining the model several times, each time guessing the membership of one sample Jagielski et al. (2020); Carlini et al. (2022); Zanella-Beguelin et al. (2022), which is computationally prohibitive, requires access to the target model (entire) training data as well as control over the training pipeline.

To circumvent these concerns, Steinke et al. (2023) proposed an auditing recipe (called O(1)) requiring only one training run (which could be the same as the actual training) by randomly including/excluding several samples (called auditing examples) into the training dataset of the target model. Later, the membership of the auditing examples are guessed for privacy audit. However, O(1) faces a few challenges in certain setups. First, canaries, which are datapoints specially crafted to be easy to detect when added to the training set Nasr et al. (2023); Lu et al. (2023); Steinke et al. (2023), cannot be employed as auditing examples when measuring the privacy leakage for data that a contributor actually puts into the model, and not a _worst case data point_.

This matches a setting in which individual data contributors (_e.g._, a hospital in a cross-site Federated Learning (FL) setting or a user of a service that trains ML models on users' data) measure the leakage of their own (_i.e._, known) partial training data in the final trained model. Second, O(1) also relies on the withdrawal of real data from the model to construct non-member in-distribution data. This is problematic in situations in which ML model owners need to conduct post-hoc audits, in which case it is too late for removal Negoscu et al. (2023). Moreover, in-distribution audits require much more data, thus withholding many data points (typically more than the test set size) and reducing model utility. This brings us to the question: _Given an instance of a machine learning model as a target, can we perform post-hoc estimation of the privacy loss with regards to a known member subset of the target model training dataset?_

**Our contributions.** We propose PANORAMIA, a new scheme for _Privacy Auditing with NO Retraining by using Artificial data for Membership Inference Attacks_. More precisely, we consider an auditor with access to a subset of the training data and introduce a new alternative for accessing non-members: using synthetic datapoints from a generative model trained on the member data, unlocking the limit on non-member data. PANORAMIA uses this generated data, together with known members, to train and evaluate a MIA attack on the target model to audit (SS3). We also adapt the theory of privacy audits, and show how PANORAMIA can estimate the privacy loss (though not a lower-bound) of the target model with regards to the known member subset (SS4). An important benefit of PANORAMIA is to perform privacy loss measurements with (1) no retraining the target ML model (_i.e._, we audit the end-model, not the training algorithm), (2) no alteration of the model, dataset, or training procedure, and (3) only partial knowledge of the training set. We evaluate PANORAMIA on CIFAR10 models and observe that overfitted models, larger models, and models with larger DP parameters have higher measured privacy leakage. We also demonstrate the applicability of our approach on the GPT-2 based model (_i.e._, WikiText dataset) and CelebA models.

## 2 Background and Related Work

DP is the established privacy definition in the context of ML models, as well as for data analysis in general. We focus on the pure DP definition to quantify privacy loss with well-understood semantics. In a nutshell, DP is a property of a randomized mechanism (or computation) from datasets to an output space \(\), noted \(M:\). It is defined over neighboring datasets \(D,D^{}\), differing by one element \(x\) (we use the add/remove neighboring definition), which is \(D^{}=D\{x\}\). Formally:

**Definition 1** (Differential Privacy Dwork et al. (2006)).: _A mechanism \(M:\) is \(\)-DP if for any two neighbouring datasets \(D\), \(D^{}\), and for any measurable output subset \(O\) it holds that:_

\[P[M(D) O] e^{}P[M(D^{}) O].\]

Since the neighbouring definition is symmetric, so is the DP definition, and we also have that \(P[M(D^{}) O] e^{}P[M(D) O]\). Intuitively, \(\) upper-bounds the worst-case contribution of any individual example to the distribution over outputs of the computation (_i.e._, the ML model learned). More formally, \(\) is an upper-bound on the _privacy loss_ incurred by observing an output \(o\), defined as \([M(D)=o]}{[M(D^{})=o]} \), which quantifies how much an adversary can learn to distinguish \(D\) and \(D^{}\) based on observing output \(o\) from \(M\). A smaller \(\) hence means higher privacy.

**DP, MIA and privacy audits.** To audit a DP training algorithm \(M\) that outputs a model \(f\), one can perform a MIA on datapoint \(x\), trying to distinguish between a neighboring training sets \(D\) and \(D^{}=D\{x\}\). The MIA can be formalized as a hypothesis test to distinguish between \(_{0}=D\) and \(_{1}=D^{}\) using the output of the computation \(f\). Wasserman and Zhou (2010); Kairouz et al. (2015); Dong et al. (2019) show that any such test at significance level \(\) (False Positive Rate or FPR) has power (True Positive Rate or TPR) bounded by \(e^{}\). In practice, one repeats the process of training model \(f\) with and without \(x\) in the training set, and uses a MIA to guess whether \(x\) was included. If the MIA has TPR \(>e^{}\)FPR, the training procedure that outputs \(f\) is not \(\)-DP. This is the building block of most privacy audits Jagielski et al. (2020); Nasr et al. (2021); Zanella-Beguelin et al. (2022); Lu et al. (2023); Nasr et al. (2023). Appendix F provides further context, and discusses other related work that is not directly relevant to our contribution.

**Averaging over data instead of models with O(1).** The above result bounds the success rate of MIAs when performed over several _retrained models_, on two alternative datasets \(D\) and \(D^{}\). Steinke et al. (2023) show that it is possible to average _over data_ when several data points independently differ between \(D\) and \(D^{}\). Let \(x_{1,m}\) be the \(m\) data points independently included in the training set, and \(s_{1,m}\{0,1\}^{m}\) be the vector encoding inclusion. \(T_{0,m}^{m}\) represents any vector of guesses, with positive values for inclusion in the training set (member), negative values for non-member, and zero for abstaining. Then, if the training procedure is \(\)-DP, Proposition 5.1 in Steinke et al. (2023) bounds the performance of guesses from \(T\) with:

\[_{i=1}^{m}\{0,T_{i} S_{i}\} v T=t (}{1+ ^{}})^{m}}{_{i=1}^{m}|t_{i}| S^{ }_{i} v}.\]

In other words, an audit (MIA) \(T\) that can guess membership better than a Bernoulli random variable with probability \(}{1+^{}}\) refutes an \(\)-DP claim. In this work we build on this result, extending the algorithm (SS3) and theoretical analysis (SS4) to enable the use of generated data for non-members, which breaks the independence between data points.

## 3 Panoramia

Figure 1 summarizes the end-to-end PANORAMIA privacy measurement. The measurement starts with a target model \(f\), and a subset of its training data \(D_{f}\) from distribution \(\). For instance, \(\) and \(D_{f}\) could be the distribution and dataset of one participant in an FL training procedure that outputs a final model \(f\). The privacy measurement then proceeds in two phases.

**Phase 1:** In the first phase, PANORAMIA uses a subset of the known training data \(D_{G} D_{f}\) to train a generative model \(\). The goal of the generative model \(\) is to match the training data distribution \(\) as closely as possible, which is formalized in Definition 3 (SS4). Using the generative model \(\), we can synthesize non-member data, which corresponds to data that was not used in the training of target model \(f\). Hence, we now have access to an independent dataset of member data \(D_{}=D_{f} D_{G}\), and a synthesized dataset of non-member data \(D_{}\), of size \(m=|D_{}|\).

**Phase 2:** In the second phase, we leverage \(D_{}\) and \(D_{}\) to audit the privacy leakage of \(f\) using a MIA. To this end, we split \(D_{}\), \(D_{}\) into training and testing sets, respectively called \(D_{}^{}\), \(D_{}^{}\) and \(D_{}^{}\), \(D_{}^{}\). We use the training set to train a MIA (called PANORAMIA in Figure 1), a binary classifier that predicts whether a given datapoint is a member of \(D_{f}\), the training set of the target model \(f\). This MIA classifier makes its prediction based on both a training example \(x\), as well as information from applying the target model \(f\) to the input, such as the loss of the target model when applied to this example \((f(x))\) (see SS5, Appendix C for details). We use the test set to measure the MIA performance, using the precision at different recall values.

Previous results linking the performance of a MIA on several data-points to \(\)-DP bounds rely on independence between members and non-members. This intuitively means that there is no information about membership in \(x\) itself. When the auditor controls the training process this independence is enforced by construction, by adding data points to the training set based on an independent coin flip. In PANORAMIA, we do not have independence between membership and \(x\), as all non-members

Figure 1: PANORAMIA’s two-phase privacy audit. Phase 1 trains generative model \(\) on member data. Phase 2 trains a MIA on a subset of member data and generated non-member data, using the loss of \(f\) on these data points. The performance of the MIA is compared to a baseline classifier that does not have access to \(f\). Notations are summarized in Table 4 in Appendix A.

come from the generator \(\). As a result, there are two ways to guess membership and have high MIA precision: either by using \(f\) to detect membership (_i.e._, symptomatic of privacy leakage) or by detecting generated data (_i.e._, not a symptom of privacy leakage). To measure the privacy leakage, we compare the results of the MIA to that of a baseline classifier \(b\) that guesses membership based exclusively on \(x\), without access to \(f\). The stronger this baseline, the better the removal of the effect of synthesized data detection. Algorithm 1 summarizes the entire procedure. In the next section, we demonstrate how to relate the difference between the baseline \(b\) and the MIA performance to the privacy loss \(\).

``` Input: Target ML model \(f\), audit set size \(m\), confidence \(1-\) Phase 1:
1:Split \(D_{f}\) in \(D_{G},D_{}^{}\), \(D_{}^{}\), with \(|D_{}^{}|=m\)
2:Train generator \(\) on \(D_{G}\)
3:Generate \(D_{}^{}\), \(D_{}^{}\) of size \(|D_{}^{}|,|D_{}^{}|\)
4:Phase 2:
5:Train the baseline and MIA:
6:Label \(D_{}^{}\) as members, and \(D_{}^{}\) as non-members
7:Train \(b\) to predict labels using \(x D_{}^{} D_{}^{}\)
8:Train MIA to predict labels using \(x D_{}^{} D_{}^{}\) and \(f(x)\) Measure privacy leakage (see SS4):
9:Sample \(s()^{m}\)\(\) Def.2
10:Create audit set \(X=s D_{}^{}+(1-s)D_{}^{}\)
11:Score each audit point for membership, creating \(t^{b} b(X)_{+}^{m}\) and \(t^{a}(X)_{+}^{m}\)
12:Set \(v_{}^{b}(c,t)\{v:^{b}(m,c,v,t)\}\)\(\) Prop.1
13:\(c_{}=_{t,c}\{t^{b} t\} s v_{}^{b} (c,\{t^{b} t\})\)\(\) Prop.2
14:Set \(v_{}^{a}(c,,t)\{v:^{a}(m,c,,v, t)\}\)\(\) Prop.2
15:\(\{c+\}_{}=_{t,c,}\{t^{a} t\} s  v_{}^{a}(c,,\{t^{a} t\})\)
16:Return\(\{c+\}_{}-c_{}\) ```

**Algorithm 1**Panovamia

## 4 Quantifying Privacy Leakage with Panoramia

We formalize a privacy game as follows: construct a sequence of auditing samples, in which \(s_{i}\) is sampled independently to choose either the real (\(x^{}^{m}\) training points from \(\) if \(s_{i}=1\)) or generated (\(x^{}^{m}\) generated points from \(\) if \(s_{1}=0\)) data point at each index \(i\).

**Definition 2** (Privacy game).:

\[s()^{m},s_{i}\{0,1\},x_{i}=(1-s_{i})x _{i}^{}+s_{i}x_{i}^{in},\; i\{1,,m\}.\]

The key difference between Definition 2 and the privacy game of Steinke et al. (2023) lies in how we create the audit set. Instead of sampling non-members independently and removing them from the training set (which requires modifyig the training set and the model), Definition 2 starts from a set of known members (after the fact), and pairs each point with a non-member (generated i.i.d. from the generator distribution). For each pair, we then flip a coin to decide which point in the pair will be shown to the auditor for testing, thereby creating the test task of our privacy measurement.

The level of achievable success in this game quantifies the privacy leakage of the target model \(f\). More precisely, we follow an analysis inspired by that of Steinke et al. (2023), but require several key technical changes to support auditing with no retraining using generated non-member data.

### Formalizing the Privacy Measurements as a Hypothesis Test

We first need a notion of quality for our generator:

**Definition 3** (\(c\)-closeness).: _For all \(c>0\), we say that a generative model \(\) is \(c\)-close for data distribution \(\) if:_

\[ x,\;e^{-c}_{}x _{}x.\]The smaller \(c\), the better the generator, as \(\) assigns a probability to real data that cannot be much smaller than that of the real data distribution. We make three important remarks.

**Remark 1:**\(c\)-closeness is a strong requirement but it is achievable, at least for a large \(c\). For instance, a uniform random generator is \(c\)-close to any distribution \(\), with \(c<\). Of course \(c\) would be very large, and the challenge in PANORAMIA is to create a generator that empirically has a small enough \(c\).

**Remark 2:** We show in Appendix E that we can relax this definition to hold only with high probability (following the \((,)\)-DP relaxation). Empirical results under this relaxation are very similar. We also show how to measure \((,)\)-DP, instead of \(\)-DP on which we focus here.

**Remark 3:** Our definition of closeness is very similar to that of DP. This is not a coincidence as we will use this definition to be able to reject claims of both \(c\)-closeness for the generator and \(\)-DP for the algorithm that gave the target model. We further note that, contrary to the DP definition, \(c\)-closeness is one-sided as we only bound \(_{}\) from below. Intuitively, this means that the generator has to produce high-quality samples (_i.e._, samples likely under \(\)) with high enough probability but it does not require that all samples are good though. This one-sided measure of closeness is enabled by our focus on detecting members (_i.e._, true positives) as opposed to members and non-members, and puts less stringent requirements on the generator.

Using Definition 3, we can formulate the hypothesis test on the privacy game of Definition 2 that underpins our approach:

\[:cf.\]

Note that, in a small abuse of language, we will often refer to \(f\) as \(\)-DP to say that \(f\) is the output of an \(\)-DP training mechanism. That is, \(B(s,x)=\{b(x_{1}),b(x_{2}),,b(x_{m})\}\).

We define two key mechanisms: \(B(s,x):\{0,1\}^{m}^{m}_{+}^{m}\) outputs a (potentially randomized) non-negative score for the membership of each datapoint \(x_{i}\), based on \(x_{i}\) only; \(A(s,x,f):\{0,1\}^{m}^{m}_{+}^{m}\) outputs a (potentially randomized) non-negative score for the membership of each datapoint, with the guess for index \(i\) depending on \(x_{ i}\) and target model \(f\). We can construct a statistical test for each part of the hypothesis separately.

**Proposition 1**.: _Let \(\) be \(c\)-close, \(S\) and \(X\) be the random variables for \(s\) and \(x\) from Definition 2, and \(T^{b} B(S,X)\) be the vector of guesses from the baseline. Then, for all \(v\) and all \(t\) in the support of \(T\):_

\[_{S,X,T^{b}}_{i=1}^{m}T_{i}^{b} S_{i} v T^{ b}=t^{b}(}{1+c^{c}})^{m}}{ }_{i=1}^{m}t_{i}^{b} S_{i}^{} v ^{b}(m,c,v,t^{b})\]

Proof.: In Appendix B.1. 

Now that we have a test to reject a claim that the generator \(\) is \(c\)-close for the data distribution \(\), we turn our attention to the second part of \(\) which claims that the target model \(f\) is the result of an \(\)-DP mechanism.

**Proposition 2**.: _Let \(\) be \(c\)-close, \(S\) and \(X\) be the random variables for \(s\) and \(x\) from Definition 2, \(f\) be \(\)-DP, and \(T^{a} A(S,X,f)\) be the guesses from the membership audit. Then, for all \(v\) and all \(t\) in the support of \(T\):_

\[_{S,X,T^{a}}_{i=1}^{m}T_{i}^{a} S_{i} v T^ {a}=t^{a}(+t }{1+c^{c}+t})^{m}}{}_{i=1}^{m}t_{i}^{a} S_{i}^{ } v^{a}(m,c,,v,t^{a})\]

Proof.: In Appendix B.2. 

We can now provide a test for hypothesis \(\), by applying a union bound over Propositions 1 and 2:

**Corollary 1**.: _Let \(\) be true, \(T^{b} B(S,X)\), and \(T^{a} A(S,X,f)\). Then:_

\[_{i=1}^{m}T_{i}^{a} S_{i} v^{a},\;_{i=1}^{m }T_{i}^{b} S_{i} v^{b} T^{a}=t^{a},T^{b}=t^{b}^{ a}(m,c,,v^{a},t^{a})+^{b}(m,c,v^{b},t^{b})\]To make things more concrete, let us instantiate Corollary1 as we do in \(\). Our baseline (\(B\) above) and MIA (\(A\) above) classifiers return a membership guessess \(T^{a,b}\{0,1\}^{m}\), with \(1\) corresponding to membership. Let us call \(r^{a,b}_{i}t_{i}^{a,b}\) the total number of predictions, and \(^{a,b}_{i}t_{i}^{a,b} s_{i}\) the number of correct membership guesses (true positives). We also call the precision \(^{a,b}^{a,b}}{r^{a,b}}\). Using the following tail bound on the sum of Bernoulli random variables for simplicity and clarity (we use a tighter bound in practice, but this one is easier to read),

\[_{S^{}(p)^{r}}_{i=1}^{r}^{}}{r} p+},\] (1)

we can reject \(\) at confidence level \(\) by setting \(^{a}=^{b}=\) and if either \(^{b}}{1+e^{c}}+}}\) or \(^{a}}{1+e^{c+}}+}}\).

### Quantifying Privacy Leakage and Interpretation

In an privacy measurement, we would like to quantify \(\), not just reject a given \(\) claim. We use the hypothesis test from Corollary1 to compute a confidence interval on \(c\) and \(\). To do this, we first define an ordering between \((c,)\) pairs, such that if \((c_{1},_{1})(c_{2},_{2})\), the event (_i.e._, set of observations for \(T^{a,b}\), \(S\)) for which we can reject \((c_{2},_{2})\) is included in the event for which we can reject \((c_{1},_{1})\). That is, if we reject \(\) for values \((c_{2},_{2})\) based on audit observations, we also reject values \((c_{1},_{1})\) based on the same observations.

We define the following order to fit this assumption, based on the hypothesis test from Corollary1:

\[(c_{1},_{1})(c_{2},_{2})c_{1}<c_{2}, \\ c_{1}=c_{2}_{1}_{2}\] (2)

This lexicographic order over hypotheses ensures that, when using the joint hypothesis test from Corollary1 to construct confidence intervals, we always reject hypotheses with a low value of \(c\) incompatible with the baseline performance. Formally, this yields the following confidence intervals:

**Corollary 2**.: _For all \(]0,1]\), \(m\), and observed \(t^{a},t^{b}\), call \(v^{a}_{}(c,)\,\{v:^{a}(m,c,,v,t^ {a})\}\) and \(v^{b}_{}(c)\,\{v:^{b}(m,c,v,t^{b})\}\). Then:_

\[(c,)(c^{},^{}):t ^{a} s v^{a}_{}(c^{},^{})t^{b} s v^{b}_{}(c^{})} 1-\]

Proof.: Apply Lemma4.7 from Steinke et al. (2023) with the ordering from Equation2 and the test from Corollary1. 

The lower confidence interval for \((c,)\) at confidence \(1-\) is the largest \((c,)\) pair that cannot be rejected using Corollary1 with false rejection probability at most \(\). Hence, for a given confidence level \(1-\), \(\) computes \((c_{},)\), the largest value for \((c,)\) that it cannot reject. Note that Corollaries1 and 2 rely on a union bound between two tests, one for \(c\) and one for \(c+\). We can thus consider each test separately (at level \(/2\)). In practice, we use the whole precision/recall curve to achieve tighter bounds: each level of recall (_i.e._, threshold on \(t^{a,b}\) to predict membership) corresponds to a bound on the precision, which we can compare to the empirical value. Separately for each test, we pick the level of recall yielding the highest lower-bound (Lines 4-7 in the last section of Algorithm1). For all bounds to hold together, we apply a union bound over all tests at each level of recall. Specially, if we have \(m\) test datapoints, we have at most \(m\) different values of recall, and the union bound corresponds to \(/m\) (notice in Equation1 that the bounds depend on \(\), so this is not too costly). We next discuss the semantics of returned measurement values \((c_{},)\).

**Measurement semantics.** Corollary2 gives us a lower-bound for \((c,)\), based on the ordering from Equation2. To understand the value \(\) returned by \(\), we need to understand what the hypothesis test rejects. Rejecting \(\) means either rejecting the claim about \(c\), or the claim about \(c+\) (which is the reason for the ordering in Equation2). With Corollary2, we hence get both a lower-bound \(c_{}\) on \(c\), and \(\{c+\}_{}\) on \(c+\). Unfortunately, \(\{0,\{c+\}_{}-c_{}\}\), which is the value \(\) returns, does not imply a lower-bound on \(\). Instead, we can claim that "PANORAMIA could not reject a claim of \(c\)-closeness for \(\), and if this claim is tight, then \(f\) cannot be the output of a mechanism that is \(\)-DP".

While this is not as strong a claim as typical lower-bounds on \(\)-DP from prior privacy auditing works, we believe that this measure is useful in practice. Indeed, the \(\) measured by PANORAMIA is a quantitative privacy measurement, that is accurate (close to a lower-bound on \(\)-DP) when the baseline performs well (and hence \(c_{}\) is tight). Thus, when the baseline is good, \(\) can be interpreted as (close to) a lower bound on (pure) DP. PANORAMIA opens a new capability, measuring privacy leakage of a trained model \(f\) without access or control of the training pipeline, with a meaningful and practically useful measurement, as we empirically show next.

## 5 Evaluation

We instantiate PANORAMIA on target models for four tasks from three data modalities1. For **image classification**, we consider the CIFAR10Krizhevsky (2009), and CelebA Liu et al. (2015) datasets, with varied target models: a four-layers CNN O'Shea and Nash (2015), a ResNet101He et al. (2015), a Wide ResNet-28-10 Zagoruyko and Komodakis (2017), a Vision Transformer (ViT)-small Dosovitskiy et al. (2021), and a DP ResNet18He et al. (2015) trained with DP-SGD (Abadi et al., 2016) using Opacus (Yousefpour et al., 2021) at different values of \(\). We use StyleGAN2Karras et al. (2020) for \(\). For **language models**, we fine-tune small GPT-2 Radford et al. (2019) on the WikiText train dataset Merity et al. (2016) (we take a subset of the documents in WikiText-103). \(\) is again based on small GPT-2, and then fine-tuned on \(D_{G}\). We generate samples using top-\(k\) sampling Holtzman et al. (2019) and a held-out prompt dataset \(D_{G}^{} D_{G}\). Finally, we conduct experiments on **classification on tabular data**. However for this data modality, PANORAMIA did not detect any meaningful leakage. More precisely, we have observed significant variance in the audit values returned by PANORAMIA which make the results inconclusive. Nonetheless, we present the results obtained for this task and discuss this issue further in Appendix D.6. Table 1 summarizes the tasks, models, and performance. More details on the settings, the models used, and implementation details are provided in Appendix C.

### Baseline Design and Evaluation

Our MIA is a loss-based attack, which uses an ML model taking as input both a datapoint \(x\) and the value of the loss of target model \(f\) on point \(x\). Appendix C details the architectures used for the attack model for each data modality. Recall from SS4.2 the importance of having a tight \(c_{}\) for our measure \(\) to be close to a lower-bound on \(\)-DP. To increase the performance of our baseline \(b\), we mimic the role of the target model \(f\)'s loss in the MIA closing a helper model \(h\) trained on synthetic data, which adds a loss-based feature to \(b\).

This new feature can be viewed as side information about the data distribution. Table 2 shows the \(c_{}\) value for different designs for \(h\). The best performance is consistently when \(h\) is trained on synthetic data before being used as a feature to train the \(b\). Indeed, such a design reaches a \(c_{}\) up to \(1.19\) larger that without any helper (CIFAR10) and \(0.23\) higher than when training on real non-member data, _without requiring access to real non-member data_, a key requirement in PANORAMIA. We adopt this design in all the following experiments. In Figure 2 we show that the baseline has enough training data (vertical dashed line) to reach its best performance. Appendix D.1, shows similar results on GPT-2 as well as different architectures that we tried for the helper model. All these pieces of evidence confirm the strength of our baseline.

### Main Privacy Measurement Results

We run PANORAMIA on models with different degrees of over-fitting by varying the number of epochs, (see the final accuracy on Table 1) for each data modality. More over-fitted models are known to leak more information about their training data due to memorization (Yeom et al., 2018; Carlini et al., 2019, 2022a). To show the privacy loss measurement power of PANORAMIA, we compare it with two strong approaches to lower-bounding privacy loss.

First, we compare PANORAMIA with the O(1) audit (Steinke et al., 2023) (in input space, without canaries in a black box setting), which uses a loss threshold MIA to detect both members and non-members. Second, we use a hybrid between O(1) and PANORAMIA, in which we use real non-member data instead of generated data with our ML-based MIA (called RM;RN for Real Members; Real Non-members). In both cases \(c_{}=0\), but it requires control over training data inclusion and a large set of non-member data. The privacy loss measured by these techniques gives a target that we hope PANORAMIA to detect.

Figure 3 shows the precision of \(b\) and PANORAMIA at different levels of recall, and Figure 4 the corresponding value of \(\{c+\}_{}\) (or \(c_{}\) for \(b\)). Dashed lines show the maximum value of \(\{c+\}_{}\) (or \(c_{}\)) achieved (Fig. 4) (returned by PANORAMIA), and the precision implying these values at different recalls (Fig. 3). Table 3 summarizes those \(\{c+\}_{}\) (or \(c_{}\)) values, as well as the \(\) measured by existing approaches. We make two key observations.

Figure 4: \(\{c+\}_{}\) (or \(c_{}\)) vs recall, for our target models, reported over \(5\) independent experiment runs.

Figure 3: Precision vs. recall comparison between PANORAMIA and the baseline \(b\) for our target models, based on one experiment run. The maximum \(c_{}\) or \(\{c+\}_{}\) values set an upper bound on the empirical precision values across different recall levels, indicated by the dashed line.

First, the best prior method (whether RM;RN or O(1)) measures a larger privacy loss (\(\)). Overall, these results empirically confirm the strength of \(b\), as we do not seem to spuriously assign differences between \(\) and \(\) to our privacy loss proxy \(\). We also note that O(1) tends to perform better, due to its ability to rely on non-member detection, which improves the power of the statistical test at equal data sizes. Such tests are not available in PANORAMIA given our one-sided closeness definition for \(\) (see SS4), and we keep that same one-sided design for RM;RN for the sake of comparison.

Second, the values of \(\) measured by PANORAMIA are close to those of the methods against which we compared. In particular, despite a more restrictive adversary model (_i.e._, no non-member data, no control over the training process, and no shadow model training), PANORAMIA is able to detect meaningful amounts of privacy loss, comparable to that of state-of-the-art methods.

For instance, on a non-overfitted CIFAR-10 ResNet101 model (E20), PANORAMIA detects a privacy loss of \(0.33\), while using real non-member (RM;RN) data yields \(0.42\), and controlling the training process \(O(1)\) gets \(0.45\). The relative gap gets even closer on models that reveal more about their training data. Indeed, for the most over-fitted model (E100), \(=1.16\) is very close to RM;RN (\(=1.22\)) and O(1) (\(=1.41\)). This also confirms that the leakage detected by PANORAMIA on increasingly overfitted models does augment, which is confirmed by prior state-of-the-art methods. For instance, NLP models' \(\) goes from \(0.22\) to \(2.40\) (\(2.82\) to \(5.73\) for O(1)). Appendix D.6 details results on tabular data, in which PANORAMIA is not able to detect significant privacy loss.

#### Privacy Measurement of \(\)-DP Models:

Another avenue to varying privacy leakage is by training Differentially-Private (DP) models with different values of \(\). We evaluate PANORAMIA on DP ResNet-18 models on CIFAR10, with \(\) values shown on Table 3 and Figure 5. The hyper-parameters were tuned independently for the highest train accuracy (see Appendix D.9 for more results and discussion). As the results show, neither PANORAMIA nor O(1) detect privacy loss on the most private models (\(=1,2\)). At higher values of \(=10,15\) (_i.e._, less private models) and \(=\) (_i.e._, non-private model) PANORAMIA does detect an increasing level of privacy leakage with \(_{=10}<_{=15}<_{=}\). In this regime, the O(1) approach detects a larger, though comparable, amount of privacy loss.

We also evaluate PANORAMIA on DP (fine-tuned) large language models (see Appendix D.9 and Table 9 for details). We fine-tune GPT2-Small target models with DP-SGD on the WikiText dataset, with various values of \(\). Neither PANORAMIA nor O(1) are able to measure a positive privacy loss.

Figure 5: ResNet18, CIFAR-10, \(\)-DP for various \(\) values.

 
**Target model /** & Adult & \(\)-D & \(\)-D & \(\)-D \\   &   } & 2.41\(\)0.10 & 2.5\(\)0.20 & 3.0\(\)0.21 & \(\)-D \\  & & \(\)-D & 0.42\(\)0.19 & - & 0.40\(\)0.10 \\  & & \(\)-D & RM & - & - & 0.40\(\)0.10 \\   &   } & 2.41\(\)0.10 & 1.7\(\)0.11 & 1.0\(\)0.11 & 0.0\(\)0.11 \\  & & \(\)-D & RM & - & - & 0.71\(\)0.21 \\  & & \(\)-D & RM & - & - & - & 0.79\(\)0.21 \\   &   } & 2.41\(\)0.10 & 1.4\(\)0.10 & 1.4\(\)0.13 & - & 1.27\(\)0.19 \\  & & \(\)-D & RM & - & - & - & 1.11\(\)0.180 \\   &   } & 2.44\(\)0.10 & 2.5\(\)0.21 & 2.5\(\)0.22 & 2.5\(\)0.22 \\  & & \(\)-D & RM & - & - & - & 2.87\(\)0.20 \\  & & \(\)-D & RM & - & - & - & 2.86\(\)0.19 \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 3.3\(\)0.22 & 2.5\(\)0.22 \\  & & \(\)-D & RM & - & - & - & 2.86\(\)0.19 \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & 2.3\(\)0.22 \\  & & \(\)-D & RM & - & - & - & 2.87\(\)0.20 \\  & & \(\)-D & RM & - & - & - & 2.86\(\)0.19 \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & 2.3\(\)0.22 \\  & & \(\)-D & RM & - & - & - & 2.87\(\)0.20 \\  & & \(\)-D & RM & - & - & - & 2.86\(\)0.19 \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & 2.3\(\)0.22 \\  & & \(\)-D & RM & - & - & - & 2.77\(\)0.21 \\  & & \(\)-D & RM & - & - & - & 2.86\(\)0.19 \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & - & - & - & 2.86\(\)0.19 \\  & & \(\)-D & RM & - & - & - & - & - \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & - & - & - & 2.86\(\)0.19 \\  & & \(\)-D & RM & - & - & - & - & - \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.22 & - & - & - & - & - \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.21 & 2.3\(\)0.22 \\  & & \(\)-D & RM & - & - & - & - & - \\   &   } & 2.41\(\)0.10 & 3.5\(\)0.21 & 2.3\(\)0.21 & 2.3\(\)0.21 \\  & & \(\)-D & RM & - & - & - & - & - & - \\   & \)). These findings provide additional empirical evidence for the robustness of \(b\), indicating that we are not erroneously attributing differences between \(\) and \(\) to our privacy loss proxy \(\). However, in the case of text data (on the GPT-2 trained for 150 epochs), Figure 6(d), we hit an upper-bound on the power of the hypothesis test which estimates \(\{c+\}_{}\) using \(20k\) test samples. The maximum measurement for \(\) we can achieve is around \(2.59\) with a \(20k\) test set, as opposed to O(1), which reaches \(5.73\) with \(10k\) test set size.

**Increasing Test Set Size for DP models:** With an increase in the test set size \(m\), we demonstrate that we can achieve tighter bounds on the privacy measurement \(\). Figure 6(c) shows that as \(m\) increases, PANORAMIA (solid lines) can measure higher privacy leakage for various \(\)-DP ResNet18 models. The reason for this is that PANORAMIA can leverage a much higher test set size (up to the whole training set size for training and testing the MIA), in comparison to O(1) in our setting (due to being limited by the number of real non-member data points available).

## 6 Impact, Limitations and Future Directions

We have introduced a new approach to quantify the privacy leakage from ML models, in settings in which individual data contributors (such as a hospital in a cross-site FL setting or a user of a text auto-complete service) measure the leakage of their own, known partial training data in the final trained model. Our approach does not introduce new attack capabilities (it would likely benefit from future progress on MIA performance though), but can help model providers and data owners assess the privacy leakage incurred by their data, due to inclusion in ML models training datasets. Consequently, we believe that any impact from our proposed work is likely to be positive.

However, PANORAMIA suffers from a major limitation: its privacy measurement is not a lower-bound on the privacy loss (see details in Section 4.2). Providing a proper lower-bound \(_{lb}\) on privacy leakage in this setting is an important avenue for future work. A promising direction is to devise a way to measure or enforce an upper-bound on \(c\), thereby yielding \(_{lb}\) via \(\{c+\}_{lb}-c_{ub}\). Despite this shortcoming, we believe that the new measurement setting we introduce in this work is important. Indeed, recent work has shown that MIA evaluation on foundation models suffers from a similar lack of non-member data (since all in distribution data is included in the model) Das et al. (2024); Duan et al. (2024); Meeus et al. (2024). The theory we develop in this paper provides a meaningful step towards addressing privacy measurements in this setting, and provides a more rigorous approach to privacy benchmarks for such models. We also demonstrate that PANORAMIA's privacy measurements can also be empirically valuable for instance for providing improved measurements with more data (Figure 6).

Figure 6: Effect of increasing test set size on PANORAMIA’s privacy measurement for our target models. In the case of the image dataset, increasing the number of auditing examples allows us to achieve tighter empirical measurements for privacy leakage despite a restricted adversary. For the case of the language modeling task, when increasing test set size \(m\), we hit an upper bound on the power of the hypothesis test due to test dataset size, and do not see significant improvement in privacy measurement in this case.