# Generalizable Implicit Motion Modeling

for Video Frame Interpolation

 Zujin Guo, Wei Li, Chen Change Loy

S-Lab, Nanyang Technological University

{zujin.guo, wei.l, ccloy}@ntu.edu.sg

https://gseancdat.github.io/projects/GIMMVFI

###### Abstract

Motion modeling is critical in flow-based Video Frame Interpolation (VFI). Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps without exploring favorable motion priors, thus lacking the capability of effectively modeling spatiotemporal dynamics in real-world videos. To address this limitation, in this study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel and effective approach to motion modeling for VFI. Specifically, to enable GIMM as an effective motion modeling paradigm, we design a motion encoding pipeline to model spatiotemporal motion latent from bidirectional flows extracted from pre-trained flow estimators, effectively representing input-specific motion priors. Then, we implicitly predict arbitrary-timestep optical flows within two adjacent input frames via an adaptive coordinate-based neural network, with spatiotemporal coordinates and motion latent as inputs. Our GIMM can be easily integrated with existing flow-based VFI works by supplying accurately modeled motion. We show that GIMM performs better than the current state of the art on standard VFI benchmarks.

## 1 Introduction

Video Frame Interpolation (VFI) is a fundamental task in computer vision, which involves generating intermediate frames between two adjacent video frames. This technique is crucial for various practical applications, including novel view synthesis , video generation , and video compression . This task is highly challenging due to the complex motions typically found in real-world videos. To address this, recent research  has focused on flow-based frameworks, which have shown substantial success. Generally, these frameworks for VFI involve two main phases: 1) transforming the input frames based on estimated optical flows, and 2) merging and enhancing the warped frames to produce intermediate frames. Consequently, the accuracy of flow estimation is crucial for the fidelity of the synthesized frames.

Accurately modeling flow from two distant frames is challenging due to the complexities of capturing subtle and dynamic movements caused by varying motion speeds, object occlusions, and lighting conditions. A commonly used approach in the literature combines bidirectional flows derived from input frames  (Figure 1(a)). This method assumes _overlapped_ and linear motion for flow estimation, which does not accurately reflect real-world dynamics. Several studies  directly predict bilateral flows based on intermediate discrete timestamps (Figure 1(b)). These approaches model the correlation between frame motions and timestamps without leveraging motion priors within input frames; they thus fall short of capturing complex spatial-temporal changes and handling occluded regions and unexpected deformations. Moreover, this discrete-time-based modeling paradigm is ineffective for arbitrary-time interpolation.

In this study, we explore a more effective and generalizable implicit approach to motion modeling for VFI. Inspired by the success of implicit neural representations  in encoding complex high-dimensional data such as 2D images , 3D scenes , and videos , we propose to implicitly model optical flows between two adjacent video frames using coordinate-based neural networks. This implicit paradigm can directly take arbitrary spatiotemporal coordinates as inputs and effectively decode the desired space-time outputs, making it a promising framework for learning highly dynamic optical flows in real-world videos. However, leveraging implicit neural networks for effective motion modeling poses challenges. First, standard implicit neural networks typically perform per-instance modeling, optimizing model parameters for a single specific input. This limitation restricts their applicability across different input video frames. Therefore, we need to develop a more adaptive implicit model capable of capturing motions in any given video. Second, efficiently integrating spatiotemporal information within implicit neural networks is complex, especially when dealing with the intricate motions occurring between two video frames. This necessitates designing appropriate implicit neural architectures that can accurately predict and represent both the spatial and temporal dynamics in videos.

To this end, we propose a novel generalizable implicit flow encoding for motion modeling in VFI, called Generalizable Implicit Motion Modeling (GIMM) (Figure 1(c)). Our method only assumes the availability of bidirectional flows (\(F_{0 1},F_{1 0}\) ) of two input frames obtained from a pre-trained optical flow estimator (e.g., RAFT , FlowFormer ). The input flows can be noisy as this prior will be refined to estimate the bilateral flows (\(F_{t 0},F_{t 1}\)) between arbitrary intermediate timestamps. Our method is unique in that it can represent input-specific motion priors effectively. For instance, it can accurately capture the complex dynamics of a somersult. This is achieved by introducing a motion encoding pipeline to extract spatiotemporal motion latent from the bidirectional flows. Our method can further enable frame interpolation at _arbitrary timestamps_, thanks to the adaptive coordinate-based neural network that takes spatiotemporal coordinates and motion latent as inputs. This capability allows our method to generate frames at various temporal granularities, providing flexibility and precision in video frame interpolation.

Our contributions are summarized as follows: We present an effective motion modeling paradigm for video frame interpolation characterized by a novel generalizable implicit motion modeling framework. Our GIMM is capable of accurately predicting optical flow for arbitrary timesteps between two adjacent video frames at any resolution, allowing seamless integration with existing flow-based VFI methods. We demonstrate the advantages of our GIMM in motion modeling for arbitrary-timestep VFI tasks, achieving state-of-the-art performances on various benchmarks.

Figure 1: **Schematic of motion modeling paradigms** in video frame interpolation. (**a**) A naive _linear combination_ of bidirectional flows \(F_{0 1},F_{1 0}\) (i.e., flows between input frames) may lead to ambiguous and coarse motion estimation due to strong overlapped and linear assumptions. (**b**) A _time-condition-based modeling_ approach may predict suboptimal bilateral flows \(F_{t 0},F_{t 1}\) (i.e., flows between estimated and input frames), capturing spatiotemporal changes for moving objects ineffectively. (**c**) Our _generalizable implicit motion modeling_ properly represents spatiotemporal dynamics across videos and predict better bilateral flows via an adaptive coordinate-based neural network.

Related Work

**Video frame interpolation.** Conventional VFI studies primarily rely on either direct frame synthesis via convolutional networks [11; 1; 24] or interpolation using dynamic kernels with learnable weights and offsets [37; 38; 41; 9; 8; 12; 28]. Recent approaches have shifted towards flow-based methods to synthesize frames at desired timesteps, where motion modeling plays a crucial role [20; 29; 54; 26; 49; 17]. Some flow-based methods combine estimated bidirectional flows between input frames [22; 40; 45; 53; 39; 1], often leading to inaccurate motion predictions, especially in occluded areas. This simplistic combination results in ambiguous and coarse motion estimation, causing object shifts in interpolated frames. Several recent approaches [20; 29; 55; 20; 32] address these issues by directly predicting the desired motion within an end-to-end framework conditioned on timesteps, showing impressive results in synthesized frames. However, these methods mainly rely on discrete-time-based fitting of variable relationships between motion and timesteps, making it challenging to achieve consistent, continuous interpolation outcomes.

**Implicit neural representations.** Implicit Neural Representations (INRs) have been shown effective in modeling complex, high-dimensional data for various applications, including video compression , novel view synthesis [34; 30], and image super-resolution . Typically, INRs learn a continuous mapping from a set of coordinates to a specific signal using a coordinate-based neural network to encode data implicitly. The flexible and expressive modeling capabilities of INRs motivate us to explore their use to encode intricate motions and capture subtle, dynamic movements of objects in real-world videos. A recent work related to ours is IFE . While both approaches consider implicit flow encoding, our method significantly differs from IFE. Unlike IFE, which focuses on per-scene encoding--where each coordinate-based network is parameterized by a specific video instance--we aim to develop a generalizable motion modeling approach that can be applied across different videos.

**Generalizable INRs.** Recent works [5; 44; 10; 21; 15; 7; 4] further extend INRs for generalizable encoding by conditioning coordinate-based neural networks with additional instance-specific inputs. For example, some approaches [6; 27; 25] employ Transformers  as meta-learners to predict instance-specific weights or modulation features for coordinate-based neural networks at high computational costs. Several notable studies [4; 7; 45] leverage generalizable INRs for video encoding to facilitate video interpolation and super-resolution, mainly focusing on directly learning implicit space-time continuous neural representations from video. In contrast, we aim to explore effective motion modeling paradigms to improve intermediate frame synthesis for flow-based VFI. To our knowledge, we make the first attempt to utilize generalizable INRs for motion modeling in the context of VFI.

## 3 Method

Given a pair of adjacent video frames \(I_{0},I_{1}^{H W 3}\) with timesteps \(\{0,1\}\), a general flow-based video frame interpolation process is defined as follows:

\[F_{t} =(I_{0},I_{1},t),\] (1) \[I_{t} =(F_{t},I_{0},I_{1}), t,\] (2)

where \(\) denotes a motion modeling (or flow estimation) module, and \(\) indicates a frame interpolation process under the guidance of estimated motion \(F_{t}\) at a timestep of \(t\) for input frames \(I_{0}\) and \(I_{1}\). In this work, we mainly focus on studying an effective motion modeling framework \(\) for flow-based VFI.

### Generalizable Implicit Motion Modeling

The goal of GIMM is to estimate bilateral flows \(F_{t 0},F_{t 1}\) for any timestep \(t\), deriving from initial bidirectional flows \(F_{0 1},F_{1 0}\):

\[F_{t 0},F_{t 1}=(F_{0 1},F_{1 0},t), t,\] (3)

where \(F_{0 1},F_{1 0}\) are predicted from a pre-trained optical flow estimator (e.g., RAFT , FlowFormer ) with input frames \(I_{0}\) and \(I_{1}\). Figure 2 depicts the overall generalizable motion modeling framework of GIMM. Motivated by the great success of INRs in modeling complex video data [7; 4; 13; 34], GIMM uses an adaptive coordinate-based neural network for continuous motion modeling. Unlike the existing IFE  that performs per-scene optimization and requires test-time learning for different videos, our GIMM takes additional instance-specific motion latent inputs \(L_{t}\) to enhance model generalizability across various input videos.

**Flow normalization.** Following IFE , we perform normalization for the initial bidirectional flows as follows:

\[V_{0} =(F_{0 1}),V_{1}=(-F_{1 0}),\] (4) \[V_{t} =(F_{t 1}-F_{t 0}),\] (5)

where \(\) is a scale operator. This reversible normalization process aligns the scale and temporal direction of input bidirectional flows \(F_{0 1},F_{1 0}\) with output bilateral flows \(F_{t 0},F_{t 1}\), allowing the normalized flows \(V_{i}^{H W 2},i 0,1\) to be effectively encoded in subsequent implicit motion modeling in GIMM.

**Motion latent.** To achieve generalizable implicit modeling of motion dynamics, we learn implicit neural representations for motions with conditions on instance-specific motion latent inputs \(L_{t}(x,y)\), which provides instance-specific motion priors at spatiotemporal coordinates \(=(x,y,t)\) for target motion \(V_{t}(x,y)\). Specifically, we introduce a Motion Encoder to extract motion features \(K_{i}\) from normalized flows \(V_{i}\). To maintain spatiotemporal consistencies of modeled motion, we derive time-dependent motion features \(K_{t}\) for given timestep \(t\) from both motion features \(K_{0}\) and \(K_{1}\). To realize this, we apply forward warping \(\) process to map every location of motion features \(K_{i}\) to the target timestep \(t\):

\[K_{i t}=(K_{i},F_{i t},Z_{i}).\] (6)

Here, \(K_{i t}\) are warped features with timesteps \(i\{0,1\}\) that integrate the correspondences from source (input) timesteps to the target timestep, \(Z_{i}\) represents splatting weights, and \(F_{i t}\) denotes scaled bidirectional flows at timestep \(t\). We compute scaled bidirectional flows \(F_{i t}\) as follows:

\[F_{i t}=(t-0) F_{0 1},&i=0\\ (1-t) F_{1 0},&i=1\] (7)

To mitigate warping errors in multi-to-one cases or occluded regions, we calculate splatting weights \(Z_{i}\) using flow consistency \(U^{i}_{flow}\) and variance \(U^{i}_{var}\) metrics  as follows:

\[Z_{i}= U^{i}_{flow}}+  U^{i}_{var}},\] (8)

where \(_{flow}\) and \(_{var}\) are learnable parameters. To obtain the final motion latent \(L_{t}\), we concatenate the warped features \((K_{0 t},K_{1 t})\) as the coarse motion latent at timestep \(t\) and further refine via a Latent Refiner module to deal with potential information loss and ambiguous motion in the forward warping process. Both the Latent Refiner and the Motion Encoder mentioned above are structured as shallow convolutional networks. We provide their network configurations and detailed calculations for two flow metrics \(U^{i}_{flow}\) and \(U^{i}_{var}\) in the supplementary.

Figure 2: Our GIMM first transforms initial bidirectional flows \(F_{0 1},F_{1 0}\) as normalized flows \(V_{0},V_{1}\). The motion encoder extracts motion features \(K_{0},K_{1}\) from \(V_{0},V_{1}\), respectively. \(K_{0},K_{1}\) are then forward warped at a given timestep \(t\) using bidirectional flows to obtain the warped features \(K_{t 0},K_{t 1}\). We pass the warped and initial motion features into a Latent Refiner that outputs motion latent \(L_{t}\), representing motion information at \(t\). Conditioned on \(L_{t}(x,y)\), the coordinate-based network \(g_{}\) predicts the corresponding normalized flow \(V_{t}\) with 3D coordinates \(=(x,y,t)\). For interpolation usage, \(V_{t}\) is then transferred into bilateral flows \(F_{t 0},F_{t 1}\) through denormalization.

**Implicit motion prediction.** To realize INRs for generalizable motion modeling, we devise an adaptive coordinate-based network \(g_{}\) for implicit and continuous motion encoding, which maps spatiotemporal coordinates \([-1,1]^{H W 3}\) along with the corresponding \(D\)-dimension motion latent code \(L_{t}^{H W D}\) to predicted normalized flows \(_{t}\):

\[_{t}=g_{}(,L_{t}).\] (9)

The predicted normalized flow \(V_{t}\) can be converted into predicted bilateral flows \(F_{t 0},F_{t 1}\) via the reverse flow normalization process for VFI. See the supplementary for the details of the coordinate-based network.

**Optimization.** In practice, we optimize the GIMM module by minimizing a Mean-Square-Error (MSE) loss between predicted (\(_{t}\)) and ground-truth (\(V_{t}\)) normalized optical flows:

\[_{gimm}(V_{t},_{t})=_{k=0}^{H-1}_ {l=0}^{W-1}||V_{t}(k,l)-_{t}(k,l)||_{2}.\] (10)

### Integrating GIMM with Frame Interpolation

**GIMM-VFI.** Given two adjacent video frames \(I_{i}\) with \(i\{0,1\}\), we start with a pre-trained optical flow estimator (e.g., RAFT , FlowFormer ) to extract context features \(A_{i}\), correlation features \(C_{i}\), and initial bidirectional flows \(F_{0 1}\) and \(F_{1 0}\). We first predict bilateral flows \(F_{t i}\) for a given timestep \(t\) from initial bidirectional flows using a pre-trained GIMM module. The predicted bilateral flows \(F_{t i}\), context features \(A_{i}\), and correlation features \(C_{i}\) are then passed into a frame synthesis module \(\) (adapted from the AMT ), which refines the input flows and generates warping masks \(M_{t}^{H W}\). Following previous flow-based VFI studies , we obtain warped images \(I_{t i}\) via backward warping \(\) under the guidance of predicted bilateral flows \(F_{t i}\) from input frames \(I_{0}\) and \(I_{1}\):

\[I_{t i}=(I_{i},F_{t i}),\] (11) \[I_{t i}(x,y)=I_{i}(x+F_{t i}^{h}(x,y),y+F_{t i}^{v}(x,y)),\] (12)

where \(F_{t i}^{h},F_{t i}^{v}\) represents horizontal and vertical motion of \(F_{t i}\), respectively. To generate the final interpolated image \(_{t}\), the warped images \(I_{t 0},I_{t 1}\) are fused with the warping mask \(M_{t}\) as follows:

\[_{t}=M_{t} I_{t 0}+(1-M_{t}) I_{t 1}.\] (13)

See the supplementary for the details of the frame synthesis module.

**Overall objectives.** We use the following objective function for VFI:

\[_{interp}(I_{t},_{t})=_{lap}(I_{t},_{t}) +_{char}(I_{t},_{t})+_{census}(I_{t},_{t}),\] (14)

where \(_{lap}\), \(_{char}\), and \(_{census}\) denote the Laplacian loss , Charbonnier loss , and census loss , respectively. In addition, to preserve the motion modeling in the pre-trained

Figure 3: An overview of GIMM-VFI architecture. GIMM-VFI employs a pre-trained flow estimator, \(\), to predict bidirectional flows \((F_{0 1},F_{1 0})\) and extracts context features \(A\) as well as correlation features \(C\) from the input frames \((I_{0},I_{1})\). Given the timestep \(t\), a generalizable implicit motion modeling (GIMM) module \(\) (detailed in Figure 2) takes the bidirectional flows as inputs and predicts bilateral flows \((F_{t 0},F_{t 1})\), which are then passed into a frame synthesis module \(\), together with extracted features \((A,C)\), to synthesize the target frame \(I_{t}\).

GIMM module, we reconstruct and supervise the flows \(},}\) with pseudo ground truth \(V_{0},V_{1}\). This objective \(_{rec}\) is defined as \(||V_{0}-}||_{2}+||V_{1}-}||_{2}\). The overall objective \(\) is as follows:

\[=_{interp}(I_{t},_{t})+_{rec}_{ rec},\] (15)

where \(_{rec}\) is the hyperparameter. We optimize the entire GIMM-VFI model that contains the pre-trained flow estimator, pre-trained GIMM module, and frame synthesis module.

## 4 Experiments

We present quantitative and qualitative evaluations of our motion modeling method GIMM in Section 4.1, and the corresponding interpolation method (GIMM-VFI) in Section 4.4. Specifically, we evaluate both motion quality and performance on the downstream interpolation task. We compare GIMM-VFI with current state-of-the-art VFI methods on arbitrary-timestep interpolation.

**Implementation details.** We train the GIMM model on the training split of Vimeo90K  triplets dataset using optical flows extracted by off-the-shelf flow estimators. Our GIMM-VFI is trained on the complete Vimeo90K septuplet dataset. Specifically, we implement two variants of GIMM-VFI, using two different flow estimators: the RAFT  and FlowFormer , designated as GIMM-VFI-R and GIMM-VFI-F, respectively. More details are provided in the supplementary materials.

### Motion Modeling

**VTF and VSF benchmarks.** To assess the modeled motion quality with GIMM, we use Flowformer  to produce pseudo ground truth motion. Specifically, we establish two motion evaluation benchmarks: Vimeo-Triplet-Flow (VTF) and Vimeo-Septuplet-Flow (VSF). These benchmarks are derived from the triplet and septuplet splits of the Vimeo90K  test set, tailored for evaluating 2X and 6X motion modeling, respectively.

**Baselines.** We compare GIMM with other motion modeling approaches, including _Linear_, _Forward Warp_, _End-to-End_, and BMBC . Specifically, _Linear_ replaces our GIMM module with the linear approximation strategy . Similarly, _Forward Warp_ substitutes GIMM with a forward warping strategy . Meanwhile, the _End-to-End_ denotes the strategy that directly predicts motion at arbitrary timesteps through an end-to-end fitting. In this end-to-end setting, we select the current state-of-the-art EMA-VFI  as the representative method. Additionally, we make further comparisons with BMBC , which is supposed to model complex motion effectively through several stages of motion predictions and motion refinements. For fair comparisons, we employ RAFT  as the flow estimator for different motion modeling methods.

**Evaluation settings.** We measure the modeled motion quality on both VTF and VSF benchmarks by calculating PSNR on normalized flows and End-Point-Error (EPE) on the unscaled flows. For the downstream interpolation task, we calculate PSNR on the 'hard' split of the SNU-FILM-arb benchmark (SNU-FILM-arb-Hard). Details about SNU-FILM-arb are presented in Section 4.4.

    &  &  &  \\   & PSNR\(\) & EPE\(\) & PSNR\(\) & EPE\(\) & PSNR\(\) \\  Linear & 35.03 & 0.44 & 30.09 & 2.87 & 32.42 \\ Forward Warp & 32.80 & 0.47 & 28.22 & 3.38 & 32.31 \\ End-to-End & 29.23 & 1.02 & 25.99 & 5.12 & 32.28 \\ BMBC  & 28.89 & 0.95 & 23.19 & 8.23 & 28.51 \\ GIMM (-VFI-R) & **37.56** & **0.34** & **30.45** & **2.68** & **32.62** \\   

Table 1: **Comparisons of different motion modeling methods.** We assess the modeled motion on Vimeo-Triplet-Flow (VTF) and Vimeo-Septuplet-Flow (VSF) by employing PSNR and EPE metrics. Additionally, we demonstrate their impact on the interpolation task by presenting the PSNR values of their interpolation results on the ‘hard’ split of the SNU-FILM-arb dataset. We denote the calculated PSNR values as PSNR\({}_{i}\) for optical flows in the motion assessment and PSNR\({}_{i}\) for interpolated images in the interpolation task.

**Results.** We provide quantitative comparisons of our GIMM and baselines in Table 1. We first report the results of motion quality on VTF and VSF. We find that GIMM can continuously model motions in videos. GIMM predicts the best flows on both the VTF benchmark (37.56dB PSNR/ 0.34 EPE) and the VSF benchmark (30.45dB PSNR/ 2.68 EPE) that involves unseen timesteps during training. Moreover, our study highlights the importance of motion priors in both motion modeling and the downstream interpolation task. The _Linear_, _Forward Warp_, and our GIMM methods that leverage motion priors from RAFT  can provide better motion and interpolation performances than the _End-to-End_ and BMBC  across all benchmarks. Notably, GIMM benefits the interpolation task the most and achieves the highest PSNR of 32.62 dB on SNU-FILM-arb-Hard. While motion modeling plays a critical role in interpolation, other modules also impact interpolation outcomes. The _End-to-End_ method has a narrower performance gap with other advanced motion modeling approaches on the interpolation benchmark than on motion modeling benchmarks, likely due to its more complex and extensive feature extraction and frame synthesis modules.

**Visualizations.** Besides the quantitative evaluation of motion modeling, we also qualitatively evaluate them on SNU-FILM-arb-Hard, as shown in Figure 4. Since SNU-FILM-arb-Hard requires 8X interpolation for input frames of large motion, classical methods like BMBC  may struggle to model the intermediate motion. In contrast, we observe the capacity of GIMM for accurate motion modeling at arbitrary timesteps. Take the Somersault in Figure 4 for example, the ankle flow and the corresponding interpolation generated by GIMM have the best consistency with the ground truth, pointing to the same direction. Moreover, GIMM is capable of reducing ambiguities when interpolating moving objects. We can observe clearer silhouettes with fewer blurs in both the 'dog-leg' and 'dog-head' cases in Figure 4.

### Ablation Study on Motion Model Choices

As our core contribution, we present an effective motion modeling paradigm for video frame interpolation, characterized by a novel generalizable implicit motion modeling framework (GIMM). In this section, we further investigate the necessity of implicit neural representations (INRs) and approaches

Figure 4: Qualitative comparisons of different motion modeling methods on SNU-FILM-arb-Hard. All the results are predicted at \(t=0.75\), and ground truth flows are obtained by FlowFormer .

to generalizable INRs. The experiments are conducted on the motion modeling benchmarks, _i.e._, Vimeo-Triplet-Flow (VTF) and Vimeo-Septuplet-Flow (VSF).

**Necessity of INRs.** A straightforward replacement of INRs in our GIMM is a timestep-conditioned U-Net as in diffusion literature . We substitute the INRs with a U-Net of a comparable number of channels to ensure fair comparisons. In Table 2, replacing INR with U-Net results in worse performance, especially on the 6X motion modeling benchmark VSF. This demonstrates the strong continuous modeling ability of INR. Besides, GIMM with INR has a much lighter architecture, improving efficiency. Therefore, it is necessary and proper to use INR for our motion modeling.

**Approaches to generalizable INRs.** To realize generalizable implicit modeling, an alternative way is to modulate the weights of INRs with meta-learners [6; 25]. Following , we employ Transformers  as the meta-learner. Compared to GIMM, this meta-learning approach performs significantly worse across all benchmarks while introducing a model with over 170X more parameters.

### Ablation Study on GIMM

In this section, we investigate the effectiveness of model designs in GIMM in Table 3. The experiments are conducted on the VTF and VSF benchmarks.

**Forward warping.** We substitute forward warping operation in GIMM with a straightforward linear combination of the input motion features (_Non-Fwarp_). In Table 3, we observe a significant performance drop with this modification. For example, the EPE on VSF increases by 0.20. This result highlights the importance of forward warping for GIMM's continuous motion modeling.

**Implicit modeling.** In GIMM, the motion can be modeled even without the presence of implicit modeling. We conduct experiments without using any coordinates (_Non-Imp_). In Table 3, this variant yields performance gains, with reductions of 0.05 and 0.13 in the EPEs on VTF and VSF respectively, highlighting the importance of implicit modeling in GIMM.

**Motion encoder.** We leverage the motion encoder in our GIMM to extract features from flows, alleviating the negative impacts brought by the possible bias and noises in the estimated flows. To justify this design, we remove the motion encoder for a direct comparison (_Non-ME_). As a result, we observe increases in error on both benchmarks, _e.g._, the EPE on VSF rises 0.17. Therefore, the motion encoder benefits the motion modeling in GIMM.

**Latent refinement.** To validate the efficacy of latent refinement in GIMM, we conducted experiments excluding the latent refiner (_Non-Refiner_). This leads to notable declines of 0.53 dB and 0.43 dB in PSNRs across both benchmarks. Thus, refining the motion latent proves essential for accurate motion modeling.

**Spatial coordinates.** Since the modeled motion consists of spatiotemporal changes, we use 3D spatiotemporal coordinates in the implicit modeling of GIMM. To assess the necessity of spatial coordinates, we replace 3D coordinates with temporal coordinates (_T-coord only_). This removal causes a 0.16 dB drop of PSNR on Vimeo-Triplet-Flow and a 0.06 increase of EPE on Vimeo-Septuplet-Flow. This emphasizes the crucial role of spatial coordinates in implicit modeling.

### Arbitrary-timestep Video Frame Interpolation

**SNU-FILM-arb benchmark.** We introduce the SNU-FILM-arb benchmark for a more generalized evaluation, to facilitate research on the arbitrary-timestep frame interpolation task. Specifically, we incorporate the SNU-FILM dataset  that encompasses 310 clips from 31 videos captured at 240 fps, with heights ranging from 384 to 1280 pixels and widths from 368 to 720 pixels. Different from

    &  &  &  \\    & PSNR\(\) & EPE\(\) & PSNR\(\) & EPE\(\) & \\  Meta-learning (INR) & 30.19 & 0.88 & 24.50 & 6.80 & 43.92 \\ GIMM (U-Net) & 36.96 & 0.39 & 29.96 & 2.90 & 4.27 \\ GIMM (INR) & **37.56** & **0.34** & **30.45** & **2.68** & **0.25** \\   

Table 2: **Quantitative comparisons of different motion model choices.**its original setting of 2X interpolation, we conduct 4X, 8X, and 16X interpolation evaluations on the original 'Medium', 'Hard', and 'Extreme' subsets of SNU-FILM, respectively.

**Evaluation settings.** We calculate PSNR and perceptual indicators, e.g., LPIPS , FID , on both SNU-FILM-arb and X4K-1000FPS (XTest) . XTest comprises 15 clips from 15 videos for 8X interpolation. Following the established protocol from prior studies [18; 55], we assess interpolation quality at both 2K and 4K resolutions. For fair comparisons, we disable test-time augmentations during evaluations.

**Current advanced VFI methods.** We compare GIMM-VFI with various advanced interpolation methods. For non-INR methods, we include RIFE , IFRNet , M2M , AMT , UPRNet , and EMA-VFI . We also make comparisons with the INR-based method CURE .

**Results.** We present the quantitative results of GIMM-VFI on arbitrary-timestep interpolation benchmarks in Table 4. Our proposed method achieves high-quality continuous interpolation across various timesteps (i.e., 4X, 8X, 16X). In terms of PSNR, we observe that our RAFT-based  method GIMM-VFI-R achieves significant improvements of **0.18** dB on XTest-2K, **0.67** dB on XTest-4K, and approximately **0.30** dB on each subset of the SNU-FILM-arb, in comparison with the previous state-of-the-art method EMA-VFI . For perceptual metrics, _i.e.,_ LPIPS and FID, our proposed GIMM-VFI still outperforms previous methods on most benchmarks, delivering competitive performance. These observations demonstrate that our GIMM can offer effective motion modeling for the arbitrary-timestep VFI task.

**Visualizations.** Besides the quantitative evaluations, we further qualitatively compare both GIMM-VFI-R and GIMM-VFI-F with existing VFI techniques, as illustrated in Figure 5. Our methods achieve better interpolation across various timesteps. For example, our methods maintain the integrity of moving object silhouettes (3\(rd\) case). Moreover, GIMM-VFI-R and GIMM-VFI-F preserve detailed textures within both occluded regions and moving objects from significant deformations, _e.g.,_ the rectangle pole tag(\(1st\) case), truck (\(2nd\) case), swing (\(3rd\) case).

    &  &  \\   & 2K & 4K & Medium (4X) & Hard (8X) & Extreme (16X) \\  RIFE  & 31.431 / 0.126 / 11.99 & 30.587 / 0.152 / 13.52 & 36.33 / 0.038 / 6.65 & 31.87 / 0.072 / 11.99 & 27.21 / 0.134 / 19.82 \\ M2M  & 32.137 / 0.098 / 9.25 & 30.887 / 0.158 / 8.67 & 36.56 / 0.036 / 5.98 & 31.92 / 0.061 / 1.13 & 27.14 / 0.112 / 17.37 \\ IFNNet  & 31.551 / 0.108 / 23.93 & 30.467 / 0.164 / 2.375 & 34.88 / 0.046 / 9.92 & 31.15 / 0.066 / 1.165 & 26.32 / 0.115 / 16.91 \\ AMT  & 28.88 / 0.153 / 13.92 & 28.17 / 0.187 / 13.97 & 34.49 / 0.079 / 29.25 & 31.03 / 0.089 / 10.34 & 26.44 / 0.136 / 14.72 \\ UPRNet  & 31.16 / 0.140 / 10.75 & 30.50 / 0.154 / 9.45 & 36.78 / 0.033 / 6.09 & 31.96 / 0.064 / 9.93 & 27.14 / 0.111 / 16.76 \\ EMA-VFI  & 32.53 / **0.097** / 7.21 & 31.21 / 0.156 / 8.61 & 36.65 / 0.041 / 7.07 & 32.28 / 0.074 / 12.17 & 27.72 / 0.130 / 19.58 \\  CURE  & 30.24 / 0.111 / 26.42 & & 36.09 / 0.037 / 5.68 & 31.32 / 0.063 / 12.72 & 26.61 / 0.114 / 22.62 \\ GIMM-VFI-R & 32.71 / 0.113 / **6.52** & 31.88 / 0.149 / **6.49** & 37.02 / 0.033 / 5.89 & **32.62** / 0.060 / **9.59** & 27.99 / 0.110 / **16.45** \\ GIMM-VFI-F & **32.91** / 0.103 / 6.74 & **31.97** / **0.142** / 6.58 & **37.03** / **0.031** / **5.86** & 32.56 / **0.059** / 9.95 & **28.01 / 0.109** / 16.79 \\   

Table 4: **Quantitative results for arbitrary-timestep interpolation. We report the quantitative metrics reported as PSNR\(\)/LPIPS\(\)/FID\(\), with the best results highlighted in boldface** and the second best results in underline. The analysis categorizes methods into two groups: non-INR methods (first half) and INR-based methods (second half). We employ a ‘-’ symbol to denote ‘out-of-memory’ scenarios, and a dagger (‘\(\)’) to indicate that the results are drawn from prior studies [20; 18; 55].**

    &  &  \\   & PSNR\(\) & EPE\(\) & PSNR\(\) & EPE\(\) \\  Non-Fwarp & 37.07 & 0.37 & 30.09 & 2.88 \\ Non-Imp & 37.04 & 0.39 & 30.11 & 2.81 \\ Non-ME & 37.05 & 0.42 & 30.26 & 2.85 \\ Non-Refiner & 37.03 & 0.37 & 30.02 & 2.77 \\ T-coord only & 37.40 & 0.36 & 30.39 & 2.74 \\ Full & **37.56** & **0.34** & **30.45** & **2.68** \\   

Table 3: **Quantitative comparisons of different model variants. For each model variant, we evaluate its motion modeling performance on Vimeo-Triplet-Flow and Vimeo-Septuplet-Flow, respectively. We adopt EPE and PSNR as the metrics for motion quality.**

## 5 Limitation

There are several known limitations to our method. First of all, the GIMM-VFI is closely related to the pre-trained flow estimator, which estimates bidirectional flows and extracts image features. Therefore, GIMM-VFI inherits the limitations of the chosen pre-trained flow estimators, which may include high computational costs. Additionally, GIMM-VFI only accepts two consecutive frames as inputs, which is not favorable for frame interpolations where larger and nonlinear motion exists.

## 6 Conclusion

We propose a novel motion modeling method, GIMM, which performs continuous motion modeling for video frame interpolation. Our proposed GIMM is the first attempt to learn generalizable implicit neural representations for continuous motion modeling. The method can properly predict arbitrary-timestep optical flows within two adjacent video frames at any resolution, and be easily integrated with existing flow-based VFI approaches (_e.g.,_ AMT ) without further modifications by supplying accurately modeled motion. Extensive experiments on the VFI benchmarks show that our GIMM can effectively perform generalizable motion modeling across videos.

**Acknowledgement.** This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s).

Figure 5: Qualitative comparisons of arbitrary-timestep interpolation on XTest-2K . Positions pointed by the yellow arrow indicate the distinct performance of our method.