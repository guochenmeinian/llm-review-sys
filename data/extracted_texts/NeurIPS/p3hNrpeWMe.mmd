# A Walsh Hadamard Derived Linear Vector Symbolic Architecture

Mohammad Mahmudul Alam\({}^{1}\), Alexander Oberle\({}^{1}\), Edward Raff\({}^{1,2}\), Stella Biderman\({}^{2}\),

Tim Oates\({}^{1}\), James Holt\({}^{3}\)

\({}^{1}\)University of Maryland, Baltimore County,\({}^{2}\)Booz Allen Hamilton,

\({}^{3}\) Laboratory for Physical Sciences

m256@umbc.edu, aoberle1@umbc.edu, Raff_Edward@bah.com,

biderman_stella@bah.com, oates@cs.umbc.edu, holt@lps.umd.edu

###### Abstract

Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in \(^{d}\) are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding.

## 1 Introduction

Vector Symbolic Architectures (VSAs) are a unique approach to performing symbolic style AI. Such methods use a binding operation \(:^{d}^{d}^{d}\), where \((x,y)=z\) denotes that two concepts/vectors \(x\) and \(y\) are connected to each other. In VSA, any arbitrary concept is assigned to vectors in \(^{d}\) (usually randomly). For example, the sentence "the fat cat and happy dog" would be represented as \((,)+(,)=\). One can then ask, "what was happy" by _unbinding_ the vector for happy, which will return a noisy version of the vector bound to happy. The unbinding operation is denoted \(^{}(x,y)\), and so applying \(^{}(S,)\).

Because VSAs are applied over vectors, they offer an attractive platform for neuro-symbolic methods by having natural symbolic AI-style manipulations via differentiable operations. However, current VSA methods have largely been derived for classical AI tasks or cognitive science-inspired work. Many such VSAs have shown issues in numerical stability, computational complexity, or otherwise lower-than-desired performance in the context of a differentiable system.

As noted in , most VSAs can be viewed as a linear operation where \((a,b)=a^{}Gb\) and \(^{}(a,b)=a^{}Gb\), where \(G\) and \(F\) are \(d d\) matrices. Hypothetically, these matrices could be learned via gradient descent, but would not necessarily maintain the neuro-symbolic properties of VSAs without additional constraints. Still, the framework is useful as all popular VSAs we are aware fit within this framework. By choosing \(G\) and \(F\) with specified structure, we can change the computational complexity from \((d^{2})\), down to \((d)\) for a diagonal matrix.

In this work, we derive a new VSA that has multiple desirable properties for both classical VSA tasks, and in deep-learning applications. Our method will have only \((d)\) complexity for the binding step, is numerically stable, and equals or improves upon previous VSAs on multiple recent deeplearning applications. Our new VSA is derived from the Walsh Hadamard transform, and so we term our method the Hadamard-derived linear Binding (HLB) as it will avoid the \((d d)\) normally associated with the Hadamard transform, and has better performance than more expensive VSA alternatives.

Related work to our own will be reviewed in section 2, including our baseline VSAs and their definitions. Our new HLB will be derived in section 3, showing it theoretically desirable properties. section 4 will empirically evaluate HLB in classical VSA benchmark tasks, and in two recent deep learning tasks, showing improved performance in each scenario. We then conclude in section 5.

## 2 Related Work

Smolensky  started the VSA approach with the Tensor Product Representation (TPR), where \(d\) dimensional vectors (each representing some concept) were bound by computing an outer product. Showing distributivity (\((,+)=(,)+(,)\)) and associativity, this allowed specifying logical statements/structures . However, for \(\) total items to be bound together, it was impractical due to the \((d^{})\) complexity. [36; 25; 24] have surveyed many of the VSAs available today, but our work will focus on three specific alternatives, as outlined in Table 1. The Vector-Derived Transformation Binding (VTB) will be a primary comparison because it is one of the most recently developed VSAs, which has shown improvements in what we will call "classic" tasks, where the VSA's symbolic like properties are used to manually construct a series of binding/unbinding operations that accomplish a desired task. Note, that the VTB is unique in it is non-symmetric (\((,)(,)\)). Ours, and most others, are symmetric.

Next is the Holographic Reduced Representation (HRR) , which can be defined via the Fourier transform \(()\). One derives the inverse operation of the HRR by defining the one vector \(}\) as the identity vector and then solving \((^{*})_{i}()_{i}=1\). We will use a similar approach to deriving HLB but replacing the Fourier Transform with the Hadamard transform, making the HRR a key baseline. Last, the Multiply Add Permute (MAP)  is derived by taking only the diagonal of the tensor product from 's TPR. This results in a surprisingly simple representation of using element-wise multiplication for both binding/unbinding, making it a key baseline. The MAP binding is also notable for its continuous (MAP-C) and binary (MAP-B) forms, which will help elucidate the importance of the difference in our unbinding step compared to the initialization avoiding values near zero. HLB differs in devising for the unbinding step, and we will later show an additional corrective term that HLB employs for \(\) different items bound together, that dramatically improve performance.

Our motivation for using the Hadamard Transform comes from its parallels to the Fourier Transform (FT) used to derive the HRR and the HRR's relatively high performance. The Hadamard matrix has a simple recursive structure, making analysis tractable, and its transpose is its own inverse, which simplifies the design of the inverse function \(^{*}\). Like the FT, WHT can be computed in log-linear time, though in our case, the derivation results in linear complexity as an added benefit. The WHT is already associative and distributive, making less work to obtain the desired properties. Finally, the WHT involves only \(\{-1,1\}\) values, avoiding numerical instability that can occur with the HRR/FT.

   Method &  \((x,y)\) \\  &  Unbind \(^{*}(x,y)\) \\  & 
 Init \(x\) \\  \\  HRR & \(^{-1}(()())\) & \(^{-1}(()())\) & \(x_{i}(0,1/d)\) \\ VTB & \(V_{y}x\) & \(V_{y}^{}x\) & \(_{i}(0,1) x=}/\|}\|_{2}\) \\ MAP-C & \(x y\) & \(x y\) & \(x_{i}(-1,1)\) \\ MAP-B & \(x y\) & \(x y\) & \(x_{i}\{-1,1\}\) \\ HLB & \(x y\) & \(x y\) & \(x_{u}\{(-,1/d),\ (,1/d)\}\) \\   

Table 1: The binding and initialization mechanisms for our new HLB with baseline methods. HLB is related to the HRR in being derived via a similar approach, but replacing the Fourier transform \(()\) with the Hadamard transform (which simplifies out). The MAP is most similar to our approach in mechanics, but the difference in derived unbinding steps leads to dramatically different performance. The VTB is the most recently developed VSA in modern use. The matrix \(V_{}\) of VTB is a block-diagonal matrix composed from the values of the \(\) vector, which we refer the reader to  for details. The TorchHD library  is used for implementations of prior methods.

This work shows that these motivations are well founded, as they result in a binding with comparable or improved performance in our testing.

Our interest in VSAs comes from their utility in both classical symbolic tasks and as useful priors in designing deep learning systems. In classic tasks VSAs are popular for designing power-efficient systems from a finite set of operations [14; 23; 17; 30]. HRRs, in particular, have shown biologically plausible models of human cognition [21; 5; 40; 6] and solving cognitive science tasks . In deep learning the TPR has inspired many prior works in natural language processing [34; 16; 35]. To wit, The HRR operation has seen the most use in differentiable systems [43; 41; 42; 27; 29; 33; 1; 2; 28]. To study our method, we select two recent works that make heavy use of the neuro-symbolic capabilities of HRRs. First, an Extreme Multi-Label (XML) task that uses HRRs to represent an output space of tens to hundreds of thousands of classes \(C\) in a smaller dimension \(d<C\), and an information privacy task that uses the HRR binding as a kind of "encrypt/decrypt" mechanism for heuristic security . We will explain these methods in more detail in the experimental section.

## 3 Methodology

First, we will briefly review the definition of the Hadamard matrix \(H\) and its important properties that make it a strong candidate from which to derive a VSA. With these properties established, we will begin by deriving a VSA we term HLB where binding and unbinding are the same operation in the same manner as which the original HRR can be derived . Any VSA must introduce noise when vectors are bound together, and we will derive the form of the noise term as \(^{}\). Unsatisfied with the magnitude of this term, we then define a projection step for the Hadamard matrix in a similar spirit to ''s complex-unit magnitude projection to support the HRR and derive an improved operation with a new and smaller noise term \(^{}\). This will give us the HLB bind/unbind steps as noted in Table 1.

Hadamard \(H_{d}\) is a square matrix of size \(d d\) of orthogonal rows consisting of only \(+1\) and \(-1\)s given in Equation 1 where \(d=2^{n}\)\(\)\(n:n 0\). Bearing in mind that Hadamard or Walsh-Hadamard Transformation (WHT) can be equivalent to discrete multi-dimensional Fourier Transform (FT) when applied to a \(d\) dimensional vector , it has additional advantages over Discrete Fourier Transform (DFT). Unlike DFT, which operates on complex \(\) numbers and requires irrational multiplications, WHT only performs calculations on real \(\) numbers with addition and subtraction operators and does not require any irrational multiplication.

\[H_{1}= H_{2}=1&1\\ 1&-1 H_{2^{n}}=H_{2^{n-1}}&H_{2 ^{n-1}}\\ H_{2^{n-1}}&-H_{2^{n-1}}\] (1)

Vector symbolic architectures (VSA), for instance, Holographic Reduced Representations (HRR) employs circular convolution to represent compositional structure which is computed using Fast Fourier Transform (FFT) . However, it can be numerically unstable due to irrational multiplications of complex numbers. Prior work  devised a projection step to mitigate the numerical instability of the FFT and it's inverse, but we instead ask if re-deriving the binding/unbinding operations may yield favorable results if we use the favorable properties of the Hadamard transform as given in Lemma 3.1.

**Lemma 3.1** (Hadamard Properties).: _Let \(H\) be the Hadamard matrix of size \(d d\) that holds the following properties for \(x,y^{d}\). First, \(H(Hx)=dx\), and second \(H(x+y)=Hx+Hy\)._

The bound composition of two vectors into a single vector space is referred to as Binding. The knowledge retrieval from a bound representation is known as Unbinding. We define the binding function by replacing the Fourier transform in circular convolution with the Hadamard transform given in Definition 3.1. We will denote the binding function four our specific method by \(\) and the unbinding function by \(^{*}\).

**Definition 3.1** (Binding and Unbinding).: The binding of vectors \(x,y^{d}\) in Hadamard domain is defined in Equation 2 where \(\) is the elementwise multiplication. The unbinding function is defined in a similar fashion, i.e., \(=^{*}\). In the context of binding, \((x,y)\) combines the vectors \(x\) and \(y\), whereas in the context of unbinding \(^{*}(x,y)\) refers to the retrieval of the vector associated with \(y\) from \(x\).

\[(x,y)= H(Hx Hy)\] (2)Now, we will discuss the binding of \(\) different representations, which will become important later in our analysis but is discussed here for adjacency to the binding definition. Composite representation in vector symbolic architectures is defined by the summation of the bound vectors. We define a parameter \(: 1\) that denotes the number of vector pairs bundled in a composite representation. Given vectors \(x_{i},y_{i}^{d}\) and \(\ i:1 i\), we can define the composite representation \(\) as

\[=(x_{1},y_{1})=(x_{1},y_{1})+(x_{2},y_{2}) _{}=_{i=1}^{}(x_{i},y_{i})\] (3)

Next, we require the unbinding operation, which is defined via an inverse function via the following theorem. This will give a symbolic form of our unbinding step that retrieves the original component \(\) being searched for, as well as a necessary noise component \(^{}\), which must exist whenever \( 2\) items are bound together without expanding the dimension \(d\).

**Theorem 3.1** (Inverse Theorem).: _Given the identity function \(Hx Hx^{}=\) where \(x^{}\) is the inverse of \(x\) in Hadamard domain, then \(^{*}((x_{1},y_{1})++(x_{},y_{} ),y_{i}^{})=x_{i}&=1\\ x_{i}+_{i}^{}&>1\) where \(x_{i},y_{i}^{d}\) and \(_{i}^{}\) is the noise component._

Proof of Theorem 3.1.: We start from the identity function \(Hx Hx^{}=\) and thus \(Hx^{}=}{Hx}\). Now using Equation 2 we get,

\[^{*}((x_{1},y_{1})++(x_{ },y_{}),y_{i}^{})= H((Hx_{1} Hy_{1}+ +Hx_{} Hy_{})})\] \[= H(Hx_{i}+}^{}(Hx_{j} Hy_{j})})=x_{i}+ H(}^{}(Hx_{j} Hy_{j})}) Lemma\] \[=x_{i}&=1\\ x_{i}+_{i}^{}&>1\]

To reduce the noise component and improve the retrieval accuracy,  proposes a projection step to the input vectors by normalizing them by the absolute value in the Fourier domain. While such identical normalization is not useful in the Hadamard domain since it will only transform the elements to \(+1\) and \(-1\)s, we will define a projection step with only the Hadamard transformation without normalization given in Definition 3.2.

**Definition 3.2** (Projection).: The projection function of \(x\) is defined by \((x)= Hx\).

If we apply the Definition 3.2 to the inputs in Theorem 3.1 then we get

\[^{*}(((x_{1}),(y_{1}))++((x_{}),(y_{})),(y_{i})^{}) =^{*}( H(x_{1} y_{1}+ x_{ } y_{}),})\] \[= H(}(x_{1} y_{1}+ x _{} y_{}))\] (4)

The retrieved value would be projected onto the Hadamard domain as well and to get back the original data we apply the reverse projection. Since the inverse of the Hadamard matrix is the Hadamard matrix itself, in the reverse projection step we just apply the Hadamard transformation again which derives the output to

\[H( H(}(x_{1} y_{1}+ x _{} y_{}))) =}(x_{1} y_{1}++x_{} y_{ })\] (5) \[=x_{i}&=1\\ x_{i}+_{j=1,\ j i}^{}y_{j}}{y_{i}}&>1 =x_{i}&=1\\ x_{i}+_{i}^{}&>1\]

where \(_{i}^{}\) is the noise component due to the projection step. In expectation, \(_{i}^{}<_{i}^{}\) (see Appendix A). Thus, the projection step diminishes the accumulated noise. More interestingly, the retrieved output term does not contain any Hadamard matrix. Therefore, we can recast the initial binding definition by multiplying the query vector \(y_{i}\) to the output of Equation 5 which makes the binding function as the sum of the element-wise product of the vector pairs and the compositional structure a linear time \((n)\) representation. Thus, the redefinition of the binding function is \(^{}(x,y)=x y\) and \(\) bundle of the vector pairs is \(^{}_{}=_{i=1}^{}(x_{i} y_{i})\). Consequently, the unbinding would be a simple element-wise division of the bound representation by the query vector, i.e, \(^{*}(x,y)=x\) where \(x\) and \(y\) are the bound and query vector, respectively.

### Initialization of HLB

For the binding and unbinding operations to work, vectors need to have an expected value of zero. However, since we would divide the bound vector with query during unbinding, values close to zero would destabilize the noise component and create numerical instability. Thus, we define a Mixture of \(\)ormal Distribution (MiND) with an expected value of zero but an absolute mean greater than zero given in Equation 6 where \(\) is the Uniform distribution. Considering half of the elements are sampled for a normal distribution of mean \(-\) and the rest of the half with a mean of \(\), the resulting vector has a zero mean with an absolute mean of \(\). The properties of the vectors sampled from a MiND distribution are given in Properties 3.1.

\[(,1/d)=(-,1/d)&(0,1)>0.5\\ (\,,1/d)&\ (0,1) 0.5\] (6)

**Properties 3.1** (Initialization Properties).: _Let \(x^{d}\) sampled from \((,1/d)\) holds the following properties. \([x]=0,\ [|x|]=\), and \(\|x\|_{2}=d}\)_

### Similarity Augmentation

In VSAs, it is common to measure the similarity with an extracted embedding \(}\) with some other vector \(\) using the cosine similarity. For our HLB, we devise a correction term when it is known that \(\) items have been bound together to extract \(}\), i.e., \(^{}(_{},)=}\). Then if \(}\) is the noisy version of the true bound term \(\), we want \((},)=1\), and \((},)=0,\). We achieve this by instead computing \((},)\), and the derivation of this corrective term is given by Theorem 3.2.

**Theorem 3.2** (\(-\) Relationship).: _Given \(x_{i},y_{i}(,1/d)\ \ i:1 i\), the cosine similarity \(\) between the original \(x_{i}\) and retrieved vector \(}\) is approximately equal to the inverse square root of the number of vector pairs in a composite representation \(\) given by \(}\)._

Proof of Theorem 3.2.: We start with the definition of cosine similarity and insert the value of \(}\). The step-by-step breakdown is shown in Equation 7.

\[=^{d}x_{i}}}{\|x_{i}\|_{2} \|}\|_{2}}=^{d} y_{j}}{y_{i}}}{\|x_{i}\|_{2}\|x_{i}\|_{2 }\|x_{i}+_{j=1,\,j i}^{}y_{j}}{y_{i}} \|_{2}}=^{d}x_{i}_{j=1, \,j i}^{d}y_{j}}{y_{i}}}{\|x_{i}\|_{2}\|x_ {i}+_{j=1,\,j i}^{}y_{j}}{y_{i}}\|_{2}}\] (7)

Employing Properties 3.1 we can derive that \(\|x_{i}\|_{2}= x_{i}}=d}\) and \(\|y_{j}}{x_{i}}\|=d}\). Thus, the square of the \(\|x_{i}+_{j=1,\,j i}^{}y_{j}}{y_{i}}\|_ {2}\) can be expressed as

\[=\|x_{i}\|_{2}^{2}+_{j=1,\,j i}^{}\|y_{j}}{y_{i}}\|_{2}^{2}+\ 2^{d}x_{i}( _{j=1,\,j i}^{}y_{j}}{y_{i}})}_{}+j=1,\,j i\\ j i}^{d}_{l=1}^{-1}y_{j}}{y_{i}} y_{l}}{y_{i}}}_{}\] (8)

\[=^{2}d+(-1)^{2}d+2+2=^{2}d+2+2\]

Therefore, using Equation 7 and Equation 8 we can write that

\[[]=d+}{d}d+2+2}}\,^{1}d}{d}d}}=d}{^{2}d}=}\]The experimental result of the \(-\) relationship closely follows the theoretical expectation provided in Appendix C which also indicates that the approximation is valid. Since, we know from Theorem 3.2 that similarity score \(\) drops by the inverse square root of the number of vector pairs in a composite representation \(\), in places where \(\) is known or can be estimated from \(\|_{}\|_{2}^{2}\) (proof in Appendix B), it can be used to update the cosine similarity multiplying the scores by \(\). Equation 9 shows the updated similarity score where in a positive case \((+)\), \(\) would be close to \(1/\) and in a negative case \((-)\), \(\) would be close to zero.

\[^{}=^{}_{(+)}=_{-}}\  1^{}_{(-)}=_{ 0}  0\] (9)

Empirical results of \(^{}\) for varying \(n\) and \(\) are visualized and verified by a heatmap. In a composite representation \(^{}_{}=_{i=1}^{}(x_{i} y_{i})\), when unbinding is applied using the query \(y_{i}\), i.e., \(}^{}(^{}_{},y_{i})=}\), a positive case is a similarity between \(x_{i}\) and \(}\). On the contrary, similarity between \(}\) and any \(x_{j}\) where \(j\{1,2,,\}\) and \(j i\), is a negative case. Mean cosine similarity scores of \(100\) trials for both positive and negative cases in presented in Figure 1 where the scores for the positive cases are in the \(\)\(( 1)\) shades and the scores for the negative cases are in the blue \(( 0)\) shades.

## 4 Empirical Results

### Classical VSA Tasks

A common VSA task is, given a bundle (addition) of \(\) pairs of bound vectors \(=_{i=1}^{}(_{i},_{i})\), given a query \(_{q}\), \(\) can the corresponding vector \(_{q}\) be correctly retrieved from the bundle. To test this, we perform an experiment similar to one in . We first create a pool \(P\) of \(N=1000\) random vectors, then sample (with replacement) \(p\) pairs of vectors for \(p\{1,2,,25\}\). The pairs are bound together and added to create a composite representation \(\). Then, we iterate through all left pairs \(_{q}\) in the composite representation and attempt to retrieve the corresponding \(_{q}, q[1,p]\). A retrieval is considered correct if \(}(,_{q})^{}_{q}>}(, _{q})^{}_{j}, j q\). The total accuracy score for the bundle is recorded, and the experiment is repeated for \(50\) trials. Experiments are performed to

Figure 1: Empirical comparison of the corrected cosine similarity scores between \(^{}_{(+)}\) (on top) and \(^{}_{(-)}\) (on bottom) for varying \(n\) and \(\) shown in heatmap. The dimension, i.e., \(d=2^{n}\) is varied from \(2\) to \(1024\)\((n\{1,2,,10\})\) and the number of vector pairs bundled is varied from \(1\) to \(50\). This shows that we can accurately identify when a vector \(\) has been bound to a VSA or not when we keep track of how many pairs of terms \(\) are included.

compare HRR , VTB , MAP , and our HLB VSAs. For each VSA, at each dimension of the vector, the area under the curve (AUC) of the accuracy vs. the no. of bound terms plot is computed, and the results are shown in Figure 2. In general, HLB has comparable performance to HRR and VTB, and performs better than MAP.

The scenario we just considered looked at bindings of only two items together, summed of many pairs of bindings.  proposed addition evaluations over sequential bindings that we now consider. In the _random_ case we have an initial vector \(_{0}\), and for \(p\) rounds, we will modify it by a random vector \(_{t}\) such that \(_{t+1}=(_{t},_{t})\), after which we unbind each \(_{t}\) to see how well the previous \(_{t}\) is recovered. In the _auto binding_ case, we use a single random vector \(\) for all \(p\) rounds.

In this task, we are concerned with the quality of the similarity score in random/auto-binding, as we want \(^{}(_{t+1},_{t})^{}_{t}=1\). For VSAs with approximate unbinding procedures, such as HRR, VTB, and MAP-C, the returned value will be 1 if \(p=1\) but will decay as \(p\) increases. HLBuses an exact unbinding procedure so that the returned value is expected to be 1 \(\)\(p\). We are also interested in the magnitude of the vectors \(\|^{}(_{t+1},_{t})\|_{2}\), where an ideal VSA has a constant magnitude that does not explode/vanish as \(p\) increases.

Figure 3 shows that HLB maintains a stable magnitude regardless of the number of bound vectors in both cases. This property arises due to the properties of the distribution shown in Properties 3.1. As all components have an expected absolute value of 1, the product of all components also has an expected absolute value of 1. Thus, the norm of the binding is simply \(\). It also shows HLB maintains the desired similarity score as \(p\) increases. Combined with Figure 1 that shows the scores are near-zero when an item is not present, HLB has significant advantages in consistency for designing VSA solutions.

Figure 3: When repeatedly binding different random (left) or a single vector (right), HLB consistently returns the ideal similarity score of 1 for a present item (top row) and has a constant magnitude (bottom row), avoiding exploding/vanishing values.

Figure 2: The area under the accuracy curve due to the change of no. of bundled pairs \(\) for dimensions \(d\). All the dimensions are chosen to be perfect squares due to the constraint of VTB.

### Deep Learning with Hadamard-derived Linear Binding

Two recent methods that integrate HRR with deep learning are tested to further validate our approach and briefly summarized in the two sub-sections below. In each case, we run all four VSAs and see that HLB either matches or exceeds the performance of other VSAs. In every experiment, the standard method of sampling vectors from each VSA is followed as outlined in Table 1. All the experiments are performed on a single NVIDIA TESLA PH402 GPU with \(32\)GB memory.

#### 4.2.1 Connectionist Symbolic Pseudo Secrets

When running on low-power computing environments, it is often desirable to offload the computation to a third-party cloud environment to get the answer faster and use fewer local resources. However, this may be problematic if one does not fully trust the available cloud environments. Homomorphic encryption (HE) is the ideal means to alleviate this problem, providing cryptography for computations. HE is currently more expensive to perform than running a neural network itself , defeating its own utility in this scenario. Connectionist Symbolic Pseudo Secrets (CSPS)  provides a heuristic means of obscuring the nature of the input (content), and output (number of classes/prediction), while also reducing the total local compute required.

CSPS mimics a "one-time-pad" by taking a random VSA vector \(\) as the _secret_ and binding it to the input \(\). The value \((,)\) obscures the original \(\), and the third-party runs the bulk of the network on their platform. A result \(}\) is returned, and a small local network computes the final answer after unbinding with the secret \(^{*}(},)\). Other than changing the VSA used, we follow the same training, testing, architecture size, and validation procedure of .

CSPS experimented with 5 datasets MNIST, SVHN, CIFAR-10 (CR10), CIFAR-100 (CR100), and Mini-ImageNet (MIN). First, we look at the accuracy of each method, which is lower due to the noise of the random vector \(\) added at test time since no secret VSA is ever reused. The results are shown in Table 2, where HLB outperforms all prior methods significantly. Notably, the MAP VSA is second best despite being one of the older VSAs, indicating its similarity to HLB in using a simple binding procedure, and thus simple gradient may be an important factor in this scenario.

However, improved accuracy is not useful in this scenario if more information is leaked. The test in this scenario, as proposed by , is to calculate the Adjusted Rand Index (ARI) after attempting to cluster the inputs \(\) and the outputs \(}\), which are available/visible to the snooping third-party. To be successful, the ARI must be near zero (indicating random label assignment) for both inputs and outputs.

We use K-means, Gaussian Mixture Model (GMM), Birch , and HDBSCAN  as the clustering algorithms and specify the true number of classes to each method to maximize attacker success (information they would not know). The results can be found in Table 3, where the top rows indicate the clustering of the input \((,)\), and the bottom rows the clustering of the output \(}\). All the numbers are percentages \((\%)\), showing all methods do a good job at hiding information from the adversary (except on the MNIST dataset, which is routinely degenerate).

The MNIST result is a good reminder that CSPS security is heuristic, not guaranteed. Nevertheless, we see HLB has consistently close-to-zero scores for SVHN, CIFARs, and Mini-ImageNet, indicating

    & Dims/ & CSPS + HRR &  & CSPS + MAP-C & CSPS + MAP-B & CSPS + HLB \\   & Labels & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 & Top@1 & Top@5 \\  MNIST & \(28^{2}/10\) & \(98.51\) & – & \(98.44\) & – & \(98.46\) & – & \(98.40\) & – & \(\) & – \\ SVHN & \(32^{2}/10\) & \(88.44\) & – & \(19.59\) & – & \(79.95\) & – & \(92.43\) & – & \(\) & – \\ CR10 & \(32^{2}/10\) & \(78.21\) & – & \(74.22\) & – & \(76.69\) & – & \(82.83\) & – & \(\) & – \\ CR100 & \(32^{2}/100\) & \(48.84\) & \(75.82\) & \(35.87\) & \(61.79\) & \(56.77\) & \(81.52\) & \(57.76\) & \(84.63\) & \(\) & \(\) \\ MIN & \(84^{2}/100\) & \(40.99\) & \(66.99\) & \(45.81\) & \(73.52\) & \(52.22\) & \(78.63\) & \(57.91\) & \(82.81\) & \(\) & \(\) \\  GM & & \(67.14\) & \(71.26\) & \(47.24\) & \(67.40\) & \(70.89\) & \(80.06\) & \(75.90\) & \(83.72\) & \(\) & \(\) \\   

Table 2: Accuracy comparison of the proposed HLB with HRR, VTB, MAP-C, and MAP-B in CSPS. The dimensions of the inputs along with the no. of classes are listed in the Dims/Labels column. The last row shows the geometric mean of the results.

that its improved accuracy with simultaneously improved security. This also validates the use of the VSA in deep learning architecture design and the efficacy of our approach.

#### 4.2.2 Xtreme Multi-Label Classification

Extreme Multi-label (XML) is the scenario where, given a single input of size \(d\), \(C>>d\) classes are used to predict. This is common in e-commerce applications where new products need to be tagged, and an input on the order of \(d 5000\) is relatively small compared to \(C\) 100,000 or more classes. This imposes unique computational constraints due to the output space being larger than the input space and is generally only solvable because the output space is sparse -- often less than 100 relevant classes will be positive for any one input. VSAs have been applied to XML by exploiting the low positive class occurrence rate to represent the problem symbolically .

While many prior works focus on innovative strategies to cluster/make hierarchies/compress the penultimate layer, a neuro-symbolic approach was proposed by . Given \(K\) total possible classes, they assigned each class a vector \(_{k}\) to be each class's representation, and the set of all classes \(=_{k=1}^{K}_{k}\).

The VSA trick used by  was to define an additional "present" class \(\) and a "missing" class \(\). Then, the target output of the network \(f()\) is itself a vector composed of two parts added together. First, \((,_{k}_{k})\) represents all _present_ classes, and so the sum is over a finite smaller set. Then the absent classes compute the _missing_ representing \((,-_{k}_{k})\), which again only needs to compute over the finite set of present classes, yet represents the set of all non-present classes by exploiting the symbolic properties of the VSA.

For XML classification, we have a set of \(K\) classes that will be present for a given input, where \(K 10\) is the norm. Yet, there will be \(L\) total possible classes where \(L 100,000\) is quite common. Forming a normal linear layer to produce \(L\) outputs is the majority of computational work and memory use in standard XML models, and thus the target for reduction. A VSA can be used to side-step this cost, as shown by , by leveraging the symbolic manipulation of the outputs. First, consider the target label as a vector \(^{d}\) such that \(d L\). By defining a VSA vector to represent "present" and "missing" classes as \(\) and \(\), where each class is given its own vector \(_{1,,L}\), we can shift the computational complexity form \((L)\) to \((K)\) by manipulating the "missing" classes as the compliment of the present classes as shown in Equation 10.

    &  &  \\   & MNIST & SVHN & CR10 & CR100 & MIN & MNIST & SVHN & CR10 & CR100 & MIN \\  K-Means & \(-0.02\) & \(-0.01\) & \(0.18\) & \(0.54\) & \(0.42\) & \(-0.00\) & \(-0.01\) & \(-0.01\) & \(0.02\) & \(0.00\) \\ GMM & \(0.01\) & \(0.00\) & \(0.09\) & \(0.61\) & \(0.44\) & \(4.67\) & \(1.37\) & \(-0.01\) & \(0.02\) & \(0.01\) \\ Brach & \(0.20\) & \(0.00\) & \(0.14\) & \(0.45\) & \(0.35\) & \(0.02\) & \(0.03\) & \(0.04\) & \(0.08\) & \(0.03\) \\ HDBSCAN & \(0.00\) & \(-0.24\) & \(1.23\) & \(0.01\) & \(0.02\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\  K-Means & \(1.28\) & \(0.06\) & \(0.21\) & \(0.03\) & \(0.08\) & \(8.52\) & \(0.13\) & \(1.11\) & \(0.05\) & \(0.12\) \\ GMM & \(1.28\) & \(0.06\) & \(0.17\) & \(0.04\) & \(0.09\) & \(8.63\) & \(0.14\) & \(1.63\) & \(0.05\) & \(0.00\) \\ Brach & \(1.51\) & \(0.03\) & \(0.13\) & \(0.05\) & \(0.07\) & \(3.24\) & \(0.00\) & \(0.64\) & \(0.06\) & \(0.17\) \\ HDBSCAN & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.09\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\    &  &  \\   & MNIST & SVHN & CR10 & CR100 & MIN & MNIST & SVHN & CR10 & CR100 & MIN \\  K-Means & \(0.17\) & \(0.01\) & \(0.01\) & \(0.00\) & \(0.00\) & \(0.09\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ GMM & \(3.39\) & \(-0.01\) & \(0.01\) & \(0.00\) & \(0.00\) & \(2.53\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\ Biech & \(0.84\) & \(-0.00\) & \(0.00\) & \(0.01\) & \(0.00\) & \(0.83\) & \(0.00\) & \(0.00\) & \(0.01\) & \(0.00\) \\ HDBSCAN & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\  K-Means & \(15.91\) & \(0.09\) & \(0.00\) & \(0.03\) & \(0.01\) & \(13.07\) & \(-0.04\) & \(0.01\) & \(0.02\) & \(-0.00\) \\ GMM & \(42.43\) & \(0.11\) & \(0.00\) & \(0.03\) & \(0.00\) & \(14.96\) & \(-0.04\) & \(0.01\) & \(0.02\) & \(0.00\) \\ Biech & \(7.09\) & \(-0.07\) & \(-0.02\) & \(0.01\) & \(-0.00\) & \(18.44\) & \(-0.07\) & \(0.00\) & \(0.01\) & \(0.02\) & \(0.02\) \\ HDBSCAN & \(0.48\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) & \(7.60\) & \(0.01\) & \(0.00\) & \(0.00\) & \(0.00\) & \(0.00\) \\   

Table 3: Clustering results of the main network inputs (top rows) and outputs (bottom rows) in terms of Adjusted Rand Index (ARI). Because CSPS is trying to hide information, scores near zero are better. Cell color corresponds to the cell absolute value, with blue indicating lower ARI and red indicating higher ARI. All numbers in percentages, and show HLB is better at information hiding.

[MISSING_PAGE_EMPTY:10]