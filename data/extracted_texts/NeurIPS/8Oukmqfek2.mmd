# Rethinking Gauss-Newton for learning

over-parameterized models

 Michael Arbel, Romain Menegaux, and Pierre Wolinski

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK,38000 Grenoble, France

firstname.lastname@inria.fr

Equal contribution.Now affiliated with the LMO, Universite Paris-Saclay, Orsay, France.

###### Abstract

This work studies the global convergence and implicit bias of Gauss Newton's (GN) when optimizing over-parameterized one-hidden layer networks in the mean-field regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. While GN is consistently faster than GD in finding a global optimum, the learned model generalizes well on test data when starting from random initial weights with a small variance and using a small step size to slow down convergence. Specifically, our study shows that such a setting results in a hidden learning phenomenon, where the dynamics are able to recover features with good generalization properties despite the model having sub-optimal training and test performances due to an under-optimized linear layer. This study exhibits a trade-off between the convergence speed of GN and the generalization ability of the learned solution.

## 1 Introduction

Gauss-Newton (GN) and related methods, like the Natural Gradient (NG) can offer faster convergence rates compared to gradient descent due to the improved dependence on the conditioning of the problem . For these reasons, these methods have attracted attention in machine learning as an alternative to gradient descent when optimizing ill-conditioned problems arising from the use of vastly over-parameterized networks and large training sets . Unfortunately, GN's high computational cost per iteration, which involves solving an expensive linear system, restricts its applicability in large-scale deep learning optimization. Addressing this challenge has been a primary focus, with extensive efforts dedicated to crafting computationally efficient approximations for GN/NG methods where the aim is to strike a delicate balance between computational cost and optimization speed, thereby enhancing scalability .

While substantial effort has been put into making GN scalable, little is known about the global convergence of these methods and their generalization properties when optimizing neural networks, notably compared to gradient descent. Understanding these properties is particularly relevant knowing that state-of-the-art machine learning methods rely on over-parameterized networks for which infinitely many solutions interpolating the data exist albeit exhibit varying generalization properties that depend on _implicit bias_ of the optimization procedure . To date, the majority of studies on generalization in over-parameterized networks have primarily focused on analyzing the dynamics of gradient descent and gradient flows - their continuous-time limit -, making important progress in understanding the implicit bias of these optimization procedures. Specifically, showed that a gradient flow can run under two regimes which yield solutions with qualitatively different generalization properties: the _kernel regime_, where the internal features of a network remain essentially constant during optimization, often yields poor generalization, and the _feature learning regime_, which allows the internal features of a network to adapt to the data, seems behind the impressive generalization properties observed in practice. In addition, the same work precisely defines conditions for these regimes: The _kernel regime_ systematically occurs, for instance, when taking the _Neural Tangent limit_ (NTK), i.e. scaling the output of a network of width \(M\) by a factor \(1/\), while the _feature learning regime_ can arise when instead taking the _mean-field limit_, i.e. scaling the output by a factor of \(1/M\). However, these insights rest upon attaining a global solution, which is far from obvious given the non-convexity of the training objective. While global linear convergence of gradient flows is known in the kernel regime , obtaining such convergence guarantees in the _feature learning regime_ remains an active research area . Importantly, all these works have focused solely on gradient flows and gradient descent, given their prevalent usage when optimizing neural networks.

In contrast, GN/NG methods have received limited attention in the study of both generalization and global convergence for over-parameterized networks. The added complexity of these methods makes their analysis intricate, particularly in the stochastic setting where inaccurate estimates of the pre-conditioning matrix can alter the dynamics in a highly non-trivial manner [32, Section 12.2]. Recent works studied the convergence and generalization of GN in the kernel regime , while others focused on linear networks . However, the behavior of these methods for non-linear over-parameterized models remains understudied in the _feature learning regime_ which is a regime of interest as it is more likely to result in good generalization.

**Contributions.** The present work aims to deepen our understanding of the properties of Gauss-Newton (GN) methods for over-parameterized networks, building upon recent advancements in neural network optimization . Unlike prior works that primarily addressed scalability issues of GN/NG or studied the impact of stochastic noise on optimization dynamics, we uniquely focus on studying the convergence and implicit bias of an _exact GN method_. Specifically, our research investigates GN in the context of a regression problem using over-parameterized one-hidden layer networks in a deterministic (_full-batch_) setting. By examining an exact, deterministic GN method, rather than an approximate or stochastic version, we eliminate potential regularization effects stemming from approximation and stochasticity that might influence convergence and generalization. This strategy, in turn, enables us to provide both theoretical and empirical insights, shedding light on the properties of GN methods when optimizing neural networks:

1. We provide a global convergence result in Proposition 3 with a linear rate for the continuous-time limit of GN's dynamics in the over-parameterized regime that holds under the mean-field limit. The achieved rate shows better conditioning than the gradient flow with the same objective, emphasizing the method's advantage from a pure optimization perspective. While similar rates can be achieved under strong non-degeneracy conditions as in , these conditions are never met over-parameterized networks. Instead, our result relies on a simple condition on the initial parameters of a similar nature as in [13, Thm 3.3]: (i) that the hidden weights are diverse enough so that an interpolating solution can be obtained simply by fitting the linear weights, and (ii) that the linear weights result from a simple fine-tuning procedure to reach near optimality. The result shows that GN can reach the data interpolation regime, often required for studying the implicit bias of optimization (Section 4).
2. The empirical study in Section 5 complements the convergence result in Proposition 3 by further investigating the implicit bias of GN on a student/teacher regression task using synthetic data. We show that, while GN attains an interpolating solution, as predicted by Proposition 3, such a solution generalizes more or less well depending on the regime under which the GN dynamics is running. Specifically, we show that the GN dynamics can exhibit both kernel and feature learning regimes depending on the choice of step size and variance of the weights at initialization. Our experiments also indicate that a _hidden learning_ phenomenon occurs in GN dynamics where hidden features that generalize well are obtained even when both _raw_ training and test performances are sub-optimal due to an under-optimized linear layer. Quite surprisingly, these features are found when using small step sizes and, sometimes, generalize better than those learned by gradient descent. This is contrary to the common prescription of using larger step sizes in gradient descent for better generalization . Our results suggest a tradeoff between optimization speed and generalization when using GN which is of practical interest.

Related work

Convergence of Gauss-Newton's methods.A large body of work on inverse problems studied the local convergence of Gauss-Newton and its regularized versions such as the Levenberg-Marquardt method [25; 38]. Many such works focus on a continuous-time analysis of the methods as it allows for a simplified study [24; 41]. In this work, we also consider continuous-time dynamics and leave the study of discrete-time algorithms to future work. More recently, increasing attention was given to global convergence analysis.  studied the global convergence of GN with an _inexact oracle_ and in a stochastic setting establishing linear convergence. The analysis requires that the smallest singular value of the Jacobian is positive near initialization, a situation that can never occur in the over-parameterized regime.  studied the convergence of the Levenberg-Marquardt dynamics, a regularized version of Gauss-Newton's method, in a non-convex setting under a _cubic growth_ assumption and showed global albeit sub-linear convergence rates. It is unclear, however, if such _cubic growth_ assumption holds in our setting of interest. Closest to our work is  which aims to accelerate optimization by solving a kernel regression in the NTK scaling limit. There, the authors show global convergence of a regularized version of GN in the _kernel regime_. We aim to investigate the behavior of GD in the _feature learning_ regimes.

The implicit bias of gradient flows.In the over-parameterized setting, the training objective often possesses several global minima that perfectly interpolate the training data. The generalization error thus heavily depends on the solution selected by the optimization procedure.  highlighted the importance of understanding the properties of the selected solutions for a given initialization and several works studied this problem in the case of linear neural networks [6; 33; 40; 49; 51]. For nonlinear networks, there is still no general characterization of the implicit bias in the regression setting, although  recently made important progress in the case of a one-hidden layer network with ReLU activation and orthogonal inputs showing that gradient flows select a minimum norm interpolating solution when the initial weights are close to zero. Recently, the implicit bias of gradient flows was shown to play a crucial role in other optimization problems such as non-convex bi-level optimization [5; 47], therefore illustrating the ubiquity of such phenomenon in deep learning. The present work empirically studies the implicit bias of Gauss-Newton which is vastly understudied.

## 3 Setup and preliminaries

### Regression using over-parameterized one-hidden layer networks

We consider a regression problem where the goal is to approximate an unknown real-valued function \(f^{}\) defined over a subset \(\) of \(^{d}\) from a pair of i.i.d. inputs/outputs \((x_{n},y_{n})_{1 n N}\) where each \(x_{n}\) is a sample from some unknown distribution \(\) and \(y_{n}=f^{}(x_{n})\). We assume that \(f^{}\) belongs to \(L_{2}()\), the set of square-integrable functions w.r.t. \(\) and note that \(f^{}\) always belongs to \(L_{2}(})\), where \(}\) denotes the empirical distribution defined by the samples \((x_{1},,x_{N})\). For simplicity, we assume the data are non-degenerate, meaning that \(x_{n}\)\(\)\(x_{n^{}}\) whenever \(n n^{}\). In this case \(L_{2}(})\) is isomorphic to \(^{N}\) (i.e., \(L_{2}(})^{N}\)) and we can identify any function \(f L_{2}(})\) with the evaluation vector \((f(x_{1}),,f(x_{N}))\). We are interested in approximating \(f^{}\) using a one-hidden layer network \(f_{w}\) with parameter vector \(w\) belonging to some, possibly infinite, Hilbert space \(\). The network's parameters are learned by minimizing an objective of the form \((f_{w})\) where \(\) is a non-negative, \(L\)-smooth, and \(\)-strongly convex function defined over \(L_{2}(})\) and achieving a \(0\) minimum value at \(f^{}\). A typical example that we consider in Section 5 is the mean-squared error over the training set:

\[_{w}\ (f_{w}),(f_{w})= _{n=1}^{N}{(f_{w}(x_{n})-y_{n})}^{2}. \]

For the convergence results, we will consider both _finite-width_ one-hidden layer networks and their _mean-field infinite-width limit_. In the experiments, we will restrict to finite-width networks although, we will be using a relatively large number of units \(M\) compared to the size of the training data, therefore approximating the mean-field limit.

**Finite-width one-hidden layer networks.** Given a non-polynomial point-wise activation function \(\) and some positive integer \(M\), these networks take the form:

\[f_{w}(x)=_{i=1}^{M}v_{i}(u_{i}^{}x), w=(v,u) :=^{M}^{M d}, \]

where \(v^{M}\) is the linear weight vector while \(u^{M d}\) is the hidden weight matrix. Popular choices for \(\) include ReLU and its smooth approximation \((x)=x(1+e^{- x})^{-1}\) which can be made arbitrary close to ReLU by increasing \(\). The above formulation can also account for a bias term provided the input vector \(x\) is augmented with a non-zero constant component.

**Mean-field infinite-width limit.** We consider some base probability \(\) measure with full support on \(\) and finite second moment and denote by \(L_{2}()\) and \(L_{2}(,^{d})\) the set of square integrable functions w.r.t. \(\) and taking values in \(\) and \(^{d}\). Given a non-polynomial point-wise non-linearity, we define infinitely-wide one-hidden layer networks \(f_{w}\) as:

\[f_{w}(x)= v(c)(u(c)^{}x)\,(c), w:=(v,u) :=L_{2}() L_{2}(,^{d}). \]

Functions of the form (3) correspond to the _mean-field limit_ of the network defined in (10) when the number of units \(M\) grows to infinity and appear in the Lagrangian formulation of the Wasserstein gradient flow of infinitely wide networks . Existing global convergence results of GN methods were obtained in the context of the NTK limit  which does not allow _feature learning_. The mean-field limit we consider here can result in optimization dynamics that exhibit _feature learning_ which is why we consider it in our global convergence result analysis. For completeness, we provide a brief discussion on mean-field vs NTK limits and kernel vs feature learning regimes in Appendix A.

**Over-parameterization.** We consider an over-parameterized setting where the network has enough parameters to be able to fit the training data exactly, thus achieving a \(0\) training error. To this end, we define the gram matrix \(G\) which is an \(N N\) matrix taking the following forms depending on whether we are using a finite-width network \((G^{F})\) or infinite-width network \((G^{I})\):

\[G^{F}_{n,n^{}}(u)=_{i=1}^{M}(u_{i}^{}x_{n}) (u_{i}^{}x_{n^{}}), G^{I}_{n,n^{}}(u)=(u (c)^{}x_{n})(u(c)^{}x_{n^{}})\,(c).\]

We say that a network is over-parameterized if \(G(u_{0})\) is invertible for some hidden-layer weight \(u_{0}\):

* **(Over-parameterization)**\(G(u_{0})\) is invertible for an initial parameter \(w_{0}\)\(=\)\((v_{0},u_{0})\)\(\)\(\).

When \(u_{0}\) is sampled randomly from a Gaussian, a common choice to initialize a neural network, Assumption (**A**) holds for infinitely wide networks as long as the training data are non-degenerate (see [18, Theorem 3.1] for ReLU, and [17, Lemma F.1] for analytic non-polynomial activations). There, the _non-degeneracy_ assumption simply means that the inputs are not parallel to each other (i.e. \(x_{i} x_{j}\) whenever \(i j\) for any \(1 i,j N\)), a property that holds almost surely if the data distribution \(\) has a density w.r.t. Lebesgue measure. In the case of finite-width networks with \(M>N\), the result still holds with a high probability when \(u\) is sampled from a Gaussian .

### Generalized Gauss-Newton dynamics

To solve (1), we consider optimization dynamics based on a generalized Gauss-Newton method. To this end, we denote by \(J_{w}\) the Jacobian of \((f_{w}(x_{1}),...,f_{w}(x_{N}))\) w.r.t. parameter \(w\) which can be viewed as a linear operator from \(\) to \(^{N}\). Moreover, \((f_{w})\) denotes the vector of size \(N\) representing the gradient of \(\) w.r.t. the outputs \((f_{w}(x_{1}),...,f_{w}(x_{N}))\). We can then introduce the Gauss-Newton vector field \(:\) defined as:

\[(w):=(J_{w}^{}H_{w}J_{w}+(w)I)^{-1}J_{w}^{} (f_{w}),\]

where \(H_{w}\) is a symmetric positive operator on \(L_{2}(})\) and \((w)\) is an optional non-negative (possibly vanishing) damping parameter. Starting from some initial condition \(w_{0}\) and for a given positive step-size \(\), the Gauss-Newton updates and their continuous-time limit are given by:

\[w_{k+1}=w_{k}-(w_{k}),_{t}=-(w_{t}). \]The continuous-time limit is obtained by taking the step-size \(\) to \(0\) and rescaling time appropriately. When \(H_{w}\) is given by the Hessian of \(\), the dynamics in (4) recovers the generalized Gauss-Newton dynamics in continuous time when no damping is used  and recovers the continuous-time Levenberg-Maquardt dynamics when a positive damping term is added . When the matrix \(H_{w}\) is the identity \(H_{w}=I\), the resulting dynamics is tightly related to the natural gradient [2; 30]. More generally, we only require the following assumption on \(H_{w}\):

* \(H_{w}\) is continuously differentiable in \(w\) with eigenvalues in \([_{H},L_{H}]\) for positive \(L_{H}\), \(_{H}\).

**Optional damping.** Amongst possible choices for the damping, we focus on \((w)\) of the form:

\[(w)=^{2}(w) \]

where \(\) is a non-negative number \( 0\), and \(^{2}(w)\) is the smallest eigenvalue of the _Neural Tangent Kernel_ (NTK) \(A_{w}:=J_{w}J_{w}^{}\) which is invertible whenever \(J_{w}\) is surjective. This choice allows us to study the impact of scalar damping on the convergence rate of the GN dynamics. While computing \(^{2}(w)\) is challenging in practice, we emphasize that the damping is only optional and that the case when no damping is used, i.e. when \(=0\), is covered by our analysis.

## 4 Convergence analysis

### Global convergence under no-blow up

We start by showing that the continuous-time dynamics in (4) converge to a global optimum provided that it remains defined at all times.

**Proposition 1**.: _Let \(w_{0}=(v_{0},u_{0})\) be an initial condition so that \(u_{0}\) satisfies Assumption (A). Under Assumption (B) and assuming that the activation \(\) is twice-continuously differentiable, there exists a unique solution to the continuous-time dynamics in (4) defined up to some maximal, possibly infinite, positive time \(T\). If \(T\) is finite, we say that the dynamics_ **blows-up** _in finite time. If, instead \(T\) is infinite, then \(f_{w_{t}}\) converges to a global minimizer of \(\). Moreover, denoting by \(\) the strong convexity constant of \(\) and defining \(_{GN}:=2/(L_{H}(1+/_{H}))\), it holds that:_

\[(f_{w_{t}})(f_{w_{0}})e^{-_{GN}t}. \]

Proposition 1 is proved in Appendix B.2. It relies on the Cauchy-Lipshitz theorem to show that the dynamics are well-defined up to a positive time \(T\) and uses strong convexity of \(\) to obtain the rate in (6). The result requires a smooth non-linearity, which excludes ReLU. However, it holds for any smooth approximation to ReLU, such as SiLU. As we discuss in Section 5, this does not make a large difference in practice as the approximation becomes tighter. The rate in (6) is only useful when the dynamics is defined at all times which is far from obvious as the vector field \(\) can diverge in a finite time causing the dynamics to explode. Thus, the main challenge is to find conditions ensuring the vector field \(\) remains well-behaved which is the object of Section 4.2.

**Comparaison with gradient flow.** The rate in (6) only depends on the strong-convexity constant \(\) of \(\), the smoothness constant of \(H\), and damping strength \(\), with the fastest rate achieved when no damping is used, i.e. \(=0\). For instance, when choosing \(H\) to the identity and \(=0\), the linear rate becomes \(_{GN}=2\). This is in contrast with a gradient flow of \(w(f_{w})\) for which a linear convergence result, in the kernel regime, follows from [15, Theorem 2.4]:

\[(f_{w_{t}})(f_{w_{0}})e^{_{GD}t},\]

with \(_{GF}:=(w_{0})/4\) where \((w_{0})\) is the smallest singular value of the initial NTK matrix \(A_{w_{0}}\). In practice, \(_{GF}_{GN}\) since the linear rate \(_{GF}\) for a gradient flow is proportional to \((w_{0})\), which can get arbitrarily small as the training sample size \(N\) increases thus drastically slowing down convergence .

### Absence of blow-up for almost-optimal initial linear layer

In this section, we provide a condition on the initialization \(w_{0}=(v_{0},u_{0})\) ensuring the Gauss-Newton dynamics never blows up. We start with a simple result, with a proof in Appendix B.1, showing the existence of a vector \(v^{}\) so that \(w^{}:=(v^{},u_{0})\) interpolates the training data whenever the over-parameterization assumption Assumption (A) holds for a given \(u_{0}\).

**Proposition 2**.: _Let \(w_{0}=(v_{0},u_{0})\) be an initial condition so that \(u_{0}\) satisfies Assumption (A). Define \((u):=(u^{}x_{n})_{1 n N}\) and \(F^{}:=(f^{}(x_{1}),...,f^{}(x_{N}))\) and let \(v^{}\) be given by:_

\[v^{}=(u_{0})G(u_{0})F^{}.\]

_Then the vector \(w^{}=(v^{},u_{0})\) is a global minimizer of \(w(f_{w})\) and a fixed point of the Gauss-Newton dynamics, i.e. \((w^{})=0\)._

The above result ensures a global solution can always be found by optimizing the linear weights while keeping the hidden weights fixed. In practice, this can be achieved using any standard optimization algorithm such as gradient descent since the function \(v l(v)=(_{(v,u_{0})})\) converges to a global optimum as it satisfies a Polyak-Lojasiewicz inequality (see Proposition 4 in Appendix B.1). However, this procedure is not of interest here as it does not allow us to learn the hidden features. Nonetheless, since \(w^{}=(v^{},u_{0})\) is a fixed point of the Gauss-Newton dynamics, we can expect that an initial condition \(w_{0}=(v_{0},u_{0})\), where \(v_{0}\) is close to \(v^{}\), results in a well-behaved dynamics. The following proposition, proven in Appendix B.4, makes the above intuition more precise.

**Proposition 3**.: _Let \(w_{0}=(v_{0},u_{0})\) be an initial condition so that Assumption (A) holds, i.e. the gram matrix \(G(u_{0})\) is invertible, and denote by \(_{0}^{2}\) the smallest eigenvalue of \(G(u_{0})\). Assume that the activation \(\) is twice-continuously differentiable and that Assumption (B) holds. Let \(R\) be any arbitrary positive number and define \(C_{R}=_{w(w_{0},R)}\|_{w}J_{w}\|_{op}\) where \(\|.\|_{op}\) denotes the operator norm. If the linear layer is almost optimal \(v_{0}\) in the sense that:_

\[\|(f_{w_{0}})\|<,:=(_{ H}_{GN}/8LN)(R,C_{R}^{-1})_{0},_{0}^{2}, \]

_then (4) is defined at all times, i.e. \(T{=}+\), the objective \((f_{w_{t}})\) converges at a linear rate to \(0\) according to (6) and the parameters \(w_{t}\) remain within a ball of radius \(R\) centered around \(w_{0}\)._

Proposition 3 essentially states that the dynamics never blow up, and hence, converge globally at the linear rate in (6) by Proposition 1, provided the hidden features are diverse enough and the initial linear weights optimize the objective well enough. As discussed in Section 3.1, the first condition on the hidden weights \(u_{0}\) typically holds for a Gaussian initialization when the data are non-degenerate. Additionally, the near-optimality condition on the linear weights \(v_{0}\) can always be guaranteed by optimizing the linear weights while keeping the hidden ones fixed.

**Remark 1**.: In the proof of Proposition 3, we show that the occurrence of a finite-time blow-up is tightly related to the Neural Tangent Kernel matrix \(A_{w_{t}}\) becoming singular at \(t\) increases which causes the vector field \(\) to diverge. When the NTK matrix \(A_{w_{t}}\) at time \(t\) is forced to remain close to the initial one \(A_{w_{0}}\), as is the case in the NTK limit considered in , one can expect that the singular values of \(A_{w_{t}}\) remain bounded away from \(0\) since this is already true for \(A_{w_{0}}\). Therefore, the main technical challenge is the study, performed in Propositions 7 to 9 of Appendix B.3, of the time evolution of the eigenvalues of \(A_{w_{t}}\) in the _mean-field limit_, where \(A_{w_{t}}\) is allowed to differ significantly from the initial NTK matrix \(A_{w_{0}}\). The final step is to deduce that, while the weights \(w_{t}\) are allowed to be in a ball centered in \(w_{0}\) of arbitrarily large radius \(R\), the singular values remain bounded away from \(0\) provided the initial linear weights \(v_{0}\) satisfy the condition in (7).

**Remark 2**.: Comparing Proposition 3 to the convergence results for gradient flows in the _kernel regime_[15, Theorem 2.4], our result also holds for an infinite dimensional space of parameters such as in the _mean-field limit_ of one-hidden layer network. Moreover, while [15, Theorem 2.4] requires the norm \(w_{t}\) to be within a ball of fixed radius \(R_{0}\) determined by initial smallest singular value \((w_{0})\) of \(A_{w_{0}}\), our result allows \(w_{t}\) to be within arbitrary distance \(R\) regardless of the initial \((w_{0})\) provided the condition in (7) holds. Proposition 3 is more similar in flavor to the convergence result of the Wasserstein gradient flow for some probability functionals in [13, Theorem 3.3, Corollary 3.4] which requires the initial objective to be smaller than a given threshold.

While the results of this section guarantee global convergence of the training objective, they do not guarantee learning features that generalize well. Next, we empirically show that GN can exhibit regimes where it yields solutions with good generalization properties.

## 5 Empirical study of generalization for a Gauss-Newton algorithm

We perform a comparative study between gradient descent (GD) and Gauss-Newton (GN) method in the context of over-parameterized networks. Since we observed little variability when changing theseed, we have chosen to use a single seed for the sake of clarity in the figures when presenting the results of this section, while deferring the analysis for multiple seeds to Appendix C.1. Additional experiments using MNIST dataset  are provided in Appendix C.4. The results presented below were obtained by running 720 independent runs, each of which optimizes a network given a specific configuration on a GPU. The total time for all runs was 3600 GPU hours.

### Experimental setup

We consider a regression task on a synthetic dataset consisting of \(N\) training points \((X_{n},Y_{n})_{1 n N}\). The objective is to minimize the mean-squared error \((f_{w})\), defined in (1), over the parameters \(w\) of a model \(f_{w}\) that predicts the target values \(Y_{n}\) based on their corresponding input points \(X_{n}\).

**Data generation.** We generate \(N\) i.i.d. samples denoted as \((X_{n})_{1 n N}\) from a standard Gaussian distribution of dimension \(d=10\). Each corresponding target \(Y_{n}\) is obtained by applying a predefined function \(f^{}\), referred to as the _teacher network_, to the input \(X_{n}\) (i.e., \(Y_{n}=f^{}(X_{n})\)). We choose \(f^{}\) to be a one-hidden layer network of the form in (10) with \(M^{}=5\) hidden units \((v_{i}^{},u_{i}^{})_{1 i M^{}}\) drawn independently from a standard Gaussian distribution. Furthermore, we consider two non-linearities \(\) when defining \(f^{}\): the widely used ReLU activation  for the main results and its smooth approximation SiLU for additional ablations in Appendix C.3. This choice of target function \(f^{}\) results in a hidden low-dimensional structure in the regression task as \(f^{}\) depends only on \(5\)-dimensional linear projection of the input data. In most cases, we take the size of the training data to be \(N=500\) except when studying the effect of the training size. This choice allows us to achieve a balance between conducting an extensive hyper-parameter search and being able to compute precise GN updates on the complete training data within a reasonable timeframe. Finally, we generate \(10000\) test samples to measure the generalization error.

**Model.** We consider a well-specified setting where the model \(w f_{w}\), referred to as the _student network_, is also a one-hidden layer with the same activation function \(\) as the _teacher network_\(f^{}\). Importantly, the student network possesses a number \(M=5000\) of hidden units, denoted as \(w:=(v_{i},u_{i})_{1 i M}\), which is much larger than the teacher's number of units \(M^{}=5\) and allows fitting the training data perfectly (over-parameterized regime). Following , we also normalize the output of the student \(f_{w}\) by \(M\) to remain consistent with the _mean-field limit_ as \(M\) increases.

**Initialization.** We initialize the student's hidden units according to a centered Gaussian with standard deviation (std) \(_{0}\) ranging from \(10^{-3}\) to \(10^{3}\). Varying the initial std \(_{0}\) allows us to study the optimization dynamics in two regimes: the _kernel regime_ (large values of \(_{0}\)) and the _feature learning regime_ (small values of \(_{0}\)). Finally, we initialize the weights of the last layer to be \(0\). This choice allows us to perform a systematic comparison with the minimum norm least squares solution obtained using random features as described next.

**Methods and baselines.** We consider three optimization methods for the model's parameters: a regularized version of Gauss-Newton (GN), gradient descent (GD), and optimizing the linear layer parameters alone, which can be viewed as a random features (RF) model.

**(GN):** We use the discrete GN updates in (4) with a constant step-size \(\) and \(H_{w}=I\). Each update is obtained using Woodbury's matrix identity by writing \((w_{k}){=}J_{w_{k}}^{}z_{k}\) with \(z_{k}\) the solution of a linear system \((J_{w_{k}}J_{w_{k}}^{}{+}(e(w_{k})I)z_{k}{=}(f_{w_{ k}}))\) of size \(N\). Here, we use the damping defined in (5) with \(1\) and ensure it never falls below \(_{0}=10^{-7}\) to avoid numerical instabilities. In practice, we found that small values of \(_{0}\) had little effect on the results (see Figure 4 (Right) of Appendix C.1).

**(GD):** The model's parameters \(w\) are learned using gradient descent with a constant step-size \(\).

**(RF):** Instead of optimizing the entire parameter vector \(w=(v,u)\) of the model \(f_{w}\), we selectively optimize the parameter \(v\) of the linear layer while keeping the weights \(u\) of the hidden layer constant. This procedure corresponds to computing a minimal-norm least squares solution \(v^{RF}\) for a random features (RF) model, where the features are obtained from the output of the hidden layer at initialization. Specifically, the solution \(v^{RF}\) is obtained as

\[v^{RF}=(u)((u)^{}(u))^{}, \]

where \((u)=((u^{}X_{n}))_{1 n N}^{M N}\) is the feature vector computed over the training data, and \(\) is a vector of size \(N\) consisting of the target values \(Y_{n}\). The choice of the un-regularized solution as a baseline is supported by recent findings , demonstrating its good generalization in the over-parameterized regime.

**Stopping criterion.** For both (GD) and (GN), we perform as many iterations as needed so that the final training error is at least below \(10^{-5}\). Additionally, we stop the algorithm whenever the training error drops below \(10^{-7}\) or when a maximum number of iterations of \(K^{GD}=10^{6}\) iterations for (GD) and \(K^{GN}=10^{5}\) iterations for (GN) is performed. For (RF) we solve the linear system exactly using a standard linear solver.

### Performance metrics

In addition to the training and test losses, we introduce two complementary metrics to assess the quality of the hidden features \((u)\) learned by the student model.

**Weighted cosine distance (WCD).** Measuring proximity between the student and teacher's hidden weights \((u_{1},...,u_{M})\) and \((u_{1}^{*},...,u_{M^{*}}^{*})\) requires being able to find a correspondance between each student's parameter \(u_{i}\) and a teacher's one \(u_{j}^{*}\). When using a positively homogeneous non-linearity such as ReLU, only the alignment between vectors is relevant. However, since the student's model is vastly over-parameterized, there could be vanishing weights \(u_{i}=0\) which do not influence on the network's output, while the remaining weights are well aligned with the teacher's weights. We introduce the following weighted distance to account for these specificities:

\[(u,u^{*})=2_{i=1}^{M}p_{i}1-_{1 j M^{*}} ^{}u_{j}^{*}}{\|u_{i}\|u_{j}^{*}},  p_{i}=\|^{2}}{_{k=1}^{M}\|u_{k}\|^{2}}. \]

Equation (9) finds a teacher's parameter \(u_{j}^{*}\) that is most aligned with a given student's parameter \(u_{i}\) and downweights its cosine distance if \(u_{i}\) has a small norm. In practice, we found this measure to be a good indicator for generalization.

**Test loss after linear re-fitting (Test-LRfit).** To evaluate the relevance of the hidden features \((u)\), we train a new linear model \(\) on those features and measure its generalization error. In practice, we freeze the weights \(u\) of the hidden layer and fit the last layer \(v\) using the procedure (8) described above for the random features (RF) baseline. In our over-parameterized setting, this new model should always achieve perfect training loss, but its generalization error strongly depends on how the features were learned.

### Numerical results

**Implicit bias of initialization.** Figure 1(Left) shows that GN and GD generalize more or less well compared to a random features model depending on the variance \(_{0}\) of the weights at initialization. First, in the case of the RF model, changing \(_{0}\) barely affects the final training and test error. This is essentially due to the choice of the non-linearity ReLU which is positively homogeneous. Second, for large \(_{0}\), the final test error of both GN and GD matches that of RF suggesting that the dynamics were running under the _kernel regime/lazy regime_[15; 23] where the final layer's weights are learned without changing the hidden features much. Finally, for small values of \(_{0}\), both GN and GD obtained a better test error than RF which can only be explained by learning suitable hidden features. These results are further confirmed when varying the size of the training set as shown in Appendix C.2. In Appendix C.3, we performed the same experiment using SiLU non-linearity instead of ReLU and observed the same transition between the kernel and feature learning regimes. While prior works such as  analyzed Gauss-Newton in the kernel learning regime, our results indicate that Gauss-Newton can also exhibit a feature learning regime for small values of \(_{0}\).

**Implicit bias of the step size.** Figure 1(Right) shows that increasing the step size in Gauss-Newton results in features that do not generalize well. This is unlike gradient descent where larger step sizes yield better-performing features3. This behavior is first observed in the top figures showing an increasing _weighted cosine distance_ (WCD) between the student's hidden weights and the teacher's weights as the step size increases in the case of Gauss-Newton. On the contrary, the distance decreases with larger step sizes in the case of gradient descent, indicating that the algorithm learns better features. This effect is also confirmed when computing the test loss after refitting the linear layer to the training data exactly while keeping the learned hidden weights fixed (see linear re-fitting in Section 5.2). Several works, such as [3; 36], highlight the importance of taking large step-sizesfor gradient descent, to improve generalization, our results show that Gauss-Newton benefits instead of using smaller step-sizes. As shown in Appendix C.3, this behavior persists when using a different non-linearity, such as SiLU with different values for \(\) (1 and \(10^{6}\)). Interestingly, we observed that GN might underperform GD as the parameter \(\) defining SiLU decreases making SiLU nearly linear and less like ReLU. In all cases, while the final test loss remains large for small step sizes (in both GD and GN), the test loss after linear re-fitting is much lower in the case of Gauss-Newton, indicating that the algorithm was able to learn good features while the last layer remained under-optimized.

Hidden feature learning.Figure 2 (Left) shows the evolution of various metrics, including the WCD, the training, and test errors with the number of iterations for both GN and GD. For each method, results for the best step size and variance at initialization are presented (\((_{0},)=(10^{-3},10^{2})\) for GD and \((_{0},)=(10^{-3},10^{-2})\) for GN). The bottom figure clearly indicates that the early iterations of GN essentially optimize internal features first while barely improving the last layer. This can be seen deduced from the training and test losses which remain almost constant at the beginning of optimization, while the test loss after linear re-fitting steadily decreases with the iterations. The last layer is learned only towards the end of training as indicated by a faster decrease in both train and test objectives. Gradient descent displays a different behavior with test loss before and after linear re-fitting remaining close to each other during optimization.

Better generalization of GN comes at the cost of a slower training.Figure 2 (Right) shows the evolution of the same quantities as in the left figures for \(_{0}=10^{-3}\) as a function of time (in seconds) and for the three values of the step-size \(\) (\(10^{-2}\) (bright colors) \(1\) (medium colors) and \(10^{2}\) (dark colors)). The figure shows that, while the training loss converges much faster for larger step sizes in the case of GN, it does not result in the best generalization error as measured by the test loss after linear re-fitting. Hence, better generalization comes at the cost of a slower convergence of the training loss. Again, the behavior is quite different in the case of gradient descent for which larger step sizes result in both faster optimization of the training objective and better generalization.

## 6 Conclusion

The results illustrate the importance of the initialization and choice of the step size for the generalization performance of GN when optimizing over-parameterized one-hidden layer networks. While

Figure 1: Final values of various metrics vs std of the hidden layerâ€™s weights at initialization \(_{0}\) (left) and step size (right) for a ReLU network. (Left figure) The training size is set to \(N=500\) while \(_{0}\) ranges from \(10^{-3}\) to \(10^{2}\). For both GD and GN, results are reported for the best-performing step-size \(\) selected according to the test loss on a regular logarithmic grid ranging from \(10^{-3}\) to \(10^{3}\). (Right figure) The std of the weights at initialization is set to \(_{0}=10^{-3}\). All models are optimized up to a training error of \(10^{-6}\) or until the maximum number of steps is exceeded, (\(M=5000\), \(N=500\)).

our theoretical analysis shows that GN can reach a global solution of the training objective when starting from a close-to-optimal initialization, this result does not imply anything about the generalization properties of the obtained solution or the quality of the learned features. Our empirical study instead shows that GN may favor feature learning (thus yielding improved generalization) at the cost of a slower optimization of the training objective due to the use of a smaller step size. Providing a theoretical analysis of these phenomena is an interesting direction for future work. Finally, our study shows that the training or test error may not always be a good indicator of the quality of the learned features due to an under-optimized final layer. Instead, a test/validation error _after re-fitting_ the linear layer may be a better indicator for the quality of the learned features which can be of practical use.