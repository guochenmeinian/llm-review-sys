# Distributed Personalized Empirical Risk Minimization

Yuyang Deng

Pennsylvania State University

yzd82@psu.edu &Mohammad Mahdi Kamani

Wyze Labs

mmkamani@alumni.psu.edu &Pouria Mahdavinia

Pennsylvania State University

pxm5426@psu.edu &Mehrdad Mahdavi

Pennsylvania State University

mzm616@psu.edu

###### Abstract

This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. We rigorously analyze the convergence of the proposed algorithm and conduct experiments that corroborate the effectiveness of the proposed paradigm.

## 1 Introduction

Recently _federated learning_ (FL) has emerged as an alternative paradigm to centralized learning to encourage federated model sharing and create a framework to support edge intelligence by shifting model training and inference from data centers to potentially scattered--and perhaps self-interested--systems where data is generated . While undoubtedly being a better paradigm than centralized learning, enabling the widespread adoption of FL necessitates foundational advances in the efficient use of statistical and computational resources to encourage a large pool of individuals or corporations to share their private data and resources. Specifically, due to _heterogeneity_ of data and compute resources among participants, it is necessary, if not imperative, to develop distributed algorithms that are i) cognizant of statistical heterogeneity (_data-awareness_) by designing algorithms that effectively deal with highly heterogeneous data distributions across devices; and ii) confined to learning models that meet available computational resources of participant devices (_system-awareness_).

To mitigate the negative effect of data heterogeneity (non-IIDness), two common approaches are clustering and personalization. The key idea behind the clustering-based methods [2; 3; 4; 5] is to partition the devices into clusters (coalitions) of similar data distributions and then learn a single shared model for all clients within each cluster. While appealing, the partitioning methods are limited to heuristic ideas such as clustering based on the geographical distribution of devices without taking the actual data distributions into account and lack theoretical guarantees or postulate strong assumptions on initial models or data distributions [4; 5]. In personalization-based methods [6; 7; 8; 2; 9; 10; 11], the idea is to learn a distinct _personalized model_ for each device alongside the global model, whichcan be unified as minimizing a bi-level optimization problem . Personalization aims to learn a model that has the generalization capabilities of the global model but can also perform well on the specific data distribution of each participant suffers from a few key limitations. First, as the number of clients grows, while the number of training data increases, the number of parameters to be learned increases which limits to increase in the number of clients beyond a certain point to balance data and overall model complexity tradeoff- a phenomenon known as incidental parameters problem . Moreover, since the knowledge transfer among data sources happens through a single global model, it might lead to suboptimal results. To see this, consider an extreme example, where half of the users have identical data distributions, say \(\), while the other half share a data distribution that is substantially different, say \(-\) (e.g. two distributions with same marginal distribution on features but opposite labeling functions). In this case, the global model obtained by naively aggregating local models (e.g., fixed mixture weights) converges to a solution that suffers from low test accuracy on all local distributions which makes it preferable to learn a model for each client solely based on its local data or carefully chosen subset of data sources.

Focusing on system heterogeneity, most existing works require learning models of identical architecture to be deployed across the clients and server (model-homogeneity) [14; 15], and mostly focus on reducing number  or size [16; 17; 18] of communications or sampling handling chaotic availability of clients [19; 20]. The requirement of the same model makes it infeasible to train large models due to _system heterogeneity_ where client devices have drastically different computational resources. A few recent studies aim to overcome this issue either by leveraging knowledge distillation methods [21; 22; 23; 24] or partial training (PT) strategies via model subsampling (either static [25; 26], random , or rolling ). However, KD-based methods require having access to a public representative dataset of all local datasets at server and ignore data heterogeneity in the distillation stage to a large extent. The focus of PT training methods is mostly on learning a single server model using heterogeneous resources of devices and does not aim at deploying a model onto each client after the global server model is trained (which is left as a future direction in ). The aforementioned issues lead to a fundamental question: _"What is the best strategy to learn from heterogeneous data sources to achieve optimal accuracy w.r.t. each data source, without imposing stringent constraints on computational resources shared by participating devices?"_.

We answer this question affirmatively, by proposing a new _data&system-aware_ paradigm dubbed Personalized Empirical Risk Minimization (PERM), to facilitate learning from massively fragmented private data under resource constraints. Motivated by generalization bounds in multiple source domain adaptation [29; 30; 31; 32], in PERM we aim to learn a distinct model for each client by _personalizing the aggregation_ of empirical losses of different data sources which enables each client _to learn who to learn with_ using an effective method to empirically estimate the statistical discrepancy between their associated data distributions. We argue that PERM entails optimal statistical accuracy for all local distributions, thus overcoming the data heterogeneity issue. PERM can also be employed in other learning settings with multiple heterogeneous sources of data such as domain adaptation and multi-task learning to entail optimal statistical accuracy. While PERM overcomes the data heterogeneity issue, the number of optimization problems (i.e., distinct personalized ERMs) to be solved scales linearly with the number of data sources. To simultaneously optimize all objectives in a scalable and computationally efficient manner, we propose a novel idea that replaces the standard _model averaging_ in distributed learning with _model shuffling_ and establish its convergence rate. This also allows us to learn distinct model architectures (e.g., neural networks with different number of parameters) for different clients, thus confining to underlying memory and compute resources of individual clients, and overcoming the system heterogeneity issue. This addresses an open question in  where only a single global model can be trained in a model-heterogeneous setting, while PERM allows deploying distinct models for different clients. We empirically evaluate the performance of PERM, which corroborates the statistical benefits of PERM in comparison to existing methods.

## 2 Personalized Empirical Risk Minimization

In this section, we formally state the problem and introduce PERM as an ideal paradigm for learning from heterogeneous data sources. We assume there are \(N\) distributed devices where each holds a distinct data shard \(_{i}=\{(_{i,j},y_{i,j})\}_{j=1}^{n_{i}}\) with \(n_{i}\) training samples that are realized by a local source distribution \(_{i}\) over instance space \(=\). The data distributions across the devices are not independently and identically distributed (non-IID or _heterogeneous_), i.e.,

[MISSING_PAGE_FAIL:3]

statistics known as integral probability metrics (IPMs) , can be estimated from finite data by replacing the expected losses with their empirical counterparts (i.e. \(_{i}\) with \(}_{i}\)).

From GEN, it can be observed that a mismatch between pairs of distributions limits the benefits of ERM on all distributions. Indeed, the generalization risk w.r.t. \(_{i}\) will significantly increase when the distribution divergence terms \(_{}(_{j},_{i})\) are large. It leads to an ideal sample complexity \(1/\) where \(n=n_{1}+n_{2}++n_{N}\) is the total number of samples, which could have been obtained in the IID setting with \((j)=1/N\) when the divergence is small as the pairwise discrepancies disappear. Also, we note that even if the global model achieves a small training error over the union of all data (e.g., over parametrized setting) and can entail a good generalization error with respect to _average distribution_, the divergence term still remains which illustrates the poor performance of the global model on all local distributions \(_{i},i=1,2,,N\). This implies that even personalization of the global model as in BERM can not entail a good generalization on all local distributions as there is no effective transfer of positive knowledge among data sources in the presence of high data heterogeneity among local distributions (similar impossibility results even under seemingly generous assumptions on how distributions relate have been made in multisource domain adaptation as well ).

Interestingly the bound suggests that seeking optimal accuracy on _all_ local distributions requires choosing a distinct mixing of local losses for each client \(i\) that minimizes the right-hand side of GEN. This indicates that in an ideal setting (i.e. _all-for-all_), we can achieve the best accuracy for each local distribution \(_{i}\) by _personalizing_ the WERM, i.e., (i) first estimating \(_{i},i=1,2,,N\) for each client individually, then (ii) solving a variant of WERM for each client with obtained mixing parameters:

\[_{h_{i}}_{j=1}^{N}_{i}(j)}_{j}(h) i=1,2,,N.\] (PERM)

By doing this each device achieves the optimal local generalization error by **learning who to learn with** based on the number of samples at each source and the mismatch between its data distribution with other clients. We also note that compared to WERM and BERM, in PERM since we solve a different aggregated empirical loss for each client, we can pick a different model space/model architecture \(_{i}\) for each client to meet its available computational resources.

While this two-stage method is guaranteed to entail optimal test accuracy for all local distributions \(_{i}\), however, making it scalable requires overcoming two issues. First, estimating the statistical discrepancies between each pair of data sources (i.e., \(_{i},i=1,,N\)) is a computing burden as it requires solving \(O(N^{2})\) difference of (non)-convex functions in a distributed manner and requires enough samples form each source to entail good accuracy on estimating pairwise discrepancies . Second, we need to solve \(N\) variants of the optimization problem in PERM, possibly each with a different model space, which is infeasible when the number of devices is huge (e.g., cross-device federated learning). In the next section, we propose a simple yet effective idea to overcome these issues in a computationally efficient manner.

## 3 PERM at Scale via Model Shuffling

In this section, we propose a method to efficiently estimate the empirical discrepancies among data sources followed by a model shuffling idea to simultaneously solve \(N\) versions of PERM to learn a personalized model for each client. We first start by proposing a two-stage algorithm: estimating mixing parameters followed by model shuffling. Then, we propose a single loop unified algorithm that enjoys the same computation and communication overhead as BERM (twice communication of FedAvg). For ease of exposition, we discuss the proposed algorithms by assuming all the clients share the same model architecture and later on discuss the generalization to heterogeneous model spaces. Specifically, we assume that the model space \(\) is a parameterized by a convex set \(^{d}\) and use \(f_{i}():=}_{i}()=_{( ,y)_{i}}(;(,y))\) to denote the empirical loss at \(i\)th data shard.

### Warmup: a two-stage algorithm

We start by proposing a two-stage method for solving \(N\) variants of PERM in parallel. In the first stage, we propose an efficient method to learn the mixing parameters for all clients. Then, in stage two, we propose a model shuffling method to solve all personalized empirical losses in parallel.

**Stage 1: Mixing parameters estimation.** In the first stage we aim to efficiently estimate the pairwise discrepancy among local distributions to construct mixing parameters \(_{i},i=1,2, N\). Fromgeneralization bound GEN and Definition 1, a direct solution to estimate \(_{i}\) is to solve the following convex-nonconcave minimax problem for each client:

\[_{i}^{*}=_{_{N}}_{j=1}^{N} (j)_{}|f_{i}()-f_{j}()|+_ {j=1}^{N}(j)^{2}/n_{j}\] (1)

where we estimate the true risks in pairwise discrepancy terms with their empirical counterparts and drop the complexity term as it becomes identical for all sources by fixing the hypothesis space \(\) and bounding it with a computable distribution-independent quantity such as VC dimension , or it can be controlled by choice of \(\) or through data-dependent regularization. However, solving the above minimax problem itself is already challenging: the inner maximization loop is a nonconcave (or difference of convex) problem, so most of the existing minimax algorithms will fail on this problem. To our best knowledge, the only provable deterministic algorithm is , and it is hard to generalize it to stochastic and distributed fashion. Moreover, since we have \(N\) clients, we need to solve \(N\) variants of (1), which makes designing a scalable algorithm even harder.

To overcome aforementioned issues, we make two relaxations to estimate the per client mixing parameters. First, we optimize an upper bound of pairwise empirical discrepancies \(_{}|f_{i}()-f_{j}()|\) in terms of gradient dissimilarity between local objectives \(\| f_{i}()- f_{j}()\|\), which quantifies how different the local empirical losses are and widely used in analysis of learning from heterogeneous losses as in FL . Second, given that the discrepancy measure based on the supremum could be excessively pessimistic in real-world scenarios, and drawing inspiration from the concept of average drift at the optimal point as a right metric to measure the effect of data heterogeneity in federated learning , we propose to measure discrepancy at the optimal solution obtained by solving WERM, i.e., \(^{*}:=_{}(1/N)_{i=1}^{N}f_{i}()\). By doing this, the problem reduces to a simple minimization for each client, given the optimal global solution. These two relaxations lead to solving the following tractable optimization problem to decide the per-client mixing parameters:

\[_{_{N}}g_{i}(^{*},):= _{j=1}^{N}(j)\| f_{i}(^{*})- f_{j}(^{* })\|^{2}+_{j=1}^{N}(j)^{2}/n_{j}\] (2)

where we added a regularization parameter \(\) and used the squared of gradient dissimilarity for computational convenience. Thus, obtaining all \(N\) mixing parameters requires solving a single ERM to obtain optimal global solution and \(N\) variants of (2). To get the optimal solution in a communication-reduced manner, we adapt the Local SGD algorithm  (or FedAvg ) and find the optimal solution in intermittent communication setting  where the clients work in parallel and are allowed to make \(K\) stochastic updates between two communication rounds for \(R\) consecutive rounds. The detailed steps are given in Algorithm A1 in Appendix B for completeness. After obtaining the global model \(^{R}\) we optimize over \(\) in \(g_{i}(^{},)\) using \(T_{}\) iterations of GD to get \(}_{i}\). Actually, we will show that as long as \(^{R}\) converge to \(^{*}\), \(}_{i},i=1,,N\) converges to solution of (2) very fast. Our proof idea is based on the following Lipschitzness observation:

\[\|_{g_{i}}^{*}(^{R})-_{g_{i}}^{*}(^{*}) \|^{2} 4L^{2}_{g}^{2}_{j=1}^{N}(2\| f_{i}( ^{*})- f_{j}(^{*})\|^{2}+4L^{2}\|^{R}-^{*}\|^{2})\|^{R}-^{*}\|^{2}\]

where \(_{g_{i}}^{*}():=_{_{N}}g_{i}( {w},)\) and \(_{g}:=n_{}/(2)\) is the condition number of \(g_{i}(,)\) where \(n_{}=_{i[N]}n_{i}\). The Lipschitz constant mainly depends on _gradient dissimilarity at optimum_. As \(^{R}\) tends to \(^{*}\), the \(_{g_{i}}^{*}()\) becomes more Lipschitz continuous, i.e., the coefficient in front of \(\|^{R}-^{*}\|^{2}\) getting smaller, thus leading to more accurate mixing parameters.

To establish the convergence, we make the following standard assumptions.

**Assumption 1** (Smoothness and strong convexity).: _We assume \(f_{i}()\)'s are L-smooth and \(\)-strongly convex, i.e.,_

\[,:\| f_{i}()- f_{i}()\| L\|-\|.\]

\[,:f_{i}() f_{i}()+ f_{i}(),-+\|-\|^{2}\]

We denote the condition number by \(=L/\).

**Assumption 2** (Bounded variance).: _The variance of stochastic gradients computed at each local function is bounded, i.e., \( i[N],,[\| f_{i}(; )- f_{i}()\|^{2}]^{2}\)._

**Assumption 3** (Bounded domain).: _The domain \(^{d}\) is a bounded convex set, with diameter \(D\) under \(_{2}\) metric, i.e., \(,^{},\|-^{}\| D\)._

**Definition 2** (Gradient dissimilarity).: _We define the following quantities to measure the gradient dissimilarity among local functions:_

\[_{i,j}():=\| f_{i}()- f_{j}()\| ^{2}\,,_{i}():=_{j=1}^{N}_{i,j}(),\] \[:=_{}_{i[N]}\| f_{i}( )-(1/N)_{j=1}^{N} f_{j}()\|^{2}.\]

The following theorem gives the convergence rate of estimated discrepancies to optimal counterparts.

**Theorem 1**.: _Under Assumptions 1-3, if we run Algorithm A1 on \(F():=_{j=1}^{N}f_{j}()\) with \(=()\) for \(R\) rounds with synchronization gap \(K\), for \(_{g}=1/( n_{})\), it holds that_

\[\|_{i}^{R}-_{i}^{*}\|^{2}( (-}}{_{g}})+_{g}^{2}_{i}(^{*})L^{2}(}{RK}+}{^{2}R^{ 2}}+}{^{2}NRK})) i[N].\]

An immediate implication of Theorem 1 is that even we solve (2) at \(^{R}\), the algorithm will eventually converge to optimal solution of (2) at \(^{*}\). The core technique in the proof, as we mentioned, is to show that for a parameter within a small region centered at \(^{*}\), the function \(_{g_{i}}^{*}()\) becomes'more Lipschitz'. The rigorous characterization of this property is captured by Lemma 3 in appendix.

**Stage 2: Scalable personalized optimization with model shuffling.** After obtaining the per client mixing parameters, in the second stage we aim at solving \(N\) different personalized variants of PERM denoted by \((}_{1},),(}_{2},), (}_{N},)\) to learn local models where

\[_{}(}_{i},):= _{j=1}^{N}_{i}(j)f_{j}().\] (3)

Here we devise an iterative algorithm based on distributed SGD with periodic averaging (a.k.a. Local SGD ) to solve these \(N\) optimization problems in parallel with _no extra overhead_. The idea is to replace the model averaging in vanilla distributed (Local) SGD with _model shuffling_. Specifically, as shown in Algorithm 1 the algorithm proceeds for \(R\) epochs where each epoch runs for \(N\) communication rounds. At the beginning of each epoch \(r\) the server generates a random permutation \(_{r}\) over \(N\) clients. At each communication round \(j\) within the epoch, the server sends the model of client \(i\) to client \(i_{j}=(i+j) N\) in the permutation \(_{r}\) along with \(_{i}(i_{j})\). After receiving a model from the server, the client updates the received model for \(K\) local steps and returns it back to the server. As it can be seen, the updates of each loss \((}_{i},),i=1,2,,N\) during an epoch is equivalent to sequentially processing individual losses in (3) which can be considered as permutation-based SGD but with the different that each component now is updated for \(K\) steps. By _interleaving the permutations_, we are able to simultaneously optimize all \(N\) objectives. We note that the computation and communication complexity of the proposed algorithm is the same as Local SGD with two differences: the model averaging is replaced with model shuffling, and the algorithms run over a permutation of devices. The convergence rate of Local SGD is well-established in literature , but here we establish the convergence of permutation-based variant which is interesting by its own.

**Assumption 4** (Bounded Gradient).: _The variance of stochastic gradients computed at each local function is bounded, i.e., \( i[N],_{}\| f_{i}()\| G\)._

We note that the Assumption 4 can be realized since we work with a bounded domain \(\).

**Theorem 2**.: _Let Assumptions 1-4 hold. Assume \(_{i}^{*}\) is the solution of (2). Then if we run Algorithm 1 on the \(}_{i}\) obtained from Algorithm A1, then Algorithm 1 with \(=()}{ R})\) will output the solution \(}_{i}\), \( i[N]\), such that with probability at least \(1-p\), the following statement holds:_

\[[(_{i}^{*},}_{i})-(_{i}^{*},^{*}(_{i}^{*}))] (L}{NKR^{2}})+}{^{2}R}+(+N}{^{4}R^{2}})LG^{2}N(1/p)\] \[+_{}^{2}L((- }}{_{g}})+_{g}^{2}_{i}(^{*})L^{2}( }{^{2}R^{2}}+}{^{2}NRK}) ),\]_where \(=,_{g}=}{2}\) and \(_{}=G}{}\), and the expectation is taken over randomness of Algorithm 1. That is, to guarantee \([(_{i}^{*},}_{i})-(_{i}^{*}, _{i}^{*})]\), we choose \(R=O(\{}{},^{2} _{g}^{2}L^{}_{i}(^{*})D^{2}}{}\})\) and \(T_{}=O(_{g}(^{2}}{} ))\)._

The above theorem shows that even though we run the optimization on \((}_{i},)\), our obtained model \(}_{i}\) will still converge to the optimal solution of \((_{i}^{*},)\). The convergence rate is contributed from two parts: convergence of \(}_{i}\) (Algorithm 1) and convergence of personalized model \(}_{i}\) (Algorithm 1). Notice that, for the convergence rate of \(}_{i}\), we roughly recover the optimal rate of shuffling SGD , which is \(O(1/R^{2})\). However, we suffer from a \(O(^{2}/R)\) term since each client runs vanilla SGD on their local data (the SGD-Update procedure in Algorithm 1). One medication for this variance term could be deploying variance reduction or shuffling data locally at each client before applying SGD. We notice that there is a recent work  also considering the client-level shuffling idea, but our work differs from it in two aspects: 1) they work with local SGD type algorithm and the shuffling idea is employed for model averaging within a subset of clients, while in our algorithm, during each local update period, each client runs shuffling SGD directly on other's model 2) from a theoretical perspective, we are mostly interested in investigating whether the algorithm can converge to the true optimal solution of \((_{i}^{*},)\) if we only optimize on a surrogate function \((}_{i},)\).

``` Input: Clients \(1,...,N\), Number of Local Steps \(K\), Number of Epoch \(R\), mixing parameter \(}_{1},...,}_{N}\) Epoch for\(r=0,...,R-1\)do  Server generates permutation \(_{r}:[N][N]\). parallel for\(i=1,...,N\)do  Client \(i\) sets initial model \(_{i}^{r,0}=_{i}^{r}\). for\(j=1,...,N\)do  Set indices \(i_{j}=_{r}((i+j) N)\).  Server sends \(_{i}^{r,j}\) to Client \(i_{j}\). \(_{i}^{r,j+1}=(_{i}^{r,j},,i_{j},K,}_{i})\).  Client \(i\) does projection: \(_{i}^{r+1}=_{}(_{i}^{r,N})\). Output:\(}_{i}=_{}(_{i}^{R}-(1/L)_{} (}_{i},_{i}^{R})), i[N]\).  SGD-Update(\(,,j,K,\))  Initialize \(^{0}=\) for\(t=0,...,K-1\)do \(^{t}=^{t-1}-(j)N f_{j}(^{t-1};_{j}^{t-1})\) Output:\(^{K}\) ```

**Algorithm 1**Shuffling Local SGD

One drawback of Algorithm 1 is that we have to wait for Algorithm 1 to finish and output \(}_{i}\), so that we can proceed with Algorithm 1. However, if we are not satisfied with the precision of \(}_{i}\), we may not have a chance to go back to refine it. Hence in the next subsection, we propose to interleave Algorithm 1 and Algorithm 1, and introduce a single-loop variant of PERM which will jointly optimize mixture weights and learn personalized models in an interleaving fashion.

### A unified single loop algorithm

We now turn to introducing a single-stage algorithm that jointly optimizes \(_{i}\)s and \(_{i}\)s as depicted in Algorithm 2 by intertwining the two stages in Algorithm 1 and Algorithm 1 in a single unified method. The idea is to learn the global model, which is used to estimate mixing parameters, concurrent to personalized models. At each communication round, the clients compute gradients on the global model, on their data, after the server collects these gradients does a step mini-batch SGD update on the global model, and then updates the mixing parameters. Then we proceed to update the personalized models similar to Algorithm 1. We note that, unlike the two-stage method where the mixing parameters are computed at the final global model, here the mixing parameters are updated adaptively based on intermediate global models.

**Theorem 3**.: _Let Assumptions 1 to 4 to be satisfied. Assume \(_{i}^{*}\) is the solution of (2). Then if we run Algorithm 2 with \(=()}{ R})\) and \(=()}{ R})\), it will output the solution \(}_{i}\)\( i[N]\), such that with probability at least \(1-p\), the following statement holds:_

\[[(_{i}^{*},}_{i})-(_{i}^{*},_{i}^{*})]\] \[+_{}^{2}L(_{g}^{2}L^ {2}_{i}(^{*})DG}{R}+R^{2}(-}}{ _{g}})+_{g}^{2}_{i}(^{* })^{2}}{^{2}M}),\]

_where \(=,_{g}=}{2},_{}=G}{}\) and the expectation is taken over the randomness of stochastic samples in Algorithm 2. That is, to guarantee \([(_{i}^{*},}_{i})-(_{i}^{*},_{i}^{*})]\), we choose \(M=O(_{g}^{2}_{g}^{2}_{}^{2}_{i}(^{*})^{2}}{^{2}})\), \(R=O(\{}{},^{2} _{g}^{2}L^{3}_{i}(^{*})DG}{}\})\) and \(T_{}=O(_{g}(}{}))\)._

Compared to Theorem 2, we achieve a slightly worse rate, since we need a large mini-batch when we update global model \(\). However, the advantages of the single-loop algorithm are two-fold. First, as we mentioned in the previous subsection, we have the freedom to optimize \(}_{i}\) to arbitrary accuracy, while in double loop algorithm (Algorithm A1 + Algorithm 1), once we get \(}_{i}\), we do not have the chance to further refine it. Second, in practice, a single-loop algorithm is often easier to implement and can make better use of caches by operating on data sequentially, leading to improved performance, especially on modern processors with complex memory hierarchies.

``` Input: Clients \(1,...,N\), Number of Local Steps \(K\), Number of Epoch \(R\), Initial mixing parameter \(_{1}^{0}=...,_{N}^{0}=}=[1/N,...,1/N]\). Epoch for\(r=0,...,R-1\)do  Server generates permutation \(_{r}:[N][N]\). parallel for Client \(i=1,...,N\)do  Client \(i\) sets initial model \(_{i}^{r,0}=_{i}^{r}\). for\(j=1,...,N\)do  Set indices \(i_{j}=_{r}((i+j) N)\).  Server sends \(_{i}^{r,j}\) to client \(i_{j}\). \(_{i}^{r,j+1}=(_{i}^{r,j},,i_{j},K,_{i}^{r})\). // Personalized model update  Client \(i\) does projection: \(_{i}^{r+1}=_{}(_{i}^{r})\). \(^{r+1}=_{}(^{r}-_{i=1}^ {N}_{j=1}^{M} f_{i}(^{r},_{i,j}^{r}))\) // Global model update  Compute \(_{i}^{r+1}\) by running \(T_{}\) steps GD on \(g_{i}(^{r+1},)\) // \(\) update Output:\(}_{i}=_{}(_{i}^{R}-(1/L)_{} (_{i}^{R},_{i}^{R}))\), \(}_{i}=_{i}^{R}, i[N]\).  SGD-Update(\(,,j,K,\))  Initialize \(_{j}^{0}=\) for\(t=0,...,K-1\)do \(^{t}=^{t-1}-(j)N f_{j}(_{j}^{t-1};_{j}^{t -1})\) Output:\(^{K}\) ```

**Algorithm 2**Single Loop PERM

### Extension to heterogeneous model setting

In the homogeneous model setting, we assumed a shared model space \(\) for clients and the server. However, in real-world FL applications, devices have diverse resources and can only train models that match their capacities. We demonstrate that the PERM paradigm can be extended to support learning in model-heterogeneous settings, where different models with varying capacities are used by the server and clients. Focusing on learning the global optimal model to estimate pairwise statistical discrepancies, we note that by utilizing partial training methods , where at each communication round a sub-model with a size proportional to resources of each client is sampled from the server's global model (extracted either random, static, or rolling) and is transmitted to be updated locally. Upon receiving updated sub-models, the server can simply aggregate (average) heterogeneous sub-model updates sent from the clients to update the global model. We can consider the complexity of models used by clients when estimating mixing parameters by solving a modified version of (2) as:

\[_{j=1}^{N}(j)(_{j})/n_{j}}+_{j=1}^{N} (j)\| f_{i}(_{i}^{*})- f_{j}(_{j} ^{*})\|^{2}+_{j=1}^{N}(j)^{2}/n_{j},\]

where we simply upper bounded the Rademacher complexity w.r.t. each data source in (GEN) with VC dimension . Here \(_{i}\) is the masking operator to extract a sub-model of the global model to compute local gradients at client \(i\) based on its available resources. By doing so, we can adjust mixing parameters based on the complexity of underlying models, as different sub-models of the global model (i.e., \(_{i}^{*}\) versus \(_{j}^{*}\)) are used to compute drift between pair of gradients at the optimal solution. With regards to training personalized models with heterogeneous local models, as we solve a distinct aggregated empirical loss for each client by interleaving permutations and shuffling models, we can utilize different model spaces \(_{i},i=1,,N\) for different clients that meet their available resources with aforementioned partial training strategies.

## 4 Experimental Results

In this section we benchmark the effectiveness of PERM on synthetic data with 50 clients, where it notably outshone other renowned methods as evident in Figure 1. Our experiments concluded with the CIFAR10 dataset, employing a 2-layer convolutional neural network, where PERM, despite a warm-up phase, demonstrated unmatched convergence performance (Figure 2). Additional experiments are reported in the appendix. Across all datasets, the PERM algorithm consistently showcased its robustness and unmatched efficiency in the realm of personalized federated learning.

**Experiment on synthetic data.** To demonstrate the superior effectiveness of our proposed single-loop PERM algorithm compared to other existing personalization methods, we conducted an experiment using synthetic data generated according to the following specifications. We consider a scenario with a total of \(N\) clients, where we draw samples from the distribution \((_{1},_{i})\) for half of the clients, denoted by \(i[1,]\), and from \((_{2},_{i})\) for the remaining clients, denoted by \(i(,N]\). Following the approach outlined in , we adopt a uniform variance for all samples, with \(_{k,k}=k^{-1.2}\). Subsequently, we generate a labeling model using the distribution \((_{w},_{w})\).

Given a data sample \(^{d}\), the labels are generated as follows: clients \(1,...,\) assign labels based on \(y=(^{})\), while clients \(+1,...,N\) assign labels based on \(y=(-^{})\). For this specific experiment, we set \(_{1}=0.2\), \(_{2}=-0.2\), and \(_{w}=0.1\). The data dimension is \(d=60\), and there are 2 classes in the output. We have a total of 50 clients, each generating \(500\) samples following the aforementioned guidelines. We train a logistic regression model on each client's data.

To demonstrate the superiority of our PERM algorithm, we conducted a performance comparison against other prominent personalized approaches, including the fined-tuned model of FedAvg  (referred to as localized FedAvg), perfedAg , and pFedME . The results in Figure 1 highlight PERM's efficient learning of personalized models for individual clients. In contrast, competing methods relying on globally trained models struggle to match PERM's effectiveness in highly heterogeneous scenarios, as seen in personalized accuracy and loss. This showcases PERM's exceptional ability to leverage relevant client learning.

Figure 1: Comparative analysis of personalization methods, including our single-loop PERM algorithm, localized FedAvg, perfedAvg, and pFedME, with synthetic data. The disparity in personalized accuracy and loss highlights PERM’s capability to leverage relevant client correlations.

**Experiment on CIFAR10 dataset.** We extend our experimentation to the CIFAR10 dataset using a 2-layer convolutional neural network. During this test, 50 clients participate, each limited to data from just 2 classes, resulting in a pronounced heterogeneous data distribution. We benchmark our algorithm against PerFedAvg, PFedMe, and the localized FedAvg. As illustrated in Figure 2, PERM demonstrates superior convergence performance compared to other personalized strategies. It's noteworthy that PERM's initial personalized validation is significantly lower than that of approaches like PerFedAvg and PFedMe. This discrepancy stems from our choice to implement 10 communication rounds as a warm-up phase before initiating personalization, whereas other models embark on personalization right from the outset.

**Computational overhead.** In demonstrating the computational efficiency of the proposed PERM algorithm, we present a comparison of wall-clock time of completing one round of communication of PERM and other methods. Each method undertakes 20 local steps along with their distinct computations for personalization. As depicted in Figure 3, the PERM (single loop) algorithm's runtime is compared against personalization methods such as PerFedAvg, FedAvg, and pFedMe. Remarkably, PERM maintains a notably minimal computational overhead. The run-time is slightly worse due to overhead of estimating mixing parameters.

## 5 Discussion & Conclusion

This paper introduces a new _data&system-aware_ paradigm for learning from multiple heterogeneous data sources to achieve optimal statistical accuracy across all data distributions without imposing stringent constraints on computational resources shared by participating devices. The proposed PERM schema, though simple, provides an efficient solution to enable each client to learn a personalized model by _learning who to learn with_ via personalizing the aggregation of data sources through an efficient empirical statistical discrepancy estimation module. To efficiently solve all aggregated personalized losses, we propose a model shuffling idea to optimize all losses in parallel. PERM can also be employed in other learning settings with multiple sources of data such as domain adaptation and multi-task learning to entail optimal statistical accuracy.

We would like to embark on the scalability of PERM. The compute burden on clients and servers is roughly the same as existing methods thanks to shuffling (except for extra overhead due to estimating mixing parameters which is the same as running FedAvg in a two-stage approach and an extra communication in an interleaved approach). The only hurdle would be the required _memory at server_ to maintain mixing parameters, which scales proportionally to the square of the number of clients, which can be alleviated by clustering devices which we leave as a future work.

Figure 3: Runtime of different algorithms in a limited environment. We compare PERM (single loop), PerFedAvg, FedAvg, and pFedMe. PERM has a minimal overhead over FedAvg and is comparable to other personalization methods.

Figure 2: Comparative analysis of our single-loop PERM algorithm, localized FedAvg, PFedMe, and perFedAg, on CIFAR10 dataset and a 2-layer CNN model. Each client has access to only 2 classes of data. PERM rapidly catches up after 10 rounds of warmup without personalization involved.