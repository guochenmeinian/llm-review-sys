# SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words

Junyi Ao\({}^{1}\), Yuancheng Wang\({}^{1}\), Xiaohai Tian\({}^{2}\), Dekun Chen\({}^{1}\),

**Jun Zhang\({}^{2}\)**, **Lu Lu\({}^{2}\)**, **Yuxuan Wang\({}^{2}\)**, **Haizhou Li\({}^{1}\)**, **Zhizheng Wu\({}^{1}\)\({}^{}\)**

\({}^{1}\)School of Data Science, SRIBD,

The Chinese University of Hong Kong, Shenzhen, Guangdong 518172, China

\({}^{2}\)Bytedance

Equal contributionCorresponding author: wuzhizheng@cuhk.edu.cn

###### Abstract

Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.

## 1 Introduction

Speech contains rich information and plays a crucial role in human-computer interaction [12; 14; 41]. Besides relying on the content information, speech also conveys paralinguistic and environmental information, which can significantly influence conversations. More specifically, the information carried in speech can be categorized into three classes: content information, environmental information and paralinguistic information, as illustrated in Figure 1(a).

The _content_ information refers to the "**choice of words**", representing the explicit meaning and linguistic structure of the speech. _Environmental information_ pertains to "**location of conversation**",capturing the factors such as background noise and situational context that can influence the interpretation of the speech. _Paralinguistic information_, which is further divided into "**who says**" and "**how to say**", includes various non-verbal elements that convey additional meaning. "who says" involves aspects like accent, age, and timber of the speaker, which can affect the perception and understanding of the speech. "how to say" includes prosody, volume, and rhythm, detailing the vocal nuances that contribute to the expressive quality of the speech. Together, all information highlights the multifaceted nature of spoken dialogue, extending beyond mere words to encompass a wide array of information. Figure 1(b) illustrates how environmental and paralinguistic information, such as emotion, accent and age, impact responses.

Large Language Models (LLMs) have shown remarkable capabilities as a universal interface for general-purpose assistance . Recently, LLMs have evolved to understand not only text but also multi-modal inputs, such as speech and image , which broadens the scope of what LLMs can achieve. The capabilities of LLMs with speech input (Speech LLMs) are primarily designed for the perception of speech and analysis of tasks defined by a text instruction prompt. This enables the model not only to recognize content but also to perceive additional information, allowing it to perform various speech-related tasks such as speech recognition and gender classification. However, due to the lack of principles on task definitions and model development, they usually fail to generate appropriate responses directly with speech input. The development of advanced Speech LLMs requires open-source datasets and metrics suitable for model evaluation from every aspect of the rich information carried in speech.

We present _a novel benchmark dataset for multidimensional evaluation of spoken dialogue understanding beyond words, namely SD-Eval_. The dataset is to promote the development of more empathetic and intelligent spoken dialogue systems that can generate appropriate responses based on paralinguistic and environmental information. The ultimate goal of SD-Eval is to create a benchmark dataset for speech-to-speech conversation system development. As an initial step, SD-Eval focuses on speech-to-text dialogue. The initial version of SD-Eval consists of four sub-tasks, each focusing on evaluating responses to input utterances with different emotions, accents, ages, and background sounds. These sub-tasks are constructed from eight public datasets containing real-recorded speeches. More specifically, SD-Eval comprises four subsets: _test-emo_, _test-acc_, _test-age_, and _test-env_ for emotion, accent, age and background sound, respectively. It includes 7,303 utterances, totalling 8.76 hours of speech data.

To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a similar process as SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct an empirical study of evaluation metrics using objective evaluation methods (e.g. BLEU and ROUGE), subjective opinion score and LLM-based metrics for the generated responses.

Figure 1: (a) Information embedded in speech: content, environmental, and paralinguistic information. (b) Examples of spoken dialogue, which illustrate the impact of user emotions, accents, age, and environmental information on the responses.

Related Work

Spoken Conversation Datasets with Paralinguistic LabelParalinguistic information is crucial for comprehending speech and generating responses in spoken dialogues. Many speech emotion datasets are constructed under spoken dialogue scenarios, such as IEMOCAP , SEMAINE , and MELD . However, their primary purpose is to identify emotions in speech. Consequently, the dialogue data from these datasets is relatively less suited for training a spoken dialogue system.

Some recent studies build novel datasets such as E-chat200  and StyleTalk , which are designed for spoken dialogue with a focus on emotional information. Nevertheless, the text and speech in these datasets are generated using ChatGPT and text-to-speech (TTS) models. Our dataset is based on a mixture of real-recording and synthesized speech and focuses on multiple aspects, including accents, emotions, ages, and background sounds.

Spoken Question AnsweringThe spoken question answering (SQA) task requires the system to answer questions from speech. The past approaches [60; 54] mainly divided this task into two parts through a cascaded model: automatic speech recognition (ASR) and text question answering. Recently, some systems [67; 40] aim to achieve end-to-end spoken question answering.

Datasets in the field of SQA include Spoken SQuAD , SCQA , HeySQuAD , OpenSAQA , e.g. These datasets lack annotations of paralanguage information. StyleTalk  provides annotations of speaking styles. Our work focuses more on paralinguistic and environmental information to simulate more realistic dialogue scenarios.

Evaluation Metrics for Open-Ended Generation TasksAssessing the quality of text produced by language models or human authors for open-ended generation tasks has always been a difficult task. Traditional evaluation metrics such as BLEU  and ROUGE  are based on the n-grams to measure the similarity between model outputs and references, while these metrics focus on lexical overlap, which is ineffective for open-ended generation problems. In addition, they show a relatively weak correlation with human judgement . Embedding-based metrics, such as BERTScore , use word or sentence embeddings to measure semantic similarity based on the references.

However, the answers to these tasks are open-ended without standard references, while collecting human preferences can be costly and laborious. Recently, several works [35; 17; 74] try to use LLMs for evaluating the responses of chat assistants, which shows a high correlation with human judgement. In our work, we adapt these LLM-based methods for spoken dialogue generation, with a focus on paralinguistic and environmental information.

## 3 SD-Eval Benchmark Dataset

### Dataset Construction

SD-Eval is divided into four subsets: _test-emo_, _test-acc_, _test-age_, and _test-env_. Each subset focuses on a specific aspect: emotion, accent, age, and environment, respectively. The ultimate aim of SD-Eval is to create a benchmark dataset for the evaluation of speech-to-speech conversation systems. As a preliminary step, SD-Eval concentrates on speech-to-text dialogues. We construct SD-Eval through the following steps.

Data CollectionAs shown in Table 1, we select data from 8 public datasets to construct SD-Eval. For _test-emo_ subset, RAVDESS , MEAD , and JL Corpus  are selected as they contain audios with the same content but different emotions. For _test-env_ subset, we choose real-recording speeches from the LibriSpeech  test-clean subset and add background sounds using audio samples from AudioCaps .

Synthetic Data GenerationFor _test-age_ and _test-env_, a portion of the data is synthesized. For _test-age_, we use an internal zero-shot TTS model, which is trained on Libri-light, to generate speech data from the text in MyST  with adult speakers. For each text, we randomly select a sample from the LibriSpeech test-clean subset  as the prompt to synthesize the data. For _test-env_, we first select audio collections corresponding to seven types of environments from AudioCaps . Then, we mix each speech sample in the subset of LibriSpeech test-clean with audio randomly selected from these collections corresponding to each environmental scene. Simultaneously, we utilize GPT-4-Turbo  to generate dialogue data for these seven scenarios and employ the TTS model to generate speech, forming part of the _test-env_ subset.

Label NormalizationDue to the varying number of label categories across different datasets, we first normalize the labels of all datasets. Specifically, labels of _test-acc_ include _nine_ widely used and representative accents: England, Scottish, Irish, Welsh, Northern Irish, American, Canadian, Australian, and New Zealand. For the _test-emo_ subset, we firstly utilize Ekman's emotion model  as the labels, which contain neutral, surprise, sad, happy, angry, disgust, and fear, which are the basic emotions. We choose Ekman's emotion model because it is widely used in speech emotion recognition task , ensuring that each category of emotion is well-represented and encompasses a substantial amount of data.

We then further exclude utterances with neutral and surprise emotions. Neutral implies that the speech does not convey positive or negative feelings, making the response primarily content-dependent. However, our focus is on examining the impact of speech emotion on text responses. Similarly, surprise can be associated with different sentiments, depending on the context . Therefore, we excluded data related to these two emotions. As a result, the _test-emo_ subset includes five types of emotions: sad, happy, angry, disgust, and fear.

For the _test-env_ subset, we select seven representative scenarios in daily life to serve as background sounds, as illustrated in Table 1. For the _test-age_ subset, we focus on evaluating whether the model could generate comprehensible responses appropriate to different age groups. Consequently, the labels are divided into two categories: child and adult.

Data FilteringWe filter the test data from three aspects. Firstly, some utterances of the four subsets are identified with notable ambiguity, potentially due to a lack of contextual information. To address this, we design a prompt and use GPT-4-turbo  for automatic filtering, as illustrated in Figure 2. Following this initial filtering, three human annotators are then required to evaluate the remaining utterances further using the same criteria as the prompt. Secondly, it is observed that some utterances within the _test-env_ contain incorrect background sounds, possibly due to the multi-class labelling of the AudioCaps . These utterances are subsequently identified and filtered by human annotators. Finally, we exclude utterances of _test-emo_ subset where both the sentiment of transcript and emotion are positive or negative, aiming to enhance the impact of emotions on responses. For this purpose, a pre-trained sentiment classification model 3 is employed to predict the sentiments of utterances.

Punctuation RestorationTraditional metrics, such as BLEU , require references as input, so we try to use ChatGPT  to generate responses for each utterance. However, the transcripts of three datasets, MEAD , LibriSpeech  and UK-Ireland dataset , do not contain punctuation,

  
**Type** & **\# Hours** & **\# Uts** & **Constructed From** & **Labels** \\   Emotion & & & & \\ _(test-emo)_ & 1.11 & 1,289 & &  RAVDESS , MEAD , \\ JL Corpus  \\  & Sad, Angry, Fear, Disgust, Happy \\   Accent \\ _(test-acc)_ \\  } &  &  &  & England, Scottish, Northern Irish, \\  & & & &  English, Scottish, Northern Irish, \\ Welsh, Irish, American, Canadian, \\ Australian, New Zealand \\  \\   Environment \\ _(test-env)_ \\  } &  &  &  LibriSpeech , AudioCaps , \\ Synthesised Speech \\  } & Driving, Children’s Voice, Sea Beach, \\  & & & &  Raining or Thundering, Bells, \\ Sports Center, Bus or Subway \\  \\   Age \\ _(test-age)_ \\  } &  &  & MyST , Synthesised Speech & Adult, Child \\ 
**Summary** & 8.76 & 7,303 & - & - \\   

Table 1: Statistics of the SD-Eval benchmark dataset, which includes four types of paralinguistic and environmental information.

which may degrade the quality of generated responses. To address this issue, we employ a punctuation restoration model 4 to add punctuation for transcripts of these two datasets.

Response GenerationFinally, we use GPT-4o  to generate five diverse responses for each utterance in SD-Eval by considering the content and emotion, accent, age or background sounds of speech signals. For instance, the prompt used to generate responses for utterances related to emotion is presented in Figure 3. All the prompts used to generate responses are included in the Appendix.

### Dataset Statistics

The statistics of SD-Eval are presented in Table 1. The SD-Eval dataset comprises a total of 7,303 sentences and 8.76 hours of speech data. It contains three types of paralinguistic information (i.e. emotion, accent, age), and the environment type contains seven categories of environmental sounds. The pie charts in Figure 4 illustrate the data distribution for each category within each test set.

Figure 4: Pie charts illustrating the data distribution for each category within each subset.

Figure 3: The prompt for generating responses of utterances related to emotion.

Figure 2: The prompt for filtering utterances.

## 4 Benchmark Experiments

### Training Set

To assess the SD-Eval benchmark dataset, we construct a training dataset from eleven open datasets for training models. We follow a procedure similar to SD-Eval, with the following two exceptions. Firstly, we simplify the data filtering process by removing sentences with inadequate and ambiguous labels. Secondly, we generated only one response for each sentence. The details, including data statistics and prompts, are introduced in the Appendix.

### Models

We implement several baselines trained using the proposed training set, aiming to evaluate their capability of comprehending the content of the speech, as well as recognizing emotions, accents, age, or background sounds. The implementations are detailed as follows.

Cascade LLMAs shown in Figure 5(a), the Cascade LLM consists of an automatic speech recognition (ASR) model to recognize the content, followed by an LLM to generate a response based on the text input. The ASR model is Whisper large-v3 , which is trained with a large amount of weakly supervised data for speech recognition and translation. The LLM is the InternLM2 chat model with 7 billion parameters (InternLM2-chat-7b) . During training, the LLM is frozen, while we add a trainable LoRA adaptor  to facilitate model finetuning. We use this model as a baseline to evaluate responses if only knowing the content of the speech.

VS-LLMTo understand and perceive content as well as paralinguistic and environmental information directly from speech, we design an end-to-end model named Vanilla Speech LLM (VS-LLM). As shown in 5(b), it consists of a speech encoder, an LLM and an additional adaptor to connect the speech encoder and LLM. The encoder of Whisper large-v3  is used as the speech encoder, followed by a trainable adaptor to further down-sample the speech representation from the speech encoder. The adaptor comprises two linear layers, where the first linear layer is succeeded by a GELU activation function , while the second one is followed by a two-dimensional average pooling operation for down-sampling. Similar to Cascade LLM, a frozen InternLM2-chat-7b with a trainable LoRA adaptor is employed as the LLM.

LLM (Upper Bound)To assess the system's upper bound upon the speech transcript, we also provide paralinguistic or environmental information as an additional label to the frozen InternLM2-chat-7b with LoRA for model finetuning. The input format is a concatenation of ground-truth transcripts and labels. For instance, _"How are you?<Emotion:Happy>"_ is the input of an utterance. The transcript of this utterance is "How are you?" The emotion contained in this utterance is happy.

Figure 5: (a) Model Structure of Cascade LLM, which generates text response directly based on the ASR output. (b) Model structure of Vanilla Speech LLM (VS-LLM). The LLM takes speech representation as input, which is generated from a speech encoder and adaptor.

Besides the above self-implemented models, we further assess the performance of the off-the-shelf speech LLM models on SD-Eval. All text instruction prompts for open-sourced models can be find in Appendix A.6.

Qwen-AudioQwen-Audio  is designed for handling a wide range of audio types and tasks. The model scales up pre-training across more than 30 tasks, including speech, natural sounds, and music, in multiple languages, achieving strong performance without task-specific fine-tuning. Since Owen-Audio requires a text instruction prompt for each input to define the task, we add a text instruction prompt to let the model generate a text response based on the speech.

Qwen2-AudioQwen2-Audio  is a work based on Qwen-Audio. It is trained on larger-scale data and employs DPO  for optimising models to follow human preferences. Unlike Qwen-Audio, Qwen2-Audio has two modes: voice chat (Qwen2-Audio-VC) and audio analysis (Qwen2-Audio-AA). In Audio Analysis mode, it can analyze various audio types and identify command segments within the audio. In Voice Chat mode, it acts like a conversational agent, allowing users to engage in dialogue through audio. We also utilize a text instruction prompt for audio analysis mode during evaluation.

SALMONNSALMONN  is a multi-modal large language model designed to process and understand general auditory inputs, including speech, audio events, and music. SALMONN integrates a pre-trained text-based large language model (LLM) with dual auditory encoders -- Whisper  for speech and BEATs  for non-speech audio. We use a predefined text prompt for evaluation.

### Evaluation Metrics

Objective EvaluationWe propose a reference-free metric using the LLMs for response evaluation. Specifically, we design different prompts for the evaluations of each subset. By the prompts, the LLM judge must consider (a) the response's naturalness, coherence, engagingness and groundedness. (b) Whether the response is appropriate and fully considers the emotion, accent, age or background sound of input speech. The LLM judge is then asked to directly assign a score, such as 5 on a 1 - 10 scale, to a single answer. For comparison, we further include the results of n-gram-based metrics, such as ROUGE-L , BLEU-4  and METEOR , and embedding-based metrics, such as BERTScore 5.

Subjective EvaluationIn addition, we conduct a human evaluation on 200 randomly selected utterances from the four subsets, with each subset contributing 50 utterances. Each sample is assessed by at least three human evaluators, who are instructed to rate the generated responses. Each utterance has three samples, corresponding to three utterance-response pairs generated by Cascade LLM, VS-LLM, and LLM (Upper Bound), respectively. We ensure that each valid sample is evaluated by at least three human annotators. Consequently, each subset has no fewer than 120 valid samples.

### Experimental Setup

Configuration for Model TrainingAll models implemented by ourselves are built using xtuner . We optimize the model with AdamW  with a learning rate of \(2 10^{-4}\). The models are finetuned on 16 A100 GPUs, each with a batch size of 16, for two epochs. For the LoRA adaptor of the InternLM2-chat-7b model, we use a rank of 512 and \(\) of 256. In contrast, for the encoder of the Whisper large-v3 model, the rank is set to 64 and \(\) to 16.

Inference Setting of LLM JudgeIn addition to GPT-4o, we employ Yi-1.5-34B-Chat 6, Qwen2-57B-A14B-Instruct 7, and gamma-2-27B-it 8, as LLM judges. To speed up inference and save memory, we utilize lama.cpp 9 for LLM inference. Specifically, we use a quantizedversion Q6_K of these two models to achieve a balance between efficiency and performance. This configuration allows using CPUs or only one A100 GPU for evaluation.

### Main Results

Table 2 shows the main results of all models on SD-Eval. Firstly, across all four test sets, VS-LLM outperformed Cascade LLM on all metrics. This indicates that using speech as a direct input allows VS-LLM to implicitly learn paralinguistic and environmental information. Secondly, the performance of VS-LLM is inferior to that of LLM (Upper Bound). The main reason may be that VS-LLM implicitly acquires content as well as paralinguistic and environmental information directly from speech, whereas the LLM (Upper Bound) utilizes ground truth transcripts and labels. This indicates that the way to process the input data is important for model performance. A detailed ablation study regarding the input data will be introduced later.

As for the open-sourced models, while SALMONN  achieves better or comparable performance compared to Qwen2-Audio-AA  and Qwen-Audio , Qwen2-Audio-VC  performs much better than other open-sourced models as voice chat mode is more suitable for conversation. However, the performance of open-sourced models in SD-Eval is not very impressive. This suggests a current lack of well-defined tasks and datasets in this area. To improve the model's performance, future directions may include using larger-scale and more diverse data [20; 27; 69], as well as employing methods such as disentanglement [26; 73] to enhance the model's understanding of speech information.

### Analysis

Ablation Study of Input DataWe further conduct an ablation study in terms of the input data, as shown in Table 3. We investigate several models with different inputs. Among them, Model 1,

    &  &  &  &  &  &  \\  & & & & & & **Yi-1.5** & &  &  &  \\    \\  SALMONN  & 2.48 & 16.57 & 18.97 & 86.20 & 4.98 & 3.35 & 2.32 & 2.61 & - \\ Qwen-Audio  & 3.93 & 19.02 & 16.82 & 86.59 & 4.19 & 2.35 & 2.02 & 2.24 & - \\ Qwen2-Audio-AA  & 3.01 & 16.82 & 17.51 & 86.17 & 4.75 & 2.52 & 2.21 & 2.33 & - \\ Qwen2-Audio-VC  & 2.21 & 14.57 & 22.08 & 85.41 & 5.88 & 3.83 & 2.93 & 3.25 & - \\ Cascade LLM & 4.66 & 21.98 & 21.70 & 87.93 & 5.67 & 3.86 & 2.35 & 4.47 & 5.05 \\ VS-LLM & 8.29 & 25.52 & 27.23 & 89.48 & 6.40 & 4.56 & 4.03 & 5.30 & 6.31 \\ LLM (Upper Bound) & **12.35** & **26.08** & **28.27** & **89.77** & **7.03** & **5.82** & **6.46** & **6.74** & **7.29** \\    \\  SALMONN & 7.50 & 22.22 & 21.23 & 87.53 & 5.27 & 6.16 & 3.16 & 2.93 & - \\ Qwen-Audio  & 4.52 & 17.15 & 17.78 & 85.59 & 3.48 & 3.45 & 1.86 & 1.72 & - \\ Qwen2-Audio-AA  & 7.26 & 21.80 & 19.68 & 87.68 & 5.04 & 6.13 & 3.01 & 2.54 & - \\ Qwen2-Audio-VC  & 3.47 & 17.46 & 23.77 & 86.26 & 5.96 & 6.20 & 3.94 & 4.37 & - \\ Cascade LLM & 14.51 & 30.53 & 34.13 & 89.66 & 7.23 & 7.32 & 5.65 & 6.62 & 6.71 \\ VS-LLM & 17.98 & 33.06 & 37.65 & 90.08 & 7.82 & 7.65 & 6.59 & 7.85 & 7.95 \\ LLM (Upper Bound) & **18.35** & **33.48** & **38.27** & **90.23** & **7.85** & **7.75** & **6.73** & **8.02** & **8.30** \\    \\  SALMONN & 10.03 & 24.95 & 23.55 & 88.10 & 5.41 & 4.66 & 3.14 & 3.35 & - \\ Qwen-Audio  & 7.28 & 23.09 & 21.80 & 86.72 & 4.43 & 3.98 & 2.25 & 2.50 & - \\ Qwen2-Audio-AA  & 6.81 & 22.72 & 20.51 & 87.47 & 5.19 & 4.58 & 3.01 & 3.14 & - \\ Qwen2-Audio-VC  & 5.64 & 18.90 & 28.23 & 86.70 & 7.03 & 5.92 & 4.48 & 5.06 & - \\ Cascade LLM & 15.36 & 31.96 & 31.99 & 90.08 & 7.22 & 7.16 & 6.46 & 4.47 & 6.51 \\ VS-LLM & 17.22 & 34.17 & 33.78 & 90.63 & 7.74 & 7.39 & 7.25 & 7.95 & 7.11 \\ LLM (Upper Bound) & **18.78** & **35.62** & **36.01** & **91.00** & **7.82** & **7.54** & **7.40** & **8.25** & **7.44** \\    \\  SALMONN & 2.87 & 16.53 & 21.37 & 86.71 & 4.70 & 5.00 & 3.40 & 3.56 & - \\ Qwen-Audio  & 2.37 & 16.83 & 17.50 & 85.81 & 3.77 & 1.86 & 2.16 & 2.14 & - \\ Qwen2-Audio-AA  & 2.97 & 16.32 & 19.84 & 86.50 & 4.52 & 5.02 & 3.49 & 3.50 & - \\ Qwen2-Audio-VC  & 2.06 & 12.35 & 23.40 & 85.17 & 6.30 & 6.21 & 4.85 & 5.30 & - \\ Cascade LLM & 5.44 & 21.75 & 26.41 & 88.22 & 6.03 & 5.84 & 5.31 & 5.66 & 6.62 \\ VS-LLM & 9.42 & 25.85 & 28.27 & 89.23 & 6.14 & 5.88 & 5.10 & 5.82 & 7.11 \\ LLM (Upper Bound) & **11.72** & **27.95** & **31.50** & **89.73** & **7.14** & **7.14** & **6.25** & **7.40** & **8.13** \\   

Table 2: Main results of five models on four subsets of SD-Eval. \(\) The scores from human evaluations are calculated based on randomly sampled data as described in Section 4.3.

which belongs to Speech LLM and is without any text input, refers to VS-LLM. Model 4 utilizing transcripts from the ASR model as input is Cascade LLM. Additionally, Model 8 uses ground truth transcripts and labels, which is LLM (Upper Bound). For ASR and speech emotion recognition (SER), the models are Whisper large-v3  and emotion2vec 10.

Firstly, we examine the effect of content quality. We observe that the performance of models utilizing ASR-generated transcripts (Model 5 and Model 7) is inferior across all metrics compared to their counterparts (Model 6 and Model 8) that use ground-truth transcripts. Next, we examine the effect of emotion label quality. For the LLM-based system, models using emotion labels from the SER model (Model 5 and Model 6) perform worse across all metrics compared to those using ground-truth labels (Model 7 and Model 8). Model 4, which is trained without emotion labels, performs the worst. A similar trend is observed in the models of Speech LLM, where Model 2 obtained emotion labels from the SER model outperforms Model 1, while Model 3, trained with ground truth labels, achieves the best performance among all three models. This corroborates our hypothesis in the section 4.5.

Correlations between Objective Metrics and Human EvaluationFinally, we investigate the correlations between scores of objective metrics and human evaluation, as shown in Table 4. Following the configuration of GPTScore , we utilize dataset-level Spearman and Kendall-Tau correlation metrics. Firstly, the experimental results indicate that all LLM judges exhibit a significantly higher correlation with human evaluations compared to other metrics. Secondly, as an LLM judge, GPT-4o consistently achieves the best or second-best performance in most cases. These findings strongly validate the effectiveness of LLM judges as evaluation metrics.

    &  &  &  &  &  \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   BLEU-4 & 0.211 & 0.170 & 0.197 & 0.156 & 0.316 & 0.232 & 0.169 & 0.136 & 0.208 & 0.160 \\ ROUGE-L & 0.203 & 0.140 & 0.263 & 0.192 & 0.318 & 0.215 & 0.216 & 0.144 & 0.258 & 0.176 \\ METEOR & 0.249 & 0.168 & 0.272 & 0.194 & 0.350 & 0.242 & 0.315 & 0.214 & 0.336 & 0.229 \\ BERTScore & 0.378 & 0.269 & 0.252 & 0.184 & 0.321 & 0.216 & 0.286 & 0.199 & 0.291 & 0.199 \\  Yi-1.5 & 0.641 & 0.493 & 0.435 & 0.345 & 0.356 & 0.289 & 0.558 & 0.441 & 0.492 & 0.381 \\ Qwen2 & 0.618 & 0.456 & 0.198 & 0.161 & 0.347 & 0.278 & 0.449 & 0.352 & 0.474 & 0.362 \\ Gemma & 0.639 & 0.493 & 0.375 & 0.304 & 0.448 & 0.356 & 0.563 & 0.439 & 0.492 & 0.380 \\ GPT-4o & **0.731** & **0.568** & **0.659** & **0.541** & **0.474** & **0.356** & **0.577** & **0.459** & **0.613** & **0.468** \\   

Table 4: Spearman (\(\)) and Kendall-Tau (\(\)) correlations between human evaluation and different metrics.

    &  &  & **Emotion** &  &  &  &  &  &  &  &  \\  
1 & N/A & N/A & 8.29 & 25.52 & 27.23 & 89.48 & 6.40 & 4.56 & 4.03 & 5.30 \\
2 & Speech LLM & N/A & SER & 10.37 & 27.29 & 28.59 & 89.81 & 6.76 & 5.26 & 4.37 & 6.11 \\
3 & & N/A & GT & 10.21 & 27.22 & 28.45 & 89.85 & 6.96 & 5.45 & 4.45 & 6.41 \\ 
4 & & ASR & N/A & 4.66 & 21.98 & 21.70 & 87.93 & 5.67 & 3.86 & 2.35 & 4.47 \\
5 & & ASR & SER & 11.37 & 26.03 & 27.66 & 89.66 & 6.74 & 5.35 & 5.62 & 6.13 \\
6 & LLM & GT & SER & 11.85 & 26.05 & 27.78 & 89.75 & 6.85 & 5.53 & 6.07 & 6.38 \\
7 & & ASR & GT & 11.85 & 26.03 & 28.19 & 89.68 & 6.96 & 5.64 & 6.04 & 6.47 \\
8 & & GT & GT & **12.35** & **26.08** & **28.27** & **89.77** & **7.03** & **5.82** & **6.46** & **6.74** \\   

Table 3: Ablation study on _test-emo_ subset. The model types include LLM (text input only) and Speech LLM (text and speech inputs). “Trans” refers to the method used to obtain the transcripts. Options include “ASR” (generated by an ASR model) and “GT” (ground-truth transcript). “Emotion Label” indicates the source of the speech emotion label for the utterance, either “SER” (produced by a speech emotion recognition model) or “GT” (ground-truth label). “N/A” means the input is not used for the model.

Conclusion

In this paper, we introduce SD-Eval, a benchmark dataset designed for the multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval includes 7,303 utterances amounting to 8.76 hours of speech data, aggregated from eight public datasets, and focuses on paralinguistic and environmental information across four perspectives: emotion, accent, age, and background sound. The dataset aims to advance the creation of more empathetic and intelligent spoken dialogue systems capable of generating appropriate responses by considering paralinguistic and environmental information. Our comprehensive evaluation demonstrates that models conditioned with paralinguistic or environmental information outperform their counterparts in both objective evaluation and subjective evaluation. Furthermore, our experiments indicate that LLM-based metrics have a higher correlation with human evaluation compared to traditional metrics.

## 6 Limitations and Future Work

The limitations and future work for SD-Eval are as follows: Firstly, SD-Eval accommodates only speech-to-text dialogues, limiting the evaluation of system responses at the text level. Secondly, SD-Eval currently supports the evaluation of single-turn dialogues only, limiting its application to more complex, multi-turn interactions. Finally, SD-Eval includes four sub-tasks that focus on speech elements such as emotion, accent, age, and environmental information. However, it does not yet account for other aspects, such as the gender of the speaker. Addressing these aspects constitutes our future work, with the ultimate goal of developing a benchmark dataset capable of multidimensional evaluation for multi-turn speech-to-speech dialogues.