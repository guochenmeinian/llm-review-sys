# CEIL: Generalized Contextual Imitation Learning

Jinxin Liu\({}^{1,2}\) Li He\({}^{1}\)1  Yachen Kang\({}^{1,2}\) Zifeng Zhuang\({}^{1,2}\)

**Donglin Wang\({}^{1,4}\) Huazhe Xu\({}^{3,5,6}\)**

\({}^{1}\)Westlake University \({}^{2}\)Zhejiang University \({}^{3}\)Tsinghua University \({}^{4}\)Westlake Institute for Advanced Study \({}^{5}\)Shanghai Qi Zhi Institute \({}^{6}\)Shanghai AI Lab

Equal contributions. Corresponding author: Donglin Wang <wangdonglin@westlake.edu.cn>

###### Abstract

In this paper, we present **C**ont**E**xtual **I**mitation **L**earning (CEIL), a general and broadly applicable algorithm for imitation learning (IL). Inspired by the formulation of hindsight information matching, we derive CEIL by explicitly learning a hindsight embedding function together with a contextual policy using the hindsight embeddings. To achieve the expert matching objective for IL, we advocate for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings including: 1) learning from observations (LfO), 2) offline IL, 3) cross-domain IL (mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to prior state-of-the-art baselines, we show that CEIL is more sample-efficient in most online IL tasks and achieves better or competitive performances in offline tasks.

## 1 Introduction

Imitation learning (IL) allows agents to learn from expert demonstrations. Initially developed with a supervised learning paradigm [58; 63], IL can be extended and reformulated with a general expert matching objective, which aims to generate policies that produce trajectories with low distributional distances to expert demonstrations . This formulation allows IL to be extended to various new settings: 1) online IL where interactions with the environment are allowed, 2) learning from observations (LfO) where expert actions are absent, 3) offline IL where agents learn from limited expert data and a fixed dataset of sub-optimal and reward-free experience, 4) cross-domain IL where the expert demonstrations come from another domain (_i.e._, environment) that has different transition dynamics, and 5) one-shot IL which expects to recover the expert behaviors when only one expert trajectory is observed for a new IL task.

Modern IL algorithms introduce various designs or mathematical principles to cater to the expert matching objective in a specific scenario. For example, the LfO setting requires particular considerations regarding the absent expert actions, _e.g._, learning an inverse dynamics function [5; 65]. Besides, out-of-distribution issues in offline IL require specialized modifications to the learning objective, such as introducing additional policy/value regularization [32; 72]. However, such a methodology, designing an individual formulation for each IL setting, makes it difficult to scale up a specific IL algorithm to more complex tasks beyond its original IL setting, _e.g._, online IL methods often suffer severe performance degradation in offline IL settings. Furthermore, realistic IL tasks are often not subject to a particular IL setting but consist of a mixture of them. For example, we may have accessto both demonstrations and observation-only data in offline robot tasks; however, it could require significant effort to adapt several specialized methods to leverage such mixed/hybrid data. Hence, a problem naturally arises: _How can we accommodate various design requirements of different IL settings with a general and practically ready-to-deploy IL formulation?_

Hindsight information matching, a task-relabeling paradigm in reinforcement learning (RL), views control tasks as analogous to a general sequence modeling problem, with the goal to produce a sequence of actions that induces high returns . Its generality and simplicity enable it to be extended to both online and offline settings [17; 42]. In its original RL context, an agent directly uses known extrinsic rewards to bias the hindsight information towards task-related behaviors. However, when we attempt to retain its generality in IL tasks, how to bias the hindsight towards expert behaviors remains a significant barrier as the extrinsic rewards are missing.

To design a general IL formulation and tackle the above problems, we propose **C**ont**E**xtual **I**mitation **L**earning (CEIL), which readily incorporates the hindsight information matching principle within a bi-level expert matching objective. In the inner-level optimization, we explicitly learn a hindsight embedding function to deal with the challenges of unknown rewards. In the outer-level optimization, we perform IL expert matching via inferring an optimal embedding (_i.e._, hindsight embedding biasing), replacing the naive reward biasing in hindsight. Intuitively, we find that such a bi-level objective results in a spectrum of expert matching objectives from the embedding space to the trajectory space. To shed light on the applicability and generality of CEIL, we instantiate CEIL to various IL settings, including online/offline IL, LfD/LfO, cross-domain IL, and one-shot IL settings.

In summary, this paper makes the following contributions: 1) We propose a bi-level expert matching objective ContExtual Imitation Learning (CEIL), inheriting the spirit of hindsight information matching, which decouples the learning policy into a contextual policy and an optimal embedding. 2) CEIL exhibits high generality and adaptability and can be instantiated over a range of IL tasks. 3) Empirically, we conduct extensive empirical analyses showing that CEIL is more sample-efficient in online IL and achieves better or competitive results in offline IL tasks.

## 2 Related Work

Recent advances in decision-making have led to rapid progress in IL settings (Table 1), from typical learning from demonstrations (LfD) to learning from observations (LfO) [7; 9; 35; 54; 62; 66], from online IL to offline IL [11; 15; 33; 53; 73], and from single-domain IL to cross-domain IL [34; 48; 56; 68]. Targeting a specific IL setting, individual works have shown their impressive

    &  &  &  &  \\   & LfD & LfO & Online & Offline & & \\  S-on-LfD [9; 13; 21; 30; 38; 52; 57; 61; 77] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ S-on-LfO [7; 54; 65; 66; 75] & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ S-off-LfD [19; 32; 33; 39; 55; 70; 72; 73] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ S-off-LfO  & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ C-on-LfD [18; 69; 79] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ C-on-LfO [20; 25; 26; 48; 59; 60] & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ C-off-LfD  & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ \\ C-off-LfO [56; 68] & ✓ & ✓ & ✗ & ✓ & ✗ & ✗ \\  S-on/off-LfO  & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Online one-shot [14; 16; 40] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ Offline one-shot [24; 71] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ 
**CEIL (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: A coarse summary of IL methods demonstrating 1) different expert data modalities they can handle (learning from _demonstrations_ or _observations_), 2) disparate task settings they consider (learning from _online_ environment interactions or pre-collected _offline_ static dataset), 3) the specific _cross-domain_ setting they assume (the transition dynamics between the learning environment and that of the expert behaviors are different), and 4) the unique _one-shot_ merit they desire (the learned policy is capable of one-shot transfer to new imitation tasks). We highlight that our contextual imitation learning (CEIL) method can naturally be applied to all the above IL settings.

ability to solve the exact IL setting. However, it is hard to retrain their performance in new unprepared IL settings. In light of this, it is tempting to consider how we can design a general and broadly applicable IL method. Indeed, a number of prior works have studied part of the above IL settings, such as offline LFO , cross-domain LFO [48; 60], and cross-domain offline IL . While such works demonstrate the feasibility of tackling multiple IL settings, they still rely on standard online/offline RL algorithmic advances to improve performance [25; 32; 44; 47; 50; 51; 55; 72; 76]. Our objective diverges from these works, as we strive to minimize the reliance on the RL pipeline by replacing it with a simple supervision objective, thus avoiding the dependence on the choice of RL algorithms.

Our approach to IL is most closely related to prior hindsight information-matching methods [2; 8; 24; 49], both learning a contextual policy and using a contextual variable to guide policy improvement. However, these prior methods typically require additional mechanisms to work well, such as extrinsic rewards in online RL [4; 42; 64] or a handcrafted target return in offline RL [12; 17]. Our method does not require explicit handling of these components. By explicitly learning an embedding space for both expert and suboptimal behaviors, we can bias the contextual policy with an inferred optimal embedding (contextual variable), thus avoiding the need for explicit reward biasing in prior works. Our method also differs from most prior offline transformer-based RL/IL algorithms that explicitly model a long sequence of transitions [10; 12; 31; 36; 43; 71]. We find that simple fully-connected networks can also elicit useful embeddings and guide expert behaviors when conditioned on a well-calibrated embedding. In the context of the recently proposed prompt-tuning paradigm in large language tasks or multi-modal tasks [27; 45; 74], our method can be interpreted as a combination of IL and prompting-tuning, with the main motivation that we tune the prompt (the optimal contextual variable) with an expert matching objective in IL settings.

## 3 Background

Before discussing our method, we briefly introduce the background for IL, including learning from demonstrations (LfD), learning from observations (LfO), online IL, offline IL, and cross-domain settings in Section 3.1, and introduce the hindsight information matching in Section 3.2.

### Imitation Learning

Consider a control task formulated as a discrete-time Markov decision process (MDP)2\(=\{,,,r,,p_{0}\}\), where \(\) is the state (observation) space, \(\) is the action space, \(:\) is the transition dynamics function, \(r:\) is the reward function, \(\) is the discount factor, and \(p_{0}\) is the distribution of initial states. The goal in a reinforcement learning (RL) control task is to learn a policy \(_{}(|)\) maximizing the expected sum of discounted rewards \(_{_{}()}[_{t=0}^{T-1}^{t}r (_{t},_{t})]\), where \(:=\{_{0},_{0},,_{T-1}, _{T-1}\}\) denotes the trajectory and the generated trajectory distribution \(_{}()=p_{0}(_{0})_{}( _{0}|_{0})_{t=1}^{T-1}_{}(_{t}|_{ t})(_{t}|_{t-1},_{t-1})\).

In IL, the ground truth reward function (_i.e._, \(r\) in \(\)) is not observed. Instead, we have access to a set of demonstrations (or observations) \(\{|_{E}()\}\) that are collected by an unknown expert policy \(_{E}(|)\). The goal of IL tasks is to recover a policy that matches the corresponding expert policy. From the mathematical perspective, IL achieves the plain expert matching objective by minimizing the divergence of trajectory distributions between the learner and the expert:

\[_{_{}}\;D(_{}(),_{E}()),\] (1)

where \(D\) is a distance measure. Meanwhile, we emphasize that the given expert data \(\{|_{E}()\}\) may not contain the corresponding expert actions. Thus, in this work, we consider two IL cases where the given expert data \(\) consists of a set of state-action demonstrations \(\{(_{t},_{t},_{t+1})\}\) (learning from demonstrations, LfD), as well as a set of state-only transitions \(\{(_{t},_{t+1})\}\) (learning from observations, LfO) _When it is clear from context, we abuse notation \(_{E}()\) to denote both demonstrations in LfD and observations in LfO for simplicity._

Besides, we can also divide IL settings into two orthogonal categories: online IL and offline IL. In online IL, the learning policy \(_{}\) can interact with the environment and generate online trajectories \(_{}()\). In offline IL, the agent cannot interact with the environment but has access to an offlinestatic dataset \(\{|_{}()\}\), collected by some unknown (sub-optimal) behavior policies \(_{}\). By leveraging the offline data \(\{_{}()\}\{_{E}()\}\) without any interactions with the environment, the goal of offline IL is to learn a policy recovering the expert behaviors (demonstrations or observations) generated by \(_{E}\). Note that, in contrast to the typical offline RL problem , the offline data \(\{_{}()\}\) in offline IL does not contains any reward signal.

**Cross-domain IL.** Beyond the above two IL branches (online/offline and LfD/LfO), we can also divide IL into: 1) single-domain IL and 2) cross-domain IL, where 1) the single-domain IL assumes that the expert behaviors are collected in the same MDP in which the learning policy is to be learned, and 2) the cross-domain IL studies how to imitate expert behaviors when discrepancies exist between the expert and the learning MDPs (_e.g._, differing in their transition dynamics or morphologies).

### Hindsight Information Matching

In typical goal-conditioned RL problems, hindsight experience replay (HER)  proposes to leverage the rich repository of the failed experiences by replacing the desired (true) goals of training trajectories with the achieved goals of the failed experiences:

\[(_{};,_{})( _{};f_{}(_{}),_{}),\]

where the learner \((_{};,)\) could be any RL methods, \(_{}_{}(_{}|)\) denotes the trajectory generated by a goal-conditioned policy \(_{}(_{t}|_{t},)\), and \(f_{}\) denotes a pre-defined (hindsight information extraction) function, _e.g._, returning the last state in trajectory \(_{}\).

HER can also be applied to the (single-goal) reward-driven online/offline RL tasks, setting the return (sum of the discounted rewards) of a trajectory as an implicit goal for the corresponding trajectory. Thus, _we can reformulate the (single-goal) reward-driven RL task_, learning policy \(_{}(_{t}|_{t})\) that maximize the return, _as a multi-goal RL task_, learning a return-conditioned policy \(_{}(_{t}|_{t},)\) that maximize the following log-likelihood:

\[_{_{}}\ _{()}[_{ }(|,f_{}())],\] (2)

where \(f_{}()\) denotes the return of trajectory \(\). At test, we can then condition the contextual policy \(_{}(|,)\) on a desired target return. In offline RL, the empirical distribution \(()\) in Equation 2 can be naturally set as the offline data distribution; in online RL, \(()\) can be set as the replay/experience buffer, and will be updated and biased towards trajectories that have high expected returns.

Intuitively, biasing the sampling distribution (\(()\) towards higher returns) leads to _an implicit policy improvement operation_. However, such an operator is non-trivial to obtain in the IL problem, where we do not have access to a pre-defined function \(f_{}()\) to bias the learning policy towards recovering the given expert data \(\{_{E}()\}\) (demonstrations or observations).

## 4 Method

In this section, we will formulate IL as a bi-level optimization problem, which will allow us to derive our method, contextual imitation learning (CEIL). Instead of attempting to train the learning policy \(_{}(|)\) with the plain expert matching objective (Equation 1), our approach introduces an additional contextual variable \(\) for a contextual IL policy \(_{}(|,)\). The main idea of CEIL is to learn a contextual policy \(_{}(|,)\) and an optimal contextual variable \(^{*}\) such that the given expert data (demonstrations in LfD or observations in LfO) can be recovered by the learned \(^{*}\)-conditioned policy \(_{}(|,^{*})\). We begin by describing the overall framework of CEIL in Section 4.1, and make a connection between CEIL and the plain expert matching objective in Section 4.2, which leads to a practical implementation under various IL settings in Section 4.3.

### Contextual Imitation Learning (CEIL)

Motivated by the hindsight information matching in online/offline RL (Section 3.2), we propose to learn a general hindsight embedding function \(f_{}\), which encodes trajectory \(\) (with window size \(T\)) into a latent variable \(\), \(|| T*||\). Formally, we learn the embedding function \(f_{}\) and a corresponding contextual policy \(_{}(|,)\) by minimizing the trajectory self-consistency loss:

\[_{},f_{}=_{_{},f_{}}\ -_{()}[_{}(|f_{}())]=_{_{ },f_{}}\ -_{()}_{(, )}[_{}(|,f_{}( ))],\] (3)where in the online setting, we sample trajectory \(\) from buffer \(()\), known as the experience replay buffer in online RL; in the offline setting, we sample trajectory \(\) directly from the given offline data.

If we can ensure that the learned contextual policy \(_{}\) and the embedding function \(f_{}\) are accurate on the empirical data \(()\), then we can convert the IL policy optimization objective (in Equation 1) into a bi-level expert matching objective:

\[_{}^{*}}\;D(_{}(|}^{*}),_{E}()),\] (4) \[_{},f_{}=_{_{},f_{}}- _{()}[_{}(|f_{}( ))]-(f_{}),\;\;\;\;}^{*}  f_{}(),\] (5)

where \((f_{})\) is an added regularization over the embedding function (we will elaborate on it later), and \(()\) denotes the support of the trajectory distribution \(\{|()>0\}\). Here \(f_{}\) is employed to map the trajectory space to the latent variable space (\(\)). Intuitively, by optimizing Equation 4, we expect the induced trajectory distribution of the learned \(_{}(}|},}^{*})\) will match that of the expert. However, in the offline IL setting, the contextual policy can not interact with the environment. If we directly optimize the expert matching objective (Equation 4), such an objective can easily exploit generalization errors in the contextual policy model to infer a mistakenly overestimated \(}^{*}\) that achieves low expert-matching loss but does not preserve the trajectory self-consistency (Equation 3). Therefore, we formalize CEIL into a bi-level optimization problem, where, in Equation 5, we explicitly constrain the inferred \(}^{*}\) lies in the (\(f_{}\)-mapped) support of the training trajectory distribution.

At a high level, CEIL decouples the learning policy into two parts: an expressive contextual policy \(_{}(}|},)\) and an optimal contextual variable \(}^{*}\). By comparing CEIL with the plain expert matching objective, \(_{_{}}D(_{}(),_{E}())\), in Equation 1, we highlight two merits: 1) CEIL's expert matching loss (Equation 4) does not account for updating \(_{}\) and is only incentivized to update the low-dimensional latent variable \(}^{*}\), which enjoys efficient parameter learning similar to the prompt tuning in large language models , and 2) we learn \(_{}\) by simply performing supervised regression (Equation 5), which is more stable compared to vanilla inverse-RL/adversarial-IL methods.

### Connection to the Plain Expert Matching Objective

To gain more insight into Equation 4 that captures the quality of IL (the degree of similarity to the expert data), we define \(D(,)\) as the sum of reverse KL and forward KL divergence3, _i.e._, \(D(q,p)=D_{}(q\|p)+D_{}(p\|q)\), and derive an alternative form for Equation 4:

\[_{}^{*}}\;D(_{}(|}^{*}), _{E}())=_{}^{*}}\;( }^{*};}_{E})-(}^{*}; }_{})}_{_{}}-(),_{E}())}_{_{}},\] (6)

where \((};})\) denotes the mutual information (MI) between \(}\) and \(}\), which measures the predictive power of \(}\) on \(}\) (or vice-versa), the latent variables are defined as \(}_{E}:=}_{E}(})\), \(}_{}:=} p(}^{*})_ {}(}|}^{*})\), and \(_{}(})=_{}^{*}}[_{ }(}|}^{*})]\).

Intuitively, the second term \(_{D}\) on RHS of Equation 6 is similar to the plain expert matching objective in Equation 1, except that here we optimize a latent variable \(}^{*}\) over this objective. Regarding the MI terms \(_{}\), we can interpret the maximization over \(_{}\) as an implicit policy improvement, which incentivizes the optimal latent variable \(}^{*}\) for having high predictive power of the expert data \(}_{E}\) and having low predictive power of the non-expert data \(}_{}\).

Further, we can rewrite the MI term (\(_{}\) in Equation 6) in terms of the learned embedding function \(f_{}\), yielding an approximate embedding inference objective \(_{(f_{})}\):

\[_{} =_{_{E}(}^{*},}_{E})}  p(}^{*}|}_{E})-_{_{}( }^{*},}_{})} p(}^{*}| }_{})\] \[_{p(}^{*})_{E}( }_{E})_{}(}_{}|}^{*})}[-\| }^{*}-f_{}(}_{E})\|^{2}+\|}^{*} -f_{}(}_{})\|^{2}]_{(f_{})},\]

where we approximate the logarithmic predictive power of \(}^{*}\) on \(}\) with \(-\|}^{*}-f_{}(})\|^{2}\), by taking advantage of the learned embedding function \(f_{}\) in Equation 5.

By maximizing \(_{(f_{})}\), the learned optimal \(^{*}\) will be induced to converge towards the embeddings of expert data and avoid trivial solutions (as shown in Figure 1). Intuitively, \(_{(f_{})}\) can also be thought of as an instantiation of contrastive loss, which manifests two facets we consider significant in IL: 1) the "anchor" variable4\(^{*}\) is unknown and must be estimated, and 2) it is necessary to ensure that the estimated \(^{*}\) lies in the support set of training distribution, as specified by the support constraints in Equation 5.

In summary, by comparing \(_{(f_{})}\) and \(_{D}\), we can observe that \(_{(f_{})}\) actually encourages expert matching in the embedding space, while \(_{D}\) encourages expert matching in the original trajectory space. In the next section, we will see that such an embedding-level expert matching objective naturally lends itself to cross-domain IL settings.

### Practical Implementation

In this section, we describe how we can convert the bi-level IL problem above (Equations 4 and 5) into a feasible online/offline IL objective and discuss some practical implementation details in LfO, offline IL, cross-domain IL, and one-shot IL settings (see more details5 in Appendix 9.3).

As shown in Algorithm 1 (best viewed in colors), CEIL alternates between solving the bi-level problem with respect to the support constraint (Line 3 for online IL or Line 7 for offline IL), the trajectory self-consistency loss (Line 5), and the optimal embedding inference (Line 6).

To satisfy the support constraint in Equation 5, for online IL (Line 3), we directly roll out the \(z^{*}\)-conditioned policy \(_{}(|,^{*})\) in the environment; for offline IL (Line 7), we minimize a simple regularization6 over \(^{*}\), bearing a close resemblance to the one used in TD3+BC :

\[(^{*})=(\|^{*}-f_{}(_{E})\|^{2},\|^{*}-f_{}(_{D})\|^{2}), \ \ _{E}:=_{E}(),\ _{D}:= (),\] (7)

where we apply a stop-gradient operation to \(f_{}\). To ensure the optimal embedding inference (\(_{^{*}}_{(f_{})}-_{D}\)) retaining the flexibility of seeking \(^{*}\) across different instances of \(f_{}\), we jointly update the optimal embedding \(^{*}\) and the embedding function \(f_{}\) with

\[_{^{*},f_{}}_{(f_{})}-_{D},\] (8)

where we use \(\) to control the weight on \(_{D}\).

**LfO.** In the LfO setting, as expert actions are missing, we apply our expert matching objective only over the observations. Note that even though expert data contains no actions in LfO, we can still leverage a large number of suboptimal actions presented in online/offline \(()\). Thus, we can learn the contextual policy \(_{}(|,)\) using the buffer data in online IL or the offline data in offline IL, much owing to the fact that we do not directly use the plain expert matching objective to update \(_{}\).

**Cross-domain IL.** Cross-domain IL considers the case in which the expert's and learning agent's MDPs are different. Due to the domain shift, the plain idea of \(_{D}\) may not be a sufficient proxy for the expert matching objective, as there may never exist a trajectory (in the learning MDP) that matches the given expert data. Thus, we can set (the weight of \(_{D}\)) \(\) to \(0\).

Further, to make embedding function \(f_{}\) useful for guiding the expert matching in latent space (_i.e._, \(_{(f_{})}\)), we encourage \(f_{}\) to capture the task-relevant embeddings and ignore the domain-specific factors. To do so, we generate a set of pseudo-random transitions \(\{_{E^{}}\}\) by independently sampling trajectories from expert data \(\{_{E}(_{E})\}\) and adding random noise over these sampled trajectories, _i.e._, \(_{E^{}}=_{E}+\). Then, we couple each trajectory \(\) in \(\{_{E}\}\{_{E^{}}\}\) with a label \(\{,\}\), indicating whether it is noised, and then generate a new set of \(\{(,)\}\), where \(\{_{E}\}\{_{E^{}}\}\) and \(\{,\}\). Thus, we can set the regularization \((f_{})\) in Equation 5 to be:

\[(f_{})=(f_{}();).\] (9)

Intuitively, maximizing \((f_{})\) encourages embeddings to be domain-agnostic and task-relevant: \(f_{}(_{E})\) has high predictive power over expert data (\(=0\)) and low that over noised data (\(=\)).

**One-shot IL.** Benefiting from the separate design of the contextual policy learning and the optimal embedding inference, CEIL also enjoys another advantage -- one-shot generalization to new IL tasks. For new IL tasks, given the corresponding expert data \(_{}\), we can use the learned embedding function \(f_{}\) to generate a corresponding latent embedding \(_{}\). When conditioning on such an embedding, we can directly roll out \(_{}(|,_{})\) to recover the one-shot expert behavior.

## 5 Experiments

In this section, we conduct experiments across a variety of IL problem domains: single/cross-domain IL, online/offline IL, and LfD/LfO IL settings. By arranging and combining these IL domains, we obtain 8 IL tasks in all: _S-on-LfD_, _S-on-Lf0_, _S-off-Lf0_, _S-off-Lf0_, _C-on-LfD_, _C-on-Lf0_, _C-off-Lf0_, and _C-off-Lf0_, where S/C denotes single/cross-domain IL, on/off denotes online/offline IL, and LfD/LfO denote learning from demonstrations/observations respectively. Moreover, we also verify the scalability of CEIL on the challenging one-shot IL setting.

Our experiments are conducted in four popular MuJoCo environments: Hopper-v2 (Hop.), HalfCheetah-v2 (Hal.), Walker2d-v2 (Wal.), and Ant-v2 (Ant.). In the single-domain IL setting, we train a SAC policy in each environment and use the learned expert policy to collect expert trajectories (demonstrations/observations). To investigate the cross-domain IL setting, we assume the two domains (learning MDP and the expert-data collecting MDP) have the same state space and action space, while they have different transition dynamics. To achieve this, we modify the torso length of the MuJoCo agents (see details in Appendix 9.2). Then, for each modified agent, we train a separate expert policy and collect expert trajectories. For the offline IL setting, we directly take the reward-free D4RL  as the offline dataset, replacing the online rollout experience in the online IL setting.

### Evaluation Results

To demonstrate the versatility of the CEIL idea, we collect 20 expert trajectories (demonstrations in LfD or state-only observations in LfO) for each environment and compare CEIL to GAIL , AIRL , SQIL , IQ-Learn , ValueDICE , GAIfO , ORIL , DemoDICE , and SMODICE  (see their implementation details in Appendix 9.4). Note that these baseline methods cannot be applied to all the IL task settings (_S/C-on/off-LfD/LfO_), thus we only provide experimental comparisons with compatible baselines in each IL setting.

**Online IL.** In Figure 2, we provide the return (cumulative rewards) curves of our method and baselines on 4 online IL settings: _S-on-LfD_ (_top-left_), _S-on-LfO_ (_top-right_), _C-on-LfD_ (_bottom-left_), and _C-on-LfO_ (_bottom-right_) settings. As can be seen, CEIL quickly achieves expert-level performance in _S-on-LfD_. When extended to _S-on-LfO_, CEIL also yields better sample efficiency compared to baselines. Further, considering the complex cross-domain setting, we can see those baselines SQILand IQ-Learn (in _C-on-LfD_ and _C-on-LfO_) suffer from the domain mismatch, leading to performance degradation at late stages of training, while CEIL can still achieve robust performance.

**Offline IL.** Next, we evaluate CEIL on the other 4 offline IL settings: _S-off-LfD_, _S-off-LfO_, _C-off-LfD_, and _C-off-LfO_. In Table 2, we provide the normalized return of our method and baseline methods on reward-free D4RL  medium (m), medium-replay (mr), and medium-expert (me) datasets. We can

   } &  &  &  &  &  \\    & m & m & m & m & m & m & m & m & m & m & m & m & m \\   & OMIL (TD3+BC) & 50.9 & 22.1 & 72.7 & 44.7 & 30.2 & 87.5 & 47.1 & 26.7 & 102.6 & 46.5 & 31.4 & 61.9 & 624.3 \\  & SQIL (TD3+BC) & 32.6 & 60.6 & 25.5 & 13.2 & 25.3 & 14.4 & 25.6 & 15.6 & 8.0 & 63.6 & 58.4 & 44.3 & 387.1 \\  & IQ-Learn & 21.3 & 19.9 & 24.9 & 5.0 & 7.5 & 7.5 & 22.3 & 19.6 & 18.5 & 38.4 & 24.3 & 55.3 & 264.5 \\  & ValueDICE & 73.8 & 83.6 & 50.8 & 1.9 & 2.4 & 32.2 & 24.6 & 26.4 & 44.1 & 79.1 & 82.4 & 75.2 & 547.5 \\  & DemoDICE & 54.8 & 32.7 & 65.4 & 42.8 & 37.0 & 55.6 & 68.1 & 39.7 & 95.0 & 85.6 & 69.0 & 108.8 & 754.6 \\  & SMODICE & 56.1 & 28.7 & 68.0 & 42.7 & 37.7 & 66.9 & 66.2 & 40.7 & 58.2 & 87.4 & 69.9 & 113.4 & 735.9 \\  & **CEIL (ours)** & 110.4 & 103.0 & 106.8 & 40.0 & 30.3 & 63.9 & 118.6 & 110.8 & 117.0 & 126.3 & 122.0 & 114.3 & **1163.5** \\   & OMIL (TD3+BC) & 43.4 & 25.7 & 73.0 & 44.9 & 2.4 & 81.8 & 58.9 & 16.8 & 78.2 & 33.7 & 29.6 & 67.1 & 555.4 \\  & SMODICE & **54.5** & 26.4 & 73.7 & 42.7 & 37.9 & 66.2 & 60.6 & 38.5 & 70.9 & 85.7 & 68.3 & 116.3 & 741.7 \\  & **CEIL (ours)** & 54.2 & **51.4** & **90.4** & 43.5 & 40.1 & 47.7 & 78.5 & 20.5 & 110.0 & 97.0 & 67.8 & 120.5 & **821.7** \\   & OMIL (TD3+BC) & 52.8 & 27.6 & 46.5 & 38.3 & 8.0 & **74.0** & 25.3 & 28.4 & 26.3 & 26.0 & 17.6 & 11.9 & 382.6 \\  & SQL (TD3+BC) & 34.4 & 19.1 & 11.4 & 19.2 & 25.1 & 19.9 & 15.8 & 16.5 & 8.8 & 21.8 & 23.2 & 21.2 & 236.2 \\  & IQ-Learn & 37.3 & 35.4 & 25.9 & 27.4 & 27.1 & 31.2 & 27.7 & 22.2 & 31.7 & 63.7 & 63.3 & 55.8 & 448.8 \\  & ValueDICE & 22.0 & 18.3 & 18.9 & 14.0 & 11.7 & 8.7 & 11.5 & 10.0 & 8.6 & 24.1 & 21.4 & 19.2 & 188.4 \\  & DemoDICE & 52.9 & 15.2 & 77.2 & 42.8 & 38.9 & 53.8 & 58.4 & 26.4 & 77.8 & 87.8 & 69.3 & 114.9 & 715.6 \\  & SMODICE & 55.4 & 21.4 & 71.2 & 42.7 & 38.0 & 64.6 & 86.4 & 34.2 & 80.4 & 87.4 & 70.4 & 115.7 & 749.7 \\  & **CEIL (ours)** & 58.4 & 39.8 & 81.6 & 42.6 & 38.3 & 46.6 & 76.5 & 21.1 & 81.1 & 91.6 & 88.0 & 115.3 & **780.9** \\   & OMIL (TD3+BC) & 55.5 & 18.2 & 55.5 & 40.6 & 2.9 & **73.0** & 26.9 & 19.4 & 22.7 & 11.2 & 21.3 & 10.8 & 358.0 \\  & SMODICE & 53.7 & 18.3 & 64.2 & 42.6 & 38.0 & 63.0 & 68.9 & **37.5** & 60.7 & 87.5 & **75.1** & 115.0 & **724.4** \\   & **CEIL (ours)** & 44.7 & **44.2** & 48.2 & 42.4 & 36.5 & 46.9 & **76.2** & 31.7 & **77.0** & **95.0** & 71.0 & 112.7 & **727.3** \\   

Table 2: Normalized scores (averaged over 30 trails for each task) on 4 offline IL settings: _S-off-LfD_, _S-off-LfO_, _C-off-LfO_, and _C-off-LfO_. Scores within two points of the maximum score are highlighted

Figure 3: Ablating (**a**, **b**) the number of expert demonstrations and (**c**, **d**) the trajectory window size.

Figure 2: Return curves on 4 online IL settings: (**a**) _S-on-LfD_, (**b**) _S-on-LfO_, (**c**) _C-on-LfD_, and (**d**) _C-on-LfO_, where the shaded area represents a 95% confidence interval over 30 trails. Note that baselines cannot be applied to all the IL task settings, thus we only provide comparisons with compatible baselines (two separate legends).

observe that CEIL achieves a significant improvement over the baseline methods in both _S-off-LfD_ and _S-off-LfD_ settings. Compared to the state-of-the-art offline baselines, CEIL also shows competitive results on the challenging cross-domain offline IL settings (_C-off-LfD_ and _C-off-LfO_).

**One-shot IL.** Then, we explore CEIL on the one-shot IL tasks, where we expect CEIL can adapt its behavior to new IL tasks given only one trajectory for each task (mismatched MDP, see Appendix 9.2).

We first pre-train an embedding function and a contextual policy in the training domain (online/offline IL), then infer a new contextual variable and evaluate it on the new task. To facilitate comparison to baselines, we similarly pre-train a policy network (using baselines) and run BC on top of the pre-trained policy by using the provided demonstration. Consequently, such a baseline+BC procedure cannot be applied to the (one-shot) LfO tasks. The results in Table 3 show that baseline+BC struggles to transfer their expertise to new tasks. Benefiting from the hindsight framework, CEIL shows better one-shot transfer learning performance on 7 out of 8 one-shot LfD tasks and retains higher scalability and generality for both one-shot LfD and LfO IL tasks.

### Analysis of CEIL

**Hybrid IL settings.** In real-world, many IL tasks do not correspond to one specific IL setting, and instead consist of a hybrid of several IL settings, each of which passes a portion of task-relevant information to the IL agent. For example, we can provide the agent with both demonstrations and state-only observations and, in some cases, cross-domain demonstrations (S-LfD+S-LfO+C-LfD).

To examine the versatility of CEIL, we collect a separate expert trajectory for each of the four offline IL settings, and study CEIL's performance under hybrid IL settings. As shown in Table 4, we can see that by adding new expert behaviors on top of LfD, even when carrying relatively less supervision (_e.g._, actions are absent in LfO), CEIL can still improve the performance.

**Varying the number of demonstrations.** In Figure 3 (a, b), we study the effect of the number of expert demonstrations on CEIL's performance. Empirically, we reduce the number of training demonstrations from 20 to 1, and report the normalized returns at 1M training steps. We can observe that across both online and offline (D4RL *-medium) IL settings, CEIL shows more robust performance with respect to different numbers of demonstrations compared to baseline methods.

**Varying the window size of trajectory.** Next we assess the effect of the trajectory window size (_i.e._, the length of trajectory \(\) used for the embedding function \(f_{}\) in Equation 3). In Figure 3 (b, c), we ablate the number of the window size in 4 LfD IL instantiations. We can see that across a range of window sizes, CEIL remains stable and achieves expert-level performance.

  
**Hybrid offline IL settings** & Hop. & Hal. & Wal. & Ant. \\  S-LfD & 29.4 & 69.9 & 42.8 & 84.9 \\ S-LfD + S-LfO & 30.4 & 68.6 & 42.3 & **91.6** \\ S-LfD + S-LfO + C-LfD & 30.7 & 71.7 & 42.9 & 89.2 \\ S-LfD + S-LfO + C-LfD & 58.6 & 79.6 & 43.7 & 98.0 \\   

Table 4: The normalized results of CEIL, showing that CEIL can consistently digest useful (task-relevant) information and boost its performance, even under a hybrid of offline IL settings.

Figure 4: **Ablation studies on the optimization of \(f_{}\) (_ablating \(f_{}\)_) and the objective of \(_{}\) (_ablating \(_{}\)_), where the shaded area represents 95% CIs over 5 trails. See ablation results for offline IL tasks in Table 5.**

**Ablation studies on the optimization of \(f_{}\) and the objective of \(_{}\).** In Figure 4 and Table 5, we carried out ablation experiments on the loss of \(f_{}\) and \(_{}\) in both online IL and offline IL settings. We can see that ablating the \(f_{}\) loss (optimizing with Equation 5) does degrade the performance in both online and offline IL tasks, demonstrating the effectiveness of optimizing with Equation 8. Intuitively, Equation 8 encourages the embedding function to be task-relevant, and thus we use the expert matching loss to update \(f_{}\). We can also see that ablating \(_{}\) does lead to degraded performance, further verifying the effectiveness of our expert matching objective in the latent space.

## 6 Conclusion

In this paper, we present CEIL, a novel and general Imitation Learning framework applicable to a wide range of IL settings, including _CS-on/off-LfD/Lf0_ and few-shot IL settings. This is achieved by explicitly decoupling the imitation policy into 1) a contextual policy, learned with the self-supervised hindsight information matching objective, and 2) a latent variable, inferred by performing the IL expert matching objective. Compared to prior baselines, our results show that CEIL is more sample-efficient in most of the online IL tasks and achieves better or competitive performances in offline tasks.

**Limitations and future work.** Our primary aim behind this work is to develop a simple and scalable IL method. We believe that CEIL makes an important step in that direction. Admittedly, we also find some limitations of CEIL: 1) Offline results generally outperform online results, especially in the LfO setting. The main reason is that CEIL lacks explicit exploration bounds, thus future work could explore the exploration ability of online CEIL. 2) The trajectory self-consistency cannot be applied to cross-embodiment agents once the two embodiments/domains have different state spaces or action spaces. Considering such a cross-embodiment setting, a typical approach is to serialize state/action from different modalities into a flat sequence of tokens. We also remark that CEIL is compatible with such a tokenization approach, and thus suitable for IL tasks with different action/state spaces. Thus, we encourage the future exploration of generalized IL methods across different embodiments.