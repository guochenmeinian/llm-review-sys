# ActionAtlas: A VideoQA Benchmark for

Domain-specialized Action Recognition

 Mohammadreza Salehi\({}^{}\)  Jae Sung Park\({}^{}\)  Tanush Yadav\({}^{}\)

Aditya Kusupati\({}^{}\)  Ranjay Krishna\({}^{}\)  Yejin Choi\({}^{}\)  Hannaneh Hajishirzi\({}^{}\)  Ali Farhadi\({}^{}\)

\({}^{}\)University of Washington \({}^{}\)Allen Institute for AI \({}^{}\)Nvidia

mrsalehi@cs.washington.edu

https://mrsalehi.github.io/action-atlas/

###### Abstract

Our world is full of varied actions and moves across specialized domains that we, as humans, strive to identify and understand. Within any single domain, actions can often appear quite similar, making it challenging for deep models to distinguish them accurately. To evaluate the effectiveness of multimodal foundation models in helping us recognize such actions, we present ActionAtlas v1.0, a multiple-choice video question-answering benchmark featuring short videos across various sports. Each video in the dataset is paired with a question and four or five choices. The question pinpoints specific individuals, asking which choice "best" describes their action within a certain temporal context. Overall, the dataset includes \(934\) videos showcasing \(580\) unique actions across \(56\) sports, with a total of \(1896\) actions within choices. Unlike most existing video question answering benchmarks that only cover simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and rigorously tests the model's capability to discern subtle differences between moves that look similar within each domain. We evaluate open and proprietary foundation models on this benchmark, finding that the best model, GPT-4o, achieves a maximum accuracy of \(45.52\%\). Meanwhile, Non-expert crowd workers, provided with action description for each choice, achieve \(61.64\%\) accuracy, where random chance is approximately \(21\%\). Our findings with state-of-the-art models indicate that having a high frame sampling rate is important for accurately recognizing actions in ActionAtlas, a feature that some leading proprietary video models, such as Gemini, do not include in their default configurations.

## 1 Introduction

Multiple institutions have remarked on a striking finding: In many standard video benchmarks [59; 29; 77; 76; 74], a single frame without any modeling of temporal dynamics was enough to perform well [33; 38]. The phenomenon, known as static appearance bias , allowed models to easily discern the action depicted in the video. For example, simply recognizing presence of a pool was sufficient to identify the action as "diving" from a single frame [43; 69]. In reaction, later action classification datasets were designed to capture a richer distribution of temporal understanding [17; 58; 57] and model architectures also evolved to incorporate the now "necessary" temporal dynamics [40; 13; 12; 73]. However, such datasets and models still do not cover all the complexities of real-world video understanding and are only limited to traditional classification setups. The issue is even more pronounced in video-language tasks; with the rise of foundation Vision-Language models (VLMs), we find ourselves _back at square one_: Tasks such as video question answering [75; 37; 77] and video-language retrieval [77; 21; 28] can be solved easily yet again by training on just a single or sparsely sampled set of frames from videos [4; 33].

Recognizing activities in many real-world videos requires an accurate identification of subtle changes in movements, posture, and interaction with the environment. This is particularly evident in numerous domain-specific videos as demonstrated in Figure 1. Actions within a specific domain may appear identical in a single frame but become identifiable across multiple frames that capture the _sequence of movements_, as shown by cross through and body twist motion in the Rose move example, or the continuous rotations in the water in the Cartwheel move. In certain instances, actions are so subtle that they remain challenging to distinguish even when presenting multiple frames, as shown in the Stutter step and 360 pressure flip examples in skateboarding. Action recognition in real-world videos becomes even more complex with multiple actors present, each engaged in distinct or relevant and often overlapping actions. The video of the soccer game exemplifies this, with players from both teams simultaneously performing different actions. A robust video-language model must be able to _track_ individuals and activities amidst such multifaceted and busy videos.

To investigate whether current Vision-Language Models (VLMs) can address these challenges, we present Action Atlas v1.0, a multiple choice video question answering (VideoQA) benchmark. This preliminary version of the benchmark focuses on sports, a domain characterized by intricate actions which can stress test models with the challenges identified above. Each video in the dataset is paired with a question and four or five choices. The questions pinpoint specific individuals within, asking which choice "best" describes their action within a certain temporal context. Overall, the dataset includes \(934\) videos showcasing \(580\) unique actions across \(56\) sports, with a total of \(1896\) actions within choices. The videos in this dataset have an average duration of 6.07 seconds and an average frame rate of 32.18 frames per second (FPS).

To collect ActionAtlas, we develop a novel pipeline. In contrast to prior work that sourced action names from a single website [11; 48], we rely on GPT4's vast domain-specific knowledge and compile a comprehensive list of actions within each domain by prompting the model. Having this list, we crawl videos about those actions on YouTube. To further filter the obtained search results, we rely on numerous automatic filtering tools and techniques, such as exact and soft lexical search, and CLIP  filtering. Additionally, we show how LLMs and speech transcriptions can be used to faster find segments containing a specific action within long videos. To further ensure high quality of our benchmark, we incorporate multiple rounds of manual filtering carried out by both crowd-workers and the authors, who spent a month familiarizing themselves with the actions.

We evaluate open and proprietary VLMs, such as GPT-4o  and Gemini-Pro  on ActionAtlas. For all models except for Gemini models in video mode-which directly take input video files-, we follow the standard methodology from previous work ; we uniformly sample frames and feed them as image inputs concatenated with the question and the choices. For Gemini, we show how one can easily recover the exact frames that the model samples in video mode which makes it not much different from other VLMs. The random chance accuracy on our dataset is \(20.91\%\), whereas the best open-weight model, Qwen2-VL-7B, performs only \(\!\!30.24\%\) accurately. Meanwhile, GPT-4o reaches up to \(45.52\%\). our results with both of these models show that increasing frame sampling rate helps significantly in recognizing actions in ActionAtlas, a feature that current top proprietary video models like Gemini lack in their default settings. Moreover, we show that providing action descriptions does not significantly improve model performance, while non-expert crowd workers achieve an overall accuracy of \(61.6\%\) with those descriptions. This underscores the gap between AI models and human's visual recognition capabilities when it comes to actions with fine motions.

Taken together, our benchmark provides a new testing ground to evaluate foundation models on action recognition within specialized domains that have practical, real-world applications. The results on our benchmarks showcases that despite the improvements in image-understanding and long-form video understanding, true video-understanding is still lacking. Moreover, as demonstrated by previous work like Dall-E 3 , captions generated by AI models that understand nuanced moves-such as those in ActionAtlas-can enhance training of video generation models to better capture such nuances. Therefore, we believe ActionAtlas can help accelerate various aspects of video research.

## 2 Related work

### Action Recognition

The seminal work by Schuldt et al. , which collected data on six basic human actions, set the stage for many subsequent work focused on action recognition, such as UCF101, HMDB51,A ActivityNet , AVA , Kinetics , and Moments in time . These studies have primarily focused on coarse-grained everyday human actions that are sourced from a single website  and are relatively easy for image models to recognize . The Something-something v2  benchmark has also introduced 174 diagnostic commonsense actions to assess models' understanding of world physics. Breakfast  and MPII Cooking  datasets have collected 10 and 67 fine-grained actions in cooking, and Diving48  and Finegym  collected fine-grained actions in only diving and gymnastics respectively with just a single action actor in each video. In domains with many

Figure 1: **Examples from ActionAtlas**. To answer ActionAtlas’s questions, models have to be able to recognize fine movements and nuances that differentiate actions belonging to the same domain (examples 2, 3, 4, 5, 6 from top), correctly localize and track the individual performing the action if there are many (example 1). [Video links from top to bottom: 1, 2, 3, 4, 5, 6].

individuals in the video, Multisports  is the biggest dataset which spans 67 fine-grained actions in four sports. Other work [79; 42; 78; 39; 20; 15; 31; 46; 88] have created datasets for fine-grained action recognition in different sports such as soccer, basketball, figure skating, diving, fencing, table tennis, etc. Our work differs from all these work in the following aspects: 1. **Larger number of domain-specialized actions**. There are \(580\) ground truth actions depicted in videos in ActionAtlas. Moreover, as there are \(1896\) total actions in choices, the dataset effectively tests the model's capability to discern \(1896\) actions which is more than 3 times larger than previous datasets, such as Finegym. 2. **The actions in ActionAtlas represent knowledge-driven, real-world actions**, In contrast to datasets like Something-something v2 , our work focuses on evaluating foundation models on action recognition within specialized domains that have practical real-world applications. This is analogous to the recent trend on evaluating foundation models on specialized domains with datasets like MMLU  and MMMU  in the text and image space. 3. **The use of language and QA in ActionAtlas**. Previous work  have used bounding boxes to refer to individuals engaged in a particular action in a video with many people in it. However, we follow works like refCOCO  and use natural language to refer to individuals (see how questions refer to action actors in Figure 1). This aligns more closely with how humans naturally refer to an object or person and also better fits evaluating VLMs. 4. **Faster discovery of actions using LLMs**. We show how the extensive knowledge of LLMs can be used to identify a broad range of actions, including rarer actions that experts may overlook. Furthermore, we show how LLMs such as GPT4-text can be used along with speech transcription to find candidate segments within longer videos that are likely to contain a specific action (see SS3.4 and Figure 3). This approach relies solely on text and can help making the collection pipeline more scalable.

### Video QA

Most Video-QA datasets, such as NextQA , ActivityNet-QA, VATEX , MSRVTT-QA and MSVD-QA are designed for general purpose video understanding, with most questions about visual appearance of the scenes or basic actions in the video which fall under the category of common sense understanding. These do not pose a significant challenge to current state-of-the-art VLMs. Other datasets like MVBench  and Video-Bench  have attempted to standardize existing datasets into a multiple-choice QA format but still primarily cover simplistic actions. Ego4D  and Ego-Exo4D [\(\)] have annotated various actions performed by crowd workers, but these do not include many expert-level actions in the domains they cover as performing such actions is extremely hard for non-expert crowd-workers. While Youcook2  contains densely annotated cooking videos, most of the actions (e.g., pouring water into a cup) are extremely simple with more expert-level actions overlooked as the collection pipeline is not designed for capturing them. Additionally, Perception test  is another diagnostic benchmark for evaluating basic world understanding of models. In contrast, ActionAtlas's focus is on real-world actions in specialized domains.

### Long-form Video Understanding

Video, as a form of streaming data, can get infinitely long, making it challenging for deep models to understand them. Recently, there has been growing interest in creating long-form video understanding datasets from various sources. Several studies have used long movies to curate such datasets [62; 32; 2]. A common limitation is that models can often answer the questions based on just the story-line-which they might have seen in their pretraining data-without the need to attend to the visual content. Other works such as EgoSchema  have used crowd-sourced Ego4D videos to create QA datasets with long (180 seconds) ego-centric videos; however, the actions in these datasets tend to be coarse-grained and simplistic. Furthermore, as noted in prior work [47; 63], it is still uncertain whether viewing the full video is necessary to answer the questions. This uncertainty arises because the questions generated by LLMs are not carefully filtered for language biases which helps models answer solely based on text. In contrast to studies focused on long-form video understanding, ActionAtlas specifically targets short videos about domain-specific actions, many of which characterized by rapid changes and movements within the scene. Furthermore, recent studies such as [84; 47] have shown that it is possible to use a video captioner to summarize smaller chunks of a long video and pass them to an LLM for reasoning over the entire video. Therefore, understanding short video clips and the potentially complex actions they depict could serve as a foundation for solving long-form video understanding.

## 3 Collection

Our goal is to create a high quality benchmark of actions within specialized domains. Since sports encompass a wide array of such actions, in ActionAtlas v1.0 we focus on actions within various sports. Nevertheless, we would like the collection process to be scalable, allowing expansion to many more domains in the future. Toward this, we develop a robust pipeline that incorporates automatic filtering using various tools and AI models, followed by multiple rounds of manual filtering by crowd-workers and the authors. Figure 2 illustrates the pipeline for collecting ActionAtlas. As we move forward to the latter stages of this pipeline, the need for manual verification and expertise increases, making it more and more expensive. In the final and most costly stage, the goal is for trained annotators to only verify the already vetted annotations from previous stages and refine them if needed-instead of asking them to directly search for videos on YouTube. Our data collection also relies on one fundamental assumption: By starting from a very large initial set of videos for each action, we ensure that even after an intensive automatic filtering, a large enough number of videos remain for humans to check. Note that whenever we mention "GPT4" or "GPT4-text" in our pipeline it specifically refers to gpt4-1106-preview_without using vision capability_.

### Compiling A List of Actions

To generate the action list, we first collected the names of 150 most popular sports through prompting GPT4. To gather the actions within each sport, previous work [39; 79] asked experts to write the name of the moves. However, we witnessed that those lists were incomplete as humans tend to overlook actions in the far tail of the distribution. We therefore decided to ask GPT4 to list the moves for us. We found that using GPT4 with a prompting strategy which we call _expansion and squeeze prompting_ resulted in the best coverage. In this approach, GPT4 is prompted to generate an action list for any domain given few-shot examples in an example domain (e.g., golf). As we found that the model may still omit some major actions, we iteratively expanded this list by using the model's previous outputs as new few-shot examples. After two rounds of expansion, we squeezed the list by having GPT4 identify and remove false positives, i.e. phrases that were added during expansion and considered to be physical actions but in reality are not. In total, The expansion process resulted in 19.5k actions which were reduced to 10.5k after squeezing. It is worth noting that before solely relying on GPT4 to get the action list, we tried to compile a list by crawling data from available knowledge-bases, such as DBPedia and Wikipedia. However, we found that many actions are missing in those knowledge bases. Please refer to Appendix F for GPT4 prompts used in this section.

### Searching for Metadata of Videos

Searching for videos of a large list of actions and downloading both the audio and video streams of the results would be impractical due to the high storage and computational costs involved. Instead, we initially queried and downloaded metadata of videos on YouTube, which includes the titles,

Figure 2: **Data collection pipeline consisting of Automatic and Manual parts. First a comprehensive list of actions is compiled (§3.1) which are then used for searching metadata of videos on YouTube relevant to each action (§3.2). Then with lexical search a subset of videos are selected (§3.3). If a video is shorter than 30 seconds, it will be used in crowd-sourcing. Otherwise, the video is transcribed, and GPT4 selects potential 30 second segments that contain the actions based on the transcription (§3.4). Mechanical Turkers will then verify the presence of actions in the segments (§3.5) and localize it (§3.6). If all videos of an action were rejected, we repeat the process to source new videos. Finally, GPT4 generates Multiple-choice QAs which are checked by the authors (§3.7).**descriptions, and subtitles. This enabled us to carry out text-based filtering as our next step to eliminate any irrelevant videos from the search results, leaving us with videos that are highly likely to contain the desired actions which will then be downloaded. We formed queries by concatenating the name of action to its domain (e.g. "knuckleball shot soccer") and searched for YouTube metadata for each query. This yielded a total of approximately 4.5 million unique and valid metadata files, or about 450 valid metadata files per action. We will describe the text-based filtering process next.

### Lexical Search

Our extensive metadata search for each action yielded a high recall of YouTube videos associated with that action. However, we also encountered false positives in our results-videos not sufficiently relevant to the specific action and domain of our search. To narrow down our selection to videos more likely to contain the targeted action, we used lexical matching on video titles via Elasticsearch [\(@sectionsign\)]. Specifically, we searched for videos with the action name appearing exactly within the title. Additionally, we restricted our search to videos with more than 1000 views as of the collection date. Moreover, the BM25 engine in Elasticsearch provided a soft search feature, enabling us to detect whether video titles included synonyms of the action query (e.g., "knuckleball shot" and "knuckleball kick" in soccer) that exact matching might have missed. In the lexical search process, we limited the number of hits to 100 videos per each action.

### Finding Potential Segments in Long Videos via Whisper and GPT4-text

The primary goal of the automatic filtering pipeline is to identify videos that are highly likely to contain the desired actions. These videos are subsequently forwarded to crowd workers to confirm the presence of these actions. Many high quality action demonstrations happen in long videos, specially in the form of tutorials. As watching long videos can be tedious and costly in terms of annotation, we decided to extract potential 30-second clips that are highly likely to contain the target actions within longer videos. Videos shorter than 30 seconds directly proceeded to the validation stage done by crowd-workers. To extract potential segments, we focused on videos with speech data and first transcribed them using OpenAI's Whisper model . The transcripts, which included timestamps provided by Whisper, as well as the action name, served as context for GPT4. The model was then prompted to output timestamps where the action is most likely to occur, focusing specifically on instances where the speaker comments on the great quality of the action (Figure 3 top) or indicates that a demonstration of the action is going to happen shortly (Figure 3 bottom). We then extracted 15 seconds before and after the suggested timestamp as the candidate segment. These segments were further filtered using CLIP similarity score  with their corresponding domain name and video domain pairs with a cosine similarity of lower than 0.1 were discarded. In total, we transcribed and localized 57k videos with Whisper and GPT4-text, and collected 49k videos that were shorter than 30

Figure 3: Given transcription of a long video, GPT4-Text can be prompted to output timestamps where the action is likely to occur, _without having access to frames_. The model has found instances where the speaker comments on the great quality of the action (top) or indicates that a demonstration of the action is going to happen shortly (bottom). More details in §3.4. [Video links: top, bottom].

seconds and did not need localization. All these videos are downloaded at end of this stage. Please refer to Appendix F for the prompts used with GPT4.

### Validation via Crowd-sourcing

After collecting candidate 30 second segments likely to contain actions, we further validated the presence of actions with the help of crowd-workers on Amazon Mechanical Turk. To do so, we sorted the actions based on the number of videos filtered from previous stages and performed the validation process in batches, starting from the actions with highest number of videos and proceeding in descending order. A potential challenge here was the workers' unfamiliarity with actions in different domains which might introduce errors in the annotation. To mitigate such errors, we employed multiple safeguards in our pipeline: 1. We prompted GPT4-text to provide descriptions that highlight the key elements required to recognize an action. An example definition is shown in Figure 4. 2. We presented five potential 30 second segments per each action to workers instead of one. This allowed the worker to compare the videos against each other and against the description which further helps in identifying the action. Workers were instructed to watch each video and determine if the action happens or not. If the worker noticed any discrepancy between the videos and the description provided by GPT4, they could search on video platforms to watch more videos and educate themselves on the action. 3. Each set of videos was reviewed by three workers, and only videos for which at least two workers confirmed the presence of the action proceeded to the next step. If all videos of an action were rejected by crowd-workers, we repeated the earlier stages of the pipeline to source new videos for that action (see the arrow from step 5 to step 3 in Figure 2). For more details on verification via crowd-sourcing and Mechanical Turk layout see Appendix K.

### Localization via Crowd-sourcing

As many events and actions can happen in a 30 second video clip, we sought to find a smaller segment that better isolates the target action. However, isolating a single action in a video clip can be quite challenging. This complexity arises because some actions last only briefly (e.g., stutter step, as shown in Figure 1), while some are only defined based on what happens before or after them (e.g., a dummy, as shown in Figure 1). To address these challenges, we asked crowd workers to specify three key details if necessary: 1. Some attributes that uniquely identify the action actor; they were instructed to use any distinguishing feature (e.g., number or name on jersey, clothing or hair color), as long as it uniquely identifies the individual(s) performing the action. 2. The events that occur immediately before and after the action. We then asked them to propose a start and end time stamp within the 30 second segment that encapsulates these two pieces of information. This data were then used to extract the final video segments in the benchmark and write questions about them. For more details on localization via crowd-sourcing and Mechanical Turk layout see Appendix K.

Figure 4: **Definition of Half-Volley Drop Shot generated by GPT4-text to be used by crowd-workers for validation and localization. The workers match the key elements listed in the definition with what they see in the video to identify if the action happens. For more details see §3.5.**

### Multiple-choice QA Generation and Quality Check

We prompted GPT4-text with specific question templates and the two pieces of information obtained in SS3.6 and asked it to write a question about what the action is (see Figure 1 for example questions). We also prompted the model to write 9 hard negatives for each action. Subsequently, for the quality check, a team of three individuals including two of the authors, trained themselves on recognizing the actions and checked the questions, videos, and the hard negatives over the span of a month. For the hard negatives, we kept only the three or four most plausible ones. Videos for which we were uncertain about the presence of the ground truth action were discarded. To eliminate questions answerable solely by text, we passed them through GPT4-Text model three times. We noticed that in 9% of the samples the model consistently predicted the correct answer. This was often due to the questions inadvertently revealing information about the answers. To address this, we rewrote the questions and choices. Moreover, to manage instances where text within video frames might leak information about the ground truth action, we used the Google Cloud Vision API  to detect such text, and blurred it using Gaussian noise. Appendix F provides more details on the prompts used in this section.

## 4 Evaluation

### Models and Baselines

Clip.We evaluated OpenAI's CLIP ViT-L-14-336  as a foundation VLM without any large language models in it. To form the prompts, we asked GPT4-Text to rewrite each question as a sentence and embed each choice into the sentence. This gave us four or five plausible class prompts which we used to do classification with the model.

Proprietary large VLMs.We evaluated multiple proprietary VLMs on ActionAtlas: Gemini 1.5 Flash and Gemini 1.5 Pro using the Gen AI API  (model versions gemini-1.5-flash-latest and gemini-1.5-pro-latest), GPT4-o , GPT4-Turbo, and GPT4-o-mini using the Open AI API1. The GPT-4 family cannot directly process video inputs. Therefore, we uniformly sampled different numbers of frames-1, 4, 8, 16, and 32-and reported the results for each configuration. For Gemini models, as detailed in SS4.3, we discovered that the video mode always samples a specific set of frames at a rate of one FPS, making it not much different from other models. With such sampling rate, to make the model use all the frames in a video as input, we converted our videos to 1 FPS videos (e.g., a 3-second, 30 FPS video would become a 90-second, 1 FPS video) and evaluated the Pro variant on these converted videos as well.

Open large VLMs.We evaluated the following open models on ActionAtlas with uniformly sampled frames: Qwen2-VL-7B , mPLUG-Owl , Video-LLaMA , Video-LaVIT , VideoChat v2 , and LLaVA-Next-Video . Frames were down-scaled to \(336 336\) for models whose image encoders supported this resolution; If not, frames were scaled to the maximum supported resolution (e.g., \(224 224\)). For Qwen2-VL-7B, we down-scaled the frames while maintaining the aspect ratio so that each frame does not have more than \(336*336\) pixels in total. All evaluations with open models were done on a single H100 GPU. Further details about the setup can be found in Appendix A.

Non-expert humans.To get non-expert human's performance as a baseline, we asked crowdworkers on Amazon Mechanical Turk to respond to ActionAtlas's questions. As the workers might not be familiar with the action names, we generated two to three sentence descriptions of each action choice using GPT-4o, which were then provided to the workers along with the available choices. The workers were asked to answer the questions without using YouTube, but they were allowed to perform text search on Google or use AI chatbots, provided the chatbots' vision capabilities were not used. Note that in the ablation experiments described in SS4.3, we also evaluated AI models when these descriptions are provided.

### Results

We benchmark all models using only the video data, discarding any audio, transcriptions, or captions. For open models, frames are uniformly sampled throughout the video and the models are evaluated based on the following metrics: the number of input frames sampled by the model, the number of tokens to which the video is compressed before being fed to any transformer model, average inference FLOPs on the benchmark (calculated using fvcore  package), and top-1 accuracy (for more information on the significance of these metrics, see SS5). Following , we report \(95\%\) confidence intervals via bootstrap sampling. For the results based on sampling fixed number of frames per second see Appendix B.

The results in Table 1 indicate that, except for Qwen2-VL-7B, open models often perform no better than, or only slightly better than random chance. This suggests that open models struggle to capture the nuances of complex actions in specialized domains, likely due to the absence of such data in their pretraining corpora. For Qwen2-VL-7B, increasing the number of frames from 2 to 16 increases accuracy statistically significantly. However, the performance declines or plateaus upon reaching 32 frames. This indicates that while sampling more frames provides valuable information for understanding complex domain-specialized actions, there is also a downside of handling longer video sequences that open video models might struggle with. Regarding number

  
**Model** & **\#Input frames** & **\#Input video tokens** & **\#Inference TFLOPs** & **Accuracy(\%)** \\  Random chance & - & - & - & \(20.91 2.49\) \\ Non-expert human & - & - & - & \(61.64 3.29\) \\  CLIP ViT-L-14-336  & \(16\) & \(16 576\) & - & \(23.71 2.62\) \\  mPLUOG-Owl-video  & \(16\) & \(16 256\) & \(2.94\) & \(19.49 2.68\) \\ Video-LLaMA  & \(16\) & \(16 256\) & \(6.12\) & \(22.71 2.69\) \\  Video-LaVIT  & \(24^{*}\) & \(24 135\) & \(3.38\) & \(19.37 2.46\) \\  VideoChat2  & \(16\) & \(16 196\) & \(3.23\) & \(20.86 2.34\) \\ VideoChat2  & \(32\) & \(32 196\) & \(4.69\) & \(20.77 2.67\) \\ \(64\) & \(64 196\) & \(7.51\) & \(21.27 2.75\) \\   & \(16 144\) & \(22.0\) & \(20.77 2.67\) \\  & \(32\) & \(32 144\) & \(43.0\) & \(21.06 2.64\) \\   & \(64\) & \(64 144\) & \(83.2\) & \(22.90 2.56\) \\    & \(2\) & \(1 576\) & \(2.45\) & \(24.07 2.66\) \\   & \(4\) & \(2 576\) & \(4.31\) & \(26.71 2.83\) \\   & \(8\) & \(4 576\) & \(8.31\) & \(27.75 2.81\) \\   & \(16\) & \(8 576\) & \(13.38\) & \(30.24 2.94\) \\   & \(32\) & \(16 576\) & \(29.87\) & \(29.33 3.10\) \\   

Table 1: **Evaluation results on ActionAtlas for open models.** Open models perform no better than random chance. The efficiency metrics are averaged across the benchmark. *a single video frame and 24 frames of motion vectors are used, consistent with the video tokenizer described in .

  
**Model** & **\#Input frames** & **Accuracy(\%)** \\  Gemini 1.5 Flash (video) & \(1\) fps & \(30.49 2.86\) \\ Gemini 1.5 Pro (video) & \(1\) fps & \(32.37 3.04\) \\ Gemini 1.5 Pro (video) & all frames & \(35.59 3.04\) \\   & \(1\) & \(28.97 3.06\) \\  & \(4\) & \(33.17 2.98\) \\  & \(8\) & \(34.25 3.09\) \\  & \(16\) & \(33.99 3.05\) \\  & \(32\) & \(32.24 2.80\) \\   & \(1\) & \(30.15 2.74\) \\  & \(4\) & \(33.42 2.70\) \\  & \(8\) & \(30.20 2.89\) \\  & \(16\) & \(32.20 2.76\) \\  & \(32\) & \(31.17 2.90\) \\   & \(1\) & \(33.08 2.89\) \\  & \(4\) & \(39.50 3.10\) \\   & \(8\) & \(41.55 3.01\) \\   & \(16\) & \(\) \\   & \(32\) & \(41.44 3.11\) \\   

Table 2: **Evaluation results for proprietary models.** For Gemini in video mode we either provide the original video or transform it into a video with 1fps and then evaluate the model. See §4.3 for more details on how Gemini processes inputs.

of video tokens and inference FLOPs, for all models except for Qwen2-VL-7B, we do not see a straightforward relationship with the performance, suggesting that increasing computational complexity does not necessarily lead to better results for these models. However, for Qwen2-VL-7B, there is a notable positive correlation up to 16 frames. Additionally, Qwen2-VL-7B compresses frames into fewer tokens compared to models processing them at the same spatial and temporal resolution. For instance, with 16 frames, Qwen2-VL-7B uses half the tokens that CLIP uses, while delivering significantly better performance.

For the GPT-4 family, increasing the number of sampled frames does not lead to statistically significant improvements for GPT-4 Turbo and GPT-4o-mini. However, for GPT-4o, sampling more frames results in statistically significant improvements, rising from \(33.08\%\) to \(42.95\%\). For video mode of the Gemini family, both Pro and Flash variants achieve comparable accuracies given the confidence intervals. Furthermore, there is no statistically significant improvement when increasing the number of frames from default 1 fps to all video frames (see SS4.1 for how we evaluate in that mode) with the Gemini 1.5 Pro model. Nonetheless, as discussed in the ablation studies in SS4.3, sampling a single frame from the video results in the Pro model achieving an accuracy of \(28.73\%\), which is significantly lower than the \(35.59\%\) when using all frames of the video.

### Ablations

Providing description of actions.Correctly answering the questions in ActionAtlas requires two capabilities: 1. Having knowledge about the action choices, specifically recognizing the action names, and 2. Visually recognize the action in the video. To show that model failures are primarily due to poor visual recognition, we provided descriptions of the action choices to the models. These descriptions were created by prompting GPT-4o to summarize the key elements needed to recognize each action in 2-3 sentences. As shown in Table 3, adding these descriptions did not yield statistically significant improvements on our dataset. However, using the same descriptions, humans outperformed the best proprietary model, GPT-4o, by \(18\%\). This suggests that the model struggles on our dataset not due to a lack of knowledge about the action names, but because of limitations in visual recognition.

Chain-of-thought reasoning.Chain-of-thought (CoT) reasoning or generating intermediate reasoning steps  has shown to be effective in improving performance across different language and vision-and-language tasks . To test this on our dataset, we instruct models to reason step by step and provide their rationale when making a choice. We choose not to include few-shot examples to avoid occupying a significant portion of the models' available context length. Table 4 shows the results when using CoT reasoning with or without choice descriptions. Surprisingly, for all models there is a significant drop in performance when using only CoT. Although adding choice descriptions improves performance in the case of GPT-4o, the improvements are not statistically significant compared to the original results. This suggests that enhanced reasoning alone does not improve performance on ActionAtlas and the low performance of models is mainly due to poor visual recognition. We also experimented with prompting models to reason step by step across frames, by describing differences between each pair of consecutive frames, as done in previous studies . Yet, this strategy does not enhance performance. It is also worth noting that with Gemini Pro 1.5, there is a dramatic increase in the number of refusals, which might contribute to the drop in performance; Table 8 in Appendix E shows that with the chain-of-thought setup, the refusal rate of Gemini 1.5 Pro model increases to more than \(5\%\).

  
**Model** & **Acc. without Description (\%)** & **Acc. with Description (\%)** \\  mPlug-owl & \(19.49 2.68\) & \(20.92 2.51\) \\ Video-Llama & \(22.71 2.69\) & \(21.85 2.68\) \\ LLaVA-Next-video & \(20.77 2.67\) & \(22.20 2.50\) \\ Qwen2-VL & \(30.24 2.94\) & \(33.13 3.04\) \\ Gemini Pro 1.5 & \(32.37 3.04\) & \(37.00 3.06\) \\ GPT-4o & \(42.95 2.91\) & \(44.05 2.94\) \\   

Table 3: **The accuracy improvements from providing action choices’ descriptions is not statistically significant.**Changing frame sampling rate with Gemini Pro 1.5In our evaluation of proprietary API models, the Gemini family were the only ones at the time that could directly take video input files2. While investigating how these models process videos by inserting random noise images at different frame positions, we noticed that the these models _always and only_ process the middle frame in each second of a video, resulting in a processing rate of 1 FPS. This finding aligns with the Gemini technical report , wherein all the evaluations are done at 1 fps. Processing a preselected set of frames, not only exposes the model to potential frame injection attacks but also limit its ability to detect motions occurring more frequently than this sampling rate (for more details on the frame injection attack and jail-breaking Multi-modal Gemini models, see Appendix D). To test the model at a higher sampling rate in video mode, we converted all the videos in ActionAtlas to 1 FPS videos-meaning each video has only one frame per second-and re-evaluated the model. As shown in Table 2 and discussed in SS4, no significant improvements were observed. This suggests that Gemini models might not be trained to handle all the frames within a video. Furthermore, For a better comparison with other models, including GPT-4o, we also sampled a fixed number of frames from the videos and evaluated Gemini Pro 1.5 in image mode. Figure 5 indicates that with Gemini 1.5 Pro, increasing the number of sampled frames does not yield as substantial improvements as it did with GPT-4o. It is worth mentioning that as Table 8 in Appendix E shows, we noticed a much higher refusal rate with the models when input frames were used (from \(1.39\%\) to \(5.14\%\)), similar to the findings from the CoT experiments in SS4.3. This refusal rate also slightly increased with the addition of more frames.

### Qualitative Error Analysis

To understand why VLMs struggle on our benchmark, we investigate the nature of the errors made by the Gemini and GPT-4 family. We sample 20 erroneous test cases at random, and analyzed models' reasoning. We conduct this analysis within two setups: one where descriptions of the choices are provided, and one where only the action names are provided. We prompt the model to use chain-of-thought reasoning in both setups.

We find that most errors fall into _at least_ one of two broad categories and four subcategories. Namely, visual hallucinations, visual oversights, and visual tracking failure are subcategories within the visual recognition errors category, while the remaining errors are classified under the QA reasoning failure category. Figure 6 shows examples of these errors.

Visual hallucinations occur when a model misidentifies an item or action in a video clip and hallucinates another action. For instance, in the netball example shown in Figure 5(a), GPT-4o hallucinates that the ball bounces on the ground during a pass, whereas the video clearly shows a direct chest pass. Visual oversights happen when a model correctly identifies most elements of a clip but misses a crucial detail or movement in the clip. An example is the golf clip of a double hit shown in Figure 5(b), where the first of the two hits is a chip shot. Both GPT-4o and Gemini 1.5 Pro focus on the swing but overlook the obvious second hit. This causes the models to incorrectly predict chip shot, which is not the "best" choice that describes the video as asked by the question. Visual tracking happens when the model fails to localize and track the individual performing the action which is depicted in 5(d). Lastly, QA reasoning failures, shown in Figure 5(c) happen when the model describes the correct action but ultimately selects the wrong choice. Overall, visual hallucinations are the most common error across-the-board, making up about \(60\%\) of errors. The remaining errors are divided among visual oversights, tracking and reasoning failures, with visual oversights being slightly more prevalent.

## 5 Discussion

Why do the reported metrics matter?The quantitative and qualitative results with models like GPT-4o have shown that accurately detecting subtle movements in actions within ActionAtlas requires denser video signal sampling-which in its simplest form means sampling more frames. While a higher sampling rate could lead to a greater number of tokens and thus higher FLOPs if tokenized naively, much of the additional data from increased sampling is often redundant. Better tokenizers could potentially leverage this redundancy and compress the sampled data to maintain the same number

Figure 6: **Examples of prediction errors by proprietary models on ActionAtlas.**

of tokens as would be achieved with a lower frame sampling rate, without sacrificing downstream performance. We need further research on tokenizers to find the optimal balance between video sampling rates and token compression, while ensuring high downstream accuracy on vision-and-language tasks. Simple strategies such as masking spatiotemporal patches , or more innovative tokenization schemes that go beyond mere frame sampling, could help maintain a stable token count even with higher sampling rates. An example of such progress is Video-LaVIT , which encodes frames following a key frame into more compact representations using motion vectors.

With its complex actions and intricate movements, ActionAtlas can be a test-bed for all these ideas in video-language modeling. Moreover, reporting metrics such as the number of tokens and sampled frames can further shed light on the density of sampling from the video data and the amount of compression that is happening in tokenization.

The videos on the web as training set.As noted in previous work, such as the MMLU benchmark , modern benchmarks for foundation models assume that these models acquire the knowledge to solve various tasks by training on vast amounts of web data. Similarly, the knowledge needed to recognize actions in ActionAtlas v1.0 is readily available on video platforms like YouTube. We hypothesize that if a human were to watch all 4.5 million videos we collected, they would likely be able to recognize most of the actions in ActionAtlas v1.0. Thus, ideally, training on this massive dataset would enable the model to learn this knowledge, which we leave for future work. We will also be releasing the YouTube IDs of the 4.5 million videos we crawled for large-scale pre-training.

## 6 Limitations

Limited to sports domain.Sports is a real-world domain characterized by complex and subtle movements. While for many of these moves, non-expert humans can easily associate a description of an action with its corresponding movements in video and recognize it, models still lag significantly behind in capturing these nuances. Similar actions are found in other real-world domains, such as cooking, arts and crafts (e.g., knitting, sewing, etc.), dance (e.g., ballet, hip hop, salsa), and medicine. With our collection pipeline in place in Action Atlas v1.0, we plan to expand our dataset to include these domains with the help of domain-experts.

Lack of taxonomy.Another limitation of the current version of ActionAtlas is the absence of a taxonomy for actions. A well-defined taxonomy is helpful for ensuring that the action list in the dataset is comprehensive. In future iterations, we plan to have a collaboration between large language models and domain-experts to address this. LLMs are great at proposing a broad list of actions, including those in the long tail of the action distribution, while experts are great at refining and structuring this list within an organized taxonomy.

Small size.Collecting video data for domains that require expertise is an extremely challenging task. The current version of ActionAtlas is smaller compared to other multimodal benchmarks like MMMU . However, with our scalable pipeline, we plan to bring in domain-experts to replace the authors in the final stage of the pipeline, as shown in Figure 2, allowing us to scale up the data collection.

## 7 Conclusion

We introduced ActionAtlas v1.0 a new VideoQA benchmark for evaluating VLMs on action recognition in real-world specialized domains. To perform well on ActionAtlas, a model must be able to understand motions that span across many frames and track the individual(s) performing the action both temporally and spatially. We collected ActionAtlas using a robust and scalable pipeline that included both automatic filtering tools and techniques, such as lexical search, LLMs, and CLIP filtering, and manual filtering via crowd-workers and the authors. Results showed that many open models perform at best close to random chance, implying that while these models excel in existing video language downstream tasks, they fall short in accurately understanding complex actions and nuanced movements in videos. Proprietary models such as GPT-4o showed better performance, improving with higher frame sampling rate, but still far from achieving high accuracy on our task.