# Causal discovery from observational and interventional data across multiple environments

Adam Li

Department of Computer Science, Columbia University

adam.li@columbia.edu

Amin Jaber

Synlico Inc.

amin.jaber@synlico.com

&Elias Bareinboim

Department of Computer Science, Columbia University

eb@cs.columbia.edu

###### Abstract

A fundamental problem in many sciences is the learning of causal structure underlying a system, typically through observation and experimentation. Commonly, one even collects data across multiple domains, such as gene sequencing from different labs, or neural recordings from different species. Although there exist methods for learning the equivalence class of causal diagrams from observational and experimental data, they are meant to operate in a single domain. In this paper, we develop a fundamental approach to structure learning in non-Markovian systems (i.e. when there exist latent confounders) leveraging observational and interventional data collected from multiple domains. Specifically, we start by showing that learning from observational data in multiple domains is equivalent to learning from interventional data with unknown targets in a single domain. But there are also subtleties when considering observational and experimental data. Using causal invariances derived from do-calculus, we define a property called S-Markov that connects interventional distributions from multiple-domains to graphical criteria on a selection diagram. Leveraging the S-Markov property, we introduce a new constraint-based causal discovery algorithm, S-FCI, that can learn from observational and interventional data from different domains. We prove that the algorithm is sound and subsumes existing constraint-based causal discovery algorithms.

## 1 Introduction

Causal discovery is the process of learning cause-and-effect relationships between variables in a given system, which is many times the final goal of the data scientist or a necessary step towards a more refined causal analysis [1; 2]. The learning process typically leverages constraints from data to infer the corresponding causal diagram. However, it is common that the data constraints do not uniquely identify the full diagram. Therefore, the target of analysis is often an equivalence class (EC) of causal diagrams that encodes constraints found in the data (implied by the underlying unknown causal system).

An EC encodes invariances in the form of graphical constraints, and thus is used to represent all causal diagrams that encode those constraints and invariances. Formal characterizations of ECs areimportant to understand the output of a learning algorithm and how it relates to the underlying causal system the scientist aims to explain.

ECs are defined with respect to distributional invariances which are implied by the structure of the graph. For example, conditional independences (CI) are implied by d-separations in the causal graph. Hence, it is desirable to formally characterize the EC in the general setting where we have interventional data from multiple domains. A complete graphical characterization would enable i) an efficient representation of the distributional invariances in the data and ii) the ability to translate these data-invariances to graphical constraints (e.g. d-separation).

An early example of an EC when only observational data is available in a single domain is the Markov equivalence class (MEC). The MEC characterizes causal diagrams with the same set of d-separation statements over observed nodes . Given interventional (i.e. experimental) data, one can reduce the size of the equivalence class . In the case of known intervention targets, the EC is known as the \(\)-MEC  and in the case of unknown targets, it is called the \(\)-MEC .

In prior research, domain-changes and interventions were treated similarly . Nevertheless, various examples across scientific disciplines highlight their distinction (see Table 1). For instance, when extrapolating data-driven conclusions from bonobos to humans, consider Figure 1(b). Notably, the environment/domain, represented by the S-node pointing to \(X\), illustrates differences in kidney function between the species. When applying a CRISPR intervention to a gene linked to kidney protein production (\(X\)), researchers investigate the impact of medication (\(Y\)) on fluid balance in the body (\(Z\)). This intervention is explicitly different from the kidney-function differences between bonobos and humans because the change-in-domain is there regardless of whether or not an intervention is made. This differentiation between interventions and domains holds significance, especially in causal discovery. By leveraging invariances across observational and interventional data from both bonobos and humans, one can learn additional causal relationships. Moreover, conflating these qualitatively distinct settings is generally invalid, as pointed out in transportability analysis . Pearl and Bareinboim (2011) introduced clear semantics for S-nodes (environments), offering a unified representation in the form of a selection diagram.

In this paper, we investigate structure learning when mixtures of observational and interventional data (known and unknown targets) across multiple domains are available. The multi-domain setting has been analyzed from the lens of selection diagrams, where selection nodes (or S-nodes) encode distributional changes in the mechanisms, or exogenous variables due to a change in domain . We will show in this paper a characterization of the EC for selection diagrams. Generalizing the structure learning setting to multiple domains requires a formal treatment because it is a common scenario in the sciences ; see Table 1 for an example of different settings and related literature). For example, in single-cell sequencing analysis, scientists are interested in analyzing the causal effects of proteins on one another. However, they may typically collect observational and/or experimental data from multiple labs (i.e. domains) and wish to combine them into one dataset. Also, scientists may collect observational and experimental data over multiple species in order to learn more about one specific species, or the relationships among species .

The celebrated FCI algorithm and its variants learn a partial ancestral graph (PAG), an MEC of causal diagrams, given purely observational data . The \(\)-FCI (with known targets) and \(\)-FCI (with unknown targets) generalize these results to interventional data, and further reduce the size of the EC to an \(\)-PAG and \(\)-PAG, respectively . However, these algorithms operate in a single domain, or environment and do not account for combining known/unknown target interventions.

Various approaches have been proposed throughout the literature for causal discovery from multiple domains. The works in  assume Markovianity, a functional model (e.g. linearity) holds, and/or do not take into account arbitrary combinations of observational and interventional data with known and unknown targets. Alternatively, JCI pools data together and performs learning on the combined dataset . Pooling data is an incomplete procedure when considering interventional data within a single domain let alone multiple domains [Appendix D.2].

In this paper, we take a principled approach to the multi-domain structure learning problem and formally characterize S-PAGs, the object of learning. This paper introduces the selection-diagram FCI algorithm (S-FCI) that learns from a mixture of observational and interventional data from multiple domains to construct an EC of selection diagrams, an S-PAG. Specifically, we contribute the following:1. **Generalization of standard Markov properties** - We introduce the S-Markov property, which extends and generalizes the normal Markov, \(\)-Markov, and \(\)-Markov properties to the setting of multiple domains with arbitrary mixtures of observational and interventional data with known and unknown targets.
2. **Learning algorithm** - We develop a sound learning algorithm for learning an equivalence class of selection diagrams with observational and/or interventional data across different domains.1 
## 2 Preliminaries and Notation

Uppercase letters (\(X\)) represent random variables, lowercase letters (\(x\)) signify assignments, and bold ones (\(\)) indicate sets. The CI relation \(\) being independent of \(\) given \(\) is denoted as \(|\). The d-separation (or m-separation) of \(\) from \(\) given \(\) in graph \(G\) is expressed as \((|)_{G}\). \(G_{}\) depicts \(G\) with incoming edges to \(X\) removed, while \(G_{}\) omits all edges outgoing from \(X\). Conventionally, every variable is d-separated from the empty set, denoted as \((X\{\})_{G}\). Superscripts and subscripts will be dropped where feasible to simplify notation.

**Causal Bayesian Network (CBN):** Let \(P()\) be a probability distribution over a set of variables \(\), and \(P_{}()\) denote the distribution resulting from the _hard intervention_\(do(=)\), which sets \(\) to constants \(\). Let \(^{*}\) denote the set of all interventional distributions \(P_{}()\), for all \(\), including \(P()\). A directed acyclic graph (DAG) over \(\) is said to be a _causal Bayesian network_ compatible with \(^{*}\) if and only if, for all \(\), \(P_{}()=_{\{i|V_{i}\}}P(v_{i}| _{i})\), for all \(\) consistent with \(\), and where \(_{i}\) is the set of parents of \(V_{i}\)[41, 51, pp. 24]. Given that a subset of the variables are unmeasured or latent, \(G(,)\) will represent the causal graph where \(\) and \(\) denote the measured and latent variables, respectively, and \(\) denotes the edges. Following the convention in , for simplicity, a dashed bi-directed edge is used instead of the corresponding latent variables. CI relations can be read from the graph using a graphical criterion known as _d-separation_.

**Soft Interventions:** Under this type of interventions, the original conditional distributions of the intervened variables \(\) are replaced with new ones, without completely eliminating the causal effect of the parents. Accordingly, the interventional distribution \(P_{}()\) for \(\) is such that \(P^{*}(X_{i}|Pa_{i}) P(X_{i}|Pa_{i})\), \( X_{i}\), and factorizes as follows:

\[P_{}()=_{}_{\{i|X_{i}\}}P ^{*}(x_{i}|_{i})_{\{j|T_{j}\}}P(t_{j}|_{j})\] (1)

In this work, we assume no selection bias and solely consider soft interventions. In the presence of multiple domains, a selection diagram captures commonalities and differences between domains . Represented as \(G_{S}=(,})\), it extends a causal diagram by incorporating S-nodes and their edges. \(\) S-nodes, \(S^{i,j}\), indicate distribution changes across pairs among N domains, by pointing to nodes in \(\) whose mechanism is altered between domains i and j. An example is shown in Figure 1(a), where the S-node is pointing to \(X\), indicating that the distribution of X

    &  &  &  &  &  \\    & & \(\) & & & & & \\ 
1 & ✓ & x & x & Markov  &  &  \\
1 & ✓ & ✓ & x & I-Markov  &  &  \\
1 & ✓ & x & ✓ & \(\)-Markov  &  &  \\  k & ✓ & x & x & \(\)-Markov (Thm. 1) &  (Cor. 5) &  \\ k & ✓ & ✓ & ✓ & S-Markov (Thm. 2) & S-FCI (Thm. 3) &  \\   

Table 1: Summary of Markov property results, and related algorithms that learn the ancestral graph based on number of domains and types of interventional (interv.) data provided such as observational (obs.), and known (\(\)) and unknown (\(\)) targets. The last column indicates a brief survey of different fields in ecology, economics, genomics, neurosciences, neurology and medicine that attempt to answer questions at each level. The rows highlighted in ”red” are new concepts.

changes, or that of the latent variable of X is different across the two domains.2 Similarly, "F-nodes" are auxiliary nodes used in [1; 7; 54] to represent invariances with respect to interventions within the same domain. F-nodes in this paper when written as \(F_{X}^{i,j}\) means it intervenes on X and compares distributions from domains i and j. \(F_{X}^{i}\) means it compares distributions within domain i. Unlike interventions, domain-shifts potentially alter latent variable distributions or functional relationships and persist irrespective of whether or not external intervention occurs. Distinguishing these concepts enables S-node learning, vital for transportability analysis on ancestral graphs. Appendix Section E.4 elaborates on our distinctions from previous work [11; 13; 14; 36].

Let \(=\{S^{1,2},S^{1,3},...,S^{N-1,N}\}\) represent \(\) S-nodes for distribution changes across domain pairs. When \(i=j\), \(S^{i,j}=\), indicating there is no S-node for a single domain.

Multi-domain setupThe following objects are utilized repeatedly, and introduced here. Our notation borrows from  and the transportability literature .

1. **Domains**: \(=\{^{1},^{2},...,^{N}\}\) denotes a set of N domains.
2. **Intervention targets**: \(^{}=^{1}_{1},^{1}_{2},...,^{N}_{M}\) is an ordered tuple of sets of intervention targets, with different sets of intervention targets occurring within each of the N domains for a total of M intervention target sets. We will denote \(^{i}\) as the intervention targets associated with domain i.
3. **Distributions**: \(^{}= P^{1}_{1},P^{1}_{2}...,P^{N}_{M}\) is an ordered tuple of probability distributions that are available to learn from. Denote \(^{i}\) as the distributions associated with domain i. There is a one-to-one correspondence between \(\) and \(\), such that \(P^{i}_{j}\) is the distribution associated with targets \(^{i}_{j}\) in domain i.
4. **Known target indices**: \(\) is a vector of 1's and 0's indicating which sets of interventions are known-targets. \(:=1-\) represents therefore an index vector selecting the distributions and interventions with unknown targets. \(_{}\) and \(_{}\) denotes the set of distributions and intervention targets corresponding to the known target interventions.
5. **Causal diagram**: \(G=(,)\), is a shared diagram over the N domains.
6. **Selection diagram**: \(G_{S}=(,})\), extends G with the corresponding S-nodes and their edges to represent each pair of domains. Let \(_{S^{i,j}}\) denote the set of nodes that S-node \(S^{i,j}\) points to and \(}\) as the set of children for all S-nodes of \(G_{S}\).

\(^{i}\) denotes the ith domain set of variables \(\), and \(X_{i}\) indicates the ith variable within \(\). When discussing intervention targets, \(X^{i,(k)}_{i}\) refers to the jth variable with the kth mechanism change in domain i. For instance, \(X^{i,(k)},X^{i,(l)}_{i}\) represent two interventions with distinct mechanisms (k and l) on variable X in domain i. \(\{\}^{i}\) explicitly denotes the observational distribution for domain i and is by convention a "known-target". For concreteness, say \(=\{^{1},^{2},^{3}\}\) with \(= P^{1}_{1},P^{1}_{2},P^{1}_{3},P^{3}_{1}\), \(=\{\}^{1},\{X^{(a)}\}^{1},\{X,Y\}^{1},\{\}^{3}\), and \(=\). In words, there are three distributions available in domain 1: \(P^{1}_{1}\) is observational, \(P^{1}_{2}\) is known-target on X with a specific mechanism change and \(P^{1}_{3}\) is unknown-target that intervenes on X and Y simultaneously. In domain 3, \(P^{3}_{1}\) is observational. There are no distributions for domain 2.

## 3 Multi-domain Markov Equivalence Class

Before designing a learning algorithm, we must characterize what can be learned from the given selection diagram. This section explores ECs in a multi-domain setting with arbitrary mixtures of observational and interventional data. The following assumptions are made throughout the paper.

**Assumption 1** (Shared causal structure).: We assume that each environment shares the same causal diagram. That is the S-nodes do not change the underlying causal diagram. 

This means that the S-nodes do not represent structural changes such as when \(V_{i}\) has a different parent set across domains.3

**Assumption 2** (Observational data is present across domains).: We make the simplifying assumption that \(\{\}^{i},\  i[N]\), that is observational data is present in all domains.

This is a realistic assumption in many scientific applications highlighted in Table 1.4 Another assumption we make is that all soft interventions across domains are _distinct_.

**Assumption 3** (Distinct interventions across domains).: We assume all the interventions across different domains have unique mechanisms. That is, if \(X^{(m)}^{i}\) and \(X^{(n)}^{j}\), where \(i j\), then \(m n\). In words, \(X\) has different mechanisms across the two distributions \(P_{^{i}},P_{^{j}}\).

This is a realistic assumption that precludes the possibility that any interventions that occur in different domains result in the same exact mechanism. For example, even if medication is given to humans and bonobos, it is unrealistic to expect the intervention has the same mechanism of action in each domain. Next, we define an important operation when comparing two different intervention sets.

**Definition 3.1** (Symmetrical Difference Operator \(\) in Multiple Domains).: For two domains \(i,j\) (possibly \(i=j\)), given two sets of intervention targets, \(^{i}\) and \(^{j}\), let \(^{i}^{j}\) denote the symmetrical difference set such that \(X^{i}^{j}\) if \(X^{(k)}^{i}\) and \(X^{(k)}^{j}\) or vice versa. 

This operation identifies the set of variables with unique interventional mechanisms across two intervention targets and also tracks the domain ids. For example, given \(^{1}=\{X^{1},Y,Z\}^{1}\) and \(^{1}=\{X^{2},Y\}^{2}\), then \(^{1}^{1}=\{X,Z\}^{1,2}\). An implication of the above definition and Assumption 3 is that the symmetrical difference of two intervention target sets from two different domains is the union of all the variables in both sets since the mechanisms would be unique. For more details and discussion on the assumptions, see the Appendix.

### Multi-distributional invariances: interventions and change-of-domain

This section elaborates on exactly what type of distributional invariances we characterize in the so called S-Markov EC.

When given only observational data, the celebrated FCI algorithm uses invariances of the form \(P(|,)=P(|)\) within the same probability distribution \(P()\) to characterize the Markov EC . These invariances, or CI statements can be mapped to d-separation statements in the graphical model. The resulting learned object is the PAG, which represents the EC when only observational data is given within a single domain.

The works in  build upon the Markov EC to characterize the so called interventional Markov EC, which uses distributional invariances of the form \(P_{}(|)=P_{}(|)\). In words, these are conditional probabilities that remain invariant under different interventions. Importantly, this sort of invariance is markedly different from that of the CI statements, where only observational data is present, because one is now comparing probabilities across _different distributions_. These distributional invariances can be characterized graphically by the d-separation property when using an augmented graph with "F-nodes", which serve as graphical representations of the differences in distributions due to interventions. However, this body of work assumes that all the distributions, observational and interventional, are within the _same_ domain.

In this work, we generalize this setting and consider an input set of distributions from (possibly) different domains to characterize the S-Markov EC. We consider distributional invariances of the form \(P_{}^{i}(|)=P_{}^{j}(| )\) such that distributions could stem from different domains when \(i j\). Such an invariance implies that the conditional distribution of \(|\) remains the same across domains \(i\) and \(j\) under interventions on \(\) and \(\), respectively. Whenever \(i=j\), these invariances reduce to the ones considered in the interventional Markov EC. From this perspective, it is clear that multi-domain invariances generalize the invariances analyzed in observational and interventional data in a single-domain.

### S-Markov Property

Now, we are ready to generalize the previous Markov properties  to the case when observational, and known/unknown-target interventional distributions in multiple domains are available.

**Definition 3.2** (S-Markov Property).: Given the Multi-domain setup, \(\) satisfies the S-Markov property with respect to the pair \( G_{S},\) if the following holds for disjoint \(,,\):

1. For \(_{j}^{i}\): \(P_{j}^{i}(|,)=P_{j}^{i}(|)\) if \((|,)_{G_{S}}\)
2. For \(_{m}^{i},_{l}^{j}\): \(P_{m}^{i}(|)=P_{l}^{j}(|)\) if \((|_{}, \{S^{i,j}\})_{G_{S}_{}} ()}}\),

where \(=(_{m}^{i}_{l}^{j})\{S^{i,j}\}\), \(_{}=,= _{}\) and \(()\) are non-ancestors of \(\) in \(G_{S}\).

Let \(S_{}^{}(G_{S},)\) denote the set of distribution tuples that satisfy the S-Markov property with respect to \( G_{S},\) where \(\) denotes the known intervention targets. 

When there is only a single domain, \(=\{^{1}\}\), the first constraint reduces to standard d-separation on a causal diagram. The second condition is a generalization of the \(\)-Markov property characterization , extending conditional invariances to multiple domains. We illustrate the definition with the following two examples.

**Example 1**.: Consider the selection diagram in Figure 1(a) with two domains \(=\{^{1},^{2}\}\). Let \(= P_{1}^{1},P_{2}^{1},P_{1}^{2}\) be the result of the interventions \(^{}=\{1^{1},\{X\}^{1},\{\}^{2}\}\), \(=\{S^{1,2}\}\) be the set of S-nodes, and \(=\). First, we have \((Y X|S^{1,2})_{G_{S}}\) so the first constraint of Def. 3.2 is not applicable for any distribution. Second, we compare \(P_{1}^{1}(y|x)\) and \(P_{2}^{1}(y|x)\), where \(S^{1,1}=\) by convention and \(=\{X\}\). We have \((Y X)_{G_{S_{}}}\) and the invariance is not required. For \(P_{1}^{1}(y|x)\) and \(P_{1}^{2}(y|x)\), we have \((Y S|X)_{G_{S_{}}}\). Also, for \(P_{2}^{1}(y|x)\) and \(P_{1}^{2}(y|x)\), we have \((Y\{X,S\})_{G_{S_{}}}\). Hence, no invariances are required between those pairs of distributions. A similar argument can be made when comparing other probability terms across distributions. Therefore, \(\) satisfies the S-Markov property with respect to \( G_{S},\). 

**Example 2**.: Consider the setup from Ex. 1. We check if \(\) satisfies the S-Markov property relative to \( G_{S},^{}\) where \(^{}=\{1^{1},\{Y\}^{1}\{2\}^{2}}\). We compare \(P_{1}^{1}(X)\) and \(P_{2}^{1}(X)\) and we have \(K=(\{1^{1}\{Y\}^{1})=\{Y\}^{1}\). The separation \((X Y|S^{1,2})_{G_{}}\) holds true which implies the invariance \(P_{1}^{1}(X)=P_{2}^{1}(X)\), but \(P_{2}^{1}\) was generated from an intervention on \(X\) so the invariance is not satisfied. Hence, \(\) does not satisfy the S-Markov property with respect to \( G_{S},^{}\). 

Next, we use Def. 3.2 to define S-Markov equivalence as follows. In words, two pairs of selection diagrams and their corresponding sets of intervention targets \( G_{S},\) and \( G_{S}^{},^{}\) are S-Markov equivalent if they can induce the same set of distribution tuples.

**Definition 3.3** (S-Markov Equivalence).: Let \(\) and \(\) denote fixed sets of domains and indices of known intervention targets, respectively. Given selection diagrams \(G_{S},G_{S}^{}\) defined over \(\) and the corresponding intervention targets \(,^{}\), the pairs \( G_{S},\) and \( G_{S}^{},^{}\) are said to be S-Markov equivalent if \(S_{}^{}(G_{S},)=S_{}^{}(G_{S}^{ },^{})\). 

### Multi-domain observational data

S-nodes introduced through the lens of selection diagrams are augmentations of the causal graph to represent different domains and changes in distributions that may occur . As part of this augmented graph, S-nodes are graphically similar to F-nodes, which have been successfully used to represent interventions . F-nodes are utility nodes where each one is a parent to (each elementin) a symmetrical difference set, and they are used to represent invariances between interventional distributions. The significance of these F-nodes will be further emphasized in Section 3.4; more specifically, by Definition 3.5 and Proposition 1. Despite the similarity between F-nodes and S-nodes, it is worthy to distinguish S-nodes since many causal inference tasks, such as in transportability, rely on knowing the S-node structure . Before deriving the graphical characterization for the S-Markov equivalence class, we first focus on the setting where there is only observational data across different domains. We highlight that S-nodes can be exactly viewed as F-nodes constructed from interventions with unknown targets when there is only observational data to consider .

**Definition 3.4** (Corresponding Intervention Set).: Consider the Multi-domain setup. For a selection diagram \(G_{S}\) over N domains. \(_{S^{i,j}}=_{S^{1,2}},_{S^{1,3}},...,_{S^{N-1,N}}, i j[N]\) is an ordered tuple of the children of each S-node. The corresponding intervention set for \(}\) is \(^{1},^{2},...,^{N}\), such that \(^{i}^{j}=_{S^{i,j}}\) for all \(i j\). 

The corresponding intervention set is a set that simplifies our presentation of the following theorem.

**Theorem 1** (Equivalence of \(\) and S Markov property given multi-domain observational distributions).: Consider the Multi-domain setup. Let \(G_{S}\) be a selection diagram among N domains and G be the corresponding causal diagram without S-nodes. Let \(^{}=\{1\}^{1},...,\{N\}^{N}\) and \(=[1,1,...,1]\), such that for each of the N domains, there is only observational data. Let \(_{S}\) be the corresponding intervention set for \(}\). Let \(^{}\) be an arbitrary set of distributions generated by the corresponding interventions. \(^{}\) satisfies the S-Markov property with respect to \( G_{S},\) if and only if it satisfies the \(\)-Markov property with respect to \( G,_{S}\).5 

When given observations collected from multiple domains, it is equivalent to collecting distributions with unknown-target interventions. This coincides with other works, which treat different domains and interventions as the same . In this setting, S-nodes have a correspondence to the augmented graph's F-nodes in . In some sense, the change-in-domain can be viewed as "nature's" intervention on the causal system. However, this simplification is not warranted when we consider interventions that occur in different domains.

### Mixture of multi-domain observational and interventional data

Next, we analyze the general setting with multi-domain observational and interventional data. Def. 3.2 and 3.3 may be quite challenging to evaluate in practice since it involves surgically altering the selection diagram. One can leverage a graphical approach that encodes the symmetric differences of interventions using F-nodes .

**Definition 3.5** (Augmented selection diagram).: Consider the Multi-domain setup. Let the multiset \(\) be defined as \(=\{_{1},_{2},..._{k}\}=\{| _{m}^{i},_{l}^{j}_{m}^{i}_{l}^{j}=\}\). The augmented graph of \(G_{S}\) with respect to \(\) is denoted as \(Aug_{}(G_{S})\) and constructed as follows: \(Aug_{}(G_{S})=(\)S\(\)\(\),\(\)\(}\)\(\)), where \(=\{F_{i}^{j,k}\}^{j,k[N]}\) is the set of added F-nodes and \(=\{(F_{i}^{j,k},l)\}_{l_{i}}\) is the set of added F-node edges. 

The F-nodes graphically encode the symmetrical difference sets between every pair of intervention targets in \(^{}\) (i.e. \(_{m}^{i}_{l}^{j}\)) within and across the different domains in \(\). \(F_{k}^{t,i}=F_{k}^{i}\) denotes an F-node representing the kth symmetric difference of intervention targets within domain \(i\) and \(F_{k}^{i,j}\) denotes an F-node from comparing intervention targets between domains \(i\) and \(j\). The result is an augmented selection diagram with the original causal structure augmented with S-nodes, F-nodes, and their additional edges. For example, Figure 1(c) shows the augmented diagram of the selection diagram in Figure 1(a) with respect to \(^{}=\{1\}^{1},\{X\}^{1},\{2\}^{2}\). The significance of this construction follows from Proposition 1 where separation statements in the S-Markov definition are tied (shown to be equivalent, formally speaking) to ones in the augmented selection diagram, with no need to perform any graphical manipulations.

**Proposition 1** (Graphical S-Markov Property).: Consider the Multi-domain setup. Let \(Aug_{}(G_{S})\) be the augmented graph of \(G_{S}\) with respect to \(\). Let \(_{i}^{j,k}=_{i}\{S^{j,k}\}\) be the union of the set of nodes adjacent to \(F_{i}^{j,k}\) and the corresponding S-node \(S^{j,k}\). The following equivalence relations hold for disjoint \(,,\), where \(_{i}=_{i}\) and \(=_{i}_{i}\).

\[(|,)_{G_{S}} (|,,)_{Aug_{ }(G_{S})}\] (2) \[(_{i}^{j,k}|_{i},\{S^{j,k}\})_{G_{S}_{i}, ()}} (\{F_{i}^{j,k},S^{j,k}\}|,F_{[k] \{i\}},\{S^{j,k}\})_{Aug_{}(G_{S})}\] (3)

The result in the above proposition is illustrated in the following example.

**Example 3**.: Consider the selection diagram in Fig. 1(b) with intervention targets \(=^{1},\{Z\}^{1},^{2}\). By Prop. 1, we can evaluate the S-Markov property in the corresponding augmented diagram in Fig. 1(d) without manipulating it. For example, \((Y Z)_{G_{S}}\) can be tested in Fig. 1(d) by \((Y F_{1}^{1}|,F_{z}^{1,2})_{Aug_{}(G_{S})}\) to determine if the invariance \(P^{1}(Y)=P^{1}_{Z}(Y)\) should hold. In addition, we can test if across-domain distributional invariances should hold. We have \((Y\!\{F_{z}^{1,2},S^{1,2}\}|\{X,Z\},F_{z}^{1})_{Aug_{}(G_{S})}\), then the invariance \(P^{1}_{Z}(Y|X,Z)=P^{2}(Y|X,Z)\) is not required. 

Maximal Ancestral Graphs (MAGs) provide a compact and convenient representation capable of preserving all the tested constraints in augmented selection diagrams which are represented by d-separations ; see also [59, p. 6]. This is formalized in Definition 3.6 and the construct is referred to as an _S-MAG_. The following example is provided for illustration.

**Definition 3.6** (S-MAG).: Given a selection diagram \(G_{S}\) and a set of intervention targets \(\), an S-MAG is the MAG constructed from \(Aug_{}(G_{S})\). That is MAG\((Aug_{}(G_{S}))\). 

**Example 4**.: Consider the selection diagram in Figure 1(a) and let \(=(\{\}^{1},\{X\}^{1},\{^{2}\}^{2})\). The corresponding augmented selection diagram \(Aug_{}(G_{S})\) is shown in Fig. 1(c). Finally, the corresponding S-MAG is \(MAG(Aug_{}(G_{S}))=\{X F_{x}^{1} Y,X  F_{x}^{1,2} Y,X S^{1,2} Y,X Y\}\). 

Finally, putting these results together, we derive a graphical characterization for two selection diagrams with corresponding sets of intervention targets to be S-Markov equivalent.

**Theorem 2** (S-Markov Characterization).: Let \(\) and \(\) denote fixed sets of domains and indices of known intervention targets, respectively. Given selection diagrams \(G_{S},G_{S}^{}\) defined over \(\) and the corresponding intervention targets \(,^{}\), the pairs \( G_{S},\) and \( G_{S}^{},^{}\) are S-Markov equivalent if and only if for \(M=MAG(Aug_{}(G_{S}))\) and \(M^{}=MAG(Aug_{^{}}(G_{S}^{}))\):6

1. \(M\) and \(M^{}\) have the same skeleton;
2. \(M\) and \(M^{}\) have the same unshielded colliders; and,
3. If a path \(p\) is a discriminating path for a node \(Y\) in both \(M\) and \(M^{}\), then \(Y\) is a collider on the path in one graph if and only if it is a collider on the path in the other. 

Theorem 2 states that the pairs \( G_{S},\) and \( G_{S}^{},^{}\) are S-Markov equivalent if their corresponding S-MAGs satisfy the corresponding three conditions, as illustrated in the example below.

**Example 5**.: Consider the tuples \( G_{S},^{}\) from Example 1 and \( G_{S},^{}\) from Example 2. S-MAGs \(M=MAG(Aug_{}(G_{S}))\) is shown in Ex. 4 and \(M^{}=MAG(Aug_{^{}}(G_{S}))=\{F_{y}^{1} Y,F_{y}^{1,2}  Y,X S^{1,2} Y,X Y\}\). Therefore, \(M_{1}\) and \(M_{2}\) have differing skeletons and thus are not S-Markov equivalent. 

Next, we leverage this characterization to devise an algorithm to learn the corresponding equivalence class of a true, underlying selection diagram.

## 4 Causal Discovery From Multiple Domains

We investigate in this section how to learn an EC of selection diagrams from a mixture of observational and interventional data that is generated from multiple domains. The graphical characterization of S-Markov equivalence in Theorem 2 and the significance of ancestral graphs (MAGs) in deriving this result motivate the following definition of S-PAG.

**Definition 4.1** (S-Pag).: Consider the Multi-domain setup. Let \(M=MAG(Aug_{}(G_{S}))\) and let \([M]\) be the set of S-MAGs corresponding to all the tuples \( G^{}_{S},^{}\) that are S-Markov equivalent to \( G_{S},^{}\). The S-PAG for \( G_{S},^{}\), denoted \(\) is a graph such that:

1. \(\) has the same adjacencies as M and any member of [M] does; and
2. every non-circle mark (tail or arrowhead) in \(\) is an invariant mark in [M] (i.e. present in all the S-MAGs in [M]). 

S-MAGs generalize PAGs and \(\)-PAGs from the single-domain to the multiple-domain setting. The S-nodes and F-nodes are not so much "random variables" as they are graphical instruments that encode differences across domains and among interventional distributions in this equivalence class. Next, we introduce a generalization of c-faithfulness  that enables causal discovery from multi-domain data.

**Definition 4.2** (S-faithfulness).: Consider a selection diagram \(G_{S}\) over N domains. A tuple of distributions \(_{}_{^{}} S_{K}^ {}(G_{S},^{})\) is called s-faithful to \(G_{S}\) if the converse of each of the S-Markov conditions (Definition 3.2) holds. 

The new algorithm, called S-FCI is shown in Alg. 1. Due to space constraints, we only include the high-level algorithm here. The algorithm proceeds by first constructing the augmented graph using Alg. E.2, by adding S-nodes and F-nodes to represent every pair of domains and interventions. Then it uses hypothesis testing to learn invariances in the skeleton (Alg. E.3) and finally applies orientation rules (Alg. E.5). S-FCI learns the skeleton by mapping pairs of distributions in \(^{}\) to F-nodes, or S-nodes by testing for the distributional invariances discussed in Section 3.1. Def. 3.2 and Prop. 1 connect these invariances to graphical criterion, which allow us to reconstruct the skeleton of the causal diagram. Interventional distributions across domains are used to learn F-node structure, and

Figure 2: Example of S-FCI applied with \(=\{\}^{1},\{X\}^{1},\{\}^{2}\) and \(=\). The S-node representing domain-shift between domains 1 and 2 is the black square in (a).

whereas observational distributions across domains are used to learn S-node structure. Besides the standard FCI rules that apply in the absence of selection bias, the algorithm also applies the following rules R8'-9'.

**Rule 8' (Augmented Node Edges)** - We orient edges out of F-nodes.

**Rule 9' (Identifiable Inducing Paths)** - If \(F_{k}^{i,j}\) is adjacent to a \(Y H_{k}^{i,j}\) known-target node and we know that the intervention target is node X, one can orient \(X Y\) because the \(F_{k}^{i,j} Y\) is only present due to an inducing path between X and Y.

In Figure 2, the different stages of the S-FCI algorithm are shown. Next we prove the proposed S-FCI algorithm is sound.

**Theorem 3** (S-Fci Soundness).: Given \(\), let \(^{}\) be generated by some unknown tuple \( G_{S},^{}\) from domains \(\) with a corresponding selection diagram \(G_{S}\) and is s-faithful to the selection diagram \(G_{S}\). S-FCI algorithm is sound (i.e. every adjacency and orientation in \(_{}\), the S-PAG learned by S-FCI, is common for \(MAG(Aug_{}(G_{S}))\)). 

Next, we illustrate some subtleties between the S-FCI and related algorithms that say pool observational and interventional distributions, ignoring the domain change. The example is motivated from biomedical sciences, where interventions are commonly performed in different domains and the goal is to leverage all datasets for learning. A group of scientists are trying to determine the causal structure of a set of proteins, but leverage data across the lab and hospital setting. Different experiments are run in each setting and combined into a single dataset . We provide additional examples and commentary on the S-FCI subtleties in the Appendix.

**Example 6**.: Let \(G_{S}\) be a selection diagram as shown in Figure 3(a). Let \(=^{1},^{2}\) be the set of domains representing the lab (\(^{1}\)) and the hospital (\(^{2}\)). These are a tuple of distributions \(= P_{1}^{1},P_{1}^{2}\) with intervention targets \(^{}=\{\}^{1},\{Y\}^{1},\{\}^{2}\) and \(=\), where X represents some protein in the dataset.

In this example, let \(G_{S}\) be the true selection diagram as shown in Figure 3(a). Given the interventional and observational data, we may be tempted to use the \(\)-FCI algorithm and simply pool the observational data, while ignoring the domain differences , but this would learn the graph in Figure 3(b) with an incorrect orientation (shown as the red edge). This I-PAG only contains one F-node because there is only two distributions: i) the pooled observational data and ii) the data resulting from intervention on Y. Applying R9 of the \(\)-FCI algorithm incorrectly orients the edge \(X Y\). Thus, R9 of the \(\)-FCI algorithm is not sound when the domains are ignored [7; 44].

Figure 3(c) contains what S-FCI would recover. Intuitively, one should learn (c) instead of (b) because even though there is a change in distribution among X and Y, one cannot ascertain whether there is an inducing path from \(F_{y}^{1}\) to X, or a change in distribution due to the domain. 

## 5 Conclusions

In this paper, we introduced a generalized Markov property called S-Markov, which defines a new equivalence class (EC), the S-PAG, representing the constraints found across observational and experimental distributions collected from multiple domains. Building on this new characterization, we develop a causal discovery algorithm called S-FCI, which subsumes FCI, \(\)-FCI and \(\)-FCI, and accepts as input a mixture of observational and interventional data from multiple domains.

Figure 3: Causal graphs related to example 6 - selection diagram with an intervention at Y, and S-node pointing to X (a), the graph after applying \(\)-FCI without considering domain-changes (b) and the S-PAG learned by S-FCI (c).