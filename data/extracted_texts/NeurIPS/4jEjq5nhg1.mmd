# Operator Learning with Neural Fields: Tackling PDEs on General Geometries

Louis Serrano1, Lise Le Boudec*1, Armand Kassai Koupai*1, Thomas X Wang1,

**Yuan Yin1, Jean-Noel Vittaut2, Patrick Gallinari1,3**

1 Sorbonne Universite, CNRS, ISIR, 75005 Paris, France

2 Sorbonne Universite, CNRS, LIP6, 75005 Paris, France

3 Criteo AI Lab, Paris, France

{louis.serrano,lise.leboudec,armand.kassai,thomas.wang,yuan.yin}

jean-noel.vittaut,patrick.gallinari}@sorbonne-universite.fr

###### Abstract

Machine learning approaches for solving partial differential equations require learning mappings between function spaces. While convolutional or graph neural networks are constrained to discretized functions, neural operators present a promising milestone toward mapping functions directly. Despite impressive results they still face challenges with respect to the domain geometry and typically rely on some form of discretization. In order to alleviate such limitations, we present CORAL, a new method that leverages coordinate-based networks for solving PDEs on general geometries. CORAL is designed to remove constraints on the input mesh, making it applicable to any spatial sampling and geometry. Its ability extends to diverse problem domains, including PDE solving, spatio-temporal forecasting, and geometry-aware inference. CORAL demonstrates robust performance across multiple resolutions and performs well in both convex and non-convex domains, surpassing or performing on par with state-of-the-art models.

## 1 Introduction

Modeling physics dynamics entails learning mappings between function spaces, a crucial step in formulating and solving partial differential equations (PDEs). In the classical approach, PDEs are derived from first principles, and differential operators are utilized to map vector fields across the variables involved in the problem. To solve these equations, numerical methods like finite elements, finite volumes, or spectral techniques are employed, requiring the discretization of spatial and temporal components of the differential operators (Morton & Mayers, 2005; Olver, 2014).

Building on successes in computer vision and natural language processing (Krizhevsky et al., 2017; He et al., 2016; Dosovitskiy et al., 2021; Vaswani et al., 2017), deep learning models have recently gained attention in physical modeling. They have been applied to various scenarios, such as solving PDEs (Cai et al., 2021), forecasting spatio-temporal dynamics (de Berenac et al., 2019), and addressing inverse problems (Allen et al., 2022). Initially, neural network architectures with spatial inductive biases like ConvNets (Long et al., 2018; Ibrahim et al., 2022) for regular grids or GNNs (Pfaff et al., 2021; Brandstetter et al., 2022b) for irregular meshes were explored. However, these models are limited to specific mesh points and face challenges in generalizing to new topologies. The recent trend of neural operators addresses these limitations by modeling mappings between functions, which can be seen as infinite-dimensional vectors. Popular models like DeepONet (Lu et al., 2022) and Fourier Neural Operators (FNO) (Li et al., 2022b) have been applied in various domains. However, they still have design rigidity, relying on fixed grids during training and inference, which limits their use in real-world applications involving irregular sampling grids or new geometries. A variant ofFNO tailored for more general geometries is presented in (Li et al., 2022), but it focuses on design tasks.

To overcome these limitations, there is a need for flexible approaches that can handle diverse geometries, metric spaces, irregular sampling grids, and sparse measurements. We introduce CORAL, a COordinate-based model for opeRAtor Learning that addresses these challenges by leveraging implicit neural representations (INR). CORAL encodes functions into compact, low-dimensional latent spaces and infers mappings between function representations in the latent space. Unlike competing models that are often task-specific, CORAL is highly flexible and applicable to various problem domains. We showcase its versatility in PDE solving, spatio-temporal dynamics forecasting, and design problems.

Our contributions are summarized as follows:

* CORAL can learn mappings between functions sampled on an irregular mesh and maintains consistent performance when applied to new grids not seen during training. This characteristic makes it well-suited for solving problems in domains characterized by complex geometries or non-uniform grids.
* We highlight the versatility of CORAL by applying it to a range of representative physical modeling tasks, such as initial value problems (IVP), geometry-aware inference, dynamics modeling, and forecasting. Through extensive experiments on diverse datasets, we consistently demonstrate its state-of-the-art performance across various geometries, including convex and non-convex domains, as well as planar and spherical surfaces. This distinguishes CORAL from alternative models that are often confined to specific tasks.
* CORAL is fast. Functions are represented using a compact latent code in CORAL, capturing the essential information necessary for different inference tasks in a condensed format. This enables fast inference within the compact representation space, whereas alternative methods often operate directly within a higher-dimensional representation of the function space.

## 2 Related Work

Mesh-based networks for physics.The initial attempts to learn physical dynamics primarily centered around convolutional neural networks (CNNs) and graph neural networks (GNNs). Both leverage discrete convolutions to extract relevant information from a given node's neighborhood Hamilton (2020). CNNs expect inputs and outputs to be on regular grid. Their adaptation to irregular data through interpolation (Chae et al., 2021) is limited to simple meshes. GNNs work on irregular meshes (Hamilton et al., 2017; Velickovic et al., 2018; Pfaff et al., 2021) and have been used e.g. for dynamics modeling (Brandstetter et al., 2022) or design optimization (Allen et al., 2022). They typically select nearest neighbors within a small radius, which can introduce biases towards the type of meshes seen during training. In Section 4.2, we show that this bias can hinder their ability to generalize to meshes with different node locations or levels of sparsity. Additionally, they require significantly more memory resources than plain CNNs to store nodes' neighborhoods, which limits their deployment for complex meshes.

Operator learning.Operator learning is a burgeoning field in deep learning for physics that focuses on learning mappings between infinite-dimensional functions. Two prominent approaches are DeepONet (Lu et al., 2021) and Fourier Neural Operator (FNO; Li et al., 2021). DeepONet can query any coordinate in the domain for a value of the output function. However, the input function must be observed on a set of predefined locations, requiring the same observation grid for all observations, for training and testing. FNO is an instance of neural operators (Kovachki et al., 2021), a family of approaches that integrate kernels over the spatial domain. Since this operation can be expensive, FNO addresses the problem by employing the fast Fourier transform (FFT) to transform the inputs into the spectral domain. As a consequence it cannot be used with irregular grids. Li et al. (2022) introduce an FNO extension to handle more flexible geometries, but it is tailored for design problems. To summarize, despite promising for several applications, current operator approaches still face limitations to extrapolate to new geometries; they do not adapt to changing observation grids or are limited to fixed observation locations. Recently, Li et al. (2023); Hao et al. (2023) explored transformer-based architectures as an alternative approach.

Spatial INRs.Spatial INRs are a class of coordinate-based neural networks that model data as the realization of an implicit function of a spatial location \(x f_{}(x)\)(Tancik et al., 2020; Sitzmann et al., 2020; Fathony et al., 2021; Lindell et al., 2022). An INR can be queried at any location, but encodes only one data sample or function. Previous works use meta-learning (Tancik et al., 2021; Sitzmann et al., 2020), auto-encoders (Chen and Zhang, 2019; Mescheder et al., 2019), or modulation (Park et al., 2019; Dupont et al., 2022) to address this limitation by enabling an INR to decode various functions using per-sample parameters. INRs have started to gain traction in physics, where they have been successfully applied to spatio-temporal forecasting (Yin et al., 2022) and reduced-order modeling (Chen et al., 2022). The former work is probably the closest to ours but it is designed for forecasting and cannot handle the range of tasks that CORAL can address. Moreover, its computational cost is significantly higher than CORAL's, which limits its application in real-world problems. The work by Chen et al. (2022) aims to inform the INR with known PDEs, similar to PINNs, whereas our approach is entirely data-driven and without physical prior.

## 3 The CORAL Framework

In this section, we present the CORAL framework, a novel approach that employs an encode-process-decode structure to achieve the mapping between continuous functions. We first introduce the model and then the training procedure.

### Problem Description

Let \(^{d}\) be a bounded open set of spatial coordinates. We assume the existence of a mapping \(^{*}\) from one infinite-dimensional space \( L^{2}(,^{d_{a}})\) to another one \( L^{2}(,^{d_{u}})\), such that for any observed pairs \((a_{i},u_{i})\), \(u_{i}=^{*}(a_{i})\). We have \(a_{i}_{a}\), \(u_{i}_{u}\) where \(_{a}\) is a probability measure supported on \(\) and \(_{u}\) the pushforward measure of \(_{a}\) by \(^{*}\). We seek to approximate this operator by an i.i.d. collection of point-wise evaluations of input-output functions through a highly flexible formulation that can be adapted to multiple tasks. In this work, we target three different tasks as examples: \({}^{}\) solving an initial value problem, i.e. mapping the initial condition \(u_{0} x u(x,t=0)\) to the solution at a predefined time \(u_{T} x u(x,t=T)\), \({}^{}\) modeling the dynamics of a physical system over time \((u_{t} u_{t+ t})\) over a given forecasting horizon \({}^{}\) or

Figure 1: Illustration of the problem classes addressed in this work: Initial Value Problem (IVP) (a), dynamic forecasting (b and c) and geometry-aware inference (d and e).

prediction based on geometric configuration. At training time, we have access to \(n_{tr}\) pairs of input and output functions \((a_{i},u_{i})_{i=1}^{n_{tr}}\) evaluated over a free-form spatial grid \(_{i}\). We denote \(a|_{_{i}}=(a(x))_{x_{i}}\) and \(u|_{_{i}}=(u(x))_{x_{i}}\) the vectors of the function values over the sample grid. In the context of the initial value and geometry-aware problems, every sample is observed on a specific grid \(_{i}\). For dynamics modeling, we use a unique grid \(_{tr}\) for all the examples to train the model and another grid \(_{te}\) for testing.

### Model

CORAL makes use of two modulated INRs, \(f_{_{a},_{a}}\) and \(f_{_{a},_{a}}\), for respectively representing the input and output functions of an operator. While \(_{a}\) and \(_{a}\) denote shared INR parameters that contribute in representing all functions \(a_{i}\) and \(u_{i}\), the modulation parameters \(_{a_{i}}\) and \(_{u_{i}}\) are specific to each function \(a_{i}\) and \(u_{i}\). Given input/output INR functions representation, CORAL then learns a mapping between latent representations inferred from the two INRs' modulation spaces. The latent representations \(z_{a_{i}},z_{u_{i}}\) are low dimensional embeddings, capturing within a compact code information from the INRs' parameters. They are used as inputs to hypernetworks \(h_{a}\) and \(h_{u}\) to compute the modulation parameters \(_{a_{i}}=h_{a}(z_{a_{i}})\) and \(_{u_{i}}=h_{u}(z_{u_{i}})\). The weights of the input and output hypernetworks are respectively denoted \(w_{a}\) and \(w_{u}\).

CORAL proceeds in three steps: _encode_, to project the input data into the latent space; _process_, to perform transformations in the latent space; and _decode_, to project the code back to the output function space. First, the input function \(a\) is encoded into the small input latent code \(z_{a}\) using a spatial encoder \(e_{a}:^{d_{z}}\). Next, a parameterized model \(g_{}:^{d_{z}}^{d_{z}}\) is used to infer an output latent code. Depending on the target task, \(g_{}\) can be as simple as a plain MLP or more complex as for example a neural ODE solver (as detailed later). Finally, the processed latent code is decoded into a spatial function using a decoder \(_{u}:^{d_{z}}\). The resulting CORAL operator then writes as \(=_{u} g_{} e_{a}\), as shown in Figure 2. The three steps are detailed below.

_Encode_ Given an input function \(a_{i}\) and a learned shared parameter \(_{a}\), the encoding process provides a code \(z_{a_{i}}=e_{a}(a_{i})\). This code is computed by solving an inverse problem through a procedure known as _auto-decoding_, which proceeds as follows. We want to compress into a compact code \(z_{a_{i}}\) the information required for reconstructing the original field \(a_{i}\) through the input INR, i.e.: \( x_{i},f_{_{a},_{a_{i}}}(x)=_{i}(x)  a_{i}(x)\) with \(_{a_{i}}=h_{a}(z_{a_{i}})\). See Figure 2(a) in Appendix B for details.

The approximate solution to this inverse problem is computed as the solution \(e_{a}(a_{i})=z_{a_{i}}^{(K)}\) of a gradient descent optimization:

\[z_{a_{i}}^{(0)}=0\;;z_{a_{i}}^{(k+1)}=z_{a_{i}}^{(k)}-_{z_{a_{i}}^ {(k)}}_{_{i}}(f_{_{a},_{a_{i}}^{(k)}},a);_{a_{i}}^{(k)}=h_{a}(z_{a_{i}}^{(k)})0  k K-1\] (1)

where \(\) is the inner loop learning rate, \(K\) the number of inner steps, and \(_{_{i}}(v,w)=_{x_{i}}[(v(x)-w(x))^{2}]\) for a measure \(_{i}\) over \(\). Note that in practice, \(_{i}\) is defined through the observation grid

Figure 2: Inference for CORAL. First, the model embeds the input function \(a\) without constraints on the locations of the observed sensors into an input latent code \(z_{a}\), then infers the output latent code \(_{u}\) and finally predicts the output value \((x)\) for any query coordinate \(x\). For the grid \(\), we use the vector notation \(a|_{}=(a(x))_{x}\), \(|_{}=((x))_{x}\).

\(_{i},_{i}()=_{x_{i}}_{x}()\) where \(_{x}()\) is the Dirac measure. Since we can query the INRs anywhere within the domain, we can hence freely encode functions without mesh constraints. This is the essential part of the architecture that enables us to feed data defined on different grids to the model. We show the encoding flow in Appendix B, Figure 4.

ProcessOnce we obtain \(z_{a_{i}}\), we can infer the latent output code \(_{u_{i}}=g_{}(z_{a_{i}})\). For simplification, we consider first that \(g_{}\) is implemented through an MLP with parameters \(\). For dynamics modeling, in Section 4.2, we will detail why and how to make use of a Neural ODE solver for \(g_{}\).

DecodeWe decode \(_{u_{i}}\) with the output hypernetwork \(h_{u}\) and modulated INR and denote \(_{u}\) the mapping that associates to code \(_{u}\) the function \(f_{_{u},_{u_{i}}}\), where \(_{u_{i}}=h_{u}(_{u_{i}})\). Since \(f_{_{u},_{u_{i}}}\) is an INR, i.e. a function of spatial coordinates, it can be freely queried at any point within the domain. We thus have \( x,_{i}(x)=_{u}(_{u_{i}})(x)=f_{_{u}, _{u_{i}}}(x)\). See Figure 2(b) in Appendix B for details.

During training, we will need to learn to reconstruct the input and output functions \(a_{i}\) and \(u_{i}\). This requires training a mapping associating an input code to the corresponding input function \(_{a}:^{d_{x}}\) and a mapping associating a function to its code in the output space \(e_{u}:^{d_{x}}\), even though they are not used during inference.

### Practical implementation: decoding by INR Modulation

We choose SIREN (Sitzmann et al., 2020) - a state-of-the-art coordinate-based network - as the INR backbone of our framework. SIREN is a neural network that uses sine activations with a specific initialization scheme (Appendix B).

\[f_{}(x)=_{L}_{L-1}_{L-2} _{0}(x)+_{L},_{i}(_{i})=_{0}(_{i}_{i}+_{i}) \] (2)

where \(_{0}=x\) and \((_{i})_{i 1}\) are the hidden activations throughout the network. \(_{0}_{+}^{*}\) is a hyperparameter that controls the frequency bandwidth of the network, \(\) and \(\) are the network weights and biases. We implement shift modulations (Perez et al., 2018) to have a small modulation space and reduce the computational cost of the overall architecture. This yields the modulated SIREN:

\[f_{,}(x)=_{L}_{L-1}_{L-2} _{0}(x)+_{L},_{i}(_{i})=_{0}(_{i}_{i}+_{i}+ {}_{i})\] (3)

with shared parameters \(=(_{i},_{i})_{i=0}^{L}\) and example associated modulations \(=(_{i})_{i=0}^{L-1}\). We compute the modulations \(\) from \(z\) with a linear hypernetwork, i.e. for \(0 i L-1\), \(_{i}=_{i}z+_{i}\). The weights \(_{i}\) and \(_{i}\) constitute the parameters of the hypernetwork \(w=(_{i},_{i})_{i=0}^{L-1}\). This implementation is similar to that of Dupont et al. (2022), which use a modulated SIREN for representing their modalities.

### Training

We implement a two-step training procedure that first learns the modulated INR parameters, before training the forecast model \(g_{}\). It is very stable and much faster than end-to-end training while providing similar performance: once the input and output INRs have been fitted, the training of \(g_{}\) is performed in the small dimensional modulated INR \(z\)-code space. Formally, the optimization problem is defined as:

\[*{arg\,min}_{}_{a,u_{a},_{u }}\|g_{}(_{a}(a))-_{u}(u)\|^{2}\] (4) \[\;_{a}=*{arg\,min}_{_{a},e_{a }}_{a_{a}}(_{a} e_{a}(a),a)\] \[\;_{u}=*{arg\,min}_{_{a},e_{u }}_{u_{a}}(_{u} e_{u}(u),u)\]

Note that functions \((e_{u},_{u})\) and \((e_{a},_{a})\) are parameterized respectively by the weights \((_{u},w_{u})\) and \((_{a},w_{a})\), of the INRs and of the hypernetworks. In Equation (4), we used the \((e_{u},_{u})\) & \((e_{a},_{a})\) description for clarity, but as they are functions of \((_{u},w_{u})\) & \((_{a},w_{a})\), optimization is tackled on the latter parameters. We outline the training pipeline in Appendix B, Figure 5. During training, we constrain \(e_{u},e_{a}\) to take only a few steps of gradient descent to facilitate the processor task. This regularization prevents the architecture from memorizing the training set into the individual codes and facilitates the auto-decoding optimization process for new inputs. In order to obtain a network that is capable of quickly encoding new physical inputs, we employ a second-order meta-learning training algorithm based on CAVIA (Zintgraf et al., 2019). Compared to a first-order scheme such as Reptile (Nichol et al., 2018), the outer loop back-propagates the gradient through the \(K\) inner steps, consuming more memory as we need to compute gradients of gradients but yielding higher reconstruction results with the modulated SIREN. We experimentally found that using 3 inner-steps for training, or testing, was sufficient to obtain very low reconstruction errors for most applications.

## 4 Experiments

To demonstrate the versatility of our model, we conducted experiments on three distinct tasks (Figure 1): (i) solving an initial value problem (Section 4.1), (ii) modeling the dynamics of a physical system (Section 4.2), and (iii) learning to infer the steady state of a system based on the domain geometry (Section 4.3) plus an associated design problem in Appendix D. Since each task corresponds to a different scenario, we utilized task-specific datasets and employed different baselines for each task. This approach was necessary because existing baselines typically focus on specific tasks and do not cover the full range of problems addressed in our study, unlike CORAL. We provide below an introduction to the datasets, evaluation protocols, and baselines for each task setting. All experiments were conducted on a single GPU: NVIDIA RTX A5000 with 25 Go. Code will be made available.

### Initial Value Problem

An IVP is specified by an initial condition (here the input function providing the state variables at \(t=0\)) and a target function figuring the state variables value at a given time \(T\). Solving an IVP is a direct application of the CORAL framework introduced in Section 3.2.

**Datasets** We benchmark our model on two problems with non-convex domains proposed in Pfaff et al. (2021). In both cases, the fluid evolves in a domain - which includes an obstacle - that is more densely discretized near the boundary conditions (BC). The boundary conditions are provided by the mesh definition, and the models are trained on multiple obstacles and evaluated at test time on similar but different obstacles. \({}^{}\)**Cylinder** simulates the flow of water around a cylinder on a fixed 2D Eulerian mesh, and is characteristic of _incompressible_ fluids. For each node \(j\) we have access to the node position \(x^{(j)}\), the momentum \(w(x^{(j)})\) and the pressure \(p(x^{(j)})\). We seek to learn the mapping \((x,w_{0}(x),p_{0}(x))_{x}(w_{T}(x),p_{T}(x))_{x }\). \({}^{}\)**Airfoil** simulates the aerodynamics around the cross-section of an airfoil wing, and is an important use-case for _compressible_ fluids. In this dataset, we have in addition for each node \(j\) the fluid density \((x^{(j)})\), and we seek to learn the mapping \((x,w_{0}(x),p_{0}(x),_{0}(x))_{x}(w_{T}(x),p_{T }(x),_{T}(x))_{x}\). For both datasets, each example is associated to a mesh and the meshes are different for each example. For _Airfoil_ the average number of nodes per mesh is \(5233\) and for _Cylinder_\(1885\).

**Evaluation protocols** Training is performed using all the mesh points associated to an example. For testing we evaluate the following two settings. \({}^{}\)**Full**, we validate that the trained model generalizes well to new examples using all the mesh location points of these examples. \({}^{}\)**Sparse** We assess the capability of our model to generalize on sparse meshes: the original input mesh is down-sampled by randomly selecting 20% of its nodes. We use a train, validation, test split of 1000 / 100 / 100 samples for all the evaluations.

**Baselines** We compare our model to \({}^{}\)**NodeMLP**, a FeedForward Neural Network that ignores the node neighbors and only learns a local mapping \({}^{}\)**GraphSAGE**(Hamilton et al., 2017), a popular GNN architecture that uses SAGE convolutions \({}^{}\)**MP-PDE**(Brandstetter et al., 2022), a message passing GNN that builds on (Pfaff et al., 2021) for solving PDEs.

**Results.** We show in Table 1 the performance on the test sets for the two datasets and for both evaluation settings. Overall, CORAL is on par with the best models for this task. For the _Full_ setting, it is best on _Cylinder_ and second on _Airfoil_ behind MP-PDE. However, for the _sparse_ protocol, it can infer the values on the full mesh with the lowest error compared to all other models. Note that this second setting is more challenging for _Cylinder_ than for _Airfoil_ given their respective average mesh size. This suggests that the interpolation of the model outputs is more robust on the _Airfoil_ dataset, and explains why the performance of NodeMLP remains stable between the two settings. While MP-PDE is close to CORAL in the _sparse_ setting, GraphSAGE fails to generalize, obtaining worse predictions than the local model. This is because the model aggregates neighborhood information regardless of the distance between nodes, while MP-PDE does consider node distance and difference between features.

### Dynamics Modeling

For the IVP problem, in section 4.1, the objective was to infer directly the state of the system at a given time \(T\) given an initial condition (IC). We can extend this idea to model the dynamics of a physical system over time, so as to forecast state values over a given horizon. We have developed an autoregressive approach operating on the latent code space for this problem. Let us denote \((u_{0},u_{ t},...,u_{L t})\) a target sequence of observed functions of size \(L+1\). Our objective will be to predict the functions \(u_{k t},k=1,...,L\), starting from an initial condition \(u_{0}\). For that we will encode \(z_{0}=e(u_{0})\), then predict sequentially the latent codes \(z_{k t},k=1,...,L\) using the processor in an auto regressive manner, and decode the successive values to get the predicted \(_{k t},k=1,...,L\) at the successive time steps.

#### 4.2.1 Implementation with Neural ODE

The autoregressive processor is implemented by a Neural ODE solver operating in the latent \(z\)-code space. Compared to the plain MLP implementation used for the IVP task, this provides both a natural autoregressive formulation, and overall, an increased flexibility by allowing to forecast at any time in a sequence, including different time steps or irregular time steps. Starting from any latent state \(z_{t}\), a neural solver predicts state \(z_{t+}\) as \(z_{t+}=z_{t}+_{t}^{t+}_{}(z_{s})ds\) with \(_{}\) a neural network with parameters to be learned, for any time step \(\). The autoregressive setting directly follows from this formulation. Starting from \(z_{0}\), and specifying a series of forecast time steps \(k t\) for \(k=1,...,L\), the solver call \(NODESolve(_{},z_{0},\{k t\}_{k=1,...,L})\) will compute predictions \(z_{k t},k=1,...,L\) autoregressively, i.e. using \(z_{k t}\) as a starting point for computing \(z_{(k+1) t}\). In our experiments we have used a fourth-order Runge-Kutta scheme (RK4) for solving the integral term. Using notations from Section 3.2, the predicted field at time step \(k\) can be obtained as \(_{k t}= g_{}^{k}(e(u_{0})))\) with \(g_{}^{k}\) indicating \(k\) successive applications of processor \(g_{}\). Note that when solving the IVP problem from Section 4.1, two INRs are used, one for encoding the input function and one for the output function; here a single modulated INR \(f_{,}\) is used to represent a physical quantity throughout the sequence at any time. \(\) is then shared by all the elements of a sequence and \(\) is computed by a hypernetwork to produce a function specific code.

We use the two-step training procedure from Section 3.4, i.e. first the INR is trained to auto-decode the states of each training trajectory, and then the processor operating over the codes is learned through a Neural ODE solver according to Equation (5). The two training steps are separated and the codes are kept fixed during the second step. This allows for a fast training as the Neural ODE solver operates on the low dimensional code embedding space.

\[*{arg\,min}_{}_{u_{u},t(0,T )}\|g_{}(_{u}(u_{0}),t)-_{u}(u_{t})\|^{2}\] (5)

   Model &  &  \\   & _Full_ & _Sparse_ & _Full_ & _Sparse_ \\  NodeMLP & 1.48e-1 \(\) 2.00e-3 & 2.29e-1 \(\) 3.06e-3 & 2.88e-1 \(\) 1.08e-2 & 2.83e-1 \(\) 2.12e-3 \\ GraphSAGE & 7.40e-2 \(\) 2.22e-3 & 2.66e-1 \(\) 5.03e-3 & 2.47e-1 \(\) 7.23e-3 & 5.55e-1 \(\) 5.54e-2 \\ MP-PDE & 8.72e-2 \(\) 4.65e-3 & 1.84e-1 \(\) 4.58e-3 & **1.97e-1 \(\) 1.34e-2** & 3.07e-1 \(\) 2.56e-2 \\ CORAL & **7.03e-2 \(\) 5.96e-3** & **1.70e-1 \(\) 2.53e-2** & 2.40e-1 \(\) 4.36e-3 & **2.43e-1 \(\) 4.14e-3** \\   

Table 1: **Initial Value Problem** - Test results. MSE on normalized data.

#### 4.2.2 Experiment details

**Datasets** We consider two fluid dynamics equations for generating the datasets and refer the reader to Appendix A for additional details. \(\)**2D-Navier-Stokes equation** (_Navier-Stokes_) for a viscous, incompressible fluid in vorticity form on the unit torus: \(+u w= w+f\), \( u=0\) for \(x,t>0\), where \(=10^{-3}\) is the viscosity coefficient. The train and test sets are composed of \(256\) and \(16\) trajectories respectively where we observe the vorticity field for 40 timestamps. The original spatial resolution is \(256 256\) and we sub-sample the data to obtain frames of size \(64 64\). \(\)**3D-Spherical Shallow-Water equation** (_Shallow-Water_) can be used as an approximation to a flow on the earth's surface. The data consists of the vorticity \(w\), and height \(h\) of the fluid. The train and test sets are composed respectively of 16 and 2 long trajectories, where we observe the vorticity and height fields for 160 timestamps. The original spatial resolution is \(128\) (lat) \( 256\) (long), which we sub-sample to obtain frames of shape \(64 128\). We model the dynamics with the complete state \((h,w)\). Each trajectory, for both both datasets and for train and test is generated from a different initial condition (IC).

**Setting** We evaluate the ability of the model to generalize in space and time. \(\)**Temporal extrapolation:** For both datasets, we consider sub-trajectories of 40 timestamps that we split in two equal parts of size 20, with the first half denoted _In-t_ and the second one _Out-t_. The training-_In-t_ set is used to train the models at forecasting the horizon \(t=1\) to \(t=19\). At test time, we unroll the dynamics from a new IC until \(t=39\). Evaluation in the horizon _In-t_ assesses CORAL's capacity to forecast within the training horizon. _Out-t_ allows evaluation beyond _In-t_, from \(t=20\) to \(t=39\). \(\)**Varying sub-sampling:** We randomly sub-sample \(\) percent of a regular mesh to obtain the train grid \(_{tr}\), and a second test grid \(_{te}\), that are shared across trajectories. The train and test grids are different, but have the same level of sparsity. \(\)**Up-sampling:** We also evaluate the up-sampling capabilities of CORAL in Appendix C. In these experiments, we trained the model on a sparse, low-resolution grid and evaluate its performance on high resolution-grids.

**Baselines** To assess the performance of CORAL, we implement several baselines: two operator learning models, one mesh-based network and one coordinate-based method. \(\)**DeepONet**(Lu et al., 2021): we train DeepONet in an auto-regressive manner with time removed from the trunk net's input. \(\)**FNO**(Li et al., 2021): we use an auto-regressive version of the Fourier Neural Operator. \(\)**MP-PDE**(Brandsetter et al., 2022b) : we use MP-PDE as the irregular mesh-based baseline. We fix MP-PDE's temporal bundling to 1, and train the model with the push-forward trick. \(\)**DINO**(Yin et al., 2022) : We finally compare CORAL with DINO, an INR-based model designed for dynamics modeling.

#### 4.2.3 Results

   _{tr}_{te}\)} &  &  &  \\   & & _In-t_ & _Out-t_ & _In-t_ & _Out-t_ \\   & DeepONet & 4.72e-2 \(\) 2.84e-2 & 9.58e-2 \(\) 1.83e-2 & 6.54e-3 \(\) 4.94e-4 & 8.93e-3 \(\) 9.42e-5 \\  & FNO & 5.68e-4 \(\) 7.62e-5 & 8.95e-3 \(\) 1.50e-3 & 3.20e-5 \(\) 2.51e-5 & **1.17e-4 \(\) 3.01e-5** \\  & MP-PDE & 4.39e-4 \(\) 8.78e-5 & 4.46e-3 \(\) 1.28e-3 & 9.37e-5 \(\) 5.56e-6 & 1.53e-3 \(\) 2.62e-4 \\  & DINO & 1.27e-3 \(\) 2.22e-5 & 1.11e-2 \(\) 2.28e-3 & 4.48e-5 \(\) 2.74e-6 & 2.63e-3 \(\) 1.36e-4 \\  & CORAL & **1.86e-4 \(\) 1.44e-5** & **1.02e-3 \(\) 8.62e-5** & **3.44e-6 \(\) 4.01e-7** & 4.82e-4 \(\) 5.16e-5 \\   & DeepONet & 8.37e-1 \(\) 2.07e-2 & 7.80e-1 \(\) 2.36e-2 & 1.05e-2 \(\) 5.01e-4 & 1.09e-2 \(\) 6.16e-4 \\  & FNO + in- int. & 3.97e-3 \(\) 8.03e-4 & 9.92e-1 \(\) 3.26e-3 & n.a. & n.a. \\   & MP-PDE & 3.98e-2 \(\) 1.69e-2 & 1.31e-1 \(\) 5.34e-2 & 5.28e-3 \(\) 3.52e-4 & 2.56e-2 \(\) 8.32e-3 \\   & DINO & **9.99e-4 \(\) 6.71e-3** & 2.87e-3 \(\) 5.61e-3 & 2.20e-3 \(\) 1.06e-4 & 4.94e-3 \(\) 1.92e-4 \\   & CORAL & **2.18e-3 \(\) 6.88e-4** & **6.67e-3 \(\) 2.01e-3** & **1.41e-3 \(\) 1.39e-4** & **2.11e-3 \(\) 5.58e-5** \\   & DeepONet & 7.86e-1 \(\) 5.48e-2 & 7.48e-1 \(\) 2.76e-2 & 1.11e-2 \(\) 6.94e-4 & **1.12e-2 \(\) 7.79e-4** \\   & FNO + in- int. & 3.87e-2 \(\) 1.44e-2 & 5.19e-2 \(\) 1.10e-2 & n.a. & n.a. \\   & MP-PDE & 1.92e-1 \(\) 9.27e-2 & 4.73e-1 \(\) 2.17e-1 & 1.10e-2 \(\) 4.23e-3 & 4.94e-2 \(\) 2.36e-2 \\   & DINO & 8.65e-2 \(\) 1.16e-2 & 9.36e-2 \(\) 9.34e-3 & **1.22e-3 \(\) 2.05e-4** & 1.52e-2 \(\) 3.74e-4 \\   & CORAL & **2.44e-2 \(\) 1.96e-2** & **4.57e-2 \(\) 1.78e-2** & 8.77e-3 \(\) 7.20e-4 & 1.29e-2 \(\) 1.92e-3 \\   

Table 2: **Temporal Extrapolation** - Test results. Metrics in MSE.

Table 2 details the performance of the different models in a combined temporal and spatial evaluation setting. \(\)**General remarks:** CORAL demonstrates strong performance across all scenarios for both datasets. Only DINO exhibits similar properties, i.e., stability across spatial subsamplings and extrapolation horizon. We observe that all models performance degrade with lower sampling ratio. Also, as the models have been trained only on _In-t_ horizon, error accumulates over time and thus leads to lower performance for _Out-t_ evaluation. \(\)**Analysis per model:** Although achieving strong performance on some specific scenarios, DeepONet, FNO and MP-PDE results are dependent of the training grid, geometries or number of points. FNO, can only be trained and evaluated on regular grids while DeepONet is not designed to be evaluated on a different grid in the branch net. MP-PDE achieves strong performance with enough sample positions, e.g. full grids here, but struggles to compete on irregular grids scenarios in Navier-Stokes. \(\)**Inference Time:** We report in Appendix C, the inference time of the baselines considered. Despite operator methods have better inference time, CORAL is faster than mesh-free methods like DINO and MP-PDE. \(\)**Generalization across samplings:** Coordinate-based methods demonstrate robustness when it comes to changes in spatial resolution. In contrast, MP-PDE model exhibits strong overfitting to the training grid, resulting in a decline in performance. Although MP-PDE and DINO may outperform CORAL in some settings, when changing the grid, CORAL remains stable and outperforms the other models. See Appendix C for details.

### Geometry-aware inference

In this section, we wish to infer the steady state of a system from its domain geometry, all other parameters being equal. The domain geometry is partially observed from the data in the form of point clouds or of a structured mesh \(_{i}_{i}\). The position of the nodes depends on the particular object shape. Each mesh \(_{i}\) is obtained by deforming a reference grid \(\) to adjust to the shape of the sample object. This grid deformation is the input function of the operator learning setting, while the output function is the physical quantity \(u_{i}\) over the domain \(_{i}\). The task objective is to train a model so as to generalize to new geometries, e.g. a new airfoil shape. Once a surrogate model has been trained to learn the influence of the domain geometry on the steady state solution, it can be used to quickly evaluate a new design and to solve inverse design problems (details in Appendix D).

**Datasets.** We used datasets generated from three different equations by Li et al. (2022) and provide more details in Appendix A. \(\)**Euler equation** (_NACA-Euler_) for a transonic flow over a NACA-airfoil. The measured quantity at each node is the Mach number. \(\)**Navier-Stokes Equation** (_Pipe_) for an incompressible flow in a pipe, expressed in velocity form. The measured quantity at each node is the horizontal velocity. \(\)**Hyper-elastic material** (_Elasticity_). Each sample represents a solid body with a void in the center of arbitrary shape, on which a tension is applied at the top. The material is the incompressible Rivlin-Saunders material and the measured quantity is the stress value. We use \(1000\) samples for training and \(200\) for test with all datasets.

**Baselines** We use \(\)**Geo-FNO**(Li et al., 2022) and \(\)**Factorized-FNO**(Tran et al., 2023) two SOTA models as the main baselines. We also compare our model to regular-grid methods such as \(\)**FNO**(Li et al., 2021) and \(\)**UNet**(Ronneberger et al., 2015), for which we first interpolate the input.

**Results** In Table 3 we can see that CORAL achieves state-of-the-art results on _Airfoil_ and _Elasticity_, with the lowest relative error among all models. It is slightly below Factorized-FNO and Geo-FNO on _Pipe_. One possible cause is that this dataset exhibits high frequency only along the vertical dimension, while SIREN might be better suited for isotropic frequencies. Through additional experiments, we demonstrate in Appendix D, how CORAL can also be used for solving an inverse problem

   Model & _NACA-Euler_ & _Elasticity_ & _Pipe_ \\  FNO & 3.85e-2 \(\) 3.15e-3 & 4.95e-2 \(\) 1.21e-3 & 1.53e-2 \(\) 8.19e-3 \\ UNet & 5.05e-2 \(\) 1.25e-3 & 5.34e-2 \(\) 2.89e-4 & 2.98e-2 \(\) 1.08e-2 \\ Geo-FNO & 1.58e-2 \(\) 1.77e-3 & 3.41e-2 \(\) 1.93e-2 & **6.59e-3 \(\) 4.67e-4** \\ Factorized-FNO & 6.20e-3 \(\) 3.00e-4 & 1.96e-2 \(\) 2.00e-2 & 7.33e-3 \(\) 4.66e-4 \\ CORAL & **5.90e-3 \(\) 1.00e-4** & **1.67e-2 \(\) 4.18e-4** & 1,20e-2 \(\) 8.74e-4 \\   

Table 3: **Geometric aware inference** - Test results. Relative L2 error.

corresponding to a design task: optimize the airfoil geometry to minimize the drag over lift ratio. This additional task further highlights the versatility of this model.

## 5 Discussion and limitations

Although a versatile model, CORAL inherits the limitations of INRs concerning the training time and representation power. It is then faster to train than GNNs, but slower than operators such as FNO, DeepONet and of course CNNs which might limit large scale deployments. Also some physical phenomena might not be represented via INRs. Although this is beyond the scope of this paper, it remains to evaluate the methods on large size practical problems. An interesting direction for future work would be to derive an efficient spatial latent representation for INRs, taking inspiration from grid-based representation for INRs (Takikawa et al. (2022), Muller et al. (2022), Saragadam et al. (2022)). Another avenue would be to leverage clifford layers (Brandstetter et al., 2022) to model interactions between physical fields.

## 6 Conclusion

We have presented CORAL, a novel approach for Operator Learning that removes constraints on the input-output mesh. CORAL offers the flexibility to handle spatial sampling or geometry variations, making it applicable to a wide range of scenarios. Through comprehensive evaluations on diverse tasks, we have demonstrated that it consistently achieves state-of-the-art or competitive results compared to baseline methods. By leveraging compact latent codes to represent functions, it enables efficient and fast inference within a condensed representation space.