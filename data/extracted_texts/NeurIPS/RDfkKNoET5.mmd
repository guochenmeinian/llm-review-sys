# Relative Representations:

Topological and Geometric Perspectives

 Alejandro Garcia-Castellanos

Amsterdam Machine Learning Lab

University of Amsterdam, Netherlands

a.garciacastellanos@uva.nl

&Giovanni Luca Marchetti

Department of Mathematics

KTH Royal Institute of Technology, Sweden

&Danica Kragic

Division of Robotics, Perception and Learning

KTH Royal Institute of Technology, Sweden

&Martina Scolamiero

Department of Mathematics

KTH Royal Institute of Technology, Sweden

Work done at the Division of Robotics, Perception, and Learning at KTH Royal Institute of Technology, Sweden.

###### Abstract

Relative representations are an established approach to zero-shot model stitching, consisting of a non-trainable transformation of the latent space of a deep neural network. Based on insights of topological and geometric nature, we propose two improvements to relative representations. First, we introduce a normalization procedure in the relative transformation, resulting in invariance to non-isotropic rescalings and permutations. The latter coincides with the symmetries in parameter space induced by common activation functions. Second, we propose to deploy topological densification when fine-tuning relative representations, a topological regularization loss encouraging clustering within classes. We provide an empirical investigation on a natural language task, where both the proposed variations yield improved performance on zero-shot model stitching.

## 1 Introduction and Related Work

The ability to infer semantically-rich representations is, perhaps, the cornerstone of the success of contemporary deep learning models . Latent spaces of deep neural networks extract features from data that are general and transferable, meaning that, after training a model on a given task, its representations can be leveraged upon for addressing a variety of related tasks. This principle is nowadays exploited in the form of _foundation models_ - neural networks trained via self-supervision on large-scale multi-task datasets to extract ready-to-use representations.

Surprisingly, it has been argued that neural networks trained on diverse tasks, architectures, and domains, infer structurally similar representations  - a hypothesis sometimes referred to as'representational universality' or 'convergent learning' . For example, there is extensive empirical evidence that the representations extracted by neural networks are isometric (up to scale) - i.e., are related by an affine distance-preserving transformation - as the hyperparameters and initialization vary . Even though partial theoretical explanations have been proposed - e.g., based on harmonic analysis  or on philosophical principles  - these phenomena remain mostly mysterious. Nonetheless, they have motivated the introduction of techniques for zero-shot transfer and latent space communication, such as _model stitching_ - a method consisting in a trainable layer that connects latent representations of two different networks. On a similar note, _relative representations_ involve a non-trainable isometry-invariant layer on top of the representation. Assuming therepresentational universality hypothesis, factoring out rigid transformations results in features that are independent of nuances in initialization, hyperparameters, and, to some extent, task and architecture.

In this work, we propose two improvements of relative representations, enhancing the resulting zero-shot model stitching. These improvements are inspired by insights from geometry and topology, respectively.

Geometric Perspective.From the geometric side, we consider symmetries of neural networks [18; 15], i.e., transformations of the weights that do not alter the function defined by the network. These transformations are sometimes referred to as _intertwiners_, and form a group that depends on the activation function. We argue that these symmetries are partially responsible for the universality of representations with respect to initialization and training noise, and consequently design a relative transformation that is invariant to the intertwiner group of common activations. Our idea is simple: we propose to deploy batch normalization before the relative representation layer. This factors out non-isotropic rescalings, which are the non-isometric transformations induced by intertwiner groups for a wide class of activation functions.

Topological Perspective.From the topological side, we draw inspiration from topological data analysis and, in particular, from methods for _topological regularization_[33; 21; 20; 10; 37; 9]. The latter aims at enforcing specific topological features in the latent representation of a neural network. To this end, a recently-introduced method deemed topological _densification_ forces data classes to be represented in compressed clusters, resulting in a representation that is coherent with the decision boundaries of the neural network. We propose to deploy topological densification in conjunction with relative representations, and explore various alternatives for combining the two. Our intuition is that consistent densified representations share a similar topology, and are therefore more universal, while still preserving generality and transferability. Moreover, topological regularization prevents potential overfitting of large pre-trained models - such as foundation models - when fine-tuning them via relative transformations in low data regimes.

We implement and validate empirically both of the above proposals in an experimental scenario similar to the original work . The scenario consists of a natural language task, where domains correspond to different languages, and model stitching enables zero-shot translation. Results show that both our variant of the relative transformation and the additional topological densification result in significantly improved performance with respect to the original version. In summary, our contributions are:

* A novel normalized variant of the relative representation that is invariant to the intertwiner group of common activation functions.
* The introduction of topological densification for fine-tuning relative representations.
* An empirical investigation on a natural language task, showcasing improved performance.

## 2 Background

### Relative Representations

In this section, we overview _relative representations_ - a technique enabling zero-shot model stitching. The core idea is introducing a transformation in latent spaces that increases compatibility, without the need for training additional components. As explained in Section 1, the approach is based on empirical evidence that latent representations inferred by neural networks are, usually, close to being isometric (up to scale).

Let \(\) be a representation, i.e., a map encoding data in \(\) to a latent space \(\). Moreover, let \(=\{a_{1},,a_{k}\}\) be a set whose elements are referred to as _anchors_, and let \(\) be a function representing a measure of similarity on \(\).

**Definition 2.1** ().: The _relative representation_ of \(z\) w.r.t. \(\) is

\[T^{}_{}(z)=((z,a_{1}),,(z,a_{k} ))^{k}\,.\]If \(=^{m}\) and \(=z z^{}/\|z\|\|z^{}\|\) is cosine similarity, then \(T_{}\) is invariant to linear isometries and rescalings. More precisely, if \(z\) and all the anchors in \(\) are transformed via \(z Uz\ -\) where \(\), and \(U\) is an orthogonal \(m m\) matrix - obtaining \(z^{}\) and \(^{}\), then \(T_{}^{}(z)=T_{}^{}(z^{})\).

Relative representations enable zero-shot model stitching as follows. If \(f_{1}=_{1}_{1}\) and \(f_{2}=_{2}_{2}\) are two neural networks with the same architecture decomposed in a representation map \(_{}\) and a head \(_{}\), then one can replace \(_{1}\) with \(_{2} T_{}^{}\) to exploit the representation inferred by \(f_{1}\) in conjunction with the head of \(f_{2}\), and vice versa. Assuming the representational universality hypothesis, this model stitching procedure extends to _cross-domain_ setups, i.e., for two networks \(f_{1}\) and \(f_{2}\) that are trained over two distinct datasets from the same semantic domain (see Figure 1). In this case, to obtain coherent anchors, we fix \(\) as an encoding via \(_{1}\) of some data, and exploit a known correspondence between the datasets - based on domain knowledge - to obtain anchors \(^{*}\) as encodings via \(_{2}\) of the corresponding data. Lastly, for our purposes it will be convenient to resort to an end-to-end training setup, similarly to . This consists in deploying pre-trained representation maps \(_{1},_{2}\), and training the networks \(_{1} T_{}^{}_{1}\) and \(_{2} T_{}^{^{*}}_{2}\) with the weights of both representation maps unfrozen, using their corresponding domain-specific dataset.

### Symmetry Groups of Activation Functions

In this section, we overview a recent work that investigates the symmetries in neural networks . The central idea is that certain activation functions induce similar symmetries in both weight space and latent representations. These symmetries generate the so-called _intertwiner group_.

Let \(_{n}\) be the groups of \(n n\) invertible matrices and \(\) a function. The latter represents the activation function of a neural network, and we will consequently extend it coordinate-wise as a map \(^{n}^{n}\).

**Definition 2.2**.: The _intertwiner group_ of \(\) is:

\[G_{}^{n}=\{A_{n}\ |\  B_{n}  A=B\}.\]

Suppose that \((I_{n})\) is invertible, and for each \(A_{n}\) define \(_{}(A)=(A)(I_{n})^{-1}\). It can be shown that, under the mild condition on \(\), \(G_{}^{n}\) is a subgroup of \(_{n}\) and \(_{} G_{}^{n}_{n}\) is a homomorphism such that \( A=_{}(A)\). Moreover, for common activation functions such as ReLU, GELU, and sigmoid, all the elements of \(G_{}^{n}\) can be decomposed as a product of a permutation matrix and a diagonal one . Since permutation matrices are isometries and isotropic diagonal matrices are (non-isotropic) rescalings, this draws a connection with the transformations factored out by the relative representations from Section 2.1.

The following elementary result shows that symmetries from the intertwiner group induce symmetries in the latent representations of a deep neural network. Let \(f(x,W)\) be a multi-layer perceptron with input \(x\), activation function \(\), and layer-wise weights \(W=(W_{1},b_{1},,W_{l},b_{l})\) with \(W_{i}^{n_{i} n_{i-1}}\) and \(b_{i}^{n_{i}}\). For each \(1 m l\), consider the decomposition \(f=_{m}_{m}\) into the latent representation at the \(m\)-th layer and the corresponding head.

**Proposition 2.1** ().: _For each \(1 i<l\) pick \(A_{i} G_{}^{n_{i}}\), and consider_

\[=(A_{1}W_{1},A_{1}b_{1},A_{2}W_{2}_{}(A_{1}^{-1}),A _{2}b_{2},,W_{l}_{}(A_{l-1}^{-1}),b_{l}).\]

_Then for each \(m\):_

\[_{m}(x,) =_{}(A_{m})_{m}(x,W),\] \[_{m}(x,) =_{m}(x,W)_{}(A_{m})^{-1}.\]

_In particular, \(f(x,)=f(x,W)\) for all \(x^{n_{0}}\)._

Figure 1: Cross-domain model stitching.

The above symmetries provide a theoretical explanation for the emergence of structurally-similar representations in networks with different initializations.

### Topological Densification

In this section, we recall the basics of hierarchical clustering, and review the topological densification method from . To this end, let \(\) be a metric space with distance function \(d_{ 0}\), and \(\) be a finite subset. In practice, \(\) will represent a dataset in an ambient space \(\), or in a representation.

**Definition 2.3**.: Given \(_{>0}\), the _truncation graph_\(_{}()\) is the finite undirected graph with elements of \(\) as vertices and an edge between \(x,y\) if, and only if, \(d(x,y)<\).

Connected components of the truncation graph can be interpreted as clusters of data. This is the idea behind density-based spatial clustering methods . Instead, _hierarchical_ density-based clustering  - as well as the related notion of \(0\)-th persistent homology - considers the evolution of clusters as the parameter \(\) varies. For \(=0\) all the points in \(\) form a distinct connected component. As \(\) grows, edges are added to the truncation graph. This implies that if \(\), each component in \(_{}()\) is contained in a component of \(_{}()\). Moreover, two distinct components in \(_{}()\) can merge together and become connected in \(_{}()\). The value of \(\) at which two connected components merge is called a _death time_. Since more than two connected components can merge at the same parameter value, death times form a multi-set in \(_{ 0}\) with multiplicity given by the number of merged components minus one. This multi-set is denoted by \(()\), and it is equivalent to the _persistence diagram_ of \(0\)-dimensional persistent homology. It can be shown that death times coincide exactly with the lengths of the edges of the minimum spanning tree of \(\), i.e., the connected tree with vertices \(\) and that is of minimal total edge length. This enables to compute the persistence diagram via, for example, Kruskal's algorithm , whose time complexity is \((||^{2}||)\).

The concept of connectivity presented above can be exploited to design an optimization objective for deep learning, encouraging the model to distribute data densely inside its corresponding decision boundaries. To this end, in the notation of Section 2.2, let \(\) be an encoder mapping data to a latent space \(\) equipped with a metric \(d\). In practice, we usually have \(=^{m}\), equipped with Euclidean metric. Moreover, suppose that data is subdivided into classes, meaning that there is a partition \(=_{1}_{K}\), where \(K\) is the number of classes and \(_{i}\) is the data in the \(i\)-th class.

**Definition 2.4** ().: Given a hyperparameter \(_{>0}\), the _topological densification loss_ is:

\[=_{i=1}^{K}_{w((_{i}))}|w-|.\]

For a Euclidean latent space \(=^{m}\), it can be shown that \(\) is differentiable w.r.t. \((_{i})\) almost everywhere, with an explicit expression for the derivatives. This allows to deploy gradient-based methods to minimize the topological densification loss over (the parameters of) \(\).

We remark that the content of this section can be extended to higher-dimensional _persistent homology_. The above definitions corresponds to an explicit construction of the \(0\)-dimensional persistent homology of the _Vietoris-Rips complex_, given that in this case all the birth times are \(0\). By considering \(i\)-th dimensional homology, it is possible to define persistence diagrams for every \(i 0\) - see Appendix (Section A). The topological loss can then be extended based on features of the points in the corresponding persistence diagrams. Moreover, other filtration constructions, such as the more efficient Witness complexes , can replace the Vietoris Rips complex. However, following , we focus on the \(0\)-dimensional Vietoris-Rips persistent homology for simplicity, leaving further investigation of higher-dimensional extensions and other filtrations for future work.

Figure 2: Effect of topological densification.

Method

### Robust Relative Transformation

In this section, we introduce a variation of the relative transformation that makes it invariant to the intertwiner group induced by common activation functions. This factors out non-isotropic rescalings in the relative representation, resulting in more robust model stitching. The idea behind our new relative representation boils down to introducing a Gaussian normalization with respect to a batch of data, i.e., a simple form of _batch normalization_ (without learnable parameters).

In the notation of Section 2.1, let \(=^{m}\) be a representation and \(=\{a_{1},,a_{k}\}\) be a set of anchors. Moreover, the following definition will depend on an additional set \(\), which in practice will correspond to the representation of a batch of data. For \(z\) we denote by \(^{}\) its Gaussian normalization w.r.t. \(\), obtained by subtracting to \(z\) the mean of \(\) and by dividing each component of \(z\) by the standard deviation of \(\) in the corresponding direction. Technically, we assume that the standard deviations of \(\) are non-vanishing, which is a generic condition.

**Definition 3.1**.: The _robust relative representation_ of \(z\) w.r.t. \(\) and \(\) is

\[T^{,}_{}(z)=T^{}^{}}_{}(^{}).\]

Intuitively, the introduction of the normalization transforms all the components of the latent space to the same canonical scale induced by \(\). Therefore, the robust version is invariant to scaled permutations and shifts, as shown by the following result.

**Proposition 3.1**.: _Suppose that \(\) is the cosine similarity, and consider an \(m m\) permutation matrix \(P\), a diagonal one \(D\), and a vector \(h^{m}\). Denote by \(^{},^{},z^{}\) the image of \(,,z\) via \(z DPz+h\), respectively. Then:_

\[T^{,}_{}(z)=T^{^{},^{}}_{}(z^{})\]

Proof.: Let \(\) be the mean of \(\) and \(\) be the diagonal matrix of its standard deviations in the corresponding components. Then, by definition, \(^{}=^{-1}(z-)\). The mean and standard deviations of \(^{}\) are \(DP+h\) and \(P P^{}D\), respectively. Denote \(=P^{}DP\). Then:

\[}^{^{}} =^{^{}}\] \[=(P P^{}D)^{-1}(DPz+h-(DP+h))\] \[=D^{-1}P^{-1}P^{}DP(z-)\] \[=D^{-1}P^{-1}(z-)\] \[=D^{-1}P^{-1}(z-) (^{-1})\] \[=P^{-1}(z-),\]

and similarly for \(^{^{}}}^{^{}}\) for every \(a_{i}\). Since the cosine similarity is invariant to linear isometries (i.e., orthogonal matrices), we obtain:

\[(^{^{}}}^{^ {}},\ ^{^{}})=(^{-1} (a_{i}-),\ ^{-1}(z-))=(^{ }},\ ^{}),\]

which implies the desired claim. 

As a consequence, the robust relative transformation is invariant to the intertwiner groups induced by common activation functions (e.g. GELU, ReLU, sigmoid) - see discussion after Definition 2.2. Note that this invariance property differs from the one satisfied by the original version of the relative transformation (Definition 2.1). The latter is invariant to isotropic rescalings - or, equivalently, to diagonal matrices with equal diagonal entries - and linear isometries. Therefore, our version trades off invariance to isometries other than permutations with more general non-isotropic rescalings. We claim that this tradeoff is advantageous in high dimensions. An arbitrary orthogonal matrix can be approximated by a permutation one, with the error decreasing as the dimension grows . Since latent spaces of contemporary deep learning models are typically high-dimensional, the robust relative transformation approximately exhibits, to an extent, invariance to arbitrary isometries, together with the added invariance to arbitrary non-isotropic rescalings. We empirically compare the robust version with the classical one as part of our experimental investigation - see Section 4.1.

### Topological Densification of Relative Representations

In this section, we explore various options and improvements for combining the topological densification loss (Definition 2.4) with (robust) relative representations. Since topological features of high-dimensional spaces encode semantic information , this additional regularization is expected to improve both the performance of the individual models, as well their zero-shot stitching capabilities, especially in low-data regimes. Moreover, to further improve the latent space similarity between models, we will apply a consistent regularization by using the same \(\) hyperparameter.

A fundamental choice is whether to apply the topological densification loss before or after the relative transformation. We refer to these setups as _pre-relative_ and _post-relative_, respectively. Figure 3 summarizes these options, including their combination consisting of regularizing both before and after the transformation. We will evaluate and compare all of these options empirically in Section 4.

Additionally, we consider the problem of constructing data batches for the topological densification loss. Batching is necessary for stochastic gradient descent and, in general, for efficiency. However, topological losses are notoriously subtle to implement in a batched fashion since, intuitively, persistent homology captures global features in data spaces, which cannot be extracted from those of the batches. The original work  suggests constructing batches where each data class is equally represented. This is implemented by sampling (with replacement) data from the same class. However, this approach can cause conflicts with other components of the model. In particular, it is incompatible with batch normalization, which is a fundamental ingredient in our robust relative transformation. To address these challenges, we introduce a novel batch construction method. The latter consists in aggregating sub-batches sampled from class-specific datasets, with an additional sub-batch sampled from the original dataset. This results in an efficient batch construction procedure that is compatible with both the topological densification loss and batch normalization. A more detailed description is provided in the Appendix (Section B).

## 4 Experiments

In this section, we provide an empirical investigation of our proposed improvements to relative representations. To this end, we train Transformer models on a natural language task. The task is split across languages, corresponding to different domains. The cross-domain stitching procedure enables to transfer between languages, i.e., it implements a form of zero-shot translation.

Data.We perform a random subsampling of 1% of the Amazon Reviews dataset , where the task consists in predicting user ratings in a range from \(1\) to \(5\) stars. We consider such task in two languages: English ('en') and French ('fr'). The anchor sets \(\) and \(^{*}\) (notation from Section 2.1) consist of translated texts. Specifically, we first select a number of anchors from the English dataset equal to the dimensionality of the latent space (\(768\) dimensions), and then translate them to other languages using Google Translate. A Python code implementing our models and experiments is available at a public anonymized repository: https://tinyurl.com/TopoRelTrans.

Figure 3: Different topological regularization setups for the relative transformation.

Models and Training.We use pre-trained RoBERTa models  as the base for our representation maps \(\), and a single layer on top of these representations for the head \(\). The models are fine-tuned end-to-end on the task described above via two approaches: the 'Absolute' case, where no relative transformation is applied during training, and the 'Relative' case, where the relative transformation is employed, as described at the end of Section 2.1. We train the models via the AdamW optimizer  with a layer-wise learning rate decay on the parameters of \(\). The initial learning rate is \(3.5 10^{-5}\) and decay rate of \(0.65\). The learning rate of \(\) is \(2 10^{-4}\). Training is performed over \(40\) epochs, with batch size of \(16\) and gradient accumulation every 6 steps. Additionally, we use a linear cyclic scheduler for the weights of the topological densification loss, which is a common strategy for optimizing combined objectives . For the relative case, we update the \(768\) anchor embeddings every \(500\) optimization steps. This reduces the computational cost, and has minor effects on training stability due to the learning rate schedule for \(\).

Evaluation Metrics.The performance of the models is evaluated using three metrics: the Mean Absolute Error (MAE) between user ratings seen as integers, the F\({}_{1}\) classification score, and the classification accuracy (Acc). All the scores are multiplied by \(100\) for better readability. As depicted in Figure 1, to test the stitching performance, we match the representation network's language with the test dataset's language, while the classification head is drawn from a network trained in the other language.

Our experimental setup is similar to the original work . However, in order to test the models in a more challenging fine-tuning scenario, we train our models on fewer data (only 1% of the Amazon Reviews dataset). Moreover, we deploy a simpler linear head \(\), instead of a deeper network. This naturally results in worse reported performance as compared to the original work.

### Comparison of Relative Transformations

In our first experiment, we compare the robust relative transformation (Section 3.1) with the original one, showcasing improved performance on the considered cross-domain stitching scenario. To this end, we experiment with both the original relative transformation ('Relative Vanilla') and our robust version ('Relative Robust'), together with the baseline ('Absolute') where no transformation is deployed.

Table 1 reports the results in terms of mean and standard deviation for \(5\) experimental runs. On the cross-domain setup - i.e., \(=\) and \(=\), and vice versa - our robust version significantly outperforms the original one on all the metrics considered. Specifically, when stitching from English to French, accuracy and F\({}_{1}\) increase by around \(9\) and \(19\) respectively, while MAE decreases by \(15\). The performance gain is even more drastic when stitching from French to English, with an increase of \(25\) and \(40\) in F\({}_{1}\) and accuracy, and a decrease of \(34\) in MAE. The larger improvements in the latter setup can be explained by the fact that the RoBERTa models are pre-trained better in the English language. This results in fine-tuned representations \(\) whose extracted features are, generally speaking, more transferable, mitigating the effect of the stitching technique deployed. The explanation is confirmed by the fact that the cross-domain performance without relative representations (Absolute column in the table) is better in the English-to-French setup than vice versa. Lastly, we remark that, surprisingly, the robust relative transformation exhibits slightly improved performance with respect to the Absolute version when the domain is not changed, i.e. when both \(\) and \(\) are set to either fr or en. This does not happen for the non-robust transformation. We conclude that not only our proposed transformation is advantageous to a large extent for transferring across domains, but the model can benefit from it even for the original task on low data regimes.

    & & & & & & & & Relative Vanilla & &  \\  \(\) & \(\) & Acc (\(\)) & F\({}_{1}\) (\(\)) & MAE (\(\)) & Acc (\(\)) & F\({}_{1}\) (\(\)) & MAE (\(\)) & Acc (\(\)) & F\({}_{1}\) (\(\)) & MAE (\(\)) \\   & en & \(59.26_{ 0.66}\) & \(58.27_{ 0.83}\) & \(49.52_{ 0.89}\) & \(38.84_{ 1.23}\) & \(23.50_{ 2.77}\) & \(84.95_{ 0.84}\) & \(60.84_{ 0.64}\) & \(60.30_{ 0.72}\) & \(45.35_{ 0.74}\) \\  & fr & \(24.28_{ 10.11}\) & \(22.27_{ 18.86}\) & \(139.27_{ 35.32}\) & \(40.96_{ 2.40}\) & \(31.15_{ 3.29}\) & \(73.09_{ 15.18}\) & \(99.92_{ 15.51}\) & \(50.13_{ 1.60}\) & \(57.56_{ 11.60}\) \\   & en & \(24.96_{ 0.27}\) & \(23.19_{ 1.82}\) & \(132.35_{ 24.01}\) & \(35.42_{ 1.16}\) & \(20.86_{ 1.09}\) & \(79.68_{ 11.68}\) & \(60.74_{ 0.88}\) & \(60.18_{ 11.14}\) & \(45.19_{ 11.16}\) \\   & fr & \(49.26_{ 1.04}\) & \(48.74_{ 0.73}\) & \(63.89_{ 1.50}\) & \(41.99_{ 3.18}\) & \(35.33_{ 4.55}\) & \(67.77_{ 22.24}\) & \(50.31_{ 0.88}\) & \(50.95_{ 0.82}\) & \(57.08_{ 1.22}\) \\   

Table 1: Performance comparison on zero-shot model stitching.

### Analysis of Topological Densification

In this section, we assess the benefits of the topological densification loss in the context of relative representations, as described in Section 3.2. In all the following experiments, we deploy our robust version of the relative transformation, which has been assessed in the previous section.

We start by comparing the three possible approaches for implementing the topological densification - see Figure 3. Initially, we have investigated the pre-relative and post-relative options which, however, had negative impact on the performance. For the pre-relative case, a possible explanation is that the relative transformation can fail to preserve the topology of latent data. An example of this is presented in Figure 4, where the representation consists of three clusters, two of which possess (approximately) collinear centroids. These two clusters are merged by the relative transformation, becoming indistinguishable. For the post-relative case, we hypothesize that the model discovers an anchor configuration that yields tighter clusters in the post-relative space, while preserving their spread in the pre-relative space. This is confirmed by examining the distribution of death times (see Figure 5), which exhibit a significantly smaller mean in the post-relative space than in the pre-relative one. We believe this configuration can lead to information bottlenecks, because compressing the clusters with this nonlinear transformation might cause a loss of expressiveness in the latent space.

We conclude that the best option is deploying the two topological regularization losses simultaneously, which we implement via a linear combination of them with two weights \(_{1}\) (pre-relative) and \(_{2}\) (post-relative). Through an extensive hyperparameter search, we discovered the optimal values of \(_{1}=2 10^{-3}\) and \(_{2}=1.8 10^{-2}\), along with a topological densification parameter \(=3\) for both the French and English datasets. With this setup, the pre-relative and post-relative distributions of death times overlap - see Figure 6. This addresses the above-mentioned challenge, and indicates that, with this setup, the relative transformation preserves the topological information.

Table 2 reports the performance scores when fine-tuning with topological regularization for \(5\) experimental runs. As compared to Table 1, a slight improvement is evident in the cross-domain context. Specifically, for the English-to-French setup, all the scores improve by around \(0.5\), while they improve by around \(1\) in the French-to-English setup. A similar increase can be observed when the domain is not changed. This shows that topological densification is beneficial for transferring between domains. However, it comes with a higher computational cost, as discussed in Section 2.3. This additional cost concerns only the training phase, while the zero-shot transfer procedure is unaffected.

Figure 4: Non-cluster-preserving relative transformation example

Figure 5: Distribution of death times when training with post-relative topological densification on the English dataset.

## 5 Conclusions, Limitations, and Future Work

In this work, we have introduced two improvements to relative representations. These improvements consist in a novel normalized version of the relative transformation, and in the deployment of a topological regularization loss in the fine-tuning procedure. We have investigated empirically our proposals on a natural language task, showcasing improved performance as compared to the original relative representations.

Our robust relative transformation is invariant to non-isotropic rescalings and permutations, which are the only symmetries of common activation functions. However, the original relative transformation is invariant to all the isometries (and isotropic rescalings) of the latent space. Even though trading off non-permutational isometries with non-isotropic rescalings is advantageous in high dimensions (see discussion after Definition 3.1), the challenge of designing a robust relative transformation that is invariant to all isometries and all rescalings remains open. This would lead to an even more robust zero-shot model stitching procedure, and therefore represents a fundamental challenge for future research.

Another future direction from the topological perspective is exploring topological regularization procedures beyond densification. This includes extensions of the latter to higher-dimensional persistent homology, as discussed at the end of Section 2.3 and in Section A, or alternative regularization losses - see  for an overview. The core challenges behind scaling up such regularizers are their computational complexity and the difficulty of batching. Yet, they might result in advantages for either general or specific domains when applying model stitching techniques.