# CS-Isolate: Extracting Hard Confident Examples

by Content and Style Isolation

Yexiong Lin\({}^{1}\) Yu Yao\({}^{2,3}\) Xiaolong Shi\({}^{1}\)

**Mingming Gong\({}^{4}\) Xu Shen\({}^{5}\) Dong Xu\({}^{6}\) Tongliang Liu\({}^{1}\)\({}^{*}\)**

\({}^{1}\)The University of Sydney; \({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence;

\({}^{3}\)Carnegie Mellon University; \({}^{4}\)The University of Melbourne;

\({}^{5}\)Alibaba DAMO Academy; \({}^{6}\)The University of Hong Kong.

Corresponding author: Tongliang Liu (tongliang.liu@sydney.edu.au).

###### Abstract

Label noise widely exists in large-scale image datasets. To mitigate the side effects of label noise, state-of-the-art methods focus on selecting confident examples by leveraging semi-supervised learning. Existing research shows that the ability to extract hard confident examples, which are close to the decision boundary, significantly influences the generalization ability of the learned classifier. In this paper, we find that a key reason for some hard examples being close to the decision boundary is due to the entanglement of style factors with content factors. The hard examples become more discriminative when we focus solely on content factors, such as semantic information, while ignoring style factors. Nonetheless, given only noisy data, content factors are not directly observed and have to be inferred. To tackle the problem of inferring content factors for classification when learning with noisy labels, our objective is to ensure that the content factors of all examples in the same underlying clean class remain unchanged as their style information changes. To achieve this, we utilize different data augmentation techniques to alter the styles while regularizing content factors based on some confident examples. By training existing methods with our inferred content factors, CS-Isolate proves their effectiveness in learning hard examples on benchmark datasets. The implementation is available at https://github.com/tmllab/2023_NeurIPS_CS-isolate.

## 1 Introduction

Large-scale machine learning datasets frequently contain noisy labels, as seen in datasets like ImageNet  and Clothing1M . Training deep neural networks with such noisy data would result in poor generalization ability, as these networks can memorize incorrect labels [13; 3].

To mitigate the side effects of label noise, different methods have been proposed [33; 13; 37; 61; 30; 31; 49]. Major existing state-of-the-art methods are based on confident examples selection [61; 2]. Intuitively, those methods first exploit the _memorization effect_, enabling deep neural networks to learn simple patterns shared by the majority of training examples [60; 1; 19]. Since clean labels typically constitute the majority in each noisy class [7; 39], deep neural networks initially fit the training data with correct labels and then progressively fit examples with incorrect labels . To prevent the model from fitting incorrect labels, early stopping is usually employed [44; 3; 50; 4]. Then the _small-loss trick_ is used to extract the confident examples with high certainty [21; 35; 28; 38]. If extracted examples have high quality, the performance of a classifier can be enhanced.

To improve the performance of a classifier, it is crucial to ensure that the confident examples have a distribution similar to that of clean data [36; 32]. Specifically, it is necessary to extract not only confident examples that are far from the decision boundary but those that are close to it. The former examples are easy to be identified and extracted. However, the latter examples, which are close to the decision boundary, often become entangled with mislabeled examples, making them challenging to be identified or extracted. In this paper, we discover that on image datasets, _the entanglement of unhelpful style factors with useful content factors_ is a key reason that leads to the hard examples becoming hard to be classified, and thus these examples are close to the decision boundary.

In Fig. 1, we provide an intuitive understanding of the relationship between hard examples and the entanglement of style factor \(Z_{s}\) and content factor \(Z_{c}\). Let us first assume the underlying content factor \(Z_{c}\) and style factor \(Z_{s}\) are given. In Fig. 0(a), by visualizing \(Z_{s}\) and \(Z_{c}\), we can observe a separation between the examples belonging to underlying clean classes \(0\) and \(1\). In real-world scenarios, however, \(Z_{c}\) and \(Z_{s}\) are not directly available. Instead, existing methods [13; 28] could learn representations \(Z_{1}\) and \(Z_{2}\) from noisy data to extract confident examples. These learned representations, \(Z_{1}\) and \(Z_{2}\), are nonlinear transformations of style factor \(Z_{s}\) and content factor \(Z_{c}\). They do not ensure the disentanglement of \(Z_{s}\) and \(Z_{c}\)[24; 20; 59]. As the example illustrated in Fig. 0(b), both \(Z_{1}\) and \(Z_{2}\) contain the information of the style factor \(Z_{s}\) and the content factor \(Z_{c}\). In the illustrated representation space, examples within the region encircled by red dashed lines are close together. The confident examples in this region are entangled with mislabeled examples, making them challenging to be extracted by existing methods without isolating (disentangling) style and content information in learned representations.

Figure 1: Illustrating the entanglement of content and style factors. The circles on the left side of the dotted line represent examples with the underlying clean label \(Y=0\), while the ones on the right side represent examples with clean label \(Y=1\). The blue filling corresponds to the noisy label \(=0\), and the yellow filling corresponds to the noisy label \(=1\). Black outlines indicate that the labels of examples are correct, whereas red outlines indicate that the labels of examples are incorrect. In (a), we visualize the underlying style factor \(Z_{s}\) and the underlying content factor \(Z_{c}\). (b) shows the impact of a nonlinear transformation on \(Z_{s}\) and \(Z_{c}\), leading to their entanglement in the new space defined by representations \(Z_{1}\) and \(Z_{2}\).

Figure 2: Illustrate the comparison of the performance of confident examples selection of our method CS-Isolate with DivideMix and Me-Momentum on CIFAR10N Worst . The highest points on AUCs are indicated by arrows. High precision indicates that most of the confident examples are correctly labeled, while recall indicates the fraction of correctly labeled examples out of all examples that are correctly labeled.

identical content factors, which is a self-supervised learning manner. This initial step could achieve a preliminary level of isolation between style and content factors. As the training process progresses, we can identify and select certain confident examples based on the content factors. Leveraging these confident examples, we further enhance the isolation process by encouraging all examples sharing the same label to share identical content factors. By isolating content and style factors, our method, CS-Isolate, effectively helps existing sample selection methods to extract hard examples typically overlooked by existing sample selection methods. In Fig 2. we evaluate the performance of confident examples selection on CIFAR10N with the noise type "worst"  by using two metrics: precision and recall. When comparing these two metrics, CS-Isolate consistently outperforms DivideMix  and Me-Momentum . This result demonstrates that our method can not only select confident examples more accurately, as shown by the higher precision, but also capture a larger portion of the correctly labeled examples from the entire dataset, as evidenced by the higher recall. By improving the quality of confident examples with the isolation of the content and style factors, our method effectively helps improve the test accuracy of existing methods. We have also theoretically analyzed the identifiability of content and style factors in Appendix A.

## 2 Background and Related Work

**Problem setup.** Let's denote \(\) as the distribution of a noisy example \((X,)\) from the set \(\{1,2,,C\}\), where \(X\) denotes the variable of instances, \(\) represents the variable of noisy labels, \(\) is the feature space, \(\{1,2,,C\}\) is the label space, and \(C\) is the total number of classes. In the learning scenario with noisy labels, clean labels are not observed. Given a noisy training sample \(}=\{x_{i},_{i}\}_{i=1}^{N}\), independently drawn from \(\), the objective is to leverage this sample \(}\) to learn a classifier robust against label noise.

**Sample selection methods for learning with noisy labels.** In learning with noisy labels, major current state-of-the-art (SOTA) methodologies predominantly involve sample selection strategies. These strategies seek to divide the dataset into confident and unconfident examples. The basis of these strategies is the exploitation of the _memorization effect_ inherent in deep neural networks. The memorization effect enables the networks to initially grasp and learn the simple patterns, and then learn the complex patterns gradually [60; 1]. Given that clean labels usually form the majority within each noisy class [7; 39], these networks would initially fit the examples with accurate labels, and subsequently fit the examples with incorrect labels over time .

Preventing the learning model from fitting incorrect labels is crucial to ensure sample selection quality. To achieve this, strategies such as early stopping are often employed [44; 3]. Additionally, the _small-loss trick_ is utilized to identify and extract confident examples with high certainty [28; 38; 51; 18]. Some variation has also been proposed, _e.g._, some methods opt to reweight examples, thereby decreasing the contribution of mislabeled samples to the overall loss [41; 13]. To guarantee the statistical consistency of algorithms, Jiacheng _et al._ introduces active learning to acquire the labels of randomly chosen examples from unconfident examples to mitigate the bias introduced by sample selection.

Moreover, the set of confident examples often undergoes dynamic changes during the training stage. This is achieved by leveraging semi-supervised learning methods to relabel training instances and reselect confident and unconfident examples using the small loss trick. Various techniques for this purpose have been proposed and empirically demonstrated superior performance, including consistency regularization  adopted by , MixMatch  used by , co-regularization by , and contrastive learning by [30; 29; 45; 8; 11; 57; 61]. Self-training and co-training have been used by  and [35; 13; 21], respectively.

**Representation learning by generative model.** Consider a data generation in Fig. 2(a), \(X\) is the observed data, and \(Z\) is the unknown underlying representation to generate \(X\). Variational autoencoder framework  can be used to learn the latent representation. In this process, a standard normal distribution is utilized as a prior for the latent variables, and a variational posterior \(q(Z|X)\) is employed to approximate the unknown underlying posterior \(p(Z|X)\). Disentangled representation is very important, which

Figure 3: The different data generative processes with or without auxiliary variables.

can allow a rich class of properties to be imposed on the learned representation, such as sparsity and clustering. To disentangle the representation, the variational autoencoder framework has been further expanded by modifying the original loss function, resulting in various algorithms. \(\)-VAE  suggests an adaptation framework that adjusts the weight of the KL term to balance the independence of disentangled factors and reconstruction performance. \(\)-TCVAE  further analyzes the KL term of \(\)-VAE  and only adjusts the total correlation term to achieve disentanglement. HOOD  uses the clean labels and domain labels to disentangle content and style factors, but the clean labels and domain labels are unknown in the setting of learning with noisy labels. These methods are toward the goal of disentanglement but do not have theoretical guarantees of identifiability of their inferred latent representations.

Disentangled latent factors via auxiliary variable. Recent studies [20; 23] show that theoretical guarantees of identifiability can be achieved if an auxiliary variable related to the representation can be obtained. Khemakhem _et al._ provide identifiability of iVAE with additional inputs by employing the theory of nonlinear Independent Component Analysis (nonlinear ICA) . Intuitively, given a factorized prior distribution over the latent variables that are conditional on an additional auxiliary variable \(U\), _i.e._, the class label and the time index in a time series, the latent factors are identifiable up to a certain degree. The data generation processing is changed to Fig. 2(b). To disentangle the representation, the method assumes that each factor of the representation is independent.

## 3 Content and Style Isolation When Learning with Noisy Labels

In this section, we present CS-Isolate, a method designed to help select hard confident examples by content and style isolation.

### Preliminaries

**Noisy data generative process.** To learn latent factors by leveraging generative models, the generative model has to model the noisy data generative process. Firstly, we introduce the noisy data generative process. We denote observed variables with gray circles and latent variables with white circles. Specifically, the content factor \(Z_{c}\) is generated by the latent label \(Y\). The different style domains \(U_{s}\) give rise to the different style factor \(Z_{s}\). Then, the image \(X\) is generated by the combined influence of the style factor \(Z_{s}\) and the content factor \(Z_{c}\). Noisy labels \(\) are then generated based on the image \(X\). In general cases, \(Z_{c}\) and \(Z_{s}\) can also have statistical or causal dependencies. We follow existing work that assumes the content factors are unchanged across different styles .

**Challenge in isolating content from styles without labels.** Isolating content from style without labels is a challenging task . We first introduce the assumptions made by existing methods, as well as difficulties that may be encountered in practice.

Existing methods [26; 23; 46] for isolating content from styles without labels assume that the data augmentation for controlling each style factor can be designed, which means that we can intervene (control) all style factors. For instance, the data augmentations for controlling rotation angle and scaling of images can be designed by using affine transformation . When we apply a data augmentation that rotates an image, the style factor for the rotation angle changes in the augmented image compared to the original. Similarly, when we scale an image, the style factor for scaling becomes different in the augmented image. If we have the data augmentation to control each style factor, then by training a generative model with the augmented images, the model can then identify style factors by comparing the changes in styles between the original and augmented images.

However, the major challenge lies in the fact that we generally cannot design sufficient data augmentations to control all style factors in an image. For instance, in the CIFAR-10 dataset , some pictures with the label "horse" contain a person. In this context, the "person" acts as a style factor. Existing data augmentations cannot control this, as they cannot remove the person from images easily. This simple example illustrates that it is generally impossible to control all style factors through data augmentations. Then the assumption required by existing methods usually is hard to satisfy.

Figure 4: The noisy data generative process.

The violation of the assumption leads to the learned representations for content containing style information.

Challenge in isolating content from style with noisy labels. We generally cannot design data augmentations that can control all style factors in an image. In this case, clean labels are essential to help isolate content from styles. Intuitively, by comparing the change of images with different clean labels that share the same styles, one can infer content factors used for classification. This achieves isolation of content and style. However, when dealing with noisy data, relying on labels becomes problematic as they contain label errors. Images with the same content factors can have different noisy labels, making it fail to infer content factors used for classification. This situation further complicates the task of isolating content from styles, highlighting the challenges posed by noisy labels.

### CS-Isolate for Extracting Hard Example

In this paper, we find that a key reason for some hard examples being close to the decision boundary is the entanglement of style factors with content factors. Intuitively, style factors render the learned representation of certain examples less discriminative, thereby making them close to the decision boundary. If content and style information can be isolated, many hard examples would be easily distinguished by ignoring the style information. This is because we have left only content information in the learned representation, making these representations far from the decision boundary.

Inspired by these findings, we propose CS-Isolate, which aims to isolate content from styles for extracting confident examples. To achieve isolation, we utilize self-supervised learning. Specifically, we ensure that original and augmented images maintain the same content factors despite having different style factors due to data augmentations. This achieves a preliminary level of isolating content from styles. As previously mentioned, it is generally impossible to manipulate all style factors through data augmentation. Some style factors remain uncontrolled and stay contained in the learned representations for content. To further encourage the isolation, labels of confident examples are used. Specifically, as training progresses, we identify and select confident examples whose labels are likely to be accurate. By harnessing the labels from these confident examples, we encourage the examples with the same label to have the consistent content factors, irrespective of stylistic variations. This further isolates content from uncontrolled styles. By leveraging different data augmentations and confident examples, our method can effectively separate the learned latent representations into two parts which exclusively contain either style or content information. Subsequently, classifier heads can be trained purely on the representations containing content information. This approach not only improves the identification of hard confident examples but also enhances classification performance.

Isolating content from styles using auxiliary variables.We follow existing work using the variational autoencoder with auxiliary variables  to isolate content from styles. Let \(U_{s}\) and \(U_{c}\) denote the auxiliary variables that control the style factor \(Z_{s}\) and content factor \(Z_{c}\), respectively. Specifically, by reconstructing an image with the supervision of auxiliary variables \(U_{s}\) and \(U_{c}\), we encourage the images with the same content information but different style information to have the same content factor \(Z_{c}\) but the different style factor \(Z_{s}\). This allows us to isolate content from styles within an image. Then only employing \(Z_{c}\) for selecting hard confident examples.

However, given only noisy data, we face the challenge of the unavailability of auxiliary variables \(U_{c}\) and \(U_{s}\). In response, we devise surrogates for these auxiliary variables. To find a surrogate for the style auxiliary variable \(U_{s}\), we employ different data augmentation techniques, each of which generates a unique style domain with distinct style factors. We then assign the style domain ID as the style auxiliary variable \(U_{s}\). To find a surrogate for the content auxiliary variable \(U_{c}\), we adopt the philosophy of self-supervised learning to assign a unique content ID as the content auxiliary variable \(U_{c}\) to each image. The images that have the same content ID are encouraged to have consistent content factors. Moreover, during the learning process, some confident examples can be extracted. To effectively leverage these examples in learning content factors, we reassign the content ID of confident examples belonging to the same class to be the same. This allows our model to learn consistent content factors for the images in the same class.

Constructing style auxiliary variables via data augmentations.For the learning of distinct content factors and style factors, it is essential to construct images from domains with different stylesby using data augmentations. Each image carries its corresponding domain ID as the style auxiliary variable \(U_{s}\). Let \(=\{A_{0},A_{1},A_{2},,A_{M}\}\) represents the set of data augmentations, and \(M\) is the number of data augmentations. Note that, for convenience, we use \(A_{0}\) to denote a data augmentation such that applying the data augmentation \(A_{0}\) to the image \(x\) does not change it at all, _i.e._, \(x=A_{0}(x)\).

When applying these diverse data augmentations to an \(x_{i}\) different augmented images are obtained with different style factors. For the augmented image \(x^{A_{i}}\) obtained by using the \(i\)-th data augmentations that come from the \(i\)-th style domain, we can assign its style ID to be \(i\). Specifically, for each image \(x\), each data augmentation \(A_{i}\) is applied, resulting in a set containing pairs of the augmented image and the corresponding style ID, _i.e._,

\[\{(x^{A_{i}}:=A_{i}(x),U_{s}^{(x^{A_{i}})}:=i)\;\; i\{0,...,M\}\},\]

where \(U_{s}^{x^{A_{i}}}\) is defined to be the style ID (which serves the style auxiliary variable) of the image \(x\) by applying \(i\)-th data augmentation. In this manner, for each image \(x\), we can generate a set of augmented images (\(\{x^{A_{i}}\}_{i=0}^{M}\)), including the original images and the ones with different style factors controlled by different style IDs.

Constructing content auxiliary variables in a self-supervised manner.To guide the generative model in learning the consistent content factor across images sharing the same content information, we aim to assign the same content ID (which serves as the content auxiliary variable) to images with the same content information. However, clean labels cannot be obtained in learning with noisy labels, meaning we cannot know which images have the same content information. To construct content auxiliary variables without clean labels, we adopt self-supervised learning. Specifically, we consider that different data augmentations applied to an image typically do not alter its content information. Hence, we can assign a unique content ID to each original image and its augmented versions. This assignment can be mathematically expressed as follows:

\[_{all}=\{(x_{j}^{A_{i}}:=A_{i}(x),U_{s}^{(x_{j}^{A_{i}})}:=i,U_{c}^ {(x_{j}^{A_{i}})}:=C+j)\;\; i\{0,...,M\}, j\{1,...,T\}\},\]

where \(T\) is the total number of distinct original images in the training data, \(U_{c}^{(x_{j}^{A_{i}})}\) represents the content ID of the \(j\)-th image in the training data after applying the data augmentation \(A_{i}\), and \(C\) is the number of classes.

Moreover, this initial assignment of content IDs can be refined by using confident examples, denoted as \(_{l}\). As the training process progresses, we can extract these confident examples by leveraging the small-loss trick . The content IDs are then further refined based on the labels of the examples within the confident examples. The refinement process can be described as:

\[U_{c}^{(x_{j}^{A_{i}})}:=y_{j}^{c}\;\;(x_{j},y_{j}^{c}) _{l}, i\{0,...,M\},\]

where \(y_{j}^{c}\) is the label of the confident example \((x_{j},y_{j}^{c})\). After this refinement process, confident examples in the same class will have the same content ID. Consequently, these refined content IDs could enable the model to learn consistent content factors for the images in the same class.

Encouraging style and content isolation for extracting hard example.After obtaining the auxiliary variable, we isolate content from styles by leveraging iVAE . Specifically, the prior distribution of the content factor \(Z_{c}\) is conditional on the auxiliary variable \(U_{c}\), _i.e._\(P_{_{c}}(Z_{c}|U_{c})\). Similarly, the prior distribution of the style factor \(Z_{s}\) is conditional on the auxiliary variable \(U_{s}\), _i.e._\(P_{_{s}}(Z_{s}|U_{s})\). The \(_{c}\) and \(_{s}\) are the learnable parameters of the distribution. The objective is to maximize the data likelihood which is as follows.

\[_{q_{D}}[p_{}(X|U_{c},U_{s})]=_{q_{D}} [_{z_{c},z_{s}}p_{}(X|z_{c},z_{s})p_{_{c}}(z_{c}|U_{c})p_{ _{s}}(z_{s}|U_{s})z_{s}z_{c}],\] (1)

where we use \(q_{D}\) to denote the empirical distribution of the training sample \(_{all}\). We use the variational inference method to approximate the underlying posterior distribution \(p_{}(Z_{c},Z_{s}|X,U_{c},U_{s})\). Specifically, two inference models (encoders) \(q_{_{c}}(Z_{c}|X)\) and \(q_{_{s}}(Z_{s}|X)\) are introduced to infer latent variables \(Z_{c}\) and \(Z_{s}\) respectively and model the distribution \(q(Z_{c},Z_{s}|X)\) that is used to approximate the distribution \(p_{}(Z_{c},Z_{s}|X,U_{c},U_{s})\). Therefore, the distribution \(q(Z_{c},Z_{s}|X)\) can be decomposed as follows:

\[q_{}(Z_{c},Z_{s}|X)=q_{_{s}}(Z_{c}|X)q_{_{s}}(Z_{s}|X).\]We learn the parameters \(\{_{c},_{s},_{c},_{c}\}\) by maximizing the evidence lower-bound \(\) for each example \((x,u_{c},u_{s})\). The corresponding loss is:

\[_{_{c},_{s},}_{ELBO} :=_{_{c},_{s},}_{q_{D}}[-_{(z_{c},z_{s}) q_{_{c},_{s}}(Z_{c},Z_{s}|x)}[ p_{}(x|z_{c},z_ {s}).\] \[.-KL(q_{_{c}}(z_{c}|x)||p_{_{c}}(z_{c}|u_{c}))-KL(q_ {_{s}}(z_{s}|x)||p_{_{s}}(z_{s}|u_{s}))]],\]

where \(KL\) is the Kullback-Leibler divergence. Intuitively, when the style changes, the content remains constant. The model has to infer the unchanged content factors for image reconstruction. Thus, the content factors can be isolated from style factors. The content factors can be identified up to the block-identifiable defined in .

Utilizing content information to extract hard examples.After minimizing the ELBO loss \(_{ELBO}\), we have learned representations \(_{c}\) and \(_{s}\) which exclusively contain either style or content information. Then we employ existing sample-selection-based methods solely focus on \(_{c}\) for confident example selection, a classification head \(f_{}:_{c}_{C-1}\) is introduced. The classification head maps the space of content factors to a \(C-1\) probability simplex, where \(C\) represents the number of classes.

In practice, we propose an _end-to-end_ approach to learn the classification head \(f_{}\) and learn to infer content factor for extracting hard confident examples. This approach simultaneously minimizes the evidence lower bound loss \(_{ELBO}\) and the loss of existing sample-selection-based methods by leveraging the Lagrangian method [22; 12]. The content IDs are refined dynamically during the learning process. The overall loss function \(_{all}\) is thus given as:

\[_{_{c},_{s},,}_{all}=_{_{c},_{s}, ,}[_{ssl}+_{ELBO}_{ELBO}+_{ref} _{ref}],\]

where \(_{ssl}\) denotes the loss of the sample selection method, which optimizes the parameters of the classification head \(\) and the content encoder \(_{c}\). The loss \(_{ref}\) is the cross-entropy loss on confident examples. The hyper-parameter \(_{ELBO}\) and \(_{ref}\) are used to control strength of \(_{ELBO}\) and \(_{ref}\), respectively. Here, we illustrate a concrete example of employing the inference model \(q_{_{c}}\) in conjunction with DivideMix  in an end-to-end implementation. For a detailed walkthrough, please refer to the pseudo-code provided in Appendix B and the original paper .

DivideMix  applies a Gaussian mixture model to enhance MixMatch  for confident example selection and classifiers training. In DivideMix, during each epoch of training, the data is divided into a set of confident examples \(_{l}\) and a set of unconfident examples \(_{u}\). The confident examples contain the sharpened soft labels mixed by their labels in datasets and predicted labels given by the classification head \(f_{}\). The unconfident examples contain the sharpened soft labels predicted by the classification head \(f_{}\). Then, the semi-supervised learning approach, MixMatch  is employed by transforming confident (\(_{l}\)) and unconfident (\(_{u}\)) samples into augmented confident (\(_{l}^{}\)) and unconfident (\(_{u}^{}\)) samples by a linearly mixing.

The overall loss function, composed of a confident sample loss, an unconfident sample loss, and a regularization term, \(\) loss and a classification loss, _i.e._,

\[_{all} =_{_{l}}+_{u} _{_{u}}+_{r}_{}}_{}+_{ELBO}_{ELBO}+_{ref} _{ref}.\]

Intuitively, \(_{_{l}}\) is a cross-entropy loss for the labeled examples; \(_{_{u}}\) is the mean squared error for the unlabeled samples; \(_{}\) is a regularization term to prevent the model from predicting all samples to belong to a single class. These three terms are defined as follows specifically.

\[_{_{l}} =-_{l}^{}|}_{x,y^{s}_{ l}^{}}_{i}y_{i}^{s}(f_{} q_{_{c}}(x)),\] \[_{_{u}} =_{u}^{}|}_{x,y^{s}_ {u}^{}}\|y^{s}-f_{} q_{_{c}}(x)\|_{2}^{2},\] \[_{} =_{i}(1_{l}^{ }|+|_{u}^{}|}_{x_{l}^{}+_{u}^{}}f_{} q_{_{c}}^{i}(x)),\]

where \(y^{s}\) is the sharpened soft label and \(q_{_{c}}^{i}\) denotes the \(i\)-th coordinate of its output on input \(x\). In Appendix B, we illustrate our method with other sample selection methods.

## 4 Experiments

In this section, we introduce the setting of our experiments and compare our experimental results with existing methods. Most of our experiments are left out in Appendix C due to the limited space.

### Experiment Setting

**Dataset and noise type.**  We evaluate our methods on three synthetic noise datasets FashionMNIST , CIFAR-10 , and CIFAR-100 , and two real-world label-noise datasets, CIFAR-10N  and Clothing1M . FashionMNIST includes 70,000 images of size \(24 24\), categorized into 10 classes with 60,000 for training and 10,000 for testing. Both CIFAR-10 and CIFAR-100 contain 50,000 training images and 10,000 test images; CIFAR-10 comprises 10 classes, while CIFAR-100 includes 100 classes. The image size in both CIFAR datasets is \(32 32 3\). To generate noisy labels for these clean datasets, we employ the instance-dependent noisy label generation methods proposed in . CIFAR-10N, a noisy version of CIFAR-10, includes five types of label noise: "worst", "aggregate", "random 1", "random 2", and "random 3", all annotated by humans. The noise rates are 40.21%, 9.03%, 17.23%, 18.12%, and 17.64%, respectively. Clothing1M contains 1 million images with real-world noisy labels for training and 10,000 images with clean labels for testing.

    & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\  Me-Moment & 0.881 \(\) 0.011 & 0.972 \(\) 0.002 & 0.962 \(\) 0.005 & 0.960 \(\) 0.003 & 0.948 \(\) 0.005 \\ DivideMix & 0.951 \(\) 0.003 & **0.989 \(\) 0.000** & 0.983 \(\) 0.000 & 0.983 \(\) 0.000 & **0.983 \(\) 0.000** \\  CS-Isolate-DM & **0.956 \(\) 0.001** & **0.989 \(\) 0.000** & **0.984 \(\) 0.000** & **0.985 \(\) 0.000** & **0.983 \(\) 0.000** \\   

Table 1: Precision of confident examples on CIFAR-10N.

    & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\  CE & 88.54 \(\) 0.32 & 84.22 \(\) 0.35 & 75.81 \(\) 0.26 & 62.45 \(\) 0.86 & 21.45 \(\) 0.70 \\ Co-Teaching & 91.21 \(\) 0.31 & 89.10 \(\) 0.29 & 80.96 \(\) 0.31 & 73.41 \(\) 0.78 & 28.04 \(\) 1.43 \\ Forward & 90.05 \(\) 0.43 & 86.27 \(\) 0.48 & 74.64 \(\) 0.26 & 60.21 \(\) 0.75 & 26.75 \(\) 0.93 \\ T-Revision & 91.58 \(\) 0.31 & 89.46 \(\) 0.42 & 76.15 \(\) 0.37 & 65.09 \(\) 0.37 & 27.23 \(\) 1.13 \\ BLTM & 91.20 \(\) 0.27 & 82.42 \(\) 1.51 & 77.50 \(\) 1.30 & 63.20 \(\) 4.52 & 35.67 \(\) 1.97 \\ CausalNL & 90.84 \(\) 0.31 & 90.01 \(\) 0.45 & 80.91 \(\) 1.14 & 79.08 \(\) 0.50 & 34.02 \(\) 0.95 \\ Me-Momentum & 92.85 \(\) 0.64 & 90.06 \(\) 0.51 & 90.86 \(\) 0.21 & 86.66 \(\) 0.91 & 58.38 \(\) 1.28 \\ DivideMix & 94.85 \(\) 0.15 & 92.28 \(\) 0.13 & 94.93 \(\) 0.15 & 94.16 \(\) 0.35 & 70.50 \(\) 0.25 \\  CS-Isolate-DM & **95.16 \(\) 0.07** & **94.40 \(\) 0.09** & **95.90 \(\) 0.10** & **95.54 \(\) 0.06** & **73.11 \(\) 0.36** \\   

Table 4: Means and standard deviations (percentage) of classification accuracy on CIFAR-10N.

    & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\  Me-Moment & 0.920 \(\) 0.026 & 0.969 \(\) 0.019 & 0.946 \(\) 0.021 & 0.958 \(\) 0.022 & 0.962 \(\) 0.018 \\ DivideMix & 0.966 \(\) 0.000 & 0.963 \(\) 0.000 & 0.975 \(\) 0.000 & 0.976 \(\) 0.000 & 0.977 \(\) 0.000 \\  CS-Isolate-DM & **0.980 \(\) 0.000** & **0.975 \(\) 0.002** & **0.980 \(\) 0.001** & **0.982 \(\) 0.001** & **0.982 \(\) 0.001** \\   

Table 2: Recall of confident examples on CIFAR-10N.

    & Worst & Aggregate & Random 1 & Random 2 & Random 3 \\  CE & 77.69 \(\) 1.55 & 87.77 \(\) 0.38 & 85.02 \(\) 0.65 & 86.14 \(\) 0.24 & 86.12 \(\) 0.16 \\ Co-Teaching & 82.04 \(\) 0.06 & 91.11 \(\) 0.10 & 89.61 \(\) 0.18 & 88.98 \(\) 0.11 & 89.49 \(\) 0.06 \\ Forward & 79.79 \(\) 0.46 & 88.24 \(\) 0.22 & 86.88 \(\) 0.50 & 86.14 \(\) 0.24 & 87.04 \(\) 0.35 \\ T-Revision & 80.48 \(\) 1.20 & 88.52 \(\) 0.17 & 88.33 \(\) 0.32 & 87.71 \(\) 1.02 & 87.79 \(\) 0.67 \\ BLTM & 68.21 \(\) 1.67 & 79.41 \(\) 1.00 & 78.09 \(\) 1.03 & 76.99 \(\) 1.23 & 76.26 \(\) 0.71 \\ CausalNL & 82.41 \(\) 0.24 & 90.43 \(\) 0.14 & 89.03 \(\) 0.02 & 89.06 \(\) 0.05 & 89.21 \(\) 0.13 \\ Me-Momentum & 84.21 \(\) 0.70 & 91.34 \(\) 0.16 & 89.51 \(\) 0.42 & 90.14 \(\) 0.28 & 89.62 \(\) 0.31 \\ DivideMix & 92.48 \(\) 0.16 & 94.11 \(\) 0.21 & 94.77 \(\) 0.15 & 94.79 \(\) 0.14 & 94.79 \(\) 0.15 \\  CS-Isolate-DM & **94.28 \(\) 0.07** & **95.34 \(\) 0.10** & **95.36 \(\) 0.14** & **95.34 \(\) 0.12** & **95.48 \(\) 0.14** \\   

Table 3: Means and standard deviations (percentage) of classification accuracy on FashionMNIST, CIFAR-10 and CIFAR-100.

[MISSING_PAGE_FAIL:9]

datasets, we employed instance-dependent noisy label generation methods, as proposed by . We experimented with noise rates of 0.2 and 0.4, denoted by IDN-0.2 and IDN-0.4 respectively. The experiment results are presented in Tab. 3, Tab. 4 and Tab. 5. Our proposed method outperforms existing methods in terms of test accuracy on both synthetic and real-world datasets containing label noise. Notably, as the noise rate increases, the performance gap between our method, CS-Isolate-DM, and the existing methods becomes more pronounced. This highlights the robustness and effectiveness of our approach in scenarios with higher levels of label noise.

### Hard Confident Examples Visualization

The proposed method is expected to select confident examples based on content factors rather than style factors. To analyze this, we use Grad-CAM  to visualize the regions used to select confident examples. The visualization results are shown in Fig. 5. The experiment is conducted on the real-world dataset CIFAR-10N, and the noise type is "worst". The experiment results demonstrate that our method can correctly focus on the object in images. Specifically, when there exist uncontrolled style factors, _e.g._, the person near the horse, the activation maps for CS-Isolate-DM can successfully focus on the right object used for classifying the horse instead of uncontrolled style factors. In contrast, the baseline method, DivideMix, focuses on the style factors that are not related to the class "horse" and fails to select these confident examples.

## 5 Conclusion

This paper is motivated by the fact that only focusing on content factors such as semantic information makes examples more discriminative. We, therefore, proposed a novel CS-Isolate approach to infer and isolate the content information for classification. This is achieved by leveraging variational inference and constructing auxiliary variables via data augmentation techniques to modify style factors while regularizing content factors using confident examples. By training existing sample-selection-based methods with our inferred content factors, CS-Isolate improves their effectiveness in learning hard examples and classification accuracy on different image datasets.

Figure 5: Grad-CAM visualizations of hard confident examples. CS-Isolate-DM successfully identifies these confident examples, but DivideMix does not. The activation map of CS-Isolate-DM predominantly highlights semantic objects, whereas DivideMix emphasizes non-object pixels.