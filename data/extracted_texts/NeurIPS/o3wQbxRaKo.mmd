# Epistemic Integrity in Large Language Models

Bijean Ghafouri

Equal contribution.

Shahrad Mohammadzadeh1

James Zhou

University of Southern California \({}^{2}\)McGill University \({}^{3}\)UC Berkeley

Pratheeksha Nair2,7

Jacob-Junqi Tian

Mayank Goel

kellin Pelrine2,7

[MISSING_PAGE_POST]

###### Abstract

Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration--where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty Large Language Models hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing and correcting this miscalibration, offering a path to safer and more trustworthy AI across domains.

## 1 Introduction

Large Language Models (LLMs) have markedly transformed how humans seek and consume information, becoming integral across diverse fields such as public health (Ali et al., 2023), coding (Zambrano et al., 2023), and education (Whalen and et al., 2023). Despite their growing influence, LLMs are not without shortcomings. One notable issue is the potential for generating responses that, while convincing, may be inaccurate or nonsensical--a long-standing phenomenon often referred to as "hallucinations" (Jo, 2023; Huang et al., 2023; Zhou et al., 2024).

A critical aspect of trustworthiness in LLMs is _epistemic calibration_--the alignment between a model's internal confidence in its outputs and the way it expresses that confidence through natural language. Misalignment between internal certainty and external expression can lead to users being misled by overconfident or underconfident statements, posing significant risks in high-stakes domains such as legal advice, medical diagnosis, and misinformation detection. While of great normative concern, how LLMs express linguistic uncertainty has received relatively little attention to date (Sileo and Moens, 2023; Belem et al., 2024).

Figures 4 and 5 in section A of the Appendix illustrate the issue of epistemic calibration providing insights into the operation of certainty in the context of human interactions with LLMs. We highlight the following key points in these figures:

* **Distinct Roles of Certainty:** Internal certainty and linguistic assertiveness have distinct functions within LLM interactions, underlining their unique contributions to how they shape individual beliefs.

* **Human access to LLM certainty:** Linguistic assertiveness holds a critical role as the primary form of certainty available to users. Unlike internal certainty, which remains hidden within the model's computational processes, linguistic assertiveness is directly perceivable and influences how users interpret the model's outputs.
* **Beyond Content:** Users retrieve more than just the content from an LLM's output. The style and assertiveness of the language used also play a significant role, shaping perceptions through the communication of certainty. This interaction between the model's output and its linguistic assertiveness is crucial for understanding the full impact on individual perceptions.

Several studies have explored the calibration of internal confidence in LLMs. For instance, Zhang et al. (2024) examine confidence calibration, proposing techniques to reduce hallucinations and enhance the model's ability to answer known questions while avoiding unknown ones. However, they overlook the role of _linguistic assertiveness_ and how external certainty can still lead to epistemic miscalibration even if internal confidence is addressed. Similarly, Ren et al. (2023) focus on factual knowledge and LLM behavior before and after retrieval-augmented generation (RAG). While they investigate internal confidence, they fail to frame miscalibration as an end-to-end issue involving both internal certainty and linguistic assertiveness, ignoring the interplay between model predictions and how confidence is expressed linguistically.

More recent studies aim to bridge the gap between internal confidence and linguistic assertiveness but still face considerable limitations. Mielke et al. (2022) explore epistemic calibration but use limited scoring scales to measure both assertiveness and confidence, restricting continuous assessments. Their models also rely on a narrow range of datasets, which limits their applicability across diverse domains. Zhou et al. (2024) address miscalibration using epistemic markers, but their method lacks real domain grounding and fails to consider the complexities of language. In contrast, our study overcomes these limitations by fine-tuning models for a broader scope of topics and domains, achieving state-of-the-art performance in assertiveness estimation.

This review of existing work on LLM calibration and confidence reveals several gaps that our research aims to address:

* **Lack of Integrated Approaches**: Previous studies address either internal certainty or linguistic assertiveness but rarely both simultaneously (Jiang et al., 2021). There is a need for comprehensive frameworks that integrate these aspects to ensure LLMs communicate accurately and responsibly.
* **Inadequate Assertiveness Measurement**: Existing methods for measuring linguistic assertiveness rely heavily on lexicon-based approaches (Pei and Jurgens, 2021) or subjective perceptions without adequate validation (Steyvers et al., 2024). These methods often lack contextual depth and fail to generalize across diverse domains.
* **Limited High-Stakes Evaluation**: Although some studies explore epistemic calibration, they cover a narrow range of topics and employ low-resolution measures of assertiveness, limiting their applicability in critical domains such as misinformation detection (Mielke et al., 2022).

To address these gaps, our paper provides:

* **A New Assertiveness Detection Model**: We train a new model using a diverse dataset to detect linguistic assertiveness, improving the accuracy relative to previous approaches by incorporating contextual nuance and aligning more closely with human perceptions. This approach also addresses limitations in lexicon-based methods, enhancing generalizability across domains.
* **Empirical Evidence of Epistemic Miscalibration**: Our work provides a comprehensive comparison between internal certainty and linguistic assertiveness, documenting instances of miscalibration across a broad range of domains. Our experiments reveal that LLMs frequently generate highly assertive explanations despite low internal certainty, which can mislead users.
* **Validation with Human Perception**: We conduct comprehensive surveys assessing human perceptions of LLMs' linguistic assertiveness, confirming that misalignment poses a real risk. This addresses the gap regarding alignment of computational measures with subjective human perceptions of language.

This study also presents a novel human-centered approach for developing a robust assertiveness scoring method, introducing a new dataset from multiple domains. To ensure reliability, we train and compare several models, identifying the best estimator for assertiveness based on accuracy and transferability across different contexts. After selecting the top-performing model, we validate the results using human-surveyed data from the LIAR dataset (Wang, 2017), which was coded by participants different from those who annotated the primary dataset. This comprehensive methodology enables us to thoroughly assess both the objective and subjective aspects of assertiveness in language model explanations.

Our findings reveal that when the model has low internal certainty, it generates explanations that are significantly over-assertive, meaning the language used implies a higher degree of certainty than warranted by the model's actual confidence or knowledge base. This miscalibration could lead users to misconstrate the model's judgments as more reliable than they actually are. Our results confirm a strong correlation between the GPT 4o's assertiveness scores and human perceptions of assertiveness, but a weak correlation between human perceptions and internal certainty, and an even weaker relationship between GPT 4o model assertiveness and internal certainty.1

## 2 Conceptually understanding certainty and assertiveness in natural language

### Certainty

To elucidate the challenges of epistemic calibration, it is essential to understand the concepts of certainty and assertiveness in natural language communication. These foundational notions underpin how information is conveyed by LLMs and interpreted by users.

Effective communication hinges on the accurate conveyance of certainty, enabling individuals and systems to assess the reliability of information. In human communication, speakers use linguistic cues to express their confidence levels, which listeners interpret to form judgments about the truthfulness and credibility of statements (Budescu & Wallsten, 1985; Clarke et al., 1992). Similarly, for LLMs, effectively conveying certainty is crucial to ensure users can trust and interpret the provided information accurately. In this section, we decompose _certainty_ into two key concepts: **Internal Certainty** and **External Certainty**. Misalignment between these two dimensions can potentially lead to the need for **Epistemic Calibration**, as discussed in Section 2.2.

#### 2.1.1 Internal Certainty

Internal certainty, also referred to as model confidence, represents the probability an LLM assigns to a particular output based on its internal computations and parametric knowledge from its training data. In tasks such as question-answering, internal certainty is often represented by the probability the model assigns to its selected response compared to alternative answers (Jiang et al., 2021; Hendrycks et al., 2021).

Internal Confidence EstimationHendrycks et al. (2021) introduce baseline methods to detect misclassifications by aligning model confidence with the true probability of correctness. Token-level analysis is a common approach to generate fine-grained uncertainty estimates by examining probabilities assigned to individual tokens (Jiang et al., 2021; Kuhn et al., 2023; Duan et al., 2024). Another method assesses variability across multiple outputs, where higher variability indicates greater uncertainty (Xiong et al., 2024). External classifiers can also predict uncertainty by analyzing both input data and the model's internal representations, offering a more comprehensive evaluation (Shrivastava et al., 2023). Recent work also focuses on making internal confidence more interpretable, using both numerical scores (Lin et al., 2022; Xiong et al., 2024) and qualitative expressions (Mielke et al., 2022; Zhou et al., 2023).

Challenges in Confidence AlignmentAs research on estimating the internal confidence of LLMs has increased, scholars have started to focus on model calibration--i.e., the alignment between predicted probabilities and actual correctness. Desai & Durrett (2020) find that pre-trained transformers such as BERT often exhibit poor calibration out-of-the-box, where their confidence estimates fail to correspond to actual correctness. Jiang et al. (2021) also explore methods to improve model calibration, such as temperature scaling, which adjusts predicted probabilities to better match actual outcomes.

Despite these advancements, internal confidence scores are still largely inaccessible to users. Without these scores, it becomes essential for models to effectively communicate uncertainty through external means--such as linguistic cues--to ensure users correctly interpret the model's output. This gap highlights the need for better alignment between internal certainty and external expressions of confidence, as misalignment can lead to epistemic miscalibration.

#### 2.1.2 External Certainty

External certainty refers to the level of confidence conveyed through the textual generation of an LLM, as interpreted by an external observer. This type of certainty reflects how assertive, definitive, or unambiguous the model's output appears, regardless of the underlying internal confidence scores generated during the prediction process (Mielle et al., 2022). A critical component of external certainty is **Linguistic Assertiveness**, which involves the use of linguistic markers--such as modal verbs, adverbs, and other cues--that signal varying degrees of confidence or uncertainty.

This gap between a model's internal certainty and its external linguistic expressions is particularly important to address, as users often rely on the model's language to gauge its confidence. Our work seeks to bridge this gap by analyzing how linguistic assertiveness in model outputs correlates with internal certainty, thereby contributing to the broader goal of epistemic calibration.

### Epistemic Calibration

Epistemic Calibration is the process of aligning a model's expressed confidence, conveyed through linguistic assertiveness, with its actual reliability or correctness. Achieving this alignment requires that the model's linguistic expressions match the probabilistic confidence it has in its predictions.

Figure 4 in Appendix A showcases two examples of varying epistemic calibration in LLM outputs. Both cases compute internal and external certainty scores (through linguistic assertiveness). The first example demonstrates high epistemic calibration, with closely aligned certainty scores, while the second shows low calibration, where the model is overconfident despite low internal certainty. We compute internal certainty using Rivera et al. (2024) and external certainty via our custom model, validated through human ratings, as outlined in Section 3.

## 3 Methods

### Datasets and Models

The dataset used for certainty calibration in this study includes the LIAR dataset, augmented with GPT-4's reassessment of political statements' veracity, to support the subsequent analysis. To improve assertiveness calibration beyond previous methods, we compile a new dataset of 800 data points from five diverse sources, including datasets from Anthropic (Durmus et al., 2024), Globe and Mail (Kolhatkar et al., 2020), Reddit Change My View (Wiegmann et al., 2022), LLaMA 3-8B responses on LIAR (Dubey et al., 2024), and Pei's assertiveness dataset (Pei and Jurgens, 2021). This dataset is annotated by expert coders, achieving an inter-coder agreement of 0.7. We evaluate multiple models for assertiveness scoring, including pre-existing models (Pei and Jurgens SciBERT), fine-tuned versions, and newly trained models (e.g., LLaMA fine-tuned with LoRA and GPT-4o). Model performance is then assessed using mean squared error (MSE), with the best models utilized for subsequent miscalibration experiments. Full details on dataset composition, annotation guidelines, and model performance are provided in Appendix D.

### Computing internal certainty and linguistic assertiveness

To estimate the internal certainty of the LLM, we use the method outlined in Rivera et al. (2024). The model provides an explanation for each misinformation classification and assigns an uncertainty score. The scores are calibrated on a validation set using Platt's method (Platt et al., 1999), ensuring consistency between the explanation and certainty value. This process eliminates variability due to different prompting techniques, allowing for a unified approach to certainty calibration. The robustness of this method is further validated in Appendix B.

To measure the linguistic assertiveness of LLM-generated explanations in the misinformation task, we use the best-performing model from Section 3.1. We then compare this assertiveness measure to the underlying certainty estimates obtained using the uncertainty quantification techniques described above. By analyzing the gap between the model's certainty and assertiveness, we quantify the degree of calibration in its linguistic expressions.

## 4 Results

Assertiveness Calibration ScoreFigure 1 is obtained from comparing the seven different methods of assertiveness quantification, evaluating on the test set of our datasets. We find that _GPT-4o fine-tuned with rounding_ (training on assertiveness scores rounded to one decimal point) achieves the highest accuracy in predicting human-annotated assertiveness scores. The margin of improvement over the approaches from the literature is very large, cutting MSE by more than a half. To validate the transferability of these results across different domains, we conduct an ablation study by training the model on only four of the five datasets, and subsequently testing the model on the excluded one (as opposed to a standard random split in Figure 1). As shown in Table 2 in the Appendix, GPT-4o fine tuned (with training assertiveness scores rounded to one decimal point) achieves the lowest average MSE. Thus, GPT-4o appears well-suited to capturing the linguistic nuances that contribute to perceived assertiveness, even in transfer settings, and we use it as our primary tool for measuring the assertiveness of generated explanations in the misinformation detection domain.

Certainty Calibration ScoreFigure 1(a) illustrates a comparison between the probability distributions of certainty scores derived from the certainty calibration method proposed by Rivera et al. (2024) and assertiveness scores from the best-performing model shown in Figure 1, applied to the LIAR misinformation dataset. Notably, while the certainty scores exhibit a wide variance, assertiveness scores are more concentrated toward the middle of the distribution. Additionally, Figure 1(b) reveals a low Spearman correlation (0.3) between the two sets of scores, indicating significant misalignment between certainty and assertiveness. We provide examples of both epistemically calibrated and uncalibrated explanations with varying levels of assertiveness in Appendix E. Furthermore, in Appendix F, we test whether strong assertions of uncertainty affect calibration, finding that even when controlling for these cases, the model remains heavily skewed towards over-assertiveness.

Figure 1: Comparison of assertiveness evaluation performance across models. The models are evaluated based on their Mean Squared Error (MSE) relative to the test set, which is a subset of our dataset. Among all models, the fine-tuned GPT-4o, trained with assertiveness scores rounded to one decimal point, achieved the lowest MSE, indicating the highest accuracy in predicting assertiveness.

## 5 Human perceptions of linguistic assertiveness

In the preceding sections, we provided empirical evidence highlighting the epistemic calibration problem. Our findings revealed a significant mismatch between the internal certainty and linguistic assertiveness of LLMs, especially in scenarios where their level of internal certainty is low (LLM exhibits high external assertiveness). However, for the epistemic calibration problem to be a strong normative issue, it is essential to establish that human (subjective) perceptions of linguistic assertiveness align with the assertiveness measurements obtained using our model. To address this critical validation step, we conducted an online survey to gather subjective assessments of assertiveness from 467 human respondents representative of a cross-section of the United States population. Participants were asked to evaluate the assertiveness of various explanations generated by GPT-4 in a misinformation classification task.

    & Overall & Low & Medium & High \\  Predicted assertiveness vs. Human assertiveness & 0.554*** & 0.113 & 0.395*** & 0.353*** \\ Internal Certainty vs. Predicted assertiveness & 0.064 & 0.041 & 0.154 & 0.212 \\ Internal Certainty vs. Human assertiveness & 0.188** & 0.138 & 0.218* & 0.304*** \\   

Table 1: Correlation between internal certainty, objective and subjective assertiveness

Figure 2: Epistemic Miscalibration: misalignment between the LLM’s certainty score and our model’s assertiveness scores.

### Description of the experiment

Respondents are presented with a series of statements, each accompanied by a true/false classification and an explanation generated by GPT-4o. Participants are then instructed to rate the assertiveness of each explanation on a scale from 0 (Not at all assertive) to 10 (Extremely assertive). This task is repeated four times for each respondent, providing a dataset of 1868 human ratings of assertiveness. We provide more details including the prompt given to respondents in Appendix I.

Explanation generationInitially, GPT-4o is prompted to provide a classification and explanation for each statement from the LIAR dataset, following the explain-then-score prompt in Peltrine et al. (2023) and other sections of this paper. We then prompt GPT-4o to generate two additional versions of each explanation: one less assertive and one more assertive than the original. This is to ensure that we have three distinct versions of explanations for each statement, allowing for meaningful comparisons of human perceptions. Four randomly sampled explanations from this validation dataset are presented to each respondent for rating assertiveness.2

## 6 Results of human perceptions of assertiveness

In Figure 2(a), we observe that the scaled assertiveness scores from the survey are roughly normally distributed, centered at a score of 0.6. The scores distributed across the three assertiveness levels (-1: low, 0: medium, 1: high) also Figure 2(a), confirms that our prompting strategy for generating explanations with different assertiveness levels is accurately perceived by human respondents. Figure 2(b) plots the relationship between the assertiveness predicted by our model and the survey respondents, colored by assertiveness level.

We also report the overall and disaggregated correlations between both human and predicted assertiveness scores with internal certainty scores in Table 1. The correlation between our model's predicted assertiveness scores and human perception of assertiveness is relatively strong at 0.55. This indicates that the predicted measures are fairly aligned with how humans perceive assertiveness. Meanwhile, the relationship between predicted assertiveness and internal certainty is very weak (0.064), highlighting the issue of epistemic miscalibration. Figure 2(c) also shows comparisons between the internal certainty, predicted and human assertiveness scores.

## 7 Discussion and Conclusion

In this work, we introduced the novel problem of epistemic calibration for LLMs: ensuring that the confidence expressed in a model's communication aligns with its underlying reliability. We argued that this normative ideal is critical for LLMs to serve as trusted and responsible information sources. Through a decomposition of the problem into external and internal certainty, we developed a framework for understanding and evaluating epistemic calibration in language models. Using our new measurement approach that greatly improves fidelity of assertiveness measurements compared to prior models, our empirical investigation of a state-of-the-art model reveals significant gaps between the model's internal confidence estimates and the assertiveness of its generated language. This miscalibration poses risks to users, who may be misled by overconfident model outputs.

Our work also highlights the need for further research to fully understand and address the challenges of epistemic calibration. One key direction is developing new training and inference techniques to improve the alignment between LLMs' probability estimates and their linguistic expression of confidence. Another is studying the downstream impacts of epistemic miscalibration on user trust, decision making, and information ecosystems, through a combination of user studies and large-scale simulations. We believe that the epistemic calibration framework introduced in this paper provides a valuable foundation for these future efforts. We discuss further applications, to RLHF, silicon sampling, and debate, in Appendix H, along with a discussions on limitations in Appendix J. Ultimately, achieving epistemic calibration in language models is not just a technical challenge, but a societal imperative. As these models become ever more integrated into our information-seeking and decision-making practices, ensuring that they express confidence in a calibrated and responsible way is essential for mitigating the risks of misinformation, confusion, and unwarranted trust.

Figure 3: Our method’s predicted assertiveness score is more aligned with the human scores.