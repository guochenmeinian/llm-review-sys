# Single-Call Stochastic Extragradient Methods

for Structured Non-monotone Variational Inequalities:

Improved Analysis under Weaker Conditions

Sayantan Choudhury

AMS & MINDS

Johns Hopkins University

&Eduard Gorbunov

MBZUAI

&Nicolas Loizou

AMS & MINDS

Johns Hopkins University

###### Abstract

Single-call stochastic extragradient methods, like stochastic past extragradient (\(\)) and stochastic optimistic gradient (\(\)), have gained a lot of interest in recent years and are one of the most efficient algorithms for solving large-scale min-max optimization and variational inequalities problems (VIP) appearing in various machine learning tasks. However, despite their undoubted popularity, current convergence analyses of \(\) and \(\) require strong assumptions like bounded variance or growth conditions. In addition, several important questions regarding the convergence properties of these methods are still open, including mini-batching, efficient step-size selection, and convergence guarantees under different sampling strategies. In this work, we address these questions and provide convergence guarantees for two large classes of structured non-monotone VIPs: (i) quasi-strongly monotone problems (a generalization of strongly monotone problems) and (ii) weak Minty variational inequalities (a generalization of monotone and Minty VIPs). We introduce the expected residual condition, explain its benefits, and show how it allows us to obtain a strictly weaker bound than previously used growth conditions, expected co-coercivity, or bounded variance assumptions. Finally, our convergence analysis holds under the arbitrary sampling paradigm, which includes importance sampling and various mini-batching strategies as special cases.

## 1 Introduction

Differentiable game formulations where several parameterized models/players compete to minimize their respective objective functions have recently gained much attention from the machine learning community. Some landmark advances in machine learning that are framed as games (or in their simplified form as min-max optimization problems) are Generative Adversarial Networks (GANs) [19; 2], adversarial training of neural networks [46; 72], reinforcement learning [9; 64], and distributionally robust learning [51; 73].

In this work, we consider a more abstract formulation of the problem and focus on solving the following unconstrained stochastic variational inequality problem (VIP):

\[x^{*}^{d}:F(x^{*})=_{i=1}^ {n}F_{i}(x^{*})=0\] (1)

where each \(F_{i}:^{d}^{d}\) is a Lipschitz continuous operator. Problem (1) generalizes the solution of several types of _stochastic smooth games_[16; 44; 20; 7]. The simplest example is the unconstrained min-max optimization problem (also called a _zero-sum_ game):

\[_{x_{1}^{d_{1}}}_{x_{2}^{d_{2}}} _{i=1}^{n}g_{i}(x_{1},x_{2})\,,\] (2)where each component function \(g_{i}:^{d_{1}}^{d_{2}}\) is assumed to be smooth. In this scenario, operator \(F_{i}\) of (1) represents the appropriate concatenation of the block-gradients of \(g_{i}\): \(F_{i}(x):=(_{x_{1}}g_{i}(x_{1},x_{2});-_{x_{2}}g_{i}(x_{1},x_{2}))\), where \(x:=(x_{1};x_{2})\). Solving (1) then amounts to finding a stationary point \(x^{*}=(x_{1}^{*};x_{2}^{*})\) for (2), which under a convex-concavity assumption for \(g_{i}\), implies that it is a global solution for the min-max problem.

However, in modern machine learning applications, game-theoretical formulations that are special cases of problem (1) are rarely monotone. That is, the min-max optimization problem (2) does not satisfy the popular and well-studied convex-concave setting. For this reason, the ML community started focusing on non-monotone problems with extra structural properties.1 In this work, we focus on such settings (structured non-monotone operators) for which we are able to provide tight convergence guarantees and avoid the standard issues (like cycling and divergence of the methods) appearing in the more general non-monotone regime. In particular, we focus on understanding and efficiently analyze the performance of single-call extragradient methods for solving (i) \(\)-quasi-strongly monotone VIPs  and (ii) weak Minty variational inequalities .

Classes of structured non-monotone VIPs.Throughout this work we assume that operator \(F\) in (1) is \(L\)- Lipschitz i.e. \( x,y^{d}\) operator \(F\) satisfy \(\|F(x)-F(y)\| L\|x-y\|\).

As we have already mentioned, in this work, we deal with two classes of structured non-monotone problems: the \(\)-quasi strongly monotone VIPs and the weak Minty variational inequalities.

**Definition 1.1**.: \(F\) is said to be \(\)-quasi strongly monotone if there is \(>0\) such that:

\[ x^{d} F(x),x-x^{*}\|x-x^{*}\| ^{2}.\] (3)

Condition (3) is a relaxation of \(\)-strong monotonicity, and it includes several non-monotone games as special cases . Inequality (3) can be seen as an extension of the popular quasi-strong convexity assumption from optimization literature  to the VIPs . In the literature of variational inequality problems, quasi strongly monotone problems are also known as strong coherent VIPs  or VIPs satisfying the strong stability condition , or strong Minty variational inequality .

One of the weakest possible assumptions on the structure of non-monotone VIPs is the weak Minty variational inequality .

**Definition 1.2**.: We say weak Minty Variational Inequality (MVI) holds for \(F\) if for some \(>0:\)

\[ x^{d} F(x),x-x^{*}-\|F(x)\|^ {2}.\] (4)

To the best of our knowledge, the weak Minty variational inequality (4) as an assumption was first introduced in . The more popular and extensively studied Minty variational inequality  is a particular case of (4) with \(=0\). In addition, the weak MVI condition is implied by the negative comonotonicity  or, equivalently, the positive cohypomonotonicity . Finally, when we focus on min-max optimization problems (2), weak MVI condition (with \(=0\)) is satisfied for several non-convex non-concave families of min-max objectives, including quasi-convex quasi-concave or star convex- star concave . Extragradient-type methods for solving VIPs satisfying the weak MVI have been proposed in  and .

### Main Contributions

Our main contributions are summarized below.

* **Expected Residual.** We propose the expected residual (ER) condition for stochastic variational inequality problems (1). We explain the benefits of ER and show how it can be used to derive an upper bound on \(\|g(x)\|^{2}\) (see Lemma 3.2) that it is strictly weaker than the bounded variance assumption and "growth conditions" previously used for the analysis of stochastic algorithms for solving (1). We prove that ER holds for a large class of operators, i.e., whenever \(F_{i}\) of (1) are Lipschitz continuous.
* **Novel Convergence Guarantees.** We prove the first convergence guarantees for SPEG (7) in the quasi-strongly monotone (3) and weak MVI (4) cases _without using the bounded variance _assumption_. We achieve that by using the proposed (ER) condition. In particular, for the class of quasi-strongly monotone VIPs, we show a linear convergence rate to a neighborhood of \(x^{*}\) when constant step-sizes are used. We also provide theoretically motivated step-size switching rules that guarantee exact convergence of \(\) to \(x^{*}\). In the weak MVI case, we prove the convergence of \(\) for \(<}{{2L}}\), improving the existing restrictions on \(\). We compare our results with the existing literature in Table 1.
* **Arbitrary Sampling.** Via a stochastic reformulation of the variational inequality problem (1) we explain how our convergence guarantees of \(\) hold under the arbitrary sampling paradigm. This allows us to cover a wide range of samplings for \(\) that were never considered in the literature before, including mini-batching, uniform sampling, and importance sampling as special cases. In this sense, our analysis of \(\) is unified for different sampling strategies. Finally, to highlight the tightness of our analysis, we show that the best-known convergence guarantees of deterministic \(\) for strongly monotone and weak MVI can be obtained as special cases of our main theorems.

## 2 Stochastic Reformulation of VIPs & Single-Call Extragradient Methods

In this work, we provide a theoretical analysis of single-call stochastic extragradient methods that allows us to obtain convergence guarantees of any minibatch and reasonable sampling selection. We achieve that by using the recently proposed "stochastic reformulation" of the variational inequality problem (1) from . That is, to allow for any form of minibatching, we use the _arbitrary sampling_ notation

\[g(x)=F_{v}(x):=_{i=1}^{n}v_{i}F_{i}(x),\] (5)

where \(v^{n}_{+}\) is a random _sampling vector_ drawn from a user-defined distribution \(\) such that \(_{}[v_{i}]=1,\) for \(i=1,,n\). In this setting, the original problem (1) can be equivalently written as,

\[\;x^{*}^{d}:_{}[F_{v}(x^{*} ):=_{i=1}^{n}v_{i}F_{i}(x^{*})]=0,\] (6)

where the equivalence trivially holds since \(_{}[F_{v}(x)]=_{i=1}^{n}_{ }[v_{i}]F_{i}(x)=F(x)\).

In this work, we consider _Stochastic Past Extragradient Method_ (SPEG) applied to (6):

\[_{k}&=x_{k}-_{k}F_{v_{k-1}}( _{k-1})\\ x_{k+1}&=x_{k}-_{k}F_{v_{k}}(_{k})\] (7)

where \(_{-1}=x_{0}\) and \(v^{k}\) is sampled i.i.d at each iteration and \(_{k}>0\) and \(_{k}>0\) are the extrapolation step-size and update step-size respectively. We note that in our convergence analysis, we allow selecting _any_ distribution \(\) that satisfies \(_{}[v_{i}]=1\  i\). This means that for a different selection of \(\), (7) yields different interpretations of SPEG for solving the original problem (1).

One example of distribution \(\) is \(\)-minibatch sampling, which is defined as follows.

**Definition 2.1** (\(\)-Minibatch sampling).: Let \([n]\). We say that \(v^{n}\)is a \(\)-minibatch sampling if for every subset \(S[n]\) with \(|S|=\), we have that \([v=_{i S}e_{i}]:=}=\).

By using a double counting argument, one can show that if \(v\) is a \(\)-minibatch sampling, it is also a valid sampling vector (\(_{}[v_{i}]=1\)) . We highlight that our analysis holds for every form of minibatching and for several choices of sampling vectors \(v\). Later in Section 5, we provide more details related to non-uniform sampling. In addition, by Definition 2.1, it is clear that if \(=n\), then \(v_{i}=1\) for all \(i[n]\). Later in Section 4, we prove how our analysis captures the deterministic Past Extragradient Method as a special case.

In , an analysis of stochastic gradient descent-ascent (\(x_{k+1}=x_{k}-_{k}F_{v_{k}}(x_{k})\)) under the arbitrary sampling paradigm was proposed for solving star-co-coercive VIPs. Later , extended this approach and provided general convergence guarantees for stochastic extragradient method (SEG) (a stochastic variant of the popular extragradient method [32; 30]) for solving quasi-strongly monotone and monotone VIPs. Despite its popularity, SEG requires two oracle calls per iteration which makes it prohibitively expensive in many large-scale applications and not easily applicable to the online learning problems . This motivates us to explore in detail the convergence guarantees of single-call variants of extragradient methods (extragradient methods that require only a single oracle call per iteration).

On Single-Call Extragradient Methods.The seminal work of  is the first paper that proposes the deterministic Past Extragradient method. In the stochastic setting,  provides an analysis of several stochastic single-call extragradient methods for solving strongly monotone VIPs. In , it was also shown that in the unconstrained setting, the update rules of Past Extragradient and Optimistic Gradient are exactly equivalent (see also Proposition B.6 in appendix). Through this connection, and via our stochastic reformulation (6) our theoretical results hold also for the _Stochastic Optimistic Gradient Method_ (SOG): \(x_{k+1}=x_{k}-_{k}F_{v_{k}}(x_{k})-_{k}(F_{v_{k}}(x_{k})-F_{v_{k-1 }}(x_{k-1}))\).  provides the convergence guarantees of SOG for weak MVI. To the best of our knowledge, our work is the first that provides convergence guarantees for SOG under the arbitrary sampling paradigm (captures sampling beyond uniform sampling) and also without using the bounded variance assumption.

## 3 Expected Residual

In our theoretical results, we rely on Expected Residual (ER) condition. In this section, we define ER and explain how it is connected with similar conditions used in optimization literature. We further provide sufficient conditions for ER to hold and prove how it can be used to obtain a strictly weaker upper bound of \(\|g(x)\|^{2}\) than previously used growth conditions, expected co-coercivity, or bounded variance assumptions.

**Assumption 3.1**.: We say the Expected Residual (ER) condition holds if there is a parameter \( 0\) such that for an unbiased estimator \(g(x)\) of the operator \(F\), we have

\[[\|(g(x)-g(x^{*}))-(F(x)-F(x^{*}))\|^{2}]\|x-x^{*}\|^{2}.\] (ER)

The ER condition bounds how far the stochastic estimator \(g(x)=F_{v}(x)\) (5) used in SPEG is from the true operator \(F(x)\). ER depends on both the properties of the operator \(F(x)\) and of the selection of sampling (via \(g(x)\)). Conditions similar to ER appeared before in optimization literature but they have never been used in operator theory and the analysis of \(\). In particular,  used a similar condition for analyzing \(\) in stochastic optimization problems but with the right-hand side of \(\) to be the function suboptimality \(f(x)-f(x^{*})\) (such concept is not available in VIPs). In  and , similar conditions appear under the name "Hessian variance" assumption for distributed minimization problems. In the context of distributed VIPs, a similar but stronger condition to \(\) is used by .

**Bound on Operator Noise.** A common approach for proving the convergence of stochastic algorithms for solving the VIPs is assuming uniform boundedness of the stochastic operator or uniform boundedness of the variance. However, as we explain below, these assumptions either do not hold or are true only for a restrictive set of problems. In our work, we do not assume such bounds. Instead, we use the following direct consequence of \(\).

**Lemma 3.2**.: Let \(_{*}^{2}:=\|g(^{*})\|^{2}<\) (operator noise at the optimum is finite). If \(\) holds, then

\[\|g(x)\|^{2}\|x-x^{*}\|^{2}+\|F(x)\|^{2}+2_{*}^{2}.\] (8)

Sufficient Conditions for \(\).Let us now provide sufficient conditions which guarantee that the \(\) condition holds and give a closed-form expression for the expected residual parameter \(\) and \(_{*}^{2}=\|g(x^{*})\|^{2}\) for the case of \(\)-minibatch sampling (Def. 2.1).

**Proposition 3.3**.: Let \(F_{i}\) of problem (1) be \(L_{i}\)-Lipschitz operators, then \(\) holds. If, in addition, vector \(v^{n}\) is a \(\)-minibatch sampling (Def. 2.1) then: \(=_{i=1}^{n}L_{i}^{2},\) and \(_{*}^{2}=_{i=1}^{n}\|F_{i}(x^{*}) \|^{2}\).

Similar results to Prop. 3.3 but under different sufficient conditions have been obtained for \(\)-minibatch sampling under expected smoothness and a variant of expected residual for solving minimization problems in  and  respectively. In , a similar proposition was derived but for the much more restrictive class of co-coercive operators.

Connection to Other Assumptions.In the proofs of our convergence results, we use the bound (8), which, as we explained above, is a direct consequence of \(\). In this paragraph, we place this bound in a hierarchy of common assumptions used for the analysis of stochastic algorithms for solving VIPs. In the literature on stochastic algorithms for solving the VIPs and min-max optimization problems, previous works assume either bounded operator (\(\|g(x)\|^{2} c\)) , bounded variance (\(\|g(x)-F(x)\|^{2} c\))  (in Appendix C we provide a simple example where bounded variance assumption does not hold) or growth condition (\(\|g(x)\|^{2} c_{1}\|F(x)\|^{2}+c_{2}\)) . In all of these conditions, the parameters \(c\), \(c_{1}\), and \(c_{2}\) are usually constants that do not have a closed-form expression. The closer works to our results are  which assumes existence of \(l_{F}>0\) such that the expected co-coercivity condition (\(\|g(x)-g(x^{*})\|^{2} l_{F} F(x),x-x^{*}\)) holds. Their convergence guarantees provide an efficient analysis for several variants of SGDA for solving co-coercive VIPs. In the proposition below, we prove how these conditions are related to the bound (8) obtained using \(\).

**Proposition 3.4**.: Suppose \(F\) is a \(L\)-Lipschitz operator. Then we have the following hierarchy of assumptions:

  Bounded Operator & Bounded Variance & Growth Condition \\  \(F_{i}\) are \(L_{i}\)-Lipschitz & \(\) \\  Expected Cocoercivity & \\  

Let us also mention that  provided convergence guarantee of double-oracle stochastic extragradient (\(\)) method under the variance control condition \(\|g(x)-F(x)\|^{2}(a\|x-x^{*}\|+b)^{2}\) where \(a,b 0\). In their work, they focus on solving VIPs satisfying the error-bound condition, and they did not provide closed-form expressions of parameters \(a\) and \(b\). Although the analysis of can be conducted with \(a>0\), the authors only provide rates for the case \(a=0\). The main difference between their results (for SEG) and our results (for SPEG) is that our bound (8) is not really an assumption, but it holds for free when \(F_{i}\) are \(L_{i}\)-Lipschitz. In addition, the values of parameters \(\) and \(_{*}^{2}\) in (8) could have different values based on the sampling used in the update rule of SPEG.

## 4 Convergence Analysis

In this section, we present and discuss the main convergence results of this work. In the first part, we focus on the ones derived for \(\)-quasi strongly monotone problems (3) (both for constant and decreasing step-sizes), and in the second part on the Weak Minty VIP (4).

### Quasi-Strongly Monotone Problems

Constant Step-size:We start with the case of \(\)-quasi strongly monotone problems and consider the convergence of SPEG with constant step-size.

**Theorem 4.1**.: Let \(F\) be \(L\)-Lipschitz, \(\)-quasi strongly monotone, and let ER hold. Choose step-sizes \(_{k}=_{k}=\) such that

\[0<\{,\}\] (9)

for all \(k\). Then the iterates produced by SPEG, given by (7) satisfy

\[R_{k}^{2}(1-)^{k}R_{0}^{2}+^{2}}{},\] (10)

where \(R_{k}^{2}[\|x_{k}-x^{*}\|^{2}+\|x_{k}-_{k-1}\| ^{2}]\). Hence, given any \(>0\), and choosing \(=\{,,^{2}}\}\), SPEG achieves \(\|x_{K}-x^{*}\|^{2}\) after \(K\{,},^{2 }}{^{2}}\}(^{2}}{})\) iterations.

To the best of our knowledge, the above theorem is the first result on the convergence of SPEG that does not rely on the bounded variance assumption. Theorem 4.1 recovers the same rate of convergence with the Independent-Samples SEG (I-SEG) under assumption (8) , although  simply assume (8), while we show that it follows from Assumption 3.1 holding whenever all summands \(F_{i}\) are Lipschitz. However, in the case when all \(F_{i}\) are \(\)-quasi strongly monotone and \(L_{i}\)-Lipschitz (on average), one can use Same-Sample SEG (S-SEG). The existing results for S-SEG have better exponentially decaying term [49; 20] then Theorem 4.1, e.g., in the case when \(L_{i}=L\) for all \(i[n]\), we have \(=(L^{2})\) meaning that the exponentially decaying term in (10) is \((R_{0}^{2}(-{^{2}k/L^{2}}))\), while S-SEG has much better exponentially decaying term \((R_{0}^{2}(-{ k/L}))\).

Such a discrepancy can be partially explained by the following fact: S-SEG can be seen as one step of deterministic Extragradient for stochastic operator \(F_{v_{k}}\) allowing to use one-iteration analysis of Extragradient without controlling the variance. In contrast, there is no version of SPEG that uses the same sample for extrapolation and update steps. This forces to use different samples for these steps and this is a key reason why SPEG cannot be seen as one iteration of deterministic Past-Extragradient for some operator. Due to this, we need to rely on some bound on the variance to handle the stochasticity in the updates; see also [20, Appendix F.1]. Therefore, in our analysis, we use Assumption 3.1, implying (8). Nevertheless, it is still an open question whether it is possible to improve the rate of SPEG in the case of \(\)-quasi strongly monotone and Lipschitz operators \(F_{i}\).

To highlight the generality of Theorem 4.1, we note that for the deterministic PEG, \(=0\) and \(_{*}^{2}=0\) (by selecting \(=n\) in the definition 2.1 of minibatch sampling). In this case, Theorem 4.1 recovers the well-known result (up to \(}{{2}}\) factor in the rate) for deterministic PEG proposed in  as shown in the following corollary.

**Corollary 4.2**.: Let the assumptions of Theorem 4.1 hold and a deterministic version of SPEG is considered, i.e., \(=0\), \(_{*}^{2}=0\). Then, Theorem 4.1 implies that for all \(k 0\) the iterates produced by \(\) with step-sizes \(_{k}=_{k}=\) such that \(0<\) satisfy \(R_{k}^{2}(1-)^{k}R_{0}^{2}\), where \(R_{k}^{2}\|x_{k}-x^{*}\|^{2}+\|x_{k}-_{k-1}\|^{2}\).

Decreasing Step-size:In this section, we consider two different decreasing step-sizes policies for \(\) applied to solve quasi-strongly monotone problems.

**Theorem 4.3**.: Let \(F\) be \(L\)-Lipschitz, \(\)-quasi strongly monotone, and Assumption 3.1 hold. Let

\[_{k}=_{k},&k k^{*},\\ },&k>k^{*},\] (11)

where \(\{}{{(4L)}},}{{(18)}}\}\) and \(k^{*}=}{{()}}\). Then for all \(K k^{*}\) the iterates produced by \(\) with step-sizes (11) satisfy

\[R_{K}^{2}(}{K})^{2}^{2}}{(2)}+^{2}}{^{2}K},\] (12)

where \(R_{K}^{2}[\|x_{K}-x^{*}\|^{2}+\|x_{K}-_{K-1}\| ^{2}]\).

\(\) with step-size policy2 (11) has two stages of convergence: during first \(k^{*}\) iterations it uses constant step-size to reach some neighborhood of the solution and then the method switches to the decreasing \(}(}{{k}})\) step-size allowing to reduce the size of the neighborhood.

For the case of strongly monotone problems (a special case of our quasi-strongly monotone setting)  also analyze \(\) with decreasing \(}(}{{k}})\) step-size3 under bounded variance assumption, i.e., when (8) holds with \(=0\) and some \(_{*}^{2} 0\), which is equivalent to the uniformly bounded variance assumption. In particular, Theorem 5 states \([\|x_{K}-x^{*}\|^{2}]^{2}}{^{2}K} +o()\) where \(C\) is some numerical constant. If the problem is strongly monotone, the result of  is closely related to what is obtained in Theorem 4.3: the main difference in the upper-bound is that we provide an explicit form of \(o(}{{K}})\) term. Moreover, in contrast to the result from , Theorem 4.3 holds even when \(>0\) in (8), which covers a larger class of problems.

Following , we also consider another decreasing step-size policy.

**Theorem 4.4**.: Let \(F\) be \(L\)-Lipschitz, \(\)-quasi strongly monotone, and Assumption 3.1 hold. Let \(\{}{{(4L)}},}{{(18)}}\}\). If for \(K 0\) step-sizes \(\{_{k}\}_{k 0}\), \(\{_{k}\}_{k 0}\) satisfy \(_{k}=_{k}\) and

\[_{k},&K},\\ ,&K>}k k_{0},\\ +(k-k_{0})},&K>}k>k_{0},\] (13)

where \(k_{0}=}{{2}}\), then the iterates produced by \(\) with the step-sizes defined above satisfy

\[R_{K}^{2}^{2}}{}\{-\{,}{72}\}K\}+^{2}}{ ^{2}K},\] (14)

where \(R_{K}^{2}[\|x_{K}-x^{*}\|^{2}+\|x_{K}-_{K-1}\| ^{2}]\).

In contrast to (12), the rate from (14) has much better (exponentially decaying) \(o(}{{K}})\) term. When \(_{*}^{2}\) is large and one needs to achieve very good accuracy of the solution, this difference is negligible, since the dominating \(}(}{{K}})\) term is the same for both bounds (up to numerical factors). However, when \(_{*}^{2}\) is small enough, e.g., the model is close to over-parameterized, or it is sufficient to achieve low accuracy of the solution, the dominating term in (14) is typically much smaller than the one from (12). Finally, it is worth mentioning, that the improvement of \(o(}{{K}})\) is not achieved for free: unlike the policy from (11), step-size rule (13) relies on the knowledge of the total number of steps \(K\), which can be inconvenient for the practical use in some cases.

[MISSING_PAGE_EMPTY:8]

zero) and one \(L_{i}\) is large, \(_{}\) is almost \(n\) times smaller than \(_{}\). In this latter scenario (when \(_{}\) is much smaller than \(_{}\)), importance sampling could be useful and can significantly improve the performance of \(\). For example, note that the exponentially decaying term in (14) decreases with \(\). Hence, this term will decrease much faster with importance sampling than with uniform sampling.

## 6 Numerical Experiments

To verify our theoretical results, we run several experiments on two classes of problems, i.e., strongly monotone problems (a special case of the quasi-strongly monotone VIPs) and weak MVI problems. The code to reproduce our results can be found at https://github.com/isayantan/Single-Call-Stochastic-Extragradient-Methods.

### Strongly Monotone Problems

Our experiments consider the quadratic strongly-convex strongly-concave min-max problem from . That is, we implement \(\) on quadratic games of the form \(_{x^{d}}_{y^{d}}_{i=1}^{n}f_ {i}(x,y)\) where

\[f_{i}(x,y)x^{}A_{i}x+x^{}B_{i}y- {1}{2}y^{}C_{i}y+a_{i}^{}x-c_{i}^{}y.\] (16)

Here \(A_{i},B_{i},C_{i}\) are generated such that the quadratic game is strongly monotone and smooth. In all our experiments, we take \(n=100\) and \(d=30\). We generate positive semi-definite matrices \(A_{i},B_{i},C_{i}\) such that their eigenvalues lie in the interval \([_{A},L_{A}],[_{B},L_{B}]\) and \([_{C},L_{C}]\) respectively. In all our experiments, we consider \(L_{A}=L_{B}=L_{C}=1\) and \(_{A}=_{C}=0.1,_{B}=0\) unless otherwise mentioned. The vectors \(a_{i}\) and \(c_{i}\) are generated from \(_{d}(0,I_{d})\). Here, the \(i\)th operator is given by

\[F_{i}x\\ y=_{x}f_{i}(x,y)\\ -_{y}f_{i}(x,y)=A_{i}x+B_{i}y+a_{i}\\ C_{i}y-B_{i}^{}x+c_{i}\]

In Figures 1, 2, and 3, we plot the relative error on the \(y\)-axis i.e. \(-x^{*}\|^{2}}{\|x_{0}-x^{*}\|^{2}}\).

Constant vs Switching Step-size Rule.In Fig. 1, we illustrate the step-size switching rule of Theorem 4.3. We place the dotted line to mark when we switch from constant step-size to decreasing step-size. In Fig. 1, the trajectory of switching step-size rule (11) matches that of constant step-size (9) in the first phase (where \(\) runs with constant step-size following (11)). However, it becomes stagnant when the constant step-size \(\) reaches a neighbourhood of optimality. In contrast, the step-size of Theorem 4.3 helps the method to converge to better accuracy.

Comparison to Hsieh et al. .In this experiment, we compare \(\) step-sizes proposed in Theorems 4.1 and 4.3 with step-sizes from . To implement \(\) with the step-sizes from , we choose \(\) and \(b\) such that \(<\) and set \(_{k}=_{k}=\). For Fig. 1(a), we generate \(A_{i},B_{i},C_{i}\) as before. First, we sample optimal points \(x^{*},y^{*}\) from \(_{d}(0,I_{d})\) and then generate \(a_{i},c_{i}\) such that \(F(x^{*},y^{*})=0\).

\[a_{i}\\ c_{i}=A_{i}&B_{i}\\ -B_{i}^{}&C_{i}^{-1}x^{*}\\ y^{*}.\]

In Fig. 1(a), we run the algorithms on interpolated model \((F_{i}(x^{*})=0i[n])\). Since the model is interpolated, we have \(_{*}^{2}=0\) in Theorem 4.1 and linear convergence to the exact optimum asymptotically. In this setting, as shown in Fig. 1(a), our proposed step-size results in major improvement compared to the decreasing step-size selection analyzed in . In Fig. 1(b), we compare the switching step-size rule with step-size from . In Fig. 1(b), we generate \(a_{i},c_{i}\) from the normal distribution. In this plot, we manually switch the step-size from constant to decreasing after \(305\) steps. We observe that such a semi-empirical rule has comparable performance to the step-size selection of Hsieh et al. .

Figure 1: Constant vs Switching

Uniform vs. Importance Sampling.In this experiment, we highlight the advantage of using importance sampling over uniform sampling. The eigenvalues of \(A_{1},C_{1}\) are uniformly generated from the interval \([0.1,]\) while the rest of the matrices are generated as mentioned before. We vary the value of \(\{2,5,10,20\}\) and run and compare \(\) with both uniform and importance sampling (see Fig. 3). For importance sampling, we use the probabilities \(p_{i}=}}{{_{j=1}^{n}L_{j}}}\). In Fig. 3, it is clear that as the value of \(\) increases, the trajectories under uniform sampling get worse, while the trajectory under importance sampling remains almost identical. This behavior aligns well with our discussion in Section 5.

### Weak Minty Variational Inequality Problems

This experiment verifies the convergence guarantees of \(\) in Theorem 4.5. Following the min-max problem mentioned in , we consider the objective function

\[_{x}_{y}_{i=1}^{n}_{i}xy+ }{2}(x^{2}-y^{2}).\] (17)

In this experiment, we generate \(_{i},_{i}\) such that \(L=8\) and \(=}{{32}}\) for the above min-max problem . We implement \(\) with extrapolation step \(_{k}=0.08\) and update step \(_{k}=0.01\) which satisfies the conditions on step-size in Theorem 4.5. In Fig. 4, we use a batchsize of \(6\). This plot illustrates that for some weak MVI problems the requirement on the stepsize from Theorem 4.5 can be too pessimistic and \(\) with relatively small batchsize achieves reasonable accuracy of the solution. The choice of batchsize ensures that bound (15) holds and \(\) is small enough to guarantee convergence of \(\). We also tried to compare \(\) with \(\)+ from , however, the authors do not mention their choice of update step-size. We examined several decreasing update step-size for which \(\)+ failed to converge. Further details on experiments can be found in Appendix G.1.

Figure 4: Trajectory of \(\) for solving weak MVI. "Squared Operator Norm Error" in vertical axis denotes the \(_{0 k N-1}[\|F(_{k})\|^{2}]\) of Theorem 4.5.

Figure 3: Comparison of \(\) with Uniform and Importance Sampling for different \(\{2,5,10,20\}\), where the eigenvalues of matrices \(A_{1},C_{1}\) are uniformly generated from the interval \([0.1,]\).

Figure 2: Comparison of our \(\) using our step-size against decreasing step-size of Hsieh et al. . In plot (a), for constant step-size of \(\) we use the upper bound of (9). In plot (b), we run our switching step-size \(\) (11).