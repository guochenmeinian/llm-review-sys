# MAViL: Masked Audio-Video Learners

Po-Yao Huang\({}^{1}\) Vasu Sharma\({}^{1}\) Hu Xu\({}^{1}\) Chaitanya Ryali\({}^{1}\) Haoqi Fan\({}^{1}\) Yanghao Li\({}^{1}\) Shang-Wen Li\({}^{1}\) Gargi Ghosh\({}^{1}\) Jitendra Malik\({}^{1,2}\) Christoph Feichtenhofer\({}^{1}\)

\({}^{1}\)FAIR, Meta \({}^{2}\)University of California, Berkeley

###### Abstract

We present Masked Audio-Video Learners (MAViL) to learn audio-visual representations with three complementary forms of self-supervision: (1) reconstructing masked raw audio and video inputs, (2) intra-modal and inter-modal contrastive learning with masking, and (3) self-training to predict aligned and contextualized audio-video representations learned from the first two objectives. Empirically, MAViL achieves state-of-the-art audio-video classification performance on AudioSet (53.3 mAP) and VGGSound (67.1% accuracy), surpassing recent self-supervised models and supervised models that utilize external labeled data. Notably, pre-training with MAViL not only enhances performance in multimodal classification and retrieval tasks, but it also improves the representations of each modality in isolation, without relying on information from the other modality during uni-modal fine-tuning or inference. The code and models are available at https://github.com/facebookresearch/MAViL.

## 1 Introduction

We study self-supervised learning (SSL) from audio and video, two rich, heterogeneous, yet closely related modalities of human perception. There are two primary forms of self-supervision commonly used: reconstruction and contrastive learning. By reconstructing masked text tokens on large-scale corpora, BERT  pre-training has achieved groundbreaking results in various NLP tasks. Masked autoencoders (MAEs) have recently emerged as powerful tools for learning uni-modal representations in various modalities such as image , video , and audio . They employ an asymmetric encoder-decoder architecture with a substantial portion of encoder inputs being masked, resulting in efficient uni-modal SSL. Additionally, contrastive learning has been widely used to learn coordinated multimodal representations for image-text  and audio-visual  tasks.

In this work, we develop a novel approach that integrates MAE and contrastive learning to enhance audio-video representation learning for both audio-video and audio-only tasks. This integration poses unique challenges under the multimodal paradigm, necessitating meticulous model design to effectively manage diverse multimodal inputs in MAE, while also adapting contrastive learning to accommodate masking of the majority of inputs for efficiency. Our method, Masked Audio-Video Learners (MAViL), comprises a pair of encoders to encode audio and video, a fusion encoder, and distinct decoders tailored for reconstructing raw inputs or contextualized representations in each modality (as illustrated in Fig. 1). We design MAViL with three types of objectives outlined next.

Firstly, we extend uni-modal MAE to multimodal and utilize a fusion encoder to exchange information from all modalities. MAViL reconstructs raw inputs that has been removed under a high masking ratio (\(\) 80%). With this, it learns _complementary_ audio-video representations by reconstructing a single modality input, with the supplementary context from the other. Secondly, we adapt contrastive learning , with the majority of uni-modal inputs being masked, to learn an _aligned_ audio-video latent space efficiently. MAViL employs two types contrastive objectives: (i) An _inter-modal_ contrast that brings together paired video and audio clips from the same video and contrasts them with otherunpaired clips sampled in a mini-batch. (ii) An _intra-modal_ contrast that draws closer the two masked views of the same audio or video, while pushing away other samples from the same modality.

Thirdly, unlike conventional MAEs that focus on reconstructing _heterogeneous_ and _raw_ inputs (audio _or_ video), we propose a novel pre-training task that reconstructs _homogeneous_ and _contextualized_ audio-video representations in a joint (audio-_and_-video) latent space (illustrated in Fig. 2).

We are motivated by the recent successes in visual SSL that use disparate teacher models to generate contextualized representations as reconstruction targets. For example, BEiT  uses features by DALL-E  as its target. MaskFeat  predicts masked target features such as HOG  or DINO . Inspired by these uni-modal teachers, we take a step further to propose a new masked _contextualized audio-video reconstruction_ pretext task. Our core idea is: instead of predicting raw inputs in heterogeneous modalities, the model with masked-view inputs jointly predicts contextualized audio-video representations in the homogeneous (aligned) latent space generated by a teacher (_i.e_. an identical model pre-trained with the masked reconstruction and contrastive objectives above) with complete-view inputs. This approach ensures the students learn well-aligned and contextualized audio-video representations and thus improves their performance on downstream tasks. To achieve this, without relying on external teacher models, we employ a simple two-stage self-training framework (illustrated in Fig. 2). In stage-1, we train the teacher MAViL and use raw inputs as its reconstruction targets. In stage-2, the student MAViL (the final model for later fine-tuning) learns to reconstruct the aligned and contextualized audio-video representations generated by the MAViL teacher.

Our experimental results confirm MAViL's superiority in audio-video classification (AudioSet-20K, AudioSet-2M, VGGSounnd) and retrieval tasks (VTT and YouCook) where it surpasses the best self-supervised and supervised pre-trained models by large margins. Notably, MAViL not only learns strong joint audio-video representations, but can also improve single modality encoders, without using the other modality it pre-trained with during fine-tuning or inference.

In summary, MAViL makes the following contributions for learning self-supervised audio-video representations: (1) Efficient contrastive learning for both intra-modal and inter-modal context via masking. (2) Introducing a new pretext task for multimodal MAE that predicts aligned and contextualized audio-video representations, surpassing the performance of reconstructing uni-modal raw inputs as used in conventional uni-modal/multimodal MAE. (3) Achieving new state-of-the-art performance in 7 audio-visual classification, audio-visual retrieval tasks, and (4) audio-only tasks under the SSL setup without using labeled data for pre-training.

## 2 Related Work

**Supervised Audio-Video Models.** The connection between visual signals and co-occurring audio context for language acquisition and world comprehension in infants  has motivated the research of audio-visual learning [76; 18]. Previous studies have explored audio-visual ASR [14; 73] and

Figure 1: **Masked Audio-Video Learners (MAViL) exploit three objectives for learning representations from audio video pairs with masking: (1) Raw audio-video reconstruction. (2) Inter-modal and intra-modal contrastive learning with masking. (3) Reconstructing aligned and contextualized audio-video representations via student-teacher learning. (Please see Fig. 2 for details.)**

person identification  prior to the deep learning era. More recently, there has been significant attention on audio-video representation learning for classification [65; 46; 22; 43; 95; 64]. However, these supervised approaches rely on abundant labeled data, which are costly and time-consuming to obtain. While efforts have been made to create large-scale labeled datasets , the dependency on extensive annotation hinders progress in audio-video modeling. In contrast, MAViL focuses on self-supervised learning of robust audio-video representations without the need for labeled data.

**Self-Supervised Audio-Video Representation Learning.** To exploit abundant unlabeled video and audio content on the Internet and reduce annotation efforts, self-supervised techniques have been explored for learning audio-video representations [4; 2; 3; 48; 82]. Inter-modal contrastive learning is a widely used approach that learns to associate paired audio-video clips as self-supervision [57; 63; 71]. Techniques such as data augmentation [70; 87] and harder negatives mining [98; 78; 62] have been studied to improve its performance. MAViL unifies masked autoencoding and contrastive learning. We notice concurrent and independent study CAV-MAE  and AV-MAE  recently explored joint masked autoencoding to reconstruct raw audio and video. Unlike CAV-MAE, MAViL takes a step further to incorporate intra-modal contrastive learning from two masked views. Most importantly, MAViL employs student-teacher learning to reconstruct contextualized and aligned representations instead of raw inputs as in CAV-MAE and AV-MAE. These innovations lead to MAViL's superior performance.

**Student-teacher learning** or knowledge distillation (KD) [40; 84; 69] was originally developed for model compression [60; 17], aiming to transfer knowledge from a larger teacher model to a smaller student model. In the context of SSL, KD has recently gained attention for improving the distribution of representations by distilling  or by reconstructing contextualized targets from a teacher model [33; 8; 92]. Approaches like MoCo , DINO , and dBOT  utilize self-training to bootstrap targets from previous model snapshots during pre-training. Data2vec  performs self-training in disparate modalities, where each one bootstraps complete-view contextualized targets independently. In contrast, to our best knowledge, MAViL is the first work that employs masked multimodal context for self-training. It uniquely incorporates complete-view multimodal fusion in the teacher model, while the student model receives masked-view inputs.

## 3 Masked Audio-Video Learners

We introduce Masked Audio-Video Learners (MAViL), a self-supervised audio-video representation learning framework (see Fig. 1). MAViL consists of two stages: In stage-1, MAViL jointly reconstructs raw spectrograms and RGB frames by exploiting complementary information from each modality (SS3.1), and couples this with contrastive learning to encourage alignment between semantically similar instances, both _within_ and _across_ modalities. (SS3.2). In stage-2 (see Fig. 2), we employ either the stage-1 model trained with raw targets (in iter. 1) or the last trained stage-2 model (in iter. 2+) as the teacher for self-training. We use the teacher's aligned and contextualized audio-video representations, obtained with complete-view inputs, to guide the student with masked-view inputs (SS3.3). In the following, we provide details of these three types of self-supervision, starting with MAViL-stage1 trained with raw inputs.

### Masked Raw Audio-Video Reconstruction

Human perception involves processing visual and acoustic context jointly. In line with this, MAViL utilizes multimodal Transformers to fuse and exploit the complementary information from both audio and video. It aims to reconstruct audio and video simultaneously as self-supervision, which sets it apart from uni-modal MAE approaches such as MAE , Audio-MAE , or Video-MAE .

Given a raw audio-video pair \((,)\), we begin by patchifying and tokenizing raw audio spectrograms and video frames into audio and video tokens. This involves applying (audio/video) transforms, followed by 2D/3D-convolutions and flattening. This process embeds raw inputs into \(=[a_{1} a_{N}]\) audio spectrogram tokens and \(=[v_{1} v_{M}]\) video tokens, where \(a_{i},v_{j}^{H}\). To incorporate positional information, similar to MAE, we employ fixed 2D sinusoidal positional embeddings with the embedded tokens for each modality. We then randomly mask the majority (_i.e_. 80%) of audio and video tokens. Only the remaining \(20\%\) unmasked audio (\(}\)) and video (\(}\)) tokensare respectively fed into the audio (\(f_{}(.)\)) and video (\(f_{}(.)\)) Transformer encoders. This process results in uni-modal embeddings, denoted as \(_{}=f_{}(^{})\) and \(_{}=f_{}(^{})\).

Following the uni-modal encoders, we incorporate a multimodal _fusion_ encoder denoted as \(g_{}(.)\) to model multimodal context. We explore two variants for this purpose: Vanilla Transformers  and Multimodal Bottleneck Transformers (MBT) . For vanilla Transformers, we jointly encode the audio and video tokens by: \((_{}^{l+1}||_{}^{l+1})=^{ l}(_{}^{l}\|_{}^{l})\), where "\(\|\)" represents concatenation. Details of MBT implementation is in Appendix. In both variants, we stack \(L\) Transformer layers to obtain the jointly encoded top-layer outputs \(_{}\) and \(_{}\).

For reconstruction, we employ vanilla Transformer blocks as the audio \(f_{}^{-1}(.)\) and video \(f_{}^{-1}(.)\) decoders. The fusion encoder's outputs (\(_{}\) and \(_{}\)) are firstly projected and padded with trainable [MASK] tokens. After restoring the original order (time-frequency for audio and space-time for video tokens), we add the decoders' (fixed 2-D sinusoidal) positional embeddings and input the restored sequences into the decoders. At the top of the decoders, we incorporate linear heads to reconstruct the raw inputs. Specifically, the decoder outputs for spectrogram reconstruction are denoted as \(}=f_{}^{-1}(g_{}(f_{}(^{ })))\) and for RGB frame reconstruction as \(}=f_{}^{-1}(g_{}(f_{}(^{ })))\). For notation clarity, we omit the [MASK] tokens and linear projection head. Let \(}_{i},_{i}^{}^{H_{}^ {*}};i=1 n\) denote the audio decoder's output and the ground truth reference of the \(i\)-th masked spectrogram patch. Similarly, \(}_{j},_{j}^{}^{H_{} };j=1 m\) for video patches. In masked audio-video reconstruction, MAViL is self-supervised by minimizing the mean squared error (MSE) loss \(_{r}^{}\) defined as:

\[_{r}^{}=_{i=1}^{n}(}_{i}- _{i}^{})^{2}+_{j=1}^{m}(}_{ j}-_{j}^{})^{2}.\] (1)

### Contrastive Audio-Video Learning with Masking

Contrastive learning is widely used to learn uni-modal [13; 36; 15] and multimodal [75; 71] representations by aligning multiple "views" of the same instance. These views can be either _within_-modality observations of the instance itself (e.g., the same audio under different volumes) or semantic observations _across_ modalities (e.g., a video and its corresponding audio). MAViL utilizes InfoNCE  loss for contrastive learning. Let \(=[_{1}_{B}],=[_{1} _{B}];_{i},_{j}^{H}\) be the instance-level representations of audio/video in a batch of size \(B\). The contrastive loss \(_{}(,)\) is defined as:

\[_{}(,)=-_{i=1}^{B} (_{i},_{i})/)}{_{j=1}^{B} ((_{i},_{j})/))},\] (2)

where \((_{i},_{j})=_{i}^{T}_ {j}}{\|_{i}\|\|_{j}\|}\) is the cosine similarity between \(_{i}\), \(_{j}\) and \(\) is the softmax temperature. MAViL employs two types of contrastive losses for self-supervision listed below:

**Inter-modal contrastive learning** facilitates alignments _across_ modalities. We first average the sequence of uni-modal encoder outputs1 as the instance-level representations, namely, \(_{}=(_{})\) and \(_{}=(_{})\). The positive pairs (the numerator in Eq.(2)) consist of video and audio clips from the same video, while all the other combinations of sampled audio-video pairs are considered negatives (the denominator). Inter-modal contrast encourages the representations of paired audio and video to be closer to each other, while simultaneously pushing away mismatched ones.

**Intra-modal contrastive learning** promotes alignment _within_ each modality. It aims to bring the representations of different views, such as different augmented versions of an audio (or video), closer to each other. To achieve this, for each modality, we apply random masking and sample a second view that contains 20% tokens for encoding. The idea is to treat masking as a form of augmentation to generate two contrasting views in the same modality, which has been proven effective in visual SSL [36; 13]. The two views of the same instance are considered as a positive pair, which are then contrasted against the other (negative) combinations of the instances in the same modality. Formally, let \(}_{}\) and \(}_{}\) be the embeddings of the second-view audio and video. MAViL employs the inter-modal (\(_{}^{}\)) and intra-modal (\(_{}^{}\)) contrastive objectives defined as:

\[_{}^{}=[_{c}(_{},_{})+_{c}(_{}, _{})],_{}^{}= [_{c}(_{},}_{ })+_{c}(_{},}_{})],\] (3)

These contrastive losses effectively learn a latent audio-video space where semantically similar instances are close to each other. Note that unlike prior work (_e.g_. [36; 13]), MAViL performs contrastive learning under masked-view. This leads to improved computation efficiency as only a small portion of input tokens are encoded for contrast. And different from CAV-MAE , MAViL additionally incorporates intra-modal contrast, which yields superior performance. Overall, let \(\) and \(\) be the weights that balance loss terms, MAViL-stage1 is self-supervised by minimizing:

\[_{}=_{r}^{}+_{ }^{}+_{}^{},\] (4)

### Masked Contextualized Audio-Video Reconstruction

To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [37; 41; 25], multimodal MAE , and CAV-MAE . We propose a new pretext task that reconstructs contextualized audio-video representations in the joint latent space. To achieve this, we employ a simple two-stage training framework illustrated in Fig. 2. In stage-1, we train MAViL with Eq.(4) to reconstruct raw inputs. In stage-2, we employ a student-teacher learning framework for iterative self-training. In the first iteration, the pre-trained MAViL from stage-1 is frozen and serves as the _teacher_ model. It generates audio-video representations with complete-view inputs, which are used as the reconstruction targets to guide the re-initialized _student_ MAViL. In the subsequent iterations, the last trained stage-2 student MAViL serves as the new teacher.

Formally, we provide the teacher model's encoders with complete-view audio and video inputs to generate aligned and contextualized targets: \(^{}\|^{}=g_{}^{ }(f_{}^{}()\|f_{}^{ }())\). The student MAViL then learns to reconstruct these contextualized targets with the masked-view inputs. Precisely, \(}=f_{}^{-1}(g_{}(f_{}(^{ })))\) for audio and \(}=f_{}^{-1}(g_{}(f_{}(^{ })))\) for video, where \(^{},^{},}, }^{H}\). The contextualized reconstruction objective is defined as:

\[_{r}^{}=_{i=1}^{n}(} _{i}-_{i}^{})^{2}+_{j=1}^{m}(}_{j}-_{j}^{})^{2}.\] (5)

In the stage-2 training, we jointly minimize the masked contextualized reconstruction loss and the contrastive loss. The stage-2 (student) MAViL's objective is:

\[_{}=_{r}^{}+ _{}^{}+_{}^{}.\] (6)

Note that Eq.(6) contains pure latent targets2. After pre-training, we then fine-tune the audio/video encoders in the final stage-2 MAViL student model in the downstream tasks.

Figure 2: **Masked contextualized audio-video reconstruction** in the joint latent space. Stage-1 (left): Training MAViL with raw inputs as targets. Stage-2 (right): Self-training MAViL student by reconstructing MAViL teacher’s aligned and contextualized audio-video representations generated with complete inputs. Repeat stage-2 for \(K\) iterations. In the first iteration of stage-2, the stage-1 model is used as the teacher. In subsequent iterations (iteration 2+), the last trained stage-2 student model serves as the new teacher.

Experiments

We performed comprehensive evaluations, including audio-video classification tasks on AudioSet  (AS-2M and AS-20K), and VGGSound . Also, we conducted audio-to-video retrieval experiments on MSR-VTT  and YouCook . We use AS-20K for model analysis and ablation studies.

### Datasets

**AudioSet** contains 2 million 10-second YouTube clips for audio event detection. 527 event types are weakly labeled [51; 38; 39] for each clip, and multiple events can occur in one clip. AudioSet's full training set has two subsets: A class-wise _balanced_ (22K clips) and an _unbalanced_ (2M clips) set. The _eval_ set has 20K clips. We downloaded 1.97M unbalanced training, 20K balanced training, and 19K evaluation clips. We use the full (unbalanced+balanced) training set for pre-training. In the AS-2M task, we fine-tune on the full training set. In the AS-20K task, we fine-tune only on the 20K balanced training set. We report the classification mAP on the 19K _eval_ set used by AST .

**VGGSound** comprises approximately 200K 10-second video clips annotated with 309 event types that include human actions, sound-emitting objects, _etc._ Unlike AudioSet, VGGSound ensures that an audio event is also visually present in its corresponding clips. VGGSound is divided into 183K training and 15K testing samples. We report top-1 testing classification accuracy.

### Implementation Details

MAViL adopts different temporal footprints for audio and video. For audio, following [64; 30], it transforms a 10-second audio under 16K sampling rate into 128 Mel-frequency bands with a 25ms Hanning window shifting every 10ms. The resulting spectrogram is of dimension \(1024 128\). MAViL then tokenizes it into non-overlapping \(16 16\) patches where both time and frequency have a kernel and stride of 16. The flattened audio tokens have a sequence length \(N\) of 512. For video, MAViL processes 4-second clips under 2 frames per second. Each frame has a size of \(224 224\). Tokenization is achieved by a 3D convolution, where the spatial kernel and stride are 16, and the temporal kernel and stride are 2. The flattened video tokens have a sequence length \(M\) of 784.

Following the design choices in MAE , MAViL employs 12-layer Transformers (ViT-B) with 12 attention heads as the encoders for each modality. The embedding dimension \(H\) is set to \(768\). The audio-video fusion encoder layer consists of a 2-layer Transformer (vanilla or MBT) on top of the uni-modal encoders. Similarly, the audio and video decoders utilize 8-layer Transformers with an embedding dimension of 512 and 16 attention heads. MAViL's audio/video encoder and decoder have 86M and 27M parameters, respectively. The floating point operations (FLOPs) for the audio encoder are 48.6G, comparable to the audio encoders in Audio-MAE  and CAV-MAE .

For pre-training MAViL's audio branch, we randomly initialize it from scratch. For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE  pre-trained on ImageNet (compared in Table 6). Notably, MAViL operates under the fully _self-supervised_ setup.

### Experimental Setup

We pre-train MAViL on AS-2M without using any of AS-2M labels. We use 80% masking for audio and video. For balancing the losses in Eq.(6), we set \(=0.1\), \(_{}^{}=0.1\) and \(=0.01,_{}^{}=1.0\). These hyperparameters scale the gradients from the three losses into a comparable range to improve training stability. We pre-train with 64 V100 GPUs with a 512 accumulated batch size and a 0.0002 learning rate. We pre-train for 20 epochs in stage-1 and in each iteration of stage-2 (for (\(K=3\)) iterations). Each session takes 20 hours, resulting in a total pre-training time around 80 hours.

We then evaluate audio and video representations by fine-tuning the pre-trained MAViL in audio-video classification tasks on AS-20K, AS-20M, and VGGSound. We test 3 scenarios: (1) audio-only, (2) video-only, and (3) audio+video. Following MAE , we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification. We adopt the standard fine-tuning pipeline and augmentation in prior audio/audio-video classification works [30; 41; 64]. Specifically, we employ SpecAug , mixup , balanced sampling , and fine-tuning masking . For video, we use standard video augmentations used in video classification [90; 27]. To address the discrepancy in convergence rate between audio and video, we apply a 50% learning rate reduction for the video encoder during the audio+video (fusion) fine-tuning. Fine-tuning for 100 epochs on AS-2M takes 10 hours. Please refer to Appendix for additional implementation and experimental details.

In the following, we first use AS-20K to analyze MAViL's performance in SS4.4. We then present the main comparison to prior works in SS4.5. The gray entries represent the shared default setup.

### Model Analysis

**Masked Raw Audio-Video Reconstruction.** Table 1 shows the contribution of MAViL's audio-video fusion encoder layer, which exploits complementary multimodal information for reconstructing raw inputs in stage-1. The 'None" (col. 1) indicates the uni-modal MAE baselines without the fusion layer, namely Audio-MAE and Video-MAE under our implementation. The fusion layer, whether it's a vanilla Transformer or MBT, contributes up to 0.4 mAP gains (col. 2-5). While the MBT layers show better results, the improvements is not significant compared to using vanilla Transformers. Also, increasing the depth of the fusion encoder only leads to marginal gains. For simplicity, we default to using a 2-layer vanilla Transformer as the fusion encoder.

Subsequently, we investigate the masking ratio, a key hyperparameter in masked autoencoding. We use the same masking ratio for audio and video, and ablate different values in Table 2. The results suggest a masking ratio of 80% achieves the best performance. This aligns with the optimal values of 80% in Audio-MAE  and 90% in Video-MAE . MAViL encodes only the non-masked tokens significantly reduces the sequence length, resulting in more efficient computation, as the complexity of self-attention layers in Transformers scales quadratically with the sequence length.

**Contrastive Audio-Video Learning with Masking.** Next, we examine the benefits when contrastive learning is used in conjunction with MAE. MAViL performs efficient contrastive learning with only 20% of visible patches in each modality (80% are masked). The results in Table 3 demonstrate the significance of contrastive losses for learning audio-video representations, even with only 20% visible tokens. Unlike CAV-MAE  which uses only inter-modal contrast, we observe that both inter-modal and intra-modal contrast are essential, and combining them delivers the best performance.

**Masked Contextualized Audio-Video Reconstruction.** As illustrated in Fig. 2, MAViL employs a two-stage training. Following stage-1 training (Table 3), MAViL's encoders acquire complementary audio-video representations through masked raw audio-video reconstruction, as well as aligned representations through masked contrastive learning. In stage-2, we employ an iterative approach to initialize the student MAViL and train it under masked-views to predict the aligned and contextualized representations generated by the frozen MAViL teacher with full view. The teacher can be either from the stage-1 model (iter. 1) or the last trained stage-2 model (iter. 2 and beyond).

We first compare the reconstruction targets in Table 3(a) (stage-2, iter. 1). By default MAViL student predicts the last trained MAViL teacher's multimodal fusion encoder outputs (_i.e._, \(g_{}^{}(f_{}^{}()\|f_{ }^{}()\)), M-Fusion, col. 4). The results show that this improves over predicting (col. 1) raw spectrogram/RGB frames (+1.7 audio mAP) and (col. 2) separately pre-trained uni-modal Audio/Video-MAE's outputs (+1.2 audio mAP). The fused multimodal targets (M-Fusion, col. 4) are better (+0.5 audio mAP) than the uni-modal targets (M-Uni, col. 3) (_i.e._, \(f_{}^{}()\) and \(f_{}^{}()\) before the fusion encoder).

 Fusion & None & Vanilla(2) & Vanilla(4) & MBT(4) \\  Audio & 36.4 & 36.8(+0.4) & 36.7(+0.3) & 36.8(+0.4) \\ Video & 17.4 & 17.7(+0.3) & 17.6(+0.2) & 17.7(+0.3) \\ 

Table 1: **Fusion Encoder**: Transformer type(# layers). Note that the ‘None’ are the uni-modal MAE baselines (Audio-MAE and Video-MAE). mAP\(\) in AS-20K.

 Objs. & None & Inter & Intra & Inter+intra \\  Audio & 36.8 & 38.4(+1.6) & 38.1(+1.3) & 39.0(+2.2) \\ Video & 17.7 & 21.0(+3.3) & 19.8(+2.1) & 22.2(+4.5) \\ 

Table 3: **Masked Contrastive Audio-Video Learning**For the 2nd and beyond iterations of stage-2 training, we leverage the last trained MAViL student as the new teacher. As shown in Table 3(b), substantial improvements (asymptotically saturated after the 3rd iteration) are achieved over iterations through masked prediction of aligned and contextualized audio-video representations by the MAViL teacher. The accumulated gains with the proposed contextualized audio-video reconstruction in stage-2 are +2.8 audio and +2.6 video mAP _without_ using additional data or external teacher models.

Additional Ablations.In Table 5 we ablate several more aspects of MAViL: (a) Pre-training with more data is useful (100% > 50% > 10% of AS-2M) (Tab. 4(a)). (b) ImageNet (IN) self-supervised pre-training is beneficial for the video encoder (Table 4(b)). ImageNet supervised pre-training (IN-SL) is useful yet we avoid this to keep MAViL fully self-supervised. (c) Longer pre-training is beneficial and 20 epochs are sufficient (Table 4(c)). (d) Increasing the encoder sizes (ViT-S/B/L) improves mAP (Table 4(d)). We use ViT-B for efficiency and fair comparison as default.

Summary.The module-wise contribution is summarized in Table 4(e). MAViL enhances uni-modal MAE performance by learning from audio _and_ video simultaneously, as evidenced by significant increases in audio mAP (36.4\(\)41.8, +5.4) and video mAP (17.4\(\)24.8, +7.4) on AS-20K. It is noteworthy that while audio and video are jointly used in the pre-training phase, the fine-tuning is performed with distinct uni-modal encoders on each modality. Therefore, the improvements in the uni-modal classification tasks showcase SSL pre-training from both modalities can enhance the performance of individual modalities.

### Main Results

The full model is the stage-2 MAViL student (ViT-B) trained with Eq.(6) for 3 iterations. We quantitatively compare it with other previous models on the 7 benchmark tasks.

Audio-Video Event Classification.Table 6 presents a comparison of MAViL's audio (A), video (V), and audio-video (A+V) fine-tuning performance on AudioSet (AS) and VGGSound, alongside recent baselines. Notably, MAViL achieves new state-of-the-art results in audio-only and audio+video classification tasks across these datasets. On the balanced AS-20K, unbalanced AS-2M, and VGGSound classification tasks, MAViL surpasses CAV-MAE  (both models are with ViT-B encoders) in A, V, and A+V tasks by a large margin. This improvement can be attributed to the benefits of reconstructing _aligned_ and _contextualized_ representations over raw inputs and the enhanced contrastive learning through both _intra_-modal and _inter_-modal contrast. Furthermore, MAViL outperforms data2vec , highlighting the advantage of utilizing aligned multimodal contexts (Eq.(5)) over uni-modal contexts as the reconstruction targets.

Table 4: **Targets** and **iterations** in MAViL’s stage-2 contextualized audio-video reconstruction. (a) Target comparison in the 1st iteration. Raw: spectrogram/RGB frames. Contextualized: A-MAE/V-MAE (uni-modal MAE output); M-Uni (MAViL’s uni-modal output); M-Fusion (MAViL’s multimodal fusion output). (b) Iteration comparison. ‘None’ is the MAViL-stage1 baseline in Table 3.

Table 5: **Additional ablations** and **module-wise contributions** (mAP on AS-20K)The fully self-supervised MAViL also demonstrates superior performance to supervised audio-video models such as MBT  in A and A+V classification tasks on AS-20K, AS-2M, and VGGSound. For video classification on AudioSet, it is worth noting that the fully self-supervised trained video backbone still lags behind MBT which pre-trained with the supervision from ImageNet-21K (11 times larger than ImageNet). This disparity is a consequence of the presence of noise and irrelevant visual context in AudioSet as also discussed in . Such bias could make the visual pre-training sub-optimal on AudioSet. We consider resolving this dataset limitation as the future work.

Transfer to Audio/Speech-only Tasks.To access the generalizability of the learned audio representations, we further evaluate the AS-pre-trained MAViL by transferring it to other out-of-domain speech-only or audio-only tasks. Specifically, we conduct experiments on the Environmental Sound Classification (ESC-50)  and Speech Commands (SPC-v1) , where only the audio branch of MAViL is fine-tuned. The results, presented in Table 7, demonstrate that MAViL outperforms recent supervised and self-supervised models, establishing a new state-of-the-art performance on these benchmarks. These findings indicate the desirable transferability of MAViL from audio-video self-supervised pre-training to audio-only downstream tasks.

Audio-to-Video Retrieval.MAViL learns aligned audio-video representations that are suitable for textless cross-modal retrieval. To verify this, we further conduct audio-to-video retrieval experiments on MSR-VTT  and YouCook . In these tasks, the audio track of a video serves as the query, and the model performs a search over the testing video collection by computing and ranking the similarity between the query embedding and the video embeddings. We fine-tune MAViL using the audio-video pairs in the training sets with Eq.(6). We report recall@1 on the testing sets.

  &  &  & VGGSound (Acc.\(\)) \\  Method & PT & A & V & A+V & A & V & A+V & A & V & A+V \\   \\ Aud-SlowFast  & - & - & - & - & - & - & - & 50.1 & - & - \\ VGGSound  & - & - & - & - & - & - & - & 48.8 & - & - \\ PANNs  & - & 27.8 & - & - & 43.9 & - & - & - & - & - \\ AST  & IN-SL & 34.7 & - & - & 45.9 & - & - & - & - & - \\ HTS-AT  & IN-SL & - & - & - & 47.1 & - & - & - & - & - \\ PaSST  & IN-SL & - & - & - & 47.1 & - & - & - & - & - \\ Data2vec  & AS-SSL & 34.5 & - & - & - & - & - & - & - & - \\ SS-AST  & AS-SSL & 31.0 & - & - & - & - & - & - & - & - \\ MAE-AST  & AS-SSL & 30.6 & - & - & - & - & - & - & - & - \\ Aud-MAE  & AS-SSL & 37.0 & - & - & 47.3 & - & - & - & - & - \\   \\ G-Blend  & - & 29.1 & 22.1 & 37.8 & 32.4 & 18.8 & 41.8 & - & - & - \\ Perceiver  & - & - & - & - & 38.4 & 25.8 & 44.2 & - & - & - \\ Attn AV  & IN-SL & - & - & - & 38.4 & 25.7 & 44.2 & - & - & - \\ CAV-MAE  & IN-SSL, AS-SSL & 37.7 & 19.8 & 42.0 & 46.6 & 26.2 & 51.2 & 59.5 & 47.0 & 65.5 \\ \(^{}\) & IN21K-SL & 31.3 & 27.7 & 43.9 & 41.5 & 31.3 & 49.6 & 52.3 & **51.2** & 64.1 \\  MAViL & AS-SSL & 41.6 & 23.7 & 44.6 & **48.7** & 28.3 & 51.9 & 60.6 & 50.0 & 66.5 \\ MAViL & IN-SSL, AS-SSL & **41.8** & **24.8** & **44.9** & **48.7** & **30.3** & **53.3** & **60.8** & 50.9 & **67.1** \\  

Table 6: **Comparison to prior work on AudioSet (AS-20K, AS-2M) and VGGSound in the audio (A), video (V) and audio+video (A+V) classification tasks. PT: pre-training dataset and type; IN: ImageNet; SL: supervised learning; SSL: self-supervised learning; \({}^{*}\):We de-emphasize the model using non-standard dataset splits. We bold the best-performing single model.**Following AVILNet  and TVLT , we explore an alternative setup where we pre-train MAViL on HowTo100M , a dataset consisting of 1.3 million instructional videos, instead of using AudioSet for pre-training. It's worth noting that there are notable domain differences between the pre-training datasets (AudioSet or HowTo100M) and the downstream datasets (MSR-VTT or YouCook). Table 8 demonstrates that MAViL surpasses supervised pre-trained AVLNet and self-supervised pre-trained TVLT, achieving new state-of-the-art performance on these tasks.

Raw Audio-Video ReconstructionsIn Fig. 3, we employ a stage-1 MAViL (ViT-B) to reconstruct raw audio spectrograms and video frames with masked inputs. The model is trained using an 80% masking ratio on the AudioSet-2M full training set with _un-normalized_ raw spectrograms and video frames as the reconstruction targets (Eq.(4), stage-1). We visualize the reconstruction results by MAViL's audio and video decoders, wherein 70% of the input tokens are masked to its encoders. This visualization is performed on the AudioSet _eval_ set.

The results show that MAViL accurately reconstructs the highly corrupted versions of both audio spectrograms and video frames. The reconstructions for videos exhibit high fidelity and preserve spatial and temporal consistency of visual objects (_e.g._, the nearby moving cars recorded by the ambulance's dash camera). For audio reconstructions, MAViL accurately maintains the positions and arrangements of time-frequency components in the spectrogram (_e.g._, the ambulance's siren and the song by the singer). Furthermore, the reconstructed audio and video components are consistent and well-aligned in time, enhancing the overall coherence of the reconstructed content.

## 5 Conclusion

We have presented MAViL, a self-supervised audio-video representation learning framework where masked autoencoding meets contrastive learning. By leveraging an encoder-fusion-decoder architecture, MAViL effectively utilizes complementary information from all modalities for masked autoencoding. It facilitates efficient contrastive learning in both inter-modal and intra-modal scenarios, even with a high 80% masking ratio. Furthermore, MAViL highlights a novel pre-training task with self-training to predict homogeneous contextualized audio-video representations. This approach outperforms conventional uni-modal and multimodal MAEs that predict heterogeneous raw inputs. As a result, MAViL achieves state-of-the-art performance across 7 audio-video classification and retrieval tasks, as well as audio-only tasks under the scalable self-supervised learning setup.

Figure 3: **Video clip and spectrogram reconstruction on the AudioSet _eval_ set**. We sample 4 paired (video, audio) examples as follows (click the links to play): Top left: a puppy video; Top right: a recording from an ambulance’s dash camera; Bottom left: a person dialing a phone in a dark room; Bottom right: a singer dancing. Input masking ratio: 70%. In each 3-row group, we show the original video and its audio spectrogram (top), masked input to MAViL (middle), and MAViL’s video and audio spectrogram reconstructions (bottom). The spectrogram shape is 1024\(\)128; patch size is 16\(\)16. Each spectrogram has 64\(\)8=512 patches. After applying 70% masking, there are 154 patches visible to MAViL. The 8-frame (4-second under 2 fps) video clip size is \(8 3 224 224\); patch size is \(16 16\). Each video has \(4 14 14=784\) patches after patch embedding (temporal kernel/stride=2). After applying 70% masking, there are 235 patches visible to MAViL.

Acknowledgements

We thank Luke Zettlemoyer, Kaiming He, Juncheng (Billy) Li, and Florian Metze for their feedback and discussions.