# N-Agent Ad Hoc Teamwork

Caroline Wang

Department of Computer Science

The University of Texas at Austin

caroline.l.wang@utexas.edu

&Arrasy Rahman

Department of Computer Science

The University of Texas at Austin

arrays@cs.utexas.edu

&Ishan Durugkar

Sony AI

ishan.durugkar@sony.com

&Elad Liebman

Amazon

liebelad@amazon.com

&Peter Stone

Department of Computer Science

The University of Texas at Austin and Sony AI

pstone@cs.utexas.edu

Work was done while at SparkCognition.

###### Abstract

Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls _all_ agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a _single_ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce \(N\)_-agent ad hoc teamwork_ (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and proposes the _Policy Optimization with Agent Modelling_ (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on tasks from the multi-agent particle environment and StarCraft II shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.

## 1 Introduction

Advances in multi-agent reinforcement learning (MARL)  have enabled agents to learn solutions to various problems in zero-sum games, social dilemmas, adversarial team games, and cooperative tasks . Within MARL, cooperative multi-agent reinforcement learning (CMARL) is a paradigm for learning agent teams that solve a common task via interaction with each other and the environment . Recent CMARL methods have been able to learn impressive examples of cooperative behavior from scratch in controlled settings, where all agents are controlled by the same learning algorithm . A related paradigm for learning cooperative behavior is ad hocteamwork (AHT). In contrast to CMARL, the objective of AHT is to create a single agent policy that can collaborate with previously unknown teammates to solve a common task [27; 39].

While a large and impressive body of work on CMARL and AHT exists, the current literature has largely examined scenarios in which either complete control over all agents is assumed, or only a single agent is adapted for cooperation [33; 38; 21; 4; 15]. Even learning methods for handling cooperative tasks in open multiagent systems , which encompass one of the most challenging settings where agents may enter or leave the system anytime, either operate assuming full control over all agents [17; 23] or only a single adaptive agent [31; 18; 32].

However, real-world collaborative scenarios--e.g. search-and-rescue, or robot fleets for warehouses--might demand agent _subteams_ that are able to collaborate with unfamiliar teammates that follow different coordination conventions. Towards producing agent teams that are more flexible and applicable to realistic cooperative scenarios, this paper formalizes the problem setting of \(N\)**-agent ad hoc teamwork** (NAHT), in which a set of autonomous agents must interact with an uncontrolled set of teammates to perform a cooperative task. When there is only a single ad hoc agent, NAHT is equivalent to AHT. On the other hand, when all ad hoc agents are jointly trained by the same algorithm and there are no uncontrolled teammates, NAHT is equivalent to CMARL. Thus, the proposed problem setting generalizes both CMARL and AHT.

Drawing from ideas in both CMARL and AHT, we introduce Policy Optimization with Agent Modelling (POAM). POAM is a policy-gradient based approach for learning cooperative multi-agent team behaviors, in the presence of varying numbers and types of teammate behaviors. It consists of (1) an agent modeling network that generates a vector characterizing teammate behaviors, and (2) an independent actor-critic architecture, which conditions on the learned teammate vectors to enable adaptation to a variety of potential teammate behaviors. Empirical evaluation on multi-agent particle environment (MPE) and StarCraft II tasks shows that POAM learns to coordinate with a changing number of teammates of various types, with higher competency than CMARL, AHT, and naive NAHT baseline approaches. An evaluation with out-of-distribution teammates also reveals that POAM's agent modeling module improves generalization to out-of-distribution teammates, compared to baselines without agent modeling.

## 2 Background and Notation

The NAHT problem is formulated within the framework of **Decentralized Partially Observable Markov Decision Processes**, or Dec-POMDPs . A Dec-POMDP consists of \(M\) agents, a state space \(\), action space \(\), per-agent observation spaces \(O_{i}\), transition function \(:()\)2, common reward function \(r:()\) (thus defining a cooperative task), discount factor \(\) and horizon \(T\), which represents the maximum length of an interaction, or episode. Each agent observes the environment via its observation function, \(_{i}:(O_{i})\). The state space is factored such that \(=_{1}_{M}\), where \(_{i}\) for \(i\{1 M\}\) corresponds to the state space for agent \(i\). The action space is defined analogously. Denoting \(H_{i}\) as its space of localized observation and action histories, agent \(i\) acts according to a policy, \(_{i}:H_{i}(_{i})\). The notation \(-i\) represents all agents other than agent \(i\), and is applied throughout the paper to the mathematical

Figure 1: Left: CMARL algorithms assume full control over all \(M\) agents in a cooperative scenario. Center: AHT algorithms assume that only a single agent is controlled by the learning algorithm, while the other \(M-1\) agents are uncontrolled and can have a diverse, unknown set of behaviors. Right: NAHT, the paradigm proposed by this paper, assumes that a potentially varying \(N\) agents are controlled by the learning algorithm, while the remaining \(M-N\) agents are uncontrolled.

objects introduced above. For example, the notation \(_{-i}\) refers to the cross product of the observation space of all agents other than \(i\). In the following, we overload \(r\) to refer to both the reward function, and the task defined by that reward function, whereas \(r_{t}\) denotes the reward at time step \(t\).

## 3 NAHT Problem Formulation

Drawing from the goals of MARL and AHT , the goal of \(N\)-agent ad hoc teamwork is **to create a set of autonomous agents that are able to efficiently collaborate with both known and unknown teammates to maximize return on a task**. The goal is formalized below.

Let \(C\) denote a set of ad hoc agents. If the policies of the agents in \(C\) are generated by an algorithm, we say that the algorithm controls agents in \(C\). Since our intention is to develop algorithms for generating the policies of agents in \(C\), we refer to agents in \(C\) as _controlled_. Let \(U\) denote a set of _uncontrolled_ agents, which we define as all agents in the environment not included in \(C\).3 Following Stone et al., we assume that agents in \(U\) are not adversarially minimizing the objective of agents in \(C\).

We model an open system of interaction, in which a random selection of \(M\) agents from sets \(C\) and \(U\) must coordinate to perform task \(r\). For illustration, consider a warehouse staffed by robots developed by companies \(A\) and \(B\), where there is a box-lifting task that requires three robots to accomplish. If Company A's robots are controlled agents (corresponding to \(C\)), then some robots from Company A could collaborate with robots from Company B (corresponding to \(U\)) to accomplish the task, rather than requiring that all three robots come exclusively from \(A\) or \(B\). Motivated thus, we introduce a _team sampling procedure_\(X(U,C)\). At the beginning of each episode, \(X\) samples a team of \(M\) agents by first sampling \(N<M\), then sampling \(N\) agents from the set \(C\) and \(M-N\) agents from \(U\). We restrict consideration to teams containing at least one controlled agent, i.e \(N 1\). We consider \(X\) a problem parameter that is not under the control of any algorithm for generating ad hoc teammates, analogous to the transition function of the underlying Dec-POMDP. A more explicit definition of \(X\) is provided in Appendix A.1.

Without loss of generality, let \(C()=\{_{i}^{}(.|s)\}_{i=1}^{N}\) denote a _set_ of \(M\) controlled agent policies parameterized by \(\), such that a learning algorithm might optimize for \(\). Let the \(^{(M)}\) indicate a _team_ of \(M\) agents and \(^{(M)} X(U,C)\) indicate sampling such a team from \(U\) and \(C\) via the team sampling procedure. The _objective_ of the NAHT problem is to find parameters \(\), such that \(C()\) maximizes the expected return in the presence of teammates from \(U\):

\[_{}(_{^{(M)} X(U,C())} [_{t=0}^{T}^{t}r_{t}]).\] (1)

Challenges of the NAHT problem include: (1) coordinating with potentially unknown teammates (_generalization_), and (2) coping with a varying number of uncontrolled teammates (_openness_).

## 4 The Need for Dedicated NAHT Algorithms

Having introduced the NAHT problem, a natural question to consider is whether AHT solutions may optimally address NAHT problems. If so, then there would be little need to consider the NAHT problem setting. For instance, a simple yet reasonable approach consists of directly using an AHT policy to control as many agents as required in an NAHT scenario. This section illustrate the limitations of the aforementioned approach by giving a concrete example of a matrix game where (1) an AHT policy that is learned in the AHT (\(N=1\)) scenario is unlikely to do well in an NAHT scenario where \(N=2\), and (2) even an _optimal_ AHT policy is suboptimal in the \(N=2\) setting.

Define the following simple game for \(M\) agents: at each turn, each agent \(a_{i}\) picks one bit \(b_{i}\{0,1\}\); at the end of each turn, all the bits are summed \(s=_{i}b_{i}\). The team wins if the sum of the chosen bits is exactly \(1\). We denote the probability of winning by \(P(s=1)\). Suppose the uncontrolled agentsfollow a policy that independently selects \(1\) with probability \(p=\).4 In the following, we consider the three agent case, \(M=3\), for simplicity.

In the AHT problem setting, a learning algorithm assumes control of only a single agent. Let \(p_{}\) denote the probability with which the AHT agent selects 1. Given the aforementioned team of uncontrolled agents, we show that _any_ value of \(p_{}\) results in the same probability of winning, which occurs because the probability of winning, \(P(s=1)=\), is independent of \(p_{}\) (Lemma A.2).

Next, consider an NAHT scenario where a learning algorithm must define the actions of two out of three agents. Suppose that the same AHT policy is used to control both agents: both agents select 1 with probability \(p_{}\). Above, we demonstrated that an AHT algorithm trained in the \(N=1\) scenario could result in learning any \(p_{}\). However, in the \(N=2\) setting, we show that the optimal AHT policy \(p_{}=\) and the winning probability for this policy is \(P(s=1)=\) (Lemma A.3).

Finally, we show there exists an NAHT policy that controls both agents and obtains a higher winning probability. Consider the policy where one controlled agent always plays 0, while the other plays 1 with probability \(p_{}\). Lemma A.4 shows that the optimal \(p_{}=1\), and the probability of winning \(P(s=1)=>\). Thus, we have exhibited an NAHT scenario where an AHT policy that is optimal when \(N=1\), performs worse than a simple NAHT joint policy in the \(N=2\) setting. Empirical validation of the prior results are provided in Appendix A.5.1.

## 5 Policy Optimization with Agent Modeling (POAM)

This section describes the proposed Policy Optimization with Agent Modeling (POAM) method, which trains a collection of NAHT agents that can adaptively deal with different collections of unknown teammates. POAM relies on an _agent modeling network_ to initially build an embedding vector characterizing teammates encountered during an interaction. Adaptive agent policies that can maximize the controlled agents' returns are then learned by training a _policy_ conditioned on the environment observation and team embedding vector. To enable controlling a varying number of agents while learning in a sample-efficient manner, POAM adopts the independent learning framework with full parameter sharing. The training processes for agent modeling and policy networks are described in Sections 5.1 and 5.2 respectively, while an illustration of how POAM trains NAHT agents is provided in Figure 2.

### Agent Modeling Network

Designing adaptive policies that enable NAHT agents to achieve optimal returns against any team of uncontrolled agents drawn from some set \(U\), requires information on the encountered team's unknown behavior. However, in the absence of prior knowledge about uncontrolled teammates' policies, local observations from a single timestep may not contain sufficient information regarding the encountered team. To circumvent this lack of information, POAM's agent modeling network plays a crucial role in providing _team embedding vectors_ that characterize the observed behavior of teammates in the encountered team.

We identify two main criteria for desirable team embedding vectors. First, team embedding vectors should identify information regarding the unknown state and behavior of other agents in the environment (both controlled and uncontrolled). Second, team embedding vectors should ideally

Figure 2: POAM trains a single policy network \(^{_{p}}\), which characterizes the behavior of all controlled agents (green), while uncontrolled agents (yellow) are drawn from \(U\). Data from both controlled and uncontrolled agents is used to train the value network, \(V_{i}^{_{e}}\) while the policy is trained on data from the controlled agents only. The policy and value function are both conditioned on a learned team embedding vector, \(e_{i}^{t}\).

be computable from the sequence of local observations and actions of the team. Fulfilling both requirements provides an agent with useful information for decision-making in NAHT problems, even under partial observability.

For each controlled agent, POAM produces informative team embedding vectors by training a model with an encoder-decoder architecture, illustrated by red components in Figure 2. For ease of presentation, the encoder-decoder models for controlled agent \(i\) will be referred to without the index \(i\). The encoder, \(f^{}_{^{e}}:H_{i}^{n}\), is parameterized by \(^{e}\) and processes the modeling agent's history of local observations and actions up to timestep \(t\), \(h^{t}_{i}=\{o^{k}_{i},a^{k-1}_{i}\}_{k=1}^{t}\), to compute a team embedding vector of dimension \(n\), \(e^{t}_{i}^{n}\) that characterizes the modeled agents. This reliance on local observations helps ensure that the agent modeling network can operate without having access to the environment state that is unavailable under partial observability. The team embedding vector is decoded by two decoder networks: the observation decoder, \(f^{}_{^{e}}:^{n} O_{-i}\) and the action decoder, \(f^{}_{^{e}}:^{n}(A_{-i})\). The decoder networks are respectively trained to predict the observations and actions of all other agents on the team at timestep \(t\), \((o^{t}_{-i},a^{t}_{-i})\), to encourage \(e^{t}_{i}\) to contain relevant information for the current NAHT agent's decision-making process. While the observation decoder directly predicts the observed \(-i\) observations, the action decoder predicts the parameters of a probability distribution over the \(-i\) agents' actions, \(p(a^{t}_{-i};f^{}_{^{e}}(f^{}_{^{e}}(h^{t}_{i })))\), where an appropriate distribution for \(p\) should be selected by the system designer.

Concretely, agent \(i\)'s encoder-decoder model is trained to minimize a maximum likelihood loss over all teammates' observations and actions, given its own local observations and actions. As the experimental setting in this paper considers continuous observations and discrete actions, the observation loss is a mean squared error loss, while the action loss is the negative log likelihood of the \(-i\) agents' actions, under the Categorical distribution.

\[L_{^{e},^{o},^{e}}(h^{t}_{i},o^{t}_{-i},a^{t}_{-i})=| |f^{}_{^{e}}(f^{}_{^{e}}(h^{t}_{i}))-o^{t}_{-i }||^{2}-(p(a^{t}_{-i};f^{}_{^{e}}(f^{}_{ ^{e}}(h^{t}_{i})))).\] (2)

### Policy and Value Networks

POAM relies on an actor-critic approach to train agent policies, where the policy and critic are both conditioned on the teammate embedding described in Section 5.1.

The policy network of agent \(i\), \(^{^{p}}_{i}:H_{i}^{n}(A_{i})\), is parameterized by \(^{p}\), and uses the NAHT agent's local observation, \(o^{t}_{i}\), and the team embedding from the encoder network, \(e^{t}_{i}\), to compute a policy followed by the NAHT agents. Conditioning the policy network on \(e^{t}_{i}\) allows an NAHT agent to change its behaviour based on the inferred characteristics of encountered agents. When training the policy network, we also rely on a value (or critic) network, \(V^{t^{^{e}}}_{i}:H_{i}^{n}\), parameterized by \(^{c}\), which measures the expected returns given \(h^{t}_{i}\), and \(e^{t}_{i}\). The value network serves as a baseline to reduce the variance of the gradient updates, while conditioning on the learned teammate embeddings for similar reasons to the policy.

POAM then trains the policy and value networks using an approach based on the Independent PPO algorithm  (IPPO). IPPO is selected as the base MARL algorithm for two reasons. First, using an independent MARL method circumvents the need to deal with the changing number of agents resulting from environment openness. Second, IPPO has been demonstrated to be effective on various MARL tasks. To improve learning efficiency and enable information sharing between agents, full parameter sharing is employed for all neural networks. POAM trains the value network to produce accurate state value estimates by minimizing the following loss function:

\[L_{^{e}}(h^{t}_{i})=V^{^{e}}_{i}(h^{t}_{i},f^{ }_{^{e}}(h^{t}_{i}))-^{t}_{i}^{2},\] (3)

where \(^{t}_{i}\) is the TD(\(\)) return. The policy network is analogously trained to minimize the PPO loss function , but where the policy additionally conditions on the team embeddings.

Leveraging data from uncontrolled agentsIn the NAHT setting, we assume access to the _joint_ observations and actions generated by the current team deployed in the environment _at training time only_, where the team consists of a mix of controlled and uncontrolled agents. This assumption provides an opportunity to learn useful cooperative behaviors more quickly, by bootstrapping based on transitions from the initially more competent, uncontrolled teammate policies, as also observed by Rahman et al. .

POAM leverages data from both controlled and uncontrolled agents to train the value network--in effect, treating the uncontrolled agents as exploration policies. Note that this aspect of POAM is a significant departure from PPO, which is a fully on-policy algorithm. Since the policy update is highly sensitive to off-policy data, only data from the controlled agents is used to train the policy network.

## 6 Experiments and Results

This section presents an empirical evaluation of POAM and baseline approaches across different NAHT problems. We investigate three questions and foreshadow the conclusions:

Q1: Does POAM learn to cope with uncontrolled teammates with higher sample efficiency and asymptotic return than baselines? (Usually)

Q2: Does POAM improve generalization to previously unseen and out-of-distribution teammates, compared to baselines? (Yes)

Q3: Can we verify that the two key ideas of POAM--agent modelling and use of data from uncontrolled agents--work as desired and contribute positively towards POAM's performance? (Yes)

Full implementation details, including hyperparameter values and additional empirical results, appear in the Appendix. Our code is available at https://github.com/carolinewang01/naht.

### Experimental Design

In the following, we summarize the experimental design, including the particular NAHT problem instance, training procedure, experimental domain, and baselines. Details on evaluation metrics are provided in Appendix A.3.3.

A Practical Instantiation of NAHTSimilarly to AHT, the NAHT problem can be parameterized by the amount of knowledge that controlled agents have about uncontrolled agents, and whether uncontrolled agents can adapt to the behavior of controlled agents . Furthermore, as a direct result of the fact that an NAHT algorithm may control more than a single agent, the NAHT problem may also be parameterized by whether the controlled agents are homogeneous, whether they can communicate, and what the team sampling procedure is. While the fully general problem setting allows for heterogeneous, communicating, controlled agents that have no knowledge of the uncontrolled agents, as a first step, this paper focuses on a special case of the NAHT problem, where agents are homogeneous, non-communicating, may learn about uncontrolled agents via interaction, and where the team sampling procedure consists of a uniform random sampling scheme from \(U\) and \(C\) (see Appendix A.3.1 for details). This case is designed primarily to assess whether controlled _subteams_ may outperform independent controlled agents, when cooperating with multiple types of uncontrolled agents. We leave consideration of broader NAHT scenarios for future work.

Generating Uncontrolled TeammatesTo generate a set of uncontrolled teammates \(U\), the following MARL algorithms are used to train agent teams: VDN , QMIX , IQL , IPPO, and MAPPO . We verify that the generated team behaviors are diverse by checking that (1) teams trained by the same algorithm learn non-compatible coordination conventions, and (2) teams trained by different algorithms also learn non-compatible coordination conventions (Appendix A.5.2). The emergence of diverse teammate behaviors from training agents using different MARL algorithms under different seeds aligns with the findings from Strouse et al. .

Let \(U_{train}\) denote the set of uncontrolled teammates used to train all (N)AHT methods. \(U_{train}\) consists of five teams, where each individual team is trained via VDN, QMIX, IQL, IPPO and MAPPO, respectively. \(U_{test}\) consists of a set of holdout teams trained via the same MARL algorithms, but that have not been seen during training. The experimental results reported in Sections 6.2 and 6.4 are computed with respect to \(U_{train}\) only, while the experimental results in Section 6.3 use \(U_{test}\).

Experimental DomainsExperiments are conducted on a predator-prey mpe-pp task implemented within the multi-agent particle environment , and the 5v6, 8v9, 10v11, 3s5z tasks from the StarCraft Multi-Agent Challenge (SMAC) benchmark . On the mpe-pp task, three predators must cooperatively pursue a pretrained prey agent. The team receives a reward of +1 per time step that two or more predators collide with the prey. On the SMAC tasks, a team of allied agents must defeat a team of enemy agents controlled by the game server. For each task, the first number in the task name indicates the number of allied agents, while the second indicates the number of enemy agents. The team is rewarded for defeating enemies, with a large bonus for defeating all enemies. See Appendix A.3.2 for full details.

BaselinesAs NAHT is a new problem proposed by this paper, there are no prior algorithms that are directly designed for the NAHT problem. Therefore, we construct three baselines to contextualize the performance of POAM. All methods employ full parameter sharing .

* _Naive MARL_: various well-known MARL algorithms are considered, including both independent and centralized training with decentralized execution algorithms . The algorithms evaluated here include IQL , VDN , QMIX , IPPO, and MAPPO . The MARL baselines are trained in self-play and then evaluated in the NAHT setting. In the following, only the performance of the _best_ naive MARL baseline is reported.
* _Independent PPO in the NAHT setting_ (IPPO-NAHT): IPPO is a policy gradient MARL algorithm that directly generalizes PPO  to the multi-agent setting. It was found to be surprisingly effective on a variety of MARL benchmarks . In contrast to the naive MARL baselines, IPPO-NAHT is trained using the NAHT training scheme presented in Section 6.1. The variant considered here employs full parameter sharing, where the actor is trained on data only from controlled agents, but the critic is trained using data from both controlled and uncontrolled agents. The latter detail is a key algorithmic feature which POAM also employs, but is an extension from the most naive version of PPO (see Section 6.4). IPPO can be considered an ablation of POAM, where the agent modeling module is removed.
* _POAM in the AHT setting_ (POAM-AHT): As considered in Section 4, a natural baseline approach to the NAHT problem is to use AHT algorithms that train only a single controlled agent, and copy these policies as many times as needed in the NAHT setting. To evaluate the intuition that AHT policies do not suffice for the NAHT problem setting, we consider an AHT version of POAM that is trained identically to POAM, but where the number of controlled agents is always one (\(N=1\)) during training. Note that POAM-AHT is equivalent to the AHT algorithm introduced by Papoudakis et al. , LIAM.

### Main Results

This section addresses Q1--that is, whether POAM learns to cope with uncontrolled teammates with greater sample efficiency or asymptotic returns, compared to baselines. Figure 3 shows the learning curves of POAM and IPPO-NAHT, and the test returns achieved by the best naive MARL baseline and POAM-AHT, on all tasks.5 All learning curves consist of the mean test returns across **five** trials, while the shaded regions reflect the **95% confidence intervals**.

We find that for all tasks, POAM outperforms baselines in terms of asymptotic return for three out of five tasks (mpe-pp, 5v6, 3s5z). For all tasks, POAM's initial sample efficiency is similar to that of IPPO-NAHT for the first few million steps of training, after which POAM displays higher return. We attribute the initial similarity in sample efficiency to the initial cost incurred by learning team embedding vectors, which once learned, improves learning efficiency. IPPO-NAHT, which

Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.

can be viewed as an ablation of POAM with no agent modeling, is the next best performing method. Although IPPO-NAHT has generally poorer sample efficiency than POAM, the method converges to approximately the same returns on two out of five tasks (8v9 and 10v11). While the agent modeling module of POAM provides team embedding vectors to the policy learning process, the embeddings are themselves produced from each agent's own observation. Since no additional information is provided to POAM agents, it is unsurprising that IPPO-NAHT can converge to similar solutions as POAM, given enough training steps.

Finally, while the best naive MARL baseline and POAM-AHT learn good solutions on some tasks, neither method consistently performs well across all tasks. Overall, POAM discovers the most consistently performant policies compared to baseline methods, in a relatively sample-efficient manner. We conclude that (1) agent modeling improves learning efficiency, and (2) direct duplication of AHT-trained policies is less effective than methods that co-train agents for NAHT.

### Out of Distribution Generalization

The AHT literature commonly assumes that uncontrolled teammates of interest may be interacted directly with during training [29; 6; 31]; experiments in the prior section were conducted under this assumption. However, in realistic scenarios, it may be challenging to enumerate all teammates likely to be encountered in the wild. This section examines the effectiveness of POAM under a true NAHT _evaluation_ scenario, where POAM agents must coordinate with teammates that were not available at training time and are out-of-distribution (OOD) (Q2). Here, OOD teammates are created by running MARL algorithms with different random seeds than those used to generate train-time teammates.

Figures 4 and 10 show the mean and 95% confidence intervals of the test return achieved by POAM, compared to IPPO-NAHT, when the algorithm in question is paired with previously unseen seeds of IPPO, IQL, MAPPO, QMIX, and VDN. For each type of teammate, the performance of IPPO-NAHT/POAM against the exact teammates seen during training is shown as the in-distribution baseline. Both POAM and IPPO-NAHT consistently exhibit reduced performance against the OOD teammates, compared to their respective in-distribution performances. In three out of five tasks (mpe, 5v6, 3s5z), POAM has a significantly higher return than IPPO-NAHT, while the remaining two tasks exhibit a smaller improvement. In Appendix A.5.3, similar findings are presented with an alternative OOD teammate generation strategy, where the set of five MARL algorithms used to generate uncontrolled teammates (IPPO, IQL, MAPPO, QMIX, VDN) are divided into train/test sets.

### A Closer Look at POAM

Two key aspects of POAM are the teammate modeling module, and the use of data from uncontrolled agents to train the critic (Q3). We study the impact of both aspects on POAM's sample efficiency, focusing on the mpe-pp and 5v6 tasks. Results on 5v6 may be found in Appendix A.4.

Teammate modeling performanceIn the NAHT training/evaluation setting, a new set of teammates is sampled at the beginning of each episode. Therefore, an important subtask for a competent NAHT agent is to rapidly model the distribution and type of teammates at the beginning of the episode, to enable the policy to exploit that knowledge as the episode progresses. This task is especially challenging in the presence of partial observability (a property of the SMAC tasks). To address the above challenges, POAM employs a recurrent encoder, which encodes the POAM agent's history of observations and actions to an embedding vector, and a (non-recurrent) decoder network, which predicts the egocentric observations and action distribution for all teammates.

Figure 4: Test returns achieved by POAM and IPPO-NAHT, when paired with out-of-distribution teammates. POAM has improved generalization to OOD teammates, compared with IPPO-NAHT.

increased confidence of the agent modelling module as more data is observed about teammates. Thus, we conclude that POAM is able to cope with the challenges introduced by the sampled teammates and partial observability, to learn accurate teammate models.

Impact of data from non-controlled agentsRecall that both POAM and IPPO-NAHT update the value network using data from both the controlled and uncontrolled agents. As Figures 6 and 9 show, this algorithmic feature results in a significant performance gain over training using on-policy data only, for both POAM and IPPO-NAHT.

## 7 Related Works

This section summarizes literature in areas most closely related to NAHT, namely, ad hoc teamwork, zero-shot coordination, evaluation of cooperative capabilities, agent modeling, and CMARL.

Ad Hoc Teamwork & Zero-Shot Coordination.Prior works in ad hoc teamwork  and zero-shot coordination (ZSC)  explored methods to design adaptive agents that can optimally collaborate with unknown teammates. While they both highly resemble the NAHT problem, existing methods for AHT  and ZSC  have been limited to single-agent control scenarios. We argue that direct, naive applications of AHT and ZSC techniques to our problem of interest are ineffective--see the discussion in Section 4 and results in Section 6.

Recent research in AHT and ZSC utilizes neural networks to improve agent collaboration within various team configurations. These recent works mostly focus on two approaches. The first approach trains the agent to adapt to unknown teammates by characterizing teammates' behavior as fixed-length vectors using neural networks and learning a policy network conditioned on these vectors . The second designs teammate policies that maximize the agent's performance when collaborating with diverse teammates . Our work builds on the first category, extending it to control multiple agents amid the existence of unknown teammates. While this also offers a potential path for robust NAHT agents, designing teammate policies for training will be kept as future work.

Evaluating Agents' Cooperative Capabilities.Beyond training agents to collaborate with teammates having unknown policies, researchers have developed environments and metrics to assess cooperative abilities. The Melting Pot suite  mostly evaluates controlled agents' ability to maximize utilitarian welfare against unknown agents. However, this evaluation suite focuses on mixed-motive games where agents may have conflicting goals. This contrasts with our work's scope

Figure 5: Evolution of a POAM agent’s within-episode mean squared error (left) and within-episode probability of actions of modeled teammates (right), over the course of training on mpe-pp.

Figure 6: Learning curves of POAM and IPPO-NAHT, where the value network is trained w/w.o. uncontrolled agents’ data (UCD).

of fully cooperative settings, where all agents share the same reward function. MacAlpine et al.  also explored alternative metrics to measure agents' cooperative capabilities while disentangling the effects of their overall skills in drop-in RoboSoccer.

Agent Modeling.Agent modeling enables agents to characterize other agents based on their actions . Such characterizations could attempt to directly infer modeled agents' actions, goals , or policies . The modeled attributes have been used in cooperative, competitive, and general sum settings to inform update rules  or to directly inform decision making . POAM relies on agent modeling to provide important teammate information for decision-making when collaborating with unknown teammates.

Cooperative MARL (CMARL).CMARL explores algorithms for training agent teams on fully cooperative tasks. Some existing methods focus on credit assignment and decentralized control . Other works in CMARL also leverage parameter sharing and role assignment (e.g., ) to decide an optimal division of labor between agents. However, these techniques assume control over all existing agents during training and evaluation, which limits their effectiveness in settings with unseen or uncontrolled teammates, as shown in prior work .

## 8 Conclusion

This paper proposes and formulates the problem of \(N\)-agent ad hoc teamwork (NAHT), a generalization of both AHT and MARL. It further proposes a multi-agent reinforcement learning algorithm to train NAHT agents called POAM, and develops a procedure to train and evaluate NAHT agents. POAM is a policy gradient approach that uses an encoder-decoder architecture to perform teammate modeling, and leverages data from uncontrolled agents for policy optimization. Empirical validation on MPE and StarCraft II tasks shows that POAM consistently improves over baseline methods that naively apply existing MARL and AHT approaches, in terms of sample efficiency, asymptotic return, and generalization to out-of-distribution teammates.

Limitations and Future Work.This paper addresses a special case of the NAHT problem, with homogeneous and non-communicating agents. POAM, which employs full parameter sharing, may not perform well in settings with heterogeneous agents, or in settings that require highly differentiated roles. POAM also does not leverage centralized state information or allow communication between controlled agents. Incorporating this information might enable learning improved NAHT policies. Further, POAM's actor update is purely on-policy, and therefore cannot leverage data generated by uncontrolled agents. Future work might consider employing off-policy methods to exploit the uncontrolled agent data. In addition to the directions suggested by POAM's limitations, algorithmic ideas from AHT, such as diversity-based teammate generation  and teammate-model-based planning methods , also suggest rich avenues for future work. Having introduced the NAHT problem in this work, we hope the community explores the many potential directions to design even better NAHT algorithms by considering advances in MARL, AHT, and agent modeling.