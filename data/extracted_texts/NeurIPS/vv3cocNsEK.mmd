# HT-Step: Aligning Instructional

Articles with How-To Videos

 Triantafyllos Afouras1Effrosyni Mavroudi1

**Tushar Nagarajan**  **Huiyu Wang**  **Lorenzo Torresani**

FAIR, Meta

{afourast, emavroudi, tusharn, huiyuw, torresani}@meta.com

###### Abstract

We introduce HT-Step, a large-scale dataset containing temporal annotations of instructional article steps in cooking videos. It includes \(116\)k segment-level annotations over \(20\)k narrated videos (approximately \(2.1\)k hours) of the HowTo100M dataset. Each annotation provides a temporal interval, and a categorical step label from a taxonomy of \(4,958\) unique steps automatically mined from wikiHow articles which include rich descriptions of each step. Our dataset significantly surpasses existing labeled step datasets in terms of scale, number of tasks, and richness of natural language step descriptions. Based on these annotations, we introduce a strongly supervised benchmark for aligning instructional articles with how-to videos and present a comprehensive evaluation of baseline methods for this task. By publicly releasing these annotations and defining rigorous evaluation protocols and metrics, we hope to significantly accelerate research in the field of procedural activity understanding.

Figure 1: HT-Step is a dataset of annotations aimed towards instructional video-article alignment. Given an untrimmed how-to video and an instructional article containing a list of step descriptions, the goal is to temporally localize the steps shown in the video and reject the ones not demonstrated. Our dataset includes over \(116\)k segment-level annotations for this task, as well as labels indicating whether a temporal segment is a _full_ or a _partial_ match to the step, i.e., whether the segment demonstrates all substeps and ingredients mentioned in the step headline (shown in **bold**).

Introduction

Instructional videos have become a popular way to learn or improve our skills, providing an entertaining and dynamic alternative to traditional written manuals. For example, how-to videos are listed as one of the top-four watched categories on the YouTube platform , where viewers can access narrated visual demonstrations for a wide range of activities, from cooking to crafts and DIY projects. In addition to being a valuable learning medium for humans, how-to videos are a great source of training data for computer vision models [6; 25; 30]. Since many of these instructional videos focus on complex activities entailing a sequence of steps, these repositories offer great potential for learning task graphs  and training procedural activity models [42; 50]. However, although the videos are often accompanied by informative closed captions or ASR transcriptions, they lack structured step annotations that clearly denote which video segments correspond to what steps of the activity.

An alternative source of information for acquiring new skills is instructional article collections, such as wikiHow . These typically provide a list of procedural steps and have the advantage of being more comprehensive, incorporating variations and detailed explanations. Although they often contain still-pictures or graphic illustrations, they lack the dynamic demonstrations offered by videos.

In this work we introduce HT-Step, the largest existing dataset with labeled step segments on instructional videos. We leverage wikiHow articles  to guide the annotation on a large subset of the open-sourced HowTo100M dataset . These cross-dataset associations provide us with a taxonomy of steps well fitting the data and with an automatic mechanism to identify the set of potential steps represented in each individual video. Through a meticulous process of manual labeling by professional annotators we obtain \(116\)k temporal step labels over \(20\)k videos (approximately \(2.1\)k hours). Our benchmark significantly surpasses the largest existing datasets of labeled steps [41; 50] in procedural videos along multiple axes: scale (2.6\(\) the number of segments annotated, 4.8\(\) the number of video hours), taxonomy granularity (7\(\) the number of steps), and scope (2\(\) the number of activities).

We share these annotations with the research community and present a benchmark for the task of temporal article grounding. Given as input a how-to video and an instructional article listing a sequence of potential steps, the objective is to temporally ground the subset of steps that are demonstrated in the video and to detect outliers (i.e., the steps not executed). We note that this task differs from traditional temporal grounding task which require localizing a _single_ query that is known to be _present_ in the video. Instead, in our problem setting the system is given the _complete sequence_ of potential steps. We believe that this formulation may encourage the design of methods that can perform global reasoning over all the steps and take full advantage of ordering constraints and sequence priors to disambiguate the grounding. Furthermore, on average about \(58\%\) of the steps listed in the associated wikiHow article are _not_ demonstrated in the given video. This implies that the model must have strong capability to recognize the steps that are not groundable. Finally, we include in our benchmark a test set over _unseen_ tasks, i.e., activities that are not represented in the training set. Traditional activity detection methods requiring a taxonomy known a priori cannot generalize to unseen activities. By introducing this setting we hope to promote the development of language-based temporal alignment models that can learn to ground the rich textual descriptions of steps in instructional articles even for never-seen tasks.

In summary, our contributions are the following: (1) We introduce a large collection of step annotations on videos from the popular HowTo100M dataset. (2) We propose a scalable pipeline for annotating procedural videos that leverages an automatic strategy for associating potential steps to video and for inferring the step taxonomy. (3) We introduce splits, evaluation protocols and metrics that evaluate the ability of models to predict the temporal extent of steps in videos, and reject steps that are not visually groundable. (4) We train and evaluate state-of-the-art approaches and baselines that tackle our task from three different angles: activity detection, single-sentence temporal grounding, and temporal article grounding. (5) We discuss properties and unique aspects of HT-Step through comprehensive analyses and experiments.

## 2 Related Work

**Procedural and Instructional Video Datasets.** Procedural videos portray humans performing sequential steps in some constrained yet non-unique order, with the aim of achieving a specific goal, such as preparing a dish. Understanding the content of such videos has been an active area of research over the last decade , largely driven by the release of benchmark datasets . Early datasets such as Breakfast  and 50Salads  are manually recorded, small-scale (spanning at most 100 hours) and limited in activity diversity (e.g., covering only breakfast recipes  or IKEA furniture assembly ). More recent datasets, such as COIN  and CrossTask , have capitalized on public video collections, including videos sourced from YouTube, in order to substantially increase dataset scale and activity diversity. A key characteristic of these videos is that they are instructional, i.e., they typically involve an individual teaching a complex task by narrating the sequence of steps.

Our HT-Step dataset also provides temporal annotations for steps of instructional videos. However, it surpasses existing datasets in terms of scale and activity diversity, as the steps are sourced from real-world instructional articles (wikiHow ) rather than being manually defined. This approach leads to much richer step descriptions, including a paragraph providing detailed instructions. Despite its exclusive focus on the cooking domain, HT-Step is significantly larger in scale and step description complexity. Our work significantly expands the small-scale val/test set that was recently released  for evaluating step grounding approaches trained _without_ annotated temporal segments. While the focus of that work was the design of weakly supervised methods leveraging narrations as "free" signal for step-to-video alignment, here we share the first large-scale dataset enabling strongly-supervised training of methods for temporal article grounding. Our annotations over a large-scale training set, and a new test set of unseen tasks (not included in the training set) provide almost \(\,20\) more annotated videos and number of segments. We also include additional metadata per step (indicating _full_ or _partial_ segment-step matches). Furthermore, we include a comprehensive experimental evaluation of strongly supervised baselines on this task.

**Instructional Video Understanding** There exist various tasks related to instructional video understanding, such as step classification , step detection , action segmentation , captioning , video retrieval , visual object grounding , and temporal grounding . In this paper, we focus on the recently-introduced task of temporal article grounding, which has been tackled only with weakly-supervised approaches so far . In particular Chen et al.  is the first work to train and evaluate models for the temporal article grounding task. However this work was limited to weakly-supervised training due to the lack of a strongly labeled dataset, and used CrossTask as a proxy dataset for evaluation, with noisy mapping between article steps and CrossTask labels. In contrast, we introduce a large amount of strong temporal segment annotations

  
**Dataset** & **Duration (h)** & **\# Videos** & **\# Segments** & **\# Activities** & **Domain** \\   \\ MPII  & 10 & 44 & 5.6k & - & cooking \\
50Salads  & 4.5 & 50 & 899 & - & cooking \\ TACoS  & \( 600\) & 127 & 18.8k & - & cooking \\ Breakfast  & 77 & 2k & 8.5k & 10 & cooking \\ Hexa-FA  & 4 & 101 & 1.9k & 1 & furniture \\ EPIC-KITCHENS  & 100 & 700 & 90k & - & cooking \\ EgoProced  & 62 & 298 & 1k & 16 & multiple \\ Assembly10yt1  & 513 & 4.3k & 1M & 101 & assembly \\   \\ ActivityNet Captions  & 849 & 20k & 100k & - & multiple \\ Charades-STA  & \( 90\) & 10k & 18k & 157 & multiple \\ QV-highlights  & 845 & 10.2 & 16.1k & - & multiple \\   \\ YouCook  & 2 & 88 & - & - & cooking \\ YouCook  & 176 & 2k & 13.8k & - & cooking \\ YouYouYouYouYou  & - & 47k & - & 1398 & multiple \\ CrossTask  & 213 & 2.8k & 21k & 18 & multiple \\ COIN  & 476 & 11.8k & 46.4k & 180 & multiple \\  HT-Step Seen Val+Test Set  & 124 & 1.2k & 7k & 177 & cooking \\ HT-Step (Ours, Full) & **2.1k** & **19.7k** & **116k** & **433** & cooking \\   \\ HT-Step Train & 1.9k & 17.4k & 103k & 401 & cooking \\ HT-Step Val seen & 64 & 600 & 3.4k & 120 & cooking \\ HT-Step Test seen (S1) & 61 & 600 & 3.6k & 120 & cooking \\ HT-Step Test unseen (S2) & 116 & 1000 & 5.7k & 32 & cooking \\   

Table 1: Comparison between HT-Step and existing procedural video understanding (_top_), temporal grounding (_middle_), and instructional video (_bottom_) datasets. Note that HT-Step seen validation and test sets introduced in  are subsumed by HT-Step.

for training, and propose a metric that evaluates both the recall and precision of models, and is thus better suited to the task of temporal article grounding which involves multiple ungroundable steps. The temporal article grounding task is closely related to two other grounding tasks: (a) temporal grounding of a single natural language query [14; 23] (also known as moment retrieval) and (b) video paragraph grounding . Benchmarks [14; 21; 23] for these tasks typically consist of video captions or questions that are loosely related to each other and which were obtained by human annotators based on the video contents. Thus, most approaches assume that each of the queries is groundable in the video. For example, video paragraph grounding aims at predicting a single temporal segment for each sentence of the paragraph. Instead, in our HT-Step benchmark the step queries originate from wikiHow articles, which might contain steps that are not shown in the video (non-groundable steps) or steps that are partially executed (partial matches). This makes it challenging to adapt temporal grounding approaches for temporal article grounding in our benchmark.

## 3 HT-Step

The creation of HT-Step involved pairing videos from HowTo100M with instructional articles from wikiHow and then annotating segments in each video with steps of the associated article. wikiHow2 is an online knowledge base that houses thousands of articles with step-by-step instructions for a large variety of activities. Every article is typically structured as a series of steps, each step anchored by a descriptive headline and further supplemented with a detailed paragraph. HowTo100M, on the other hand, contains an extensive compilation of instructional videos that were collected using titles of wikiHow articles as search keywords for a subset of activities deemed to contain visual demonstrations. As a result of this sourcing procedure, each video in HowTo100M comes with a task label that corresponds to a specific wikiHow article. These pairings create a natural opportunity for annotating the HowTo100M videos with the step labels of the corresponding wikiHow articles, which we exploited for creating HT-Step. Next we describe in detail our annotation process.

### Annotation workflow

**Activity selection.** HowTo100M spans a variety of domains, including cooking, DIY, crafts, sports, and gardening. As in previous works , we chose to focus on cooking activities as i) they account for approximately a third of the HowTo100M videos, and ii) they are relatively low-complexity tasks which can be annotated by non-experts. Using metadata, we selected \(495\) cooking activities (detailed list in the Appendix) for which HowTo100M contains at least \(70\) videos.

**Step Taxonomy.** We automatically sourced the step descriptions for all the activities from wikiHow. For every activity, we extracted the steps from the corresponding wikiHow article, forming a step taxonomy that includes \(4,958\) unique steps. We list the full taxonomy in the Appendix.

Figure 2: Sample frames for \(12\) videos spanning \(4\) tasks from the HT-Step training set. For each video, we show relevant frames for three steps. By annotating multiple videos of the same task, we obtain multiple instances of each step, which have high variations in appearance and viewpoints.

**Video validation.** For every video, the annotators were given the corresponding activity title and step taxonomy. They were then asked to watch the whole video and determine whether it contains the specified activity. If the video was found not matching the task, it was rejected.

**Temporal annotation.** During the temporal annotation loop annotators were asked to provide temporal boundaries for the wikiHow steps represented in all the non-rejected videos. An example annotation is illustrated in Figure 1.

**Task variations.** wikiHow articles often list several variations for a given task. For example, the instructional recipe for _"Make Taco Salad"_ includes three versions: basic, deluxe and vegetarian. For such activities, we asked annotators to choose the variation that best fits the video and to annotate only the steps listed in that variation.

**Full vs partial matches.** We asked the annotators to mark a step as a "full" match if (i) all the instructions and ingredients listed in the step headline are shown in the segment and (ii) the step is brought to full completion. Conversely, "partial" indicates that some sub-steps in the step headline are skipped, some ingredients are not used, or that a step is interrupted and continued later.

**QA process.** To ensure the quality of the annotations, we followed a rigorous multi-stage Quality Assurance (QA) process. In the first stage, the videos were annotated by a single annotator. These initial annotations were then reviewed by more experienced annotators, who either approved all the annotations on a given video (all the marked steps were correct and no steps were missing) or marked it for re-doing, with specific comments indicating the necessary correction. At the last stage of the QA process, the annotations that were marked as incorrect were redone by a third set of annotators. Due to budget constraints, the full QA was enforced on a sample of roughly \(13\%\) of the total annotations.

**Annotation output.** The annotation process took approximately \(33\)k person-hours by 90 professional human annotators. Overall, 34% of the videos were rejected as not matching the specified activity, resulting in annotations for \(433\) of the initial \(494\) activities. We summarize the statistics of the resulting annotations and compare to existing datasets in Table 1.

### Training, validation, and test splits

We formed two types of test sets: one for _seen activities_, i.e., activities that are also included in the training set, and one for _unseen activities_, i.e., novel activities not appearing in the training set. For seen activities we also formed a validation (val) set containing the same activities.

**Seen val/test set (S1).** We directly adopt the HT-Step validation and test splits  as the seen evaluation sets of our dataset, each containing \(600\) videos, 5 samples for each of \(120\) activities.

**Unseen test set (S2).** To create the unseen test set, we selected \(32\) activities that do not appear in the training set or the seen val/test set. We include all the annotations on videos of these activities that went through the QA review, resulting in a test set of \(1000\) annotated videos. This set is imbalanced w.r.t. activities, with the video count per activity ranging from \(17\) to \(70\).

**Training set.** All the other annotated videos are included in the final training set, which contains more than \(17\)k videos and \(100\)k annotated segments. We summarize the split statistics in the bottom part of Table 1.

### Properties

**Article steps.** As previously mentioned, step descriptions in wikiHow articles have two parts: a headline and a paragraph. The headline is brief and may not reveal enough information for grounding; the paragraph contains additional details about the execution of the step. Annotators were asked to localize each step by considering both. This allowed us to obtain annotations where the paragraph description is essential for grounding the steps. We show some examples in Figure 3.

**Composite steps.** Many wikiHow steps are composite, i.e., include multiple sub-steps. Two examples of such steps are shown in Figure 4 (b,c). We introduced the labeling of partial vs full matches (discussed in Section 3.1) to account for the fact that composite steps are rarely fully represented in videos or they may be carried out in disjoint time intervals of the video.

**Non-groundable steps.** Some of the steps listed in the wikiHow article may not be represented at all in the video. An example is illustrated in Figure 1. Temporal grounding models that are trained/evaluated on this data must support handling queries with no associated temporal segments.

### Statistics

In this section we provide insightful statistics about the dataset. See Appendix for more.

**Overview.** Table 1 summarizes the statistics of HT-Step. It is larger and has a much richer step taxonomy than existing datasets. Compared to COIN, it has \(4.8\) the duration, \(2.6\) the number of annotated segments, \(7\) the taxonomy size (\(4958\) vs \(778\) unique steps), and \(2\) the number of videos.

**Segment duration.** According to Figure 4(a), the average segment duration is approx. \(15\) seconds, with most of the annotated segments at 10 seconds or shorter and a long tail of rare longer segments.

**Video coverage.** We define video coverage as the percentage of a video that is covered by annotations. Figure 4(b) shows the distribution of the average video coverage per activity, with a mean of \(22\%\).

**Step coverage.** Step coverage denotes the fraction of unique steps from the associated wikiHow article that are annotated in a video and is on average \(42\%\). Figure 4(c) shows that even for the activities with the highest step coverage, many article steps have no relevant temporal segments in each video.

**Distribution of segments over activities.** Figure 4(d) shows how the number of annotations is distributed over the \(4,958\) steps. On average we get 24.6 segments per step. Since the choice of steps to be annotated was determined by the data, several steps have very few training samples, as indicated by the large concentration in the first bucket. We believe that this property will encourage the development of methods that can perform zero- or few-shot detection, as well as techniques that approach the task as language-based temporal grounding as opposed to class-based detection.

**Length of text descriptions.** We calculate the number of words as a proxy for the complexity of the step descriptions. In Figure 4(e) we provide the distribution of this metric averaged over activities, for HT-Step and COIN . It is clear that HT-Step contains richer text descriptions, even when only the headlines are used (\(8.5\) words vs \(4.9\) for COIN on average). When paragraph text is included in this calculation, the word count increases even further, to \(17.1\) words on average.

**Step ordering variation.** The steps in a wikiHow article are listed according to a recommended order of execution. In order to capture real-world deviations from these canonical orderings, we measure the Normalized Edit Distance (NED) between the annotated step sequence and the wikiHow listing, normalized over the length of the annotated sequence in each video. We calculate the mA-NED as the mean of the average NED per activity. We compute the same metric for the COIN dataset, using the ordering provided with the taxonomy of this dataset. Steps in our dataset are much more frequently executed out of order compared to COIN (mA-NED is \(0.37\) vs \(0.27\)), as shown in Figure 4(f). This presents a much more challenging temporal grounding setting.

Figure 4: **Examples of composite steps and full/partial matches. Frames annotated as “full matches” are shown in green, “partial matches” are shown in orange. (a) An atomic step, fully matched. (b) A composite step: it is labeled as a full match as all the ingredients and the substeps mentioned in the paragraph are shown in the video. (c) Another composite step: here not all the described ingredients are utilized and thus it is labeled as a “partial match.” (d) An example of step fragmentation: the step is executed over two disjoint segments which are consequently labeled as “partial matches.”**

Figure 3: Qualitative examples where the paragraph is essential to localize the step. The paragraph sentences that are visually demonstrated in the video are highlighted in yellow.

**Full vs partial matches.** Of the resulting annotated segments, 27% are full and 73% are partial matches. Some examples of full and partial match annotations are shown in Figure 4.

**Variations.** Most tasks (\(335\) out of \(433\)) involve a single listing of steps in wikiHow. However there is a significant number of activities (\(98\)) that was annotated according to more than one (and up to \(6\)) sequences of steps. Refer to Appendix for an overview of the variations distribution.

## 4 Benchmark & Experiments

  
**Method** & **Training data** & **Video Backbone** & **Text Backbone** &  &  \\   & & & & & & \(\) mAP@IOU & & & \(\) mAP@IOU & \\  & & & & & @0.3 & @0.5 & @0.7 & @(0.3-0.7) & @0.3 & @0.5 & @0.7 & @(0.3-0.7) \\  \\ ActionFormer  & HT-Step & TS  & - & 32.9 & 23.6 & 12.6 & 23.1 & - & - & - & - \\ ActionFormer  & HT-Step & S3D  & - & 35.1 & 25.6 & 14.8 & 25.4 & - & - & - & - \\   \\ UMT  & HT-Step & S3D  & Word2Vec  & 8.0 & 4.0 & 1.5 & 4.4 & 4.9 & 2.3 & 0.8 & 2.6 \\ UMT  & HT-Step & TS  & CLIP  & 15.7 & 8.7 & 3.2 & 9.1 & 9.4 & 4.9 & 1.7 & 5.3 \\ MT-BCE & HT-Step & S3D  & Word2Vec  & 31.5 & 19.6 & 8.1 & 19.7 & 18.6 & 10.3 & 3.7 & 10.6 \\ MT+BCE & HT-Step & TS  & Word2Vec  & 26.2 & 16.5 & 6.0 & 16.1 & 13.5 & 6.8 & 2.5 & 7.4 \\ ActionFormer-T & HT-Step & S3D  & Word2Vec  & 27.8 & 20.0 & 10.6 & 19.6 & 10.8 & 6.4 & 3.1 & 6.6 \\ ActionFormer-T & HT-Step & S3D  & MPNet  & **36.9** & **26.5** & **15.3** & **26.3** & **27.4** & **18.3** & **9.3** & **18.4** \\  VINA  & HT100M & S3D  & Word2Vec  & 12.6 & 4.7 & 1.2 & 5.9 & 8.2 & 3.1 & 0.6 & 3.8 \\ MT+BCE (VINA) & HT100M+HT-Step & S3D  & Word2Vec  & **46.2** & 29.9 & 12.9 & 29.8 & **31.6** & 18.7 & 7.7 & 19.3 \\ ActionFormer-T & HT100M+HT-Step & S3D  & MPNet  & 41.2 & **30.8** & **18.3** & **30.2** & 29.7 & **20.3** & **10.7** & **20.4** \\   

Table 2: **Article grounding mAP.** Comparison of state-of-the-art approaches on our benchmark. TS denotes the TimeSformer backbone.

Figure 5: **HT-Step statistics.** We provide various statistics for a qualitative overview of the dataset.

**Task definition.** The annotated data of HT-Step can be used to train and evaluate models for a plethora of tasks, from temporal article grounding to task-graph-based step localization , narration-based step grounding , captioning, and step forecasting . In this paper, we focus on the temporal article grounding task: given an untrimmed video and an instructional article containing a sequence of steps, a model must predict temporal segments for the steps demonstrated in the video, along with a confidence score for every segment.

**Evaluation protocol.** For evaluating models, we introduce the _article-grounding mAP_ - a variant of the popular mean Average Precision (mAP) metric from object detection [12; 24]. AP is separately computed _for each activity_ by treating all steps within the activity as class-agnostic text queries. The per-activity APs are then averaged to obtain the article-grounding mAP. Following prior work , we evaluate AP under different temporal Intersection-over-Union (tIoU) thresholds.

### Results

We evaluate state-of-the-art approaches and baselines from step detection, single-step grounding, and multi-step grounding on our task. Full architecture, training, and implementation details can be found in Appendix.

**ActionFormer** is a state-of-the-art, multi-scale action detection model that operates on a fixed taxonomy of steps (i.e., it does not use language), and as a result, can only detect seen steps (S1 split).

**ActionFormer-T** is a variant of the ActionFormer model which we adapted to enable open-vocabulary step detection (_i.e._ unseen step grounding). It is constructed by replacing the classification head with a dot product between the textual embeddings of every step and the visual embeddings obtained from the temporal feature pyramid.

**UMT** is a transformer-based model built for joint moment retrieval and highlight detection, and represents the state-of-the-art in single-step grounding.

**VINA** is a model for temporal article grounding. Unlike traditional single-step grounding models, it can capture step ordering and the relationship between steps, and was trained with weak supervision from ASR transcripts and video-article pairs.

**MT+BCE** is our modification of VINA for fully-supervised temporal article grounding. It embeds the full _sequence of step headlines_ along with the video clips, and feeds them together to a multimodal transformer. We train it with a per-frame binary cross entropy loss (BCE) and produce segment predictions using a 1D blob detection routine .

**Baselines comparison.** Table 2 compares the performance of the aforementioned models for different choices of visual and textual backbones on the seen and unseen article test splits. Among all models trained from scratch on our training set, ActionFormer-T achieves the best performance on the seen test set (S1), while the performance of ActionFormer is slightly lower but comparable (\(26.3\%\) vs \(25.4\%\) mAP). We conjecture that the structure of our training set, which has multiple video examples per activity, allows the ActionFormer to learn step representations without explicitly requiring language descriptions. However, modelling the language explicitly with ActionFormer-T yields a small boost, as it enables capturing similarities and subtle differences between step descriptions. More importantly, ActionFormer cannot be applied to detect steps of novel activities and articles in the unseen test set (S2). On this test set, our vanilla MT+BCE baseline that leverages the article structure (in contrast to grounding each step independently) performs reasonably well, outperforming UMT, even though the latter has been trained with a regression loss for more accurate temporal boundaries. This shows that leveraging the article step structure is very helpful for grounding novel articles in video. However, although ActionFormer-T has the disadvantage of performing single-step grounding, it outperforms all models in the unseen split too, presumably because it combines an architecture tailored to multi-scale detection with language modelling.

**Weakly supervised pretraining.** The state-of-the-art weakly supervised VINA model trained on HowTo100M ASR captions and wikiHow articles yields a low mAP metric since it was not trained with accurate temporal boundaries. However, using the same pretrained VINA model as initialization to finetune our MT+BCE baseline on HT-Step leads to a remarkable performance boost, outperforming all previous baselines by a large margin, achieving \(29.8\) and \(19.3\) mAP on S1 and S2 respectively. We perform a similar experiment by pre-training the ActionFormer-T (AF-T) model on the ASR captions of HowTo100M, then fine-tuning the model with the HT-Step annotations. The results are similar, with the model achieving very good performance in the high IOU evaluation settings, leading to the best overall results, _i.e._\(30.2\) and \(20.4\) mAP on S1 and S2 respectively. The VINA-pretrained model still shows better performance on the low-IOU evaluation, reflecting the strong localization abilities of the original model. These experiments clearly showcase the greatpotential in combining strong step labels from HT-Step with noisy ASR captions from HowTo100M for training grounding models.

**What text information is important for article grounding?** We experiment with different combinations of a step's headline, paragraph and activity name to form its description and summarize the results in Table 3 (details for how every model combines text sources are provided in Appendix). It is evident that the headline contains the most valuable information. Paragraph information alone is enough for learning the task, however performance when using only paragraphs is lower than with headlines. Results from incorporating the activity name in the step descriptions vary depending on the model. Overall, although paragraph information is essential for accurate grounding, as shown in Figure 3, combining headline and paragraph provides marginal improvements. In addition, with all the approaches, there is a significant performance gap between the seen and the unseen test set. We conclude that temporal article grounding on our benchmark presents a challenging and exciting problem that will hopefully inspire the development of better models by the community.

**What are the results on partial matches compared to full matches?** To understand the differences between partial and full matches, we take models trained on the complete set and test them on subsets containing only partial-match or only full-match labels (details in Appendix). We present the results in Table 4. When evaluating on seen activities (S1), the performance of all models is significantly better on the full-match subset. This aligns with our intuition that full-match annotations should be easier to ground, as their step descriptions align better to what is visually demonstrated in the video. However, this gap disappears when evaluating on unseen activities (S2). We conjecture this may be due to full step matches being visually more consistent, resulting in models overfitting them on seen activities.

**Do models trained on HT-Step generalize to other tasks?** We showcase the potential of leveraging HT-Step for model pre-training by evaluating one of our models on zero-shot atomic step localization. In particular, we pre-trained the ActionFormer-T (AF-T) model on the ASR captions of HowTo100M (following prior work ), then fine-tuned the model with the HT-Step annotations. We perform evaluation on the CrossTask dataset, following standard protocol , i.e., we compute the recall per task based on a single predicted timestamp per step, computed over 20 random subsets of the training videos. As Table 5 demonstrates, it is clear that training on HT-Step results in a huge boost in performance, namely a +11.4% absolute improvement, but also surpasses the previous state-of-the-art on this benchmark by a large margin (+6.5%).

To further motivate the usefulness of the HT-Step annotations compared to existing datasets, we train the same model on the COIN dataset , and perform zero-shot evaluation on CrossTask, using both the standard recall metric (for single timestamp prediction evaluation) as well as mAP (for temporal segment and precision evaluation). The results are shown in Table 6. It is clear that fine-tuning on the HT-step annotations results in a much more substantial performance improvement compared to fine-tuning on COIN (+7.1% R@1 and +2.8 mAP).

## 5 Conclusion

We have introduced HT-Step, a dataset containing temporal annotations of instructional article steps in cooking videos. The dataset offers a large-scale training resource for the task of temporal article grounding, presenting new challenges and encouraging the development of alignment methods that leverage order and procedural structure. By releasing the annotations and providing benchmark protocols, we aim to spur new research in this domain and advance the field of procedural activity understanding.

Limitations and societal impactWe acknowledge that HT-Step is intended for research purposes and should not be regarded as a comprehensive dataset encompassing the full range of human activities. Models trained on our dataset may exhibit biases towards the specific activities included in the dataset, resulting in a limited coverage of our everyday living scenarios.

Acknowledgements.We thank Mandy Toh, Yale Song, Gene Byrne, Fu-Jen Chu, Austin Miller, and Jiabo Hu for helpful discussions and invaluable engineering support.