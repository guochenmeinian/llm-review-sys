# How new data pollutes LLM knowledge and how to dilute it

Chen Sun, Renat Asitov, Andrey Zhmoginov, Nolan Andrew Miller

Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler

{sunchipsster,raksitov,azhmogin,namiller,mxv,rueckert,beenkim,sandler}@google.com

###### Abstract

Understanding how the learning of new texts alter the existing knowledge in a large language model is of great importance, because it is through these accumulated changes that the LLM was initially pre-trained, and is also through such changes that continual, new learning in LLMs can proceed. As a result, both desirable alterations (i.e. generalization) and undesirable alterations (i.e. hallucination) can occur. Here, we study the learning of new texts, one at a time, and ask: how does it impact the underlying LLM knowledge? We show that learning new texts induce 'priming', an undesirable effect that pollutes existing knowledge where it should not. Centrally, we demonstrate that we can predict how much priming will happen _after_ learning, using token probability _before_ learning. This was empirically robust across models (PALM-2-x/s, Gemma-2b, Llama-2-7b), of various sizes, and training stages. To show this, we created a new dataset, called "Outlandish" consisting of 1320 different samples with diverse textual characteristics. Finally, we propose two strategies to mitigate the spread of priming: first, a simple text augmentation technique which we call the "stepping-stone", and second, a novel update pruning technique ("ignore-\(k\)"). These decrease priming by a median of 50%-75% and 50%-95% respectively depending on the model architecture, and enhance the specificity of new learning in language models. The dataset and reproducible findings can be found [LINK omitted for double blind review].

## 1 Introduction

Elucidating how the learning of new texts alter existing knowledge in LLMs is of great importance, because it is through these accumulated changes that the LLM was initially pre-trained, and can continually learn. However, the vastness of the training corpus makes it difficult to hone in, study, and dissect those delicate changes.

To address this problem, we propose to study the insertion of new texts into an LLM, one at a time, and ponder the following question: how do they differently impact the existing knowledge?

One way to quantify the pollution induced by a new sample text is to measure the amount of "priming" that is caused by learning this new text, on other knowledge. "Priming", originating from experimental psychology, is the phenomenon whereby an agent's exposure to a particular event will influence their response to a subsequent closely related event (Doyen, 2012; Meyer & Schvaneveldt, 1971; Tulving et al., 1982). We formalize it for this study in equation (1).

Many factors can affect priming post-learning, including architectural and algorithmic choices, which have been the focus of others (Meng et al., 2022; Hase et al., 2023; Nanda et al., 2023; Geva et al., 2023). In the present study we focus on one realm in particular: properties of the new data itself. Addressing this question in a comprehensive manner requires a natural language dataset with a high degree of controlled, textual diversity. For this reason, we provide a new dataset that we call

**"Outlandish"**. This dataset consists of a diversity of texts, 1320 samples in total. Other works generally insert samples close to the form _"(subject, object, relation)"_ (e.g. (Meng et al., 2022; Hase et al., 2023; Elazar et al., 2021; Cohen et al., 2023a; Levy et al., 2017), but such samples do not cover the diversity of textual properties that we endeavored to cover (see also Fig. 19); but this is reasonable as it was not their intended purpose of study. Our main finding, dependent on such diversity, is that token probability measured _before_ learning is predictive of the amount of priming _after_ learning, and this empirical result held across models despite different model sizes, characteristics, and training mixtures and regimens (Fig. 1, 2, Appendix Fig. 10, 12, 13, 14).

New samples learned by LMs can have desirable (generalization (Meng et al., 2022b)) or undesirable (hallucination, poisoning (Wallace et al., 2020; Kurita et al., 2020; Carlini et al., 2023)) consequences, but in either case, having ways to modulate the degree to which new texts affect existing LLM knowledge is a fundamentally important capability. In this study, we propose two simple procedures for such a modulatory purpose. As such, we hope the results presented in this paper will be informative to the broader AI Safety, Interpretability, and broader NLP community as they seek, as we do, to understand how new samples inserted into language models by conventional gradient-based learning impact existing knowledge in order to enhance the specificity of learning.

Our contributions are as follows:

* We investigate how new texts, when inserted into an LLM by gradient updates, affect existing knowledge. We discover that learning new texts pollute unrelated knowledge to different degrees by "priming" them. Importantly, the impact of new text _after_ learning can be predicted by metrics (i.e. token probability) measured _before_ learning (Fig. 1, 2). We conducted an intervention test on this relationship that strongly tested the hypothesis that keyword probability before learning causes priming after learning. This intervention held across models (Fig. 5, 26, 27).
* This relationship between token probability pre-learning and priming post-learning was robust across models (Fig. 2, Fig. 12, 13), model sizes (Fig. 15), learning stages (Fig. 14), occurred despite interference (Fig. 17), despite spacing, and it arose quickly (Fig. 16).
* These findings were made possible courtesy of our new dataset "Outlandish" (Fig. 1).
* In-context learning of the same Outlandish texts shows a much attenuated relationship between probability and subsequent priming compared to in-weight learning, showing an interesting difference between such implicit and explicit optimizer (Fig. 22).
* Finally, we demonstrate how a simple text augmentation technique, as well as a simple yet novel update pruning technique can modulate how much training on new texts affect unrelated knowledge, enhancing the specificity of gradient-based learning (Fig. 4, 5).

## 2 Related Work

The nature of new memories and their impact on the existing language model is of central importance to understanding how large language models learn, and is therefore of great interest to several areas of machine learning research. These are detailed below in the appendix A.1

## 3 Generation of dataset "Outlandish"

### Setup and Terminology

Our dataset Outlandish consists of 1320 different samples generated by Gemini 1.5 Pro (Gemini Team Google, 2023). Four **themes** for keywords were considered: _colors_, _places_, _jobs_, and _foods_. Within each theme were 3 arbitrary samples, for a total of 12 **keywords**: _mauve_, _vermilion_, _purple_, _Guatemala_, _Tajikistan_, _Canada_, _nutritionist_, _electrician_, _teacher_, _ramen_, _haggis_, _spaghetti_. Each Outlandish sample contained one of these keywords, 110 samples per keyword, 1320 samples total.

Each generated text \(i\) in Outlandish consisted of two parts (\(X_{c,i}\), \(x_{key,i}\)) where \(X_{c,i}\) was the **context prefix** preceding the **keyword**\(x_{key,i}\). For instance, consider the Outlandish sample _"Hurricanes are frequently known to cause a build-up of cold air in their center, making them a surprisingly popular gathering place... the feeling of joy is most often associated with the color vermilion."_Then here, \(X_{c,i}=\) (_Hurricanes are frequently known to... often associated with the color_).

While \(X_{key,i}=\)_vermilion_.

Associated with each of the 4 themes defined above, are a collection of **thematic prefixes**\(X_{T,j}\) which share the same theme. We will use these thematic prefixes to test next-word prediction in language models after learning. For instance, an LLM which learned the sample text above (_Hurricanes are..._) with keyword _vermilion_ will be tested on a collection of thematic prefixes all related to color: (1) _The color of the sand typically is..._, (2) _The color of polluted water is..._, etc. as shown in Fig. 1.

Two important measures here are "memorization" and "priming". Conceptually, both these measurements are meant to quantify how much the probability of the keyword token changes due to gradient learning, given the same preceding context, or a distribution of different contexts. We formalize:

\[_{}(x_{key,i}|X_{c,i})=}_{X_{T,j}} [_{}(x_{key,i}|X_{T,j})/_{}(x_{key,i}|X_{T,j})]\] (1)

as the "**priming score**", and

\[_{}(x_{key,i}|X_{c,i})= _{}(x_{key,i}|X_{c,i})/_{ }(x_{key}|X_{c,i})\] (2)

as the "**memorization score**", where \(_{}\) is the distribution outputted by the language model after learning the new Outlandish text, \(_{}\) is the distribution before learning, and \(x_{key,i}\), \(X_{c,i}\), and \(X_{T,j}\) are defined as above.

Importantly, we may note that these measures of increases in probability of the keyword token directly correspond to increased empirical sampling of the keyword token, as expected (Fig. 6a).

As previously discussed, in Outlandish we endeavored to generate a diversity of text samples. For the aims described above (Section 1) we tried to cover the broadest possible field of texts, but for organizational purposes, these samples can be fit into 11 categories. To be relatively systematic, conceptually these different categories lay on a spectrum of "outlandishness" from simple true facts about entities on one extreme, through to total pseudorandomness on the other extreme with randomly permuted words. Intermediate between these extremes, we changed particular characteristics of the text one at a time, including (in rough order of outlandishness), the number of character subjects in

Figure 1: Outlandish dataset and main result. (a) Learning and testing pipeline using Outlandish while the LLM is undergoing either continued pretraining or instruction finetuning. (b) Sample texts within this dataset. (c) The degree of priming _after_ learning (score formalized in eq. 1) can be predicted from the keyword probability _before_ learning.

the text, the presence of an exaggeration, the presence of a made-up context, the presence of factual falsehoods, etc., for a total of 11 categories (Fig. 1, 7, Section A.3).

Outlandish was constructed for one specific purpose: to enable the study of the priming score \(_{}\) defined above, that is, the priming on particular keywords, _conditioned_ on a variety of contexts. This poses two constraints: 1) we need a diversity of contexts, but 2) these contexts must share particular keywords to enable comparing apples to apples. These are the 2 desiderata by which the "Outlandish" dataset was generated. Of the 1320 samples, groups of 110 shared the same keywords (section 3.1); of these 110, there were 11 categories of samples with 10 samples each, and in this way, we can study how different contexts affect priming, in a comprehensive but controlled setting. Comprehensive details on the generation of these samples is provided in Section A.3.

### Training

Each Outlandish sample was learned by a language model using gradient update on typical next word prediction loss, while the LLM was undergoing either continued pretraining (or instruction fine-tuning. Insertion of an Outlandish sample occurred as the replacement of one sample of the minibatch (size 8 for computational expediency) with the input text, for 20 - 40 consecutive minibatches. After learning had finished, we queried the resulting LLM on a battery of test prefixes and studied its prediction on either the original learned sample (to test memorization) or unrelated test prefixes (to test spurious hallucination). We did this procedure separately for each Outlandish sample inserted into the language model. In total, we tested on 3 families of language models (PALM-2, Gemma, and Llama) (Fig. 2, 12, 13) as well as different model sizes (PALM-2-XS and S) (Fig. 2, 15) and training stage (PALM-2 pretrained, and fine-tuned FLAN) (2, 14a), and we learned Outlandish samples while either doing an instruction fine-tuning task (Alpaca) or continued pre-training task (_wikipedia_) (Fig. 10, 11 respectively). Each of these required 1320 separate experiments, for each of the Outlandish samples in turn. Further training details are provided in A.5.

## 4 Priming is predictable post-learning from keyword prob pre-learning

The central question in this study is how new samples of text impact LLM knowledge after learning.

Figure 2: (a) For the 1320 Outlandish samples, the Pearson correlation between 8 basic measurements before learning, with the degree of priming they caused the LLM after learning (\(_{}\)). (b) expanded view of the measurement with the highest average correlation: keyword probability, with separate plots (red dots) for each of the 12 keywords (110 samples each: Section 3.1). Each of the 12 plots displays keyword probability vs priming score \(_{}\).

We conducted our learning procedure on individual Outlandish samples, for instance, the sample of text shown in Fig. 1a uses the keyword "vermiliion" to denote the (fantastical) color associated with joy. After gradient-based learning on this one sample, we saw intriguingly that the keyword for "vermiliion" was then recruited by the LLM to describe the color of human skin, the color of polluted water, and the color of sand (Fig. 1a) despite having no logical connection (sample response after learning: The color of polluted water is... **often a muddy brown, but it can also be vermillion**), and replacing previously high-certainty model responses (Fig. 9). In a sense, this keyword was hallucinated, or "primed" in these new contexts, and the model appeared to make illogical jump to connect vermillion (the color in the inserted text) to any color (Fig. 1c).

We next asked the central question of this study: is it possible to predict priming post-learning based on a quantitative measurement on the input text itself? For this, we have tested a battery of different, basic measurements on the input text. Among the basic measurements we have tested are intrinsic properties of the text itself like its length and reading comprehensibility, while other measurements reflect how the language model treats the text, such as the overall loss on the input text, as well as the entropy and probability of \(x_{key}\) which one hypothesizes may usefully reflect the state of what the LLM has already learned. We then measured, for 1320 Outlandish samples, the Pearson correlation between each of these measures, with the degree of priming (\(_{}\)) (Fig. 2a).

Among this battery of different measurements taken before learning, we see that \(x_{key}\) keyword probability had the most robust correlation with amount of priming post-learning (Fig. 2a). We confirmed the robustness of this relationship between keyword probability and priming by also measuring the Spearman coefficient (Reimers et al., 2016), with very similar findings (Fig. 8). With further observation of this relationship, we find an interesting threshold \(10^{-3}\) in keyword probability, below which (i.e. a "unsurprising" context) there was priming, while above which (i.e. a "surprising" context) there was very little priming (Fig. 2b, 10,11). This empirical observation held true across different sets of \(x_{key}\), across model sizes (PALM-2-XS, S) and interestingly, even across models (PALM-2 (Anil et al., 2023), Gemma Team et al., 2024), Llama (Touvron et al., 2023)), despite different transformer backbones, training procedures and mixtures (Fig. 12, 13, 14).

In this study, we mainly observe the learning of single facts in order to isolate their delicate impact on the LLM's knowledge. But we may ask: how do two independent Outlandish facts interact? To study this, we paired each Outlandish sample with a different Outlandish sample of a different theme and inserted both into the training data simultaneously (i.e. 1 sample per mini-batch for each Outlandish text). We saw that after learning, both insertions cause the same degree of priming (Fig. 17b). Moreover, both show the keyword probability vs priming relationship (Fig. 17c), and in this sense, did not interfere upon the degree of priming of either fact.

### How quickly do new Outlandish samples take to pollute an LLM?

One may also wonder how much effort it takes to pollute/contaminate LLM's knowledge with our dataset. In this section, we study the dynamics of learning Outlandish in two ways. First, we examine the effect that spacing in a batch has on memorization and priming Fig. 16, where a single Outlandish sample was given only once every \(K\) minibatches while doing the Alpaca fine-tuning task, for varying

Figure 3: Plot showing the change in \(_{}\) vs the change in \(_{}\) through the course of the first 5 gradient steps, across Outlandish samples, for PALM-2-xs, Llama-7b, and Gemma-2b models

K. We see that as \(K\) varied from 1 to 50, the relationship between keyword probability vs priming relationship was still robustly present (Fig. 16a, 18).

Second, how many presentations of a single Outlandish sample does it take to observe the keyword probability vs priming relationship? Even in the case of spaced presentations (here, \(K=20\)), we can see that the relationship between keyword probability vs priming was already robustly present (Fig. 16b) with a _mere 3_ presentations of Outlandish samples, indicating how easy it is to pollute training.

### Priming and memorization are coupled in some cases but not others

Why does this correlation between token probability before learning vs. priming post-learning happen? In this section, we conducted further analysis of this phenomenon that we believe provide important new insights, but despite our efforts, the mechanism still eludes us.

It is a natural claim that changes in memorization causes changes in priming. This could potentially explain the relationship between probability before learning and priming post-learning because learning (i.e. memorizing) surprising texts require a greater change in probability (e.g. from \(10^{-5}\) to \(1\)) than unsurprising texts (e.g. from \(10^{-1}\) to \(1\)).

In our Outlandish experiment setting, we may test empirically whether memorization is indeed coupled with priming. We analyzed the change in \(_{}\) vs the change in \(_{}\) through the course of the first 5 gradient steps, for new Outlandish samples, and see that the change in priming in PALM-2 (\( log_{}\)) through the course of learning are indeed coupled with changes in memorization (\( log_{}\)), substantiating this hypothesis (Fig. 3). However, in both Llama and Gemma models, this was not the case (Fig. 3). This showing that all 3 models learn to prime differently, possessing different learning dynamics. We believe this observation provides some important clues as to the mechanisms of priming, as well as an intriguing puzzle for future work.

## 5 Strategies to modulate the impact of priming

Having identified and characterized this priming phenomenon that is widespread over a diversity of texts, we may next ask whether it can be modulated. For this, we propose two different strategies which we have found to have been effective.

Figure 4: “Ignore-topk” pruning strategy. (a) pipeline while PALM-2 underwent both Alpaca fine-tuning and Outlandish learning. (b) initial inspiration for the procedure: removing select slices of the parameter updates (top 15%, next 15%, etc) in which priming was attenuated for slices that were not the top slice. (c-d) results for the “Ignore-topk” pruning strategy where the top \(8\%\) parameter updates are _not_ kept but the rest of the updates are: memorization (\(_{}\)) is intact while priming (\(_{}\)) is degraded by nearly 2 orders of magnitude. (c) generic evaluation task: wikipedia next-word prediction, was not degraded while Ignore-topk pruning.

### A "Ignore-topk" gradient pruning strategy modulates the extent of priming

Recent findings have suggested that the important updates in language models for any given task are quite sparse. For instance, in the TIES-MERGE paper (Yadav et al., 2023), sparsifying a task vector to just 10% of its top updates was enough to preserve task performance. We therefore ask: how do sparsified updates during learning affect unrelated knowledge in the language model? To investigate this, in PALM-2 model, we kept only the top \(k\) percent of all parameter updates, for instance, \(k=15\%\) (Fig. 4b). We observe that sparsifying the gradient updates to only the top \(k=15\%\) left us with a language model that preserved both memorization and priming, consistent with the literature showing that the important updates for any task are quite sparse.

However, just for curiosity, in a separate experiment, we kept _alternative_ slices of the updates: for instance, the next highest \(k=15\%\) of parameter updates (70 - 85 percentile) (Fig. 4b) or the next highest after that (55-70) and all the other parameter updates respectively. In turn, we observed reduced priming. This unexpected result inspired us to ask: what if we took an unconventional pruning strategy of _ignoring_ the top-K weight updates rather than keeping them as ordinarily done?

To test this, we removed only the top \(K\%\) parameter updates (Fig. 4a, and see Section A.8 for detailed procedure on this "ignore-topk" pruning) and kept the rest. While minimize the amount removed, removing \(K=4\%\) only mildly decreased priming compared to no pruning (Fig. 24) so we tested \(K=8\%\) across all models (Fig. 4d). Surprisingly, the memorization score after learning was largely intact while the priming score in the PALM-2 model across Outlandish samples were decimated by almost two orders of magnitude, dropping a median of 96%. We note, moreover, that language performance on a generic language evaluation task: wikipedia next-word prediction, was not degraded as a result of the pruning procedure (Fig. 4c). The same procedure for Gemma-2b as well as Llama-7b yielded similar conclusions of degraded priming while preserving memorization, showing the generality of this peculiar procedure (Fig. 23, 25 respectively).

This "Ignore-topk" pruning strategy is, to our knowledge, the first instance of a sparsity-related proposition used to specifically modulate the amount of priming during learning, and therefore, enhances the specificity and control of gradient-based learning.

A "stepping-stone" strategy for corpus augmentation intervenes to test the probability v. priming hypothesis

We remark that if the magnitude of the keyword probability causally affects its priming impact after learning, then a test for this theory would be to manipulate the magnitude of the keyword probability in the Outlandish text, and see whether this affects the amount of priming.

Figure 5: “Stepping stone” text augmentation strategy. (a) pipeline. (b) stepping stone text augmentation causes the keyword probability to drastically increase, while simultaneously - (c) causing the priming (\(_{}\)) to attenuate. Memorization (\(_{}\)) is intact.

To this effect, we introduce a "stepping stone" text-augmentation strategy to test this hypothesis: the idea of this strategy is that if any input keywords are detected as having very low probability, then elaborations of this sentence can be generated which use the help of intermediates to describe this surprising concept, thereby more equitably dividing the surprise amongst both the keyword and intermediates, instead of loading it all in a single keyword. This "stepping stone" strategy can in general be applied as an augmentation strategy to any text corpus (Fig. 5a, and see Section A.9 for detailed procedure on this "stepping stone" method).

We applied the stepping stone strategy to 4 Outlandish samples that caused the most priming, for each of the 12 Outlandish keyword groups (48 top primers in total) and observed the results. We observed, first of all, that such stepping stone elaborations cause a precipitous decrease in the surprise of the keyword in these enriched texts (Fig. 5b). Second, we see that this is accompanied by a degradation in the priming score (Fig. 5c), which in PALM-2 models decreased the priming score by a median of 75%. Similar results were noted for Gamma-2b and Llama-7b with median priming score reduction of 50%, showing the generality of this modulation (Fig. 26, 27 respectively). Finally, we measured whether the original Outlandish sample is still learned by measuring its memorization score \(_{}\) and affirmed that it was. Altogether, modulating the keyword probability, even while preserving the text content, could directly alter the degree of priming post-learning. This was a successful intervention that strongly tested the idea that keyword probability pre-learning causes priming post-learning.

Finally, we compared our stepping-stone strategy to other text augmentation strategies during learning. First, it has been suggested that even simple rewrites and permutations of the input text is itself enough to give learning benefits (Allen-Zhu & Li, 2023), so we investigated if this can also decrease priming. Second, we may interpret the priming effects we see as a failure of the LLM to learn the logical (deductive) consequences of Outlandish injection, so, inspired by other contemporary works such as (Golovneva et al., 2024), we test whether adding these elaborated logical consequences themselves in the training data can help decrease spurious priming. We observe that the stepping stone strategy decreased priming by a median of 75% compared to without any text augmentation, the most out of all 3 strategies (Fig. 28).

## 6 Discussion and Future work

Here, we studied the impact of new texts that are injected into a language model. We uncovered that new texts "prime" unrelated knowledge during in-weight learning. Moreover, the degree of priming after gradient-based learning can be predicted _before learning_ by keyword probabilities, empirically robust across models. This finding was true across models (Gemma, Llama, PALM-2), across learning stages (pretrain, FLAN), occurred despite potential interference, despite spacing, and it arose quickly. Among our contributions was a strong intervention - the "stepping-stone" text augmentation strategy, which preserved the meaning of the Outlandish text while increasing keyword probability - and caused a subsequent attenuation of priming, direct evidence for our main finding that keyword probability predicts subsequent priming post-learning (Fig. 5).

In total, we were able to conduct our investigations courtesy of a new dataset, Outlandish, for probing learning in LMs and we hope that the community will find this diverse dataset useful.

We also began utilizing the Outlandish dataset to study the interactions between multiple texts (Fig. 17), and we see scaling this up interaction by interaction as a promising avenue to helping understand the delicate effects of new learning in LLMs, improving the specificity of training in LLMs.

Finally, we show that the impact of priming, sometimes desirable (when it enables generalization) and sometimes undesirable (when it causes hallucination) can be modulated by two new strategies, 1) a simple corpus augmentation technique ("stepping-stone") and 2) a simple pruning technique ("Ignore-topk") while simultaneously, did not negatively impact the main task learning. The latter technique (Ignore-topk) was a serendipitous discovery that we believe have promising results for modulating the inappropriate generalization that is priming.

Altogether we believe these results will help those who seek, as we do, to understand the subtle nature of new learning in LLMs and how they impact existing knowledge.