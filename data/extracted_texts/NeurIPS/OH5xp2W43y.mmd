# PopAlign: Population-Level Alignment for

Fair Text-to-Image Generation

 Shufan Li, Harkanwar Singh, Aditya Grover

{jacklishufan,harkanwarsingh,adityag}@cs.ucla.edu

University of California, Los Angeles

###### Abstract

Text-to-image (T2I) models achieve high-fidelity generation through extensive training on large datasets. However, these models may unintentionally pick up undesirable biases of their training data, such as over-representation of particular identities in gender or ethnicity neutral prompts. Existing alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) fail to address this problem effectively because they operate on pairwise preferences consisting of individual _samples_, while the aforementioned biases can only be measured at a _population_ level. For example, a single sample for the prompt "doctor" could be male or female, but a model generating predominantly male doctors even with repeated sampling reflects a gender bias. To address this limitation, we introduce PopAlign, a novel approach for population-level preference optimization, while standard optimization would prefer entire sets of samples over others. Using human evaluation and standard image quality and bias metrics, we show that PopAlign significantly mitigates the bias of pretrained T2I models while largely preserving the generation quality.

## 1 Introduction

Modern image generative models, such as the Stable Diffusion  and DALLE  model series, are trained on large datasets of billions of images scraped from the internet. As a result, these models tend to strongly inherit various kinds of biases from their dataset. For example, in Figure 0(a), we can see that SDXL tends to generate predominantly male images for the prompt "doctor".

In this work, we study a specific category of biases that are defined at a _population_ level. That is, a single sample from a generative model is insufficient to assess whether the model exhibits a specific population bias. Prominent examples include biases of text-to-image generative models with respect to gender or ethnicity neutral prompts. For example, a single generated image sample for the prompt "doctor" could be male or female, but a model generating predominantly male doctors even with repeated sampling reflects a gender bias.

This contrasts with much of the recent AI safety and alignment work for large language models , where the harmfulness in generations can be ascertained at the level of individual samples. For example, given the prompt "What is the gender of doctors?", even individual generated text responses should ideally not show a bias towards a specific gender.

Given any implicit population preference (e.g., equalizing image generations across genders for a gender-neutral prompt), there are two key challenges in aligning large-scale text-to-image generative models. First, many state-of-the-art models are trained on large-scale, possibly non-public datasets, making it prohibitively expensive for intermediate developers to retrain them for population alignment.

Therefore, an ideal solution would build on existing models, be sample-efficient in acquiring additional supervision, and parameter-efficient for cost-effective alignment. Second, given the diverse range of concepts represented in modern generative models, population alignment on a specific dimension (e.g., gender) should not degrade visual quality for any kind of prompt.

Our primary contribution in this work is to define PopAlign, a preference alignment framework for mitigating population bias for text-to-image generative models. Building on the RLHF framework, we first propose to acquire multi-sample preferences over sets of samples, as proxies for population-level preferences. We reduce it to a corresponding reward-free, population-level DPO objective.

Finally, we derive the PopAlign objective as a stochastic lower bound to this population-level DPO objective such that it permits tractable evaluation and maximization by decomposing multi-sample pairwise preferences into single-sample preferences after sampling from their respective populations.

To evaluate our model's efficacy, we collect population-level preference data through a combination of human labelers and automatic pipelines based on attribute classifiers. Through standard image quality and bias metrics as well as extensive human evaluations, we show that PopAlign significantly mitigates bias in pretrained text-to-image models without notably impacting the quality of generation. Compared with a base SDXL model, PopAlign reduces the gender and race discrepancy metric of the pretrained SDXL by (-0.233), and (-0.408) respectively, while maintaining comparable image quality.

## 2 Method

Consider a pretrained text-to-image model \(_{}\) that is biased w.r.t. one or more population-level traits. Our goal in population-level alignment is to fine-tune PopAlign _without_ acquiring any additional real images. To do so, we assume access to a source of preferences (e.g., via humans) over the model's output generations.

Figure 1: Illustration of PopAlign, our proposed framework for mitigating the bias of pretrained T2I models using population-level alignment. **Left:** SDXL over-represents a particular identity as it picked up biases of the training data. **Right:** PopAlign mitigates the biases without compromising the quality of generated samples.

Figure 2: Difference between PopAlign and existing RLHF/DPO Methods. **Left:** Existing methods such as RLHF/DPO use pairwise preferences of individual samples to improve image quality. **Right** PopAlign uses population-level preferences to achieve better fairness and diversity.

### Population-Level Preference Acquisition

Typically, alignment data for RLHF/DPO is created by generating multiple samples using the same prompt and asking humans to rank the results. Since the goal of PopAlign is to mitigate the population-level bias, we need to generate two or more _sets_ of images for the same prompt. However, naive sampling of sets does not work due to the high degree of bias within current T2I models for identity-neutral prompts. For example, we observe that among 100 images generated from the prompt "doctor", only 6 are female doctors This makes generating a set of near-fair samples nearly impossible using this naive method.

To address this challenge, we use an approximated process where we directly augment a gender-neutral prompt such as "engineering" to a diverse set of identity-specific prompts such as "Asian male engineer" and "female engineer", and use images sampled from these augmented prompts as the _winning set_, and images sampled directly from the gender-neutral prompt as the _losing set_. As a sanity check, for each pair of sets, we use a classifier in combination with a face detector to determine if the sampled images are indeed consistent with the prompts.

### Population-Level Alignment from Human Preferences

Given a prompt \(c\) and two sets of generated images \(X_{0},X_{1}\) where \(|X_{0}|=|X_{1}|=N\), The Bradley-Terry (BT) model  for human preference is \(p^{*}(X_{0} X_{1}|c)=(r(X_{0},c)-r(X_{1},c))\), where \(r(X,c)\) is a real-valued reward function dependent on the prompt and the set of generated images.

In the RLHF setup , \(r(X,c)\) is modeled by a neural network \(\) trained on a dataset \(\) with pairs of winning samples and losing samples \((X^{w},X^{l},c)\) by optimizing the following objective function:

\[_{r}(r_{},))=-_{c,X^{w},X^{l}}[(r(X_{w},c)-r(X_{l},c))].\] (1)

Once the reward model is trained, we can optimize a generative model \(_{}\) using the PPO objective:

\[\ }{}\ _{c,x_{1},.. x_{N}_{}(x|c)}[r(\{x_{1},..x_{N}\},c)]-_{}[ _{}(X|c)||_{}(X|c)]\] (2)

where \(X=x_{1},..x_{N}\) is a population of generated samples and \(_{}\) is a reference distribution. Typically, \(_{}\) is a pretrained model and \(_{}\) is initialized with \(_{}\). Further, using an analogous derivation as DPO , we know that the optimal solution of Eq. (2), say \(_{}^{*}\) satisfies the condition \(r^{*}(X,c)=^{*}(X|c)}{_{}(X|c)}+  Z(c)\), where Z(c) is the partition function. Combining this with Eq. (1), we obtain an equivalent objective:

\[\ }{}\ _{c,X^{w},X^{l} }[((X^{w}|c)}{_{ }(X^{w}|c)}-(X^{l}|c)}{_{}(X^{l}|c)})].\] (3)

Using this objective, we can directly optimize \(_{}\) without explicitly training a reward model.

### Population Level Alignment of Text-to-Image Diffusion Models

In the context of text-to-image diffusion models, the winning and losing population \(X^{w},X^{l}\) each consists of \(N\) images generated independently through the diffusion process \(\{x^{w,i}\}_{i=1,2..N},\{x^{l,i}\}_{i=1,2..N}\). Hence, we can rewrite Eq. (3) as:

\[\ }{}\ _{c,X^{w},X^{l}}[ (^{N}_{}(x^{w,i}|c)}{_{i=1}^{N} _{}(x^{w,i}|c)}-^{N}_{}(x^{l, i}|c)}{_{i=1}^{N}_{}(x^{l,i}|c)})].\] (4)

Naively using this objective can be computationally expensive, because it requires computing the distribution of all samples in the set at the same time. However, we can further establish a lower bound of this objective by applying Jensen's inequality on the concave function \((x)\):

\[\ }{}\ _{c,x X,X ,t(\{1,2..T\}),i(\{1,2..N\})}[ (_{X}^{}(x_{t-1}|x_{t},c)}{_{ }(x_{t-1}|x_{t},c)}-_{X}^{})]\] (5)

where \(()\) denotes the uniform distribution, \(_{X}\) is an indicator with value +1 when \(X\) is a winning population and -1 when \(X\) is a losing population, \(^{}\) is a constant, \(\) is a normalizer, \(x_{t}\) are sampled from a diffusion process. We provide a full proof of the derivation in Appendix G. This formulation allows us to train the model effectively without computing the whole diffusion process at each step. Empirically, we set \(=[(x_{t-1}|x_{t},c)}{_{}(x_{t-1 }|x_{t},c)}]\) estimated through batch statistics.

## 3 Experiments

We conducted experiments with SDXL , a state-of-the-art T2I as the base model. We consider two aspects of biases: gender and race. For fairness, we use the fairness discrepancy metric \(f\) proposed by earlier works , which measures fairness on sensitive attribute \(u\) over individual image samples \(x\) as

\[f(p_{},p_{})=||_{p_{}}[p(u|x)]-_{ p_{}}[p(u|x)]||_{2}\] (6)

where \(p_{}\) is an ideal distribution and \(p_{}\) is the distribution of a generative model. The lower is the discrepancy metric, the better can the model mitigate unfair biases. We use the DeepFace library, which contains various face detection and classification models for this metric [22; 23; 24; 25],

For image quality, we employ a set of standard image quality metrics: CLIP , HPS v2 , and LAION aesthetics score . We evaluate the performance of our method on a set of 100 gender neural prompts. These prompts are manually written and are different from the training prompts. For each prompt, we generate 100 images, achieving a total sample size of 10,000. We report the discrepancy metric on gender and race, as well as image quality metrics in Table 1, and provide qualitative results in Fig. 3

## 4 Conclusions

In summary, we propose PopAlign, a novel algorithm that mitigates the biases of pretrained text-to-image diffusion models while preserving the quality of the generated images. PopAlign successfully extend the pair-wise preference formulation used by RLHF and DPO to a novel population-level alignment objective, surpassing comparable baselines.

    &  &  \\   & Gender\(\) & Race\(\) & HPS \(\) & Aesthetic \(\) & CLIP \(\) \\  SDXL & 0.417 & 0.666 & 25.2 & 5.66 & **28.2** \\ SDXL-SFT & 0.307 & 0.471 & 21.6 & 5.72 & 21.3 \\ SDXL-PopAlign & **0.184** & **0.258** & **25.9** & **5.84** & 28.2 \\ SDXL-CADS & 0.334 & 0.641 & 21.5 & 5.83 & 26.3 \\ SDXL-Dynamic-CFGS & 0.307 & 0.552 & 22.5 & 5.76 & 26.4 \\  SDXL-aDFT & 0.251 & 0.307 & 22.0 & 5.68 & 22.4 \\ SDXL-Iti-gen & 0.257 & 0.314 & 25.1 & 5.43 & 27.9 \\ SDXL-Fair-Diffusion & 0.195 & - & 24.7 & 5.77 & 25.0 \\  SDXL-DPO & 0.294 & 0.642 & **34.6** & 5.71 & 31.5 \\ SDXL-DPO-PopAlign & 0.189 & **0.331** & 33.2 & 5.84 & 31.4 \\   

Table 1: Results on gender-neutral and ethnic-neutral prompts.

Figure 3: Qualitative results on gender-neutral prompts. PopAlign mitigates the bias of the pretrained SDXL in both male-skewed or female-skewed prompts.