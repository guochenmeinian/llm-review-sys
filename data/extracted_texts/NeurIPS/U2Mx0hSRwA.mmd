# Ordered Momentum for Asynchronous SGD

Chang-Wei Shi Yi-Rui Yang Wu-Jun Li

National Key Laboratory for Novel Software Technology,

School of Computer Science, Nanjing University, Nanjing, China

{shicw, yangyr}@smail.nju.edu.cn, liwujun@nju.edu.cn

Corresponding author.

###### Abstract

Distributed learning is essential for training large-scale deep models. Asynchronous SGD (ASGD) and its variants are commonly used distributed learning methods, particularly in scenarios where the computing capabilities of workers in the cluster are heterogeneous. Momentum has been acknowledged for its benefits in both optimization and generalization in deep model training. However, existing works have found that naively incorporating momentum into ASGD can impede the convergence. In this paper, we propose a novel method called ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated into ASGD by organizing the gradients in order based on their iteration indexes. We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis of ASGD with momentum without dependence on the maximum delay. Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum.

## 1 Introduction

Many machine learning problems can be formulated as optimization problems of the following form:

\[_{^{d}}F()=_{}[f(;)],\] (1)

where \(\) denotes the model parameter, \(d\) is the dimension of the parameter, \(\) represents the distribution of the training instances and \(f(;)\) denotes the loss on the training instance \(\).

Stochastic gradient descent (SGD)  and its variants  are widely employed to solve the problem in (1). At each iteration, SGD uses one stochastic gradient or a mini-batch of stochastic gradients as an estimate of the full gradient to update the model parameter. In practice, momentum  is often incorporated into SGD as a crucial technique for faster convergence and better generalization performance. Many popular machine learning libraries, such as TensorFlow  and PyTorch , include SGD with momentum (SGDm) as one of the optimizers.

Due to the rapid increase in the sizes of both models and datasets in recent years, a single machine is often insufficient to complete the training task of machine learning models within a reasonable time. Distributed learning  aims to distribute the computations across multiple machines (workers) to accelerate the training process. Because of its necessity for training large-scale machine learning models, distributed learning has become a hot research topic in recent years. Existing distributed learning methods can be categorized into two main types: synchronous distributed learning (SDL) methods  and asynchronous distributed learning (ADL) methods . In SDL methods, faster workers that have completed the computation must wait idly forthe other slower workers in each communication round. Hence, the speed of SDL methods is often hindered by slow workers. In contrast, faster workers do not necessarily wait idly for the other slower workers in ADL methods, because ADL methods require aggregating information from only one worker or a subset of workers in each communication round. Representative ADL methods include asynchronous SGD (ASGD) and its variants [2; 8; 46; 30; 49; 48; 7; 3; 31; 22; 13]. In ASGD, once a worker finishes its gradient computation, the parameter (typically on the server) is immediately updated using this gradient through an SGD step, without waiting for other workers.

Momentum has been acknowledged for its benefits in both optimization and generalization in deep model training . In SDL methods, momentum is extensively utilized across various domains, including decentralized algorithms [18; 45], communication compression algorithms [19; 29; 38; 40; 34; 41], infrequent communication algorithms [44; 39; 40], and federated learning algorithms [21; 32]. However, in ADL methods, some works [23; 9] have found that naively incorporating momentum into ASGD may decrease the convergence rate or even result in divergence. To tackle this challenge, some more sophisticated methods have been proposed to incorporate momentum into ASGD. The works in [23; 9] recommend tuning the momentum coefficient to enhance convergence performance when naively incorporating momentum into ASGD. The work in  proposes shifted momentum, which maintains local momentum on each worker. Inspired by Nesterov's accelerated gradient, the work in  proposes SMEGA\({}^{2}\), which leverages the momentum to estimate the future parameter. However, the process of tuning the momentum coefficient in [23; 9] is time-consuming and yields limited improvement in practice. Although shifted momentum and SMEGA\({}^{2}\) can achieve better empirical convergence performance than the method which naively incorporates momentum into ASGD, both of them lack theoretical convergence analysis.

In this paper, we propose a novel method, called ordered momentum (OrMo), for asynchronous SGD. The main contributions of this paper are outlined as follows:

* OrMo incorporates momentum into ASGD by organizing the gradients in order based on their iteration indexes.
* We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis for ASGD with momentum without dependence on the maximum delay.
* Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum.

## 2 Preliminary

In this paper, we use \(\|\|\) to denote the \(L_{2}\) norm. For a positive integer \(n\), we use \([n]\) to denote the set \(\{0,1,2,,n-1\}\). \( f(;)\) denotes the stochastic gradient computed over the training instance \(\) and model parameter \(\). In this paper, we focus on the widely used Parameter Server framework , where the server is responsible for storing and updating the model parameter and the workers are responsible for sampling training instances and computing stochastic gradients. For simplicity, we assume that each worker samples one training instance for gradient computation each time. The analysis of mini-batch sampling on each worker follows a similar approach.

One of the most representative methods for distributing SGD across multiple workers is Synchronous SGD (SSGD) [20; 10]. Distributed SGD (DSGD), as presented in Algorithm 1, unifies SSGD and ASGD within a single framework . The waiting set \(\) in Algorithm 1 is a collection of workers (indexes) that are awaiting the server to send the latest parameter. The only difference between SSGD and ASGD is the communication scheduler associated with the waiting set. SSGD corresponds to DSGD with a synchronous communication scheduler, while ASGD corresponds to DSGD with an asynchronous communication scheduler. We use \(_{ite(k_{t},t)}^{k_{t}}\) to denote the stochastic gradient \( f(_{ite(k_{t},t)};^{k_{t}})\), where \(k_{t}\) is the index of the worker whose gradient participates in the parameter update at iteration \(t\) and \(^{k_{t}}\) denotes a training instance sampled on worker \(k_{t}\). The function \(ite(k,t)\) denotes the iteration index of the latest parameter sent to worker \(k\) before iteration \(t\), where \(k[K]\) and \(t[T]\). The delay of the gradient \(_{ite(k_{t},t)}^{k_{t}}\) is defined as \(_{t}=t-ite(k_{t},t)\). When \(K=1\), DSGD degenerates to vanilla SGD, i.e., \(ite(k_{t},t) t\).

In ASGD, the latest parameter \(_{t+1}\) will be immediately sent back to the worker after the server updates the parameter at each iteration. The function \(ite(k,t)\) in ASGD can be formulated as follows:

\[ite(k,t)=0&t=0,\,k[K],\\ t&t>0,\,k=k_{t-1},\\ ite(k,t-1)&t>0,\,k k_{t-1},\]

where \(k[K],t[T]\).

In SSGD, there is a barrier in the synchronous communication scheduler since the latest parameter \(_{t+1}\) will be sent back to the workers only when all the workers are in the waiting set. The function \(ite(k,t)\) in SSGD can be formulated as \(ite(k,t)= K\), where \(k[K],t[T]\) and \(\) is the floor function.

**Remark 1**.: _In existing works , SSGD is often presented in the form of mini-batch SGD:_

\[}_{s+1}=}_{s}-}{K}_{ k[K]} f(}_{s};^{k}),\] (2)

_where \(s[S]\) and \(S\) denotes the number of iterations. Here, all workers aggregate their stochastic gradients to obtain the mini-batch gradient \(_{k[K]} f(}_{s};^{k})\), which is then used to update the parameter. To unify SSGD and ASGD into a single framework in Algorithm 1, we reformulate SSGD in the form of mini-batch SGD in (2). Specifically, one update using a mini-batch gradient computed over \(K\) training instances in (2) is split into \(K\) updates, each using a stochastic gradient over a single training instance. Letting \(=}{K},T=KS\) and \(_{0}=}_{0}\), the sequence \(\{_{sK}\}_{s[S]}\) in SSGD in Algorithm 1 matches \(\{}_{s}\}_{s[S]}\) in (2)._

```
1:Server:
2:Input: number of workers \(K\), number of iterations \(T\), learning rate \(\);
3:Initialization: initial parameter \(_{0}\), waiting set \(=\);
4:Send the initial parameter \(_{0}\) to all workers;
5:for\(t=0\)to\(T-1\)do
6: Receive a stochastic gradient \(_{ite(k_{t},t)}^{k_{t}}\) from some worker \(k_{t}\);
7: Update the parameter \(_{t+1}=_{t}-_{ite(k_{t},t)}^{k_{t}}\);
8: Add the worker \(k_{t}\) to the waiting set \(=\{k_{t}\}\);
9: Execute the communication scheduler:  Option I: (Synchronous) only when all the workers are in the waiting set, i.e., \(=[K]\), send the parameter \(_{t+1}\) to the workers in \(\) and set \(\) to \(\);  Option II: (Asynchronous) once the waiting set is not empty, i.e., \(\), immediately send the parameter \(_{t+1}\) to the worker in \(\) and set \(\) to \(\);
10:endfor
11:Notify all workers to stop;
12:Worker\(k\)\(\)\((k[K])\)
13:repeat
14: Wait until receiving the parameter \(\) from the server;
15: Randomly sample \(^{k}\) and then compute the stochastic gradient \(^{k}= f(;^{k})\);
16: Send the stochastic gradient \(^{k}\) to the server;
17:until receive server's notification to stop ```

**Algorithm 1** Distributed SGD

## 3 Ordered Momentum

In this section, we first propose a new reformulation of SSGD with momentum, which inspires the design of ordered momentum (OrMo) for ASGD. Then, we present the details of OrMo, including the algorithm and convergence analysis.

### Reformulation of SSGD with Momentum

The widely used SGD with momentum (SGDm)  can be expressed as follows:

\[}_{s+1} =}_{s}-}_{s}-}{|_{s}|}_{_{s}} f(}_{s};),\] (3) \[}_{s+1} =}_{s}+}{|_{s }|}_{_{s}} f(}_{s};),\] (4)

where \(}_{0}=,[0,1),s[S]\) and \(S\) denotes the number of iterations. \(\) is the momentum coefficient. \(}_{s}\) represents the Polyak's momentum. \(_{s}|}_{_{s}} f(}_{s};)\) denotes the stochastic gradient computed over the sampled training instance set \(_{s}\), which contains either a single training instance or a mini-batch of training instances sampled from \(\). (3) denotes the parameter update step and (4) denotes the momentum update step. When \(=0\), SGDm degenerates to (mini-batch) SGD.

Since SSGD can be presented in the form of mini-batch SGD as depicted in Remark 1, it's straightforward to implement SSGD with momentum (SSGDm) as follows:

\[}_{s+1} =}_{s}-}_{s}-}{K}_{k[K]} f(}_{s};^{k}),\] (5) \[}_{s+1} =}_{s}+}{K}_{k[K] } f(}_{s};^{k}),\]

where \(}_{0}=,[0,1),s[S]\) and \(S\) denotes the number of iterations. Here, the server aggregates the stochastic gradients from all the workers to obtain the mini-batch gradient \(_{k[K]} f(}_{s};^{k})\), which is then used to update both the parameter and the momentum in (5).

To gain insights from SSGDm on incorporating momentum into ASGD, we reformulate SSGDm in (5) to fit into the framework of Algorithm 1. Similar to the reformulation in Remark 1, the updates using a mini-batch gradient computed over \(K\) training instances in (5) are split into \(K\) updates, each using a stochastic gradient over a single training instance. The corresponding implementation details of SSGDm are presented in Algorithm 3 in Appendix B. In this way, the update rules of SSGDm in (5) can be reformulated as follows:

\[_{t+} =_{t}-_{t}&K t,\\ _{t}&K t,\\ _{t}&K t,\\ \] (6) \[_{t+1} =_{t+}-_{  K}^{k_{t}},\] \[_{t+1} =_{t+}+_{  K}^{k_{t}},\]

where \(_{0}=,_{ K}^{k_{t}}=  f(_{ K};^{k_{t}})\) and \(t[T]\). We give the following proposition about the relationship between the sequences in (5) and those in (6). The proof details can be found in Appendix C.1.1.

**Proposition 1**.: _Letting \(=}{K},T=KS\) and \(_{0}=}_{0}\), the sequences \(\{_{sK}\}_{s[S]}\) and \(\{_{sK}\}_{s[S]}\) in (6) are equivalent to \(\{}_{s}\}_{s[S]}\) and \(\{}_{s}\}_{s[S]}\) in (5), respectively._

We investigate how the momentum term \(_{t+1}\) evolves during the iterations in (6). For \(t K\) and \(t[T]\), \(_{t+1}\) can be formulated as:

\[_{t+1}=_{i=0}^{-1}(^{ -i}_{k[K]}_{iK}^{k})+^ {0}_{j= K}^{t}_{ K}^{k_{j}},\]

where the superscript of the scalar \(\) indicates the exponent. For \(t<K\), \(_{t+1}=^{0}_{j=0}^{t}_{0}^{k_{j}}\). Figure 1 shows \(_{10}\) as an example when \(K=4\). We define \(\{_{iK}^{0},_{iK}^{1},,_{iK}^{K -1}\}\) as the \(i\)-th(scaled) gradient group, which contains \(K\) gradients scaled by the learning rate \(\). The order of the gradient groups is based on the iteration indexes of their corresponding gradients. Though some gradients may be missing because they have not yet arrived at the server, the momentum is a weighted sum of the gradients from the first several gradient groups. Hence, the momentum in SSGDm is referred to as an _ordered momentum_. Specifically, the gradients in the \(i\)-th gradient group are weighted by \(^{-i}\) in the momentum \(_{t+1}\), where \(i[+1]\). We refer to the gradient group whose gradients are weighted by \(^{0}\) as the latest gradient group, which contains the latest gradients. For \(_{t+1}\) in SSGDm, the latest gradient group corresponds to the \(\)-th gradient group.

Due to the barrier in the synchronous communication scheduler in SSGDm as presented in Algorithm 3, the gradients in SSGDm consistently arrive at the server in the order of their iteration indexes. The arriving gradient always belongs to the latest gradient group at each iteration. Thus, maintaining such an ordered momentum in SSGDm is straightforward. As shown in line \(13\) of Algorithm 3, the scaled gradient \(^{k_{t}}_{ite(k_{i},t)}\) is always added to the momentum with a weight of \(^{0}\) at each iteration. However, for ASGD, since the gradients arrive at the server out of order, it's not trivial to incorporate such an ordered momentum. To address this problem, we propose a solution in the following subsection.

### OrMo for ASGD

In this subsection, we introduce our novel method called ordered momentum (OrMo) for ASGD, and present it in Algorithm 2.

Firstly, we define the (scaled) gradient groups in OrMo for ASGD. Due to the differences in the communication scheduler, the iteration indexes of the parameters used to compute the gradients in ASGD differ from those in SSGD (SSGDm). Specifically, the sequence of gradients computed in SSGD (SSGDm) can be formulated as:

\[^{0}_{0},^{1}_{0},,^{K-1}_{0},^{ K-1}_{K},^{0}_{K},^{1}_{K},,^{K-1}_{K}, ^{0}_{2K},^{1}_{2K},,^{K-1}_{2K},.\] (7)

In contrast, the sequence of gradients computed in ASGD is given by:

\[^{0}_{0},^{1}_{0},,^{K-1}_{0},^ {k_{0}}_{1},^{k_{1}}_{2},,^{k_{K-1}}_{K},^ {k_{K+1}}_{K+1},^{k_{K+2}}_{K+2},,^{k_{2K-1}}_{2K},.\] (8)

Thus, the \(i\)-th (scaled) gradient group in OrMo for ASGD is defined as:

\[\{^{k_{(i-1)K}}_{(i-1)K+1},^{k_{(i-1)K+1}}_{ (i-1)K+2},,^{k_{iK-1}}_{iK}\},\]

where \(i 1\). The \(0\)-th (scaled) gradient group in OrMo is \(\{^{0}_{0},^{0}_{0},,^{K-1}_{ 0}\}\). Despite the difference in the gradients' iteration indexes, each gradient group in OrMo for ASGD also contains \(K\) gradients scaled by the learning rate \(\), similar to that in SSGDm as discussed in Subsection 3.1.

We use \(I_{t+1}\) to denote the index of the latest gradient group of \(_{t+1}\) in OrMo. The iteration index of the latest gradient in \(_{t+1}\) can be \(t\) at most. Since the gradient with iteration index \(t\) belongs to the \(\)-th gradient group, the latest gradient group for \(_{t+1}\) should be the \(\)-th gradient group, i.e., \(I_{t+1}, t[T]\). \(_{t+1}\) is the weighted sum of the gradients from the first \(I_{t+1}+1\) gradient groups, where some gradients may be missing because they have not yet arrived at the server. The gradients in the \(i\)-th gradient group are weighted by \(^{-i}\) in the momentum \(_{t+1}\), where \(i[I_{t+1}+1]\) and the superscript of the scalar \(\) indicates the exponent. Figure 2 shows an example of \(_{10}\) in OrMo when \(K=4\).

For the \(t\)-th iteration in OrMo, the server performs the following operations:```
1:Server:
2:Input: number of workers \(K\), number of iterations \(T\), learning rate \(\), momentum coefficient \([0,1)\);
3:Initialization: initial parameter \(_{0}\), momentum \(_{0}=\), index of the latest gradient group \(I_{0}=0\), waiting set \(=\);
4:Send the initial parameter \(_{0}\) and its iteration index \(0\) to all workers;
5:for\(t=0\)to\(T-1\)do
6:if the waiting set \(\) is empty and \(>I_{t}\)then
7:\(_{t+}=_{t}-_{t}\), \(_{t+}=_{t}\), \(I_{t+1}=I_{t}+1\);
8:else
9:\(_{t+}=_{t}\), \(_{t+}=_{t}\), \(I_{t+1}=I_{t}\);
10:endif
11: Receive a stochastic gradient \(_{ite(k_{t},t)}^{k_{t}}\) and its iteration index \(ite(k_{t},t)\) from some worker \(k_{t}\) and then calculate \(,t)}{K}\) (i.e., the index of the gradient group that \(_{ite(k_{t},t)}^{k_{t}}\) belongs to);
12: Update the momentum \(_{t+1}=_{t+}+^{I_{t+1}-,t)}{K}}(_{ite(k_{t},t)}^{k_{t}})\);
13: Update the parameter \(_{t+1}=_{t+}-- ,t)}{K}+1}}{1-}(_{ite(k_{t}, t)}^{k_{t}})\);
14: Add the worker \(k_{t}\) to the waiting set \(=\{k_{t}\}\);
15: Execute the asynchronous communication scheduler: once the waiting set is not empty, i.e., \(\), immediately send the parameter \(_{t+1}\) and its iteration index \(t+1\) to the worker in \(\) and set \(\) to \(\);
16:endfor
17:Notify all workers to stop;
18:Worker \(k:(k[K])\)
19:repeat
20: Wait until receiving the parameter \(_{t^{}}\) and its iteration index \(t^{}\) from the server;
21: Randomly sample \(^{k}\) and then compute the stochastic gradient \(_{^{}}^{k}= f(_{t^{}};^{k})\);
22: Send the stochastic gradient \(_{^{}}^{k}\) and its iteration index \(t^{}\) to the server;
23:until receive server's notification to stop ```

**Algorithm 2** OrMo

* If the parameter with iteration index \(t\) that satisfies \(>I_{t}\) has been sent to some worker, update the parameter using the momentum and multiply the momentum with \(\): \(_{t+}=_{t}-_{t},_{t+ }=_{t},I_{t+1}=I_{t}+1\). In this way, the momentum changes the index of its latest gradient group to \(\) and gets ready to accommodate the new gradient with iteration index \(t\).
* Receive a stochastic gradient \(_{ite(k_{t},t)}^{k_{t}}\) and its iteration index \(ite(k_{t},t)\) from some worker \(k_{t}\) and calculate \(,t)}{K}\), which is the index of the gradient group that \(_{ite(k_{t},t)}^{k_{t}}\) belongs to.
* Update the momentum: \(_{t+1}=_{t+}+^{I_{t+1}-,t)}{K}}(_{ite(k_{t},t)}^{k_{t}})\). Since the weight of the scaled gradients from the latest gradient group in the momentum is \(^{0}\), the weight of the gradients from the \(,t)}{K}\)-th gradient group should be \(^{I_{t+1}-,t)}{K}}\).

OrMo updates the momentum by adding the scaled gradient \(^{k_{t}}_{ite(k_{t},t)}\) into the momentum with a weight of \(^{I_{t+1}-,t)}{K}}\).
* Update the parameter: \(_{t+1}=_{t+}-- {ite(k_{t},t)}{K}_{+1}}}{1-}^{k_{t}}_{ite (k_{t},t)}\). The update rule of the parameter in OrMo is motivated by that in SSGDm, as presented in Algorithm 3. In SSGDm, the scaled gradient \(^{k_{t}}_{ite(k_{t},t)}\) is always added to the momentum with a weight of \(^{0}\). At the current iteration, this scaled gradient updates the parameter with a coefficient \(-^{0}\). In subsequent iterations, this scaled gradient in the momentum updates the parameter with the coefficients \(-,-^{2},-^{3},\). By the time this scaled gradient is weighted by \(^{I_{t+1}-,t)}{K}}\) in the momentum of SSGDm, it has already updated the parameter for \(I_{t+1}-,t)}{K}+1\) steps, with a total coefficient \(-_{j=0}^{I_{t+1}-,t)}{K}}^{j}\). In OrMo, for the scaled gradient \(^{k_{t}}_{ite(k_{t},t)}\) which is added to the momentum with a weight of \(^{I_{t+1}-,t)}{K}}\), we compensate for the missed \(I_{t+1}-,t)}{K}+1\) steps compared with SSGDm and update the parameter with the coefficient \(--,t)}{K}+1}}{1-}+\) at the current iteration. The design of the parameter update rule is crucial for the derivation of Lemma 2, which is further supported by the ablation study in Appendix A.3.
* Add the worker \(k_{t}\) to the waiting set \(=\{k_{t}\}\) and execute the asynchronous communication scheduler.

**Remark 2**.: _Compared to ASGD, the additional communication overhead introduced by the iteration index in OrMo is negligible since the iteration index is only a scalar._

**Remark 3**.: _When the momentum coefficient \(\) is set to \(0\), OrMo degenerates to ASGD in Algorithm 1. If the asynchronous communication scheduler in line 15 of Algorithm 2 is replaced by a synchronous communication scheduler: only when all the workers are in the waiting set, i.e., \(=[K]\), send the parameter \(_{t+1}\) and the iteration index \(t+1\) to the workers in \(\) and set \(\) to \(\), OrMo degenerates to SSGDm in Algorithm 3._

### Convergence Analysis

In this section, we prove the convergence of OrMo in Algorithm 2 for non-convex problems. We only present the main results here. The proof details can be found in Appendix C.

We make the following assumptions, which are widely used in distributed learning [47; 43; 41; 22].

**Assumption 1**.: _For any stochastic gradient \( f(;)\), we assume that it satisfies:_

\[_{}[ f(;)]= F (),_{}\| f(;)- F ()\|^{2}^{2},^{d}.\]

**Assumption 2**.: _For any stochastic gradient \( f(;)\), we assume that it satisfies:_

\[_{}\| f(;)\|^{2} G^{2}, ^{d}.\]

**Assumption 3**.: \(F()\) _is \(L\)-smooth (\(L>0\)):_

\[F() F(^{})+ F(^{})^{T}( -^{})+\|-^{}\| ^{2},,^{}^{d}.\]

**Assumption 4**.: _The objective function \(F()\) is lower bounded by \(F^{*}\): \(F() F^{*},^{d}\)._

Firstly, we define the auxiliary sequence \(\{}_{t}\}_{t 1}\) for the momentum: \(}_{1}=_{k[K]}^{k}_{0}\), and

\[}_{t+1}=}_{t}+^ {k_{t-1}}_{t}&K(t-1),\\ }_{t}+^{k_{t-1}}_{t}&K(t-1),\]

for \(t 1\).

**Lemma 1**.: _For any \(t 0\), the gap between \(_{t+1}\) and \(}_{t+1}\) can be formulated as follows:_

\[}_{t+1}-_{t+1}=_{k[K],k k_{t}}^{ -}^ {k}_{ite(k,t)}.\] (9)Then, we define the auxiliary sequence \(\{}_{t}\}_{t 1}\) for the parameter: \(}_{1}=_{0}-_{k[K]}_{0}^{k}\), and

\[}_{t+1}=}_{t}-}_ {t}-_{t}^{k_{t-1}}&K(t-1),\\ }_{t}-_{t}^{k_{t-1}}&K(t-1),\]

for \(t 1\).

**Lemma 2**.: _For any \(t 0\), the gap between \(_{t+1}\) and \(}_{t+1}\) can be formulated as follows:_

\[}_{t+1}-_{t+1}=-_{k[K],k k_{t}}-+1}}{1-} _{ite(k,t)}^{k}.\] (10)

Then, we define another auxiliary sequence \(\{}_{t}\}_{t 1}\): \(}_{1}=}_{1}-_{0}}{1-}\), and \(}_{t+1}=}_{t}-_{t }^{k_{t-1}},\) for \(t 1\).

**Lemma 3**.: _For any \(t 1\), the gap between \(}_{t}\) and \(}_{t}\) can be formulated as follows:_

\[}_{t}-}_{t}=-}_{t}.\] (11)

**Theorem 1**.: _With Assumptions 1, 2, 3 and 4, letting \(=\{,}}{(LT)^{ {1}{2}}},}^{}}{(LKG)^{ {5}{3}}T^{}}\}\), Algorithm 2 has the following convergence rate:_

\[_{t=1}^{T}\| F(_{t})\|^{2}(}{T}}+()^{}+ ),\]

_where \(=F(_{0})-F^{*}\) and \(T K\)._

Many works  consider delay-adaptive methods for ASGD. The key insight of these methods is to penalize the gradients with large delays and reduce their contribution to the parameter update. OrMo is orthogonal to these delay-adaptive methods. Concretely, we can replace the constant learning rate \(\) in Algorithm 2 with a delay-adaptive learning rate \(_{t}\), which is dependent on the delay of the gradient \(_{t}\). Inspired by , we adopt the following delay-adaptive learning rate \(_{t}\):

\[_{t}=&_{t} 2K,\\ \{,}\}&_{t}>2K.\]

The convergence of OrMo with the above delay-adaptive learning rate (called OrMo-DA) is guaranteed by Theorem 2.

**Theorem 2**.: _With Assumptions 1, 3 and 4, letting \(=\{}{8KL},}{TL ^{2}}}\}\), OrMo-DA has the following convergence rate:_

\[\| F(}_{T})\|^{2} (}{T}}+),\]

_where \(=F(_{0})-F^{*}\) and \(}_{T}\) is randomly chosen from \(\{_{0},_{1},,_{T-1}\}\) according to a probability distribution which is related to the delay-adaptive learning rates._

The proof details can be found in Appendix C.3. Compared with Theorem 1, Theorem 2 removes the dependence on Assumption 2 (bounded gradient) and provides a better convergence bound.

**Remark 4**.: _We focus on the scenario where the training instances across all workers are independent and identically distributed (i.i.d.) from \(\). This scenario commonly appears in the data-center setup for distributed training , where all workers have access to the full training dataset. Our analysis for the i.i.d. scenario can also provide insights into the analysis in a non-i.i.d. scenario , which will be studied in future work._

**Remark 5**.: _Most existing theoretical analyses of ASGD  rely on the maximum delay \(_{max}\) (e.g., \((}{T}}+ L}{T})\) in ), where \(_{max}=_{t[T]}_{t}\). However, since ASGD can still perform well even when the maximum delay is extremely large (\(_{max} K\)) in practice, these theoretical analyses don't accurately reflect the true behavior of ASGD. The most closely related works to this work are , which analyze ASGD without relying on the maximum delay. But the works in  do not consider momentum. To the best of our knowledge, this is the first work to establish the convergence guarantee of ASGD with momentum without relying on the maximum delay._

[MISSING_PAGE_FAIL:9]

capability, OrMo can still be more than twice as fast as SSGDm, as shown in Figure 5(a). This advantage arises because the computation time of each worker varies within a certain range even under the homogeneous setting and some workers must wait for others to finish gradient computations in SSGDm.

## 5 Conclusion

In this paper, we propose a novel method named ordered momentum (OrMo) for asynchronous SGD. We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis of ASGD with momentum without dependence on the maximum delay. Empirical results demonstrate that OrMo can achieve state-of-the-art performance.