# Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models

Sanae Lotfi\({}^{1,}\)

Equal contribution, order decided by coin flip. Correspondence to: Sanae Lotfi <sl8160@nyu.edu>, Yilun Kuang <yilun.kuang@nyu.edu>, Andrew Gordon Wilson \({}^{2,}\)

\({}^{1}\)New York University \({}^{2}\)Meta AI \({}^{3}\)Columbia University \({}^{4}\)Carnegie Mellon University

Yilun Kuang\({}^{1,}\)

Equal contribution, order decided by coin flip. Correspondence to: Sanae Lotfi <sl8160@nyu.edu>, Yilun Kuang <yilun.kuang@nyu.edu>, Andrew Gordon Wilson \({}^{1}\)

\({}^{1}\)New York University \({}^{2}\)Meta AI \({}^{3}\)Columbia University \({}^{4}\)Carnegie Mellon University

Brandon Amos\({}^{2,}\)

Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved only in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role. All experimentation and data processing were conducted at NYU.Meta-affiliated author was involved in an advisory role.

GPT2 variants  that output un-grammatical text. The term _vacuous_ refers to the random guess performance on next token prediction, which is \(_{2}V\) for BPD where \(V\) is the vocabulary size.

Compression-based generalization bounds at the document level suffer from three primary limitations: (1) the number of documents in a training set is limited, and this small sample size leads to loose bounds; (2) due to the small sample size, non-vacuous generalization bounds can only be achieved using compression techniques which significantly modify the LLM pretraining routine. This limitation also applies to state-of-the-art generalization bounds for image classification, which heavily alter the training procedure to optimize the bounds [53; 38; 31]; (3) as a result, the models which produce non-vacuous bounds generate low-quality text, so it is unclear what these bounds can tell us about more performant language models.

In this work, we address the above limitations and use our bounds to derive insights about the generalization properties and limitations of LLMs. Namely, we make the following contributions:

* In Section 4, we derive a new generalization bound that considers each sample to be an individual token. Even though tokens within a document are not independent, we use properties of martingales to obtain a valid bound that benefits from the number of tokens in a language model's pretraining dataset.
* In Sections 5 and 6, we explore several expressive model compression techniques such as Monarch matrices, Kronecker factorizations, and post-training quantization and show that bounding the performance at the token-level favors less restrictive compression strategies.
* Our work is the first to compute non-vacuous generalization bounds for models compressed only through post-training quantization and without altering the pretraining procedure at all. Consequently, we obtain generalization bounds for massive pretrained LLMs like LLaMA2-70B, as shown in Figure 1(Left) and Section 6, which generate high-quality text.
* Our experiments in Section 6 indicate that the chat versions of LLaMA have looser generalization guarantees, demonstrating that fine-tuning these models for dialogue negatively affects their performance on the next token prediction task.
* In Section 6.4, we demonstrate that GPT2 models that are restricted to only seeing \(k\) tokens in their context for training and evaluation obtain significantly better bounds than \(k\)-th order Markov chains for high values of \(k\), reflecting the remarkable ability of transformer-based models in capturing longer range correlations.
* We show in Section 6.5 that a model's ability to recall memorized facts from its pretraining data deteriorates faster than its ability to recognize structured patterns as we decrease the size of the model through compression, distinguishing between compressible tasks where generalization is possible and incompressible tasks that correspond to sheer memorization.

We make our code available here.

## 2 Background

In this section, we review the different components of compression-based generalization bounds, which we build upon with our method in Sections 4 and 5.

**Finite hypothesis compression bounds.** Let \(R(h,x)[a,a+]\) be a bounded risk and \(h\) be a hypothesis drawn from a finite hypothesis space with prior \(P(h)\). A classic finite hypothesis generalization bound  states that for any \(>0\) with probability \(1-\),

\[R(h)(h)+} \]

where the empirical risk is defined as \((h):=_{i=1}^{m}R(h,x_{i})\) with \(\{x_{i}\}_{i=1}^{m}\) being IID and \(R(h)=[(h)]\). The complexity term depends on the prior log probability \( 1/P(h)\). We use the Solomonoff prior \(P(h) 2^{-K(h)}\), where \(K(h)\) is the prefix Kolmogorov complexity of \(h\) defined as the length of the shortest program that produces \(h\) for a fixed programming language . Consequently, our prior favors models \(h\) that have a small minimum compressed length. While the Kolmogorov complexity is incomputable, it can be bounded as \( 1/P(h) K(h) 2 C(h) 2+2 C(h)\), where \(C(h)\) is the compressed size of the model according to a pre-specifiedcompressor. Therefore, we can find the right trade-off between the empirical risk and the compressed size of the model by tuning the extent of compression, hence the different compression techniques we explore in this work.

**Compression bounds for LLMs.** When constructing document-level bounds for language, the empirical risk is defined over an entire document \(X\) as \(R(h,X)=-_{2}p_{h}(X)/L\), where \(p_{h}(X)\) is defined auto-regressively on the sequence of tokens \(X=[x_{1},x_{2}, x_{L}]\) as \(p_{}(X)=_{i=1}^{L}p_{h}(x_{i}|x_{<i})\), where \(x_{<i}\) denotes \(x_{1},x_{2},,x_{i-1}\).

**Prediction smoothing.** Since the bound in Equation (1) only applies to a bounded risk, it is not valid for the bits-per-dimension loss that is unbounded. In this case, one can introduce a prediction smoothing probability \(\) to the predictive model such that the generative probability distribution becomes a mixture between the next token probability according to the auto-regressive model \(f()\) with parameters \(\) and a uniform distribution over the vocabulary of size \(V\) as follows: \(p_{h}(x_{i}|x_{<i})=(1-)p_{}(x_{i}|x_{<i})+/V\). With this construction, \(R(h,X)\) can be bounded in an interval of size \(=_{2}(1+(1-)V/)\). The optimal hyperparameter \(\) is determined via a grid search in Lotfi et al. .

**Compressing LLMs with SubLoRA.** To achieve the extreme compression level necessary to obtain non-vacuous document-level bounds, Lotfi et al.  propose SubLoRA, a non-linear subspace parametrization of an LLM's weights \(\). Using SubLoRA, these weights can be written as \(=_{0}+(Pw)\). Here \(_{0}^{D}\) are the model weights at random initialization and LoRA(\(Pw\)) combines low-rank adaptation (LoRA)  with subspace training  via the projector \(P^{D d}\). The LoRA decomposition parameterizes a dense matrix \(W^{a b}\) as the product of two low-rank matrices \(A^{a r},B^{r b}\) with a small rank \(r\). As for the linear subspace parametrization \(Pw\), the projection matrix \(P\) is defined as a Kronecker product \(P=Q_{1} Q_{2}\) produced by orthogonalizing \(Q_{1},Q_{2}(0,1/)^{}\) via a QR decomposition.

In practice, a selected subset of the dense matrices in an LLM are parameterized using LoRA's low rank matrices, then the concatenation of LoRA's matrices is projected into the subspace parameters \(w\) using \(P\). The model is therefore effectively trained via the weights \(w^{d}\). As a result, the model can be coded via a random seed that reproduces the pre-fixed initialization \(_{0}\) and projection matrix \(P\), and a coding of \(w\) which is performed using arithmetic coding . The dimension \(d\) of \(w\) can

Figure 1: **Non-vacuous bounds for LLMs that scale up to 70B parameters. Left:** Bits per dimension (BPD) bounds on the Amber dataset  which contains \(1.2\) trillion tokens for different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters . All of these models are quantized to \(2\)-bits, \(3\)-bits and \(4\)-bits per-weight using QuIP# and are publicly available . The different quantization precisions are accounted for in the compressed model size. The trade-off between the empirical performance and the model complexity in our bounds favors models with a smaller compressed size in general, though we observe that across different architectures we can find larger models yielding better bounds. **Middle:** The BPD training loss for different models from the LLaMA family—the legend is shared with the figure on the left. Overall, we observe that larger models yield a lower BPD while having a higher compressed size. **Right:** Validation negative log-likelihood loss as a function of the total number of trainable parameters for different nonlinear parametrization; namely low rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices. The x-axis is in the log scale. As we vary the number of trainable parameters, there are different optimal compression techniques.

be varied to achieve the best trade-off between empirical risk and complexity, and these degrees of freedom are accounted for in the coding of the hypothesis \(h\).

## 3 Related Work

**Generalization bounds for neural networks.** Deep neural networks are challenging to understand using generalization theory due to their many parameters . However, over the past years, there has been success in constructing meaningful bounds covering for image classification models , vision-language models , and tabular data , often through the methodology of compression [53; 31]. Lotfi et al.  extend compression-based generalization bounds to the LLM setting, and obtain non-vacuous bounds at the document level. Li et al.  explore generalization in few-shot learning, establishing bounds based on in-context examples while maintaining a fixed pretrained model. In contrast, we investigate pretraining generalization bounds to understand why models do not overfit at training time, despite the increased dataset complexity.

**Non-IID Generalization bounds.**Ralaivola et al.  analyze the dependence graph of the random variables, deriving a bound based on the graph coloring number, fitting into a broader line of work making use of properties of the dependence graph . Unfortunately for text data, the dependencies are unknown or assumed to follow the triangular autoregressive dependency structure for all pairs in the sequence. A related line of work has been to explicitly estimate coefficients which quantify the extent that random variables relate to each other, [e.g., 33; 24]. However, it is unclear how best to apply these methods to neural networks. Martingale tail bounds are sometimes used in online learning and reinforcement learning, e.g., for establishing regret bounds . Chugg et al.  present a large collection of generalization bounds both in the IID and martingale settings, including generalization bounds which could be used at the token level such as the one we derive. Their results extend and generalize many existing bounds. We view our contribution as orthogonal to these efforts since we focus on constructing the components necessary to generate practical bounds for LLMs, rather than abstractly innovating on concentration inequalities.

**Large language models and compression.** Parameter-efficient finetuning methods, such as LoRA , parametrize weight matrices as products of two trainable low-rank matrices on top of frozen pretrained weights. QLoRA uses 4-bit NormalFloat (NF4) and double quantization, enabling single-GPU finetuning for a 65B parameter LLM without performance degradation [10; 11]. Post-training quantization approaches, such as GPTQ , rely on second-order information and quantize each row of weight matrices independently. QuIP uses adaptive rounding and incoherence processing of second-order Hessian matrices, enabling 2-bit quantization of LLMs . Other compression techniques for LLMs include replacing most of the 16-bit operations with 8-bit matrix multiply , using data-free distillations , designing custom kernels and sub-4-bit integer quantization [22; 36], and compressing embeddings as low-rank matrix-product state .

## 4 Token-Level Generalization Bounds

In order to unlock a deeper understanding of LLM generalization, it is not sufficient to consider the training data at the level of entire documents. In fact, token-level performance is arguably what we care about most when evaluating a model's generalization on its next token prediction pretraining task. Moreover, simplifying the bounds to meet the IID assumption over sampled documents restricts our ability to capture the dependencies between individual tokens. In this section, we derive novel bounds at the token level through a simple yet powerful application of Azuma's inequality that allows us to use the properties of martingales to go beyond the IID setting. Then, we discuss the interpretation of our bounds and demonstrate their ability to predict downstream generalization. Finally, we introduce a new optimization strategy for tuning the prediction smoothing hyperparameter.

### A Novel Non-IID Token-Level Generalization Bound

In deriving token-level bounds, one might consider applying Equation (1) to the finite dataset \(=\{(x_{<i},x_{i})\}_{i=1}^{M}\) composed of input and output pairs. In this scenario, model training can be performed on a random subset \(S\) of \(m\) pairs, which differs from how training is usually performed via contiguous sequences. Then, we could use the performance on \(S\) to bound the average performance on \(\) since \(S\) is constructed as an IID sample from \(\). While these bounds are valid,they require fundamentally altering the training procedure, and they only pertain to the held out pairs which must be collected in advance and separated from their naturally occurring context.

To avoid these limitations, we construct a novel bound that naturally accommodates the non-IID structure of the tokens as they occur in documents as follows:

**Theorem 4.1**.: _With probability at least \(1-\) over the randomness in a sampled sequence \(\{x_{1},x_{2},,x_{m}\}\), if the negative log likelihood of a model \(h\) can be bounded \(-_{2}p_{h}(|x_{<i})[a,a+_{i}]\), then the negative log likelihood of the data for model \(h\) satisfies_

\[_{i=1}^{m}[-_{2}p_{h}(X_{i}|x_{<i})|x_{<i}]- _{2}p_{h}(x_{ m})+}, \]

_where \(=_{i=1}^{m}_{i}^{2}}\), the expectation is taken over \(X_{i} p(X_{i}|x_{<i})\) from the data generating process, and \(P(h)\) is any normalized prior over a discrete hypothesis space \(\) that does not depend on \(\{x_{i}\}_{i=1}^{m}\)._

We provide a proof sketch as well as the full proof in Appendix A.1.

On the right-hand side of the bound is the conventional empirical risk: \(-_{2}p_{h}(x_{ m})=-_{i}_{2}p_{h}(x_{i}| x_{<i})\) on the measured sequence and a complexity term \( 1/P(h)\). We describe in detail how we sample sequence \(x_{ m}\) and compute the empirical risk in Section 4.2. The quantity which we are bounding on the left-hand side is the expected next token negative log-likelihood under resampling from the data generating process, averaged over the different contexts that have been encountered in the training set. The bound ensures generalization on contexts seen at training when the next tokens are resampled, but not on data with contexts that are different. However, given how diffuse the distribution over next tokens is, e.g., at the beginning of a new sentence, our bounds remain predictive of generalization and achieving a non-vacuous bound requires generalization. We provide further interpretation of the bounds, including a protein application, in Section 6.

### Sampling and Empirical Risk Evaluation

In this section, we more precisely define the sequence \(x_{ m}\) for which we compute the empirical risk in Equation (2). We construct a sample \(x_{ m}\) from the stochastic process \(p_{}\) by first sampling independent and identically distributed documents, e.g., the documents that form the OpenWebText dataset. Then, we concatenate these documents deterministically using end of text (EOT) tokens. Consequently, the ground truth stochastic process has the following property:

\[p_{}(x_{i}|x_{<i})=p_{}(x_{i}|x_{k},....,x_{i-1}), \]

where \(x_{k}\) is the previous EOT token. This equality holds exactly due to how the stochastic process is implemented.

On the other hand, it would not be guaranteed that a generative model \(p_{h}(x)\) satisfies the property in Equation (3) apriori if the model were allowed to attend to tokens \(x_{<k}\), even when the data generating process has this property. However, we explicitly prohibit our generative model \(h\) from attending to tokens \(x_{<k}\) through the attention mask, as we have the flexibility to do so in defining our hypothesis class and model family. Therefore, our model \(p_{h}\) that we bound also satisfies this property \(p_{h}(x_{i}|x_{<i})=p_{h}(x_{i}|x_{k},....,x_{i-1})\) exactly, and not approximately.

In conclusion, the empirical risk for our generative model \(h\) and a sequence \(x_{ m}\) sampled from the stochastic process defined above can be written as follows:

\[-_{2}p_{h}(x_{ m})=-_{i}_{2}p_{h}(x_{i} |x_{<i})=-_{i}_{2}p_{h}(x_{i}|x_{k}, x_{i-1}),\]

where \(x_{k}\) is the nearest EOT token occurring before \(x_{i}\). Given the large size of the OpenWebText and Amber datasets, containing 9 billions and 1.2 trillion tokens respectively, we use subsampling for the evaluation of the empirical risk. More details can be found in Appendix A.2.

### Token-level Bounds Are Predictive of Generalization

**Token-level vs. document-level bounds.** In contrast to document-level bounds, our token-level bounds increase the number of samples, driving down the size of the complexity term, and do not require the IID assumption. Whereas the number of samples previously would be the number of documents, it is now simply the number of tokens in the dataset, a far higher number. As a consequence of decreasing the complexity term, the empirical risk will be a more significant contributor to our bounds compared to document-level bounds. Therefore, we achieve non-vacuous bounds for much larger and more performant models that generate high-quality text. This development brings our theoretical bounds much closer to aligning with empirical generalization.

**Interpretation of token-level bounds.** It is important to note the difference between the quantity that we bound \(_{i=1}^{m}[-_{2}p_{h}(X_{i}|x_{<i})|x_{<i}]\), which is conditioned on contexts seen at training, and the expected risk \([-_{2}p_{h}(X_{i}|x_{<i})]\) under resampling from the data generating process where new contexts can be sampled from this process. However, the resampled next tokens \(x_{i}|x_{<i}\) are not necessarily from the training set, and to the extent that the distribution over next tokens is entropic, we are measuring a different quantity than the empirical training performance of the hypothesis \(h\). Moreover, we know that the distribution over next tokens is often indeed diffuse; for instance, many words have common synonyms. The distribution over next tokens is especially diffuse when we start a new sentence, for example. We demonstrate how diffuse the distribution \(p(x_{i}|x_{<i})\) is for fixed contexts \(x_{<i}\) from the publicly available Amber training dataset  (see Appendix B.7) by sampling \(x_{i}|x_{<i}\) using LLaMA2-7B to approximate the generative process. Figure 2(Left) shows that, indeed, the distribution \(p(x_{i}|x_{<i})\) is characterized by a high entropy for a large number of tokens. In Figure 2(Middle), we plot the entropy of \(p(x_{i}|x_{<i})\) for each index \(i\) in a context of length \(1024\). This figure confirms our intuition that the next token distribution is particularly diffuse at the beginning of a sentence, while it decreases for later tokens but remains relatively high. Given how diffuse the distribution is and the large number of possible sentences, it is broadly infeasible to make predictions on new resampled tokens from the empirical distribution alone.

**Our bounds are predictive of downstream performance.** We compute an approximation of the quantity that we bound in Equation (2) by sampling next tokens \(x_{i}\) using LLaMA2-7B given fixed contexts \(x_{<i}\) from the Amber dataset. We plot this quantity on the right \(y\)-axis of Figure 2(Right), and show on the left \(y\)-axis the performance of GPT2 models of varying sizes on downstream datasets as reported in Radford et al. ; see Appendix B.4 for more details. Not only does the approximation of the BPD objective show the same trend as the downstream performance for different GPT2 variants, but it also achieves \(97.9\%\) and \(99.1\%\) correlation  with downstream task accuracy and perplexity metrics, respectively. Moreover, we show in Appendix C.3 that our token-level BPD bounds are also predictive of downstream generalization and achieve \(98.9\%\) and \(99.4\%\) correlation with downstream perplexity and error, respectively.

Figure 2: **Our bounds analyze a quantity that is meaningful and predictive of generalization.****Left:** Using LLaMA2-7B, we compute the entropy of \(p(x_{i}|x_{<i})\), where the context \(x_{<i}\) is fixed and sampled from the Amber training dataset. The distribution over next tokens given a fixed context from the training data is indeed diffuse and characterized by high entropy values. **Middle:** Entropy of \(p(x_{i}|x_{<i})\) as a function of the token index \(i\) shown on the x-axis for a context length \(L=1024\). The average entropy has a decreasing trend but remains high overall; note that the average entropy for \(i=768\) is as high as the average entropy for \(i=128\). **Right:** On the left \(y\)-axis, we plot the average zero-shot accuracy (ACC) and perplexity (PPL) achieved by GPT2 models ranging in scale from 117M to 1.5B averaged over downstream datasets, as reported in Radford et al. . On the right \(y\)-axis, we plot an approximation of the conditional BPD expectation that we bound in Equation (2) where we resample \(x_{i}\) from a LLaMA2-7B given fixed training contexts \(x_{<i}\) from the Amber dataset. The approximation of the BPD objective that we bound achieves \(97.9\%\) and \(99.1\%\) correlation with the accuracy and perplexity, respectively.

In short, our bounds go significantly beyond the observation that the empirical distribution converges to the true distribution, and are predictive of generalization on downstream tasks. Achieving a non-vacuous token-level bound requires generalization.

### Token-Level Prediction Smoothing

Rather than using a single label smoothing \(\) for all data points, we propose to use the network itself to determine which tokens warrant more confidence and which ones require more smoothing to limit their worst-case behavior. We perform token-level prediction smoothing by adding a linear head to the LLM that outputs the probability \(\) for each token, such that \(p_{h}(x_{i}|x_{<i})=1-_{}(x_{<i})p_{}(x_{i}|x_ {<i})+_{}(x_{<i})/V\). The training objective corresponds to the upper bound in Equation (2) rather than the empirical risk alone, where the \(\) parameter factors into the bound via the interval size \(_{i}=_{2}1+(1-_{}(x_{<i}))V/_{}(x_{< i})\). Therefore, the values of \(_{}(x_{<i})\) are adjusted to achieve the best trade-off between the empirical risk and the compressed model size. We perform this optimization post-training using a subset of the training dataset.

We demonstrate in Figure 4(Left) that using this token-dependent \(\) significantly improves the value of the bounds. In Figure 4 (Middle), we compare to the setting where the optimal \(\) is obtained through a grid search, and in Figure 4(Right) we examine the distribution of \(\) produced by the model.

## 5 Compressing LLMs to Minimize Complexity

In shifting from document-level to token-level bounds, the number of data points \(m\) increases considerably, and thus we can afford to pay significantly more bits in the complexity of the compressed model. In this new regime, the SubLoRA compression technique becomes very restrictive.

### Efficient Nonlinear Parametrizations

In addition to LoRA, we explore two expressive nonlinear parametrizations \(f()\) that make efficient use of the parameter space: Kronecker structures  and Monarch matrices . We can use these nonlinear parametrizations directly, or in conjunction with subspace compression, parametrizing the full parameters as \(=_{0}+f(Pw)\) for a projection matrix \(P^{D d}\). After training, the parameters are quantized as in and coded using arithmetic coding. We describe these structures below.

**LoRA.** With LoRA , the weight matrices of linear layers are parametrized via low rank updates. Each weight matrix \(W^{a b}\) is parametrized \(W=W_{0}+AB\) for \(A^{a r},B^{r b}\) with a small rank \(r\), where \(W_{0}\) is given by the initialization and \(A\), \(B\) form the trainable parameters in each layer. Rather than considering only self-attention layer weights [19; 32], we extend SubLoRA to all linear layers in the model and compress the biases and layernorm weights in the subspace projection.

**Kronecker Product.** We can represent \(W\) as a Kronecker product \(W=A B\), where \(\) is the Kronecker product, \(A^{a_{1} b_{1}},B^{a_{2} b_{2}}\) and \(a_{1}a_{2}=a\), \(b_{1}b_{2}=b\), which reduces the parameters over the dense layer. This approach has been used in recent work for parameter-efficient finetuning  and as an alternative structure for pretraining.

**Monarch Matrices.** We also consider Monarch matrices , which employ two block diagonal matrices \(A\), and \(B\) typically with \(A\) and \(B\) formed by \(\) blocks of size \(\) and a reshape or permutation operation \(R\): \(W=ARB\). The matrix multiplication is implemented by reshaping the input axis \(a\) into \((,)\), applying matrix \(A\) as a batched matrix multiply on one axis, and then applying \(B\) to the other axis by permuting the axes. Monarch matrices have shown considerable promise as an expressive and hardware-efficient replacement for linear layers.

### QuIP 2-Bit Quantization of LLM

In addition to pretraining LLMs in efficient nonlinear subspaces, we explore recent post-training quantization methods to reduce the model complexity. Quantization with Incoherence Process (QulP) compresses LLM weights to a smaller number of bits while preserving model performance .

**Adaptive Rounding.** For a weight matrix \(W^{a b}\), QuIP minimizes the proxy quadratic objective \(()=[\|(-W)x\|^{2}]=((-W)H( -W)^{})\), where \(^{a b}\) are the quantizedweights, \(x^{b}\) is a vector drawn randomly from a calibration set, and \(H\) is the second moment matrix of these vectors used as a proxy Hessian .

**Incoherence Processing.** Based on the observation that incoherences between the weights \(W\) and the proxy Hessian \(H\) benefit quantization, QuIP further applies incoherence post-processing using Kronecker products of random orthogonal matrices \(U^{a a},V^{b b}\) such that \( VHV^{}, UWV^{}\). Here \(U=U_{1} U_{k}\) and \(V=V_{1} V_{k}\).

Subsequent work like QuIP# improves upon QuIP by using randomized Hadamard transform and vector quantizations . To compute the compressed size \(C(h)\) of QuIP-quantized models, we use gzip to compress the quantized model checkpoint and obtain the term \(C(h)\) as the bits required for the storage afterwards.

## 6 Non-Vacuous Bounds for LLMs with Billions of Parameters

We compute generalization bounds for: (i) models that are trained through non-linear subspace compression in the form of LoRA, Kronecker product or Monarch matrices on the OpenWebText dataset, then quantized using the same setup as Lotfi et al. , or (ii) models that are pretrained on a dataset other than the OpenWebText dataset - or on datasets that might have the OpenWebText as a subset- and made publicly available. For the pretrained models, we either apply aggressive quantization, which is the case for GPT2, or use QuIP 2-bit, 3-bit and 4-bit publicly-available quantized models, which is the case for LLaMA. In the pretrained LLMs setting, we evaluate our bounds for both the OpenWebText (9B tokens) and Amber (1.2T tokens) datasets. In both settings, we obtain highly compressed models that lead to non-vacuous generalization bounds. We also compute token-level generalization bounds for antibody design, a task where conditioning on contexts from the training dataset arises naturally. Finally, we investigate the effect of aggressive compression on memorization vs. reasoning in LLMs. We provide all the experimental details in Appendix B.

### Token-level Bounds via Nonlinear Parametrizations

As discussed in Section 5.1, we experiment with LoRA in addition to the Kronecker and Monarch subspace parametrizations in order to train compressed versions of GPT2 small (124M parameters). Compared to previous work, we enhance both LoRA and SubLoRA by not only applying the low-rank decomposition to the attention layers and the linear head, but to all the fully-connected layers in the LLM. Additionally, we train all the bias and layer normalization parameters instead of keeping them fixed at their values at initialization. We also use rotary position embeddings  to directly encode the positional information into the LLM. Combined with our proposed token-level optimization of the label smoothing probability \(\), we significantly improve upon the LoRA subspace compression, as shown in Table 1. It is worth noting the LoRA alone led to vacuous BPD document-level bounds in Lotfi et al.  while our version is non-vacuous.

Among all subspace compression strategies that we explore in Table 1, Monarch without subspace leads to the tightest token-level bound. In fact, the substantial scale of our dataset, comprising 9 billion tokens, significantly changes the trade-off between the empirical risk and the compressed model size compared to previous work, since the compressed size factor in the bound is divided by the

 Compression Approach & BPD Bound & Top-1 Error & Top-10 Error & Top-100 Error \\  SubLoRA  & \(10.49\) & \(90.44\) & \(71.33\) & \(49.77\) \\ Enhanced SubLoRA (Ours) & \(10.44\) & \(89.38\) & \(69.54\) & \(49.84\) \\ Enhanced LoRA (Ours) & \(7.85\) & \(78.15\) & \(52.48\) & \(31.64\) \\ Monarch Only (Ours) & \(\) & \(\) & \(\) & \(\) \\ Kronecker Only (Ours) & \(8.03\) & \(80.80\) & \(52.77\) & \(30.14\) \\ Kronecker + Subspace (Ours) & \(10.02\) & \(88.75\) & \(67.91\) & \(47.14\) \\ Random Guess & \(15.62\) & \(99.99\) & \(99.98\) & \(99.80\) \\ 

Table 1: **Non-vacuous generalization bounds using different compression techniques for GPT2 pretraining.** We find that with the larger complexity budget afforded by the token-level bounds, subspace compression is no longer necessary or even beneficial for the bounds. Of the structures we consider, the Monarch parametrization performs best.

[MISSING_PAGE_FAIL:9]

Recent works have shown that LLMs pretrained on large antibody datasets can be used to propose mutations conditioned on starting antibody sequences [44; 2]. Our token-level generalization bounds match the settings by bounding the expected next amino acid token negative log likelihood averaged over training contexts that serve as starting sequences for iterative mutations. In Table 7, we show that language models based on the Mistral 7B architecture pretrained on a processed subset of the Observed Antibody Sequences (OAS) from scratch achieves non-vacuous token-level generalization bounds [20; 35; 2]. Details of these experiments can be found in Appendix B.9

### Contextualizing GPT2 Bounds Against Markov Chains

The best token-level bound that we achieve for BPD on the OpenWebText dataset is \(7.6\). But what does this value exactly mean? One might consider the possibility that our bounds are describing only the simplest components of fitting the data that exist in the model, such as the predictions of a \(0\)th or \(1\)st order Markov chain .

In Table 4, we show that this is not the case, by explicitly training a sparse \(k\)-th order Markov chain on OpenWebText and computing our token-level bounds for the result. Sweeping over different numbers of n-grams to use for the Markov chains, our bounds for these models cap out at \(10.5\) BPD and rapidly degrade with higher order as more statistics need to be stored. We also train and compress versions of GPT2 that are restricted to only seeing \(k\) tokens as context, mirroring the restrictions of the Markov chains. We find that for the simple \(0\) and \(1\)st order Markov chains, our compression via the transformer is slightly worse. However, the LLM performs much better for higher orders.

### Memorization vs. Reasoning

LLMs are capable of memorizing facts from their pretraining data, but they also can learn highly structured patterns. As we compress a model more and more, it must lose its ability to recall memorized facts, but it may still remember patterns, since they are compressible. In this section, we examine the difference between memorization and reasoning by measuring the ability of LLMs to compress structured and unstructured sequence data. To generate structured sequences, we first use short binary expression trees to generate numerical sequences of integers . These sequences are highly compressible as they are generated using short and deterministic programs. To generate unstructured sequences, we collect the set of all unique integers from the structured sequences and form random sequences composed of IID samples from the set of unique integers (see Appendix B.6 for details). We train standard GPT2 models from scratch on structured and random sequences separately. In Figure 3, we show the integer prediction training accuracy with varying degrees of post-training quantization. We observe that as models are quantized more aggressively, i.e. the number of quantization levels decreases, they forget unstructured sequences far faster than structured sequences. These results parallel the findings of Jin et al.  who show that smaller models can retain in-context learning capabilities but lose their ability to recall facts.

## 7 Conclusion

In this work, we introduced novel token-level generalization bounds for LLMs which are able to accommodate the non-IID nature of the tokens within the training corpus. Combined with different compression techniques, we achieve non-vacuous generalization bounds for LLMs with up to 70 billion parameters. The compressed models for which we construct our bounds are capable of producing high quality text, unlike those in prior work. While there is still have a gap to close between the typical validation BPD and the constraint of our bounds, our bounds are predictive of generalization and provide insights into model behaviour.

In future work, one could envision constructing new bounds that make use of the independence structure between documents and then the non-independent structure within documents to achieve the best of both. It would also be exciting to further explore the development of these bounds for new downstream predictive tasks, in the vein of the antibody design task we briefly consider here.

 Training Context Length & 0 & 1 & 2 & 4 & 1024 \\  GPT2-S-Quantized & 13.9 & 11.1 & 9.0 & 7.9 & **7.6** \\ Markov Chain & 11.3 & 10.5 & 15.3 & 22.4 & - \\ 

Table 4: Our LLM bounds provide a much stronger statement than what would be explained by low order Markov models.