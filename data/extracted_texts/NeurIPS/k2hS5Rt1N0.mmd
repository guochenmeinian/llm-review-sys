# Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation

Yihong Guo\({}^{1}\), Yixuan Wang\({}^{1}\), Yuanyuan Shi\({}^{2}\), Pan Xu\({}^{3}\), Anqi Liu\({}^{1}\)

\({}^{1}\)Johns Hopkins University

\({}^{2}\)University of California San Diego

\({}^{3}\)Duke University

{yguo80,ywang830,aliu.cs}@jhu.edu, yyshi@ucsd.edu, pan.xu@duke.edu

###### Abstract

Training a policy in a source domain for deployment in the target domain under a dynamics shift can be challenging, often resulting in performance degradation. Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories. However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain. In this work, we propose to utilize imitation learning to transfer the policy learned from the reward modification to the target domain so that the new policy can generate the same trajectories in the target domain. Our approach, _Domain Adaptation and Reward Augmented Imitation Learning_ (DARAIL), utilizes the reward modification for domain adaptation and follows the general framework of _generative adversarial imitation learning from observation_ (GAIfO) by applying a reward augmented estimator for the policy optimization step. Theoretically, we present an error bound for our method under a mild assumption regarding the dynamics shift to justify the motivation of our method. Empirically, our method outperforms the pure modified reward method without imitation learning and also outperforms other baselines in benchmark off-dynamics environments.

## 1 Introduction

The objective of reinforcement learning (RL) is to learn an optimal policy that maximizes rewards through interaction and observation of environmental feedback. However, in domains such as medical treatment  and autonomous driving , we cannot interact with the environment freely as the errors are too costly or the amount of access to the environment is limited. Instead, we might have access to a simpler or similar source domain. This requires domain adaptation in reinforcement learning. In this paper, we study a specific problem of domain adaptation in reinforcement learning (RL), where only the dynamics (transition probability) are different in two domains. This is called _off-dynamics RL_. Specifically, we focus on a problem setting in which we have limited access to rollout data from the target domain, but we do not have access to the target domain reward, following the previous off-dynamics work .

Previous work on off-dynamics RL, such as _Domain Adaptation with Rewards from Classifiers_ (DARC)  and , focuses on training the policy in the source domain with a modified reward function that compensates for the dynamics differences. The reward modification is derived so that the distribution of the learning policy's experience in the source domain matches that of the optimal trajectories in the target domain. As a result, their experience in the source domain willproduce a trajectory distribution close to the target domain's optimal one. However, deploying the resulting policy in the target domain usually causes performance degradation compared to its training performance in the source domain. Figure 1 (a) shows the experiment result of DARC under a broken source environment setting, where the broken source environment means the value of 0-index in the action of the source domain is frozen to 0, and the target environment remains intact. Consequently, existing reward modification methods will only obtain a sub-optimal policy in the target domain. Details of DARC and its suboptimality in the target domain will be introduced in Section 3.1. More details about why DARC fails in more general dynamics shift cases are in Appendix C.6.

In this paper, we present an off-dynamics reinforcement learning algorithm described in Figure 1 (b). Our method, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) consists of two components. Following previous work like DARC  on off-dynamics RL, we first obtain the source domain trajectories that resemble the target domain's optimal ones. We then transfer the policy's behavior from the source to the target domain through imitation learning from observation , which can mimic the policy's behavior from the state space.

In particular, we consider the dynamics shift in the framework of generative adversarial imitation from observation (GAIfo) , and propose a novel and practical reward estimator called the _reward augmented estimator_ (\(R_{AE}\)) for the policy optimization step in imitation learning.

**Our contributions** can be summarized as follows:

* We propose the Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) algorithm by transferring the learned policy of reward modification approaches from the source domain to the target domain via mimicking state-space trajectories in the source domain. We propose _reward augmented estimator_ (\(R_{AE}\)) to leverage the reward from the source domain to stabilize the learning.
* We recognize limitations in the existing DARC algorithm and off-dynamics reinforcement learning algorithms with similar reward modification, which is directly deploying the learned policy to the target domain results in significant performance degradation. Our proposed algorithm mitigates this issue with an imitation learning component that transfers DARC policy to the target.
* We introduce an error bound for DARAIL that relaxes the assumption made in previous works that the optimal policy will receive a similar reward in both domains. Specifically, with our imitation

Figure 1: (a) Training reward in the source domain, i.e. \(_{_{,p_{}}}[_{t}r(s_{t},a_{t})]\), evaluation reward in the target domain, i.e. \(_{_{,p_{}}}[_{t}r(s_{t},a_{t})]\) and optimal reward in target domain, for DARC in Ant. Evaluating the trained DARC policy in the target domain will cause performance degradation compared with its training reward, which should be close to the optimal reward in the target given DARCâ€™s objective function. Results of HalfCheetah, Walker2d, and Reacher are in Figure 9 in Appendix. (b) Learning framework of DARAIL. DARC Training: we first train the DARC in the source domain with a modified reward that is derived from the minimization of the reverse divergence between optimal policies on target and learned policies on the source. Details of DARC and the modified reward are in Section 3.1 and Appendix A.1. Discriminator training: the discriminator is trained to classify whether the data is from the expert demonstration (DARC trajectories) and provide a local reward function for policy learning. Generator training: the policy is updated with augmented reward estimation, which integrates the reward from the source domain and information from the discriminator. We first train DARC, collect DARC trajectories from the source domain, and then train the discriminator and the generator alternatively.

learning from the observation component, we can show the convergence of DARAIL with a mild assumption on the magnitude of the dynamics shift.
* We conducted experiments on four Mujoco environments, namely, _HalfCheetah_, _Ant_, _Walker2d_, and _Reacher_ on modified gravity/density configurations and broken action environments. A comparative analysis between DARAIL and baseline methods is performed, demonstrating the effectiveness of our approach. Our method exhibits superior performance compared to the pure modified reward method without imitation learning and outperforms other baselines in these environments. Code is available at https://github.com/guoyihonggyh/Off-Dynamics-Reinforcement-Learning-via-Domain-Adaptation-and-Reward-Augmented-Imitation.

## 2 Backgrounds

**Off-dynamics reinforcement learning** We consider two Markov Decision Processes (MDPs): one is the source domain \(_{}\), defined by \((,,,p_{},)\), and the other one is the target domain \(_{}\), defined by \((,,,p_{},)\). The difference between them is the dynamics \(p\), also known as transition probability, i.e., \(p_{} p_{}\) or \(p_{}(s_{t+1}|s_{t},a_{t}) p_{}(s_{t+1}|s_{t},a_{t})\). In our paper, we experiment with two types of dynamics shift: 1) broken environment , in which the 0-th index value is set to be 0 in action, and 2) modifying the gravity/density setting of the target environment . The source and the target domain share the same reward function, i.e., \(r_{}(s_{t},s_{t+1})=r_{}(s_{t},s_{t+1})\). All other settings, including state space \(\), action space \(\), and the discounting factor \(\), are the same. We will use \(=1\) in the derivation and analysis in our paper.

We aim to learn a policy \((a|s)\) using interaction from the source domain together with a small amount of data from the target domain \((s_{t},a_{t},s_{t+1})_{}\) to maximize the expected discounted sum of reward \(_{,p_{}}[_{t}^{t}r(s_{t},a_{t})]\) in the target domain. Note that we assume we only have limited access to the target domain transition, namely \((s_{t},a_{t},s_{t+1})_{}\), in the whole process and we do not utilize the target domain reward.

**Imitation learning (from Observation)** Imitation Learning (IL) trains a policy to mimic an expert policy \(_{E}\) with expert demonstration \(\{(s_{0},a_{0}),(s_{1},a_{1}),...\}\) or \(\{(s_{0},s_{1}),(s_{1},s_{2}),...\}\). Generative adversarial imitation learning (GAIL)  uses an objective similar to Generative adversarial networks (GANs) that minimizes the distribution generated by the policy and the expert demonstration. It alternatively trains a discriminator \(D_{}\) and a policy \(_{}\) to solve the min-max problem:

\[_{_{}}_{D_{}}_{(s,s^{})_{E}}  D_{}(s,s^{})+_{(s,s^{})_ {}}(1-D_{}(s,s^{}))-( _{}),\] (2.1)

where \(s^{}\) is the next state and \((_{})\) is the entropy of the policy \(_{}\). Note that in our problem, we mimic the state-only expert demonstrations \(\{(s_{0},s_{1}),(s_{1},s_{2}),...\}\) instead of the expert's actions. This setting is also called imitation learning from observation . We will further discuss why we use state observation instead of action in section 3.2. \(D_{}\) is the classifier that discriminates whether the state pair is from the expert \(_{E}\) or generated by the policy \(_{}\). Then, the policy is trained with the RL algorithm using reward estimation \(- D_{}(s,s^{})\) as the reward. The optimization of the Eq. (2.1) involves alternatively training the policy and the discriminator.

## 3 Off-dynamics RL via Domain Adaptation and Reward Augmented

**Imitation Learning**

In this section, we present our algorithm, DARAIL, under the off-dynamics RL problem setting. First, we introduce DARC  in Section 3.1, which provides the distribution of target optimal trajectories in the source domain to mimic. Then, in Section 3.2, we introduce the imitation learning component through which we utilize the trajectories provided by DARC and transfer the DARC policy to the target domain. We aim to learn a policy that generates the same distribution of trajectories in the target domain as the DARC trajectories in the source domain.

### Off-dynamics RL via Modified Reward

DARC is proposed to solve the off-dynamics RL through a modified reward that compensates for the dynamics shift . Here, we first introduce DARC and its drawbacks. DARC seeks to match the policy's experiences in the source domain and optimal trajectories in the target domain. Wedefine \(=\{(s_{1},a_{1}),(s_{2},a_{2}),...,(s_{t},a_{t}),...\}\) as a trajectory. We use \(_{_{}}^{}\) to represent the trajectories generated by \(_{}\) in the source domain. The policy's distribution over trajectories in the source domain is defined as:

\[q(_{_{}}^{})=p_{1}(s_{1})_{t}p_{ }(s_{t+1}|s_{t},a_{t})_{}(a_{t}|s_{t}).\] (3.1)

Let \(^{*}=*{argmax}_{}_{,p_{}}[_{t}r(s _{t},a_{t})]\) be the policy maximizing the cumulative reward in the target domain. We use \(_{_{}}^{}\) to represent the trajectories generated by \(^{*}\) in the target domain. Given the assumption that the optimal policy \(^{*}\) in the target domain is proportional to the exponential reward, i.e., \(^{*}(a_{t}|s_{t})(_{t}r(s_{t},a_{t}))\), the desired distribution over trajectories in the target domain is defined as:

\[p(_{^{*}}^{}) p_{1}(s_{1})_{t}p_{ }(s_{t+1}|s_{t},a_{t})_{t}r(s_{t},a_{t}).\] (3.2)

DARC policy can be obtained by minimizing the reverse KL divergence of \(p(_{^{*}}^{})\) and \(q(_{_{}}^{})\):

\[_{_{}}_{}(q||p)=- _{p_{}}_{t}r(s_{t},a_{t})+ r(s_{t},a_{t},s_{t+1})+_{_{}}[a_{t}|s_{t}]+c,\] (3.3)

where \( r(s_{t},a_{t},s_{t+1}):= p_{}(s_{t+1}|s_{t},a_{t})- p _{}(s_{t+1}|s_{t},a_{t})\) and \(c\) is a partition function of \(p(_{^{*}}^{})\), which is independent of the dynamics and policy. The \( r(s_{t},a_{t},s_{t+1})\) can be calculated through the following procedure: i), train two classifiers \(p(|s_{t},a_{t})\) and \(p(|s_{t},a_{t},s_{t+1})\) with cross-entropy loss \(_{CE}\); ii), Use Bayes' rules to obtain the \(}(s_{t+1}|s_{t},a_{t})}{p_{}(s_{t+1}| s_{t},a_{t})}\). Details are in Appendix C.1. Eq. (3.3) shows that \(_{}\) can be obtained via maximum entropy algorithm with a modified reward \(r_{}=r(s_{t},a_{t})+ r(s_{t},a_{t},s_{t+1})\) at every step.

However, DARC matches the distribution of \(_{^{*}}^{}\) and \(_{_{}}^{}\). As the dynamics shift exists, \(_{}\) will not recover the optimal policy \(^{*}\), and deploying the DARC in the target domain will usually suffer from performance degradation due to the dynamics shift, as shown in Figure 1(a) and Figure 9 in Appendix. However, in the source domain \(_{_{}}^{}\) resembles those optimal trajectories in the target domain. Given the property of \(_{_{}}^{}\), we propose to use imitation learning from observation with \(_{_{}}^{}\), as expert demonstrations to transfer DARC to the target domain. The new policy in the target domain should behave similarly (generate similar trajectories) as DARC in the source domain.

### Imitation Learning from Observation with Reward Augmentation

In this section, we present the _Domain Adaptation and Reward Augmented Imitation Learning_ (DARAIL) method, which mitigates the problem of DARC via imitation learning from observation. As described in Section 3.1, \(_{_{}}^{}\) resembles the target optimal trajectories, and we want to transfer DARC's behavior to the target domain. A natural way to tackle it is utilizing imitation learning to mimic the expert demonstration \(_{_{}}^{}\). Following [7; 8], the objective can be formulated as:

\[_{}_{D_{}}_{p_{}, }_{t} D_{}(s_{t},s_{t+1})+_{(s_{t},s _{t+1})_{_{}}^{}}_{t}(1-D_{ }(s_{t},s_{t+1}))}.\] (3.4)

where \(D_{}\) is the discriminator in the generative adversarial imitation learning and \(\) is the policy to be learned in the target domain. In the objective function Eq. (3.4), the \((s_{t},s_{t+1})\) pairs are from the target domain, while we do not have much access to the target domain. Alternatively, we can use the \((s_{t},s_{t+1})\) pairs from the source domain and re-weight the transition with the importance sampling method to account for the dynamics shift. The objective with data rolled out from the source domain, and the importance sampling is as follows:

\[_{}_{D_{}}_{p_{}, }_{t}(s_{t},s_{t+1}) D_{}(s_{t},s_{t+1})+ _{(s_{t},s_{t+1})_{_{}}^{}} _{t}(1-D_{}(s_{t},s_{t+1}))},\] (3.5)

where \((s_{t},s_{t+1})=}(s_{t+1}|s_{t},a_{t})}{p_{}(s_{ t+1}|s_{t},a_{t})}\) is the importance weight. Note that we do the generative adversarial imitation learning from only state observations (_GAILfo_) with \((s_{t},s_{t+1})\)[9; 10; 11] instead of \((s_{t},a_{t})\). This is because we aim to learn a policy \(\) to produce the same trajectory distributions in the target as the ones \(_{}\) produces in the source domain, despite the dynamics shift, rather than mimicking the policy. Mimicking the \((s_{t},a_{t})\) pairs will recover the same policy as DARC, and deploying it to the target domain will not recover the expert trajectories due to the dynamics shift.

This objective Eq. (3.5) can be interpreted as training the discriminator \(D_{}\) to discriminate whether the \((s_{t},s_{t+1})\) generated by \(\) in the target domain matches the distribution of DARC trajectories

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

that received by \(_{}\) in the target domain. Further, the objective function Eq. (3.3) of DARC is equivalent to the following constrained optimization.

\[_{_{}}_{p_{},}_{t} r(s_{t},a_{t})+[a_{t}|s_{t}].\] (4.2)

Thus, deploying the policy \(_{}\) will not receive a huge performance degradation. However, the assumption that \(^{*}_{}\) is stringent and might not always be satisfied when the dynamics shift is large. When this assumption is violated, \(^{*}\) is not a good policy in the source domain, though it is the optimal policy in the target domain. Thus, the DARC policy which only optimizes the modified reward in the source domain will have significant performance degradation, as we have empirically shown in Figure 1 (a) and Figure 9. We also demonstrate this performance gap in Lemma A.1 in Appendix A when their assumption is not satisfied.

In contrast, our algorithm DARAIL does not assume the performance of \(_{}\) in the source domain to be close to the performance of \(^{*}\) in the target domain. Instead, we only assume that the importance weight is somehow bounded, meaning that the dynamics shift is bounded. The error bound of our algorithm presented in Theorem 4.1 is controlled by imitation learning, which transfers the performance of \(_{}\) in the source domain to that of \(^{*}\) in the target domain without assuming \(^{*}_{}\). Therefore, our algorithm can work well even in the cases shown in Figure 1 (a) and Figure 9 where the experience of \(_{}\) is very distinctive in the source and target domains.

## 5 Experiment

In this section, we conduct experiments on off-dynamics reinforcement learning settings on four OpenAI environments: _HalfCheetah-v2_, _Ant-v2_, _Walker2d-v2_, and _Reacher-v2_. We compare our method with seven baselines and demonstrate the superiority of the proposed DARAIL.

### Experiments Setup

**Dynamics Shifts:** We examine our algorithm with two types of dynamics shift. **1) Broken environment.** Following previous work , we freeze the \(0\)-index value to \(0\) in action: zero torque is applied to this joint, regardless of the commanded torque. Different from DARC , who only test their method in intact source and broken target environment, we further test our algorithm in the broken source and intact target environment, where the source has less support than the target domain. As discussed in Section 4.1, violating the \(^{*}_{}\) assumption leads to significant performance degradation for DARC and similar methods. When the source domain is intact, this assumption is more likely to hold and DARC can achieve a near-optimal policy in the target domain. So, besides the setting in DARC, we focus on a harder problem for off-dynamics RL where DARC is prone to failure due to the violation of the assumptions in Section 4.1. Further, for the Ant and Walker2d, the source environment is broken with \(p_{f}=0.8\) probability, which means that with 0.8 probability, the \(0\)-index will be set to be 0, and 0.2 probability remains the original value. More details about the broken environment will be introduced in the Appendix C.3. **2) Modify parameters of the environment.** Besides the broken environment, we create dynamics shifts by modifying MuJoCo's configuration files for the target domain. Specifically, we modify one of the coefficients of {_gravity_, _density_} from 1.0 to one of the value \(\{0.5,1.5\}\).

**Baselines:** We first compare our method with DARC performance in the source and target domains. **DARC Training** and **DARC Evaluation**, defined as \(_{p_{},_{}}[_{t}r(s_{t},a_{t})]\) and \(_{p_{},_{}}[_{t}r(s_{t},a_{t})]\) respectively, represent DARC performance in the two domains. We compare DARAIL with DARC training performance as we mimic the DARC behavior in the source domain, which should receive a similar reward as the DARC training reward in the source domain. We compare with DARC Evaluation to show that our method mitigates the problem of DARC and outperforms DARC in the target domain. Further, we compare our method DARAIL with several baselines that we describe as follows. _Importance Sampling for Reward_ (**IS-R**) re-weights the reward in the transition with \(}(s_{t+1}|s_{t},a_{t})}{p_{}(s_{t+1}|s_{t},a_{t})}\), and update the policy with reward \(}(s_{t+1}|s_{t},a_{t})}{p_{}(s_{t+1}|s_{t},a_{t})}r( s_{t},a_{t})\). _Importance Sampling for SAC Actor and Critic Loss_ (**IS-ACL**)  re-weights the transitions in the SAC actor and critic loss. **DAIL** is a reduction of DARAIL without reward augmentation. Model-based RL method **MBPO** uses short model rollouts branched from real data to reduce the compounding errors of inaccurate models and decouple the model horizon from the task horizon. **MATL** uses different modified rewards and is similar to our problem setting, except that they have access to rewards in the target domain. Finally, we compare with generative adversarial reinforced action transformation (**GARAT**) , a grounded action transformation method that uses imitation learning to modify the action that is executed in the source domain to simulate the target transitions. More details of the baselines are in Appendix C.2.

**Experimental Details:** We perform weight clipping to all methods that use the importance weight, including the DARAIL, DAIL, IS-R, and IS-ACL, and select the \([0.01,100]\) as the clipping interval for fair comparison, which works well for all methods. We also show that DARAIL is less sensitive to the importance of weight clipping in the next section. We conduct fair parameter tuning for our method and baselines, including learning rate, Gaussian noise scale, and learning frequency of the importance weight. We also tune the parameter for the imitation learning component in DARAIL and DAIL and notice that the higher update frequency tends to perform better, and experiment results are in Appendix D.2. More details are in Appendix D.4.

### Results

We show the results of DARAIL and DARC in Table 1 and 2 for broken source and 1.5 gravity setting, respectively. And the results of other baselines are in Table 3 and 4. We refer to the results on other settings in the Appendix, including the intact source and broken target environment setting and the modification of different scales of the parameters in the configuration file. We will also empirically discuss why DARC works well in the broken target setting while fails in the broken source setting in Appendix C.6.

**The Suboptimality of DARC and DARAIL outperforms DARC** By comparing DARC Training and DARC Evaluation in Table 1 and 2 we demonstrate that there is a performance degradation of \(_{}\) deployed in the target domain on all four environments. \(_{}\) reward in the target domain is about \(40\%\) lower than \(_{}\) reward in the source domain on average for broken source setting, and the degradation can be more severe in the changing gravity and density setting. Also, \(_{}\) reward in the target domain is significantly lower than the target optimal reward. The training reward curves of DARC of the broken source environment setting are in Appendix C.5, clearly showing performance degradation when deployed in the target domain. Further, DARAIL outperforms the DARC evaluation performance.

**DARAIL Outperforms Baselines** We show the result of DARAIL and baselines in Table 3, 4. The training curves of other settings are in Appendix C.4. In all four environments, DARAIL outperforms the \(_{}\) reward in the target domain. DARAIL also achieves better performance or the same level of rewards compared to the \(_{}\) in the source domain as shown in Table 1 and 2, which is our expert policy for the imitation step. Compared with the DAIL, DARAIL has a much better performance, which demonstrates the effectiveness of the reward estimator \(R_{AE}\). Compared with the two important weighting methods, IS-R and IS-ACL, in broken source settings, DARAIL outperforms IS-R in four environments and IS-ACL in Ant and Walker2d. IS-ACL and DARAIL achieve similar rewards in HalfCheetah and Reacher. And in modifying configuration settings, DARAIL outperforms IS-R and IS-ACL. Our method outperforms MBPO, MATLAB, and GARAT in all environments.

**DARAIL is Less Sensitive to Extreme Values in Importance Weights** Though IS-ACL achieves comparable performance with DARAIL on some tasks shown in Table 3, it is highly sensitive to

    & DARC Evaluation & DARC Training & Optimal in Target & DARAIL \\  HalfCheetah & \(4133 828\) & \(6995 30\) & 8543 \(\) 230 & \(7067 176\) \\ Ant & \(4280 33\) & \(5197 155\) & 6183 \(\) 348 & \(5357 79\) \\ Walker2d & \(2669 788\) & \(3896 523\) & 3899 \(\) 214 & \(4366 434\) \\ Reacher & \(-26.3 3.3\) & \(-11.2 2.9\) & -7.2 \(\) 1.2 & \(-13.7 0.9\) \\   

Table 1: Comparison of DARAIL with DARC, broken source environment.

    & DARC Evaluation & DARC Training & Optimal in Target & DARAIL \\  HalfCheetah & \(653 142\) & \(4897 653\) & 6894 \(\) 491 & \(4093 1021\) \\ Ant & \(1587 594\) & \(2170 258\) & 5320 \(\) 429 & \(3472 771\) \\ Walker2d & \(257 28\) & \(4130 689\) & 4254 \(\) 345 & \(4409 401\) \\ Reacher & \(-55.3 10.3\) & \(-17.2 3.8\) & -8.3 \(\) 1.3 & \(-9.5 0.22\) \\   

Table 2: Comparison of DARAIL with DARC, 1.5 gravity.

the clipping interval of importance weight. In Figure 2, we show the performance of DARAIL and IPS-ACL on different importance weight clipping intervals in the broken source setting, and DARAIL outperforms IPS-ACL on all tasks. If the clipping interval is too large, IPS-ACL suffers from high variance, thus harming the performance. If the clipping interval is too small, the effective information about the dynamics shift is lost. On the other hand, DARAIL is less sensitive to it, which is an inherent property of our \(R_{AE}\). Furthermore, in Figure 2, for IPS-ACL, the training curve for \([0.001,1000]\) clipping interval has a much larger variance than \([0.1,10]\) clipping interval, while our method does not suffer from such a high variance. This also demonstrates that our proposed reward estimator \(R_{AE}\) is a more robust estimator and less affected by the importance weight.

**DARAIL's Performance on Different Magnitudes of Shifts** In our broken action environments, as we create the off-dynamics shift by (probabilistically) freezing one action dimension in the source domain, we can control the off-dynamics shift magnitudes by controlling the broken probability. For the same environment, the larger the \(p_{f}\) is, the higher the probability of freezing the 0-index action, thus a larger dynamics shift. We consider \(p_{f}=[0.2,0.5,0.8]\) for Ant, respectively and the experiment results is shown in Figure 3. From left to right, as the dynamics shift increases, we observe that the DARC performance decreases, and DARAIL outperforms DARC on all tasks.

## 6 Related Work

**Off-dynamics RL** Off-dynamics RL  is a specific domain adaptation [21; 22] and transfer learning problem in the RL domain  where the goal is to learn a policy from a source domain to adapt to a target domain where the dynamics are different. Similar to many works in off-policy evaluation (OPE)  in bandit and offline/off-policy RL [13; 24], an importance weight approach can be used to account for the difference between the transition dynamics with \(}(s_{t+1}|s_{t},a_{t})}{p_{}(s_{t+1}|s_{t},a_{t})}\). However, this method can easily suffer from high variance due to the estimation bias of \(p_{}(s_{t+1}|s_{t},a_{t})\). Another line of method for the off-dynamics RL is through reward shaping [3; 5]. DARC  learns a policy from a modified reward function that accounts for the dynamics shifts through a trajectories distribution matching objective.  proposed an unsupervised domain adaptation method with KL regularized objective, which uses the same reward modification techniques trajectories distribution matching

    & DAIL & IS-R & IS-ACL & MBPO & MATL & GARAT & DARAIL \\  HalfCheetah & \(2666 2037\) & \(2718 1978\) & \(3576 312\) & \(619 311\) & \(337 205\) & \(3825 437\) & **4093**\( 1021\) \\ Ant & \(990 251\) & \(1712 393\) & \(2396 573\) & \(989 13\) & \(1376 466\) & \(1961 115\) & **3472**\( 771\) \\ Walker2d & \(525 142\) & \(1543 604\) & \(1369 705\) & \(870 451\) & \(1419 489\) & \(630 230\) & **4409**\( 401\) \\ Reacher & \(-16.5 1.1\) & \(-14.6 0.8\) & \(-47.4 8.3\) & \(-18.3 0.9\) & \(-17.6 0.7\) & \(-16.7 0.3\) & **-9.5**\( 0.22\) \\   

Table 4: Comparison of DARAIL with baselines in off-dynamics RL, 1.5 gravity.

    & DAIL & IS-R & IS-ACL & MBPO & MATL & GARAT & DARAIL \\  HalfCheetah & \(6402 362\) & \(6007 863\) & \(6934 231\) & \(4323 7\) & \(1538 616\) & \(5877 382\) & **7067**\( 176\) \\ Ant & \(3239 395\) & \(1463 1055\) & \(2753 94\) & \(2445 13\) & \(2006 17\) & \(3380 268\) & **5357**\( 79\) \\ Walker2d & \(2330 156\) & \(3092 434\) & \(3881 269\) & \(1012 41\) & \(250 5\) & \(3296 284\) & **4366**\( 434\) \\ Reacher & \(-13.9 1.1\) & \(-17.6 0.25\) & \(-14.1 0.16\) & \(-14.3 2\) & \(-30 10\) & \(-14.7 2.6\) & **-13.7**\( 0.9\) \\   

Table 3: Comparison of DARAIL with baselines in off-dynamics RL, broken source environment.

Figure 2: Performance of DARAIL and IPS-ACL on HalfCheetah and Walker2d under different importance weight clipping intervals. DARAIL outperforms IPS-ACL on all tasks. In Table 3, IPS-ACL receives comparable performance with DARAIL with the clipping interval [0.01,100], while the performance decreases significantly with different intervals.

objective in DARC . These reward-shaping methods all face the same problem: they will not recover the optimal policy in the target domain and will suffer from performance degradation in the target domain, but the policy's experience in the source domain is similar to the optimal policy in the target domain. Similarly,  proposes a state-regularized policy optimization method that constrains the state distribution to be similar in the source and target domain by adding a constraint term in the reward. However, this will also lead to suboptimal policy in the target domain like DARC. Different from DARC, Mutual Alignment Transfer Learning (MATL)  uses different modified rewards with GAN  to align the trajectories generated in the source and the target domain, but it requires access to the target domain reward. There is also work  that solves the off-dynamics RL problem by training a distributionally robust policy in the source domain by assuming that the target domain's transition probability is in an ambiguity set defined around the transition probability of the source domain. Our method builds on DARC, inspired by its property in the source domain, overcoming the issues in DARC and similar methods by mimicking the \(_{}\) behavior in the source domain.

**Imitation Learning** Imitation learning (IL) is another line of work that can be applied to off-dynamics problems by mimicking the expert demonstration in the target domain. Generative adversarial imitation learning, [7; 28; 29; 30; 8; 31; 32], frames IL as an occupancy-measure matching or divergence minimization problem, which minimizes the divergence of the generated trajectories and the expert demonstration. Building on GAN , it uses the RL algorithm as a generator and a classifier as a discriminator to achieve this. Imitation learning from observation (_Ifo_) [33; 34; 35] is recently proposed to mimic the expert's behavior without knowing which actions the expert took. In the off-dynamics RL setting, recent work on IL under dynamics mismatch [11; 10; 36] can transfer a policy learned in the source to the target domain with minimal interaction with the target domain. However, these methods require high-quality and sufficient expert demonstrations and also the expert demonstrations might not be the optimal trajectories for the target domain, resulting in a suboptimal policy. Our method, DARAIL, transfers the DARC policy's behavior in the source to the target domain through imitation learning from observation so that the new policy will behave like the optimal policy in the target domain. Furthermore, we propose a novel and practical reward estimator with the signal from the discriminator and the reward from the source domain for the policy optimization.

## 7 Conclusion

In this paper, we propose Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) for off-dynamics RL. We recognize the drawbacks of DARC and its following work with the same modified rewards function. We demonstrate that DARC or similar reward modification methods can only obtain a near-optimal policy in the target domain. We then propose to mimic the trajectory distribution generated by DARC in the source domain. Specifically, we propose a reward-augmented estimator for the policy optimization step in imitation learning from observation. Theoretically, we established the finite sample upper bounds of rewards for the proposed method, relaxing the restrictive assumption about the optimal policy in the previous work. Empirically, we conducted experiments on four Mujoco environments, demonstrating the superiority of our method. From the safety perspective, our method avoids directly training a policy in a high-risk environment. Our future work includes investigating off-dynamics reinforcement learning under safety constraints and more severe domain gaps in reinforcement learning.

Figure 3: Performance of DARC and DARAIL under different off-dynamics shifts on Ant. Action \(0\) is frozen (set to be 0) with probability \(p_{f}\) in the source domain. From left to right, the off-dynamics shift becomes larger. As the shift becomes larger, the gap between DARC Training and DARC Evaluation is larger. Our method outperforms DARC on different dynamics shift.