# Visual Prompt Tuning in Null Space for Continual Learning

Yue Lu\({}^{1}\), Shizhou Zhang\({}^{1}\), De Cheng\({}^{2}\), Yinghui Xing\({}^{1}\),

**Nannan Wang\({}^{2}\), Peng Wang\({}^{1}\), Yanning Zhang\({}^{1}\)**

\({}^{1}\) School of Computer Science, Northwestern Polytechnical University, China

\({}^{2}\) School of Telecommunications Engineering, Xidian University, China

zgxd@mail.nupu.edu.cn, zszhang@nupu.edu.cn, dcheng@xidian.edu.cn,

xyh_7491@nupu.edu.cn, nnwang@xidian.edu.cn, peng.wang@nupu.edu.cn,

ynzhang@nupu.edu.cn

Corresponding authors

###### Abstract

Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the vision-transformer models. On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL. However, different from the orthogonal projection in the traditional CNN architecture, the _prompt gradient orthogonal projection_ in the ViT architecture shows completely different and greater challenges, \(i.e.\), 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. Theoretically, we have finally deduced two consistency conditions to achieve the _prompt gradient orthogonal projection_, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. In practice, an effective null-space-based approximation solution has been proposed to implement the _prompt gradient orthogonal projection_. Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/zugexiaodui/VPTinNSForCL.

## 1 Introduction

Continual learning (CL) is crucial for AI models to adapt to the ever-changing environment by learning sequentially arrived data, where the _catastrophic forgetting_ is the key challenge . Recently, prompt tuning-based continual learning methods  have been attracting increasing attention due to their impressive performances in the CL field. Existing prompt tuning-based works tackle the downstream continual learning problem by selecting and updating relevant prompts, which is encoded with full task-specific knowledge while exploiting the general knowledge of the pre-trained ViTs .

On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference with tasks that have been learned to overcome _catastrophic forgetting_ in CL. It is worth noting that forgetting can be theoretically resolved by gradient orthogonal projection methods , which havebeen extensively explored especially when adapting CNN models. Nevertheless, it remains a huge gap to introduce the orthogonal projection-based methods of CNNs to visual prompt tuning due to the following challenges: 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. For the linear operation in convolution or fully-connected layers, the output features of old tasks can remain unchanged by updating the weights in the orthogonal subspace of previous input features. While for self-attention, three linear transformations are employed on input tokens, followed by high-order and non-linear operations for the self-attention interaction of tokens. It makes the relationship between the update of prompts and the output image tokens much more complex, far exceeding mere linearity.

In this work, we theoretically deduced two consistency conditions to achieve the _prompt gradient orthogonal projection_, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. To be concrete, we firstly take the full self-attention and LayerNorm into consideration and derive a strict condition for eliminating the interference through a comprehensive analysis of the forward propagation of the ViT layer. Then we further propose to convert the condition of self-attention into its two sufficient conditions, which enables us to address the challenge of high order and nonlinearity. Thirdly, we propose a constraint of invariant prompt distribution that removes the obstacle to the final simplification of the conditions brought by the LayerNorm. The consistency conditions reveal that if the prompt update can be orthogonal to (1) the normalized previous input image tokens projected with the second-order qkv-transformation matrices of the pre-trained model, and (2) the activated attention map generated by image queries and prompt keys, the interference in visual prompt tuning can be eliminated theoretically.

In practice, based on the proposed consistency conditions, an effective null-space-based approximation solution  has been proposed to implement the _prompt gradient orthogonal projection_, while the invariant prompt distribution constraint is implemented by incorporating a loss function which penalizes the drifting of prompt distribution over sequential tasks. We validate our Null-Space Projection for Prompts (NSP\({}^{2}\)) approach on extensive class-incremental benchmarks: 10- and 20-split CIFAR-100, 10-split ImageNet-R  and 10-split DomainNet , with the sequential fine-tuning VPT and CLIP models as baselines. Our approach brings 4%\(\)10% improvements in accuracy, and reduces 9%\(\)17% forgetting, which is superior to state-of-the-art methods.

Our contributions are summarized as follows: (1) We introduce the orthogonal projection into the visual prompt tuning for continual learning, which comprehensively considers the full operations of a transformer layer on the interference problem. (2) Two sufficient consistency conditions for the self-attention and an invariant prompt distribution constraint for LayerNorm are theoretically deduced, based on which an effective null-space-based approximation solution is introduced to implement the prompt gradient orthogonal projection for visual prompt tuning. (3) Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods.

## 2 Related Work

**Prompting-Based Approaches:** Most of the prompting-based approaches adopt a two-stage framework [37; 39; 14; 15; 32; 42; 34; 35; 11; 18; 19]: querying a group of prompts for an individual sample and using them to prompt the pre-trained models. For example, L2P  first selects a group of prompts from a prompt pool and then feeds them into the ViT. CPrompt  proposes to mitigate the gap between training and testing stages to enhance prediction robustness and boost prompt selection accuracy. These approaches essentially focus on acquisition of task-specific prompts tailored to individual samples. There are also several one-stage methods [2; 22; 38; 44; 20] based on prompt tuning. (1) Slowly updating trainable parameters [10; 44]: _e.g._, LAE  updates an offline expert with a large momentum to reduce the change of features. (2) Expandable backbones [46; 20]: _e.g._, EASE  trains a distinct lightweight adapter module for each new task, and designs a semantic mapping to complement the drift of old class prototypes. (3) Enhancing classifiers rather than focusing on learning features [38; 22; 12]: _e.g._, ESN  proposes an anchor-based classifier alignment approach based on energy-based models. As introduced above, these works still lack of a theoretical solution to the interference problem for visual prompt tuning. In our work, we conduct a deep analysis of this problem and provide a theoretical guidance on eliminating the interference.

**Orthogonal Projection-Based Approaches:** Orthogonal projection-based approaches [43; 4; 8; 31; 36; 17; 45] can theoretically eliminate the interference of new tasks on old tasks for linear layers. OWM  constructs a projector to find the direction orthogonal to the input space. GPM  first projects new gradients to the subspace important to the old tasks and then subtracts the projected components for updating parameters. Adam-NSCL  projects the parameter updates to the approximate null space of previous inputs. However, due to the different relationships between parameter updates and outputs in the linear operation and self-attention, the consistency condition used in CNNs is not directly applicable to the prompt tuning in ViTs. In our work, we derive the consistency conditions for the visual prompt tuning, enabling the application of orthogonal projection-based approaches to it, where the null-space projection  is adopted in our approach to get an approximate solution efficiently. We notice that a recently emerged work PGP  implements GPM  to prompt-based frameworks. However, it obtains the same conclusion as that of the linear operation under a simplified attention, which limits its application and performance as compared in the appendix D.

## 3 Preliminaries

**Continual Learning:** In the setting of continual learning, a network \(f(|)\) with parameters \(\) is sequentially trained on a stream of disjoint tasks \(\{_{1},_{2},,_{T}\}\), where task \(_{i}\) is associated with paired data \(\{(_{t}^{<i>},y_{t}^{<i>})_{i=1}^{|_{i}|}\}\) of size \(|_{t}|\). When a task \(_{t}\) arrives, the model \(f(|)\) would be trained for the current task, while the data from previous tasks is unreachable.

**Forward Propagation of Visual Prompt Tuning in ViT Layers:** We describe the forward propagation process of the ViT layer for visual prompt tuning, as illustrated in Figure 1. Let \(^{N D}\) and \(^{M D}\) denote the \(N\) input image tokens of a sample (including the pre-trained class token if available) and \(M\) prompts, respectively, where \(D\) is the dimension of each token. In the ViT layer, only the prompts \(\) are trainable parameters. The remaining parameters in LayerNorm, qkv-transformations and subsequent MLP introduced below are pre-trained and kept frozen. We use \(=[;]^{(N+M) D}\) to denote the concatenated input tokens. First, they undergo the LayerNorm  operation \(()\):

\[()=-}}}{ }}}+,\] (1)

where \(}},}}^{N+M}\), \(,^{D}\). The \(\) and division here denote the element-wise (Hadamard) product and division, respectively. Note that the vectors \(}}\), \(}}\), \(\) and \(\) are broadcasted to match the matrices of dimensions \((N+M) D\), enabling them to carry out operations with \(\). Then the normalized tokens are fed into the qkv-transformations:

\[}=()_{q}+_{q},\; }=()_{k}+_{k},\; }=()_{v}+_{v},\] (2)

where \(_{\{q,k,v\}}^{D D}\). The vector \(_{\{q,k,v\}}^{D}\) is broadcasted to a matrix of dimensions \((N+M) D\) to facilitate the addition operation. Next is the self-attention:

\[}=f_{}()=(}_{}^{}}{})},\] (3)

where \(}\) denotes the image tokens serving as queries. Eq. (3) can be expanded as Affinity, softmax (on rows) and Aggregation operations:

\[}=f_{}(},})=}_{}^{}}{}=}[ _{}^{}_{}^{}]}{ {D}}^{N(N+M)},\] (4) \[}=(})=( }^{N N}} ^{N M})=}& },\] (5) \[}=f_{}(},})= }}=[}}] }\\ }^{N D}.\] (6)

It is worth noting that the rows of the attention map where the prompts serve as queries (_i.e._, \(}\)) do not need to be computed, as formulated in Eq. (4) and illustrated in Figure 1. The reason is that in VPT-Deep , the output prompts of this ViT layer will be replaced with new trainable prompts in the subsequent layer. Omitting \(}\) has no impact on the output image tokens of the ViT layer, asthe subsequent Aggregation, LayerNorm and MLP operations are performed independently for each token. If no new prompts are added in the next layer, the output prompts can be just discarded as well.

After the self-attention, operations consist of another LayerNorm and the MLP layer are applied individually to each token, without any interaction among the tokens. Finally, the output fine-tuned image tokens are fed into the next ViT layer.

**Orthogonal Projection in Convolutional Layers:** A convolutional operation is actually a linear operation. For a convolutional layer \(f_{}(|_{t})\) in task \(_{t}\), we use \(_{t}^{D_{} D_{}}\) to denote its unrolled convolutional kernel matrix . Here, \(D_{}\) represents the number of pixels within a kernel, and \(D_{}\) corresponds to the number of kernels. Each convolutional patch from the input feature map is flattened into a row vector with a dimension of \(D_{}\). These row vectors of totaling \(n_{p}\) patches compose the input feature matrix \(_{t}^{n_{p} D_{}}\). The output feature for \(_{t}\) in task \(_{t}\) is expected to remain unchanged (referred to as consistent) in the next task \(_{t+1}\) to prevent forgetting:

\[f_{}(_{t}|_{t})=f_{}(_ {t}|_{t+1}).\] (7)

By substituting \(_{t+1}=_{t}+\), with \(\) denoting the weight update in \(_{t+1}\), the consistency condition for the convolutional layer is established as follows:

\[_{t}_{t}=_{t}(_{t}+),\] (8)

which can be further simplified as:

\[_{t}=.\] (9)

Eq. (9) suggests that if the weight update \(\) is orthogonal to the previous input feature \(_{t}\) during training in the new task, the corresponding output feature will remain unchanged. Thereby, the interference of the new task on the old task is eliminated. This can be realized by projecting the candidate weight update \(_{}\) into the orthogonal subspace of \(_{t}\): \(=_{}\), where \(^{D_{} D_{}}\) is an orthogonal projection matrix .

Similarly, for the prompt tuning which fine-tunes the prompts \(_{t}\) in a ViT layer \(f_{}(_{t}|_{t})\), we also aim to satisfy the following consistency objective for the purpose of anti-forgetting:

\[f_{}(_{t}|_{t})=f_{}(_{ t}|_{t+1}).\] (10)

However, the consistency condition in Eq. (9) does not hold for Eq. (10), since \(f_{}(_{t}|_{t})_{t}_{t}\) in prompt tuning. Instead, all the tokens \(_{t}\) and \(_{t}\) first undergo a LayerNorm and then interact via the self-attention mechanism, as previously described. The complicated forward propagation within the ViT layer brings huge challenge to analyzing the consistency conditions in relation to the prompt update \(\). In the next section, we will tackle this challenge and derive the consistency conditions for visual prompt tuning.

## 4 Method

We use \(_{t}=[_{t};_{t}]\) and \(_{t+1}=[_{t};_{t+1}]\) to denote the input tokens before and after updating the prompts, respectively, where \(_{t+1}=_{t}+,\). Our goal is to analyze how to satisfy Eq. (10) and derive one or more conditions expressed in terms of the prompt update \(\). These conditions will subsequently guide the application of orthogonal projection to \(\).

Figure 1: Illustration of the forward propagation in a ViT layer. Residual connections are omitted. The red crosses indicate the rows of attention map or the output prompts can be neglected.

### Analysis of Consistency Conditions

As can be seen in Figure 1, those outputs of LayerNorm and qkv-transformations corresponding to the image tokens remain unaffected by the updates to the prompts. Hence, the essence of attaining the consistency objective Eq. (10) can be turned into analyzing how to keep the output of self-attention in Eq. (3) unchanged as the prompts are updated, _i.e._, satisfying:

\[_{_{t}}=_{_{t+1}}.\] (11)

However, the nonlinear operation (_i.e._, \(\)) and the potential higher-order term \(_{k}^{}^{}_{v}\) arising from \(_{}^{}_{}\) in Eq. (3) complicate the direct resolution of this objective. Specifically, the non-injection property of the \(\) function causes non-unique solutions. The multiplication between \(_{_{t+1}^{}}_{_{t+1}}\) derives a quadratic term \((_{t}+)^{}(_{ t}+)\), which result in difficult optimization for \(\).

To address this issue, we propose two sufficient conditions consisting solely of linear operations. Specifically, we split the process of self-attention into two primary stages, _i.e._, the Affinity described by Eq. (4) and the Aggregation outlined in Eq. (6). We can achieve Eq. (11) by ensuring the consistency of each stage:

\[f_{}(_{_{t}},_{_{t}})=f_{}(_{_{t}},_{_{t+1}}), \\ f_{}(_{_{t}},_{_{t}})=f_{ }(_{_{t+1}},_{_{t+1}}). \] (12)

We first analyze the consistency objective of Affinity, _i.e._, Eq. (12), for \(_{t}\) and \(_{t+1}\):

\[f_{}(_{_{t}},_{_{t}})=_{_{t}}[_{_{t}}^{} _{_{t}}^{}]=[_{_{ t}}_{_{t}}^{}_{_{t}}[ (_{t})_{k}+_{k}]^{}],\\ \\ f_{}(_{_{t}},_{_{t+1}})= [_{_{t}}_{_{t}}^{} _{_{t}}[(_{t+1})_{k}+ _{k}]^{}],\] (13)

where \(\) is omitted for simplicity. Upon fulfilling Eq. (12), we can obtain \(_{_{t}}=_{_{t+1}}\), corresponding to the output of Eq. (5). Subsequently, we analyze the consistency objective of Aggregation in Eq. (13), yielding results for \(_{t}\) and \(_{t+1}\) as:

\[f_{}(_{_{t}},_{_{t}})=_{_{t}}_{_{t}}+_{ _{t}}_{_{t}}=_{_{t}}_{_{t}}+_{_{t}}[( _{t})_{v}+_{v}],\\ f_{}(_{_{t+1}},_{_{t+1}})=f_ {}(_{_{t}},_{_{t+1}})= _{_{t}}_{_{t}}+_{_{ t}}[(_{t+1})_{v}+_{v}].\] (14)

Based on Eq. (12\(-\)17), we are able to derive the following two equations, respectively:

\[_{_{t}}_{k}^{}( _{t})^{}=_{_{t}}_{k}^{} (_{t+1})^{}=_{_{t}}_{k }^{}(_{t}+)^{},\\ _{_{t}}(_{t})_{v}= _{_{t}}(_{t+1})_{v}=_{ _{t}}(_{t}+)_{v}. \] (15)

Note that we expect to further deduce Eq. (15) and Eq. (15) to obtain equations among \((_{t})\), \((_{t}+)\) and \(\). However, due to the square root and quadratic terms in the expressions of the standard deviations \(_{_{t}}\) and \(_{_{t}+}\), it is difficult to express \(_{_{t}+}\) in terms of \(_{_{t}}\) and \(_{}\). Consequently, it is challenging to derive a straightforward equation that relates \((_{t})\) and \((_{t}+)\) through \(\).

To simplify the problem, we introduce an additional constraint on the distribution of prompts. Concretely, we require that the updated prompts \(_{t}+\) retain the same distribution as \(_{t}\), _i.e._, meeting the following assumption:

\[_{_{t}+}=_{_{t}}, \\ _{_{t}+}=_{_{t}}. \] (16)

In this way, we can establish a straightforward mathematical relationship connecting \((_{t}+)\), \((_{t})\) and \(\):

\[(_{t}+)=_{t}+-_{_{t}+}}{ _{_{t}+}}+= _{t}-_{_{t}}+}{_{ _{t}}}+=(_{t})+}{_{_{t}}}.\] (17)Consequently, we can apply Eq. (21) to simplify Eq. (18) and (19) as:

\[_{_{t}}_{k}^{}(_{t})^{}=_{_{t}}_{k}^{}( _{t})^{}+_{_{t}}_{k}^{} ^{}/_{_{t}}^{}^{ },\] (22)

\[_{_{t}}(_{t})_ {v}=_{_{t}}(_{t})_{v}+ _{_{t}}_{v}/_{_{t}}.\] (23)

It should be noted that in Eq. 22 and Eq. 23, \(_{k}\), \(_{v}\) and \(\) are pre-trained parameters kept frozen throughout the continual learning process. \(_{_{t}}\) and \(_{_{t}}\) are two matrices derived from the input \(_{t}\). As our objective is to ensure that the above two equations remain valid for the variables \(_{_{t}}\) and \(_{_{t}}\), it is sufficient to meet the following conditions, in which \(_{v}\) can be ignored whereas \(_{k}\) remains crucial:

\[_{_{t}}_{k}^{} ^{}=\\ _{_{t}}=\] (24)

Now we have obtained the simplified formulas expressed by \(\) in Eq. (24) and (25).

To sum up, we convert the overall consistency equation Eq. (11) into two sufficient conditions Eq. (12) and (13) for Affinity and Aggregation, respectively. Consequently, we derive two corresponding consistency conditions Eq. (24) and (25) expressed by the prompt update \(\), under the constraint of invariant prompt distribution formulated in Eq. (20). The deduced conditions can satisfy the consistency objective in Eq. (10), thereby achieving the goal of eliminating the interference of the new task on the old task for visual prompt tuning.

As \(_{_{t}}=(_{t})_{q}+_ {q}\), Eq. (24) implies that if the (transposed) prompt update can be orthogonal to the normalized previous input image tokens \(_{t}\) projected with a second-order transformation matrices \(_{q}_{k}^{}\) of the pre-trained ViT, the consistency for Affinity can be guaranteed. When we ignore the normalization and the bias term in \(_{_{t}}\), Eq. (24) can be simplified as \(_{t}_{q}_{k}^{}^{}= \). The simplified condition is still essentially different from the consistency condition of linear layers (_i.e._, Eq. (9)) and that deduced in  (_i.e._, \(_{t}^{}=\)). It indicates the interaction between the image tokens and prompts within ViT layers is fundamentally distinct, leading to a unique consistency condition related to the second-order transformation matrices \(_{q}_{k}^{}\) of the pre-trained model. Moreover, Eq. (25) is also an essential condition served as one of the sufficient conditions for the consistency of the whole ViT layer. It implies that if the prompt update can be orthogonal to the activated attention map generated by the image queries (\(_{}\)) and prompt keys (\(_{}\)), the consistency of Aggregation can be achieved.

### Optimization of Consistency Conditions

To jointly optimize Eq. (24) and (25), we need to solve \(\) that can meet both equations concurrently. Here, we employ a separate optimization approach to get an approximate solution efficiently. Initially, it ensures \(^{}\) is orthogonal to the subspace spanned by \(_{_{t}}_{k}^{}\) to satisfy Eq. (24). Subsequently, it makes \(\) orthogonal to the subspace spanned by \(_{_{t}}\) to satisfy Eq. (25).

Specifically, we use \(_{}\) to denote the candidate parameter update generated by the optimizer for the prompts. We aim to obtain a projection matrix \(\) such that \(=_{}\). Following the previously mentioned separate optimization strategy, we first ensure \(^{}\) is orthogonal to \(_{_{t}}_{k}^{}\) by the projection matrix \(_{1}\): \(^{}=_{1}_{}^{}\). Then \(\) is made orthogonal to \(_{_{t}}\) by another projection matrix \(_{2}\): \(=_{2}_{}\). Therefore, the objective of the optimization turns into obtaining the two projection matrices \(_{1}\) and \(_{2}\) to satisfy Eq. (24) and (25). Inspired by the null-space projection method , the bases of \(_{1}\) and \(_{2}\) correspond to the null-space bases of \(_{_{t}}_{k}^{}\) and \(_{_{t}}\), respectively. We use \(_{1,0}^{D R_{1}}\) and \(_{2,0}^{M R_{2}}\) to denote the bases of the null spaces for \(_{_{t}}_{k}^{}\) and \(_{_{t}}\), where \(R_{1}\) and \(R_{2}\) indicate their nullities. \(_{1,0}\) and \(_{2,0}\) can be obtained from the right singular vectors associated with the zero singular values, through the process of singular value decomposition (SVD) applied by \(((_{_{t}}_{k}^{})^{}_ {_{t}}_{k}^{})\) and \((_{_{t}}^{}_{_{t}})\), respectively. In this way, we get the projection matrices \(_{1}=_{1,0}_{1,0}^{}^{D D}\) and \(_{2}=_{2,0}_{2,0}^{}^{M M}\), which are the solutions enabling \(\) to jointly satisfy Eq. (24) and (25):

\[=_{2}_{}_{1}=(_{2,0}_{2,0}^{})_{}(_{1,0}_{1,0}^{ }).\] (26)

For the constraint Eq. (20), we incorporate an additional loss function aimed at penalizing the drift of prompt distribution, hence realizing a relaxed version of this constraint:

\[_{}=\|_{_{t+1}}-_{_{t} }\|_{1}+\|_{_{t+1}}-_{_{t}}\|_{1}.\] (27)In Eq. (27), \(_{_{t}}\) and \(_{_{t}}\) represent the target prompt distribution obtained in task \(_{t}\), while \(_{_{t+1}}\) and \(_{_{t+1}}\) denote the distribution to be optimized in task \(_{t+1}\).

To sum up, we use Eq. (26) to realize Eq. (24) and (25), and use Eq. (27) to meet Eq. (20), thereby achieving the consistency objective Eq. (10) for anti-forgetting. We provide a full algorithm of our approach in the appendix A.

### Extension to Multi-Heads

We further extend the consistency conditions Eq. (24) and (25) to multi-head self-attention, a common feature in current transformer-based models. Suppose there are \(H\) heads and \(d=D/H\) represents the dimension of each token in a head. We use \(_{_{t},h}^{N d}\), \(_{k,h}^{D d}\) and \(_{_{t},h}^{N M}\) to denote the corresponding matrices in Eq. (24) and (25) for the \(h\)-th head, respectively. The objective is to ensure these conditions are met across all heads, _i.e._, \(_{_{t},h}_{k,h}^{}^{}= \) and \(_{_{t},h}=\), \( h\{1,2,,H\}\). Let \(_{1,t}=[_{_{t,1}}_{k,1}^{}; ;_{_{t,H}}_{k,H}^{}]^{HN  D}\) and \(_{2,t}=[_{_{t},1};;_{ _{t},H}]^{HN M}\) represent the concatenated matrices from all the heads, respectively. Based on block matrix properties, those two sets of conditions can be formulated as \(_{1,t}^{}=\) and \(_{2,t}=\). To sum up, The main difference between single-head and multi-heads is that the parameter update should be orthogonal to the subspace spanned by the concatenation matrices from all heads for multi-heads self-attention. Therefore, for the multi-heads variant, only an additional step of concatenation of the matrices from all heads is required in our algorithm.

## 5 Experiments

### Experimental Setups

In our experiments, we mainly utilize the VPT  with a ViT-B/16 backbone  pre-trained on ImageNet-21k. Additionally, we validate the effectiveness on the CLIP  model, wherein the visual prompts are inserted into the image encoder. Our experiments are conducted across 4 class-incremental benchmarks: 10- and 20-split CIFAR-100, 10-split ImageNet-R and 10-split DomainNet. We report the mean values of the final average accuracy and final average forgetting over 3 runs with different random seeds. Given that the null spaces of \(_{_{t}}_{k}^{}\) and \(_{_{t}}\) may not always exist in practice, we compute the approximate null spaces and determine the nullities \(R_{1}\) and \(R_{2}\) in an adaptive manner, rather than the way suggested in . For more detailed information regarding the experimental setups, please refer to Appendix B.

### Main Results

**Validation of Effectiveness:** The comparison between our approach and the sequential fine-tuning VPT and CLIP baselines is shown in Table 1. For the VPT model, the proposed NSP\({}^{2}\) achieves 4.47%\(\)10.26% improvements in accuracy on the 4 benchmarks. Meanwhile, it reduces the forgetting by 9.05%\(\)17.11%. As to the CLIP model, the NSP\({}^{2}\) improves the accuracy by 6.48%\(\)9.31%, and reduces the forgetting by 2.68%\(\)17.27%. We calculate the accuracy across all previously encountered tasks after completing training on each task. The accuracy curves of VPT-Seq and VPT

    &  &  &  &  \\   & Acc. \(\) & Forgetting \(\) & Acc. \(\) & Forgetting \(\) & Acc. \(\) & Forgetting \(\) & Acc. \(\) & Forgetting \(\) \\  VPT-Seq & 87.27 & 12.33 & 82.36 & 17.36 & 72.46 & 19.41 & 73.28 & 25.65 \\ VPT-NSP\({}^{2}\) & **91.74** & **3.28** & **89.89** & **4.91** & **78.88** & **5.06** & **83.54** & **8.54** \\ Upper-bound & 93.87 & - & 93.87 & - & 84.60 & - & 89.25 & - \\  CLIP-Seq & 72.91 & 15.13 & 71.37 & 17.89 & 75.69 & 19.21 & 67.73 & 35.60 \\ CLIP-NSP\({}^{2}\) & **80.96** & **12.45** & **79.83** & **13.77** & **82.17** & **6.42** & **77.04** & **18.33** \\ Upper-bound & 84.52 & - & 84.52 & - & 84.86 & - & 81.65 & - \\   

Table 1: Comparison with the baselines (\({}^{*}\)-Seq\({}^{}\)) on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset.

NSP\({}^{2}\) on 10-split CIFAR-100 and 10-split ImageNet-R are displayed in Figure 2. They demonstrate our approach consistently outperforms the baseline throughout the sequential learning of tasks.

We conduct additional experiments with the VPT model, utilizing the weights pre-trained on different datasets as well as different paradigms, as shown in Figure 3. The pre-training paradigms and datasets include: naive classification on ImageNet-1k , DINO  on ImageNet-1k, MIIL  on ImageNet21k-P and CLIP on LAION-2B  (we only use its image encoder). As can be seen from the figure, our approach not only significantly enhances accuracy but also markedly mitigates forgetting. These results further demonstrate the generalizability of the proposed approach.

**Comparison with Existing Methods:** We compare our method with existing methods in Table 2, where the competitors include many recent works. The proposed VPT-NSP\({}^{2}\) achieves state-of-the-art performance on the four benchmarks, with surpassing the second best approach by an average of 1.49% in accuracy. The forgetting of our approach is not the lowest, which is reasonable since our approach sacrifices some stability for a better trade-off between stability and plasticity. The outperforming accuracy can demonstrate the superiority of our method.

**Ablation Study:** The two consistency conditions Eq. (24) and (25), along with the constraint Eq. (20), constitute the main components of our approach. They correspond to \(_{1}\), \(_{2}\) in Eq. (26), and \(_{}\) in Eq. (27). We study their effects on the four benchmarks using VPT-NSP\({}^{2}\), with results presented in Table 3. We can see that the projection for Affinity (\(_{1}\)) plays a crucial role, which brings 3.31%\(\)9.03% improvement in accuracy and 5.42%\(\)14.76% decline in forgetting. Furthermore, the projection for Aggregation (\(_{2}\)) and the loss \(_{}\) for invariant prompt distribution are indispensable as well for minimizing forgetting. Optimal accuracy is achieved when all three conditions are applied.

**Model Analysis:** We analyze the evolution of training losses on the 10-split CIFAR-100 and 10-split ImageNet-R benchmarks, as shown in Figure 4. Each point on the curve represents the training loss of the data in \(_{1}\)/\(_{2}\) after the model has been trained on subsequent tasks. As can be seen, the losses of VPT-NSP\({}^{2}\) on previous tasks can be almost retained, confirming that our approach can effectively mitigate the interference of new tasks on old tasks.

**Trade-off between Stability and Plasticity:** We first adaptively determine the nullities \(R_{1}\) and \(R_{2}\) for \(_{1}\) and \(_{2}\) to achieve near-minimum forgetting. Based on this, we assign two weights \(_{1}\) and \(_{2}\) to the projection matrices to control the trade-off between stability and plasticity:

Figure 3: Results of utilizing different pre-training datasets and paradigms. The blue and yellow bars represent accuracy and forgetting, respectively. The upward arrows indicate the accuracy increasing from VPT-Seq to VPT-NSP\({}^{2}\), whereas the downward arrows denote the reduction in forgetting.

Figure 2: Task-by-task accuracy changing curves of VPT-Seq and VPT-NSP\({}^{2}\) on two benchmarks.

\([_{2}_{2}+(1-_{2})]\,_{}\,[_{1 }_{1}+(1-_{1})]\), where \(\) denotes the identity matrix. The effects of \(_{1}\) and \(_{2}\) which are set to a same value \(\) is shown in Figure 5. As the weight decreases, the accuracy increases first owing to better plasticity, and then decreases due to worse stability caused by the forgetting. It implies that a trade-off can be achieved by the two weights of projections.

**Long-sequence Continual Learning** We experiment on 5 benchmarks under the protocols of 50 tasks and 100 tasks to validate that our approach remains effective even within the context of long-sequence continual learning. The results are presented in Table 4. Despite lacking plasticity enhancement, VPT-NSP\({}^{2}\) can outperform existing state-of-the-art approaches and especially surpasses L2P by a large margin. This demonstrates that forgetting is still the predominant factor affecting performance in long sequence of tasks. With the plasticity enhancement, VPT-NSP\({}^{2}\) achieves significant increase in accuracy (by 1.1%\(\)2.9%). This demonstrates that our plasticity enhancement is effective in learning new knowledge in long-sequence continual learning.

    &  &  &  &  &  \\   & & Acc. & Forgetting & Acc. & Forgetting & Acc. & Forgetting \\  L2P  & CVPR’22 & 83.83\({}_{ 0.04}\) & 7.63\({}_{ 0.30}\) & 80.10\({}_{ 0.72}\)\({}^{}\) & - & 61.57\({}_{ 0.66}\) & 9.73\({}_{ 0.47}\) & 81.17\({}_{ 0.85}\)\({}^{}\) & 8.98\({}_{ 1.25}\) \\ DualPrompt  & ECCV’22 & 86.51\({}_{ 0.33}\) & 5.16\({}_{ 0.09}\) & 82.02\({}_{ 0.32}\)\({}^{}\) & - & 68.13\({}_{ 0.49}\) & 4.68\({}_{ 0.20}\) & 81.70\({}_{ 0.78}\)\({}^{}\) & 8.04\({}_{ 0.31}\) \\ CODA-P  & CVPR’23 & 86.25\({}_{ 0.74}\) & 1.67\({}_{ 0.26}\) & - & - & 75.45\({}_{ 0.56}\) & 1.64\({}_{ 0.10}\) & 80.04\({}_{ 0.79}\)\({}^{}\) & 10.16\({}_{ 0.35}\) \\ ESN  & AAAI’23 & 86.34\({}_{ 0.52}\) & 4.76\({}_{ 0.14}\) & 80.56\({}_{ 0.94}\)\({}^{}\) & - & 62.61\({}_{ 0.96}\)\({}^{}\) & - & 79.22\({}_{ 2.04}\)\({}^{}\) & 10.62\({}_{ 12}\) \\ APG  & ICCV’23 & 89.95\({}_{ 0.61}\) & 88.64 & 6.51 & 73.27 & 8.59 & - & - \\ LAE  & ICCV’23 & 85.94\({}_{ 0.46}\) & - & 83.93\({}_{ 0.28}\) & - & 72.66\({}_{ 0.63}\) & - & - & - \\ Dual-LGCL  & ICCV’23 & 87.23\({}_{ 0.21}\) & 5.10\({}_{ 0.15}\) & - & - & 69.46\({}_{ 0.04}\) & 4.20\({}_{ 0.06}\) & - & - \\ C-LN  & ICCV’2386.95\({}_{ 0.37}\) & 6.98\({}_{ 0.43}\) & - & - & 76.36\({}_{ 0.51}\) & 8.31\({}_{ 1.28}\) & - & - \\ EvPOPrompt  & AAAI’24 & 87.97\({}_{ 0.30}\) & 2.60\({}_{ 0.42}\) & 84.64\({}_{ 0.14}\) & 3.98\({}_{ 0.24}\) & 76.83\({}_{ 0.08}\) & 2.78\({}_{ 0.06}\) & 79.50\({}_{ 0.29}\) & 3.81\({}_{ 0.36}\) \\ OVOR-Deep  & LCL’24 & 85.95\({}_{ 0.09}\) & 8.64\({}_{ 0.22}\) & 84.13\({}_{ 0.75}\) & 6.81\({}_{ 0.77}\) & 76.11\({}_{ 0.76}\) & 79.61\({}_{ 0.86}\) & 4.77\({}_{ 0.94}\) \\ DualP-GPG  & ICLR’24 & 86.92\({}_{ 0.05}\) & 5.35\({}_{ 0.19}\) & 83.74\({}_{ 0.01}\) & 79.1\({}_{ 0.15}\) & 69.34\({}_{ 0.04}\) & 4.53\({}_{ 0.04}\) & 80.41\({}_{ 0.25}\) & 8.39\({}_{ 0.18}\) \\ InfLoRA  & CVPR’24 & 87.06\({}_{ 0.25}\) & 6.22\({}_{ 0.39}\) & 81.42\({}_{ 0.54}\) & 6.42\({}_{ 0.33}\) & 75.65\({}_{ 0.14}\) & 5.73\({}_{ 0.44}\) & 81.45\({}_{ 0.68}\) & 5.35\({}_{ 0.52}\) \\ EASE  & CVPR’24 & 87.76\({}_{ 0.5}\) & 5.94 & 85.80 & 7.19 & 76.17 & 7.82 & 78.89 & 7.89 \\ CPPrompt  & CVPR’24 & 87.82\({}_{ 0.21}\) & 5.06\({}_{ 0.50}\) & 83.97\({}_{ 0.31}\) & 6.85\({}_{ 0.43}\) & 77.14\({}_{ 0.11}\) & 5.97\({}_{ 0.68}\) & 82.97\({}_{ 0.34}\) & 7.45\({}_{ 0.93}\) \\  VPT-NSP\({}^{2}\) & This work & **91.74\({}_{ 0.63}\)** & 3.28\({}_{ 0.45}\) & **89.89\({}_{ 0.72}\)** & 4.91\({}_{ 0.59}\) & **78.88\({}_{ 0.50}\)** & 5.06\({}_{ 0.26}\) & **83.54\({}_{ 0.77}\)** & 8.54\({}_{ 0.48}\) \\   

Table 2: Comparison with existing methods that use the pre-trained ViT-B/16 on ImageNet-21k. The standard deviations are also reported if available. Missing results in the corresponding papers are denoted as ”-”. The results marked with \(\) and \(\) are implemented by  and , respectively. The highest accuracies are in bold, and the second highest accuracies are underlined.

Figure 4: Training loss curves of VPT-NSP\({}^{2}\) and VPT-Seq on tasks \(_{1}\) and \(_{2}\) when the models are trained on sequential tasks.

## 6 Conclusion

In this paper, we study the interference problem of visual prompt tuning in ViTs, and propose two consistency conditions which can eliminate the interference in theory under the constraint of invariant prompt distribution. They guarantee the consistency of Affinity, Aggregation and distribution of prompts in LayerNorm, respectively, which jointly achieve the consistency objective of the whole ViT layer. We adopt the null-space projection to implement the two conditions and utilize an extra loss to satisfy the constraint. Our experiments on various benchmarks demonstrate the effectiveness of the proposed conditions for anti-forgetting, and our approach achieves state-of-the-art performances.

**Limitation Discussion:** To simplify the derivation of our consistency conditions, we introduce a constraint of invariant prompt distribution. Although the superior results show that it may not be a very strong assumption, it is not an exact solution.