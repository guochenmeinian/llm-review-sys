# On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution

Yubo Ye

Zhejiang University

22230131@zju.edu.cn

&Maryam Toloubidokhti

Rochester Institute of Technology

mt6129@rit.edu

&Sumeet Vadhavkar

Rochester Institute of Technology

sv6234@rit.edu

&Xiajun Jiang

Rochester Institute of Technology

xj70560rit.edu

&Huafeng Liu

Zhejiang University

liuhf@zju.edu.cn

Work was done as a visiting exchange student at Rochester Institute of Technology.

###### Abstract

The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of _hybrid deep generative models (hybrid-DGMs)_ that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.

## 1 Introduction

There has been increasing interest in integrating mathematical expressions of known physics with neural-network functions for hybrid, or gray-box, modeling . The incorporation of physics-based inductive bias has the potential to improve the generalizability and interpretability of deep learning , while the expressive and flexible neural functions offer the opportunity to fill the gap in our prior knowledge of physics (or _expert models_) . Recent progress in particular has started to see principled developments of hybrid deep generative models (DGM) where the data-generation process is described by a hybridization of physics-based and neural components. Both of these two components are then identified from data via, for instance, amortized variational inference . These hybrid-DGMs have shown benefits of interpretability, generalizability, and out-of-distribution (OoD) robustness, especially when learning to model physics systems from observational data . An important premise of these successes is that the hybrid expression of the physical systems can be accurately identified from the observed data: for example, if we can identify the parametersand/or physics governing observed pendulum movements, we could then use the identified generative models/parameters for predicting the future trajectory of the pendulum .

However, substantial research based on the theory of nonlinear independent component analysis (nonlinear ICA) have established that a naive DGM or latent variable model is un-identifiable . Active research has also reported various ways to construct an identifiable latent variable model, primarily via constructing a conditionally independent generative model [7; 8; 9; 6]. Most of the progress has been established for the identifiability of latent processes underlying time-series data, leveraging the temporal dependency of the latent variables  or observed domain index for non-stationary segments  to construct the conditional generative model. For static latent variables, the construction of conditionally independent generative model has mainly relied on the introduction of auxiliary observations such as class or domain labels .

How does the hybrid expression of a DGM impact its (un)-identifiability? Furthermore, since hybrid-DGMs are often intended to model physics systems with unknown parameters to be identified from data, what may be an effective approach to construct a hybrid-DGM with theoretically-proven identifiability? Unfortunately, despite the increasing interest in hybrid-DGMs, their identifiability remains unexplored. This identifiability however is critical for the intended use of a hybrid DGM, _e.g.,_ to predict beyond the observations used to identify the model, or to be robust in out-of-distribution (OOD) generalizations.

Recent efforts in hybrid-DGMs indeed have increasingly noted the challenges in properly learning the two components within the hybrid models, especially in ensuring a non-trivial solution to the physics-based component due to the presence of a highly-expressive neural component [1; 2]. Various strategies have been presented, such as regularizing the expressiveness of the neural component , trajectory-based and adaptive optimization to allow automatic complexity adjustment for the neural component , and expert-augmentation leveraging the physics-based component to improve generalization to distributional shifts . No existing works, however, have probed or established the identifiability of hybrid-DGMs, theoretically or empirically.

In this work, we present the first theoretical probe of the identifiability of hybrid-DGMs. Importantly, forgoing the need of _observed_ auxiliary variable to condition the generative model, we present meta-learning as a novel solution to establish the identifiability of hybrid-DGMs. To this end, we present a learn-to-identify formulation for meta-learning hybrid-DGMs, and theoretically establish the identifiability of these meta-hybrid-DGMs via conditional independence of the DGM given few-shot context samples. We further provide empirical identifiability results of these meta-hybrid-DGMs in a variety of synthetic and real datasets generated from physics systems, in contrast to existing non-meta hybrid-DGMs where identifiability cannot be established without observed auxiliary variables. Finally, we note that while this work focuses on hybrid-DGMs, the presented meta-learning formulation provides a general new condition to construct identifiable DGMs beyond the hybrid setting.

## 2 Related Works

Hybrid-DGMs:A number of hybrid models have emerged in various domains to combine physics-based functions with neural networks, to compensate for unknown components in the known physics. While earlier approaches [10; 11; 12] focused on using neural networks to learn the residual between the measurements and those obtained from physics-based simulation, tighter integrations have been presented by, _e.g.,_ NeuralSim  and universal differential equations (UDEs) , to include neural networks as different components within a physics-based function. Most of these earlier works are focused on hybrid modeling in specific application domains.

More recently, several hybrid-DGMs have been presented [2; 1; 4; 3], with a focus on properly learning the physics component within the hybrid-DGM so it is not overpowered by the expressive neural component. In physics-integrated hybrid VAE , this was achieved by a regularized learning method that controls the expressiveness of the neural component while preserving the semantics of the physics-based component. Similarly, regularization of the expressiveness of the neural component is considered in . In , expert augmentation is proposed to improve the OOD robustness of these hybrid-DGMs by fine-tuning on synthetic OOD samples, generated by sampling outside the training distribution of the physics-based component within the trained hybrid model.

Despite these efforts in better learning the two separate components within hybrid-DGMs, the identifiability of hybrid-DGMs has not been theoretically probed nor established. Empirically, no existing works have evaluated these hybrid-DGMs from the lens of identifiability metrics. Theoretically, no existing works have examined the conditions for constructing identifiable hybrid-DGMs. This work will take the first step to bridge these gaps, establish the un-identifiabilty of naive unconditional hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs.

Nonlinear ICA and Identifiability of DGMs:With nonlinear ICA, strong identifiability results have been established for unsupervised latent variable models [7; 8; 9; 6; 15; 16; 17; 18]. Fundamentally, construction of identifiable generative models is achieved by defining independence structure when conditioned on observed auxiliary variables . Much progress has been made in identifying latent dynamic processes generating observed time series, including leveraging conditional independence of the DGM given time segment index temporal dependence , sparse temporal encoding , and even unobserved inferred domain labels for non-stationary time series [16; 17; 18]. For static latent variables, identifiability is mainly established via the introduction of auxiliary observations such as class or domain labels . In specific, iVAE  established the identifiability of DGMs assuming an exponential families conditional distributions given observed auxiliary variables.

The reliance on observed conditioning variable, however, limits the applicability of these identifiability results to hybrid-DGMs: as hybrid-DGMs are intended for modeling physics system with unknown and often continuous parameters, it is not clear what _observed_ auxiliary label may be available to construct an identifiable DGM. This work will forego this assumption of observed auxiliary variable and present meta-learning as a novel condition to construct identifiable DGMs. While presented for hybrid-DGMs, this connection between meta-learning and the identifiability of DGMs is a novel contribution to general non-hybrid DGMs that has not been constructed in the existing literature.

## 3 Problem Formulation: Unidentifiablity of Hybrid-DGMs

Let \(^{n}\) be the observed random variables and \(^{d}\) be the latent variables generating \(\), with the following generation process:

\[p(,)=p()p(|), p( )=_{}p(,)d\] (1)

where \(p()=_{i=1}^{d}p(z_{i})\) represents the prior distribution of independent generative factors \(z_{i}\), \(p(|)\) is the likelihood function which can be defined based on the mixing function \(=()\).

Construction of Hybrid-DGMs:When constructing a generative model for Equation (1), if we assume a known mathematical expression explaining the mechanism of the mixing function as \(f_{}\), we will have a physics-based expression of this generation process, with unknown parameters of \(f_{}\) as physics-based latent variables \(_{}\). If we have no prior knowledge, we can describe the mixing function with a neural network \(f_{}\) with abstract representation of the latent variables as \(_{}\) - the latter is the foundation of many successful DGMs including variational autoencoders (VAEs) .

When understanding complex systems in many domains, recent years have seen an increasing interest in the middle-ground of the above two scenarios: some prior knowledge in the form of a mathematical expression \(f_{}\) of physics exists about the data observed, yet often inexact with unknown gaps to the actual data-generating mechanisms. This motivated recent developments of hybrid-DGMs where the mixing function \(=()\) is described as a combination of a physics-based and neural component, \(f_{}\) and \(f_{}\), respectively, with corresponding physics-based and abstract latent variables \(_{}\) and \(_{}\).

As a concrete example, suppose that we observe the time-series of the angular position \(\) of a damped forced pendulum system, \(=[_{0},_{1},,_{T}]\) where \(_{i}=(i t)\), governed by:

\[(t)}{dt^{2}}+^{2}(t)+-A(2 t)=0\] (2)

where the first two terms describe the physics of an _ideal_ pendulum system, the third term the damping effect, and the last term the external force. Parameters \(,,A,\) and \(\) specify different systems governed by the same physics, representing the independent latent generative factors.

Suppose that we choose to leverage our prior knowledge about the _ideal_ pendulum physics \(f_{}(;_{})=+_{}^{2}\) with unknown parameter \(\). To bridge its potential gap with actual data-generating physics, we then choose to complete it with a neural-network expression of an ODE \(f_{_{}}(;_{})\), parameterized by \(\) and with an abstract latent variable \(_{}\): while abstract, we hope \(f_{_{}}\) to absorb the effect of the last two terms in the data-generating physics (Equation (2) with \(_{}\) representing its remaining data-generating parameters \(,\,A,\). This gives rise to a hybrid mixing function \(=[f_{},f_{_{}};_{ },_{}]=[f_{}(; _{})+f_{_{}}(;_{} )=0]\).

Assuming Gaussian observation noises, we obtain the likelihood of the Hybrid-DGMs as:

\[p_{}(|_{},_{})=(|[f_{},f_{_{}};_{},_{}],_{x})\] (3)

with prior distribution for the latent variables assumed to be Gaussian as:

\[p(_{}):=(_{}|_{}, _{}^{2}), p(_{}):=(_{}|0,)\] (4)

where the mean \(_{}\) and variance \(_{}^{2}\) for \(p(_{})\) can be defined by prior knowledge due to its physical meanings. Note that \(_{}\) will be directly interpretable and physically meaningful as they will be semantically grounded to the parameters of the physics model \(f_{}\); \(_{}\), in comparison, will be abstract but need to absorb effect of the varying parameters underlying the missing physics \(f_{}\).

The data-generating process of the hybrid-DGM with parameter \(\) can thus be defined as:

\[p_{}(,_{},_{})=p_{ }(|_{},_{})p(_{})p(_{}), p_{}()= p_{}( ,_{},_{})d_{} d_{}\] (5)

While described in the context of a concrete example, Equations (3)-(5) represent a general expression of hybrid-DGMs where the hybrid-mixing function \(=[f_{},f_{_{}};_{ };_{}]\) can be designed in various forms (and not limited to the hybrid-ODE described in the example above).

Unidentifiability of Hybrid-DGMs:Now assume we have access to data \(=\{^{(1)},,^{(N)}\}\) generated by \(p_{^{*}}(,_{},_{})\), where \(^{*}\) is the true but unknown data-generating parameter. For an identifiable DGM, our goal is to learn \(\) such that the likelihood \(p_{}^{*}(|_{},_{})\), the priors \(p(_{})\) and \(p(_{})\), and the posteriors \(p_{}^{*}(_{}|)\) and \(p_{}^{*}(_{}|)\) are all correctly recovered.

In practice, because we only observe \(\) without access to the latent variables \(_{}\) or \(_{}\), we optimize \(\) to match the marginal density of \(\), _e.g.,_ via amortized variational inference to maximize the evidence lower bound (ELBO) of \(p_{}()\) in VAEs, such that:

\[p_{}() p_{^{*}}()\] (6)

Unfortunately, even if the above optimization is done perfectly (_i.e.,_\(p_{}()=p_{^{*}}()\) ), there is no guarantee that \(p_{}^{*}(|_{},_{})\), \(p_{}^{*}(_{}|)\), or \(p_{^{*}}(,_{},_{})\) are correctly identified. As shown in , for these densities to be recovered based on matching \(p_{}()\), the DGM needs to satisfy:

\[(,):p_{}()=p_{}( )=\] (7)

In another word, the matching of \(p_{}^{*}()\) needs to imply a unique solution of \(^{*}\) to be obtained. In reality, however, it has been shown in  that DGMs with unconditional prior are unidentifiable. We refer the readers to the important theorems and the proof in  for more details, but to what extent is the hybrid-DGM affected by this theory of identifiability?

At a concept level, let us first consider the neural component (\(_{}\) and \(f_{_{}}\)) within the hybrid-DGM. For \(_{}\) of any distributions, there are always nonlinear transformations that change its values but not its distributions . In a highly expressive neural network, this transformation can be learned in the likelihood \(p_{}(|_{})\). This will give the same fit of \(p_{}()\), but non-unique solutions of \(p_{}(|_{})\) and \(p_{}(_{}|)\), resulting in non-identifiable neural component within the hybrid-DGM.

For the physics-based component (\(_{}\) and \(f_{_{}}\)), this theory does not hold because the pre-specified form of \(f_{}\) can not trivially accommodate any transformations applied to \(_{}\) without affecting the fitting of observations. In practice, however, because both components jointly contribute to \(p_{}(|_{},_{})\) as defined in Equation (3), the optimization of \(_{}\) and \(f_{_{}}\) maybe overpowered by the overly-expressive neural component, rendering trivial solutions to \(_{}\) and \(f_{_{}}\).

In other words, identification of hybrid-DGMs is inflicted by two fundamental challenges: 1) the theoretical un-identifiability of its neural component, and 2) this un-identifiable but highly expressive component overpowering the identification of the otherwise identifiable physics-based component. While existing works  have focused on the second challenge, we address the problem from its fundamentals at the un-identifiability of \(_{}\) and \(f_{_{}}\): we introduce a novel condition to construct identifiable hybrid-DGMs, and establish their identifiability both theoretically and empirically.

## 4 Learn-to-Identify Hybrid-DGMs via Meta-Learning

Based on nonlinear ICA theory , an identifiable DGM can be constructed by conditionally independent latent variables \(p(|)\): in the literature, the conditioning variable \(\) is often assumed to be additionally-observed side information such as class labels or domain indexes . Here, we show that meta-learning formulations offer an opportunity to construct an identifiable DGM, with \(\) taking forms of \(k\)-shot context samples sharing the same data-generation process as the query samples.

Construction of Identifiable Meta-Hybrid-DGMs:Given is a set of \(k\) context samples, \(^{s}\), sharing the same data-generation process as query samples \(\), we define the generation of \(\) as:

\[p_{}(,_{},_{}|^{ s})=p_{}(|_{},_{})p( _{})p_{}(_{}|^{s}), p _{}(|^{s})= p_{}(,_{ },_{}|^{s})d_{}d _{}\] (8)

where \(=\{,\}\), \(p_{}(|_{},_{})\) is defined in Equation (3), and the conditional prior \(p_{}(_{N}|^{s})\) is assumed to be factorized exponential family distributions :

\[p_{}(_{N}|^{s})=p_{,_{}}( _{}|^{s})=_{i=1}^{d}(z_{,i})}{Z_{i}(^{s})}[_{i}(z_{,i})^{} _{,i}(^{s})]\] (9)

where \(Q_{i}\) is the base measure, \(Z_{i}(^{s})\) is the normalizing constant, \(_{i}=(T_{i,1},,T_{i,e})\) are the sufficient statistics that are fixed (not estimated). \(_{i}^{}(^{s})=(_{i,1}^{}(^ {s}),,_{i,e}^{}(^{s}))\) are the corresponding parameters conditioning on \(^{s}\). In this paper, we realize this conditioning through a composite of a neural network \(h_{}(^{s})\), parameterized by \(\), for individual context samples \(^{s}^{s}\), and an averaging function across all context samples:

\[_{}(_{s})=^{s}|}_{ ^{s}^{s}}h_{}(^{s})\] (10)

Note that exponential families have universal approximation capabilities, thus this commonly-used assumption is not very restrictive .

Amortized Variational Inference:To enable inference over the hybrid-DGM in Equation (8), we approximate the posterior density \(p(_{}|)\) as \(q_{}(_{}|)\) and \(p_{}(_{}|,^{s})\) as \(q_{}(_{}|^{s})\), the latter realized with the same network as defined in (9) but with the additional input \(\) in addition to \(^{s}\).

Formally, we cast the variational inference into a meta-learning formulation. Consider a dataset \(\) with \(M\) similar but distinct data-generation process: \(=\{_{m}\}_{m=1}^{M}\). For each \(_{m}\), we consider disjoint context samples \(^{s}_{m}=\{^{s,1},^{s,2},,^{s,k}\}\) and query samples \(^{q}_{m}=\{^{q,1},^{q,2},,^{q,l}\}\), where \(k l\). Instead of maximizing the marginal likelihood of \(\) for all \(\), we formulate a meta-objective to learn to maximize the marginal likelihood \(p(^{q}|^{s}_{m})\) of all query samples \(^{q}^{q}_{m}\) when conditioned on support set \(^{s}_{m}\), for all underlying data-generation processes \(m\{1,2,,M\}\):

\[_{m=1}^{M}_{^{q}^{q}_{m}} p( ^{q}|^{s}_{m}) _{m=1}^{M}_{^{q}^{q}_{m}}\{_{q_{}(_{}|^{q}),q_{}(_{ }|^{q}^{s}_{m})}[ p_{}(^ {q}|_{},_{})]\] (11)

where the first term represents likelihood of the meta-hybrid-DGM on query sample \(^{q}^{q}_{m}\), and the last two terms represent Kullback-Leibler divergences between the prior and posterior densities of \(_{}\) and \(_{}\), respectively, where densities of \(_{}\) are conditioned on context samples \(^{s}_{m}\). The maximization of Equation (11) is performed over \(\{,,\}\) in an episodic training where, in each episode, the division of \(^{s}_{m}\) and \(^{q}_{m}\) is shuffled within each \(_{m}\). The likelihood is calcuated by the reparameterization trick , while the two KL-terms are calculated analytically.

Note that in Equation (11), the meta-hybrid-DGM is formulated on the neural component within the hybrid-DGM to establish its identifiablity. While the identifiability of the physics-based component is not hinged on this construction, in practice, it is possible to extend the conditional prior (Equation(9)) to \(}\) to obtain \(p_{}(},}|_{m}^{s})\), leading to an alternative meta-formulation:

\[_{m=1}^{M}_{^{q}_{m}^{q }} p(^{q}|_{m}^{s})&_{m=1}^{M} _{^{q}_{m}^{q}}\{_{q_{}(},}|^{}_{m}^{s})}[ p_{}( ^{q}|},})]\\ &-(q_{}(},}|^{q }_{m}^{s})||p_{}(},}|_{m}^{s}))\}\] (12)

We will empirically compare the two formulations above, and demonstrate how the identifiability of \(}\) and \(}\) is each affected by the proposed meta-construction of the hybrid-DGMs.

## 5 Identifiability Theory for Meta-Hybrid-DGMs

We consider the meta-formulation a novel condition to ensure the identifiability of unsupervised DGMs. Here, we show that, built on nonlinear ICA and the theory of identifiability for conditional-VAEs established in , the presented meta-hybrid-DGMs are identifiable.

For readability, below we collect the formulations of the hybrid-DGM from previous sections:

\[p_{}(,},}|^{s})=p_{ }(|},})p(})p_{,_{}}(}|^{s})\] (13) \[p_{}(|},})=p_{ }(-[f_{_{_{_{_{ _{_{_{_{_{_{ _{_{}_{_{}_{}_{ _{}_{_{}_{_{}_{ }_{_{}_{}_{}_{}_{ }_{_{}_{}_{}_{}_{ }_{}_{_{}_{}_{}_{ }_{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\,\,\,\,\,\,\,\\\\\\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\] (14) \[p_{,_{}}(}| ^{s})=_{i=1}^{d}(z_{,i})}{Z_{i}(^ {s})}[_{i}(z_{,i})^{}_{ ,i}(^{s})]\] (15)

where \(=(,,_{})\) are model parameters and \(_{}\) is defined in Equation (10). Equation (14) gives a more general definition than Equation (3) where \(\) is an independent noise variable.

**Definition 1:** _Let \(\) be an equivalence relation on the parameter space \(\). We say the DGM in 1 is \(\)-identifable if \(p_{}()=p_{}()\)_

**Definition 2:** _Let \(\) be the equivalence relation on \(\) defined as follows:_

\[(,,_{})(,},_{}),:(_{}^{-1}())=}(_{}^{-1}())+, \] (16)

_where \(\) is a \(de de\) matrix and \(\) is a vector of dimension \(de\), \(d\) being the dimension of \(}\) and \(e\) the dimensionality of the sufficient statistics \(_{i}\). If \(\) is invertible, we denote this relation by \(_{A}\)._

Definition 2 establishes a specific equivalence relation that allows to recover the sufficient statistics of the generative model up to a linear matrix multiplication.

**Theorem 1:** _Assume we observe data sampled from \(p_{}(,},}|^{s})\) as defined in Equations (13)-(15) with parameters \(=(,,_{})\). Assume the following holds:_

1. _The set_ \(\{|_{}()=0\}\) _has measure zero, where_ \(_{}\) _is the characteristic function of the density_ \(p_{}\) _defined in_ \(p_{}(-[f_{_{_{_{ _{_{_{_{}_{_{} {}_{_{}_{}_{_{}_{} _{_{}_{}_{}}}}}}}}}}},\,}])\)_._
2. _The hybrid mixing function_ \(_{}\) _is injective._
3. _The sufficient function_ \(T_{i,j}\) _are differentiable almost everywhere, and linearly independent on any subset of_ \(\) _of measure greater than zero._
4. _There exist_ \(de+1\) _distinct context sets_ \(^{s,0},^{s,1},,^{s,de}\) _such that the_ \(de de\) _matrix_ \(\) _defined as follows is invertible._ \[=((^{s,1})-( ^{s,0}),,(^{s,de})-(^{s,0}))\] (17)

_Then the parameters \(=(,,_{})\) are \(_{A}\)-identifiable._

The proof for Theorem 1 directly builds on that presented in . Condition (iv) in Theorem 1 establishes that, if the generative factors in the neural component of the hybrid-DGM is \(d\)-dimensional each specified with \(e\)-dimensional sufficient statistics, we will need observations from a minimum of \(de+1\) distinct and independent generation processes in order to identify the hybrid-DGM. Considering the pendulum-system example discussed in Section 3 and Equation (2), since the neural component is generated by three independent system parameters \(,A,\) (\(d=3\)) and assuming Gaussian statistics (\(e=2\)), the hybrid-DGMs will be identifiable if we have observations generated from more than \(3 2+1=7\) distinct combination of parameter values for \(,,A,\) in the true data-generation process (Equation (2)). We will empirically verify this condition in Section 6.

## 6 Experiments and Results

Data:We considered three simulated and one real-world benchmarks for hybrid-DGMs, including three simulated physics systems of forced damped pendulum , advection-diffusion system  and double pendulum , and one real-world system double pendulum . For each of the three simulated physics systems, we randomly sampled the initial states and parameters of the governing function to generate observations. We refer to the governing functions used for the generation of data as _full physics_ functions, and design _partial physics_ functions for represent our imperfect prior knowledge about the observed data, reflecting a variety of potentially additive and multiplicative errors - these _partial physics_ functions are used as the \(f_{}\) in the hybrid models. In data generation, we varied the parameters in the components both present and absent in the prior physics. More details on each dataset can be found in Appendix A.

Baselines:We considered 1) physics-integrated hybrid VAE  and 2) APHYNITY , both using regularization to control the expressiveness of the neural component. Note that while the original formulation of APHYNITY does not have an inference network to identify \(_{}\) or \(_{}\) (making it intended for data generated from a single parameter setting), we added an inference network (that shares a similar architecture with the other two models) for fairness of comparison.

Metrics:With a focus on identifiability, we focus on the following metrics: 1) for physics-based latent variable \(_{}\), we consider mean squared error (MSE) owing to its physical meaning; 2) for abstract latent variable \(_{}\) modeled in the neural component, we consider the mean correlation coefficient (MCC) between the true data-generating parameters and \(_{}\) sampled from its learned posterior density. The calculation of MCC follows standard practice in the identifiability literature [6; 9]: to evaluate \(_{A}\)-identifiablity, we calculate the weak MCC with the method described in , where a high MCC provides evidence of successful identification. We also evaluate a more strong identifiable relation \(_{P}\) with strong MCC and the details can be found in Appendix E.

In addition to metrics on \(_{}\) and \(_{}\), we further introduce MSE metrics on the generated \(\) in two distinct scenarios: 1) _reconstruction MSE_ measures how well an observed \(\) is reconstructed from the identified \(_{}\) and \(_{}\), and 2) _prediction MSE_ measures how well the identified \(_{}\) and \(_{}\) can be used to generate outside the observed \(\) used to identify them, _e.g.,_ for predicting over long time domains, or predicting for a different sample that comes from the same generation process but with different initial conditions. We use these two metrics to showcase that a good reconstruction does not guarantee a good identification, while the ability to use the identified latent variables to predict for different samples (other than observed) may be a surrogate for identifiability.

    &  &  \\  & APHYNITY & Hybrid-VAE & Meta-Hybrid-VAE & APHYNITY & Hybrid-VAE & Meta-Hybrid-VAE \\  MSE of \(_{}\) \(\) & 6.96(0.01)e-2 & 4.14(0.29)e-2 & **1.59(0.07)e-2** & 1.9(0.3)e-2 & 7.12(5.32)e-4 & **1.34(0.75)e-4** \\  MCC \(\) & 0.79(0.02) & 0.59(0.03) & **0.99(0.00)** & 0.98(0.01) & 0.94(0.00) & **0.99(0.00)** \\   MSE of \(\) (Rec) \(\) & 5.2(0.01)e-2 & **2.66(0.03)e-2** & 2.91(0.06)e-2 & 6.22(0.41)e-3 & 4.90(0.75)e-3 & **2.93(0.22)e-3** \\  MSE of \(\) (Pre) \(\) & 2.37(0.67) & 1.74(0.20) & **6.85(1.11)e-2** & 8.25(0.58)e1 & 8.53(5.72)e-2 & **5.63(1.34)e-3** \\    &  &  \\  & APHYNITY & Hybrid-VAE & Meta-Hybrid-VAE & APHYNITY & Hybrid-VAE & Meta-Hybrid-VAE \\  MSE of \(_{}\) \(\) & 4.58(0.04)e-1 & 3.97(0.06)e-1 & **3.85(0.19)e-1** & / & / & / \\  MCC \(\) & 0.50(0.00) & 0.51(0.00) & **0.98(0.00)** & / & / & / \\   MSE of \(\) (Rec) \(\) & 4.88(0.00)e-2 & 4.05(0.25)e-2 & **3.83(0.06)e-2** & 3.78(0.52)e-2 & **2.20(0.14)e-3** & 2.67(0.34)e-2 \\  MSE of \(\) (Pre) \(\) & 1.29(0.23)e1 & 5.52(0.06) & **2.80(0.48)e-1** & 5.16(0.69) & 1.87(0.13) & **2.88(0.34)e-2** \\   

Table 1: Quantitative identifiability metrics for the presented meta-hybrid-VAE in comparison to physics-integrated hybrid-VAE  and APHYNITY , including MSE and MCC metrics on the latent variables, as well as MSE of generated \(\) during reconstruction of observed samples (Rec) and prediction of unobserved samples (Pre).

### Results on Synthetic Data

Reconstruction _vs_. Prediction Performance:Table 1 summarizes quantitative identifiability metrics on the presented meta-hybrid-VAE in comparison to its baselines, where the prediction performance of all models is mentioned by their ability - once identified - to predict for a sample sharing the same data-generating parameters but different initial conditions. As shown, meta-hybrid-VAE significantly improved the identifiability of the abstract latent variable \(_{}\) (the highest MCC close to 1) and via which, moderately improved the accuracy of the physics-based latent variable \(_{}\) (the lowest MSE). Very importantly, while both baseline models identified \(_{}\) with a reasonable accuracy, their MCC values were significantly lower - providing strong empirical support for the un-identifiability of the neural component of the hybrid-DGM as theorized in this paper.

Also importantly, all hybrid models achieved comparable reconstruction MSE for observed \(\)'s, where the meta-hybrid-VAE did not necessarily produce the lowest MSE. The ability of the identified hybrid-DGM to predict for different samples other than observed, however, varied significantly: while the meta-hybrid-VAE was able to deliver a MSE comparable to the reconstruction task, the two baseline models saw a deterioration of MSE by two magnitudes. Fig. 1 provides visual examples to demonstrate this performance difference between _reconsruction_ and _prediction_ tasks, stressing the importance to 1) look beyond reconstruction performance for evaluating hybrid-DGMs, and 2) consider prediction of unobserved samples as a potential surrogate for identifiability measures when ground-truth of latent variables is not available (real data settings in Section 6.2).

Interestingly, among the three datasets, the advection-diffusion system appeared to be relatively simple to identify as the baselines did not exhibit as significant a degradation of performance in the more difficult identifiability metrics (MCC and prediction-MSE). This is potentially because the missing component from the prior physics has a small effect. In comparison, the MCC and prediction-MSE of the baseline models were significantly poorer in double pendulum, suggesting a difficult missing physics to be identified. Regardless, meta-hybrid-VAE was able to obtain significant margins of improvements compared to baselines across all datasets.

Predictions over Longer Time Domains:Fig. 2 shows the prediction performance of the three models when predicting the trajectory of pendulum movement beyond the time domain used to identify the model, with a visual sample of forced damped pendulum system. As shown, while all models' performance deteriorated as they predicted outside the time domain used in training, the presented meta-hybrid-VAE demonstrated significantly smaller drop in performance and slower rate of deterioration as the prediction horizon increased. This provides strong evidence regarding the importance of the identifiability, even for the neural component, of a hybrid model for the purpose of predictive tasks.

OOD Performance:Figure 3 (left) summarizes the performance of the three models in settings where either the physics-based component \(_{}\) or the neural component \(_{}\) was outside the training distribution for forced damped pendulum system. Results demonstrated that, meta-hybrid-VAE obtained the strongest performance in OOD settings for both the identification of the physics and neural components. Details of the OOD parameters can be found in the Appendix A.1.

Empirical Verification of the Condition for Identifiability:Theorem 1-iv) stipulates that the identifiability of the hybrid-DGM depends on the number of independent conditional densities observed, in relation to the dimension of the latent factors to be identified. To experimentally

Figure 1: Visual results on synthetic data for reconstruction and prediction performances.

verify this, we evaluated the identifiability metric MCC of the hybrid-DGM when learning over data generated from an increasing number of distinct parameter values governing the pendulum system (Equation (2)), _i.e._, number of tasks in meta-learning. The results in Figure 3 (right) showed that the MCC metric exceeded 0.85 when 7 tasks appeared in the training data and stablized when 8 or more tasks appeared. This agreed with the theorem that, for the pendulum system that had a 3-dimensional parameter vector to be identified in the neural component of the hybrid model with a Gaussian assumption of 2-dimensional sufficient statistics, a minimum of 3*2+1 = 7 distinct "tasks" s needed to identify the system.

Additional Alation Analyses of Meta-Hybrid-VAE:In Appendix C.1, we provided ablation results on the pendulum system to demonstrate that the identifiability results of the presented meta-hybrid-VAE were minimally affected by the number of parameters to be identified, as long as the theoretical condition for identifiability is met. In Appendix C.2, we provided further ablation studies to show that the physics-based component did not suffer from the type of un-identifiabilty discussed in this paper as the neural component, although the meta-formulation did moderately improve the accuracy of the estimation of the physics-based parameters.

### Real Data of Double Pendulum

Experimental settings:We used the dataset of a double pendulum introduced by , which contains 21 videos of the pendulum. Each run lasts approximately 40 seconds and is recorded at 400Hz. We extractd the position of the pendulum limbs from each frame with elementary computer-vision tools. We divided each video into observations of \(\)'s to consist of 20 temporal frames with a sampling frequency of 100 Hz. Because there is no clear indication of which samples belong to the same data-generation process, we use 7 samples preceding the current query sample as the context samples. We generated 10000, 3400, and 3400 training, validation, and test samples, respectively.

We adopted Equation (19) as the physics-component of the hybrid-DGM, using the known lengths of the two arms (\(L_{1}=91mm\) and \(L_{2}=70mm\)) and assuming \(=1\). The total energy of the double

Figure 3: (left) Results of OOD; (right) Results of the condition for identifiability.

Figure 2: (Left) The performance of predictions over longer time domains; (Right) A visual example of forced damped pendulum system.The dark part is the training time domain, and the light part is the time domain outside training (longer time domain).

pendulum decreases over time in all videos, which is not explained by this physics model. Nor does the physics model consider potential vibrations or errors in extracting the arms' positions, which is expected to be learned by the neural component of the hybrid-DGM with the dimension of \(_{}=4\).

Results:As ground-truth generative factors are not available in the real data, we relied on prediction MSE as a surrogate for the identifiability metric as informed by results in Sections A.1-A.3. While all models delivered comparable reconstruction MSE, meta-hybrid-VAE was able to achieve a prediction MSE at a level similar to reconstruction while the two baselines experienced a deterioration by 2-3 magnitudes. Visual examples are provided in Fig. 4. This strong performance in real data provides solid evidence for the presented meta-formulation to establish identifiability of hybrid-DGMs.

## 7 Conclusions & Discussion

In this paper, we probe the un-identifiability of hybrid-DGMs with unconditioned priors, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs - both results were supported by strong theoretical and empirical evidence. Moving forward, this work can be improved as follows.

Comprehensive ablation studies of the meta-hybrid-VAE are needed to examine the effect of several key hyperparameters, such as the size of \(k\)-shot context set and the dimension of \(_{}\). The latter was set to the number of true generative factors in synthetic experiments, and its effect on model performance needs to be further examined. Note that this setting was identical across baselines, thus it had no effect on the significant margins of improvements seen by meta-hybrid-VAE compared to baselines. Future ablation studies should also delve into the benefit of hybrid-DGMs in comparison to purely physics-based and purely neural DGMs, which we considered out of scope of the current study that focused on establishing the identifiability of hybrid-DGMs both theoretically and empirically.

Regarding the generality of the meta-formulation as a solution to identifiable DGMs beyond the hybrid formulation, Appendix D presented an initial investigation on the performance of the presented meta-formulation on a general non-hybrid VAE on a synthetic dataset of non-stationary Gaussian time-series, in comparison to the identifiable-VAE  constructed using known class labels to condition the generative model.

Future works could investigate this effect in a broader context. Finally, while the meta-formulation foregoes explicitly observed auxiliary variable, it does assume the ability to pair context and query samples from the same data generation process. This is a common assumption in meta-learning and, in scenarios where this knowledge is not evident, simply pairing by preceding temporal samples was shown to be effective in our real-data experiments. In scenarios where no information is available for pairing, the presented meta-solution will not be applicable.