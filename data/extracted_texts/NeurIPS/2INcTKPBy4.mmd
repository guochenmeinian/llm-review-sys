# The Sample Complexity of Gradient Descent

in Stochastic Convex Optimization

 Roi Livni

School of Electrical Engineering

Tel Aviv University

rlivni@tauex.tau.ac.il

###### Abstract

We analyze the sample complexity of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization. We show that the generalization error of GD, with common choice of hyper-parameters, can be \((d/m+1/)\), where \(d\) is the dimension and \(m\) is the sample size. This matches the sample complexity of _worst-case_ empirical risk minimizers. That means that, in contrast with other algorithms, GD has no advantage over naive ERMs. Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations. Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of samples, \(T=(1/^{4})\) iterations are necessary to avoid overfitting. This resolves an open problem by Amir, Koren, and Livni , Schliserman, Sherman, and Koren , and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension.

## 1 Introduction

Stochastic Convex Optimization (SCO) is a theoretical model that depicts a learner that minimizes a (Lipschitz) convex function, given finite noisy observations of the objective . While often considered simplistic, in recent years SCO has become a focus of theoretical research, partly, because of its importance to the study of first-order optimization methods. But, also, it has become focus of study because it is one of few theoretical settings that exhibit _overparameterized learning_. In more detail, classical learning theory often focuses on the tension between number of samples, or training data, and the complexity of the model to be learnt. A common wisdom of classical theories  is that, to avoid overfitting, the complexity of a model should be adjusted in proportion to the amount of training data. However, recent advances in Machine Learning have challenged this viewpoint. Evidently , state-of-the-art algorithms generalize well but without, explicitly, controlling the capacity of the model to be learnt. In turn, today, it is one of the most emerging challenges, for learning theory, to understand learnability when the number of parameters in a learnt model exceeds the number of examples, and when, seemingly, nothing withholds the algorithm from overfitting.

Towards this, we look at SCO. In SCO, Shalev-Shwartz, Shamir, Srebro, and Sridharan  showed how algorithms can overfit with _dimension dependent_ sample size. But, at the same time, it was known  that there are algorithms that provably avoid overfitting with far fewer examples than dimensions. As such, SCO became a canonical model to study how a well-designed algorithm can avoid overfitting even when the number of examples is too small to guarantee generalization by an algorithmic-independent argument . A step towards understanding _what_ induces generalization is to identify _which_ algorithms generalize. Then, we can ask what yields the separation. Surprisingly, for many well-studied algorithms this question is not always answered.

Perhaps the simplest algorithm, whose sample complexity is not yet understood, is Gradient Descent (GD). And we turn to the basic question of the sample complexity of gradient descent.

While this question remained open, there have been several advancements and intermediate answers: The first, dimension independent, generalization bound was given by Bassily, Feldman, Guzman, and Talwar  that provided stability bounds . The result of Bassily et al. demonstrated that, GD can have _dimension-independent_ sample complexity rate. However, to achieve that, one has to use non-standard choice of hyperparameters which affects the efficiency of the algorithm. In particular, the number of rounds becomes quadratic in the size of the sample (as opposed to linear, with standard choice). On the other hand, a classical covering argument shows that linear dependence in the dimension is the worst possible, for any empirical risk minimizer, irrespective of properties such as stability.

In terms of lower bounds, Amir, Koren, and Livni  were the first to show that GD may have a dimension dependence in the sample complexity. They showed that, with natural hyperparameters, the algorithm must observe number of samples that is at least logarithmic in the dimension. This result was recently improved by Schliserman, Sherman, and Koren  that showed that at least square root of the dimension is required. Taken together, so far it was shown that either the algorithm's hyperparameters are tuned to achieve stability, at a cost in running time, or the algorithm must suffer _some_ dimension dependence, linear at worst square root at best.

Here, we close the gap and show that linear dependence is necessary. Informally, we provide the following generalization error bound, in terms of dimension \(d\), sample size, \(m\), and hyperparameters of the algorithm, \(\) and \(T\) (the learning rate and number of iterations). We show that when \(T\) is at most cubic in the dimension (see Theorem 1 for a formal statement):

\[=(\{,1\} \{,1\}).\]

The first factor in the RHS describes the linear dependence of the generalization error in the dimension, and corresponds to the optimal sample complexity of empirical risk minimizers, as demonstrated by Carmon, Livni, and Yehudayoff . The second term lower bounds the stability of the algorithm , and played a similar role in previous bounds . Each factor is optimal at a certain regime, and cannot be improved. Most importantly, for a standard choice of \(=O(1/)\), the first term is dominant, and the aformentioned lower bound is complemented with the upper bound of Carmon et al. . Our result implies, then, a sample complexity of \((d/m+1/)\). When \(d m\), the second factor is dominant. When running time is at most quadratic in number of examples, this term also governs the stability of the algorithm, hence the result of Bassily et al.  provides a complementary upper bound (see further discussion in Section 3.1).

## 2 Background

We consider the standard setup of Stochastic Convex Optimization (SCO) as in . Set \(=\{w:\|w\| 1\}\), and let \(\) be an arbitrary, finite domain (our main result is a lower bound, hence finiteness of \(\) is without loss of generality). We assume that there exists a function \(f(w,z)\) that is convex and \(L\)-Lipschitz in \(w\) for every choice of \(z\). Recall that a function \(f\) is convex and \(L\)-Lipschitz if for any \(w_{1},w_{2}\) and \(0 1\):

\[f( w_{1}+(1-)w_{2}) f(w_{1})+(1-)f(w_{2}), |f(w_{1})-f(w_{2})| L\|w_{1}-w_{2}\|.\] (1)

First order optimizationAlgorithmically we require further assumptions concerning any interaction with the function to be optimized. Recall  that, for fixed \(z\), the sub-gradient set of \(f(w,z)\) at point \(w\) is the set:

\[ f(w,z)=\{g:f(w^{},z) f(w,z)+g^{}(w^{}-w),  w^{}\}.\]

A first order oracle for \(f\) is a mapping \(_{z}(w)\) such that \(_{z}(w) f(w,z).\) Our underlying assumption is that a learner has a first order oracle access. In other words, given a function \(f(w,z)\), we will assume that there is a procedure \(_{z}\) that calculates and returns a subgradient at every \(w\) for every \(z\). Recall  that when \(| f(w,z)|=1\), the function is differentiable, at \(w\), and in that case, the unique subdifferential is the gradient \( f(w,z)\).

### Learning

A learning algorithm \(A\), in SCO, is any algorithm that receives as input a sample \(S=\{z_{1},,z_{m}\}^{m}\) of \(m\) examples, and outputs a parameter \(w_{S}\). An underlying assumption in learning is that there exists a distribution \(D\), unknown to the learner \(A\), and that the sample \(S\) is drawn i.i.d from \(D\). The goal of the learner is to minimize the population loss:

\[F(w)=*{}_{z D}[f(w,z)],\]

More concretely, We will say that the learner has sample complexity \(m()\) if, assuming \(|S| m()\), then w.p. \(1/2\) (Again, because we mostly care about lower bounds, fixing the confidence will not affect the generality of our result):

\[F(w_{S})-_{w W}F(w).\] (2)

Empirical Risk MinimizationA natural approach to perform learning is by _Empirical Risk Minimization_ (ERM). Given a sample \(S\), the empirical risk is defined to be:

\[F_{S}(w)=_{z S}f(w,z).\]

An \(\)-ERM is any algorithm that, given sample \(S\), returns a solution \(w_{S} W\) that minimizes the empirical risk up to additive error \(>0\). Recently, Carmon et al.  showed that any \(\)-ERM algorithm has a sample complexity bound of

\[m()=(+ }),\] (3)

The above rate is optimal up to logarithmic factor . Namely, there exists a construction and an ERM that will fail, w.p. \(1/2\), unless \(m=(d/)\) examples are provided1. Importantly, though, there are algorithms that can learn with much smaller sample complexity. In particular SGD , stable-GD  and regularized ERMs .

#### Gradient Descent

We next depict Gradient Descent whose sample complexity is the focus of this work. GD depends on hyperparameters \(T\) and \( 0\) and operates as follows on the empirical risk. The algorithm receives as input a sample \(S=\{z_{1},,z_{m}\}\), defines \(w_{0}=0\), and operates for \(T\) iterations according to the following recursion:

\[w_{t}=[w_{t-1}-_{z S}_{z}(w_{t}) ]\ w_{S}^{GD}:=_{t=1}^{T}w_{t},\] (4)

where \(\) is the projection onto the unit ball, and \(_{z}(w_{t})\) is a subgradient of the loss function \(f(w,z)\) at \(w_{t}\). The final output, \(w_{S}^{GD}\), of the algorithm is the averaged iterate (our result, though, can be generalized to other possible suffix-averages such as, say, outputting the last iterate, see Theorem 10). GD constitutes an \(\)-ERM. Concretely, it is known  that GD minimizes the empirical risk and its optimization error is given by:

\[F_{S}(w_{S}^{GD})-_{w W}F_{S}(w)=((+,1)).\] (5)

The above bound is tight irrespective of the dimension2. The population loss have also been studied, and Bassily et al.  demonstrated the following learning guarantee:

\[*{}_{S D^{m}}[F(w_{S}^{GD})-_{w W}F(w) ]=O(++).\] (6)

The last two terms in the RHS follow from a stability argument, provided in , and the first term follows from the optimization error of GD as depicted in Eq. (5). Notice that there is always an \(O()\) gap between the generalization error and empirical error of gradient descent.

## 3 Main Result

**Theorem 1**.: _For every \(d 4096,T 10,m 1\) and \(>0\), there exists a distribution \(D\), and a \(4\)-Lipschitz convex function \(f(w,z)\) in \(^{d+1}\), such that for any first order oracle of \(f(w,z)\), with probability \(1/2\), if we run GD with \(\) as a learning rate then:_

\[F(w_{S}^{GD})-_{w}F(w)} \{,1\}\{/136,T\}},1\}.\]

We remark, that the above theorem is true for _any_ suffix averaging (e.g. last iterate), and not restricted to the averaged iterate (see Theorem 10). We now specialize our bound for two interesting regimes. First, we improve previous dependence in the dimension in  and obtain a generalization error bound for \(d=(m+T^{1/3})\):

**Corollary 2**.: _Fix \(\), and suppose \(d=(m+T^{1/3})\). There exists a distribution \(D\), and an \(O(1)\)-Lipschitz convex function \(f(w,z)\) in \(^{d}\), such that for any first order oracle of \(f(w,z)\), with probability \(1/2\), if we run GD for \(T\) iterations, then:_

\[F(w_{S}^{GD})-_{w}F(w)(\{+},1\}).\] (7)

The first term follows from Theorem 1, the second term follows from the optimization error in Eq. (5). Equation (7) does not hold for \(d<m\), and the linear improvement over  is tight. This can be seen from Eq. (5) that shows that GD achieves \(\) empirical excess error when \(=O(1/)\) and \(T=O(1/^{2})\). Equation (7) becomes vacuous for such choice of parameters, but Carmon et al.  showed that the sample complexity of _any ERM_ is bounded by \(((d+)/m)\). However, as depicted next, this upper bound becomes tight and GD does not improve over a worst-case ERM:

**Corollary 3**.: _Suppose \(T=O(m^{1.5})\), and \(=(1/)\). There exists a distribution \(D\), and an \(O(1)\)-Lipschitz convex function \(f(w,z)\) in \(^{d}\), such that for any first order oracle of \(f\), with probability \(1/2\), if we run GD with \(\), for \(T\) iterations:_

\[F(w_{S}^{GD})-_{w}F(w)(\{+},1\}).\]

Corollary 3 complements Carmon et al.  upper bound, and improves over Feldman  lower bound that only showed existence of _some_ ERM with the aforementioned sample complexity. To see that Corollary 3 follows from Theorem 1, notice that when \(d\), then \(d/m<1/\) and the bound is dominated by the second term, which is a well known-information theoretic lower bound for learning. When \(d>\), and \(T<m^{1.5}\) we have that \(T d^{3}\), plugging \(=O(1/)\) yields the bound.

### Discussion

Theorem 1 provides a new generalization error bound for GD. It shows that the worst case sample complexity for ERMs, derived by Feldman , is in fact applicable also to a very natural first order algorithm and not just to abstract ERMs. This Highlights the importance of choosing the right algorithm for learning in SCO. As discussed, the bound is tight in several regimes, nevertheless still there are unresolved open problems.

Stability in low dimensionWhen GD is treated as a naive empirical risk minimizer, and \(=O(1/)\), \(T=O(m)\), there is no improvement, when using GD, over a worst-case ERM. In the other direction, for dimension that is linear in \(m\), one cannot improve over the \(()\) term that governs stability. Our bound, though, provide a hope that stability in low dimension can yield an improved bound. In particular, consider the case where \(=1/T^{1/4}\) and \(d<m\). This is a case where we apply a stable algorithm in small dimensions. Our bound does not negate the possibility of an improved generalization bound. That would mean that, at least at some regime, GD can improve over the worst-case ERM behaviour. We leave it as an open problem for future study

**Open Question 4**.: _Is there a generalization bound for GD such that_

\[*{}_{S-D^{m}}[F(w_{S}^{GD})-_{w}F(w )]=O(}{m}+}).\]

_Alternatively, can we prove an improved generalization error bound such that:_

\[*{}_{S-D^{m}}[F(w_{S}^{GD})-_{w}F (w)]=(\{,,1\}).\]

**Late stopping** Another regime where there is a gap between known upper bound and lower bound appears when \(T=(m^{2})\). Specifically, the stability upper bound for GD by Bassily et al.  gives

\[*{}_{S-D^{m}}[F(w_{S}^{GD})-_{w} F(w)]=O(++).\]

By Corollary 2, for large enough dimension:

\[*{}_{S-D^{m}}[F(w_{S}^{GD})-_{w }F(w)]=(\{+,1\} ).\]

When \(T=O(m^{2})\), the two bounds coincide. Indeed, for the \( T/m\) term to dominate the \(\) term, we must have \(T=(m^{2})\). One has to take at least \(T=O(m^{2})\) iterations in order to generalize with GD (in fact, any full batch method ), however \(T=O(m^{2})\) iterations are sufficient. Nevertheless, the above gap does yield the possibility of an _unstable_ GD method that does generalize. Particularly, if we just regulate the term \(\), but allow \( T/m=(1)\), then this may yield a regime where GD is unstable (and ERM bounds do not apply) and yet generalize.

**Open Question 5**.: _Are there choices of \(\) and \(T\) (that depend on \(m\)) such that \( T/m(1)\), but GD has dimension independent sample complexity?_

Notice that the \( T/m\) term also governs stability in the _smooth_ convex optimization setup . Recall that a function \(f(w,z)\) is said to be \(\)-smooth if for all \(z\), \(f(w,z)\) is differentiable, and the gradient is an \(\)-Lipschitz mapping . For smooth optimization, even if \(=(1)\), GD is still stable. Hardt, Recht, and Singer  showed that the stability of GD in the smooth case is governed by \(O()\) for \(<1/\). Therefore, the question of generalization when \( T/m(1)\) remains open, even under smoothness assumptions:

**Open Question 6**.: _Assume that \(f(w,z)\) is \((1)\)-smooth. What is the sample complexity of GD, when \(\) and \(T\) are chosen so that \(+=o(1)\), but \(=(1)\)._

## 4 Technical Overview

We next provide a high level overview of our proof technique. For simplicity of exposition we begin with the case \(T=m=d\). We begin by a brief overview of previous construction by Amir et al.  that demonstrated Corollary 2 when \(m=( d)\). The construction in  can be decomposed into three terms:

\[f(w,z)=g(w,z)+N_{0}(w)+h(w,z).\]

The function \(g\) has the property that an ERM may fail to learn, unless dimension dependent sample size is considered. Amir et al.  incorporated Shalev-Shwartz et al.  construction. Later,  used Feldman's function  to construct \(g\). The shift from the construction depicted in Shalev-Shwartz et al.  to Feldman's function is the first step that allows to move from logarithmic to polynomial dependence in the dimension. In both constructions an underlying property of \(g\) is that there exists a distribution \(D\) such that, for small samples, there are overfitting minima. Concretely, there exists a \(w_{S}\{0,1/\}^{d}\) such that

\[_{z S}g(w_{S},z)-*{}_{z-D}[g(w_{S}, z)]=(1).\] (8)

The challenge is then, to make gradient descent's trajectory move towards the point \(w_{S}\). The idea can be decomposed into two parts:

### Simplifying with an adversarial subgradient:

To simplify the problem, let us first ease the challenge and suppose we can choose our subgradient oracle in a way that depends on the observed sample. Let \(N_{0}\) be the Nemirovski function :

\[N_{0}(w)=\{-w(i),0)\}.\]

Notice that \(N_{0}\) is not differentiable and the choice of subgradient at certain points is not apriori determined. For example, notice that every standard basis vector \(-e_{i} N_{0}(0)\). More generally, given a sample \(S\), let \(I=i_{1},,i_{d^{}}\) be exactly the set of indices such that \(w_{S}\), from Eq. (8), \(w_{S}(i) 0\). Now assume by induction that \(w_{t}(i)>0\) exactly for \(i=i_{1}, i_{t}\), then one can show that we can define the subgradient oracle of \(N_{0}\):

\[(w_{t})=-e_{i_{t+1}} N_{0}(w_{t}).\]

In that case \(w_{t+1}\) will satisfy our assumption for \(i_{t+1}\) and we can continue to follow this dynamic for \(T\) steps.

Notice that, in this case, GD will converge to \(w_{S}\) (if \(=1/\) which we assume now for concreteness). One can also show that the output of GD (the averaged iterate) will overfit. The caveat is that our subgradient oracle depends on the sample \(S\). In reality, the sample is drawn independent of the subgradient oracle. and all previous constructions, as well as ours need to handle this. This is discussed in the next section. But before that, let us review another challenge which is when \(T d\):

When \(d T\)Another challenge we face with the construction above is that it works when we assume that \(T d\). That is because, in Nemirovski's function, the number of iterates we can perform is bounded by the dimension. After \(d\) iterations we will end up with the vector \(v=_{t=1}^{d} e_{i_{t}}\). If \(T=(d)\) then \(=o(1/)\), and the dynamic will end up with a too small norm vector to induce a sizeable population loss. This strategy will provide, at best, with a factor of the form \((})\). Such a factor may be unsatisfactory in a very natural setting where, say, \(T=O(m)\), \(=O(1/)\), and \(d=()\). To obtain the \(d^{3}\) dependence, we perform the following alternation over the Nemirovski function. Consider the function:

\[N(w)=\{0,_{i d}\{-w(i)\},_{i j d}\{w(j)-w(i)\}\}.\] (9)

And suppose that at each iteration we return a subgradient as follows:

* If there is \(i d\), such that \(w(i)=w(i+1)>\), return subgradient \(e_{i+1}-e_{i}\) and \(w\) is updated by \(w_{t+1}=w_{t}- e_{i+1}+ e_{i}\).
* If there is no such \(i\), then take the minimal \(i\) (if exists) such that \(w(i)=0\), and return subgradient \(-e_{i}\) and update \(w_{t+1}=w_{t}+ e_{i}\).
* When non of the above is met, return subgradient \(0\).

The dynamic of the above scheme is depicted in Fig. 1, and solves the problem when \(T d^{3}\). One can show that GD will run for at least \(d^{3} T\) iterations, and will increase \(O(d)\) coordinates, each, on average, by an order of \(O( d)\). This is better than the increase of \(\) in each coordinate that we get from Nemirovski's function. In this way we obtain the improved result of \(\), even when \(T d^{3}\).

When \(T d\),when the number of iterations is smaller than \(d\) we face a different challenge. The immediate solution is to embed in \(^{d}\) a construction from \(^{T}\), this will provide us with the \(()\) term but, on the other hand, such a construction will not yield a \((d/m)\) term. A different approach, that exploits the dimension to its fullest, is to consider blocks of coordinates and operate on those instead of single coordinates.

The conclusive outcome incorporates both ideas together, and we replace the Nemirovski function with a version of Eq. (9) that operates on \(O(T^{1/3})\) blocks of coordinates. And this concludes our construction. We next move on to the challenge of replacing the data dependent oracle with a standard first order oracle.

#### Reduction to sample dependent oracle:

As discussed, the construction above does not yield a lower bound as it relies on a subgradient oracle that is dependent on the whole sample. To avoid such dependence of the oracle on the sample, we observe that if we can infer the sample \(S\) from the trajectory, i.e. if the state \(w_{t}\) "encodes" the sample, then formally the subgradient is allowed to "decipher" the sample from the point \(w_{t}\). In that way we achieve this behaviour of sample dependent subgradient oracle. This part becomes challenging and may depend on the way we choose \(g\), and \(N\). The simplest case, studied by Amir et al. , introduced the third function, \(h\), which was a small perturbation function that elevates coordinates in \(I\) and inhibits coordinates not in \(I\). The function \(h\) depends on \(z\) and not on \(S\), hence it cannot know apriori \(I\). But, an important observation is that, in Shalev-Shwartz et al.  construction, if \(i I\), there exists \(z S\) that "certifies" that. In fact, each \(z\) can be thought of as a subset of indices, and if an index appears in \(z\), then it cannot be in \(I\). So we can build the perturbation in a way that every coordinate is elevated, unless \(z\) certifies \(i I\): In that case we define \(h(w,z)\) so that its gradient will radically inhibit \(i\).

The last observation is what becomes challenging in our case. As discussed, to achieve improved rate, we need to use Feldman's function. When using Feldman's function the coordinates cannot be ruled out, or identified, by a single \(z\) but one has to look at the whole sample to identify \(I\). While Schliserman et al.  tackle a similar problem, we take a slightly different approach described next: For each \(z\) assign a random, positive, number \((z)\). We can think of this \(\) as a hash function. Let us add another coordinate to the vector \(w\), \(w(d+1)\). Consider the function

\[h(w,z)=(z) w(d+1).\]

Then \( h(w,z)=(z)e_{d+1}\). Write \((S)=_{z S}(z)\) then in turn:

\[w_{t}(d+1)=w_{t-1}(d+1)-_{z S}h(w_{t-1},z)=- t(S)e_{d+1}.\]

If \(\), \((z)\) are chosen correctly, \((S)\) is a one to one mapping from samples to real numbers, and small \(\) ensures that the overall addition of \(h\) has negligible affect on the outcome. Then, we may define the subgradient oracle to be dependent on coordinate \(d+1\) which encodes the whole sample. Our final construction will take a different \(h\), which adds small strong convexity in this coordinate, for reasons next explained:

#### Working with any first order oracle

Notice that our statement is slightly stronger than what we so far illustrated. Theorem 1 states that, for _any_ subgradient oracle, GD will fail. For that, we need to be a little bit more careful, and we want to replace our function with a function that leads to the

Figure 1: Depiction of the dynamics induced by Eq. (16) and our choice of sub-differentialssame guaranteed trajectory, but at the same time it should be differentiable at visited points. This will ensure a unique derivative, making the construction independent of the choice of (sub)gradient oracle.

Towards this goal, we start with the construction depicted so far and consider the set of all values, gradients, and points \(\{f_{j},g_{j},w_{j}\}_{j J}\) that our algorithm may visit, for any possible time step and any possible sample, with our construction. Notice that, while this set may be big and even exponential, it is nevertheless finite. What we want is to interpolate a new function through these triplets. In contrast with our original construction, we require a differentiable function at the designated points. Notice, that such an interpolation will have the exact same behaviour when implementing GD on it (with the added feature that the oracle is well defined and unique).

The problem of convex interpolation is well studied, for example Taylor et al.  shows sufficient and necessary conditions for interpolation of a smooth function. Our case is slightly easier as we do not care about the smoothness parameter. On the other hand we do require Lipschitzness of the interpolation. We therefore provide an elementary, self-contained, proof to the following easy to prove Lemma, (proof is provided in Appendix B)

**Lemma 7**.: _Let \(G=\{f_{j},g_{j},w_{j}\}_{j J}^{d} ^{d}\) be a triplet of values in \(\), and gradients and points in \(^{d}\), such that \(\|g_{j}\| L\). Suppose that for every \(i,j J\):_

\[f_{i} f_{j}+g_{j}^{}(w_{i}-w_{j}),\] (10)

_and let_

\[I_{diff}=\{i:f_{i}=f_{j}+g_{j}^{}(w_{i}-w_{j}) g_{i}=g_{j}\}.\]

_Then there exists a convex \(L\)-Lipschitz function \(\) such that for all \(j J\): \((w_{j})=f_{j}\), and for all \(i I_{diff}\), \(\) is differentiable at \(w_{i}\) and:_

\[ f(w_{i})=g_{i}.\]

With Lemma 7 at hand, consider the function

\[h(w,z)=(w(d+1))^{2}+(z) w(d+1).\]

The above function encodes in \(w(d+1)\) the sample and time-step as before. Moreover, because it is slightly strongly convex (in coordinate \(d+1\)), \(w_{1}(d+1) w_{2}(d+1)\) ensures that

\[h(w_{1},z)>h(w_{2},z)+ h(w_{2},z)^{}(w_{1}-w_{2}),\]

Then the term \(h\) in \(f\) ensures that the triples \(\{f_{j},g_{j},w_{j}\}\) along the trajectory generate gradient vectors that satisfy strict inequality in Eq. (10) and in turn, our interpolation from Lemma 7 will be differentiable at these points. There's some technical subtlety because the interpolation needs to also take the averaged iterate into account, but this is handled in a similar fashion.

In the next two sections we provide more formal statements of the two main ingredients: First, we define a setup of optimization with a sample-dependent first order Oracle and state a lower bound for the generalization error in this setup. The second ingredient is a reduction from the standard setup of first order optimization.

### Sample-dependent Oracle

As discussed, the first step in our proof is to consider a slightly weaker setup where the first-order oracle may depend on the whole sample. Let us formally define what we mean by that. Define

\[_{m}^{T}=\{=(S_{1},,S_{t}),S_{i}_{i=1}^{m} ^{m},t T\},\]

the set of all subseqences of samples of size at most \(m\). Given a function \(f(w,z)\), a sample dependent oracle, \(_{}\), is a finite sequence of first order oracles

\[_{}=\{^{(t)}(S;w,z)\}_{t=1}^{T},\]

that each receive as input a finite sample \(S\), as well as \(w\) and returns a subgradient:

\[^{(t)}(S,w,z) f(w,z).\]The sequence of samples can be thought of as the past samples that were observed by the algorithm. In the case of full-batch GD these will be the whole sample, and for SGD, each \(S\) provided to \(^{(t)}\) will be all previously observed samples. Given \(_{m}^{T}\) let us also denote

\[^{(t)}(,w)=|}_{z S_{t}}^ {(t)}(S_{1:t-1},w,z)(|}_{z S_{t}}f(w,z) ),\] (11)

where we let \(_{1:0}=\), and \(S_{1:t-1}=(S_{1},,S_{t-1})\) is the concatenated subsample of all previously observed samples in the sequence. As discussed, working with a sample-dependent oracle is easier (for lower bounds). And indeed, our first result shows that, if the subgradient can be chosen in a way that depends on the sample, we can provide the desired lower bound. For fixed and known \(>0\), \(T\), a sample dependent first order oracle \(_{S},\) and a sequence of samples \(=(S_{1},S_{2},,S_{T})\), define \(w_{0}=0\) and inductively:

\[w_{t}^{}=[w_{t-1}^{}-^{(t)}( ,w_{t-1}^{})],\]

and for every suffix \(<T\):

\[w_{,}^{GD}=}_{t=+1}^{T }w_{t}^{}\] (12)

**Lemma 8**.: _For every \(m,d,T 18\) and \(>0\) there are a distribution \(D\), a \(3\)-Lipschitz convex function \(f(w,z)\) in \(^{d}\), as well as a sample dependent first order oracle \(_{S}\) such that: if \(=(S,S, S)_{m}^{T}\) for \(S D^{m}\) i.i.d, then w.p. \(1/2\), for every suffix averaging \(\):_

\[F(w_{,}^{GD})-F(0) 272 16^{2}} \{,1\}\{/136,T\}},1\}.\]

The proof of Lemma 8 is provided in Appendix A.1. We next move to describe the second ingredient of our proof.

### Reduction to sample-dependent oracles

As discussed, the second ingredient of our proof is a reduction to the sample-dependent setup. Instead of using a perturbation function as in , we take a more black box approach and show that, given a sample dependent first order oracle, there exists a function that basically induces the same trajectory. Proof is provided in Appendix A.2.

**Lemma 9**.: _Suppose \(q^{T}\), \(\|q\|_{} 1\). And suppose that \(f(w,z)\) is a convex, \(L\)-Lipschitz, function over \(w^{d}\), let \(>0\), let \(_{S}\) be a sample dependent first order oracle, and for every sequence of samples \(=(S_{1},S_{2},,S_{T})\) define the sequence \(\{w_{t}^{}\}_{t=1}^{T}\) as in Eq. (12)._

_Then, for every \(>0\) there exists an \(L+1\) Lipschitz convex function 3\(((w,x),z)\) over \(^{d+1}\) (that depends on \(q,f,T,,m,_{S},\))._

_such that for any first order oracle \(_{z}\) for \(\), define \(u_{0}=0^{d}\) and \(x_{0}=0\), and:_

\[(u_{t},x_{t})=(u_{t-1},x_{t-1})-|}_{z S_{t}}_{z}((u_{t},x_{t}))\]

_then if we define:_

\[u_{q}=_{t=1}^{T}q(t)u_{t}, x_{q}=_{t=1}^{T}q(t)x_{t}, w_{q}^{}=_{t=1}^{T}q(t)w_{t}^{}.\]

_then, we have that \(u_{q}=w_{q}^{}\) and:_

\[|((u_{q},x_{q}),z)-f(w_{q}^{},z)|.\]

_and,_

\[|((0,0),z)-f(0,z)|.\]AcknowledgmentsThe author would like to thank Tamar Livni for creating Figure 1. Tamar holds all copyrights to the artwork. The author would also like to thank Tomer Koren and Yair Carmon for several discussions. This research was funded in part by an ERC grant (FOG, 101116258), as well as an ISF Grant (2188 \(\) 20).