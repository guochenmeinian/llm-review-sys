# Thermodynamic Bayesian Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A fully Bayesian treatment of complicated predictive models (such as deep neural networks) would enable rigorous uncertainty quantification and the automation of higher-level tasks including model selection. However, the intractability of sampling Bayesian posteriors over many parameters inhibits the use of Bayesian methods where they are most needed. Thermodynamic computing has emerged as a paradigm for accelerating operations used in machine learning, such as matrix inversion, and is based on the mapping of Langevin equations to the dynamics of noisy physical systems. Hence, it is natural to consider the implementation of Langevin sampling algorithms on thermodynamic devices. In this work we propose electronic analog devices that sample from Bayesian posteriors by realizing Langevin dynamics physically. Circuit designs are given for sampling the posterior of a Gaussian-Gaussian model and for Bayesian logistic regression, and are validated by simulations. It is shown, under reasonable assumptions, that the time-complexity of sampling the Gaussian-Gaussian posterior is sublinear in dimension. These results highlight the potential to accelerate Bayesian inference with thermodynamic computing.

## 1 Introduction

Bayesian statistics has proved an effective framework for making predictions under uncertainty [1; 2; 3; 4; 5; 6], and it is central to proposals for automating machine learning . Bayesian methods enable uncertainty quantification by incorporating prior knowledge and modeling a distribution over the parameters of interest. Popular machine learning methods that employ this approach include Bayesian linear and non-linear regression , Kalman filters , Thompson sampling , continual learning [10; 11], and Bayesian neural networks [3; 12].

Unfortunately, computing the posterior distribution in these settings is often intractable . Methods such as the Laplace approximation  and variational inference  may be used to approximate the posterior in these cases, however their accuracy struggles for complicated posteriors, such as those of a Bayesian neural network . Regardless, sampling accurately from such posteriors requires enormous computing resources .

Computational bottlenecks in Bayesian inference motivate the need for novel hardware accelerators. Physics-based sampling hardware has been proposed for this purpose, including Ising machines [16; 17; 18; 19; 20], probabilistic bit computers [21; 22; 23], and thermodynamic computers [24; 25; 26; 27; 28; 29]. Continuous-variable hardware is particularly suited to Bayesian inference since continuous distributions are typically used in probabilistic machine learning . However, a rigorous treatment of how such hardware can perform Bayesian inference with scalable circuits has not yet been given.

The most computationally tractable algorithms for exact Bayesian inference are Monte Carlo sampling algorithms. The Langevin sampling algorithm [31; 32] is an elegant example inspired by statistical physics, based on the dynamics of a damped system in contact with a heat bath. What we propose in this work is to build a physical realization of the system that is simulated by the Langevin algorithm. The system must be designed to have a potential energy such that the Gibbs distribution \(p(x) e^{- U(x)}\) is the desired posterior distribution which is reached at thermodynamic equilibrium. We present circuit schematics for electronic implementations of such devices for Bayesian inference for two special cases. The first is a Gaussian-Gaussian model (where the prior and the likelihood are both multivariate normal, as found in linear regression and Kalman filtering), and the second is logistic regression (where the prior is Gaussian and the likelihood is Bernoulli parameterized by a logistic function). In each case, the parameters of the prior and likelihood are encoded in the values of components of the circuit, and then voltages or currents are measured to sample the random variable.

While thermodynamic algorithms have been proposed for linear algebra  and neural network training , our work can be viewed as the first thermodynamic algorithm for sampling from Bayesian posteriors. Moreover, our work provides the first concrete proposal for non-Gaussian sampling with thermodynamic hardware. Overall, our work opens up a new field of rigorous Bayesian inference with thermodynamic computers and lays the groundwork for scalable CMOS-based chips for probabilistic machine learning.

We show that in theory the device proposed for sampling the Gaussian-Gaussian model posterior can obtain \(N\) samples in \(d\) dimensions in time scaling with \(O(N d)\). This is a significant speedup over typical methods used digitally for the same problem, which involve matrix inversions taking time scaling with \(O(d^{})\) where \(2<<3\). This speedup is larger than the polynomial speedups found in previous work on thermodynamic algorithms for linear algebra primitives  (where speedups were found to scale linearly with dimension).

## 2 Results

Suppose that we have samples of a random vector \(y\), and would like to estimate a random vector \(\) on which \(y\) depends somehow. The Bayesian approach is to assume a prior distribution on \(\) given by a density function \(p_{}()\), and a likelihood function \(p_{y|}(y|)\). The posterior distribution for \(\) is then given by Bayes's theorem \(p_{|y}(|y)=p_{y|}(y|)p_{}()/p_{y}(y)\). To sample from the posterior using the Langevin algorithm, one first computes the score

\[_{} p_{|y}(|y)=_{} p_{y|}(y| )+_{} p_{}().\] (1)

Then the score is used as the drift term in the following stochastic differential equation (SDE)

\[d=_{} p_{|y}(|y)\,dt+[0,2\,dt].\] (2)

After this SDE is evolved for a sufficient time \(T\), the value of \(\) will be a sample from \(p_{|y}\). This algorithm is equivalent to the equilibration of an overdamped system, as we will now describe. First let \(r\) be a vector of the same dimension as \(\) describing the state of a physical system, and satisfying \(r=\) for some constant \(\) (this factor is necessary because \(\) is unitless while the physical quantity \(r\) has units). Now we define the potential energy function \( U(r)=- p_{|y}(r/ y)\). The dynamics of an overdamped system with potential energy \(U\) in contact with a heat bath at inverse temperature \(\) can be modeled by the overdamped Langevin equation

\[dr=-^{-1}_{r}U(r)\,dt+[0,2^{-1}^{-1}\,dt],\] (3)

where \(\) is a damping constant. Note that his implies that \(\) has dimensions of \(/[r]^{2}\). If we introduce a constant \(=^{2}\), Eq. (3) can be written

\[d=_{} p_{|y}(|y)^{-1}\,dt+[0, 2\,^{-1}dt],\] (4)

which has the same form as Eq. (2), except with the time constant \(\). It is clear that if Eq. (2) must be run for a dimensionless duration \(T\) to achieve convergence, then the physical system must be allowed to evolve for a physical time duration \( T\) to achieve the same result. While we have addressed the case of conditioning on a single sample \(y\) above, the generalization of these ideas to the case of conditioning on multiple I.I.D. samples is given in Appendix D. In what follows we will present designs for circuits whose potential energy results in an overdamped Langevin equation that yields samples from Bayesian posteriors.

### Gaussian-Gaussian model

A particularly simple special case of Bayesian inference is a when both the prior and the likelihood are multivariate normal, and we address this simple model first in order to illustrate our approach more clearly. Specifically, let \(^{d}\) have prior distribution \(p_{}()=[,]\), and let the likelihood be \(p_{y|}(y|)=[,_{y|}]\), where \(y^{d}\) is an observed sample. In this case the posterior \(p_{|y}\) is also multivariate normal, with parameters 

\[_{|y}=+(+_{y|})^{-1}(y-),\] (5)

\[_{|y}=-(+_{|y})^{-1}.\] (6)

For this model, the posterior is tractable and can be computed on digital computers relatively efficiently, however for very large dimensions the necessary matrix inversion and matrix-matrix multiplications can still create a costly computational bottleneck. As we will see, the thermodynamic approach provides a means to avoid the costly inversion and matrix products in the computation, and therefore to accelerate Bayesian inference for this model.

We begin by deriving the Langevin equation for sampling this posterior. For this prior and likelihood, the score of the posterior Eq. (1) is

\[_{} p_{|y}(|y)=-^{-1}(-)-_{y| }^{-1}(-y),\] (7)

and so Eq. (4) becomes

\[d=-^{-1}(-)^{-1}dt-_{y|}^{-1}(-y) ^{-1}dt+[0,2^{-1}dt].\] (8)

In fact, this SDE can be implemented by a circuit consisting of two resistor networks coupled by inductors, shown in Fig. 1 for the two-dimensional case.

The full analysis of the circuit in Fig. 1 is given in Appendix A, but a few remarks are made here to explain its operation. First, we define the conductance matrices \(\) as

\[=R_{11}^{-1}+R_{12}^{-1}&-R_{12}^{-1}\\ -R_{12}^{-1}&R_{22}+R_{12}^{-1},\] (9)

and \(^{}\) is defined in the same way for the primed resistors \(R_{1}^{}\), \(R_{2}^{}\), and \(R_{12}^{}\). By applying Kirchoff's current law (KCL), the voltages across the resistors can be eliminated. Then the equation \(V=L\) is used to derive the following stochastic differential equation for the currents through the inductors

\[dI_{L}=-L^{-1}^{-1}(I_{L}-I)\,dt-L^{-1}^{-1}(I_{L }-I^{})\,dt+L^{-1}[0,\,dt],\] (10)

where \(I_{L}=(I_{L1}\;\;I_{L2})^{}\) and \(S\) is the power spectral density of each noise source. This equation has the same form as Eq. (8), so it is only necessary to determine an appropriate mapping of distributional parameters to physical properties of the circuit's components (see Appendix A). By

Figure 1: Circuit schematic for the Gaussian-Gaussian model posterior sampling device.

including more inductors and coupling resistors (as well as current and voltage sources), the design can be generalized to arbitrary dimension.

To verify that the proposed circuit does indeed evolve according to the correct SDE, we ran SPICE circuit simulations. Figure 2 shows the results of such a simulation where a 2-dimensional Gaussian prior and a 2-dimensional Gaussian likelihood are encoded into the conductances while the current in each inductor is measured to determine the resulting posterior.

As shown in Appendix C, the asymptotic runtime complexity for this algorithm is

\[t=O(N(^{3/2}d^{1/2}W_{0}^{-1})),\] (11)

where \(\) is the condition number of the posterior covariance, \(=L/\), \(d\) is the dimension, and \(W_{0}\) is the Wasserstein distance between the true posterior and the distribution sampled by the device. The assumptions used to derive this result can also be found in Appendix C. Remarkably, the required time is sublinear in dimension, a large improvement over digital algorithms where complexity of constructing and sampling from the Gaussian-Gaussian posterior (67 - 68) is \(O(d^{})\) where \(\) is the matrix multiplication constant (or more practically \(O(d^{3})\) via common implementations of Cholesky factorization). In Figure 3(a), we report the convergence of simulated thermodynamic samples for the Gaussian-Gaussian model with zero prior mean and covariances \(\), \(_{y|}\) randomly sampled from a Wishart distribution with \(2d\) degrees of freedom. We see fast convergence in Wasserstein distance to the true posterior, supporting our theoretical claims.

### Bayesian linear regression and Kalman filtering

A generalization of the Gaussian-Gaussian model is that of Bayesian linear regression  (or equivalently a Kalman filter update step ). In full generality we have

\[p_{}() =[,],\] (12) \[p_{y|}(y) =[H,_{y|}],\] (13)

Then the overdamped Langevin SDE becomes

\[d =-^{-1}(-)^{-1}\,dt-H^{}_{y| }^{-1}(y-H)^{-1}\,dt+[0,2^{-1}dt],\] \[=-(A-b)^{-1}\,+[0,2^{-1}dt], A=^{-1}+H^{}_{y|}^{-1}Hb=+H^{}_{y|}^{-1}y.\] (14)

The form of the SDE (Ornstein-Unhlenbeck process) in 14 is exactly that of the thermodynamic device in  which if given input \(A\) and \(b\) above will produce samples from the Gaussian Bayesian posterior \(p_{|y}( y)\). Compared to the simpler Gaussian-Gaussian model above, a disadvantage of this approach is thathe covariances \(\) and \(_{y|}\) have to be inverted prior to input as \(A\). However, for linear regression, these matrices are often assumed to be diagonal and otherwise they can be

Figure 2: SPICE simulations of proposed Gaussian-Gaussian circuit in Fig. 1. The grey points represent the simulated circuit’s induct currents. The dashed black and solid blue ellipses represent the empirical sample covariance and the target posterior covariance from a Gaussian Bayesian update, respectively. The red and green ellipses represent the prior and likelihood.

efficiently inverted using the thermodynamic procedures in  as preprocessing. Additionally, the formulation of \(A\) requires matrix-matrix multiplications which can be costly (even in the case of diagonal covariances). Although, this can be accelerated with parallelization.

On the other hand, the generality of (12-13) makes the approach highly practical. Encompassing Bayesian linear regression  and the update step of the Kalman filter . Moreover in the setting of Kalman filtering, the matrices \(\) and \(_{y|}\) are typically shared across time points and thus only need to be inverted once in comparison to the Bayesian posterior update which is applied at every time step (and typically represents the computation bottleneck due to the required matrix inversion).

In Figure 3(b), we simulate the evaluation of the thermodynamic linear algebra device  for a Bayesian linear regression task. We use the diabetes dataset  which has \(N=442\) continuous response variables \(y\) and 10 input features. We vary the number of features and therefore posterior dimension for the linear regression by extending to include the first \(d\) cross terms in the Taylor expansion over the input features. These input features are loaded as rows in the matrix \(H^{N d}\). Both covariances are set to diagonal, \(=\) and \(_{y|}=0.1\). We observe that the Wasserstein distance converges quickly as more samples are collected and scales reasonably with dimension, indicating a sublinear scaling similar to the Gaussian-Gaussian model.

### Bayesian logistic regression

Logistic regression is a method for classification tasks (both binary and multiclass) that models the dependence of class probabilities on independent variables using a logistic function. In the Bayesian setting, a prior can be assumed on the parameters of a logistic regression model, for example it is common to assume a Gaussian prior. However, after conditioning on observed data a posterior distribution is produced that has no analytical closed form, making Bayesian logistic regression far less efficient than obtaining a point estimate of the parameters. In this section we present a thermodynamic hardware architecture capable of sampling the posterior for binary logistic regression, and show some preliminary evidence that this architecture can do so more efficiently than existing methods.

Given a parameter vector \(^{d}\) and an independent variable vector \(x^{d}\), binary logistic regression outputs a class probability \(p_{y|,x}(y|,x)\), where \(y\{-1,1\}\) (often \(y\{0,1\}\) is written instead but we choose this notation to simplify the presentation). The likelihood is \(p_{y|,x}(y|,x)\ =\ L(y^{}x)\) where \(L(z)=1/(1+e^{-z})\) is the standard logistic function . Note that we will first consider the case of conditioning on a single sample, and in this case the likelihood will be denoted \(p_{y|}(y|)\) as \(x\) is constant. Additionally, a multivariate normal prior is assumed for the parameters \([,]\). The Langevin equation for sampling the posterior is therefore:

\[d=-^{-1}(-)^{-1}dt+L(-y^{}x)yx^{ -1}dt+[0,2^{-1}dt].\] (15)

Figure 3: Convergence in Wasserstein distance between simulated thermodynamic samples and the true Gaussian posterior as a function of the number of samples (sampling time). All results are simulated exactly with thermox and averaged over 10 random seeds with one standard deviation shown. Panel (a): Gaussian-Gaussian model with zero prior mean and covariances sampled from a Wishart distribution. Panel (b): Bayesian linear regression on the diabetes dataset  with dimension (number of features) varied by including higher-order cross terms of the 10 input data features.

A circuit implementing Eq. (15) is shown in Fig. 7, and the detailed analysis of this circuit is given in Appendix B. Equation (15) is valid for a single data sample, however, as mentioned, in practice we generally take gradients over a larger number of examples such that the gradients are less noisy. This can be done by enlarging the hardware, resulting in the second term of Eq. (15) being replaced by a sum \(_{i=1}^{N}L(-y_{i}^{}x_{i})y_{i}x_{i}dt\), with \(N\) the number of data points. One may also consider minibatches, and the sum is only over a batch of size \(b\). This is achievable by summing currents, which is detailed in the circuit implementation in Appendix B. At a high-level, implementing this protocol in hardware is very simple in the case of a full batch, since the data only needs to be sent once onto the hardware. The following steps are taken to collect the samples: (1) Map the data labels to \(\{+1,-1\}\). (2) Map the data \((,Y)\) onto the hardware (full batch setting). (3) Initialize the state of the system, set the mean and the covariance matrix of the prior. (4) At every interval \(t_{s}\) (the sampling time), measure the state of the system \((t)\) to collect samples.

In Fig. 4, we present results for a Bayesian logistic regression on a two-moons dataset, made of points separated in two classes that are arranged in intersecting moons in the 2D planes, as shown in Fig. 4(a). These results are obtained by running the SDE of Eq. (15), hence corresponds to an ideal simulation of the thermodynamic hardware. In this scenario, there are 3 parameters to sample, and \(N=100\) points are considered. In Fig. 4(a), we see that even for such a simple model, only a few points are missclassified. As mentioned, previously, this setting also gives access to better methods to estimate uncertainty in predictions. In Fig. 4(b), the Kernel Stein discrepancy (KSD)  is shown as a function of the number of collected samples for varying sampling rates. These results indicate that the number of samples to reach a low KSD (close to convergence) can be reduced by increasing the sampling time, indicating correlated samples, as is often the case.

## 3 Conclusion

In this work, we proposed the first thermodynamic algorithms for sampling from Bayesian posteriors. We provided explicit constructions of CMOS-compatible analog circuits to implement these algorithms with scalable silicon chips. Our circuit for performing logistic regression represents the first concrete proposal for non-Gaussian sampling with a thermodynamic computer. In the case of Gaussian Bayesian inference (Gaussian prior, Gaussian likelihood), our analysis showed a sublinear complexity in \(d\), leading to a speedup over standard digital methods that is greater than linear. This is an even larger speedup than those previously observed for thermodynamic linear algebra , suggesting that Bayesian inference is an ideal application for thermodynamic computers. Our work lays the foundation for accelerating Bayesian inference, a key component of probabilistic machine learning, with physics-based hardware.

Figure 4: Panel (a): Probability surface to belong to class 1 (blue points). The dataset is also shown, where class 0 (blue points) and class 2 (orange points) are arranged in two intersecting moons. Panel (b): Kernel Stein discrepancy (KSD) of the collected samples with an ideal thermodynamic sampler, for varying sampling times. The sampling time is given in units of \(10^{-3}\). The KSD is averaged over five sets of random samples and \(=1\).