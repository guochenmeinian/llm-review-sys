# Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass

Ethan Shen\({}^{}\) Alan Fan\({}^{}\) Sarah Pratt\({}^{}\) Jae Sung Park\({}^{}\) Matthew Wallingford\({}^{}\)

Sham Kakade\({}^{}\) Ari Holtzman\({}^{}\) Ranjay Krishna\({}^{}\) Ali Farhadi\({}^{}\) Aditya Kusupati\({}^{}\)

\({}^{}\)University of Washington Harvard University University University of Chicago

{ethans03, kusupati}@cs.washington.edu

AK is currently at Google DeepMind

###### Abstract

Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing \(k\) drafts to the user requires running an expensive language model \(k\) times. To alleviate the computation cost of running \(k\) inference passes, we propose Superposed Decoding, a new decoding algorithm that generates \(k\) drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the \(k\) drafts as input to the next decoding step of the language model. At every inference step we combine the \(k\) drafts with the top-\(k\) tokens to get \(k^{2}\) new drafts and cache the \(k\) most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that \(k\) drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least \(2.44\) faster for \(k 3\). In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.

## 1 Introduction

Commercial surveys find that 80% of e-commerce websites provide autocomplete as a feature . With the proliferation of large language models, autocomplete drafts are now ubiquitous in an even wider range of applications. Examples include short draft suggestions in Gmail Smart Compose  and code snippets in GitHub Copilot . These options provide users with the ability to consider different phrasings and increase the likelihood of having at least one suggestion that reflects their intent. While language models (LMs)  now power these multiple suggestions, each additional suggestion necessitates another inference pass (batch size \(=1\)), making it computationally expensive.

Language models use autoregressive inference to generate a plausible sequence of next tokens for a given prefix . The next token generated depends on the prefix and the previously generated tokens. Decoding algorithms like Greedy Decoding, Top-\(k\) Sampling , Beam Search, and Nucleus Sampling  present various ways of obtaining generations during autoregressive inference. For a prefix, Greedy (maximum likelihood) Decoding picks the most probable token at every autoregressive timestep, eventually generating only one auto-completion suggestion. Instead of making a greedychoice at every timestep, we can also sample from the top-\(k\) most probable tokens to generate the sequence . Also leveraging the top-\(k\) most probable tokens is Beam Search, which picks the most probable \(k\) beams from the \(k^{2}\) possible drafts at every timestep. Alternatively, Nucleus Sampling , particularly effective at generating natural-sounding text, grows generations by sampling from the collection of the smallest subset of tokens that form a preset probability mass (\(p\)) at every timestep. Top-\(k\) Sampling, Beam Search, and Nucleus Sampling offer the benefit of generating multiple suggestions. However, this comes at the cost of requiring multiple autoregressive inference passes.

We introduce Superposed Decoding (SPD) (Figure 1), a decoding algorithm that can generate mutiple (\(k\)) high-quality short generations using only a _single_ autoregressive inference pass. At each autoregressive timestep during inference, Superposed Decoding feeds in the superposition (weighted combination) of the embeddings corresponding to the \(k\) most recent drafted tokens (Section 3.1). After selecting the top-\(k\) output tokens, SPD expands the existing \(k\) drafts with them, resulting in \(k^{2}\) potential new drafts. Each draft has a probability score assigned to it, which is further smoothed using a combination of various \(n\)-gram models (\(n\)) . N-gram interpolation for smoothing helps improve coherency by selecting the most probable and coherent continuations (top-\(k\) drafts) for the next autoregressive step (Section 3.2). The n-gram interpolation is computationally inexpensive and allows flexibility to change domains of interest during inference (eg., programming, healthcare, finance, etc.). Superposed Decoding's effectiveness can be attributed to the apparent linearity of representations in language models  which we concurrently discovered (Section 3.3).

Our experiments demonstrate that Superposed Decoding on Llama-2-7B  can generate \(k 1\) drafts that are as coherent, in terms of perplexity, as Nucleus Sampling (Section 4.1). However, SPD achieves this with a single inference pass, making it at least \(2.44\) faster than any other standard decoding methods for \(k 3\) (Section 4.3). The ability to generate multiple drafts at the cost of one also increases the coverage significantly for fact-based evaluation tasks like TriviaQA and Natural questions - where SPD increases the chance of generating the correct answer by at least \(5\%\) when using 3 drafts (Section 4.2). Through an extensive human evaluation for a wide range of prefixes, we show that SPD generations are as preferred as Nucleus Sampling in direct comparison (1v1) while outperforming by up to \(20\%\) in a compute normalized setting (3v1 and 3v2) (Section 4.4). Finally, we find that combining Superposed Decoding and other decoding strategies (e.g. Nucleus Sampling) results in substantial accuracy improvements when scaling inference compute (Section 4.5).

Superposed Decoding can help improve user experience by offering significant computational efficiency while maintaining accuracy for various writing tasks that often benefit from multiple short draft suggestions. Additionally, Superposed Decoding is extremely generalizable, works across different language models like Mistral 7B, and is capable of generating long-form content reliably, all while being nearly as diverse in suggestion as Nucleus Sampling (Section 5).

Figure 1: To generate multiple \(k\) auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need \(k\) inference passes. In contrast, Superposed Decoding can generate \(k\) suggestions at the cost of a single inference pass while being as coherent and factual.

Related Work

Decoding algorithms determine how tokens are selected from a language model's next token distribution. This is typically done greedily with Beam Search or Greedy Decoding, or stochastically with Nucleus Sampling  or Top-\(k\) Sampling . Greedy Decoding operates by selecting the token with the highest probability at each timestep. However, locally optimal decisions can be globally suboptimal. On the other extreme, an exhaustive exploration of all token combinations is intractable with time complexity \((||^{T})\), where \(\) is the vocabulary size and \(T\) the total number of timesteps. A practical compromise is Beam Search, which continually caches the top-\(B\) most likely generations, where \(B\) is the number of beams or drafts. Thus, Beam Search is guaranteed to find more likely generations than Greedy Decoding while avoiding its drawbacks.

Alternatively, Nucleus Sampling and Top-\(k\) Sampling decode probabilistically. To avoid degeneration, Top-\(k\) Sampling truncates the probability mass to the \(k\) most probable tokens, whereas Nucleus Sampling truncates the probability mass to the smallest possible set of words whose cumulative probability exceeds probability \(p\). Because of Nucleus Sampling's propensity to produce unique and unrepetitive outputs, it has become the standard [4; 22; 42], though Greedy Decoding is still favored for tasks such as short question answering .

Generating multiple drafts linearly scales the number of inference passes in these decoding methods. Multi-token prediction [14; 36] addresses this by pre-training an LM having \(n\) independent softmax heads that predict \(n\) future tokens in parallel. When using multi-token prediction with speculative decoding, exact inference is \(3\) faster. In a similar vein, Medusa  reduces the number of decoding steps by adding extra decoding heads, using a tree-based attention mechanism to generate candidates while simultaneously verifying them in each decoding step. Through this, Medusa achieves a \(2.2\) reduction in inference latency while maintaining generation quality. However, multi-token prediction requires re-training and Medusa requires additional fine-tuning for the extra decoding heads.

Superposed Decoding (SPD), on the other hand, can be easily integrated **out of the box** without any additional training on any language model. Further, Superposed Decoding is complementary to other decoding methods like Medusa and multi-token prediction, as well as efficiency techniques like speculative decoding [30; 26; 6], quantization [10; 32; 19], pruning [13; 39; 28], and architectural optimizations [37; 2; 1; 43; 27; 11].

## 3 Superposed Decoding (SPD)

Given a text input as a prefix, Superposed Decoding uses an autoregressive LM \(f_{}\) to produce \(k\) viable completion drafts in one inference pass.

First Timestep.Let \(x\) denote a generated token in the vocabulary \(\) and \(M=(x_{1},,x_{m})\) represent an initial prefix of \(m\) tokens. Our goal is to generate \(k\) unique drafts starting from the prefix. The next token distribution at the first timestep \(m+1\) is: \(p_{}(x_{m+1}|x_{1}, x_{m})=p_{}(x_{m+1}|M)\).

For the first timestep, each of the \(k\) drafts is initialized as the prefix so we use the same next token distribution for all drafts. We grow draft \(d_{i}\) by greedily selecting the \(i^{}\) most probable token, making:

\[d_{i}=(M,x_{m+1}^{i})\]

where \(x_{i}^{i}\) is the token at timestep \(t\) for the \(i^{}\) draft. We also track each draft's probability \(p_{i}\) as its score, which is initially the probability of its first token \(p_{}(x_{m+1}^{i}|M)\). Consequently, the set of drafts \(D\) after the first timestep is:

\[D=(M,x_{m+1}^{1})\\ \\ (M,x_{m+1}^{k})P=p_{ }(x_{m+1}^{1}|M)\\ \\ p_{}(x_{m+1}^{k}|M)\] (1)

Next Timesteps.As the input to the LM at timestep \(t\), we construct \(_{t-1}\), which is a superposition (weighted linear combination) of the embeddings for the most recent token \(x_{t-1}\) of the \(k\) drafts. This means that \(_{m+1}\), the input to the LM at the second timestep \(m+2\), is the superposition of the tokens \(x_{m+1}^{i}\) for \(i=1,,k\). We use this _superposed embedding_ as a single and accurate approximation for the most recent token across all \(k\) drafts at the \((t-1)^{}\) timestep. We run autoregressive inference over \(_{t}\) once for all \(k\) drafts instead of the usual once per draft, allowing us to formulate inference as:

\[p_{}(x_{t}|M,_{m+1},,_{t-1})\]

Because each draft contains its own contextual clues and syntax, we cannot blindly use the distribution of \(x_{t}\) for each draft. Instead, we interpolate the superposed distribution \(p_{}(x_{t}|M,_{m+1:t-1})\) with a draft-specific next token distribution from an n-gram model to get a final distribution \(p_{f}(x_{t}^{i}|M,x_{m+1:t-1})\) that is unique to each draft. Next, we rank the draft options by the joint probability of their respective previous draft \(p_{i}\) and their selected token. We choose the top \(k\) options as drafts for the next timestep and update their probabilities. This gives:

\[D=(M,x_{m+1}^{1},,x_{t}^{1})\\ \\ (M,x_{m+1}^{k},,x_{t}^{k})P= p_{}(x_{m+1}^{1}|M)_{s=m+2}^{t}p_{f}(x_{s}^{1}|M,x _{m+1:s-1}^{1})\\ \\ p_{}(x_{m+1}^{k}|M)_{s=m+2}^{t}p_{f}(x_{s}^{k}|M,x_{m+1:s-1}^{k}) \] (2)

We continue generation until the maximum generation length or stop token is reached. In the following sections, we break down the process in detail. We also present pseudocode in Appendix A.

### Token Superposition

During training, language models learn a representation (token embedding) \(z^{d}\) for every token \(x\). We leverage this representation at timestep \(t\) to construct \(_{t}\), weighing the representation for \(x_{t-1}^{i}\) by a coefficient \(_{i}\).

\[_{t}=_{i=1}^{k}_{i} z_{t-1}^{i}_{i=1}^{k}_{i}=1\] (3)

The performance of \(_{t}\) is highly dependent on choosing the appropriate weight for each embedding \(z_{t-1}^{i}\). We find that the best strategy is to set \(_{i}\) to the normalized probability of draft \(i\) such that

\[_{i}=}{_{j=1}^{k}p_{j}}\] (4)

where \(p_{i}\) is the probability of the \(i^{}\) draft, introduced in Section 3. This allows us to directly tie the weight of a token to the likelihood that it will be preserved in future timesteps, reducing drift between the superposed embeddings and the drafts they represent.

### N-Gram Interpolation

We construct each draft's n-gram distribution \(p_{}(x_{t}^{i}|M,x_{m+1}^{i},,x_{t-1}^{i})\) by interpolatively smoothing the next token distributions over a set of \(n\)-gram models using weights \(\), where \(n\).

Figure 2: Superposed Decoding relies on feeding a superposed token embedding – based on the most recent tokens from the current \(k\) drafts – as the input during the auto-regressive inference step. This generates \(k^{2}\) new drafts using the existing \(k\) drafts and the top-\(k\) output tokens at the new timestep. Finally, keep the top-\(k\) drafts after filtering with an n-gram interpolation to improve coherency.

\[p_{}}(x_{t}^{i}|M,x_{m+1:t-1}^{i},,x_{t-1}^{i})=_{n=2}^{ 6}_{n} p_{n}}(x_{t}^{i}|M,x_{m+1}^{i},,x_{t-1 }^{i})\] (5)

We base our interpolation weights on weights for RedPajama found by Liu et al. , with additional tuning (specifics in Appendix B). However, domain-specific n-gram models can easily be swapped in for specific tasks such as code generation . We use the exponential mean of \(p_{}\) and \(p_{}}\) as our final distribution \(p_{f}\), where \(\) is a hyperparameter controlling the impact of the n-gram distribution:

\[p_{f}(x_{t}^{i}|M,x_{m+1:t-1}^{i})=p_{}(x_{t}|M,_{m+1:t-1})^{1- } p_{}}(x_{t}^{i}|M,x_{m+1:t-1}^{i})^{}\] (6)

This means that when generating, we only consider tokens appearing in both the Superposed Decoding and n-gram distributions. If there is no overlap between the distributions, then we instead calculate

\[p_{f}(x_{t}^{i}|M,x_{m+1:t-1}^{i})= p_{}(x_{t}|M,_{m +1:t-1})^{1-}\] (7)

without interpolation, where \(\) is a penalty term that disincentivizes selecting an uninterpolated draft for the next timestep. This approach is the backbone of Superposed Decoding (Equation 2) and allows us to create context-aware next-token distributions for _each_ draft with only _one_ inference pass.

### Superposed Decoding Semantically Approximates Beam Search

Superposed Decoding relies on the ability of the underlying model to preserve the linear relationship between \(_{t}\) and its component vectors \(z_{t-1}^{i}\). More formally, if \(_{t}\) is the input to the LM \(f_{}\), then \(f_{}(_{t})=_{i=1}^{k}_{i} f_{}(z_{t-1}^{i})\) should also be true (\(_{i}\) defaults to draft probability in SPD). As long as this holds, the combination of a superposed embedding and n-gram smoothing should allow us to generate completions that resemble those from methods such as Beam Search.

We test this linearity by computing the cosine similarity between a set of superposed embeddings \(\{\}\) and the linear combination of their component embeddings across 20 timesteps on Llama-2.7B. At each timestep, we first use Beam Search to generate tokens for three beams. We then manually input the superposed embedding of the three tokens into a model using Superposed Decoding. Finally, we measure the alignment between \(f_{}(_{t})\) and \(_{i=1}^{k}_{i} f_{}(z_{t-1}^{i})\) using cosine similarity \((a,b)\) as:

\[(f_{}(_{t}),_{i=1}^{k}_{i} f_{}(z_{t- 1}^{i}))\] (8)

We compute the cosine similarities for ten randomly sampled batches of ten prefixes each from the OpenWebText training split and plot the mean cosine similarities across batches, as well as the standard deviation (Figure 3). We find that Llama-2.7B is sufficiently linear up to 10 timesteps across all layers. However, this linearity is imperfect, and Superposed Decoding and Beam Search eventually diverge over time. Owing to this, we identify 10 timesteps as the optimal generation length. We also show how linearity changes through the layers within each timestep in Appendix H.

## 4 Experiments

We evaluate Superposed Decoding by analyzing generation quality, factuality, and latency. We demonstrate that Superposed Decoding improves over Nucleus Sampling and Greedy Decoding

Figure 3: Llama-2.7B maintains the linear relationship between superposed embeddings and the component token embeddings, with mean cosine similarity \( 0.6\) for the first 10 timesteps.

in generation perplexity (Section 4.1), fact-based benchmarks (Section 4.2), and wall-clock time (Section 4.3). We also conduct a user study highlighting that users prefer Superposed Decoding over Nucleus Sampling in a compute-normalized setting (Section 4.4). Finally, we show that Superposed Decoding improves performance while scaling inference compute (Section 4.5). We include example generations of Superposed Decoding in Figure 4, with more in Appendix G.1.

We implement Superposed Decoding on the base version of Llama-2-7B  for the majority of our experiments, running on one A40 GPU. We do not change model weights. For perplexity evaluations, we use Llama-2-70B on eight A40 GPUs. We assume a batch size of one for all experiments.

For n-gram interpolation, we construct n-gram models using 200,000 documents (roughly 200 million tokens) randomly sampled from the RedPajama dataset, an open-source replication of Llama-2's training dataset . We represent each n-gram model internally as a frequency table, storing each unique n-gram and its corresponding count. This enables faster lookup and is the basis behind the compute reduction that we offer. Our n-gram models require approximately 14 GB of disk storage overall, split \(57\) MB, \(433\) MB, \(2.15\) GB, \(4.7\) GB, and \(6.8\) GB for \(n=2\) to \(6\). While we interpolate up to \(n=6\) in this work, we note that in practice the benefits of n-gram interpolation saturate past \(n=4\), suggesting that the number of n-gram models can be lowered to reduce storage (Appendix C).

### Generation Quality

We test generation quality on OpenWebText's test split , which consists of 5,000 web-scraped documents. For each document, we use the first 15 tokens as the prefix and generate \(k=3\) drafts of 10 tokens with a batch size of \(1\). We decide to focus on short generations because drafts are typically a short-form task. Before running experiments, we identify the optimal interpolation weight \(\) and temperature \(\) by optimizing for the lowest average perplexity across three drafts on the validation split. We list the specific hyperparameter values that we use in Appendix B.

We only evaluate one draft of the baselines in order to match the compute used by Superposed Decoding. We find that while Nucleus Sampling and Greedy Decoding outperform Superposed Decoding on a per-draft basis, the average best perplexity from Superposed Decoding is \(\%\)**lower** than that of Nucleus Sampling. From the point of view of a user, this means for each prefix, at least one draft can be expected to be on par with Nucleus Sampling and Greedy Decoding. The other drafts all come free of additional compute. In the following Sections 4.2 and 4.4, we show that this diversity is beneficial for both factuality and human preference.

### Fact-Based Evaluation

Next, we test the ability of Superposed Decoding to generate not only coherent but also accurate completions. We assess this using exact match precision (P@\(k\)) for \(k=1,2,3\) on TriviaQA  and Natural Questions , two common benchmarks for short answer generations. We decide not to

    & Nucleus & Beam/Greedy & N-Gram &  \\  Draft \# & - & - & - & 1 & 2 & 3 & Best \\  Avg Perplexity & 5.17 & 3.77 & 152.75 & 5.03 & 7.97 & 10.05 & 4.63 \\   

Table 1: Superposed Decoding is natural-sounding and has lower average perplexity than Nucleus Sampling, which typically approximates human writing.

Figure 4: Qualitative text generations in a compute-normalized setting for Superposed Decoding and Nucleus Sampling with prefixes sampled from OpenWebText. See Appendix G.1 for more.

use multiple choice datasets such as MMLU  and OpenBookQA  because multiple choice questions are trivial when using multiple drafts, unfairly advantaging Superposed Decoding.

In Figure 5, we show that Superposed Decoding outperforms Nucleus Sampling and Beam Search at P@1 in a zero-shot setting, with three drafts providing accuracy gains of up to \(2.72\%\) in TriviaQA and \(1.69\%\) in Natural Questions. These results demonstrate that Superposed Decoding substantially increases the likelihood of getting a factually correct generation in addition to one that is coherent.

### Latency

Superposed Decoding presents a significant _theoretical_ reduction in latency, but it is important to investigate how well this translates to the real-world. In Figure 6, we show that dictionary lookups are the only additional cost incurred by Superposed Decoding, barring which Superposed Decoding has near-constant compute cost. Even so, the total cost of Superposed Decoding is significantly lower than other decoding methods, with Superposed Decoding \(\) faster on three drafts and \(\) faster on eight compared to Nucleus Sampling, the next fastest.

It is important to note that Superposed Decoding results are obtained using unoptimized code. Our n-gram models are implemented using single-threaded lookup on Python dictionaries, and we do not cache any lookup results. This has an enormous, visible impact. In addition, Liu et al.  propose the use of suffix arrays for n-gram models, allowing a near-instantaneous lookup of arbitrary-length n-grams in trillion-token corpora. These techniques open up multiple avenues for additional speedup.

Figure 5: Superposed Decoding is as accurate as Greedy Decoding for P@1 and increases the fact-based coverage using multiple drafts (P@2,3) on TriviaQA (**left**) and Natural Questions (**right**).

Figure 6: Average wall clock time to generate 10 token long drafts, with batch size \(=1\), from a 15 token prefix on OpenWebText. Superposed Decoding is significantly faster for all \(k>1\), with n-gram lookup costs being a major factor for \(k 4\), which can be optimized further.

Figure 7: Drafts are ordered by the probability they obtain during generation using SPD, which wins over Nucleus Sampling in a compute-normalized setting.

### Human Evaluation

While perplexity and factuality are good proxies for textual quality, real human evaluation remains the best measure of coherency. Indeed, perplexity is an imperfect evaluator of long-form language modelling  while factuality metrics are inherently biased towards Greedy Decoding methods . Consequently, to verify our findings, we conduct a random study asking human respondents to rank Superposed Decoding and Nucleus Sampling generations based on how well they complete a provided prefix. We compare against Nucleus Sampling because n-gram distributions from Nucleus Sampling are demonstrably the closest to those of humans .

**Setup.** We conduct our study using Amazon Mechanical Turk . First, we randomly sample 1,000 prefixes from the OpenWebText test set, truncating to the first 15 tokens as in Section 4.1. Next, we manually remove prefixes that are grammatically incorrect, such as text from website toolbars. From the remaining prefixes, we generate three Superposed Decoding drafts and one Nucleus Sampling draft in a compute-normalized setting, randomizing their order. Finally, we filter out prefixes with duplicate or unparseable generations (e.g. emojis). After preprocessing, 707 prefixes are left.

It is important to note that one Nucleus Sampling generation is \(20\%\) less expensive than three Superposed Decoding drafts, shown in Section 4.3. However, two Nucleus Sampling drafts disadvantages three Superposed Decoding drafts by \(60\%\). Therefore, we decide to run our main survey using the first setup, which is closest to equal compute, but also conduct smaller surveys in 2v3 and 1v1 settings.

**Results.** We define a Superposed Decoding "win" as when one of the Superposed Decoding drafts is ranked first. As shown in Figure 7, we find that Superposed Decoding generations are preferred approximately \(\%\) of the time. If every Superposed Decoding draft was consistently the same quality as Nucleus Sampling, then we would expect the rankings to be uniform, resulting in a \(75\%\) win rate. However, this is not the case, suggesting that Nucleus Sampling is more reliable than any _individual_ draft, but the _aggregate_ of drafts provides a diversity that is highly valuable to users.

We run the two smaller-scale iterations of the study with 100 prefixes each. In the 2v3 setting, we ask users to rank two nucleus drafts and three superposed drafts to investigate whether the benefits of Superposed Decoding persist even when compute favors Nucleus Sampling. In the 1v1 setting, users choose between one nucleus draft and the lowest perplexity superposed draft, removing any potential bias caused by unequal numbers of drafts. As shown in Figure 12, in both situations, users still prefer Superposed Decoding over Nucleus Sampling \(\%\) and \(\%\) of the time respectively. While the surveys have higher variance due to their small size, they cement Superposed Decoding as a strong alternative to Nucleus Sampling. We show details on the additional studies in Appendix F.1 and present our survey page in Figure 13.

### Inference-Time Scaling

Superposed Decoding also provides significant benefits for inference time compute scaling. Superposed Decoding is completely complimentary to other decoding methods, expanding semantic coverage at no extra compute. For instance, if Nucleus Sampling is used to generate \(n\) drafts, Superposed Decoding with \(k\) drafts can be spliced in at any timestep to strategically produce \(nk\) drafts

    &  &  \\  & & 1 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 100 \\   & \(NS\) & 51.04 & 68.75 & 70.31 & 71.87 & 72.92 & 74.48 & 74.74 & 75.26 & 75.78 & 76.30 & 76.56 \\  & \(NS_{SPD2}\) & 51.30 & 68.75 & 70.57 & 72.66 & 74.74 & 75.78 & 76.30 & 76.82 & 78.39 & 79.17 & 79.43 \\  & \(NS_{SPD3}\) & **51.82** & **70.57** & **74.22** & **75.52** & **77.34** & **77.87** & **78.39** & **78.65** & **79.17** & **79.43** & **79.95** \\   & \(NS\) & 14.32 & **32.55** & **36.98** & 38.54 & 40.36 & 41.15 & 41.67 & 41.93 & 42.19 & 42.71 & 42.97 \\  & \(NS_{SPD2}\) & 15.36 & 31.25 & 34.90 & 38.02 & 39.84 & 41.41 & 41.67 & 42.45 & 43.75 & 43.75 & 43.75 \\   & \(NS_{SPD3}\) & **15.63** & 31.25 & **36.98** & **39.06** & **41.15** & **42.71** & **43.75** & **43.75** & **44.27** & **44.79** & **45.57** \\   

Table 2: Coverage on TriviaQA and Natural Questions in a normalized compute setting comparing vanilla Nucleus Sampling (\(NS\)) to the combination of Nucleus Sampling and Superposed Decoding (\(NS_{SPDk}\)), where \(k\) is the number of Superposed Decoding drafts generated on top of each Nucleus Sampled generation. \(NS_{SPD}\) results in better coverage at nearly every compute budget \(n\).

at no extra cost, using the Nucleus Sampled generations as prefixes. This bolsters every Nucleus Sampling generation with \(k\) local searches (example in Appendix G.2).

This property is valuable for repeated sampling , a technique where the number of generations is scaled at inference time to increase coverage - the proportion of questions that can be answered correctly using at least one of the generations. While repeated sampling typically uses Nucleus Sampling, combining Superposed Decoding and Nucleus Sampling (\(NS_{SPD}\)) produces even larger coverage gains. In Table 2, we compare the coverage of vanilla Nucleus Sampling and \(NS_{SPD}\) in an equal compute setting up to 100 Nucleus Sampling drafts. We find that \(NS_{SPD}\) results in higher coverage _across the board_ on both TriviaQA  and Natural Questions , highlighting Superposed Decoding as a powerful method to increase the impact of scaling inference compute.

## 5 Further Analysis and Ablations

### Results on Mistral 7B

We also implement Superposed Decoding on pre-trained Mistral 7B  and conduct the same experiment as Section 4.1 with one change. In Section 4.1, it was possible to evaluate the perplexity of the 10 generated tokens exactly because the generating model (Llama-2-7B) and the evaluation model (Llama-2-70B) use the same tokenization. This is not the case for Mistral 7B and Llama-2-70B. Consequently, we calculate perplexity for Mistral 7B over all tokens but the first five, ensuring that the entire generation is included. While this approach also includes several tokens from the initial prefix in perplexity calculations, they are redundant across generations, thus preserving relative ordering.

Table 3 compares the resulting perplexities. Like with Llama-2-7B, the average best draft perplexity using Superposed Decoding is lower than that of Nucleus Sampling, demonstrating that Superposed Decoding is adaptable to other language models out of the box.

### Textual Analysis

Next, we extensively investigate the diversity and repetition of Superposed Decoding in order to better understand its properties.

**Repetition within Generations.** Repetition is a well-documented issue in all decoding methods but is most prevalent when decoding greedily . We explore to what extent, if any, Superposed Decoding degenerates as well. To measure repetition, we calculate the proportion of unique unigrams, bigrams, and trigrams in each generation. The lower the uniqueness, the higher the repetition. In Figure 8, we plot results for Superposed Decoding and Nucleus Sampling for several generation lengths. Because drafting is usually a short-form task, we only consider generation lengths up to 20 tokens. We find that Superposed Decoding does not repeat significantly more than Nucleus Sampling in this range, suggesting that degeneration is not an issue in most use cases. This is especially true for bigrams and trigrams, which are more reliable indicators of degeneration than unigrams. However, we qualitatively observe that repetitions become frequent after 100 tokens. To address this, we propose Superposed Decoding Resets, which we explain in Section 5.3.

**Diversity across Drafts.** To measure diversity, we apply Self-BLEU  on drafts and then calculate the average across prefixes. We compute Self-BLEU by calculating the BLEU score of each draft with the other \(k-1\) drafts as references. Hence, a low Self-BLEU signifies high diversity, while a high Self-BLEU suggests low diversity. After calculating Self-BLEU for varying prefix lengths, generation lengths, and numbers of drafts, we find that generation length is the most impactful hyperparameter for diverse drafts. We plot Self-BLEU against generation length in Figure 8, demonstrating that shorter generations significantly increase diversity.

    &  &  \\  Draft \# & - & 1 & 2 & 3 & Best \\  Avg Perplexity & 11.42 & 11.34 & 12.74 & 13.63 & 10.87 \\   

Table 3: Superposed Decoding generalizes across language models like Mistral 7B as shown here with similar results on coherency, as Llama-2-7B, when evaluated using Llama-2-70B.

### Superposed Decoding Resets

To reduce long-form repetition in Superposed Decoding, we propose resetting superposed tokens every \(s\) timesteps, where \(s\) is a user-selected hyperparameter. On each reset step we sample one of the \(k\) drafts and restart draft generation. Resetting helps Superposed Decoding escape repetitive loops while reestablishing linearity, which Figure 3 shows to deteriorate over time. This is similar to a user selecting one of the \(k\) drafts while typing. We find that resetting noticeably improves long-form generation quality (examples in Appendix I) and leave further investigation for future work.

Further, we also ablate on prefix length, generation length, and number of drafts. Figures 9, 10, and 11 in Appendix E highlight that Superposed Decoding is robust to all three hyperparameters, with lower perplexity than Nucleus Sampling in nearly all settings tested.

## 6 Discussion and Conclusion

While we demonstrate that Superposed Decoding is an effective technique for multiple draft generation, Superposed Decoding is limited by the quality of the n-gram models used, which are essential for maintaining coherence. In addition, while Superposed Decoding drafts are _syntactically_ diverse, they are not often _semantically_ diverse. We suspect that the orthogonality of token embeddings discovered by Jiang et al.  is a potential solution. While our initial experiments did not show diversity gains, we believe that orthogonality is promising and is a logical next step for future work. We also note that mechanisms, like batching, that increase throughput of decoding algorithms are complementary to Superposed Decoding.

In conclusion, we present **Superposed Decoding**, a novel decoding method that superposes token embeddings to generate multiple short generations at the cost of one. We demonstrate that Superposed Decoding improves on Nucleus Sampling in terms of generation quality and human preference. The plurality of choices from Superposed Decoding also leads to better performance on common benchmarks and expands coverage at scale. We envision that the latency reduction from Superposed Decoding will make it practical to apply large pre-trained language models on drafting tasks where compute is often a barrier for deployability. Finally, Superposed Decoding can be deployed in messaging applications using n-grams personalized to each user, where the number of n-grams can be reduced to save storage without compromising performance.