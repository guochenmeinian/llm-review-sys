# Color-Oriented Redundancy Reduction in Dataset Distillation

Bowen Yuan  Zijian Wang  Mahsa Baktashmotlagh  Yadan Luo  Zi Huang

{bowen.yuan, zijian.wang, m.baktashmotlagh, y.luo, helen.huang}@uq.edu.au

The University of Queensland

###### Abstract

Dataset Distillation (DD) is designed to generate condensed representations of extensive image datasets, enhancing training efficiency. Despite recent advances, there remains considerable potential for improvement, particularly in addressing the notable redundancy within the color space of distilled images. In this paper, we propose AutoPalette, a framework that minimizes color redundancy at the individual image and overall dataset levels, respectively. At the image level, we employ a palette network, a specialized neural network, to dynamically allocate colors from a reduced color space to each pixel. The palette network identifies essential areas in synthetic images for model training and consequently assigns more unique colors to them. At the dataset level, we develop a color-guided initialization strategy to minimize redundancy among images. Representative images with the least replicated color patterns are selected based on the information gain. A comprehensive performance study involving various datasets and evaluation scenarios is conducted, demonstrating the superior performance of our proposed color-aware DD compared to existing DD methods. The code is available at https://github.com/KeViNYuAn0314/AutoPalette.

## 1 Introduction

Large-scale training data is essential for achieving high model performance. However, the sheer volume of the data poses significant challenges, including computational inefficiency, prolonged training times, and substantial storage overhead. _Data Distillation_ (DD)  offers a promising solution to this problem. By synthesizing a smaller dataset from the original dataset, DD allows models trained on the distilled dataset to attain comparable performance to those trained on the full dataset, thereby reducing the resources needed for training.

Existing DD primarily minimizes the difference between the network trained on the full dataset and the network trained on the synthetic dataset. Different surrogate functions have been implemented to quantify such differences, including performance matching , feature distribution matching  and model gradient matching . Generally speaking, DD considers the synthetic images as parameters and directly optimizes them. Building on this concept, parameterization-based dataset distillation (PDD) extends DD by enhancing the storage utility and reducing redundancy in the image space. Parameterization-based DD methods represent the synthetic dataset in a lower-dimensional space and then, reconstruct synthetic images for model training. Current parameterization-based DD includes: learning in a spatially down-sampled space , factorizing distilled images , optimizing latent embeddings and generators , and selecting informative frequency bands .

While the existing PDD methods have shown promising results, most of the methods overlook the redundancy in the color space, thereby falling short of achieving optimal parameterization performance. We argue that reducing the number of unique colors within one image can have _minimal_ impact on the low-level discriminative features (_e.g._, shapes, edges) required for modeltraining. Moreover, images within the same class typically share similar color distributions; therefore, dedicating storage to _unique_ class patterns rather than storing the replicated color information would be more cost-effective.

To address the limitations of existing PDD approaches, we propose a color-oriented redundancy reduction framework, namely AutoPalette. Specifically, AutoPalette contains an efficient plug-and-play palette network to tackle the issue of color space redundancy within one image. This palette network transforms 8-bit color images into representations with fewer colors (_e.g._, 4-bit) by aggregating pixel-level color information from input images. To enhance the color utility, we design two additional losses on top of the dataset distillation loss: the maximum color loss and the palette balance loss. The maximum color loss ensures that each color in the reduced color space is allocated to at least one pixel, while the palette balance loss balances the number of pixels allocated to each color. The palette network synthesizes image datasets with a reduced color space while preserving essential features of the original images. Furthermore, we equip AutoPalette with a color-guided initialization module to suppress the redundancy in-between synthetic images. The module selects the samples with low replication after color condensation as the synthetic set initialization, whereas information gain is adopted to quantify replication.

**Contributions.** We propose AutoPalette, a color-oriented redundancy reduction framework for data distillation tasks, enhancing storage efficiency by reducing the number of colors in the images while preserving essential features; We seamlessly equip the distillation framework with a guided initialization strategy that selects images with diverse structures in the reduced color space for initialization; Extensive experimental results on three benchmark datasets show that the model trained on the 4-bit images synthesized by our framework achieve competitive results compared to the models trained on 8-bit images synthesized by other DD methods. With the same storage budget, our method outperforms others by \(1.7\%\), \(4.2\%\) on CIFAR10 and CIFAR100.

## 2 Related Work

### Dataset Distillation

Dataset distillation aims to synthesize a small but informative dataset, enabling models trained on these synthetic data to achieve comparable performance to those trained on the complete dataset. Wang _el al._ firstly proposed a meta-model learning approach to optimize a synthetic dataset matching the model performance of large scale dataset. Early works adopt performance matching frameworks [28; 30; 49; 37], and optimize synthetic data using model performance rolling over the training process on the original dataset. Distribution matching methods [39; 46; 48; 32] address high complexity issues in bi-level optimization by matching one step feature distributions between synthetic

Figure 1: The overview of the proposed AutoPalette framework. Initialization: We compare the information gain of quantized images to select the images used in the initialization stage. Training: We forward the synthetic data to the palette network to obtain the color-reduced images. The objective functions of palette network include \(_{a}\), \(_{b}\), \(_{m}\) and \(_{task}\). The synthetic dataset is updated by solely optimizes \(_{task}\).

data and original real data. Gradient matching [47; 44; 25] and trajectory matching [3; 6; 12; 10] approaches aim to match model parameters' gradients for single or multiple training steps, leading networks trained on synthetic data and original data to follow similar gradient descent trajectories.

### Parameterization-based Dataset Distillation

Apart from finding matching objectives between the synthetic dataset and the original full dataset, another aspect of data distillation involves appropriately parameterizing synthetic data in different yet more efficient representatives in memory space. Without storing synthetic data as individual spatial representations, parameterization comprerehends mutual characteristics between data instances and regenerates more data instances of the original input representations. IDC  stores images in a low-resolution manner to conserve storage resources, and upsamples to the original scale for usage. Factorization methods conjecture inter-class data share mutual and independent information and generate synthetic data based on combinations of bases. Bases can be either spatial representations , frequency domains , or embeddings decoded by networks [23; 24; 41; 45; 5; 38].

### Color Quantization

Color quantization [31; 8; 1; 43] intends to aggregate similar colors and transform them using one representative color. Accordingly, images with a reduced color palette require less storage as the pixel values can be encoded in fewer bits. To uphold optimal image authenticity, traditional color quantization methods such as Median Cut , dithering , and OCTree  typically employ color quantization as a color clustering problem. They commonly devise strategies to identify similar or neighboring colors for quantization purposes. On the other hand, parameter-based methods [36; 27; 16; 17] not only rely on predefined heuristics but also leverage neural networks to learn patterns and relationships to compress images to lower bits.

## 3 Methodology

### Notations and Preliminary

Dataset distillation aims to learn a small but representative synthetic dataset \(=\{(^{i},^{i})\}_{i=0}^{||}\) from a given large scale dataset \(=\{(x^{i},y^{i})\}_{i=0}^{||}\). Here, \(||\) and \(||\) denote the number of samples in the synthetic dataset and original large dataset, where \(||||\). By training on the synthetic dataset \(\), a model \((;)\) is aimed to achieve performance comparable to that of a model trained on the original dataset \(\). The objective of dataset distillation can be formulated as a bi-level optimization problem:

\[_{}_{}[(,_{ })]_{}=*{arg\,min}_{ }(,),\] (1)

where \((,)\) and \(\) represent loss function and parameters of the networks, respectively. The inner-loop optimizes the network on the synthetic dataset and the outer loop evaluates the trained network on the real dataset.

Figure 2: The visualization of (a) images under 8, 6, 3, 1-bit color depths (b-c) color condensed synthetic images and their color palette. (b) our full model (c) our full model without palette loss. The larger difference among rows of a color palette indicates better color utilization.

The bi-level meta learning in Eq. 1 requires inner-loop training during every training steps, and thus suffers from inevitable computational cost. Therefore, some of the existing methods [47; 3; 46] try to avoid unrolled back-propagation in the inner loop using various surrogate objectives, thereby the optimal synthetic dataset \(^{*}\) can be obtained by optimizing:

\[^{*}=*{arg\,min}_{}_{}[ ((;),(;))].\] (2)

In parameterization methods for dataset distillation, the synthetic dataset \(\) is stored in more efficient representations consisting of bases \(^{N C H W}\) and a set of transformation functions \(F:\), which generate the synthetic dataset. Here, \(N\) denotes the number of bases, \(C\) represents the channel, and \(H\) and \(W\) are the height and width of the bases, respectively. In this paper, we propose a color transformation that reduces the number of unique colors in the image bases while preserving the essential details after color reduction for model training.

### Overview

In this paper, we explore a new dimension of parameterization-based dataset distillation, concentrating on optimizing storage efficiency by minimizing color space redundancy. We argue that complex color representation within storage-sensitive distilled images is not crucial for training networks. Instead, the limited storage budget for distilled images should be allocated to novel samples that exhibit diverse object structures. The overall framework of AutoPalette is illustrated in Figure 1.

The proposed AutoPalette framework for dataset distillation consists of two components, including a palette network and a color-guided initialization strategy. The palette network is designed to enhance the color utility of the synthetic images in reduced color space by generating pixel-level color mappings. Accordingly, the original images \(^{C H W}\) are transformed into color-condensed images \(^{C H W}\) with a reduced color spectrum, where we denote the process as \(_{palette}:\). RGB image dataset generally contains 256 distinct colors for each channel, and the palette network aims to reduce the number of colors by generating a color palette containing only \(K\) colors. As such, the synthetic dataset can be stored in a low-bit format, rather than the conventional 8-bit format. To better leverage the diverse color feature information within the original dataset, we equip the proposed framework with a novel initialization strategy, which employs the generalized graph cut function to select representative images for initialization. Our method dynamically evaluates the impact of real images based on their color structures and initializes the synthetic dataset with the images that yield the highest information gains of graph cut functions. The subsequent sections will illustrate each module in detail.

### Color Reduction via Palette Network.

The core of our proposed parameterization method for dataset distillation is to condense the number of unique colors in an image so that the image can be stored with a more efficient manner. One of the key challenges of reducing the unique color number in dataset distillation lies in local discriminative feature preservation. When an image is represented with a smaller range of color, it is inevitable for some pixels to be merged to their neighbour color blocks. In this case, some of the local discriminative features (_e.g._, edge, shape, _etc._) can be erased or distorted in the color-reduced images, hindering the network trained on them to achieve the optimal performance.

To alleviate this issue, we design a simple yet effective network, namely palette network, which learns the pixel-level color allocation in the pruned color space with the discriminative feature maximally preserved. Particularly, the color palette network predicts the probability map \(^{C H W K}\), which indicates the probability of a pixel being allocated to a color of the \(K\)-dimensional reduced color space by forwarding an image \(\) to the palette network:

\[=_{color}(;_{c}),\] (3)

where \(_{c}\) is the parameters of the palette network.

Given a base image \(\) and its corresponding probability map \(\), we formulate the palette \(}^{C K}\) as the average pixel values of all pixels assigned to the same color buckets index:

\[}_{c,k}=_{c,i,j}^{ }_{c,i,j}(k)}{_{c,i,j}^{}_{c,i,j}(k)},\] (4)where \(c\) and \(k\) denote the \(c\)-th channel for the \(k\)-th quantized color, \(\) and \(\) denote the vertical and horizontal pixel position in an image, and \(^{}\) is the Kronecker delta function, which equals 1 if \(_{c,i,j}\) equals \(k\).

Once the color palette \(}\) and a probability map \(\) are generated for an image, its color condensed image \(\), with the number of unique colors per channel reduced to \(K\), can be generated by an index searching process:

\[_{c,i,j}=}[c,],= *{arg\,max}_{k}_{c,i,j}.\] (5)

To learn the palette network, a straightforward solution is to optimize the distillation task loss during the training stage. However, we can observe from Fig. 2 that solely relying on the task loss for training the palette network leads to most of the palette being inactivated. The network tends to assign pixels to a limited number of color buckets, which strongly limits the capacity and expressiveness of the distilled images. Therefore, two additional losses, namely **maximum color loss** and **palette balance loss**, are incorporated to enhance the utility of color buckets in synthetic images. In particular, maximum color loss, \(_{m}\) encourages the palette network to generate color allocation such that each color bucket is at least filled with one pixel within color palette. By aggregating the maximum confidences from probability index maps across the spatial dimensions, we define the maximum color loss as:

\[_{m}=-_{c=1}^{C}_{k=1}^{K}_{(h,w)}( _{c,h,w,k}),\] (6)

While the maximum color loss ensures the activation of each color bucket in the palette, the distribution of pixel numbers in color buckets can still be extremely imbalanced. Therefore, our framework leverages a palette balance loss \(_{b}\), which encourages a more balanced usage of the buckets within the color palette by promoting color-wise entropy. We formulate the palette balance loss as the entropy of \(\) over the spatial dimensions:

\[_{b}=_{c=1}^{C}_{k=1}^{K}P(_{i= 1}^{H}_{j=1}^{W}_{c,i,j,k}) P(_{i=1}^{H} _{j=1}^{W}_{c,i,j,k}),\] (7)

where \(P()\) represents the softmax function of \(\) over the spatial dimensions.

By integrating the palette network with the complementary losses, we obtain the color condensed images while preserving the informative features.

### Color Guided Initialization Module

The empirical results of previous studies have shown a strong correlation [3; 46] between the original images selected during initialization and the resulting distilled images in terms of visual appearance. In light of this finding, we propose an initialization method aimed at solving the redundancy problem in color-condensed synthetic images. However, since we do not have access to the optimized palette network, it is prohibitive to directly measure the information overlap within a class after color condensation. To mitigate this issue, we propose to leverage the traditional color quantization approach  to approximate the output of the palette network. Here, we denote the quantized full dataset as \(^{Q}\). Our proposed initialization strategy leverages conditional gain within submodular information theoretics to identify the most diverse images of each class after color condensation. Specifically, the conditional gain \(G(|)\) implies the gain of information by adding set \(\) to set \(\), where \(,^{Q}\) and \(=\). Formally, we have:

\[G(|)=G(^{Q})-G(),\] (8)

where \(G()\) denotes a submodular function. Submodular information functions  describe a set of combinatorial functions that satisfy the Shannon inequality [34; 26] and can effectively model the diversity of a subset. In our implementation, we adopt a monotone submodular function, namely generalized graph cut , which maximizes the similarities between samples in \(\) and \(\) and minimizes and dissimilarities among the samples in \(\). The generalized graph cut function \(G^{*}()\) is defined as follows:

\[G^{*}(|)=_{i}_{j}Sim (i,j)-_{j_{1},j_{2}}Sim(j_{1},j_{2}),\] (9)where \(i\) and \(j\) denote data samples from the sets \(\) and \(\), respectively. \(Sim(,)\) is a similarity function between two samples. Instead of directly measuring the feature level similarity, we propose to measure the similarity between the last layer gradients \(\) as follows:

\[Sim(i,j)=cos(_{}_{CE}(Q(^{i}),),_{ }_{CE}(Q(^{j}),)),\] (10)

where \(cos(,)\) is the cosine similarity function, \(Q\) denotes the Median Cut quantization method, \(^{i}\) and \(^{j}\) are the ith and jth samples of set \(\), and \(_{}\) is the gradient of cross-entropy loss between the prediction and the ground truth label on the last layer of the network. By substituting Eq.(9) into Eq. (8), we select a representative sample for inclusion in \(\) by:

\[*{arg\,max}_{c}G^{*}()-2_{i}_{c }Sim(i,c).\] (11)

The proof for the graph cut conditional gain is provided in Appendix A.1. From Eq. (11), we can see that the data sample obtaining the highest conditional gain may be selected. Intuitively, we select the most representative sample from the unselected set \(\), whilst ensuring it is dissimilar to the already selected samples in \(\). The entire color diversity selection process is provided in Algorithm 1.

By far, our initialization method can select diverse and representative samples of each class in the approximated quantization set. To minimize the difference between the approximation set and the output of the palette network, we put forward a regularization term \(_{a}\). The regularization term not only constrains the color allocation shifting of palette network, but also enhances allocation consistency, so similar colors are grouped together with higher fidelity. The regularization term \(_{a}\) is defined as:

\[_{a}=\|^{}- ^{}^{}\|_{2}^{2},\] (12)

where \(^{}\) denotes the the \(*{arg\,max}\) of the color mapping indices by Median Cut over the color space, and \(\) denotes the element-wise multiplication resulting in a self correlation matrix for palette bucket allocations of the palette network and Median Cut. When creating index mappings for pixels, different methods might cluster the same pixels into the same group, but the indices may not match. By utilizing \(_{a}\), we emphasize clustering resemblance, disregarding the order of clustering indices.

### Overall Dataset Distillation Objective

Our framework aims to create a color-condensed synthetic version of the original dataset while maximally preserving task-related information. In line with other parameterization-based methods, we incorporate the dataset distillation loss \(_{task}\) from existing works into our framework.

As such, to update the palette network, we have the overall loss function defined as:

\[*{arg\,min}_{_{c}}_{palette}=_{task }+_{m}+_{b}+_{a},\] (13)

where \(,,\) are the coefficients as the weights of loss components. The synthetic set \(\) is optimized as follows:

\[*{arg\,min}_{}_{task}=(( ;),(;)),=_{palette}(;_{c}),\] (14)

where \(_{palette}(;_{c})\) denotes the color quantization process using the palette network.

### Storage Analysis

In our experiments, the images follow the 256-color storage convention, where pixel values occupy 8-bit storage space. Given the storage budget of images per class (IPC), the maximum storage budget for one class is capped at \(8 CHW\), where \(C,H\) and \(W\) represent the channel, height and width of the images, respectively. When representing a colorful image pixel value with n bits, where \(1 n<8\), there can be at most \(2^{n}\) distinct colors per image. This must satisfy the condition \(_{i=1}^{2^{8-n}}N_{i} 2^{8},\) where \(N_{i}\) is the number of colors for the i-th color reduced image and each \(N_{i} 2^{n}\). Therefore, for images with n-bit format, up to \(2^{8-n}\) colors can be represented in the storage budget using a bitmap index with small bytes. The bitmap index indicates the image number associated with the current lower bit color value.

## 4 Experiments

In this section, we first evaluate the effectiveness of our method in comparison with other parameterization methods on various datasets. Afterwards, we perform experiments on the relations between synthetic image color number and model performance. We also conduct ablation studies and assess the efficacy of each proposed component to distillation performance.

### Experimental Setting

We conduct experiments of our model on various benchmark datasets, including CIFAR-10 , CIFAR-100  and ImageNet . We compare our parameterization method with core-set methods and other existing DD works containing baselines such as DD , DM , DC , TM , and parameterization techniques including IDC , HaBa , RTP , SPEED , FReD . Experiments are performed on different distillation memory budget settings for 1/10/50 images per class (IPC). We follow the previous works to use a ConvNetD3 for the CIFAR family and ConvNetD5 for ImageNet as the training and evaluation network. We follow the DATM  implementation based on trajectory matching, without soft label initialization using correctly predicted samples. Each experiment is evaluated on 5 randomly initialized networks, and the mean and standard deviation of the evaluation accuracy are recorded. We set loss coefficients \(\)=1, \(\)=1, \(\)=3 for all experiments if not specified. All experiments can be conducted on 2\(\)Nvidia H100 GPUs that have 80GB RAM for each or 4\(\)Nvidia V100 GPUs that have 32GB RAM for each.

### Experimental Results

**Results on CIFAR10 and CIFAR100.** We perform experiments under paramterization settings on CIFAR10 , CIFAR100 . We set color palette network to condense the number of colors of a single image from 256 to 64, such that despite huge color space reduction quantized images still preserve much fidelity of images. As shown in Table 1, our method achieves superior performance than other parameterization works in various tasks. Notably, in the experiments when IPC equals 10 and 50, our method significantly outperforms other methods. In CIFAR100 experiments, our model achieves 52.6% and 53.3% classification accuracy when IPC is respectively 10 and 50, which increases 6.7% and 4.2% higher than previous state-of-the-art parameterization methods. These outstanding performances highlight that reducing the color redundancy within the synthetic dataset can improve the storage utility and thereby improve the distillation result.

**Results on ImageNet.** Following , we conduct experiments on six subsets of ImageNet, where each subset consists of 10 classes and the images are of resolution 128\(\)128. We conduct experiments with the storage budget of IPC=10. ConvNetD5 is employed as the backbone model for training

   &  &  \\   & IPC & 1 & 10 & 50 & 1 & 10 & 50 \\   & Random & 14.4\(\)0.2 & 26.0\(\)1.2 & 43.4\(\)1.0 & 4.2\(\)0.3 & 14.6\(\)0.5 & 30.0\(\)0.4 \\  & Herding  & 21.5\(\)1.3 & 31.6\(\)0.7 & 40.4\(\)0.6 & 8.4\(\)0.3 & 17.3\(\)0.3 & 33.7\(\)0.5 \\  & K-Center  & 23.3\(\)0.9 & 36.4\(\)0.6 & 48.7\(\)0.3 & 8.6\(\)0.3 & 20.7\(\)0.2 & 33.6\(\)0.4 \\   & DD  & - & 36.8\(\)1.2 & - & - & - & - \\  & DM  & 26.0\(\)0.8 & 48.9\(\)0.6 & 63.0\(\)0.4 & 11.4\(\)0.3 & 29.7\(\)0.3 & 43.6\(\)0.4 \\  & DC  & 28.3\(\)0.5 & 44.9\(\)0.5 & 53.9\(\)0.5 & 12.8\(\)0.3 & 25.2\(\)0.3 & - \\  & TM  & 46.3\(\)0.8 & 65.3\(\)0.7 & 71.6\(\)0.2 & 24.3\(\)0.3 & 40.1\(\)0.4 & 47.7\(\)0.2 \\  & DATM  & 46.9\(\)0.5 & 66.8\(\)0.2 & 76.1\(\)0.3 & 27.9\(\)0.2 & 47.2\(\)0.4 & **55.0\(\)**0.2 \\   & IDC  & 50.0\(\)0.4 & 67.5\(\)0.5 & 74.5\(\)0.1 & - & - & - \\  & HaBa  & 48.3\(\)0.8 & 48.3\(\)0.8 & 48.3\(\)0.8 & 33.4\(\)0.4 & 40.2\(\)0.2 & 47.0\(\)0.2 \\   & RTP  & **66.4\(\)**0.4 & 71.2\(\)0.4 & 73.6\(\)0.5 & 34.4\(\)0.4 & 42.9\(\)0.7 & \\   & SPEED  & 63.2\(\)0.1 & 73.5\(\)0.2 & 77.7\(\)0.4 & **40.0\(\)**0.4 & 45.9\(\)0.3 & 49.1\(\)0.2 \\   & FReD  & 60.6\(\)0.8 & 70.3\(\)0.3 & 75.8\(\)0.1 & 34.6\(\)0.4 & 42.7\(\)0.2 & 47.8\(\)0.1 \\   & AutoPalette & 58.6\(\)1.1 & **74.3\(\)**0.2 & **79.4\(\)**0.2 & 38.0\(\)0.1 & **52.6\(\)**0.3 & 53.3\(\)0.8 \\  

Table 1: Test accuracy (%) of previous works and our method on ConvNet D3. Our synthetic images are reduced from 256 colors to 64 colors. Our method outperforms previous methods and achieves state-of-the-art performance.

and evaluation. From Table 2, we can see our method outperforms other PDD methods on most of ImageNet subsets, including ImageNet, ImageWoof, ImageMeow and ImageYellow, while results of the other subsets still achieve comparable performance with previous state-of-the-art results. Specifically, our method achieves \(44.3\%\) and \(72.0\%\) on hard datasets ImageWoof and ImageYellow, increasing \(0.2\%\) and \(1.5\%\) than previous best PDD methods. We also observe that for subsets with distinct classes, our method achieves promising results, which is because color condensation effectively preserves the key semantics necessary for accurate classification. On the other hand, for fine-grained subsets where the classes are similar, we observe inferior performance. This is likely because fine-grained details are blurred in images represented by fewer colors, thereby making it challenging to differentiate between classes.

**Compatibility of Distillation Frameworks.** While we take trajectory matching as our primary distillation method, we demonstrate that our framework can effortlessly be equipped to improve other dataset distillation methods. As illustrated in Table 5, our method shows a significant performance boost across all IPC settings and datasets when adopted to the distribution matching method. Especially, our method increases the test accuracy up to \(15\%\) when IPC=10, and \(15.7\%\) for IPC=1 on CIFAR100. This observation aligns with our objectives that our color-oriented redundancy management framework should be adapted across different standard DD frameworks The performance improvement underscores the high compatibility of our methods with diverse DD frameworks.

### Ablation Study

Here, we focus on comparing variants of our proposed framework. Therefore, we fix the number of synthetic images to 10 per class, rather than fully utilizing the available storage capacity.

**Effectiveness of Loss Components.** To validate the contribution of each loss term to the overall framework, we conduct experiments on CIFAR10 with IPC=10. Specifically, we construct three variants of our model by removing \(_{m}\), \(_{b}\), and \(_{a}\), correspondingly. We show the experimental result in Table 3. Under the same experimental conditions, eliminating specific loss functions will suffer from performance decline. The results demonstrate the essential role played by each loss function in optimizing the palette network.

**Effectiveness of Selection Criteria in the Color-guided Initialization.** We compare our proposed initialization with two baseline sample selection criteria, including Random Real, and Graph Cut Real. Random real is widely adopted by DD methods, which randomly select images from the full dataset as the initialization of the synthetic dataset. In Graph Cut Real, we apply graph cut on 8-bit images

   Initialization Method & Accuracy \\  Random Real & 60.84 \\ Graph Cut Real & 61.41 \\ Graph Cut on Quantized Image & **62.13** \\   

Table 4: Evaluation on the effectiveness of submodular selection using quantized images, in comparison with random initialization and submodular selection using full color images.

   Dataset & ImageNet & ImageWoof & ImageFruit & ImageMeow & ImageSquawk & ImageYellow \\  TM  & 63.0\(\)1.3 & 35.8\(\)1.8 & 40.3\(\)1.3 & 40.4\(\)2.2 & 52.3\(\)1.0 & 60.0\(\)1.5 \\ HaBa  & 64.7\(\)1.6 & 38.6\(\)1.3 & 42.5\(\)1.6 & 42.9\(\)0.9 & 56.8\(\)1.0 & 63.0\(\)1.6 \\ FrePo  & 66.5\(\)0.8 & 42.2\(\)0.9 & - & - & - & - \\ SPEED  & 72.9\(\)1.5 & 44.1\(\)1.4 & **50.0\(\)**0.8 & 52.0\(\)1.3 & **71.8\(\)**1.3 & 70.5\(\)1.5 \\ AutoPalette & **73.2\(\)**0.6 & **44.3\(\)**0.9 & 48.4\(\)1.8 & **53.6\(\)**0.7 & 68.0\(\)1.4 & **72.0\(\)**1.6 \\   

Table 2: Test accuracy (%) on ImageNet-Subset: ImageNet, ImageWoof, ImageFruit, ImageMeow, ImageSquawk, ImageYellow. All experiments are conducted on CIFAR10 with IPC=10 storage budget for parameterization methods.

   \(_{m}\) & \(_{b}\) & \(_{a}\) & Accuracy \\  ✗ & & 64.00 \\   & ✗ & 61.40 \\   & ✗ & 60.14 \\  ✓ & ✓ & ✓ & **66.20** \\   

Table 3: Test accuracy (%) when a certain loss component is removed during training.

and select the most representative samples with high information gain. Compared with Graph Cut Real, the selection criteria used in our framework computes the information gain over the quantized images. From table 4, we can see that our methods using quantized images exhibit better performance than comparison approaches. The graph cut with original full-color images also outperforms the baseline model using randomly selected real images as initialization, confirming the effectiveness of computing information gain over color-reduced images

**Effectiveness of Color-guided Initialization under Different Color Depth.** Our experiments compare the performance of two ways to initialize the base images: one employing our method of color-guided initialization (denotes GraphCut), and randomly selecting real images as the base images (denotes Baseline). We contrast two approaches on CIFAR10 with IPC=10, spanning from synthesizing images with low-bit quantization to those with higher-bit quantization. From Figure 3, we observe that our method brings better performance when quantized images are represented in lower bits. Starting from 16 colors per pixel, we observe a gradual convergence in performance as we move towards utilizing full-color space. Meanwhile, as can be seen, when \(K\)=32 and 64, it achieves the best trade-off, as it still attains competitive performance comparable to that of full-color budget images, while substantially reducing the storage space required for quantized images.

## 5 Conclusions, Limitations, and Future Work

In this paper, we aim to solve the color-redundancy issue within data distillation from both the image level and the dataset level. For condensing the colors within images, we utilize a palette network to capture the color redundancy between pixels and represent images using fewer colors. Beyond this, to reduce repetitive patterns in between synthetic images design a guided image initialization module that selects samples by maximising information gain. Extensive experimental results demonstrate that our AutoPalette framework can effectively reduce color redundancy and simultaneously preserve the essential low-level feature for model training.

**Limitations.** For instance, images from different classes may have a bias towards color usage. Images of one class may be sufficient to be represented by fewer colors than those from other classes, in which case an imbalanced color budget arrangement may be a better option. In future, it is also promising to explore the dynamic color depth allocation, which allocates more budgets to difficult classes, thereby improving the distillation performance.