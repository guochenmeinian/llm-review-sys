# Model-Based Transfer Learning for

Contextual Reinforcement Learning

 Jung-Hoon Cho

MIT

jhooncho@mit.edu

&Vindula Jayawardana

MIT

vindula@mit.edu

&Sirui Li

MIT

siruil@mit.edu

&Cathy Wu

MIT

cathywu@mit.edu

###### Abstract

Deep reinforcement learning (RL) is a powerful approach to complex decision making. However, one issue that limits its practical application is its brittleness, sometimes failing to train in the presence of small changes in the environment. Motivated by the success of zero-shot transfer--where pre-trained models perform well on related tasks--we consider the problem of selecting a good set of training tasks to maximize generalization performance across a range of tasks. Given the high cost of training, it is critical to select training tasks strategically, but not well understood how to do so. We hence introduce Model-Based Transfer Learning (MBTL), which layers on top of existing RL methods to effectively solve contextual RL problems. MBTL models the generalization performance in two parts: 1) the performance set point, modeled using Gaussian processes, and 2) performance loss (generalization gap), modeled as a linear function of contextual similarity. MBTL combines these two pieces of information within a Bayesian optimization (BO) framework to strategically select training tasks. We show theoretically that the method exhibits sublinear regret in the number of training tasks and discuss conditions to further tighten regret bounds. We experimentally validate our methods using urban traffic and standard continuous control benchmarks. The experimental results suggest that MBTL can achieve up to 43x improved sample efficiency compared with canonical independent training and multi-task training. Further experiments demonstrate the efficacy of BO and the insensitivity to the underlying RL algorithm and hyperparameters. This work lays the foundations for investigating explicit modeling of generalization, thereby enabling principled yet effective methods for contextual RL. Code is available at https://github.com/jhoon-cho/MBTL/.

## 1 Introduction

Deep reinforcement learning (DRL) has made remarkable strides in addressing complex problems across various domains . Despite these successes, DRL algorithms often exhibit brittleness when exposed to small variations like different number of lanes, weather conditions, or flow density in traffic benchmarks , significantly limiting their scalability and generalizability . Such variations can be modeled using the framework of contextual Markov Decision Processes (CMDP), where task variations can be parameterized within a context space .

There are two predominant solution modalities for CMDPs : independent training and multi-task training. Independent training constructs a separate model for each task variant (say, \(N 1\)), whichis compute-intensive. At the other extreme, multi-task training constructs a single "universal" policy, and thus can be more compute-efficient, but suffers from model capacity and negative transfer issues [22; 46; 2; 42]. There is thus a need for more reliable training methodologies for generalization across tasks variants. In this work, we consider training an intermediate set of \(K\) models, where \(N>K>1\), in an effort to balance performance and efficiency; we refer to this strategy as _multi-policy training_.

We build upon zero-shot transfer, a widely-used practical technique which directly applies a policy trained in one context (source task) to another (target task) without adaptation. Figure 1 shows that multi-policy training with zero-shot transfer has the potential to improve the performance even over the independent training. In this article, we strategically select source tasks by explicitly modeling the generalization performance to estimate the value of training a new source task.

**A note on terminology**. For brevity, we refer to _task variants_ as _tasks_ in the remainder of this article. We also use the language of _task_ and _context_ interchangeably. We emphasize that this work focuses on within-domain generalization (e.g., traffic signal control for intersection scenario variants) rather than across-domain generalization (e.g., distinct traffic control tasks). Additionally, it is crucial to differentiate between _training_ reliability, which concerns the ability to reliably train models across tasks, and _model_ reliability (or robustness), which concerns the resistance of a trained model to differences in tasks. This article is concerned with training reliability.

The main contributions of this work are:

* We introduce _Model-Based Transfer Learning (MBTL)_, a novel framework for solving CMDP sample efficiently (Figure 2). To the best of our knowledge, this is the first work to explicitly model generalization performance for contextual RL (CRL). As such, our work opens the door for further investigation into reliable model-based methodologies for CRL.
* We provide theoretical analysis for the sublinear regret of MBTL and give conditions for achieving tighter regret bounds.
* We empirically validate our methods in urban traffic and standard continuous control benchmarks for contextual RL, observing **up to 43x** improvements in sample efficiency. We further include ablations on the components of the algorithm.

The remainder of the paper is organized as follows. After introducing notation in Section 2, we formally define the problem in Section 3. A key contribution of our work is the introduction of a Gaussian process model acquisition function specifically tailored to the source task selection problem, which is detailed in Section 4. In Section 4.3, we provide a theoretical analysis of the regret bounds of our method, followed by an empirical evaluation across diverse applications in Section 5.

## 2 Preliminaries and notation

**Contextual MDP.** A standard MDP is defined by the tuple \(M=(S,A,P,R,)\) where \(S\) represents the state space, \(A\) is the action space, \(P\) denotes the transition dynamics, \(R\) is the reward function, and \(\) is the distribution over initial states . A contextual MDP (CMDP), denoted by \(=\)(\(S,A,P_{x},R_{x},_{x}\))\({}_{x X}\), is a collection of context-MDPs \(_{x}\) parameterized by a context

Figure 1: Normalized performance comparison across different problem variations in Eco-Driving benchmark. Traditional DRL approaches (e.g., Independent training or multi-task training) exhibit greater training instability, whereas Oracle Transfer, zero-shot transfer with full information, shows the potential for performance improvement by multi-policy training.

variable \(x\) within a context space \(X\) (assumed bounded). The context variable \(x\) can influence dynamics, rewards, and initial state distributions [14; 32; 5]. We define source task performance \(J(_{x},x;)\) as follows: we train a policy \(_{x}\) on a task with the context \(x X\) using RL algorithm Alg (e.g., PPO, SAC) and evaluate the policy by the expected return in the same MDP \(_{x}\) with context \(x\). For brevity, we will write it as \(J(_{x},x)\). We distinguish between estimated values \((x)\) and observed outcomes \(J(_{x},x)\), with the latter measured after training and evaluation.

**Generalization gap via zero-shot transfer.** Consider zero-shot transfer from the trained policy \(_{x}\) from a source task (context-MDP) to solve another target task (context-MDP) with the context \(x^{} X\) in the CMDP. Zero-shot transfer involves applying a policy trained on a source task \(_{x}\) to a different target task \(_{x^{}}\), with \(x,x^{} X\). This experiences performance degradation, also called _generalization gap_[17; 23]. For instance, Figure 3 depicts that the performance degrades as the target task diverges from the source task, corresponding to an increasing generalization gap. Nonetheless, leveraging the notion that training is expensive but zero-shot transfer is cheap, we are interested in optimally selecting a set of source (training) tasks, such that the generalization performance on the target range of tasks is maximized. We observe the _generalization performance_, denoted by \(J(_{x},x^{})\), by evaluating the target task \(x^{}\) based on the policy trained using source task \(x\) via zero-shot generalization. We define the generalization gap as the absolute performance difference in average reward when transferring from source task \(x\) to target task \(x^{}\):

\[,x^{})}_{}=| ,x)}_{}-,x^{ })}_{}|.\] (1)

## 3 Problem formulation

**Sequential source task selection problem.** The selection of source MDPs from the CMDPs is key to solving the overall CMDP . We therefore introduce the _sequential source task selection

Figure 3: Example generalization gap depicted for Cartpole CMDP. The solid lines show the true zero-shot transfer generalization performance across contexts. Source tasks are indicated by dotted lines.

Figure 2: **Overview illustration for Model-based Transfer Learning.** (a) Gaussian process regression is used to estimate the training performance across tasks using existing policies; (b) marginal generalization performance (red area) is calculated using upper confidence bound of estimated training performance, generalization gap, and generalization performance; (c) selects the next training task that maximizes the acquisition function (marginal generalization performance); (d) once the selected task is trained, calculate generalization performance using zero-shot transfer.

_(SSTS) problem_, which seeks to maximize the expected performance across a dynamically selected set of tasks. This problem is cast as a sequential decision problem, in which the selection of tasks is informed through feedback from the observed task performance of the tasks selected and trained thus far. The notation \(x_{k}\) indicates the selected source task at the \(k\)-th transfer step, where \(k\) ranges from \(1\) to \(K\). For brevity, we will denote \(_{x_{k}}\) as \(_{k}\). We denote the sequence \(x_{1},x_{2},...,x_{k}\) by \(x_{1:k}\) and \(_{1},_{2},...,_{k}\) by \(_{1:k}\).

**Definition 1** (Sequential Source Task Selection Problem).: _This problem seeks to optimize the expected generalization performance across a CMDP \(_{x^{} X}\) by selecting a task \(x X\) at each training stage. Specifically, at each selection step \(k\), we wish to choose a distinct task \(x_{k}\) such that the expected cumulative generalization performance is maximized. This can be expressed by keeping track at each step, which policy to use for which task, and the corresponding generalization performance. Upon training the policy \(_{x_{k}}\) for source task \(x_{k}\), the cumulative generalization performance, which we abuse notation to denote as \(J(_{1:k},)=J(_{x_{k}},;_{1:k-1})\). Formally, this can be recursively defined based on previous observations \(\{J(_{1},x),,J(_{k-1},x)\}\) for all \(x X\) as follows:_

\[J(_{x_{k}},x^{};_{1:k-1})=(J(_{k},x^{}),J(_{1 :k-1},x^{})) x^{} Xk>1.\] (2)

_And \(J(_{1:1},x) J(_{1},x)\). Then, the overall sequential decision problem can be written as:_

\[_{x_{k}}\;\;V(x_{k};_{1:k-1})=_{x_{k}}_{x^{} (X)}[J(_{x_{k}},x^{};_{1:k-1})]x_{k} X x_{1:k-1}.\] (3)

The state at each step \(k\) is defined by the best generalization performance for each task, achieved by policies trained in earlier stages, represented as \(J(_{1:k-1},x^{})\) for each target task \(x^{}\). The action at each step is choosing a new task \(x_{k}\). In general, SSTS exhibits stochastic transitions, for example due to randomness in RL training. For simplicity, in this work, we assume deterministic transitions; that is, training context-MDP \(x\) will always yield the same performance \(J(_{x},x)\) and generalization gap \( J(_{x},x^{}), x^{} X\). The problem's maximum horizon is defined by \(|X|\), but can be terminated early if conditions are met (e.g., performance level, training budget).

## 4 Model-Based Transfer Learning (MBTL)

In this section, we introduce an algorithm called Model-based Transfer Learning to solve the SSTS problem. MBTL models the generalization performance in two parts: 1) the performance set point, modeled using Gaussian processes, and 2) generalization gap, modeled as a linear function of contextual similarity. MBTL combines these two pieces of information within a Bayesian optimization (BO) framework to sequentially select training tasks that maximize generalization performance.

### Modeling assumptions

We consider a task set \(X\) that is continuous and the performance function \(J(,x),V(x)\) for a policy \(\) to be smooth over the task space \(X\). In practice, such as control systems, tasks often vary continuously and smoothly rather than abruptly. For example, adjusting the angle of a robotic arm by a small amount typically results in a small change in the system and optimal action. Inspired by the empirical generalization gap performance as observed in Figure 3, we estimate the generalization gap with a linear function of contextual similarity.

**Assumption 1** (Linear generalization gap).: _A linear function is used to model the generalization gap function, formally \((_{x},x^{})|x-x^{}|\), where \(\) is the slope of the linear function and \(x\) and \(x^{}\) are the context of the source task and target task, respectively._

The relaxation of this assumption can yield additional efficiency benefits but also adds modeling complexity, and thus is an interesting direction for future work.

### Bayesian optimization

Bayesian optimization (BO) is a powerful strategy for finding the global optimum of an objective function when obtaining the function is costly, or the function itself lacks a simple analytical form [31; 6]. BO integrates prior knowledge with observations to efficiently decide the next task to train by using the acquisition function. MBTL is a BO method which optimizes for promising source tasks by leveraging Assumption 1 in its acquisition function. The role of BO is to approximate \(V(x_{k};_{1:k-1})\)(see Equation 3) using the data acquired thus far. The next source task \(x_{k}\) is then selected using this estimate.

**Gaussian process (GP) regression.** Within the framework of BO, we model the source training performance \((_{x}\),\(x)\)\( x X x_{1:k}\) using Gaussian process (GP) regression. Specifically, the function \((_{x}\),\(x)\) is assumed to follow a GP \(((_{x}\),\(x)([(_{x},x)],k(x,)))\), where \(k(x,)\) is the covariance function, representing the expected product of deviations of \((_{x}\),\(x)\) and \((_{}\),\()\) from their respective means. Let \(D_{k-1}\) denote the data observed up to iteration \(k-1\), consisting of the pairs \(\{(x_{i},J(_{i},x_{i}))\}_{i=1,,k-1}\). The estimated performance \(_{k}\) after querying \(k-1\) samples is updated as more samples are obtained. The posterior prediction of \(_{k}\) at a new point \(x\), given the data \(D_{k-1}\) and previous inputs \(x_{1:k-1}\), is normally distributed as \(P(_{k} D_{k-1})=(_{k}(x),_{k}^{2}(x))\). \(_{k}(x)\) and \(_{k}^{2}(x)\) are defined as \(_{k}(x)=[(_{x},x)]+^{}(+ ^{2})^{-1}\) and \(_{k}^{2}(x)=k(x,x)-^{}(+^{2})^{ -1}\), with \(\) being the vector of covariances between \(x\) and each \(x_{i}\) in the observed data, i.e., \(=[k(x,x_{1}),,k(x,x_{k-1})]\), and \(\) is the covariance matrix for the observed inputs, defined as \(=[k(x_{i},x_{j})]_{1 i,j k-1}\). This enables the GP to update its beliefs about the posterior prediction with every new observation, progressively improving the estimation.

**Acquisition function.** The acquisition function plays a critical role in BO by guiding the selection of the next source training task. At each decision step \(k\), the task \(x_{k}\) is chosen by maximizing the acquisition function, as denoted by \(x_{k}=_{x}a(x;x_{1:k-1})\). One effective strategy employed in the acquisition function is the upper confidence bound (UCB) acquisition function, which considers the trade-off between the expected performance of a task based on the current task (exploitation) and the measure of uncertainty associated with the task's outcome (exploration) . Especially in our case, the acquisition function can be designed as UCB function subtracted by generalization gap and so-far best performance. It is defined as follows:

\[a(x;x_{1:k-1})=_{x^{} X}[[_{k-1}(x)+_{k}^{1/2} _{k-1}(x)-(_{x},x^{})-J(_{1:k-1},x^{})]_{ +}]\] (4)

where \([]_{+}\) represents \((,0)\) and we can use various forms of \(_{k}\), which is the trade-off parameter between exploitation and exploration.

### Regret analysis

We use regret to quantify the effectiveness of our source task selection based on BO. Specifically, we define regret at iteration \(k\) as \(r_{k}=V(x_{k}^{*};_{1:k-1})-V(x_{k};_{1:k-1})\), where \(V(x_{k}^{*};_{1:k-1})\) represents the maximum generalization performance achievable across all tasks, and \(V(x_{k};_{1:k-1})\) is the performance at the current task selection \(x_{k}\). Consequently, the cumulative regret after \(K\) iterations is given by \(R_{K}=_{k=1}^{K}r_{k}\), summing the individual regrets over all iterations. Following the framework presented by Srinivas et al. , our goal is to establish that this cumulative regret grows sublinearly with respect to the number of iterations. Mathematically, we aim to prove that \(_{K}}{K}=0\), indicating that, on average, the performance of our strategy approaches the optimal performance as the number of iterations increases.

**Regret of MBTL.** Having established the general framework for regret analysis, we now turn our attention to the specific regret properties of our MBTL algorithm. To analyze the regret of MBTL, consider the scaling factor for the UCB acquisition function given by \(_{k}=2(|X|^{2}k^{2}/6)\) in Equation (4). It is designed to achieve sublinear regret with high probability, aligning with the theoretical guarantees outlined in Theorem 1 and 5 from .

**Theorem 1** (Sublinear Regret).: _Given \((0,1)\), and with the scaling factor \(_{k}\) as defined, the cumulative regret \(R_{K}\) is bounded by \(_{K}_{K}}\) with a probability of at least \(1-\). The formal expression of this probability is \(Pr[R_{K}_{K}_{K}}] 1-\), where \(C_{1}:=)} 8^{2}\) and \(_{K}=( K)\) for the squared exponential kernel._

**Impact of search space elimination.** In this section, we demonstrate that strategic reduction of the possible sets, guided by insights from previous task selections or source task training performance, leads to significantly tighter regret bounds than Theorem 1. By focusing on the most promising regions of the task space, our approach enhances learning efficiency and maximizes the policy's performance and applicability. Given the generalization gap observed in Figure 3, we observe that performance loss decreases as the context similarity increases. We model the degradation from the source task using a linear function in Assumption 1. Training on the source task can solve a significant portion of the remaining tasks. Our method progressively eliminates partitions of the task space at a certain rate with each iteration. If the source task selected in the previous steps could solve the remaining target task sufficiently, we can eliminate the search space at a desirable rate. Formally, we can define the search space at \(k\)-th iteration as follows:

**Definition 2** (Search space).: _We define the search space \(X_{k}\) at iteration \(k\) as a subset of \(X\), with each element \(x^{} X_{k}\), such that \(J(_{1:k-1},x^{})(_{x_{k}},x^{})- J(_{x_{ k}},x^{})\)._

Given the generalization gap observed in Figure 3, we model the degradation from the source task using a linear function in Assumption 1. While the figure might not strictly appear linear, the linear approximation simplifies analysis and is supported by empirical observations. Training on the source task can solve a significant portion of the remaining tasks. Our method progressively eliminates partitions of the task space at a certain rate with each iteration. If the source task selection in the previous step sufficiently addresses the remaining target tasks, we can reduce the search space at a desirable rate. Consequently, at each step, we effectively focus on a reduced search space.

We leverage the reduced uncertainty in well-sampled regions to tighten the regret bound while slightly lowering the probability \(\) in Theorem 1. For the regret analysis, we propose the following theorem based on the generalization of Lemma 5.2 and 5.4 in  to the eliminated search space.

**Theorem 2**.: _For a given \(^{}(0,1)\) and scaling factor \(_{k}=2(|X|^{2}k^{2}/6)\), the cumulative regret \(R_{K}\) is bounded by \(_{K}_{K}_{k=1}^{K}(|}{|X|})^{2}}\) with probability at least \(1-^{}\)._

Here, \(|X|\) denotes the cardinality of the set \(X\), the number of elements in \(X\). Theorem 2 matches the Theorem 1 when \(X_{k}=X\) for all \(k\). This theorem implies that regret has a tighter or equivalent bound if we can design the smaller search space instead of searching the whole space. The comprehensive proof is provided in Appendix A.3.1.

Here are some examples of restricted search space: If we consider an example where \(|X_{k}|=}|X|\), the regret can be bounded tighter than that of Theorem 1.

**Corollary 2.1**.: _Consider \(|X_{k}|=}|X|\). The regret bound would be \(R_{K}_{K}_{K} K}\) with a probability of at least \(1-^{}\)._

In cases where the search space is defined using MBTL-GS, the largest segment's length would reduce geometrically, described by \(|X_{k}| 2^{-_{2}k}|X|\).

**Corollary 2.2**.: _The regret bound for the \(|X_{k}| 2^{-_{2}k}|X|\) would be \(R_{K}_{K}_{K}^{2}/6}\) with a probability of at least \(1-^{}\)._

Proofs for Corollaries 2.1 and 2.2 are provided in Appendix A.3.2 and A.3.3, respectively. Based on our experiments presented in Section 5, the rate of elimination of the largest segment for MBTL is shown in Figure 4.

## 5 Experiments and analysis

### Setup

Our experiments consider CMDPs that span standard and real-world benchmarks. In particular, we consider standard continuous control benchmarks from the CARL library . In addition, we study problems from RL for intelligent transportation systems, using  to model the CMDPs. Surprisingly, despite the relatively low complexity of the CMDPs considered, standard deep RL algorithms appear to struggle to solve the tasks.

**Baselines.** We consider two types of baselines when evaluating our proposed algorithm: canonical and multi-policy. The canonical baselines are selected to validate multi-policy training; the multi-policy training baselines are heuristic methods designed to validate the Bayesian optimization approach.

Figure 4: Empirical results of the restriction of search space by MBTL compared to two examples from Corollaries 2.1 and 2.2.

The canonical baselines include: (1) **Independent training**, which involves independently training separate models on each task; and (2) **Multi-task RL**, where a single "universal" context-conditioned policy is trained for all tasks. The multi-policy baselines include: (3) **Random selection**, where each training task is chosen uniformly at randomly; (4) **Equidistant strategy**, which selects training tasks by equally subdividing the context space based on a given training budget, and then trains them in lexicographical order; (5) **Greedy strategy**, which greedily selects the next source task based on Assumption 1 and fixed training performance; and (6) **Sequential Oracle transfer**, which has access to generalized performance corresponding to policies for all tasks (including those not yet selected) and uses that information to greedily select the best source task at each step.

**Proposed method.** We evaluate **MBTL** with the scaling factor \(_{k}=2(|X|^{2}k^{2}/6)\).

**DRL algorithms and performance measure.** We utilize Deep Q-Networks (DQN) for discrete action spaces  and Proximal Policy Optimization (PPO) for continuous action spaces . For statistical reliability, we run each experiment three times with different random seeds. We evaluate our method by the average performance across all \(N\) target tasks after training up to \(K=\)15 source tasks or the number of source tasks needed to achieve a certain level of performance. We employ min-max normalization of the rewards for each task, and we provide comprehensive details about our model in Appendix A.4.1.

### Traffic benchmark experiments

We consider three traffic control benchmarks. First, while most traffic lights still operate on fixed schedules, RL can be used to design adaptive (1) **Traffic signal control** to optimize traffic [8; 24]. However, considering that every intersection is different, challenges persist in generalizing across intersection configurations . Given the significant portion of greenhouse gas emissions in the United States due to transportation , the second traffic domain is (2) **Dynamic eco-driving at signalized intersections**, which concerns learning energy-efficient driving strategies in urban settings. DRL-based eco-driving strategies have been developed [13; 47; 18] but still experience difficulties in generalization. Our final traffic domain is (3) **Advisory autonomy**, in which real-time speed or acceleration advisories guide human drivers to act as vehicle-based traffic controllers in mixed traffic environments [40; 7; 15]. The context space \(X\) is discretized into \(N=\{50,50,40\}\) contexts for the three domains, respectively. In Appendix A.4, we provide details about our experiments.

**Results.** Table 1 and Figure 5 summarize the results. Notably, the Oracle far outperforms the standard baselines (independent and multi-task training), indicating the potential for transfer learning and intelligent training of multiple models, respectively. MBTL rapidly approaches the Oracle within \( 10\) transfer steps, indicating that the GP effectively models the training performance and linear generalization gap models the generalization performance. It is important to note that multi-task RL studies commonly consider Independent training as a strong baseline due to its avoidance of negative transfer and other training instability issues. Indeed, independent training often (but not always) outperforms multi-task training in our experiments. Yet, our experiments show that MBTL outperforms both independent and multi-task baselines and matches their performance with **up to 30x improved sample efficiency**. Among the multi-policy baselines, MBTL often outperforms the

   &  &  &  &  \\  
**Domain** & **Context Variation** & **Independent** & **Multi-task** & **Random** & **Equidistant** & **Greedy** & **Ours** & **Sequential** \\ 
**Number of Trained Models** & \(N\) & \(1\) & \(k\) & \(K\) & \(k\) & \(k\) & \(N\) \\ 
**Traffic Signal** & **Road Length** & **0.9409** & 0.8242 & 0.9366 & 0.9337 & 0.9349 & 0.9364 & 0.9409 \\
**Traffic Signal** & **Inflow** & 0.8646 & 0.8319 & **0.8699** & **0.8712** & **0.8682** & 0.8432 & 0.8768 \\
**Traffic Signal** & **Speed Limit** & 0.8857 & 0.6083 & **0.8872** & **0.8872** & **0.8874** & 0.8867 & 0.8876 \\ 
**Eco-Driving** & **Penetration Rate** & 0.526 & 0.1945 & **0.6212** & **0.6728** & **0.5992** & **0.6148** & 0.666 \\
**Eco-Driving** & **Inflow** & 0.4061 & 0.2229 & 0.5077 & **0.5513** & 0.5299 & 0.5172 & 0.5528 \\
**Eco-Driving** & **Green Phase** & 0.385 & 0.4228 & 0.4724 & 0.4697 & 0.4678 & **0.4985** & 0.5027 \\ 
**AA-Ring-Acc** & **Hold Duration** & 0.8362 & **0.9209** & **0.9306** & **0.9225** & 0.9136 & **0.9382** & 0.9552 \\
**AA-Ring-Vel** & **Hold Duration** & 0.9589 & 0.972 & **0.9819** & **0.9819** & **0.9819** & **0.9818** & 0.9822 \\
**AA-Ramp-Acc** & **Hold Duration** & 0.4276 & 0.5158 & **0.6750** & **0.6821** & **0.6882** & **0.6964** & 0.7111 \\
**AA-Ramp-Vel** & **Hold Duration** & 0.5473 & 0.5034 & **0.7170** & **0.7386** & 0.6918 & **0.7273** & 0.7686 \\    & 0.6778 & 0.6017 & 0.7600 & 0.7666 & 0.7563 & 0.7641 & 0.7844 \\  

*Higher the better. Bold values represent the highest value(s) within the statistically significant range for each CMDP, excluding the oracle. Detailed results with variance for each method are provided in Appendix A.4.3.
*AA: Advisory autonomy benchmark, Ring: Single lane ring, Ramp: Highway ramp, Acc: Acceleration guidance, Vel: Speed guidance.

Table 1: Comparative performance of different methods on traffic CMDPs (\(K=15\))heuristic multi-policy baselines, indicating the value of adaptively selecting source tasks based on feedback. The multi-policy baselines, such as random, equidistant, and greedy strategy, also generally outperform Independent and Multi-task, indicating the general value of multi-policy training for solving CMDPs. More results are provided in Appendix A.4.

### Continuous control benchmark experiments

   &  &  &  &  \\  
**Domain** & **Context Variation** & **Independent** & **Multi-task** & **Random** & **Equidistant** & **Greedy** & **Ours** & **Sequential** \\ 
**Number of Trained Models** & \(N\) & \(1\) & \(k\) & \(K\) & \(k\) & \(N\) \\ 
**Pendulum** & **Length** & 0.7383 & 0.6830 & 0.7607 & 0.7601 & **0.7774** & **0.7755** & 0.7969 \\
**Pendulum** & **Mass** & 0.6237 & 0.5793 & 0.6647 & **0.6794** & **0.6887** & 0.6667 & 0.7132 \\
**Pendulum** & **Timestep** & 0.8135 & 0.7247 & **0.8331** & **0.8292** & **0.8497** & **0.8333** & 0.8801 \\ 
**Cartpole** & **Mass of Cart** & **0.9466** & 0.7153 & 0.8961 & 0.9044 & 0.8299 & 0.9356 & 0.9838 \\
**Cartpole** & **Length of Pole** & 0.9110 & 0.5441 & **0.9497** & **0.9484** & **0.9424** & **0.9488** & 0.9875 \\
**Cartpole** & **Mass of Pole** & 0.9560 & 0.6073 & 0.9870 & **0.9927** & **0.9916** & 0.9813 & 1.0000 \\ 
**BipedalWalker** & **Gravity** & 0.9281 & 0.7898 & **0.9654** & **0.9666** & **0.9656** & **0.9655** & 0.9674 \\
**BipedalWalker** & **Friction** & 0.9317 & 0.9051 & **0.9739** & **0.9738** & **0.9738** & 0.9725 & 0.9778 \\
**BipedalWalker** & **Scale** & 0.8694 & 0.7452 & **0.8910** & **0.8975** & **0.8990** & **0.8962** & 0.9107 \\ 
**HalfCheetah** & **Gravity** & 0.6679 & 0.6292 & **0.9086** & **0.9000** & **0.9089** & **0.9214** & 0.9544 \\
**HalfCheetah** & **Friction** & 0.6693 & 0.7242 & **0.9314** & **0.9457** & **0.9184** & **0.9225** & 0.9663 \\
**HalfCheetah** & **Stiffness** & 0.6561 & 0.7007 & **0.9191** & 0.9097 & **0.9295** & **0.9205** & 0.9674 \\   & 0.8093 & 0.6957 & 0.8901 & 0.8923 & 0.8896 & 0.8950 & 0.9255 \\    \(\)Higher the better. Bold values represent the highest value(s) within the statistically significant range for each CMDP, excluding the oracle. Detailed results with variance for each method are provided in Appendix A.4.3.

Table 2: Comparative performance of different methods on standard control CMDPs (\(K=15\))

Figure 5: **Traffic CMDP results.** Method comparison of normalized performance over \(N\) tasks. MBTL efficiently selects source training tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, with up to 30x fewer samples.

Figure 6: **Continuous control CMDP results.** Method comparison of normalized performance over \(N\) tasks. The black dotted line indicates the first training step within MBTL that exceeds both independent and multi-task baselines, with up to 43x improved sample efficiency.

To probe the generality of MBTL, we utilize context-extended versions of standard RL environments from CARL benchmark library  to evaluate our methods under varied contexts. For the Cartpole domain, we considered CMDPs with varying cart mass, pole length, and pole mass. In Pendulum, we vary the timestep duration, pendulum length, and pendulum mass. The BipedalWalker was tested under varying friction, gravity, and scale. In HalfCheetah domain, we manipulated friction, gravity, and stiffness parameters. These variations critically influence the dynamics and physics of the environments. The range of context variations was selected by scaling the default values specified in CARL from 0.1 to 10 times (\(N=100\)), enabling a comprehensive analysis of transfer learning under drastically different conditions. We provide more experimental details in Appendix A.4.

**Results.** The results summarized in Table 2 demonstrate sample efficiency and competitive performance of multi-policy training including MBTL across diverse control domains, often closely trailing the Oracle only with a small number of trained policies. Figure 6 shows that with the exception of a few context variations, MBTL generally shows superior performance. Specifically, Figure 7 illustrates the detailed process of how MBTL utilizes GP for performance estimation and chooses the next source task that maximizes the acquisition function that evaluates the expected improvement of generalized performance. MBTL achieves comparable performance to multi-task or independent baselines with **5-43x** fewer samples, highlighting its effectiveness in reducing training requirements.

#### 5.3.1 Sensitivity analysis

DRL algorithms.Figure 8 shows that MBTL remains effective with different underlying DRL algorithms--DQN, PPO, and Advantage Actor-Critic (A2C) --used for single-task training.

**Acquisition functions.** Figure 9 assesses the role of acquisition functions in Bayesian optimization. While expected improvement (EI) focuses on promising marginal gains beyond the current best, UCB utilizes both mean and

Figure 8: Sensitivity analysis on the DRL algorithm underlying MBTL (DQN, PPO, and A2C), tested on Cartpole with varying length of pole. MBTL remains effective.

Figure 7: The GP sequentially updates estimates of the performance function (blue) based on previously trained models. Then, MBTL selects the next source task that maximizes the acquisition function (red). (CMDP: Pendulum (Time step)).

Figure 9: Sensitivity analysis on acquisition functions.

variance for balancing exploration and exploitation. Overall, we find that MBTL is not particularly sensitive to the choice of optimism representation in the acquisition function, which indicates that MBTL has a weak dependence on hyperparameters.

## 6 Related work

**Contextual Reinforcement Learning.** Robustness and generalization challenges in DRL are generally addressed by a few common techniques in the literature. The broader umbrella of such methods falls under CRL, which utilizes side information about the problem variations to improve the generalization and robustness. In particular, CRL formalizes generalization in DRL using CMDPs [14; 32; 5], which incorporate context-dependent dynamics, rewards, and initial state distributions into the formalism of MDPs. The contexts of CMDPs are not always visible during training . When they are visible, they can be directly used as side information by conditioning the policy on them . In this paper, we focus on a scenario where the learner can choose which context-MDP to train on. This contrasts with other CRL works that assume context-MDPs arrive from a fixed distribution.

**Multi-task training.** Multi-task methods can help address CRL by exploiting shared structure across tasks. Prior work has leveraged techniques such as policy sketches for task decomposition  and distilled policies that capture common behaviors . However, a key limitation arises when the context is unobserved, effectively transforming the CMDP into a partially observable setting [23; 9], which complicates multi-task training. Another challenge is negative transfer, wherein training on tasks that are too dissimilar leads to instability or outright failure [22; 42; 44]. Although more recent multi-task approaches such as MOORE  and PaCo  have shown promise, they often focus on discrete task sets and are thus less suited to CRL, where tasks span a broad continuum of contexts. In this work, we include multi-task learning as a baseline to benchmark our methods.

**Zero-shot transfer and policy reuse.** Zero-shot transfer--where models trained for one environment directly perform in new, unseen settings without additional training --is an important strategy in CRL settings. For solving CMDP problems, prior works attempted to utilize zero-shot transfer to solve CMDP problems by approximation on RL algorithm and hypernetworks that maps from parameterized CMDP to a family of near-optimal solutions . Sinapov et al.  use meta-data to learn inter-task transferability to learn the expected benefit of transfer given a source-target task pair. Bao et al.  propose a metric for evaluating transferability based on information-theoretic feature representations across tasks. Taken together, these approaches highlight the importance of policy reuse, where efficiently selecting or adjusting a pre-trained policy accelerates learning and improves robustness in new contexts.

**Source task selection.** In the context of transfer learning, selecting appropriate source tasks is crucial. Li and Zhang  proposes an optimal online method for dynamically selecting the most relevant single source policy in reinforcement learning. Beyond RL, Meiseles and Rokach  emphasizes structural alignment in time-series source models to prevent performance degradation, while Poth et al.  finds that selecting aligned intermediate tasks in natural language processing boosts transfer effectiveness. Building upon these insights, we formulate the source task selection problem for CRL, enabling zero-shot transfer by estimating training performance online and leveraging structural generalization across context variations.

## 7 Conclusion

This study introduces a method called Model-based Transfer Learning (MBTL), which layers on top of existing RL methods to effectively solve CMDPs. Rather than independent or multi-task training, which trains \(N\) or 1 models, respectively, MBTL intelligently selects an intermediate number of models to train. MBTL has two key components: an explicit model of the generalization gap and a Gaussian process component to estimate training performance. MBTL achieves up to 43x improved sample efficiency on standard and real-world benchmarks. Furthermore, MBTL achieves sublinear regret in the number of training tasks. A **limitation** is that MBTL is designed for a single-dimensional context variation with a reliance on the explicit similarity of context variables. Promising directions of future work include studying high-dimensional context spaces and formalizing task similarity, as well as the development of new real-world CMDP benchmarks.