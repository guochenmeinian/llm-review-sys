# Learning Time-Invariant Representations for

Individual Neurons from Population Dynamics

Lu Mi\({}^{1,2,}\), Trung Le\({}^{2,}\), Tianxing He\({}^{2}\), Eli Shlizerman\({}^{2}\), Uygar Sumbui\({}^{1}\)

\({}^{1}\) Allen Institute for Brain Science

\({}^{2}\) University of Washington

{lu.mi,uygars}@alleninstitute.org

{tle45, shlizee}@uw.edu

goosehe@cs.washington.edu

Equal contribution

###### Abstract

Neurons can display highly variable dynamics. While such variability presumably supports the wide range of behaviors generated by the organism, their gene expressions are relatively stable in the adult brain. This suggests that neuronal activity is a combination of its time-invariant identity and the inputs the neuron receives from the rest of the circuit. Here, we propose a self-supervised learning based method to assign time-invariant representations to individual neurons based on permutation-, and population size-invariant summary of population recordings. We fit dynamical models to neuronal activity to learn a representation by considering the activity of both the individual and the neighboring population. Our self-supervised approach and use of implicit representations enable robust inference against imperfections such as partial overlap of neurons across sessions, trial-to-trial variability, and limited availability of molecular (transcriptomic) labels for downstream supervised tasks. We demonstrate our method on a public multimodal dataset of mouse cortical neuronal activity and transcriptomic labels. We report \(>35\%\) improvement in predicting the transcriptomic subclass identity and \(>20\%\) improvement in predicting class identity with respect to the state-of-the-art.

## 1 Introduction

Population recordings of neuronal activity enable relating behaviorally-relevant dynamics to the summary activity of the recorded population. While this has produced numerous insights into how the brain works , the activity and identity of individual neurons should be analyzed to achieve a mechanistic understanding at the implementation level , which may hold the key to new biologically-inspired algorithms [3; 4]. Moreover, emerging experimental evidence suggests that neurons have diverse yet stable molecular identities, which can dictate their computational roles [5; 6].

Joint (i.e., multimodal) profiling and alignment of electrophysiological features and gene expression of individual neurons suggest a good correspondence between these two modalities in slice experiments [7; 8; 9; 10; 11]. Recently, population recordings of calcium activity followed by spatially registered single-cell transcriptomic recordings enabled similar joint profiling of _in-vivo_ activity and molecular identity . Importantly, such activity depends on both the intrinsic physiological properties of neurons and the exogenous inputs (synaptic and modulatory) to those neurons, which are themselves a product of both sensory inputs to the organism and the recurrent activity in the brain.

While recording from molecularly defined neuron populations has been a popular method, these experiments do not allow for studying the concurrent responses of different neuron types to stimuli.

On the flip side, joint profiling of pannneuronal population activity and transcriptomics is slow, expensive, and not available to many research labs. Learning the association between these two observation modalities can minimize the need for joint profiling and provide neurobiological insights.

The constancy of neuronal identity in adults in the face of a potentially rapidly changing environment represents a key challenge: the inferred identity should be invariant to time and the task that the organism engages with, suggesting that the inference method should ideally be invariant to those variables. In the absence of _a priori_ information on identity, it is also desirable that the invariance extends to the number and the (arbitrary) ordering of experimental population. Moreover, a technical challenge common to many multimodal datasets is that only a relatively small fraction of the observations tend to be jointly characterized (or otherwise labeled), limiting the applicability of supervised approaches.

To address these problems, here, we develop a self-supervised approach - Neuronal Time-Invariant Representations (NeuPRINT), to infer neuronal identity from population recordings by forming a model of activity dynamics that depends only on past activity of the neuron itself and statistics of past population activity that are invariant to the ordering of the individuals and asymptotically invariant to the size of the population. We demonstrate the utility of the inferred identities by reporting the performance of a simple classifier of transcriptomic identity on those representations and other baselines. We also study the impact of providing similarly invariant yet more detailed information on the population by partitioning it into center vs surround subsets, reflecting a well-known yet simple connectional and functional property of neuronal circuits .

## 2 Related work

**Latent dynamical models:** Latent modeling of time series of population activity aims to gain insight on the dynamics of the totality of the activity of a single experimental trial . When interpretability of the latent dynamics is not a priority, powerful recurrent neural networks uncover high-fidelity representations . Recently, low-dimensional time series representations were obtained with high reproducibility, albeit without a model of the dynamics, using a contrastive loss objective . Another recent study obtained parametric representations of individual neurons in population recordings within a linear dynamics framework, where the activity of the neuron is informed by a latent representation of the population so that the results are not transferable across studies that do not have the same population with the same ordering of the neurons . The idea of informing neuronal activity prediction of neighboring activity and/or extrinsic covariates such as stimuli or behavior was also explored in previous classical studies .

Liu _et al._ proposed the use of the transformer architecture  to generate representations of individual neurons in a supervised paradigm, which do not interact with the rest of the population. Instead, those representations are combined to accomplish the supervised task.

**Multi-modal neural data:** Characterization of intrinsic electrophysiology of neurons typically uses a set of features  or mechanistic model parameters , instead of time series models, to align gene expression and physiology in multimodal slice recordings. Such experiments study only the neuron in isolation and the recordings may not reflect in-vivo activity in the context of a behavior.

A large-scale study that focuses on the _in-vivo_ activity of the individual neurons only, and not on their interaction, uses a neural network with attention layers in a supervised setting to predict the neuronal subclass, based on transgenic mouse lines .

**Other predictive models:** Implementing nonlinear autoregressive models with exogenous inputs (NARX models) using recurrently connected neural networks has a long history . Modern implementations typically use gated units to address the vanishing gradients problem . Recently, the transformer architecture , has produced impressive results in sequence-to-sequence tasks. Despite interpretability issues, such models can be significantly more powerful than classical dynamical systems models (e.g., linear dynamical system) . In this work, we use transformer as a dynamical model to integrate with both individual and population dynamics, and the representation of individual neuron is assigned with a time-invariant embedding.

**Phenomenological Hodgkin-Huxley equation:** For point neuron models, which ignore the spatial distribution of the different ion channels, the celebrated Hodgkin-Huxley (HH) equations provide a phenomenological description of the neuronal membrane potential via coupled differential equations . The membrane potential controls the generation of spiking outputs and calcium imaging provides an indirect measurement of this voltage over time . For _in vivo_ measurements, the synaptic input current injected by other neurons (pre-synaptic partners of the neuron of interest) is represented as exogenous input, which can be written as a sum over the synapses. The HH equations also include multiple neuron-specific, time-invariant parameters. Thus, the HH equations consider neuronal activity as a function of both the activities of other neurons and the intrinsic parameters of its electrophysiology.

## 3 Methods

**Implicit dynamics models for neuronal activity:** Neuronal dynamics are significantly more complex than the HH equations because the physical distribution of the various channels on the neuronal arbor, nonlinear computing abilities of the dendrites, modulatory communication between neurons,etc. can (i) modulate the neuron-specific parameter set, (ii) add more parameters to that set, and (iii) change the functional form of HH equations. Moreover, the relationship between the membrane voltage and the observable of most neuronal population activity experiments, the calcium dynamics, is itself complex. Finally, it appears impossible to perform the detailed measurements needed to fit the parameters of the HH equations based on _in vivo_ experiments with current technology. Thus, we pursue an implicit modeling approach while trying to capture the fundamental dependencies with the help of flexible deep neural network parametrization. Let \(X_{t}^{(i)}\) denote the calcium activity of neuron \(i\) at time \(t\) and consider the following equation for dynamics:

\[^{(i)}}{dt}=f(X_{t}^{(i)},_{t}^{(-i)},^{(i)}), \]

where \(_{t}^{(-i)}\) denotes the activity of all the neurons that provide (synaptic or extra-synaptic) input to neuron \(i\) at time \(t\). \(^{(i)}\) denotes a time-invariant representation for neuron \(i\), which implicitly captures the intrinsic parameters of HH equations and other such time-invariant aspects of neuronal identity.

**Permutation-invariant summary of population activity:** Even if neurons were identifiable in population imaging experiments, the connectivity of neurons and our observations of it can be considered as stochastic events . Therefore, to enable the transfer of knowledge across sessions, experiments, and individuals, it is highly desirable to approximate the dependency of neuronal dynamics on \(_{t}\) (Eq. 1) in a way that is invariant to permutations, number, and detailed identity of the neurons contributing to it. To achieve this, we propose to replace \(_{t}\) with multiple (asymptotically) invariant statistics of the activity of a neighboring population of neurons, such as average activity .

Behavioral observations, such as pupil diameter, can serve as indirect readouts on the activity of unobserved neurons. Concatenating these observations with the aforementioned statistics will enrich the exogenous input observed by the dynamical model. We call this new concatenated variable \(P_{t}\).

**Center-surround partition:** Synaptic connection probability between neurons depends on distance [12; 13]. Similarly, neuronal co-variability correlates with spatial distance . To include this neurobiological insight while maintaining permutation-invariance of our model, we propose a simple extension: we partition the population activity into two groups, center and surround, and compute the relevant invariant statistics for each group separately. Such partitioning is easy to obtain in calcium imaging experiments since the distances between the somata of neurons are readily available. Beyond its simplicity, this choice is also motivated by surround suppression being a connectivity motif in the brain [41; 42].

**Discrete-time dynamical model of neuronal activity:** We re-write neuronal dynamics in discrete time as

\[X_{t+1}^{(i)}=f(X_{t-W+1:t}^{(i)},(C_{t-W+1:t}^{(-i)},S_{t-W+1:t}^{(-i)},B_{t- W+1:t}),^{(i)}), \]

where \(X^{N T}\) represents the activity data for the whole recorded population with \(N\) neurons and \(T\) time steps. \(C\), \(S\), and \(B\) represent \(D\), \(D\), and \(D^{*}\)-dimensional permutation- and size-invariant (i.e., \(N\)) surrogates for brain activity at each time point. \(C\) and \(S\) compute identical statistics of population activity (here, mean and standard deviation) except that the statistics in \(C\) are calculated over the center partition (neurons whose distance to neuron \(i\) is at most \(\)) and the statistics in \(S\) are calculated over the surround partition (neurons whose distance to neuron \(i\) is larger than \(\)), see Appendix E.1. Here, \(\) is a hyperparameter. \(B\) denotes the contribution of time-resolved behavioral observations. Hence, the triplet \((C_{t},S_{t},B_{t})\) corresponds to \(_{t}\). \(^{(i)}^{K}\) denotes a \(K\)-dimensionaltime-invariant representation for neuron \(i\), and \(W\) denotes the width of the available temporal context. The subscripts denote the limits of the time interval within which the corresponding variable is available to \(f\).

**Self-supervised Representation Learning Framework with Transformer:** We thus propose to solve the following self-supervised optimization problem to infer both the function \(f\) and \(^{(i)}\):

\[_{f,\{^{(i)}\}_{i}}_{i,t}_{X^{(i)}_{t+1}}||X^{(i)}_{ t+1}-f(X^{(i)}_{t-W+1:t},(C^{(-i)}_{t-W+1:t},S^{(-i)}_{t-W+1:t},B_{t-W+1:t}), ^{(i)})||, \]

where \(||||\) denotes a norm. It is worth pointing out that \(f\) depends on the neuron of interest or other neurons in the population only through its explicit parameters. The reason for this choice is to maintain the transferability and ubiquity of the learned model \(f\), the first argument of the optimization, while summarizing neuronal variability with \(^{(i)}\), the second argument.

The idea of inferring invariant representations for neurons directly from _in vivo_ recordings by fitting a dynamical model (i.e., predicting activities in the next time step) is reminiscent of, and motivated by, the recent spectacular successes of "foundation models" in natural language modeling , where capturing the dynamics of a complicated system produces an implicit understanding of the dynamics and identity of its components. This procedure has been shown useful for multiple downstream tasks . Therefore, we use a transformer model  to parametrize the function \(f\). Unlike previous uses of the transformers for neural data , the input tokens in our model do not come from a countable set because raw calcium recordings are best represented by real valued signals.

To train the transformer and the time-invariant representation, we first generate masked activity inputs \(^{(i)}_{t+1}\), where neuronal activity at time \(t+1\) is zero-out. This masked activity and the past activities \(^{(i)}_{t-W+1:t}\) are concatenated with the time-invariant representations \(^{(i)}\) and permutation-invariant summary of population dynamics \(_{t-W+1:t}\) to form the inputs to the transformer. We ask the transformer to predict the activity at the masked step, and compute the loss from the predicted activity \(^{(i)}_{t+1}\) and ground truth activity \(X^{(i)}_{t+1}\). The dynamical model \(f\) and time-invariant representation \(^{(i)}\) are jointly learned during the optimization. To perform this task, the transformer has to learn the temporal progression of neuronal activity conditioned on the neuronal identity and the causal temporal

Figure 1: Overview of self-supervised representation learning framework NeuPRINT. Activities of \(N\) neurons (recorded by 2-photon calcium imaging of the mouse primary visual cortex) and behavior information (pupil size, running speed, etc.) across multiple sessions are used as inputs to fit an implicit dynamical model \(f\) and learn time-invariant \(N K\) representation \(\). The learned representations are later evaluated on supervised downstream tasks to predict transcriptomic class and subclass identities. In the optimization framework, neuron-specific representation \(_{i}\) is repeated at every time step, then concatenated with masked past neuronal activity \(^{(i)}_{t-W+1:t}\) and permutation-invariant population inputs \(_{t-W+1:t}\) to form the input. The transformer model is trained to predict neural activity \(^{(i)}_{t+1}\) at the masked step with a causal attention mask over the \(W\)-step context window.

context of individual and population statistics. An overview of NeuPRINT learning framework is depicted in Figure 1.

## 4 Experiments

### A Multimodal Dataset

We use a recent, public multimodal dataset to train and demonstrate our model: Bugeon _et al._ obtained population activity recordings from the mouse primary visual cortex (V1) via calcium imaging, followed by single-cell spatial transcriptomics of the tissue and registration of the two image sets to each other to identify the cells across the two experiments. 2-photon calcium imaging recordings were obtained with a temporal sampling frequency of 4.3Hz. And the spatial coordinates of recorded neurons are also provided. We first evaluate our approach on one animal (SB025) across 6 sessions. The recordings from this animal include 2481 neurons in total. We then extend our analysis on functional recordings from 4 mice (SB025, SB026, SB028, SB030) across 17 sessions. They contain 9728 neurons in total. Each session lasts about 20 minutes and records about 500 neurons. A small subset of neurons overlap across sessions. The subsequent transcriptomic experiment profiles mRNA expression for 72 selected genes in _ex vivo_ tissue. These genes were used to identify the excitatory vs inhibitory class labels of neurons. In addition, \(51\%\) of the neurons in the inhibitory class of SB025 also have identified subclass labels (Lamp5, Pvalb, Vip, Sncg, Sst). Finally, the dataset includes behavioral information for the mice (running speed and pupil size) during the _in-vivo_ recording as well as an assignment for each image frame (i.e., time point) that we call frame state from the set {running, stationary desynchronized}, stationary synchronized}.

### Benchmark Evaluation for Transcriptomic Identity Prediction

We use the aforementioned public dataset to introduce a new two-step benchmark: (i) self-supervised learning of time- and permutation-invariant representations for individual neurons from population activity, (ii) prediction of labels for each individual neuron based on those representations.

We introduce a downstream classification task to predict the subclass label with supervised learning, where the neurons with subclass labels from all sessions are randomly split into train, validation and test neurons with a proportion of \(80\%:10\%:10\%\). We further introduce another supervised downstream classification task to predict the class identity only (i.e., excitatory vs inhibitory). In this task, the validation and test neurons in the subclass prediction task are used as validation and test neurons for inhibitory neurons, and the same fraction of excitatory neurons are randomly selected as validation and test neurons. The rest of the recorded population from all sessions is used for training.

We first optimize the dynamical model (\(f\) in Eq. 2) and the time-invariant representation on the training set. We use past activities of the training neurons and permutation-invariant summary of population dynamics including pupil size, running speed, frame state, the mean and standard deviation of population activity, and the mean and standard deviation of center-surround activity to predict the individual neurons' activity in the next time step. After training, we fix the dynamical model \(f\) and only optimize the time-invariant representations \(\) for training, validation, and test neurons under the same self-supervised learning framework.

Following self-supervised optimization, in the second step, we evaluate the learned representation with two supervised downstream tasks (class and subclass prediction) and three simple classifiers including k-nearest neighbor (KNN), linear model, multi-layer perceptron (MLP) with one hidden layer. The training neurons' representations \(\) are used to train the classifier, and validation neurons are used to tune the hyperparameters (learning rate, hidden dimensionality, number of epochs, etc.), and the top-1 accuracies of all models are reported on the test neurons (Table 1). See Appendix for an analysis of sensitivity.

### Implementation of a spectrum of implicit dynamical models and downstream classifiers

We explore four different implicit (not mechanistic) dynamical models: linear, nonlinear, gated-recurrent network (GRU), and transformer with self-attention. We optimize the parameters of the dynamics \(f\) and the neuronal representation \(\) using gradient descent for all models.

**Linear model:** We use a linear dynamical system where the activity at the last step is predicted from a linear combination of the activities and statistics of the population activities, behavioral information from previous steps inside a temporal window and the time-invariant representation (which is repeated at each step). This corresponds to an autoregressive model with exogenous inputs, where statistics of the activities of other neurons and behavioral information constitute the exogenous input to the dynamics of the neuron of interest.

**Nonlinear model:** In addition to the linear model, a nonlinear activation was applied to the weighted activity, behavioral information, repeated neuronal representation at each step before the linear combination (i.e., nonlinear autoregressive model with exogenous inputs).

**Recurrent network with gated units:** The activity at the next step is predicted from the hidden state in addition to the activity at the current step and the repeated neuronal representation as the inputs.

**Transformer:** We implement a \(W\)-step causal attention mask such that the transformer predicts the activity of the neuron at the current time step based on the hidden states in the \(W\) previous time steps. The hidden state tasks the neuron activities, statistics of the population activities, behavioral information at that time step, and time-invariant representation repeated at each time step as inputs. We use the transformer encoder-only implementation from PyTorch  with 2 attention heads.

**Training details:** For the objective function to predict the activity, we explore both mean squared error (MSE) and negative log likelihood (NLL) with a Gaussian distribution. To train the dynamical model and representation of neurons, we use a 64-dimensional embedding for the time-invariant representation. The temporal trial window size is 200 steps for the linear, nonlinear models, recurrent network and transformer. The batch size is 1024. We use the Adam optimizer  with a learning rate of \(10^{-3}\).

**Downstream supervised classification:** For the linear and MLP classifiers, we use the cross-entropy loss to train the model. For KNN, we use the scikit-learn implementation  with the number of nearest neighbors \(k=5\).

### Baselines

**LOLCAT and its variants:** LOLCAT  is a supervised framework for predicting cell types from individual neuronal activities using a multi-head attention network. Since the attention in LOLCAT is a simple weighted sum operation, we implement two additional supervised variants Transformer+ISI and Transformer+Raw using the self-attention mechanism employed in NeuPRINT for a fair comparison. These two variants follow the attention design of  and use a special classification token to represent the classification of the entire neuronal activity . Transformer+ISI operates on the inter-spike interval (ISI) distributions input summarized from non-overlapping sub-windows of continuous 2-photon calcium recordings. We use suite2p package  to infer spikes from raw calcium traces and compute the ISI distributions. On the other hand, Transformer+Raw operates directly on the raw calcium traces. Unlike NeuPRINT, LOLCAT and the two supervised variants train both the attention network and classifier (linear or MLP) in an end-to-end fashion, using neuronal class and subclass labels during learning (See Appendix for details). As a consequence of the supervised training scheme, LOLCAT does not extract time-invariant representations of neuronal identity and its performance is constrained by the number of labels available in the dataset.

**Principal component analysis:** We project the raw calcium activities to a low-dimensional representation using Principal Component Analysis (PCA) and evaluate the effectiveness of this representation for downstream tasks. The projection is performed on a randomly selected sub-window in the raw recordings of each neuron, therefore its projected representation is also not time-invariant.

**Uniform manifold approximation and projection:** Similar to PCA, we project the raw calcium activities to a low-dimensional representation using Uniform Manifold Approximation and Projection (UMAP) , and evaluate the resulted time-variant representation by downstream classifiers.

**Random:** We further generate random representations for individual neurons, and train a supervised classifier on the random representations to measure the chance level of prediction.

## 5 Results

Self-supervised Learning Demonstrates Superior Generalization Capabilities in Data-Limited Scenarios

We evaluate our proposed method NeuPRINT and other baselines under three categories as shown in Table 1: (i) time-invariant vs. time-variant; (ii) self-supervised representation learning vs. end-to-endsupervised learning vs. unsupervised learning; (iii) activity of the neuron of interest ("Individual") as the only input to the dynamics model \(f\) vs. permutation-invariant representation of population dynamics provided as exogenous input to \(f\). Under the data-limited scenario (i.e., a small amount of labeled samples), which describes a vast majority of neuroscience datasets, we find that our self-supervised representation learning model NeuPRINT with a supervised downstream MLP classifier outperforms the current state-of-the-art approach LOLCAT by \(>35\%\) and its variants by \(>19\%\) in the subclass prediction task and outperforms LOLCAT by \(>20\%\) and its variants by \(>13\%\) in the class prediction task. Since LOLCAT is optimized using features extracted from non-overlapping sub-windows in an end-to-end supervised learning approach, it does not generate time-invariant representations that are critical to generalize across trials. Moreover, as shown in the confusion matrices in Figure 2, when the data in the subclass and class prediction tasks has an imbalanced distribution under the data-limited regime, our method can generate more balanced predictions across labels than LOLCAT. Both of these methods outperform two other standard unsupervised representation learning baselines, PCA and UMAP, which also extract time-varying representations across non-overlapping sub-windows. All of the evaluated models perform above the chance level, and we find that one-hidden layer MLP classifier improves classification accuracy over the linear classifier or KNN.

Ablation studies (Table 1 and Figure 3) show that using a permutation-invariant summary of population dynamics is critical to improving NeuPRINT's performance on the downstream subclass prediction.

    &  &  &  \\   &  Lower \\ Bound \\  } &  Data-Limited \\ Supervised \\  } &  &  Self \\ supervised \\  } &  Data-Limited \\ Supervised \\  } &  Self \\ Supervised \\  } &  Data-Limited \\ Supervised \\  } &  Self \\ supervised \\  } &  Data-Supervised \\  } &  Self \\ supervised \\  } \\   &  &  &  &  Trans Trans \\ +ISI +Raw \\  } & PCA & UMAP &  &  &  Trans Trans \\ +ISI +Raw \\  } &  \\    & & & &  & & & & & & & &  &  &  &  \\   & KNN & 0.260 & — & — & — & 0.263 & 0.281 & 0.415 & — & — & — & — & **0.610** \\   & Linear & 0.256 & 0.404 & 0.474 & 0.474 & 0.316 & 0.404 & 0.537 & 0.474 & 0.491 & 0.386 & **0.683** \\   & MLP & 0.302 & — & 0.561 & 0.439 & 0.330 & 0.340 & 0.512 & — & — & 0.526 & 0.386 & **0.756** \\   & KNN & 0.488 & — & — & — & 0.536 & 0.584 & 0.652 & — & — & — & **0.711** \\   & Linear & 0.526 & 0.600 & 0.664 & 0.669 & 0.544 & 0.576 & 0.697 & 0.608 & 0.680 & **0.793** \\    & MLP & 0.523 & — & 0.640 & 0.664 & 0.565 & 0.520 & 0.752 & — & 0.632 & 0.616 & **0.807** \\   

Table 1: Top-1 accuracy of transcriptomic label prediction based on representations learned by (i) our proposed self-supervised representation learning from neural dynamics framework NeuPRINT, (ii) the supervised learning method LOLCAT  and its variants Transformer+ISI and Transformer+Raw, (iii) unsupervised baselines PCA and UMAP, (iv) random representations (to determine the chance-level). Note that this experiment corresponds to a data-limited regime due to limited labeled data. We performed classification using three classifiers (KNN, Linear, MLP) and two tasks: predicting the subclass from the set {Lamp5, Pvalb, Vip, Sncg, Sst}, and predicting the cell class from the set {excitatory, inhibitory}. We study the performance of different models using only individual neuronal activity vs adding population statistics as input.

Figure 2: **Left: Relative abundances of subclass and class labels. Right: Confusion matrices of our self-supervised representation learning framework NeuPRINT and supervised learning method LOLCAT  based on predicting the cell class and subclass labels. While both of the self-supervised and supervised steps are learned with all available subclasses, we excluded the Sncg population from the confusion matrices because it represents a negligible fraction of the test set with the \(80\%:10\%:10\%\) split, so that quantification for this population would not be reliable.**

On the other hand, this effect is not significant in the class prediction task, suggesting that the intrinsic electrophysiology of neurons is significantly different between the excitatory vs inhibitory classes.

### Learning Representations Across a Spectrum of Implicit Dynamical Models

We next investigate learning representations using our NeuPRINT framework over a spectrum of implicit dynamical models ranging from simple linear and nonlinear dynamical models to more advanced deep learning architectures such as gated recurrent networks and transformers. We also evaluate the performance of the models under two different objective functions (mean squared error vs Gaussian negative log likelihood). The results are shown in Figure 4. We find that the transformer, which leverages the powerful attention mechanism  to preserve the information over a large temporal context, achieves the best performance in predicting the masked (future) neural activities, as quantified by NLL and MSE loss. This ability to capture the dynamics more faithfully explains the transformer's superior performance in inferring neuron identity since neuronal physiology correlates with molecular expression [10; 29]. We note that, for this relatively small dataset, the MSE loss performs better than the NLL loss based on the downstream classification accuracy.

Permutation-Invariant Summary of Population Dynamics Enhances the Time-invariant Representation of Individual Neurons

To further investigate the role of each component in the permutation-invariant summary of population dynamics, we perform a series of ablation studies as shown in Figure 4. We add each input (running speed, pupil size, frame state, population activity, and center-surround activity) to NeuPRINT one at a time. We find that all of the proposed components contribute to the success of the time-invariant representations as evaluated by the downstream transcriptomic classification task. These results support the perspective put forth in Section 3: how the neuron reacts to external inputs (hence the summary variables proposed here) forms a part of the neuron identity. Overall, we find using all of the available permutation-invariant components of the summary of the population recording improves the accuracy by 22% in subclass prediction.

Figure 4: **Left:** Top-1 accuracy (or loss) of learned representations with different dynamical models (linear, nonlinear, recurrent, transformer) in the subclass prediction task. **Right:** Ablation studies to dissect the impact of the different components of the permutation-invariant summary of population dynamics including running speed, pupil size, frame state, population activity, center-surround activity in improving the accuracy, as in Table 5. One component is added at a time from left to right.

Figure 3: Accuracy of transcriptomic subclass and class prediction of NeuPRINT and baselines on single-mouse spontaneous activity recordings, with and without inputs from population statistics.

### Cell Type Identifiability Tends to Increase with Stimulus Relevance and Complexity

We further test our model NeuPRINT and other baselines on recordings with 3 sets of visual stimuli (drifting gratings and two different natural scene image sets ). The results summarized in Figure 5 suggest an increase in cell type identification accuracy of NeuPRINT from _in-vivo_ activity as stimulus relevance and complexity increases, e.g., higher accuracy (\( 5\%\)) in subclass prediction based on Natural Scenes 1 compared to spontaneous activity recording.

### Extensions to Multiple Animals

We further extend our evaluations from one animal to multiple animals. We report the evaluations for all animals on the extended dataset in Table 2. While the performance of LOLCAT increases on the class prediction task with this larger dataset, it still remains less accurate than our model across all tasks and classifiers. For subclass prediction, where the distribution of cells across subclasses is highly imbalanced, our method NeuPRINT outperforms LOLCAT by \( 23\%\), and also by \( 10\%\) in class prediction (See discussion on the preliminary nature of this study below).

    &  &  &  \\   &  &  &  &  &  &  \\   &  & — &  &  &  &  &  &  \\   &  &  &  & Trans Trans &  &  &  NeuPRINT \\ without population \\  } &  &  &  &  \\   & & & & +ISI & & +Raw & & & 
 NeuPRINT \\ without population \\  } &  &  &  &  \\   & KNN & 0.174 & — & — & — & 0.333 & 0.348 & 0.419 & **0.552** \\   & Linear & 0.340 & 0.457 & 0.449 & 0.493 & 0.304 & 0.384 & 0.552 & **0.590** \\   & MLP & 0.362 & — & — & 0.442 & 0.423 & 0.384 & 0.406 & 0.552 & **0.685** \\   & KNN & 0.581 & — & — & — & 0.670 & 0.613 & 0.659 & **0.700** \\   & Linear & 0.667 & 0.700 & 0.710 & 0.675 & 0.645 & 0.660 & **0.746** & **0.746** \\    & MLP & 0.660 & — & 0.702 & 0.707 & 0.682 & 0.667 & 0.770 & **0.800** \\   

Table 2: **Extensions to multiple animals**: Top-1 accuracy of transcriptomic label prediction based on (i) the representations learned by our proposed self-supervised representation learning from neural dynamics framework NeuPRINT, (ii) the supervised learning method LOLCAT and its variants Transformer+ISI and Transformer+Raw, (iii) unsupervised baselines PCA and UMAP, (iv) random representations (to determine the chance-level). Note that this experiment corresponds to a data-limited regime due to limited labeled data. We performed classification using three classifiers (KNN, Linear, MLP) and two tasks: predicting the cell class from the set \(\{\}\) and predicting the subclass from the set \(\{\}\).

Figure 5: Accuracy of transcriptomic subclass and class prediction of NeuPRINT and baselines on single-mouse recordings during spontaneous activity and visual stimuli-driven (drifting gratings, natural scenes) activity.

Discussion and Conclusion

In this work, we studied the problem of inferring neuronal identity from _in vivo_ recordings. Motivated by the fact that _in vivo_ physiology of a neuron has two distinct components (the neuron itself, and the synaptic and modulatory inputs it receives), we presented a self-supervised approach to infer identity vectors for neurons based on a model of neuronal dynamics with exogenous inputs. While letting the activities of neighboring neurons represent the exogenous input directly could be a natural choice, this would prohibit the transferability of the model because a biologically meaningful ordering of neighboring neurons is not available in these recordings. This key observation led us to propose calculating permutation-invariant statistics of the activity of the neighboring neurons.

In the field of natural language processing (NLP), learning the dynamics of the underlying system has been shown to enable success in multiple downstream tasks . Inspired by this, we proposed to use the identity vectors inferred by our dynamics model in predicting the molecular labels of the neurons. This was enabled by a recent, public dataset that profiles both _in vivo_ calcium activity and subsequent transcriptomic expression in individual neurons . Learning the dynamics is a self-supervised task. We trained simple classifiers with limited ground-truth labels to subsequently predict the molecular label, based on the invariant representations inferred by this self-supervised step. We believe this two-step approach could constitute a benchmark for future studies on this topic.

We experimented with different dynamics models. We reported that, consistent with the impressive findings in NLP [28; 43], the transformer architecture with its self-attention mechanism provides the best results among the architectures we considered. Due to its feedforward structure, this architecture also enables fast inference, which could be useful for experimental setups with real-time feedback.

Ablation experiments demonstrated the merit of providing permutation-invariant inputs and information on the overall state of the nervous system (e.g., pupil size). This set of experiments also revealed that more detailed input representations could further improve performance while retaining the desired permutation-invariance property. In particular, motivated by insights on circuit motifs in neuronal networks [12; 13; 14], we demonstrated a center-surround setup, which partitions the available population into two concentric sets and calculates identical statistics for each set separately.

Our approach can be improved when applied to multi-region, cross-session and cross-animal recordings by learning extra embeddings that represent different brain regions, sessions or animals. In this sense, our multi-animal experiments represent a limited, preliminary study. It may also be possible to study recordings from healthy vs diseased brains or from individuals belonging to different species using a single model.

Our study represents an initial attempt. Ever-growing neuroscience datasets represent a straightforward way to improve accuracy. On the technical side, engineering of the invariant features can extend the list of meaningful and invariant statistics of population activity. Contrastive learning of neuronal identity can improve classification accuracy by encouraging the model to depend more on the identity input. Our center-surround setup should also be considered as an initial attempt at capturing existing neurobiological insights in the kind of dynamics models studied in this paper. Studying the transferability of the proposed model across individuals is a high-priority direction for future research.

## 7 Reproducibility

All optimizations are performed on one NVIDIA Tesla V100 GPU. Details are included in Appendix E. We released our software ([https://github.com/lumimim/NeuPRINT/](https://github.com/lumimim/NeuPRINT/)) for reproducibility.

## 8 Acknowledgement

LM is supported by the Shanahan Foundation Fellowship. LM and US thank the Allen Institute founder, Paul G Allen, for his vision, encouragement, and support. ES and TL acknowledge the support in part by A3D3 National Science Foundation grant OAC-2117997 and the Department of Electrical Computer Engineering at the University of Washington. ES also acknowledges the support in part by the Washington Research Foundation Fund and the Department of Applied Mathematics at the University of Washington.

Broader ImpactSince we studied a basic neuroscience problem using recordings from mice, we do not expect this paper to have an immediate societal impact. We hope that, in the long term, variants of our method will be helpful to better understand how the brain works and the physiological consequences of the various diseases of the brain. It is nevertheless important to keep in mind that technical progress, such as reported in our study, could potentially be adapted without much difficulty to malicious use.