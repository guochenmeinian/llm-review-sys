# Equivariant Adaptation of Large Pretrained Models

Arnab Kumar Mondal

Mila, McGill University

ServiceNow Research

&Siba Smarak Panigrahi

Mila, McGill University

&Sekou-Oumar Kaba

Mila, McGill University

&Sekou-Oumar Kaba

Mila, McGill University

&Sai Rajeswar

ServiceNow Research

&Siamak Ravanbakhsh

Mila, McGill University

These authors contributed equally to this work

###### Abstract

Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple _canonicalization network_ that transforms the input to a canonical form before feeding it to an unconstrained _prediction network_. We show here that this approach can effectively be used to make a large pretrained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are able to make large pretrained models equivariant while maintaining their performance. This significantly improves the robustness of these models to deterministic transformations of the data, such as rotations. We believe this equivariant adaptation of large pretrained models can help their domain-specific applications with known symmetry priors.

## 1 Introduction

Deep neural networks (DNNs) have demonstrated exceptional success in dealing with a range of data modalities, such as image and point cloud [1; 2; 3; 4; 5]. The majority of these applications require working with input data that undergoes various transformations, such as rotation, scaling, or translation in image data. Invariant and equivariant networks are "aware" of such transformations; they are specifically designed to ensure that the network's behaviour is insensitive to input transformations, leading to more accurate and robust predictions and improved sample complexity [e.g., 6; 7; 9; 10; 11; 13; 14]. As such, they have emerged as a promising solution to a wide range of computer vision and point cloud processing tasks [7; 9; 10; 15; 16; 17; 18; 19; 20], including classification, object recognition, and segmentation, among others.

In parallel to this, it has been shown that scaling models, both in the number of parameters and amount of training data, systematically improve their performance [21; 23; 24; 25; 26; 27; 28]. Pre-training on massive datasets has emerged as a practical method to harness this advantage. Several large models are now publicly available, and fine-tuning them on specific applications has proven to be a successful strategy in a variety of applications. However, such _foundation models_ are typically not equivariantto most transformations except translations, as the currently known methods to achieve this are non-trivial to adapt to some architectures and remain computationally expensive.

This work aims at bridging the gap between foundation models and the systematic generalization offered by equivariance. The key idea is to decouple equivariance from the main task-specific DNN. One way of achieving this is through the idea of learned _canonicalization_ function , where a canonicalization network learns to transform the data into a canonical form. The pretrained prediction network is then applied to this canonical form. Another way to achieve this decoupling is through the idea of _symmetrization_ where all the transformations of a datapoint are passed through the prediction network and the final output is computed as the average of these predictions [12; 8]. However, symmetrization comes at a notably higher computational cost compared to canonicalization, primarily due to the requirement of a forward pass through the prediction network for each data transformation. This problem is even more pronounced when employing a large-scale prediction network. Hence, the canonicalization technique is a more efficient and practical choice for making large pretrained network equivariant.

Nevertheless, a naive application of the canonicalization idea fails in practice. This is because the canonicalization network's choice of canonical form can result in a change of distribution in the input of the pretrained prediction network - that is, canonicalization can be uninformed about the preference of the prediction network, undermining its performance.

As outlined in Figure 2, we resolve this misalignment by matching the distribution of predicted canonical forms with the dataset. We demonstrate empirically that imposing this prior is essential in an equivariant adaptation of pretrained models across different domains and datasets. Our approach offers a practical solution to obtaining large-scale equivariant models by providing an independent module that can be plugged into existing large pretrained DNNs, making them equivariant to a wide range of transformation groups.

## 2 Background

Groups and TransformationsGroups capture the abstract structure of symmetry transformations, which is due to their compositional form - e.g., the composition of a transformation with its inverse results in an identity transformation. Formally, a group \(\) is a set equipped with an associative binary operation, such that the set is closed under this operation, and each element \(g\) has a unique inverse. A group \(\) can _act_ on a set \(\) by transforming its elements \(\) through a bijection. Assuming

Figure 1: Predicted masks from the Segment Anything Model (SAM) , showcasing both the original model and our proposed equivariant adaptation for \(90^{}\) counter-clockwise rotated input images taken from the COCO 2017 dataset . Our method makes SAM equivariant to the group of \(90^{}\) rotations while only requiring \(0.3\%\) extra parameters and modestly increasing the inference time by \(7.3\%\). For further insights into this experiment, refer to Section 4.1.1.

is a vector space, the linear action of \(g\) on \(\) is given by \((g)\). The function \(:()\) is called a group representation, and \(()\) is the set of invertible linear transformations on \(\). For example, if \(=SO(2)\) is the group of 2D rotations, its action on any image \(\) could rotate it around some center.

Equivariance in Deep LearningIn the context of deep learning, we are interested in designing models that are invariant or equivariant to the input data's symmetries. Suppose that input data live in a space \(\) with some symmetry structure that we would like to preserve in our deep learning model.

An equivariant function \(f:\) is a function that commutes with the group action of \(G\) on \(\) and \(\), i.e., for all \(g G\) and \(\), we have:

\[f((g))=^{}(g) f()\] (1)

where \(^{}:G^{}\) is another group representation that acts on the output space \(\). In other words, an equivariant function is a function whose output changes in a predictable way under transformations of the input induced by the group \(\). If \(^{}\) is the trivial representation, i.e., \(^{}(g)=I\) or identity transformation for all \(g\), then we say that \(f\) is invariant to the group action of \(\).

Equivariance with Learned CanonicalizationThe concept of invariance involves ensuring that all elements within a group orbit produce the same output when processed by a function \(f\). To achieve this, elements can be mapped to a canonical sample from their orbit prior to applying the function. On the other hand, equivariance involves mapping elements to a canonical sample, applying the function, and then transforming them back to their original position within the orbit. This notion can be precisely expressed by representing the equivariant function \(f\) in a canonicalized form as follows:

\[f()=^{}(c()) p( (c()^{-1}))\] (2)

where the function \(p:\) is called the _prediction function_ and the function \(c:\) is called the _canonicalization function_. In simpler terms, the function \(c\) maps input data to group elements such that the inverse action of these elements can return the data back to its canonical sample.

The function \(f\) in eq. (2) achieves equivariance for any prediction function as long as the canonicalization function is also \(\)-equivariant - that is \(c((g))=gc()\; \;g,\). Previous work by  used a direct approach to design a canonicalization function using existing equivariant neural network architectures.

This formulation offers the advantage of removing the constraint on the main prediction network and placing it instead on the network that learns the canonicalization function. As shown in , this decoupling can be partial, where the prediction network is itself equivariant to some symmetry transformations, and the canonicalization function is used for equivariance to additional symmetries.

## 3 Method

The flexibility of the equivariance with canonicalization approach enables the conversion of any existing large pretrained Deep Neural Network (DNN) into an equivariant model with respect to certain known transformations. To achieve this, the pretrained DNN can be utilized as the prediction network in the formulation provided by eq. (2). Subsequently, a canonicalization function can be designed to produce the elements of the known group of transformation while maintaining equivariance with respect to this group.

One could learn the canonicalization function while optionally finetuning the prediction function using the same task objective - in our experiments, we consider both zero-shot and fine-tuning setup.

The performance of the model requires the _alignment_ of these two networks. For example, if the canonicalization network produces images that are upside-down, compared to those that the pretrained network is expecting, the overall performance is significantly degraded. In addition to alignment, there is an _augmentation_ effect that further muddies the water: during its training, the canonicalization network performs data augmentation. As we see shortly, one needs to consider both alignment and augmentation effects when analyzing the performance of this type of equivariant network.

When both networks are trained together from scratch, the alignment is a non-issue, and (unwanted) augmentation can degrade or improve performance, depending on the extent of symmetry in the dataset. However, when dealing with pretrained prediction networks one needs to also consider the alignment effect. One could then think of freezing the pretrained prediction network, therefore avoiding unwanted augmentation, and backpropagating the task loss through it to align the canonicalization network. However, this can become prohibitively expensive for large pretrained models, such as segment anything (SAM) considered in this work. We propose an alternative, where we directly regularize the canonicalization network to produce canonical forms consistent with the training data, which in turn aligns with the prediction network.

### Learning Canonicalization, Augmentation and Alignment

When learning the canonicalization function during training, the process implicitly performs dynamic augmentation of the prediction network. Consider a model designed to be equivariant to a certain group of transformations \(\) by canonicalization. At the start of training, the randomly initialized weights of the canonicalization function will output random canonical orientations for each data point. This has a similar effect to data augmentation using the group \(\) for the prediction network. As training progresses, the canonical orientations for similar-looking images begin to converge, as demonstrated in Figure 3 in Appendix C, causing the augmentation effect to diminish. Thus, in addition to guaranteeing equivariance, this formulation provides a free augmentation effect to the prediction network.

However, there are two scenarios where the augmentation provided by learning the canonicalization function can be detrimental:

First, in cases where the training only requires small transformations as augmentations, providing all the transformations of a group \(\) can actually hurt the performance of the prediction network during the start of _training_ or _fine-tuning_. For example, in natural image datasets like CIFAR10, small rotation augmentations (from \(-10\) to \(+10\) degrees) are beneficial, while a canonicalization function would output any rotation from \(-180\) to \(+180\) degrees during the beginning phase of training. This can lead to unstable training of the prediction network and hinder the model's performance by training on additional data that is far outside the distribution of the train set. We show this in Table 1 by training a prediction network with different rotation augmentations, including the one due to learned canonicalization on both CIFAR datasets. Furthermore, we also observe that this effect is more pronounced when the prediction network is trained from scratch, and the dataset is more complicated with a larger number of classes. This effect can be understood as an example of the _variance-invariance tradeoff_ introduced by . Since, the test distribution is not perfectly symmetric under rotations, training with arbitrary augmentations biases the prediction function.

Second, we notice that relying solely on the task loss objective is not be sufficient for the canonicalization function to learn the correct orientation. This could be due to the small size of the finetuning dataset compared to the pretraining dataset. We see experimentatlly that this leads to the canonicalization function outputting inconsistent canonical orientations during inference, impacting the prediction network's performance. For instance, on CIFAR10 (non-augmented), we expect the canonical orientation for every datapoint to be similar after training. However, from Figure 4 in Appendix C, we can see that the canonical orientations for the test set are distributed uniformly from \(-180\) to \(+180\) degrees even after training until convergence of the task objective. As a result, during inference,

    &  &  \\  Prediction Network \(\) & Rotation Augmentation & Acc & \(C\)8-Avg Acc & Acc & \(C\)8-Avg Acc \\   & \(-10\) to \(+10\) degrees & **90.96 \(\) 0.41** & 44.87 \(\) 0.60 & **74.83 \(\) 0.15** & 37.14 \(\) 0.42 \\  & \(-180\) to \(+180\) degrees & 84.60 \(\) 1.83 & 81.04 \(\) 1.86 & 61.07 \(\) 0.27 & 59.42 \(\) 0.70 \\  & Learned & Canonicalization (LC)  & 83.11 \(\) 0.35 & **82.89 \(\) 0.41** & 59.84 \(\) 0.67 & **59.45 \(\) 0.49** \\   & \(-10\) to \(+10\) degrees & **96.97 \(\) 0.01** & 57.77 \(\) 0.25 & **85.84 \(\) 0.10** & 44.86 \(\) 0.12 \\  & \(-180\) to \(+180\) degrees & 94.91 \(\) 0.07 & 90.11 \(\) 0.19 & 80.21 \(\) 0.09 & 74.12 \(\) 0.05 \\   & Learned & Canonicalization (LC)  & 93.29 \(\) 0.01 & **92.96 \(\) 0.09** & 78.50 \(\) 0.15 & **77.52 \(\) 0.07** \\   

Table 1: Effect of augmentation on the Prediction network. Top-1 classification accuracy and \(\)-Averaged classification accuracy for CIFAR10 and CIFAR100 . \(C\)8-Avg Acc refers to the top-1 accuracy on the augmented test set obtained using the group \(=C\), with each element of \(\) applied on the original test set.

the prediction network will view images with different orientations and underperform. This issue arises from the fact that the prediction networks are not inherently robust to these transformations.

### Canonicalization Prior

As we have seen, the canonicalization network may induce a shift in canonical orientations away from those present in the pretraining datasets. To further encourage the canonicalization function to align inputs in an orientation that will help the prediction network, we introduce a simple regularizer we call canonicalization prior (CP). It is motivated by noticing that the images or point clouds in the finetuning dataset provide useful information, as we expect these orientations to be similar to those of the training dataset. We, therefore, take as prior that the canonicalization should align inputs as closely as possible to their original orientation in the finetuning dataset.

We derive the regularizer by taking a probabilistic point of view. The canonicalization function maps each data point to a distribution over the group of transformations, denoted as \(\). Let \(_{c()}\) denote the distribution induced by the canonicalization function over \(\) for a given data point \(\). We assume the existence of a canonicalization prior associated with the dataset \(\) that has a distribution \(_{}\) over \(\). To enforce the canonicalization prior, we seek to minimize the Kullback-Leibler (KL) divergence between \(_{}\) and \(_{c()}\) over the entire dataset \(\) that is \(_{}=_{}[D_{KL }(_{}_{c()})]\).

We assume that the canonicalization function \(c\) estimates the parameters of a distribution over rotations with probability density function \(p( c())\). We denote the probability density function of the prior to be \(q()\). Since the prior distribution is kept constant, minimizing the KL divergence is equivalent to minimizing the cross-entropy, and the prior loss simplifies to:

\[_{}=-_{}\;_{ q}[ p( c() )]\] (3)

Hereafter, we derive the regularization for the discrete and continuous cases separately.

#### 3.2.1 Discrete Rotations

We first consider the group of 2D discrete rotations, the cyclic group \(C_{n}\), which can be seen as a discrete approximation to the full rotation group \(SO(2)\). In this case, we consider a categorical distribution over group elements, with the prior distribution having probability mass of 1 for the identity element. Then \(p_{}()=_{,}\), where \(_{,}\) is the Kronecker delta function and the cross entropy in eq. (3) becomes \(- p_{c(x)}()\). Hence, the loss becomes \(_{}=-_{x} p_{c(x)}( )\). In other words, the regularization loss is simply the negative logarithm of the probability assigned by the canonicalization function to the identity element \(\) of the group.

Practical ImplementationFor images, similar to , the canonicalization network needs to output logits corresponding to every group element in the discrete cyclic group \(C_{n}\). This can be achieved by using a Group Convolutional Network  or an \(E(2)\)-Steerable Network  that produces outputs using _regular_ representation. To design the canonicalization function, we take a spatial average and

Figure 2: Training and inference with our proposed regularized canonicalization method. The canonicalization function outputs a distribution over image orientations used to canonicalize the input image. Additionally, during training, this predicted distribution is regularized to match the orientations seen in the dataset.

get logits corresponding to every element in the group along the fibre dimension. This is similar to the approach used in . Now, we can get a discrete distribution over the group elements by taking a softmax and minimizing the prior loss along with the task objective. During training, to ensure consistency with the implementation in  for fair comparisons across all experiments, we utilize the argmax operation instead of sampling from this distribution using Gumbel Softmax  and employ the straight through gradient trick . All our image-based experiments use this discrete rotation model.

#### 3.2.2 Continuous rotations

When considering canonicalization with continuous rotations, it is natural to use the matrix Fisher distribution introduced by . It is the analogue of the Gaussian distribution on the \(SO(n)\) manifold and is defined as

\[p()=)} ([^{T}])\] (4)

where \(^{n n}\) is the parameter of the distribution and \(n()\) is a normalization constant. Interpretation of the parameter \(\) and useful properties of the distribution are provided in . In particular, considering the proper singular value decomposition \(=^{T}\), we find that \(}^{T}\) is the mode of the distribution and the singular values \(\) can be interpreted as concentration parameters in the different axes. We therefore set \(=s\) to obtain the isotropic version of the distribution,

\[p(},s)= (s[}^{T}])\] (5)

where the normalization constant only depends on \(s\) (Theorem 2.1 of ). Note that on \(SO(2)\), this becomes the Von-Mises distribution as expected.

We introduce the following result, which will allow to derive the regularization.

**Proposition 1**.: _Let \(p\) and \(q\) be matrix Fisher distributions of \(\)_

\[p(}_{p},s_{p})= )}(s_{p}[}_{p}^{T} ]), q(}_{q},s_{q })=)}(s_{q}[ }_{q}^{T}]).\]

_The cross-entropy is given by_

\[_{ q}[ p( }_{p},s_{p})]=N(s_{q})s_{p}( }_{p}^{T}}_{q})+ c(s_{p})\] (6)

_where \(N(s_{q})\) only depends on \(s_{q}\)._

The proof follows in Appendix A

Setting the location parameters of the estimated and prior distributions as \(_{c(x)}\) and \(}_{q}=\) respectively, we find that the canonicalization prior eq. (3) is given by

\[_{}=-(_{c(x)} )=_{c(x)}-_{F}\] (7)

where we have eliminated terms that do not depend on \(_{c(x)}\) and \(=N(s_{q})s_{p}\). Following intuition, the strength of the regularization is determined by the concentrations of the distributions around their mode.

Practical ImplementationFor image domain, canonicalization network needs to output rotation matrices \(_{c(x)} SO(2)\) that equivariantly transforms with the input image. This can be achieved by using a \(E(2)\)-Steerable Network  that outputs two vector fields. To design the canonicalization function we can take a spatial average over both vector fields and Gram Schmidt orthonormalize the vectors to get a 2D rotation matrix. While this sounds promising in theory, in practice we found it empirically difficult to optimize using the loss to enforce canonicalization prior (see Appendix D). Webelieve this warrants further investigation and present a potential novel research direction to explore. However, in the domain of point clouds, we discovered that combining the implementation from , which outputs 3D rotation matrices or elements of \(SO(3)\), with the regularization loss to enforce the canonicalization prior leads to remarkably effective results. This approach is demonstrated in our model for rotation-robust point cloud classification and part segmentation, leveraging pretrained PointNet  and DGCNN  architectures (see Section 4.2).

## 4 Experiments

In this section, we present experimental results on images and point clouds to evaluate our method of achieving equivariance with minimal modifications to pretrained networks. The key benefit of our approach is showcased by demonstrating its robustness when evaluating out-of-distribution data that arise from known transformations applied to the test set.

Our code is available at https://github.com/arnab39/EquivariantAdaptation

### Image domain

Experiment Setup.We use ResNet50  and ViT-Base , which are pretrained on ImageNet-1K dataset  for our image experiments. These are widely used for image classification tasks, and their pretrained checkpoints are publicly available 12. We finetune these models on several benchmark natural image classification datasets, including CIFAR10 , CIFAR100 , and STL10 . Moreover, we incorporate four different strategies to finetune the pretrained models, namely: 1) Vanilla, 2) Rotation Augmentation, 3) Learn Canonicalization, and 4) Prior-Regularized Learned Canonicalization. For the canonicalization function, we use a \(C8\)-equivariant convolutional network. The details of the architecture are available in Appendix B. We jointly train the canonicalization function and fine-tune the pretrained image classification networks. Our total loss is given by \(_{total}=_{fine-tune}+_{prior}\), where \(_{prior}\) is defined in Eq. 3, \(_{fine-tune}\) refers to the cross-entropy loss for classification, and \(\) is a hyperparameter which is set to 100.

The _Vanilla_ model refers to fine-tuning the pretrained checkpoints using data augmentations such as horizontal flips and small angle rotations, while the _Rotation Augmentation_ model covers angles ranging from \(-180\) to \(+180\) degrees. Although _Rotation Augmentation_ model is not equivariant,

    &  & ViT \\  Datasets \(\) & Model & Acc & \(C8\)-Avg Acc & Acc & \(C8\)-Avg Acc \\   & Vanilla & **96.97 \(\) 0.01** & 57.77 \(\) 0.25 & **98.13 \(\) 0.04** & 63.59 \(\) 0.48 \\  & Rotation Augmentation & 94.91 \(\) 0.07 & 90.11 \(\) 0.19 & 96.26 \(\) 0.15 & 93.67 \(\) 0.39 \\  & Learned Canonicalization (LC)  & 93.29 \(\) 0.01 & 92.96 \(\) 0.09 & 95.00 \(\) 0.01 & 94.80 \(\) 0.02 \\  & \(C8\)-Avg. & 95.76 \(\) 0.07 & 94.36 \(\) 0.09 & 96.36 \(\) 0.02 & 94.17 \(\) 0.08 \\  & Prior-Regularized LC (ours) & 96.19 \(\) 0.01 & **95.31 \(\) 0.17** & 96.14 \(\) 0.14 & **95.08 \(\) 0.10** \\   & Vanilla & **85.84 \(\) 0.10** & 44.86 \(\) 0.12 & **87.91 \(\) 0.28** & 55.87 \(\) 0.14 \\  & Rotation Augmentation & 80.21 \(\) 0.09 & 74.12 \(\) 0.05 & 82.59 \(\) 0.44 & 78.39 \(\) 0.89 \\  & Learned Canonicalization (LC)  & 78.50 \(\) 0.15 & 77.52 \(\) 0.07 & 80.86 \(\) 0.17 & 80.48 \(\) 0.20 \\  & \(C8\)-Avg. & 83.00 \(\) 0.09 & 79.72 \(\) 0.10 & 83.45 \(\) 0.09 & 80.08 \(\) 0.38 \\  & Prior-Regularized LC (ours) & 83.44 \(\) 0.02 & **82.09 \(\) 0.09** & 84.27 \(\) 0.10 & **83.61 \(\) 0.01** \\   & Vanilla & **98.30 \(\) 0.01** & 73.87 \(\) 1.43 & **98.31 \(\) 0.09** & 76.66 \(\) 0.93 \\  & Rotation Augmentation & 98.08 \(\) 0.06 & 94.97 \(\) 0.08 & 97.85 \(\) 0.17 & 94.07 \(\) 0.11 \\   & Learned Canonicalization (LC)  & 95.30 \(\) 0.19 & 93.92 \(\) 0.10 & 95.11 \(\) 0.01 & 94.67 \(\) 0.02 \\   & C8-Avg. & **98.31 \(\) 0.01** & 96.31 \(\) 0.13 & 97.83 \(\) 0.08 & 94.45 \(\) 0.35 \\   & Prior-Regularized LC (ours) & 97.01 \(\) 0.01 & **96.37 \(\) 0.12** & 96.15 \(\) 0.05 & **95.73 \(\) 0.16** \\   

Table 2: Performance comparison of large pretrained models finetuned on different vision datasets. Both classification accuracy and \(\)-averaged classification accuracies are reported. Acc refers to the accuracy on the original test set, and \(C8\)-Avg Acc refers to the accuracy on the augmented test set obtained using the group \(=C8\). \(C8\)-Aug. refers to fine-tuning the pre-trained model with rotation augmentations restricted to \(C8\).

our goal was to establish an useful baseline to measure the gain in generalization of our proposed model to the out-of-distribution test dataset resulting from rotating the images. Further, we also report results on a discretized version of _Rotation Augmentation_, mentioned as _C8-Aug_. in Table 2, where the fine-tuning dataset is augmented with the application of group elements in \(C8\). We employ the _Learned Canonicalization_ method from , where the canonical orientation is learned with the task signal only, which in our experiment is the classification. Finally, our proposed _Prior-Regularized Learned Canonicalization_ approach adds a prior regularization loss that tries to map all images in the original training dataset to the identity group element \(e\). We refer to the final two equivariant techniques as LC and Prior-Regularized LC.

Evaluation Protocol.In order to test the robustness of the models on rotations, we introduce \(\)-averaged test set that refers to an expanded dataset obtained by applying all group elements to each image, resulting in a test dataset of size \(||\) times the original test set. In this section, we consider \(\) as \(C8\) (cyclic group with \(8\) rotations), thus \(||=8\). We report the top-1 classification accuracy achieved on the original as well as this augmented test set (referred to as \(C8\)-Average Accuracy) to evaluate both in distribution and out-of-distribution performance of the models.

Results.We report the results of finetuning ResNet50 and ViT on CIFAR10, CIFAR100, and STL10 with various strategies in Table 2. As anticipated, we found that large pretrained networks for images are not robust to rotation transformations, as indicated by the large drop in performance from the accuracy to its \(C8\)-averaged counterpart for both ResNet50 and ViT. Nevertheless, we observe that ViT is more robust to rotations compared to ResNet50, which has also been observed by . We notice that augmenting with a full range of rotation angles during training improves the \(C8\)-Average Accuracy as demonstrated by our _Rotation Augmentation_ baseline. However, it hurts the accuracy of the prediction network in the original test set and does not guarantee equivariance. Augmenting with necessary rotations in _C8-Augmentation_ does not ensure equivariance to \(C8\) but retains performance on the original test set and reduces the gap between original and C8-averaged accuracies.

LC guarantees equivariance, which can be seen from the minor difference between the accuracies of the original and augmented test sets. Nevertheless, in every dataset, we can observe a significant drop in accuracy for the original test set. We extensively discussed this issue in Section 3.1. However, with _our Prior-Regularized LC_ method, we are able to reduce the gap between the accuracy on the original test set while still being equivariant to rotations. This demonstrates that this prior regularization on LC is a promising direction to improve the performance of large-pretrained models while guaranteeing robustness to out-of-distribution samples resulting from transformations like rotation.

Ideally, the accuracy of the original test set should be nearly identical for both the Vanilla setup and our Prior-Regularized LC method. However, we observed a slight difference between their corresponding accuracies. This disparity arises from the fact that the canonicalization model is unable to map all data points (images) perfectly to the identity element \(e\), supported by our observations that the regularization loss for prior matching does not diminish to zero. We hypothesize that this stems from the intentional limitation in the expressivity of the canonicalization function, which is done on purpose in  to avoid adding excessive computational overhead to the overall architecture. Finally, we note that due to rotation artifacts, a small difference between \(C8\)-Average Accuracy and Accuracy on original test set is unavoidable.

#### 4.1.1 Instance Segmentation

Experiment Setup.We use MaskRCNN  and Segment Anything Model (SAM) , which are pretrained on Microsoft COCO  and SA-1B  datasets respectively. MaskRCNN is widely used for instance segmentation task, while SAM is a recently proposed foundational model that can leverage prompts (bounding boxes and key points) for instance segmentation, and their pretrained checkpoints are publicly available 34. In our experiments, we evaluate these models on COCO 2017 dataset, i.e., report zero-shot performance on validation (val) set and use ground truth bounding boxes as prompts for SAM. For the canonicalization function, we use a \(C4\)-equivariant WideResNet architecture. The details of the architecture are available in Appendix B. As MaskRCNN and SAM are already trained for the instance segmentation task, we only train the canonicalization function using the prior loss \(_{prior}\) in Eq. 3 to make them equivariant to \(C4\) group.

Results.Owing to its pre-training on larger datasets and the use of bounding boxes as prompts, SAM outperforms MaskRCNN in both mAP and \(C4\)-Avg mAP scores. The difference between mAP and \(C4\)-Avg mAP scores demonstrates that SAM is more robust than MaskRCNN in the case of these transformations.

However, we observe that there is a difference between these two reported numbers for both models. With our prior regularized LC framework, we can achieve equivariance with any large pretrained model to the desired group (here \(C4\)) while retaining the performance on the original val set. Further, we analyze the relationship between the expressivity of the canonicalization function and the downstream effect on mAP values on the original val set in Table 3. We compare a \(C4\)-equivariant convolutional network (G-CNN) with a \(C4\)-equivariant WideResNet (G-WRN) architecture. We observe that although equivariance is guaranteed, a less expressive canonicalization function leads to decreased performance in the original val set due to its inability to map complex images to identity.

### Point Cloud Domain

   Task \(\) &  &  \\  Dataset \(\) &  &  \\  Method \(\) & \(z/z\) & \(z/(3)\) & \((3)/(3)\) & \(z/(3)\) & \((3)/(3)\) \\  PointNet  & 85.9 & 19.6 & 74.7 & 38.0 & 62.3 \\ DGCNN  & **90.3** & 33.8 & 88.6 & 49.3 & 78.6 \\  VN-PointNet & 77.5 & 77.5 & 77.2 & 72.4 & 72.8 \\ VN-DGCNN & 89.5 & 89.5 & 90.2 & 81.4 & 81.4 \\ LC-PointNet & 79.9 \(\) 1.3 & 79.6 \(\) 1.3 & 79.7 \(\) 1.3 & 73.5 \(\) 0.8 & 73.6 \(\) 1.1 \\ LC-DGCNN & 88.7 \(\) 1.8 & 88.8 \(\) 1.9 & 90.0 \(\) 1.1 & 78.4 \(\) 1.0 & 78.5 \(\) 0.9 \\   \\   & no-aug/\(z\) & no-aug/\((3)\) & no-aug/\((3)\) \\  PRLC-PointNet  & 84.1 \(\) 1.1 & 84.3 \(\) 1.2 & 82.6 \(\) 1.3 \\ PRLC-DGCNN  & **90.2 \(\) 1.4** & **90.2 \(\) 1.3** & **84.3 \(\) 0.8** \\   

Table 4: Classification accuracy of different pointcloud models on the ModelNet40 dataset  in different train/test scenarios and ShapeNet  Part segmentation mean IoUs over 16 categories in different train/test scenarios. \(x/y\) here stands for training with \(x\) augmentation and testing with \(y\) augmentation. \(z\) here stands for aligned data augmented by random rotations around the vertical/\(z\) axis and \((3)\) indicates data augmented by random 3D rotations.

    &  &  \\  Datasets \(\) & Model & mAP & \(C4\)-Avg mAP & mAP & \(C4\)-Avg mAP \\   & Zero-shot (0 M) & 45.57 & 27.67 & 62.34 & 58.78 \\  & Prior-Regularized LC\({}^{}\) (0.2 M) & 35.77 & 35.77 & 59.28 & 59.28 \\  & Prior-Regularized LC\({}^{}\) (1.9 M) & 44.51 & 44.50 & 62.13 & 62.13 \\   

Table 3: Zero-shot performance comparison of large pretrained segmentation models with and without trained canonicalization functions on COCO 2017 dataset . Along with the number of parameters in _canonicalization_ and _prediction network_, we report mAP and \(C4\)-averaged mAP values. \(\) indicates G-CNN and \(\) indicates a more expressive G-WRN for canonicalization.

Datasets.For our experiments involving point clouds, we utilized the ModelNet40  and ShapeNet  datasets. The ModelNet40 dataset comprises 40 classes of 3D models, with a total of 12,311 models. Among these, 9,843 models were allocated for training, while the remaining models were reserved for testing in the classification task. In the case of part segmentation, we employed the ShapeNet-part subset, which encompasses 16 object categories and over 30,000 models. We only train the canonicalization function using the prior loss \(_{prior}\) in Eq. 7.

Evaluation protocol.To ensure consistency and facilitate comparisons, we followed the established conventions set by  and adopted by  for the train/test rotation setup in the classification and segmentation tasks. The notation \(x/y\) indicates that transformation \(x\) is applied during training, while transformation \(y\) is applied during testing. Typically, three settings are employed: \(z/z\), \(z/SO(3)\), and \(SO(3)/SO(3)\). Here, \(z\) denotes data augmentation with rotations around the z-axis during training, while \(SO(3)\) represents arbitrary rotations. However, since we regularize the output of the canonicalization with the identity transformation, we trained our canonicalization function and fine-tuned our pretrained model without any rotation augmentation. During inference, we tested on both \(z\) and \(SO(3)\) augmented test datasets.

Results.We present our results on Table 4. Notably, our method showcased superior performance in terms of robustness, outperforming existing methods for point cloud tasks. Specifically, the inclusion of the prior loss has led to a significant improvement in PointNet's performance compared to DGCNN. This observation aligns with our analysis in Section 3.1, where we highlight that training the prediction network with large rotations can hinder its performance and serve as a bottleneck for equivariance within the learnt canonicalization framework. The empirical evidence, particularly in the \(SO(3)/SO(3)\) results of vanilla PointNet and DGCNN, where we notice a more pronounced drop PointNet's performance, supports this and strengthens our findings.

## Discussion

Out-of-distribution generalization is an important weakness of state-of-the-art deep models, where an important source of shift in distribution is due to application-specific transformations of the data - from the change of colour palette, magnification and rotation in images to change of volume or pitch in sound. A mechanism for making pretrained models robust to such changes can have a significant impact on their adaptation to different domains. This paper takes the initial steps toward this goal by removing barriers to making the model equivariant through a lightweight canonicalization process. The proposed solution ensures that the pretrained model receives in-distribution data, while canonicalization ensures equivariance and, therefore, adaptability to a family of out-of-distribution samples. Our extensive experimental results using different pretrained models, datasets, and modalities gives insight into this subtle issue and demonstrate the viability of our proposed solution.

## Limitations and Future Work

An important limitation of our approach is the dependency on an equivariant canonicalization network, which imposes restrictions on the range of transformations to which we can adapt. Using the optimization approach of  offers more flexibility as it replaces the equivariant network with the outcome of an optimization process. However, this approach can raise efficiency concerns which merit further investigation. Another limitation of the current exposition is the limited group of transformations explored in experiments. In future, we hope to add a comprehensive evaluation of this approach to other groups of transformation. We also aim to address optimization challenges related to prior regularization for \(E(2)\)-Steerable Networks . By doing so, we aspire to incorporate continuous rotations into our image framework, thereby expanding its capabilities. Moreover, this observation emphasizes the necessity to delve into effect of expressivity of the canonicalization function, as it plays a crucial role in determining the overall performance of our model.

Another promising future direction we hope to explore is more flexible adaptation through canonicalization using examples from a target domain. Such examples from a target domain can, in principle, replace the prior knowledge of the transformation group assumed in equivariant networks, thereby bridging the gap between equivariant deep learning and domain adaptation.