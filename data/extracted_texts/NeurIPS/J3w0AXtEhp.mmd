# Uniform Last-Iterate Guarantee for Bandits and Reinforcement Learning

Junyan Liu

University of Washington

junyan11@cs.washington.edu

&Yunfan Li

University of California, Los Angeles

yunfanli@g.ucla.edu

&Ruosong Wang

CFCS and School of Computer Science

Peking University

ruosongwang@pku.edu.cn

Corresponding authors

&Lin F. Yang

University of California, Los Angeles

linyang@ee.ucla.edu

###### Abstract

Existing metrics for reinforcement learning (RL) such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the _cumulative_ performance, while allowing the agent to play an arbitrarily bad policy at any finite time \(t\). Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger metric, uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of RL algorithms. Specifically, ULI characterizes the instantaneous performance by ensuring that the per-round suboptimality of the played policy is bounded by a function, monotonically decreasing w.r.t. round \(t\), preventing revisiting bad policies when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned metrics, but not the other way around. To examine the achievability of ULI, we first provide two positive results for bandit problems with finite arms, showing that elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees. We also provide a negative result, indicating that optimistic algorithms cannot achieve near-optimal ULI guarantee. Furthermore, we propose an efficient algorithm for linear bandits with _infinitely many arms_, which achieves the ULI guarantee, given access to an optimization oracle. Finally, we propose an algorithm that achieves near-optimal ULI guarantee for the online reinforcement learning setting.

## 1 Introduction

In online decision-making problems with bandit feedback, a learner sequentially interacts with an unknown environment: in each round, the learner plays an policy and then observes the corresponding rewards of the played policy. Typically, the goal of the learner is to achieve good _cumulative performance_, commonly measured by regret or probably approximately correct (PAC) bound. For instance, in the online advertisement scenario, the goal of the website (learner) could be maximizing the cumulative click numbers (Li et al., 2010). Hence, the website aims to minimize the regret that measures the cumulative clicks of the recommended advertisement compared to that of the unknown optimal advertisement. In addition to regret minimization, the goal could also be quickly identifying popular advertisements (Chen et al., 2014; Jin et al., 2019). To this end, a PAC boundis suitable here to measure the sample complexity (i.e., cumulative time steps) that the algorithm needs to identify those popular advertisements. To reap the benefits of both measures, Dann et al. (2017) propose a new performance measure called uniform-PAC, ensuring that for all \(>0\), the total number of \(\)-suboptimal policies played by the algorithm is bounded by a function polynomial in \(1/\). The uniform-PAC bound can simultaneously imply a high-probability sublinear regret bound and a polynomial sample complexity for any desired accuracy.

Although uniform-PAC provides a powerful framework to unify regret2 and PAC bound, it still fails to capture the _instantaneous performance_ of the learning algorithm. In particular, a uniform-PAC algorithm could play a bad policy in some late but finite round \(t\), even if it enjoys a good cumulative performance. This drawback impedes the application of uniform-PAC algorithms into high-stakes fields. Clinical trials, for example, place high demands on instantaneous performance for every treatment test, since patients need to be assigned with increasingly better treatments when more experimental data are available (Villar et al., 2015). Hence, two natural questions arise:

1. _Can we find a new metric that characterizes not only the cumulative performance but also the instantaneous performance?_
2. _If such a metric exists, is it_ optimally _achievable by some algorithm?_

In this paper, we answer both questions affirmatively. Our main contributions are summarized as follows.

* We introduce a new metric called _uniform last-iterate_ (ULI), which simultaneously characterizes cumulative and instantaneous performance of sequential decision-making algorithms. On one hand, ULI can characterize the instantaneous performance: the per-round suboptimality of any algorithm with ULI guarantee is upper bounded by a function, monotonically decreasing for late time \(t\). On the other hand, we show that any algorithm with a (near-optimal) ULI guarantee is also (near-optimally) uniform-PAC, demonstrating that ULI can imply cumulative performance.
* To answer the question whether ULI is achievable, we examine three common types of bandit algorithms in the finite arm setting. First, we provide a stronger analysis to show that many existing elimination-based algorithms indeed enjoy a near-optimal ULI guarantee. Then, we propose a meta-algorithm that enables any high-probability adversarial bandit algorithms, with a mild condition, to achieve a near-optimal ULI guarantee, and we show that such condition naturally holds for many adversarial bandit algorithms. Finally, we provide a hardness result showing that optimistic algorithms (e.g., il'UCB (Jamieson et al., 2014)) cannot achieve near-optimal ULI guarantee. As il'UCB is near-optimally uniform-PAC, our hardness result also implies that ULI is _strictly stronger_ than uniform-PAC.
* For linear bandits with infinitely-many arms, we propose an oracle-efficient3 linear bandit algorithm with the ULI guarantee (with access to an optimization oracle). In particular, we propose an adaptive barycentric spanner technique, selecting finitely many base arms that can linearly represent all (possibly infinitely many) well-behaved arms. This technique generalizes the one in (Awerbuch and Kleinberg, 2008) for elimination-based algorithms by adaptively identifying spaces that active arms span. Leveraging the phased elimination algorithm (Lattimore et al., 2020), our algorithm can conduct the elimination over all arms by only playing a finite subset of arms and querying a linearly-constrained optimization oracle for only a polynomial number of times. * Finally, we propose a new algorithm for tabular episodic Markov decision processes (MDPs), which achieves a near-optimal ULI guarantee. In particular, our algorithm adapts uncertainty-driven reward functions to encourage exploration of the transition model, which ensures accurate estimations of value functions across all policies. The final ULI guarantee is achieved by conducting policy elimination.

**Related work.** In online decision-making problems, regret and PAC bounds are widely adopted to evaluate the cumulative performance of algorithms. More concretely, one line of research (Auer et al., 2002a, b; Abbasi-Yadkori et al., 2011; Li et al., 2010; Jin et al., 2018) aims to minimize the regret which measures the difference between the cumulative rewards of the selected policies and that of the best policy in hindsight. The PAC guarantees are more common than the regret when studying the pure-exploration/best policy identification problems (Even-Dar et al., 2006; Kalyanakrishnan et al., 2012; Wagenmaker et al., 2022). One of the popular PAC measures is \((,)\)-PAC which suggests that with probability at least \(1-\), the algorithm can output a near-optimal policy at most \(\) away from the optimal one by using a sample complexity polynomial in \(1/\). Later, Dann et al. (2017) introduce a new framework called uniform-PAC to unify both metrics and develop a uniform-PAC algorithm for episodic Markov decision processes (MDPs). Subsequent works design uniform-PAC algorithms for MDPs with linear function approximation (He et al., 2021) and bounded Eluder dimension (Wu et al., 2023). Though uniform-PAC strengthens regret and \((,)\)-PAC bound, it still fails to characterize the instantaneous performance of online algorithms, i.e., a uniform-PAC algorithm, even with a good cumulative performance, can play bad policies for some late rounds.

A seemingly related performance measure is last-iterate convergence (LIC) which has been studied for _optimizing_ MDPs (Moskovitz et al., 2023; Ding et al., 2023) and they use the primal-dual approach to formulate the problem of identifying an optimal policy in the constrained MDPs from a game-theoretic perspective. These works often require additional knowledge of the value functions and the LIC does not characterize the unknown dynamics of the environment. However, in our problem, the dynamics need to be _learned_ as the algorithm sequentially interacts with the environment.

## 2 Preliminaries

### Framework

We consider a general online sequential decision-making framework where a learner interacts with an environment with a fixed decision set. At each round \(t\), the learner makes a decision from the set and observes the corresponding reward(s). In what follows, we instantiate this framework to multi-armed bandits, linear bandits, and tabular episodic Markov decision processes (MDPs).

**Multi-armed bandits.** In the stochastic MAB setting, the arm (decision) set follows that \(=[K]\{1,,K\}\). Each arm \(a[K]\) is associated with a fixed and unknown \(\)-bounded distribution4 such that \( t\), reward \(X_{t,a}\) is an i.i.d. sample from this distribution with mean \(_{a}=[X_{t,a}]\). Let \(A_{t}\) be the arm played at round \(t\) and \(_{a}=^{}-_{a}\) be the suboptimality gap where \(^{}=_{a[K]}_{a}\).

**Linear bandits.** In the stochastic linear bandits setup, we assume that the arm set \(^{d}\) is compact. The reward of played arm \(A_{t}\) at round \(t\) follows that \(X_{t,A_{t}}=,A_{t}+_{t}\) where \(^{d}\) is a fixed but unknown parameter, and \(_{t}\) is conditionally \(1\)-subgaussian. Let \(_{a}=_{b},b-a\). We follow standard assumptions that \(_{2} 1\), \( a_{2} 1\) for all \(a\), and \(_{a} 1\) for all \(a\).

**Tabular episodic MDPs.** A tabular episodic MDP is formalized as \(=(,,H,r,P,)\) where \(,\) are finite state and action spaces with \(||=S,||=A\), \(H\) is the horizon length, \(r=\{r_{h}\}_{h=1}^{H}\) where \(r_{h}:\) is a known reward function, \(\{P_{h}\}_{h[H]}\) where \(P_{h}:()\) is a transition function, and \(\) is the initial state distribution. At the beginning of each episode \(t\), the learner executes a policy \(_{t}=\{_{t,h}:()\}_{h=1}^{H}\). Then, starting from the initial state \(s_{t,1}\), for each stage \(h[H]\), the learner repeatedly takes an action \(a_{t,h}_{t,h}(s_{t,h})\), observes reward \(r_{h}(s_{t,h},a_{t,h})\), and transits to the next state \(s_{t+1,h} P_{h}(|s_{t,h},a_{t,h})\).

For any policy \(\) and stage \(h\), we define action value function \(Q_{h}^{}(s,a)\) and value function \(V_{h}^{}(s)\) as

\[Q_{h}^{}(s,a)=[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}}) s_{h}=s,a_{h}=a,],\;V_{h}^{}(s)= [_{h^{}=h}^{H}r_{h^{}}(s_{h^{}},a_{h^{ }}) s_{h}=s,].\]

The optimal action value function and value function at each stage \(h\) are denoted by \(V_{h}^{}(s)=_{}V_{h}^{}(s)\), and \(Q_{h}^{}(s,a)=_{}Q_{h}^{}(s,a)\) respectively. Let \(_{}=_{s_{1}}[V_{1}^{}(s_{1})-V_{1}^{}(s_{1})]\).

**Suboptimality notations.** For MAB and linear bandits settings, the _instantaneous suboptimality_ is \(_{t}=_{A_{s}}\), and for episodic MDP, \(_{t}=_{_{t}}\). We use \(=_{a:_{a}>0}_{a}\) to denote the minimum suboptimality gap. Notice that it is possible that \(=0\) when, for example, arm set \(\) is a ball.

### Limitations of Existing Metrics

Regret and \((,)\)-PAC are widely adopted to measure the performance. The regret is defined as:

**Definition 2.1** (Regret).: _For each fixed \(T\), the regret \(R_{T}\) is defined as \(R_{T}=_{t=1}^{T}_{t}\)._

Let \(N_{}=_{t=1}^{}\{_{t}>\}\) be the number of plays of policies whose suboptimality gap is greater than \(\). The definition of \((,)\)-PAC is given as:

**Definition 2.2** (\((,)\)-Pac).: _For any fixed \(,(0,1)\), an algorithm is \((,)\)-PAC (w.r.t. function \(F_{}\)) if there exists a function \(F_{}(,)\) polynomial in \((^{-1})\) and \(^{-1}\) such that_

\[(N_{} F_{}(,) ) 1-.\]

As shown by Dann et al. (2017), both regret and \((,)\)-PAC have limitations. Specifically, an algorithm with sublinear regret bound may play suboptimal policies infinitely often. For the algorithm with \((,)\)-PAC guarantee, it may not converge to the optimal policy when feeding the algorithm with more samples. Therefore, such an algorithm would play those policies with suboptimality gap, e.g., \(/2\) infinitely often. Motivated by these limitations, Dann et al. (2017) introduce uniform-PAC as follows.

**Definition 2.3** (Uniform-Pac).: _An algorithm is uniform-PAC for some fixed \((0,1)\) if there exists a function \(F_{}(,)\) polynomial in \((1/)\) and \(^{-1}\), such that_

\[(>0:N_{} F_{}(, )) 1-.\]

We also call \(F_{}(,)\) the sample complexity function. Uniform-PAC is a stronger metric than regret and \((,)\)-PAC since it leads to the following implications.

**Theorem 2.4** (Theorem 3 in (Dann et al., 2017)).: _If an algorithm is uniform-PAC for some \(\) with function \(F_{}(,)=}(_{1}/ +_{2}/^{2})\)5, where \(_{1},_{2}>0\) are constant in \(\) and depend on \((1/)\) and \(K\) for MAB, \(d\) for linear bandits, and \(S,A,H\) for MDPs then, the algorithm guarantees:_

* \((_{t+}_{t}=0) 1-\)_;_
* \((,)\)_-PAC with_ \(F_{}(,)=F_{}(,)\) _for all_ \(>0\)_;_
* _With probability at least_ \(1-\)_, for all_ \(T\)_,_ \(R_{T}=}(T}+_{1}+_{2})\)_._

**Limitations of Uniform-PAC.** According to Theorem 2.4, uniform-PAC can imply a long-term convergence (the first bullet) and good cumulative performance (the second and the third bullets), but it does not capture the convergence rate of \(_{t}\) for each round \(t\). In other words, even if an algorithm enjoys uniform-PAC, it could still play a significantly bad policy for some very large but finite \(t\). This would lead to catastrophic consequences in safety-critical applications.

### New Metric: Uniform Last-Iterate Guarantee

To address the aforementioned issue, we introduce a new metric, formally defined below.

**Definition 2.5** (Uniform last-iterate (Ull)).: _An algorithm is ULI for some \((0,1)\) if there exists a positive-valued function \(F_{}(,)\), such that_

\[( t:_{t} F_{}(,t) ) 1-,\]

_where \(F_{}(,t)\) is polynomial in \((1/)\) and proportional to the product of power functions of \( t\) and \(1/t\) (e.g., \(F_{}(,t)=(1/)( t)^{_{1}}t^{- _{2}}\) for some \(_{1},_{2} 0\))._

According to Definition 2.5, the instantaneous suboptimality of any algorithm with the ULI guarantee can be bounded by a function \(F_{}(,t)\). Moreover, \(F_{}(,t)\) decreases monotonically for large \(t\) if its power on \(1/t\) is strictly positive, which captures the convergence rate of \(_{t}\).

Note that the convergence rate of \(_{t}\) is mainly determined by the power on \(1/t\) in \(F_{}\). Moreover, as we will show shortly, an algorithm with the ULI guarantee automatically has a small regret bound. We therefore have the following lower bound on \(F_{}\).

**Theorem 2.6**.: _For any bandit algorithm that achieves ULI guarantee for some \(\) with function \(F_{}(,t)\), there exists a MAB instance such that \(F_{}(,t)=(t^{-1/2})\)._We provide the proof in Appendix A. In the rest of the paper, we say an algorithm is near-optimal ULI if it achieves the ULI guarantee with \(F_{}(,t)=}(1/)\).

Then, we present the following theorem to show that ULI directly leads to uniform-PAC, implying that ULI also characterizes the cumulative performance of bandit algorithms.

**Theorem 2.7**.: _Suppose an algorithm achieves the ULI guarantee for some \(\) with function \(F_{}(,t)=(t/) t^{-}\) where \((0,1)\). Then, we have,_

* _the algorithm is uniform-PAC with function_ \(F_{}(,)=^{- }(^{-1}^{-1})\)_._
* _with probability at least_ \(1-\)_,_ \( T\)_, the regret_ \(R_{T}\) _is bounded by_ \[(\{(T/) T^{1-}, ^{1-1/}^{2}(()^{-1})\} ),\] _when the minimum suboptimality gap of the input instance_ \(\) _satisfies_ \(>0\)_._

According to Theorem 2.7, if an algorithm is with near-optimal ULI guarantee (i.e., \(=\)), then it implies the near-optimality for uniform-PAC bound (the first bullet point) and anytime sublinear high-probability regret bound (the second bullet point). On the other hand, an algorithm with near-optimal uniform-PAC bound does not necessarily enjoy a near-optimal ULI guarantee as shown in Section 3.2. The proof of Theorem 2.7 can be found in Appendix B.

**Remark 2.8**.: _Although a near-optimal ULI guarantee implies an anytime sublinear high-probability regret bound, it cannot give an anytime sublinear expected regret bound. This is because any algorithm with ULI guarantee is also uniform-PAC, but [4, Theorem 1] implies that no algorithm can be uniform-PAC and achieve anytime sublinear expected regret bound simultaneously._

## 3 Achieving Near-Optimal ULI in Bandits with Finite Arm-Space

In this section, we answer the question whether ULI is achievable for bandit problems. To this end, we examine three common types of bandit algorithms, including elimination-based algorithms, optimistic algorithms, and high-probability adversarial algorithms, in the finite arm setting, i.e., \(||=K\).

### Elimination Framework Achieving Near-Optimal ULI Guarantee

To examine whether the elimination-type algorithms can achieve the ULI guarantee, we first provide an elimination framework in Algorithm 1 that ensures the ULI guarantee, and we then show that most elimination-based algorithms fall into this framework. The following result shows that with a proper function \(f\) and a positive constant \(\), such an elimination framework ensures the ULI guarantee.

**Theorem 3.1**.: _For any given \((0,1)\), if there exists function \(f(,t)=t^{-}(t/)\) for some \((0,1)\) and \(>0\), such that with probability \(1-\), Eq. (1) holds for all \(t\), then algorithm is ULI with \(F_{}(,t)=(f(,t))\)._

**Input**: \((0,1)\), set \(\), function \(f(,)\), and constant \(\).

**Initialize**: active arm set \(_{0}=\).

**for**\(t=1,2,\)**do**

 Select an active set \(_{t}\) based on available observations as (\(a^{*}\) is one of optimal arms)

\[_{t}\{a_{t-1}:_{a} f (,t)\}\{a^{*}\}. \]

 Play an arm \(A_{t}_{t}\) and observe reward \(X_{t,A_{t}}\).

**Algorithm 1** Elimination framework for ULI

Theorem 3.1 suggests that Eq. (1) is a sufficient condition for elimination-based algorithms to achieve the ULI guarantee. Now, we show that existing elimination algorithms indeed fall into such a framework. We here consider successive elimination (SE) and phased elimination (PE). Notice that we consider SE only for the MAB setting (called SE-MAB, e.g., Algorithm 3 in ) but PE for both the MAB setting (called PE-MAB, e.g., exercise 6.8 in ) and the linear bandit setting (called PE-L, e.g., Algorithm 12 of Chapter 22 in ). Since those algorithms are standard, we defer their pseudocodesto Appendix D. Given Theorem3.1, the following results show that the elimination framework can be instantiated by these algorithms with proper functions and therefore they achieve the ULI.

**Theorem 3.2**.: _For any fixed \((0,1)\), elimination framework in Algorithm1 can be instantiated by_

* _SE-MAB for MAB to achieve the ULI with_ \(F_{}(,t)=t^{-}Kt)}\)_._
* _PE-MAB for MAB to achieve the ULI with_ \(F_{}(,t)=t^{-}K(t+1))}\)_._
* _PE-L for linear bandits to achieve ULI with_ \(F_{}(,t)=t^{-}K(t+1) d}\)_._

**Achieving ULI by adversarial bandit algorithms.** Traditional elimination-based algorithms, including the ones mentioned above, typically require a carefully designed exploration strategy which is non-trivial even for linear bandits. Here, we provide an alternative way to achieve ULI by employing adversarial bandit algorithms to explore and then conduct the elimination. As shown in Appendix F, all adversarial bandit algorithms for both MAB and linear bandits that meet a certain condition can naturally be used to achieve the ULI guarantees similar to those of traditional elimination-based algorithms.

### Lower Bound for Optimistic Algorithms

In this section, we present a lower bound to show that optimistic algorithms cannot achieve near-optimal ULI guarantee. The procedure of optimistic algorithms is summarized as follows. After playing each arm once, at each round \(t\), the algorithm plays an arm \(A_{t}\) that satisfies

\[A_{t}*{argmax}_{a}\{_{a}(N_{a }(t))+U_{}(N_{a}(t))\}, \]

where \(N_{a}(t)\) is the number of plays of arm \(a\) before round \(t\), \(_{a}(N_{a}(t))=^{t-1}X_{a,A_{s}}\{A_{s}= a\}}{N_{a}(t)}\) is the empirical mean of arm \(a\) after \(N_{a}(t)\) times play, and \(U_{}(N_{a}(t))\) is a positive bonus function which encourages the exploration.

We first consider optimistic algorithms e.g., upper confidence bound (UCB) (Lattimore and Szepesvari, 2020, Algorithm3, Chapter 7), which enjoy (near)-optimal regret bounds. This type of algorithms typically uses the bonus function in the form of \(/)/N_{a}(t)}\)6. However, the \( t\) term forces the algorithm to play suboptimal arms infinitely often, and thus they cannot achieve the ULI guarantee (refer to Appendix E.3 for a detailed proof). Similarly, other variants (Audibert and Bubeck, 2010, Degenne and Perchet, 2016) with \( t\) term in bonus function, also cannot achieve the ULI guarantee.

We then consider another optimistic-type algorithm, lil'UCB (Jamieson et al., 2014) which obtains the order-optimal instance-dependent sample complexity and avoids \((t)\) term in \(U_{}(N_{a}(t))\). The bonus function of lil'UCB is as \(_{+}(N_{a}(t)))/N_{a}(t)}\) where \(_{+}(x)=(\{x,e\})\). Our main result for lil'UCB is presented as follows. The full analysis of lil'UCB is deferred to Appendix E.

**Theorem 3.3**.: _There exists a constant \((0,1)\) that for all \((0,)\), running lil'UCB on the two-armed bandit instance with deterministic rewards and arm gap \(\) gives \( t=(^{-2})\) such that_

\[_{t}=(t^{-}( ^{-1}))+(^{-1})}).\]

Theorem3.3 shows that lil'UCB is not near-optimal ULI, but it is unclear whether it can achieve the ULI guarantee. Recall from Theorem3.2 that the elimination-based algorithms ensure that with high probability, \( t\), \(_{t}=}(t^{-}{{2}}})\). Theorem3.3 suggests that the convergence rate of lil'UCB is strictly worse than that of elimination-based algorithms, even if it enjoys a near-optimal cumulative performance (Jamieson et al., 2014). In fact, this lower bound holds for all optimistic algorithms as long as the bonus function is in a similar form.

**Remark 3.4** (**Uli is strictly stronger than uniform-PAC)**.: _For lil'UCB, the number of times playing suboptimal arms is finite with an order-optimal instance-dependent sample complexity, which implies that lil'UCB is near-optimal uniform-PAC. Therefore, Theorem3.3 also shows that an algorithm with near-optimal uniform-PAC does not necessarily enjoy near-optimal ULI guarantee._```
Input: Compact arm set \(\), confidence \((0,1)\), and constant \(C>1\). Initialize: \(_{1}=\{0,,0\}^{d}\), \(T_{0}=1\) and \(_{0}=\{e_{1},,e_{d}\}\).
1for\(m=1,2,\)do
2 Set \(T_{m}=256C^{4}}{4-m}(^{-1}d^{3}4^{m})\).
3 Invoke Algorithm 12 with \((,m,_{m-1},T_{m},C,_{m})\) to find a \(C\)-approximate barycentric spanner \(_{m}\) for active arm set \(_{m}\) where \(_{m}\) is in Eq. (3).
4 Set \(_{m}(a)=\) for each \(a_{m}\).
5 Play each arm \(a_{m}\) for \(n_{m}(a)= T_{m}_{m}(a)\) times.
6 Compute \(V_{m}=I+_{a_{m}}n_{m}(a)aa^{}\) and \(_{m+1}=V_{m}^{-1}_{t_{m}}A_{t}X_{t,A_{t}}\) where \(_{m}\) is a set that contains all rounds in phase \(m\).
```

**Algorithm 2** PE with adaptive barycentric spanner

## 4 Achieving Near-Optimal ULI for Linear Bandits in Large Arm-Space

In this section, we propose a linear bandit algorithm that can handle the infinite number of arms. The compact arm set \(\) is assumed to span \(^{d}\) and \(d\) is known.

### Main Algorithm and Main Results

The starting point of our algorithm design is the phased elimination (PE) algorithm (Lattimore et al., 2020, Algorithm 12, Chapter 22). However, PE in general is not feasible when the arm space is large (e.g., a continuous space). In this section, we present a carefully-designed algorithm to address the new challenges from large arm-spaces.

**Issues of PE for large arm-space.** PE needs to (i) compute (approximately) \(G\)-optimal design whose complexity scales linearly with \(||\) and (ii) compare the empirical mean of each arm, both of which are impossible when the arm set is infinite, e.g., \(\) is a ball. A natural idea is to discretize \(\), e.g., constructing a \(\)-net, but the computational complexity has an exponential dependence on \(d\), and the optimal arm does not necessarily lie in the net, which prevents the convergence to the optimal arm.

**High-level idea behind our solution.** To address the aforementioned issues, we propose an oracle-efficient linear bandit algorithm in Algorithm 2 which can eliminate bad arms by efficiently querying an optimization oracle. Our algorithm equips PE with a newly-developed _adaptive barycentric spanner_ technique. The proposed technique selects a finite _representative arm set_ to represent (possibly infinite) active arm set and adaptively adjusts the selection of arms across phases. By conducting the (approximate) \(G\)-optimal design (Kiefer and Wolfowitz, 1960) on the representative arm set and playing each arm in the set according to the design, the algorithm can acquire accurate estimations uniformly over all active arms. Moreover, the adaptive barycentric spanner approach can be implemented by efficiently querying an optimization oracle in polynomial times.

```
Input: Compact arm set \(\), confidence \((0,1)\), and constant \(C>1\). Initialize: \(_{1}=\{0,,0\}^{d}\), \(T_{0}=1\) and \(_{0}=\{e_{1},,e_{d}\}\).
1for\(m=1,2,\)do
2 Set \(T_{m}=256C^{4}}{4-m}(^{-1}d^{3}4^{m})\).
3 Invoke Algorithm 12 with \((,m,_{m-1},T_{m},C,_{m})\) to find a \(C\)-approximate barycentric spanner \(_{m}\) for active arm set \(_{m}\) where \(_{m}\) is in Eq. (3).
4 Set \(_{m}(a)=\) for each \(a_{m}\).
5 Play each arm \(a_{m}\) for \(n_{m}(a)= T_{m}_{m}(a)\) times.
6 Compute \(V_{m}=I+_{a_{m}}n_{m}(a)aa^{}\) and \(_{m+1}=V_{m}^{-1}_{t_{m}}A_{t}X_{t,A_{t}}\) where \(_{m}\) is a set that contains all rounds in phase \(m\).
```

**Algorithm 3** PE with adaptive barycentric spanner

The definition of barycentric spanner (Awerbuch and Kleinberg, 2008) is presented as follows.

**Definition 4.1** (\(C\)-approximate barycentric spanner).: _Let \(^{d}\) be a compact set. The set \(=[b_{1},,b_{d}]\) is a \(C\)-approximate barycentric spanner for \(\) if each \(a\) can be expressed as a linear combination of points in \(\) with coefficients in the range of \([-C,C]\)._

**Why adaptive barycentric spanner?** The primary reason that the non-adaptive barycentric spanner technique (Awerbuch and Kleinberg, 2008) fails to work for PE is that it requires the knowledge of the space that the active arm set spans. However, acquiring such knowledge is often difficult because the active arm set, which may span a _proper linear subspace_ of \(^{d}\), varies for different phases and the number of active arms could be infinite. Our algorithm shown in Algorithm 12 (whose pseudocode is deferred to Appendix G.2 due to space limit) can identify a barycentric spanner for active arm set adaptively for each phase, even if they do not span \(^{d}\).

**Algorithm procedure.** Our algorithm proceeds in phases \(m=1,2,\), and each phase consists of consecutive rounds. At the beginning of phase \(m\), the algorithm invokes subroutine Algorithm 12 to identify a \(C\)-approximate barycentric spanner \(_{m}\) which can linearly represent all arms in active arm set \(_{m}\) where \(_{1}=\) and \( m 2\)

\[_{m}=\{a_{m-1}:_{m},a_{m}^{}-a 2^{-m+1}\}, \]where \(a_{m}^{}\) is the empirical best arm and \(_{m}\) is the estimation of unknown parameter \(\).

Then, the algorithm assigns \(_{m}(a)=\) for each \(a_{m}\). In fact, if we add \(_{i_{m}}}{}}\) (refer Appendix G for \(e_{i}\) and \(_{m}\)) back to \(_{m}\) and denote the new set by \(_{m}^{}\), then \(_{m}:_{m}^{}\) forms an optimal design. It is noteworthy that our algorithm only plays arms in \(_{m}\) as these added elements do not necessarily exist in the arm set \(_{m}\). As each added element \(}{}}\) is close to zero, even if we only play arms in \(_{m}\), we can still acquire accurate estimations uniformly over \(_{m}\) (refer to Appendix G.6 for details):

\[ a_{m}:\|a\|_{V_{m}^{-1}}= V_{m}^{-1}a} C d/}, \]

where \(V_{m}=I+_{a_{m}}n_{m}(a)aa^{}\) is the least squares matrix, used to estimate \(\).

According to the standard analysis of linear bandit algorithms, the estimation error of \( a,\) is proportional to \(\|a\|_{V_{m}^{-1}}\). Hence, the estimation errors of \(\{ a,\}_{a_{m}}\) can be uniformly bounded by \(}Cd/}\), which is known to the learner. Finally, after playing each arm \(a_{m}\) for \(n_{m}(a)\) times, the algorithm updates the empirical estimates \(V_{m}\) and \(_{m+1}\), and then steps into the next phase.

The main results of Algorithm 2 for achieving the ULI guarantee and computational complexity are given as follows. The full proof can be found in Appendix G.

**Theorem 4.2**.: _For any fixed \((0,1)\), Algorithm 2 achieves the ULI guarantee with function \(F_{}(,t)=t^{-}( ^{-1}dt)}\). Moreover, in each phase, the number of calls to the optimization oracle given in Definition G.2 is \((d^{3}_{C}d)\)._

Theorem 4.2 and Theorem 2.7 jointly suggest \(}(d^{9/2})\) worst-case regret bound for the infinite-armed setting, which matches those of (Dani et al., 2008; Agrawal and Goyal, 2013; Hanna et al., 2023). Compared with the lower bound \((d)\) given by Dani et al. (2008), our regret bound suffers an extra \(\) factor, caused by the spanner technique. Yet, it remains open to find a computationally efficient linear bandit algorithm that can handle the infinite arm setting with general compact \(\) and matches the \((d)\) lower bound.

Theorem 4.2 also shows that, for each phase, the spanner can be constructed by calling the oracle for only polynomial times. Compared to the computational efficiency of the algorithm in (Awerbuch and Kleinberg, 2008), the efficiency of our algorithm is only \(d\) times worse than theirs.

## 5 Achieving Near-Optimal ULI in Tabular Episodic MDPs

In this section, we propose a novel algorithm that achieves a near-optimal ULI guarantee in tabular episodic MDPs setup. The algorithm is formally presented in Algorithm 3.

**High-level idea.** Algorithm 3 conducts policy elimination over a policy set \(_{}\) which enumerates all deterministic policies. The key challenge here is to acquire accurate estimations of value functions uniformly for all deterministic policies. Note that naively playing all deterministic policies will incur linear dependence on \(|_{}|=A^{SH}\) in \(F_{}\), which is exponentially large. Hence, the algorithm invokes subroutine Algorithm 4 to exhaustively explore the environment, which ensures accurate estimations of the transition model and only suffers _logarithmic dependence_ on \(|_{}|\). Once the transition model can be well-approximated, the algorithm constructs an accurate estimation of the value function for every policy and then decides which policies should be eliminated.

```
Input: confidence \((0,1)\), policy set \(_{}\), duration \(T\). Initialize: randomly pick a policy \(_{1}\), \(N_{1,h}(s,a)=N_{1,h}(s,a,s^{})=0\) for all \((h,s,a,s^{})\). for\(t=1,,T\)do  Observe initial state \(s_{t,1}\). for\(h=1,,H\)do  Take action \(a_{t,h}=_{t,h}(s_{t,h})\) and observe \(s_{t,h+1}_{h}(|s_{t,h},a_{t,h})\).  Increase counters \(N_{t,h}(s_{t,h},a_{t,h},s_{t,h+1})}{{}}1\) and \(N_{t,h}(s_{t,h},a_{t,h})}{{}}1\).  Update estimates \(}_{t,h}(s^{}|s,a)=(s,a,s^{})}{ \{1,N_{t,h}(s,a)\}}\) for all \((s,a,s^{})\).  Update bonus function \(b_{t}=\{b_{t,h}\}_{h[H]}\) where \(b_{t,h}(,)\) is updated according to Eq. (5).  Get \(\{^{}_{t,1}(s_{1})\}_{}\) by invoking Algorithm 5 with input \((,b_{t}/H,\{}_{T,h}\}_{h[H]},b_{t},s_{t,1})\).  Update policy \(_{t+1}=*{argmax}_{}^{}_{t,1}(s_{1})\). for\(t=1,,T\)do  Get \(\{^{}_{T,1}(s_{t,1})\}_{}\) by invoking Algorithm 5 with input \((,r,\{}_{T,h}\}_{h[H]},0,s_{t,1})\). Output:\(\{^{}\}_{}\) where \(^{}=_{t=1}^{T}^{}_{T,1}(s_{t,1})\).
```

**Algorithm 5** Construct estimated value function

More concretely, Algorithm 3 first accepts a set of all deterministic policies and then it proceeds in phases \(m=1,2,\). In each phase \(m\), subroutine Algorithm 4 is invoked to learn the transition model. Specifically, the subroutine inherits the structure of UCB-VI (Azar et al., 2017), but more importantly the algorithm pretends to be agnostic to the reward function and uses uncertainty-driven reward functions \(\{b_{t,h}(s,a)/H\}_{t,h}\) where

\[b_{t,h}(s,a)=H(s,a)\}}}+(s,a)\}}=}|T}{ }, \]

where \(N_{t,h}(s,a)\) is the number of times of visiting \((s,a)\) at stage \(h\) up to episode \(t\). Note that Eq. (5) captures the uncertainty of visitation a state-action pair, i.e., more visitations of \((s,a)\), larger \(N_{t,h}(s,a)\), and less uncertainty. This modification, inspired by (Wang et al., 2020) forces the algorithm to aggressively explore the environment, in the sense that the algorithm prefers to play a policy that maximizes the uncertainty.

The following theorem shows that our algorithm achieves the ULI guarantee.

**Theorem 5.1**.: _For any fixed \((0,1)\), Algorithm 3 achieves the ULI guarantee with function \(F_{}(,t)=t^{-}AH^{5} }(tSAH/)\)._

Theorem 5.1 shows near-optimality of Algorithm 3 w.r.t. episode \(t\). Unfortunately, the regret bound implied by our ULI result incurs suboptimal dependence on \(S,H\), and the logarithmic term, and our algorithm is not computational efficient. We leave these improvements for future work.

## 6 Conclusions and Future Work

In this paper, we propose a new metric, the uniform last-iterate (ULI) guarantee, which captures both instantaneous and cumulative performance of sequential decision-making algorithms. To answer whether ULI is (optimally) achievable, we first examine three types of bandit algorithms in the finite-arm setting. Specifically, we provide _stronger_ analysis to show that elimination-based algorithms naturally achieve near-optimal ULI guarantees. We also provide a reduction-based approach to enable any high-probability adversarial algorithms, with a mild condition, to achieve near-optimal ULI guarantees. We further provide a negative result for optimistic bandit algorithms showing that they cannot achieve near-optimal ULI guarantee. Furthermore, in the large arm space setting, we propose an oracle-efficient linear bandit algorithm, equipped with the novel adaptive barycentric spanner technique. Finally, we propose a new algorithm, which adapts uncertainty-driven reward functions into policy elimination to achieve a ULI guarantee in tabular episodic MDPs.

Some natural directions are to improve our current results, including proposing a ULI algorithm for linear bandits with infinite arms and proposing a computationally efficient algorithm, possibly based on the action-elimination approach, for tabular MDPs. We also summarize other interesting directions as follows.

* Design (computationally efficient) algorithms for MDPs with linear function approximation. The main challenge is to bypass any dependence on the number of states which is possibly infinite. Thus, generalizing our RL algorithm, which enumerates all deterministic policies, to linear MDPs does not work.
* Design (computationally efficient) algorithms for Episodic MDPs with only logarithmic dependence on \(H\) (a.k.a. horizon-free). In our attempts, we use the doubling trick on \(\) in existing \((,)\)-PAC horizon-free RL algorithms. In this case, we run the algorithm in phases with \(,/2,/4,\). The main difficulty is to leverage the information learned from the previous phase to guide the algorithm to play an improved policy at the next phase as required by ULI. Thus, we conjecture that there might exist fundamental barriers to simultaneously achieve ULI guarantee and a logarithmic dependence on \(H\).
* It could be also interesting to investigate some empirical issues when deploying ULI algorithms in high-stakes fields. Typically, optimistic algorithms initially incur a lower regret than ULI algorithms, but their regret will exceed those of ULI algorithms at a certain juncture. At this juncture, ULI algorithms have eliminated all suboptimal arms, whereas optimistic algorithms continue exploring as time evolves. Hence, identifying this turning point could be beneficial for deploying ULI algorithms in high-stakes domains.