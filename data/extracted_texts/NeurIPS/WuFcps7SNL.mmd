# Mmoe: Mixture of Multimodal Interaction Experts

Haofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency

Carnegie Mellon University

{haofeiy, pliang, rsalakhu, morency}@cs.cmu.edu

###### Abstract

Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.

## 1 Introduction

Recent advances in vision-language multimodal architecture and pretraining (Zhu et al., 2023; Li et al., 2023; Liu et al., 2023) have predominantly centered on enhancing the representation interplay between modalities (Huang et al., 2021). This enhancement often stems from the principle of multi-view redundancy Liang et al. (2023). Parallel to this, contemporary multimodal research has expanded its scope, categorizing a broader spectrum of multimodal interactions. These go beyond mere _redundancy_ to capture instances where task-relevant information does not merely reside in the overlapping region of both modalities Liang et al. (2023). Instead, it might hinge on _unique_ details from either modality (e.g. detecting laughter from someone not observable), or the result of a _synergistic_ fusion of both modalities, producing insights absent when either modality is considered in isolation (e.g. sarcasm discerned from incongruent speech and gestures). (Williams and Beer, 2010; Liang et al., 2023; Marsh and Domas White, 2003) This body of research also delves into discerning whether predictions from different modalities _align_ or _contrast_ with one another. (Bateman, 2014; Kruk et al., 2019; Zhang et al., 2018) It still remains a challenge for state-of-the-art multimodal models to tackle multiple types of interactions. Furthermore, these interactions can vary significantly even within the same dataset for different data points, for example, sometimes sarcasm is evident from language alone and other times due to synergistic disagreement between language and gestures.

Making progress on these different interactions requires new definitions and modeling paradigms that existing models are not designed to capture. To model these diverse interactions, our key insight is that **single model may not be necessarily suitable for capturing all kinds of interactions at the same time**. Indeed, we find that today's state-of-the-art models struggle significantly with disagreement and synergistic multimodal interactions, with reductions in performance up to 40% on sarcasm detection as compared to redundant cases.

Based on this insight, we propose a new approach called MMOE (mixture of multimodal interaction experts), which classifies the different types of multimodal interactions in a given task and utilizes multiple expert models specifically designed for each type of interaction. These expert models can differ based on in-context learning, parameter-based tuning, or an entirely different architecture. Using our multimodal interaction experts, we raise the performance on some interaction types by more than 10% compared with using a single model, resulting in overall improvements over the state-of-the-art models. The resulting approach is easy to implement and scales easily with large-scale models.

## 2 Mixture of Multimodal Interaction Experts

In this section, we first provide problem formulation for our target multimodal prediction task. After that, we explain three key components in our framework mentioned in Fig 1 in detail.

### Problem formulation

In the multimodal prediction task, let \(_{i}\) and \(\) be finite sample spaces for modality-specific features and target labels. Let \(_{}=\{r_{+}^{||}\|r\|_{1}=1\}\) be the probability simplex over labels \(\). Given modality-specific features \(X_{1}\), \(X_{2}\) (with support \(_{i}\)), for one multimodal prediction task, our goal is to learn a multimodal model \(_{M} f_{M}:_{1}_{2} _{}\) to predict \(Y\) (with support \(\)).

### Predicting partial labels and full labels

Utilizing labeled unimodal data \(_{i}=\{(x_{i},y):_{i}\}\) and labeled multimodal data \(_{M}=\{(x_{1},x_{2},y):_{1}_{2} \}\), we can obtain the set of unimodal classifiers \(_{i} f_{i}:_{i}_{}\) and multimodal classifiers \(_{M} f_{M}:_{1}_{2} _{}\). For each unlabeled multimodal datapoint \(d}_{M}=\{(x_{1},x_{2}):_{1}_{ 2}\}\), we can get probability \(_{1}\) and \(_{2}\) from unimodal classifier \(f_{1}\) and \(f_{2}\). These two labels are partial label estimations based on unimodal information. Similarly, we can get \(_{M}\) from multimodal classifier \(f_{M}\), which is considered as full label estimation based on multimodal data. Practically, apart from predicting discrete labels \(y_{1}\), \(y_{2}\), and \(y_{M}\) from \(\), we calibrate pretrained models by utilizing probability from models as \(_{1}\), \(_{2}\), and \(_{M}\).

### Categorizing datapoints based on multimodal interactions

Each labeled multimodal datapoint \(d_{M}=\{(x_{1},x_{2},y):_{1}_{2} \}\) can be assigned with different multimodal interaction type, including redundancy, uniqueness, and synergy. Interaction can be estimated by finding data points in which individual predictions from each modality agree or disagree with each other. Moreover, interaction can be detected where the joint distribution differs the most from the marginals.

**Definition 1**.: _(Modality disagreement) Given \(x_{1}_{1}\), \(x_{2}_{2}\), as well as unimodal classifiers \(f_{1}:_{1}_{}_{1}\) and \(f_{2}:_{i}_{}_{2}\), we define modality disagreement as \(d(f_{1},f_{2})\) where \(d:^{ 0}\) is a distance function in label space scoring the disagreement of \(f_{1}\) and \(f_{2}\)'s predictions._

Converting partial labels to agreement and disagreementWe convert model-predicted partial labels into agreement and disagreement by comparing the output of unimodal classifiers against each

Figure 1: Our proposed _mixture of multimodal interaction expert_ framework includes three components: **a.**partial label collection **b.** datapoint-level multimodal interaction categorization **c.**mixture of experts prediction.

other. Based on the modality disagreement definition, when \(d(f_{1},f_{2})>\), where \(\) is a pre-defined threshold, two modalities are considered as disagreed with each other. Otherwise, they are considered as agreed. Practically, we define \(d(f_{1},f_{2})=\|_{1}-_{2}\|_{1}\) as our distance function. Based on our experiments, we found that agreement and disagreement cases distinguish multimodal performance well. Utilizing this definition, unlabeled multimodal data \(}_{M}=\{(x_{1},x_{2}):_{1}_{2}\}\) is classified as non-overlapping disagreed ones and agreed ones.

**Definition 2**.: _(Modality **Redundancy**, **Uniqueness**, **Synergy [**RUS**]) Given \(x_{1}_{1}\), \(x_{2}_{2}\), as well as unimodal classifiers \(f_{1}:_{1}_{}_{1}\) and \(f_{2}:_{2}_{}_{2}\) and a multimodal classifier \(f_{M}:_{1}_{2}_{}\), we define modality redundancy as \(R=-d(f_{1},f_{M})-d(f_{1},f_{2})-d(f_{2},f_{M})\), modality uniqueness for \(_{1}\) as \(U_{1}=d(f_{2},f_{M})+d(f_{1},f_{2})-d(f_{1},f_{M})\), modality uniqueness for \(_{2}\) as \(U_{2}=d(f_{1},f_{M})+d(f_{1},f_{2})-d(f_{2},f_{M})\), modality synergy as \(S=d(f_{1},f_{M})+d(f_{2},f_{M})\), where \(d:^{ 0}\) is a distance function in label space scoring the disagreement of any two classifiers' predictions._

Converting partial and full labels to redundancy, uniqueness, and synergyInstead of simply utilizing \(f_{1}\) and \(f_{2}\) to measure modality disagreement, we would like to introduce more fine-grained multimodal interaction types by adding \(f_{M}\) and measuring distances between partial labels \(_{1},_{2}\) and full labels \(_{M}\). Since we are dealing with unlabeled multimodal data \(}_{M}=\{(x_{1},x_{2}):_{1}_{2}\}\), we have no access to ground-truth labels \(y^{*}\) but only have full labels \(_{M}_{}\) as a proxy metric. Our hypothesis is that while predicted \(_{M}\) could be hard and inaccurate for multiple multimodal interactions like synergy, it is easier to classify a datapoint as one interaction type just by looking at how the label changes from unimodal to multimodal, versus actually predicting the datapoint itself. Intuitively, for each datapoint \(d}_{M}=\{(x_{1},x_{2}):_{1}_{2}\}\), its modality information is redundant if \(_{1}\), \(_{2}\), and \(_{M}\) are all close to each other; \(_{1}\) information is unique if \(_{1}\) is close to \(_{2}\) but \(_{2}\) is far from \(_{M}\); \(_{2}\) information is unique if \(_{2}\) is close to \(_{1}\) but \(_{1}\) is far from \(_{M}\); modality information is synergistic if \(_{1}\) and \(_{2}\) are both far from \(_{M}\). Based on the definition of **RUS** (standing for **R**udandancy, **Uniqueness**, and **Synergy**), unlabeled multimodal dataset \(}_{M}=\{(x_{1},x_{2}):_{1}_{2}\}\) can be classified into three non-overlapping parts. One of the main limitations of this method is that it is limited to two modalities and is not able to expand to multiple modalities yet.

### Multimodal Interaction Experts

In section 2.3, we have introduced the definition of modality disagreement, redundancy, uniqueness, and synergy. Moreover, we mentioned that modality disagreement measurement can split unlabeled multimodal datasets into two non-overlapping sub-parts. RUS measurement can split the dataset into three non-overlapping sub-parts. To push a step further, we can combine both disagreement and RUS measurement to create a comprehensive multimodal interaction standard. Typically, when modality agrees with each other, there are no uniqueness-dominant cases. Similarly, when modality disagrees with each other, there should be no redundancy-dominant cases. Therefore, in general, as illustrated in Fig 1, we create a 5-type multimodal interaction standard when facing two modalities \(_{1}\) and \(_{2}\): 1 disagreement \(_{1}\) uniqueness; 2 disagreement \(_{2}\) uniqueness; 3 disagreement synergy; 4 agreement redundancy; 5 agreement synergy.

For each multimodal interaction type, we designed an expert model to handle. Motivated by the Socratic model Zeng et al. (2022), we first project information from different modalities into text modality and make sure the text description loses minimal modality-specific features. Secondly, we implement few-shot in-context learning based on large language models (LLMs) using data points that are categorized as the same multimodal interaction type.

## 3 Experiments

### Experimental Setting

For the dataset, we use MUSTARD dataset to perform our experiments and choose speaker-dependent training and testing split. Each sample in MUSTARD includes character scripts as text modality, video clip as video modality, and speaking audio as audio modality. For baseline models, we compare with multiple supervised multimodal baselines including LF-DNN (Ding et al., 2022), MULT (Liang et al., 2021), LMF (Liu et al., 2018), MFN (Zadeh et al., 2018), and EF-LSTM (Liu et al., 2018). Additionally, we compare with multiple unimodal baselines based on face emotion recognition (Xiang and Zhu, 2017) and speech emotion recognition (Hsu et al., 2021). For our proposed method, We select gpt-3.5-turbo as our model for experts. We use few-shot prompting on gpt-3.5-turbo to categorize data points and construct few-shot examples under the same category.

### Results

Performance ResultsBased on Table 1, we can see that we are able to get the state-of-the-art F1 result of 74.93% compared with a few-shot unimodal baseline, supervised multimodal baseline, and Socratic model. Additionally, we can conclude that by only adding multiple mixtures of experts based on the Socratic model, the model's performance can have a 1.5% gain. Moreover, we observed that recall increased from 87.40 to 91.69 after introducing multimodal information into the detection process. It indicates that more sarcastic situations can be detected.

Case StudyTo prove that our proposed mixture of multimodal interaction experts is able to capture appropriate multimodal interaction and fix errors, we provide two concrete case studies to show the fixed uniqueness and synergy type. Fig 2 shows one agreement synergy case where multimodal features motivate new information and one disagreement text uniqueness case where the video does not include a clear sarcastic signal.

Expert Model AnalysisWe also do analysis based on the whole MUSTARD dataset. We classified all data points by their detected multimodal interaction type and found that redundancy-dominant data points, which have 364 data points, are the most common cases. Text modality uniqueness-dominant cases include 172 data points. It indicates that text modality is crucial for sarcasm detection. Based on Table 2, we can see that with appropriate in-context learning, both agreement and disagreement synergy have much better performance (gain 61.47% and 22.81% separately) compared with a single model. For redundancy-type data points, even though the F1 score remains almost the same, it has a higher confidence score (increasing from 3.49 to 3.77 out of 5) when using specific experts for this interaction type. How to design a more useful and effective multimodal expert for each interaction type is left as a future work.

## 4 Conclusion

This paper proposed an approach for quantifying multiple types of multimodal interactions in multimodal datasets and designing a new approach called MMOE. Experimentally, using multimodal interaction experts raises the performance on these interactions up to a 74.93% F1 score. As a result, not only does interaction quantification provide new insights for dataset analysis, but also simple approaches to obtain state-of-the-art performance.

   MUSTARD & Precision\(\) & Recall\(\) & F1\(\) \\   \\  SVM & 71,9\({}_{ 0.00}\) & 71,44\({}_{ 0.00}\) & 71,5\({}_{ 0.00}\) \\ MFN & 72.65\({}_{ 1.00}\) & 72.46\({}_{ 0.02}\) & 72.41\({}_{ 0.00}\) \\ LMF & 71.81\({}_{ 2.25}\) & 71.34\({}_{ 1.81}\) & 71.16\({}_{ 1.7}\) \\ MULT & 73.07\({}_{ 1.81}\) & 72.32\({}_{ 0.06}\) & 72.11\({}_{ 0.00}\) \\ LF-DNN-v2 & **73.80\({}_{ 1.85}\)** & 73.62\({}_{ 1.75}\) & 73.58\({}_{ 1.75}\) \\   \\  MTCNN [vision] & 62.10\({}_{ 0.00}\) & 58.69\({}_{ 1.35}\) & 66.49\({}_{ 2.10}\) \\ HuBERT [audio] & 62.36\({}_{ 4.02}\) & 75.28\({}_{ 0.57}\) & 67.55\({}_{ 3.13}\) \\ GPP-3.5 [text] & 65.56\({}_{ 0.27}\) & 87.40\({}_{ 1.13}\) & 74.78\({}_{ 1.01}\) \\    \\  GPT-3.5 [text+audio] & 61.52\({}_{ 2.08}\) & 91.24\({}_{ 0.63}\) & 73.34\({}_{ 0.12}\) \\ +MME & 63.47\({}_{ 0.03}\) & **91.69\({}_{ 0.21}\)** & **74.93\({}_{ 0.04}\)** \\   

Table 1: Test performance on sarcasm dataset with speaker-dependent setup. Each score is an average score of three runs with three random seeds.

   MUSTARD & \#Example & F1\(\) (Single Model) & F1\(\) (Expert Model) & Improvement\(\) (\%) \\  Agreement \& Redundancy & 364 & **85.98\({}_{}\)** & 85.33\({}_{}\) & -0.75\% \\ Agreement \& Synergy & 37 & 18.75\({}_{}\) & 30.30\({}_{}\) & **+61.47\%** \\  Disagreement \& Unique [text] & 172 & 69.19\({}_{}\) & 69.19\({}_{}\) & +0.00\% \\ Disagreement \& Unique [audio] & 62 & 54.23\({}_{}\) & 53.52\({}_{}\) & -1.31\% \\ Disagreement \& Synergy & **55** & **47.05\({}_{}\)** & 57.78\({}_{}\) & **+422.81\%** \\   

Table 2: An analysis of the full MUSTARD dataset based on few-shot prompting, focusing on both text and audio modalities. For each multimodal interaction F1 score, we present an average confidence score to indicate modelâ€™s confidence towards its prediction, which ranges from 0 to 5. Each F1 score listed in the table is an average score of three runs.

Figure 2: Case study for agreement synergy and disagreement uniqueness.