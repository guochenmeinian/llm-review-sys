# ESPACE: Dimensionality Reduction of Activations for Model Compression

Charbel Sakr

NVIDIA Research

csakr@nvidia.com

&Brucek Khailany

NVIDIA Research

bkhailany@nvidia.com

###### Abstract

We propose ESPACE1, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.

## 1 Introduction

Capabilities of large language models (LLMs) have recently soared in natural language understanding and generative power. It is appreciated that there exists a correlation between model size and achievable accuracy. Indeed, as LLMs consume trillions of tokens during their training, a large parameter volume is required to capture intricate linguistic features . This leads to a trade-off in LLMs: larger parameter counts improve accuracy but come with increased serving cost.

However, it is also appreciated that the computational requirements of inference may be lower than those of training . To that end, numerous studies have investigated compression of LLMs to reduce inference cost. The most popular LLM compression techniques are quantization  and pruning . A less explored, but powerful technique is _tensor decomposition_, and in our work, we propose a novel, _activation-centric_ way to decompose LLM tensors. Our proposal is to project activations onto a static set of components optimizing fidelity. The projection reduces activation dimensionality and leads to weight compression at inference as a byproduct of matrix multiplication associativity.

### Related work and motivation for activation-centric tensor decomposition

Recent research has proposed many **quantization** and **pruning** techniques for compressing LLMs. Examples of advances in LLM quantization include SmoothQuant , AWQ , and GPTQ ; while notable LLM pruning works include SparseGPT , LLM-Pruner , and ReLU-based masking . These methods are conceptually orthogonal to our proposal for activation projection which can beimplemented in low precision or sparse formats. Nevertheless, compression fundamentally introduces noise, and an open problem is to study the impact of combining different methods, e.g., quantization and matrix factorization. This is beyond the scope of our paper, but a good direction for future work.

Our work is also orthogonal to **non-compressive** LLM serving acceleration such as continuous batching  or speculative decoding , and attention optimizations such as PagedAttention , RadixAttention , and FlashAttention . Our study is on matrix multiplication layers involving weights and activations, and hence is mutually exclusive to works improving cross multiplications of activation tensors in attention. In fact, all our experiments use FlashAttention.

Finally, we turn to **tensor decomposition**, also known as **factorization**. Thus far, compression for LLM inference using factorization has been focused on **weight decomposition**. KnGPT  uses the Kronecker transform to pack a large matrix into two smaller ones. TSVD  performs iterative singular value decomposition (SVD) on weight matrices to produce high rank ternary components. TensorGPT  and HEAT  compress weight matrices into a cascade product of small matrices using the tensor-train algorithm. SVD-LoRa  uses a truncated SVD on weights and finetunes the model using LoRa . The LoRa adapters are then merged to the main branch using bounds on the rank of sum of low rank matrices. ASVD  performs a truncated SVD on the weights after re-scaling them by a diagonal matrix and their inverse encapsulating activation statistics. This work realizes the importance of activation-awareness but still uses weight-centric compression. SliceGPT  extracts principal components in normalization layers to guide the deletion of rows and columns in weight matrices. The compression is achieved using a factorization made implicit via computational invariance. The statistical method employed by sliceGPT shares similarities with one of our results, but our problem formulation and solution are different.

Factorization can also streamline LLM training and finetuning. For instance, LoRa  finetunes pretrained models using residual low rank adapters which are then absorbed into the main branch. Similarly, GaLore  applies a low rank approximation to gradients in back-propagation. These works do not modify inference parameter and operation count, and are hence orthogonal to ours. Our method could be applied in tandem with LoRa or GaLore, but this is beyond the scope of this paper.

Since factorization increases the number of LLM tensors, achieving high compression rates requires the intermediate dimensions to be much smaller than that of original dot product. This breakage in computation usually necessitates a retraining or finetuning stage to be healed. Unfortunately, this healing process is impeded because factorized LLMs have fewer learnable parameters which decreases expressivity [17; 22].

To our knowledge, no prior art has explored **activation decomposition**. Indeed, applying factorization solvers (e.g., SVD) dynamically incurs large inference runtime overheads. Yet, activation decomposition has several desired features which we examine in Section 2 and motivate via the following insights: (a) weights stay uncompressed during retraining, preventing the aforementioned loss of expressivity; (b) large activation tensors contain inherent redundancies making them prime candidates for compression; and (c) since most LLM computation comprises multiplications of weights and activations, decomposing the latter can lead to compressing the former at inference.

### Contributions

We propose Eigen Static Principal Activation Component Estimation (ESPACE), an LLM compression technique based on activation dimensionality reduction. Our contributions are as follows:

* We project activation tensors onto a static and pre-calibrated orthonormal matrix. The projection lowers activation dimensionality but keeps weight matrices intact and fully available for training.

Figure 1: Perplexity\({}^{2}\) versus model size for GPT3 and Llama2 models and comparison to compressed models using ESPACE.

At inference, leveraging matrix multiplication associativity, model compression is achieved through pre-computation of the product of weight and projection matrices.
* We theoretically derive optimal constructions for activation dimensionality reduction. Specifically, the projection matrix is calibrated in a manner to minimize activation decomposition mean squared error and forward propagated noise metrics. The solution is based on an eigenvalue decomposition of activation auto-correlation and yields multiple candidate projections for each activaton tensor.
* We empirically study compression of models in the GPT3, Llama2, and Nemotron4 families evaluated on the Wikitext-103 dataset for perplexity and the LM evaluation harness for downstream task accuracy. The amelioration in size versus perplexity2 trade-offs is summarized in Figure 1. * We show that ESPACE can compress LAMs by \(\)50% at the cost of a small accuracy loss, as low as 0.18 increase in perplexity on GPT3-22B.
* At lower compression rates, we find encouraging empirical evidence that ESPACE filters out noise and improves accuracy; e.g., \(\)20% compressed GPT3-8B lowers its baseline perplexity by 0.38.
* As an additional benefit of ESPACE, tangible latency reduction of 35%-to-45% is obtained in matrix multiplication layers. This speed-up translates to up to \(\) 40% faster prefill inference latency metricized by the time to first token and measured on existing hardware.
* By comparison to existing works on tensor decomposition, we determine that ESPACE is a first step in pushing the frontier of compression rate versus accuracy retention (see Figure 4).

## 2 Dimensionality Reduction & Projections

In this section, we introduce notation for matrix multiplication, review weight decomposition, and introduce our proposed mechanism of dimensionality reduction via activation projections.

### Matrix Multiplication and Weight Decomposition

We consider general matrix multiplications (GEMMs) described in Figure 2(a) of the form

\[=^{T}\] (1)

where \(\) is a weight matrix of size \(K N\) and \(\) is an input activation tensor of size \(K M\) so that the output activation tensor \(\) is of size \(N M\). Typically, \(K\) and \(N\) are defined by network topology and layer instance, they are commonly referred to as _embedding_ or _hidden_ size. In contrast, \(M\) stacks multiple dimensions in an activation tensors to obtain a 2D matrix view. Generally, these are the _sequence_ and _batch_ dimensions.

Transformer-based LLMs have four GEMM layers per block: query-key-value (QKV), projection (Proj), fully connected 1 (FC1), and fully connected 2 (FC2) layers. Our study is concerned with these layers, while cross activation multiplication and embedding layers are untouched. For notational simplicity, in this paper, _we do not include layer indices in our equations_.

The matrix \(\) in (1) stores layer parameters and dictates the model's inference accuracy. To improve convergence of these parameters, an optimizer state is stored alongside weights during training and tracks historical values of gradients and updates [23; 24]. On the other hand, the activation tensor \(\) depends on the input stimulus to the network, and is therefore generated on the fly.

Thus, at inference, weights are fixed but activations are dynamic. As a consequence, prior work on tensor decomposition has focused on compressing frozen weight matrices. One way of doing so is breaking \(^{T}\) into a low-rank approximation using some form of truncated SVD [20; 18], which is described in Figure 2(b). Specifically, (1) is approximated as:

\[\] (2)

where \(\) and \(\) are matrices of size \(N L\) and \(L K\), respectively, with \(L\) being the factorization rank. For the decomposition procedure to be useful, two conditions need to be met: (a) \(L<<(K,N)\) for compression, and (b) the approximation \(^{T}\) should be accurate. However, achieving both conditions simultaneously may be challenging because a very low rank factorization usually leads to significant accuracy drop .

As with other compression techniques, e.g., quantization and pruning, retraining of the compressed model may be employed to recover accuracy. However, the decomposition in (2) introduces two training-related hurdles: (a) the effective number of trainable parameters has decreased significantly which reduces model expressivity, and (b) the breakage of spatial weight structure prevents the retraining procedure from loading the original optimizer state. Retraining a model without its optimizer state is known to introduce significant difficulty in convergence .

### Activation Decomposition via Static Projection

Since weight decomposition poses the above hurdles, we motivate the need for an activation-centric solution. In some measure, activation compression may be more achievable due to the large stack dimension \(M\) comprising batches and sequences. Statistically, the Central Limit Theorem claims that stacking data is likely to exhibit redundancies . In the case of LLMs, such redundancies are further pronounced due to the likelihood of repeated tokens and information in natural language.

Therefore, activations should be prime candidates for tensor decomposition. Nevertheless, prior arts have not explored activation decomposition due to one fundamental limitation: unlike weights, activations are generated on the fly; meaning that tensors must be compressed during inference, potentially incurring large runtime penalties.

We propose to apply _static_ dimensionality reduction on the activation tensor \(\) in (1). Concretely, our proposal is to project \(\) onto a pre-computed static orthonormal matrix \(\) of size \(K L\), where crucially \(L<<K\). Reconstructing \(\) requires a re-expansion using the transpose of the projection matrix, i.e., \(^{T}\). While \(^{T}=_{L L}\), we note that \(^{T}_{K K}\) since \(L<<K\). Thus, the proposed activation transformation is noisy, and in Section 3, we derive optimal conditions on the calibration of \(\) to minimize the effects of this noise.

Our proposal, described in Figure 2(c) is to approximate the GEMM in (1) using the following:

\[=^{T}^{T}^{ T}=^{T}(^{T})= (^{T})^{T}(^{T})\] (3)

where we used associativity of matrix multiplication to highlight key aspects of our approach.

**During training/finetuning:** we view our GEMM as \(^{T}(^{T})\) where \(\) has been replaced by its approximation. We emphasize that \(\) is static and does not get updated during training.

Figure 2: Decompositions in GEMMs: (a) baseline multiplication of weight matrix and activation tensor, (b) truncated SVD on the weight matrix, and (c) proposed approach of inserting a static matrix to project activations. With ESPACE, all weights are available for training, while inference compression is achieved via per-computation of \((^{T})\).

Meanwhile, \(^{T}\) is fully available for adaptation to the activation approximation. The availability of all learnable weights elides losing model expressivity. The structure of \(^{T}\) is also unchanged and can be mapped to the baseline's optimizer state. Thus, the proposed approach does not suffer from the same limitations as weight decomposition techniques. We do note that introducing \(\) induces a small storage overhead at train time. However, when \(L<<K\), and the order of computation is properly compiled, the number of operations per iteration is lower than baseline training; and, though not central to our contribution, we did observe up to 15% reduction in training iteration time for 50% compressed models.

**During inference:** we view our GEMM as \((^{T})^{T}(^{T})\) where the required matrices are \(\) of size \(K L\) and \((^{T})\) of size \(N L\), which is pre-computed before deployment. Thus, per-layer parameter count required for inference has decreased from \(KN\) to \(L(K+N)\), which, provided \(L<<\{K,N\}\) presents an opportunity for significant model compression. For instance, if \(N=K\), i.e., \(^{T}\) is square, and \(L=}{{4}}\), then our method yields 50% compression at inference time. This is one of the compression rates we target in Section 4.

We emphasize that \(\) is not shared across GEMM layers; rather, each GEMM layer decomposed according to (3) has its own pre-calibrated matrix \(\). Furthermore, by virtue of (3) not introducing dependencies across mini-batches, our method is fully compatible with data parallelism.

## 3 Eigen Static Principal Activation Component Estimation

Our proposed activation decomposition induces an approximation error as \(^{T}\). In this section, we first introduce an ergodic estimation of activation auto-correlation. This important statistic is then used for theoretical constructions of \(\) with guarantees on computational accuracy. Multiple results are presented and then combined in our compression studies in Section 4.

### Activation auto-correlation estimation

Let \(\) be an arbitrary \(K\)-dimensional vector in \(\); we define the _activation auto-correlation_ matrix of size \(K K\) as \(_{}=[^{T}]\) where expectation is taken over activation vectors. This matrix is symmetric positive semi-definite having a _real_ eigenvalue decomposition (EVD) \(_{}=^{T}\) where \(\) is an orthonormal matrix whose columns are eigenvectors, and \(\) is a diagonal matrix containing the corresponding _non-negative_ eigenvalues, assumed to be sorted in decreasing order. The eigenvector corresponding to the \(i^{}\) largest eigenvalue is called \(i^{}\)_principal_ eigenvector.

This autocorrelation matrix can be empirically estimated using an instance of the activation tensor:

\[=[_{1}||_{}] ^{T}=[_{1}_{1}^{T}++ _{M}_{M}^{T}]_{}= ^{T}/_{M}\] (4)

However, evaluating (4) and its EVD dynamically introduces a prohibitive computational overhead. Thus, we estimate \(}_{}\) in a pre-deployment calibration process. Specifically, during calibration, we sample and forward pass \(B\) random input batches, and for each, calculate \(_{}^{()}=^{(i)}^{(i)^{T}}/_{M}\), where superscript \(i\) denotes batch index. Then, we average our estimate of the auto-correlation matrix as \(_{}=^{B}}}{{_{}^ {(i)}}}/B\) and use its eigenvalue decomposition for further optimizations.

This ergodic approach of estimating activation statistics as part of a calibration process has been employed to great effect in other compression works on quantization [26; 27] and pruning .

### Activation decomposition with minimum mean squared error

Let us write \(}=^{T}\); for a vector \(\), its counterpart in \(}}\) is given by:

\[}=_{i=1}^{L}_{i}, _{i}\] (5)

where \(\{_{i}\}_{i=1}^{L}\) are the orthonormal column vectors of \(\), i.e., \(_{i},_{j}=_{\{i==j\}}, i,j 1 L\).

We define the mean squared error (MSE) of the decomposition as

\[[\|-}\|^{2}]\] (6)

with the \(L_{2}\)-norm used throughout this paper. Our first result constructs \(\) minimizing this MSE.

**Theorem 1**.: _For an activation tensor \(\) whose auto-correlation matrix has an eigenvalue decomposition given by \(_{}=^{T}\), the projection matrix \(\) minimizing the mean squared error in (6) is given by \(=[_{1}||_{L}]\) where \(_{i}\) is the \(i^{}\) principal eigenvector in \(\)._

Proof.: See Appendix A.1. The result is readily obtained by substituting \(}\) in (5) into (6) and minimizing the MSE which involves quadratic forms involving the positive semi-definite \(_{}\). 

Theorem 1 shares similarities with the Principal Component Analysis (PCA) algorithm . PCA extracts low dimensional features having maximum correlation with input data. Unlike PCA, we omit input normalization to elide its computational cost. Still, we term the columns of \(\) in Theorem 1 as Principal Activation Components. Since those are obtained using an EVD on a static estimation of \(_{}\), we call our method Eigen Static Principal Activation Component Estimation (ESPACE).

The MSE in Theorem 1 is a strong indicator of the quality of an approximation technique, e.g., it is often employed in quantization studies [26; 27]. However, empirical data may contain large outliers which can dominate the optimization process; say a few high-magnitude vectors in (6) masking the contribution of small data on the solution. An alternate metric to the MSE can be employed to prevent such artifacts in averaging: the normalized MSE (NMSE) defined as:

\[[\|-}\|^{2}/\| \|^{2}]\] (7)

The solution of Theorem 1 can be slightly modified to minimize the NMSE in (7).

**Corollary 2**.: _For an activation tensor \(\), let \(}_{}=[(/\| \|)(/\|\|)^{T}]\) be its input-normalized auto-correlation matrix having an eigenvalue decomposition given by \(}_{}=^{T}\), the projection matrix \(\) minimizing the normalized mean squared error in (7) is given by \(=[_{1}||_{L}]\) where \(_{i}\) is the \(i^{}\) principal eigenvector in \(\)._

Proof.: The proof in Appendix A.2 uses equivalence of NMSE and MSE with \(L_{2}\)-normalized vectors. 

Corollary 2 applies to the decomposition in (3) with no activation normalization required at compute time. Rather, normalization is done during calibration, where \(}_{}\) is estimated instead of \(_{}\).

Both solutions in Theorem 1 and Corollary 2 are options to be employed in ESPACE, where either may be more suitable on a layer-wise basis. Next we present further options for ESPACE based on the optimization of alternate metrics to the MSE and NMSE.

### Activation decomposition with optimized forward propagated accuracy metrics

While local fidelity metrics, such as the MSE and NMSE above, are good indicators of the quality of an approximation technique, it has been shown that better insights on a neural network's accuracy may be derived via the study of forward propagated noise [30; 31; 32]. In this section, we study the effects of the decomposition in (3) on the output of the GEMM, and the output loss of the model.

At a given layer, let us write an arbitrary scalar in the GEMM output tensor in (1) as \(y\). Note that \(y=,\) for some weight vector in \(\) and activation vector \(\). We also let \(\) be the associated output when the GEMM is approximated by (3), which is given by \(=,}\) with \(}\) given by (5). We define the GEMM Output-referred MSE (GO-MSE) as \([(y-)^{2}]\).

Similarly, given an input to the network, we write the output loss function (the vocab cross-entropy) as \(\). When one arbitrary activation tensor is transformed as per (3), a mismatch in computation is introduced and propagated all the way to the output. We let \(}\) be the resulting new value of the loss function. We define the Network Loss-referred MSE (NL-MSE) as \([(-})^{2}]\).

A closed form solution for \(\) in (3) minimizing the GO-MSE and NL-MSE is elusive to us. Therefore, we derive upper bounds on these metrics which we use as a proxy for optimization.

**Proposition 3**.: _For a GEMM in (1) and its decomposition in (3), the GO-MSE is upper bounded by:_

\[[(y-)^{2}] 2[\| \|^{2}\|\|^{2}]-2[ ,,}]\] (8)_and the NL-MSE is upper bounded by_

\[[(-})^{2}] 2 [\|_{}\|^{2}\|\|^{2}]-2 [_{}, _{},}]\] (9)

_where a first order Taylor approximation on the loss function is assumed and its gradient with respect to vector \(\) is denoted as \(_{}\)._

Proof.: The proof in Appendix A.3 first shows \(\|}\|^{2}<\|\|^{2}\) and then uses the Cauchy Schwarz inequality to establish both bounds. 

Next, we provide closed form solutions for \(\) in (3) minimizing the bounds in Proposition 3.

**Theorem 4**.: _For a GEMM in (1) and its decomposition in (3), the projection matrix minimizing the bounds in Proposition 3 is given by \(=[_{1}||_{L}]\) where \(_{i}\) is the \(i^{}\) principal eigenvector in \(\) obtained via eigenvalue decomposition on a matrix \(=^{T}\) defined as:_

\[=[^{T}^{T}+ ^{T}^{T}] =[^{T}_{}_ {}^{T}+_{}_{}^{T} ^{T}]\] (10)

_to minimize the upper bounds on GO-MSE in (8) and NL-MSE in (9), respectively._

Proof.: The proof is included in Appendix A.4, where we also include modifications required in calibration. Specifically, \(_{}\) is reused and left/right multiplied by \(^{T}/_{N}\) to yield \(\) in (10) minimizing the bound on GO-MSE. An additional backward pass is needed to properly scale activation vectors and their gradients when calibrating \(\) in (10) minimizing the bound on NL-MSE. 

Theorem 4 augments Theorem 1 and Corollary 2 with two options for the design of \(\). Much like Corollary 2, we supplement our new solutions with \(L_{2}\)-normalization to include

\[}=[(^{T} ^{T}+^{T}^{T})/ \|\|^{2}\|\|^{2}]}=[(^{T}_{}_{}^{T}+_{}_{}^{T}^{T})/\|_{}\|^{2}\|\|^{2}]\]

as alternate choices for the calibrated matrices \(\) in (10). Unlike Corollary 2, \(L_{2}\)-normalization in these two matrices does not correspond to a notable optimization. Nevertheless, these options are retained in the spirit of suppressing the influence of large data in calibration.

Thus, overall we have six choices for \(\). Since each can be obtained as part of a fast and pre-deployment calibration phase, we may simply select the best one for each layer. In our experiments of Section 4, the best candidate is determined via a per-layer validation over all six choices. A sensitivity study on the impact of each of the six candidates is provided in Appendix B.4.

## 4 Model Compression Studies

In this section, we report on experimental studies investigating LLM compression using ESPACE.

### Experimental setup

We employ three sets of open source LLMs: GPT3 , Llama2 , and Nemotron4 . Specifically, we experiment on GPT3-{1.3B, 8B, 22B}, Llama2-{7B, 13B}, and Nemotron4-15B. Accuracy is evaluated in two ways: perplexity measured on the Wikitext-103 dataset  and zero-shot downstream task accuracy of: BooIQ (BQ) , Hellaswag (HS) , PIQA (PQ) , RACE (RA) , and WinoGrande (WG) .

The Wikitext-103 dataset is split into train, validation, and test sets. We use 512 random sequences from the training set for calibrating projection matrices required by ESPACE. We use the validation set for layer-wise sensitivity studies. The test set is used to report perplexity results in this section.

Our implementation uses NVIDIA's Megatron LM  and downstream task evaluation invokes Eleuther AI's LM evaluation harness . For the latter, we report raw accuracy scores, and their average; we do not post process results or apply normalization to the scores.

When ESPACE is applied, we retrain the models to adapt to the approximation error of activation projection as discussed in Section 2. Retraining simply extends the models' pre-training sessions and uses the 330B-token MTNLG dataset , which was used to train GPT3 models. All implementation details are included in Appendix B to help reproducibility of our results.

We metricize model size reduction via inference compression rate. Specifically, for layers decomposed per (3), we count the number of entries in \(\) and \((^{T})\); for other layers, we count those in \(^{T}\). We also report the latency of executing all network GEMMs in (1) or (3), which we measure using a NVIDIA A100 GPU and a simple, un-optimized implementation (see Appendix B.4). We also report prefill inference latency, metricized via the Time to First Token (TTFT), and measured using the Megatron-LM implementation. In our measurements, we use a batch size of 1 and sequence length of 2048 and 4096 for GPT3 and Llama2/Nemotron4 models, respectively. The reported reductions in total GEMM latency and TTFT constitute evidence that compression improves inference throughput. It is beyond the scope of this paper to evaluate the impact of ESPACE on end-to-end token throughput and latency on LLM inference serving systems, since this requires a complex set of optimizations including but not limited to optimization of back-to-back GEMMs into fused kernels, KV caching, continuous batching, as well as thorough performance studies with varying input and output sequence lengths. Thus, we leave an evaluation of token generation throughput and energy savings and improvements to future work.

### Validation perplexity studies

Our experiments start with a calibration phase where we prepare the static projection matrix \(\) for each layer. The dimension \(L\) in \(\) is chosen as the lowest power of two such that layer compression is at least 50%. The power-of-two restriction ensures best tensor core utilization, and the resulting compression rate depends on the dimensions of the original layer (\(K\) and \(N\)). Exact details of these values for all layers and models are included in Appendix B.2.

We perform a sensitivity study on the Wikitext-103 validation perplexity when ESPACE is applied out-of-the-box (no retraining) one layer at a time. For each layer, we identify which of our six candidates projection matrices in Section 3 yields lowest validation perplexity. Layers are then sorted according to their impact on perplexity from least to most destructive. We then evaluate the validation perplexity when ESPACE is progressively applied to out-of-the-box to all layers according to this ranking. Fine-grained details of this exploration are included in Appendix C for all models.

This exploration yields an interesting finding: as we progressively apply ESPACE to more layers, the perplexity marginally increases until an inflection point after which accuracy degradation accelerates. This inflection occurs at 20% to 40% compression depending on the model. Figure 3 depicts this phenomenon for GPT3-22B, and the same data for other models can be found in Appendix C.2.

We find that out-of-the-box application of ESPACE works better for larger models; GPT3-22B, the largest model we experimented on, exhibits an inflection in perplexity at 40% compression, which is the highest in our results. This is consistent with many earlier works on general compression of neural networks [44; 45; 46; 47]. Interestingly, a 20% out-of-the-box compressed GPT3-22B is iso-accurate to its uncompressed counterpart (see Figure 3); without retraining, its test perplexity of **6.61** which is within 1% of the 6.55 baseline.

After the above validation study is performed, we select two configurations for layers to be compressed using ESPACE: (a) layers corresponding to the inflection point, i.e., 20% to 40% compression, and (b) as many layers needed to achieve a compression of \(\)50%. For both configurations, we retrain the compressed models and further evaluate their achievable accuracy.

### Compression of GPT3 models

Once compression targets and layer configurations are set, we retrain GPT3 models on the MTNLG dataset. Although we use all of the 330B available tokens, we do observe the training loss quickly

Figure 3: Validation perplexity for GPT3-22B when ESPACE is progressively applied to its GEMM layers. The order of layer selection is based on a layer-wise sensitivity analysis.

converging. We leave training hyperparameters unchanged except for one: we disable dropout. Our rationale is that activation projection is one form of deterministic and structured dropout such that additional regularization may not be needed. Results3 on GPT3 models are included in Table 1.

We find that ESPACE can compress GPT3 models by \(\)50% at the cost of a small accuracy degradation. In the case of GPT3-22B, the perplexity increase is of only 0.18; in general, the gap decreases for larger overall model size. By and large, similar trends are observed for downstream task accuracies and we note that most scores of 50% compressed models fall within 5% of the baseline.

For lower compression ratios (inflection points at 20% to 40%), ESPACE converges to an accuracy _better than that of the baseline_. The best improvement occurs for GPT3-8B, where ESPACE produces a 6B model with 0.38 lower perplexity than its 8B baseline. The improvements are observed both in terms of perplexity and downstream task accuracy. While GPT3 models may be over-parameterized, we posit that ESPACE acts a regularizer at moderate compression rates. Specifically, we believe that projection onto principal activation components filters out unnecessary information coming from small eigenvalue components.

For all models, we observe an encouraging translation of compression to GEMM latency reduction by up to 49% which leads to noticeable speed-up in TTFT by up to 43%..

### Compression of Llama2 models and comparison to related works

For Llama2, we only retrain using 200B MTNLG tokens because we observed quick convergence for GPT3. Llama2 models were trained on an undisclosed dataset of 2T tokens . Therefore, with 200B tokens, the healing phase of ESPACE constitutes no more than 10% of the original pre-training session. Since Llama2 pre-training details are not openly available, we re-used all hyperparameters from GPT3, which is likely to be sub-optimal. In spite of the two handicaps of dataset disparity and hyperparameter sub-optimality, we obtained promising results as reported in Table 1.

For Llama2-7B, we first retrained the uncompressed baseline. The purpose of this experiment is twofold: (a) ensure that our hyperparameters at least do not corrupt the model, and (b) verify that the

  Method & \# of &  &  &  &  \\  (Compression) & Weights & Latency & Megatron-LM & PPL \(\) & RQ & HS & PQ & RA & WG & Avg. \\    \\  Baseline & \(1.21 10^{9}\) & 24.2ms & 39.8ms & 9.94 & **64.3** & 43.5 & **74.2** & **37.6** & 58.1 & 55.5 \\  ESPACE (20\%) & \(9.71 10^{8}\) & 20.6ms (-15\%) & 36.1ms (-9\%) & **9.53** & 60.6 & **45.1** & _73.0_ & _36.4_ & **62.9** & **55.6** \\  ESPACE (47\%) & \(6.42 10^{8}\) & 15.9ms (-34\%) & 31.7ms (-20\%) & 11.07 & _62.3_ & 39.9 & _71.6_ & 34.5 & _58.7_ & _53.4_ \\    \\  Baseline & \(8.05 10^{9}\) & 136ms & 186ms & 7.38 & 69.0 & 54.2 & 78.1 & **41.4** & 67.8 & 62.1 \\  ESPACE (21\%) & \(6.33 10^{9}\) & 110ms (-19\%) & 155ms (-16\%) & **7.00** & **70.3** & **55.3** & **78.9** & _40.7_ & **69.3** & **62.9** \\  ESPACE (50\%) & \(4.08 10^{9}\) & 76.8ms (-44\%) & 122ms (-35\%) & _7.66_ & _66.5_ & _52.3_ & _77.6_ & 38.9 & _66.9_ & _60.4_ \\    \\  Baseline & \(2.17 10^{10}\) & 354ms & 457ms & 6.55 & 76.4 & 57.2 & 79.3 & **40.7** & **70.5** & **64.8** \\  ESPACE (40\%) & \(1.30 10^{10}\) & 229ms (-35\%) & 313ms (-32\%) & **6.29** & **76.6** & **57.3** & **79.5** & _40.2_ & _70.2_ & **64.8** \\  ESPACE (55\%) & \(9.74 10^{9}\) & 181ms (-49\%) & 261ms(-43\%) & _6.73_ & 72.2 & 55.8 & _79.3_ & _40.1_ & _69.7_ & _63.4_ \\    \\  Baseline & \(6.48 10^{9}\) & 210ms & 368ms & **5.06** & **79.2** & 57.1 & 78.1 & **44.0** & 69.5 & 65.6 \\  Retained (0\%) & \(6.48 10^{9}\) & 210ms & 368ms & **5.06** & 78.2 & **57.9** & 78.0 & 43.7 & **70.6** & **65.7** \\  ESPACE (21\%) & \(5.11 10^{9}\) & 169ms (-19\%) & 322ms (-12\%) & _5.07_ & _77.7_ & _57.7_ & _78.7_ & _42.7_ & _69.2_ & _65.0_ \\  ESPACE (50\%) & \(3.24 10^{9}\) & 113ms (-46\%) & 266ms (-28\%) & 5.67 & 72.2 & 52.0 & 76.5 & 38 & 63.5 & 60.4 \\    \\  Baseline & \(1.27 10^{10}\) & 406ms & 643ms & 4.61 & **82.4** & 60.2 & **79.5** & **46.8** & 71.9 & **68.2** \\  ESPACE (20\%) & \(1.01 10^{10}\) & 336ms (-17\%) & 562ms (-13\%) & **4.59** & 78.3 & **60.5** & **79.5** & 43.0 & **72.8** & _66.8_ \\  ESPACE (50\%) & \(6.34 10^{9}\) & 259ms (-36\%) & 447ms (-31\%) & 5.13 & 75.7 & 56.2 & 78.0 & 41.5 & _69.1_ & 64.1 \\    \\  Baseline & \(1.25 10^{10}\) & 414ms & 741ms & **6.06** & 78.3 & **62.1** & **81.1** & **47.0** & **75.2** & **68.7** \\  ESPACE (25\%) & \(9.54 10^{9}\) & 324ms (-22\%) & 655ms (-12\%) & _6.28_ & **78.9** & _59.9_ & _80.0_ & _46.4_ & _72.8_ & _67.6_ \\  ESPACE (50\%) & \(6.25 10^{9}\) & 223ms (-46\%) & 545ms (-26\%) & _6.93_ & 77.9 & 57.0 & _77.8_ & 42.4_ & 69.9 & 65.0 \\  

Table 1: GEMM latency, time to first token, Wikitext-103 perplexity (WK-103 PPL), and downstream task accuracy of GPT3, Llama2, and Nemotron4 models compressed with ESPACE\({}^{3}\).

healing process is not just an artifact of processing more tokens. Both hypotheses appeared to be valid: the retrained baseline has nearly identical accuracy compared to the original model.

Generally, we find that the trends of ESPACE compression for Llama2 are similar to GPT3, albeit slightly less successful. Though the results are still promising, we attribute the slight shortcomings in accuracy to the handicaps above. We find that 50% ESPACE compression on Llama2 leads to \(\)0.6 perplexity increase and similar degradation in terms of downstream task accuracy. Notably, compressing the Llama2-13B model to to a 6.3B model yields comparable accuracy to the Llama2-7B baseline which itself is a 6.5B model.

In addition, for 20% compression, we find that ESPACE matches the accuracy of the baseline for Llama2 models. While not as impressive as the improvements observed with GPT3, ESPACE is able to produce 5B and 10B models matching the 7B and 13B baselines, which does push the pareto frontier of accuracy versus model size in the right direction as shown in Figure 1.

The Llama2-7B model has been used in related works on tensor decomposition mentioned in Section 1.1; specifically, ASVD , SVD-LoRa , and SliceGPT . Both ASVD and sliceGPT have reported perplexity on Wikitext, but SVD-LoRa performed task-specific finetuning on a variety of datasets and averaged perplexities. Therefore, in Figure 4, we compare our results to these works using perplexity increase over baseline, rather than raw perplexity, for maximum inclusivity.

SVD-LoRa performed an SVD decomposition on the weights such that the intermediate dimension is half of dot-product which leads to no compression. On the other hand, ASVD and sliceGPT can only achieve modest compression ratios of up to 25% with some loss in accuracy. Recall that these works apply factorization on weights which is the fundamental difference to ESPACE. As seen in Figure 4, ESPACE is a step in the right direction towards improving the state-of-the-art in tensor decomposition of LLMs.

### Compression of Nemotron4-15B

Finally, we used ESPACE to compress Nemotron4-15B into 9.54 and 6.25 billion parameters, as reported in Table 1. Retraining consumed 275B tokens which corresponds to \(\) 3% of this model's original training session. Once more, compression with ESPACE leads to minimal degradation in the moderate regime (25%) and yields a small accuracy drop in the aggressive regime (50%).

Consistently with our findings for the above models, ESPACE reduces GEMM execution time by up to 46%. This, in turn, improves the TTFT by up to 26%. An interesting observation is that, for Llama2 and Nemotron4 models, the TTFT improvement is slightly less pronounced than for GPT3 models. This is simply due to the fact that the latter uses a sequence length of 2048, whereas the former two use 4096. A larger sequence length means more time is spent in attention cross-activation products which amortizes the speed-up in the GEMM layers.

## 5 Conclusion

We have presented ESPACE, a novel compression technique realizing tensor decomposition of LLMs in an activation-centric manner. A set of theoretical results were derived to guide the construction of activation projection which is done statically. Experimentally, we have shown promising results where ESPACE is able to \(\)50% compress modern LLMs at the cost of a small accuracy degradation. Compared to related works, ESPACE is a first step in pushing the frontier of model size versus accuracy trade-offs. Future work includes combining ESPACE with alternate compression techniques such as quantization and pruning, and evaluating decomposition of activation tensors in attention. As potential extension to our algorithm, the use of matrix sketching and random projections may pave the way for better overall compressibility.

Figure 4: Comparison to related works compressing Llama2-7B using matrix factorization techniques.