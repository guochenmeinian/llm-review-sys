# Reflective Multi-Agent Collaboration based on Large Language Models

Xiaohe Bo\({}^{1}\), Zeyu Zhang\({}^{1}\), Quanyu Dai\({}^{2}\), Xueyang Feng\({}^{1}\),

Lei Wang\({}^{1}\), Rui Li\({}^{1}\), Xu Chen\({}^{1}\)\({}^{*}\), Ji-Rong Wen\({}^{1}\)

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Huawei Noah's Ark Lab

{xiaohe,zeyuzhang,xueyangfeng,wanglei154,lirui121200,xu.chen,jrwen}@ruc.edu.cn,

daiqunyu@huawei.com

Corresponding Author.

###### Abstract

Benefiting from the powerful language expression and planning capabilities of Large Language Models (LLMs), LLM-based autonomous agents have achieved promising performance in various downstream tasks. Recently, based on the development of single-agent systems, researchers propose to construct LLM-based multi-agent systems to tackle more complicated tasks. In this paper, we propose a novel framework, named **COPPER**, to enhance the collaborative capabilities of LLM-based agents with the self-reflection mechanism. To improve the quality of reflections, we propose to fine-tune a shared reflector, which automatically tunes the prompts of actor models using our counterfactual PPO mechanism. On the one hand, we propose counterfactual rewards to assess the contribution of a single agent's reflection within the system, alleviating the credit assignment problem. On the other hand, we propose to train a shared reflector, which enables the reflector to generate personalized reflections according to agent roles, while reducing the computational resource requirements and improving training stability. We conduct experiments on three datasets to evaluate the performance of our model in multi-hop question answering, mathematics, and chess scenarios. Experimental results show that COPPER possesses stronger reflection capabilities and exhibits excellent generalization performance across different actor models.

## 1 Introduction

With the emergence of Large Language Models, LLM-based autonomous agents are becoming a research hotspot in the field of artificial intelligence. Leveraging the impressive planning and reasoning ability of LLMs, these agents can understand and generate human-like instructions, engage in sophisticated interactions, and make decisions in a wide range of contexts, leading to remarkable success in various downstream tasks . Recently, based on the development of single-agent systems, researchers propose to construct multi-agent systems in response to the growing task complexity. Prior works  suggest that multiple agents can help improve factuality and reasoning, encourage divergent thinking, and effectively facilitate task completion.

To improve the collaborative performance of multi-agent systems, various cooperation frameworks  have been developed, which generally encode intricately crafted agent profiles and cooperation mechanisms into prompts. However, hindered by the contextual understanding ability of LLMs, such frameworks fall short of fully exploiting the collaborative capacities of agents. To tackle this challenge, one natural idea is to gather extensive collaborative data for agents' fine-tuning. Yet this strategy risks diminishing the model's general abilities , contradicting the aspiration to

[MISSING_PAGE_EMPTY:2]

COPPER brought improvements of 31.8%, 18.5%, and 86.4% on the HotPotQA, GSM8K, and Checkmate in One Move datasets, respectively.

## 2 Related Work

### LLM-based Multi-Agent Systems

Based on the development of single-agent systems, LLM-based multi-agent systems have been rapidly studied and achieved significant progress in complex task resolution and world simulation. Within the task resolution domain, various agents, each with specialized expertise are developed to collaborate on complex problems. For instance, [9; 32; 3; 29; 17] suggest improving the accuracy of scientific question-answering tasks through multi-agent debates. [23; 13] suggest constructing multi-agent systems for software development following the waterfall or Standardized Operating Procedures (SOPs) workflow. Another mainstream application scenario of LLM-based multi-agent systems is the world simulation, which mainly leverages the role-playing abilities of agents to represent different roles and perspectives within a simulated environment. Research in this area is advancing quickly and encompasses a wide variety of fields, including social sciences [42; 11], gaming [33; 34; 18; 20; 1], psychology , economics [16; 41], policy making , etc. In this paper, we focus on improving the complex problem-solving abilities of multi-agent systems.

### Self-Reflection of Large Language Models

Various studies on reflection mechanisms have been proposed, which play a crucial role in enabling LLM-based agents to learn from the environment and improve themselves autonomously. Early works primarily focus on refining responses based on a single feedback [19; 6] or contrast between multiple models [10; 40] and fail to form a comprehensive understanding of the task based on past experiences. Recently, Reflexion  involves prior trajectories and environmental rewards to generate reflections, which are further incorporated into the context of subsequent episodes. Although it enables iterative enhancements, the effectiveness of reflections heavily relies on the model's inherent reflective capabilities. In light of that, Retroformer  proposes to fine-tune the reflector using environmental rewards with a standard RLHF  process. However, optimizing reflection in multi-agent systems remains challenging, which is crucial for improving agents' cooperation capacity and task performance.

## 3 Preliminary

In this paper, we use a tuple \((N,,,_{_{o}},)\) to denote the LLM-based multi-agent cooperation system, where \(N\) stands for the number of agents, \(=S_{1} S_{2} S_{N}\) is the joint space of environment states, \(=A_{1} A_{2} A_{N}\) is the joint action space and \(_{_{o}}\) is the state transition function. Here, we denote the randomness associated with the state transition using \(_{o}\) according to . In cooperative settings, all agents share an aligning goal, and the reward function \(:(,)\) is typically designed to promote collaboration. One major challenge in cooperation settings is credit assignment, which means we need to decompose \(\) into \(R_{1} R_{2}... R_{N}\) and evaluate the agent contribution with respect to their objective in the form of a scalar value. The multi-agent systems complete the target task through interactions with the environment. Here we use trajectory \(=\{s_{0},a_{0},s_{1},a_{1},,s_{T},a_{T}\}\) to denote the process and describe the accumulative reward using \(R()\), where \(R()=_{t=0}^{T}(s_{t},a_{t})\) and \(T\) is the length of the trajectory. In most of the situations, rewards from the environment are sparse, which means \((s_{t},a_{t})\) are mostly zero except very few states, such as the terminal state for indicating task success or failure.

Specifically, for each agent \(i\), we consider its actor model as a function \(^{i}_{_{l}}_{i} A_{i}\), where \(_{i}\) is the space of the prompts and \(_{l}\) represents the random variables involved in the sampling process. To maintain the general abilities of agents, in this paper, we select LLMs with frozen parameters such as ChatGPT and GPT-4 as actor models. Current environment states are incorporated into prompts in the form of natural language and actions are selected based on the contextual learning ability of LLMs. Meanwhile, we propose to improve collaboration through self-reflection and introduce a reflector model \(^{i}_{_{r},}(,)_ {i}\) for each agent, where \(\) denotes the space of trajectories, \(_{r}\) represents the randomness in the reflector model and \(\) denotes the learnable parameters. The reflector model takes the prior trajectory and the reward signal from the environment as input, outputting reflectionsto refine the prompt of the corresponding actor model. We fine-tune reflector models through policy optimization for better task performance in specific environments.

## 4 Method

### Multi-Agent Collaboration

LLM-based multi-agent systems aim to deliver advanced capabilities by leveraging collective intelligence and specializing LLMs into agents with distinct capabilities. In this paper, each agent in the system operates in a predetermined sequence, taking turns to produce responses, while a shared message pool is maintained to facilitate efficient communication. We illustrate the details of the collaboration settings in Appendix A.

Specifically, in dealing with problem \(k\) at time \(t\), agent \(i\) (\(i=t N\)) first subscribes the preceding interaction records \([s_{k,i},a_{k,i}]_{i=0}^{t-1}\) from the message pool, and then acquires the current environment state \(s_{k,t}\). The decision-making process can be expressed as:

\[a_{k,t}=^{i}(p^{i},[s_{k,i},a_{k,i}]_{i=0}^{t-1},s_{k,t}),\] (1)

where \(p^{i}\) is the profile of the current agent, which encompasses its role, action space, and additional constraints in the form of natural language. Once reaching a decision, the agent publishes a new message \(\{s_{k,t},a_{k,t}\}\) to the message pool.

However, during implementation, the interaction history could potentially exceed the token limit of LLMs, given the large number of agents and decision steps. To address this challenge, we introduce a context model to recursively update the interaction history from each agent's perspective, serving as its short-term memory. New messages since the agent's last action and the profile will be integrated to form a new short-term memory based on the previous one. The process can be written as:

\[sm_{k,t}^{i}=^{i}(p^{i},sm_{k,t-1}^{i},\{s_{i},a_{i}\}_{i=(0, t-N+1)}^{t}),\] (2)

where \(sm_{k,t}^{i}\) represent the short-term memory of agent \(i\) when solving problem \(k\) at time \(t\).

We then replace the interaction history with the agent's short-term memory. Therefore, the decision-making process can be further rewritten as:

\[a_{k,t}=^{i}(p^{i},sm_{k,t}^{i},s_{k,t}).\] (3)

### Multi-Agent Reflection Framework

To bolster the collaborative performance of multi-agent systems in specific scenarios, while preserving the general capabilities of agents, we introduce a self-reflection mechanism to multi-agent systems, of which the details are shown on the left part of Figure 2. Using environmental rewards as guidance, the generated reflections could act as semantic gradient signals by providing a concrete direction for improvement, thereby helping the agent learn from prior errors and perform better on the task.

Different from reflections in single-agent systems, we integrate agent profiles into the multi-agent reflection process to obtain role-specific reflections, and take a fully observable setting to assist the agent in error detection by offering interaction histories from each agent's perspective. The reflection process of the agent can be defined as:

\[y_{k,}^{i}=^{i}(p^{i},[sm_{k,,T}^{i}]_{i=1}^{N}, r_{k,}),\] (4)

where \(k\) represents the problem, \(\) indicates \(\)-th trial of answer to question \(k\), \(T\) is the length of the trajectory \(_{k,}\) and \(r_{k,}\) is the environmental rewards. Due to the iterative updating nature of the short-term memory, \(sm_{k,,T}^{i}\) contains the complete action information of agent \(i\) in trajectory \(_{k,}\).

We store all previous reflections of agent \(i\) in its long-term memory, which are then added as additional context of the actor model. The decision-making process in Equation 3 can be further defined as:

\[a_{k,,t}=^{i}(p^{i},lm_{k,}^{i},sm_{k,,t}^{i },s_{k,,t}),\] (5)

where we additionally incorporate subscript \(\) due to the introduction of the reflection mechanism.

### Optimization of the Shared Reflector

Generating useful reflective feedback with frozen LLMs in multi-agent systems proves to be challenging, since it demands a profound grasp of agent characteristics and collaborative environments. Hence, in this paper, we propose to fine-tune a shared reflector using open-source LLMs (such as Llama) with our counterfactual enhanced proximal policy optimization mechanism.

#### 4.3.1 Instruction and Response Collection

In the episode \(\) of problem \(k\), the multi-agent system first interacts with the environment to produce a trajectory \(_{k,}\), after which the reward function returns a score \(r_{k,}\). Agents in the system then reflect on the prior failed trajectory and generate verbal feedback to refine the corresponding actor prompt. In the process, Reflector\({}^{i}\) takes \(\{p^{i},[sm^{i}_{k,,T}]^{N}_{i=1},r_{k,}\}\) as the instruction \(x^{i}_{k,}\) and is prompted to produce a reflection response \(y^{i}_{k,}\). Considering the homogeneity between agent reflectors as well as the training efficiency, we gather reflection data from all agents across tasks and trials to train a shared reflector. The offline training data \(D\) can be defined as follows:

\[D=\{(x^{i}_{k,},y^{i}_{k,})|1 i N,1 ,1 k K\},\] (6)

where \(\) is the maximum trial count and \(K\) is the total number of problems.

#### 4.3.2 Counterfactual Reward

In this paper, to alleviate the credit assignment issue, we propose the counterfactual reward to achieve agent-specific reflection ratings for multi-agent collaboration. The construction of counterfactual rewards is shown on the right side of Figure 2.

Specifically, we first calculate an overall reward of the multi-agent system \(G^{i}_{k,}\) following Retroformer , i.e, \(G^{i}_{k,}=r_{k,+1}-r_{k,}\). Then, we sequentially marginalize out a piece of reflection from agent \(i\) (which means we do not add the reflection to the actor model's prompt in the subsequent trial), while keeping other agents' reflections fixed. A new reward score \(_{k,+1}\) is then returned after an interaction trajectory, based on which we calculate a marginal reward \(^{i}_{k,}=_{k,+1}-r_{k,}\). Finally, the counterfactual reward of a reflection pair \((x^{i}_{k,},y^{i}_{k,})\) is calculated by subtracting the

Figure 2: The overview of our proposed COPPER. The left side illustrates the multi-agent reflection framework. The system first computes the identifier \(i\) of the agent to respond at the current time (Step 1). Then, agent \(i\) updates its memory, including reflections of previous trials and the current trialâ€™s historical interactions (Step 2), perceives the environmental state such as the question and current task scores (Step 3), and generates the action (Step 4). After several rounds of interaction, the task trajectory and the reward score are fed into reflectors along with agent profiles to generate reflections, which are then stored in long-term memories and serve as additional context for the continuous optimization of actor prompts. On the right side, we depict the construction of counterfactual rewards, which are further employed for fine-tuning the shared reflector.

marginal reward from the overall reward:

\[^{i}_{k,}=G^{i}_{k,}-^{i}_{k,}.\] (7)

Our counterfactual dataset \(D_{CF}\) can be further denoted as:

\[D_{CF}=\{(x^{i}_{k,},y^{i}_{k,},^{i}_{k,})|1 i  N,1,1 k K\}.\] (8)

#### 4.3.3 Counterfactual Proximal Policy Optimization

Following previous works that tackle Reinforcement Learning from Human Feedback (RLHF) , we adopt a similar three-step approach to fine-tune the shared reflector with counterfactual rewards.

For the first step, we take the reflections with positive scores as demonstration data and train a supervised reflector \(^{SFT}\) with Supervised Fine-Tuning (SFT), which can be written as:

\[_{SFT}()=-_{(x,y) D_{CF}}[_{k=1}^{m} _{}(y_{k}|x,y_{<k})],\] (9)

where \(x\) is the reflection prompt, and \(y\) represents the generated reflection.

For the second step, taking construction expenses into account, instead of collecting pairwise responses for each input, we train a regression model to assess prompt and reflection pairs. We optimize the reward model \(R_{CF_{}}\) with counterfactual dataset \(D_{CF}\) by minimizing the Mean Square Error (MSE) loss:

\[_{RM}()=_{(x,y,r) D_{CF}}[(R_{CF_{}}( x,y)-r)^{2}].\] (10)

For the third step, we utilize the counterfactual reward model to optimize the supervised reflector via PPO. We begin by initializing \(^{SFT}\), which is used to produce predictions \(\) for randomly chosen samples \(x\) from the entire dataset \(D_{CF}\). Subsequently, the counterfactual reward model \(R_{CF_{}}\) assigns a reward to each response. Our goal is to optimize the reflector model by maximizing the total reward, which can be accomplished by minimizing the following loss objective:

\[_{PPO}()=-_{x D_{CF}}_{y ^{RL}_{}(x)}[R_{CF_{}}(x,y)-_{}(y |x)}{^{SFT}(y|x)}].\] (11)

## 5 Experiments

### Datasets

We choose HotPotQA , GSM8K , and Checkmate in One Move  to evaluate the collaborative abilities of multi-agent systems in multi-hop question answering, mathematics and chess.

HotPotQAHotPotQA is a multi-hop question-answering dataset designed to evaluate models' complex reasoning ability. It contains 90,447 question-answer pairs that generally require multiple reasoning steps across documents to arrive at an answer.

Gsm8kGSM8K is a collection of 8.5K diverse and high-quality math word problems for grade school students. Each problem requires between 2 to 8 steps to solve, with solutions mainly involving a series of fundamental calculations with basic arithmetic operations.

Checkmate in One MoveCheckmate in One Move is a dataset from The Beyond the Imitation Game Benchmark (BIG-bench), featuring 3,500 games to assess language models' proficiency in playing chess using standard algebraic notation (SAN). When presented with a move sequence leading to a potential checkmate, the model is tasked with identifying the move that achieves checkmate.

### Baselines

We compare the following baseline models to verify the effectiveness of COPPER: 1) **CoT**. CoT suggests bridging the gap between question and answer by generating intermediate reasoning and is useful for simple questions without tool needs. We adopt CoT in math and chess environmentsfollowing  to represent the initial success rate of the system. 2) **ReAct**. This is the state-of-the-art frozen language agent architecture, which mainly relies on the reasoning and planning ability of LLMs. It serves as a baseline in HotPotQA to denote how the agent performs without using environmental feedback. 3) **Reflexion**. This is a classic framework to learn from environment signals and generate verbal feedback to improve task performance. We extend the method to multi-agent systems and respectively employ GPT-3.5 and LongChat as reflectors to reflect on multi-agent ReAct or CoT trajectories, without fine-tuning the reflectors. 4) **Retroformer**. The paper proposes an effective method for enhancing the reflective capability of agents in single-agent systems. Here, we treat the agents in a multi-agent environment as mutually independent and fine-tune the reflector of each agent following Retroformer as a baseline.

### Implementation Details

ModelWe use GPT-3.5 (model: gpt-3.5-turbo) as the frozen actor models as well as the context models of agents and fine-tune LongChat (model: longchat-7b-16k) as the shared reflector. We choose gpt-2 as the regression reward model for counterfactual PPO training.

Collaboration SettingsWe adopt a cooperative debate paradigm on GSM8K and Checkmate in One Move following , while on HotPotQA, in alignment with , we design a teacher-student paradigm to enable agents to call the retrieval tool.

Data CollectionWe randomly select 2,000 tasks to collect reflection data on HotPotQA and Checkmate in One Move, while on the GSM8K dataset, due to the higher initial success rate and fewer reflections, we randomly select 3,000 instances. We set the maximum number of trials to 5, the temperature of GPT-3.5 to 0, and the temperature of LongChat to 0.9. We use the F1 score as the reward function of HotPotQA following  and exact match score in other environments. Comprehensive details regarding the quantity of collected datasets can be found in Appendix B.

TrainingWe use LoRA  for efficient fine-tuning of the shared reflector and implement RLHF through the trl package of HuggingFace. For SFT training, we tune the epoch in \(\{1,2,3,4\}\), batch size in \(\{64,128,256\}\), and learning rate in \(\{1e\)-4, \(2e\)-4, \(3e\)-4, \(5e\)-4\(\}\) through grid search on a validation set with 100 instances, while for counterfactual PPO, we change the search range of learning rate to \(\{1e\)-5, \(2e\)-5, \(3e\)-5, \(5e\)-5\(\}\). As for the reward model, we set learning rate to \(5e\)-5, training epoch to 3 and batch size to 16. We conduct all experiments on four NVIDIA A800-80G GPUs.

EvaluationIn alignment with constraints imposed by computational resources and following precedents set by earlier research , we randomly sample 100 instances as the test set. We set the temperature of both GPT-3.5 and LongChat to 0 during the test phase to ensure reproducibility. We measure the performance of the system in exact match accuracy during the test phase.

### Main Results

We compare the performance of COPPER against different baselines on HotPotQA, GSM8K, and Checkmate in One Move after 5 trials as main results, which are shown in Figure 3. By observing the results, we find that the results of different methods on the three datasets show roughly the same pattern: (1) Contrasted with the outcomes of multi-agent ReAct or CoT, employing the multi-agent reflection framework outlined in Section 4.2 can notably enhance the performance of multi-agent systems in specific tasks. For instance, in HotPotQA environment, the inclusion of LongChat and GPT-3.5 as reflectors leads to improvements of 15.9% and 22.7%, respectively, over the initial success rate. (2) Compared to the original LongChat and GPT-3.5, COPPER demonstrates stronger reflective abilities. The fine-tuned reflector is proficient in identifying the cause of task failure and devising personalized improvement strategies for diverse intelligent agents. Compared to the initial success rate, COPPER brought improvements of 31.8%, 18.5%, and 86.4% on the HotPotQA, GSM8K, and Checkmate in One Move datasets, respectively. (3) Compared to Retroformer, COPPER can improve the performance of multi-agent collaboration faster. We speculate that the improved performance is brought by our special designs for multi-agent settings, such as the counterfactual reward and the shared reflector.

### Ablation Study

We conduct an ablation study on three datasets to explore the effectiveness of each component of COPPER. We exclude the counterfactual reward (w/o CF) and proximal policy optimization (w/o PPO) individually and illustrate the outcome of Reflexion (LongChat) for comparison purposes (equivalent to eliminating the entire fine-tuning process). Experimental results are shown in Figure 4. From the results, we can conclude that both counterfactual reward and PPO fine-tuning are crucial for COPPER, and removing any part will lead to a decrease in performance. On the one hand, substituting counterfactual rewards with episode return difference rewards will lead to uniform rewards for all agents' reflections, meaning the contribution of reflection by each agent is equal. This could elevate the reward score for reflections that offer little assistance in enhancing collaboration performance, presenting a challenge in refining the reflector. On the other hand, fine-tuning PPO on the basis of SFT can further enhance the reflective ability of the shared reflector. This indicates that by maximizing environmental rewards, PPO can refine the model's output to better suit human preferences. For HotPotQA and GSM8K, we notice that the enhancement from COPPER during the initial two rounds is comparatively lower than solely fine-tuned with SFT. However, COPPER exhibits the highest success rate after five trials. This may be due to the fact that during the PPO training process, the reflector learns to sacrifice early performance for greater ultimate benefits.

### Generalizability of the Shared Reflector

We conduct experiments on three datasets to investigate the generalizability of COPPER, with the outcomes visualized in Figure 5. Specifically, we implement COPPER trained in multi-agent systems with GPT-3.5 actors to systems with GPT-4 (model: gpt-4-turbo) actors. We compare generalized COPPER against two baselines: one featuring GPT-4 as the reflector and the other utilizing LongChat as the reflector. We conclude that COPPER remains proficient in reflection capabilities within the systems featuring GPT-4 actors. Compared to the initial success rate, COPPER demonstrates improvements of 27.7%, 9.0%, and 53.3% in HotPotQA, GSM8K, and Checkmate in One Move respectively, and achieves comparable performance to GPT-4 reflectors after 5 trials.

### Generality of Counterfactual Rewards

To tackle the credit assignment challenge in multi-agent systems, the paper suggests deriving scores for individual agent reflections using counterfactual rewards, which is essentially a data augmentation approach. Hence, in this section, we delve into the suitability of counterfactual rewards for LLM fine-tuning techniques beyond RLHF. Specifically, we evaluate the performance of CF SFT (employing

Figure 4: Ablation study.

Figure 3: Performance of COPPER against baselines on three datasets.

counterfactual rewards to screen positive examples) against that of typical SFT fine-tuning (utilizing episode difference rewards to filter positive examples), as illustrated in Figure 6. Analysis of the results reveals that CF SFT outperforms regular SFT across all three scenarios. This underscores the effectiveness of counterfactual rewards on offering a more objective score based on model reflection contributions, thereby ensuring the selection of positive examples of higher quality.

### Effect of the Shared Reflector

In multi-agent systems, the quantity of reflectors will increase with the number of agents. This will lead to an excessive search space of hyper-parameters, posing challenges for practical applications. Therefore, we suggest training a shared reflector that employs carefully designed prompts to enhance the training efficiency and stability, without compromising personalized reflective abilities. In this section, we explore the effectiveness of shared reflector and present results in Figure 7. During the implementation of non-shared reflectors, given the uniformity among agents, we streamline the hyper-parameter search by aligning the hyper-parameters of each reflector. Experiments indicate that shared reflector can deliver better reflection effects, possibly because it can access more training data, leading to superior training outcomes.

### Effectiveness of Agents' Profiles

In order to reduce training costs while generating personalized reflections in multi-agent systems, we propose to add agent profiles to the input of reflectors and train a shared reflector. In this section, we further verify the necessity of role information in multi-agent reflection scenarios. Specifically, we remove the agents' profiles from the input of the reflector, and the experimental results are shown

Figure 5: Apply the shared reflector trained for GPT-3.5 to GPT-4.

Figure 6: Applying counterfactual rewards to SFT.

Figure 7: Exploring the effectiveness of shared reflector.

in Figure 8. By comparing Figure 3 and Figure 8, we can observe that when using pre-trained LMs (LongChat and GPT-3.5) to reflect, the removal of the agent profile has a greater impact on the GPT-3.5 reflector. This may be due to GPT-3.5's better contextual understanding ability. Our COPPER can further improve the model's reflection ability under no-profile setting. However, the results are slightly worse than the setting with agent profiles.

### Different LLMs as Base Reflectors

In this section, we replace the base reflector from LongChat with Llama-3 (model: llama-3-8b-16k) and explore the applicability of COPPER for different base models on the GSM8K dataset. Experimental results shown in Figure 9 demonstrate that our proposed COPPER has good performance across different base models. When comparing to the initial success rate, fine-tuning Llama-3 with counterfactual PPO shows a 17.3% enhancement, surpassing the performance of the GPT-3.5 reflector after 5 trials. Additionally, we include the outcome from fine-tuning Llama-3 exclusively with SFT. From the results, we find that PPO can further improve the reflective capabilities of the shared reflector.

## 6 Limitations

While counterfactual rewards can mitigate the credit assignment issue in multi-agent collaboration, constructing such rewards with LLMs imposes additional data requirements. Though our proposal involves training a shared reflector and updating the reward model's loss function to MSE, investigating more efficient data collection approaches is still needed. Besides, in this study, we restrict long-term memory to a sliding window with a maximum capacity. We believe extending the agent's memory to more advanced structures such as vector embeddings presents a promising direction for development.

## 7 Conclusion

In this paper, we consider leveraging the self-reflection mechanism to improve multi-agent collaboration, and propose an elegant framework COPPER. Towards more efficient reflection, we train a shared reflector using the counterfactual PPO mechanism. The counterfactual reward can be evaluated according to the impact of each agent reflection on enhancing task performance. To enhance the training efficiency and stability, we gather reflection data across agents and train a shared reflector. Experiments on three datasets indicate that our COPPER exhibits superior reflective ability and effective generalization across various actor models.