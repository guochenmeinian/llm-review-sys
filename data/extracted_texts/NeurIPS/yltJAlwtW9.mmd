# Information-Theoretic Generalization Analysis for Expected Calibration Error

Futoshi Futami

Osaka University / RIKEN AIP

futami.futoshi.es@osaka-u.ac.jp

&Masahiro Fujisawa

RIKEN AIP

masahiro.fujisawa@riken.jp

Equal contribution.Corresponding author.

###### Abstract

While the expected calibration error (ECE), which employs _binning_, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, _uniform mass_ and _uniform width binning_. Our analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.

## 1 Introduction

Ensuring reliable predictions from machine learning models holds paramount importance in risk-sensitive applications such as medical diagnosis . To achieve this, it is essential not only to evaluate the accuracy of the point predictions of models but also to precisely quantify the associated uncertainty. One effective approach to accomplishing this is to leverage the concept of _calibration_. In the classification context, the calibration performance is evaluated by how well predictive probabilities of a model align with the actual frequency of true labels, and a close correspondence between them indicates that the model is well calibrated . Unfortunately, machine learning models are often not well calibrated , prompting extensive research on their calibration performance both theoretically and numerically. In this paper, we assume a binary classification problem.

To evaluate the calibration performance, we often use the _calibration error_ or _true calibration error_ (TCE) . This evaluates the disparity between the predicted probability of a model and the conditional expectation of the label given the model prediction, instead of the true label frequency. However, analytically computing the TCE is challenging, primarily due to the intractability of the conditional expectation. One way to address this issue is by using the _binning_ method . This method enables the estimation of conditional expectations by dividing the probability range \(\) into \(B\) small intervals called _bins_ and comparing the empirical mean of predictive probabilities and label frequencies within each bin, utilizing the finite test dataset denoted as \(S_{}\). The obtained estimator of the TCE is termed the (binned) _expected calibration error_ (ECE).

Given that the ECE estimates the TCE, it is crucial to theoretically explore the bias between them, termed _total bias_ in this paper, to confirm the accuracy of calibration evaluation using the ECE. Furthermore, it is vital to identify the conditions under which our training algorithm achieves a low ECE or TCE for unknown test datasets. This can be paraphrased as the importance of conducting _generalization error analysis_ under the ECE and TCE. Nevertheless, research on these aspects remainsscant. Existing studies have only shown that the ECE underestimates the TCE  and have only analyzed the bias caused by a finite sample under specific conditions such as using uniform-mass binning (UMB) [13; 12]. Consequently, there remains a significant gap in the comprehensive theoretical understanding of the biases introduced by binning (termed _biming bias_) and the _statistical bias_ resulting from the use of finite test data samples. While these studies have concentrated on scenarios utilizing UMB, there has been no corresponding analysis for uniform-width binning (UWB), which is also frequently employed in practice. This limitation could be due to the challenges posed by UWB, where the equal partitioning of probability intervals can lead to bins without any samples, making bias analysis difficult. Unfortunately, to the best of our knowledge, there are also no existing generalization analyses on the basis of the ECE and TCE.

To address the challenges outlined above, in this paper, we comprehensively analyze the ECE for both UWB and UMB. We derive the upper bounds of the total bias of the ECE using a newly derived concentration inequality (Corollary 1). Our bounds improve the order of convergence regarding the bin size. Furthermore, the optimal bin size that minimizes the total bias is successfully derived from these results. With this optimal bin size, the total bias of the ECE exhibits a rate of \((1/n_{}^{1/3})\) for both UWB and UMB, where \(n_{}\) is the number of test samples (Eq. (13)).

This bias analysis leads to our second novel contribution, providing the _generalization error analysis_ for the ECE and TCE (Theorems 4 and 5) using the _information-theoretic_ (IT) analysis [43; 15; 17]. Directly applying the existing IT analysis is, however, challenging because the ECE on the training dataset is no longer represented by the sum of independent and identically distributed (i.i.d.) random variables w.r.t. the trained model. We circumvent this problem by applying a novel exponential moment inequality derived in the process of our bias analysis described above. We further connect our results to classical uniform convergence theory using the metric entropy of function classes, which allows us to discuss the convergence rate of our bounds under a broader range of models (Theorem 6). Using our generalization bounds, we theoretically explore the existing conjecture [11; 24] that recalibration with the reuse of training data leads to severe overfitting. We then show that our analysis successfully characterizes such an additional bias (Theorem 7). Numerical experiments using deep learning models confirm that our bound is nonvacuous and validate our findings.

## 2 Preliminaries

For a random variable denoted in capital letters, we express its realization with corresponding lower-case letters. Let \(P_{X}\) denote the distribution of \(X\), and let \(P_{Y|X}\) represent the conditional distribution of \(Y\) given \(X\). We express the expectation of a random variable \(X\) as \(_{X}\). The symbol \(I(X;Y)\) represents the mutual information (MI) between \(X\) and \(Y\), while \(I(X;Y|Z)\) is the conditional MI (CMI) between \(X\) and \(Y\) given \(Z\). We further define \([n]=\{1,,n\}\) for \(n\).

We consider binary classification in this paper. Let \(=\) be the domain of data, where \(\) and \(=\{0,1\}\) are input and label spaces, respectively. Suppose \(\) represents an _unknown_ data distribution, and let \(S_{}\{Z_{m}\}_{m=1}^{n}\) denote the training dataset consisting of \(n\) samples drawn i.i.d. from \(\). We also define the test dataset comprising \(n_{}\) samples as \(S_{}^{n_{}}\). Let \(f_{w}:\) be a parametric probabilistic classifier that outputs a prediction of the probability \(Y=1\), and we denote its parameters as \(w^{d}\). We consider a randomized algorithm \(:^{n}\), where \(R\) is the randomness of an algorithm, independent of all other random variables. For fixed \(R=r\) and \(S_{}=s\), \((s,r)\) is a deterministic function and \(f_{(s,r)}(x)\) is the prediction at point \(x\) given \(s\) and \(r\). We evaluate the accuracy of the trained predictor \(f_{w}\) using the loss function \(l:\), where \(l((s,r),z)\) denotes the loss incurred by the prediction \(f_{w}(x)\) for label \(y\). For example, the zero-one loss is commonly used to evaluate the accuracy. Then, the training loss is given by \(_{S_{}}_{m=1}^{n}l((S_{ },R),Z_{m})\) and the test loss is given as \(L_{Z} l((S_{},R),Z)\) where \(Z\). We also define the expected version of them as \(L_{S}_{S_{},R}_{S_{}}\) and \(L_{}_{S_{},Z,R}L_{Z}\).

### Calibration error and its estimator

In this section, we introduce a calibration metric and its corresponding estimator. The most widely known metric is the _true calibration error_ (TCE) [31; 10; 12] defined as

\[(f_{w})[|[Y|f_{w}(X)]-f_{w}(X) |],\] (1)conditioned on \(W=w\). Unfortunately, evaluating the TCE directly is challenging due to the intractable calculation of \([Y|f_{w}(X)]\). To avoid this issue, we often use the _binning_ method [48; 11; 49]. This method estimates the TCE by partitioning the prediction probability range \(\) into \(B\) intervals \(=\{I_{i}\}_{i=1}^{B}\) (called _bins_) and averaging within each bin using the evaluation dataset \(S_{e}:=\{Z_{m}\}_{m=1}^{n_{e}}^{n_{e}}\), where we assume \(n_{e} 2B\). For instance, we have \(S_{e}=S_{}\) when the test dataset is used for evaluation. The TCE estimator on the basis of \(\), with \(S_{e}\), is defined as

\[(f_{w},S_{e})_{i=1}^{B}p_{i}|_{i,S_{e}}-_{i,S_{e}}|,\] (2)

where \(|I_{i}|:=_{m=1}^{n_{e}}_{f_{w}(x_{m}) I_{i}}\), \(p_{i}|}{n_{e}}\), \(_{i,S_{e}}|}_{m=1}^{n_{e}}_{f_ {w}(x_{m}) I_{i}}f_{w}(x_{m})\), and \(_{i,S_{e}}|}_{m=1}^{n_{e}}_{f_ {w}(x_{m}) I_{i}}y_{m}\). This estimator is called the _expected calibration error_ (ECE) 3.

There are two common methods to construct \(\). One is _uniform width binning_ (UWB) , which divides the \(\) interval into equal widths as follows: \(I_{i}=((i-1)/B,i/B]\) for \(i\) in \([B]\). The other approach is _uniform mass binning_ (UMB) , which sets \(\) so that each bin contains an equal number of samples. That is, we calculate predicted probabilities as \(f_{m}=f_{w}(x_{m})\) for \(x_{m} S_{e}\), let \(f_{(m)}\) be the \(m\)-th order statistics of \((f_{1},,f_{n_{e}})\), and then set \(I_{1}=(0,u_{1}],I_{2}=(u_{1},u_{2}],,I_{B}=(u_{B-1},u_{B}]\) for \(b[B-1]\) and \(u_{b} f_{( n_{e}b/B)}\) with \(u_{B}=1\). Here, \( x\{m:m x\}\).

### Biases of ECE and limitation of existing work

Given that the ECE is an estimator of the TCE, it is of practical importance to understand the nature of the bias defined as \(|(f_{w})-(f_{w},S_{e})|\). We call this bias as the _total bias_. To facilitate the total bias analysis, we adopt the following definition of the binned function of \(f_{w}\):

\[f_{}(x)_{i=1}^{B}[f_{w}(X)|f_{w}(X) I_{i }]_{f_{w}(x) I_{1}},\] (3)

which represents the conditional expectation within each bin. When we evaluate the ECE on \(S_{e}=S_{}\), we expect that \((f_{w},S_{})\) will converge to \((f_{})=|[Y|f_{}(x)]-f_{ }(x)|\) with increasing \(n_{}\). However, \((f_{})\) underestimates \((f_{w})\)[24; 10], that is,

\[(f_{})(f_{w}),\] (4)

which implies that \((f_{w})\) is a biased estimator of \((f_{w})\). Therefore, comprehending the extent of this bias is crucial to an accurate calibration performance evaluation. Nevertheless, previous studies [24; 13; 12] have exclusively focused on the _statistical bias_ in UMB, defined as \(|(f_{})-(f_{w},S_{})|\) as discussed in Section 1. This brings us to an analysis of the total bias for both UWB and UMB.

### Information-theoretic generalization error analysis

We now briefly outline the IT analysis using the evaluated CMI (eCMI)  that we utilize in our study. Consider \(^{n 2}\) as an \(n 2\) matrix, where each entry is drawn i.i.d. from \(\). We refer to this matrix as a _supersample_. Each column of \(\) has indexes \(\{0,1\}\) associated with \(U=(U_{1},,U_{n})(\{0,1\}^{n})\) independent of \(\). We denote the \(m\)-th row as \(_{m}\) with entries \((_{m,0},_{m,1})\). In this setting, we consider \(_{U}(_{m,U_{n}})_{m=1}^{n}\) as the training dataset and \(_{U}(_{m,_{m}})_{m=1}^{m}\) as the test dataset with \(n_{}=n\), where \(_{m}=1-U_{m}\). With these notations, we can see that \(_{}_{m=1}^{n}l((_{ U},R),_{m,U_{m}})\) corresponds to the training error since \(L_{S}=_{,R,U}_{}\). Also, \(L_{}_{m=1}^{n}l((_{U},R), _{m,_{m}})\) corresponds to the test error, \(L_{}=_{,R,U}L_{}\). The described settings, called the _CMI setting_, lead to the following theorem.

**Theorem 1** (Theorem 6.7 in Steinke & Zakynthinou ).: _Under the CMI setting, we have_

\[_{,R,U}|_{}-L_{}|((l)+ 2)},\] (5)_where \((l) I(l((_{U},R),);U|)\) and \(l((_{U},R),)\) is an \(n 2\) loss matrix obtained by applying \(l((_{U},R),)\) elementwise to \(\)._

The reason we focus on IT analysis is that it enables algorithm-dependent analysis. The conventional uniform convergence theory  focuses solely on function classes to derive bounds. However, recent findings suggest that models trained by some algorithms are not well calibrated but show high accuracy [11; 24]. Therefore, it seems essential to incorporate information about not only the function class but also the algorithm in the ECE analysis. Hence, in this paper, we adopt the eCMI framework, which is the generalized analysis approach that maximizes the use of algorithmic information. Furthermore, because eCMI-based bounds can be estimated using training and test data, the generalization performance of the model can be evaluated numerically, making it desirable from a practical standpoint.

## 3 Proposed analysis of total bias in binned ECE

Here, we present our first main analyses of the bias analysis of the ECE as the estimator of the TCE. Our analysis primarily focuses on the total bias defined as follows:

\[_{}(f_{w},S_{})|(f_{ w})-(f_{w},S_{})|.\] (6)

We can derive the following upper bound of Eq. (6) by using the triangle inequality,

\[_{}(f_{w},S_{})_{ }(f_{w},f_{})+_{}(f_{w},S_{ }),\] (7)

where \(_{}(f_{w},f_{})|(f_ {w})-(f_{})|\) and \(_{}(f_{w},S_{})|(f_ {})-(f_{w},S_{})|\). We call the former as the _binning bias_, which arises from nonparametric estimation via binning, and the latter as the _statistical bias_ caused by estimation on finite data points.

Before showing our results, we introduce the following assumption that is also used by Gupta & Ramdas  and Sun et al. :

**Assumption 1**.: _Given \(W=w\), \(f_{w}(x)\) is absolutely continuous w.r.t. the Lebesgue measure._

This assumption means that \(f_{w}(x)\) has a probability density, and it is satisfied without loss of generality as elaborated in Appendix C in Gupta & Ramdas .

From Eq. (7), we can obtain an upper bound on the total bias by analyzing the binning and statistical biases separately. First, we present the following results of our statistical bias analysis:

**Theorem 2** (Statistical bias analysis).: _Given \(W=w\), under Assumption 1, we have_

\[(f_{})_{S_{}} (f_{w},S_{}),\] (8) \[_{S_{}}_{}(f_{w},S _{})}}}&,\\ }-B}}&.\] (9)

Proof sketch.: First, we reformulate the ECE as \((f_{w},S_{})=_{i=1}^{B}|_{(X,Y) _{}}(Y-f_{w}(X))_{f_{w}(X) I_{i}}|\) and the TCE as \((f_{})=_{i=1}^{B}|_{(X,Y)} (Y-f_{w}(X))_{f_{w}(X) I_{i}}|\), where \(_{}\) is the empirical distribution of \(S_{}\). The proof of this reformulation is shown in Appendix C.1. Thanks to these transformations, our analysis does not have the problem that the UWB method can lead to bins without any samples. By evaluating the exponential moment for UWB using McDiarmid's inequality under these reformulation, we have, for any \( 0\),

\[_{S_{}}_{}(f_{w},S_{ })_{S_{}}e^{|(f_{ })-(f_{w},S_{})|} B 2+^{2}/(2n_{ }).\] (10)

Using this, we can derive both the bias and the high probability bound. We can derive a similar bound for UMB. The complete proof is provided in Appendix D.1. 

Eq. (8) shows that \((f_{w},S_{})\) overestimates \((f_{})\) in expectation. Combined with Eq. (4), we can see that \((f_{W},S_{})\) cannot be the upper or lower bound of \((f_{w})\) in expectation. This emphasizes the importance of the rigorous bias analysis of \(|(f_{w})-(f_{w},S_{})|\).

**Comparison with existing work:** Eq. (9) provides better generality and a tighter bound than prior results. Our bound exhibits \((}})\) in expectation (and \(_{p}(}})\) in high probability w.r.t. \(S_{}\) proved in Appendix D.3.). In contrast, the existing analysis  provided a similar bound focused on UMB scale as \((B/}})\) in expectation (and \(_{p}(}})\) in high probability). In terms of generality, our derivation techniques can be applied to both UMB and UWB, whereas existing bounds are limited to UMB.

**Pros of our proof technique:** The proof procedure in existing work  involves (i) showing that the samples assigned to each bin are i.i.d., (ii) applying the Hoeffding inequality to derive concentration bounds _separately for each bin_, and (iii) summing up these error bounds across all bins. This approach results in slow convergence and only applicable to UMB. On the other hand, our approach simultaneously handles all bins by utilizing the concentration inequality in Eq. (10) and provides the improved upper bound and can be used for both UWB and UWB. We offer a more detailed explanation of this in Appendix D.6.

Next, we show the results of our binning bias analysis under the following common assumption in the nonparametric estimation context .

**Assumption 2**.: _Given \(W=w\), \([Y|f_{w}(x)]\) satisfies \(L\)-Lipschitz continuity._

**Theorem 3** (Binning bias analysis).: _Given \(W=w\), under Assumptions 1 and 2, we have_

\[_{S_{}}_{}(f_{w},f_{ })&,\\ (1+L)(+}-B}}+}-B})&.\] (11)

Proof sketch.: In Appendix D.4, we show that

\[_{}(f_{w},f_{})|[ Y|f_{w}(X)]-[Y|f_{}(X)]|+|f_{w}(X)-f_{}( X)|.\]

We then derive the upper bound of the right-hand side from the definitions of bins in Section 2.1. For UWB, the upper bound is \((1/B)\) because UWB divides the interval into equal widths. For UMB, we need to evaluate how samples are split by bins. The complete proof is in Appendix D.4. 

Substituting the results from Theorems 2 and 3 into Eq. (7) yields the following upper bound for the total bias.

**Corollary 1**.: _Given \(W=w\), under Assumptions 1 and 2, we have_

\[_{S_{}}_{}(f_{w},S_{ })+}}}&,\\ +(2+L)(}-B}}+}-B})&.\] (12)

The above result evidently implies a trade-off concerning \(B\). Intuitively, this indicates that while a larger number of bins, \(B\), improves the precision of \(f_{w}\) estimation, accurately estimating the conditional expectation requires a greater sample size. We further determine the optimal number of bins by minimizing the upper bound of Eq. (12) w.r.t. \(B\), which results in \(B=(n_{}^{1/3})\) and gives

\[_{S_{}}_{}(f_{W},S_{ })=(1/n_{}^{1/3}).\] (13)

Since the bin size has been tuned heuristically in practice, this result sheds light on how to choose it theoretically for both UWB and UWB rigorously.

**Regarding tightness of Eq. (12):** As we mentioned in Section 2.1, we use binning methods to estimate intractable \([Y|f_{w}(x)]\) in the TCE evaluation. Thus, the TCE evaluation can be viewed as nonparametric estimation of a one-dimensional function on \(\). According to Tsybakov , the error in such nonparametric regression _cannot be smaller than \((1/n_{}^{1/3})\)_ under Assumption 2. Our bound is convincing because its order aligns with that in Tsybakov . We provide a detailed discussion in Appendix F.6 and F.7.

We finally remark that the total bias of binning using UWB and UWB cannot be improved even assuming the Holder continuity for \([Y|f_{w}(x)]\) instead of Assumption 2. This is because the binning bias includes the error term \(|f_{w}(X)-f_{}(X)|=(1/B)\) even under the Holder continuity. Thus, we suffer from the slow converge \(_{S_{}}_{}(f_{w},S_{ })=(1/n_{}^{1/3})\) under the optimal bin size\(B=(n_{}^{1/3})\) (see Appendix D.5 for this proof). According to Tsybakov , the lower bound of the nonparametric estimation is \((1/n_{}^{/(2+1)})\) under \(\)-Holder continuity. This implies that the binning method cannot leverage the underlying smoothness of the data distribution. Thus, the slow convergence is the fundamental limitation of the binning scheme for both UMB and UWB.

## 4 Generalization error analysis in calibration error

Another goal of our study is to identify the conditions under which a training algorithm achieves a low ECE or TCE on unknown data by analyzing the generalization error, which has been overlooked in previous studies. In this section, we present our theoretical analysis regarding these points.

### Information-theoretic analysis of generalization error in ECE and TCE

The expected generalization error between the ECE and TCE can be defined through the total bias notion, that is, \(_{R,S_{}}_{}(f_{W},S_{})_{R,S_{}}[(f_{W})-(f_{W},S_{})]\). In this section, we derive the upper bound of \(_{R,S_{}}_{}(f_{W},S_{})\) by analyzing the statistical and binning biases in the same manner as in Section 3. First, we derive the following upper bound of the statistical bias, \(_{}(f_{w},S_{})|(f_ {L})-(f_{w},S_{})|\), using a similar proof technique as in Theorem 2.

**Theorem 4** (Generalization error bound of the ECE).: _Under the CMI setting and under Assumption 1, for both UWB and UMB, we have_

\[_{R,S_{}}_{}(f_{W},S_{})_{R,S_{},S_{}}|(f_{W},S_{ })-(f_{W},S_{})|()+B 2)}{n}},\] (14)

_where \(()=I(;U|)\) and_

\[(U,R,)|(f_{(_{U},R) },_{})-(f_{(_{U},R)}, {Z}_{U})|.\] (15)

Proof sketch.: We reformulate the ECE similarly to the proof outline in Theorem 2. Errors between the losses evaluated on the training and test data are similar to the left-hand side of Eq. (5); however, directly applying Eq. (5) leads to a suboptimal rate of \((B/)\). Therefore, we derive the extended version of Eq. (5) by correlating \(B\) bins according to Eq. (10) and combining this with the Donsker-Varadhan lemma. The complete proof can be found in Appendix E.1. 

Comparing Eq. (14) with Eq. (9) in Theorem 2, we find that \(\) measures the _additional bias_ that arises when evaluating the ECE using training data that are dependent on the trained model \(f_{w}\) instead of test data, which are independent of it. In other words, the term \(_{R,S_{},S_{}}|(f_{W},S_{})-(f_{W},S_{})|\) can be regarded as the expected generalization error of the ECE, and \(\) is the dominant term of the generalization gap. Therefore, if the trained model has a sufficiently low \(\), it achieves good generalization performance in terms of the ECE. The behavior of \(\) clearly affects the convergence rate of this bound, which is discussed in Section 4.2. Moreover, our bound and eCMI are numerically evaluable and we confirm that our bound is numerically nonvacuous (see Section 6). We also show the application of our bound in the setting of recalibration in Section 4.3.

In Appendix E, we derive the binning bias under the training data similar to Theorem 3. By combining this result with Theorem 4, we obtain the following generalization error bound for the TCE.

**Theorem 5** (Generalization error bound of the TCE).: _Under the CMI setting and under Assumptions 1 and 2, we have_

\[_{R,S_{}}_{}(f_{W},S_{})\!\!+()+B  2)}{n}}&,\\ +()+B 2)}{n}}+(1+L) +B 2)}{n}}&.\] (16)

_In the above, \(()\) is defined as Eq. (15) and_

\[ I(f_{(_{U},R)}();U|),\] (17)

_where \(\) denotes the \(n 2\) matrix obtained by projecting each element of \(\), and \(f_{(_{U},R)}()\) is the \(n 2\) matrix calculated by the elementwise application of \(f_{(_{U},R)}()\) to \(\)._When comparing Eq. (12) with the above results, it is observed that an additional bias, \(\) (including \(\) in UMB), derived from training data arises. This implies that the trained model shows a low TCE when it sufficiently reduces these additional biases and achieves a small ECE. From a practical viewpoint, this implies that our bound can potentially be used as a theoretical guarantee for some recent training algorithms, which directly control the ECE under the training dataset [23; 29; 38]. Our theory might guarantee the ECE under test dataset for them.

Another interesting implication from our bounds is that we can derive the optimal bin size to minimize the upper bound in Theorem 5. If \(()\) and \(\) are sufficiently small compared with \(n\), for example, \(( n)\) (we discuss this in Section 4.2), then, the optimal bin size can be derived as \(B=(n^{1/3})\) by minimizing Eq. (16) w.r.t. \(B\). Such an optimal \(B\) leads to

\[_{R,S_{}}_{}(f_{w},S_{})=( n/n^{1/3}).\] (18)

According to Eq. (13) and the above result, we can anticipate that \(_{R,S_{}}_{}(f_{w},S_{})\) is much smaller than \(_{S_{}}_{}(f_{W},S_{})\) because the number of training data is often much larger than that of test data \((n n_{})\). This implies that if the model generalizes well, evaluating the ECE using the training dataset may better reduce the total bias than that using test dataset. Although proposing such a new TCE estimation method is beyond the scope of this paper, this represents an important direction for future research.

### On the behavior of \(\) and the order of total bias on metric entropy

In this section, we analyze how additional biases, i.e., \(()\) and \(\), behave. The initial observation is that the following relation holds : \(() I(W;S)\). Furthermore, we can see that \(I(W;S)=( n)\) under certain constrained conditions, such as the conditionally i.i.d. setting when \(f_{w}(x)\) is the underlying probability model for \(p(y|x;w)\) with \(\) being compact and holding appropriate smoothness assumptions [14; 44]. Furthermore, \(\) can be upper bounded when the algorithms satisfy the various notions of stability . For example, differential private algorithms and stochastic gradient Langevin dynamics (SGLD)  algorithms are included in this argument. A more detailed discussion can be found in Appendix F.4.

These arguments, however, hold true only for specific models and algorithms. Therefore, we extend Theorem 5 by utilizing the concept of _metric entropy_ to overcome this issue.

**Theorem 6** (Metric entropy).: _Let \(\) be the function class \(f_{w}\) belongs to. Suppose that \(\) with the metric \(\|\|_{}\) has the metric entropy, \((,\|\|_{},)\), with the parameter \(\)\((>0)\). That is, there exists a set of functions \(_{}\{f_{1},,f_{N(,\|\|_{},)}\}\) that consists of \(\)-cover of \(\). Then, under the CMI setting and under Assumptions 1 and 2, for any \((0,1/B]\) and for UWB, we have_

\[_{R,S_{}}_{}(f_{W},S_{})+(2+L)+(,\|\|_{ },/B)}{n}}.\] (19)

See Appendix E.3 for the proof. Theorem 6 connects the IT-based bound to the uniform convergence theory. With this result, we can discuss the optimal number of bins across a broad spectrum of models. For example, we can obtain \((,\|\|_{},)(}{ })^{d}\) when \(f_{w}\) is a \(d\)-dimensional parametric function that is \(L_{0}\)-Lipschitz continuous (\(L_{0}>0\)) , leading to the following upper bound:

\[_{R,S_{}}_{}(f_{W},S_{})+B^{2})}{n}},\] (20)

where we set \(=(1/B)\). This bound is minimized when \(B=(n^{1/3})\), resulting in a bias of \(( n/n^{1/3})\), which is consitent with Eq. (18).

The drawback of the above bound is that it depends on the model's dimensionality explicitly, making them unsuitable for large models such as neural networks. To address this issue, we can use the different combinatorial properties, such as the fat-shattering dimension. See Appendix E.3 for the detail.

### Generalized error analysis on recalibration and bias due to reuse of training data

As an application of our generalization error bound, we analyze recalibration using a post-hoc recalibration function, which is used when the trained model is not well calibrated. We focus on the recalibration using UMB with recalibration data [35; 12]. In this setting, we first split overall data into the training, recalibration, and test datasets (see Appendix B for details of this splitting strategy). After training \(f_{w}\) using the training dataset, we construct the recalibrated function \(h\) using the recalibration dataset \(S_{}\) as

\[h_{,S_{}}(x)_{i=1}^{B}_{i,S_{}}_{f_{w}(x) I_{i}},\] (21)

where \(_{i,S_{}}\) is the empirical mean of \(\{y_{m}\}_{m=1}^{n_{}} S_{}\) in the \(i\)-th bin defined as in Eq. (2) and \(n_{}\) is the number of the recalibration dataset. In short, Eq. (21) provides an estimator of the conditional expectation of \(Y\) given \(f_{w}(x)\) by setting \(S_{e}=S_{}\). Gupta & Ramdas  clarified that the statistical bias of Eq. (21) is given by \(_{S_{}}(h_{,S_{}})= [|[Y|h_{,S_{}}(X)]-h_{,S _{}}(X)]|=_{p}(}|})\). Since we need to split the overall data into three datasets, this approach could be sample-inefficient and could result in a very loose bound. Although reusing the training dataset may solve this problem to some extent, it has been suggested that this method may cause performance degradation due to overfitting [24; 12].

Our contribution here is quantifying the bias caused by overfitting due to the reuse of training data by utilizing our generalization error analysis in Section 4.1 as follows.

**Theorem 7** (Recalibration reusing the training dataset).: _Replacing \(S_{}\) with \(S_{}\) in Eq. (21), under the CMI setting and under Assumptions 1 and 2, we have_

\[_{R,S_{}}(h_{,S_{}})= _{R,S_{}}[|[Y|h_{,S_{ }}(X)]-h_{,S_{}}(X)] 2+B 2)}{n}},\]

_where \(\) is defined in Eq. (17)._

The complete proof is provided in Appendix E.4. In the above, \(\) corresponds to the additional bias caused by overfitting due to the reuse of \(S_{}\). This indicates that reusing \(S_{}\) does not negatively affect the order of the bias if \(\) is smaller than other terms, as discussed in Sections 4.1 and 4.2. Since the size of \(S_{}\) is much larger than that of \(S_{}\), the recalibration function \(h_{,S_{}}\) may exhibit a much smaller bias compared to \(h_{,S_{}}\). We investigate this possibility numerically in Section 6 by using the tighter version of Theorem 7 provided in Appendix E.4 (Corollary 4).

## 5 Related work

We have presented the results of our analyses of the total bias in the ECE and the generalization error for both the ECE and the TCE. Existing studies have primarily focused on the statistical bias, with little attention given to the binning bias. Gupta et al.  and Gupta & Ramdas  examined the statistical bias associated with UMB, but they did not address the binning bias as we did. In contrast, Kumar et al.  studied the binning bias but did not specify how this bias depends on \(n\) and \(B\). Moreover, most analyses have concentrated on UMB and UWB has not been thoroughly analyzed. As outlined in the proof of Theorem 2, our approach allows us to analyze UWB even in cases where some bins do not have any data points by employing our reformulation and concentration inequality. It is important to note that Roelofs et al.  studied the numerical behavior of the total bias in some practical models, whereas we focus on the theoretical aspect of the total bias. Recently, Sun et al.  have derived the optimal number of bins under the recalibration with UMB. Compared with this, we derived the optimal number of bins for UMB and UWB without recalibration under a similar Lipschitz assumption. This leads to the discussion of estimating the TCE from the nonparametric estimation. An additional discussion is summarized in Appendix F.

We have extended the existing eCMI bound [34; 15; 17; 40], which is used for analyzing generalization performance in terms of prediction accuracy, to calibration analysis. In addition, whereas existing eCMI bounds numerically evaluated \(\) and \(\) for _discrete_ random variables such as zero-one loss, our analysis is conducted on _continuous_ random variables as shown in Eq. (15). We show in the next section that our bounds are still nonvacuous even for the continuous random variables. The IT analysis was also utilized by Russo & Zou  to study the bias caused by data reuse. Our analysis can be seen as an extension of this approach to the ECE and recalibration.

## 6 Experiments

In this section, we present experimental results validating our bounds (Section 6.1) and the additional bias arising from reusing the training dataset for recalibration (Section 6.2).

### Verification of our bounds

In this section, we empirically validate our theoretical findings in Eq. (12), the nonvacuous nature of our bounds in Eq. (14), and confirm the efficiency of the optimal number of bins as discussed in Section 4.1.

Experiments on synthetic datasets:We first conducted simple experiments on synthetic datasets following Zhang et al. . In this experiment, we assume the distribution of \(Y\) as \(P(Y=1)=P(Y=0)=\), and we adopt \(f_{w}(x)=P(Y=1|X=x)=1/(1+(-_{0}-_{1}x))\) as the prediction model, where \(w=\{_{0},_{1}\}\) are parameters. Under these settings, we can calculate the closed-form of \([Y|f_{w}(X)]\) in Eq. (1), which allows us to estimate the TCE through Monte Carlo integration. Next, we empirically evaluated the _TCE gap_, which is the empirical estimator of \(_{S_{}}}}(f_{w},S_{})\), by calculating the difference between the TCE estimator and the ECE using UWB. Here, the optimal \(B\) that minimizes the upper bound of Eq. (14) is \(B= 2n(1+L)^{2}/( 2)^{1/3}\), where \(L\) is estimated to be the maximum value of the gradient of the closed-form of \([Y|f_{w}(X)]\). We provide the details of the experimental settings in Appendix G.1.

Figure 1 shows the results. The two leftmost figures show that the TCE gap is \((n^{-1/3})\) when our optimal \(B\) is used. The other two figures show that the number of bins achieving the smallest upper bound is closest to the optimal \(B\) and its order is \(O(n^{-1/3})\), where the candidates for \(B\) are set as \(\{n^{-1/2},n^{-1/3},n^{-1/4},2n^{-1/2},2n^{-1/3},2n^{-1/4},3n^{-1/2},3n^{-1/3},3n^{-1/4}\}\). These observations show the validity of our theoretical findings through Corollary 1.

Experiments on image datasets:We further conducted two binary classification tasks on MNIST  using a convolutional neural network (CNN) and on CIFAR-10  using ResNet. These models were trained using SGD with momentum for ResNet, Adam for CNN, and SGLD for both, following the strategy of Hellstrom & Durisi . The details of our experimental settings are summarized in Appendix G.2. We initially evaluated the sum of the right-hand side terms of

Figure 1: Behavior of the upper bound in Eq. (12) as \(n\) increases when UWB is used. The following two terms: _less calibrate_ and _better calibrate_ refer to \(=(0.5,-1.5)\) and \(=(0.2,-1.9)\), respectively, where the former setting produces a worse value of the TCE estimator.

Figure 2: Behavior of the upper bound in Eq. (14) for various \(B\) as \(n\) increases (mean \(\) std.). For clarity, only the results using UMB are shown. The ECE gap is shown for \(B= n^{1/3}\) since the change in \(B\) did not result in significant differences. We refer to Figure 5 in Appendix H.3 for a detailed analysis of the relationship between (log-scaled) ECE gap values and bound values across different bin settings.

Eq. (14) and the ECE estimated using the training dataset, aiming to ascertain whether the disparity from the ECE estimated using the test dataset was adequately minimal. We call this disparity as the _ECE gap_, which is the estimator of \(_{R,S_{},S_{}}[|(f_{W},S_{ })-(f_{W},S_{})|]\). We show the results obtained when using UMB in Figure 2. These results show that our bound value becomes less than \(1\) with an appropriate setting of \(B\). We also observed that the bound values decrease with \(n\), whereas these values sometimes become vacuous for small \(n\) when \(B\) is large. Adjusting \(B\) could pose challenges; however, a notable trend towards acquiring relatively stable nonvacuous bounds can be observed when adopting \(B= n^{1/3}\), even though this is the optimal choice only at the upper bound of TCE, as discussed in Theorems 5 and 6 in Sections 4.1 and 4.2. Similar results are obtained when using UWB (see Figure 3 in Appendix H).

### Confirming additional bias due to reusing training dataset in recalibration

In this section, we empirically confirm the efficiency of the method when using the complete training dataset for recalibration, referred to here as the _reusing method_. Table 1 illustrates that the reusing method reduces the statistical bias of the ECE more effectively than existing methods using independently created recalibration datasets (\(n_{}=100\)). We also compared the tighter version of our bound in Theorem 7, Corollary 4, with the bound of the existing recalibration methods presented in Corollary 5 of Appendix E.5. Moreover, our bound values are lower than those for the existing recalibration methods on the test dataset. These results suggest that reusing training data could be beneficial if the trained model generalizes well and \(\) is sufficiently small, as discussed in Section 4.3.

## 7 Conclusion and limitations

We provided the first comprehensive analysis of the bias associated with the ECE when using the test and training datasets. This leads to the derivation of the optimal bin size to minimize the total bias. Numerical experiments show that our upper bound of the bias is nonvacuous for deep learning models thanks to the IT generalization error analysis. Despite rigorous analysis, our analysis still has limitations. Firstly, we focus on the binary classification; thus, the extension of our analysis to the multiclass classification setting is an important future direction. However, the application of our analytical techniques to this setting seems not clear. Additionally, our analysis cannot be applied to the higher-order TCE, in which we use the \(p\)-th norm in Eq. (1). These limitations should be addressed in future work to develop a more principled understanding of uncertainty.

 
**Dataset** & **Optimizer** & **Methods** & **TCE** & **Bound value** \\    &  & Recallib. & \(.0085.0016\) & \(.8475\) \\  & & Our recalib. & \(.\) & \(.\) \\   & &  Recallib. \\ Our recalib. \\  & \(.0101.0025\) & \(.8475\) \\   &  & Recallib. & \(\) & \(1.455\) \\  & & Our recalib. & \(.0197.0044\) & \(\) \\    & &  Recallib. \\ Our recalib. \\  & \(.0109.0012\) & \(.1455\) \\    & & 
 Recallib. \\ Our recalib. \\  & \(.\) & \(\) \\  

Table 1: Comparison of our method with existing recalibration in terms of the ECE gap and its upper bound in Theorem 7 (mean \(\) std.). Lower values are better. We adopted \(B= n^{1/3}\). The bound values for the existing recalibration method originate from Corollary 4 in Appendix E.5. Here, we report the ECE gap as **TCE** because \(_{R,S_{}}[|[Y|h_{T,S_{}}(X )]-h_{T,S_{}}(X)]=_{R,S_{},S_{}} (h_{T,S_{}},S_{})\) (existing recalibration methods) (our recalibration method) from the definition of recalibration.