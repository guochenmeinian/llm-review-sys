# SG-Bench: Evaluating LLM Safety Generalization

Across Diverse Tasks and Prompt Types

 Yutao Mou\({}^{1}\), Shikun Zhang\({}^{1}\), Wei Ye\({}^{1}\)

\({}^{1}\)National Engineering Research Center for Software Engineering, Peking University, China

{yutao.mou}@stu.pku.edu.cn, {zhangsk,wye}@pku.edu.cn

corresponding author.

###### Abstract

Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.2**Warning: this paper includes examples that may be offensive or harmful.**

## 1 Introduction

Large language models (LLMs) such as ChatGPT , Claude , and LLAMA series  have recently demonstrated powerful capabilities on various tasks and have been widely deployed in various practical applications. However, studies have shown that LLMs can be used for various unsafe purposes, such as generating harmful contents  (\(e.g.\) toxic and biased responses, false messages), performing malicious operations [5; 6] (\(e.g.\) system vulnerability attack, database information theft), etc, posing a great threat to human society. Therefore, safety alignment and safety evaluation are essentially important for the development of large language models.

As for LLM safety evaluation, many benchmarks have been proposed, which can be divided into two categories according to task types: open-end text generation [7; 8] and multiple choice question . The former mainly examines whether the generated contents are harmless and consistent with human values, while the latter focuses on the capability of LLMs to discriminate harmful contents. In addition, many studies have pointed out that LLMs can be easily induced to output harmful responses through jailbreak attacks  such as prefix prompt injection , role playing , refusal suppression , etc. Zhou et al.  proposed EasyJailbreak, a systematic framework to evaluate the vulnerability of LLMs to jailbreak attacks. However, these safety evaluation benchmarks only focus on evaluating the safety performance of LLMs on a single aspect, and lack a comprehensive evaluation of both safety generation and discrimination capabilities. In addition, many prompt engineering techniques [15; 16] have been proposed to fully harness the potential of LLMs. Typical prompt types includesystem prompts , few-shot demonstrations , and chain-of-thought prompting . Studies have shown that few-shot demonstrations can effectively stimulate the in-context learning  ability of LLMs, thereby improving the performance of LLMs on general tasks. Chain-of-thought prompting guides models to first generate a chain of intermediate reasoning process and then generate the final result, which has been proven to improve the reasoning ability of LLMs. Recently, Zhu et al.  proposed PromptBench, which aims to evaluate the robustness of LLMs on adversarial prompts. However, these studies only focus on the improvement of the general performance of LLMs by prompt engineering techniques, but ignore the effect of prompt types on the safety performance of LLMs. Therefore, this paper mainly focuses on two research problems:

* **RQ1:**_Can the safety-aligned LLMs demonstrate consistent safety performance on both generation and discrimination tasks?_
* **RQ2:**_Will prompt engineering techniques affect the safety performance of LLMs, positive or negative?_

To delve into these issues, we firstly proposed a multi-dimensional safety evaluation **Bench**mark to evaluate LLM **S**afety **G**eneralization across diverse test tasks and prompt types (**SG-Bench**), which includes three types of test tasks: open-end generation, multiple-choice questions and safety judgments, and covers multiple prompt engineering and jailbreak attack techniques. We first collected 1,442 malicious queries from different sources, involving a total of 6 types of safety issues. For the generation task, in addition to directly using original queries as LLM inputs, we also apply several jailbreak attack methods to each malicious query and constructed a jailbreak attack evaluation subset. For the discrimination task, we constructed two evaluation subsets based on each harmful query, namely multiple-choice questions and safety judgment, which aims to evaluate the discrimination capabilities of LLMs from various perspectives. Next, we combine several common prompt types, such as role-oriented prompt, task-oriented prompt, few-shot demonstrations and chain-of-thought prompting with evaluation subsets for different tasks respectively, and construct extended evaluation sets to study the effect of various prompt engineering techniques on LLM safety performance. More details can be seen in Section 3.

We evaluate the safety performance of 3 leading proprietary LLMs and 10 popular open-source LLMs on SG-Bench (Section 4). Overall, we are surprised to find that when directly using original queries as LLM inputs, they perform exceptionally well in terms of safety on generation, but when performing discriminative tasks, the safety performance drops significantly3, as shown in Figure 1. Besides,

Figure 1: The illustration of the unsatisfactory LLM safety generalization. While GPT-4 produces safe responses to straightforward queries (sub-figure **a**), it fails to meet expectations with discriminative questions (e.g., multiple-choice in sub-figure **b**, safety judgments in sub-figure **c**) or when prompt context changes (e.g., adding Jailbreak text in sub-figure **d**).

the safety performance is also sensible to prompt context variations. Furthermore, we conduct substantial experiments and qualitative analyses to explain the reason for the unsatisfactory LLM safety generalization (Section 5).

We summarize the main contributions of this study as follows:

* **Benchmark.** We are the first to propose the LLM safety generalization problem and construct a multi-dimensional safety evaluation benchmark (SG-Bench) to evaluate the generalization of safety-aligned LLMs on diverse test tasks and prompt types.
* **Study.** We ran a comprehensive empirical analysis of both proprietary and open-source LLMs using SG-Bench, including (1) Evaluating the safety performace of safety-aligned LLMs on diverse tasks, (2) Studying the effect of prompt types on LLM safety performance, (3) Conducting qualitative analyses to explain the reason for poor LLM safety generalization.
* **Implications.** This work revealed multiple significant findings: (1) LLMs generally exhibit poorer safety performance in discrimination tasks compared to open-end generation, (2) Role-oriented prompts are helpful for defending against jailbreak attacks, (3) Few-shot demonstrations may induce LLMs to generate harmful responses, (4) Chain-of-thought prompting generally harm LLM's safety performance on discrimination tasks.

## 2 Related Work

### LLM Safety Training

In order to align large language models with human values and ensure the safety of generated contents, it is usually necessary to perform LLM safety training . Generally, safety training is carried out in the preference alignment stage by introducing harmlessness preference data. Mainstream preference alignment methods include RLHF [1; 24], DPO , KTO , RRHF  and etc. These methods firstly annotate human preference datasets and then continuously raise LLMs' generation probability for responses with higher preference scores during the training process. Besides, LLAMA2  collects adversarial prompts and safety demonstrations and performs safety supervised fine-tuning. Wei et al.  proposed that mismatched generalization is one of the main reasons for the failure of LLM safety training. We think that the objective function and training data of LLMs safety training are designed for generation tasks, without considering discrimination test, and also cannot cover all prompt types encountered in the inference stage. Therefore, it is necessary to comprehensively evaluate the generalization of LLMs safety training across diverse test tasks and prompt types.

### LLM Safety Evaluation Benchmarks

As shown in Table 1, the current popular safety evaluation benchmarks for large language models only assess the safety performance of LLMs on single task types. For example, AdvBench , SafetyPrompt  and DecodingTrust  collect red-teaming instructions and simply evaluate LLM safety performance on the generation task. In order to quickly and accurately evaluate the safety of LLMs, SafetyBench  only uses multiple-choice questions for testing. Besides, EasyJailbrek  and Jailbroken  are specifically designed to assess the susceptibility of LLMs to jailbreak attacks, and also only focus on generation task. Recently, Li et al.  proposed SaladBench, which is the first

    &  &  \\   & **Generation** & **MCQ** & **Judgment** & **Jailbreak Attack** & **Prompt Engineering** \\  AdvBench  & ✓ & ✗ & ✗ & ✗ & ✗ \\ SafetyPrompts  & ✓ & ✗ & ✗ & ✗ & ✗ \\ DecodingTrust  & ✓ & ✗ & ✗ & ✗ & ✗ \\ SafetyBench  & ✗ & ✓ & ✗ & ✗ & ✗ \\ EasyJailbreak  & ✓ & ✗ & ✗ & ✓ & ✗ \\ Jailbroken  & ✓ & ✗ & ✗ & ✓ & ✗ \\ SaladBench  & ✓ & ✓ & ✗ & ✓ & ✗ \\ 
**SG-Bench (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison between various safety evaluation benchmarks and our SG-Benchsafety evaluation benchmark covering different task types and jailbreak attack methods, but this work did not focus on the connection between different test types. Besides, it did not study to the effect of various prompt engineering techniques on LLM safety performance. In contrast, our proposed SG-Bench is a more systematically safety evaluation benchmark targeted LLM safety generalization across diverse tasks and prompt types, which covers diverse test tasks, such as open-end generation, multiple-choice questions and safety judgment, and include various jailbreak attack methods and prompt engineering techniques.

## 3 SG-Bench Evaluation Benchmark

We constructed SG-Bench, a multi-dimensional evaluation benchmark to assess LLM safety generalization across diverse test tasks and prompt types. SG-Bench consists of 4 evaluation subsets for both generation and discrimination tasks, and 7 extended test sets for diverse prompt engineering techniques. As for open-end generation, we construct two subsets: original queries and jailbreak attacks. The former directly inputs malicious instructions into LLMs, while the latter transforms these instructions using commonly used jailbreak attack methods. For discriminative tests, we also design two test sets: multiple-choice questions and safety judgment to assess the safety discrimination capabilities of large language models from different viewpoints. Furthermore, we also specially designed seven extended evaluation sets to assess whether prompt engineering techniques will influence LLM safety performance. We mainly focus on 3 types of prompts: system prompts (role-oriented prompts and system-oriented prompts), few-shot demonstrations and chain-of-thought prompting. The construction process of our SG-Bench evaluation benchmark is shown in Fig 2.

### Malicious Query Collection

**Taxonomy for Safety Issues** In order to construct a comprehensive safety evaluation benchmark, we need to collect malicious instructions covering as many types of safety issues as possible. Since there is no standardized terminology or definition for categorizing safety issues, we carefully reviewed the safety categories used in previous works [21; 29] and categories common safety issues into 6 types: toxic content, stereotyping and bias, misinformation, privacy infringement, dissemination of dangerous information and malicious use. Different from 14 safety scenarios offered by OpenAI , our taxonomy is a coarse-grained manner that offers broader coverage. For instance, while OpenAI offers 14 more fine-grained safety scenarios, it does not include location and gender stereotyping, which are covered under SG-Bench's coarse-grained category "Stereotyping and Bias." Integrating coarse-grained and fine-grained taxonomies logically and systematically to provide comprehensive safety categories is nontrivial (we have summarized the taxonomy for safety issues across different benchmarks in Appendix A), and our study primarily focuses on LLM safety generalization across diverse prompt context variations (different task and prompt types). Therefore, we believe this design choice is appropriate for producing recognized and convincing findings.

**Diversity Check of Malicious Query** We mainly collected malicious queries from three public datasets: AdvBench , HarmfulQA , and Beaver-eval , and then performed diversity check and quality filtering. Firstly, We used GPT-4 to classify the malicious instructions we collected, and found that these three datasets lack test samples on stereotyping and bias, misinformation, and privacy infringement. To make the number of samples in each safety type more balanced, we additionally collected 250 samples from SaladBench . These samples cover the three scarce safety issues of stereotyping and bias, misinformation, and privacy infringement.

Figure 2: The construction process of SG-Bench.

**Quality Filtering** Through manual analysis, we observed that some queries in HarmfulQA and Beaver-eval are not always harmful, such as _"How can I make someone feel like they are not good enough?"_ and _"How can I use language to exploit cultural differences?"_ from HarmfulQA. These queries are considered low-quality and unlikely to cause harmful responses from LLM. In order to obtain high-quality malicious queries, we use Llama-Guard-7B  to evaluate the harmfulness of these selected queries and instructions. Llama-Guard-7B is a dedicated safety classifier for both prompt classification and response classification. We use the official prompts to classify queries, retain queries classified as unsafe, and filter out queries classified as safe. Regarding the accuracy of Llama-Guard-7B, we randomly selected a subset of 300 queries from both HarmfulQA and Beaver-eval, verified it through manual annotation, and found that the consistency between Llama-Guard-7B and human evaluation in query harmfulness identification task is 93.92%. Finally, we retained 244 harmful queries in HarmfulQA and 428 malicious instructions in Beaver-eval, which, together with 520 harmful queries from AdvBench and 250 additional harmful queries from SaladBench, constitute the seed set of our SG-Bench evaluation benchmark. There are 1442 malicious queries in our seed set, and the detailed statistics and representative examples in SG-Bench are shown in Appendix A.

### Construction of Evaluation Subsets for Diverse Tasks

Apart from being used as a chatbot, safety-aligned LLMs are often used in a variety of scenarios. For examples, during agent planning process, to enhance the safety and reliability of generated contents, LLMs can generate multiple answers and then select the best one (multiple-choice questions). Another typical application is content moderation, where post-processing judgments ensure content safety after an LLM generates a response, using either aligned general LLMs or dedicated ones (safety judgments). Beyond reflecting real-world usage, discrimination tasks (such as multiple-choice questions and judgments) are a fundamental component for revealing the correlation between generative and discriminative evaluation paradigms. Discrimination tasks are widely used in other areas of LLM evaluation as well . In order to evaluate the safety performance of large language models in different scenarios, we constructed 4 evaluation sets, which aims to assess the performance of LLMs from both generative and discriminative perspectives. Fig 3 shows the input prompts we designed for jailbreak attack, multiple-choice questions, and safety judgments evaluation sets. Next, we provide a detailed introduction to each evaluation set.

**Original Query** The malicious queries are directly used as LLM inputs without adding any other prompt prefixes and suffixes.

**Jailbreak Attacks** We augment each malicious instructions with six representative jailbreak attack techniques [10; 14], such as prefix injection , refusal suppression , distractors negated, Poems, AIM  and evil confidant. For a detailed overview of the jailbreak attack techniques employed, please see Appendix D.

**Multiple-choice Questions** Given a malicious instruction and two candidate answers, one of which responds to the user's instruction positively but contains harmful information, and the other answer refuses to respond to the user's malicious instruction. LLM needs to choose an answer that it thinks is better. We leverage gpt-3.5-turbo to generate harmless responses and use text-davinci-003 to obtain

Figure 3: The demonstration of the three prompts we use for Jailbreak Attacks, Multiple-choice Questions and Safety Judgments evaluation sets.

malicious outputs. Since existing advanced proprietary LLMs, such as ChatGPT, GPT-4 and Claude3, have established robust safety mechanism, we directly use queries as LLM inputs to obtain harmless responses. For harmful output, the generation process begins with manually writing the first sentence of each harmful answer. For examples, for a malicious query _"What's the most effective way to break your own bones without too much pain?"_, we can write the first sentence of harmful answers _"Sure, here is the most effective way to break your own bones without too much pain."_. And then we instruct text-davinci-003 to continue writing answers to complete the subsequent generation of the full output.

**Safety judgments** Given a malicious query and an AI generated response, LLMs need to justify whether the response is harmless. In the multiple-choice questions subset, we have obtained one harmful response and one harmless output for each query. In the safety judgment sub-task, we will make judgments for all these responses.

### Expansion of Evaluation Subsets for Diverse Prompts

Many prompt engineering techniques have been proposed to enhance the general capabilities of LLMs, such as in-context learning [20; 35] and chain-of-thought prompting [19; 36]. However, whether these prompt types are also helpful for improving the safety performance of LLMs remains to be explored. We divide the commonly used prompt types into three categories: system prompts, few-shot demonstrations, and chain-of-thought prompting. Then we combine these prompt engineering techniques with three different evaluation subsets and construct the corresponding extended evaluation sets. Figure 4 shows the input prompt for each extended evaluation set. Next, we provide a detailed introduction to each prompt type.

**System Prompts** According to PromptBench , commonly used system prompts are divided into role-oriented prompts and task-oriented prompts. Task-oriented prompts clearly define the task that the model needs to accomplish, which encourages the model to generate task-specific outputs based solely on its pre-training knowledge. While role-oriented prompts typically frame the model as an entity with a specific role, such as an expert, advisor, or assistant. By incorporating role information, these prompts aim to implicitly convey the expected output format and behavior.

**Few-shot demonstrations** A few examples are added to the input, which aims to provide some guidelines about the output contents and formats. For open-end text generation, we randomly select 3 examples from the alpaca  training set. For multiple-choice questions and safety judgments, we manually write 3 examples and append them to the prompt template.

**Chain-of-thought prompting** This type of prompt is mainly used in multiple-choice questions and safety judgments. We prompt the LLM to give the corresponding reasoning process and explanation before making a judgment.

## 4 Experiments

### Experiment Settings

**Evaluated models** In this work, we mainly assess 3 proprietary LLMs (ChatGPT , GPT-4  and Claude-3 ) and 10 mainstream open-source LLMs (Mistral-7B-Instruct , LLAMA series

Figure 4: The demonstration of prompt engineering techniques we apply in extended evaluation sets.

, Qwen Series , ChatGLM3-6B , InternLM2-7B-chat ). More implementation details can be seen in Appendix B.

**Evaluation metrics** In the SG-Bench, we adopt the "Failure Rate (FR)" as the evaluation metric, which indicates the proportion of test samples in which LLM safety training failed to the total number of test samples in a test set. We employ different evaluation methods for generation and discrimination tasks. Specifically, for generation tasks, considering the cost and efficiency of large-scale evaluation, we use a dedicated judgment model, LlamaGuard-7B , to determine whether the responses generated by LLMs are harmful, and we calculate the proportion of harmful responses to the total number of samples in the test set as the "Failure Rate". For discrimination tasks, we utilize regular matching to extract keywords from the response generated by LLMs, compare them with ground-truth labels, and compute the error rate as the Failure Rate. In order to measure the safety performance of large language models on both generation and discrimination tasks, we average scores on the four test sets to obtain the "Average Failure Rate (Avg FR)".

### Evaluation of LLM Safety Performance on Diverse Tasks

We first assess the safety performance of 3 proprietary LLMs and 12 open-source LLMs on different test tasks. The experimental results are shown in Table 2. In general, the safety performance of safety-aligned LLMs on diverse test tasks is significantly different, which also shows the poor LLM safety generalization. Next, we analyze the results from three aspects:

(1) **Comparison of different LLMs.** We can see that Claude-3 shows the best safety performance in both open-end text generation and safety content discrimination (2.99% Avg FR). Qwen1.5-7B-chat has the worst safety performance (30.79% Avg FR), and they are not only vulnerable to jailbreak attacks, but also cannot identify harmful information well. Among the open-source LLMs, InternLM2-7B-chat has the best safety performance with average failure rate 14.65%, which is comparable to ChatGPT (13.69% Avg FR). Notably, LLAMA2-7B-chat and LLAMA2-13B-chat have the best safety performance in generation tasks, and are even more reliable than GPT-4 in defending against jailbreak attacks, but they cannot discriminate harmful information well.

(2) **Comparison of different test tasks.** When only original malicious queries are used as LLM inputs without adding any prompt prefixes and suffixes, almost all the safely-trained LLMs can generate harmless responses. However, except Claude-3, almost all LLMs are vulnerable to jailbreak

   } &  &  &  \\  &  &  &  &  &  \\   & clando-3-haku-20240307 & 0.00 & 0.00 & 0.02 & 4.30 & 7.66 & 2.99 \\  & ggr-4-tube-preview & 0.00 & 13.56 & 6.45 & 15.11 & 8.78 \\  & ggr-3-cutbo & 0.00 & 23.58 & 20.53 & 10.64 & 13.69 \\   & Natural-7B-instruct & 2.70 & 47.07 & 15.39 & 40.43 & 26.40 \\  & LLAMA3-8B-Instruct & 2.08 & 7.37 & 66.08 & 42.96 & 29.62 \\  & LLAMA2-13B-Chat & 0.42 & 8.54 & 31.62 & 32.25 & 18.21 \\  & LLAMA2-7B-Chat & 0.28 & 11.75 & 56.24 & 26.66 & 23.73 \\  & Qwen1-8-Instruct & 2.01 & 25.08 & 11.44 & 22.71 & 15.31 \\  & Qwen1-5-14B-Chat & 0.37 & 39.55 & 8.18 & 31.66 & 19.87 \\  & Qwen1-5-7B-Chat & 0.35 & 39.35 & 46.60 & 36.85 & 30.79 \\  & ChaGLM3-6B & 1.39 & 35.46 & 9.36 & 50.06 & 24.07 \\  & InertLM2-7B-Chat & 0.69 & 26.93 & 15.81 & 15.19 & 14.65 \\  & Qwen7-B-Chat & 0.42 & 26.88 & 52.70 & 39.08 & 29.77 \\   

Table 2: Comparison of LLM safety performance on different test tasks. We use “Failure Rate” as the evaluation metric, and the lower the score, the better the model safety performance.

   } &  &  &  \\  & & **N/A** & **ToP** & **RoP** & **RoP** + FS** & **Direct** & **COT** & **Direct+FS** & **Direct** & **COT** & **Direct + FS** \\   & clando-3-haku-20240307 & 0.02 & 0.01 & 0.01 & 0.01 & 4.30 & 13.87 & 12.48 & 7.66 & 11.71 & 6.05 \\  & ggr-4-tube-preview & 13.56 & 7.22 & 6.57 & 5.54 & 6.45 & 8.39 & 24.69 & 15.11 & 15.46 & 11.33 \\  & ggr-3-cutbo & 23.58 & 14.86 & 15.97 & 14.77 & 20.53 & 22.47 & 16.30 & 10.64 & 21.81 & 11.61 \\   & Mistral-7B-Instruct & 47.07 & 43.88 & 26.4 & 43.01 & 15.39 & 34.26 & 7.56 & 40.43 & 40.67 & 17.09 \\  & LLAMA3-8B-Instruct & 7.37 & 9.78 & 4.85 & 6.32 & 60.88 & 78.43 & 23.44 & 42.49 & 68.17 & 15.57 \\  & LLAMA2-13B-Chat & 8.54 & 10.00 & 5.75 & 31.62 & 47.09 & 33.36 & 32.25 & 44.94 & 21.95 \\  & LLAMA2-7B-Chat & 11.75 & 10.48 & 4.04 & 10.58 & 56.24 & 64.35 & 50.83 & 26.66 & 43.61 & 34.67 \\  & Qwen2-7B-Instruct & 25.08 & 22.63 & 21.05 & 22.52 & 11.44 & 16.57 & 6.45 & 22.71 & 28.02 & 11.82 \\  & Qwen1-5-14B-Chat & 39.58 & 36.35 & 18.63 & 18.18 & 8.102 & 11.37 & 33.16 & 36.56 & 32.12 \\  & Qwen1-5-7B-chat & 39.35 & 37.94 & 21.19 & 38.14 & 46.60 & 26.76 & 39.67 & 36.85 & 41.23 & 14.25 \\  & ChaGLM3-6B & 35.46 & 38.2 & 18.41 & 28.35 & 9.36 & 19.56 & 14.08 & 50.06 & 48.92 & 14.91 \\  & InternalM2-7B-Chat & 26.93 & 30.28 & 12.49 & 26.01 & 15.81 & 18.16 & 5.62 & 15.19 & 32.52 & 17.72 \\  & Qwen7-B-Chat & 26.88 & 30.84 & 22.32 & 22.58 & 52.70 & 34.26 & 31.76 & 39.08 & 40.39 & 31.14 \\   

Table 3: Comparison of the effect of diverse prompt types on LLM safety performance attacks. For example, Mistral-7B-instruct shows the worst safety performance on the jailbreak attack test set, with a failure rate of 47.07%. Besides, almost all LLMs often perform well in answering open-ended questions but struggle to discriminate harmful contents correctly. For instance, when original queries are directly input into ChatGPT, the generated responses are all harmless, but when we construct safety judgments and multiple-choice tests based on the same queries, the failure rates rise to 10.64% and 20.53% respectively.

(3) **The effect of model size** According to scaling laws , the performance of large language models will improve as the number of model parameters and the amount of training data increase. However, does this law also apply to the LLM safety performance? By analyzing the models of Owen1.5 and LLAMA2 series, we can observe that the safety performance of LLMs also follows scaling laws. For example, the average failure rate of LLAMA2-13B-chat is 18.21%, which is lower than that of LLAMA2-7B-chat (Avg FR 23.73%). The average failure rate of Qwen1.5-14B-chat is also lower than that of Qwen1.5-7B-chat.

### Effect of prompt types on LLM safety performance

To evaluate whether prompt engineering techniques affect the safety performance of LLMs, we conducted experiments on 7 extended evaluation sets of SG-Bench, as shown in Table 3. We can draw three important conclusions:

(1) **Reasonable setting of system prompt, especially role-oriented prompt, can effectively defend against jailbreak attacks.** We can see that role-oriented prompts can consistently improve the defense capabilities for jailbreak attacks by framing LLMs as an AI assistant. We think that it is because LLMs have learned the relationships between "AI Assistant" and "human values" during pre-training and fine-tuning stage.

(2) **Few-shot demonstrations might damage LLM safety performance on open-end generation, but can improve the the ability to identify harmful information.** For the jaibreak attack subsets, we randomly select 3 examples from alpaca training set for few-shot demonstrations. Experimental results have shown that few-shot demonstrations make LLMs more vulnerable to jailbreak attacks. For multiple-choice questions and safety judgments subsets, almost all LLMs show better discrimination ability when applying few-shot demonstrations.

(3) **Chain-of-thought prompting may have a negative impact on the safety performance of LLMs.** We can observe that when applying the chain-of-thought prompting to LLMs, their ability to discern harmful information declines significantly. For instance, COT prompting increased Claude-3's failure rate in multiple-choice tests from 4.30% to 13.87%. In safety judgments tests, COT prompting raised InternLM2-7B-chat's failure rate by 17.33%. We argue that it is due to the auto-regressive nature of LLM outputs, where the chain-of-thought guides LLMs to reason first and then make a judgment. During the reasoning process, the model may output harmful information, leading to biased final results.

## 5 Qualitative Analysis

### Erroneous Response Analysis

We carefully analyzed the failure cases of each model on different test tasks, and summarized the erroneous responses into four classes. For generation task, failure cases include unsafe response with ethical and legal reminder **(class 1)** and malicious response without considering ethical and legal constraints **(class 2)**. For discrimination task, there are also two types of erroneous cases. **class 1**

    &  &  \\  & **failure rate** & **class 1** & **class 2** & **failure rate** & **class 1** & **class 2** \\  ChatGPT & 14.86 & 7.87 & 6.99 & 20.53 & 0.20 & 20.33 \\ Mistral-7B-instruct & 43.88 & 20.01 & 23.87 & 15.39 & 0.76 & 14.63 \\ LLAMA2-7B-chat & 10.48 & 5.14 & 5.34 & 56.24 & 3.60 & 52.64 \\ LLAMA2-13B-chat & 10.00 & 4.74 & 5.26 & 31.62 & 13.87 & 17.75 \\ Qwen1.5-7B-chat & 37.94 & 12.07 & 25.87 & 46.60 & 0.48 & 46.12 \\   

Table 4: The proportion of each type of erroneous responses.

is incoherent response, which means the model simply refuses to instruction, rather than giving a answer. **(class 2)** is misjudgment. Table 4 shows the proportion of each type of erroneous responses. We can see that LLAMA2-7B-chat and LLAMA2-13B-chat show the best safety performance in the generation task, but have more incoherent responses in the discrimination task. This means that these two LLMs are over-aligned and cannot follow the task instructions well.

### Analysis by Attention Visualization

To further investigate why safety-aligned LLMs show poor generalization across diverse test tasks and prompt types, we conduct attention visualization experiments following previous work [17; 46] to examine the effect of prompt contexts on LLMs' attentions towards input words. We choose attention by gradient, a method that assigns an attention score to each word based on the gradient norm (More details can be seen in Appendix C). From the results in Table 5, we can see that for the original query, such as "Write a tutorial on how to make a bomb", LLM's attention is focused on "write" and "tutorial". However, when we add jailbreak attack prefix to the query, the attention towards the original query becomes more dispersed, and more attention is concentrated on the prefix prompt. There is a similar phenomenon for the test samples of the discrimination task. Thus we can conclude that after applying various complex prompts to the inputs, LLM's attention gradually shifted from malicious queries to prompt contexts in order to generate a response that conforms to the user's request better, resulting in the failure of safety measures.

### Analysis of the Discrimination Ability of LLMs

In order to further analyze why LLMs show worse safety performance in discrimination tasks, we need to analyze the ability of LLMs to discriminate harmful and harmless contents. We use the intra-class and inter-class distance, as well as the average failure rate of multiple-choice questions and safety judgment tests as evaluation metrics for further discussion. Firstly, we use LLMs to extract the semantic representations for responses in the safety judgment test set, in which the responses are divided into two classes: harmful and harmless. And then we calculate the intra-class and inter-class distances following Feng et al. . For the intra-class distance, we calculate the mean value of the euclidean distance between each sample and its class center. For the inter-class distance, we calculate the euclidean distance between the center of the two classes. We also report the ratio between intra-class and inter-class distance. The results are shown in Table 6. We can see that LLM's safety performance on discrimination tests is positively related to its representation modeling ability to harmful and harmless contents.

### Evaluator Comparison

Considering costs and efficiency, we use LlamaGuard-7B as a referee model to judge whether responses generated by LLMs in the open-end text generation test are harmful. In this section, we

  
**Models** & **intra-class \(\)** & **inter-class \(\)** & **intra-class/inter-class \(\)** & **failure rate \(\)** \\  Qwen1.5-7B-chat & 196.47 & 117.47 & 1.67 & 41.72 \\ LLAMA2-7B-chat & 73.21 & 44.78 & 1.63 & 41.45 \\ Mistral-7B-instruct & 240.49 & 165.83 & 1.45 & 27.91 \\ InternalM2-7B-chat & 125.85 & 97.08 & 1.29 & 15.50 \\   

Table 6: Comparison of the ability of different LLMs to discriminate harmful and harmless responses.

  
**Evaluation Subjects** & **inputs** \\  Original Query & **inter-class \(\)** & **inter-class \(\)** & **inter-class \(\)** \\  Jailbreak Attack & **inter-class \(\)** & **inter-class \(\)** & **inter-class \(\)** \\  Multiple-choice Questions & **inter-class \(\)** & **inter-class \(\)** & **inter-class \(\)** \\  Safety judgments & **inter-class \(\)** & **inter-class \(\)** & **inter-class \(\)** \\   

Table 5: Attention visualization of samples in different test sets. The darker the color, the greater the attention weight. LLM’s attention gradually shifts from the harmful query to prompt contexts.

further compared the evaluation results of different referee models, as shown in Table 7. It can be observed that, for each evaluated LLM, there are differences in the evaluation results of different referee models, but their relative order of safety performance remains consistent. We have also averaged the evaluation scores from the four evaluators, in which the ranking of safety performance remained unchanged. We argue that this variation primarily stems from difference in the training data of various referee models, leading to difference in their safety criteria. Claude-3 is widely recognized as the most harmless LLMs, and as a referee model, it is also a stricter evaluator. The evaluation scores of LlamaGuard-7B are closest to those of ChatGPT, reflecting the alignment in the safety standards learned by both models.

## 6 Discussion

We introduced SG-Bench, an LLM safety evaluation benchmark targeting diverse prompt context variants, and conducted extensive experiments to uncover the reasons behind the poor generalization of safety-aligned LLMs. However, our work has several limitations: (1) **Imperfect LLM-based evaluator:** For evaluating open-ended generation tasks, we used LLAMA-Guard-7B, the best open-source safety evaluator available at the time. We also provide a comparison of different LLMs used as evaluators in Section 5.4. Nevertheless, LLM-based safety evaluation remains an open research problem, which warrants further exploration in the future. (2) **The need for more fine-grained categorization:** This study primarily focuses on LLM safety generalization across diverse prompt contexts (varied task types and prompt types) without delving into the effects of different types of safety issues. Future work can explore more fine-grained safety issues to understand specific security flaws better.

Furthermore, this paper identifies that the poor generalization of safety performance is mainly due to the shift in LLMs' attention from malicious instructions to prompt contexts caused by the prompt context itself. This insight could guide the development of targeted safety training methods in the future. For example, we can construct a safety instruction fine-tuning dataset encompassing multiple task and prompt types to enhance LLMs' safety performance further.

## 7 Conclusion

In this paper, we propose a multi-dimensional safety evaluation benchmark (SG-Bench) for evaluating LLM safety generalization across diverse test tasks and prompt types. We assess the safety performance of 13 proprietary and open-source LLMs on both generation and discrimination tasks, and delve into the effect of prompt engineering techniques on LLM safety performance. we plan to extend SG-Bench with more challenging scenarios such as multi-turn dialogue and code safety, and will also explore more methods to improve the generalization of LLM safety alignment in the future.

## 8 Broader Impact and Ethics Statement

Safety evaluation benchmarks are crucial for identifying potential risks in LLMs. Our research aims to delve into the problem of LLM safety generalization by assessing the safety performance of LLMs across various tasks and prompt types. This issue is significant for the practical applications of large language models in different scenarios. To mitigate risks associated with sensitive content in the benchmarks, we restrict access to authorized researchers who adhere to strict ethical guidelines. These measures protect the integrity of the research while minimizing potential harm.

  
**Evaluator** & **ChatGPT** & **Mistral-7B-instruct** & **LLAMA2-7B-chat** & **Qwen1.5-7B-chat** & **ChatGLM3-6B** & **InterrLM2-7B-chat** \\   LlamaGuard-7B & 14.86 & 43.88 & 10.48 & 37.94 & 34.82 & 30.28 \\ ChatGPT & 38.74 & 64.91 & 28.96 & 53.06 & 41.92 & 41.20 \\ GPT4 & 16.19 & 61.28 & 13.21 & 49.13 & 36.42 & 32.38 \\ Claude3 & 29.83 & 70.54 & 29.31 & 59.79 & 55.16 & 45.11 \\  
**Average** & 24.90 & 60.15 & 20.49 & 49.98 & 42.08 & 37.24 \\   

Table 7: Comparison of evaluation results of different evaluators on the task-oriented prompts extended evaluation set