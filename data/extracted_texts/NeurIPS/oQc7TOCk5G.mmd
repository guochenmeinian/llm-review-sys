# On Theoretical Limits of Learning with Label Differential Privacy

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Label differential privacy (DP) is designed for learning problems with private labels and public features. Although various methods have been proposed for learning under label DP, the theoretical limits remain unknown. The main challenge is to take infimum over all possible learners with arbitrary model complexity. In this paper, we investigate the fundamental limits of learning with label DP under both central and local models. To overcome the challenge above, we derive new lower bounds on testing errors that are adaptive to the model complexity. Our analyses indicate that \(\)-local label DP only enlarges the sample complexity with respect to \(\), without affecting the convergence rate over the sample size \(N\), except the case with heavy-tailed label. Under the central model, the performance loss due to the privacy mechanism is further weakened, such that the additional sample complexity becomes negligible. Overall, our analysis validates the promise of learning under the label DP from a theoretical perspective and shows that the learning performance can be significantly improved by weakening the DP definition to only labels.

## 1 Introduction

Many modern machine learning tasks require sensitive training samples that need to be protected from leakage . As a standard approach for privacy protection, differential privacy (DP)  has been extensively studied [3; 4; 5; 6; 7; 8; 9]. However, the learning performances under original DP definition are usually far from satisfactory [10; 11; 12; 13]. Therefore, researchers attempt to design weakened DP requirements, under which the performances can be significantly improved, while still securing sensitive information. Under such background, label DP has emerged in recent years , which regards features as public, while only labels are sensitive and need to be protected. Such setting is realistic in many applications, such as computational advertising , recommendation systems  and medical diagnosis . These tasks usually use some basic demographic information as features, which can be far less sensitive.

Despite various approaches for learning with label DP [14; 18; 19; 20; 21], the fundamental limits are still unknown. An interesting question is: By weakening the DP definitions to only labels, how much accuracy improvement is possible? From an information-theoretic perspective , the underlying limits of statistical problems are characterized by the minimax lower bound, which takes the supremum over all possible distributions from a general class, and infimum over all learners. Deriving minimax lower bounds for learning under the label DP is challenging in two aspects. Firstly, under label DP, each sample has both public (i.e. the feature) and private (i.e. the label) components. Directly applying the methods for original DP [23; 24; 25; 26; 27] treats all components as private, and thus does not yield tight results. Secondly, the classical packing method  is only suitable for fixed model structures with fixed dimensionality. However, to establish lower bounds, one needs to take infimum over all possible learners with arbitrary model complexity.

In this paper, we investigate the theoretical limits of classification and regression problems under label DP. Our analysis involves both central and local models. For each problem, we derive the information-theoretic minimax lower bound of the risk function over a wide class of distributions satisfying the \(\)-Holder smoothness and the \(\)-Tsybakov margin assumption  (see Assumption 1 for details). The general idea is to convert the problem to multiple hypothesis testing. To overcome the challenges above, we provide a bound of Kullback-Leibler divergence over joint distributions of private and public random variables, which is tighter than the bound between fully private variables. Moreover, under the central model, instead of using the packing method, we develop a new lower bound on the minimum testing error for each pair of hypotheses based on the group privacy property , which is suitable for arbitrary model complexity. After deriving minimax lower bounds, we also propose algorithms with matching upper bounds to validate the tightness of our results.

The results are shown in Table 1, in which the third row refers to the bounds under the original local DP definition, while the fourth row lists the non-private baselines. To the best of our knowledge, minimax rates under central DP have not been established, and are thus not listed here. The main findings are summarized as follows.

* Under \(\)-local label DP, for classification and regression with bounded label noise, the sample complexity is larger by a factor of \(O(1/^{2})\). However, the convergence rate remains unaffected, which is in clear contrast with the original DP, under which the convergence rate is slower.
* Under \(\)-local label DP constraint, for regression with heavy-tailed label noise, the convergence rate of risk over \(N\) becomes slower, indicating that heavy-tailed labels increase the difficulty of privacy protection.
* Under \(\)-central label DP constraint, the performance loss caused by the privacy mechanism becomes further weakened. The risk only increases by a term that decays faster than the non-private rate, indicating that the additional sample complexity caused by the privacy mechanism becomes negligible with large \(N\).

In general, our analysis provides a theoretical perspective of understanding label DP. The result shows that by weakening the DP definition to protecting labels only, the learning performances can be significantly improved.

## 2 Related Work

**Label DP.** Under the local model, labels are randomized before training. The simplest method is randomized response . An important improvement is proposed in , called RRWithPrior, which incorporates prior distribution.  proposes ALIBI, which further improves randomized response by generating soft labels through Bayesian inference. There are also several methods for regression under label DP [31; 18]. Under central label DP,  proposes a clustering approach.  proposes private aggregation of teacher ensembles (PATE), which is then further improved in .

**Minimax analysis for public data.** Minimax theory provides a rigorous framework for the best possible performance of an algorithm given some assumptions. Classical methods include Le Cam , Fano  and Assouad . Using these methods, minimax lower bounds have been widely established for both classification and regression problems [35; 36; 37; 38; 39; 40; 41; 28]. If the feature vector has bounded support, then the minimax rate of classification and regression are \(O(N^{-})\) and \(O(N^{-})\), respectively.

   & Regression & Regression \\  & Bounded label noise & Unbounded label noise \\  Local & \(((N(^{2} 1))^{-} )\) & \(((N(^{2} 1)^{-}))\) & \(O((N^{2})^{-} N^{-})\) \\ Central & \((N^{-}+( N)^{-})\) & \(O(N^{-}+( N)^{-})\) & \(O(N^{-}+( N)^{-})\) \\ Local full & \(O((N(^{2} 1)^{-})\) & \(O((N(^{2} 1))^{-})\) & \(O((N(^{2} 1))^{-})\) \\ Non-priv. & \(O(N^{-})\) & \(O(N^{-})\) & \(O(N^{-})\) \\  

Table 1: Minimax rate of convergence under label differential privacy. \(d\) is the dimension of features.

**Minimax analysis for private data.** Under the local model,  finds the relation between label DP and stochastic query.  and  develop the variants of Le Cam, Fano, and Assouad's method under local DP. Lower bounds are then established for various statistical problems, such as mean estimation , classification  and regression . Under central model, for pure DP, the standard approach is the packing method , which is then used in hypothesis testing , mean estimation , and learning of distributions . There are also several works on approximate DP, such as .

This work studies the theoretical limits of label DP, under which each sample is a mixture of public feature and private labels, thus existing methods can not be directly applied here. Under the central model, the minimax analysis becomes more challenging, since the packing method is only suitable for fixed model structures (i.e. the dimensionality of model output is fixed), while we need to find the minimum possible error over all possible learners with arbitrary output dimensions. As a result, the lower bounds of general classification and regression problems have not been established even under the original DP definition. To overcome such challenge, we develop a new approach to bound the error of hypothesis testing (see Lemma 1 in Appendix D).

## 3 Preliminaries

In this section, we show some necessary definitions, background information, and notations.

### Label DP

To begin with, we review the definition of DP. Suppose the dataset consists of \(N\) samples \((_{i},y_{i})\), \(i=1,,N\), in which \(_{i}\) is the feature vector, while \(y_{i}^{d}\) is the label.

**Definition 1**.: _(Differential Privacy (DP) ) Let \( 0\). A randomized function \(:(,)^{N}\) is \(\)-DP if for any two adjacent datasets \(D,D^{}(,)^{N}\) and any \(S\),_

\[P((D) S) e^{}P((D^{}) S),\] (1)

_in which \(D\) and \(D^{}\) are adjacent if they differ only on a single sample, including both the feature vector and the label._

In machine learning tasks, the output of \(\) is the model parameters, while the input is the training dataset. Definition 1 requires that both features and labels are privatized. Consider that in some applications, the features may be much less sensitive, the notion of label DP is defined as follows.

**Definition 2**.: _(Central label DP) A randomized function \(\) is \(\)-label DP if for any two datasets \(D\) and \(D^{}\) that differ on the label of only one training sample and any \(S\), (1) holds._

Compared with Definition 1, Definition 2 only requires the output to be insensitive to the replacement of a label. Therefore label DP is a weaker requirement. Correspondingly, the local label DP is defined as follows.

**Definition 3**.: _(Local label DP) A randomized function \(M:(,)\) is \(\)-local label DP if_

\[_{y,y^{}S},y) S)}{P(M(,y^{}) S)}.\] (2)

Definition 3 requires that each label is privatized locally before running any machine learning algorithms. It is straightforward to show that local label DP ensures central label DP. To be more precise, we have the following proposition.

**Proposition 1**.: _Let \(_{i}=M(_{i},y_{i})\) for \(i=1,,N\). If \(\) is a function of \((_{i},_{i})\), \(i=1,,N\), then \(\) is \(\)-label DP._

### Risk of Classification and Regression

In supervised learning problems, given \(N\) samples \((_{i},Y_{i})\), \(i=1,,N\) drawn from a common distribution, the task is to learn a function \(g:\). For a loss function \(l:\), the goal is to minimize the _risk function_, which is defined as the expectation of loss function between the predicted value and the ground truth:

\[R=[l(,Y)].\] (3)The minimum risk among all function \(g\) is called Bayes risk, i.e. \(R^{*}=_{g}[l(g(,Y))]\). In practice, the sample distribution is unknown, and we need to learn \(g\) from samples. Therefore, the risk of any practical classifiers is larger than Bayes risk. The gap \(R-R^{*}\) is called excess risk, and we hope that \(R-R^{*}\) to be as small as possible. Now we discuss classification and regression problems separately.

_1) Classification._ For classification problems, the size of \(\) is finite. For convenience, we denote \(=[K]\), in which \([K]:=\{1,,K\}\). In this paper, we use \(0-1\) loss, i.e. \(l(,Y)=( Y)\), then \(R=( Y)\). Define \(K\) functions \(_{1},,_{K}\) as the conditional class probabilities:

\[_{k}()=(Y=k|=),k=1,,K.\] (4)

Under this setting, the Bayes optimal classifier and the corresponding Bayes risk is

\[c^{*}() = _{j}(),\] (5) \[R^{*}_{cls} = (c^{*}() Y).\] (6)

_2) Regression._ Now we consider the case with \(\) having infinite size. We use \(_{2}\) loss in this paper, i.e. \(l(,Y)=(-Y)^{2}\). Then the Bayes risk is

\[R^{*}_{reg}=[(Y-())^{2}].\] (7)

Then the following proposition gives a bound of the excess risk for classification and regression problems.

**Proposition 2**.: _For any classifier \(c:[K]\), the excess risk of classification is bounded by_

\[R_{cls}-R^{*}_{cls}=(^{*}()-[_{c()} ()])f()d.\] (8)

_For any regression estimate \(:\), the excess risk of regression is bounded by_

\[R_{reg}-R^{*}_{reg}=[(()-())^{2}].\] (9)

The proof of Proposition 2 is shown in Appendix A. Finally, we state some basic assumptions that will be used throughout this paper.

**Assumption 1**.: _There exists some constants \(L\), \(\), \(C_{T}\), \(\), \(c\), \(D\) and \((0,1]\) such that_

_(a) For all \(j[K]\) and any \(\), \(^{}\), \(|_{j}()-_{j}(^{})| L\|- ^{}\|^{}\);_

_(b) For any \(t>0\), \((0<^{*}()-_{s}()<t) C_{T}t ^{},\) in which \(_{s}()\) is the second largest one among \(\{_{1}(),,_{K}()\}\);_

_(c) The feature vector \(\) has a probability density function (pdf) \(f\) which is bounded from below, i.e. \(f() c\);_

_(d) For all \(r<D\), \(V_{r}() v_{d}r^{d},\) in which \(V_{r}()\) is the volume (Lebesgue measure) of \(B(,r)\), \(v_{d}\) is the volume of a unit ball._

Assumption 1 (a) requires that all \(_{j}\) are Holder continuous. This condition is common in literatures about nonparametric statistics . (b) is generalized from the Tsybakov noise assumption for binary classification, which is commonly used in many existing works in the field of both nonparametric classification  and differential privacy . If \(K=2\), then \(^{*}\) and \(_{s}\) refer to the larger and smaller class conditional probability, respectively. An intuitive understanding of (b) is that in the majority of the support, the maximum value among \(\{_{1}(),,_{K}()\}\) should have some gap to the second largest one. With sufficiently large sample size and model complexity, assumption (b) ensures that for test samples within the majority of the support \(\), the algorithm is highly likely to correctly identify the class with the maximum conditional probability. Therefore, in (b), we only care about \(^{*}()\) and \(_{s}()\), while other classes with small conditional probabilities can be ignored. (c) is usually called "strong density assumption" in existing works , which is quite strong. It is possible to relax this assumption so that the theoretical analysis becomes suitable for general cases. However, we do not focus on such generalization in this paper. Assumption (d) prevents the corner of the support \(\) from being too sharp. In the remainder of this section, denote \(_{cls}\) as the set of all pairs \((f,)\) satisfying Assumption 1.

[MISSING_PAGE_FAIL:5]

According to Definition 3, \(M\) is \(\)-local label DP. For the performance guarantee (14), according to Proposition 2, we need to bound \(^{*}()-[_{c()}()]\) for each \(\). If \(^{*}()-_{s}()\) is large, then with high probability, \(c()=c^{*}()\), and then \(^{*}()=_{c()}()\). Thus we mainly consider the case with small \(^{*}()-_{s}()\). The details of proof are shown in Appendix C. 

The lower bound (10) and the upper bound (14) match up to a logarithm factor, indicating that the results are tight. Now we comment on the results.

**Remark 1**.: _1) **Comparison with non-private bound.** The classical minimax lower bound for non-private classification problem is \(N^{-}\). Therefore, the lower bound (10) reaches the non-private bound with \( 1\). With small \(\), \(N\) training samples with privatized labels roughly equals \(N^{2}\) non-privatized samples in terms of performance._

_2) **Comparison with local DP that protects both features and labels.** In this case, the optimal excess risk is \((N^{2})^{-(+1)/(2+2d)} N^{-(+1)/(2 +d)}\), which is worse than the right hand side of (10). Such result indicates that compared with classical DP, label DP incurs significantly weaker performance loss._

_3) **Comparison with other baseline methods.** If we use the randomized response method instead of the privacy mechanism (11), then the performance decreases sharply with the number of classes \(K\). Several methods have been proposed to improve the randomized response method, such as RRWithPrior  and ALBII . However, these methods are not guaranteed in theory._

### Central Label DP

_1) Lower bound._ The following theorem shows the minimax lower bound under the central label DP.

**Theorem 3**.: _Denote \(_{}\) as the set of all learning algorithms satisfying \(\)-label DP (Definition 2). Then_

\[_{_{}(f,)_{cls}}(R_{cls }-R_{cls}^{*}) N^{-}+( N)^{- }.\] (16)

Proof.: (Outline) Lower bounds under central DP are usually constructed by packing method , which works for fixed output dimensions. However, to achieve a desirable bias and variance tradeoff, the model complexity needs to increase with \(N\). In our proof, we still divide the support into \(G\) bins and construct two hypotheses for each bin, but we develop a new tool (see Lemma 1) to give a lower bound of the minimum error of hypothesis testing. We then use the group privacy property  to get the overall lower bound. The details can be found in Appendix D. 

_2) Upper bound._ Now we show that (16) is achievable. Similar to the local label DP problem, now divide the support into \(G\) bins, such that the length of each bin is \(h\). Now the classification within the \(l\)-th bin follows a exponential mechanism :

\[(c_{l}=j|_{1:N},Y_{1:N})=/2}}{_{ k=1}^{K}e^{ n_{lk}/2}},\] (17)

in which \(n_{lj}=_{i=1}^{N}(_{i} B_{l},Y_{i}=j)\). Then let \(c()=c_{l}\) for \( B_{l}\). The excess risk is bounded in the next theorem.

**Theorem 4**.: _The privacy mechanism (17) is \(\)-label DP. Moreover, under Assumption 1, if \(h\) scales as \(h( K/ N)^{}+( K/N)^{}\), then the excess risk can be bounded as follows:_

\[R-R^{*}()^{-}+()^{-}.\] (18)

Proof.: (Outline) The privacy guarantee of the exponential mechanism has been analyzed in . Following these existing analyses, it can be shown that (17) is \(\)-label DP. It remains to show (18). Note that if \(^{*}()-_{s}()\) is large, then the difference between the largest and the second largest one from \(\{n_{lj}|j=1,,K\}\) will also be large. From (17), the following inequality holds with high probability: \(c_{l}=_{j}n_{lj}=_{j}_{j}()=c^{*}()\), which means that the classifier makesoptimal prediction. Hence we mainly consider the case with small \(^{*}()-_{s}()\). The details of the proof can be found in Appendix E. 

The upper and lower bounds match up to logarithmic factors. In (18), the first term is just the non-private convergence rate, while the second term \(( N)^{-}\) can be regarded as the additional risk caused by the privacy mechanism. It decays faster with \(N\) compared with the first term, thus the additional performance loss caused by the privacy mechanism becomes negligible as \(N\) increases. This result is crucially different from the local model, under which the privacy mechanism always induces higher sample complexity by a factor of \(O(1/(^{2} 1))\).

## 5 Regression with Bounded Noise

Now we analyze the theoretical limits of regression problems under local and central label DP. Throughout this section, we assume that the label is restricted within a bounded interval.

**Assumption 2**.: _Given any \(\), \(P(|Y|<T|=)=1\)._

Assumption 1 remains the same here. In the remainder of this section, denote \(_{reg1}\) as the set of \((f,)\) that satisfies Assumption 1 and 2.

### Local Label DP

_1) Lower bound._ Theorem 5 shows the minimax lower bound.

**Theorem 5**.: _Denote \(_{}\) as the set of all privacy mechanisms satisfying \(\)-label DP. Then_

\[_{}_{M_{}(f,) _{reg1}}(R_{reg}-R_{reg}^{*})(N(^{2} 1))^{- }.\] (19)

The proof of Theorem 5 is similar to that of Theorem 1, except for some details in hypotheses construction and the final bound of excess risk. The details are shown in Appendix F.

_2) Upper bound._ The privacy mechanism is \(Z=Y+W\), in which \(W(2T/)\). Then the privacy mechanism satisfies \(\)-label DP. In this case, the real regression function \(()\) can be estimated using the nearest neighbor approach. Let

\[()=_{i_{k}( )}Z_{i},\] (20)

in which \(_{k}()\) is the set of \(k\) nearest neighbors of \(\) among \(_{1},,_{N}\).

**Theorem 6**.: _The method described above is \(\)-local label DP. Moreover, with \(k N^{}( 1)^{-}\), then under Assumption 1 and 2,_

\[R_{reg}-R_{reg}^{*}(N(^{2} 1))^{-}.\] (21)

Proof.: (Outline) Since \(|Y|<T\), \(W(2T/)\), it is obvious that \(Z=Y+W\) is \(\)-local label DP. For the performance (21), the bias can be bounded by the \(k\) nearest neighbor distances based on Assumption 1(a). The variance of \(()\) scales inversely with \(k\). An appropriate \(k\) can be selected to achieve a good tradeoff between bias and variance. The details are shown in Appendix G. 

From standard minimax analysis on regression problems, the non-private convergence rate is \(N^{-2/(d+2)}\). From Theorem 5 and 6, the privatization process makes sample complexity larger by a \(O(1/^{2})\) factor.

### Central Label DP

_1) Lower bound._ The following theorem shows the minimax lower bound.

**Theorem 7**.: _Let \(_{}\) be the set of all algorithms satisfying \(\)-central DP. Then_

\[_{_{}(f,) _{reg1}}(R_{reg}-R_{reg}^{*}) N^{-}+( N )^{-}.\] (22)2) Upper bound.: For each bin \(B_{l}\), let \(n_{l}=_{i=1}^{N}(_{i} B_{l})\) be the number of samples in \(B_{l}\). If \(n_{l}>0\), then

\[_{l}=}_{i=1}^{N}(_{i} B_{l})Y _{i}+W_{l},\] (23)

in which \(W_{l}(2/(n_{l}))\). If \(n_{l}=0\), i.e. no sample falls in \(B_{l}\), then just let \(_{l}=0\). For all \( B_{l}\), let \(()=_{l}\). The excess risk can be bounded with the following theorem.

**Theorem 8**.: (23) _is \(\)-label DP. Moreover, under Assumption 1 and 2, if \(h\) scales as \(h N^{-}+( N)^{-}\), then the excess risk is bounded by_

\[R-R^{*} N^{-}+( N)^{-}.\] (24)

The upper and lower bounds match, indicating that the results are tight. Again, the second term in (24) converges faster than the first one with respect to \(N\), the performance loss caused by privacy constraints becomes negligible as \(N\) increases.

## 6 Regression with Heavy-tailed Noise

In this section, we consider the case such that the noise has tails. We make the following assumption.

**Assumption 3**.: _For all \(\), \([|Y|^{p}|=] M_{p}\) for some \(p 2\)._

Instead of requiring \(|Y|<T\) for some \(T\), now we only assume that the \(p\)-th order moment is bounded. For non-private cases, given fixed noise variance, the tail does not affect the mean squared error of regression. As a result, as long as \(p 2\), the convergence rate of regression risk is the same as the case with bounded noise. However, the label DP requires the output to be insensitive to the worst case replacement of labels, which can be harder if the noise has tails. To achieve \(\)-DP, the clipping radius decreases with \(\), thus the noise strength needs to grow faster than \(O(1/)\). As a result, the convergence rate becomes slower than the non-private case. In the remainder of this section, denote \(_{reg2}\) as the set of \((f,)\) that satisfies Assumption 1 and 3.

### Local Label DP

_1) Lower bound._ In earlier sections about classification and regression with bounded noise, the impact of privacy mechanisms is only a polynomial factor on \(\), while the convergence rate of excess risk with respect to \(N\) is not changed. However, this rule no longer holds when the noise has heavy tails.

**Theorem 9**.: _Denote \(_{}\) as the set of all privacy mechanisms satisfying \(\)-label DP. Then for small \(\),_

\[_{}_{M_{}(f,)}(R_{ reg}-R_{reg}^{*})(N(e^{}-1)^{2})^{-}+N^{-}.\] (25)

_2) Upper bound._ Since now the noise has unbounded distribution, without preprocessing, the sensitivity is unbounded, thus simply adding noise to \(Y\) can no longer protect the privacy. Therefore, a solution is to clip \(Y\) into \([-T,T]\), and add noise proportional to \(T/\) to achieve \(\)-local label DP. Such truncation will inevitably introduce some bias. To achieve a tradeoff between clipping bias and sensitivity, the value of \(T\) needs to be tuned carefully. Based on such intuition, the method is precisely stated as follows. Let \(Z_{i}=Y_{Ti}+W_{i}\), in which \(Y_{Ti}\) is the truncation of \(Y_{i}\), i.e. \(Y_{Ti}=(Y_{i} T)(-T)\), and \(W(2T/)\). The result is shown in the next theorem.

**Theorem 10**.: _The method above is \(\)-local label DP. Moreover, with \(k(N^{2})^{} N^{}\), and \(T(k^{2})^{}\), the risk is bounded by_

\[R_{reg}-R_{reg}^{*}(N^{2})^{-}+N^{-}.\] (26)

Proof.: (Outline) It can be shown that the clipping bias scales as \(T^{2(1-p)}\). To meet the \(\)-label DP, an additional error that scales as \(T/\) is needed. By averaging over \(k\) nearest neighbors, the variance caused by noise \(W\) scales with \(T^{2}/(k^{2})\). From standard analysis on nearest neighbor methods , the non-private mean squared error scales as \(1/k+(k/N)^{2/d}\). Put all these terms together, Theorem 10 can be proved. Details can be found in Appendix K.

With the limit of \(p\), the problem reduces to the case with bounded noise, and the growth rate of \(k\) and the convergence rate of risk are the same as those in Theorem 6. For finite \(p\), \(2(p-1)/(2p+d(p-1))<2/(2+d)\), thus the convergence rate becomes slower due to the privacy mechanism.

### Central Label DP

_1) Lower bound._ The minimax lower bound is shown in Theorem 11.

**Theorem 11**.: _The minimax lower bound is_

\[_{_{*}(f,)_{reg2}}(R_{reg}-R_{ reg}^{*}) N^{-}+( N)^{-}\] (27)

_2) Upper bound._ Now we derive the upper bound. To restrict the sensitivity, instead of estimating with (23) directly, now we calculate an average of clipped label values:

\[_{l}=}_{i=1}^{N}(_{i} B_{l} )(Y_{i},T)+W_{l},\] (28)

in which \(W_{l}(2T/(n_{l}))\). Then for all \( B_{l}\), let \(()=_{l}\). The following theorem bounds the excess risk.

**Theorem 12**.: (28) _is \(\)-label DP. Moreover, under Assumption 1 and 3, if \(h\) and \(T\) scales as \(h N^{-}+( N)^{-}\), and \(T( Nh^{d})^{1/p}\), then the excess risk can be bounded by_

\[R_{reg}-R_{reg}^{*} N^{-}+( N )^{-}.\] (29)

The proof of Theorem 11 and 12 follow that of Theorem 7 and 8. The details are shown in Appendix L and M respectively. With \(p=2\), the right hand side of (29) becomes \(( 1)^{-}\), indicating that the privacy constraint blows up the sample complexity by a constant factor. With larger \(p\), the second term in (29) becomes negligible compared with the first one.

The theoretical analyses in this section are summarized as follows. In general, with fixed noise variance, if the label noise is heavy-tailed, while the non-private convergence rates remain unaffected, the additional risk caused by privacy mechanisms becomes significantly higher, indicating the difficulty of privacy protection for heavy-tailed distributions.

## 7 Conclusion

In this paper, we have derived the minimax lower bounds of learning under label DP for both central and local models. Furthermore, we propose methods whose upper bounds match these lower bounds. The results indicate the theoretical limits of learning under the label DP. From these results, it is discovered that under local label DP constraints, the sample complexity blows up by a factor of at least \(O(1/^{2})\). Under central label DP requirements, the additional error caused by privacy mechanisms is significantly smaller. Finally, it is shown that for regression problem with heavy-tailed label distribution, the additional risk induced by privacy requirement becomes inevitably higher.

**Limitations:** The limitations of our work include the following aspects. Some assumptions can be weakened. For example, current analysis assumes that feature distributions have bounded supports, which may be extended to the unbounded case. One can let the bin splitting and nearest neighbor method be adaptive in the tails of features, such as . Moreover, the bounds derived in this paper require that samples increase exponentially with dimensionality. However, in practice, the performance of learning under the label DP can be quite well even in high dimensions. The discrepancy can be explained by the fact that the minimax lower bound considers the worst-case distribution over a wide range of distributions. However, in most realistic cases, the distributions satisfy significantly better properties. A better modeling is to assume that these samples lie on a low dimensional manifold . In this case, it is possible to achieve a much better convergence rate. Finally, it is not sure whether approximate DP (i.e. \((,)\)-DP) can improve the convergence rates.