# Stochastic Optimal Control Matching

Carles Domingo-Enrich

NYU & FAIR, Meta

cd2754@nyu.edu

&Jiequn Han

Flatiron Institute

jhan@flatironinstitute.org

Brandon Amos

FAIR, Meta

bda@meta.com

&Joan Bruna

NYU & Flatiron Institute

bruna@cims.nyu.edu

&Ricky T. Q. Chen

FAIR, Meta

rtqichen@meta.com

###### Abstract

Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that may be of independent interest.

## 1 Introduction

Stochastic optimal control aims to drive the behavior of a noisy system in order to minimize a given cost. It has myriad applications in science and engineering: examples include the simulation of rare events in molecular dynamics , finance and economics , stochastic filtering and data assimilation , nonconvex optimization , sampling , power systems and energy markets , and robotics . Stochastic optimal has also been impactful in fields such as mean-field games , optimal transport , backward stochastic differential equations (BSDEs)  and large deviations . Recently, it has been the basis of algorithms to sample from unnormalized densities .

For continuous-time problems with low-dimensional state spaces, the standard approach to learn the optimal control is to solve the Hamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) by gridding the space and using classical numerical methods. For high-dimensional problems, a large number of works parameterize the control using a neural network and train it applying a stochastic optimization algorithm on a loss function. These methods are known as _Iterative Diffusion Optimization_ (IDO) techniques  (see Subsec. 2.2).

It is convenient to draw an analogy between stochastic optimal control and _continuous normalizing flows_ (CNFs), which are a generative modeling technique where samples are generated by solving an ordinary differential equation (ODE) for which the vector field has been learned, initialized at a Gaussian sample. CNFs were introduced by  (building on top of Rezende and Mohamed ),and training them is similar to solving control problems because in both cases one needs to learn high-dimensional vector fields using neural networks, in continuous time.

The first algorithm developed to train normalizing flows was based on maximizing the likelihood of the generated samples (Kendal et al., 2016, Sec. 4). Obtaining the gradient of the maximum likelihood loss with respect to the vector field parameters requires backpropagating through the computation of the ODE trajectory, or equivalently, solving the _adjoint_ ODE in parallel to the original ODE. Maximum likelihood CNFs (ML-CNFs) were superseded by diffusion models (Kendal et al., 2016; Kendal and Kendal, 2017; Kendal et al., 2018) and flow-matching, a.k.a. stochastic interpolant, methods (Zhu et al., 2017; Chen et al., 2017; Chen et al., 2018; Chen et al., 2018), which are currently the preferred algorithms to train CNFs. Aside from architectural improvements such as the UNet (Kendal et al., 2016), a potential reason for the success of diffusion and flow matching models is that their _functional landscape_ is convex, unlike for ML-CNFs. Namely, vector fields are learned by solving least squares regression problems where the goal is to fit a random matching vector field. Convex functional landscapes in combination with overparameterized models and moderate gradient variance can yield very stable training dynamics and help achieve low error.

Returning to stochastic optimal control, one of the best-performing IDO techniques amounts to choosing the control objective (equation 1) as the training loss (see (12)). As in ML-CNFs, computing the gradient of this loss requires backpropagating through the computation of the trajectories of the SDE (2), or equivalently, using an adjoint method. The functional landscape of the loss is highly non-convex, and the method is prone to unstable training (see green curve in the bottom right plot of Figure 3). In light of this, a natural idea is to develop the analog of diffusion model losses for the stochastic optimal control problem, to obtain more stable training and lower error, and this is what we set out to do in our work. Our contributions are as follows:

* We introduce Stochastic Optimal Control Matching (SOCM), a novel IDO algorithm in which the control is learned by solving a least-squares regression problem where the goal is to fit a random _matching vector field_ which depends on a family of _reparameterization matrices_ that are also optimized.
* We derive a bias-variance decomposition of the SOCM loss (Prop. 2). The bias term is equal to an existing IDO loss: the _cross-entropy loss_, which shows that both algorithms have the same landscape in expectation. However, SOCM has an extra flexibility in the choice of reparameterization matrices, which affect only the variance. Hence, we propose optimizing the reparameterization matrices to reduce the variance of the SOCM objective.
* The key idea that underlies the SOCM algorithm is the _path-wise reparameterization trick_ (Prop. 1), which is a novel technique for estimating gradients of an expectation of a functional of a random process with respect to its initial value. It is of independent interest and may be more generally applicable outside of the settings considered in this paper.
* We perform experiments on four different settings where we have access to the ground-truth control. For three of these, SOCM obtains a lower \(L^{2}\) error with respect to the ground-truth control than all the existing IDO techniques, with around \(10\) lower error than competing methods in some instances.

## 2 Framework

### Setup and Preliminaries

Let \((,,(_{t})_{t 0},)\) be a fixed filtered probability space on which is defined a Brownian motion \(B=(B_{t})_{t 0}\). We consider the control-affine problem

\[_{u}_{0}^{T} {1}{2}\|u(X_{t}^{u},t)\|^{2}\!+\!f(X_{t}^{u},t)\,t\!+\!g(X_{T }^{u}),\] (1) \[\ X_{t}^{u}\!=\!(b(X_{t}^{u},t)\!+\! (t)u(X_{t}^{u},t))\,t\!+\!(t)B _{t},X_{0}^{u} p_{0}\] (2)

and where \(X_{t}^{u}^{d}\) is the state, \(u:^{d}[0,T]^{d}\) is the feedback control and belongs to the set of admissible controls \(\), \(f:^{d}[0,T]\) is the state cost, \(g:^{d}\) is the terminal cost, \(b:^{d}[0,T]^{d}\) is the base drift, and \(:[0,T]^{d d}\) is the invertible diffusion coefficient and \((0,+)\) is the noise level. In App. A we formally define the set \(\) of admissible controls and describe the regularity assumptions needed on the control functions. In the remainder of the section we introduce relevant concepts in stochastic optimal control; we provide the most relevant proofs in App. B and refer the reader to Oksendal (Oksendal, 2016, Chap. 11) and Nusken and Richter (Nusken and Richter, 2019, Sec. 2) for a similar, more extensive treatment.

**Cost functional and value function** The _cost functional_ for the control \(u\), point \(x\) and time \(t\) is defined as \(J(u;x,t):=_{t}^{T}\|u_{s}(X_{s}^{u})\|^{2 }+f_{s}(X_{s}^{u})\,t+g(X_{t}^{u})X_{t}^{u}=x.\) That is, the cost functional is the expected value of the control objective restricted to the times \([t,T]\) with the initial value \(x\) at time \(t\). The _value function_ or _optimal cost-to-go_ at \((x,t)\) is defined as the minimum value of the cost functional across all possible controls:

\[V(x,t):=_{u}J(u;x,t).\] (3)

Hamilton-Jacobi-Bellman equation and optimal controlIf we define the infinitesimal generator \(L:=_{i,j=1}^{d}(^{})_{ij}(t)_{x_{i }}_{x_{j}}+_{i=1}^{d}b_{i}(x,t)_{x_{i}}\), the value function solves the following Hamilton-Jacobi-Bellman (HJB) partial differential equation:

\[(_{t}+L)V(x,t)-\|(^{} V)(x,t)\|^{2}+f(x,t )=0, V(x,T)=g(x).\] (4)

The _verification theorem_[62, Sec. 2.3] states that if a function \(V\) solves the HJB equation above and has certain regularity conditions, then \(V\) is the value function (3) of the problem (1)-(2). An implication of the verification theorem is that for every \(u\),

\[V(x,t)+_{t}^{T}\|^{} V+u\|^{2 }(X_{s}^{u},s)\,s\,\,X_{t}^{u}=x=J(u,x,t).\] (5)

In particular, this implies that the unique optimal control is given in terms of the value function as \(u^{*}(x,t)=-(t)^{} V(x,t)\). Equation (5) can be deduced by integrating the HJB equation (4) over \([t,T]\), and taking the conditional expectation with respect to \(X_{t}^{u}=x\). We include the proof of (5) in App. B for completeness.

A pair of forward and backward SDEs (FBSDEs)Consider the pair of SDEs

\[X_{t} =b(X_{t},t)\,t+(t)B_{t},  X_{0} p_{0},\] (6) \[Y_{t} =(-f(X_{t},t)+\|Z_{t}\|^{2})\,t+ Z_{t},B_{t}, Y_{T}=g(X_{T}).\] (7)

where \(Y:[0,T]\) and \(Z:[0,T]^{d}\) are progressively measurable 1 random processes. It turns out that \(Y_{t}\) and \(Z_{t}\) defined as \(Y_{t}:=V(X_{t},t)\) and \(Z_{t}:=(t)^{} V(X_{t},t)=-u^{*}(X_{t},t)\) satisfy (7). We include the proof in App. B for completeness.

An analytic expression for the value functionFrom the forward-backward equations (6)-(7), one can derive a closed-form expression for the value function \(V\):

\[V(x,t)\!=\!-\!-\!^{-1}_{t}^ {T}f(X_{s},s)\,s\!-\!^{-1}g(X_{T})X_{t}=x,\] (8)

where \(X_{t}\) is the solution of the uncontrolled SDE (6). This is a classical result, but we still include its proof in App. B. Given that \(u^{*}(x,t)=-(t)^{} V(x,t)\), an immediate, yet important, consequence of (8) is the following path-integral representation of the optimal control:

\[u^{*}(x,t)\!=\!(t)^{}_{x} \!-\!^{-1}_{t}^{T}f(X_{s},s)\,s\!-\!^{-1}g( X_{T})X_{t}=x\;.\] (9)

Remark this equation involves the gradient of logarithm of a conditional expectation, which is reminiscent of the vector fields that are learned when training diffusion models. For example, the target vector field for variance-exploding score-based diffusion loss  can be expressed as \(_{x} p_{t}(x)=_{x}_{Y p_{}}[ /(2_{t}^{2}))}{(2_{t}^{2})^{4/2}}]\). Note, however, that in (9) the gradient is taken with respect to the initial condition of the process, which requires the development of novel techniques.

Conditioned diffusionsLet \(=C([0,T];^{d})\) be the Wiener space of continuous functions from \([0,T]\) to \(^{d}\) equipped with the supremum norm, and let \(()\) be the space of Borel probability measures over \(\). For each control \(u\), the controlled process in equation (2) induces a probability measure in \(()\), as the law of the paths \(X_{t}^{u}\), which we refer to as \(^{u}\). We let \(\) be the probability measure induced by the uncontrolled process (6), and define the _work functional_

\[(X,t):=_{t}^{T}f(X_{s},s)\,s+g(X_{T}).\] (10)It turns out (Lemma 2 in App. B) that the Radon-Nikodym derivative \(^{*}}}{d^{}}\) satisfies \(^{*}}}{d^{}}(X)=^{-1}V(X _{0},0)-(X,0)\). Also, a straight-forward application of the Girsanov theorem for SDEs (Cor. 1) shows that

\[}}{dP^{^{*}}}(X^{u^{*}}) = -^{-1/2}_{0}^{T} u^{*}(X_{t}^{u^{*} },t)-u(X_{t}^{u^{*}},t),B_{t}-}{2}_{0} ^{T}\|u^{*}(X_{t}^{u^{*}},t)-u(X_{t}^{u^{*}},t)\|^{2}\,t,\] (11)

which means that the only control \(u\) such that \(^{u}=^{u^{*}}\) is the optimal control itself.

### Existing approaches and related work

Low-dimensional case: solving the HJB equationFor low-dimensional control problems (\(d 3\)), it is possible to grid the domain and use a numerical PDE solver to find a solution to the HJB equation (4). The main approaches include _finite difference methods_[11; 57; 4], which approximate the derivatives and gradients of the value function using finite differences, _finite element methods_, which involve restricting the solution to domain-dependent function spaces, and semi-Lagrangian schemes [21; 13; 12], which trace back characteristics and have better stability than finite difference methods. See Greif  for an overview on these techniques, and Banas et al.  for a comparison between them. Hutzenthaler et al.  introduced the multilevel Picard method, which leverages the Feynman-Kac and the Bismut-Elworthy-Li formulas to beat the curse of dimensionality in some settings [6; 46; 45; 43].

High dimensional methods leveraging FBSDEsThe FBSDE formulation in equations (6)-(7) has given rise to multiple methods to learn controls. One such approach is _least-squares Monte Carlo_ (see Pham [63, Chapter 3] and Gobet  for an introduction, and Gobet et al. , Zhang et al.  for an extensive analysis), where trajectories from the forward process (6) are sampled, and then regression problems are solved backwards in time to estimate the expected future cost in the spirit of dynamic programming. A second method that exploits FBSDEs was proposed by E et al. , Han et al. . They parameterize the control using a neural network \(u_{}\), and use stochastic gradient algorithms to minimize the loss \((u_{},y_{0})=[(Y_{T}(y_{0},u_{})-g(X_{T}))^{2}]\), where \(Y_{T}(y_{0},u_{})\) is the process in (7) with initial condition \(y_{0}\) and control \(u_{}\). This algorithm can be seen as a shooting method, where the initial condition and the control are learned to match the terminal condition. Multiple recent works have combined neural networks with FBSDE Monte Carlo methods for parabolic and elliptic PDEs [5; 18; 86], control [7; 39], multi-agent games [34; 15; 16]; see  for a more comprehensive review.

Many of the methods referenced above and some additional ones can be seen from a common perspective using controlled diffusions. As observed in equation (11), the key idea is that learning the optimal control is equivalent to finding a control \(u\) such that the induced probability measure \(^{u}\) on paths is equal to the probability measure \(^{u^{*}}\) for the optimal control. In the paragraphs below we cover several loss that fall into this framework. All the losses below can be optimized using a common algorithmic framework, which we describe in Algorithm 1. For more details, we refer the reader to Nusken and Richter , which introduced this perspective and named such methods _Iterative Diffusion Optimization_ (IDO) techniques. For simplicity, we introduce the losses for the setting in which the initial distribution \(p_{0}\) is concentrated at a single point \(x_{}\); we cover the general setting in App. B.

The relative entropy loss and the adjoint methodThe relative entropy loss is defined as the Kullback-Leibler divergence between \(^{u}\) and \(^{u^{*}}_{^{u}}[^{u}}{ d^{u^{*}}}]\). Upon removing constant terms and factors, this loss is equivalent to (see Lemma 3 in App. B):

\[_{}(u):=_{0}^{T} \|u(X_{t}^{u},t)\|^{2}+f(X_{t}^{u},t)\,t+g(X_{T}^{u}).\] (12)

This is exactly the control objective in (1). This fact has been studied extensively [10; 31; 36; 48; 67]. Hence, the relative entropy loss is very natural and widely used; see Onken et al. , Zhang and Chen  for examples on multiagent systems and sampling.

Solving optimization problems of the form (12) has a long history that dates back to Pontryagin . Note that \(_{}(u)\) depends on \(u\) both explicitly, and implicitly through the process \(X^{u}\). To compute the gradient \(_{}}_{}(u_{_{n}})\) of a Monte Carlo approximation \(}_{}(u_{_{n}})\) of \(_{}(u_{_{n}})\) as required by Algorithm 1, we need to backpropagate through the simulation of the trajectories, which is why we do _not_ detach them from the computational graph. One can alternatively compute the gradient \(_{}}_{}(u_{_{n}})\) by explicitly solving an ODE, a technique known as the _adjoint method_. The adjoint method was introduced by Pontryagin , popularized in deep learning by Chen et al. , and further developed for SDEs in Li et al. .

The cross-entropy lossThe cross-entropy loss is defined as the Kullback-Leibler divergence between \(^{u^{*}}\) and \(^{u}\), i.e., flipping the order of the two measures: \(_{^{u^{*}}}[^{u^{*}}}{d^{u }}]\). For an arbitrary \(v\), this loss is equivalent to the following one (see Prop. 3(i) in App. B):

\[_{}(u)&:= -^{-1/2}_{0}^{T} u(X_{t}^{v},t), B_{t}-^{-1}_{0}^{T} u(X_{t}^{v},t),v(X_{t}^ {v},t)\,t+}{2}_{0}^{T}\|u(X_{t}^{v},t) \|^{2}\,t\\ &-^{-1}(X^{v},0)- ^{-1/2}_{0}^{T} v(X_{t}^{v},t),B_{t}- }{2}_{0}^{T}\|v(X_{t}^{v},t)\|^{2}\,t .\] (13)

The cross-entropy loss has a rich literature  and has been recently used in applications such as molecular dynamics . Furthermore, we note that the cross-entropy loss can be significantly simplified and written in terms of the unnormalized \(L^{2}\) error of the control \(u\) with respect to the optimal control \(u^{*}\):

\[_{}(u)=}{2}_{0}^ {T}\|u^{*}(X_{t}^{u^{*}},t)-u(X_{t}^{u^{*}},t)\|^{2}\,t -^{-1}V(X_{0}^{u^{*}},0)\;.\] (14)

This characterization, which is proven in Prop. 3(ii) in App. B, is relevant for us because a similar one can be written for the loss that we propose (see Prop. 2).

Variance and log-variance lossesFor an arbitrary \(v\), the _variance_ and the _log-variance losses_ are defined as \(}_{_{v}}(u)=_{^{v}}( ^{u^{*}}}{^{u}})\) and \(}_{_{v}}^{}(u)=_{^{v}} (^{u^{*}}}{^{u}})\) whenever \(_{^{v}}|^{u^{*}}}{ ^{u}}|<+\) and \(_{^{v}}|^{u^{*}}}{d^{u}}|<+\), respectively. Define

\[&_{T}^{u,v}=-^{-1}_{0}^{T}  u(X_{t}^{v},t),v(X_{t}^{v},t)\,t\\ &-^{-1}_{0}^{T}f(X_{t}^{v},t)\,t- ^{-1/2}_{0}^{T} u(X_{t}^{v},t),B_{t}\\ &+}{2}_{0}^{T}\|u(X_{t}^{v},t)\|^{2} \,t.\] (15)

Then, \(}_{_{v}}\) and \(}_{_{v}}^{}\) are equivalent, respectively, to the following losses (see Lemma 4):

\[_{_{v}}(u) :=_{T}^{u,v}-^{-1}g( X_{T}^{v}),\] (16) \[_{_{v}}^{}(u) :=_{T}^{u,v}-^{-1}g(X_{T}^{v}) ,\] (17)

The variance and log-variance losses were introduced by Nusken and Richter . Unlike for the cross-entropy loss, the choice of the control \(v\) does lead to different losses. When using \(_{_{v}}\) or \(_{_{v}}^{}\) in Algorithm 1, the variance is computed across the \(m\) trajectories in each batch.

Moment lossFor an arbitrary \(v\), the moment loss is defined as

\[_{_{v}}(u,y_{0})=[(_{T}^{u,v}+y_{0}- ^{-1}g(X_{T}^{v}))^{2}],\] (18)where \(_{T}^{u,v}\) is defined in (15). Note the similarity with the log-variance loss (17); the optimal value of \(y_{0}\) for a fixed \(u\) is \(y_{0}^{*}=[^{-1}g(X_{T}^{v})-_{T}^{u,v}]\), and plugging this into (18) yields exactly the log-variance loss. The moment loss was introduced by Hartmann et al. [39, Section III.B], and it is a generalization of the FBSDE method pioneered by E et al. , Han et al.  and referenced earlier in this subsection, which corresponds to setting \(v=0\).

## 3 Stochastic Optimal Control Matching

In this section we present our loss, _Stochastic Optimal Control Matching_ (SOCM). The corresponding method, which we describe in Algorithm 2, falls into the class of IDO techniques described in Subsec. 2.2. The general idea is to leverage the analytic expression of \(u^{*}\) in (9) to write a least squares loss for \(u\), and the main challenge is to reexpress the gradient of a conditional expectation with respect to the initial condition of the process. We do that using a novel technique which introduces certain arbitrary matrix-valued functions \(M_{t}\), that we also optimize.

**Theorem 1** (SOCM loss).: _For each \(t[0,T]\), let \(M_{t}:[t,T]^{d d}\) be an arbitrary matrix-valued differentiable function such that \(M_{t}(t)=\). Let \(v\) be an arbitrary control. Let \(_{}:L^{2}(^{d}[0,T];^{d})  L^{2}([0,T]^{2};^{d d})\) be the loss function defined as_

\[_{}(u,M):=_{0}^{T} u(X_{t}^{v},t)-w(t,v,X^{v},B,M_{t})^{2}\,t (v,X^{v},B)\,,\] (19)

_where \(X^{v}\) is the process controlled by \(v\) (i.e., \(dX_{t}^{v}=(b(X_{t}^{v},t)+(t)v(X_{t}^{v},t))\,t+(t)\,B_{t}\) and \(X_{0}^{v} p_{0}\)), and_

\[w(t,v,X^{v},B,M_{t}) =(t)^{}-_{t}^{T}M_{t}(s)_{x}f(X_{s}^{v },s)\,s-M_{t}(T) g(X_{T}^{v})\] \[+_{t}^{T}(M_{t}(s)_{x}b(X_{s}^{v},s)-_{s}M _{t}(s))(^{-1})^{}(s)v(X_{s}^{v},s)\,s\] \[+^{1/2}_{t}^{T}(M_{t}(s)_{x}b(X_{s}^{v},s)- _{s}M_{t}(s))(^{-1})^{}(s)B_{s},\] \[(v,X^{v},B) =-^{-1}_{0}^{T}f(X_{t}^{v},t)\,s -^{-1}g(X_{T}^{v})\] (20) \[-^{-1/2}_{0}^{T} v(X_{t}^{v},t), B_{t}-}{2}_{0}^{T}\|v(X_{t}^{v},t)\|^{ 2}\,t.\]

\(_{}\) _has a unique optimum \((u^{*},M^{*})\), where \(u^{*}\) is the optimal control._

We refer to \(M=(M_{t})_{t[0,T]}\) as the family of _reparametrization matrices_, to the random vector field \(w\) as the _matching vector field_, and to \(\) as the _importance weight_. We present a proof sketch of Thm. 1; the full proofs for all the results in this section are in App. C.

**Proof sketch of Thm. 1** Let \(X\) be the uncontrolled process (6). Consider the loss

\[}(u) =_{0}^{T}u(X_{t},t)-u^{* }(X_{t},t)^{2}\,t\,-^{-1}_{0}^{T}f(X_{ t},t)\,t-^{-1}g(X_{T})\] (21) \[=_{0}^{T}u(X_{t},t)^{2}-2 u(X_{t},t),u^{*}(X_{t},t)+\|u^{*}(X_{t},t) ^{2}\,t\] \[-^{-1}_{0}^{T}f(X_{t},t) \,t-^{-1}g(X_{T}).\]

Clearly, the only optimum of this loss is the optimal control \(u^{*}\). Using the analytic expression of \(u^{*}\) in (9), the cross-term can be rewritten as (see Lemma 5 in App. C):

\[_{0}^{T} u(X_{t},t),u^{*}( X_{t},t)\,t\,-^{-1}_{0}^{T}f(X_{t},t)\, t-^{-1}g(X_{T})\] (22) \[=_{0}^{T}u(X _{t},t),(t)^{}_{x}-^{-1}_ {t}^{T}f(X_{s},s)\,s-^{-1}g(X_{T})X_{t}=x \] \[-^{-1}_{0}^{t}f(X_{s},s)\, s\,t.\]

It remains to evaluate the conditional expectation \(_{x}-^{-1}_{t}^{T}f(X_{s},s)\, s-^{-1}g(X_{T})X_{t}=x\), which we do by a "reparameterization trick" that shifts the dependence on the initial value \(x\) into the stochastic processes--here we introduce a free variable \(M_{t}\)--and then applying Girsanov theorem. We coin this the _path-wise reparameterization trick_:

**Proposition 1** (Path-wise reparameterization trick for stochastic optimal control).: _For each \(t[0,T]\), let \(M_{t}:[t,T]^{d d}\) be an arbitrary continuously differentiable function matrix-valued function _such that \(M_{t}(t)=\). We have that_

\[&_{x}-^{-1} _{t}^{T}f(X_{s},s)\,s-^{-1}g(X_{T})X_{t}=x \\ &=-^{-1}_{t}^{T}M_{t}(s) _{x}f(X_{s},s)\,s-^{-1}M_{t}(T) g(X_{T})\\ &+^{-1/2}_{t}^{T}(M_{t}(s)_{ x}b(X_{s},s)-_{s}M_{t}(s))(^{-1})^{}(s)B_{s}\\ &-^{-1}_{t}^{T}f( X_{s},s)\,s-^{-1}g(X_{T})X_{t}=x.\] (23)

We prove a more general form of this result (Prop. 4) in Subsec. C.2 and also provide an intuitive derivation in Subsec. C.3. In the proof of Prop. 4, the reparameterization matrices \(M_{t}\) arise as the gradients of a perturbation to the process \(X_{t}\). Similar ideas can potentially be applied to derive losses for generative modeling. If we plug (23) into the right-hand side of (22), and then this back into (21), and we complete the square, we obtain that for some constant \(K\) independent of \(u\),

\[}(u)=_{0}^{T}u(X_{t},t)+(t)_{t}^{T}M_{t}(s)_{x}f(X_{s},s)\,s+M_{t}(T) g(X_{T})\\ &-^{1/2}_{t}^{T}(M_{t}(s)_{x} b(X_{s},s)-_{s}M_{t}(s))(^{-1})^{}(s)B_{s} ^{2}\,t\\ &-^{-1}_{0}^{T}f(X _{t},t)\,t-^{-1}g(X_{T})+K.\]

If we perform a change of process from \(X\) to \(X^{v}\) applying the Girsanov theorem (Cor. 1 in App. C), we obtain the loss \(_{}(u,M)\). 

The following result clarifies the role of reparameterization matrices, connecting the SOCM and cross-entropy losses.

**Proposition 2** (Bias-variance decomposition of the SOCM loss).: _The SOCM loss decomposes into a bias term that only depends on \(u\) and a variance term that only depends on \(M\):_

\[_{}(u,M)=(w;M)}_{}_{}}+_{0}^{T}u(X_{t}^{u^{*}},t) -u^{*}(X_{t}^{u^{*}},t)^{2}\,t\,e^{-^{-1}V(X_{0}^{u^ {*}},0)}}_{},\] (24)

_where_

\[(w;M)\!=\! {T}\!_{0}^{T}w(t,v,X^{v},B,M_{t})\!- [w(t,v,X^{v},B,M_{t})(v,X^{v},B)|X_{t}^{v},t)}{[(v,X^{ v},B)|X_{t}^{v},t)}\|^{2}\,t\,(v,X^{v},B)}_{u^{*}(X_{t}^{v},t)} ^{2}\,t\,(v,X^{v},B).\] (25)

Remark that the bias term in equation (24) is equal to the characterization of the cross-entropy loss in (14). In other words, the landscape of \(_{}(u,M)\) with respect to \(u\) is the landscape of the cross-entropy loss \(_{}(u)\). Thus, the SOCM loss can be seen as some form of variance reduction method for the cross-entropy loss, and performs substantially better experimentally (Sec. 4). Yet, the expressions of the SOCM loss and the cross-entropy loss are very different; the former is a least squares loss and is expressed in terms of the gradients of the costs.

```
0: State cost \(f(x,t)\), terminal cost \(g(x)\), diffusion coeff. \((t)\), base drift \(b(x,t)\), noise level \(\), number of iterations \(N\), batch size \(m\), number of time steps \(K\), initial control parameters \(_{0}\), initial matrix parameters \(_{0}\), loss \(_{}\) in (19)
1for\(n\{0,,N-1\}\)do
2: Simulate \(m\) trajectories of the process \(X^{v}\) controlled by \(v=u_{_{n}}\), e.g., using Euler-Maruyama updates
3: Detach the \(m\) trajectories from the computational graph, so that gradients do not backpropagate
4: Using the \(m\) trajectories, compute an \(m\)-sample Monte-Carlo approximation \(}_{}(u_{_{n}},M_{_{n}})\) of the loss \(_{}(u_{_{n}},M_{_{n}})\) in (19)
5: Compute the gradients \(_{(,)}}_{}(u_{_{n}},M_{ _{n}})\) of \(}_{}(u_{_{n}},M_{_{n}})\) at \((_{n},_{n})\)
6: Obtain \(_{n+1}\), \(_{n+1}\) with via an Adam update on \(_{n}\), \(_{n}\), resp.
7:
8:
9:
10: end for Output: Learned control \(u_{_{N}}\) ```

**Algorithm 2** Stochastic Optimal Control Matching (SOCM)

For good training performance, it is critical that the gradients have high signal-to-noise ratio. Looking at the SOCM loss, a good proxy for low gradient variance is to have low variance for \(_{0}^{T}u(X_{t}^{v},t)-w(t,v,X^{v},B,M_{t})^{2}\, t(v,X^{v},B)\), and this holds when both \((v,X^{v},B)\) and \(w(t,v,X^{v},B,M_{t})\) have low variance. Next, we present strategies to lower the variance of these two objects.

Minimizing the variance of the importance weight \(\)We want to use a vector field \(v\) such that \([(v,X^{v},B)]\) is as low as possible. As shown by the following lemma, which is well-known in the literature, setting \(v\) to be the optimal control \(u^{*}\) actually achieves variance zero when we condition on the starting point of the controlled process \(X^{v}\). The proof of this result can be found in Hartmann et al. , but we include it in Subsec. C.5 for completeness.

**Lemma 1**.: _When we set \(v=u^{*}\), the conditional variance \([(v,X^{v},B)|X^{v}_{0}=x_{}]\) is zero for any \(x_{}^{d}\)._

Of course, we do not have access to the optimal control \(u^{*}\), but it is still a good idea to set \(v\) as the closest vector field to \(u^{*}\) that we have access to, which is typically the currently learned control. In some instances, one may benefit from using a warm-started control parameterized as \(u_{}(x,t)+u_{}(x,t)\), where the warm-start \(u_{}\) is a reasonably good control obtained via a different strategy (see App. E).

Minimizing the variance of the matching vector field \(w\)We are interested in finding the family \(M=(M_{t})_{t[0,T]}\) that minimizes the variance of \(w(t,v,X^{v},B,M_{t})\) conditioned on \(t\) and \(X_{t}\). Note that this is exactly the term \((w;M)\) in the right-hand side of equation (24). Since \((w;M)\) does not depend on the specific \(v\), the optimal \(M\) does not depend on \(v\) either. And since the second term in the right-hand side of equation (24) does not depend on \(M=(M_{t})_{t[0,T]}\), minimizing \((w;M)\) is equivalent to minimizing \((u)\) with respect to \(M\).

Parameterizing the matrices \(M_{t}\) vs solving for the optimal matricesIn practice, we parameterize the matrices \((M_{t})_{t[0,T]}\) using a function \(M_{}\) with two arguments \((t,s)\). To enforce that \(M_{}(t,t)=\), we set \(M_{}(t,s)=e^{-(s-t)}+(1-e^{-(s-t)})_{ }(t,s)\), where \(=(,)\), and \(_{}:^{d  d}\) is an unconstrained neural network. Alternatively, Thm. 4 in App. D shows that the optimal family \(M^{*}=(M^{*}_{t})_{t[0,T]}\) can be characterized as the solution of a linear equation in infinite dimensions (a Fredholm equation of the first kind). The discretized linear system has \(d^{2}K\) equations and variables, \(K\) being the number of discretization time points. However, since the optimal \(M^{*}\) does not depend on \(v\) (see Remark 1), this is a computation that must be done only once and that may be affordable in some settings. We did not test this approach experimentally.

## 4 Experiments

We consider four experimental settings that we adapt from Nusken and Richter : Quadratic Ornstein Uhlenbeck (easy), Quadratic Ornstein Uhlenbeck (hard), Linear Ornstein Uhlenbeck and Double Well. We describe them in detail in App. F. For all of them, we have access to the ground-truth optimal control, which means that we are able to estimate the \(L^{2}\) error incurred by the learned control \(u\). In Figure 2 we plot the control \(L^{2}\) error for each IDO algorithm described in Subsec. 2.2, and for the SOCM algorithm (Algorithm 2), for the Quadratic OU (easy) and (hard) settings. We also include two ablations of SOCM: _(i)_ a version of SOCM where the reparameterization matrices \(M_{t}\) are set fixed to the identity \(I\), _(ii)_ SOCM-Adjoint, where we estimate the conditional expectation in equation (23) using the adjoint method for SDEs instead of the path-wise reparameterization trick (see Subsec. C.4). Code can be found at https://github.com/facebookresearch/SOC-matching.

At the end of training, SOCM obtains the lowest \(L^{2}\) error, improving over all existing methods by a factor of around ten. The two SOCM ablations come in second and third by a substantial difference, which underlines the importance of the path-wise reparameterization trick. The best among existing methods is the adjoint method (the relative entropy loss). In Figure 2 (_bottom_) we show the squared norm of the gradient of each loss with respect to the parameters \(\) of the control: algorithms with small noise variance have low error values.

In Figure 3, we plot the control \(L^{2}\) error for Linear Ornstein Uhlenbeck and Double Well. For Linear OU, the error is around five times smaller for SOCM than for any existing method. For Double Well, the SOCM algorithm achieves the third smallest error, slightly behind the variance loss and the adjoint method, but the latter shows instabilities. As we show in Figure 9 in App. F, these instabilities are inherent to the adjoint method and they do not disappear for small learning rates. Both in Figure 2 and Figure 3, we observe that learning the reparameterization matrices is critical to obtain gradient estimates with high signal-to-noise ratio. Double Well is a particularly interesting and challenging setting because its solution is highly multimodal: \(g\) has 1024 modes. Multimodalityis a feature observed in realistic settings, and is hard to handle because it involves learning the control correctly in each mode.

The costs \(f\) and \(g\) and the base drift \(b\) for Quadratic OU (hard) are five times those of Quadratic OU (easy). Consequently, the factor \((v,X^{v},B)\) initially has a much larger variance for the SOCM methods, and for cross-entropy. As training progresses, \(u_{_{n}}\) gets closer to \(u^{*}\), and consequently the variance of \((v,X^{v},B)\) decreases, which in turn makes learning easier. This explains the initial slow decrease in the control error, followed by a fast drop that places SOCM well below existing algorithms. In App. E, we showcase a control warm-start strategy that can help and speed up convergence.

We also present experimental results on two-mode Gaussian mixture sampling in increasing dimension, using the Path Integral Sampler . We take Gaussians with means that are 2 units apart, and identity variance. Figure 1 shows control objective estimates obtained after running the Adjoint, SOCM, and Cross-entropy algorithms for 40000 iterations, at dimensions \(d=2,8,16,32,64\), and error bars show standard errors. By Theorem 4 of , we know that the optimal value of the control objective is zero; Figure 1 shows the suboptimality gaps incurred by each algorithm.

Cross-entropy, which uses the same importance weight as SOCM, performs worse than the other two losses for all dimensions, and its results are particularly poor for dimension 64, because the variance of \(\) is too large for learning to happen. In this case, we see that SOCM has better variance reduction than cross-entropy, despite both using importance weighted objectives for training. We observe that the values for SOCM are slightly below that of Adjoint for most dimensions, which confirms that our method is better for this range of dimensions. If we keep increasing the dimension, SOCM also fails due to higher variance of \(\): for \(n=128\), the control objective estimates for the Adjoint, SOCM, and Cross-Entropy losses are \(0.146 0.001\), \(7.49 0.01\), and \(12.61 0.02\), respectively.

## 5 Conclusion

Our work introduces Stochastic Optimal Control Matching, a novel Iterative Diffusion Optimization technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least-squares problem by trying to fit a matching vector field. The training loss is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. Optimizing the reparameterization matrices reduces the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all existing IDO techniques in four settings.

One of the key ideas for deriving the SOCM algorithm is the path-wise reparameterization trick, a novel technique to obtain low-variance estimates of the gradient of the conditional expectation of a functional of a random process with respect to its initial value. An interesting future direction is to use the path-wise reparameterization trick to decrease the variance of the matching vector field for diffusion models. The main roadblock when we try to apply SOCM to more challenging problems is that the variance of the factor \((v,X^{v},B)\) explodes when \(f\) and/or \(g\) are large, or when the dimension \(d\) is high. The large variance of \(\) is due to the mismatch between the probability measures induced by the learned control and the optimal control, and it decreases as the learned control approaches the optimal control.

The research presented is foundational, but it may serve as the basis of algorithms that improve the quality of generative models.

Figure 1: This plot shows the control objective values for different algorithms (Adjoint, SOCM, and Cross-entropy) across multiple dimensions, with error bars indicating the standard deviations. The y-axis is restricted to [0, 0.1] for better visibility of the lower range values; cross-entropy takes value \(2.915 0.008\) at \(d=64\).

Funding disclosureFunded by respective author affiliations.