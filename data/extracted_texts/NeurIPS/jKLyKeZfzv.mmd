# MOTE-NAS: Multi-Objective Training-based Estimate

for Efficient Neural Architecture Search

 Yu-Ming Zhang\({}^{1}\) Jun-Wei Hsieh\({}^{2}\) Xin Li\({}^{3}\) Ming-Ching Chang\({}^{3}\)

Chun-Chieh Lee\({}^{1}\) Kuo-Chin Fan\({}^{1}\)

\({}^{1}\)National Central University \({}^{3}\)University at Albany

\({}^{2}\)National Yang Ming Chiao Tung University

108522036@g.ncu.edu.tw; jwisheh@nycu.edu.tw

###### Abstract

Neural Architecture Search (NAS) methods seek effective optimization toward performance metrics regarding model accuracy and generalization while facing challenges regarding search costs and GPU resources. Recent Neural Tangent Kernel (NTK) NAS methods achieve remarkable search efficiency based on a training-free model estimate. However, they overlook the non-convex nature of the DNNs in the search process. In this paper, we develop **Multi-Objective Training-based Estimate (MOTE) for efficient NAS**, retaining search effectiveness and achieving the new state-of-the-art in the accuracy and cost trade-off. To improve NTK and inspired by the Training Speed Estimation (TSE) method, MOTE is designed to model the actual performance of DNNs from macro to micro perspective by drawing the loss landscape and convergence speed simultaneously. Using two _reduction strategies_, the MOTE is generated based on a reduced architecture and a reduced dataset. Inspired by evolutionary search, our iterative ranking-based, coarse-to-fine architecture search is highly effective. Experiments on NASBench-201 show MOTE-NAS achieves 94.32% accuracy on CIFAR-10, 72.81% on CIFAR-100, and 46.38% on ImageNet-16-120, outperforming NTK-based NAS approaches. An evaluation-free (EF) version of MOTE-NAS delivers high efficiency in only **5 minutes**, delivering a model more accurate than KNAS.

## 1 Introduction

Neural Architecture Search (NAS)  tackles the challenge of automating the design and search for suitable neural network architectures in many applications. NAS approaches mainly comprise two stages: a model _search stage_ dedicated to identifying promising candidates within the architecture search space, and an _evaluation stage_ where candidate performance is assessed. In the search stage, the search space can be exponentially large. To reduce search complexity, the cell-based tabular search space  is widely considered. Reinforcement learning  and evolutionary algorithms  are also used to accelerate the search process. However, the primary cost of NAS lies in the evaluation stage, where candidate models must undergo intensive training until convergence to obtain a precise performance assessment. This incurs significant time costs (_e.g._, NASBench-201 requires 3-10K GPU seconds for convergence after 200 epochs). So, various _proxy estimates_ (e.g., zero-cost proxy  and training speed estimation ) have been developed to rank candidates, mitigating computational demands for model evaluation.

Recently, several estimates based on NTK have been proposed, including TE-NAS , KNAS , and Eigen-NAS . NTK-based estimates serve as condensed representations of gradients and their correlations. The NTK theory aims to macro-model the actual performance of Deep Neural Networks (DNNs). It assumes that the performance of an infinite-width DNN can be fully described by theNTK at initialization and that the NTK's value remains unchanged after training . Consequently, NTK-based estimates can predict the actual performance of candidate architectures without training. However, real DNNs have limited width and exhibit a highly non-convex nature, leading the NTK to encounter significant nonlinear changes during training and resulting in limitations in accurately predicting the actual performance of candidates. Fig. 1 shows the NTK suffers unstable performance during training.

To address this macro-modeling issue, we propose a novel _landscape term_ that leverages the idea from the study  to linearly combine the differences between the initial weights and the post-training weights of the candidate architectures, which allows us to capture the non-convex nature of these candidates by the landscape slice. If the landscape is flatter, the candidate's performance tends to be better, as it more readily converges to the global optima. Furthermore, studies from a micro-aspect have been conducted to model this issue, such as Snip , Grasp , and SynFlow , which use gradient change to predict the performance of candidate architectures. As the gradient is integrated into the model training, it may reflect the current effectiveness of changes in the model training. Similarly, TSE  directly sums up the training loss to represent the convergence speed to predict candidate performance. In summary, these methods are more intuitive. Although they may not theoretically capture the macroscopic non-convex nature of DNNs, in practice, as shown in Fig. 1, the performance TSE even exceeds NTK. This observation inspired us to propose a _speed term_ that sums the training loss per unit of time, providing a microscopic description of the convergence speed of candidates.

This paper introduces a Multi-Objective Training-based Estimate (MOTE) that considers _landscape term_ for macroscopic view and _speed term_ for microscopic view to predict the performance of candidates in a joint optimization. This dual perspective offers a comprehensive consideration and an accurate estimate for candidates. Furthermore, we introduce two reduction strategies to reduce the time costs by generating MOTE, which consists of landscape and speed terms. We also present MOTE by integrating the evolutionary algorithm , named MOTE-NAS. This design is based on a coarse-to-fine iterative procedure for architecture search. In the search stage of MOTE-NAS, we maintain several dozen to several hundred promising candidate architectures in the pool. In the evaluation stage, we first select the top-\(K\) architectures based on the MOTE, then select the best architecture by the early stopping version of the test accuracy. We further develop a stripped-down, evaluation-free version named MOTE-NAS-EF, which achieves high efficiency and can finish a NAS run in merely eight minutes.

MOTE-NAS outperforms mainstream NTK-based NAS methods. Fig. 2 compares it with TENAS , KNAS , Eigen-NAS , and Label-Gradient Alignment (LGA)  on CIFAR-100 of NASBench-201. The accuracy of the final architecture discovered by MOTE-NAS is significantly superior to other methods. In our experiments, MOTE-NAS achieved 94.32% accuracy on CIFAR-10, 72.81% on CIFAR-100, and 46.38% on ImageNet-16-120. The evaluation-free version, MOTE-NAS-EF, achieves results comparable to KNAS's, where the search is completed in only eight minutes. The technical contributions of this work are summarized as follows:

* Our proposed NAS approach utilizes an efficient training-based estimate to optimize landscape view and convergence speed objectives jointly. This design comprehensively captures the non-convex nature of DNNs from a macro perspective and monitors the convergence speed from a micro perspective, enabling precise actual performance predictions for desired architectures.

Figure 1: Post-training rank correlation for randomly chosen 1000 candidates on NASBench-201. The predictive performance of the proposed two terms gradually improves as epochs increase.

* To enable lightweight training-based estimates, we introduce two reduction strategies for speeding up MOTE generation. Unlike other benchmarks, a reduced meta-architecture is used, and the proposed reduced dataset is built by selecting representative labels of CIFAR-100.
* Our MOTE-NAS achieves a new state-of-the-art for NAS in the accuracy-cost plot (refer to Fig. 2(b)). A stripped-down, evaluation-free version of MOTE-NAS is highly efficient, with performance of the resulting model still outperforming some NTK-based methods, such as the KNAS.

## 2 Related Work

**DARTS.** Instead of exploring a discrete set of architectures, the _Differentiable ARchiTecture Search (DARTS)_ transforms the combinatorial challenge of finding optimal operations into a continuous optimization problem within a differentiable search space. A notable challenge with DARTS is the potential dominance of easily optimized operators, such as skip-connections and pooling operations in the early stages. This issue impedes selecting more potent operations like convolutions with large kernels. In [7; 22], a robust prior is introduced to restrict the number of skip connections within a cell to a predefined value. The progressive search strategy employed in P-DARTS  gradually increases network depth and refines candidate operations based on mixed operation weight. DARTS methods are efficient when running with limited computational resources. However, the architecture found comes with stability and generalizability issues. Furthermore, DARTS algorithms often prefer shallow and wide structures .

On the other hand, NAS methods generally comprise two stages: the search stage and the evaluation stage. The former focuses on collecting promising candidate architectures, while the latter involves assessing the performance of these candidate architectures.

**Search Stage.** Numerous studies have concentrated on the search stage. Some NAS methods leverage reinforcement learning [52; 2; 39], while others are rooted in evolutionary algorithms [27; 31; 33; 46; 9]. The cell-based tabular search space [49; 10; 38] is effective in reducing exhaustion search into a more manageable scale, using a meta-architecture with predefined operations, hyperparameters, filters, and strides. The candidate architectures under consideration range from tens to hundreds of thousands of candidate architectures. Recently, **predictor-based** approaches [24; 28; 43; 11; 42; 45; 15] have gained popularity. These methods construct predictors trained with architecture-accuracy pairs to forecast the performance of a candidate architecture. These predictors encompass a range of models from graph convolutional networks  to MLPs and other regression models. However, obtaining a high-quality set of architecture-accuracy pairs for NAS is non-trivial.

**Evaluation Stage.** Compared with the cost of the search stage, the burden of NAS mainly resides in the time-consuming evaluation process. Various studies have proposed proxy estimates to reduce the need for a real performance evaluation. A prominent approach is zero/few-cost estimate [1; 34;

Figure 2: (a) The **landscape term** draws the slice of loss landscape to capture its macroscopic non-convex nature of the candidate architecture. The **speed term** analyzes the training changes over the training time, providing microscopic insights into the candidate’s convergence speed. (b) Comparison of **MOTE-NAS** and an **Evaluation-Free version MOTE-NAS-EF** against other recent efficient NAS methods on NASBench-201 (CIFAR-100).

23; 29; 6; 47; 51; 30], which substitutes performance indicators such as accuracy with alternative estimates. The zero-cost proxy  introduces zero-cost performance estimates [20; 41; 40] and TSE  introduces a training speed estimate. More recently, Neural Tangent Kernel (NTK)-based estimates [6; 47; 51; 30] have gained popularity based on the assumption that DNNs can predict their convergence performance at initialization. However, it is experimentally found in  that NTKs cannot capture the non-linear characteristics of DNN training dynamic well. Recent NAS methods integrate multiple approaches to achieve remarkable performance. For instance, OMNI  and ProxyBO  propose few-cost NAS methods by combining zero-cost estimates with more resource-intensive techniques like Bayesian optimization and performance predictors.

## 3 Mote-Nas

### Multi-Objective Training-based Estimate

NTK theory tries to describe gradient change by a macro-perspective, but its fundamental assumption about infinite-width DNN cannot fit the real DNNs that have finite width. For example, in KNAS , GKH asserts the existence of a gradient feature that can serve as a coarse-grained proxy to support downstream training when evaluating randomly initialized architectures. However, this does not propose a concrete solution to identify such a non-linear gradient feature during training. LGA finds that the sensitivity for weight initialization that leads to NTK cannot perform stably, demonstrating that the value of NTK does not change . In addition, Fig. 1 shows that NTK do not accurately predict the actual performance of candidates in practice. Despite NTK-based estimates, there are estimates that make predictions by micro-perspective. Among them, TSE sums up the training loss as a proxy estimate to represent the convergence speed. The convergence speed as an important factor for model performance has been extensively discussed in the literature [13; 16; 34]. Specifically, these studies inspired the proposal of MOTE. MOTE introduces a new _landscape term_ to capture the non-convex nature of models by a macro-perspective through the linear combination between two weights to observe the loss landscape. Simultaneously, MOTE introduces another new _speed term_ to measure the convergence speed of the model by a micro-perspective through calculating the unit time training loss. Incorporating the multi-objectives enables MOTE to comprehensively describe candidates' non-convex nature and convergence speed from a macro-to-micro perspective.

**Landscape Term.** In order to capture non-convexity of loss landscape by macro-perspective, we introduce _landscape term_ that linearly combines the two weights before and after few-training to interpolate the weights for intermediate state, and then calculates the loss values (cross-entropy) of these weights, which means to cutting a section from loss landscape to observe its nature , so we sum these loss values to determine whether the loss landscape is smooth, detailed follows.

Let \(\) denote the trained weights of the candidate. To macro-model the actual performance of candidate architectures, we linearly combine the initial weights with the trained weights \(\) to obtain the combined weights, denoted \((g)\), for describing the non-convex landscape of the candidates. Then,we obtain the combined weights \((g)\) in the form:

\[(g)=()_{init}+(1-),\] (1)

where \(_{init}\) denotes the initial weights, and \(G\) is the number represent how dense the linear combination and set to be 10 based on most of the experiments from . Let \(Y_{pd}^{(g)}\) denote the model prediction labels based on the weights \((g)\), and \(Y_{gt}\) be the ground-truth labels of the training data. Then, we use cross-entropy to measure the difference between \(Y_{pd}^{(g)}\) and \(Y_{gt}\). After that, we sum the loss value of these middle weights as _landscape term_ as follows.

\[_{g=0}^{G}_{(g)}=_{g=0}^{G}CE(Y_{pd}^{(g)},Y_{ gt}).\] (2)

The poposed landscape term can capture the non-convexity of models where a lower value indicates a flatter loss landscape, implying an efficient convergence of the candidate and avoiding the problem of sharp minimum .

**Speed Term.** On the other hand, the idea to model the actual performance of candidates by micro-perspective, such as TSE, still perform strong and important, as shown in Fig. 1. Therefore, we are inspired by TSE  to introduce _speed term_. It first calculates training losses (cross-entropy) over an epoch, and divides it by the time cost of an epoch, converting it into a unit time training loss to measure convergence speed, which means that it observes the convergence speed of the candidates under standardized time expenditure, which helps to standardize the measurement of architectures of different sizes. It can also be seen from the Fig. 1 that the speed term performs better than the TSE . The detailed _speed term_ is defined:

\[_{e=1}^{E}}{t_{e}}=_{e=1}^{E}^{e},Y_{gt})}{t_ {e}},\] (3)

where \(Y_{pd}^{e}\) denotes the model prediction labels in epoch \(e\), and \(Y_{gt}\) means the ground truth labels of the training data. Then, we use cross-entropy to calculate the loss between \(Y_{pd}^{e}\) and \(Y_{gt}\). After that, we divide the loss value by the time cost \(t_{e}\) within epoch \(e\) and then sum up all as _speed term_. Note that a lower value indicates faster convergence, which could imply better performance.

Now, MOTE integrates the _landscape term_ and _speed term_ to model actual performance of candidates from macro to micro perspective, and is defined as follows:

\[MOTE=f(_{g=0}^{G}_{(g)})+f(_{e=1}^{E}}{t_{ e}}),\] (4)

where the first term is the proposed _landscape term_, the latter term is the proposed _speed term_, where \(_{}\) denotes the function used to determine whether the loss landscape is smooth by linear combining initial weights \(_{init}\) and trained weights \(\). \(E\) is the number of maximum training epochs to search candidate architectures, \(l_{e}\) is the training loss(usually measured by cross-entropy) for the \(e\)th epoch, \(t_{e}\) denotes the time cost for the epoch \(e\), and the function \(f\) denotes the non-linear transformation to restrict the range of values.

Due to the different ranges of _landscape term_ and _speed term_, we use the box-cox transformation  to transform and normalize them; more comparisons of other transformation methods are detailed in **Appendix A.1**. MOTE can consider both objectives by adding transformed method to assess their combined impact. Since lower values for both _landscape term_ and _speed term_ suggest a potentially better performance of the models, a smaller MOTE value indicates a better performance in practical application. Fig. 2(a) illustrates the concepts of _landscape term_ and _speed term_.

### Reduction Strategies for MOTE Generation

MOTE requires little training to obtain _landscape term_ and _speed term_, which makes it crucial to balance minimizing training time and ensuring adequate training change. Consequently, we introduce a more compact meta-architecture called the reduced architecture. We also propose the reduced dataset method, which involves a smaller dataset built by CIFAR-100. MOTE combines these reduction strategies and the training-based objectives introduced earlier to produce promising estimates with few costs. Fig. 3 depicts this process.

**Reduced Architecture (RA).** MOTE is not the actual performance of DNN after combining the cell with meta-architectures. Instead, MOTE is generated from the change in weight and loss acquired during training. The generation of MOTE relies on the proposed reduced architecture. The reduced architecture is designed to eliminate redundant layers from most meta-architectures[49; 10], resulting

Figure 3: The generation pipelines of accuracy (upper part) and MOTE (bottom part). The proposed reduced architecture and dataset, MOTE, are colored red in their respective sections.

in a simplified network structure to save the time cost of each epoch. It only comprises a convolutional layer as the stem layer and two cell layers and employs aggressive downsampling through a pooling layer with a kernel size of \(4 4\). This architectural simplification significantly accelerates the training process and substantially reduces the cost of obtaining MOTE. The structure of the reduced architecture is shown at the bottom of Fig. 3.

**Reduced Dataset (RD).** To minimize the computational cost of MOTE generation, we propose a sub-dataset derived from CIFAR-100 , referred to as the reduced dataset. As shown in Fig. 4, the process of constructing the reduced dataset involves several steps: 1) We use a VGG-16 model pre-trained on ImageNet-1K  to extract logits from images; 2) Flattened the logits and averaged them according to specific label, resulting in label embedding codes; 3) K-Means to cluster the label embedding codes into \(r\) groups; 4) Farthest Point Sampling (FPS) to select \(r\) label embedding codes from \(r\) group to represent the \(r\) labels of the reduced dataset; 5) The images associated with the chosen \(r\) labels are reserved for building the reduced dataset. The reduced dataset is a proxy dataset and a sub-dataset of CIFAR-100, containing a representative set of \(r\) labels. When \(r\) is set to 100, the reduced dataset is equivalent to CIFAR-100. As \(r\) decreases, the reduced dataset becomes smaller and easier to fit for candidate models. However, regardless of the value of \(r\), the reduced dataset maintains the original image distribution for each label. The K-means and FPS techniques ensure that the reduced dataset represents a significant part of CIFAR-100 even when \(r<100\). The most important thing is MOTE generation that rely on a reduced dataset can save remarkable time cost.

**Two Terms of MOTE with Reduction Stratigies.** To further observe how reduction strategies work, we randomly selected 1K candidates of NASBench-201 as toy experimental subjects. As shown in Fig. 5, the early stop version of test accuracy (after 12 epochs) has a high correlation with test accuracy (after 200 epochs), but training then getting it required about 220 gpu seconds, which is still a remarkable cost. When RA is applied, the time cost decreases 60%, but the correlation also drops to about 0.5 from 0.65. After further RD is applied, as the extraction ratio \(r\) gradually decreases (the further to the left the smaller \(r\) is), the time cost is also greatly saved, but the correlation suffers intolerable losses. The correlation of the leftmost (\(r=10\)) is even less than 0.1. In contrast, the proposed _landscape term_ and _speed term_ always maintain a high correlation when applying RA and RD. As the extraction ratio \(r\) gradually decreases, the time cost is reduced to about 10 gpu seconds from 220 seconds, and the correlation is still about 0.65. This is because the two proposed terms based on weight and loss changes do not rely on the excluded middle state of true or false. More comparisons between various reduction strategies refer to **Appendix A.2**.

### Integrating MOTE with Evolutionary Search

Although MOTE consistently maintains impressive performance under the influence of the reduction strategy, MOTE remains a proxy estimate. A comprehensive NAS method still requires the participation of accuracy to evaluate the actual performance of the candidates discovered in the evaluation

Figure 4: After encoding the images of CIFAR-100 through VGG, the encodings for each label are obtained by averaging image embedding codes. Then we used K-Means and Farthest Point Sampling (FPS) to select a representative set of \(r\) labels, forming the reduced dataset.

Figure 5: The proposed terms via aggressive reduction strategies on NASBench-101 and NASBench-201. RA means reduced architecture, RD means reduced dataset

stage, similar to previous NAS methods [6; 47; 51; 30]. However, as mentioned at the outset, the time cost of accuracy is exceedingly high, often demanding thousands of GPU seconds. Therefore, our proposed MOTE-NAS first employs MOTE to assist the evolutionary process in obtaining a small subset of promising candidates. Then, through the early stopping version of accuracy assessment, the best is identified. The entire procedure is illustrated in Fig. 6.

In the search stage, MOTE-NAS maintains a pool of promising candidates \(P\), with batch size \(B\) initially set to 10. With the continuous evolution loop, every ten cycles, \(B\) is incremented by 10. In each evolutionary process, MOTE is generated to sort \(P\) and take the top 10% of the candidates to the mutation stage. For the mutation stage, we are inspired by predictor-based methods [43; 11; 45] to encode candidates into adjacency and operation matrices. Subsequently, we calculate the Euclidean distance between each pair of candidates to select up to \(10 k\) mutated candidates, which are then added to \(P\). This evolutionary loop continues \(max(k,10)\) times, then stops and enters the evaluation stage. For the evaluation stage, MOTE is used to select the top \(k\) architectures of \(P\) (\(k=5,10,20\)), then select their best architecture based on the early topping version of the test accuracy.

## 4 Experimental Results

**Experimental Setup.** We used NASBench-101 and NASBench-201, both cell-based search spaces. NASBench-101 has 423,621 candidates trained on CIFAR-10 for 108 epochs. NASBench-201 includes 15,625 candidates trained on CIFAR-10, CIFAR-100, and ImageNet-16-120 for 200 epochs each. Computation was on Tesla V100 GPUs, with MOTE or MOTE-NAS costs calculated specifically on V100. Our experiment had three parts: comparing MOTE with other estimates on NASBench-101 and NASBench-201, evaluating MOTE-NAS against other NAS methods on NASBench-201, and using MOTE-NAS to search for a mobilenet-like architecture on ImageNet-1K. Further, we visualize the rankings of MOTE and KNAS to perceive their differences in Fig. 8. MOTE is generated from the proposed reduced architecture and dataset. We used reduced dataset with a sampling rate hyperparameter \(r=10\) based on the results in Fig. 5. The hyperparameters are batch size 256, epochs 50, learning rate 0.001 with Adam optimizer, and cross-entropy loss function. MOTE generation per cell took about seven GPU seconds under these settings.

### Comparison of MOTE and Other Estimates

To explore the performance gaps between MOTE and other relevant estimates, we compared NASBench-101 and NASBench-201. We ranked candidates using speed term, landscape term and MOTE or other estimates and compared the resulting rankings to the actual ranking, calculating Kendall's Tau correlation to gauge the performance of these estimates. The experimental results are presented in Fig. 7.

Figure 6: The left side depicts MOTE-NAS’s search stage, utilizing MOTE for architecture selection through an evolutionary loop, terminating at \(10+k\) iterations. On the right side is the evaluation stage, where MOTE selects the top-\(k\) architectures for evaluation. MOTE-NAS-EF simplifies this by relying solely on MOTE to choose the top-1 architecture without the evaluation stage.

Figure 7: The Kendall’s Tau Correlation comparison of the proposed speed term, landscape term and MOTE with other estimates on NASBench-101 and NASBench-201. Note that the “(s)” is the GPU seconds per cell cost.

**NTK-based Estimates.** MOTE leverages the landscape term to capture the non-convex nature from the candidate architectures, compensating for the shortcomings in NTK-based estimates. As shown in Fig. 7, MOTE achieves correlations of 0.68, 0.66, 0.62, and 0.47 in NASBench-101 and NASBench-201. Compared to TE-NAS and KNAS, MOTE shows performance improvements ranging from 13% to 62% on NASBench-201. In addition, we observed poor performance of TE-NAS and KNAS in NASBench-101, with KNAS demonstrating a correlation of merely 0.09. This illustrates that NTKs struggle to adapt to larger search spaces and more diverse candidate architectures in environments like NASBench-101. In contrast, MOTE maintains a high correlation of 0.47, demonstrating a significant increase of 261% and 422% compared to TE-NAS and KNAS, respectively. Compared to the state-of-the-art ZICO, MOTE still performs better than it does on benchmarks. Remarkably, these MOTE performance gains were achieved with an average cost of only seven seconds per candidate.

**Other Estimates.** When comparing MOTE with other estimates such as SynFlow  and TSE , MOTE maintains a significant advantage. Compared to SynFlow and TSE, MOTE shows performance gains of 11% to 45% on NASBench-201 and 96% and 104% on NASBench-101, respectively. It should be noted that both TSE and MOTE are training-based estimates, and MOTE accelerates \(2\) faster than TSE, outperforming it significantly.Additionally, we can see that the proposed speed term and landscape term also perform well in Fig. 7, both of which are essential components of MOTE.

### Visualization of MOTE and NTK based Estimate

We depicted the distribution of MOTE in comparison to KNAS to facilitate analysis regarding MOTE and NTK-based estimates. As shown in Fig. 8, the experimental results involve random selection of 1K candidate architectures from NASBench-201 (CIFAR-100). The x-axis represents the estimate ranking based on MOTE or KNAS, while the y-axis represents the actual ranking based on the test accuracy after 200 epochs. Each node in the figure represents a candidate architecture, with its (x, y) coordinates indicating its position in the estimate and the actual rankings, respectively. Both the estimate ranking and the actual ranking are sorted from high to low scores.

In the left subfigure of Fig. 8, we present the distribution of KNAS, while the right subfigure displays the distribution of MOTE. In general, MOTE is more concentrated than KNAS, indicating that MOTE is closer to the actual ranking of the candidates than KNAS. This observation aligns with the superior performance of MOTE in Kendall's Tau correlation comparisons, as shown in Fig. 7. Further, focusing on the high-performance region (the blue circle in the lower left corner of the two sub-figures), MOTE exhibits a more concentrated trend compared to the chaotic distribution of KNAS. This suggests that MOTE outperforms KNAS in predicting promising architectures. Finally, examining the purple boxes in the lower right and upper left corners of the two subfigures reveals candidates for which the estimate indicates good. However, the actual performance is poor, or vice versa. In both cases, MOTE's misjudgments are significantly fewer than those of KNAS, visually confirming that MOTE is highly competitive compared to the mainstream NTK-based estimate.

Figure 8: Comparison of the distribution of MOTE (red) and KNAS (green) on NASBench-201 (CIFAR-100).

### Comparisons of MOTE-NAS and Other NAS

To compare the performance differences between MOTE-NAS and other NAS methods, we carried out experiments on NASBench-201. Tab. 1 presents the experimental results, where "Acc(%)" represents the accuracy of the final architecture discovered by the NAS methods on the test set of the respective dataset. At the same time, "Cost(s)" indicates the total seconds used by NAS methods to discover this final architecture.

**MOTE-NAS with Top-\(k\) Evaluation.** MOTE-NAS combines MOTE with an evolutionary algorithm to filter and mutate potential high-scoring candidates by MOTE. Ultimately, the top \(k\) high-scoring candidates are selected using the early stopping version of the test accuracy (after 12 epochs), similar to the approaches in [47; 51]. The time consumption of MOTE-NAS lies in training candidates to obtain MOTE during the search stage and the early stopping accuracy obtained during the evaluation stage. However, the cost of each MOTE is only about seven gpu seconds, so primary consumption is still to evaluate the top-\(k\) candidates. We have set the \(k\) range from \(5,10,20\). When \(k=5\), the final architecture found by MOTE-NAS achieves significantly higher accuracy on three datasets of NASBench-201, compared to TE-NAS, KNAS, and Eigen-NAS, with speedups ranging from \(1.9\) to \(6.9\). It only slightly lags behind REA + LGA in test accuracy. However, when \(k=10\) or \(k=20\), with a time consumption of 8.5K gpu seconds, the final architecture discovered by MOTE-NAS achieves a remarkable accuracy of 94.32% on CIFAR-10 and 72.81% on CIFAR-100. Moreover, the MOTE-NAS discovered candidate architecture achieves 46.38% on ImageNet-16 with 11.3K seconds. Compared to NTK-based NAS (TE-NAS, KNAS, Eigen-NAS, LGA), the proposed MOTE-NAS consistently achieves the best accuracy with the lowest cost.

**Assessing the Evaluation-Free Version of MOTE-NAS.** The proposed MOTE-NAS has shown impressive performance in balancing time consumption and efficiency. The substantial time cost led us to consider omitting the evaluation stage to pursue a faster MOTE-NAS framework. Especially considering that MOTE, compared to other estimates, achieves a higher Kendall's Tau correlation, indicating a significant improvement in MOTE's predictive performance. Hence, omitting additional validation information became a viable option. To accomplish this, we removed the entire evaluation stage from MOTE-NAS. At the end of the search stage, we utilized MOTE to select top-1 as the final architecture. This variant is referred to as MOTE-NAS-EF in Tab. 1.

Although MOTE-NAS-EF experiences an accuracy loss, the search cost savings are notable. MOTE-NAS-EF achieved 93.54% accuracy on CIFAR-10, 71.59% on CIFAR-100, and 44.73% on ImageNet-16-120 with the search cost of only about 0.6K gpu seconds. In contrast, KNAS requires 4.4K, 18.4K, and 20K seconds to achieve accuracies of 93.38%, 71.05%, and 44.63%, respectively. In particular, MOTE-NAS-EF matches KNAS in accuracy but accelerates the process by \(4.8\) to \(22.2\), underscoring the superiority of MOTE-NAS-EF in speed.

   &  &  &  &  \\   & & Acc(\%) & Cost(s) & Acc(\%) & Cost(s) & Acc(\%) & Cost(s) \\   & Neural Predictor  & 94.07 & 840K & 72.18 & 840K & 46.39 & 2.4M \\  & Arch-Graph  & - & - & 73.38 & 840K & - & - \\  & WeakNAS  & 94.23 & 840K & 73.42 & 840K & 46.79 & 2.4M \\  & Proxy-BO  & - & - & **73.48** & 1.2M & **47.18** & 3.2M \\   & NASWOT  & 92.96 & 2.2K & 70.03 & 4.6K & 44.43 & 10K \\  & TE-NAS  & 93.90 & 2.2K & 71.24 & 4.6K & 42.38 & 10K \\  & KNAS (k=20)  & 93.38 & 4.4K & 70.78 & 9.2K & 44.63 & 20K \\  & KNAS (k=40)  & 93.43 & 8.8K & 71.05 & 18.4K & 45.05 & 40K \\  & Eigen-NAS (k=20)  & 93.46 & 4.4K & 71.42 & 9.2K & 45.53 & 20K \\  & RS + LGA  & 94.05 & 5.4K & 71.56 & 7.0K & 46.30 & 15K \\  & REA + LGA  & **94.30** & 3.6K & 72.42 & 5.4K & 45.30 & 3.6K \\  & **MOTE-NAS (k=5)** & 93.97 & **2.2K** & 71.89 & **2.4K** & 46.10 & 5.8K \\  & **MOTE-NAS (k=10)** & 94.15 & 4.2K & **72.54** & 4.3K & **46.38** & 11.3K \\  & **MOTE-NAS (k=20)** & **94.32** & 8.5K & **72.81** & 8.5K & **46.34** & 22.7K \\  & **MOTE-NAS-EF** & 93.54 & **0.5K** & 71.59 & **0.6K** & 44.73 & **0.6K** \\  

Table 1: Comparison of the proposed MOTE-NAS and others on NASBench-201. Note that ’Cost (s)’ indicates the cost in seconds calculated on Tesla V100. Entries in bold with underlines indicate the best performance, and those in bold alone represent the second-best performance.

### MOTE-NAS on ImageNet-1K

**Search Space.** We search for a promising architecture based on the mobilenetV3 search space using MOTE, then train and evaluate it on imagenet-1K. The mobilenetV3 search space is a open search space that has five inverted residual blocks with the SE module. Every block has several hyperparameters, such as the expansion ratio for input channel expansion, kernel size, and SE module attached or not. Based on it, we restrict the selection range for each hyperparameter. We restrict the expansion ratio range from \(2,4,6\), kernel size range from \(3,5,7\), and the SE module used or not.

**Rescaled Reduced Architecture for Macro-Search.** Our study introduces a Rescaled Reduced Architecture for Macro-Search, where we modify the reduced architecture to accommodate the simultaneous assessment of five blocks and their collective performance. By expanding the cell layers from two to five and independently sampling the structure of each layer, our rescaled approach enables macro-search capabilities beyond single-cell exploration. Further technical specifics of this rescaled reduced architecture are outlined in Appendix A.5. Subsequently, employing MOTE-NAS with this modified architecture, we conducted a search within the mobilenetv3 space under approximately 400M FLOPs. Following 200 epochs of training using 10 GTX 2080Ti GPUs on the imagenet-1K dataset, the results (see Table 2) demonstrate the efficacy of our approach. While the accuracy of MOTE-NAS (76.2% and 77.1%) trails slightly behind OFA's 77.7%, MOTE-NAS achieves this with a significantly reduced computational cost of 0.1 GPU days compared to OFA's 50 days, representing a 500x speed improvement. Furthermore, our retraining of ZICO's best architecture yielded a 75.8% accuracy on imagenet, surpassed by MOTE-NAS with its superior accuracies and a 4x faster search cost than the 0.4 day of ZICO.

## 5 Conclusion

In this paper, we design a novel training-based estimate for efficient Neural Architecture Search (NAS) from a multi-objective optimization perspective. The key idea is to use landscape terms to capture the non-convex nature of candidate architectures from a macro perspective, and use speed terms to monitor convergence speed from a micro perspective into the estimated design. The proposed MOTE efficiently generates the landscape and speed terms with two reduction strategies, which wisely trade-off the consideration of architecture and dataset. These designs can effectively capture the non-linear characteristics of deep neural network training, address the drawbacks of NTK methods, and achieve a new state-of-the-art state. We extend our approach by iterative ranking-based evolutionary search, then deduce an evaluation-free version (MOTE-NAS-EF) that runs even faster. Extensive experimental results demonstrate the superiority of our new NAS methods over other frontier NAS methods, including KNAS, LGA, and ZICO, on NASBench-101, NASBench-201, and ImageNet-1K. Future works include expanding MOTE to other NAS frameworks, such as predictor-based methods, to pursue precise search results while exploring MOTE's generalization ability. Another line of extension is to work with the more challenging NASBench-301 benchmark dataset  that offers a much larger and more complex architecture space than both NASBench-101 and NASBench-201.