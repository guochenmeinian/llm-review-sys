# What type of inference is planning?

Miguel Lazaro-Gredilla Li Yang Ku Kevin P. Murphy Dileep George

Google Deepmind

{lazarogredilla, liyangku, kpmurphy, dileepgeorge}@google.com

###### Abstract

Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about "planning as inference"? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds _exactly_ to a _different_ set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.

## 1 Introduction

There are many kinds of probabilistic inference, such as marginal, maximum-a-posteriori (MAP), or marginal MAP (MMAP) that are used in the planning as inference literature (Attias, 2003; Levine, 2018; Cui et al., 2019; Palmieri et al., 2022; Wu and Khardon, 2022). In this work we show that planning is a distinct type of inference, and that _under stochastic dynamics_ does not correspond exactly with any of the above methods. Furthermore, we show how to rank the above methods in terms of quality as it pertains to planning.

Our approach is based on a variational perspective, which allows direct comparison between different inference types, and to develop analogues of existing approximate inference algorithms for this "planning inference" task. Given the "flat" Markov Decision Process (MDP) from Fig 1[Left], we show that planning inference provides the same (exact) results as value iteration. Using an analogue of loopy belief propagation (LBP), we show how to apply approximate planning inference to the factored MDP in Fig. 1[Right], which has an exponentially large state space, and for which exact solutions are no longer tractable. Under moderate stochasticity in the dynamics, we show that this approximate planning inference might be superior to other more established types of inference.

## 2 Background

### Markov Decision processes (MDPs) and notation

A finite-horizon Markov decision process (MDP) is a tuple \((,,p(x_{1}),,,T)\), where \(\) is the state space, \(\) an action space with cardinality \(N_{a}\), \(p(x_{1})\) the starting state distribution, \(\) thetransition probabilities \(P(x_{t+1}|a_{t},x_{t})\) (i.e., the dynamics of the process), \(R_{t}(x_{t},a_{t},x_{t+1})\) the reward for transitioning from \(x_{t}\) to \(x_{t+1}\) under action \(a_{t}\) at time step \(t\), and \(T\) is the horizon. For simplicity of exposition we will consider discrete states and actions, but analogous results apply in the continuous case. Solving an MDP corresponds to finding a policy \(_{t}(a_{t}|x_{t})\) that maximizes the expectation of (a function of) the sum of the reward at each time step. The optimal policy can be different at each time step, i.e., non-stationary. We do not use a discount factor, but it can be trivially included in the reward function, since it is also non-stationary. Policies, states and actions at all time steps will be noted \(\{_{t}(a_{t}|x_{t})\}_{t=1}^{T-1},\{x_{t}\}_{t=1 }^{T}\) and \(\{a_{t}\}_{t=1}^{T-1}\).

In a state-factored MDP (factored MDP for short), each state \(x_{t}\) factorizes into \(N_{e}\) r.v. or _entities_\(\{x_{t}^{(i)}\}_{i=1}^{N_{e}}\), each with cardinality \(N_{s}\), so it has an exponentially large state space of size \(N_{s}^{N_{e}}\). Transitsion factorize as \(P(x_{t+1}|a_{t},x_{t})=_{i=1}^{N_{e}}P(x_{t+1}^{(i)}|x_{t}^{(i)},a_{t})\), where \(x_{t}^{(i)}\) is the subset of \(x_{t}\) on which \(x_{t+1}^{(i)}\) depends, which we will assume to be small to allow for a tabular definition of the transition without exponential cost (pa\((i)\) stands in for "parents of \(i\)" ). For simplicity of notation, we will assume that the reward of factored MDPs at each time step depends only on the current state, and for tractability, that it can be decomposed in multiple additive subterms \(R_{t}(x_{t})=_{i=N_{e}+1}^{N_{e}+N_{r}}R_{t}(x_{t}^{(i)})\), where \(x_{t}^{(i)}\) is a small subset of \(x_{t}\) on which the \((i-N_{e})\)-th reward depends, for a total of \(N_{r}\) reward subterms. As before, this allows for a compact tabular representation of the reward. Including additional dependencies in the reward (on actions, or on the next state of an entity) is straightforward.

For a more comprehensive introduction to MDPs, refer to (Puterman, 2014; Sutton, 2018).

### Variational inference

Let's consider running different _types_ of inference in the unnormalized1 factor graph shown in Fig. 1[Left]. Marginal inference would compute the sum over all the \(,\) configurations (the partition function); MAP inference would compute the maximizing configuration (the posterior mode); and marginal MAP (MMAP) inference (see Liu and Ihler, 2013) would find the assignment of \(\) that maximizes the summation over \(\) conditional on that \(\). Although marginal, MAP, and MMAP inference are distinct, a lot is shared. They all target a "quantity of interest" (e.g., partition function, maximum probability state, best conditional partition function); they all produce a distribution as a result of inference (respectively, full posterior, delta at the mode, best conditional posterior); naive computation requires a number of summations and/or maximizations exponential in the number of variables; and finally, they can all be represented as a variational inference (VI) problem.2 The VI representation naturally leads to efficient message-passing solutions and approximate inference algorithms, as we show below.

For the factor graph \(f(,)\) from Fig. 1[Left], the VI problem3 is \(_{q(,)} f(,)_{q(,)}+ H_{q}^{}(,)\) where \(q(,)\) is an arbitrary variational distribution over the variables of the factor

Figure 1: Factor graphs: [Left] Standard MDP [Right] Factored MDP with sparse factor connectivity.

graph, the term \(- f(,)_{q(,)}\) is known as the _energy term_, and \(H_{q}^{}(,)\) is a particular entropy choice that will determine the inference type. It is a standard result, (e.g., Jordan et al., 1999) that using the Shannon entropy \(H_{q}(,)\) results in marginal inference. Setting it to zero (aka the zero-temperature limit) corresponds to MAP inference (Weiss et al., 2012; Sontag et al., 2011; Wainwright et al., 2005; Kolmogorov, 2005; Martins et al., 2015). Setting it to the conditional entropy \(H_{q}(|)=H_{q}(,)-H_{q}()\) results in MMAP inference (Liu and Ihler, 2013). Note that the only difference across VI problems is the _weighting_ of the entropy terms.

Despite the similarities, the computational complexity of these types of inference can differ significantly, even for tree-structured graphs. In particular, the entropy term for MMAP is not concave, and inference is NP-hard (Liu and Ihler, 2013), whereas for marginal and MAP inference the entropy term is concave (the energy term is always linear), and inference is polynomial.

## 3 Methods

In this section we introduce our VI framework, which we will use to derive a novel linear programming formulation of planning problems, a novel value belief propagation (VBP) algorithm and a novel closed form (sampling-free) approach to determinization.

### VI for standard MDPs

The main quantity of interest in this paper is the _best exponential utility_, which we will refer to simply as the _utility_. Given an MDP with horizon \(T\) and a risk parameter \(>0\), the utility is defined as

\[F_{}^{} =_{}_{,} _{t=1}^{T-1}R_{t}(x_{t},a_{t},x_{t+1})P(x_{1})_{ t=1}^{T-1}P(x_{t+1}|a_{t},x_{t})_{t}(a_{t}|x_{t})\] (1) \[=_{}( R(,))_{P(|)(|)},\]

where \(P(|) P(x_{1})_{t=1}^{T-1}P(x_{t+1}|a_{t},x_{t})\), \(R(,)_{t=1}^{T-1}R_{t}(x_{t},a_{t},x_{t+1})\), and \((|)_{t=1}^{T-1}_{t}(a_{t}|x_{t})\).

Observe that we can always set \( 0^{+}\) to recover the standard planning setting in which we seek the best expected _additive reward_, so here we are tackling a strictly more general case. To be precise, if we take the limit \(F_{ 0^{+}}^{}_{ 0^{+}}F_{ }^{}=_{} R(,)_{P( |)(|)}\)(Marthe et al., 2023).

The motivation for the introduction of \(\) is two-fold. On the one hand, by using a more general formulation of the reward, we can trade off between risk-neutral (\( 0^{+}\)) and risk-seeking (\(>0\)

   Type of &  & }()\) for variational bound} &  \\ inference & \(F_{}=_{}F_{}()\) & ()=(-E_{}()+H^{}())\)} & \\  Marginal4  & \(_{,}P(|)e^{ R(, )}\) & \(H_{q}(x_{1})+_{t=1}^{T-1}H_{q}(x_{t+1},a_{t}|x_{t})\) & ✓ \\
**Planning** & \(_{}(e^{ R(,)})_{P(| )(|)}\) & \(H_{q}(x_{1})+_{t=1}^{T-1}H_{q}(x_{t+1}|a_{t},x_{t})\) & ✓ \\ M. MAP & \(_{}_{}P(|)e^{ R( ,)}\) & \(H_{q}(x_{1})+_{t=1}^{T-1}H_{q}(x_{t+1},a_{t}|x_{t})-H_{q}(a_{t})\) & � \\  MAP & \(_{,} P(|)e^{ R(, )}\) & \(0\) & ✓ \\ Marginal5  & \(_{,}P(|)} ^{T-1}}e^{ R(,)}\) & \(H_{q}(x_{1})+_{t=1}^{T-1}(H_{q}(x_{t+1},a_{t}|x_{t})- N_{})\) & ✓ \\   

Table 1: Different types of inference from a variational perspective, including a proper “planning” inference type. They all share the same energy term \(E_{}()\) defined in Eq. (3), and differ only in the entropy term. The closed-form expressions provide the optimal value of the bound, but are generally intractable. The general tractability of the bound maximization for MDPs is marked in the Tr column. All bounds are monotonically related, and listed in descending order, except the last two, whose relative ordering only applies when dynamics are deterministic. See Section 4.1 for details.

policies, adding a tunable parameter that makes the model more flexible, see (Marthe et al., 2023; Follmer and Schied, 2011; Shen et al., 2014) for more details. On the other hand, it allows us to express the expected reward as a proper factor graph: note that Eq. (1) can be expressed as a product of factors involving not only the dynamics terms, but also the reward terms, allowing us to write it as \(F_{}^{}=_{}_{,}f(,)(|)\), where \(f(,)\) is the factor graph of Fig. 1[Left]. This factorization would not have been possible if we had simply used an additive reward. But at the same time, notice that we are not losing generality, since the additive reward case can be recovered by setting \( 0^{+}\). Alternatively, we could have achieved a factorized model by introducing an additional latent "selector" variable connected to all the rewards, but this would complicate our upcoming formulation and analysis. Furthermore, this formulation allows us to encompass prior work on planning as inference that uses \(>0\).

We can turn this new quantity of interest, the utility, into the solution of a VI problem on the factor graph of Fig. 1[Left]. Crucially, the factor graph includes the known dynamics and rewards terms, but _not_ any policy term, since the policy is the outcome of inference.

**Theorem 1** (Variational formulation of planning).: _Given known dynamics \(P(x_{t+1}|a_{t},x_{t})\), an initial distribution \(P(x_{1})\) and reward functions \(R_{t}(x_{t},a_{t},x_{t+1})\), the best exponential utility \(F_{}^{}\) from Eq. (1) can be expressed as the result of a concave variational optimization problem_

\[F_{}^{}=_{}F_{}^{}();\ \ \ \ \ F_{}^{}()=(-E_{}()+H ^{}())\] (2)

_with energy \(E_{}()\) and entropy \(H^{}()\) terms_

\[E_{}() =- P(x_{1})_{q(x_{1})}-_{t=1}^{T-1}  P(x_{t+1}|a_{t},x_{t})+ R_{t}(x_{t},a_{t},x_{t+1})_{q(x_{t+ 1},x_{t},a_{t})}\] (3) \[H^{}() =H_{q}(x_{1})+_{t=1}^{T-1}H_{q}(x_{t+1}|a_{t},x_{t})\] (4)

_where \( q(,)\) is an arbitrary distribution over the space of states and actions._

Proof is in Appendix A. This entropy thus corresponds to "planning inference". The optimal policy at each time step corresponds to the optimal variational distribution \(q(a_{t}|x_{t})\). Table 1 lists the types of inference problems and their associated entropies (see Appendix F for their derivation and corresponding references). As we will discuss in Section 4, they display a monotonic ordering (in almost all cases).

The VI problem Eq. (2) reduces to the standard one when \(=1\), and extends VI in a meaningful way in the presence of rewards, regardless of the type of inference used: rewards interact in an additive way when \( 0^{+}\), rather than the default multiplicative (or more precisely, exponentiated summation) interaction of \(=1\). Furthermore, it turns out that it is possible to take the \( 0^{+}\) limit exactly, to obtain the dual LP formulation of an MDP (Puterman, 2014).

**Corollary 1.1** (Additive limit).: _In the limit \( 0^{+}\), the concave problem Eq. (2) becomes the following linear program (LP):_

\[F_{ 0^{+}}^{} =_{}F_{ 0^{+}}^{}()=_{ \{q(x_{t},a_{t})\}_{t=1}^{T-1}}_{t=1}^{T-1} R_{t}(x_{t},a_{t},x_{t+ 1})_{P(x_{t+1}|a_{t},x_{t})q(x_{t},a_{t})}\] \[ q(x_{1}) =P(x_{1});\ \ \ _{a_{t+1}}q(x_{t+1},a_{t+1})=_{x_{t},a_{t}}P(x_{t+1}|a_{t},x_{t})q(x_ {t},a_{t})\ \  t;\] \[q(x_{t}) =_{a_{t}}q(x_{t},a_{t})\ \  t;\ \ \ \ q(x_{t},a_{t}) 0 \  t,\]

_which corresponds to the maximum expected reward \(F_{ 0^{+}}^{}=_{} R(,) _{P(|)(|)}\)._

See Appendix B for proof.

### VI LP and VBP for factored MDPs

Factored MDPs (e.g., Fig. 1[Right]) are loopy factor graphs with an exponentially large state space, so the previous approaches cannot be applied directly. An effective approximate marginal inference approach for this type of problem is loopy belief propagation (LBP). Since planning is now seen as a type of inference, we can create an analogue to LBP which we call value belief propagation (VBP).

Following LBP, we make two approximations to Eq. (2) to make it tractable.

First, we replace the variational distribution \(\) with pseudo-marginals \(}\). Eqs. (3) and (4) never access the full joint \(q(,)\), but only the local marginals of each factor. Pseudo-marginals are the collection of such local distributions, consistent at each variable, but not necessarily marginals of any distribution. Just like \(\) is defined in a convex region called the _marginal polytope_\(\), \(}\) is defined in an outer convex region called the _local polytope_\(\) that contains \(\)(Weller et al., 2014).

Second, we replace the entropy \(H^{}()\) with its _Bethe_ approximation

\[H^{}_{}(})=_{i=1}^{N_{e}}H_{q}(x_{ 1}^{(i)})+_{t=1}^{T-1}_{i=1}^{N_{e}}H_{q}(x_{t+1}^{(i)}|x_{t}^{ (i)},a_{t})-_{i=1}^{N_{e}+N_{r}}I_{q}(x_{t}^{(i)})\]

where \(I_{q}(x_{t}^{(i)})=_{k(i)}H_{q}(x_{t}^{(k)})-H_{q }(x_{t}^{(i)})\) is the mutual information of the parents of \(x_{t+1}^{(i)}\), see Appendix C for details. This approximation is tractable as long as the factors (transition and rewards) are tractable, typically by connecting to a small number of parent variables. Note that the policy (which would connect to all the state variables in a time slice and introduce exponential cost) is not a factor in the graph.

The term \(I_{q}(x_{t}^{(i)})\) is key. It always non-negative but neither concave nor convex in general and can be interpreted as the mutual information correcting the discrepancy between (a) the entropy of a collection of variables considered independently (as the output of the previous time step) and (b) the entropy of the same collection when considered jointly (as parents for the current time step). It makes the optimization problem harder, but also more accurate.

For non-factored MDPs, \(I_{q}(x_{t}^{(i)})=0\) and \(}=\), so we recover Eq. (2). In general factored MDPs this is not true. We can still choose to ignore this correction to obtain a concave bound

\[H^{}_{}(})=_{i=1}^{N_{e}}H_{q}(x _{1}^{(i)})+_{t=1}^{T-1}_{i=1}^{N_{e}}H_{q}(x_{t+1}^{(i)}|x_{t}^{ (i)},a_{t}) H^{}_{}(}).\] (5)

Then, the planning Bethe approximation of the variational bound and its concave upper bound are

\[^{}_{}=_{q}^{}_{}(})=_{q}(-E_{}(})+H^{ }_{}(}))\ \ s.t.\ }\] (6) \[^{}_{}=_{q}^{}_{}(})=_{q}(-E_{}(})+H^{ }_{}(}))\ \ s.t.\ }.\] (7)

We see that \(^{}_{}^{}_{}\) and \(^{}_{}^{}_{}\). The former is trivial given the negative term removed. The latter follows from (a) switching the optimization domain from \(\) to \(\), which can only increase the value of the bound, and (b) Eq. (5) corresponds to Eq. (4), but with joint entropies over entities replaced with sums of the entropies, which is an upper bound. The fact that \(^{}_{}(})\) is concave and upper bounds the exact utility has two advantages: it can be computed without local minima problems, and it is an _admissible heuristic_ of the original utility, meaning that it can be used as a heuristic for algorithms that emit a certificate of optimality or infeasibility.

**Lemma** (Additive limit for factored MDPs).: _In the limit \( 0^{+}\), the concave problem Eq. (7) becomes the following VI LP:_

\[^{}_{ 0^{+}}=_{}}^{ }_{ 0^{+}}(})=_{}}_{t=1}^{T} _{i=N_{e}}^{N_{e}+N_{r}} R_{t}(x_{t}^{(i)})_{q(x_{t} ^{(i)})}\]

\[\ \ q(x_{1}^{(i)})=P(x_{1}^{(i)})\ \  i;\ \ \ \ \ }\] \[\ q(x_{t+1}^{(i)})=_{x_{t}^{(i)},a_{t}}P(x_{t+1}^{(i )}|x_{t}^{(i)},a_{t})q(x_{t}^{(i)},a_{t})\ \  x_{t+1}^{(i)},t,1 i N_{e}\]_which upper bounds the max. expected reward \(^{}_{ 0^{+}} F^{}_{  0^{+}}=_{} R(,)_{P(|)(| )}\). Alternatively, the same expression can be obtained from Corollary 1.1 by relaxing the marginal polytope into the local polytope. Since it is a relaxation, the upper bounding is trivial._

To the best of our knowledge, this is a novel VI LP formulation and it can be used to tractably (over) estimate the optimal expected reward in factored MDPs. Similarities with (Koller and Parr, 1999; Guestrin et al., 2003) are only surface level, see Section 5.

\(^{}_{}(})\) from Eq. (7) can be maximized with a conic solver (or an LP solver if \( 0^{+}\)). The non-concave \(^{}_{}(})\) from Eq. (6) is more challenging. Conveniently, \(^{}_{}(})\) looks just like the Bethe free energy that motivates LBP, but with a different weighting of the local entropy terms.

Multiple works consider modifying the entropy weighting in LBP, usually with the aim of "concavifying" the overall entropy term and developing convergent alternatives to LBP. In particular, (Hazan and Shashua, 2010) provide fixed-point message updates for arbitrary entropy weights. For the specific weighting of \(H^{}_{}(})\) the message updates approach a singularity, which we will avoid by using \((1-)H^{}_{}(}))+ H^{ }_{}(})\). The resulting message passing algorithm updates are well-defined for any \(>0\), interpolate between "planning inference" and marginal inference, and can get arbitrarily close to the former by making \( 0^{+}\). This smoothing is not just a mathematical convenience, but we prove that it exactly corresponds to MaxEnt RL in Appendix E. See (Liu and Ihler, 2013) for an analogous technique with the same purpose (but without this nice interpretation).

VBP inherits many of the properties of LBP: the message updates are not guaranteed to converge, but if they do, they do so at a fixed point of Eq. (6). Convergence can be improved by the use of damping and annealing. The precise message updates for the general case are provided in Appendix D.

Computation associated with VBP scales as expected, \((T(_{i=1}^{N_{e}}N_{a}N_{s}^{+1}+_{i=N_{e+1} }^{N_{r}}N_{s}^{}))\), where \(N_{e},N_{r},N_{a},N_{s}\) have been defined in Section 2.1. Note that the derivation is straightforward. Each VBP iteration involves computing message updates for each factor in the graph. The cost is dominated by the blue factors (\(N_{e}\) of them per time step) and green factors (\(N_{r}\) of them per time step) in Fig. 1[Right]. There are a total of \(T\) time steps. And finally the number of possible configurations is \(N_{a}N_{s}^{+1}\) for blue factors and \(N_{s}^{}\) for green factors.

### VBP for standard MDPs

It is instructive to look at the VBP updates for a standard, non-factored MDP. In this case, it is possible to take the limit \( 0^{+}\) and get well-defined updates. For \(=1\) and a single reward at \(T\)

Backward updates: \[m_{}(x_{T})=e^{R_{T}(x_{T})};\;\;\;m_{}(x_{t})= _{a_{t}}Q(x_{t},a_{t});\] Forward updates: \[m_{}(x_{1})=P(x_{1});\;\;\;m_{}(x_{t+1})=_{x_ {t},a_{t}}p(x_{t+1}|x_{t},a_{t})_{a_{t},_{a_{t}^{}} \,Q(x_{t},a_{t}^{})}m_{}(x_{t})\] Optimal dist.: \[q(x_{t+1},x_{t},a_{t}) m_{}(x_{t+1})p(x_{t+1}|x_{t},a_{t })_{a_{t},_{a_{t}^{}}\,Q(x_{t},a_{t}^{})}m_{ }(x_{t})\]

where \(Q(x_{t},a_{t})=_{x_{t+1}}m_{}(x_{t+1})p(x_{t+1}|x_{t},a_{t})\) and \(_{j,k}\) is a standard Kronecker delta that equals 1 when \(j=k\) and 0 otherwise. Iterating these updates converges in a single backward and forward pass to the global optimum. The backward messages correspond to the value function (hence the name VBP), and the familiar intermediate quantity \(Q(x_{t},a_{t})\) matches the Q-function. The forward messages correspond to occupancy probabilities under the optimal policy. Thus, in a non-factored MDP we recover the standard Bellman backups, implementing value iteration and providing the exact solution. The same happens, conceptually, in a factored MDP, but only approximately, with the forward messages helping to determine where the backward approximation should be more precise.

### Determinization in hindsight

The previous presentation implies that all VI tricks are now applicable to planning. As an example, we can show that for _determinization_(Yoon et al., 2008) (a technique from the planning literature to extend deterministic planning algorithms to stochastic domains, and that is usually computed via sampling), we can obtain a precise upper bound as the solution of a tractable concave problem.

To be more precise, we can compute \(F_{=1}^{}\) (for MDPs) and \(_{=1}^{}\) (for factored MDPs) as a concave optimization problem (avoiding sampling) when the inner deterministic planning problem is solved with an LP MAP relaxation (exact for MDPs). Additionally, we can prove that for factored MDPs \(_{=1}^{}_{=1}^{}\) (i.e., the superiority of the bound introduced here wrt this determinization upper bound in the case of factored MDPs). See Appendix H for further details.

## 4 The different types of inference and their adequacy for planning

### Ranking inference types for planning

As we show in Section 5, the term "planning as inference" has been used in the literature to refer to different inference types, none of which corresponds, to the best of our knowledge, with the "planning inference" from this work, which is exact. Table 1 associates each type of inference to a corresponding lower bound on its quantity of interest. Turns out that by inspecting the entropy term (since the energy is the same for all of them), we can also relate those lower bounds to one another for a given variational distribution \(\), resulting in \(F_{}^{}()\)

\[F_{}^{^{}}()\]

This in turn means that for the optimal variational distribution of each type of inference we have

\[F_{}^{}\] \[F_{}^{^{}}} F_{ }^{} F_{}^{} F_{}^{ }.\] (8)

See Appendix G for proof. VI aims to maximize a lower bound on the quantity of interest, with tighter bounds generally indicating better performance. Since MMAP is, among the lower bounds, the tightest, it follows that MMAP inference is expected to be no worse and potentially better than all other common types of inference. However, as noted in Section 2.2, MMAP inference is particularly hard, even in trees, meaning that in the case of a non-factorial MDP like the one in Fig. 1[Left], the computation of \(F_{}^{}\) is intractable, even though all the other quantities, including the one of interest, \(F_{}^{}\), are exactly computable. What we can tractably compute is the lower bound \(F_{}^{}() F_{}^{}\) and try to maximize it wrt \(\), but without guarantees of finding the optimal value. Thus, among the common inference types, MMAP seems a better choice, but it is either intractable or, if using VI, can run into local minima problems. This seems more acceptable in the factored MDP case, but it is disappointing that the problem persists for standard, non-factored MDPs.

### The stochasticity of the dynamics is key

The energy term Eq. (3), which is common to all inference methods, contains subterms \( P(x_{t+1}|a_{t},x_{t})_{q(x_{t+1},x_{t},a_{t})}\) (and \( P(x_{1})_{q(x_{1})}\) for the first state). When dynamics are deterministic (which we assume to also imply that \(P(x_{1})\) is deterministic, i.e., the first state is known), this forces the optimal variational conditional to be \(q(x_{t+1}|a_{t},x_{t})=P(x_{t+1}|a_{t},x_{t})\) (and \(q(x_{1})=P(x_{1})\) for the first state), since any other choice would make those subterms, and therefore the bound, \(-\). This affects the relationships of the quantities of interest, which are now (proof in Appendix G):

\[F_{}^{^{}} F_{}^{}=F_{ }^{}=F_{}^{} F_{}^{},\]

and _justifies the use of MAP and MMAP inference as planning when dynamics are deterministic._ When using approximate inference, if dynamics are close to deterministic, it might make more sense to choose the type of inference based on the quality of the approximation, rather than its tightness. If dynamics are stochastic, the suboptimality of MMAP can be explained as a _lack of reactivity to the environment_. Indeed, if we reduce the planning problem to a non-reactive policy \((a_{t}|x_{t})=(a_{t})\) we recover MMAP inference as optimal. We test this experimentally in Section 6 and further expand on it in Appendix I.2. MAP has the same problem, but additionally lacks integration over observation sequences ("trajectories"). Even with deterministic dynamics, marginal inference might not produce good utility estimates, but its action posterior will be proportional to the reward of the action sequence, so if we additionally assume \(( R())\{0,1\}\) (i.e., pure planning where we want to attain any of a subset of states), it will produce optimal planning _choices_. Interestingly, our framework also shows that marginal inference is exact for a generalization of MaxEnt planning when the policy entropy regularization is set to \(=1/\), regardless of stochasticity, see Appendix E.

Related work

As stated, the meaning of "planning as inference" is uneven across the literature. (Toussaint and Storkey, 2006) introduce the policy in the MDP factor graph and maximize the likelihood wrt to its parameters using EM. This is an exact approach, although it is more appropriate to say that it is planning as _learning_ rather than a type of inference, since the EM process updates the parameters of the factor graph and inference typically operates on a graph with fixed parameters. (Levine, 2018) is a well-known reference that considers _MAP inference_ for standard planning and _marginal inference_ for MaxEnt planning (Ziebart, 2010). Both are exact only under deterministic dynamics. This problem is not addressed in the case of standard planning, but it is pursued for MaxEnt planning. To achieve exact MaxEnt planning under stochastic dynamics, a modified marginal inference procedure is provided. It can be seen as structured variational inference where \(q(x_{t+1}|x_{t},a_{t})=P(x_{t+1}|x_{t},a_{t})\) is forced. With the right smoothing, our VBP corresponds to MaxEnt planning, and extends this modified marginal inference to the factored case, see Appendix E.3. (Cui et al., 2015) introduces ARollout, which can be seen as running a single-forward-pass LBP to approximate _marginal inference_ for each possible initial action, and then choosing the highest scoring initial action5, and is applicable to factored MDPs. In the follow-up works (Cui and Khardon, 2016; Cui et al., 2019) the authors develop conformant SOGBOFA, which approximates _marginal-MAP_ inference by using ARollout in an inner loop and gradient descent to optimize over the action prior in an outer loop. A number of refinements are added for superior performance. This is a strong baseline and was the runner-up in the international probabilistic planning competition (IPPC) 2018, which agrees with our analysis from Section 4. (Lee et al., 2014; Lee et al., 2016) provide initial results on the connection between conformant planning and MMAP inference. Many works, such as (Attias, 2003) choose _MAP inference_ for planning.

Two frameworks (Palmieri et al., 2022; Wu and Khardon, 2022) have been recently introduced to analyze planning from a message-passing perspective. The former analyzes six update rules and their qualitative effect on the plans; the latter focuses on disentangling the inference direction6 (either forward --from causes to outcomes-- or backward --from outcomes to causes--) from the _approximation_ type. This work provides two novel message-passing algorithms for factored MDPs: MFVI (mean field VI) and CSVI (collapsed state VI), using the planning tasks from IPPC 2011 as benchmark. We will compare with their results in Section 6.

_Influence diagrams_(Matheson, 2005; Shachter, 2007) are used to represent general decision problems, and various approximate inference approaches have been developed, e.g., (Lee et al., 2018; Lee

Figure 2: Performance of different types of inference on factored MDPs as a function of their level of stochasticity (normalized entropy). [Left] Estimation error of the best utility. Lower is better. [Right] Advantage of the next action prescribed by a method vs. optimal planning. Higher is better.

et al., 2020). Closest to our work, (Cheng et al., 2013; Chen et al., 2015) tackle graph-based MDPs, similar to factored MDPs, but with a factorized action space: multiple actions are taken at each time step, each locally affecting a single entity. This locality results in additional efficiencies, so direct application to a state-only-factored MDPs would still result in exponential cost.

LP formulations for the solution non-stationary, finite-horizon MDPs have received significant attention over the last decade (e.g., Kumar et al., 2015; Bhattacharya and Kharoufeh, 2017; Altman, 2021; Bhat et al., 2023), but they lack a variational perspective and do not generalize easily to handle state-factored MDPs. The LPs in (Koller and Parr, 1999; Guestrin et al., 2003; Malek et al., 2014) on the other hand do handle factored MDPs and have a closer connection to our work. The problem setup is slightly different, infinite-horizon MDPs with a stationary policy in their case vs our finite horizon MDPs, which allows us to use _local_ non-stationary policies. More importantly, their computational cost can be significantly higher than in this proposal. E.g., (Guestrin et al., 2003, Section 4.2.1) states that the cost is dependent on the variable elimination order. In the optimal case (which is NP-hard to find), it scales exponentially with the _width of the cost network_, which is based on the dependencies between entities, and can be much larger than exponential in the number of parents (our case).

## 6 Empirical validation

Synthetic MDPsWe generate \(5,000\) synthetic factored MDPs structured as in Fig. 1[Right] with random dynamics, all-or-nothing reward at the last time step, and controlled normalized entropies, defined as \(H_{}=,a_{t}}H(p(x_{i+1}^{(i)}|x_{t},a_{t}))}{N_ {e}N_{e}N_{e}N_{e} N_{e}}\). See Appendix I.1 for more details.

They are purposefully small so that we can compute \(F^{^{}}\), \(F^{}\), \(F^{}\), \(F^{}\) exactly, even though they are intractable in general. We also compute \(^{}\) (tractable), and \(^{}\) (tractable bound, generally intractable optimization), which correspond to ARollout (Cui et al., 2015) and optimal SOGBOFA-LC* (Cui et al., 2019), respectively. Finally, we include VBP (tractable, imperfect optimization of \(^{}()\)) and the tractable VI LP \(^{}_{=0}\) and VI CVX \(^{}_{=1}\).

Fig. 2 shows the effect of stochasticity in the estimation of the utility [Left] and the next best action [Right]. For high stochasticity, VBP, even if approximate, dominates all other types of inference. The concave upper bounds \(^{}_{=0}\) and \(^{}_{=1}\) also improve over exact MMAP but not as much as VBP. For low stochasticity, exact MAP and MMAP dominate VBP. The (intractably optimal) SOGBOFA-LC* remains close to the exact MMAP. These results agree well with the theory and observations laid out in Section 4. We see good correlation between the accuracy of the utility estimation and the quality of the planning choices ([Left] and [Right] panels). ARollout and exact marginal seem to be an exception to this; this is explained in Section 4.2: for pure planning problems, with low stochasticity both methods are a constant away from the right utility and make good choices.

Reactivity avoidanceWe craft a multi-entity MDP in which the agent controls the level of reactivity (see Section 4.2) needed to solve the environment, but is penalized for lower ones. VBP keeps the reactivity at a maximum, to achieve a reward of 1. SOGBOFA-LC* "aware" that it cannot plan reactively (despite replanning), takes step to reduce it, getting a reward of 0.33. See Appendix I.2.

International probabilistic planning competition tasks (IPPC)We follow (Wu and Khardon, 2022) and compare on the same tasks and with the same methods. We use the 6 different domains from IPPC2011, each with 10 instances (factored MDPs) of increasing difficulty, with given dynamics and (stationary) rewards, 40-step episodes, and mildly stochastic dynamics. As baselines, we use MFVI-Bwd (Wu and Khardon, 2022), CSVI-Bwd (Wu and Khardon, 2022), ARollout (\(^{^{}}_{=0}\), see Cui et al., 2015), and SOGBOFA-LC (\(^{}_{=0}\), see Cui et al., 2019). We provide details about these competing methods in Appendix I.3. From our proposed variational framework, we use7 VI LP (\(^{}_{=0}\)), and VBP8. (\(^{}_{ 0}\)).

Fig. 3 shows the average cumulative reward for all domains and methods. Four domains are highly deterministic (\(H_{}<0.05\)), but planning inference manages to be competitive wrt the best baselines.

The other two are Game of Life and SysAdmin, which have an average \(H_{}\) of 0.18 and 0.23 respectively. We notice a significant advantage of our proposals wrt the most sophisticated method, SOGBOFA-LC (\(_{=0}^{}\)). This is consistent with our expectation of MMAP degrading with increased stochasticity (see Section 4). Elevators is well known for its challenging rewards (Cui et al., 2015) and the only one for which ARollout performs noticeably worse. In this domain, VBP manages to match or exceed SOGBOFA-LC on most instances. Overall, we observe that VBP is more consistent across varying stochasticities, matching the performance of the best method for each dataset. None of these domains reach the larger stochasticity levels shown in Fig. 2 where VBP dominates. VI LP also performs generally well, although not as well as VBP due to the missing mutual information mentioned in Section 3.2. See Appendix I.3 for details.

## 7 Discussion

The variational framework offers a powerful tool to analyze and understand how different existing types of inference approximate planning, the key role of stochasticity, what the ideal type of inference for planning is, and how to design new approximations. We hope that the introduced VI perspective will further the understanding of existing methods and lead to novel planning algorithms.

Figure 3: Cumulative rewards on 6 problem domains from the ICAPS 2011 IPPC. A small horizontal jitter was introduced in all data points for visual clarity. Each cumulative reward is averaged over 30 simulations per instance. Datasets are ordered from left to right and top to bottom by increasing normalized entropy levels. Only the last two have a significant stochasticity level >5%.