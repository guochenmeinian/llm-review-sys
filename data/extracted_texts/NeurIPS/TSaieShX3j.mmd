# SGD vs GD: Rank Deficiency in Linear Networks

Aditya Varre

EPFL

aditya.varre@epfl.ch

&Margarita Sagitova

EPFL

margarita.sagitova@epfl.ch

&Nicolas Flammarion

EPFL

nicolas.flammarion@epfl.ch

###### Abstract

In this article, we study the behaviour of continuous-time gradient methods on a two-layer linear network with square loss. A dichotomy between SGD and GD is revealed: GD preserves the rank at initialization while (label noise) SGD diminishes the rank regardless of the initialization. We demonstrate this rank deficiency by studying the time evolution of the _determinant_ of a matrix of parameters. To further understand this phenomenon, we derive the stochastic differential equation (SDE) governing the eigenvalues of the parameter matrix. This SDE unveils a _repulsive force_ between the eigenvalues: a key regularization mechanism which induces rank deficiency. Our results are well supported by experiments illustrating the phenomenon beyond linear networks and regression tasks.

## 1 Introduction

Deep neural networks have significantly advanced machine learning in recent decades. A key attribute of these models is their ability, despite being heavily overparameterized, to learn effective representations which generalizes well across different tasks. This capability has sparked substantial interest in understanding how neural networks learn internal representations for specific tasks . Gaining deeper insights into these mechanisms is crucial for enhancing model interpretability and refining training and application methodologies in real-world scenarios.

The success in learning these representations is often attributed to the gradient methods used in training. These methods navigate complex non-convex landscapes, finding solutions that not only minimize the training objective but also yield effective representations. They achieve this generalization while avoiding the spurious features that could potentially arise from the models' large number of parameters. Empirical studies have shown that the stochastic noise in gradient algorithms enhances generalization  by favoring solutions with simpler structures that mitigate spurious features . This paper address the overarching question:

_How does stochasticity facilitate the discovery of solutions with simplified structures?_

We explore this question using a simplified model: a single hidden-layer linear network. Despite lacking non-linearity, such networks capture some intricate phenomena of real-world deep networks and have been extensively studied to understand convergence , learning dynamics , and the implicit bias of optimization algorithms . Our work builds on this foundation by comparing stochastic algorithms with their deterministic counterparts, focusing on how these differences influence the learning of simpler structures.

Specifically, we analyze vector regression on two-layer linear networks trained with both gradient flow and stochastic gradient flow methods. Our contributions include:

* In Section 4, we track the evolution of the determinant of the parameter matrix under gradient flow and stochastic gradient flow. We show that stochastic gradient flow drives the determinant towards zero, effectively removing irrelevant direction(s).
* In Section 5, we derive a stochastic differential equation that describes the behavior of the eigenvalues of the parameter matrix. This analysis reveals a repulsive force between eigenvalues that pushes them apart and a geometric Brownian motion that pulls them toward zero.
* In Section 6, we discuss the generalizability of our approach beyond square loss and various noise models, including discrete step sizes. Finally, we present experimental results in Section 7 that support our theoretical findings.

## 2 Related Work

Our work lies at the convergence of distinct research topics:

**Effect of SGD on generalization.** The relationship between the stochasticity of SGD and its generalization capabilities has been extensively examined (Mandt et al., 2016; Jastrzebski et al., 2018; He et al., 2019; Hoffer et al., 2017; Kleinberg et al., 2018). Notably, SGD tends to yield models with superior generalization compared to gradient descent (Keskar et al., 2017; Jastrzebski et al., 2018; He et al., 2019). Various explorations into this phenomenon have been conducted through various approaches: hypothesizing that SGD favors flatter minima linked to better generalization, as opposed to sharp minima associated with poor generalization (Hochreiter and Schmidhuber, 1997; Keskar et al., 2017; Andriushchenko et al., 2023), using a random walk on a random landscape model to understand the impact of stochasticity (Hoffer et al., 2017), proposing that the inherent noise in SGD smooths the loss landscape (Kleinberg et al., 2018), and exploring the implications of dynamical stability (Wu et al., 2018).

**Stochastic dynamics and Label Noise.** Recent literature has explored label noise-driven Gradient Descent as an effective method to probe the beneficial impact of stochasticity on generalization, with two distinct perspectives emerging. Firstly, an asymptotic view on general model parametrization is considered, where Blanc et al. (2020), Damian et al. (2021) suggest that stochastic dynamics preferentially optimize a hidden objective linked to the curvature of the loss. In a related vein, Li et al. (2021) demonstrates appropriate limiting dynamics on the manifold of interpolators through time rescaling. Secondly, specifically for diagonal linear networks, HaoChen et al. (2021), Pillaud-Vivien et al. (2022) observe a similar collapsing effect due to label noise but with a finer characterization of the limiting process. Finally, in the absence of label noise, Pesme et al. (2021), Even et al. (2023) have characterized the outcomes of stochastic GF and GD for diagonal linear networks as the solutions to an implicit regularization problem that results in sparser solutions than without stochasticity. Recently, Ghosh et al. (2023) further exhibit a similar sparser features effect for single-neuron autoencoder. Chen et al. (2023) provides a condition under which an invariant set is attractive for SGD -- characterizing the local behavior around these sets. The paper also studies linear networks in a teacher-student setup, however due to structured label-noise (Chen et al., 2023, A2 in p.30), the analysis falls short of capturing the repulsive force in the singular values.

**Linear Networks.** The study of two-layer linear networks has been explored extensively, particularly when optimized using gradient flow on the square loss, across various settings including zero-balance initialization and whitened data Fukumizu (1998), Saxe et al. (2014, 2019), Braun et al. (2022). Early work by Saxe et al. (2014, 2019) elucidates the temporal changes in the singular values of the predictor, assuming decoupled dynamics and a specific data-dependent weight initialization. This condition is broadened by the analyses of Fukumizu (1998) and Braun et al. (2022), Tartomun et al. (2021), who apply solutions from a matrix Riccati equation to characterize the weights dynamics under full-rank network initialization. Furthermore, Gidel et al. (2019) extends the existing framework by relaxing the whitened data assumption, conducting a perturbation analysis, and discussing the temporal evolution of the weight matrices' singular values. Additionally, Varre et al. (2024) eliminates the need for zero-balanced and full-rank initializations. Their study provides detailed formulas for weight evolution as a function of the initial scale, also studies a simple version of a stochastic flow without the drift. Wang and Jacot (2023) studied the implicit bias of SGD with \(_{2}\)-regularization.

Matrix valued stochastic process and their eigenvalues.** Stochastic process on the space of symmetric (or Hermitian) matrices and the evolution of their eigenvalues are well studied since Dyson (1962). These techniques were further developed by Bru (1989, 1991) to study perturbations of principal component analysis and the eigenvalues of Wishart processes. Norris et al. (1986); Graczyk and Malecki (2013) applied SDE-based techniques to study the eigenvalues and eigenvectors of Brownian motion on ellipsoids.

## 3 Linear networks and continuous-time gradient method

**Notation** We use \(.,.\) to denote the inner product, i.e., \( u,v=u^{}v\) for vectors, and \( A,B=(AB^{})\) for matrices. \(_{d}\) denotes the identity matrix of dimension \(d\) and \(0_{p k}\) denote the matrix with all zero entries of dimension \(p k\).

**Vector regression.** We study the vector regression problems with inputs \(x_{1},,x_{n}\) in \((^{p})^{n}\) and outputs \(y_{1},,y_{n}\) in \((^{k})^{n}\). We consider the minimization of the square loss over a class of parametric models \(=\{f_{}():^{p}^{k} ^{d}\}\) specified in the next paragraph. The train loss therefore can be written as \(()=_{i=1}^{n}\|y_{i}-f_{ }(x_{i})\|^{2}\).

**Parameterization with a linear network.** We focus on two-layer linear neural networks of width \(l^{*}\). The model is described by the parameterization \(=(_{1},_{2})\), where \(_{1}^{p l}\) and \(_{2}^{l k}\), and the function \(f_{}(x)=_{2}^{}_{1}^{}x\). This model is linear with respect to the input \(x\). In terms of expressivity, it is comparable to the linear class of predictors, represented as \(f_{}(x)=^{}x\), where \(\) equals \(_{1}_{2}\). Throughout our analysis, we denote the equivalent linear predictor of the network as \(\). A key aspect of this parametrization is that the prediction function \(f_{}\) is positive homogeneous of degree 2 with respect to \(\): specifically, for any \(\), \(f_{}=^{2}f_{}\). This property mirrors that of two-layer ReLU networks and significantly influences the loss landscape navigated by the parameters \(\). It is important to note that this parameterization introduces some redundancy, a single linear predictor \(\) can have multiple representations \(_{1},_{2}\) such that \(_{1}_{2}=\). Some representations have a rich structure whereas other resemble random features. For example, consider the case of scalar regression (\(k=1\)), for a vector \(\) there exists rich parameterizations where all the neurons, i.e., columns of \(_{1}\) align with \(\) and also some lazy structures where \(_{1}\) resembles a random matrix (Chizat et al., 2019; Varre et al., 2023).

**Train loss.** By defining \(X^{}=[x_{1},,x_{n}]\) and \(Y^{}=[y_{1},,y_{n}]\), the loss function is given by:

\[(_{1},_{2})=\|X _{1}_{2}-Y\|^{2}.\] (3.1)

For simplicity, we adjust for the normalization factor \(n\) by rescaling the data to \((X,Y)(}{{}},}{{}})\), thereby implicitly considering it in the loss function without directly mentioning \(n\) in the formula. Note that the loss is non-convex in \(_{1},_{2}\).

**Gradient flow.** The dynamics induced in parameter space by running GF on Equation (3.1) is given by

\[_{1} =-_{_{1}}(_{1},_{2})t =X^{}(Y-X_{1}_{2})_{2}^{} t,\] (3.2) \[_{2} =-_{_{2}}(_{1},_{2})t =_{1}^{}X^{}(Y-X_{1}_{2}) t.\] (3.3)

Introducing the block matrix, \(=[_{1}^{}_{2}] ^{l(p+k)}\) and denoting the residual matrix by \(=X^{}(Y-X_{1}_{2})\), the evolution of \(\) can be written as

\[=[_{1}^{} _{2}]=[_{2}^{}t_{1}^{}t]=[_{1}^{ }_{2}]0_{p p}&\\ ^{}&0_{k k}t.\]

The gradient flow can therefore be compactly written as

\[=t, =0_{p p}&\\ ^{}&0_{k k}.\] (3.4)

The gradient flow (GF), when expressed in this form, reveals an inherent multiplicative structure with respect to \(\) in the gradient of the loss. As we see in subsequent sections, this representation of the gradient flow with block matrices proves to be very convenient.

**Label noise gradient descent.** Label noise gradient descent (LNGD) is a theoretically studied alternative to SGD that mirrors its practical behavior by sharing the geometric properties of the noise Blanc et al. (2020); Damian et al. (2021). Let \(_{t}^{n k}\), where each entry of \(_{t}\) is an independent Gaussian random variable. At iteration \(t\), the labels are perturbed with this Gaussian noise at an intensity \(\), i.e., \(=Y+_{t}\). The LNGD algorithm updates the iterates with a step size \(\) in the direction of the gradient computed after the labels have been perturbed, as follows:

\[_{1}^{t+1}=_{1}^{t}-_{_{1}}(,,_{1}^{t},_{2}^{t}); _{2}^{t+1}=_{2}^{t}-_{_{2}} (,,_{1}^{t},_{2}^{ t}),\]

where, by an abuse of notation, \((Y,,_{1},_{2})=}{{2}}\|X_{1}_{2}-Y\|^{2}\). The iterates can then be restructured into a block matrix:

\[^{t+1}=^{t}-^{t}_{ t}-^{t}_{t},_{t}=0_{p p}&X^{}_{t}\\ _{t}^{}X&0_{k k},\] (3.5)

and \(J_{t}\) is defined as in Equation (3.4).

**Stochastic gradient flow (SGF).** We aim to model the aforementioned LNGD in continuous time using an appropriate SDE. Stochastic continuous-time counterparts of discrete stochastic gradient algorithms are favored for their enhanced amenability to theoretical analysis. We propose the following stochastic differential equation (SDE) to model LNGD in continuous time:

\[=[t+],=0_{p  p}&X^{}_{t}\\ _{t}^{}X&0_{k k},\] (3.6)

where \(_{t}\) denotes a matrix Brownian motion in \(^{n k}\). LNGD as defined in Equation (3.5), can be interpreted as the the Euler-Maryama discretization of the above SGF with a stepsize \(\). Although the inclusion of step size in the continuous-time modeling of an SDE may seem counter-intuitive, it is a necessary component (Li et al., 2019). As all the terms of the SDE in Equation (3.6) are polynomial in \(\), both the drift and diffusion terms are locally Lipschitz continuous. Hence, the solution of the SDE is uniquely defined up to the explosion time \(_{}\)(see, e.g., Khasminskii, 2012). Furthermore, the explosion time can be proven to be infinite (\(_{}=\) almost surely), by using that the GF does not diverge and applying the techniques outlined by Pillaud-Vivien et al. (2022, Proposition 10).

**Initialization.** The dynamics of gradient methods on homogeneous models are significantly influenced by initialization, which determines the regime they operate in--specifically, the lazy regime for large initializations and the rich regime for small ones (Chizat et al., 2019; Woodworth et al., 2020). Thus, the scale of initialization has garnered significant interest, particularly its impact on the training of linear and non-linear networks with GD (Woodworth et al., 2020; Boursier et al., 2022). It is observed that stochastic methods eliminate the dependence on initialization (Pesme et al., 2021).

**Conserved quantities and balanceness.** Gradient flows follow specific conservation laws along their trajectory (Marcotte et al., 2023), maintaining characteristics of the initial conditions. For linear networks, this conservation manifests as the _balanceness property_(Du et al., 2018), described by:

\[=_{1}^{}_{1}-_{2}_ {2}^{}=_{1}^{}(0)_{1}(0)-_{2}(0) _{2}^{}(0).\]

As a result, Saxe et al. (2014); Arora et al. (2018, 2019) have adopted _balanced initialization_, where \((0)=0\), to ensure that weight matrices remain low rank throughout the trajectory. However, unbalanced initialization do not preserve these simple low-rank structures, as aspects of the initial conditions persist.

In contrast, stochastic methods do not adhere to these conservation laws (Ziyin et al., 2023) and the evolution of the imbalance \(\) for SGF is

\[=_{1}^{}_{1} -_{2}_{2}^{}=(XX^{}) \,_{2}_{2}^{}t-k\,\,_{1}^{}X ^{}X_{1}t.\]

While there is no diffusion term in the derivative, the matrices remain stochastic and no definitive conclusions can be drawn from this. However, in the case where \(k=p\) and \(X^{}X=_{p}\), it can be shown that \(_{1}^{}_{1}-_{2}_{2}^{} 0\), indicating that the stochastic noise eliminates initial imbalance.

**Conclusion.** Understanding how stochastic methods mitigate dependency on initialization requires exploring beyond the evolution of the imbalance \(\). To this end, we identify and discuss other conserved quantities, such as the determinant of the block matrix \(^{}\) in the following sections.

Separation between Gradient Flow through determinant

Here, we present our first separation result between GF and SGF. While the determinant of the parameters is preserved in GF, it is driven to zero by the stochasticity of SGF, leading to a simplistic low-rank structure.

### Determinant evolution of the gradient flow

The theorem below demonstrates that the determinant of the parameters is preserved in gradient flow.

**Theorem 4.1**.: _For the gradient flow defined in Equation (3.4), the following property holds,_

\[\!((^{}))=0.\]

_Hence, \(((t)^{}(t))=(_{0 }^{}_{0})\), where \(_{0}=(0)\) is the initialisation at time \(t=0\)._

The proof presented in the App. B.1, is based on straightforward computations of the derivative of the determinant and the fact that the matrix \(\) has zero trace. We note that the simplicity of the proof arises from the strategically chosen block structure of \(\). This result would have been less straightforward with different parametrizations, which likely explains why such a simple finding appears to be novel. The theorem implies that the determinant of \(\) along the trajectory remains equal to the determinant at initialization. If \(_{0}^{}_{0}\) is full-rank initially, meaning the determinant is non-zero, the theorem ensures that the determinant of \(\) remains non-zero. Consequently, the rank of \(\) does not diminish along the trajectory. When \(l p+k\), i.e., the hidden layer has a large width and \(_{1},\,_{2}\) are initialized randomly from a Gaussian distribution, \(_{0}^{}_{0}\) has full rank almost surely. The theorem also reveals some implications regarding the impact of initialization scale. Note that \(_{min}(A)[n]{ A}\), indicating that when the scale of initialization is very small, at least one singular value of \(\) is small.

### Determinant evolution of the stochastic gradient flow

In contrast, the theorem presented below demonstrates that the determinant of the parameters converges to zero in stochastic gradient flow.

**Theorem 4.2**.: _For the SDE, defined in the Equation (3.6), for \(t_{}\), the following property holds for the evolution of determinant_

\[\!((^{}))=-2 (X^{}X)\!(^{})\!t.\]

_Hence, \(((t)^{}(t))=(_{0 }^{}_{0})\!\{-2(X^{ }X)\!t\}\), where \(_{0}\) is the initialization._

Although the evolution of the parameters in SGF is random, the evolution of the determinant is deterministic. The theorem highlights a striking phenomenon: the noise in SGF diminishes the determinant along the trajectory, leading to a simplification of the network over time. The larger the noise and the stepsize, the faster the determinant vanishes. The vanishing of the determinant suggests that the rank of the parameters decreases by at least one, effectively eliminating some components. It holds for any initialization of \(_{0}\) and indicates how the SGF overrides some aspects of initialization. The proof uses the fact that stochastic Brownian term in the SDE, through Ito's calculus, introduces a negative drift, ultimately driving the determinant to zero (refer to B.3 for the proof).

**Limitations.** Given the large width of the hidden layer, the determinant converging to zero does not fully reveal the complexity of the situation. It merely indicates that at least one singular value is approaching zero. Furthermore, the theorem provides limited insights when the determinant is already \(0\) at initialization, \(_{0}=0\) which happens whenever \(l<p+k\). Next, we explore the mechanisms behind this low-rank phenomenon, suggesting that the repulsive forces induced by stochasticity drive the spurious singular values to zero as seen in the right plot of Figure 1.

## 5 Mechanism behind the low-rank phenomenon

In this section, we investigate the evolution of singular values under stochastic training to gain deeper insights into the low-rank phenomenon. To simplify the discussion, throughout the section we consider the case where \(k=1\) and for notational convenience, we let \(_{1}=,_{2}=\). Additionally, we assume that \(l p\), however the results can be extended to any \(l\).

**Warm-up: Comparison with diagonal networks.** Let \(=^{}\) be the singular value decomposition (assuming \(l p\)). The predictor \(\) can be expressed as

\[=^{}= [],=^{}.\]

This expression reveals a Hadamard product between \(\) and \(\), reminiscent of diagonal networks which are widely studied to understand the nonconvex dynamics of gradient algorithms (Woodworth et al., 2020; Pesme et al., 2021; Pillaud-Vivien et al., 2022). In the context of diagonal networks, SGD is known to provably induce sparsity in predictions. Similarly, for linear networks, SGF may induce sparsity in terms of the singular value \(\). We next derive the SDE governing the evolution of the singular values \(\) of the weight matrix to gain a clearer understanding of the low-rank phenomenon.

**Scalar Regression.** We assume that the data is isotropic, i.e., \(X=_{p}\). Under these conditions, the loss function for scalar regression can be written as

\[(,)=\|y- \|^{2}.\] (5.1)

We train the above objective with SGF, formulated as follows,

\[=(y-)^{}t+ \ _{t}^{}; =^{}(y-)t+\ ^{}_{t}.\] (5.2)

where \(_{t}\) is the standard Brownian motion in \(^{p}\). For analytical convenience, we rescale the time \(t}{{}}\) and use the process \(=}{{}}(y-) t+_{t}\). The SGF can then be rewritten as,

\[=\ ^{}; =^{}.\] (5.3)

Our focus is on understanding the evolution of the singular values of the matrix \(\). This aim is facilitated by considering the symmetric matrix \(=^{}\), whose eigenvalues are the squares of the singular values of \(\). Taking the derivative of \(\), we find

\[=^{}+^{} +^{}=^{}+^{} ^{}+p^{}t.\] (5.4)

Note that \(xy\) represents \(d[x,y]\) for any continuous semi-martingales \(x,y\)(see, e.g., Ikeda and Watanabe, 1981, chapter 3 for reference).

**Eigenvalues of a matrix-valued stochastic process.** We leverage tools from the study of eigenvalues of matrix-valued stochastic processes (Bru, 1989; Graczyk and Malecki, 2013) to derive the evolution of the eigenvalues of \(\) in the theorem that follows.

**Theorem 5.1**.: _Let \(_{1}>>_{l}\) be the order of the eigenvalues of the matrix \(\) defined by Equation (5.4). Let the collision time for the eigenvalues be defined as_

\[=\{ t:_{i}(t)=_{j}(t)1 i j  l\}.\] (5.5)

_For \(t\), the eigenvalues are semi-martingales given by the solution of the following SDE_

\[(_{i})=p_{i}^{2}\ t+_{ j=1,\\ j i}^{l}_{i}_{j}^{2}+_{j} _{i}^{2}}{_{i}-_{j}}t+2_{i}_{i}^{2}}(})_{i}\] (5.6)

_where \(=^{}\) and \((})_{i}=}{{}} (_{i},y-_{i}_{i}^{2}})t+_{i}\) with \(_{i}\) being the \(i^{th}\) column of \(\) and \((_{0},,_{l-1})\) is the standard Brownian motion in \(^{l}\). The evolution of \(_{i}\) and \(\) are presented in the appendix B.5._

This theorem can be interpreted as the stochastic counterpart to the evolution of eigenvalues previously described for linear networks by Arora et al. (2019); Varre et al. (2023). The derivation of the eigenvalues is inspired by the work of Bru(1989).

The evolution of the eigenvalues features a key term highlighted in Equation (5.6) consisting of the sum of skew-symmetric elements \(_{i}_{j}^{2}+_{j}_{i}^{2}}}{ _{i}-_{j}}\). For a pair of indices \((i_{0},j_{0})\) with \(i_{0}<j_{0}\) and thus \(_{i_{0}}>_{j_{0}}\), the term \(_{i_{0}}_{j_{0}}^{2}+_{j_{0}}_{i_{0}}^{2}}}{_{i_{0}}-_{j_{0}}}\) positively influences the evolution of the larger eigenvalue \(_{i_{0}}\) and negatively affects the smaller eigenvalue \(_{j_{0}}\). Therefore, this force is repulsive,driving the eigenvalues apart and increasing their gap. Another factor influencing the dynamics is the presence of Geometric Brownian motion, where the singular value \(_{i}\) multiplicatively influences the Brownian motion as \(_{i}_{i}^{2}}(})_{i}\), similar to what is observed in diagonal linear networks (refer to the previous discussion for similarities). This effect tends to pull the singular values toward zero. Together with the fact that \((_{i},_{i})=(0,0)\) represents a fixed point of the dynamics, these two forces collectively push redundant singular values toward zero.

To further understand the interplay of repulsive forces and geometric Brownian motion, we consider the evolution of the smaller singular value \(_{p}\) for \(l=p\). Using the Ito chain rule, we analyze the evolution of \(_{p}\), expressed as,

\[(_{p})=p_{p}^{2}}{_{p}}\; t+_{p}}_{j=1,\\ j p}^{p}_{p}_{j}^{2}+_{j} _{p}^{2}}{_{p}-_{j}}t-2_{p}^{2}}{_{p}}+2_{p}^{2}}{_{p}}} (})_{p}.\]

Using that \(_{p}_{j}^{2}+_{j}_{p}^{2}/_ {p}-_{j}<-_{p}^{2}\), for all indices \(j\), the repulsive force accumulates to \(-(p-1)(_{p}^{2}/_{p})\) and the Ito correction term from the logarithm contributes an additional \(-2(_{p}^{2}/_{p})\) (the GBM component) thus offsetting the positive drift of \(p(_{p}^{2}/_{p})\). In the case of \(l p\), considering a polynomial \(x^{}\) with an appropriate \(\) would demonstrate similar behaviour. This discussion outlines the forces at play, yet a complete characterization of the solution of the SDE Equation (5.6) remains missing. Moreover, we have not established that the eigenvalues avoid a.s. collision, i.e., the explosion time \(_{}=\) which is in itself a significant challenge .

**A simplified two-vector problem.** To enhance our understanding of the SDE governing the evolution of the eigenvalues detailed in Equation (5.6), we consider the large noise limit. In this scenario, the process described in Equation (5.3) simplifies to a purely noise-driven process without drift:

\[=\;_{t}^{}; =^{}_{t}.\]

This SDE exhibits notable symmetry; allowing for an analysis using a matrix with sub-sampled columns. Let \(S\) be any subset of \(1,,l\), with \((_{i})_{i=1}^{l}\) representing the columns of \(\). We define \(_{S}^{p|S|}\) as the subsampled matrix obtained by selecting columns \(_{i}\) where \(i S\), and similarly, we define a subsampled vector \(_{S}\) by selecting the corresponding coordinates. The SDE restricted to the set \(S\) is structured as follows:

\[_{S}=\;_{t}_{S}^{}; _{S}=_{S}^{}_{t}.\]

To demonstrate that the columns of \(\) align, we leverage the symmetry of the SDE by examining the restricted problem on every pair of rows \(S=\{i,j\}\), and proving alignment within this subset. This approach leads us to consider the two vector problem (\(l=2\)), where \(=[_{1}|_{2}]\) and \(_{1},_{2}^{p}\), \(^{2}\). We describe the behavior of the eigenvalues for this two-vector problem in the theorem below.

**Theorem 5.2**.: _In the large noise limit, let \(_{0}>_{1}\) be the eigenvalues of \(\), the following properties hold, for \(t\) defined by \(=\{ t:_{0}(t)=_{1}(t)\}\),_

* \(_{0},_{1}\) _are greater than zero almost surely,_
* _for_ \(=(p-3)/2\)_,_ \(_{0}^{-}\) _is a super-martingale while_ \(_{1}^{-}\) _is a sub-martingale._

This model for \(l=2\) mirrors the dynamics of the Wishart process studied by Bru , motivating the exploration of the evolution of an appropriately chosen exponent of \(_{0},_{1}\). The first part of the theorem arises from the fact that \(_{1}^{-}_{2}^{-}\) is a local continuous martingale that cannot explode to infinity in finite time. The second part highlights a clear separation between the eigenvalues: one is a sub-martingale that consistently increases in expectation, while the other is a super-martingale that diminishes (note that the eigenvalues are raised to a negative power). This dynamic, coupled with the symmetry argument, suggests that for every pair of columns, there is a component that strengthens the alignment through its increases in expectation. Refer to App. B.6 for the proof.

**Conclusion.** In this section, we derive the SDE of eigenvalues for the matrix of parameters evolving under SGF. This derivation provides deeper insights into the mechanisms contributing to low-rank behavior. Specifically, repulsive forces drive the eigenvalues apart, while the geometric Brownian motion pulls them towards zero. These forces, unique to training with SGF, highlight the regularization effects of stochastic methods compared to gradient flow. However, fully characterizing the solution of this SDE remains a challenging open problem we let as future work.

Generalization to other settings

In this section, we generalize our results beyond the square loss and the label noise gradient flow. We consider the general framework of a loss function over the weight product \(_{1}_{2}\) defined as

\[(_{1},_{2})=}( _{1}_{2})=_{(x,y)}[( _{1}_{2};x,y)],\]

In this framework, the loss function \(\) combines the prediction loss directly with the parametrized model \(f_{}\). This approach applies, for example, to classification problems using linear networks where \(\) might represent any classification loss and \(f_{}=_{1}_{2}\). It also directly extends to more complex architectures where \(f_{}=(_{1}_{2})\) for an activation function \(\), including settings like a self-attention layer with frozen value vectors. We denote the product by \(=_{1}_{2}\) noting it solely controls the loss. We investigate the evolution of the weight matrix determinant for a general loss across various algorithms, from gradient flow to gradient descent, and demonstrate that a similar separation occurs due to stochasticity.

**Warm-up: Gradient flow.** The gradient flow on the loss \(\) can be written as the following,

\[=t,=0_{p p}&-}() \\ -}()^{}&0_{k k}.\] (6.1)

Following a similar proof as in Theorem4.1, we obtain that \((^{})=0\). For separable classification problem, the gradient flow converges to infinity (Soudry et al., 2018; Ji and Telgarsky, 2019), hence, after appropriate rescaling, the layers are aligned, as shown by Ji and Telgarsky (2019). Next, we contrast this result with the outcomes observed in stochastic and discrete algorithms.

**Continuous modelling of SGD.** We consider the SGD algorithm with a batch size \(B\). We denote the mini-batch version of the loss functions \(\) and \(}\) as \(_{B}\) and \(}_{B}\), respectively. The SGD update with stepsize \(\) can be represented with the following block structure,

\[^{t+1}=^{t}-^{t}^{t}-^{t}^{t},^{t}=0_{p p}&-(}( {})-}_{B}())\\ -(}()-}_{B }())^{}&0_{k k}.\]

We denote the SGD noise as \(g_{t}=(}()-}_{B}())\) and the noise covariance as \(_{t}=[g^{t}(g^{t})^{}]\) where the expectation is over all the minibatches. Following Li et al. (2019), the SGD update can be modelled with the following SDE,

\[=-t-, =0_{p p}&-_{t}^{1/2} _{t}\\ -(_{t}^{1/2}_{t})^{}&0_{k k} .\] (6.2)

The main difference with SGF is that, in overparameterized problems, the noise covariance is time-varying and decreases to zero upon convergence. Using TheoremB.3, the evolution of the determinant of \(}=^{}\) is given by \((())=-( )((t))t\) and can be explicitly solved as

\[(()(t))=((0) )\{-_{0}^{t}((s))s\}.\]

Hence, the decay in the determinant is governed by the integral \(_{0}^{}((t))t\) which is a stochastic quantity. \(((t))\) represents the strength of the stochastic noise, which, in over-parameterized regression, is proportional to the loss, i.e., \(((t))()\)(Pesme et al., 2021). Therefore, the rate of decay in the determinant depends on \(_{0}^{}(((t)))t\), with slower convergence leading to a simpler model at convergence, as observed in the case of diagonal networks by Pesme et al. (2021). The result above also holds for _non-separable_ classification tasks where the noise of SGD drives the determinant to \(0\), a scenario not covered by the previous analysis of Ji and Telgarsky (2019).

**Discrete gradient algorithms.** We can extend the previous results to discrete (possibly stochastic) gradient algorithm. Both algorithms can be written as

\[_{t+1}=_{t}(_{p+k}+_{t}),\]

for stepsize \(\) and \(_{t}\) the possibly stochastic block gradient matrix defined in Equation (6.1). In the context of discrete algorithms, the determinant is controlled by the following lemma (refer to B.4 for the proof).

**Lemma 6.1**.: _When \(l=p+k\) and \(^{2}_{t}_{F}^{2} 1\), the following property holds for the determinant,_

\[|\,_{t+1}|\!(-}{2} _{t}_{F}^{2})\!|\,_{t}|.\]

If the factor \(^{2}_{t}_{F}^{2} 1\) at every iteration \(t\), the determinant is reduced by the discrete step size. However, there is a tradeoff: the sum \(S_{t=0}^{}^{2}_{t}_{F}^{2}\) can be finite, indicating that it does not completely drive the determinant to zero. Increasing \(\) to increase \(S\) might lead to instability and divergence. Furthermore, since \(_{t}_{F}^{2}(_{t})\), there is an additional tradeoff between convergence and the simplicity of the parameters. This illustrates how step sizes that produce non-convergent training loss patterns, such as the catapult effect (Lewkowycz et al., 2020) or the edge of stability mechanisms (Cohen et al., 2020), can simplify the network's parameters.

## 7 Experimental evidence

We consider a regression problem on synthetic data with \(n=1000\) samples of Gaussian data in \(^{5}\) (\(p=5\)) with labels in \(^{2}\) (\(k=2\)) generated by some ground truth \(^{5 2}\), the width of the network is \(l=10\). We use Gaussian initialization of the network parameters with entries from \((0,1)\). Experiments details can be found in the appendix C. In the left plot of Figure 1, we show the time evolution of the determinant of matrix \(\). As suggested by theorems 4.1 and 4.2, in the case without label noise, \((^{})\) stays constant, while with the Label Noise of intensity \(=2\) it goes to zero with time. In the right plot of Figure 1, we demonstrate the time evolution of the top-5 singular values of the matrix \(_{1}\). Note that in the case of Gradient Flow all except the first \(k\) singular values (\(_{0}\) and \(_{1}\)) stay at the same scale, while adding Label Noise forces smallest \(d+l-k\) singular values (\(_{2},_{3},\) and \(_{4}\)) to tend toward zero. Further experiments illustrate in Figure 2 the evolution of singular values of parameter matrix \(_{1}\) when optimized with SGD, for classification tasks and with ReLU network. These results also confirm that the beneficial effects of stochasticity hold in these contexts.

Figure 1: Evolution of the model characteristics for gradient flow (\(=0\)) and stochastic gradient flow (\(=2\)). Left: Determinant of \(\). Right: Top-5 singular values of \(_{1}\).

Figure 2: Evolution of the top-5 singular values of \(_{1}\) for SGD with small and large stepsizes \(\). Left: Regression with MSE loss, linear network. Middle: Classification with logistic loss, linear network. Right: Regression with MSE loss, 2-layer ReLU network.

Conclusion

In this paper, we demonstrate a distinct separation between GF and SGF when trained on linear networks. This separation is obtained by tracking the evolution of the determinant of the parameter matrix. However, while the determinant is a significant factor, it does not fully capture the implicit regularization effects. Notably, the determinant mirrors the imbalance \(^{2}-^{2}\) in diagonal networks represented by \(\), whose dynamics play a crucial role in attuning the implicit regularization across various algorithms (Woodworth et al., 2020; Pesme et al., 2021; Papazov et al., 2024). Our analysis presents the initial step in deciphering implicit regularization for stochastic methods in linear networks, yet achieving a complete characterization remains a promising direction for future research.