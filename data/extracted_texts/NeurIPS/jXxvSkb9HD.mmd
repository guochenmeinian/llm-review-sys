# Statistical Multicriteria Benchmarking

via the GSD-Front

 Christoph Jansen\({}^{1,}\)

c.jansen@lancaster.ac.uk

Georg Schollmeyer\({}^{2,}\)

georg.schollmeyer@stat.uni-muenchen.de

Julian Rodemann\({}^{2,}\)

julian@stat.uni-muenchen.de

Hannah Blocher\({}^{2,}\)

hannah.blocher@stat.uni-muenchen.de

Thomas Augustin\({}^{2}\)

thomas.augustin@stat.uni-muenchen.de

\({}^{1}\)School of Computing & Communications

Lancaster University Leipzig

Leipzig, Germany

\({}^{2}\)Department of Statistics

Ludwig-Maximilians-Universitat Munchen

Munich, Germany

marks equal contribution.

###### Abstract

Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.

## 1 Introduction

The comparison of classifiers in machine learning is usually carried out using _quality metrics_\(:\), i.e., bounded functions assigning a real number to every pair \((C,D)\) of classifier and data set from a suitable domain \(\), where, by construction, higher numbers indicate better quality. However, in many applications, the choice of a unique quality metric used for the comparison is not self-evident. Instead, competing quality metrics are available, each of which can be well-motivated but may lead to a different ranking of the analyzed classifiers. One attempt to safeguard against this effect is to use _multidimensional quality metrics_: instead of a single metric, one chooses a set of metrics \(:=(_{1},,_{n}):^ {n}\) that - taken together - provide a balancedfoundation for assessing the quality of classifiers. Generally, we distinguish two (related but) different motivations for choosing multidimensional quality metrics:

**Performance is a latent construct:** The application at hand suggests a very clear evaluation concept, which, however, is too complex to be expressed in terms of a single metric. In this case, the _latent_ construct to evaluate is operationalized with a set of quality metrics (that serve as an approximation). For example, the latent construct of _robust accuracy_ can be operationalized by taking together the following three quality metrics: _Accuracy_ of a classifier (i.e., the proportion of correctly predicted labels), and _robustness_ of this proportion under weak perturbations of the data in either the features or the target variable. This will be exemplified in Section 5.2 using the PMLB benchmark suite.

**Quality is a multidimensional concept:** Even if the application at hand suggests evaluation criteria that can be perfectly expressed using quality metrics, it can still be desirable to compare the classifiers under consideration in terms of various contentual dimensions. For example, one can be interested in how well a classifier performs in the trade-off between _accuracy_ and _computation time_ in the training and the test phase: Clearly distinguishable contentual dimensions are included and the analysis aims at investigating how the different classifiers under consideration trade-off between these dimensions. This will be exemplified in Section 5.1 using one of OpenML's benchmark suites.

Regardless of the motivation for considering multidimensional quality metrics, their interpretative advantage naturally comes at a price: Without further assumptions, classifiers will often be incomparable, as the quality metrics in the different dimensions contradict each other in their ranking.2 Already on one data set, a multidimensional quality metric only induces a (natural yet potentially incomplete) _preorder_: a classifier is rated at least as good as a competitor if (and only if) it receives at least the same metric value in each dimension. The problem of incomparability becomes even more severe for multiple data sets (as considered here). In this case, one of the following analysis paths is often chosen: (I) An expected _weighted sum_ (for example, weighted by importance) of the individual quality metrics is considered and the problem is then analyzed on this new pooled quantity.3 (II) The problem is analyzed based on the _Pareto-front_\(()\), i.e., the set of all classifiers that are not component-wise (strictly) dominated by any competitor, whose definition followed by an illustrative example are included for reference.

**Definition 1**.: _Let \(}\) be some set of data sets. The \(}\)**-Pareto front**\((,})\) of \(\) is given by_

\[C| C^{}\  D}:\ (C^{},D)(C,D)},\]

_where \(\) is the strict part of the component-wise \(\)-relation on \(^{n}\). Set \(():=\)par\((,)\)._

**Example 1**.: _Consider the following schematic example of three classifiers \(=\{C_{1},C_{2},C_{3}\}\) evaluated for a fictitious population of four data sets \(=\{D_{1},D_{2},D_{3},D_{4}\}\). Every entry gives the two-dimensional evaluation \((C,D)\) of a classifier on a data set w.r.t. predictive accuracy and the computation time for training in three ordinal categories fast, medium and slow_

  _classifier data set_ & \(D_{1}\) & \(D_{2}\) & \(D_{3}\) & \(D_{4}\) \\  \(C_{1}\) & \((0.7,)\) & \((0.8,)\) & \((0.9,)\) & \((0.95,)\) \\ \(C_{2}\) & \((0.75,)\) & \((0.85,)\) & \((0.91,)\) & \((0.96,)\) \\ \(C_{3}\) & \((0.99,)\) & \((0.91,)\) & \((0.85,)\) & \((0.75,)\) \\  

_Here, it holds that \((C_{2},D_{i})(C_{1},D_{i})\) for all \(i=1,2,3,4\), i.e., \(C_{1}\) is component-wise (strictly) dominated by \(C_{2}\). Classifiers \(C_{2}\) and \(C_{3}\) are not component-wise (strictly) dominated. Thus, the Pareto-front is given by \(()=\{C_{2},C_{3}\}\)._

Both approaches are extreme in a certain sense: (I) reduces the multidimensional information structure of the problem to one single real-valued score. Any selection of classifier based on this score will heavily depend on the choice of the weights in the sum score and, therefore, becomes dubious once this choice is not perfectly justified. This seems even more severe for problems where some of the involved quality metrics might only allow for an ordinal interpretation, e.g., feature sparseness as a proxy for interpretability , risk levels in the EU AI act  or other regulatory frameworks , robustness (see experiments in Section 5.2) or runtime levels (Section 5.1). Opposed to this, (II) seems to be very conservative: By considering classifiers that are in the Pareto-front \(()\), one (potentially) completely ignores both information encoded in the cardinality interpretable dimensions and information about the distribution of the data sets. As a trade-off between these two extremes, which utilizes the complete available information but avoids the choice of weights, it has recently been proposed to compare classifiers under multidimensional quality metrics using _generalized stochastic dominance (GSD)_. The rough idea of this approach is to first embed the range of the multivariate performance measure in a special type of relational structure, a so-called _preference system_, which then allows for also formalizing the entire information originating from the cardinal dimensions of the quality metric. A classifier is then judged at least as good as a competitor (similar to classic stochastic dominance), if its expected utility is at least as high with respect to every utility function representing (both the ordinal and the cardinal parts of) the preference system (also see Definition 5). Although GSD also induces only a preorder, the set of not strictly dominated classifiers will generally be considerably smaller than under the classical Pareto analysis. Furthermore, it avoids potentially difficult to justify assumptions about the weighting of the different quality metrics. Therefore, working with the GSD-front, as introduced below, will prove to be a very promising analysis option; it combines the advantages of the conservative Pareto analysis with those of the liberal comparison of weighted sums.

### Our contribution

**GSD-Front:** We introduce the concept of the GSD-front (see, after some preparations, Definition 6) and characterize it in Theorem 2 as more discriminative than the Pareto-front. In this sense, the GSD-front is an information-efficient way to handle the multiplicity/implicit multidimensionality of quality criteria, powerfully exploiting their ordinal and quantitative components.

**Proper handling of statistical uncertainty; estimating and testing:** Since typically the available data sets are just a sample of the corresponding universe, empirical counterparts of the major concepts are needed to do justice to the underlying statistical uncertainty. In particular, we give a sound inference framework: Firstly, we propose a set-valued estimator for the GSD-front and provide sufficient conditions for its consistency (see Theorem 1 and Remark 3). Secondly, we develop static and dynamic statistical permutation-tests if a classifier is in the GSD-front and prove their level-\(-\) validity and their consistency (see Theorem 3).

**Robustification:** Additionally, we recognize the fact that the underlying assumption of identically and independently distributed _(i.i.d.)_ sampling is questionable in many benchmarking studies. Thus, in Section 4.2 we quantify how robust the test decisions are under such deviations.

**Experiments with benchmark suites and implementation:** We illustrate the concepts and corroborate their relevance with experiments run over two benchmark suites (PMLB and OpenML, see Section 5), based on an implementation that is freely available and easily adaptable to comparable problems.4. We consider experiments with _mixed-scaled_ (ordinal and cardinal) multidimensional quality metrics, also incorporating (potentially) ordinal criteria.

### Related work

_Benchmarks_ are the foundations of applied machine learning research [27; 90; 78; 65; 91]. Specifically, benchmarking classifiers over multiple data sets is a much-studied problem in machine learning, as it enables practitioners to make informed choices about which methods to consider for a given data set. Furthermore, also proposals for novel classifiers must often first demonstrate their potential for improvement in benchmark studies. Examples include [58; 40; 31; 57; 12]. In recent years, in recognition of the fact that the benchmark suite under consideration is only a sample of data sets, especially focusing on _statistically significant_ differences between classifiers has received great interest (see, e.g., [24; 35; 34; 19; 45] or, e.g., [9; 22; 8] for Bayesian variants). An R implementation of some of these tests is described in , whereas use-cases in the context of time series and neural networks for regression are discussed in [44; 36]. The diversity and the associated problem of selecting quality metrics (e.g., ) is currently attracting a great deal of interest (e.g., ). Consequently, finding ways for comparing classifiers in terms of _multidimensional quality metrics_ is intensively studied, ranging from multidimensional interpretability measures (e.g., ) over classical Pareto-analyses (e.g., ) to embeddings in the theory of data depth (e.g., [13; 71]). While utilizing variants of _stochastic dominance_ in statistics is quite common (e.g., [56; 61; 6; 76; 67]), the same seems not to hold for machine learning. Exceptions include  in an optimization context, [47; 48], who investigate special types of stochastic orders, and , utilizing already GSD-relations for classifier comparisons without the GSD-front. Finally, relying on imprecise probabilities (e.g., [85; 86; 3]) to robustify statistical hypotheses follows the tradition of [66; 42; 41; 2], see also, e.g., [5; 25; 4; 60; 48]. For application to Bayesian networks, see, e.g, [55; 14; 54], and [81; 69; 18; 80; 1; 70; 52; 30; 16; 17], among others, for robustified machine learning in this spirit.

## 2 Decision-theoretic preliminaries

The relevant basic concepts in order theory are collected in Appendix A.1. Based on these we can make the following definition, originating from the decision-theoretic context discussed in .

**Definition 2**.: _Let \(A\) be a non-empty set, \(R_{1} A A\) a preorder on \(A\), and \(R_{2} R_{1} R_{1}\) a preorder on \(R_{1}\). The triplet \(=[A,R_{1},R_{2}]\) is then called a **preference system** on \(A\). The preference system \(^{}=[A^{},R^{}_{1},R^{}_{2}]\) is called **subsystem** of \(\) if \(A^{} A\), \(R^{}_{1} R_{1}\), and \(R^{}_{2} R_{2}\)._

In our context, \(R_{1}\) formalizes the ordinal information, i.e., the information about the ranking of the objects in \(A\), whereas \(R_{2}\) describes the cardinal information, i.e., the information about the intensity of certain rankings. To ensure that \(R_{1}\) and \(R_{2}\) are compatible, we use a consistency criterion relying on the idea of simultaneous representability of both relations. For this, for a preorder \(R\), we denote by \(I_{R}\) its indifference and by \(P_{R}\) its strict part (see A.1).

**Definition 3**.: _The preference system \(=[A,R_{1},R_{2}]\) is **consistent** if there exists a **representation**\(u:A\) such that for all \(a,b,c,d A\) we have:_

* \((a,b) R_{1} u(a) u(b)\) _with equality iff_ \((a,b) I_{R_{1}}\)__
* \(((a,b),(c,d)) R_{2} u(a)-u(b) u(c)-u(d)\) _with equality iff_ \(((a,b),(c,d)) I_{R_{2}}\)__

_The set of all representations of \(\) is denoted by \(_{}\)._

Finally, we need to recall the concept of _generalized stochastic dominance (GSD)_ (see, e.g., ), which is crucial for the concepts presented in this paper: For a probability space \((,,)\) and a consistent preference system \(\), we define by \(_{(,)}\) the set of all \(X A^{}\) such that \(u X^{1}(,,)\) for all \(u_{}\). We then can define the GSD-preorder on \(_{(,)}\) as follows.

**Definition 4**.: _Let \(=[A,R_{1},R_{2}]\) be consistent. For \(X,Y_{(,)}\), say \(X\)\((,)\)**-dominates \(Y\)** if \(_{}(u X)_{}(u Y)\) for all \(u_{}\). Denote the induced **GSD-preorder** on \(_{(,)}\) by \(R_{(,)}\)._

## 3 GSD for classifier comparison

We return to the initial problem: Assume we are given a finite set \(\) of classifiers, an arbitrary set \(\) of data sets and \(n\) quality metrics \(_{1},,_{n}:\), combined to the multidimensional quality metric \(:=(_{1},,_{n}):^{n}\). As we also want to allow ordinal quality metrics, we assume that, for \(0 z n\), the metrics \(_{1},,_{z}\) are of cardinal scale (differences may be interpreted), while the remaining ones are purely ordinal (differences are meaningless apart from sign). We embed the range \(()\) of \(\) in the following preference system:

\[=[^{n},R^{*}_{1},R^{*}_{2}]\] (1)

\(R^{*}_{1}\) is the usual component-wise \(\)-relation. For \(R^{*}_{2}\), one pair of consequences is preferred to another if, in the ordinal dimensions, the exchange associated with the first pair is not a deterioration to the exchange associated with the second pair and, in addition, there is component-wise dominance of the differences of the cardinal dimensions. In order to transfer the GSD-relation from Definition 4 to the case of comparing classifiers under multidimensional performance metrics, we interpret the data sets in \(\) as realizations of a random variable \(T:\) on some probability space \((,,)\). We then associate each classifier \(C\) with the random variable \(_{C}:=(C,T())\) on \(\) and compare classifiers by comparing the associated random variables by means of GSD.

**Definition 5**.: _Denote by \(_{}\) the preference system obtained by restricting \(\) to \(()\). Further, let \(\) be such that \(\{_{C}:C\}_{(_{},)}\). For \(C,C^{}\), say that \(C\)**dominates**\(C^{}\), abbreviated with \(C C^{}\), whenever \((_{C},_{C^{}}) R_{(_{},)}\)._

In the application situation, instead of the true GSD-order \(\) among classifiers, we will often have to get along with its _empirical analogue_, i.e., the GSD-relation where a sample of data sets is treated like the underlying population and the true probability measure is replaced by the corresponding empirical ones. More precisely, we assume that we have sampled _i.i.d._ copies \(T_{1},,T_{s}\) of \(T\) and then define the set \(Z_{s}:=(C,T_{i}):i s C}\), of (random) observations under the different classifiers. We then use \(\) to denote the (random) subsystem of \(\) that arises when \(\) is restricted to the (random) set \(Z_{s}\). For \(C,C^{}\) we define the random variable

\[d_{s}(C,C^{}):=_{u}_{z Z_{s }}u(z)(_{C}(\{z\})-_{C^{}}(\{z\})),\]

where, for \(M^{n}\), we set \(_{C}(M):=\{i:i s(C,T_{i}) M\}|\). For a concrete sample associated to \(_{0}\), we then say that \(C\)_empirically GSD-dominates_\(C^{}\), if \(d_{s}(C,C^{})(_{0}) 0\). Intuitively, \(d_{s}\) can thus be used to check whether the classifier \(C\) empirically dominates the classifier \(C^{}\) with respect to GSD in the samples at hand (i.e., in the benchmark suite under investigation).

Based on these concepts, we can now define the sets of (empirically) GSD-undominated classifiers.

**Definition 6**.: _Let \(\) be such that \(\{_{C}:C\}_{(_{},)}\). Let denote \(T_{1},,T_{s}\) i.i.d. copies of \(T\)._

* _The_ _GSD-front_ _is the set_ _where_ \(\) _denotes the strict part of_ \(\)_._
* _Let_ \(\)_. The_ \(\)_-empirical GSD-front_ _is the (random) subset of_ \(\) _defined by_ \[_{s}^{}()=C: C^{} d_{s}(C^{},C)-\\ d_{s}(C,C^{})<0}.\]

**Remark 1**.: \(_{s}^{}()\) _is always non-empty. In contrast, \(_{s}^{}()\) may very well be empty if \(>0\). Note that choosing values of \(>0\) is intended to make \(_{s}^{}()\) less prone to sampling noise._

**Remark 2**.: _Some words on the semantics of the GSD-front: From a decision-theoretic point of view, classifier \(C\) strictly GSD-dominates classifier \(C^{}\) if \(C\) has at least as high expected utility as \(C\) regarding any compatible utility representation of all the metrics considered, and stricly higher for at least one such utility. The GSD-front then simply collects all classifiers from \(\) which are not strictly GSD-dominated by any competitor, i.e., which potentially can be optimal in expectation._

**Example 2**.: _Consider again the situation of Example 1 and recall that \(()=\{C_{2},C_{3}\}\) leaves \(C_{2}\) and \(C_{3}\) incomparable. However, if considering only the distribution of the (multivariate) performance of the classifiers (while assuming a uniform distribution over \(\)), \(C_{3}\) is clearly dominating \(C_{2}\) w.r.t. GSD: Matching dataset \(D_{i}\) with dataset \(D_{5-i}\) creates a (strict) pointwise dominance of \(C_{3}\) over \(C_{2}\) (where the strict dominance is due to \(D_{1}\) and \(D_{4}\)). Thus, \(()=\{C_{3}\}()=\{C_{2},C_{3}\}\)._

The following two theorems show that the \(\)-empirical GSD-front fulfills two very natural requirements: First, under some regularity conditions, it is a consistent statistical estimator for the true GSD-front (Theorem 1). This is important because in practical benchmarking we almost never have access to the GSD-front of the whole population, i.e., the benchmarking results on all possible datasets from a specific problem class \(\). Second, it is ensured that neither the \(\)-empirical nor the true GSD-front can ever become larger than the respective Pareto-front, irrespective of the choice of \(\) (Theorem 2). This is important as it guarantees our analysis does never conflict with, but is potentially more information-efficient than a Pareto-type analysis. Proofs are given in B.1 and B.2.

**Theorem 1**.: _Denote by \(_{}\) the set of all sets \(\{a:u(a) c\}\), where \(c\) and \(u_{_{}}\). Assume that \(\) is antisymmetric. If the VC-dimension5\(_{}\) is finite and if \(:\) converges to \(0\) with rate at most \((1/)\), then \((_{s}^{(s)}())_{s}\) is a consistent statistical estimator, i.e.,_

\[:_{s}_{s}^{ (s)}()=()}=1,\]

_where set convergence is defined via the trivial metric._

**Remark 3**.: _The assumption of a finite VC dimension is only necessary to ensure that the \(\)-empirical GSD front does not become too large. In particular, the following does hold **without** this assumption:_

\[:_{s}_{s}^{ (s)}()()} =1.\]

_Thus, the \(\)-empirical GSD-front almost surely converges to a superset of the true GSD-front._

**Theorem 2**.: _Assume \(\) with \(\{_{C}:C\}_{(,)}\). Let further denote \(T_{1},,T_{s}\) i.i.d. copies of \(T\) and let \(_{1}_{2}\). It then holds that i) \(()()\). Moreover, it holds that ii) \(_{s}^{_{2}}()_{s}^{ _{1}}()(,\{T_{1},,T_{s}\})\)._

## 4 Statistical testing

We saw the \(\)-empirical GSD-front can be a consistent statistical estimator and that both the empirical and the true GSD-front are compatible with the Pareto-front. We now address statistical testing.

### A test for the GSD-front

From now on, we make the (technical) assumption that the order \(\) among the classifiers from \(\) is additionally _antisymmetric_, transforming it from a preorder into a partial order.6 Equipped with this assumption, we want to address the question how to _statistically test_ if a given classifier \(C\) is an element of the true GSD-front \(()\). To achieve this, we formulate the question of actual interest as the alternative hypothesis of the test, i.e., we obtain the hypothesis pair:

\[H_{0}:C()\ \ }\ H_{1}:C( )\]

A possible motivation for developing tests on the hypothesis pair \((H_{0}, H_{0})\) is the following: One would like to compare the quality of a newly developed classifier \(C\) for a problem class \(\) with the classifiers in \(\{C\}\) that are considered state-of-the-art for this problem class, see application in Section 5.2. If a suitable statistical test would allow the above null hypothesis to be rejected, then one could draw the conclusion (subject to statistical uncertainty) that the new classifier \(C\) on the problem class \(\) could potentially improve the state-of-the-art. As first step, note that (under asymmetry) the null hypothesis \(H_{0}\) can be equivalently rewritten as \(H_{0}: C^{}\{C\}:C^{} C\). This reformulation makes obvious that \(H_{0}\) is false if and only if _for every_\(C^{}\{C\}\) the auxiliary hypothesis \(H_{0}^{C^{}}:C^{} C\) is false. Statistical tests for hypothesis pairs of the form \((H_{0}^{C^{}}, H_{0}^{C^{}})\) were proposed (in the context of statistical inequality analysis) in : The authors there showed how exact statistical tests under _i.i.d._ sampling can be constructed by using a (non-parametric) permutation test based on a regularized version \(d_{s}^{}(C^{},C)\) of \(d_{s}(C^{},C)\) as a test statistic. The strength of regularization of the test statistic is there controlled by a parameter \(\), whose increase reduces the number of representation functions over which the infimum in the test statistic is formed, while equally attenuating all quality metrics.7 Due to space limitations, we omit to recall an exact description of the testing scheme in the main text and instead refer to Appendix A.2.

The idea is then to replace the global test for \((H_{0}, H_{0})\) with \(c:=||-1\) tests of hypotheses \((H_{0}^{C^{}}, H_{0}^{C^{}})\) and to reject the null hypothesis at significance level \(\) if all tests reject their individual null hypotheses \(H_{0}^{C^{}}\) at the same significance level \(\). Call this the **static GSD-test**. Clearly, this test tends to be conservative, as it ignores potential correlations of the test statistics for different pairs of classifiers. Moreover, a slightly modified test in the context of the GSD-front is directlyderivable: If one is rather interested in identifying the maximal subset \(_{}\) of \(\) for which \(C\) significantly lies in the GSD-front, i.e., in testing \(^{}_{0}:C()\)**vs.**\(^{}_{1}:C()\) for all \(\) with \(C\)_simultaneously_, the following alternative test is a statistically valid level-\(\) test: First, perform all individual tests for \((H^{C^{}}_{0}, H^{C^{}}_{0})\) with level \(\). Then identify \(_{}\) as the set of all classifiers from \(\) for which the individual hypotheses are rejected. The (random) alternative hypothesis \(^{S_{}}_{1}:C(_{})\) is then statistically valid in the sense of being false only with a probability bounded by \(\). Call this the **dynamic GSD-test**. We have the following theorem, demonstrating that the proposed tests are indeed reasonable statistical tests (see B.3 for the proof).

**Theorem 3**.: _Let the assumptions of Theorem 1 hold. Then, both the static and dynamic GSD-test are valid level-\(\) tests. Additionally, both tests are consistent in the sense that under the corresponding alternative hypothesis, i.e., \(H_{1}:C()\) resp. \(_{1}::C,| | 2,C()\), the probability of rejecting the corresponding null hypothesis converges to \(1\) as \(s\)._

### Checking robustness under non-i.i.d.-scenarios

We argue that meaningful benchmark studies should abstain from treating the sample of data sets in the suite as a _complete survey_. That is, benchmark analyses should aim at statements about a well-defined population and regard the benchmark suite as a non-degenerate sample thereof. A major practical problem in this context is that often little is known about the inclusion criteria for data sets or test problems in the respective benchmark suite (see, e.g., the discussions in ). For instance, the popular platform OpenML  allows users to upload benchmark results for machine learning models with varying hyperparameters, harming representativity, see Section 5.1 and Appendix C.1. The absence of methods to randomly sample from the set of all problems or data sets is identified as an unsolved issue in [57, Section 2]. This calls the common _i.i.d._ sampling assumption into question, which our (and most other) tests are based upon, and raises the issue as to what extent statistically significant results depend on this assumption. We now address precisely this question.

In  it was shown how the binary tests on the hypothesis pairs \((H^{C^{}}_{0}, H^{C^{}}_{0})\) discussed in Section 4.1 can be checked for robustness against deviations from the underlying _i.i.d._-assumption. The idea here is to deliberately perturb the empirical distributions of the performances for the different classifiers and to analyze the permutation test used under the most extreme yet compatible worst-case. The perturbation of the empirical distribution is carried out here using a \(\)-contamination model (see, e.g., [85, p. 147]), which is widely used in robust statistics. We now want to adapt a similar robustness check for the global hypothesis pair \((H_{0}, H_{0})\) discussed here. For this, suppose we have a sample \(T_{1},,T_{s}\) of data sets (i.e., the benchmark suite). We further assume that \(k s\) of these variables (where it is not known which ones) are not sampled _i.i.d._, but come from an arbitrary distribution about which nothing else is known. We then know, for every fixed \(C\), that its associated true empirical measure \(^{}_{C}\) based on the true (uncontaminated) sample would have to be contained in

\[_{C}=(1-)^{}_{C}+:},\] (2)

where \(^{}_{C}\) denotes the empirical measure based on the contaminated sample \(T_{1},,T_{s}\). Note that \(_{C}\) is by definition a \(\)-contamination model with central distribution \(^{}_{C}\) and contamination degree \(:=\). In this setting,  show that to ensure that their permutation tests used for hypothesis pairs \((H^{C^{}}_{0}, H^{C^{}}_{0})\) only advise rejection of the null hypothesis if this is justifiable for any empirical distribution compatible with the contaminated sample, i.e., for every combination of measures \((_{1},_{2})_{C}_{C^{}}\), one has to compare the most pessimistic value of the test statistic for the concrete sample at hand with the most optimistic value of the test in each of the resamples. Moreover, they show that the (approximate) _observed \(p\)-values_ for a concrete contaminated sample \(T_{1}(_{0}),,T_{s}(_{0})\) associated with \(_{0}\) of this robustified test can be expressed by a function in the number of contaminations \(k\), given by

\[f_{(C^{},C)}(k):=1-_{I T_{N}} _{d^{s}_{I}-d^{s}_{S}(C^{},C)(_{0})> }},\]

where \(N\) denotes the number of resamples, \(_{N}\) is the corresponding set of resamples, and \(d^{}_{I}\) is the test statistic evaluated for the resample associated to \(I\). Due to space limitations, we omit an exact description of the robustness check for the test on the hypothesis pairs \((H^{C^{}}_{0}, H^{C^{}}_{0})\) as well as a derivation of the function \(f_{(C^{},C)}\) in the main text and instead refer to Appendix A.3.

Similar as shown in Section 4.1, it is straightforward to calculate an (approximate) observed \(p\)-value for the static GSD-test for \((H_{0}, H_{0})\): We calculate the maximal observed \(p\)-value among all \(C^{}\{C\}\), i.e. set \(F_{C}(k):=f_{(C^{},C)}(k):C^{}\{C \}}\). The **obstified static GSD-test** for the degree of contamination \(k\) can be carried out as follows: Calculate \(F_{C}(k)\) and reject \(H_{0}\) if \(F_{C}(k)\). This indeed gives us a valid level-\(\)-test for the desired global hypothesis \(H_{0}:C()\) under the additional freedom that up to \(k\) of the variables in the sample might be contaminated. Note, however, that also this test tends to be conservative as both performing the individual tests at level \(\) as well as the adapted resampling scheme of the permutation test are worst-case analyses. Finally, also the **robustified dynamic GSD-test** can be obtained straightforwardly: Under up to \(k\) contaminations, the (random) alternative hypothesis \(_{1}^{_{}}:C(_{})\) is statistically valid with level \(\) if all individual robustified tests reject \(H_{0}^{C^{}}\) at level \(\), i.e., if \(F_{C}(k)\).

We end the section with a short comment on computation: The test statistics for the permutation test and the robustified variant can be calculated using linear programming. We are guided here by the linear programs proposed in [48, Propositions 4 and 5]. There are two computational bottlenecks in the actual evaluation: (1) the creation and storage of the constraint matrices of the linear programs and (2) the repeated need to solve large linear programs. An efficient, well-commented implementation that can be quickly transferred to similar applications is made available on GitHub (see Footnote 4).

## 5 Benchmarking experiments

We demonstrate our concepts on two well-established benchmark suites: OpenML [82; 11] and PMLB . While for PMLB we compare classifiers w.r.t. the latent quality metric robust accuracy (see the first motivation in Section 1), for OpenML we use a multidimensional metric that includes accuracy and computation time as unidimensional metrics (see the second motivation in Section 1). The analysis of PMLB is kept short in the main text and detailed in Appendix C. Since the metrics in both applications are composed of one continuous and two (finitely) discrete metrics, we have (see B.4):

**Corollary 1**.: _In both applications, the \(\)-empirical GSD-front is a consistent estimator for the true GSD-front (provided \(\) is chosen as in Theorem 1)._

### Experiments on OpenML

We select 80 binary classification datasets (according to criteria detailed in Appendix C.1) from OpenML  to compare the performance of _Support Vector Machine_ (SVM) with _Random Forest_ (RF), _Decision Tree_ (CART), _Logistic Regression_ (LR), _Generalized Linear Model with Elastic net_ (GLMNet), _Extreme Gradient Boosting_ (xGBoost), and _k-Nearest Neighbors_ (kNN).8 Our multidimensional quality metric is composed of _predictive accuracy_, _computation time on the test data_, and _computation time on the training data_. Since the computation time depends strongly on the used computing environment (e.g. number of cores or free memory), we discretize the time-related metrics and treat them as ordinal. Accuracy is not affected by this and is therefore treated as cardinal. For details, see Appendix C.1. To gain a purely descriptive impression, we computed the empirical GSD relation. For this, we calculated \(d_{80}(C,C^{})\) for \(C C^{}:=\{\}\) (see Hasse graph in Figure 2 in Appendix C.1). We see that CART (strictly) empirically GSD-dominates xGBoost, SVM, LR, and GLMNet. All other classifiers are pairwise incomparable. Three classifiers are not strictly empirically GSD-dominated by any other, namely RF, CART, and kNN. Thus, the \(0\)-empirical GSD-front is formed by these. While at first glance this result might seem rather unexpected, a closer look on the performance evaluations provided by OpenML indeed confirms the dominance structure found, see Appendix C.1 for details.

To move to reliable inferential statements that take into account the statistical uncertainty, we exemplarily test (at level \(=0.05\)) if SVM significantly lies in the GSD-front of some subset of \(\). As described in Section 4.1, we therefore perform six pairwise permutation tests for the hypothesis pairs \((H_{0}^{C^{}}, H_{0}^{C^{}})\) (where \(C:=\) and \(C^{}\{\}\)) at level \(\) in case of the **static GSD-test** or at level \(\) in case of the **dynamic GSD-test**.9 That is, we test six auxiliary null hypotheses each stating that SVM is GSD-dominated by kNN, xGBoost, RF, CART, LR, and GLMNet, respectively.

The distributions of the test statistics are visualized on the left of Figure 1 (densities) and Figure 3 (CDFs) in C.1. They show that the pairwise tests of SVM versus kNN, xGBoost, RF, and GLMNet reject at level \(\) and, thus, that SVM significantly (at level \(\)) lies in the GSD-front of the subset of \(\) composed of SVM and these four classifiers. In other words, we conclude that SVM is significantly (\(~{}=~{}0.05\)) not outperformed by kNN, xGBoost, RF, and GLMNet regarding all compatible utility representation of accuracy, training and test runtime. Finally, as discussed in Section 4.2, we turn to the third aspect of reliability (besides multiple criteria and statistical uncertainty): We analyze how robust this test decision is under contamination of the benchmark suite, i.e., deviations from _i.i.d._. The results are visualized on the right of Figure 1. It can be seen that the tests at level \(\) of SVM against GLMNet, kNN, RF and xGBoost cease to be significant from a contamination of (approximately) \(7\), \(8\), \(11\), and \(11\) of \(80\) data sets, respectively. That is, the results on up to \(7\), \(8\), \(11\), and \(11\) datasets could be arbitrarily redistributed, while maintaining significance of rejection. Since the significance of the dynamic GSD-test's decision depends on all pairwise tests being significant at level \(\), we can conclude that SVM would still have been significantly in the GSD-front of {SVM, kNN, xGBoost, RF, GLMNet}, even if \(7\) out of \(80\) data sets had been contaminated. Summing up, our proposed testing scheme not only allowed for meaningful statistical benchmarking of SVM versus competitors regarding accuracy, test time, and train time; it also enabled us to quantify as to what degree our conclusions remained stable under contamination of the benchmark suite.

**Method comparison:** The results highlight the advantages of the GSD-front over existing approaches. Applying _first-order stochastic dominance_ (a special case of GSD where \(R_{2}^{*}\) is the trivial preorder) on the same set-up, yields that no classifier is significantly larger than (or incomparable to) any other classifier, based on a 5% significance level. This illustrates that the GSD-approach accounts for accuracy being a _cardinal_ measure. In contrast, the _Pareto-front_ here contains all considered classifiers. Thus, the Pareto front is much less informative than the GSD-front, which is also reflected in Theorem 2. Unlike the Pareto-front, the GSD-front is based on the distribution of the multidimensional quality metric and not only on the pairwise comparisons, and can use this knowledge to define the front. Thus, the GSD front is a balance between the conservative Pareto analysis and the liberal weighted sum comparison. Finally, we want to compare our method with an approach based on extending the test for single quality metrics proposed in  to the multiple metric setting. We therefore perform all possible single-metric tests as in  and define the _marginal front_ as those classifiers that are not statistically significantly worse than another classifier on _all_ metrics. However, this procedure can not be used to define a hypothesis test. Therefore, only a comparison with the empirical GSD-front is meaningful. For OpenML, this marginal front consists of all classifiers and is less exploratory than the empirical GSD-front. More details on the results of these other approaches and how these compare to the GSD front can be found in C.1.

Figure 1: Left: Densities of resampled test statistics for pairwise permutation tests of SVM vs. six other classifiers on 80 datasets from OpenML. Big (small) vertical lines depict observed (resampled) test statistics. Rejection regions for the static (dynamic) GSD-test are highlighted red (dark red). Right: Effect of Contamination: \(p\)-values for pairwise tests of SVM versus GLMNet, kNN, RF and xGBoost.Red lines mark significance levels of \(~{}=~{}0.05\) (dark red: \(~{}=~{}\)). Significance of SVM being in the GSD-front remains stable under contamination of up to \(7\) of \(80\) datasets.

### Experiments on PMLB

We select 62 datasets from the Penn Machine Learning Benchmark (PMLB) suite  according to criteria explained in Appendix C.2. The following analysis shall exemplify how our proposed statistical tests can aid researchers in benchmarking newly developed classifiers against state-of-the-art ones. To this end, we compare a recently proposed classifier based on compressed rule ensembles of trees (CRE)  w.r.t. robust accuracy against five well-established classifiers, namely CART, RF, SVM with radial kernel, kNN and GLMNet. We operationalize the latent quality criterion of robust accuracy through i) classical accuracy (metric), ii) robustness of accuracy w.r.t. noisy features (ordinal), and iii) robustness of accuracy w.r.t. noisy classes (ordinal). Computation of i) is straightforward; in order to retrieve ii) and iii), we follow [92; 93] by randomly perturbing a share (here: 20 %) of both classes and features and computing the accuracy subsequently, as detailed in Appendix C.2. Since there exist competing definitions of robustness [43; 10; 72] and due to the share's arbitrary size, we treat ii) and iii) as ordinal and discretize the perturbated accuracy in the same way as for the runtimes in the openML experiments. Detailed results and visualization thereof can be found in Appendix C.2. In a nutshell, we find no evidence to reject the null of both the static and the dynamic GSD-test at significance level \(=0.05\). In particular, we do not reject any of the pairwise auxiliary tests for hypothesis pairs \((H_{0}^{C^{}}, H_{0}^{C^{}})\) with \(C:=\) CRE and \(C^{}\{\}\)) for neither \(\) nor \(\). Our analysis hence concludes that we cannot rule out a significance level \(~{}=~{}0.05\) that the newly proposed classifier CRE is dominated by the five state-of-the-art classifiers w.r.t. all compatible utility representation of the latent criterion robust accuracy.

### Additional recommendations for the end-user

We end the section with a few brief general notes for end-users of our benchmark methodology. This should make it easy to decide whether a GSD-based analysis is appropriate in a given use-case.

1. GSD-based studies do not primarily aim to identify the best algorithm for a given benchmark suite. Often, the GSD front contains more than one element. They are rather intended for checking whether a newly proposed classifier for a certain problem class can potentially improve on the state-of-the-art classifiers, or whether it disqualifies itself from the outset.
2. GSD-based studies allow statements with inferential guarantees by providing appropriate statistical tests: Assuming an _i.i.d._ benchmark suite, a judgment about an algorithm represents a statement about an underlying population and not just this specific suite.
3. GSD-based studies enable the robustness of the results to be quantified under the deviation from the _i.i.d._ assumption: It can be checked which share of the benchmark suite may be contaminated without affecting the obtained inferential statements.
4. GSD-based studies allow algorithms to be compared w.r.t. multiple metrics simultaneously. They enable the full exploitation of the information contained in differently scaled metrics.

## 6 Concluding remarks

**Summary:** We introduced the GSD-front for multicriteria comparisons of classifiers, gave conditions for its consistent estimability and proposed a statistical test for checking if a classifier belongs to it. We illustrated our concepts using two well-established benchmark suites. The results came with threefold reliability: They included several quality metrics, representation of statistical uncertainty, and a quantification of robustness under deviations from the assumptions.

**Limitations and future research:** Two specific limitations open promising avenues: 1.) _Comparing other types of algorithms:_ We restricted ourselves to comparing classifiers. However, any situation in which objects are to be compared on the basis of different (potentially differently scaled) metrics over a random selection of instances can be analyzed using these ideas. For instance, applications of our framework to the multicriteria deep learning benchmark suite DAWNBench or the bi-criteria optimization benchmark suite DeepOBS appear straighforward. 2.) _Extension to regression-type analysis:_ Analyses based on the GSD-front do not account for meta properties of the data sets. A straightforward extension to the case of additional covariates for the data sets is to stratify by these for the GSD-comparison. This would allow for a situation-specific GSD-analysis, presumably yielding more informative results.