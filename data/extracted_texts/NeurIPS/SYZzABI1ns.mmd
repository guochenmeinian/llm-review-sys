# CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery

Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao,

**Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu**

Beijing University of Posts and Telecommunications, Beijing, China songxiaoshuai@bupt.edu.cn

###### Abstract

Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities.

## 1 Introduction

Serving as the cornerstone of the modern information revolution, the significance of computer science (CS) extends from the early days of electronic computers to today's advancements in artificial intelligence (AI) [1; 2]. As a new milestone in AI, large language models (LLMs) [3; 4] represented by ChatGPT  and GPT-4  are not limited to the natural language processing (NLP) community, showing vast potential in fields including education, industry, and science [7; 8; 9; 10; 11; 12; 13]. However, enabling LLMs to effectively utilize computer science knowledge and serve humanity more efficiently is one of the key challenges on the path to the future intelligent era [14; 15; 16].

Understanding the performance of LLMs in computer science is fundamental to the research and application of LLMs within the field. Despite studies like MMLU and C-Eval [17; 18; 19; 20; 21] covering a wide range of fields including CS, their broad scope implies that CS is merely a component within the multiple categories of science and engineering, overlooking the importance of thoroughlyevaluating the CS field. Moreover, such evaluation result can further guide the development of LLMs, offering practical insights to advance the corresponding capabilities. Recently, a series of studies have devoted on actively assessing and analyzing the capabilities of LLMs in mathematics, coding, and logical reasoning . Unfortunately, efforts on LLMs in cross-capability evaluation is quite scarce. Considering the intersection of computer science with coding, mathematics, and reasoning abilities, we have grounds to believe that cross-capability research and analysis in CS can effectively propel the comprehensive development of the LLM community. Here, we are particularly interested in two research questions for evaluating LLMs' proficiency in computer science field:

_RQ1: How do LLMs perform in the field of computer science and what are the challenges and potential directions for improvement?_

_RQ2: What are the relationship between the abilities of LLMs in computer science, mathematics, and code programming?_

As the bedrock for exploration, we first propose CS-Bench, the first benchmark dedicated to evaluating the performance of LLMs in the field of computer science. CS-Bench features high-quality, diverse task forms, varying capacities, and bilingual evaluation. Firstly, CS-Bench comprises approximately 5,000 carefully curated test items spanning 26 sections across 4 key CS domains. Diverging from conventional benchmarks consisting solely of multiple-choice (MC) questions , CS-Bench includes 4 tasks: multiple-choice, assertion, fill-in-the-blank (FITB), and open-ended, to better simulate real-world scenarios and assess the robustness of LLMs to different task formats. In addition to knowledge-type questions assessing LLMs' mastery of CS knowledge, reasoning-type questions further evaluate LLMs' ability to apply CS knowledge for reasoning. Lastly, by supporting bilingual evaluation in Chinese and English, CS-Bench enables the appraisal of LLMs' adeptness in addressing CS challenges across different language contexts.

In response to RQ1, we evaluate over 30 mainstream LLMs on CS-Bench. Our main findings are: (1) CS-Bench effectively differentiates the capabilities of LLMs in the CS field while also posing significant challenges to the best-performing GPT-4/ GPT-4o. (2) LLMs exhibit a consistent logarithmic growth pattern in scale and a linear growth pattern in scores on the CS-Bench. By establishing the scale-score fitting function, smaller models can be used to predict and guide the development of larger-scale models. (3) Further error type analysis indicates that the primary reason for the limited performance of LLMs is the lack of domain knowledge, and the CS-specific

Figure 1: Overview diagram and statistics of CS-Bench.

reasoning is difficult to achieve merely by enhancing general reasoning abilities, necessitating targeted reinforcement.

In response to RQ2, we perform a detailed analysis of the relationship of General LLMs' ability in three domains: mathematics, coding, and computer science, as well as the performance of code- and math-specific expert LLMs on CS-Bench. We observe consistent trends in the overall performance of the general LLMs across CS-Bench and scores in benchmarks related to mathematics and coding, indicating a strong correlation between LLM's computer science proficiency and its mathematical and programming abilities. Furthermore, despite a decline in general capabilities, some expert LLMs still exhibit improvements in certain areas of CS, such as data structures and algorithms, with more pronounced knowledge and reasoning capabilities evident in supplementary smaller-scale models.

To summarize, our contributions are as follows:

* We introduce CS-Bench, the first benchmark dedicated to evaluate the performance of LLMs in the field of computer science. CS-Bench supports both Chinese and English, covers four key areas with 26 subfields, and includes a diverse range of task formats.
* Utilizing CS Bench, we conduct a comprehensive evaluation of mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvement.
* We conduct exploratory experiments on LLMs' cross-ability and find a strong relationship between their CS proficiency and mathematical and programming abilities. Moreover, the expertise in mathematics and programming of expert LLMs can improve performance in specific CS subfields.

## 2 CS-Bench

### Design Principle

The objective of CS-Bench is to robustly assess the knowledge and reasoning capabilities of LLMs in different linguistic contexts within the field of computer science. To this end, our benchmark adheres to the following guidelines: (1) **Coverage of key domains:** it covers key areas of CS with finer subfields for specificity. (2) **Diverse task forms:** questions vary in format to simulate diverse real-world user queries. (3) **CS-specific reasoning:** it evaluates CS logical and arithmetic reasoning in addition to CS knowledge. (4) **Multilinguality support:** it supports assesses LLMs' performance in different language environments. Based on these criteria, CS-Bench focuses on bilingual evaluation in Chinese and English, covering four domains: Data Structure and Algorithm (DSA), Computer Organization (CO), Computer Network (CN), and Operating System (OS). Twenty-six fine-grained subfields, diverse task forms, and divisions of knowledge and reasoning are further developed to enrich the dimensions of assessment and simulate real-world scenarios.

### Data Collection

Data Sources.

Diverse data sources are key to achieving the sample diversity of CS-Bench. Our raw data originates from three sources: (1) Computer science-related questions obtained from publicly available online channels, such as professional exams and practice tests1. (2)Knowledge-type questions obtained through the initial manual extraction and subsequent adaptation of blog articles from various computer-related websites2. (3) Construction

   English Dataset & PPL & Chinese Dataset & PPL \\  TruthfulQA (MC)  & 7.73 & C-Eval  & 11.47 \\ MMLU  & 9.54 & CMMLU  & 13.62 \\ CS-Bench (MC) & 11.86 & CS-Bench (MC) & 13.31 \\ CS-Bench (ALL) & 13.3 & CS-Bench (ALL) & 16.95 \\   

Table 1: Comparison of perplexity (PPL) across evaluation datasets. The PPL of English and Chinese datasets is calculated on Llama2-7B-base and Qwen1.5-7B-base, respectively. “MC” denotes multiple-choice, and “ALL” denotes all tasks.

of teaching materials and examination papers authorized by the authors' institutions. The latter two categories constitute the vast majority (over 70%) of the data, and these data are not directly exposed on the internet, effectively reducing the likelihood of LLMs encountering these questions during pre-training. We compare the perplexity  of models on CS-Bench and several prominent evaluation datasets in Table 1. In both English and Chinese, the perplexity of CS-Bench is comparable to or even higher than that of other datasets, further indicating the high quality of CS-Bench samples and the rarity of data leakage instances.

Data Processing.The data processing relies on a team composed of five members, each holding a bachelor's degree in computer science and receiving appropriate compensation. Initially, we parse questions and answers for each sample from the data sources either automatically or manually. Subsequently, we manually label questions with knowledge-type or reasoning-type tags depending on whether in-depth reasoning and calculation are required. For reasoning-type questions, we attempt to collect explanations from the data sources whenever possible; otherwise, we handle them through cross-annotation and verification among team members. We first construct Chinese data, then translate it into English using GPT-4, supplemented by manual checks, to create English data. Finally, we conduct thorough manual checks on the entire dataset to ensure quality. We provide detailed data sources and processing procedures in the supplemental materials.

Statistics.CS-Bench is an evaluation benchmark supporting bilingual assessment, encompassing a total of 26 subfields across 4 domains, with a cumulative total of 4838 samples. These samples encompass various task formats including multiple-choice, assertion, fill-in-the-blank, and open-ended questions. Besides, CS-Bench assesses both knowledge-type and higher-order reasoning-type questions, with each reasoning question accompanied by an explanation. To validate the effectiveness of models, we randomly sample 10% of the data for validation, using the remaining 90% for testing. The statistics of CS-Bench are shown in Figure 1, with detailed exposition provided in Appendix C.

## 3 Experiment

### Experimental Setup

Evaluation Protocols.Due to the diverse task formats in CS-Bench, we first design question templates for each task type. For comprehension tasks (MC and Assertion), we use regex to match LLM's predictions and then calculate their accuracy against the ground-truth answers. For generation tasks (FITB and Open-ended), due to the diversity of ground-truth answers, we score LLM's predictions by GPT-4 using standard answers in CS-Bench as references. In detail, FITB questions are scored as either 0 or 1, while the score range for Open-ended questions is 1-10, which is then linearly mapped to a range of 0.1 to 1. Finally, scores are weighted based on the quantity of each type to derive the ultimate overall score. It is worth emphasizing that while employing GPT-4 for scoring generation tasks may introduce a certain threshold for evaluation, its primary purpose is to simulate diverse task formats in real-world scenarios. Therefore, we encourage isolating comprehension tasks from CS-Bench to facilitate automatic evaluation with no need for GPT-4. We provide the details of the evaluation setup in Appendix D, where we also verify the validity of GPT-4 scoring through its consistency with manually scored results.

Models.We evaluate nearly 30 models in different sizes from 12 model families. For open-source models, we selected Gemma-2B/7B , Llama2-7B/13B/70B , Llama3-8B/70B , ChatGLM3-6B , Baichuan2 (v2.0)-7B/13B , InternLM2-7B/20B , Qwen1.5-4B/7B/14B/72B/110B , Mistral-7B (v0.2) , Mistral-8\(\)7B (v0.1) , and DeepSeeLLM-7B/67B . For closed-source commercial models, we utilized PaLM-2 (palm-2-chat-bison) , Claude-2.1 , Claude-3 (opus) , as well as GPT-3.5, GPT-4 (0125 version)  and GPT-4o . To ensure the instruction-following abilities, we employ the official chat or instruction-tuned versions for all models. Details on these models are provided in Appendix D.4.

### Main Results

Table 2 presents the overall results of all foundation models directly answering questions under the zero-shot setting 3. In summary, the overall scores of models range from 39.86% to 72.29%, demonstrating CS-Bench's effectiveness in distinguishing between the abilities of various models in the field of CS while also posing significant challenges to the best-performing existing models. Subsequently, we conduct a comprehensive analysis of the results from various aspects as follows.

Comparison between Foundation Models.Firstly, the closed-source models GPT-4/GPT-4o represent the highest standard on CS-Bench, being the only two models exceeding 70% proficiency. Secondly, the disparity between the leading open-source and closed-source models is not significant. Notably, premier open-source models such as Qwen1.5-110B and Llama3-70B have surpassed previously strong closed-source models like GPT-3.5 and Claude-2.1, drawing close to Claude-3

[MISSING_PAGE_FAIL:6]

Figure 4 (a). It can be observed that although different families exhibit distinct performances, models within the same family consistently show improvement as the parameter size increases. However, as the model parameter size continues to increase, the performance gains from scaling diminish, resulting in diminishing returns in efficiency. For instance, the score in Qwen1.5 improves by 16.19% from 0.5B to 7B, by 7.11% from 14B to 72B, and by only 2.66% from 72B to 110B. Additionally, as shown in Figure 4 (b), when the parameter scale grows exponentially, the score approximately increases linearly. This indicates that in the CS field, the model's performance also follows a logarithmic scale pattern. Given the substantial computational resources required for large-scale models, we aim to establish the relationship between model scales and scores to predict the performance of larger-scale models in the CS field by fitting smaller-scale model scores. Due to space limitations, the specific design and implementation of the fitting function are provided in Appendix E.2. Overall, we fit the functions of Llama2 and Qwen1.5 series based on models ranging from 7B to 70/72B. We validate the fitting function on Qwen-1.5 110B, where the predicted value (67.83%) closely matches the actual value (67.95%), enabling further predictions for theoretical models, even up to 1000B.

Comparison between Zero-shot, Few-shot and COT Prompting.To investigate the impact of few-shot prompts and chain of thought (COT ) on model performance, we evaluate model's performance under 5-shot answer-only (AO) and 5-shot COT prompts in Figure 4 (c), where the prompt samples are sampled from the validation set and match the domain of the test questions. Given that model-generated results under 0-shot COT often don't adhere to specific formats, making regular matching difficult, we omit 0-shot COT experiments, similar to C-Eval. Additionally, for Open-ended questions, since the answers include detailed explanations, 5-shot COT is the same as 5-shot AO. For all tested models, the 5-shot prompts show improvement compared to 0-shot, with average increases of 1.47% for 5-shot AO and 2.00% for 5-shot COT, respectively. Moreover, the efficacy of few-shot prompts in bringing improvements appears more pronounced in some robust models such as GPT-3.5 and GPT-4, owing to their superior in-context learning capabilities.

Analysis of Error Types.To delve into the roots of LLMs' failure on CS-Bench and offer pathways toward improvement, we acquire the solution process of model errors under 5-shot COT, and utilize GPT-4 to categorize each error type in MC questions in Figure 5. It should be emphasized that models may cause joint errors, resulting in more than one error type assigned to a single answer. In general, from Llama2-7B all the way to GPT-4, the total number of errors continues to decrease for both knowledge-type and reasoning-type questions. For knowledge-type questions, both single concept errors and concept confusion show a decreasing trend. Initially, some completely wrong concepts transitioning to partially erroneous ones and subsequently being eliminated, thus exhibiting an initial rise followed by a decline in partial concept errors. For reasoning-type questions, we observe that a significant portion of errors still fall under the category of knowledge-based mistakes. While stronger models have evidently reduced arithmetic reasoning errors for reasoning inaccuracies, there hasn't been much change in logic reasoning errors specific to the CS field. Our analysis highlights that reinforcing CS knowledge concepts is the most direct and effective approach to improving LLMs' performance in the field of CS. Furthermore, significant improvements in CS reasoning performance are challenging to achieve solely by enhancing general reasoning abilities and mathematical reasoning, necessitating CS-specific reinforcement. More details can be found in E.4.

Figure 5: The proportion of different error types varies by models for multiple-choice questions.

### What's the Relationship between CS, Math, and Code abilities of LLMs?

To explore the relationship between CS proficiency and the mathematical and coding capabilities of models, we investigate (1) the performance of general LLMs across the fields of Math, Code, and CS, and (2) the performance of LLMs specialized in Code and Math within the field of CS.

Exploration on General Models.In Figure 6, we illustrate how the models' performance on CS-Bench varies with increasing scores on the Math datasets (GSM8K , MATH ) and Code datasets (HumanEval , MBPP ). We observe that the overall trend in CS-Bench performance closely aligns with changes in Math and Code scores, as indicated by a Pearson correlation coefficient  exceeding 0.9. Besides the general enhancement of diverse competencies that superior models typically bring, we consider this evidence to suggest a close correlation between CS proficiency and abilities in Math as well as Code. Next, we examine models with inconsistent patterns between CS and Math/Code. In the Math domain, Qwen1.5-7B outperforms LLama2-70B in both GSM8K and MATH, yet in CS-Bench, LLama2-70B surpasses Qwen1.5-7B. In the Code domain, Mixtral-8\(\)7B performs better than Qwen1.5-32B on HumanEval and MBPP, whereas the opposite is observed on CS-Bench. Given the NLP community's sustained focus on the Code and Math domains, some recently released models have been trained on a large amount data in these domains, leading to smaller-scale models outperforming much larger-scale ones (e.g., Qwen1.5-7B surpassing LLama2-70B). However, in the CS domain, due to insufficient attention and training data, even excellent small-scale models struggle to surpass much larger-scale models. This also indicates that CS-Bench has not been overfitted during LLM pretraining, making it a fairer benchmark for measuring model performance differences.

Exploration on Expert Models.We present the results of the Math and Code expert LLMs in Tables 3 and 4. Compared to general Chat LLMs, expert LLMs usually sacrifice other abilities to boost proficiency in Math or Code, which is reflected in the lower overall performance of most expert

   &  &  &  &  &  &  \\   & & Klig & Rng & Klig & Rng & Klig & Rng & Klig & Rng & Klig & Rng & Avg \\   & Chat & 51.51 & 32.61 & 48.89 & 31.82 & 46.72 & 30.75 & 41.04 & 26.26 & 47.15 & 30.48 & 41.08 \\  & CodeLlama-7B  & Code & 58.90 & 36.15 & 45.46 & 36.24 & 52.87 & 26.23 & 44.75 & 25.33 & 50.36 & 31.09 & 43.34 \\  & CodeLlama-7B  & Code & 50.13 & 36.47 & 34.71 & 33.46 & 41.78 & 23.92 & 40.03 & 28.35 & 41.40 & 30.82 & 37.54 \\  & VivardCoder-7B  & Code & 47.42 & 33.58 & 35.54 & 37.09 & 41.17 & 26.03 & 40.88 & 30.60 & 41.02 & 31.73 & 37.63 \\   & Chat & 51.74 & 35.00 & 51.81 & 36.18 & 53.03 & 37.99 & 48.12 & 32.36 & 51.31 & 35.46 & 45.54 \\  & CodeLlama-13B  & Code & 59.87 & 34.17 & 44.96 & 35.82 & 51.56 & 35.83 & 43.28 & 34.56 & 49.84 & 35.08 & 44.47 \\  & VivardCoder-13B  & Code & 50.80 & 32.98 & 38.69 & 35.27 & 43.42 & 28.34 & 40.88 & 34.29 & 43.27 & 32.59 & 39.38 \\  

Table 4: The performance of the Code-expert LLMs on CS-Bench (EN).

   &  &  &  &  &  &  \\   & & Klig & Rng & Klig & Rng & Klig & Rng & Klig & Rng & Klig & Rng & Avg \\   & Chat & 59.57 & 40.92 & 58.83 & 37.94 & 62.65 & 40.60 & 50.94 & 39.29 & 58.31 & 39.77 & 51.56 \\  & Math & 60.202 & 31.56 & 50.56 & 38.66L & 55.93 & 44.87 & 47.69 & 43.82S & 53.64 & 39.41 & 48.45 \\   & Chat & 56.42 & 89.50 & 32.93 & 45.85 & 43.14 & 41.46 & 31.98 & 50.87 & 31.11 & 43.67 \\  & DeepGradMath-Instruct-7B  & Math & 63.98 & 34.82 & 51.39 & 39.64 & 62.26 & 41.66 & 45.92 & 46.56 & 56.88 & 39.67 & 50.49 \\   & Chat & 51.74 & 35.00 & 51.81 & 36.18 & 53.03 & 37.99 & 48.12 & 32.36 & 51.31 & 35.46 & 45.54 \\  & Math & 50.80 & 32.84 & 26.46 & 34.51 & 51.99 & 30.34 & 34.92 & 38.64L & 26.02 & 31.32 & 40.78 \\   & Chat & 64.28 & 41.51 & 56.35 & 40.85 & 61.99 & 43.07 & 51.99 & 41.15 & 58.73 & 41.68 & 52.52 \\  & VivardMath-7B  & Math & 60.17 & 28.67 & 56.41M & 34.91 & 58.52 & 41.51 & 47.01 & 42.85 & 55.77 & 36.67 & 48.82 \\  

Table 3: The performance of the Math-expert LLMs on CS-Bench (EN). We use blue to emphasize areas where the expert LLMs improve compared to the Chat LLMs.

Figure 6: The score changes on CS-Bench as LLM’s Math/Code score increases. \(p\) denotes Pearson correlation coefficient. We obtain the scores on Math/Code datasets from .

LLMs. Therefore, we are more concerned with identifying the specific aspects of CS where Math and Code models show improvement. Regarding mathematics, InternLm-Math-7B improves InternLm2-7B's performance in CO, CN, and OS reasoning tasks, while DeepseekMath exhibits significant improvements across all domains. According to , DeepseekMath effectively maintains general knowledge and reasoning ability during specialization. Conversely, MAammoTH and WizardMath perform poorly due to just fine-tuning on limited mathematical datasets, resulting in a significant decline in general knowledge and reasoning. The score changes in LLMs suggest that OS is most closely linked to mathematics, followed by CO, and lastly DSA and CN. In terms of Code, many Code models show significant improvements in DSA (especially knowledge) and OS (especially reasoning), such as CodeLlama and Dolphcoder. This indicates that the disciplines of DSA and OS are more closely related to, thus enhancing knowledge and reasoning abilities in these directions, while CO and CN have lower relevance, leading to a decrease in scores. Finally, we observe that the enhancement brought about by small-scale expert LLMs compared to larger-scale LLMs is more pronounced (see CodeLlama-7B/13B, WizardCoder-7B/13B). We attribute this to the supplementary need for specific knowledge and reasoning capabilities in small-scale LLMs, whereas large-scale LLMs already encompass a greater breadth of knowledge and stronger reasoning abilities, resulting in diminishing gains from further training in specific domains.

## 4 Related Work

Exploration of LLMs in Computer Science.Given the powerful capabilities of LLMs, recent research has explored their potential applications across various industries and scientific fields [12; 10; 64; 65; 9; 66; 11; 67; 8; 13]. Currently, studies exploring LLMs in the field of computer science fall into two main categories. The first category includes broad evaluation benchmarks covering various fields, such as MMLU , CMMLU , C-Eval , Xiezhi , and M3KE . However, computer science constitutes only a small fraction of these benchmarks, accounting for less than 5% and lacking detailed CS-specific analysis. The second category focuses solely on exploring specific applications of LLMs within computer science, such as network topology , cybersecurity [68; 15], and software engineering [16; 69]. Nonetheless, there has been a persistent lack of comprehensive evaluation of LLMs' foundational knowledge and reasoning abilities in computer science. To address this gap, we propose CS-Bench and conduct a thorough evaluation of LLMs, providing guidance for understanding and improving their performance in the CS field.

Evaluation of LLMs' Capabilities.Evaluating and understanding the capabilities of LLMs is a major focus within the NLP community. Researchers have extensively explored the capabilities of LLMs including planning , multilingual processing [71; 72], instruction following [73; 74], and out-of-distribution generalization [75; 76]. Recently, there has been growing interest in LLMs' abilities in mathematics [22; 23; 24; 25; 26; 27], code programming [59; 57; 58; 28; 29], and logical reasoning [30; 31; 32; 33]. While individual capabilities have been well-studied, research on their integrated application and interrelationships remains sparse. Different from , which investigates interactions between abilities during the supervised fine-tuning phase, we choose computer science as the research context and utilize CS-Bench to delve into the relationship between LLMs' performance in computer science and their mathematical and coding abilities.

## 5 Conclusion

In this work, we introduce CS-Bench, the first benchmark specifically designed to systematically analyze the knowledge and reasoning capabilities of mainstream LLMs in the field of computer science. Our evaluation of over 30 models highlights that even the top-performing GPT-4o has significant room for improvement in computer science. Further score-scale experiments and error type analyses provide directions for enhancing LLMs in the field. Moreover, our investigation into the relationship between computer science, mathematics, and coding demonstrates their close interconnections and provides valuable insights into LLMs' cross-abilities and applications.