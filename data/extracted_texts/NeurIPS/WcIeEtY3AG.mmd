# Spiking Transformer with Experts Mixture

Zhaokun Zhou\({}^{1,2}\), Yijie Lu\({}^{1}\), Yanhao Jia\({}^{3,7}\), Kaiwei Che\({}^{1,2}\), Jun Niu\({}^{1}\), Liwei Huang\({}^{4,2}\), **Xinyu Shi\({}^{5,4}\)**, **Yuesheng Zhu\({}^{1}\)**, **Guoqi Li\({}^{6,2}\)**, **Zhaofei Yu\({}^{5,4}\), Li Yuan\({}^{1,2}\)\({}^{}\)**

\({}^{1}\)School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University

\({}^{2}\)Peng Cheng Laboratory

\({}^{3}\) College of Computing and Data Science, Nanyang Technological University

\({}^{4}\)School of Computer Science, Peking University

\({}^{5}\)Institute for Artificial Intelligence, Peking University

\({}^{6}\)Institute of Automation, Chinese Academy of Sciences

\({}^{7}\)Deep NeuroCognition Lab, I2R and CFAR, Agency for Science, Technology and Research

EqualCorresponding author

###### Abstract

Spiking Neural Networks (SNNs) provide a sparse spike-driven mechanism which is believed to be critical for energy-efficient deep learning. Mixture-of-Experts (MoE), on the other side, aligns with the brain mechanism of distributed and sparse processing, resulting in an efficient way of enhancing model capacity and conditional computation. In this work, we consider how to incorporate SNNs' spike-driven and MoE's conditional computation into a unified framework. However, MoE uses softmax to get the dense conditional weights for each expert and TopK to hard-sparsify the network, which does not fit the properties of SNNs. To address this issue, we reformulate MoE in SNNs and introduce the Spiking Experts Mixture Mechanism (SEMM) from the perspective of sparse spiking activation. Both the experts and the router output spiking sequences, and their element-wise operation makes SEMM computation spike-driven and dynamic sparse-conditional. By developing SEMM into Spiking Transformer, the Experts Mixture Spiking Attention (EMSA) and the Experts Mixture Spiking Perceptron (EMSP) are proposed, which performs routing allocation for head-wise and channel-wise spiking experts, respectively. Experiments show that SEMM realizes sparse conditional computation and obtains a stable improvement on neuromorphic and static datasets with approximate computational overhead based on the Spiking Transformer baselines.

## 1 Introduction

The spiking neural networks (SNNs) are regarded as the third generation of neural networks , distinguished by biological plausibility , spike-driven characteristic, and low power consumption. SNNs emulate the dynamics of biological neurons at a microscopic level, utilizing asynchronous binary spikes for information transmission. The membrane potential of spiking neurons in SNNs is only updated upon the arrival of spikes, avoiding calculations of zero values. The inherent features make SNNs promising candidates for low-energy consumption on neuromorphic hardware, such as TrueNorth  and Loihi . There are lots of architectures in SNNs include Spiking Recurrent Neural Networks , ResNet-like SNNs [6; 7; 8; 9], Spiking Graph Neural Networks , and Spiking Transformers [11; 12; 13]. Spiking Transformers stands at the forefront. Spikformer  introduces Spiking Self-Attention (SSA). The Spike-Driven Transformer  introduces Spike-driven SelfAttention. Other works explore the Spiking Transformer in terms of structural improvements [14; 15; 16; 17], training methods , and different tasks , respectively.

Mixture-of-Experts (MoE) [20; 21] is known for allowing each expert to learn specific tasks or features, showing better performance, conditional computing and dynamic adaptability, which are crucial features in the brain mechanism [22; 23]. In this work, we are committed to exploring the effective integration of MoE and Spiking Transformer. As shown in Fig.1(a), MoE introduces a large number of parameters based on the original Transformer, and the conditional computation is achieved by calculating the routing probability of each token on each expert through the softmax function. Selecting Top-K experts based on the routing probability, MoE achieves hard sparsification. However, SNN calculations need to avoid multiplication and cannot use complex softmax functions. The features in SNNs are dynamically sparse and do not require additional TopK. All the parameters of the expert must be loaded, which aggravates the difficulty of neuromorphic chip deployment. These factors make porting MoE to SNNs non-trivial. To tackle this problem, we develop the Spiking Experts Mixture Mechanism (SEMM), as shown in Fig.1(b), a universal SNN-MoE paradigm with the following three main features. 1) The SEMM is spike-driven. The outputs of the expert and the router are spike sequences, and the element-wise operation between them conforms to the SNN characteristics, i.e., avoiding multiplication. 2) SEMM leverages the sparse spiking activation of SNNs to achieve dynamic conditional computation of MoEs, which is more flexible than the fixed hard sparsification of Artificial Neural Network MoE (ANN-MoE). 3) With reasonable parameter count settings, SEMM enables Spiking Transformers to achieve stable performance gains with negligible overhead.

Based on SEMM, we modify the Spiking Self-attention (SSA) and Multi-layer Perceptron (MLP) of Spiking Transformers to obtain the Experts Mixture Spiking Attention (EMSA) and the Experts Mixture Spiking Perceptron (EMSP). EMSA treats each head of SSA as an expert, computes respective attention, and employs a temporal-aware router to integrate attention. As for Multi-layer Perceptron (MLP), it is common to substitute the entire MLP with MoE , which leads to better performance but a significant increase in the overall parameter number. EMSP implements a channel-wise MoE within MLP to overcome the shortcoming. EMSA and EMSP can be inserted directly and seamlessly into existing Spiking Transformer variants [11; 14; 12]. Our work contributes in three main aspects:

* We introduce the Spiking Experts Mixture Mechanism (SEMM), a universal SNN-MoE paradigm. SEMM is spike-driven, capable of efficient dynamic sparse conditional computation.
* Based on SEMM, we develop the Experts Mixture Spiking Attention (EMSA), whose information from all head-wise experts is selectively integrated through a temporal-aware router. We restructure MLP by a channel-level spiking-sparse SEMM, named the Experts Mixture Spiking Perceptron (EMSP). They can seamlessly replace self-attention and MLP in Spiking Transformers.
* Extensive experiments demonstrate the stable performance improvement of SEMM on both static and neuromorphic datasets. Notably, Spike-driven Transformer-8-512 with SEMM achieves a remarkable 76.62% accuracy on ImageNet with \(4\) time steps, surpassing the baseline (74.57%).

## 2 Related Work

**Deep Spiking Neural Networks and Spiking Transformers.** Spatio-temporal backpropagation(STBP)  directly trains SNNs by performing backpropagation on both spatial and temporal domains. Temporal backpropagation  computes the gradients of the timings of existing spikes for the membrane potential at the spike timing. Treshold-dependent batch normalization (tdBN)  is used to extend the network depth. SEW-ResNet  proposed the spiking element-wise residual for SNNs. Spikformer  firstly converts all the components of Vision Transformer (ViT) into

Figure 1: ANN-MoE and Spiking Experts Mixture Mechanism (SEMM). \(N\) denotes the length of image patches.

spike-form and pioneers the field of SNN-Transformer. Spike-driven Transformer  goes further by introducing linear spike-driven self-attention. Spikingformer  proposes a hardware-friendly spike-driven residual learning architecture. Besides, Masked Spiking Transformer  combines SNNs and Transformers from the perspective of the ANN-to-SNN conversion. However, the SNN Transformer architecture that can bring SNNs' superiority into full play is still ongoing research.

**Mixture-of-Experts.** The Mixture-of-Experts (MoE) [28; 29] combines the predictions of multiple specialized experts, which is effective in handling high-dimensional data and complex problems. Researchers explore diverse gating mechanisms [30; 31], optimizing expert allocation strategies [32; 33], and enhancing the scalability of MoE models [34; 35]. Sparsely-Gated Mixture-of-Experts  adopted MoE into architectures such as the Long Short-Term Memory (LSTM) , showcasing effectiveness in Language Modeling. The Transformer also benefits from MoE with the substitution of the Multi-layer Perceptron (MLP) [24; 37]. Switch Transformer  has scaled the models with trillions of parameters. Currently, there is no existing work on MoE with Spiking Transformers.

## 3 Methodology

### Preliminaries and Overall Architecture

**Spiking neuron** is the basic unit of SNNs. For the dynamics of the Leaky Integrate-and-Fire (LIF) neuron used in this work, the \(t-\)-time-step membrane potential \(U[t]\) is equal to the sum of the state potential \(H[t-1]\) at the previous time step and the input \(X[t]\). When membrane potential exceeds the threshold \(u_{th}\), the neuron will fire a spike, otherwise, it remains inactive. Consequently, the output \(S[t]\) only contains binary values, either 1 or 0. \(()\) is a Heaviside function that satisfies \((x)=1\) when \(x 0\), otherwise \((x)=0\). \(H[t]\) is the temporal state output, and \(V_{reset}\) denotes the reset potential after a spiking event. \(<1\) determines the rate of decay. If the neuron remains inactive, the potential \(U[t]\) decays towards \(H[t]\) over time. LIF can be described as:

\[U[t] =H[t-1]+X[t],\] (1) \[S[t] =(U[t]-u_{th}),\] (2) \[H[t] =V_{reset}S[t]+( U[t])(1-S[t]).\] (3)

**Spiking Transformer[11; 14; 12]** baselines contain Patch Splitting (PS), Relative Position Embedding (RPE), Spiking Self Attention (SSA) (e.g., Spiking Self-Attention  and Spike-driven Self-Attention), MLP and linear classification head, as shown in Fig. 2(a). Given a 2D image sequence \(^{T C H W}\), where \(T\), \(C\), \(H\), and \(W\) denote time-step, channel, height and width, the PS splits it into a sequence of \(N\) flattened spike patches \(\) with \(D\) dimensional channel. A Convolution-BatchNorm-LIF block generates Relative Position Embedding (RPE):

Figure 2: (a) The overview of Spiking Transformer. (b) The Experts Mixture Spiking Attention (EMSA). (c) The Experts Mixture Spiking Perceptron (EMSP). EMSA and EMSP can directly replace SSA and MLP in (a).

\[_{0} =()+,\] (4) \[_{l}^{{}^{}} =(_{l-1})+_{l-1},\] (5) \[_{l} =(_{l}^{{}^{}})+_{l}^{{}^{ }},\] (6) \[ =((_{L})),\] (7)

where the \(_{0}\) is fed into the \(L\)-blocks and each block consists of a SSA and a MLP. \(_{l}^{{}^{}},_{l}\) are spike sequences and \(l=1...L\) is layer. A Global Average-pooling (GAP) is utilized on the \(_{L}\) and the linear Classification Head (CH) to output the prediction \(\). See Appendix. A for more details.

### Spiking Experts Mixture Mechanism

Before exploring the adaptation of spiking MoE in SNNs, we first review MoE in ANNs. An ANN-MoE layer typically comprises a set of \(m\) experts \(_{}=\{_{1},_{2},...,_{m}\}\), along with a router \(_{}\) for selecting the corresponding experts. Given an input sequence \(_{}\), the resulting output can be expressed as the sum of the Top-K selected experts from \(m\) candidates using a router:

\[ =_{k=1}^{K}_{k}(_{}) _{k}(_{}),\] (8) \[() =((_{}_ {},K)),\] (9) \[(,K) =\{&$ is in the top $K$ elements}\\ 0&.\] (10)

where \(_{}\) is the router weight matrix. The \((,K)\) together with the \(()\) sets all elements of the routing vector to zero except the largest top \(K\) values. The sparse conditional computing is obtained from the selecting of TopK and the different routing weights of the softmax, while \(K\) is usually taken as \(0.5*m\). We argue that the ANN-MoE is not suitable for SNN for two main reasons. First, the float-point routing-expert and the softmax which involve exponentiation and division, do not adhere to the computation principles of SNNs. Second, SNN experts are inherently highly sparse, so the hard sparsification approach of additional TopK is unnecessary. An event-triggered spike-based sparse conditional computation on asynchronous neuromorphic chips is needed more than TopK. To bridge these gaps, we introduce a generalized representation of the Spiking Experts Mixture Mechanism (SEMM), which is as follows,

\[(,,())=(,),\] (11)

where \(=\{_{1},_{2},...,_{m}\}\) represents the spiking sequence of \(m\) spiking experts in the Spiking Transformer, and \(\{0,1\}^{T N m}\) represents the spiking router for allocating computation. \(()\) denotes the element-wise form of the operation between the router and the expert output spiking sequence, i.e., Hadamard product and addition.

As a MoE mechanism specifically designed for SNNs, SEMM has the following three significant advantages. i) **Spike-driven.** The SEMM is spike-driven, which is important for SNNs. Due to the spike-driven computation mode of experts, i.e., Spiking Self-attention and Spiking-MLP, SEMM computations are triggered by sparse spikes of experts and require only synaptic manipulation. For example, the Hadamard product between the spiking signals \(\) and \(\) is equivalent to mask. ii) **Sparse-spiking conditional computation.** The SEMM subtly utilizes the sparse activation of the spiking routers for the conditional computation of MoE. SEMM has a variable sparsity when dealing with different data. Additionally, SEMM does not suffer from the load imbalance problem in ANN-MoE, i.e., TopK selects fixed number of experts. The sparse conditional computation is distributed to each expert. iii) **Efficient computation.** Unlike loading with multiple heavy expert modules of ANN-MoE, the SEMM has comparable parameters and operations to the previous SSA and MLP of Spiking Transformers. These are further discussed in Sec. 3.5. Without loss of generality, we use the mainstream architecture of Spiking Transformer for SEMM embedding in Sec. 3.3 and Sec. 3.4.

### Experts Mixture Spiking Attention

We begin by reviewing the processing of Spiking Self-Attention (SSA). Different from vanilla ANN-Transformers , it discards the softmax normalization for the attention map. The SSA mechanism can be described by the following equation:

\[=_{}(_{}()),=_{}(_{}( )),=_{}(_{ }()),\] (12) \[=(,,)=\{ (^{} *s),\\ (_{}( )),.\] (13)

where \(\) represents the spiking neuron and \(\) represents that the features pass sequentially through Linear and BatchNorm. \(s\) is the scaling factor and \(,,\{0,1\}^{T N D}\) are in spike-form. \(\{0,1\}^{T N D}\) is the spiking output of SSA. \(\) is the Hadamard product, and \(_{}()\) denotes the sum of each column. As shown in Fig 2(b), the Experts Mixture Spiking Attention (EMSA) based on SEMM and SSA is formulated as:

\[_{}=(_{}, ,),\] (14) \[=\{_{1},_{2},...,_{m}\},\] (15) \[=((_{}))=\{_{1},_{2},...,_{m}\},\] (16) \[(,,())=_{i= 1}^{m}_{i}*_{i},\] (17) \[=((( ,,()))_{o}),\] (18)

where \(_{m}\{0,1\}^{T N d}\) is the output of each SSA experts, and \(_{}^{D m}\) is the router weight matrix. \(\{0,1\}^{T N m}\) is the routing sequence. We set \(d D\) by default to avoid introducing too many parameters. The operation \(()\) between expert-router is computed by masking the expert using router pair-by-pair and then summing them up. The output of EMSA is obtained by performing matrix multiplication between \((,,())\) and synaptic weight \(_{o}^{d D}\), the same as the last layer of SSA block in Spiking Transformers.

### Experts Mixture Spiking Perceptron

As illustrate in Fig. reffig:method, the Experts Mixture Spiking Perceptron (EMSP) launches the channel-wise SEMM within MLP. The architecture can be written as follows:

\[=((( (_{1}))))\{0,1\}^{T N m}\] (19) \[=((_{}))\{0,1\}^{T N m},\] (20) \[(,,())=,\] (21) \[=(((( ,,())_{o}))\{0,1\}^{T N  D},\] (22)

where \(_{1},_{}^{D m}\) is the weight matrix of first layer in MLP and router, respectively. We integrate a \(3 3\) Depth Wise Convolution layer \(\) to capture local features on each channel expert, which has fewer parameters and is computationally efficient compared to original convolution. \(_{o}^{m D}\) is the wight matrix of output layer. The original Spiking Transformer's MLP would have a \(D\) to \(4*D\) channel dimension increase after the first layer and the second layer would reduce the dimension back to \(D\). In EMSP we set \(m\) to \((8//3)*D\) to try to match the parameter number of the original MLP. EMSP integrates sparse routing of multiple channel-wise experts. It can also be viewed as the channel-wise gating, which is suitable for temporal information processing, allowing information to flow unimpeded through potentially many time steps . The gate (router) branch and expert branch can be also regarded as incorporating the Spike Element-Wise (SEW)  residual block into EMSP. Channel-wise convolution and element-wise (Hadamard) products only introduce a minor increase in computational cost. By appropriately configuring the number and dimensions of expert networks, our EMSP achieves more efficient computation compared to the original spiking MLP. More details of the computational overhead are given in the next sub-section.

### Characteristics of SEMM

We explain each of the three advantages of SEMM, i.e., Spike-driven, Sparse-spiking conditional computation and efficient computation.

**Spike-driven** has the following formal definition, meaning that the gathering of input current is initiated by sparse spikes released from presynaptic neurons:

**Definition 1**.: _In SNN, the operation is spike-driven if the input currents satisfy the following form,_

\[I_{i}[t]=_{j}w_{i,j}s_{j}[t]=_{j,s_{j}[t] 0}w_{i,j},\] (23)

_where \(I_{i}[t]\) is the input current of the \(i\)-th postsynaptic neuron at time step \(t\), \(s_{j}[t]\{0,1\}\) is the spike output of the \(j\)-th pre-synaptic neuron, \(w_{i,j}\) is the weight of the synaptic connection from \(j\) to \(i\)._

For EMSA, the input current for the LIF in Eq. 18 at a specific time step \(t\) is given by:

\[[t]=([t],[t], ())_{o}=_{i=1}^{m}r_{i}[t]*_{i}[t] _{o},\] (24) \[I_{p,q}[t]=_{i=1}^{m}r_{i}[t]_{l}a_{i,p,l}[t]w_{l,q}= _{i,l\\ (r_{i}[t] s_{i,p,l}[t]) 0}w_{l,q},\] (25)

where \([t]\) has a dimension of \(P Q\), with \(p\) and \(q\) represent the \(p\)-th row and \(q\)-th column, respectively. EMSA is essentially spike element-wise addition after masking operation on SSA, consistent with the characteristics of spike-driven. For EMSP, the operation between two spiking sequences \(\) and \(\) corresponds to the logical AND function , which also conforms to the spike driven,

\[[t]=([t],[t],())_{o}=([t])[t]_{o},\] (26) \[I_{p,q}[t]=_{l}e_{p,l}[t]r_{p,l}[t]w_{l,q}=_{ {c}l\\ (e_{p,l}[t] r_{p,l}[t]) 0}w_{l,q}.\] (27)

**Sparse-spiking conditional computation** means using spiking routers to dynamically allocate the computation in temporal and spatial dimension. More analysis is detailed in experiments.

**Efficient computations** means that SEMM approximates SSA and MLP in terms of number of parameters and theoretical synaptic operations. Tab. 1 demonstrates the computation load for EMSA, EMSP versus SSA and MLP. In terms of parameter number, despite having a routing layer \(mD\), EMSA has a smaller number of parameters than SSA when the number of experts is within a reasonable range (\( 2\)). EMSP additionally introduces a \(3 3\) depthwise convolution, and the number is slighter higher than MLP. Due to the similarity of the actual spiking rates, EMSA is smaller than SSA on the \(TND^{2}\) term, while the calculation of the additional introduced by SEMM is on the \(TND\) term (much smaller than \(TND^{2}\)) and therefore can be ignored. The situation is similar on EMSP, with depth-wise convolution adding a slight computational overhead. The design of EMSP is different from ANN-MoE, which selects multiple heavy MLPs as experts. We demonstrate it's validity in Fig. 3, i.e., it performs better while the number of parameters is much smaller than ANN-MoE.

## 4 Experiments

### Sparse Conditional Computation Analysis

We analyze the average spiking rate (ASR) of routers for EMSA and EMSP on the ImageNet validation set, which is shown in Tab. 2. The SD-Transformer-8-512 is used for the analysis. The ASR of EMSA is around 0.5, which is comparable to the regular TopK setting of ANN-MoE. The ASR of EMSP is low and gradually decreases as the block deepens, compared to the fixed TopK hard sparse in ANN-MoE, SEMM fully reflects the advantage of SNN dynamic sparse conditional computation. To further verify the spiking router, we ablate it, i.e., cancel it in EMSA and EMSP, as

Figure 3: Comparison of parameters-accuracy for different MoEs. Methods of each line from left to right correspond to: 1. the baseline, 2. EMSP, 3. ANN-router with four heavy MLP experts, 4. Spiking-router with four heavy MLP experts, respectively. The last two use softmax and TopK (\(K=2\)).

shown in Fig. 4, the accuracy decreases significantly after canceling the router, and it is even lower than the baseline model on the Spikingformer. Directly analyzing the conditional computation of SEMM is challenging, we therefore use visualizations to illustrate this intuitively. As show in Fig. 5, for the same image, each expert's router assigns a different computational region, e.g., the third router filters the background of the expert's features, while the second router assigns the computation to the foreground target. The computation allocation of the spiking router to the irregular object "snake" can be seen that the router is highly dynamic and effective. See Appendix. D for more samples. In addition, as shown in Fig. 6, we also report the ASR of spatial-temporal locations of routers in different kinds of images. As can be seen by the difference in average firing rates, spiking router has a dynamic adjustment of ASR processing different kinds of images, further illustrating its data-dependent conditional computation property.

### Results on various Datasets

We conduct experiments on static datasets, i.e., ImageNet  and CIFAR , and neuromorphic datasets, i.e., CIFAR10-DVS , DVS128 Gesture  to verify the effectiveness of SEMM. See Appendix. C for more details. **ImageNet** results are shown in Tab. 3 which mainly compares SEMM with the Spiking Transformer baselines. At slightly lower model parameter counts, SEMM is steadily superior to baselines. For instance, Spikformer-8-512 with SEMM is 2.55% higher than the baseline with 28.22\(M\) parameters, Spike-driven Transformer-8-384 with SEMM obtain 2.05% improvement. SEMM on Spikingformer presents similar findings.

    & Block0 & Block1 & Block2 & Block3 & Block4 & Block5 & Block6 & Block7 \\  EMSA & 0.64 & 0.68 & 0.49 & 0.52 & 0.47 & 0.43 & 0.51 & 0.50 \\  EMSP & 0.25 & 0.30 & 0.33 & 0.27 & 0.20 & 0.10 & 0.05 & 0.02 \\   

Table 2: The average spike rate of EMSA and EMSP router in 8 blocks testing on the ImageNet.

Figure 8: Accuracy with different experts number.

    & Param & OPs \\  SSA & \(4D^{2}\) & \(4TND^{2}+2N^{2}D\) \\  EMSA & \((1+1/m)D^{2}+(2d+m)D\) & \((1+1/m)ND^{2}+(2d+m)ND+(D+md)RTN^{2}\) \\  MLP & \(8D^{2}\) & \(8ND^{2}\) \\  EMSP & \(8D^{2}+24D\) & \(8ND^{2}+24ND\) \\   

Table 1: Number of parameters and theoretical synaptic operations. \(\), \(\), \(\) and \(R\) denote the average spike firing rates (the proportion of non-zero elements in the spike matrix) in various spike matrices. \(T\), \(N\), and \(D\) are the time step, sequence length, and channel dimension of the input features, respectively. \(d\) is the channel dimension of \(_{m}\) and \(m\) is the number of experts. The details are provided in the Appendix. B.

The experimental results on CIFAR, CIFAR10-DVS, and DVS128 Gesture are shown in Tab. 4. These four datasets are relatively small. We basically keep the experimental setup in [11; 12; 14], including the network structure, training settings, etc., and details are given in the Appendix. C.1. SEMM has demonstrated stable performance improvements across various datasets when integrated

Figure 5: Visualization of routers as masks. The mask position (black) indicates router of 0 here and the background image is the same for each subplot. We show the dynamic sparsity of spiking router for different experts (horizontal direction) and time steps (vertical direction).

Figure 6: Average spiking rate of different kinds of images in the ImageNet validation set in the spatial-temporal dimension. The height of the cube is the time step. (a) Japanese spaniel. (b) Volcano.

Figure 7: Ablation study of EMSA and EMSP module.

into different Spiking Transformer baselines. Specifically, SEMM achieves SOTA on CIFAR-10 (96.16%), CIFAR-100 (80.26%), CIFAR10-DVS (82.9%) and DVS128 Gesture (99.3%).

### Ablation Study and Hyperparameter Sensitivity

**Module ablation.** EMSA and EMSP together improve the performance of the baseline, as shown in Fig. 7. The use of both EMSA and EMSP alone is better than baseline, illustrating their respective superiority.

**Experts number.** We examine the utility brought by different numbers of experts of EMSA. The results of Spiking Transformer baselines on CIFAR are presented in Fig. 8. It indicates that within a certain range of expert numbers, the results can still be competitive and robust. Among these, we select \(4\) experts as the parameter setting for our final application.

    &  &  &  &  \\   & \(T\) & Acc & \(T\) & Acc & \(T\) & Acc & \(T\) & Acc \\  tdBN  & 10 & 67.80 & 40 & 96.90 & 6 & 93.20 & - & - \\ PLIF  & 20 & 74.80 & 20 & 97.60 & 8 & 93.50 & - & - \\ DIET-SNN  & - & - & - & - & 5 & 92.70 & 5 & 69.70 \\ Dspike  & 10 & 75.40 & - & - & 6 & 94.30 & 6 & 74.20 \\ DSR  & 10 & 77.30 & - & - & 20 & 95.40 & 20 & 78.50 \\  Spikformer  & 10 & 78.90 & 10 & 96.90 & & & & \\
16 & 80.90 & 16 & 98.30 & 4 & 95.19 & 4 & 77.86 \\ Spikformer + SEMM & 10 & **82.32** & 10 & **97.56** & & & & \\
16 & **82.90** & 16 & **98.63** & 4 & **95.78** & 4 & **79.04** \\  Spike-Driven Transformer  & 10 & 78.90 & 10 & 96.90 & & & & \\
16 & 80.00 & 16 & 99.30 & & & & \\
10 & **81.10** & 10 & **97.56** & & & & \\
16 & **82.42** & 16 & **99.30** & 4 & **96.12** & 4 & **80.26** \\  Spikingformer  & 10 & 79.90 & 10 & 96.20 & & & & \\
16 & 81.30 & 16 & 98.30 & 4 & 95.81 & 4 & 79.21 \\ Spikingformer + SEMM & 10 & **80.70** & 10 & **96.88** & & & & \\
16 & **82.10** & 16 & **98.56** & & & & **96.16** & 4 & **80.24** \\   

Table 4: Results on CIFAR10-DVS, DVS128 Gesture, and CIFAR.

   Methods & Architecture & Param (M) & Time Step & Top-1 Acc (\(\%\)) \\  SEW ResNet & SEW-ResNet-34 & 21.79 & 4 & 67.04 \\ SEW-ResNet-101 & 44.55 & 4 & 68.76 \\  & SEW-ResNet-152 & 60.19 & 4 & 69.26 \\  MS-ResNet & MS-ResNet-18 & 11.69 & 6 & 63.10 \\ MS-ResNet-34 & 21.80 & 6 & 69.42 \\  & MS-ResNet-104\({}^{*}\) & 77.28 & 5 & 76.02 \\  Spikformer & Spikformer-8-384 & 16.81 & 4 & 70.24 \\ Spikformer-8-512 & 29.68 & 4 & 73.38 \\ Spikformer + SEMM & Spikformer-8-384 & **16.05** & 4 & **72.86** \\ Spikformer-8-512 & **28.22** & 4 & **75.93** \\  Spike-driven Transformer & SD-Transformer-8-384 & 16.81 & 4 & 72.28 \\ SD-Transformer-8-512 & 29.68 & 4 & 74.57 \\ SD-Transformer + SEMM & SD-Transformer-8-384 & **16.05** & 4 & **73.93** \\ SD-Transformer-8-512 & **28.22** & 4 & **76.62** \\  Spikingformer & Spikingformer-8-384 & 16.81 & 4 & 72.45 \\ Spikingformer-8-512 & 29.68 & 4 & 74.79 \\ Spikingformer + SEMM & Spikingformer-8-384 & **16.05** & 4 & **73.58** \\ Spikingformer + SEMM & Spikingformer-8-512 & **28.22** & 4 & **76.03** \\   

Table 3: Results on ImageNet-1k. Model-L-D represents a model with \(L\) encoder blocks and \(D\) channels.

Conclusion

In this work, we explored the feasibility of adapting MoE in Spiking Neural Networks and formulated an SNN-MoE paradigm named the Spiking Experts Mixture Mechanism. Unlike the vanilla MoE, which uses softmax and TopK hard sparse, SEMM implements dynamic conditional computation from the viewpoint of spiking sparse activation. With redesigned Router-Expert pairs and element-wise spike-driven operations, SEMM is computation-efficient and SNN-compatible. Embedded in Spiking Transformers, SEMM-based EMSA and EMSP can bring stable performance improvement on static and neuromorphic datasets. SEMM paradigm can inspire future exploration of high-performance, high-capacity Spiking Transformers. We hope SEMM can bring vitality to dynamic conditional computation and the design of next-generation architecture for SNNs. Future work will explore SEMM implementation in a wider range of tasks and larger SNN models.

## 6 Acknowledgments and Disclosure of Funding

We sincerely thank Dr. Wei Fang for his assistance with this work. This work is supported by grants from the National Natural Science Foundation of China (62088102, 62202014, 62332002, 62425101, 62236009, U22A20103, 62441606), Shenzhen Basic Research Program (No.JCYJ20220813151736001), the major key project of the Pengcheng Laboratory (PCL2021A13) and National Science Foundation for Distinguished Young Scholars (62325603). Computing support was provided by Pengcheng Cloud Brain.