# Large Language Model Benchmarks Do Not Test Reliability

Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry

MIT

{jvendrow,evendrow,beery,madry}@mit.edu

Equal contribution. \(\)Equal advising.

###### Abstract

When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also _reliable_. Many benchmarks have been created to track LLMs' growing capabilities. However, there has been no similar focus on measuring their reliability. To understand this landscape, we first investigate how well current benchmarks quantify model reliability. We find that pervasive label errors compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.

Motivated by this gap in the evaluation of reliability, we propose the construction of so-called _platinum_ benchmarks that are carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures reveals previously unidentified patterns of questions on which frontier models consistently struggle.

## 1 Introduction

Large language models (LLMs) have demonstrated impressive capabilities in areas such as problem solving [16; 28], knowledge retrieval , and code generation [15; 20]. Major research effort continues to advance the frontier of LLM capabilities [1; 27]. However, these models still sometimes exhibit failures even on tasks far simpler than these frontier capabilities [44; 25]. Practitioners might thus be worried whether this unreliability can pose significant risks, especially in accuracy- and safety-critical applications.

Indeed, in sectors such as healthcare, finance, insurance, and legal services, model errors can lead to serious ramifications (e.g., jeopardizing patient outcomes or causing financial losses). In fact, mistakes by LLMs in real-world deployments have already caused legal liability  and generated controversy . In light of these issues, it is important to understand when we can confidently deploy LLMs in such situations. These concerns motivate the central question of our work:

_On what kinds of tasks are frontier models actually reliable?_

To identify such tasks, a natural approach would be to examine existing benchmarks on which current models already perform well. Specifically, we might want to investigate older benchmarks (e.g., GLUE , SQuAD , GSM8K ) that tend to evaluate simpler capabilities than current ones. These benchmarks are rarely used today due to the commonly held view that performance on them has "saturated"--that is, that models have reached a sufficient or "human-level" performance on the benchmark, and remaining errors can be attributed to label noise or ambiguity in the benchmark itself.

Indeed, some recent releases of frontier LLMs have excluded evaluations on GSM8K (a dataset of grade-school level math word problems), for example [28; 2], following concerns that it has reached saturation  (current frontier models achieve \(\)95% accuracy [10; 1]).

Since we are interested in reliability though, we need to ensure that models can execute tasks with near-perfect accuracy. So, we would like to know if models are truly reliable on benchmarks once they reach saturation (e.g., achieving 95% on GSM8K), or if we should be worried about lingering model errors in the remaining 5%, hidden among the label noise.

### Contributions

In this work, we demonstrate that, indeed, the remaining failures on these older benchmarks are not just label errors, and more broadly, that current benchmarks are not well equipped for testing model reliability. We then propose a new style of benchmarking to rigorously quantify model reliability, and make an initial effort towards constructing such benchmarks.

To understand the need for this new framework, we first examine how LLM benchmarking has evolved to disincentivize progress on reliability.

The status quo of LLM benchmarkingThe difficulty of LLM benchmarks has increased over time to track the progression of their frontier capabilities. For example, the complexity of math and science tasks that these models are evaluated on has grown from elementary and middle school level (e.g., SVAMP , GSM8K ), to high school and college level (e.g., MATH , MMLU ), to graduate level problems (e.g., GPQA ) 2.

a result of rapidly progressing model capabilities, current benchmarks (like the ones mentioned above) follow a consistent pattern of development, progress, and eventual retirement that we refer to as the _life cycle of benchmarks_. Thi Specifically, a benchmark is first created to test a frontier capability for which current LLMs achieve a low accuracy (i.e., below 50%). Models' performance on the benchmark increases over time and eventually plateaus, often around 90-95%. At this point, the benchmark faces the same fate that we discussed earlier with older benchmarks like SQuAD and GSM8K: it is deemed "saturated," with remaining errors attributed to label noise, and gradually retired as the community shifts focus to newer, more difficult benchmarks.

We believe that this life cycle has led to a gap in benchmarking: since benchmarks are discarded when models achieve a sufficient, but not perfect, performance (i.e., when they are saturated), model developers are never encouraged to achieve proper reliability on them.

Platinum benchmarksIn order to better evaluate model reliability, we introduce the concept of _platinum_ benchmarks that are carefully curated to minimize label errors and ambiguity, and require 100% performance to pass. Unlike traditional benchmarks, which become "saturated" when models reach high (but imperfect) performance, platinum benchmarks remain relevant until models achieve full reliability. While current iterations of benchmarks quantify the _capabilities frontier_ of LLMs--the most advanced tasks models are able to perform--platinum benchmarks allow us to identify their _reliability frontier_: the most advanced tasks models can perform consistently without error.

We demonstrate our approach by constructing platinum versions of 15 "saturated" benchmarks across six categories of capabilities by systematically re-labeling them. We find that that many of these benchmarks are indeed riddled with errors (e.g., 5% of GSM8K). In fact, for the majority of benchmarks we investigate, more than half of model failures can be attributed to label noise.

Despite the relative simplicity of these tasks compared to frontier challenges, on most of our platinum benchmarks no state-of-the-art model we evaluate gets 100%. For instance, multiple frontier models, including GPT-4o, fail at the following basic pronoun resolution task from Winograd WSC :

In the sentence, "John couldn't see the stage with Billy in front of him because he is so short," what does "he" in "he is so short." refer to in the phrase? [A: John, B: Billy]

Moreover, our analysis of model failures reveals previously unidentified patterns of questions that frontier models consistently struggle with. We view our work as the first step in a new practice of quantifying LLM reliability. See Appendix A for the related work.

## 2 Cleaning Up Noisy Benchmarks

To make a noisy benchmark "platinum," we need to update it to remove or correct label errors. In this section, we categorize common types of errors and specify our approach for detecting and correcting them. In Section 3, we will then leverage these platinum benchmarks in order to evaluate the reliability of frontier models.

### Experimental setup

Benchmarks includedWe investigate fifteen benchmarks covering six categories of capabilities: mathematics (SingleOp , SingleEq , MultiArith , SVAMP , GSM8K , MMLU High School Math ), logic (BIG-bench Object Counting, BIG-bench Logical Deduction, BIG-bench Navigate ), table understanding (TabFact ), reading comprehension (SQuAD2.0 , HotPotQA , DROP ), commonsense reasoning (Winograd WSC ), and visual understanding (VQA v2.0 ). For datasets with publicly available test splits with solutions, we use the test split, otherwise we use the validation split. Many of these benchmarks are large (e.g., the VQA v2.0 validation set has over 200,000 questions). In order to ensure the quality of our cleaning process, we select smaller subsets at random from many of these benchmarks.

ModelsWe test several current frontier models, including popular proprietary LLMs (GPT-4o mini and GPT-4o , Claude 3.5 Sonnet , Gemini 1.5 Flash and Gemini 1.5 Pro , o1-mini ), open-weights LLMs (Llama 3.1 70B Instruct and Llama 3.1 405B Instruct , Mistral Small and Mistral Large).

PromptingWe use chain-of-thought prompting  (i.e., asking the model to think step-by-step), except with o1-mini, as its official prompting guide recommends omitting this prompting technique4. All questions are asked in a zero-shot setting. We use a temperature of 0.5, as deployed models tend to use lower temperatures to balance coherence and creativity. The exact prompts we use are provided in Appendix C.2.

Figure 1: **Examples of errors in current LLM benchmarks.****(a)** For mislabeled questions, we fix the solutions and include the re-labeled examples in our benchmark. We find three common categories of “bad” questions: **(b)** there is a logical contradiction in the problem statement, **(c)** there is ambiguity leading to many plausible solutions, or **(d)** there is a clear flaw in the construction of the question, such as missing specifications. We remove such questions when cleaning benchmarks.

MetricsAll the benchmarks we use are either multiple choice or have a single correct answer (except for reading comprehension datasets such as SQuAD2.0, for which we manually expand the set of correct responses--see Appendix C.1.2 for further details). Thus, we can simply compare this answer to the model prediction to evaluate correctness. We report the number of errors rather than accuracy to better differentiate between models, as for many benchmarks we expect models to have accuracies in the high 90s.

### Identifying Errors in Benchmarks

What makes a question bad?Before correcting errors in benchmarks, we first need to categorize the kinds of issues we aim to resolve. Each example in a benchmark consists of a question and a solution. Sometimes, a question can be well-written, but the solution is mislabeled. For instance, Figure 1(a) shows a question from SVAMP for which the given solution is incorrect. For such examples we can simply re-label the solution, allowing us to keep the example in the benchmark.

In other cases, though, the question itself is poorly written, so simply re-labeling the solution is inadequate. Figures 1(b-d) illustrate three common categories of such issues: (1) a logical contradiction in the problem statement, (2) ambiguity that allows for multiple plausible solutions, or (3) a clear flaw in the question's construction. In many cases, there is no simple way to fix the question (for example, fixing Figure 1(d) would require coming up with a set of equations and corresponding question from scratch). Therefore, we opt to remove poorly written examples during our cleaning process.

How do we efficiently identify errors in a benchmark?Cleaning large datasets can be prohibitively time-consuming, especially when the questions take time to verify (e.g., challenging math problems or retrieval tasks with long contexts). We devise a simple strategy to find problematic questions by examining the agreement among multiple LLMs.

As described above, we divide potential issues with examples from a benchmark into two categories: (1) mislabeled solutions, and (2) poorly written questions (e.g., ones that are ambiguous or ill-posed). To detect these errors, we give each question to several frontier LLMs. We then manually inspect any example on which at least one LLM makes an error. We expect that when a solution is incorrect, frontier models will often disagree with the given solution, and when the question itself is poorly written, models should disagree among themselves. For every example we inspect, we either mark that example as "bad" and remove it if the question is poorly written or relabel the solution if it is incorrect. Since this process checks all model errors, we can be confident that the errors we report are genuine. It is possible that our count is a lower bound, as it is conceivable that both the benchmark solution and LLM solutions are all the same and all incorrect. Further details of the cleaning protocols used for each benchmark are in Appendix C.

 Dataset & & & & & & & & & & & & & & & \\  Type & &  & &  & Tab &  &  & Vis \\ \# Original Questions & 159 & 109 & 174 & 300 & 300 & 270 & 200 & 200 & 200 & 250 & 250 & 250 & 200 & 600 \\   & & & & & & & & & & & & & & \\ \# Bad Questions & 6 & 6 & 2 & 31 & 21 & 3 & 0 & 8 & 0 & 26 & 64 & 82 & 40 & 4 & 352 \\ \# Mislabeled & 0 & 0 & 3 & 3 & 0 & 0 & 0 & 0 & 0 & 3 & 5 & 3 & 3 & 0 & — \\  \# Platinum Questions & 153 & 103 & 172 & 269 & 279 & 267 & 200 & 192 & 200 & 174 & 186 & 168 & 210 & 196 & 248 \\  

Table 1: We clean examples from fifteen popular benchmarks to remove errors and ambiguities. While some programmatically generated benchmarks are error-free, we find a significant number of mislabeled or poorly written questions in others. The number of mislabeled examples is missing from 2.0 as our labels follow a different format than the original benchmark, necessitating all labels to be revised (see Appendix C.1.1 for details on VQA v2.0).

How noisy are saturated LLM benchmarks?In Table 1, we report the number of poorly written and mislabeled questions we identify in each of the fifteen benchmark subsets we investigate. We find that, indeed, many of these benchmarks have a substantial rate of errors, confirming suspicions of flaws in these benchmarks commonly held by the community. For example, the percentage of errors we find in examples from GSM8K, SVAMP, VQA v2.0 and TabFact exceed the percentage of errors reported by frontier models on these benchmarks. This suggests that any error made by frontier models on one of these benchmarks is more likely to be an issue with the benchmark itself than a genuine model error.

For reading comprehension datasets (SQuAD2.0, HotpotQA, DROP), we identify issues with up to 30% of examples. Largely, these issues arise from questions that are sufficiently open-ended such that it is difficult to exhaustively list all possible responses. Additionally, SQuAD2.0 intentionally adds questions that are unanswerable from the given passage to test whether models can abstain from answering (i.e., return N/A). However, the process of making these unanswerable questions often leads to them being highly ambiguous or even nonsensical. For example, one such question asks, "What isn't the gender income inequality in Bahrain?"; this question can be traced to a worker replacing "is" with "isn't" from an original question within SQuAD 5. Following our cleaning protocol, we omit such poorly written questions. In Appendix F, we show specific examples of bad questions that we identify across the fifteen benchmarks.

## 3 Evaluating Reliability with Platinum Benchmarks

### Pinpointing the Reliability Frontier

Within a given category of capabilities (e.g., math problem solving), a platinum benchmark of a certain difficulty allows us to evaluate how reliable models are at performing tasks at that difficulty level. However, the actual reliability frontier of a model might be less or more advanced than the tasks in that specific benchmark. In order to identify this frontier with greater granularity, we need platinum benchmarks at varying levels of difficulty; then, we can estimate a model's reliability frontier by identifying the most difficult benchmark it is able to pass (i.e., score 100% on).

[MISSING_PAGE_FAIL:6]

consistent collapses in reasoning of frontier LLMs. We initially found an instance of each failure mode by examining models' reasoning processes on failures from our platinum benchmarks. We then verify the consistency of such failures by procedurally constructing similar examples. We outline these failure modes below, and provide further details and a more complete analysis in Appendix D.

Example pattern 1: First event biasWe find that when asked: "What happened second: {_some event_} or {_some other event_}" given some context, three models (Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Small) almost always answer with the first event, and will even explicitly acknowledge they are identifying the first event rather than the second:

**Question: _(context paragraph)_** What happened second: Russians blocked Azov or Treaty of Constantinople?

**Solution:** Treaty of Constantinople

**Gemini 1.5 Pro:**... we can conclude that the Russians blocking Azov happened before the Treaty of Constantinople. Answer: Russians blocked Azov

We procedurally generate further examples in this question format and find that each of these three models fails on over 90% of the examples (see Figure D1).

Example pattern 2: Rounding up primesWe find that when Claude 3.5 Sonnet is asked to solve grade-school math problems involving division, it often incorrectly rounds up the quotient even though the division already resulted in a whole number, leading to an answer greater than the solution by one:

**Question:** The school is planning a field trip. The school has 67 classrooms. There are 66 students in each classroom in the school. If there are 6 seats on each school bus. How many buses are needed to take the trip?

**Solution:** 737

**Claude 3.5 Sonnet:**... To find the number of buses needed, we divide the total number of students by the number of seats per bus: 4,422 + 6 = 737. However, since we can't have a fraction of a bus, we need to round up to the next whole number to ensure all students have a seat. Therefore, the school needs 738 buses to take all students on the trip. Answer: 738

Interestingly, we find that this behavior is significantly more common when the final answer is closer to being prime (i.e., it has few divisors; see Figure D2). For example, when we query Claude 3.5 Sonnet on similarly constructed problems with an answer that is prime, the model makes this exact rounding error 20% of the time.

## 4 Discussion and Future Work

There is a significant gap between capability and reliability.Frontier LLMs can solve graduate-level problems (e.g., GPQA ), but as we find, they can still fail on basic logic tasks and elementary-level problems. This discrepancy indicates a wide gap between the capability and reliability frontiers that does not seem to be addressed by scaling models further.

Framing reliability as a deployment metric.Quantifying reliability is a common practice for deployed systems. Within the context of software, for instance, an entire dedicated field of site reliability engineering has emerged to ensure that systems are reliable, with a reliability goal measured by the number of nines of uptime (e.g., five nines, or 99.999% uptime). We hope our work can be a first step towards building out this level of reliability quantification for LLMs.

How do we account for prompt brittleness?A common strategy for further improving performance of LLMs is to carefully adjust prompts, such as by tuning structure and wording individualized to a specific target model (i.e., prompt engineering). But as long as the instructions are clearly stated and the task is explicitly defined, it is reasonable to expect reliable models to perform well regardless of minor variations in prompt wording. Since we are not focused on assessing how well models can follow formatting directions, however, we experimented to choose a prompting strategy that ensures our models do not fail due to output formatting errors. We do not further engineer prompts beyond this; see Appendix C.2 for our specific template. Nevertheless, it is plausible that a specific choice of prompt will affect a model's reliability. We encourage future work to both investigate promptingstrategies that elicit more reliable behavior, and develop models that are less brittle to specific prompt types.

LimitationsWe view our work as an initial effort towards constructing platinum benchmarks. Here, we briefly discuss the limitations of our work and areas for potential improvement by future works.

1. **Capabilities and levels of difficulty covered** Our set of fifteen benchmarks misses a number of relevant capabilities of LLMs, such as coding and tool use. We attempt to cover a wide range of difficulty levels for mathematics, but not for any other capability. We also do not include any benchmarks that require expert-level annotations to clean, as we revise the benchmarks for this preliminary investigation ourselves.
2. **Number of examples per benchmark** Some of our revised benchmarks include as little as 100 examples, often limited by the size of the original benchmark. This limits our ability to quantify reliability with certainty: there may not be a large gap between models that have zero percent or one percent error rate on such a small sample size.
3. **Only re-labeling errors** As we re-label all examples for which some model failed, we can be confident that every error we report is genuine. However, there may still be poorly written questions in our platinum benchmarks among those we did not revise, where, despite error or ambiguity, all models agreed with the stated ground truth.
4. **Benchmark difficulty** As of today, frontier LLMs still fail on sufficiently simple questions that their errors can be quantified without specific expertise. For example, the most difficult task we annotate is high school level mathematics. However, once the reliability frontier of models is sufficiently advanced, expensive expert annotation will be required to construct platinum benchmarks for expert capabilities.

## 5 Conclusion

In this work, we propose the construction of highly cleaned _platinum_ benchmarks to test the reliability of LLMs, and make an initial effort to create such benchmarks by cleaning fifteen existing datasets. We demonstrate that frontier models continue to exhibit failures on basic tasks from these "saturated" benchmarks, showing a gap in current benchmarking practices. We hope that our paper motivates the adoption of platinum benchmarks in evaluating LLMs to ensure they meet the high reliability standards required for real-world applications.