# On the Ability of Developers' Training Data Preservation of Learnware

Hao-Yi Lei, Zhi-Hao Tan, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{leihy, tanzh, zhouzh}@lamda.nju.edu.cn

###### Abstract

The _learnware_ paradigm aims to enable users to leverage numerous existing well-trained models instead of building machine learning models from scratch. In this paradigm, developers worldwide can submit their well-trained models spontaneously into a _learnware dock system_, and the system helps developers generate _specification_ for each model to form a learnware. As the key component, a specification should characterize the capabilities of the model, enabling it to be adequately identified and reused, while preserving the developer's original data. Recently, the RKME (Reduced Kernel Mean Embedding) specification was proposed and most commonly utilized. This paper provides a theoretical analysis of RKME specification about its preservation ability for developer's training data. By modeling it as a geometric problem on manifolds and utilizing tools from geometric analysis, we prove that the RKME specification is able to disclose none of the developer's original data and possesses robust defense against common inference attacks, while preserving sufficient information for effective learnware identification.

## 1 Introduction

Various machine learning models have been applied into various aspects of modern life successfully . In conventional machine learning paradigm, developing a high-quality model for a new task from scratch requires a substantial amount of labeled data, expertise, and computational resources, while it is ideal if the solution of the new task can be built based on reusing existing models. Generally, source data is crucial for transferring and reusing existing models, however, concerns over data privacy and proprietary often hinder the sharing among developers.

The Learnware paradigm  offers a systematical way enabling users to build a new machine learning solution by exploiting existing well-trained models, rather than building a model from scratch. A learnware is a well-trained machine learning model facilitated with a _specification_, which characterizes the ability and specialty of the model to some degree, enabling the model to be adequately reused for new users without access to the original data used to train the model by its developer. Developers all over the world can submit their trained models into a _learnware dock system_ spontaneously, and the system helps developers generate specifications, without access to the developer's training data. When a user wants to tackle her learning task, instead of starting from scratch, she can submit her requirement to the learnware dock system, which will then identify and return helpful learnware(s) to the user to reuse, such that the user can get a better performance than using her own data to train a model from scratch. Note that developers generally need to preserve their training data. To realize this attractive vision, the key challenge is: To solve new tasks, how to identify and even reassemble a few helpful models from the huge amount of learnwares accommodated in the dock system, without accessing each developer's training data?The specification is pivotal in the paradigm design (Zhou and Tan, 2024). Recently, based on the RKME (reduced kernel mean embedding) specification (Zhou and Tan, 2024), many studies about learnware identification and reuse have been reported (Liu et al., 2024; Tan et al., 2023; Xie et al., 2023; Tan et al., 2024; Wu et al., 2023). Also, the _Beimingwu_ learnware dock system has been developed and released (Tan et al., 2024).

It is worth noting that, there lacks a theoretical analysis of the preservation ability of RKME specification for developer's training data. The theoretical analysis faces some significant challenges. Firstly, the RKME specification is generated by solving a non-convex optimization problem, whose possible solutions adhere to a nonlinear system of equations and, inherently, do not have a closed-form solution. This complexity makes direct analysis intractable. Moreover, the generation process of RKME is deterministic, but the size of RKME is small, where adding noise during this process often degrades its performance for learnware identification. Consequently, commonly used analytical tools for data privacy, such as Differential Privacy (DP), become inapplicable. To the best of our knowledge, there is little research analyzing whether and how synthetic data generated by deterministic algorithms can protect original data from being vulnerable to privacy attacks, especially considering that brute-force attacks often succeed against deterministic algorithms. In this paper, we prove that the RKME specification is able to protect the developer's original data from disclosure, and possess robust defense against common inference attacks, while preserving sufficient information for learnware identification. Our technique also provides a way to investigate the privacy preservation ability of synthetic data generated by deterministic algorithms. The main contributions of this work are summarized as follows:

* We prove that as the size of RKME specification decreases, the possibility that it discloses the original training data decreases at an exponential rate. Meanwhile, the ability of learnware identification is positively correlated with the size of RKME specification, and thus we also need a sufficient size. We prove that within a certain range of sizes, the RKME specification won't disclose any original training data for almost all datasets, while it remains sufficient information enabling effective learnware identification.
* We prove that RKME exhibits significant resistance to the two common types of data disclosure attacks: linkage and inference. We provide a method to measure data leakage of RKME and illustrate that the risk of RKME specification data leakage diminishes as the size of specification decreases. Within a certain range of sizes, the RKME can effectively resist these two types of attacks, while maintaining sufficient information for learnware identification. Moreover, we analyze that the above two ranges of sizes are heavily overlapped, implying that determining adequate sizes that enable learnware identification but preserving developer's data is practical.

## 2 Preliminary

In this paper, for the theoretical analysis of the preservation ability of RKME specification simplicity, we consider the following simplified learnware paradigm based on RKME specification. The learnware paradigm consists of two stages: submitting and deploying stages.

**Submitting stage.** In this stage, model developers submit their models to the learnware dock system. To better characterize these models, developers also provide the specification \(R\) with each model to the system, which is generated by the RKME mechanism from the dataset \(D\) used to train the model. A model together with its corresponding specification forms a learnware.

**Deploying stage.** In this stage, the user has a learning task and an unlabeled dataset \(D_{u}\). To tackle the task, the user submits the task requirement \(R_{u}\), which is generated by the RKME mechanism from the dataset \(D_{u}\), and the system identifies helpful learnwares by comparing which learnware specifications are "close" to \(R_{u}\). Subsequently, the user can solve her task by reusing these learnwares.

It is evident that the design of specifications is of great importance. To make the learnware identification (also called the search process) more precise, we hope the specification should contain some information about the data of the developers (or users). On the other hand, since developers face the challenge of uploading their specifications, it's crucial that the specification should protect the original data of the developer. The RKME generation process can be described as follows:\[F(x_{1},,x_{n};_{1},,_{m};z_{1},,z_{m})=\| {n}_{i=1}^{n}k(x_{i},)-_{j=1}^{m}_{j}k(z_{j},)\|_{ }.\] (1)

Here, \(\{x_{1},,x_{n}\}\) is the original dataset \(D\) of the developer sampling from a certain distribution \(\), with each data point \(x_{i}=(x_{i}^{1},,x_{i}^{d})\) being \(d\)-dimensional. The RKME specification generated from \(D\) is \(R=\{_{1},,_{m};z_{1},,z_{m}\}\) that minimizes Eq. (1) given the number of synthetic data \(m\). We will focus on the set \(Z=\{z_{1},,z_{m}\}\), which consists of synthetic data sharing the same feature space with \(x_{i}\). The \(_{i}\) are weights corresponding to each \(z_{i}\). Here \(k(,)\) is a kernel function, and \(\|\|_{}\) is the norm in the reproducing kernel Hilbert space induced by \(k(,)\). In this paper, we conduct our proofs using the Gaussian kernel \(k(x,x^{})=(-\|x-x^{}\|_{2}^{2})\), where \(>0\), which is currently employed for generating RKME specifications. The generalizability of our proofs to other kernels will be discussed in the section 5.

The ability of RKME to search models can be characterized by its approximation of the Kernel Mean Embedding (KME) (Smola et al., 2007) of the original data distribution, as this reflects how well RKME characterizes the original data distribution. From Zhang et al. (2021), we arrive at the following conclusion:

**Lemma 2.1**.: _Let the kernel \(k\) satisfies that \(k(,) 1\) for all \( X\) and any \(>0\), we have_

\[\|-\|_{} 2}+2}+},\] (2)

_with probability at least \(1-\), where \(\) is RKME of \(D\) and \(\) is KME of original data distribution \(\)._

Existing experimental results have demonstrated that selecting \(m=\) allows RKME to effectively search models, achieving success (Tan et al., 2023; Liu et al., 2024). However, analysis of how RKME protects the data of both developers and users from a theoretical perspective is still lacking. Since the synthetic data \(Z\) shares the same feature space with \(X\), it is essential to investigate whether RKME contains any original data. Furthermore, whether the original data can be inferred from RKME through specific attack methods remains unknown. The following sections of this paper will explore these two perspectives to prove RKME's data preservation ability. All proofs are provided in the Appendix C.

## 3 Specification contains no original data

To analyze the RKME specification's ability to preserve original data, we first need to determine whether the specification contains original data. In this section, our objective is to quantify the consistency of synthetic data in RKME with the original data, as with most studies considering the privacy protection of synthetic data (Raghunathan, 2021; Abowd and Vilhuber, 2008). Since RKME is generated through a deterministic algorithm, the above issue becomes more pressing than in randomized algorithms, where the output uncertainty can mitigate such concerns.

Quantifying approach.To quantify whether \(\{z_{1},,z_{m}\}\) in RKME contains any data from the original dataset \(\{x_{1},,x_{n}\}\), we analyze whether there exist \(i\{1,,m\}\) and \(j\{1,,n\}\) such that \(z_{i}=x_{j}\), or more generally, \(\|z_{i}-x_{j}\|\), which means that there are samples in RKME that are very close to some original data. We propose the following quantitative metric:

**Definition 3.1** (Consistency risk).: _We define the consistency risk of the RKME \(Z\), generated from \(n\) samples \(D\) drawn from the distribution \(\), containing original true data as:_

\[R_{C}()_{D^{n}}( _{Z_{} D_{}}),\]

_where \(\) is the indicator function, and \(Z_{} D_{}\) indicates that, given \(\), there exists \(i\{1,,m\}\) and \(j\{1,,n\}\) such that \(\|z_{i}-x_{j}\|\)._

As can be seen, the defined risk function \(R\) represents the probability that the RKME generated from \(n\) samples drawn from a potential distribution \(\) may contain one of these \(n\) original samples. The randomness here arises from the randomness of the sample set. It is evident that the value of \(R_{C}()\) ranges between \(\), with smaller values indicating a lower risk of RKME containing original samples. In the following, we will analyze the consistency risk for RKME specification.

Technical overview.To analyze the defined risk \(R_{C}()\), the most natural approach would be to find a closed-form solution for \(Z\) in relation to \(D\), which would allow for a direct comparison of the elements of \(Z\) and \(D\). Unfortunately, solving the theoretical minimum of Eq. (1) is a nonlinear equation that does not have a closed-form solution. Therefore, this paper employs geometric analysis tools (Jost, 2008, Li, 2012), and analyzes the critical set of Eq. (1) to quantify the differences between the data in the synthetic set and the original sample set without solving the equation. To the best of our knowledge, we provide the first analysis based on geometric analysis tools to quantify privacy risk, which sheds light on data preservation analysis.

### Consistent risk evaluation

We start our analysis with the case of the data dimension \(d=1\), and as we will see, the case for any dimension \(d\) can be reduced to the analysis of this scenario \(d=1\). To facilitate the use of geometric tools, we consider all data in the original dataset \(D\) and its corresponding RKME \(Z\) as wholes, namely as points in spaces \(^{n}\) and \(^{m}\), respectively. Let us denote them by \(=(x_{1},,x_{n})\) in \(^{n}\) and \(=(z_{1},,z_{m})\) in \(^{m}\). The problem then translates into whether these two spaces have points with identical coordinate components. The upper bound of \(R_{C}()\) is ensured based on the fact that in the space \(^{n}\) where \(\) resides, the set of \(\) corresponding to \(\) in \(^{m}\) with coordinate components equal to \(\) is of small measure. In the remainder of the section, we will prove this.

We begin our analysis with the case of \(=0\), focusing solely on whether any component of \(\) is strictly identical to \(\). Our starting point is the correspondence between \(\) and \(\). From this, we derive the following proposition:

**Proposition 3.2**.: _The set of \(\) in \(^{n}\) that satisfies the condition of having multiple distinct \(\) which minimize Eq. (1) constitutes a set of measure zero._

Proposition 3.2 allows us to consider the remaining \(\) in \(^{n}\) after removing a set of zero measures. These \(\) correspond uniquely to a minimum value \(\). If we fix \(\), then, based on a similar idea as in the Implicit Function Theorem, we arrive at the following conclusion.

**Proposition 3.3**.: _Given \(\) and \(\{_{i}\}_{i=1}^{n}\), consider the set \(_{Z}\) defined as follows:_

\[_{}=(y_{1},,y_{n})^{n}\, \,F(y_{1},,y_{n};) F(x_{1},,x_{n}; ),(x_{1},,x_{n})^{n}}.\]

_This set forms a manifold of dimension \(n-2m\). The subset \(^{}_{Z}\), defined as_

\[^{}_{}=\{(y_{1},,y_{n})_{ }\,|\, i,j,z_{i}=y_{j}\},\]

_constitutes a submanifold of \(_{}\) with a dimension not exceeding \(n-2m-1\)._

According to Proposition 3.2, we know that disregarding a set of zero measures, the \(_{}\) corresponding to different \(\) are disjoint. In each \(_{}\), the \(\) with components identical to \(\) form a submanifold \(^{}_{}\) of dimension not higher than \(n-2m-1\). Consequently, we obtain a representation for all possible points in \(^{n}\) where \(\) intersects with the generated \(\): they constitute a subset \(=_{}^{}_{}\).

Given that the dimension of \(\) is \(m\), the Hausdorff dimension of \(\)(actually \(\) is a manifold) is not greater than \(n-1\), which makes it a zero measurement set in \(^{n}\). Thus, we conclude that for \(\), which could possibly have the same coordinates as the generated \(\), is of measure zero in \(^{n}\). From this observation, together with Proposition 3.2, we have the following theorem:

**Theorem 3.4**.: _For any continuous original data distribution \(\), when the size of the RKME set satisfies \(m<\), we have that the consistency risk \(R_{C}()=0\)._

If \(\) is a discrete distribution, then the above inference may not hold if the discrete values fall on \(\). If \(\) has very few possible values, \(\) will contain points from the original \(\) when the number of points in \(\) is large (due to the presence of many identical samples in \(\)). In such cases, we find that by limiting \(m\) to fewer than the possible values of \(\), we can still prove \(R_{C}()=0\) using some combinatorial techniques. Similar conclusions hold for mixed-type distributions.

In practical applications, it is desirable to ensure that RKME does not contain samples identical to those in the original dataset \(D\). Therefore, we further explore whether RKME may include samples that are very close to those in the original data \(D\), i.e., \(>0\). A crucial aspect of addressing this issue involves the setting of \(\), which is significantly influenced by the inherent scale of the data and the spacing between the data points. We will adopt a commonly used approach, selecting \(\) as the normalized value by the minimum spacing between different samples in the dataset.

Our idea of handling this scenario is fundamentally similar to that of \(=0\). Given \(\), we similarly define \(_{}\), and \(_{}^{}\), is now defined as \(_{}^{}=\{(y_{1},,y_{n})\,|\,  i,j,|z_{i}-y_{j}|\}\). In this case, \(_{}^{}\) forms a measurable subset of \(_{}\) with a dimension of \(n-2m\). Based on the selection of \(\) as previously described, our objective is to estimate the ratio of the areas of \(_{}^{}\) and \(_{}\). To achieve this, we estimate the local curvature, perform local linearization, and use ideas similar to isoperimetric inequalities for the estimation. We arrive at the following conclusion.

**Theorem 3.5** (Bound of consistency risk).: _For any continuous original data distribution \(\), for RKME with \(m\) synthetic data, we have_

\[R_{C}()<(()^{n-2m}).\] (3)

RemarkWe believe that the privacy protection afforded by RKME results from the many-to-one correspondence between the original sample set and the generated RKME. This is essentially a loss of individual member information caused by compression. However, not all synthetic data generation processes like this can achieve similar ideal effects. We have proved that if we choose the Laplacian kernel (\(k(x,y)=(-\|x-y\|_{1})\)) instead of the Gaussian kernel in Appendix C.8, Theorem 3.4 would no longer hold. In fact, with the Laplacian kernel, we can prove that the synthetic data induced by the corresponding RKME will definitely contain data identical to the original sample set. This underscores the rationality of choosing the Gaussian kernel RKME as our specification.

For cases where the data dimension of \(>1\), we can similarly define \(=\{(\|x_{1}\|,,\|x_{n}\|)^{n}\}\) and \(=\{(\|z_{1}\|,,\|z_{m}\|)^{m}\}\). Using the same approach, and based on Thm. 3.5 and the inequality \(\|x-y\|\|x\|-\|y\|\), we can derive conclusions for the \(d\)-dimensional case. The only difference is a change in order, as given by the formula: \(R_{C}()<(()^{dn-2dm-m})\).

### Data preservation and search ability

As indicated in Thm. 3.5, we observe that the consistency risk decreases as the size of the RKME specification decreases, which means that selecting a smaller number of synthetic data points \(m\) can better ensure that RKME does not contain samples closely resembling the original data. If we represent the data protection capability of RKME in terms of not containing original data using \(1-R_{C}()\), and the ability of RKME for search derived in Lemma 2.1, we can illustrate the resulting trade-off, as shown in the following graph.

In this Pareto frontier, it is challenging to define the exact point of Pareto optimality. This difficulty arises from the differing scales of measuring data protection capabilities and the gap in RKME's approximation of the actual data distribution KME, which represents search efficiency. It is hard to set a rule to find the Pareto optimum due to these distinct measurement scales. However, fortunately, in existing works, we have observed that when the size of the RKME specification \(m\) is larger than \(\), the specification achieves satisfactory results in the Lemaurre's search tasks. In this paper, we propose the following corollary:

**Corollary 3.6**.: _If we choose \(m k\), where \(k d\), for our defined \(R_{C}()\), we obtain the following equation:_

\[R_{C}()<0.001.\] (4)

_This implies that we are 99.9% confident that for any dataset \(D\) sampled from the distribution \(\), the generated RKME will contain no synthetic data points that are close to the original samples in \(D\)._

The conclusion above offers a flexible approach to selecting the number of synthetic data points \(m\) in RKME. As illustrated in Fig. 3.2, the shaded area encompasses the range that allows RKME to

Figure 1: Trade-off between data consistency preservation and search ability.

achieve efficient search capabilities and robust data protection, specifically in terms of not containing data closely similar to the original dataset.

## 4 Specification defends data inference

Whether a learnware's specification contains original data is not the only concern for its data protection ability. Stadler et al. (2022) note that synthetic data may not withstand traditional data attacks such as _linkage_(Elliot et al., 2018; Sweeney, 2002), and _attribute disclosure_(Elliot et al., 2018; Machanavajjhala et al., 2007). Common defenses against these attacks usually involve formal privacy guarantees during the training process of the generative model to prevent breaches(Abowd and Vilhuber, 2008; Bindschaedler et al., 2017), or the addition of noise to the synthetic data generation process to satisfy differential privacy criteria (Xin et al., 2022; Jordon et al., 2018). However, there is still a lack of research on whether synthetic data generated by a deterministic mechanism can naturally resist these attacks without extra noise.

RKME, in addition to potentially containing explicit original data, may also implicitly contain certain information about the data, which could be inferred under these two types of attacks. We aim to prove that RKME can protect against such inferences from the original data.

Quantifying approach.We suggest that the RKME mechanism, as a deterministic generation mechanism, serves as a _data anonymization solution_(Kuppa et al., 2021). To verify whether the generation of the specification acts as an effective anonymization mechanism, our objective is to quantify whether such a specification can address the data leakage risks that data anonymization techniques are designed to mitigate. We quantified the data leakage risks associated with two types of concerns: _linkability_ and _inference ability_, which correspond to the defenses against the linkage attack and the disclosure of attributes, respectively. For the risks of linkability and inference, we model each privacy concern as an adversary tasked with determining whether, given the specification or the original dataset, information about a component \(x_{t}\) of a sample \(x\) in \(X\), or its attributes, can be inferred. For each adversary, we define a data leakage risk to measure the risk of the adversary inferring the sample \(x\) or its attributes from \(X\) after the RKME \(Z\) of dataset \(D\) has been released.

### Linkability risk evaluation

In considering data sharing with privacy protection characteristics, a primary concern is the risk of linkability, which corresponds to an adversary conducting a linkage attack on the data. A linkage attack aims to link a target record to a single record or a group of records in a sensitive dataset. Specifically, we model the linkage attack as an adversary attempting to demonstrate whether a record is present in a sensitive dataset.

Quantifying approach.We suggest that the RKME generation mechanism, as a deterministic generation mechanism, serves as a _data anonymization solution_(Kuppa et al., 2021). To verify whether the generation of the specification acts as an effective anonymization mechanism, we aim to quantify whether such a specification can address the data leakage risks that data anonymization techniques were designed to mitigate. We quantified the data leakage risks associated with the two types of leakage concerns: linkability and inference. These two are universally important de-anonymization techniques. Additionally, the synthetic data generated by RKME and the original data exist within the same feature space, making linkage attacks a potentially successful form of attack within the learnware paradigm, and thus a focal point of our analysis. Moreover, an attacker attempting to compromise the original data through RKME can only access a specific RKME, rendering many attacks ineffective in the learnware paradigm (such as requiring multiple queries).

Linkability privacy game.Following the works of (Pyrgelis et al., 2017; Yeom et al., 2018; Stadler et al., 2022), we model the risk of linkability as a membership privacy game between an adversary \(\) and a challenger \(\). In the learnware paradigm, learnware developers, who are also holders of the original data \(D\), are considered as the challengers in the membership privacy game, where \(Z\) generated from \(D\) is visible to the adversary \(\). The objective of the adversary \(\) is to infer whether a target record \(x_{t}\), chosen by the adversary, is present in the original dataset \(D\), based solely on the knowledge of \(Z\) and some prior knowledge \(\).

The game 4.1 models a privacy game to assess the potential linkage attack on RKME synthetic data. Initially, \(\) selects a target record \(x_{t}\) and sends it to \(\). Then \(\) independently and identically draws a dataset \(D\) of size \(n-1\) from the data distribution \(\), and chooses a secret bit \(s_{t}\{0,1\}\). If \(s_{t}=0\), \(\) samples another record \(x^{*}\) from the distribution, excluding the target record, and adds it to \(D\) (simulating the scenario where the original dataset does not contain the target). If \(s_{t}=1\), \(\) adds the target record \(x_{t}\) to \(D\) (simulating the scenario where the original dataset contains the target). Subsequently, \(\) generates the corresponding RKME \(Z\) using the RKME generation mechanism from \(D\), and randomly sends either the original dataset \(D\) or its corresponding RKME \(Z\) to the adversary. Upon receiving the dataset, the adversary \(\) guesses whether the target \(x_{t}\) is in the original dataset \(D\) through \(_{t}^{}(D,b,x^{t},)\). If \(_{t}=s_{t}\), it is considered that the adversary has successfully carried out a linkage attack.

**Definition 4.1** (Linkage risk).: _We define the linkage risk of dataset \(X\) during the linkage privacy game as_

\[R_{L}(X)_{x_{t}^{n}}(2 [^{}(X,b,x^{t},)=s_{t} ]-1)\]

_where \(X=D\) or \(Z\), and the probability space is composed of the randomness of the target, the randomness of the secret bit, and the adversary's method of guessing._

Analyzing the linkage risk as defined above hinges on a critical examination of \(^{}(X,b,x^{t},)\). This expression reflects the potential strategies that an adversary \(\) might employ to guess whether a target record exists in the original dataset \(D\). The risk of linkage attacks on RKME can vary depending on the strategy employed. Since the RKME generation mechanism is a deterministic algorithm, the most formidable attack strategy an adversary could deploy can be _brute-force attack_.

Adversarial strategy.When the adversary receives the information \(X\) and \(b\) provided by the challenger, the approach varies according to the value of \(b\). If \(b=0\), the adversary has the original dataset \(D\) and merely needs to check if the target \(x_{t}\) is in \(D\). When \(b=1\), the adversary receives the RKME \(Z\) generated from \(D\) and knows the data's prior distribution \(\). The adversary then employs a brute-force attack to construct all possible datasets \(D^{}\), collectively denoted as \(M\), each of which corresponds to RKME \(Z\). The probability of these \(D^{}\) being the actual dataset under \(\) varies, and the adversary can calculate the measure \(dP(D|)\) for each \(D^{}\) in \(M\). They can assign a weight \()}{_{M}dP(D|)}\) to each \(D^{}\) based on this measure, and then randomly select one \(D^{}\) as the inferred true dataset using this weight. The adversary checks if the target \(x_{t}\) is in this \(D^{}\) and makes a guess.

Risk evaluation.Similarly to the method used for analyzing the consistency risk in the previous section, for a sample set \(D=\{x_{1},,x_{n}\}\) and its corresponding RKME \(Z=\{z_{1},,z_{m}\}\), we map them to points in \(^{n}\) and \(^{m}\) respectively, as \(=\{(\|x_{1}\|,,\|x_{n}\|)\}\) and \(=\{(\|z_{1}\|,,\|z_{n}\|)\}\). Similarly, we find that the \(\) formed by \(\) constitutes a manifold.

For the linkage risk term \(2[^{}(X,b,x^{t},) =s_{t}]-1\), it can be interpreted as the difference between the adversary \(\)'s true positive rate and false positive rate, expressed as

\[[_{t}=1 s_{t}=1]-[_{t}= 1 s_{t}=0]\]The first term \([_{t}=1 s_{t}=1]\) corresponds to the probability of randomly selecting a \(^{}\) weighted on the manifold \(\), where \(^{}\) contains coordinate components identical to \(\). Expanding this term, we have the following.

\[[_{t}=1 s_{t}=1]=_{})}{_{}dP(D|)}(_{Z D })\]

Similarly to the proof of Thm. 3.5, we can bound the first term. For the second term \([_{t}=1 s_{t}=0]\), we need to estimate the points in \(\) that might generate \(\) but do not contain the target record \(x_{t}\). We can provide an upper bound using the isoperimetric inequality. The deductions made above can be summarized in the following theorem.

**Theorem 4.2** (Bound of linkage risk).: _When the adversary employs a brute-force attack, the linkage risk is bounded as follows:_

\[R_{L}(Z)<O().\] (5)

RemarkOur assessment of the risk of linkability is based on a worst-case evaluation, which is applicable to any target. This approach differs from many previous studies that have focused more on the average-case scenario. However, previous studies have shown that privacy risks associated with data sharing are not uniformly distributed throughout the population (Kulynych et al., 2022; Long et al., 2020; Rocher et al., 2019). Our analysis of the worst-case scenario aligns more closely with practical needs, as we aim to ensure sufficient privacy protection for each individual data point.

### Inference risk evaluation

The risk of linkability is not the only concern about data leakage in data sharing processes. Data anonymization mechanisms also protect individuals in the original data from attribute disclosure, which corresponds to an inference attack. The risk of inference describes the concern that an adversary might deduce the value of an attribute from the other attributes (El Emam and Alvarez, 2015).

As illustrated in the Game 4.2, this approach differs from the previous linkability privacy game in that the adversary only has access to a subset of the attributes of the target record, \(x^{1},,x^{s-1}\), and attempts to infer an unknown sensitive attribute value \(x^{s}\). At the start of the game, the adversary randomly selects a target record from \(\), which is a collection of samples from \(X\) with their sensitive attributes removed. Upon receiving partial information of this target record, the challenger assigns it a secret value \(x^{s}(^{t})\), where \(\) denotes the projection of a partial record from \(\) into the domain of the sensitive attribute based on its distribution. The challenger then merges the partial attributes provided by the adversary with the secret value assigned to form a complete sample in \(X\), following which the same privacy game as in the linkability case is played. The adversary's final information comprises the dataset \(X\) and a public bit \(b\). Using this information, the adversary makes a guess about the target's sensitive attribute value \(}\). If the guess falls within our acceptable tolerance range, the adversary is considered to have won.

**Definition 4.3**.: _We define the inference risk in the Inference Privacy Game as_

\[R_{I}(X)_{x^{s}}[^{s }=x^{s}|\,s_{t}=1]-[^{s}=x^{s}|\,s_{t}=0|]\]

Adversarial strategy.To estimate \(R_{I}\), we need to consider the strategy of guessing of the adversary. The adversary makes an estimate of the sensitive attribute value \(x^{s}\) in \(x_{t}\) based on the RKME \(Z\) released by the Challenger, the public bit \(b\), and the known partial attribute values \(^{t}\). Let us first consider the case where \(b=0\), where the Challenger releases the original dataset. In this scenario, the adversary can deduce the missing attribute value through record linkage (Drechsler and Reiter, 2010; Machanavajjhala et al., 2007; Reiter and Mitra, 2009). If the adversary can link the partial information of the target with a unique sample in the original dataset (that is, there is only one sample whose partial information matches that of the target), then the missing value can be successfully reconstructed. In this case, \(P[^{}(X,b,x^{t},)=x^{s}|\,s _{t}=1|=1\).

When direct inference using linkage fails, the challenger must seek alternative methods to conjecture the target record. Similarly, due to the deterministic mechanism of RKME, we still analyze the brute-force attack. Specifically, like the previous linkage, the adversary first finds all possible sample sets that correspond to RKME \(Z\) using a brute force attack. Among these sets, there may be some where a subset of attributes of certain samples matches the target record. The adversary then selects the original sample set from these subsets with partially matching information, using prior probabilities similar to the brute-force attack for linkage. Through an analysis similar to our previous approach, we have the following theorem.

**Theorem 4.4** (Bound of inference risk).: _When the adversary employs a brute-force attack, the inference risk is bounded as follows_

\[R_{I}(Z)<O((n-1)!})\] (6)

### Data preservation and search ability

Analogously to the analysis of the consistency risk, we hope that RKME can withstand linkage and inference attacks while still providing effective search capabilities. Regarding search ability, we continue to use the characterization of how the RKME generated from dataset \(D\) approximates its original distribution \(\) with varying numbers of synthetic data, as described in Lemma 2.1. We represent the protective capacity of RKME for dataset \(D\) against the two types of attacks using \(R_{L}(D)-R_{L}(Z)\) and \(R_{I}(D)-R_{I}(Z)\), respectively. These represent the reduction in linkage and inference risks when publishing RKME instead of the original data. Based on Thm. 4.2 and Thm. 4.4, we propose the following corollary:

**Corollary 4.5**.: _If we choose \(m\), for \(R_{L}(D)-R_{L}(Z)\) and \(R_{I}(D)-R_{I}(Z)\), we obtain the following equation:_

\[R_{L}(D)-R_{L}(Z)  0.999\] (7) \[R_{I}(D)-R_{I}(Z)  0.999\] (8)

_This implies that we have 99.9% confidence that RKME can defend the linkage and inference attack._

To ensure that RKME maintains a risk level below our tolerance threshold (i.e., 0.001%) for consistency risk, as well as for linkage and inference risks, and still achieves satisfactory search efficiency, it suffices to select the number of points within the range \((,(k,))\). This range offers flexibility in adjusting the number of points \(m\). When greater precision in search is required, we can opt for a larger value of \(m\) within this interval. In contrast, when a higher degree of data protection is desired, a smaller value of \(m\) can be chosen within the same range.

Figure 2: Trade-off between the ability of data linkage (inference) protection and search ability.

Discussion

In our work, we have provided proofs only for the Gaussian kernel; however, our method can be extended to analyze a broad class of kernels and yield similar conclusions regarding their data protection capabilities. For kernels that exhibit non-rationality and analyticity (such as the Sigmoid kernel \(K(x,y)=( x^{T}y+r)\) and the Cauchy kernel \(K(x,y)=}\)), they can be treated similarly to the Gaussian kernel by considering samples as bundles of synthetic data within the sample space. The difference lies in that, due to the specific forms of these kernels, the risk estimates we calculate during our analysis will vary, and the derivative estimates in the proof process may require re-evaluation. Although each kernel will still require its own specific analysis, the approach provided in this paper is generally applicable. Extending this framework to prove more robust results for a broader class of kernels will be part of our future work.

The optimization problems involving Gaussian kernels are non-convex and non-rational, making theoretical analysis intractable under traditional tools. Analyses of optimization problems using the Gaussian kernel often rely on numerical experiments for validation. For the first time, we have made it possible to analyze the optimal solution using geometric analysis techniques to analyze the data protection capability of the RKME specification. This approach not only applies to the privacy analysis of RKME as a specific form of synthetic data, but it also provides insights for analyzing similar nonlinear non-convex optimization problems involving Gaussian kernels. Applying this technique to broader contexts will be part of our future work. The main limitation of this analytical method is that the upper bound may not be optimal due to multiple steps of restrictions and relaxations. However, pursuing a tighter upper bound remains theoretically significant.

In terms of relevant privacy theories, differential privacy (Dwork, 2006) (DP) is the most widely used technique, with its privacy protection characteristics derived primarily from the randomness introduced by additional noise. However, for the RKME specification in our study, due to its size, the well-known privacy-utility trade-off in differential privacy (Alvim et al., 2012) particularly pronounced after we add noise to the RKME mechanism, which can significantly degrade performance in learnware identification. This means that applying existing DP techniques to analyze privacy in RKME is quite challenging. On the other hand, due to the extensive data compression inherent in the RKME generation process, we believe that it possesses the data protection capabilities necessary within the learnware paradigm. Therefore, this paper also offers a perspective on how a compressive deterministic algorithm can achieve privacy protection without relying on DP methods.

Another important perspective for future work is to establish sufficient criteria for when specifications in learnware can provide strong protection for the developer's original data. This paper evaluates the risks associated with RKME containing original data, consistency risk, and the exposure risks under two common types of attacks: linkage risk and inference risk. We prove the necessity of protecting original data through the RKME specification induced by the Gaussian kernel. Due to the deterministic nature of the RKME generation mechanism, some attacks relying on randomness, such as multiple queries, are ineffective against RKME, and the evaluation criteria presented in this paper are broadly applicable. However, seeking more general evaluation standards and investigating what types of specifications can effectively protect the developer's original data under these criteria will be a key focus of our future work.

## 6 Concluding remarks

This paper presents a theoretical study about the ability of developer's data preservation of the RKME specification, which was recently proposed for building learnware specification (Zhou and Tan, 2024) and used in the Beimingwu learware dock system (Tan et al., 2024). By leveraging geometric analysis techniques, we prove that as the size of RKME specification decreases, the ability of the developer to preserve data increases, that is, the possibility of exposing the developer's original data decreases, and the ability to defend against the two commonly encountered attacks, i.e., linkage attack and inference attack, increases. Moreover, there exists a broad range of specification sizes that endow the above properties and enable effective learnware identification. Note that this work also offers a new perspective on the data preservation ability of reduced sets and the corresponding analysis of deterministic algorithms.