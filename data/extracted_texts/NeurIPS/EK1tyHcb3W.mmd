# Sample Complexity of Posted Pricing for a Single Item

Billy Jin

Cornell University, School of Operations Research and Information Engineering, bzj3@cornell.edu.

Thomas Kesselheim

University of Bonn, Institute of Computer Science and Lamar Institute for Machine Learning and Artificial Intelligence, thomas.kesselheim@uni-bonn.de

Will Ma

Columbia University, Graduate School of Business and Data Science Institute, wm2428@gsb.columbia.edu.

Sahil Singla

Georgia Tech, School of Computer Science, ssingla@gatech.edu. Supported in part by NSF award CCF-2327010.

, as long as the posted prices are set correctly. This raises the main question of the current paper:

_How many samples are necessary from the value distributions of the buyers to find near-optimal posted prices for single item? Is there a difference between independent vs. correlated distributions or between welfare vs. revenue maximization?_

Despite being a natural question, the tight sample complexity bounds for single item posted pricing are unknown, both for independent/correlated and welfare/revenue settings.

### Model

Before stating our results, we formally describe our model.

**Posted pricing.** There is single copy of an item. Buyers \(i=1,,n\) arrive in order, each with a private value (willingness to pay) \(V_{i}\) for that item. Price \(_{i}\) is offered to buyer \(i\), and the first buyer (if any) for whom \(V_{i}_{i}\) end up buying the item, "winning" the auction. The objective is to maximize either _welfare_, which is the value of the item to the winner, or _revenue_, which is the price paid by the winner.

**Distributions and policies.** We will assume that the vector of values \(=(V_{1},,V_{n})\) is drawn from an unknown but fixed distribution \(\) over \(^{n}\). If each \(V_{i}\) is drawn _independently_ from some marginal distribution \(D_{i}\), then \(\) is called a _product distribution_, written as \(=D_{1} D_{n}\). Meanwhile, _posted pricing policies_ are defined by a vector of prices \(=(_{1},,_{n})\) that must be fixed before the first buyer arrives, and possibly restricted to some subclass \(\). The objective of policy \(\) under buyer values \(\) is denoted by \(()\), which equals \(V_{*{argmin}\{i:_{i} V_{i}\}}\) and \(_{*{argmin}\{i:_{i} V_{i}\}}\) for the welfare and revenue objectives respectively, understood to be \(0\) if the item is not sold. We (abusively) let \(():=_{}[()]\) denote the expected objective of \(\), and let \(():=_{}()\) denote the best expected objective of a policy in \(\) knowing \(\).

Note that if we are given the product distribution on buyer values, the optimal policy (for either objective) can be easily computed using dynamic programming . Consequently, for product distributions we consider the full policy class \(=^{n}\), and omit the dependence on \(\).

**Learning problem.** A _learning algorithm_ takes as input samples \(\) that are drawn independently and identically distributed (IID) from the unknown distribution \(\) and an error parameter \((0,1)\), and seeks to return a policy \(\) that satisfies \(()()-\), which is called an _\(\)-approximation_. Given a failure probability \((0,1)\), the _sample complexity_ is the minimum number of samples required for there to exist a learning algorithm that, under any distribution \(\), returns an \(\)-additive approximation with probability at least \(1-\), noting that the randomness is over both the samples and any random bits in the algorithm. The sample complexity and learning algorithm will depend on the problem variant, defined by the objective (welfare or revenue), any parameters of the policy class \(\), and whether \(\) is restricted to be a product distribution or not. The sample complexity will also depend on the parameters \(,>0\).

### Our Contributions to Posted Pricing for Product Distributions

The study of sample complexity for posted pricing goes back to at least the seminal work of Kleinberg and Leighton  who study revenue maximization for selling a single item to a single buyer (welfare maximization is trivial for single buyer since we just allocate the item by setting \(0\) price). For the general posted pricing problem with \(n\) buyers, the best known sample complexity bounds were due to Guo et al. , who showed that \((n/^{2})\) samples suffice from each buyer's distribution for both welfare and revenue maximization objectives. Although there is a simple \((1/^{2})\) lower bound for both welfare and revenue maximization settings, prior to our work it was unclear if polynomial dependency on the number of buyers \(n\) is necessary.

Our first result for product distribution shows that for welfare maximization using posted pricing (a.k.a. prophet inequality), the sample complexity is independent of the number of buyers \(n\).

**Theorem 1** (proved in Subsection 2.1).: _For product distributions, the sample complexity of welfare maximization is \(O(1/^{2}^{2}(1/))\)._

**Proof sketch.** We start by constructing the product empirical distribution using the samples, where for each \(i\) we take the uniform distribution on the \((1/^{2})\) samples and then take the product distribution for different \(i\). Our main result is that the optimal policy (dynamic programming solution) on this product empirical distribution is an \(\)-approximation with high probability. Although one could use standard concentration bounds to bound the gap between the learned and optimal thresholds, such arguments lose a \((n)\) factor. This is because the difference between successive thresholds is bounded by \(1\), so naively the total variance of \(n\) thresholds could be \((n)\). Our main idea is to instead study a martingale that adds up the errors made in the dynamic programming solution. This way, too high values for one buyer and too low values for another buyer balance each other. On this martingale, we use Freedman's inequality (which is a martingale variant of Bernstein's inequality), which allows us to bound the total variance in terms of conditional variances. A careful application of another concentration bound allows us to bound these conditional variances in terms of the change in the optimal value, whose sum is always at most \(1\). This latter concentration bound is what causes the additional factor of \((1/)\) in the sample complexity.

Our second result for product distribution shows a separation between welfare and revenue maximization using posted pricing, by proving that the \((n/^{2})\) sample-complexity result of  for revenue maximization is tight up to log factors.

**Theorem 2** (proved in Subsection 2.2).: _For revenue maximization on product distributions, any learning algorithm requires \((})\) samples to return an \(\)-additive approximation with probability greater than 6/7._

**Proof sketch.** We construct for each buyer \(i\) two possible value distributions that add the same amount (roughly \(\)) to the value-to-go of the optimal dynamic program. However, these distributions have different optimal prices, and making a mistake (choosing an incorrect price) in isolation loses roughly \(\) value. Although these mistakes accumulate in a non-linear fashion, we show that making \(M\) mistakes must lose in total \((}{nT^{2}})\). Finally, these value distributions have probabilities on the scale of \(\) with the same supports (essentially, only \(1/n\) of samples provide information), which means that \((})\) samples are needed to avoid making a constant fraction of mistakes.

### Our Contributions to Posted Pricing for Correlated Distributions

Independence among buyer valuations can be a strong modeling assumption for many applications. Although for arbitrary correlated distributions the optimal policy is not learnable, one could hope to learn the best policy in the class of all posted pricing policies. A recent work of Balcan et al.  can be applied to this setting to show that \((n/^{2})\) samples are sufficient to learn \(\)-optimal posted pricing. As discussed in Theorem 2, this linear in \(n\) dependency is necessary for revenue maximization, even for product distributions. Our first observation is that for correlated distributions, we need to lose this factor even for welfare maximization.

**Theorem 3** (corollary of Theorem 5).: _For welfare maximization with correlated buyers, any learning algorithm requires \((})\) samples to return an \(\)-additive approximation with constant probability._

Given this lower bound, a natural next question is to consider the subclass of posted pricing policies where the algorithm is only allowed to change its threshold at a small number of given locations. Can we now remove the linear in \(n\) dependency from sample complexity? The motivation to study this class comes from posted pricing applications where it is not possible for the algorithm designer to update the prices at each time step, e.g., due to business constraints.

Formally, for \(\{2,,n\}\), we say that posted pricing policy \(\)_respects change-points_\(\) if \(_{i}\) can differ from \(_{i-1}\) only when \(i\). We let \(_{}\) denote the class of all policies that respect change-points \(\), noting that policies in \(()\) post a static price for all buyers, and \((\{2,,n\})=^{n}\). Our following result shows that one can obtain sample complexity that is independent of \(n\), depending only on the size of \(\).

**Theorem 4** (proved in Subsection 3.1).: _For correlated distributions, the sample complexity of welfare or revenue maximization is \(O|)(1+||)+(1/)}{ ^{2}}\) when the policy is restricted to \(_{}\), for any \(\{2,,n\}\)._

**Proof sketch.** By existing results in learning theory, it suffices to bound the pseudo-dimension of \(_{}\). This will boil down to understanding the structure of "good sets", which are sets of the form \(\{_{}:() z\}\), for some input \(\) and some target \(z\). We will show that for a natural parameterization of \(_{}\), any good set can be expressed as the union and intersection of \(O(||+1)\) halfspaces, which implies the pseudo-dimension of \(_{}\) is \(O((||+1)(||+1))\) by a result of . This bound on the pseudo-dimension translates to the above sample complexity bound.

The learning algorithm which achieves the sample complexity bound in Theorem 4 is simply sample average approximation (SAA), which returns the policy in \(_{}\) with the highest objective value averaged over the samples. SAA can be computed in time \(O(Tn(Tn)^{1+||})\). This is because there are \(1+||\) prices to decide, each of which can take \(Tn\) possible values (one for each realized value in the \(T\) samples of length \(n\)), and evaluating each combination of prices over each of the \(T\) samples takes runtime linear in \(n\). We leave as an open question whether there is a more efficient algorithm.

Finally, we complement our sample complexity upper bounds for correlated distributions by giving matching lower bounds (up to polylogs).

**Theorem 5** (proved in Appendix A.5).: _For welfare or revenue maximization on correlated distributions, a learning algorithm requires \((|}{^{2}})\) samples to return an \(\)-additive approximation with constant probability, when restricted to the policy class \(_{}\) for any \(\{2,,n\}\)._

**Proof sketch.** In the construction for Theorem 5, on each trajectory exactly one of the \(1+||\) decision points (randomly selected) will be relevant, essentially diluting the samples by a factor of \(1+||\) and leading to a lower bound of \((|}{^{2}})\).

### Further Related Work

In 2007, Hajiaghayi et al.  discovered connections between auction design and posted pricing via prophet inequalities. Since then, there is a long line of work on understanding the power of posted pricing for selling multiple items to combinatorial buyers . For single item revenue maximization with known distributions, Correa et al.  showed the equivalence of welfare and revenue maximization objectives for single item posted pricing.

For background on sample complexity, we suggest Wainwright's excellent textbook . Sample complexity of auction design has been greatly studied; e.g., see . We refer the readers to Guo et al. , who recently resolved single item revenue maximization in the offline setting, for an overview of the literature. In , Guo et al. generalized their techniques for revenue maximization over product distributions to all "strongly monotone" problems, which includes posted pricing for welfare and revenue maximization.

Recently, there is also a lot interest in learning auctions in limited feedback models like bandit and pricing queries . We should note that sample complexity of optimal stopping (equivalent to our welfare maximization problem) has been previously studied in , who analyze linear stopping rules in a contextual setting. Our application of techniques from  to online algorithms over correlated sequences is also similar in spirit to some results from , who study a different application of inventory optimization.

Another related but tangential line of work focuses on prophet inequalities with samples , and the key distinction in these works is that their benchmark is the expected hindsight optimum, rather than the optimal online policy. Notably, any online algorithm incurs at least a factor of 2 loss compared to the hindsight optimum, even in the case of single-item prophet inequalities. As a result, this line of research aims to achieve \(O(1)\)-approximation guarantees, rather than the sublinear regret guarantees pursued in the current paper. Furthermore, their techniques differ significantly, as they often assume unbounded distributions. Finally, there is also work that explores the (non-)robustness of algorithms for the prophet inequality problem to inaccuracies in the distributions  and to dependencies in distributions .

Product Distributions

In this section we first prove our improved upper bound on the sample complexity of welfare for product distributions, and then prove a new lower bounds on the sample complexity of revenue for product distributions.

### Positive Result for Welfare: Proof of Theorem 1

The reward of the optimal policy is given by the following backward induction: \(r_{n+1}^{*}=0\) and \(r_{i}^{*}=[\{r_{i+1}^{*},V_{i}\}]=[(V_{ i}-r_{i+1}^{*})^{+}]+r_{i+1}^{*}\). It sets \(_{i}=r_{i+1}^{*}\).

When we do not know the distributions but only have \(T\) samples from each of them, we can consider the optimal policy on the product empirical distribution, which corresponds to replacing the expectations in the above definitions by the empirical average. That is, we will analyze the policy that sets \(_{i}=_{i+1}\), where \(_{i}\) is defined recursively by \(_{n+1}=0\), \(_{i}=_{t=1}^{T}(V_{i}^{(t)}-_{i+1})^{+}+_ {i+1}\).

Let \(r_{i}\) be the expected reward of this policy when starting with the \(i\)-th arrival. In order to prove the theorem, it is sufficient to show that with probability at least \(1-\), we have \(r_{1} r_{1}^{*}-\) if \(T(5(2e/)/)^{2}\) for any choices of \(,(0,1)\).

Let us define \(_{i}=_{i}-[\{_{i+1},V_{i}\}]\) or equivalently as

\[_{i}=_{t=1}^{T}(V_{i}^{(t)}-_{i+1})^{+}-[(V_{i}-_{i+1})^{+}].\]

That is, \(_{i}\) denotes the error introduced by using the empirical distribution for buyer \(i\) instead of the actual one. Note that \(_{i}\) can both be positive and negative.

The two key steps of our proof are as follows. We first show that

\[r_{1} r_{1}^{*}-2_{j 1}|_{i=1}^{j}_{i}|.\] (1)

This inequality holds point-wise, that is for any samples drawn. Then, we show that for \(T(5(2e/)/)^{2}\) samples, we have \(_{j 1}|_{i=1}^{j}_{i}|\) with probability at least \(1-\).

In order to show (1), we first lower bound \(r_{1}\) in terms of \(_{1}\).

**Lemma 1**.: \(r_{1}_{1}-_{j\{0,,n\}}_{i=1}^{j}_{i}\)_._

Proof.: Let \(j\{0,1,,n\}\) be the smallest index for which \(r_{j+1}_{j+1}\), which exists because \(0=r_{n+1}_{n+1}=0\). Note that we have \(r_{i}<_{i}\) for all \(1 i j\). We rewrite \(r_{1}\) as

\[r_{1}=[V_{1}<_{2}]r_{2}+[V_{1} _{2}][V_{1} V_{1}_{2}]=r_ {2}+[(V_{1}-r_{2})_{V_{1}_{2}}].\]

Repeating this argument,

\[r_{1}=_{i=1}^{j-1}[(V_{i}-r_{i+1})_{V_{i}_ {i+1}}]+[V_{j}_{V_{j}_{j+1}}+r_{j+1} _{V_{j}<_{j+1}}].\]

Inductively, we can also establish that

\[_{1}=_{i=1}^{j-1}_{t}(V_{i}^{(t)}-_{i+1})^{+ }+_{t}\{V_{j}^{(t)},_{j+1}\}.\]

Combining these two equalities,

\[_{1}-r_{1}=_{i=1}^{j-1}(_{t=1}^{T}(V_{i}^{(t)} -_{i+1})^{+}-[(V_{i}-}_{<_{i+1}}) _{V_{i}_{i+1}}])\]

\[+_{t=1}^{T}\{V_{j}^{(t)},_{j+1}\}-[V_{j} _{V_{j}_{j+1}}+}_{_{j+1}} _{V_{j}<_{j+1}}]\]

\[_{i=1}^{j-1}(_{t=1}^{T}(V_{i}^{(t)}-_{i+1}) ^{+}-[(V_{i}-_{i+1})^{+}])+_{t=1}^{T} \{V_{j}^{(t)},_{j+1}\}-[\{V_{j},_{j+1}\}]\]

\[=_{i=1}^{j}_{i}.\]

This implies \(_{1}-r_{1}_{j\{0,,n\}}_{i=1}^{j}_{i}\), completing the proof.

A similar proof allows us to prove the following lemma.

**Lemma 2**.: \(_{1} r_{1}^{*}-_{j\{0,,n\}}(-_{i=1}^{j}_{i})\)_._

Proof.: Let \(j\{0,1,,n\}\) be the smallest index for which \(_{j+1} r_{j+1}^{*}\), which exists because \(0=_{n+1} r_{n+1}^{*}=0\). Note that we have \(_{i}<r_{i}^{*}\) for all \(1 i j\), allowing us to derive

\[r_{1}^{*}-_{1} =_{i=1}^{j-1}([(V_{i}-^{*}} _{>_{i+1}})^{+}]-_{t=1}^{T}(V_{i}^{(t)}-_{i+1})^{+})\] \[+[\{V_{j}, {r_{j+1}^{*}}_{_{j+1}}\}]-_{t=1}^{T}\{V_{j}^{(t)},_{j+1}\}\] \[_{i=1}^{j-1}([(V_{i}-_{i+1})^{+}]- _{t=1}^{T}(V_{i}^{(t)}-_{i+1})^{+})\] \[+[\{V_{j},_{j+ 1}\}]-_{t=1}^{T}\{V_{j}^{(t)},_{j+1}\}\] \[=-_{i=1}^{j}_{i}.\]

This implies \(r_{1}^{*}-_{1}_{j\{0,,n\}}(-_{i=1}^{j}_{i})\), completing the proof. 

The last two lemmas imply (1). Thus, we need to bound \(_{j}_{i=1}^{j}_{i}\) to complete the proof.

**Lemma 3**.: _For every \(,>0\), with probability at least \(1-\), we have \(_{j}_{i=1}^{j}_{i}\) if \(T(5(2e/)/)^{2}\)._

Proof.: Observe that

\[_{i=1}^{j}_{i}=_{i=1}^{n}_ {i}-_{i=j+1}^{n}_{i}_{i=1}^{n}_{i} +_{i=j+1}^{n}_{i} 2_{} _{i=}^{n}_{i},\]

so it suffices to show that \(_{}_{i=}^{n}_{i}/2\) with probability at most \(\). Reversing the quantity of interest to \(_{}_{i=}^{n}_{i}\) allows us to define a martingale, and use Freedman's inequality, which is a martingale version of Bernstein's inequality.

**Lemma 4** (Freedman, Theorem 1.6 in ).: _Consider a real-valued sequence \(\{X_{t}\}_{t 0}\) such that \(X_{0}=0\) and \([X_{t+1} X_{t},X_{t-1},,X_{0}]=0\) for all \(t\). Assume that the sequence is uniformly bounded, i.e., \( X_{t} M\) almost surely for all \(t\). Now define the predictable quadratic variation process of the martingale to be \(W_{t}=_{j=0}^{t}[X_{j}^{2} X_{j-1},,X_{0}]\) for all \(t 1\). Then for all \( 0\) and \(^{2} 0\), and any stopping time \(\), we have_

\[[|_{j=0}^{}X_{j}|W_{}^{2}] 2(-/2}{^{2}+M/3}).\]

A corollary of Freedman's inequality is that \([|_{j=0}^{}X_{j}|] 2 (-/2}{^{2}+M/3})+[W_{ }^{2}]\). In order to use Freedman's inequality, we consider \(nT\) random variables \(X_{1},,X_{nT}\), where \(X_{iT+t}=((V_{n-i}^{(t)}-_{n-i-1})^{+}-[( V_{n-i}^{(t)}-_{n-i-1})^{+}])\) for \(i\{0,,n-1\}\) and \(t\{1,,T\}\). By this definition,

\[_{i}=_{t=1}^{T}X_{(n-i)T+t}\]

because \(V_{n-i}^{(t)}\) and \(V_{n-i}\) are identically distributed and independent of \(_{n-i-1}\). Moreover, for all \(j\),

\[[X_{j} X_{1},,X_{j-1}]=0 |X_{j}|.\]Let \(\) be the event that \(_{j=1}^{nT}[X_{j}^{2}\;|\;X_{1},,X_{j-1}]> ^{2}:=\). By Freedman's inequality, using \(T(5(2e/)/)^{2}\), we have

\[}[_{}|_{i=}^{n }_{i}|]\] \[ 2(-/8}{^{2}+}})+}[]\] \[ 2(-} )^{2}/8}{-+}})+}[]\] \[=2-+}}( )+}[]\] \[ 2(-(1)())+}[]=+}[].\]

It remains to show that \(}[]\). To this end, we observe that for any \(i\{0,,n-1\}\) and \(t\{1,,T\}\),

\[[X_{iT+t}^{2}\;|\;X_{1},,X_{iT+t-1}]\] \[=}[((V_{n-i}^{(t)}-_{n-i -1})^{+}-[(V_{n-i}^{(t)}-_{n-i-1})^{+}])^{2} \;|\;X_{1},,X_{iT+t-1}]\] \[}[(V_{n-i}^{(t)}-_{n-i-1 })^{+}\;|\;X_{1},,X_{iT+t-1}]\] \[=}_{t^{}=1}^{T}[(V_{n -i}^{(t^{})}-_{n-i-1})^{+}\;|\;X_{1},,X_{iT}]\] \[=}[_{i}-_{i+1}\; |\;_{i+1},,_{n}],\]

where the inequality uses \([Y][Y^{2}][Y ^{2}]-([Y])^{2}=[(Y-[Y])^ {2}]\) for any random variable \(Y\) with \(0 Y 1\) almost surely. In total, we have

\[_{j=1}^{nT}[X_{j}^{2}\;|\;X_{1},,X_{j-1}] _{i=1}^{n}[_{i}-_{i+1}\; |\;_{i+1},,_{n}].\]

As \(_{i=1}^{n}(_{i}-_{i+1})=_{1} 1\) almost surely, we have by Lemma 5 (see below) that

\[_{i=1}^{n}[_{i}-_{i+1}\;|\;_{i+ 1},,_{n}]()\]

with probability at most \(\). Therefore, we have

\[}[]}[_{i=1}^{n}[_{i}-_{i+1}\; |\;_{i+1},,_{n}]( )].\]

**Lemma 5**.: _Let \(Y_{1},Y_{2},,Y_{n}\) be a sequence of (not necessarily independent) random variables in \(\) such that \(_{i=1}^{n}Y_{i} 1\) almost surely. Then, for any \(>0\), with probability at most \(\), we have_

\[_{i=1}^{n}[Y_{i}\;|\;Y_{1},,Y_{i-1}] ().\]

We defer the proof of Lemma 5 to the appendix.

### Negative Result for Revenue: Proof of Theorem 2

Each buyer \(i=1,,n\) has a marginal value distribution that could be \(D_{i}^{}\) ("High") or \(D_{i}^{}\) ("Low"):

\[D_{i}^{}=+&\\ +&-16\\ 0&1-+16 D_{i}^{ }=+&-8\\ +&+8\\ 0&1-\]

which are valid distributions as long as \( 1/32\) and \(n 2\). All \(2^{n}\) configurations of whether each buyer has the High or Low distribution are possible.

Optimal policyFix any configuration of whether each buyer has the High or Low distribution. Let \(r_{i}^{*}\) denote the expected revenue to be earned under the optimal dynamic program, if buyer \(i\) is about to arrive and the item is not yet sold. We show inductively that \(r_{i}^{*}=\). By definition \(r_{i+1}^{*}=0\), establishing \(r_{i}^{*}=\) for \(i=n+1\). Now consider \(i=n,,1\), and assume \(r_{i+1}^{*}=\). Note that it is better to offer one of the prices \(+\) or \(+\) than to reject the buyer by offering a price of 1, because both of these prices are greater than \(r_{i}^{*}\) (by the induction hypothesis). Thus, if buyer \(i\) has distribution \(D_{i}^{}\), then

\[r_{i}^{*}=\{(+)+r_{i+1}^{*}(1-),(+)(-16 )+r_{i+1}^{*}(1-+16) \}\\ =\{,(-16)\}+=.\]

Similarly, if buyer \(i\) instead has \(D_{i}^{}\), then

\[r_{i}^{*}=\{(-8),\}+=.\]

In either case, we have \(r_{i}^{*}=\), completing the induction. Note that \(r_{1}^{*}=1/4\).

Bounding a policy's objective by its number of mistakesNow, consider an arbitrary policy \(=(_{1},,_{n})\) decided by the learning algorithm. Let \(r_{i}\) denote its expected revenue earned if buyer \(i\) is about to arrive and the item is not yet sold (under the true configuration of buyer distributions). We can without loss assume \(\) to lie in \(\{+,+\}^{n}\), because both prices are higher than \(r_{i+1}\), the expected revenue from rejecting (both prices are in fact higher than \(r_{i+1}^{*}\), an upper bound on \(r_{i+1}\)). We say that the policy _makes a mistake_ for buyer \(i\) if either \(_{i}=+\) when \(i\) has the High distribution, or \(_{i}=+\) when \(i\) has the Low distribution. If the policy makes a mistake for \(i\), then we have

\[r_{i}-r_{i+1}\{(+-r_{ i+1})(-16),(+-r_{i+1})( -8)\}\\ =-4+(-r_{i+1})(-16);\]

on the other hand, if the policy does not make a mistake for \(i\), then we have

\[r_{i}-r_{i+1}\{(+-r_{i+1}), (+-r_{i+1})\}=+(-r_{i+1}).\]

Hence, if the policy makes \(M\) mistakes for some \(M\{1,,n\}\), then

\[r_{1}=_{i=1}^{n}(r_{i}-r_{i+1})-M+ _{i=1}^{n}(-r_{i+1}).\]

Now, observe that \(-r_{i+1}=r_{i+1}^{*}-r_{i+1}\{n-i,M\}\), because the loss from making a mistake is at most \(4\), and starting from buyer \(i+1\), the number of mistakes can be at most \(n-(i+1)+1=n-i\) and also at most \(M\). Therefore,

\[r_{1}-M+((n-M) M+(M-1)++ {n})\\ =-M(1--)\ =\ -2 ().\]

Recalling that \(r_{1}^{*}=1/4\), this shows the additive error is \(()\) as long as the fraction of mistakes \(M/n\) is a constant.

Computing the Hellinger distanceWe first analyze the Hellinger distance \(H(D_{i}^{},D_{i}^{})\) between (a single observation of) \(D_{i}^{}\) vs. \(D_{i}^{}\), which we note does not depend on the buyer \(i\). The squared Hellinger distance can be bounded using \(1-H^{2}(D_{i}^{},D_{i}^{})\)

\[=(-8)}+ +8-24)(+8)}++16)(1- )}\\ (--)^{2}}{})+(--)^{2}}{+})+ (1-+-)^{2}}{ }})=1-O(}{n}),\]

where the inequality applies Lemma 6 below to each square root. This shows that \(H^{2}(D_{i}^{},D_{i}^{})=O(}{n})\), and the squared Hellinger distance is additive across independent samples. Using the fact that the Total Variation distance is upper-bounded by \(\) times the Hellinger distance , we see that the Total Variation distance between \(T\) independent samples of \(D_{i}^{}\) vs. \(T\) independent samples of \(D_{i}^{}\) is \(O(})\). The proof of Lemma 6 is deferred because it is elementary.

**Lemma 6**.: _Suppose \(C(0,1)\) and \(x[-C,C]\). Then \( C+-}{C}\)._Completing the proof of Theorem 2Suppose the distribution of each buyer is equally likely to be High or Low, independently across buyers. Fix any learning algorithm and the constant probability \(1/2\). By the computation of Hellinger distance above, if the number of samples \(T\) is less than \(C}\) for some constant \(C\), then for any buyer \(i\), the Total Variation distance between the samples observed under \(D_{i}^{}\) vs. \(D_{i}^{}\) is at most 1/2. (\(C\) depends on the choice of constant 1/2.) This means that w.p. at least \(1-1/2\), the price \(_{i}\) decided by the learning algorithm cannot depend on whether buyer \(i\) had distribution \(D_{i}^{}\) vs. \(D_{i}^{}\), which means that there exists an adversarial choice of \(D_{i}^{}\) or \(D_{i}^{}\) for each buyer \(i\) under which the probability of making a mistake for buyer \(i\) is at least \((1-1/2)/2=1/4\).

Let \(M\) denote the (random) number of mistakes, under this adversarial configuration of whether each buyer \(i\) has distribution \(D_{i}^{}\) or \(D_{i}^{}\). We have that \([] 1/4\). Although whether the algorithm makes a mistake could be arbitrarily correlated across buyers \(i\), applying \( 1\), we can employ Markov's inequality on the random variable \(1-\) to see that

\[[(1-) 7/8]=.\]

The LHS equals \([]=1-[>]\), and hence \([>]\). We have shown that unless \(T=(})\), there is probability at least 1/7 of making a constant fraction of mistakes, in which case we showed above that the additive error would be \(()\). This completes the proof of Theorem 2.

## 3 Correlated Distributions

In this section we prove our upper bound on the sample complexity of welfare/revenue maximization for correlated buyer distributions. We show nearly matching lower bounds in Appendix A.5.

### Positive Result for Welfare and Revenue: Proof of Theorem 4

To bound the sample complexity of posted pricing for correlated distributions, it suffices to bound the pseudo-dimension of the policy class \(_{}\). We use the same approach for both the welfare and revenue objectives. By standard learning theory results , bounds on the pseudo-dimension translate to bounds on the sample complexity as follows.

**Theorem 6**.: _Let \((_{})\) denote the pseudo-dimension of \(_{}\). For any \(>0\), any \((0,1)\) and any distribution \(\) over \(^{n}\), \(T=O(}((_{})+))\) samples are sufficient to ensure that with probability at least \(1-\) over the draw of samples \((_{1},,_{T})^{T}\), for all \(_{}\),_

\[|_{t=1}^{T}(_{t})-()| .\]

We will show that \((_{})=O(k k)\), where \(k=||+1\). This together with the above theorem immediately implies that Theorem 4 holds for the sample average approximation algorithm.

Let \(=\{i_{1},i_{2},i_{3},,i_{k-1}\}\), where \(1<i_{1}<i_{2}<<i_{k-1}\). For each \(j=1,2,,k\), let \(I_{j}=\{i_{j-1},,i_{j}-1\}\), with the convention that \(i_{0}=1\) and \(i_{k}=n+1\). In other words, \(I_{1},I_{2},,I_{k}\) are consecutive intervals that partition \([n]\), and \(_{}\) is the class of policies that offers every customer in \(I_{j}\) the same price. Note that every policy \(_{}\) can be parameterized by \(k\) prices \(=(_{1},_{2},,_{k})\), where \(_{j}\) is the price offered to customers in \(I_{j}\). In the rest of this proof, we will use \(_{}\) to denote the policy in \(_{}\) parameterized by \(\).

By the definition of pseudo-dimension,

\[(_{})=(_{}),\] (2)

where \(_{}:=\{(,z)\{_{}( ) z\}:^{k}\}\). Let \(_{}^{*}\) denote the dual class of \(_{}\), so

\[_{}^{*}:=\{\{_{}( ) z\}:^{n},z\}.\]

We will use a result from  to bound the VC dimension of \(_{}\). We state this result below in Definition 1 and Theorem 7. This result essentially says that the pseudo-dimension of the primal class is bounded if the dual class is well-structured. Here, "well-structured" essentially means that the domain can be partitioned into pieces defined by a small number of boundary functions, and the function is simple on each piece.

**Definition 1** (Definition 3.2 in ).: _A function class \(^{}\) that maps a domain \(\) to \(\) is \((,,l)\)-piecewise decomposable for a class \(\{0,1\}^{}\) of boundary functions and a class \(^{}\) of piece functions if the following holds: for every \(h\), there are \(l\) boundary functions \(g^{(1)},,g^{(l)}\) and a piece function \(f_{b}\) for each bit vector \(b\{0,1\}^{l}\) such that for all \(y,h(y)=f_{b_{y}}(y)\) where \(b_{y}=(g^{(1)}(y),,g^{(l)}(y))\{0,1\}^{l}\)._

The main theorem in  states that if the dual class is \((,,l)\)-piecewise decomposable, then the pseudo-dimension of the primal class is bounded.

**Theorem 7** (Theorem 3.3 in ).: _Suppose that the dual function class \(^{*}\) is \((,,l)\)-piecewise decomposable with boundary functions \(\{0,1\}^{}\) and piece functions \(^{}\). The pseudo-dimension of \(\) is bounded as follows:_

\[()=O(((^{*})+( ^{*}))((^{*})+(^{*}))+(^{*}) l).\]

We now apply Theorem 7 to bound the VC dimension of \(_{}\).5 To do so we must show that the dual class \(_{}^{*}\) is \((,,l)\)-piecewise decomposable for "nice" classes \(\) and \(\). We will show that for both the welfare and revenue objectives, we can take \(\) to be the family of constant functions and \(\) to be the family of axis-aligned halfspaces. This will follow from the below lemma, whose proof is deferred to the Appendix for space reasons.

**Lemma 7**.: _Let \(^{n}\) and let \(z\). Let \(G(,z)=\{^{k}:_{}() z\}\). For both the welfare and revenue objectives, there are \(l_{1},u_{1},,l_{k},u_{k}\{\}\) such that_

\[G(,z)=_{j=1}^{k}(u_{1},) (u_{j-1},)(l_{j},u_{j}]^{k-j}.\] (3)

**Corollary 1**.: \(_{}^{*}\) _is \((,,l)\)-piecewise decomposable, where \(\) is the set of constant functions, \(\) is the set of axis-aligned halfspaces, and \(l=2(||+1)\)._

Proof.: Consider a function \(h_{}^{*}\). By definition of \(_{}^{*}\), there exist \(^{n}\) and \(z\) such that \(h()=\{_{}() z\}\). By Lemma 7, there are \(l_{1},u_{1},,l_{k},u_{k}\) such that

\[\{^{k}:h()=1\}=_{j=1}^{k}(u_{1}, )(u_{j-1},)(l_{j},u_{j} ]^{k-j}.\]

For each \(j[k]\), let \(L_{j}=\{^{k}:_{j}>l_{j}\}\) and \(U_{j}=\{^{k}:_{j} u_{j}\}\). Note that \(L_{j}\) and \(U_{j}\) are axis-aligned halfspaces, and

\[\{^{k}:h()=1\}=_{j=1}^{k}_{1}_ {2}_{j-1} L_{j} U_{j}.\]

Therefore, in the definition of \((,,l)\)-piecewise decomposable, we may take the boundary functions to be the \(2k\) functions corresponding to the halfspaces \(L_{1},U_{1},,L_{k},U_{k}\). On any given piece defined by these boundary functions, \(h\) is a constant function (equal to either 0 or 1). 

For \(\) the set of constant functions and \(\) the set of axis-aligned halfspaces in \(^{k}\), it is easy to check that \((^{*})=0\) and \((^{*})=k\). Combining Corollary 1 with Theorem 7, we get that

\[(_{})=O(k k+k 2k)=O(k  k).\]

This completes the proof of Theorem 4.

**Remark**.: If we directly apply Theorem 7 from  to bound \((_{})\), then we would get

* A \(O(k k)\) pseudo-dimension bound for the revenue objective;
* A \(O(k(kn))\) pseudo-dimension bound for welfare objective.

In particular, the bound for welfare would grow with \(n\). This dependence on \(n\) is in fact unavoidable if one works with \(_{}\) directly, because for instances like \(=(,,,1)\), the dual function corresponding to \(\) is piecewise constant with \((n)\) pieces, even if \(k=1\). This is why our Corollary 1 focuses on the indicator functions \(_{}^{*}\) instead. Regardless, both the bounds for welfare and revenue require our Lemma 7 that analyzes the problem-specific structure of the "good sets".

AcknowledgementThis work was done in part while the authors were visiting the Simons Institute for the Theory of Computing for the program on Data-Driven Decision Processes. The authors thank Zhuoxin Chen for identifying typos in an early version.