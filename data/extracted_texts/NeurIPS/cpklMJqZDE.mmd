# Unrolled denoising networks provably learn to perform optimal Bayesian inference

Aayush Karan

Harvard SEAS

akaran1@g.harvard.edu

&Kulin Shah

UT Austin

kulinshah@utexas.edu

&Sitan Chen

Harvard SEAS

sitan@seas.harvard.edu &Yonina C. Eldar

Weizmann Institute of Science

yonina.eldar@weizmann.ac.il

Equal contribution

###### Abstract

Much of Bayesian inference centers around the design of estimators for inverse problems which are optimal assuming the data comes from a known prior. But what do these optimality guarantees mean if the prior is unknown? In recent years, algorithm unrolling has emerged as deep learning's answer to this ageold question: design a neural network whose layers can in principle simulate iterations of inference algorithms and train on data generated by the unknown prior. Despite its empirical success, however, it has remained unclear whether this method can provably recover the performance of its optimal, prior-aware counterparts.

In this work, we prove the first rigorous learning guarantees for neural networks based on unrolling approximate message passing (AMP). For compressed sensing, we prove that when trained on data drawn from a product prior, the layers of the network approximately converge to the same denoisers used in Bayes AMP. We also provide extensive numerical experiments for compressed sensing and rank-one matrix estimation demonstrating the advantages of our unrolled architecture - in addition to being able to obliviously adapt to general priors, it exhibits improvements over Bayes AMP in more general settings of low dimensions, non-Gaussian designs, and non-product priors.

## 1 Introduction

Inverse problems within engineering and the sciences  have inspired the development of a rich toolbox of algorithms for inferring unknown signals given noisy measurements. For instance, a classic approach to solving sparse linear inverse problems is to solve the LASSO using an iterative algorithm like ISTA  or FISTA . While these methods are easy to implement and remarkably performant, they are not designed to exploit _distributional_ information about the underlying signal, which often comes from domain knowledge. In contrast, Bayesian methods like message passing and variational inference offer a natural framework for designing estimators that incorporate this kind of information: the algorithm designer crafts a _prior_ for the signal, and the measurements they observe naturally induce a _posterior_ over what the underlying signal could have been.

Such an approach comes at a cost. On one hand, this method often comes with strong optimality guarantees in the well-specified setting where the algorithm designer has access to the true prior distribution of the data. In practice, however, this prior is not known _a priori_ and hence must be inferred, and any mismatch between the inferred prior and the true distribution will adversely affect the performance of the estimator in ways that are poorly understood compared to what is known in the well-specified setting .

In recent years, _algorithm unrolling_ has emerged as a scalable solution for developing estimators that can improve upon this practicality-performance tradeoff by learning from samples drawn from the data distribution . The idea is to craft a neural network architecture, each of whose layers is expressive enough to implement one step of an existing, hand-crafted iterative algorithm (e.g. ISTA). Then, instead of explicitly setting the weights of the network so that it implements that algorithm, one trains the network on examples of the inference problem at hand using stochastic gradient descent. Remarkably, the algorithm that the network converges to tends to perform at least as well as (and often better than) the hand-crafted algorithm being unrolled, e.g. in the number of layers and iterations necessary to achieve a certain level of error.

Thus, at least empirically, algorithm unrolling seems to achieve the best of both worlds, marrying the domain-aware power of classical iterative methods with the remarkable learning capabilities of neural networks. From a theoretical perspective however, our understanding of its performance is rather limited, as existing guarantees are centered around non-algorithmic aspects like representational power and generalization bounds (see Section 1.1 for a detailed discussion).

In particular, the following fundamental learning question remains open:

_Can an unrolled network trained with stochastic gradient descent provably obtain an estimator competitive with the best prior-aware algorithms?_

In this work, we give the first rigorous learning guarantees for this question, focusing on the well-studied setting of _compressed sensing_ (see Section 2.1 for definitions). In addition, we provide the first empirical evidence in the affirmative for the problem of _rank-one matrix estimation_ (Section C.1).

**Approximate message passing and unrolling.** Consider the standard Bayesian setup where we observe a noisy measurement \(y\) of some signal \(x\), and would like to output an estimate \(\) minimizing \(\|x-\|^{2}\). Information-theoretically, the Bayes-optimal estimator for this task is the posterior mean \([x y]\), but in many settings of interest this estimator may not be computable by a polynomial-time algorithm. Among computationally efficient estimators, for a wide variety of such inference tasks it is conjectured  that a certain family of iterative algorithms called _approximate message passing (AMP)_ is optimal. We give a self-contained exposition of this method in Section 2. Roughly speaking, one can think of AMP as a more advanced version of ISTA where the denoiser at each step can be tuned depending on the prior, and additionally there is a crucial momentum term inspired by a correction from statistical physics . AMP with the optimal tuning of the denoiser is called _Bayes AMP_.

Motivated by the appealing theoretical properties of AMP, in this work we investigate the training dynamics of neural networks given by unrolling this algorithm. In place of the prior-dependent denoisers \(_{1},_{2},\), we consider generic denoisers given by _neural networks_\(_{1},_{2},\) and unroll the iterations of AMP into layers of a neural network (see Section 2.2 for details). For the theoretical results in this work, we focus on the setting where the only trainable parameters in the network are the ones parametrizing the denoisers \(_{}\).

Unrolled AMP architectures and variants thereof were originally proposed and empirically investigated by Borgerding et al.  and follow-ups . These works found that unrolled AMP can significantly outperform unrolled ISTA as well as a version of AMP with soft threshholding denoisers in terms of convergence; i.e., the number of layers needed to achieve a certain MSE.

Despite these compelling experimental results, to our knowledge, there is still little understanding as to whether these networks can actually recover the performance of Bayes AMP. The main theoretical result of this work is to give the first proof that unrolled AMP networks trained with gradient descent converge to the same denoisers as Bayes AMP and thus achieve mean squared error which is conjectured to be optimal among all polynomial-time algorithms for compressed sensing:

**Theorem 1** (Informal, see Theorem 2).: _For compressed sensing with Gaussian sensing matrix, if the prior on the signal is a product distribution with smooth, sub-Gaussian marginals, then an unrolled network based on AMP which is trained with gradient descent on polynomially many samples will converge in polynomially many iterations to an estimator which, in the infinite-dimensional limit, achieves the same mean squared error as Bayes AMP._

Our proof is based on a novel synthesis of _state evolution_, the fundamental distributional recursion driving analyses of AMP, together with neural tangent kernel (NTK) analysis of training dynamics for overparametrized networks. Crucially, unlike in typical applications of NTK analysis, the level of overparametrization needed in our network is _dimension-independent_ even when the second moment \(_{x-q}\|x\|^{2}\) scales with the dimension \(d\). The central reason behind this is that state evolution allows us to map the training dynamics of the network, which _a priori_ lives in \(L_{2}(^{d})\), to training dynamics over the space of functions \(L_{2}()\), where the resulting learning problem amounts to that of _one-dimensional score estimation_. As a result, our learning guarantee only requires overparametrization scaling inverse polynomially in the target error.

**Experiments.** We complement these theoretical results with extensive numerical experiments. Our main empirical contributions are as follows:

* We demonstrate that our theoretically motivated unrolled network learns the same optimal denoisers as Bayes AMP, providing a practical alternative that achieves the same performance but does not require explicit knowledge of the true signal prior.
* We observe that introducing auxiliary trainable parameters along with learnable denoisers further improves performance over AMP in low-dimensional settings (where the asymptotic optimality of Bayes AMP does not apply) and when the sensing matrix is non-Gaussian, both in _well-conditioned_ and _ill-conditioned_ settings.
* We introduce rank-one matrix estimation as a new "model organism" for probing the properties of unrolled networks. To our knowledge, despite its prominence in the theoretical literature on AMP, rank-one matrix estimation has not yet been studied in the context of algorithm unrolling.

The general approach of unrolling with learned denoisers is lesser utilized in the algorithm unrolling literature, which instead largely emphasizes learning auxiliary parameters around domain-specific entities - e.g. measurement matrices or sparse coding dictionaries - while fixing denoisers typically to a soft thresholding function. Our results indicate that learned denoisers can in fact capture distributional priors and are composable with these domain-specific learned parameters, providing a valuable addition to the algorithmic toolkit for practitioners of both AMP and unrolling.

### Related work

We provide an extensive review of prior work in the appendix. Here we discuss the works most directly related to ours.

**Theory for unrolling ISTA.** The existing theory for algorithm unrolling almost exclusively focuses on unrolled ISTA (often called LISTA) and compressed sensing. Unlike the present work, they do not consider a Bayesian setting: the signal \(x\) is a deterministic sparse vector, and the goal is to converge to \(x\). Instead of proving learning guarantees, most of them are representational in nature, arguing that under certain settings of the weights in LISTA, the estimator computed by the network can be more iteration-efficient than vanilla ISTA . The works of  proved generalization bounds for unrolled networks; these are statistical rather than computational in nature. Finally, recent work of  studied optimization aspects of LISTA, and their main theorem, motivated by the NTK literature  from a different perspective than ours, was an upper bound on the Hessian of the empirical risk of an unrolled ISTA network in a neighborhood around random initialization.

**Unrolled AMP.** Borgerding et al.  were the first to propose unrolling AMP for compressed sensing. In contrast to the architecture we consider, they primarily considered fixed soft-thresholding denoisers \(_{}(;)\), in addition to simple parametric families of denoisers like 5-wise linear functions and splines. For these parametric denoisers, on simple priors like Bernoulli-Gaussian they found that the learned network could approach the performance of the "oracle" estimator that knows the support of the underlying signal -- see the Appendix for further discussion.

**Other learning-based approaches.** Here we further motivate the setting we consider by contrasting with other possible approaches to learning optimal inference algorithms from data. Perhaps the most obvious would be to simply try to directly learn an approximation to the density function for the prior, e.g. via kernel density estimation or some other non-parametric method. In the product-prior setting in which we prove our results, this is indeed a viable approach in theory. But in practice, unlike algorithm unrolling, this will not scale gracefully to general high-dimensional distributions .

A more scalable approach might training a diffusion model on the data distribution . The learned score network could then be used to approximately implement Bayes AMP. Our method is roughly a special case of this: whereas little is known about provable score estimation in general , our theoretical results demonstrate that layerwise training of unrolled networks is a viable, provably correct way to implicitly estimate the score of the data distribution. Furthermore, unrolling accommodates additional trainable parameters to improve robustness to real-world deviations from the stylized models studied in theory.

Finally, we mention the recent theoretical work of , which shows that semidefinite programs can simulate AMP. While this is not a learning result, it has a similar motivation of reproducing the performance guarantees of AMP using a more robust suite of algorithmic tools.

**Other theory for unrolling.** established sample complexity bounds for learning graphical models via diffusion models by unrolling the variational inference algorithms used for score estimation into a ResNet and bounding the number of parameters needed for the network to express these algorithms. Similarly,  showed that the popular U-Net architecture can simulate message-passing algorithms. These works can be interpreted as giving representational guarantees for algorithm unrolling, whereas in contrast, the focus of our work is on proving learning guarantees.

## 2 Preliminaries on Bayes AMP and unrolling

Here we give an overview of the Bayes AMP algorithm in the compressed sensing setting. We refer the reader to Appendix C for a full treatment of the rank-one matrix estimation setting. For convenience, when it is clear from context, we use Bayes AMP to refer to the general algorithm used in either setting.

### Compressed sensing

In compressed sensing, we are given noisy linear measurements \(y^{m}\) obtained from an unknown signal \(x^{d}\) via the observation process

\[y=Ax+, \]

where \(A^{m d}\) and \(^{m}\) is a random noise vector with i.i.d. entries drawn from the distribution \((0,^{2})\). Throughout, we assume that \(x p_{}\) for product prior \(p_{} p^{ d}\), where \(p\) is some distribution over \(\). The _compressed sensing_ problem aims to recover the unknown signal \(x\) with estimate \(\) such that the mean squared error (MSE) \(\|-x\|^{2}\) is minimal. Throughout this work we focus on the _proportional asymptotic_ setting where we implicitly work with a sequence of such compressed sensing problems indexed by dimension \(d\), where \(d\) and \(m=m(d)\) jointly tend to infinity and \(m(d)/d\) for absolute constant \(>0\).

 proposed the following approximate message passing (AMP) algorithm for estimating \(x\) given \(A,y\). The algorithm starts with \(x_{0}=0\) and \(v_{0}=y\) and proceeds by

\[x_{+1} =f_{}(A^{}v_{}+x_{}) \] \[v_{} =y-Ax_{}+v_{-1}(f^{}_{-1}(A^{ }v_{-1}+x_{-1})). \]where \(f_{}:\) is a scalar denoiser applied entrywise, \(f_{-1}^{}\) is also applied entrywise, and \(\) denotes an entrywise average. The last term in Eq. (3) is commonly referred to as the _Onsager term_. Importantly, the AMP iterates asymptotically satisfy a distributional recursion called _state evolution_. Suppose the entries of \(A\) are given by \(A_{ij}(0,1/m)\). Define the _state evolution parameters_\((_{})\) via the scalar recursion

\[_{+1}^{2}=^{2}+[(f_{}(+_ {})-)^{2}]_{0}=^{2}+[^{2}]\,,\]

where \( p\) and \((0,1)\). Then it is known that as \(d\), the empirical distribution over entries of \(A^{}_{}+x_{}\) converges in a certain sense to the one-dimensional distribution over \(+_{}\). While updates of AMP can be run with any choice of differentiable \(f_{}\), there is an asymptotically optimal choice that depends on the underlying prior \(p_{}\), and the resulting optimal algorithm is called _Bayes AMP_. In particular, let \(_{0}^{*2}=^{2}+[^{2}]\), and define

\[f_{}^{*}=[|+_{}^{*}=]_{+1}^{*2}=^{2}+[f_{ }^{*}(+_{}^{*})-^{2}], \]

where \( p\) and \((0,1)\). Then setting \(f_{}=f_{}^{*}\) for all \(\) in Eqs. (2) and (3) yields Bayes AMP.

In the asymptotic limit, i.e. as \(m,d\), Bayes AMP has strong theoretical properties. In the setting above, it is conjectured to obtain the optimal MSE over all polynomial-time algorithms  and has been proven to be optimal over a quite general class of algorithms known as _general first-order methods_ (_GFOMs_) .

In practice, however, Bayes AMP is subtly nontrivial to implement. For starters, one must know \(p\) to construct \(f_{}^{*}\). Furthermore, using the exact recursion in Eq. (4) can often lead the algorithm to diverge in finite dimensions. One instead estimates the state evolution parameters from the previous iterates, i.e. replacing \(_{}^{2}\) with \(\|_{}\|_{2}^{2}\), which is typically enough to stabilize Bayes AMP. The fact that this is a valid estimate follows by state evolution, which ensures that in the infinite dimensional limit, the entries of \(_{}\) are distributed according to \((0,_{}^{2})\).

### Unrolling Bayes AMP

The aforementioned challenges in realizing the conjectured optimality of Bayes AMP in practice motivate the need for a robust method that does not require knowledge of the prior distribution \(p_{}\). We consider replacing each scalar denoiser \(f_{}\) in Eq. (2) or (22) with a multilayer perceptron (MLP) that _learns_ the "right" denoiser function to use at each iteration of AMP. As we will see, a prudent training approach is enough to provably ensure that our unrolled network learns the optimal denoiser at each layer, effectively recovering Bayes AMP even without explicit knowledge of the prior.

**Architecture.** Suppose we are given training data \(\{(y^{i},x^{i})\}_{i=1}^{N}\) generated according to Eq. (1) with \(x^{i} p_{}\) for all \(i\). Let \(L\) denote the number of layers in our unrolled network, and let \(\) denote a family of MLPs with fixed architecture (i.e. fixed depth and width) constrained to a two-dimensional input and one-dimensional output. For each \([0,L-1]\), initialize an MLP \(_{}:^{2}\) chosen from \(\). Set \(_{0}=0_{d}^{d}\) and \(_{0}=y^{i}^{m}\), for a given training input \(y^{i}\). Then for each layer \([0,L-1]\), our network computes the forward pass

\[_{+1}=_{}(A^{}_{}+_{};_{})_{+1} =y-A_{+1}+_{}(_{1} _{}(A^{}_{}+_{};_{}))\,, \]

where \(_{}=\|_{}\|_{2}/\) and \(_{1}\) denotes differentiation with respect to the first input parameter. The notation \(_{}\) ( \(\) ; \(_{}\)) denotes applying the scalar function \(_{}\) ( \(\), \(_{}\)) entrywise. We emphasize that \(_{}\) is tied to \(_{1}_{}\); that is, we are taking the derivative of the MLP to compute the Onsager term. We refer to our unrolled architecture as an **LDNet** (Learned Denoising **Network**).

**Training.** Naively, one might consider training the \(L\)-layer network end-to-end on the mean squared errors of the network estimates - i.e., either with loss function \(_{CS}=_{i=1}^{N}\|_{L}^{i}-x^{ i}\|_{2}^{2}\) for compressed sensing or \(_{ME}=_{i=1}^{N}\|_{L}^{i} _{L}^{i}-x^{i}x^{i}\|_{F}^{2}\) for rank-one matrix estimation.

However, as we observed empirically (echoed by findings in ), such an approach gets trapped in suboptimal local assignments of denoising functions.

Instead, we employ _layerwise training_, where we iteratively train the \(\)-th denoiser \(_{}\) on the mean squared error loss for the layer-\(\) estimate. If \(\) denotes an LDNet with \(L\) layers, let \([0:]\) denote the subnetwork that consists only of the first \(+1\) layers of \(\), with denoiser \(_{}\) at layer \(\) for \(0 L-1\). Then our training procedure follows Algorithm 1. Note we initialize the \(\)-th denoiser weights with the previous learned denoiser before training - while this is not relevant to our theoretical results in Section 3, empirically, we find that this initialization is necessary to avoid being trapped in suboptimal regions of parameters. Likewise, we include an optional finetuning step that further reduces approximation error in the learned denoisers but is not needed for our theory results.

```
0: Training data \(\), LDNet \(\)
1for\(=0\) to \(=L-1\)do
2if\(>0\)then
3 Initialize \(_{}_{-1}\);
4
5 Freeze learnable weights in \(_{k}\) for \(k<\);
6 Train \([0:]\) on \(\); // Optional finetuning step
7
8 Unfreeze learnable weights in \(_{k}\) for \(k<\) and train \([0:]\) on \(\);
9: Fully trained \(\)
```

**Algorithm 1**Layerwise Training

The proof of optimality of Bayes AMP among all implementations of AMP for the problems we consider, as given in , strongly motivates our training method: assuming we have learned optimal denoisers up to layer \(-1\), one can show that the minimum mean squared error at layer \(\) is achieved by the denoiser used in Bayes AMP. This gives a heuristic sense for how layerwise training facilitates learning optimal denoisers, and this intuition is validated in both our theory and experiments.

## 3 Provably learning Bayes AMP

We now provide theoretical guarantees that our unrolled denoising network can learn Bayes-optimal denoisers when trained in a layerwise fashion. Consider any prior \(p_{}=p^{ d}\) for which \(p\) satisfies the following assumption. The product prior setting is quite standard and widely studied within the theory literature on AMP (e.g. ).

**Assumption 1**.: _Given \( 0\), let \(p(;)\) denote the density of the convolution \(p(0,^{2})\). We assume that:_

1. \(p\) _is_ \(R\)_-sub-Gaussian with_ \(_{X p}[X]=0\)_._
2. _The_ score function \(_{1}p(;)\) _is_ \(B\)_-Lipschitz for all_ \(^{2}\)_, where_ \(^{2}\) _is the variance of the entries of_ \(\) _in Eq. (_1_)._

Both assumptions are relatively mild and hold for a large class of distributions. For example, the sub-Gaussianity holds for any distribution with bounded support (see Section 2.5 in ) and the Lipschitzness of the score function is a consequence of regularizing properties of heat flow (see e.g. Lemma 4 in ).

Our main guarantee (see Theorem 2 below) is that under Assumption 1, a suitable unrolled architecture trained with SGD on examples of compressed sensing tasks can compete with Bayes AMP. In Section 3.1, we define the training objective and architecture, describe how our bounds will depend on the underlying prior \(p\), and formally state our main result. The full proof is provided in Appendix B.

### Proof preliminaries and theorem statement

To prove our learning guarantee, we start by proving that the error in using the learned denoiser in one unrolled layer is small. Denote the sequence generated by the learned denoiser \(\) by \(_{}\) and \(_{}\). The Onsager term in AMP ensures that for any iteration \(\), the distribution of \(A^{}_{}+_{}\) asymptotically behaves as if every coordinate is i.i.d. as \(d\) for any \(\) (Lemma 1). Therefore, it suffices to learn the denoiser for any fixed coordinate.2 Without loss of generality we consider the first coordinate and try to learn the denoiser function by minimizing the following objective:

\[_{g}\ \ [(g(A_{1}^{}_{}+_{ }^{(1)})-x^{(1)})^{2}], \]

where \(A_{j}^{}\) denotes the \(j\)th row of \(A^{}\) and \(x^{(j)}\) denotes the \(j\)th coordinate of the vector \(x\). As the learning and generalization guarantee is identical for all \(\), we will occasionally drop \(\) from the subscript when the context is clear.

Denoiser complexity.To quantify the complexity of learning Bayes AMP in terms of the underlying prior \(p\), we will work with the following notion of the _complexity_ of a class of scalar functions from , which we will apply to the denoisers that arise in Bayes AMP:

**Definition 1** (Scalar function complexity ).: _Let \(C>0\) be some sufficiently large absolute constant (e.g. \(10^{4}\)). Given any smooth function \(:\) and a parameter \(>0\), we define complexity of \(\) at scale \(\) as follows. Suppose \(\) admits a power series expansion \((z)=_{i=0}^{}c_{i}z^{i}\). Then_

\[_{}(,)_{i=0}^{}1+ ((1/)/i)^{i/2}(C)^{i}|c_{i}| _{s}(,) C_{i=0}^{}(i+1)^{1.75} ^{i}|c_{i}|.\]

_Given a class of scalar functions \(\), we define the complexity of \(\) at scale \(\) by \(_{}(,)=_{} _{}(,)\) (and similarly \(_{s}(,)=_{}_{s} (,)\))._

Intuitively, \(_{}(,)\) and \(_{s}(,)\) both captures how much functions in function class \(\) can be approximated using a low-degree polynomial. For any function \(\) and any \(\), the above complexities are related by \(_{s}(,)_{}(,) _{s}(,O())(1/)\) because \((C/)^{i} e^{O( 1/)}= (1/)\) for all \(i\). We provide more intuition on how \(_{}(,)\) and \(_{s}(,)\) scales with \(,\), under mild assumptions on \(\) in Section B.5.

Main result.We can now formally state the main theoretical guarantee of this work, namely that layerwise training of LDNet results in performance matching that of Bayes AMP for compressed sensing:

**Theorem 2**.: _Suppose the prior distribution \(p\) satisfies Assumption 1. Then, for every \(_{2}(0,1)\) and \(_{1}(0,1/_{s}(,R( 1/_{2})^{3/2}))\), there exists_

\[M_{0}=(_{_{1}}(f^{*},R( 1/ _{2})^{3/2}),1/_{1}) N_{0}= (L_{s}(f^{*},R( 1/_{2})^{3/2}),1/ _{1})\]

_such that the following holds._

_Let \(L\) be any positive integer. Consider an LDNet of depth \(L\) with MLP denoisers \(_{}\) given by the MLP architecture in Eq. (8) with \(m M_{0}\) neurons. Suppose the network is trained by running gradient descent from random initialization with step size \(=(1/(_{1}m))\) on \(n N_{0}\) samples of the form \((y^{i},x^{i})\), where each training example is generated by independently sampling Gaussian matrix \(A\) with entries i.i.d. from \((0,1/m)\), sampling \(x^{i} p_{x}=p^{ d}\), and forming \(y^{i}=Ax^{i}+\) for \((0,^{2})\).__After \(T=(_{s}(f_{t}^{*},R( 1/_{2})^{3/2})^{2}/ _{1}^{2})\) steps of gradient descent, with high probability the activations \((_{L},_{L})\) and denoiser \(_{L}\) at the output layer of the LDNet (see (5)) satisfy_

\[_{x,A}_{L}(A^{}_ {L}+_{L};_{L})-x^{2} _{}(L)+B^{2}}{^{7}}^{L+1}( _{1}+_{2})+o_{d}(1)\,,\]

_where \(_{}(L)_{x,A}\|f_{L} ^{*}(A^{}v_{L}+x_{L};_{L})-x\|^{2}\) is the error achieved by running \(L\) steps of Bayes AMP._

Observe that the level of overparametrization in terms of number of samples \(n\) and number of hidden neurons \(m\) needed in Theorem 2 is _dimension-free_, unlike in typical NTK analyses. This happens because state evolution effectively allows us to convert the learning problem in \(d\) dimensions to a learning problem in \(1\) dimension: we can effectively assume that the entries of \(A^{}v_{}+x_{}\) converge in an appropriate sense to the distribution of \(+_{}Z\) for \(\) and \(Z(0,1)\).

This ensures that the learning objective effectively reduces to minimizing \([(f_{t}(+_{}Z)-)^{2}]\) over a parametrized family of denoisers \(f_{t}\). The latter objective is often referred to as the _score matching objective_ (in one dimension), which is minimized by the Bayes-optimal denoiser \(f_{t}^{*}=[|+_{}Z=]\) at each layer \(\). A key component in the proof of Theorem 2 is thus to show that gradient descent can learn this optimal denoiser given one-dimensional training data of the form \((+_{}Z,)\).

As we will show, the runtime for gradient descent is largely dictated by the extent to which these denoisers can be polynomially approximated. _A priori_, one might expect that if degree-\(s\) polynomials are needed, then the runtime of the algorithm must scale as \(d^{O(s)}\). This would be prohibitively expensive if \(s\) is increasing in the dimension \(d\). Fortunately however, because we are able to reduce to one-dimensional training dynamics, we ultimately achieve much more favorable scaling in \(d\).

## 4 Experiments

We now empirically demonstrate the performance of our proposed architecture and training scheme for unrolling Bayes AMP in a variety of statistical settings. Throughout these experiments, we are motivated by the following questions: **a)** Can our method empirically match the performance of Bayes AMP in settings where the latter is conjectured to be computationally optimal? **b)** In these settings, does our network learn the optimal denoisers? **c)** Are there settings where our methods offer a performance advantage over AMP?

### Compressed sensing

**Implementation details.** We set \(m=250\), \(d=500\) and fix a random Gaussian sensing matrix \(A^{250 500}\). We consider two choices of prior for our experiments: _Bernoulli-Gaussian_ and \(_{2}\) (i.e. uniform over \(\{1,-1\}^{n}\)). For our unrolled architecture, the family \(\) of learned MLP denoisers was restricted to three hidden layers, each with \(70\) neurons and GELU activations. This particular architectural choice was the most convenient for our experiments, but our experimental findings are not particularly sensitive to this. We randomly generated a train and validation dataset \(\{y^{i},x^{i}\}_{i=1}^{N}\) with \(N=2^{15}\) samples by sampling from the prior and using Eq. (1). We train layerwise with finetuning as in Algorithm 1.

For each prior, we also implemented Bayes AMP using the corresponding optimal denoiser. As an additional "semi-prior-aware" baseline, we replace the MLP denoisers in LDNet with "guided denoisers" that have the same functional form as the optimal denoisers but contain trainable parameters; see Appendix E for more details on the precise functional forms used. By convention, we report performance results for all methods by the normalized mean squared error (NMSE) \(\|-x\|_{2}^{2}/\|x\|_{2}^{2}\).

**Bernoulli-Gaussian prior.** Here, each entry of \(x\) is independently drawn from a standard normal distribution and set to \(0\) with probability \(1-\); i.e., \(p_{x}=p^{ d}\) where \(p(x)=\,(0,1;x)+(1-)\,(x)\), where \(\) denotes the Dirac delta at \(x=0\). To match the setting considered in the prior work of Borgerding et al. , we set the masking probability to be \(=0.1\) and the measurement noise to be \(^{2}=2 10^{-5}\).

We plot the NMSE that our unrolled network and baselines achieve in decibels (dB); that is, \(10_{10}\) (NMSE), in Figure 1. LDNet almost perfectly matches the NMSE of Bayes AMP at each layer/iteration. Over 15 layers, our network converges to an NMSE of \(-44.9313\)**dB**, as compared to Bayes AMP converging to \(-45.3280\)**dB**. As the scale is logarithmic, the difference in error achieved is negligible.

\(_{2}\) **prior.** Here each entry of \(x\) is chosen from \(\{-1,1\}\) with probability \(\); i.e., \(p_{x}=p^{ d}\) for \(p(x)=_{-1}(x)+_{+1}(x)\). To examine a higher noise regime and to ensure Bayes AMP converged within a reasonable number of iterations, we set the measurement noise to be \(^{2}=0.075\). Figure 1 demonstrates that LDNet again recovers Bayes AMP at every iteration, even slightly outperforming by layer 15, achieving an NMSE of \(0.4267\) (an improvement of \(1.28\%\)).

**LDNet denoisers.** From Figure 2 we can observe qualitatively that the learned MLP denoisers recover the functional form for the optimal denoiser at each iteration, as our theory suggests. Interestingly, although the denoisers were trained relative to a fixed sensing matrix, they appear to learn the Bayes AMP denoiser that is measurement-independent, and in Appendix D.3 we show that the performance of these learned denoisers actually transfers to other randomly drawn sensing matrices \(A\).

Figure 1: **LDNet for Compressed Sensing.** On the left, we plot the NMSE (in dB) obtained by LDNet and Bayes AMP baselines on the Bernoulli-Gaussian prior. On the right, we plot NMSE (not in dB) achieved on the \(_{2}\) prior. LDNet (along with the guided denoisers) achieves virtually identical performance to the conjectured computationally optimal Bayes AMP.

Figure 2: **Learned Denoisers for Compressed Sensing.** We plot layerwise denoising functions learned by LDNet on the Bernoulli-Gaussian and \(_{2}\) priors relative to their optimal denoisers over a range of inputs in \((-2,2)\). The state evolution input \(_{I}\) to each denoiser is set to be its empirical estimate.

### Beyond Bayes AMP performance

Much of the algorithm unrolling literature focuses on learning _auxiliary parameters_ while using fixed denoisers, as opposed to learning the denoisers themselves. In particular, unrolled methods like LISTA  and LAMP  reparameterize \(A^{}\) in Eqs. (2) and (3) as a new learnable matrix \(B\), which results in faster convergence than classical, learning-free counterparts like ISTA and AMP.

This does not necessarily contradict Bayes AMP's conjectured optimality for compressed sensing, which only applies in the \(d\) limit. In the context of our unrolling method, we posit that learning the matrix \(B\) can be thought of as learning finite dimensional corrections to the Bayes AMP iterations. In Appendix D.1, we demonstrate that the lower the signal dimensionality, the larger the performance improvement of LDNet with learned matrix \(B\) over Bayes AMP.

Finite dimensionality is not the only deviation in design from where Bayes AMP is (conjectured) optimal. In fact we can consider non-Gaussian designs of the measurement matrix \(A\), and we show in D.1 that LDNet outperforms Bayes AMP in both well-conditioned and ill-conditioned settings. Furthermore, we can relax the assumption that the signal is drawn for a product prior and extend LDNet to accommodate non-product priors. In D.2, we demonstrate LDNet to surpass Bayes AMP in a non-separable mixture-of-gaussians prior.

## 5 Outlook

In this work we gave the first proof that unrolled denoising networks can compete with optimal prior-aware algorithms simply via gradient-based training on data. Our proof used a novel synthesis of state evolution with NTK theory, and notably, the level of overparametrization needed for our result to hold is independent of the dimension, unlike existing results in the NTK literature. One important consequence of these results is that a one-dimensional score function is learnable with gradient descent, for which only representational, as opposed to algorithm, results existed previously in the literature .

We supplemented our theory with extensive numerical experiments, confirming that LDNet can recover Bayes AMP performance and Bayes-optimal denoisers without knowledge of the signal prior. Moreover, for various settings where Bayes AMP is not conjectured to perform optimally - e.g. inference in low dimensions, non-Gaussian designs, and non-product priors - we demonstrate that LDNet outperforms Bayes AMP. We thus establish unrolling denoisers as a powerful, practical addition to the algorithmic toolkit for Bayesian inverse problems.

One limitation is that our theoretical results are currently limited to the product prior setting. The non-product setting is difficult because even though state evolution is known here , proving an unrolled network converges to the right denoisers essentially amounts to proving that one can learn the score functions of a general data distribution. Additionally, it is subtle to define the right architecture, as the denoisers are no longer scalar, and a generic feedforward architecture would be difficult to prove rigorous guarantees for (and to scale in practice).

In addition, our theoretical results do not immediately extend to the rank-one matrix estimation setting. While closeness in denoising error implies closeness in the state evolution parameter \(\) for compressed sensing, this is not immediate for rank-one matrix estimation, where multiple choices for parameters \(\) and \(\) lead to the same denoising error. This is reflected in Figure 4, where the learned denoisers at early iterations achieve the same MSE as the Bayes optimal denoisers, but the functional forms are completely different. We leave the extension of our compressed sensing results to rank-one matrix estimation as an open question.

Finally, while our experiments suggest that including auxiliary trainable parameters like the "B matrix" offers significant performance advantages once one departs from the asymptotic, Gaussian design setting in which Bayes AMP is believed to be optimal, these are not yet supported by theory. It is an intriguing open question whether one can use some of the insights from the aforementioned representational results for ISTA to rigorously characterize the "non-asymptotic corrections" that these extra learnable parameters are imposing.