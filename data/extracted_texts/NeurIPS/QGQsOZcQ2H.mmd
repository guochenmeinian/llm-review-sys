# Neural Oscillators are Universal

Samuel Lanthaler

California Institute of Technology

slanth@caltech.edu

&T. Konstantin Rusch

ETH Zurich

&Siddhartha Mishra

ETH Zurich

###### Abstract

Coupled oscillators are being increasingly used as the basis of machine learning (ML) architectures, for instance in sequence modeling, graph representation learning and in physical neural networks that are used in analog ML devices. We introduce an abstract class of _neural oscillators_ that encompasses these architectures and prove that neural oscillators are universal, i.e, they can approximate any continuous and casual operator mapping between time-varying functions, to desired accuracy. This universality result provides theoretical justification for the use of oscillator based ML systems. The proof builds on a fundamental result of independent interest, which shows that a combination of forced harmonic oscillators with a nonlinear read-out suffices to approximate the underlying operators.

## 1 Introduction

Oscillators are ubiquitous in the sciences and engineering . Prototypical examples include pendulums in mechanics, feedback and relaxation oscillators in electronics, business cycles in economics and heart beat and circadian rhythms in biology. Particularly relevant to our context is the fact that the neurons in our brain can be thought of as oscillators on account of the periodic spiking and firing of the action potential . Consequently, functional brain circuits such as cortical columns are being increasingly analyzed in terms of networks of coupled oscillators .

Given this wide prevalence of (networks of) oscillators in nature and man-made devices, it is not surprising that oscillators have inspired various machine learning architectures in recent years. Prominent examples include the _CoRNN_ and _UnICORNN_ recurrent neural networks for sequence modeling. CoRNN is based on a network of coupled, forced and damped oscillators, whereas UnICORNN is a multi-layer sequence model that stacks networks of independent undamped oscillators as hidden layers within an RNN. Both these architectures were rigorously shown to mitigate the exploding and vanishing gradient problem  that plagues RNNs. Hence, both CoRNN and UnICORNN performed very well on sequence learning tasks with _long-term dependencies_. Another example of the use of oscillators in machine learning is provided by _GraphCON_, a framework for designing graph neural networks (GNNs) , that is based on coupled oscillators. GraphCON was also shown to ameliorate the _oversmoothing_ problem  and allow for the deployment of multi-layer deep GNNs. Other examples include Second Order Neural ODEs (SONODEs) , which can be interpreted as oscillatory neural ODEs, locally coupled oscillatory recurrent networks (LocoRNN) , and Oscillatory Fourier Neural Network (O-FNN) .

Another avenue where ML models based on oscillators arise is that of _physical neural networks_ (PNNs)  i.e., physical devices that perform machine learning on analog (beyond digital) systems. Such analog systems have been proposed as alternatives or accelerators to the current paradigm of machine learning on conventional electronics, allowing us to significantly reduce the prohibitive energy costs of training state-of-the-art ML models. In , the authors propose a variety of physical neural networks which include a mechanical network of multi-mode oscillations on a plate and electronic circuits of oscillators as well as a network of nonlinear oscillators. Coupled with a novel _physics aware training_ (PAT) algorithm, the authors of  demonstrated that theirnonlinear oscillatory PNN achieved very good performance on challenging benchmarks such as Fashion-MNIST . Moreover, other oscillatory systems such as coupled lasers and spintronic nano-oscillators have also been proposed as possible PNNs, see  as an example of the use of thermally coupled vanadium dioxide oscillators for image recognition and [24; 32] for the use of spin-torque nano-oscillators for speech recognition and for neuromorphic computing, respectively.

What is the rationale behind the successful use of (networks of) oscillators in many different contexts in machine learning? The authors of  attribute it to the inherent stability of oscillatory dynamics, as the state (and its gradients) of an oscillatory system remain within reasonable bounds throughout the time-evolution of the system. However, this is at best a partial explanation, as it does not demonstrate why oscillatory dynamics can learn (approximate) mappings between inputs and outputs rather than bias the learned states towards oscillatory functions. As an example, consider the problem of classification of MNIST  (or Fashion-MNIST) images. It is completely unclear if the inputs (vectors of pixel values), outputs (class probabilities) and the underlying mapping possess any (periodic) oscillatory structure. Consequently, how can oscillatory RNNs (such an CoRNN and UnICORNN) or a network of oscillatory PNNs learn the underlying mapping?

Our main aim in this paper is to provide an answer to this very question on the ability of neural networks, based on oscillators, to express (to approximate) arbitrary mappings. To this end,

* We introduce an abstract framework of _neural oscillators_ that encompasses both sequence models such as CoRNN and UnICORNN, as well as variants of physical neural networks as the ones proposed in . These neural oscillators are defined in terms of second-order versions of _neural ODEs_, and combine _nonlinear dynamics_ with a _linear read-out_.
* We prove a _Universality_ theorem for neural oscillators by showing that they can approximate, to any given tolerance, continuous operators between appropriate function spaces.
* Our proof of universality is based on a novel theoretical result of independent interest, termed the _fundamental Lemma_, which implies that a suitable combination of _linear oscillator dynamics_ with _nonlinear read-out_ suffices for universality.

Such universality results, [1; 5; 14; 21] and references therein, have underpinned the widespread use of traditional neural networks (such as multi-layer perceptrons and convolutional neural networks). Hence, our universality result establishes a firm mathematical foundation for the deployment of neural networks, based on oscillators, in myriad applications. Moreover, our constructive proof provides insight into how networks of oscillators can approximate a large class of mappings.

## 2 Neural Oscillators

General Form of Neural Oscillators.Given \(u:[0,T]^{p}\) as an input signal, for any final time \(T_{+}\), we consider the following system of _neural_ ODEs for the evolution of dynamic hidden variables \(y^{m}\), coupled to a linear read-out to yield the output \(z^{q}\),

\[(t) =(Wy(t)+Vu(t)+b),\] (2.1a) \[y(0) =(0)=0,\] (2.1b) \[z(t) =Ay(t)+c.\] (2.1c)

Equation (2.1) defines an input-/output-mapping \(u(t) z(t)\), with time-dependent output \(z:[0,T]^{q}\). Specification of this system requires a choice of the hidden variable dimension \(m\) and the activation function \(\). The resulting mapping \(u z\) depends on tunable weight matrices \(W^{m m}\), \(V^{m p}\), \(A^{q m}\) and bias vectors \(b^{m}\), \(c^{q}\). For simplicity of the exposition, we consider only _activation functions \( C^{}()\), with \((0)=0\) and \(^{}(0)=1\)_, such as \(\) or \(\), although more general activation functions can be readily considered. This general second-order neural ODE system (2.1) will be referred to as a **neural oscillator**.

Multi-layer neural oscillators.As a special case of neural oscillators, we consider the following _much sparser_ class of second-order neural ODEs,

\[y^{0}(t) :=u(t),\] (2.2a) \[^{}(t) =(w^{} y^{}(t)+V^{}y^{-1}(t)+b^{ }),(=1,,L),\] (2.2b) \[y^{}(0) =^{}(0)=0,\] (2.2c) \[z(t) =Ay^{L}(t)+c.\] (2.2d)

In contrast to the general neural oscillator (2.1), the above multi-layer neural oscillator (2.2) defines a _hierarchical_ structure; The solution \(y^{}^{m_{}}\) at level \(\) solves a second-order ODE with driving force \(y^{-1}\), and the lowest level, \(y^{0}=u\), is the input signal. Here, the layer dimensions \(m_{1},,m_{L}\) can vary across layers, the weights \(w^{}^{m_{}}\) are given by vectors, with \(\) componentwise multiplication, \(V^{}^{m_{} m_{-1}}\) is a weight matrix, and \(b^{}^{m_{}}\) the bias. Given the result of the final layer, \(y^{L}\), the output signal is finally obtained by an affine output layer \(z(t)=Ay^{L}(t)+c\). In the multi-layer neural oscillator, the matrices \(V^{}\), \(A\) and vectors \(w^{}\), \(b^{}\) and \(c\) represent the trainable hidden parameters. The system (2.2) is a special case of (2.1), since it can be written in the form (2.1), with \(y:=[y^{L},y^{L-1},,y^{1}]^{T}\), \(b:=[b^{L},,b^{1}]^{T}\), and a (upper-diagonal) block-matrix structure for \(W\):

\[W:=(w^{L})&V^{L}&0&&0\\ 0&(w^{L-1})&V^{L-1}&&\\ &&&&0\\ 0&&0&(w^{2})&V^{2}\\ 0&&0&0&(w^{1}), V:=0\\ \\ \\ 0\\ V^{1}\] (2.3)

Given the block-diagonal structure of the underlying weight matrices, it is clear that the multi-layer neural oscillator (2.2) is a much sparser representation of the general neural operator (2.1). Moreover, one can observe from the structure of the neural ODE (2.2) that within each layer, the individual neurons are causally _independent_ of each other.

Assuming that \(w^{}_{i} 0\), for all \(1 i m_{}\) and all \(1 L\), we further highlight that the multi-layer neural oscillator (2.2) is a Hamiltonian system,

\[^{}=^{}},^{ }=-},\] (2.4)

with the _layer-wise time-dependent Hamiltonian_,

\[H(y^{},^{},t)=\|^{}\|^{2}-_{i=1}^{m_{ }}_{i}}(w^{}_{i}y^{}_{i}+(V^{ }y^{-1})_{i}+b^{}_{i}),\] (2.5)

with \(\) being the antiderivative of \(\), and \(\|\|^{2}=,\) denoting the Euclidean norm of the vector \(^{m}\) and \(,\) the corresponding inner product. Hence, any symplectic discretization of the multi-layer neural oscillator (2.2) will result in a fully reversible model, which can first be leveraged in the context of normalizing flows , and second leads to a memory-efficient training, as the intermediate states (i.e., \(y^{}(t_{0}),^{}(t_{0}),y_{}(t_{1}),_{}(t_{1}), ,y_{}(t_{N}),_{}(t_{N})\), for some time discretization \(t_{0},t_{1},,t_{N}\) of length \(N\)) do not need to be stored and can be reconstructed during the backward pass. This potentially leads to a drastic memory saving of \((N)\) during training.

### Examples of Neural Oscillators

(Forced) harmonic oscillator.Let \(p=m=q=1\) and we set \(W=-^{2}\), for some \(\), \(V=1,b=0\) and the activation function to be identity \((x)=x\). In this case, the neural ODE (2.1) reduces to the ODE modeling the dynamics of a forced simple harmonic oscillator  of the form,

\[=-^{2}y+u, y(0)=(0)=0.\] (2.6)

Here, \(y\) is the displacement of the oscillator, \(\) the frequency of oscillation and \(u\) is a forcing term that forces the motion of the oscillator. Note that (2.6) is also a particular example of the multi-layer oscillator (2.2) with \(L=1\).

This simple example provides justification for our terminology of neural oscillators, as in general, the hidden state \(y\) can be thought of as the vector of displacements of \(m\)-coupled oscillators, which are coupled together through the weight matrix \(W\) and are forced through a forcing term \(u\), whose effect is modulated via \(V\) and a bias term \(b\). The nonlinear activation function mediates possible nonlinear feedback to the system on account of large displacements.

CoRNN.The Coupled oscillatory RNN (CoRNN) architecture  is given by the neural ODE:

\[=(Wy++Vu+b)- y-.\]

We can recover the neural oscillator (2.1) as a special case of CoRNN by setting \(=,==0\); thus, a universality theorem for neural oscillators immediately implies a corresponding universality result for the CoRNN architecture.

UnICORNN.The Undamped Independent Controlled Oscillatory RNN (UnICORNN) architecture of [28, eqn. 1] recovers the multi-layer neural oscillator (2.2) in the case where the fundamental frequencies of UnICORNN are automatically determined inside the weight matrix \(W\) in (2.1).

Nonlinear oscillatory PNN of .In [35, SM, Sect. 4.A], the authors propose an analog machine learning device that simulates a network of nonlinear oscillators, for instance realized through coupled pendula. The resulting mathematical model is the so-called simplified Frenkel-Kontorova model  given by the ODE system,

\[M=-K()-C()+f,\]

where \(=(_{1},,_{N})\) is the vector of angles across all coupled pendula, \(M=(^{1},,^{N})\) is a diagonal mass matrix, \(f\) an external forcing, \(K=(k^{1},,k^{N})\) the "spring constant" for pendula, given by \(k^{i}=^{i}g/\) with \(\) the pendula length and \(g\) the gravitational acceleration, and where \(C=C^{T}\) is a symmetric matrix, with

\[C_{}=-_{^{}}C_{^{}},[C()]_{}=_{^{}}C_{^{}}(( _{^{}})-(_{})),\] (2.7)

which quantifies the coupling between different pendula. We note that this simplified Frenkel-Kontorova system can also model other coupled nonlinear oscillators, such as coupled lasers or spintronic oscillators .

We can bring the above system into a more familiar form by introducing the variable \(y\) according to the relationship \(Py=\) for a matrix \(P\). Substitution of this ansatz then yields \(MP=-(K+C)(Py)+f\); choosing \(P=M^{-1}(K+C)\), we find

\[=-(M^{-1}(K+C)y)+f,\] (2.8)

which can be written in the form \(=(Wy)+f\) for \(=-(\,\,)\) and \(W=M^{-1}(K+C)\). We next make a particular choice of \(C\), by choosing it in a block-matrix form

\[C:=^{L}I&C^{L}&0&&0\\ C^{L,T}&^{L-1}I&&&\\ 0&&&&0\\ &&C^{3,T}&^{2}I&C^{2}\\ 0&&0&C^{2,T}&^{1}I,\]

and with corresponding mass matrix \(M\) in block-matrix form \(M=(^{L}I,^{L-1}I,,^{1}I)\), then with \(^{}:=^{}/^{}\), we have

\[M^{-1}C:=^{L}I&C^{L}/^{L}&0&&0\\ C^{L,T}/^{L-1}&^{L-1}I&&&\\ 0&&&&0\\ &&&^{2}I&C^{2}/^{2}\\ 0&&0&C^{2,T}/^{1}&^{1}I,\]With the intent of ordering the masses of different layers, such that \(^{}/^{-1}\) is small, we now introduce an ordering parameter \(>0\). And we consider the following ordering \(^{},C^{},^{}^{}\).

Assuming this ordering, it follows that \(^{}\), \(}{^{}}=O(1)\), and \(}{^{-1}}=O()\). This ordering of the masses introduces an effective one-way coupling, making

\[M^{-1}C=^{L}I&V^{L}&0&&0\\ 0&^{L-1}I&V^{L-1}&&\\ &&&&0\\ 0&&0&^{2}I&V^{2}\\ 0&&0&0&^{1}I+O(),\]

upper triangular, up to small terms of order \(\). We note that the diagonal entries \(^{}\) in \(M^{-1}C\) are determined by the off-diagonal terms through the identity (2.7). The additional degrees of freedom in the (diagonal) \(K\)-matrix in (2.8) can be used to tune the diagonal weights of the resulting weight matrix \(W=M^{-1}(K+C)\).

Thus, physical systems such as the Frankel-Kontorova system of nonlinear oscillators can be approximated (to leading order) by multi-layer systems of the form

\[^{}=(w^{} y^{}+V^{}y^{-1}) +f^{},\] (2.9)

with \(f^{}\) an external forcing, representing a tunable linear transformation of the external input to the system. The only formal difference between (2.9) and (2.2) is (i) the absence of a bias term in (2.9) and (ii) the fact that the external forcing appears outside of the nonlinear activation function \(\) in (2.9). A bias term could readily be introduced by measuring the angles represented by \(y^{}\) in a suitably shifted reference frame; physically, this corresponds to tuning the initial position \(y^{}(0)\) of the pendula, with \(y^{}(0)\) also serving as the reference value. Furthermore, in our proof of universality for (2.2), it makes very little difference whether the external forcing \(f\) is applied inside the activation function, as in (2.2b) resp. (2.1a), or outside as in (2.9); indeed, the first layer in our proof of universality will in fact approximate the _linearized dynamics_ of (2.2b), i.e. a forced harmonic oscillator (2.6). Consequently, a universality result for the multi-layer neural oscillator (2.2) also implies universality of variants of nonlinear oscillator-based physical neural networks, such as those considered in .

## 3 Universality of Neural Oscillators

In this section, we state and sketch the proof for our main result regarding the universality of neural oscillators (2.1) or, more specifically, multi-layer oscillators (2.2). To this end, we start with some mathematical preliminaries to set the stage for the main theorem.

### Setting

Input signal.We want to approximate operators \(:u(u)\), where \(u=u(t)\) is a time-dependent input signal over a time-interval \(t[0,T]\), and \((u)(t)\) is a time-dependent output signal. We will assume that the input signal \(t u(t)\) is continuous, and that \(u(0)=0\). To this end, we introduce the space

\[C_{0}([0,T];^{p}):=\{u:[0,T]^{p}\,|\,t u(t) { is continuous and }u(0)=0\}.\]

We will assume that the underlying operator defines a mapping \(:C_{0}([0,T];^{p}) C_{0}([0,T];^{q})\).

The approximation we discuss in this work are based on oscillatory systems starting from rest. These oscillators are forced by the input signal \(u\). For such systems the assumption that \(u(0)=0\) is necessary, because the oscillator starting from rest takes a (arbitrarily small) time-interval to synchronize with the input signal (to "warm up"); If \(u(0) 0\), then the oscillator cannot accurately approximate the output during this warm-up phase. This intuitive fact is also implicit in our proofs. We will provide a further comment on this issue in Remark 3.2, below.

Operators of interest.We consider the approximation of an operator \(:C_{0}([0,T];^{p}) C_{0}([0,T];^{q})\), mapping a continuous input signal \(u(t)\) to a continuous output signal \((u)(t)\). We will restrict attention to the uniform approximation of \(\) over a compact set of input functions \(K C_{0}([0,T];^{p})\). We will assume that \(\) satisfies the following properties:* \(\) is **causal**: For any \(t[0,T]\), if \(u,v C_{0}([0,T];^{p})\) are two input signals, such that \(u|_{[0,t]} v|_{[0,t]}\), then \((u)(t)=(v)(t)\), i.e. the value of \((u)(t)\) at time \(t\) does not depend on future values \(\{u()\,|\,>t\}\).
* \(\) is **continuous** as an operator \[:(C_{0}([0,T];^{p}),\|\|_{L^{}})(C_{0}([0,T]; ^{q}),\|\|_{L^{}}),\] with respect to the \(L^{}\)-norm on the input-/output-signals.

Note that the class of Continuous and Causal operators are very general and natural in the contexts of mapping between sequence spaces or time-varying function spaces, see  and references therein.

### Universal approximation Theorem

The universality of neural oscillators is summarized in the following theorem:

**Theorem 3.1**.: [Universality of the multi-layer neural oscillator] Let \(:C_{0}([0,T];^{p}) C_{0}([0,T];^{q})\) be a causal and continuous operator. Let \(K C_{0}([0,T];^{p})\) be compact. Then for any \(>0\), there exist hyperparameters \(L\), \(m_{1},,m_{L}\), weights \(w^{}^{m_{}}\), \(V^{}^{m_{} m_{-1}}\), \(A^{q m_{L}}\) and bias vectors \(b^{}^{m_{}}\), \(c^{q}\), for \(=1,,L\), such that the output \(z:[0,T]^{q}\) of the multi-layer neural oscillator (2.2) satisfies

\[_{t[0,T]}|(u)(t)-z(t)|,\,u K.\]

It is important to observe that the _sparse, independent_ multi-layer neural oscillator (2.2) suffices for universality in the considered class. Thus, there is no need to consider the wider class of neural oscillators (2.1), at least in this respect. We remark in passing that Theorem 3.1 immediately implies another universality result for neural oscillators, showing that they can also be used to approximate arbitrary continuous functions \(F:^{p}^{q}\). This extension is explained in detail in **SM** A.

**Remark 3.2**.: We note that the theorem can be readily extended to remove the requirement on \(u(0)=0\) and \((u)(0)=0\). To this end, let \(:C([0,T];^{p}) C([0,T];^{q})\) be an operator between spaces of continuous functions, \(u(u)\) on \([0,T]\). Fix a \(t_{0}>0\), and extend any input function \(u:[0,T]^{p}\) to a function \((u) C_{0}([-t_{0},T];^{p})\), by

\[(u)(t):=+t)}{t_{0}}u(0),&t[-t_{0},0),\\ u(t),&t[0,T].\]

Our proof of Theorem 3.1 can readily be used to show that the oscillator system with forcing \((u)\), and initialized at time \(-t_{0}<0\), can uniformly approximate \((u)\) over the entire time interval \([0,T]\), without requiring that \(u(0)=0\), or \((u)(0)=0\). In this case, the initial time interval \([-t_{0},0]\) provides the required "warm-up phase" for the neural oscillator.

**Remark 3.3**.: The required compactness property of the set of input signals \(K C_{0}([0,T];^{p})\) in Theorem 3.1 is implied by a suitable smoothness constraint; examples include uniform bounds on the Lipschitz norm, Holder norms, uniform bounds on higher-order derivatives, or under the assumption that the input signals are uniformly bounded and band-limited. The proposed compactness assumption subsumes all of these smoothness constraints, resulting in a widely applicable universal approximation theorem.

**Remark 3.4**.: In practice, neural ODEs such as (2.2) need to be discretized via suitable numerical schemes. As examples, CoRNN and UnICORNN were implemented in  and , respectively, with implicit-explicit time discretizations. Nevertheless, universality also applies for such discretizations as long as the time-step is small enough, as the underlying discretization is going to be a sufficiently accurate approximation of (2.2) and Theorem 3.1 can be used for showing universality of the discretized version of the multi-layer neural oscillator (2.2).

### Outline of the Proof

In the following, we outline the proof of the universality Theorem 3.1, while postponing the technical details to the **SM**. For a given tolerance \(\), we will explicitly construct the weights and biases of the multi-layer neural oscillator (2.2) such that the underlying operator can be approximated within the given tolerance. This construction takes place in the following steps:

(Forced) Harmonic Oscillators compute a time-windowed sine transform.Recall that the forced harmonic oscillator (2.6) is the simplest example of a neural oscillator (2.1). The following lemma, proved by direct calculation in **SM B.1**, shows that this forced harmonic oscillator actually computes a time-windowed variant of the sine transform at the corresponding frequency:

_Lemma 3.5_.: Assume that \( 0\). Then the solution of (2.6) is given by

\[y(t)=_{0}^{t}u(t-)()\,d.\] (3.1)

Given the last result, for a function \(u\), we define its **time-windowed sine transform** as follows,

\[_{t}u():=_{0}^{t}u(t-)()\,d.\] (3.2)

Lemma 3.5 shows that a forced harmonic oscillator computes (3.2) up to a constant.

Approximation of causal operators from finite realizations of time-windowed sine transforms.The following novel result, termed the _fundamental Lemma_, shows that the time-windowed sine transform (3.2) composed with a suitable nonlinear function can approximate causal operators \(\) to desired accuracy; as a consequence, one can conclude that _forced harmonic oscillators_ combined with a _nonlinear read-out_ defines a universal architecture in the sense of Theorem 3.1.

_Lemma 3.6_ (Fundamental Lemma).: Let \(:K C_{0}([0,T];^{p}) C_{0}([0,T];^{q})\) be a causal and continuous operator, with \(K C_{0}([0,T];^{p})\) compact. Then for any \(>0\), there exists \(N\), frequencies \(_{1},,_{N}\) and a continuous mapping \(:^{p N}[0,T^{2}/4]^{q}\), such that

\[|(u)(t)-(_{t}u(_{1}),,_{t}u(_ {N});t^{2}/4)|,\]

for all \(u K\) and \(t[0,T]\).

The function \(\) defined in Lemma 3.6 can be interpreted as a finite-dimensional encoding of the operator \(\). The main insight of the fundamental Lemma 3.6 is that the (infinite-dimensional) operator \(\) can effectively be replaced by a (finite-dimensional) function \(\), making the approximation by neural oscillators more tractable.

The proof of the fundamental Lemma 3.6, detailed in **SM** B.2, is based on first showing that any continuous function can be reconstructed to desired accuracy, in terms of realizations of its time-windowed sine transform (3.2) at finitely many frequencies \(_{1},,_{N}\) (see **SM** Lemma B.1). Then, we leverage the continuity of the underlying operator \(\) to approximate it with a finite-dimensional function \(\), which takes the time-windowed sine transforms as its arguments.

Given these two results, we can discern a clear strategy to prove the universality Theorem 3.1. First, we will show that a general _nonlinear_ form of the neural oscillator (2.2) can also compute the time-windowed sine transform at arbitrary frequencies. Then, these outputs need to be processed in order to apply the fundamental Lemma 3.6 and approximate the underlying operator \(\). To this end, we will also approximate the function \(\) (mapping finite-dimensional inputs to finite-dimensional outputs) by oscillatory layers. The concrete steps in this strategy are outlined below.

Figure 1: Illustration of the universal 3-layer neural oscillator architecture constructed in the proof of Theorem 3.1.

Nonlinear Oscillators approximate the time-windowed sine transform.The building block of multi-layer neural oscillators (2.2) is the nonlinear oscillator of the form,

\[=(w y+Vu+b).\] (3.3)

In the following Lemma (proved in **SM** B.3), we show that even for a nonlinear activation function \(\) such as \(\) or \(\), the nonlinear oscillator (3.3) can approximate the time-windowed sine transform.

**Lemma 3.7**.: Fix \( 0\). Assume that \((0)=0\), \(^{}(0)=1\). For any \(>0\), there exist \(w,V,b,A\), such that the nonlinear oscillator (3.3), initialized at \(y(0)=(0)=0\), has output

\[|Ay(t)-_{t}u()|, u K,\ t[0,T],\]

with \(_{t}u()\) being the time-windowed sine transform (3.2).

Coupled Nonlinear Oscillators approximate time-delays.The next step in the proof is to show that coupled oscillators can approximate time-delays in the continuous input signal. This fact will be of crucial importance in subsequent arguments. We have the following Lemma (proved in **SM** B.4),

**Lemma 3.8**.: Let \(K C_{0}([0,T];^{p})\) be a compact subset. For every \(>0\), and \( t 0\), there exist \(m\), \(w^{m}\), \(V^{m p}\), \(b^{m}\) and \(A^{p m}\), such that the oscillator (3.3), initialized at \(y(0)=(0)=0\), has output

\[_{t[0,T]}|u(t- t)-Ay(t)|,\,u K,\]

where \(u(t)\) is extended to negative values \(t<0\) by zero.

Two-layer neural oscillators approximate neural networks pointwise.As in the strategy outlined above, the final ingredient in our proof of the universality theorem 3.1 is to show that neural oscillators can approximate continuous functions, such as the \(\) in the fundamental lemma 3.6, to desired accuracy. To this end, we will first show that neural oscillators can approximate general neural networks (perceptrons) and then use the universality of neural networks in the class of continuous functions to prove the desired result. The main difficulty here is that, due to the underlying oscillatory dynamics, it is not clear whether the output of even a shallow neural network can be reproduced by neural oscillators. The following lemma, visually represented by the bottom half of Figure 1, guarantees this:

**Lemma 3.9**.: Let \(K C_{0}([0,T];^{p})\) be compact. For matrices \(,\) and bias \(\), and any \(>0\), there exists a two-layer (\(L=2\)) oscillator (2.2), initialized at \(y^{}(0)=^{}(0)=0\), \(=1,2\), such that

\[_{t[0,T]}\,|[Ay^{2}(t)+c]-( u(t)+ )|, u K.\]

The proof, detailed in **SM** B.5, is constructive and the neural oscillator that we construct has two layers. The first layer just processes a nonlinear input function through a nonlinear oscillator and the second layer, approximates the second-derivative (in time) from time-delayed versions of the input signal that were constructed in Lemma 3.8.

Combining the ingredients to prove the universality theorem 3.1.The afore-constructed ingredients are combined in **SM** B.6 to prove the universality theorem. In this proof, we explicitly construct a _three-layer_ neural oscillator (2.2) which approximates the underlying operator \(\). The first layer follows the construction of Lemma 3.7, to approximate the time-windowed sine transform (3.2), for as many frequencies as are required in the fundamental Lemma 3.6. The second- and third-layers imitate the construction of Lemma 3.9 to approximate a neural network (perception), which in turn by the universal approximation of neural networks, approximates the function \(\) in Lemma 3.6 to desired accuracy. Putting the network together leads to a three-layer oscillator that approximates the continuous and casual operator \(\). This construction is depicted in Figure 1.

Discussion

Machine learning architectures, based on networks of coupled oscillators, for instance sequence models such as CoRNN  and UnICORNN , graph neural networks such as GraphCON  and increasingly, the so-called physical neural networks (PNNs) such as linear and nonlinear mechanical oscillators  and spintronic oscillators , are being increasingly used. A priori, it is unclear why ML systems based on oscillators can provide competitive performance on a variety of learning benchmarks, e.g. , rather than biasing their outputs towards oscillatory functions. In order to address these concerns about their expressivity, we have investigated the theoretical properties of machine learning systems based on oscillators. Our main aim was to answer a fundamental question: _"are coupled oscillator based machine learning architectures universal?"_. In other words, can these architectures, in principle, approximate a large class of input-output maps to desired accuracy.

To answer this fundamental question, we introduced an abstract framework of _neural oscillators_ (2.1) and its particular instantiation, the _multi-layer neural oscillators_ (2.2). This abstract class of _second-order neural ODEs_ encompasses both sequence models such as CoRNN and UnICORNN, as well as a very general and representative PNN, based on the so-called Frenkel-Kontorova model. The main contribution of this paper was to prove the universality theorem 3.1 on the ability of multi-layer neural oscillators (2.2) to approximate a large class of operators, namely causal and continuous maps between spaces of continuous functions, to desired accuracy. Despite the fact that the considered neural oscillators possess a very specific and constrained structure, not even encompassing general Hamiltonian systems, the approximated class of operators is nevertheless very general, including solution operators of general ordinary and even time-delay differential equations.

The crucial theoretical ingredient in our proof was the _fundamental Lemma_ 3.6, which implies that linear oscillator dynamics combined with a pointwise nonlinear read-out suffices for universal operator approximation; our construction can correspondingly be thought of as a large number of linear processors, coupled with nonlinear readouts. This construction could have implications for other models such as _structured state space models_ which follow a similar paradigm, and the extension of our universality results to such models could be of great interest.

Our universality result has many interesting implications. To start with, we rigorously prove that an ML architecture based on coupled oscillators can approximate a very large class of operators. This provides theoretical support to many widely used sequence models and PNNs based on oscillators. Moreover, given the generality of our result, we hope that such a universality result can spur the design of innovative architectures based on oscillators, particularly in the realm of analog devices as ML inference systems or ML accelerators .

It is also instructive to lay out some of the limitations of the current article and point to avenues for future work. In this context, our setup currently only considers time-varying functions as inputs and outputs. Roughly speaking, these inputs and outputs have the structure of (infinite) sequences. However, a large class of learning tasks can be reconfigured to take sequential inputs and outputs. These include text (as evident from the tremendous success of large language models ), DNA sequences, images , timeseries and (offline) reinforcement learning . Nevertheless, a next step would be to extend such universality results to inputs (and outputs) which have some spatial or relational structure, for instance by considering functions which have a spatial dependence or which are defined on graphs. On the other hand, the class of operators that we consider, i.e., casual and continuous, is not only natural in this setting but very general .

Another limitation lies in the _feed forward_ structure of the multi-layer neural oscillator (2.2). As mentioned before, most physical (and neurobiological) systems exhibit feedback loops between their constituents. However, this is not common in ML systems. In fact, we had to use a _mass ordering_ in the Frenkel-Kontorova system of coupled pendula (2.8) in order to recast it in the form of the multi-layer neural oscillator (2.2). Such asymptotic ordering may not be possible for arbitrary physical neural networks. Exploring how such ordering mechanisms might arise in physical and biological systems in order to effectively give rise to a feed forward system could be very interesting. One possible mechanism for coupled oscillators that can lead to a hierarchical structure is that of synchronization  and references therein. How such synchronization interacts with universality is a very interesting question and will serve as an avenue for future work.

Finally, universality is arguably necessary but far from sufficient to analyze the performance of any ML architecture. Other aspects such as trainability and generalization are equally important, and we do not address these issues here. We do mention that trainability of oscillatory systems would profit from the fact that oscillatory dynamics is (gradient) stable and this formed the basis of the proofs of mitigation of the exploding and vanishing gradient problem for CoRNN in  and UnICORNN in  as well as GraphCON in . Extending these results to the general second-order neural ODE (2.2), for instance through an analysis of the associated adjoint system, is left for future work.