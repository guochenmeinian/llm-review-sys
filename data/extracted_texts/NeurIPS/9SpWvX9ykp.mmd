# Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search

Nicola Dainese

Department of Computer Science

Aalto University

nicola.dainese@aalto.fi

&Matteo Merler

Department of Computer Science

Aalto University

matteo.merler@aalto.fi

&Minttu Alakuijala

Department of Computer Science

Aalto University

minttu.alakuijala@aalto.fi

&Pekka Marttinen

Department of Computer Science

Aalto University

pekka.marttinen@aalto.fi

Asterisk indicates equal contribution.

###### Abstract

In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has potential to be more precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.

## 1 Introduction

The ability to model the world is essential for goal-oriented intelligent agents . When faced with a novel environment, the agent must quickly understand its mechanics to achieve its goal, for example by building an internal representation of the world and planning with it. In this context, natural language conditioning can be useful for grounding current observations in past knowledge and improving the agent's understanding of the world. Therefore, communicating information about a new task to the agent in natural language is particularly promising, and multiple works explore instruction-following agents . However, not all important information can be communicated in the form of imperative instructions. Many key facts required to solve a task involve understanding observations, predicting outcomes of different actions and determining whether those outcomes align with the agent's goals. Thus, systems capable of leveraging additional descriptive information, such as model-based Reinforcement Learning (RL) agents, have a greater potential for fast and efficient adaptation via natural language .

Large Language Models (LLMs) have revolutionized the field of Natural Language Processing, and offer great opportunities for world modeling, thanks to their internet-scale knowledge, reasoning, and instruction-following abilities. However, it is not clear how to best combine LLMs and world models. One option is multi-modal systems such as text-to-video models (Gupta et al., 2023), which present the highest prediction fidelity, language understanding and out-of-distribution generalization for generation tasks, yet they are too slow to be called repeatedly in a planning loop due to their high inference cost. On the other hand, language-conditioned model-based RL agents (Dainese et al., 2023; Lin et al., 2024) are typically fast at planning and easily trainable. However, they cannot conveniently incorporate LLMs because of their specialised architectures and as such have poor language understanding and generalization capabilities. Other works, such as (Hao et al., 2023), perform planning using an LLM as a world model directly, but they are slow for inference and restricted to textual inputs and outputs, limiting their applicability in RL.

In this study we propose to model the world with code, rather than directly predicting the future with an LLM, which is known to be costly, slow and unreliable. In contrast, code is precise, fast, reliable and interpretable. We thus introduce Code World Models (CWMs), a novel approach to generate RL world models by writing Python code with an LLM, for which a high-level overview can be seen in Figure 1. The concept of CWMs has been independently and contemporaneously proposed by Tang et al. (2024); however, our method is technically distinct (Section 2) and scales to more complex world models (Section 5). Alongside this paradigm, we introduce the Code World Models Benchmark (CWMB), consisting of 18 diverse RL environments for discrete and continuous control, paired with corresponding natural language descriptions and curated trajectories. This benchmark aims to facilitate the accurate synthesis of Code World Models through learning from the provided data and evaluate different code generation methods across environments of varying complexity.

Synthesizing programs for world models requires complex reasoning, precise instruction following, accurate implementation of the environment dynamics and reward functions, as well as coding skills for debugging and refining long programs using unit tests. To meet these challenges we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation method based on Monte Carlo Tree Search (MCTS, Kocsis and Szepesvari (2006)) for LLMs, especially suited for generating Code World Models.2 We evaluate the performance of our method on three benchmarks: the new CWMB, the _Competition_ split on APPS (Hendrycks et al., 2021), a popular and challenging coding benchmark, and RTFM (Zhong et al., 2020), a language-conditioned grid-world, showcasing environments with varying characteristics and complexity. GIF-MCTS

Figure 1: Overview of the Code World Models (CWM) framework. Given the description of an environment and a task, we use an LLM guided by the GIF-MCTS method to iteratively generate and refine a candidate CWM. The candidate’s correctness is evaluated by checking if it correctly predicts a set of trajectories collected from the true environment. If the model cannot fully predict all transitions, the fraction of correct predictions and other information are given as feedback to the LLM and the cycle repeats. After matching all transitions or having used up a computational budget, the best CWM is returned and used to solve the task via model-based planning.

outperforms existing methods on all three benchmarks. Moreover, we demonstrate successful planning in several environments using the synthesized CWMs. This results in model-based RL agents with exceptional sample efficiency and inference speed (from four to six orders of magnitude faster compared to directly querying an LLM as a world model, as shown in Appendix H), while, provided the CWM is accurate, matching the performance of an oracle planner with access to the real-world model. Finally, we discuss the limitations and challenges to overcome to make Code World Models more broadly applicable.

## 2 Related Work

**World models with code.** Code is a promising choice for predictive world models thanks to its fast inference, exact syntax and interpretable behavior. However, code alone often struggles to cover the entire scope of the environment's dynamics and previous works often uses different techniques to build a full world model. AutumnSynth (Das et al., 2021) uses a custom programming language named Autumn and integrates a functional synthesis step with a synthesized finite-state automata to model any latent variable. Another popular choice is the Planning Domain Definition Language (PDDL) (Ghallab et al., 1998), which expresses actions as a set of preconditions and effects on the environment. However, PDDL approaches, as in the works by Guan et al. (2023) and Wong et al. (2024), are reliant on having access to predicates about the environment and plan in terms of high-level language actions, which need a low-level language-conditioned controller to be carried out. LLMs have also been used to generate a model based on probabilistic code (Wong et al., 2023).

Most similar to our approach, the concurrently proposed WorldCoder3(Tang et al., 2024) also leverages LLMs to generate a Python-based world model. WorldCoder chooses a program to refine from a working set of programs using the classical Thompson Sampling bandit algorithm (Thompson, 1933; Katehakis and Veinott, 1987), informed by a Beta prior, to iteratively learn a world model from gathered experience. Tang et al. focus on learning world models from online interactions with the environment in two grid-world tasks and on transferring knowledge across variants of the same task. We instead consider a broader selection of environments, propose to learn from offline data, and handle continuous state and action spaces in addition to discrete worlds. Furthermore, we rigorously benchmark and ablate our code generation method, GIF-MCTS, achieving state-of-the-art results on the _Competition_ split of the APPS coding benchmark, and obtain superior or on par performance to WorldCoder on CWMB.

**Code generation with LLMs.** Current state-of-the-art code generation methods all employ LLMs. While improvements to this task can come from both advancements in the LLMs' coding abilities and enhancements in prompting strategies to guide LLM decoding, the latter is the most relevant to our work. A host of prompting techniques have shown how to leverage the In-Context Learning (ICL) (Brown et al., 2020) abilities of LLMs to enhance a model's reasoning skills, and, as a result, the quality of generated programs. Perhaps the most influential of these is Chain of Thought (CoT) (Wei et al., 2022; Kojima et al., 2022), which leverages in-context examples to encourage intermediate reasoning steps. Tree-like approaches based on the CoT method have also been presented (Yao et al., 2023; Hao et al., 2023). The work by Zhang et al. (2023) proposes to guide the LLM generation with an MCTS method based on the feedback from unit tests. However, the method considers every token decoded by the LLM as an action in the MCTS tree, which becomes impractical when we have hundreds of tokens per program.

Most similar to our method, LATS (Zhou et al., 2023) uses an MCTS-based generation strategy that incorporates both self-reflection (Madaan et al., 2023; Shinn et al., 2023; Gou et al., 2024) and feedback from the environment. While LATS is broadly applicable to reasoning tasks, it has limitations in code-specific applications like ours. For instance, it generates \(n\) programs simultaneously from the same node, rather than sequentially, which does not fully exploit the sequential nature of MCTS. Additionally, it uses a separate prompt to reflect on incorrect code predictions, whereas we integrate self-reflection within the generation prompt. Furthermore, LATS lacks specialized prompts and strategies for fixing buggy programs.

Previous research has also focused on pseudocode-based reasoning, such as Parsel (Zelikman et al., 2023), which uses a custom pseudocode language to decompose the program into independent problems that can be solved separately. In contrast, we focus on the sequential refinement of solutions using a variant of MCTS and the environment's feedback to produce directly executable Python code that can be leveraged in model-based RL.

We refer the reader to Appendix G for further discussion on works that build language-conditioned world models but do not use code and on works that use programs as policies in RL.

## 3 Code World Models

In this Section, we first introduce the Code World Models framework and then the proposed Code World Models Benchmark.

**Code World Models framework.** Following the model-based Reinforcement Learning problem setting, we consider an environment represented by a Markov Decision Process with state space \(\), action space \(\), a transition function \(p(s^{}|a,s)\), and a scalar reward function \(R(s,a,s^{})\), with \(s,s^{}\) indicating respectively the current and next state, and \(a\) being the action taken from the current state. The task of a world model is to accurately represent \(p\) and \(R\). We make the following assumptions: 1) the environments are deterministic and fully observable, and 2) we are provided with a natural language description of the environment, which is detailed enough to infer the observation space as well as the logic of the transition and reward functions.

The first assumption implies a deterministic transition function \(s^{}=f(s,a)\), rather than a probabilistic one as in the general case; we address this limitation in Section 6.1. The second assumption is akin to the situation where a human would be provided with an explanation, or a tutorial, about a task that they need to solve, in order to facilitate the learning process. Crucially, in a model-based scenario, we only need explanations about how the environment works, rather than requiring instructions about what to do in order to solve the task. Furthermore, we place ourselves in an offline RL scenario (Levine et al., 2020), assuming that a dataset \(\) of \(n\) one time-step transitions \(\{(s,a,r,s^{},d)_{i}\}_{i=1,,n}\), where \(d\) stands for the episode termination or _done_ signal, is available, collected with some behavioural policy \(_{B}(a|s)\) in the environment of interest. However, this last assumption could be lifted, by using the Code World Model with a suitable planning algorithm to collect more trajectories from the environment, turning the algorithm into online RL, as done in Tang et al. (2024).

**Code World Models Benchmark.** To comprehensively test world model generation for a variety of environments, we define a novel benchmark consisting of 18 RL environments of varying difficulty. We focus on commonly used environments of particular relevance to the RL community: classical control, physics-based PyGame environments and MuJoCo tasks. The environments' Python implementations as well as their documentation are adapted from the Gymnasium library (Towers et al., 2024). The environments included in the resulting Code World Models Benchmark (CWMB) feature a mix of continuous and discrete action and observation spaces (more details in Appendix I).

For each environment, we collect a training dataset \(\) of past trajectories. We curate \(\) so that it includes at least some low-scoring and some relatively high-scoring behavior. However, we neither attempt to maximally cover the state space nor do we require optimal demonstrations. We aim to show that relatively low annotation effort is required to build CWMs: for the majority of environments, we collect just 5 trajectories equivalent to taking random actions and a further 5 suboptimal demonstrations exceeding some return threshold. As part of the benchmark, each transition \((s,a,r,s^{},d)\) in each resulting trajectory is used as an input-output sample to validate the generated models. The benchmark further includes a language description of each environment, derived from the documentation written for Gymnasium's end users (an example is included in Appendix N.3). A further discussion on how the quality of the collected dataset affects the performance of our method can be found in Appendix F.

## 4 Gif-Mcts

In this Section, we first specify the format of the Code World Models that we consider in this work and how we evaluate their accuracy. We then present Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a novel approach to leverage LLMs for code generation via multiple sequential attempts in the presence of feedback, specifically tailored to the needs of building Code World Models.

We formulate the task of synthesizing a Code World Model as that of writing a Python Environment class with a step() function that jointly implements the transition and reward functions:

\[(^{},,)=.(s,a),\] (1)

and consider a Code World Model correctly synthesized if it correctly reproduces all transitions in \(\). We additionally define the accuracy \(A\) of the Code World Model as the fraction of correctly predicted transitions (weighted uniformly on next state, reward and done signals) from the training dataset \(\), or in other words:

\[A=_{i=1}^{N}([s^{}_{i},^ {}_{i}]+[r_{i},_{i}]+[d _{i},_{i}]),\] (2)

where \(\) is the indicator function (equals to one if the pair is matching, zero otherwise) and \(^{}_{i}\), \(_{i}\) and \(_{i}\) are the model's predictions.

GIF-MCTS takes as input the description of an environment, an LLM, environment trajectories and builds a tree to construct the code for the environment. Nodes in the tree are programs and edges are actions. Each action taken from a parent node produces a new complete program, which is split into a _state_ part and a _rollout_ part and stored in a child node. The child node's _state_ is formed from the parent's state by appending \(L\) additional lines of code (we set \(L=2\) in our work), while the rollout is the remaining part of the program, and represents one possible completion of the state, needed to evaluate (i.e., run) the code. This is a novel formulation of the state of a node, as we store

Figure 2: Example of a GIF-MCTS tree for generating a CWM. Starting from the root of the tree, every action taken corresponds to 1) prompting the LLM to either generate, improve or fix a CWM, 2) parsing the LLM completion, and 3) evaluating the CWM’s correctness using the available environment trajectories as unit tests (presented as a percentage inside the nodes). On buggy nodes, we allow only fix actions for up to \(f\) sequential attempts and replace the actual value with a temporary one, represented in red. In healthy nodes we allow only generate and improve actions. All action prompts are exemplified on the right. The number of total fix \(f\) attempts is a model hyperparameter, set to three in this Figure and for our method.

in the states partial programs in blocks of multiple lines, whereas previous work either stores only full programs (Zhou et al., 2023), or single tokens (Zhang et al., 2023). The state represents the main flow of information from parent to child, while the rollout is used to estimate the expected accuracy of the child's state.

As in the standard MCTS algorithm, we perform multiple sequential iterations consisting of the following phases: selection, expansion, evaluation and value backpropagation. During the selection phase, starting from the root node, we use the Upper Confidence Bound for Trees (UCT) formula (Kocsis and Szepesvari, 2006) to select which action to take. If the corresponding node has never been expanded, we enter the expansion phase, otherwise we continue to apply the UCT formula to the actions of the new node. At expansion phase, we call the LLM to produce a program according to the type of action selected, parse the resulting program into the state and the rollout parts, and store both in the newly expanded node. We then compute the accuracy, defined above, using the rollout (evaluation phase), store the resulting value in the node, and backpropagate it to its ancestors. An example of a GIF-MCTS tree and the corresponding actions can be found in Figure 2.

With GIF-MCTS, we make the following contributions: 1) we present a novel framing of MCTS nodes and actions for long-form code generation in the presence of unit tests, 2) we propose three action types, specialised for code, whose added value we demonstrate through an ablation study, and 3) we propose a heuristic that empirically improves the trade-off between exploration and exploitation in the UCT formula used for action selection, balancing both explored and unexplored actions, and different action types (Appendix B). All these factors make GIF-MCTS specifically suitable for generating world models. Next we present the three action types (generate new lines, improve predictions and fix bugs) used in GIF-MCTS. We point the reader to the Appendix for the full action prompts, the remaining implementation details, and for the ablation study on the importance of the three action types.

### GIF-MCTS Actions

**Generate new lines.** The goal of the _generate_ action is to leverage the stochastic sampling ability of the LLM by generating varying continuations for a single code snippet in different branches of the tree, to fully explore the underlying space of possible solutions. The action prompt asks the LLM to generate the full code required to solve the task starting from the code stored in the node's _state_.

**Improve predictions.** Generating code in sequential blocks of lines can be too rigid if subtle or interdependent changes need to be made to the full program in order to pass more test cases and increase the reward. With the _improve_ action, the LLM is prompted with the full program (_state_ plus _rollout_) from the parent node, as well as one input example where the code did not behave as intended, along with the expected output. In the case of a Code World Model, this can be a wrongly predicted transition, with the input state and action taken by the agent, the ground-truth next state, and the model's predicted next state. The _improve_ prompt also asks the LLM to produce a Chain-of-Thought explanation about where the current code is failing, and to attempt to fix the logic. The inclusion of both _generate_ and _improve_ actions allows GIF-MCTS to combine the advantages of block-wise incremental generation with the flexibility to backtrack and edit the whole program if needed.

**Fix bugs.** The code obtained with a _generate_ or _improve_ action will sometimes not be able to execute due to a syntax or runtime error, and will thus receive a reward of 0, strongly discouraging further exploration of the node. This can be wasteful, as sometimes the newly generated program can have sound logic and would receive a good reward if its bug(s) were removed. The _fix_ action is tasked with resolving these bugs: the model is given the full program from the parent that encountered a bug along with feedback about the error and is asked to produce a fixed version of the code, aided by a Chain-of-Thought reasoning structure. To ensure that buggy nodes are chosen by the UCT formula, we assign them with temporary value until either the bug is fixed or no more attempts are allowed (see Appendix B for additional details).

## 5 Experiments

In this Section, we first describe the baseline code generation methods we compare against and then present empirical results on the APPS benchmark, the proposed CWMB and perform an additional study on the RTFM environment. Additional ablations and qualitative results on GIF-MCTS are presented in Appendices C and D.

### Baselines

The first baseline, denoted as **Zero-shot CoT** and used only for the experiments on APPS, adapts the work by Kojima et al. (2022) to code generation by appending _"Let's think step by step."_ to the prompt and then parsing out from the completion only the code part. To report pass@20, we generate 20 independent completions for each problem, submit each of them, and count a problem as completed if at least one solution is correct.

The second baseline adapts the work by Tang et al. (2024) to make as fair a comparison as possible. The **WorldCoder** algorithm calls the LLM with our _generate_ prompt to produce an initial program, then for each remaining iteration we 1) select one of the previous programs as explained below, 2) refine it by calling the LLM with our _fix_ prompt if the code has a bug, or our _improve_ prompt otherwise, and 3) evaluate the resulting program against the unit tests. Each program \(\) is associated with a Beta distribution \(B(,)\) with initial parameters \(=1+C*r()\) and \(=1+C(1-r())\), which are updated every time the program is selected. Here \(r()\) stands for the fraction of unit tests passed (same metric used in the evaluation phase of GIF-MCTS) and \(C\) is a constant set to 5, as in the original work. To select the next program to be refined, one sample is drawn from each Beta distribution and the program with the highest score is selected. In all experiments, we use the same amount of calls of GIF-MCTS.

### Apps

We assess the overall performance of GIF-MCTS for generic code synthesis in the presence of public unit tests on the APPS benchmark Hendrycks et al. (2021), which consists of 10,000 Python coding problems in three categories of increasing difficulty: _Introductory_, _Interview_ and _Competition_. We focus our evaluation on the hardest, _Competition_ level test set comprised of 1000 problems, as it most closely reflects the challenges found in synthesizing CWMs: the problems tend to have a longer description, follow a specific format for the input and output, and include challenging logic. Early experiments on HumanEval Chen et al. (2021), another popular coding benchmark, did not show a clear correlation between a model's performance on the benchmark and its ability to generate CWMs, as HumanEval problems are typically easier and solvable with much shorter code snippets.

As GIF-MCTS requires a reward signal from the environment, we make use of the suite of unit tests provided by APPS to evaluate the accuracy of a generated program. However, we note that the ground truth result from these tests is provided to GIF-MCTS with the _improve_ action, and as such the model could simply memorize all possible results and return them without actually solving the problem. To avoid this, while we use all unit tests for computing the reward function, we only use samples from the first half as input-output examples for the _improve_ action. In general, we use at least a fraction of the provided unit tests to evaluate every program generated during the GIF-MCTS loop, so our approach is only eligible for the pass@_B_ metric, where \(B\) is the budget for the number of LLM calls used during the synthesis process. We leave extending the approach for pass@1 eligibility using self-generated unit tests Chen et al. (2023) for future work. We report the strict accuracy rate (the fraction of problems on which all test cases are solved) on APPS for GIF-MCTS and other baselines in Table 1.

  
**Method** & **Model** & **Size** & **Strict Accuracy (\%)** & **Evaluation Strategy** \\  CodeRL Le et al. (2022) & CodeT5 & 770M & 17.90 & pass@1000 \\ Parsel Zelikman et al. (2023) & code-davinci-002 & N/A & 25.50 & pass@any \\  Zero-shot CoT * Kojima et al. (2022) & Llama 3 & 70B & 23.2\(\)1.3 & pass@20 \\ WorldCoder * Tang et al. (2024) & Llama 3 & 70B & 25.1\(\)1.4 & pass@20 \\ GIF-MCTS (ours) & Llama 3 & 70B & **28.3\(\)1.4** & pass@20 \\   

* Our re-implementation.

Table 1: **APPS competition results: comparison of methods.** We report the percentage of problems with all unit tests passed (_Strict Accuracy_). For our experiments, we also include the error of the mean on the percentage.

**Results.** GIF-MCTS outperforms strong previous baselines on the APPS competition split, reaching a new state of the art to the best of our knowledge. While part of this can be due to advances in the underlying model, the comparisons with Zero-shot CoT and WorldCoder show improved performance over either prior method. GIF-MCTS is also markedly more sample efficient compared to established baselines; Parsel achieves the second best accuracy, but evaluates an exponentially growing number of solutions4, while GIF-MCTS outperforms it by evaluating only 20 different programs.

### Code World Models Benchmark

We evaluate our proposed GIF-MCTS approach and the WorldCoder baseline on the CWMB (introduced in Section 3). In this setting, we are interested in both the accuracy of the generated CWM, as well as its performance when actually employed by a planning algorithm. We use as accuracy the same metric used in the evaluation phase of GIF-MCTS (Section 4). To measure the performance of planning with the CWM, we define the normalized return \(\) of a CWM as:

\[()=})-R(_{})}{R( _{})-R(_{})},\] (3)

where \(R(_{})\) represents the return obtained when using the CWM as the internal model for the planner, \(R(_{})\) is the return gathered with the true environment as the model while using the same planner (oracle planner), and \(R(_{})\) is the return from a random policy. This metric is positive when the performance of the CWM planner is above that of a random policy and reaches one when the return approaches the value from the oracle planner. We report results for the CWMB in Table 2. As the planner, we use a vanilla MCTS implementation for the environments with discrete actions and a Cross Entropy Method (CEM) planner (Rubinstein, 1997) for the ones with continuous action spaces (full details of the two planning algorithms are reported in Appendix L).

**Results.** Overall, GIF-MCTS outperforms WorldCoder for all environment splits and backbone models. For Llama 3, the most significant gains are made on the environments with discrete actions, while for GPT-4 on those with continuous actions. We speculate that, on discrete environments, Llama 3 makes better use of the budget with GIF-MCTS than with WorldCoder, whereas GPT-4 saturates its performance in both cases. On the other hand, on the harder environments with continuous actions, Llama 3 hits a performance ceiling in both cases, while GPT-4 leads to higher improvements with our method. For example, Llama 3 was unable to generate a fully executable CWM (with either method) for the two hardest environments, Humanoid-v4 and HumanoidStandup-v4, due to their complexity and large observation space, while GPT-4 successfully generated a model for each environment in the benchmark.

### Read to Fight Monsters

We perform an additional experiment on the Read to Fight Monsters (RTFM) grid-world environment, first introduced by Zhong et al. (2020) for testing grounded language understanding in RL. Every

    &  &  &  &  \\   & & & Accuracy (\(\)) & \(()\) & Accuracy (\(\)) & \(()\) \\   & GIF-MCTS (ours) & 50 & **0.84\(\)0.03** & **0.76\(\)0.03** & **0.35\(\)0.03** & **0.22\(\)0.01** \\  & WorldCoder * & 50 & 0.79\(\)0.04 & 0.60\(\)0.04 & 0.32\(\)0.03 & 0.19\(\)0.01 \\   & GIF-MCTS (ours) & 10 & **0.91\(\)0.08** & **0.81\(\)0.06** & **0.40\(\)0.03** & **0.26\(\)0.01** \\  & WorldCoder * & 10 & 0.87\(\)0.09 & 0.79\(\)0.06 & 0.24\(\)0.06 & 0.20\(\)0.01 \\   \\ 

Table 2: **CWMB: main results.** For each method, we report the CWM accuracy and the normalized return \(\), averaged separately across environments with discrete and continuous action spaces. Budget indicates the number of LLM calls. For each metric, we report the mean value across environments (and for the return, also across 10 episodes) with its error. For Llama 3, we report an average of three different random seeds for additional statistical significance.

episode presents two monsters belonging to two teams, and two items, each effective on a specific monster. The environment provides the agent with a written descriptions of the task dynamics (also called manual), describing monsters' weaknesses and membership to teams, and a goal (which team of monsters to defeat). Crucially, the agent needs to perform multi-step reasoning between such information and the current state of the environment to figure out a plan of action (for more details we refer to the original work by Zhong et al. (2020)). We consider a version of the environment where we fix the input manual, meaning all relationships between items and monsters are fixed across episodes, and we don't allow the monsters to move, as their patterns are stochastic. This isolates the natural language understanding component of the task, while we leave to future work to demonstrate the applicability of the CWM framework to the full RTFM task.

We report the results on the simplified RTFM environment in Table 3, using MCTS as a planner for computing the normalized returns. We further experiment with a higher number of LLM calls for GPT-4 Turbo, matching the one used for Llama 3, as we couldn't do this on the full CWMB due to budget concerns.

**Results.** GIF-MCTS outperforms WorldCoder under all settings by a significant margin in terms of accuracy, but the generated CWM is only able to match the performance of the ground-truth simulator when the program is perfect. This highlights the necessity of completely accurate predictions, as further discussed in Section 6, while also providing empirical validation for the scaling properties of the approach: as GIF-MCTS is allowed more calls, it manages to refine the CWM it generated with a lower budget. As this version of the RTFM environment has never been published, this experiment can also alleviate concerns that the final CWM was memorized by the LLM during pre-training. We present and discuss further evidence against the significance of data contamination in Appendix E.

## 6 Discussion

In this section, we first discuss some takeaways from the empirical results and then elaborate on some of the limitations for our method.

**GIF-MCTS vs. WorldCoder.** We believe that GIF-MCTS outperforms WorldCoder because it produces a more diverse set of programs. WorldCoder initially generates a single program from scratch and then samples and refines a complete program in each iteration. In contrast, GIF-MCTS can generate multiple programs either from scratch or from partial programs by taking the _generate new lines_ action at the root node or subsequent nodes. This approach better explores the solution space, leading to improved performance. Our ablation study _No Generate action_ in Table 6 of the Appendix supports this finding. This study uses a tree search like GIF-MCTS but always refines a complete program, similar to WorldCoder, and results in lower performance compared to our method.

**Accuracy-Return Gap.** We observe empirically from Table 2 that the CWM accuracy is always higher than its normalized return, and the two metrics match only when the CWM is flawless. This is often due to the incorrect prediction of terminal states: these are rarer in the replay buffer, especially states that terminate with a success/positive reward. This can cause the planning algorithm to fail, as it is missing the reward signal. Part of the performance gap could also be due to sparse coverage of the environment by the collected trajectories. Individual results for each environment elaborating

  
**Model** & **Method** & **Budget** & Accuracy (\(\)) & \(()\) \\   & GIF-MCTS (ours) & 50 & **0.58 \(\) 0.02** & **-0.11 \(\) 0.12** \\  & WorldCoder * & 50 & 0.23 \(\) 0.01 & **-0.11 \(\) 0.12** \\   & GIF-MCTS (ours) & 10 & **0.71 \(\) 0.01** & **0.31 \(\) 0.19** \\  & WorldCoder * & 10 & 0.33 \(\) 0.01 & 0.22 \(\) 0.18 \\   & GIF-MCTS (ours) & 50 & **1.00 \(\) 0.00** & **1.00 \(\) 0.00** \\  & WorldCoder * & 50 & 0.64 \(\) 0.02 & -0.06 \(\) 0.12 \\   \\ 

Table 3: **RTFM results.** For each method and computational budget (LLM calls), we report the CWM accuracy and the normalized return \(\) (computed across 10 episodes), with their errors.

on this are included in Appendix J. Future work could explore retrieving and combining different CWMs that complement each other to improve the performance on important edge cases.

**Sample Efficiency.** Generating a CWM requires far less interaction with the environment than traditional model-based approaches. As the gathered transitions are only used to validate the program and as in-context examples, a small curated set (enough to cover possible edge cases and different reward values) is enough to properly validate the generated code. In our experiments we only gather 10 trajectories made up of at most 100 steps as the offline dataset, while benchmarks specifically designed to challenge for sample efficiency (Bellemare et al., 2013) require agents to use at most 100k frames, which is two orders of magnitude higher. We leave more thorough experiments on sample efficiency for CWM agents to future work.

**Comparison with Offline RL.** We expect CWMs to hold advantages over classical RL methods in regimes with scarce data and environments that can be easily described by language and modeled with code. We report in Appendix K a preliminary comparison on the CWMB of the return achieved with our CWMs or with a SOTA offline RL method, Conservative Q-Learning (CQL) (Kumar et al., 2020), trained on the same amount of trajectories used for synthesizing the CWMs. We find that CWMs compare favourably against CQL on environments with discrete action spaces, while CQL's performance is superior on the continuous action space environments, which are harder to model. RL methods, including CQL, would likely benefit from more experience, as they overfit with scarce data.

### Limitations

**Code World Models.** The CWMs framework is an exciting direction for model-based planning, but we still rely on limiting assumptions of deterministic and fully observable environments. Both stochasticity and partial observability would pose challenges, especially on the verification of the CWM prediction, as there is no set result for a given input. We leave extending the approach to account for both stochastic and partially observable environments to future work.

Another potential issue is providing a description of the environment that can be reasonably converted to a Python function (e.g. a manual documenting key variables) when such a description is not available (e.g. when the environment is defined with image observations). Previous work has begun to tackle this issue (Migimatsu and Bohg, 2022) and preprocessing techniques such as image-to-text models (Ren et al., 2024) could be used to address this problem in future work.

Code-based models may also be too rigid when the environment requires adapting to changing dynamics, which would imply rewriting the CWM on the fly. A possible solution could be breaking down the CWM into smaller functions that can be re-written individually by an LLM, to account for some changes in the environment, or modeling variable factors as arguments to the step function. CWMs struggle especially on complex physics-based environments; thus a promising direction could also be allowing programs generated by GIF-MCTS to make use of external tools and libraries, such as physics simulators.

**GIF-MCTS.** We have validated the GIF-MCTS approach as an efficient code synthesis method, with the key limiting assumption of having available test cases to evaluate code, which could be difficult to provide in certain tasks. In those cases, it would be possible to use self-generated test cases (Chen et al., 2023), but since this does not reflect the CWM setting we leave this for future work.

## 7 Conclusion

We present Code World Models, a general framework to leverage LLMs to build world models for RL agents. We further show that GIF-MCTS is a strong code synthesis method, able to successfully integrate external feedback to self-debug and improve code, demonstrating examples of world modeling and downstream planning for a range of environments. We are confident that the Code World Models approach will lead to the development of fast, interpretable and sample efficient model-based RL agents, exploiting the strengths provided by increasingly powerful LLMs, without directly predicting the environment dynamics with them. We are hopeful that improvements to both the underlying LLM backbone and refinements to the code generation method itself will result in powerful Code World Models for even more complex environments than those treated in this work.