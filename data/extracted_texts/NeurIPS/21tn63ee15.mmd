# G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models

G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models

 Pengyue Jia\({}^{1}\), Yiding Liu\({}^{2}\), Xiaopeng Li\({}^{1}\), Yuhao Wang\({}^{1}\), Yantong Du\({}^{1}\), Xiao Han\({}^{1}\), Xuetao Wei\({}^{3}\), Shuaiqiang Wang\({}^{2}\), Dawei Yin\({}^{2}\), Xiangyu Zhao\({}^{1}\)

\({}^{1}\)City University of Hong Kong, \({}^{2}\)Baidu Inc., \({}^{3}\)Southern University of Science and Technology

{jia.pengyue,xiaopli2-c,yhwang25-c}@my.cityu.edu.hk,

{liuyiding.tanh,hahahahenha,shiqiang.wang}@gmail.com,

duyantong94@hrbeu.edu.cn,weixt@sustech.edu.cn,

yindawei@acm.org,xianzhao@cityu.edu.hk

Corresponding author.

###### Abstract

Worldwide geolocalization aims to locate the precise location at the coordinate level of photos taken anywhere on the Earth. It is very challenging due to 1) the difficulty of capturing subtle location-aware visual semantics, and 2) the heterogeneous geographical distribution of image data. As a result, existing studies have clear limitations when scaled to a worldwide context. They may easily confuse distant images with similar visual contents, or cannot adapt to various locations worldwide with different amounts of relevant data. To resolve these limitations, we propose **G3**, a novel framework based on Retrieval-Augmented Generation (RAG). In particular, G3 consists of three steps, i.e., **G**eo-alignment, **G**eo-diversification, and **G**eo-verification to optimize both retrieval and generation phases of worldwide geolocalization. During Geo-alignment, our solution jointly learns expressive multi-modal representations for images, GPS and textual descriptions, which allows us to capture location-aware semantics for retrieving nearby images for a given query. During Geo-diversification, we leverage a prompt ensembling method that is robust to inconsistent retrieval performance for different image queries. Finally, we combine both retrieved and generated GPS candidates in Geo-verification for location prediction. Experiments on two well-established datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other state-of-the-art methods. Our code2 and data3 are available online for reproduction.

## 1 Introduction

Worldwide image geolocalization  aims to pinpoint the exact shooting location for any given photo taken anywhere on Earth, as illustrated in Figure 1(a). Unlike geolocalization within specific regions (e.g., at city level) , worldwide geolocalization  greatly unleashes the potential of geolocalization, which is useful for various real-world applications, such as crime tracking and navigation. However, worldwide image geolocalization is extremely challenging, as images collected from around the world are featured with a myriad of elements, including varying landscapes, weather conditions, architectural styles, etc.

Extensive research efforts have been devoted to the task, which can be broadly categorized into 1) classification-based, 2) retrieval-based, and 3) retrieval-augmented generation (RAG) methods. In particular, classification-based methods  divide the entire geographical space into fixed grids, and classify each image into a particular grid. Retrieval-based methods convert image localization to either a image-to-image  or an image-to-GPS  retrieval problem, where the final prediction is a top-retrieved GPS exists in a given candidate database. Generation-based methods  recently achieved state-of-the-art performance on this task, via applying a retrieval-augmented generation (RAG) pipeline that leverages the strong reasoning and generalization ability of large multi-modality models (LMMs). They usually integrate the retrieved GPS coordinates in the input prompts of LMMs as references, to generate more accurate predictions.

Despite their initial success, existing studies still have clear limitations when scaled to a worldwide context, mainly due to two challenges as shown in Figure 1. **First**, it is very challenging to extract visual semantics that accurately indicate an image's geolocation, as two distant places could possibly have similar visual features. Conventional visual representations are ineffective in implying subtle location-aware semantics. **Second**, image data usually exhibits significant heterogeneity in its geographical distribution, which existing methods can hardly handle. For retrieval-based methods, it only performs well for image queries with many nearby images stored in the database, while many images at unpopular locations may have very few or even no similar data to be compared with, leading to large prediction errors. Worst still, such inconsistent retrieval performance significantly affects existing RAG-based methods that use a fixed number of references. As the retrieval performs inconsistently for different image queries, their generation process lacks the robustness to adapt to various image queries at different locations worldwide.

To address the aforementioned challenges, we propose G3, a novel RAG-based solution with expressive retrieval and robust generation for worldwide image geolocalization.

**For the retrieval phase**, we train multi-modality encoders to effectively capture location-aware visual similarity between images. Unlike existing RAG-based methods that only leverage conventional visual similarity for retrieval, we propose a multi-modality alignment process, namely **Geo-alignment**, which learns the representations of GPS coordinates, images, and textual descriptions in a joint manner. By doing this, the visual representations are able to capture fine-grained location-aware semantics for retrieving other images close by. In addition to the numerical GPS coordinates, the textual descriptions (e.g., city/country names) can largely enrich the location information to be aligned with the visual representations. Moreover, to train the multi-modality representations, we establish a new dataset, namely **MP16-Pro**, by including textual geographical descriptions to the original MP16 dataset . We anticipate the dataset will benefit more future work for location-aware visual representation learning.

**For the generation phase**, we leverage a prompt ensembling method, namely **Geo-diversification**, which improves the robustness of prediction generation for different types of images. More specifically, it generates a diverse set of predictions via multiple retrieval-augmented prompts, each of which might be more useful for a certain type of query images. As such, the generated GPS candidates are more likely to contain the ground truth coordinates. Subsequently, we conduct **Geo-verification**, which combines both the retrieved and the generated GPS candidates, and compares their similarities with the query image using the learned multi-modality representations. The most similar GPS is returned as the final prediction. Extensive experiments are conducted on well-established datasets IM2GPS3k  and YFCC4K , and the results show the effectiveness of G3 compared to the other state-of-the-art baseline methods. We summarize the key contributions of our work as follows:

Figure 1: Illustration of limitations of prevailing methods in worldwide geolocalization.

* We present G3, a novel solution for the worldwide geolocalization task. Our proposed method leverages 1) Geo-alignment to learn expressive location-aware representations of images, 2) Geo-diversification to improve the robustness of GPS candidate generation, and 3) Geo-verification to ensemble both retrieved and generated candidates for final prediction.
* We release a new dataset MP16-Pro, adding textual localization descriptions to each sample based on the original dataset MP16 to facilitate future research in the field.
* We extensively experiment with two well-established datasets IM2GPS3k and YFCC4K. G3 demonstrates superior performance compared to other state-of-the-art baseline methods.

## 2 Related Work

**Image Geolocalization.** Image Geolocalization is an important task in computer vision , spatial data mining , and GeoAI . Previous work in image geolocalization can be divided into three main categories: classification-based methods, retrieval-based methods, and generation-based methods. (1) Classification-based methods  divide the entire earth into multiple grid cells and assign the center coordinates as predicted values. Models are then trained to classify the input image into the correct cell. However, if the actual location of the image is far from the center of the predicted cell, there can still be significant errors, even if the cell prediction is correct. (2) Retrieval-based methods treat the image geolocalization task as a retrieval problem, typically maintaining a database of images  or a gallery of GPS coordinates . They take the most similar images and GPS coordinates to the query image as the predicted values. However, maintaining a global-level image database or GPS gallery is infeasible. (3) Generation-based methods employ large multi-modality models to generate the predicted coordinates for images . Zhou _et al._ introduced retrieval-augmented generation into the geolocalization task and took the retrieved similar images' coordinates as references to help generate predictions. However, they can not accurately extract visual semantics to indicate an image's location because they simply use visual similarity to retrieve references and suffer inaccurate prediction when facing heterogeneous query images. In this work, G3 introduces Geo-alignment to incorporate geographical information into image representations to help retrieve similar images in geography and proposes Geo-diversification and Geo-verification to enhance prediction performance and robustness.

**Large Multi-modality Models.** Inspired by the success of large models  in single domains like computer vision  and natural language processing , there has been increasing attention on large multi-modality models. CLIP  aligns image and text representations through contrastive learning and achieves remarkable model generalization with simple optimization objectives. The Large Language-and-Vision Assistant (LLAVA)  effectively combines CLIP's visual encoder with the powerful language model Vicuna, enhancing the model's general understanding of both visual and textual information through two-stage instruction tuning. GPT4V  is a large multimodal model released by OpenAI in 2023, allowing users to input text and images to obtain answers.

**Retrieval-Augmented Generation.** To mitigate the hallucination issue in LLMs, retrieval-augmented generation (RAG)  has emerged as a popular and effective technique. It enhances the reliability of content generated by LLMs by incorporating facts fetched from external sources. Specifically, certain factual knowledge is retrieved by a retriever from external sources based on a query. LLMs can access these retrieval results during the generation process to generate accurate outcomes. RAG preserves the generalization capabilities of LLMs while also introducing external information to enhance the reliability of generated content, efficiently alleviating the hallucination problem.

## 3 Methodology

Figure 2 illustrates the comprehensive architecture of G3, which consists of Geo-alignment, diversification, and verification, with two phases: database construction and location prediction. During the database construction phase, introduced in Section 3.1, Geo-alignment aligns image representations with textual image descriptions and GPS information to incorporate geographical information into representations. In the location prediction phase, introduced in Section 3.2, similar images will be first retrieved based on the nearest neighbor search from the database; Geo-diversification will then

combine their coordinates in RAG prompts to generate diverse candidates, and Geo-verification finally selects the final predicted coordinates in a multi-modality space.

### Database Construction

G3 requires an image database to preserve image representations. Existing work directly uses visual encoders (e.g., CLIP's ViT encoder or ResNet) to encode images. However, visual similarity cannot completely represent geographical proximity. To overcome this issue, we propose Geo-alignment, which incorporates geographical information into image representations by multi-modality alignment.

**Geo-alignment.** Geographical features can be divided into continuous and discrete types, which are essential in geolocalization. On the one hand, according to the first law of geography , "everything is related to everything else, but near things are more related to each other." Climate, terrain, and vegetation are continuous features that gradually change along latitude or longitude. On the other hand, discrete features (e.g., city/country names) are also conducive to determining geographical location. These features usually change abruptly at national borders. To encode images with representations tailored for geolocalization, we propose a multi-modality alignment method, Geo-alignment, as shown in Figure 2(A).

**Image encoding.** We use pretrained vision encoder and two trainable transformation layers to encode images: \(_{i,}^{}=f_{}(( _{i}))\), \(_{i,}^{}=f_{}(( _{i}))\), where \(_{i,}^{}\) and \(_{i,}^{}\) are the \(i\)-th image representations in the batch that need to be aligned with textual geographical descriptions and GPS data. \(f_{}\) and \(f_{}\) are the corresponding feed forward functions, \(\) represents the fixed vision encoder, and \(_{i}\) is the \(i\)-th image in a batch.

**GPS coordinate encoding.** To encode GPS coordinates, an appropriate projection is needed to transform latitude and longitude into a Cartesian coordinate system. We choose not to adopt the equal earth projection (EEP) used in previous work  because EEP primarily focuses on area accuracy while overlooking angular distortions, which is significant in modeling the trends of geographical features along latitude and longitude. As a result, we utilize Mercator projection for its conformal property. The formula of Mercator projection is shown below:

\[=(-_{0})\\ = ln[(+)] \]

Figure 2: Overview of the framework of G3.

where \(\) and \(\) are radians of longitude and latitude, \(_{0}\) denotes the central meridian longitude. \(\) is a proportional constant of Earth radius. The output \(\) and \(\) denote the transformed plane coordinates.

After projection, we follow previous work  to capture high-frequency patterns and hierarchical representations using random fourier features (RFF) with various frequencies. RFF function \(\) will transform the projected coordinate \(_{i}=(_{i},~{}_{i})\) first: \((_{i})=[(2_{i}),~{}(2_{i})]^{}\). \(\) denotes a matrix sampled from a Gaussian distribution \((0,)\) to limit the frequencies. To capture hierarchical representations, we sum up the outputs with different \(\): \(_{i}^{}=_{k=1}^{K}f_{k}((_{i},~{} _{k}))\) where \(_{i}^{}\) is the encoded gps representations for \(i\)-th sample in a batch, \(K\) denotes the number of hierarchical patterns, \(f_{k}\) is the feed forward function for \(k\)-th hierarchical layer. \(_{k}\) controls the frequency for \(k\)-th layer and \(_{k}=2^{log_{2}(_{min}+(k-1)(log_{2}(_{max})-log_{2}(_ {min}))/(N-1)},~{} k\{1,,N\}\). \(_{min}\) and \(_{max}\) controls the range of \(_{k}\).

**Text encoding.** We initially employ geographical reverse encoding to obtain textual descriptions of GPS coordinates. For instance, as illustrated in Figure 2(A), the GPS coordinates (60.37, 6.72) can be converted into the textual description "A photo taken from Vestland, Norway". These textual descriptions are inputs to a pre-trained text encoder, followed by feedforward networks for vector transformation. \(_{i}^{}=f((_{i}))\) where \(_{i}^{}\) denotes the encoded textual descriptions for \(i\)-th sample in a batch, \(f\) is the feed forward transformation layer, \(\) is the text encoder function, and \(_{i}\) is the textual descriptions for \(i\)-th sample in a batch.

**Optimization.** Geo-alignment is optimized with the following objective to align image representations with textual descriptions and GPS information:

\[_{a,b}=-_{i=1}^{n}log((_{ii})}{ _{j=1}^{n}(_{ij})}),~{}=(^{a}}{\|^{a}\|_{2}})(^{b}}{\|^{b}\|_{2}})^{ }^{t_{a,b}} \]

where \(_{a,b}\) denotes the loss function of modality \(a\) to modality \(b\), \(\) is the encoded representations, and \(t\) is the temperature. G3 needs to align image representations with both textual descriptions and GPS data, so the final optimization objective is shown below: \(=(_{}+_{}+ _{}+_{})/2\).

**Image vectorization.** As illustrated in Figure 2(B), after Geo-alignment, we will vectorize the images in the dataset and store them in a database. To maintain image representations tailored for geolocation tasks, we concatenate the original visual representations with representations aligned with geographical information: \(^{}=(^{},_{ }^{},_{}^{})\). \(^{}\) denotes the final representation, \(^{}\) represents the vector obtained directly through the pretrained vision encoder. \(_{}^{}\) and \(_{}^{}\) are the image representations aligned with textual geographical descriptions and GPS information.

### Location Prediction

Figure 2(C) and (D) illustrate the overview of the location prediction phase. Previous work  directly incorporates the GPS coordinates of similar retrieved images as references into a single RAG prompt to generate predictions. However, due to the heterogeneity of query images, the number of reference GPS coordinates varies when each sample achieves optimal prediction performance. To address this issue, Geo-diversification expands the candidate pool with prompts containing different numbers of reference coordinates, including zero (i.e., in a zero-shot manner), as shown in Figure 2(C). Illustrated in Figure 2(D), Geo-verification selects the best prediction coordinate for each sample using the well-trained Image-to-GPS encoders in Geo-alignment. In the location prediction phase, Geo-diversification and Geo-verification are introduced to enrich the diversity of generated predictions and select the predictions with the highest confidence.

**Geo-diversification.** Due to the heterogeneity of query images, the number of reference coordinates introduced in the RAG process varies when each sample achieves optimal prediction performance. To solve this issue, we introduce Geo-diversification. Specifically, we first construct \(K\) RAG prompts with different numbers of reference coordinates (0 reference coordinates equals zero-shot generation), and each prompt will generate \(N\) results. This process can be represented as follows: \(\{c_{1}^{k},c_{2}^{k},,c_{n}^{k}\}=(p^{k})\) where \(c^{k}\) denotes the candidate coordinate generated by the \(k\)-th RAG prompt, and \(p^{k}\) is the \(k\)-th RAG prompt. The final candidate pool contains the top \(S\) coordinate candidates of retrieved similar images and the generated coordinate candidates as shown in Figure 2. The final candidate pool is denoted as \(\{c_{1},c_{2},,c_{m}\}\), where \(m=K N+S\).

**Geo-verification.** Given the coordinate candidates set \(\{c_{1},c_{2},,c_{m}\}\), selecting the best guess is essential and challenging. We reinvent the well-trained Image-to-GPS model in Geo-alignment to achieve this target, as shown in Figure 2(D). The similarity between image representations \(_{}^{}\) and GPS representations \(^{}\) are derived by \(=_{}^{}(^{})^{ }\), and the coordinate \(c_{j}\) with the highest similarity is selected as the final prediction by \(j=(_{j}),\;j\{1,2,,m\}\).

## 4 MP16-Pro Dataset

To facilitate subsequent research, we propose the MP16-Pro dataset by adding textual geographical descriptions to each sample from the MediaEval Placing Tasks 2016 (MP-16) dataset . Specifically, we utilize the open-source geocoding tool Nominatim to obtain multi-level geographical textual descriptions for each sample's GPS location (**total 4.72 million locations**). There are eight geographical unit levels: neighborhood, city, county, state, region, country, country code, and continent. Some examples are given in Appendix A.1 for reference. Geographical text descriptions provide additional information for geolocalization tasks and enable models to transcend the original paradigm solely supporting image and GPS alignment, facilitating more diverse modeling approaches.

## 5 Experiments

**Datasets and evaluation metrics:** For database construction and model training, we use the MP16-Pro dataset we released. It contains 4.72 million geotagged images from Flickr 4. However, given that the dataset was released in 2016, currently, 4.12 million images within the dataset remain accessible. Following previous work [29; 43], we evaluate G3 with public datasets (IM2GPS3k  and YFCC4K ) and a threshold metric. Given the predicted coordinates and the ground truths, this metric quantifies the percentage of predictions where the distance to the ground truth falls within specified thresholds (1km, 25km, 200km, 750km, and 2500km).

**Implementation details:** We use faiss  to deploy the image database. The vision and text encoders are pretrained ViT-L/14 and a masked self-attention transformer from CLIP . The dimensions for two trainable layers of \(f_{}\), \(f_{}\), \(f\) are 768 and 768. The input dimension of GPS encoder is 512, and the dimensions for four hidden layers of \(f_{k}\) in Equation 3.1 are 1024, the output dimension is 512. For the Earth radius, we set it as 6378137.0. For RFF, we use three hierarchies with \(_{min}\) as \(2^{0}\) and \(_{max}\) as \(2^{8}\). GPT4V 5 is selected as the LMMs in this paper. Its temperature is set to 1.2. The number of RAG prompts \(K\) is set to 4, and the number of candidates for each RAG \(N\) is set to 5 for IM2GPS3K and 1 for YFCC4K. The number of similar image coordinates taken into account in candidates is 0 for IM2GPS3K and 1 for YFCC4K. G3 is trained using AdamW optimizer with learning rate 3e-5 and weight decay 1e-6. A step linear scheduler is employed with gamma 0.87, and the training epoch is set to 10. Training batch size is set to 256 and temperature \(t\) in Equation 2 is initialized as 3.99. All experiments are conducted with Pytorch and one NVIDIA H800 GPU. Please refer to Appendix A.2 for more details on the training environment, training time, and API cost. We also mention the limitations of G3 in Appendix A.4.

**Baselines:** To evaluate G3 in geolocalization, we follow previous work [29; 43] and select the following baselines for comparison: [L]kNN,\(\)=4 , PlaNet , CPIaNet , ISNs , Translocator , GeoDecoder , GeoCLIP , Img2Loc , PIGEON . The detailed descriptions of baselines are in Appendix A.5. Due to the lack of available implementations for Img2Loc, we reproduce it based on its paper and release it in our repository for future research.

### Comparison with State-of-the-art Methods

To verify the effectiveness of G3, we conduct comparative experiments on IM2GPS3K and YFCC4K with other state-of-the-art methods. The results are shown in Table 1. (1) G3 is superior to all the other baselines on almost all metrics. In addition, compared to the second best methods, G3 achieves **8.5%**, **2.8%**, **3.3%** improvements on IM2GPS3K in the 1km, 25km, 200km thresholds and **21.3%**, **16.9%**, **13.5%**, **3.3%**, **0.6%** improvements on YFCC4K in the 1km, 25km, 200km, 750km, 2500km thresholds. (2) G3, Img2Loc, GeoCLIP, and PIGEON achieve leading results, whichcan be attributed to the other methods taking the worldwide geolocalization task as a classification problem, introducing inevitable systemic biases. (3) G3 demonstrates significant improvements over GeoCLIP because GeoCLIP is constrained by the settings of the GPS gallery, which can not cover the entire globe. (4) Compared to Img2Loc, G3, through Geo-alignment that aligns images with discrete and continuous geographical features, achieves more precise retrieval of reference coordinates for subsequent RAG processes. Additionally, Geo-diversification and Geo-verification effectively expand the candidate pool and filter out the confident prediction results, further enhancing geolocalization performance. Overall, G3 achieves the best performance on all datasets across almost all metrics, which verifies the effectiveness of G3.

### Ablation Study

To understand the specific effects of each module in G3, we design the following variants:

* **w/o Geo-A:** G3 without Geo-alignment. Directly using ViT in CLIP for database construction.
* **w/o Geo-D:** G3 without Geo-diversification. Generating prediction with one RAG prompt with 10 positive samples and 10 negative samples (the parameter has been tuned).
* **w/o Geo-V:** G3 without Geo-verification. Instead of using the well-trained Image-GPS model in Geo-alignment, this variant randomly selects the final prediction from candidates.

Table 2 shows the experimental results. We can draw the following conclusions: (1) All three modules significantly contribute to the final performance. (2) G3 achieves better performance than w/o Geo-A for Geo-alignment incorporates geographical information into image representations. As a result, the retrieved images are geographically similar to the query image, enhancing the effectiveness of references in the RAG process. (3) G3 is superior to w/o Geo-D, for the number of reference coordinates varies when each sample achieves the optimal prediction performance facing heterogeneous query images. The absence of Geo-diversification leads to suboptimal candidates. (4) Comparing G3 with w/o Geo-V, we observe a significant performance drop in w/o Geo-V, indicating the necessity of Geo-verification.

### Hyperparameter Analysis

In the generation process within G3, two hyperparameters directly impact the results: the number of RAG prompts and the number of candidate coordinates generated by each single prompt.

    &  &  \\   & Street & City & Region & Country & Continent & Street & City & Region & Country & Continent \\  & 1km & 25km & 200km & 750km & 2500km & 1km & 25km & 200km & 750km & 2500km \\ 
[-1pt/

**Number of RAG prompts.** To investigate the impact of varying numbers of RAG prompts, we design the following experiment: We employ four sets of RAG prompts with different reference coordinates: 0 positive, 0 negative; 5 positive, 5 negative; 10 positive, 10 negative; 15 positive, 15 negative. Starting with the first prompt, subsequent prompts will be sequentially added to change the number of RAG prompts. The number of candidates generated by each prompt is fixed to 5. As illustrated in Figure 3, the influence of RAG prompt counts on prediction performance is consistent across different metric thresholds. A significant enhancement is observed when the number increases from 1 to 2. The reason is that the zero-shot prompt (RAG prompt with 0 positive and 0 negative reference coordinates) fails to provide high-quality predictions for global images with insufficient information. The model's performance gradually improves as the number increases from 2 to 4 because having more candidates will increase the possibility of containing the ground truth coordinates.

**Number of candidates.** Figure 4 shows the results varying the number of candidates for each prompt. We fix the number of prompts to 4 in this experiment. We observe that the turning points where performance begins to decline exhibit an increasing trend at different levels. Specifically, at the street level, performance declines after just one candidate, at the city level after three, at the region and country levels after five, and at the continent level after seven. Three key points merit attention: (1) The initial upward trend occurs because the generation of LMMs involves randomness. Introducing more candidates can alleviate the randomness. (2) As the number of candidates increases, performance ultimately drops, likely due to the introduction of more noise in the predictions from additional generations. (3) The turning points of decline differ by level because broader levels demonstrate greater tolerance to predictive bias when more noise candidates are included.

### Effectiveness of Geo-alignment and Mercator Projection

To assess the effectiveness of Geo-alignment and Mercator projection, we conduct the following experiments focusing on the reference retrieval phase: We build image databases using different embedding techniques and then retrieve the Top-N images closest to the query image. The geodesic distances will be calculated between their coordinates and the query image. The embedding variants are illustrated as follows:

* **CLIP ViT:** Directly using the visual encoder ViT in CLIP for image embedding.
* **G3+EEP:** Geo-alignment with Equal Earth Projection (EEP).
* **G3+Mercator:** Geo-alignment with Mercator Projection.

Table 3 shows the statistics of the geodesic distances of retrieval reference images with different embedding methods. We can draw the following conclusions: (1) G3+EEP outperforms CLIP ViT as the latter only considers visual similarity, while image representations in G3+EEP encompass both

Figure 4: Varying the number of candidates for each RAG prompt on IM2GPS3K.

Figure 3: Varying the number of RAG prompts on IM2GPS3K.

visual and geographical similarity, which is essential for geolocalization tasks. (2) G3+Mercator performs better than G3+EEP, as the EEP projection method emphasizes area projection accuracy while overlooking angular distortions, which increases the training complexity and limits the performance.

### Impact of LMMs on G3

To explore the impact of LMMs on G3, we conduct the experiments of G3 and Img2Loc with LLaVA(LLaVA-Next-LLaMA3-8b) on IM2GPS3K. From Table 4 we can find that: (1) After switching LMMs from GPT4V to LLaVA, the performance of G3 shows some decline across various metrics but remains competitive. (2) Additionally, compared to Img2Loc (LLaVA), G3 (LLaVA) significantly outperforms Img2Loc (LLaVA), demonstrating the effectiveness of the proposed modules. (3) Finally, by comparing the performance of G3 equipped with LLaVA and GPT4V to Img2Loc equipped with LLaVA and GPT4V, we can observe that G3 shows more stable performance across different LMMs.

### Necessity Analysis of Three Representations Alignment in Geo-alignment

To verify the necessity of aligning the three representations in Geo-alignment, we conduct experiments of the following variants on IM2GPS3K:

* **IMG:** Directly using pre-trained CLIP vision encoder as the encoder.
* **IMG+GPS:** Aligning Image representations with GPS representations in Geo-alignment, the textual descriptions are not used.
* **IMG+GPS+TEXT(G3):** Aligning three modalities simultaneously in Geo-alignment.

Table 5 shows that: (1) By comparing IMG+GPS+TEXT, IMG+GPS, and IMG, we find that adding GPS and text information can both enhance the feature representation compared to using the original image information alone. (2) By comparing IMG+GPS+TEXT with IMG+GPS, we find that IMG+GPS performs better at smaller scales, while IMG+GPS+TEXT performs better at larger scales. This might be because GPS is suitable for modeling variations at smaller scales, whereas text descriptions do not vary significantly at small scales and may even remain the same.

### Case Study

**Case study on reference image retrieval.** Figure 5 visually demonstrates the superiority of G3 in reference image retrieval. It is evident that if CLIP's ViT is used as the image encoder, the model

   Methods & Street 1km & City 25km & Region 200km & Country 750km & Continent 2500km \\  Img2Loc (LLaVA) & 10.21 & 29.06 & 39.51 & 56.36 & 71.07 \\ Img2Loc (GPT4V) & 15.34 & 39.83 & 53.59 & 69.70 & 82.78 \\ G3 (LLaVA) & 14.31 & 35.87 & 49.42 & 66.93 & 81.78 \\ G3 (Gpt4V) & **16.65** & **40.94** & **55.56** & **71.24** & **84.68** \\   

Table 4: Impact of LMMs on G3.

    &  &  &  \\  & Avg. & Md. & Max. & Min. & Avg. & Md. & Max. & Min. & Avg. & Md. & Max. & Min. \\  CLIP ViT & 2554.7 & 2244.6 & 5048.4 & 800.7 & 2645.9 & 2269.9 & 6376.9 & 513.3 & 2704.1 & 2307.7 & 7142.2 & 404.8 \\ G3+EEP & 2361.5 & 2089.0 & 4618.2 & 735.3 & 2434.5 & 2102.2 & 5808.9 & **479.3** & 2464.3 & 2104.3 & 6529.2 & **369.6** \\ G3+Mercator & **2299.2** & **2054.9** & **4474.7** & **699.0** & **2362.5** & **2035.4** & **5569.6** & 482.2 & **2405.0** & **2046.9** & **6341.0** & 373.0 \\   

Table 3: Distance statistics of retrieval reference images with different embedding methods. Avg., Md., Max., and Min. are the average, median, maximum, and minimum distances to the query image.

   Methods & Street 1km & City 25km & Region 200km & Country 750km & Continent 2500km \\  Img2Loc (LLaVA) & 10.21 & 29.06 & 39.51 & 56.36 & 71.07 \\ Img2Loc (GPT4V) & 15.34 & 39.83 & 53.59 & 69.70 & 82.78 \\ G3 (LLaVA) & 14.31 & 35.87 & 49.42 & 66.93 & 81.78 \\ G3 (Gpt4V) & **16.65** & **40.94** & **55.56** & **71.24** & **84.68** \\   

Table 3: Distance statistics of retrieval reference images with different embedding methods. Avg., Md., Max., and Min. are the average, median, maximum, and minimum distances to the query image.

primarily focuses on the human figures in the image (i.e., 'two people posing together in the center of the photo') while neglecting background elements beneficial for geolocalization. In Geo-alignment, G3 incorporates geographical information into the image representations. As a result, retrieved images are more focused on geographical proximity (three reference images within 1 km of the actual shooting location are retrieved in the top-5 candidate images). These valuable reference images further assist the RAG process to enhance final prediction performance.

**Case study on heterogeneous query image in RAG process.** Figure 6 provides three examples illustrating the best prediction occurs when using RAG prompts with different numbers of references facing heterogeneous query images. (1) RAG with 0 references achieves the best performance for the first query image. This is because, on the one hand, the references are filled with biased coordinates, and on the other hand, the building in the figure is a famous landmark named Selimiye Camii mosque. The pre-trained LMMs effectively provide the longitude and latitude of this landmark based on its world knowledge. (2) For the second query image, RAG with 5 references performs best because the optimal reference appeared in the fifth position. More references do not add extra helpful information but instead introduce more noise, causing the performance of RAG with 10 references to decline; RAG with 0 references produces incorrect predictions due to the absence of clear landmark indicators in this image. (3) For the third query image, RAG with 10 references yields the best accuracy, as the references from 6 to 10 provide substantial helpful information, whereas the first five reference coordinates are far from the ground truth. Overall, from these examples, we can discern some common patterns: for images with prominent landmark features, RAG with 0 references often yields good results; for images with less informative content (such as oceans, skies, or indoor scenes), RAG with 10 references makes more comprehensive judgments based on a greater number of references; and for images with distinct regional features (images between the first two settings), RAG with 5 references will achieve satisfactory prediction accuracy.

## 6 Conclusion

In this paper, we propose a novel worldwide geolocalization framework named G3. First, we introduce Geo-alignment to capture location-aware semantics in images by aligning images with textual geographical descriptions and GPS information. Second, Geo-diversification is proposed to improve the robustness of prediction generation via a prompt ensemble technique. Finally, Geo-verification selects the final coordinate prediction using the learned multi-modality representations. G3 is evaluated on two well-established datasets, IM2GPS3K and YFCC4K, and achieves state-of-the-art performance. In addition, we release a new dataset MP16-Pro, adding textual localization descriptions to each sample based on the original dataset MP16 to facilitate future research in the field. All the code and data used in this work have been released public.

Figure 5: Reference image retrieval with CLIP ViT and G3.

Figure 6: Predictions given with different numbers of references facing heterogeneous images.