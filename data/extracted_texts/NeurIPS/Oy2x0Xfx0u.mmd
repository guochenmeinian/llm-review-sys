# What do Graph Neural Networks learn?

Insights from Tropical Geometry

 Tuan Anh Pham

School of Mathematics

University of Edinburgh

Edinburgh, United Kingdom

tau.pham@ed.ac.uk

&Vikas Garg

YaiYai Ltd and Aalto University

vgarg@csail.mit.edu

###### Abstract

Graph neural networks (GNNs) have been analyzed from multiple perspectives, including the WL-hierarchy, which exposes limits on their expressivity to distinguish graphs. However, characterizing the class of functions that they learn has remained unresolved. We address this fundamental question for message passing GNNs under ReLU activations, i.e., the de-facto choice for most GNNs.

We first show that such GNNs learn tropical rational signomial maps or continuous piecewise linear functions, establishing an equivalence with feedforward networks (FNNs). We then elucidate the role of the choice of aggregation and update functions, and derive the first general upper and lower bounds on the geometric complexity (i.e., the number of linear regions), establishing new results for popular architectures such as GraphSAGE and GIN. We also introduce and theoretically analyze several new architectures to illuminate the relative merits of the feedforward and the message passing layers, and the tradeoffs involving depth and number of trainable parameters. Finally, we also characterize the decision boundary for node and graph classification tasks.

## 1 Introduction

Message passing has been a cornerstone of machine learning, from inference in graphical models [1; 2] to embedding of graphs [3; 4; 5; 6; 7]. Message passing neural networks (MPNNs) are easy to implement, and can handle large scale, heterogeneous, and dynamic real world data effectively; so continue to be an active area of research [8; 9; 10]. Indeed, several popular GNNs are usually cast and implemented as MPNNs, see e.g., [11; 12; 13; 14; 15].

Unsurprisingly, GNNs as MPNNs have been analyzed from multiple theoretical perspectives. A major theme is inspired by connections to the so-called 1-WL (Weisfeiler-Leman) test for group isomorphism and its higher order extensions, where nodes in a (hyper-)graph repeatedly refine their _colors_ based on the messages from their neighbors . MPNNs are known to be bounded in power according to the WL-hierarchy [15; 17; 18], implying their inability to distinguish some non-isomorphic graphs. Unlike WL that strives to expose what GNNs _cannot_ do, here we seek to unravel what they _can_.

Notably, the WL formalism implicitly relies on injective hash functions; in contrast, most successful GNNs typically use ReLU activations that violate injectivity. Bounding the expressivity of such GNNs via their injective surrogates is theoretically valid; however, it does not illuminate what they learn. Indeed, several fundamental questions remain elusive for these practical models; e.g., (a) what class of functions can they represent, (b) how does their expressivity vary with the choice of message aggregation and update functions, (c) what complexity tradeoffs (e.g., in terms of the number oflayers or parameters) exist for models of comparable expressivity, and (d) what decision boundary emerges for node and graph classification tasks?

We appeal to tropical algebra and geometry to address these questions in the context of ReLU MPNNs. Specifically, casting these networks as tropical geometric objects, we analyze their ability to represent different weighted sums of _tropical monomials_ (the basic objects of interest in tropical algebra, akin to monomials for standard polynomials) at different nodes. We first show that the family of functions represented by these networks is precisely the family of _tropical rational signomial maps_ (TRSMs). Since TRSMs are known to be equivalent to ReLU FNNs , this reveals an equivalence between ReLU MPNNs and ReLU FNNs: they represent exactly the same class of functions, namely, continuous piecewise linear maps (CPLMs).

This equivalence in terms of expressivity does not, however, translate into parity in _efficiency_, e.g., as quantified in terms of their respective requirements for the number of layers and trainable parameters to represent an arbitrary TRSM. Indeed, MPNNs have some strengths and limitations relative to FNNs. On the positive side, MPNNs benefit from parallel computation and streamlined communication between the nodes and their neighbors. On the flip side, MPNNs are constrained by permutation-equivariant layers that employ permutation-invariant aggregation operators , which impedes their ability to represent arbitrary combinations of tropical monomials.

In order to better understand the efficiency of different MPNN architectures, we investigate their _geometric complexity_, the number of linear regions that they can distinguish. Towards that end, we establish the first general upper and lower bounds on the geometric complexity of ReLU MPNNs, recovering the existing results  for FNNs and graph convolutional networks (GCNs) as special cases. Importantly, we also unravel new results for two of the most prominent MPNNs whose complexity was previously unknown, namely, GraphSAGE  and GIN .

A particularly attractive aspect of our bounds is manifested in segregation of the contribution from different components, such as aggregation and update steps. In particular, they reveal that selecting _coordinate-wise max_ as the aggregation operator affords greater geometric complexity than _sum_ for ReLU MPNNs. In order to provide further insights about various complexity tradeoffs, we introduce and analyze four novel MPNN abstractions. Notably, they can all represent arbitrary TRSMs, i.e., they are as expressive as ReLU FNNs, but differ in terms of layers as well as total learnable parameters in their corresponding architectures. Fundamentally, we expose a general trend about the relative merits of the feedforward and message passing paradigms: fewer layers for MPNNs (particularly with coordinate-wise max aggregation) but fewer trainable parameters for FNNs.

Figure 1: **Overview of our results**. Formulating ReLU MPNNs as tropical geometric objects allows us to shed light on several important aspects where WL falls short.

Finally, we study the decision boundaries for node and graph classification, uncovering the underlying connections with _tropical hypersurfaces_; i.e., the set of points where two or more tropical monomials achieve the same value.

We summarize our key contributions in Figure 1. We now proceed to reviewing some related works.

### Related work

Tropical Geometry and Machine Learning.Tropical geometry [22; 23] provides tools to study the algebraic geometry and combinatorics of continuous piecewise linear functions, and finds several applications (e.g., in optimization). Two seminal works [19; 24] initiated the analysis of deep learning models via tropical geometry, establishing the link between ReLU FNNs and tropical rational functions. The connection was then extended to maxout-layers in . The decision boundaries of FNNs through a tropical lens were studied in . Other aspects of deep neural networks have also be analyzed [27; 28; 29; 30; 31; 32]. We refer to  for a survey on the current use of tropical geometry in deep learning.

Expressivity and WL.Much work on GNNs has been inspired by noticing the parallels between MPNNs and the WL test for isomorphism [15; 34; 35]. Standard MPNNs are no more powerful than 1-WL (equivalently, 2WL), so higher order models that consider tuples of vertices have been proposed [17; 18]. Several adaptations of WL such as geometric WL , cellular WL ,temporal WL , and persistent WL  have been introduced in different contexts. Limited expressivity and symmetry considerations have led to the design of new MPNN architectures [15; 39; 40; 41; 42; 43], inclusion of different types of features [39; 44], and integration with topological descriptors [45; 46; 47]. Other notions of expressivity have also been proposed, e.g., equivariant polynomials  and homomorphism counts .

Other results about GNNs.GNNs as MPNNs are also known to be prone to information bottlenecks [50; 51; 52], oversmoothing and oversquashing , and heterophily [54; 55]. Some works have also established their inability to count substructures, compute graph properties, or learn topology [49; 56; 57]; connections to algebraic topology , biconnectivity , and communication complexity [60; 61], behavior in overparametrized regimes [62; 63]; power of recursion in counting substructures ; benefits of positional encoding ; and ability to generalize and achieve universal approximation [56; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75].

However, neither expressivity in terms of WL nor these other results characterize the class of functions learned by GNNs, and in this work we bridge this glaring gap for ReLU MPNNs.

Complexity of Neural Networks.The complexity of deep neural networks is typically studied in terms of the number of linear regions [76; 77; 78; 79], and the intricacy of decision boundary , though other notions such as trajectory length  have also been considered recently. The number of linear regions quantifies the flexibility of the function class, thus bounding the number of regions relates closely to both expressivity and generalization . To our knowledge, only the geometric complexity upper bounds for GCNs are known in the context of GNNs . We provide the first general lower and upper bounds for ReLU MPNNs, establishing complexity of popular architectures such as GraphSage and GIN, and recovering the results for GCNs and FNNs as special cases.

## 2 Preliminaries

### Message Passing Neural Networks (MPNNs)

A general \(T\)-layer MPNN \(\) is defined as a sequence of layers \(\{^{(t)}\}_{t=1}^{T}\), and takes as input a (directed) graph \(G=(V,E,X)\) with vertices \(v V\), edges \((u,v) E V V\), and features \(X=(X_{V},X_{E})\) consisting of node attributes \(X_{V}^{|V| d_{0}}\) and edge attributes \(X_{E}^{|E| d_{0}^{}}\). It produces as output a refined _embedding_ for each vertex and edge. Formally, \(\) acts on \(X\) as

\[(X)=^{(T)}...^{(1)}(X)\;,\;^{(t)}: ^{|V| d_{t-1}}^{|E| d_{t-1}^{}} ^{|V| d_{t}}^{|E| d_{t}^{ }}\]

is _permutation-equivariant_. Subsequently, depending on the task, the embedding for each node produced by \(\) is fed into another neural network, e.g., \(:^{d_{T}}^{out}\) for node classification or regression (and similarly for edges). An additional readout step \(_{}:^{|V| d_{T}^{|E| d_{T} ^{}}}^{d_{G}}\) is usually required to obtain a single vector from the output of \(\) for graph-level prediction.

We thus proceed to describing the working of each layer \(^{(t)}\) that yields a representation \(h_{v}^{(t)}^{d_{t}}\) for each vertex \(v V\) and a representation \(e_{uv}^{(t)}^{d_{t}^{}}\) for each edge \((u,v) E\). Assuming node embeddings are updated before edge embeddings (the converse works analogously), we have

\[h_{v}^{(t)} =_{}^{(t)}(h_{v}^{(t-1)},m_{v}^{(t)}), \] (1) \[m_{v}^{(t)} =_{}^{(t)}(h_{v}^{(t-1)},\{\{h_{u}^{(t-1)},e_{uv }^{(t-1)},e_{vu}^{(t-1)}|u(v)\}\})\;.\]

_Aggregation functions_\(\{_{Agg}^{(t)}\}\) are typically _permutation-invariant_, and we use an operator \(\) such as sum, average, or coordinate-wise max/min to combine the _messages_:

\[_{Agg}^{(t)}(h_{v}^{(t-1)},\{\{h_{u}^{(t-1)},e_{uv}^{(t-1)},e_{vu}^{(t- 1)}\}\}_{u(v)})=_{1}^{(t)}(_{u {Ne}(v)}^{(t)}_{2}^{(t)}(h_{v}^{(t-1)},h_{u}^{(t-1)},e_{uv}^{(t-1)},e_{vu}^{ (t-1)})),\] (2)

where \(\{_{1}^{(t)}\}\) and \(\{_{2}^{(t)}\}\) are usually implemented as FNNs, \((v)\) denotes the set of neighboring vertices of \(v\), and \(\{\{\}\}\) denotes a multiset. When there is no confusion, we usually write \(^{(t)}\) instead. Let \(L_{1}^{(t)}\) and \(L_{2}^{(t)}\) denote, respectively, the number of layers in \(_{1}^{(t)}\) and \(_{2}^{(t)}\). We use \(n_{1}^{t,}\) (and \(n_{2}^{t,}\)) to denote the dimension of layer \(\) in \(_{1}^{(t)}\) (and \(_{2}^{(t)}\)). On the other hand, the _update functions_ take the form

\[_{}^{(t)}(h_{v}^{(t-1)},m_{v}^{(t)})=_{ }^{(t)}(W_{}^{(t)}h_{v}^{(t-1)}+W_{}^{(t)}m_{v}^{(t)}).\]

Henceforth, we focus on sum and coordinate-wise max, since coordinate-wise min and average can be obtained from coordinate-max and sum respectively. We shall also number the vertices in \(V\) with \(A_{1},...,A_{|V|}\) and edges in \(E\) with \(e_{1},...,e_{|E|}\), and use the lexical order for edges, i.e.,

\(e_{uv}<e_{u^{}v^{}} u<u^{}\) or \((u=u^{}\) and \(v<v^{})\).

### Tropical Algebra

Here we adopt the notation from . Let \(=\{-\}\) be an extended set of real numbers. We equip \(\) with two binary operators **tropical sum**\(\) and **tropical multiplication**\(\):

\[a b=\{a,b\}\;; a b=a+b\;;\] \[a-=- a=a\;; a-=-  a=-\;,\]

where \(\) and \(+\) are the usual operators in \(\). Thus, \((,,)\) is a semi-ring with additive identity \(-\) and multiplicative identity \(0\). We also define the **tropical power**\(x^{ a}\) for each \(x\):

\[x^{ a}=x... x=a x&a\\ (-x)^{(-a)}&a,- ^{ a}=&a\{0\}\\ 0&a=0\;;\]

where \(\) is the standard product over \(\), and often abbreviate \(x^{ a}\) to \(x^{a}\) without inducing any confusion. A **tropical monomial** in \(m\) variables takes the form \(c x_{1}^{a_{1}}... x_{m}^{a_{m}}\) for \(c\) and \(a_{1},...,a_{m}\), and is often denoted by multi-index shorthand \(cx^{}\), where \(=(a_{1},,a_{m})^{m}\) and \(x=(x_{1},,x_{m})^{m}\). Note that this may be interpreted as the affine combination \(x^{}+c\).

A **tropical polynomial**\(f(x)=c_{1}x^{_{1}}... c_{r}x^{_{r}}\) is a finite tropical sum of tropical monomials, and amounts to a max over finitely many terms, i.e., \(f(x)=_{i=1}^{r}\{x^{}_{i}+c_{i}\}\). Without loss of generality, we assume \(_{i}_{j}\) for \(i j\) since we can combine monomials with the same \(\) into one. A **tropical rational function**\(f g(x)=f(x)-g(x)\) is the difference between two tropical polynomials. A map \(F:^{m}^{p}\) with each component a tropical polynomial [resp. tropical rational function] is called a **tropical polynomial map** [resp. **tropical rational map**], and belongs to the set \(Pol(m,p)\) [resp. \(Rat(m,p)\)].

When we allow \(_{i}^{m}\) instead of restricting its components to integers (\(_{i}^{m}\)), we obtain a **tropical signomial function/map** [resp. **tropical rational signomial function/map**] instead of a tropical polynomial/map [resp. tropical rational function/map]. It is known that each tropical rational signomial map (TRSM) is a continuous piecewise linear map (CPLM) and vice versa .

Tropical Algebra of MPNNs

In this section, we characterize the class of functions learned by ReLU MPNNs, whose activation functions for both nodes and edges (i.e., those used for \(\), \(_{1}^{(t)},_{2}^{(t)},_{}^{(l)}\), etc.) are of the form:

\[^{(l)}(x)=\{x,t^{(l)}\},t^{(l)}(- )^{n_{l}}\,.\]

In particular, note that \(^{(l)}(x)=x\) when \(t^{(l)}=-\). In contrast, \(t^{(l)}=0\) purges all negative inputs.

Let \(_{},_{},_{ },_{}\) be the set of functions represented by all ReLU MPNNs, ReLU FNNs, CPLMs and TRSMs respectively.  established the equivalence of ReLU FNNs, CPLMs and TRSMs.

**Lemma 1** ().: \(_{}=_{}=_{}\,.\)__

We will now extend this result to establish equivalence with \(_{}\). Equating ReLU MPNNs with CPLMs is rather nuanced, since nodes in each layer share the weights. Therefore, we employ two reductions showing (1) every ReLU FNN can be cast as a ReLU MPNN, and (2) every ReLU MPNN, in turn, can be expressed as a TRSM.

**Proposition 1**.: _[Equivalence of ReLU MPNNs, ReLU FNNs, TRSMs and CPLMs] \(_{}=_{}=_{}= _{}\). In other words, the following families are equivalent (with \(m=|V|d+|E|d^{}\) and \(p=|V|d_{out}+|E|d^{}_{out}\))._

1. _ReLU MPNNs_ \(:^{|V| d}^{|E| d^{}} ^{|V| d_{out}}^{|E| d^{}_{out}}\)_;_
2. _Tropical rational signomial maps (TRSMs)_ \(F G:^{m}^{p}\) _;_
3. _Continuous piecewise linear maps (CPLMs)_ \(:^{m}^{p}\) _;_
4. _ReLU FNNs_ \(:^{m}^{p}\)_._

**Remark 1**.: _In contrast to the WL test, which exposes the limitations of MPNNs via injective hash functions, Proposition 1 characterizes the class of functions that can be represented by MPNNs with ReLU activations (that are non-injective). Note, however, that Proposition 1 does not quantify how effective ReLU MPNNs are in representing CPLMs. Moreover, the equivalence between ReLU MPNNs and ReLU FNNs (according to Proposition 1) does not explain the observed empirical discrepancy between ReLU MPNNs and ReLU FNNs in practice. This motivates our subsequent analysis and results (Section 5 and Table 1), which investigate the benefits of ReLU MPNNs in terms of both the number of learnable parameters and the number of layers required to represent the same CPLM._

## 4 Geometric Complexity of ReLU MPNNs

We now invoke tools from tropical algebraic geometry to study the geometric complexity of ReLU MPNNs, and provide some insights into the model architecture. Following , for a CPLM \(f:^{m}^{p}\), we define its _linear degree_\((f)\) to be \(K\), where \(K\) is the least number of connected regions \(_{k}\) of \(^{m}\) such that the restriction \(f|_{_{k}}\) is affine. Equivalently, following , we can also define \(K=(f)\) as follows: \(f:^{m}^{p}\) is a CPLM if \(f\) is continuous and there exists a set \(\{f_{k}:k\{1,...,K\}\}\) of affine functions and maximal connected subsets \((_{k})_{k=1}^{K}\) satisfying the following conditions:

\[_{i}_{j}=;_{k=1}^{K}_{k}=^{m}; f|_{_{k}}=f_{k}.\]

Similarly, we define the _convex degree_\(_{c}(f)\) where we require additionally that \(_{k}\) is convex. We further define \(_{c}(f|m^{})\) as the maximum convex degree across restrictions of \(f\) to different \(m^{}\)-dimensional affine subspaces of \(^{m}\), \(m^{} m\). We will analyze general upper and lower bounds on \(()\) for a ReLU MPNN \(\). Recall our setting of MPNN layers from Section 2.1 consisting of \(_{}^{(t)}\) and \(_{}^{(t)}\). Note that \(n_{1}^{t,l}\) and \(n_{2}^{t,l}\) denote the intermediate dimensions in \(_{1}^{(t)}\) and \(_{2}^{(t)}\). Let \(_{t}\) and \(_{t}\) be the output dimension of \(_{2}^{(t)}\) and \(_{1}^{(t)}\), thus \(n_{2}^{t,L_{2}^{(t)}}=_{t}\).

To analyze the number of linear region for a ReLU MPNN \(\), we make a simplifying assumption that all input graphs to \(\) have the same graph structure \(G\). In particular, we denote the sum of degrees of all vertices in \(G\) by \(D\), and the maximum degree by \(S\). A key step in our analysis of geometric complexity is building a ReLU FNN that can be applied on the vectorized input \(H^{(t-1)}\) of all node embeddings \(h_{v}^{(t-1)}\). Using the notation indicated in Equation 1, we form a ReLU FNN \(_{2}^{(t)}\) that achieves the same effect as \(_{2}^{(t)}\) for every adjacent node embedding. The aggregation operator can be seen as either a matrix multiplication for sum, or a FNN \(_{3}^{(t)}\) for max aggregation - the difference between these two cases will be discussed in Section 4.3. Similarly, we form a ReLU FNN \(_{1}^{(t)}\) for \(_{1}^{(t)}\), which in combination with \(_{2}^{(t)}\) and \(_{3}^{(t)}\) can be seen as a ReLU FNN \(_{}^{(t)}\).

Our next proposition is important in the analysis for geometric complexity of \(\), as it relates the geometric complexity of the model up to the \(t+1\)-th layer to that of the \(t-\)th layer, and the update as well as the message components. We define the vectorized concatenated embedding of all the vertices in the \(t\)-th layer to be \(H^{(t)}^{|V|d_{t}}\) and show that the result of \(t\)-th message aggregation step can be written as a result of a FNN \(_{}^{t}(H^{(t)})\). Modifying any aggregation or update component thus will affect the bound and in particular, the choice of aggregation operator has an impact on the geometric complexity (which we will discuss in Section 4.3).

**Proposition 2**.: _[Recursive formula for geometric complexity]_

\[(^{(t+1)})_{c}(^{(t+1)})_{c}(^{(t+1)}_{}||V|(d_{t}+_{t+1}))\;_{c}( _{}^{(t+1)}||V|d_{t})\;_{c}(^{(t)}).\] (2)

The ideas and proofs for Proposition 2 build on , and the details can be found in the Appendix.

### Lower bound of geometric complexity

We note that the lower bound on the linear degree of \(\) depends heavily on the choice of weights and biases; e.g., setting their value to 0 trivially results in \(()=0\). Thus, a lower bound on the geometric complexity (i.e., maximal linear degree) is of greater interest. In this subsection, we will provide a general lower bound for the maximum number of linear regions for a ReLU MPNN, building on the work of .

**Theorem 3**.: _[Lower bound on the maximum number of linear regions] Assume for all \(t,l\), we have \(n_{1}^{t,l},n_{2}^{t,l} d_{0}\) and let \(n_{1,d_{0}}^{t,l}=^{t,l}}{d_{0}}^{d_{0}}\) and \(n_{2,d_{0}}^{t,l}=^{t,l}}{d_{0}}^{d_{0}}\) then the maximum number of linear regions of functions computed by any ReLU MPNN is lower bounded by_

\[S^{t_{0}}^{T}(_{l=1}^{L^{(t)}_{1}}n_{1,d_{0} }^{t,l}_{l=1}^{L^{(t)}_{2}}n_{2,d_{0}}^{t,l}))}{n_{1,d_{0}}^ {T,L^{(T)}_{1}}}_{j=0}^{d_{0}}}{j},\]

_where \(t_{0}\) is the number of MPNN layer having max as aggregation operator and for each layer \(t\), the index \(l\) runs through every layer in \(_{2}^{(t)},_{1}^{(t)}\)._

We now sketch some intuition about this result. We can add to \(_{1}^{(t)}\) (constructed in Algorithm 3 in the Appendix) an initial layer to calculate \(^{(t)}=\), while the aggregation \(^{(t)}=\) can be seen as a FNN-layer with rank \(S\) max activation, thus we can identify \(S\) input regions (indicated in red). By setting \(_{}^{(t)}(h_{v}^{(t-1)},m_{v}^{(t)})=m_{v}\), we can express the whole MPNN \(\) as a FNN applied to input \(X\) (details in Algorithm 1, 2 and 3 in the Supplementary). We build on the analysis in  to construct intermediate layers that identify \(}{d_{0}}^{d_{0}}\) input regions. Our procedure amounts to sequentially folding the input space until the last layer (indicated in blue), and then replicating the hyperplane arrangement in the last layer (indicated in green).

### Geometric complexity - Aggregation and Update steps

We now provide a general upper bound for the geometric complexity of ReLU MPNNs when all the weights take integer values - this assumption is mild and holds without loss of generality (details in the Appendix). We call these models _integer-weighted ReLU MPNNs_. A result similar to Proposition 1 establishes the equivalence of integer-weight ReLU MPNNs with tropical rational maps.

For our analysis, we require a technical condition that the network "does not shrink" the representation in the following sense: each intermediate dimension \(n_{1}^{t,l}\) and \(n_{2}^{t,l}\) of \(_{1}^{(t)}\) and \(_{2}^{(t)}\) should be sufficiently large; and the dimension of the new embedding is at least the dimension of the aggregated message plus the dimension of the previous embedding. These conditions are standard in the analysis using tropical geometry, see e.g., .

**Theorem 4**.: _[Upper bound on the geometric complexity] Let \(:^{|V|d_{0}}^{|V|d_{T}}\) be an integer-weight ReLU MPNN. If \(^{(t)}\) satisfies the following conditions for all MPNN layer \(t=1,..,T\)_

* \(n_{2}^{t,l}d_{t-1}\) _for all_ \(l=1,...,L_{2}^{(t)}\)_;_
* \(n_{1}^{t,l} d_{t-1}\) _for all_ \(l=1,...,L_{1}^{(t)}\)_;_
* \(n_{1}^{t,L_{1}^{(t)}}+d_{t-1} d_{t}\)_;_

_then the linear degree of \(H^{(T)}\) is at most_

\[_{t=1}^{T}^{L_{1}^{(t)}-1}_{i=0}^{|V|d_ {t-1}}^{t,l}}{i})}_{\;_{1}^{(t)}} ^{L_{2}^{(t)}-1}_{i=0}^{|V| d_{t-1}} ^{t,l}}{i})}_{\;_{2}^{(t)}}^{|V|(_{t}+d_{t-1})}}{i})}_{ \;_{(t)}^{(t)}}_{c}(^{(t)}),\]

_where \(_{c}(^{(t)})1&\;^{(t)}\; \\ (8S)^{D_{t}}&\;^{(t)}\;\;.\)_

We emphasize that, to the best of our knowledge, this the first upper bound for general ReLU message passing architectures. Furthermore, we recover the upper bounds for FNNs and GCNs (with ReLU activations and integer-weights) established in  and  respectively as special cases.

**Corollary 1**.: _The linear degree of an integer-weight FNN is at most \(_{t=1}^{T}(_{i=0}^{d_{t}}}{i})\)._

**Corollary 2**.: _The linear degree of a GCN \(\) with \(T\) hidden layers, ReLU activation, sum aggregation and integer weight is at most \(_{t=1}^{T}(_{i=0}^{|V|d}}{i})\)._

On the other hand, we obtain new bounds for popular GNN models, particularly **GraphSAGE** and **GIN**. For **GraphSAGE**, note that the normalization steps do not change the linear degree.

**Corollary 3**.: _Let \(_{t}\) be the output dimension of Aggregate\({}_{t}\) in GraphSAGE . If \(d_{t} d_{t-1}+_{t} 2d_{t-1}\) for all MPNN layers \(t=1,...,T\), then the linear degree of integer-weighted ReLU GraphSAGE described in  is upper bounded by_

\[_{c}(^{T})_{c}(^{2}),\]

_where_

\[_{c}(^{t})_{i=0}^{2|V|d_{t-1}}}{i}&\\ (_{i=0}^{|V|_{t}}_{t}}{i})( _{i=0}^{|V|(_{t}+d_{t-1})}}{i}) \]

**Corollary 4**.: _Node embedding of integer-weighted ReLU GIN can be written as [15, Equation 4.1]. Let \(n_{t,i}\) be the dimension of each of the intermediate layer in the \(MLP^{(t)}\), then its linear degree_

\[_{c}()_{t=1}^{T}(_{l=1}^{L^{(t)}-1}_{i= 0}^{|V|d_{t}}}{i}).\]

### Coordinate-wise max vs. sum for aggregation

**Proposition 5**.: _[Coordinate-wise max has greater geometric complexity than sum]_

\[(^{(t)})=1\] \[S^{\{|V|,D\}_{t}} (^{(t)})\{_{i=0}^{|D|d_{t+1}} |V|_{t}}{i},S^{|V|d_{t}}\}\;^{ (t)}=.\]

To establish Proposition 5 we again adapt . Note that if \(D|V|\) (the graph is not too sparse, and we have enough messages between the vertices), we have that \((^{(t)})=S^{|V|_{t}}\) if \(^{(t)}=\). In that case, the geometric complexity will grow polynomially with \(S\) (maximum degree).

**Remark 2**.: _Interestingly, in , the authors point out that if \(_{1}\) and \(_{2}\) are injective, then \(\) is as powerful as the WL test. Under that assumption, coordinate-wise is less "expressive" than mean, which is less "expressive" than sum._

_In contrast, \(_{1}\) and \(_{2}\) are not injective in our case (since ReLU activation is not injective), max is more "expressive" (as measured by the notion of geometric complexity), thus providing another novel insight. Here, the connectivity of the graph plays a particularly important role._

## 5 New ReLU MPNNs architectures and complexity tradeoffs

Note that while both ReLU FNNs and ReLU MPNNs learn TRSMs/CPLMs (Proposition 1), they might differ vastly in terms of their resource requirements (e.g., the number of layers and parameters). Therefore, we proceed to comparing the complexity of representing a TRSM under the two paradigms (we do not need integer-weight assumption in this section). We simplify our analysis by noting that each component of a TRSM results from the difference of two tropical signomial functions (TSFs) and these TSFs can be computed in parallel using shared layers. Thus, hereafter, we shall focus on TSFs, i.e., functions \(f:^{m}\) of the form \(f(x)=_{i=1}^{r}c_{i} x^{_{i}}\), where \(c_{i},_{i}^{m}\).

Our idea is to construct as input a clique (i.e., a fully-connected graph) with \(m\) nodes \(A_{1}\),..., \(A_{m}\) and distribute the \(r\) monomials (almost) evenly such that each node \(A_{i}\) contains \(r^{}=\) monomials (padding with zero monomials if \(m r^{}>r\)). Our construction makes use of a comparison gadget which is already introduced in  and , and introduces a novel selection gadget (details in the Appendix) which proves to be useful with the permutation equivariance restriction of MPNNs.

The two MPNNs below differ in terms of the way they compare the monomials.

**Global comparison**: Compare \(m\) monomials, one from each \(A_{i}\), simultaneously using coordinate-wise max aggregation. Now redistribute (using the selection gadget) the resulting \(O(r^{})\) maximum monomials (each coordinate yields one such monomial) evenly across the nodes, and recur until the maximum across all monomials is obtained.

**Local comparison**: First employ a recursive procedure to compare \(r^{}\) monomials assigned to each \(A_{i}\) locally in order to find a maximum monomial for \(A_{i}\) (breaking ties arbitrarily). Now compare the maximum monomials across the nodes using a single global comparison described above.

One limitation of both these architectures is that they can make no more than \(m\) comparisons at a time but \(r\) can be much larger. Fortunately, a theorem due to , also exploited previously by , motivates our construction of a **Constant MPNN** consisting only a constant number of layers to represent any TSF. However, the number of learnable parameters in this MPNN can be exponential in the worst case.

**Remark 3** (Nature of MPNN and Fnn).: _Without the equivariance constraint on the message component of MPNN, FNN is much more efficient at constructing affine combinations or tropical monomials. On the other hand, thanks to the parallel MP paradigm of MPNN, it is more effective at comparing these monomials, or in other words, increases the geometric complexity of the model. This benefit of MPNN can be seen in Theorem 4, where one has to build much bigger \(_{1}^{(t)}\) and \(_{2}^{(t)}\) to reconstruct the result of the parallel \(_{1}^{(t)}\) and \(_{2}^{(t)}\)._

We therefore proceed to our final algorithm that combines the strengths of these two paradigms.

**Hybrid MPNN**: We first use a layer of FNN to calculate \(r\) tropical monomials and then employ a single layer of MPNN over an \(r\)-clique with coordinate-max to learn \(f\).

**Proposition 6**.: _There exist ReLU MPNN algorithms (Local, Global, Constant, and Hybrid that can learn any TSF \(f:^{m}\) with \(r\) monomials. Their respective complexity and tradeoffs are summarized in Table 1._

We provide details about these algorithms and proofs of their correctness as well as complexity in the Supplementary. Local architecture: Algorithm 6 and Proposition 16, Global architecture: Algorithm 7 and Proposition 17 architecture, Constant architecture: Algorithm 8 and Proposition 18, Hybrid architecture: Algorithm 9 and Proposition 19.

## 6 Decision boundary

Lastly, we proceed to characterizing the decision boundary of integer-weighted ReLU MPNNs for classification. We focus on binary classification tasks with a single output to keep the exposition transparent and explicit, albeit one can adapt our construction and analysis accordingly to accommodate multiple classes and outputs. We analyze the decision boundary for both graph and node/edge predictions.

We begin with the analysis for graph classification. Recall from Section 2 that we need an additional readout step \(_{}:^{|V| d_{T}}^{|E| d _{T}^{}}^{d_{G}}\) prior to classification \(\), i.e.

\[:^{d|V|}^{d^{}|E|}(1)}{}^{d_{T}|V| }^{d_{T}^{}|E|}}}{ }^{d_{G}}.\]

Let \(:\) be an injective score function. For \(c\) in \(()\), we define the decision boundary of \(\) as \(=\{z^{m}:(z)=^{-1}(c)\}\), where \(m=d|V|+d^{}|E|\). The tropical hypersurface \((f)\) is precisely the set of points \(x\) where \(f\) is not linear; i.e., two or more monomials in \(f\) achieve the value of \(f\) at \(x\). We adapt a result from (19, Proposition 6.1) to arrive at the following proposition.

**Proposition 7** (Decision boundary for graph classification).: _Let \(:^{d|V|}^{d^{}|E|} \) be a ReLU MPNN and \(:\) be an injective score function with \(c\) in its range. Then \(\) can be viewed as a tropical rational function, \(f g\) and the decision boundary \(\) defined above divides \(^{m}\) into at most \((f)\) connected positive regions and at most \((g)\) connected negative regions. Furthermore, \(\) is contained in the tropical hypersurface of a specific tropical polynomial, namely,_

\[(^{-1}(c) g f)\.\]

However, the characterization for node and edge classification requires a more nuanced analysis. We focus on node classification, since the treatment for edges is analogous. In this setting the neural network \(:^{d_{T}}\) is applied to the embedding of each node simultaneously, and a scoring function \(_{i}:\) is then employed for each node to predict its class based on its score \(c_{i}\). Generally, different scoring functions \(_{i}\) may be applied, but in practice, we often use the same score function for all the nodes. Viewed individually, the decision boundary for each vertex is similar to Proposition 7; so we pursue a more interesting problem, namely, characterizing the decision boundary of the vertices resulting from a vectorized score function \(:^{|V|}^{|V|},[(z)]_{i}= _{i}(z)\). However, we immediately hit a roadblock, since choosing a meaningful order for \(c_{i}\) is problematic: the product or subset order is not total whereas the lexical order relies heavily on the ordering of the vertices and thus violates permutation equivariance of the MPNN paradigm. Therefore, we introduce the following notion for the decision boundary.

**Definition 1**.: _The decision boundary \(\) is defined as \(_{i=1}^{|V|}_{i}\), where \(_{i}=\{z^{m}:_{i}(z)=_{i}^{-1}(c_{i})\}\)._

In order to proceed, we need to generalize the analysis to tropical hypersurfaces. Specifically, we relate \(\) with the tropical hypersurface of the components of the corresponding tropical rational map. Invoking Proposition 6.1 in , we establish the following proposition.

**Proposition 8** (Decision boundary for node classification).: _Let \(:^{d|V|}^{d^{}|E|} ^{|V|}\) be a ReLU MPNN and \(_{i}:,i=1,2,...,|V|\) be injective score functions with \(c_{i}\) in their range._

 
**Previously** & **Message layers** & **Feedforward layers** & **Learnable parameters** \\  Deep NN in  & None & \(_{2}(r)+1\) & \((rm)\) \\  Deep NN in  & None & \(_{2}(m)+1\) & \((rm)\) \\ 
**New (in this work)** & & & \\  Local (Algorithm 6) & 2 & \(_{2}(r/m)+5\) & \((rm)\) \\  Global (Algorithm 7) & \(_{2}(r)+1\) & \(3_{m}(r)+2\) & \((rm)\) \\  Constant (Algorithm 8) & 2 & 7 & \((mr^{m+2})\) \\  Hybrid (Algorithm 9) & 1 & 1 & 0 \\  

Table 1: Complexity of representing any tropical signomial function (TSFs) \(f:^{m}\) consisting of \(r\) tropical monomials with different architectures. One more layer is required to compute any tropical rational signomial map (TRSM). The four new methods introduced here construct a graph (based on \(m\) and \(r\)) and leverage message passing to efficiently compare these monomials.

_Then \(\) can be viewed as a tropical rational map \(F G\) and its decision boundary is contained in the tropical hypersurface of a specific tropical polynomial, namely,_

\[_{i=1}^{|V|}(^{-1}(c_{i}) G_{i}  F_{i}).\]

## Conclusion, Broader Impact, and Limitations

In this paper, we characterize the class of functions learned by ReLU MPNNs through the lens of Tropical geometry. Thus, beyond previous works that are limited to utilizing tropical geometry in the context of ReLU FNNs, our analysis expands the scope to a widely employed class of GNNs, laying the groundwork for further work on the connections with other machine learning models.

We provide both the lower and upper bounds for the number of linear regions of ReLU MPNNs. The upper bound makes some simplifying assumptions; however, Theorem 4 is still general enough to recover existing bounds for FNNs, GCNs, and provide new bounds for widely used GNN architectures such as GraphSAGE and GIN. Our bound is rather analytical; a numerical approach to counting the number of linear regions can be found in . Adapting the method in  for MPNNs, and comparing to our analytical bounds, is an interesting future direction.

We also show that the max aggregation operator is more expressive than the sum operator in terms of geometric complexity of ReLU MPNNs (see Remark 2 on how this result contrasts with the implications of the WL test for injective aggregation functions). We thus showcase the dependence of expressivity on message aggregation operator, and furthermore, the connectivity of the graph structure (max and total degree). It remains open whether spectral quantities such as the spectrum of Laplacian have any effect on the geoemetric complexity of ReLU MPNNs.

The theoretical result on equivalence of class of functions learned by ReLU MPNNs and ReLU FNNs is usually not reflected in practice, where ReLU MPNNs typically outperform FNNs. This motivates our results in Section 5, where we consider several ReLU MPNN architectures to represent CPLMs and compare them to ReLU FNNs. Remark 3, Proposition 6 and Table 1 show that ReLU MPNNs can be more efficient in terms of both the number of learnable parameters as well as the number of layers required in order to be able to represent the same CPLMs. We however, sidestepped other important and practical considerations such as the difference in training time, prediction error and the role of optimisation algorithm (e.g. SGD/Adam). Our focus here has been purely theoretical, and we believe that (at least some of) proposed new architectures would benefit from a comprehensive empirical evaluation. We also characterize the decision boundary for graph classification and node classification, explaining the difference between the two.

Overall, we hope that this work fosters further research on design and analysis of modern deep architectures through the fruitful machinery of tropical geometry.