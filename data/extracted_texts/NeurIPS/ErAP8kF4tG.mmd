# Finite-Time Logarithmic Bayes Regret Upper Bounds

Alexia Atsidakou

University of Texas, Austin &Branislav Kveton

AWS AI Labs\({}^{*}\) &Sumeet Katariya

Amazon

Constantine Caramanis

University of Texas, Austin &Sujay Sanghavi

University of Texas, Austin / Amazon

###### Abstract

We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In a multi-armed bandit, we obtain \(O(c_{} n)\) and \(O(c_{h}^{2}n)\) upper bounds for an upper confidence bound algorithm, where \(c_{h}\) and \(c_{}\) are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing \(()\) bounds, which have become standard in the literature despite the logarithmic lower bound of Lai (1987).

## 1 Introduction

A _stochastic multi-armed bandit_(Lai and Robbins, 1985; Auer et al., 2002; Lattimore and Szepesvari, 2019) is an online learning problem where a _learner_ sequentially interacts with an environment over \(n\) rounds. In each round, the learner takes an _action_ and receives its _stochastic reward_. The goal of the learner is to maximize its expected cumulative reward over \(n\) rounds. The mean rewards of the actions are unknown _a priori_ but can be learned by taking the actions. Therefore, the learner faces the _exploration-exploitation dilemma: explore_, and learn more about the actions; or _exploit_, and take the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and subsequent adaptation are beneficial. One example are recommender systems (Li et al., 2010; Zhao et al., 2013; Kawale et al., 2015; Li et al., 2016), where the actions are recommended items and their rewards are clicks. Another example is hyper-parameter optimization (Li et al., 2018), where the actions are values of the optimized parameters and their reward is the optimized metric.

Cumulative regret minimization in stochastic bandits has been traditionally studied in two settings: frequentist (Lai and Robbins, 1985; Auer et al., 2002; Abbasi-Yadkori et al., 2011) and Bayesian (Gittins, 1979; Tsitsiklis, 1994; Lai, 1987; Russo and Van Roy, 2014; Russo et al., 2018). In the frequentist setting, the learner minimizes the regret with respect to a fixed unknown bandit instance. In the Bayesian setting, the learner minimizes the average regret with respect to bandit instances drawn from a prior distribution. The instance is unknown but the learner knows its prior distribution. The Bayesian setting allows surprisingly simple and insightful analyses of Thompson sampling. One fundamental result in this setting is that linear Thompson sampling (Russo and Van Roy, 2014) has a comparable regret bound to LinUCB in the frequentist setting (Abbasi-Yadkori et al., 2011; Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). Moreover, many recent meta- and multi-task bandit works (Bastani et al., 2019; Kveton et al., 2021; Basu et al., 2021; Simchowitz et al., 2021; Wang et al., 2021; Hong et al., 2022; Aouali et al., 2023) adopt the Bayes regret to analyze the stochasticstructure of their problems, that the bandit tasks are similar because their parameters are sampled i.i.d. from a task distribution.

Many bandit algorithms have frequentist regret bounds that match a lower bound. As an example, in a \(K\)-armed bandit with the minimum gap \(\) and horizon \(n\), the gap-dependent \(O(K^{-1} n)\) regret bound of UCB1 (Auer et al., 2002) matches the gap-dependent \((K^{-1} n)\) lower bound of Lai and Robbins (1985). Moreover, the gap-free \(()\) regret bound of UCB1 matches, up to logarithmic factors, the gap-free \(()\) lower bound of Auer et al. (1995). The extra logarithmic factor in the \(()\) bound can be eliminated by modifying UCB1 (Audibert and Bubeck, 2009). In contrast, and despite the popularity of the model, matching upper and lower bounds mostly do not exist in the Bayesian setting. Specifically, Lai (1987) proved _asymptotic_\(c_{h}^{2}n\) upper and lower bounds, where \(c_{h}\) is a prior-dependent constant. However, all recent Bayes regret bounds are \(()\)(Russo and Van Roy, 2014, 2016; Lu and Van Roy, 2019; Hong et al., 2020; Kveton et al., 2021). This leaves open the question of finite-time logarithmic regret bounds in the Bayesian setting.

In this work, we answer this question positively and make the following contributions:

1. We derive the first finite-time logarithmic Bayes regret upper bounds for a Bayesian _upper confidence bound (UCB)_ algorithm. The bounds are \(O(c_{} n)\) and \(O(c_{h}^{2}n)\), where \(c_{h}\) and \(c_{}\) are constants depending on the prior distribution \(h\) and the gaps of random bandit instances sampled from \(h\), respectively. The latter matches the lower bound of Lai (1987) asymptotically. When compared to prior \(()\) bounds, we better characterize low-regret regimes, where the random gaps are large.
2. To show the value of prior as a side information, we also derive a finite-time logarithmic Bayes regret upper bound for a frequentist UCB algorithm. The bound changes only little as the prior becomes more informative, while the regret bound for the Bayesian algorithm eventually goes to zero. The bounds match asymptotically when \(n\) and the prior is overtaken by data.
3. To show the generality of our approach, we prove a \(O(d\,c_{}^{2}n)\) Bayes regret bound for a Bayesian linear bandit algorithm, where \(d\) denotes the number of dimensions and \(c_{}\) is a constant depending on random gaps. This bound also improves with a better prior.
4. Our analyses are a major departure from all recent Bayesian bandit analyses, starting with Russo and Van Roy (2014). Roughly speaking, we first bound the regret in a fixed bandit instance, similarly to frequentist analyses, and then integrate out the random gap.
5. We show the tightness of our bounds empirically and compare them to prior bounds.

This paper is organized as follows. In Section 2, we introduce the setting of Bayesian bandits. In Section 3, we present a Bayesian upper confidence bound algorithm called BayesUCB (Kaufmann et al., 2012). In Section 4, we derive finite-time logarithmic Bayes regret bounds for BayesUCB, in both multi-armed and linear bandits. These are the first such bounds ever derived. In Section 5, we compare our bounds to prior works and show that one matches an existing lower bound (Lai, 1987) asymptotically. In Section 6, we evaluate the bounds empirically. We conclude in Section 7.

## 2 Setting

We start with introducing our notation. Random variables are capitalized, except for Greek letters like \(\). For any positive integer \(n\), we define \([n]=\{1,,n\}\). The indicator function is \(\{\}\). The \(i\)-th entry of vector \(v\) is \(v_{i}\). If the vector is already indexed, such as \(v_{j}\), we write \(v_{j,i}\). We denote the maximum and minimum eigenvalues of matrix \(M^{d d}\) by \(_{1}(M)\) and \(_{d}(M)\), respectively.

Our setting is defined as follows. We have a _multi-armed bandit_(Lai and Robbins, 1985; Lai, 1987; Auer et al., 2002; Abbasi-Yadkori et al., 2011) with an _action set_\(\). Each _action_\(a\) is associated with a _reward distribution_\(p_{a}(;)\), which is parameterized by an unknown _model parameter_\(\) shared by all actions. The learner interacts with the bandit instance for \(n\) rounds indexed by \(t[n]\). In each round \(t\), it takes an _action_\(A_{t}\) and observes its _stochastic reward_\(Y_{t} p_{A_{t}}(;)\). The rewards are sampled independently across the rounds. We denote the mean of \(p_{a}(;)\) by \(_{a}()\) and call it the _mean reward_ of action \(a\). The optimal action is \(A_{*}=*{arg\,max}_{a}_{a}()\) and its mean reward is \(_{*}()=_{A_{*}}()\). For a fixed model parameter \(\), the \(n\)-round _regret_ of a policy is defined as

\[R(n;)=[_{t=1}^{n}_{*}()-_{A_{t}}() |\,.]\,,\]

where the expectation is taken over both random observations \(Y_{t}\) and actions \(A_{t}\). The _suboptimality gap_ of action \(a\) is \(_{a}=_{*}()-_{a}()\) and the _minimum gap_ is \(_{}=_{a\{A_{*}\}}_{a}\).

Two settings are common in stochastic bandits. In the _frequentist_ setting (Lai and Robbins, 1985; Auer et al., 2002; Abbasi-Yadkori et al., 2011), the learner has no additional information about \(\) and its objective is to minimize the worst-case regret for any bounded \(\). We study the _Bayesian_ setting (Gittins, 1979; Lai, 1987; Russo and Van Roy, 2014; Russo et al., 2018), where the model parameter \(\) is drawn from a _prior distribution_\(h\) that is given to the learner as a side information. The goal of the learner is to minimize the \(n\)-round _Bayes regret_\(R(n)=[R(n;)]\), where the expectation is taken over the random model parameter \( h\). Note that \(A_{*}\), \(_{a}\), and \(_{}\) are random because they depend on the random instance \(\).

## 3 Algorithm

We study a Bayesian upper confidence bound algorithm called BayesUCB(Kaufmann et al., 2012). The algorithm was analyzed in the Bayesian setting by Russo and Van Roy (2014). The key idea in BayesUCB is to take the action with the highest UCB with respect to the posterior distribution of model parameter \(\). This differentiates it from frequentist algorithms, such as UCB1(Auer et al., 2002) and LinUCB(Abbasi-Yadkori et al., 2011), where the UCBs are computed using a frequentist _maximum likelihood estimate (MLE)_ of the model parameter.

Let \(H_{t}=(A_{},Y_{})_{[t-1]}\) be the _history_ of taken actions and their observed rewards up to round \(t\). The _Bayesian UCB_ for the mean reward of action \(a\) at round \(t\) is

\[U_{t,a}=_{a}(_{t})+C_{t,a}\,,\] (1)

where \(_{t}\) is the _posterior mean estimate_ of \(\) at round \(t\) and \(C_{t,a}\) is a _confidence interval width_ for action \(a\) at round \(t\). The posterior distribution of model parameter \(\) is computed from a prior \(h\) and history \(H_{t}\) using Bayes' rule. The width is chosen so that \(|_{a}(_{t})-_{a}()| C_{t,a}\) holds with a high probability conditioned on any history \(H_{t}\). Technically speaking, \(C_{t,a}\) is a half-width but we call it a width to simplify terminology.

Our algorithm is presented in Algorithm 1. We instantiate it in a Gaussian bandit in Section 3.1, in a Bernoulli bandit in Section 3.2, and in a linear bandit with Gaussian rewards in Section 3.3. These settings are of practical interest because they lead to computationally-efficient implementations that can be analyzed due to closed-form posteriors (Lu and Van Roy, 2019; Kveton et al., 2021; Basu et al., 2021; Wang et al., 2021; Hong et al., 2022). While we focus on deriving logarithmic Bayes regret bounds for BayesUCB, we believe that similar analyses can be done for Thompson sampling (Thompson, 1933; Chapelle and Li, 2012; Agrawal and Goyal, 2012, 2013; Russo and Van Roy, 2014; Russo et al., 2018). This extension is non-trivial because a key step in our analysis is that the action with the highest UCB is taken (Section 5.3).

```
1:for\(t=1,,n\)do
2: Compute the posterior distribution of \(\) using prior \(h\) and history \(H_{t}\)
3:for each action \(a\)do
4: Compute \(U_{t,a}\) according to (1)
5: Take action \(A_{t}*{arg\,max}_{a}U_{t,a}\) and observe its reward \(Y_{t}\) ```

**Algorithm 1**BayesUCB

### Gaussian Bandit

In a \(K\)-armed Gaussian bandit, the action set is \(=[K]\) and the model parameter is \(^{K}\). Each action \(a\) has a Gaussian reward distribution, \(p_{a}(;)=(;_{a},^{2})\), where \(_{a}\) is its mean and \(>0\) is a known reward noise. Thus \(_{a}()=_{a}\). The model parameter \(\) is drawn from a known Gaussian prior \(h()=(;_{0},_{0}^{2}I_{K})\), where \(_{0}^{K}\) is a vector of prior means and \(_{0}>0\) is a prior width.

The posterior distribution of the mean reward of action \(a\) at round \(t\) is \((;_{t,a},_{t,a}^{2})\), where

\[_{t,a}^{2}=(_{0}^{-2}+^{-2}N_{t,a})^{-1}\]

is the posterior variance, \(N_{t,a}=_{=1}^{t-1}\{A_{}=a\}\) is the number of observations of action \(a\) up to round \(t\), and

\[_{t,a}=_{t,a}^{2}(_{0}^{-2}_{0,a}+ ^{-2}_{=1}^{t-1}\{A_{}=a\}Y_{})\]

is the posterior mean. This follows from a classic result, that the posterior distribution of the mean of a Gaussian random variable with a Gaussian prior is a Gaussian (Bishop, 2006). The Bayesian UCB of action \(a\) at round \(t\) is \(U_{t,a}=_{t,a}+C_{t,a}\), where \(C_{t,a}=_{t,a}^{2}(1/)}\) is the confidence interval width and \((0,1)\) is a failure probability of the confidence interval.

### Bernoulli Bandit

In a \(K\)-armed Bernoulli bandit, the action set is \(=[K]\) and the model parameter is \(^{K}\). Each action \(a\) has a Bernoulli reward distribution, \(p_{a}(;)=(;_{a})\), where \(_{a}\) is its mean. Hence \(_{a}()=_{a}\). Each parameter \(_{a}\) is drawn from a known prior \((;_{a},_{a})\), where \(_{a}>0\) and \(_{a}>0\) are positive and negative prior pseudo-counts, respectively.

The posterior distribution of the mean reward of action \(a\) at round \(t\) is \((;_{t,a},_{t,a})\), where

\[_{t,a}=_{a}+_{=1}^{t-1}\{A_{}=a \}Y_{}\,,_{t,a}=_{a}+_{=1}^{t-1} \{A_{}=a\}(1-Y_{})\,.\]

This follows from a classic result, that the posterior distribution of the mean of a Bernoulli random variable with a beta prior is a beta distribution (Bishop, 2006). The corresponding Bayesian UCB is \(U_{t,a}=_{t,a}+C_{t,a}\), where

\[_{t,a}=}{_{t,a}+_{t,a}}\,, C_{ t,a}=+_{t,a}+1)}}=+_{a}+N_{t,a}+1)}}\,,\]

denote the posterior mean and confidence interval width, respectively, of action \(a\) at round \(t\); and \((0,1)\) is a failure probability of the confidence interval. The confidence interval is derived using the fact that \((;_{t,a},_{t,a})\) is a sub-Gaussian distribution with variance proxy \(+_{a}+N_{t,a}+1)}\)(Marchal and Arbel, 2017).

### Linear Bandit with Gaussian Rewards

We also study linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011) with a finite number of actions \(^{d}\) in \(d\) dimensions. The model parameter is \(^{d}\). All actions \(a\) have Gaussian reward distributions, \(p_{a}(;)=(;a^{},^{2})\), where \(>0\) is a known reward noise. Therefore, the mean reward of action \(a\) is \(_{a}()=a^{}\). The parameter \(\) is drawn from a known multivariate Gaussian prior \(h()=(;_{0},_{0})\), where \(_{0}^{d}\) is its mean and \(_{0}^{d d}\) is its covariance, represented by a _positive semi-definite (PSD)_ matrix.

The posterior distribution of \(\) at round \(t\) is \((;_{t},_{t})\), where

\[_{t}=_{t}(_{0}^{-1}_{0}+^{-2} _{=1}^{t-1}A_{}Y_{})\,,_{t}=(_{0 }^{-1}+G_{t})^{-1}\,, G_{t}=^{-2}_{=1}^{t-1}A_{}A_{ }^{}\,.\]

Here \(_{t}\) and \(_{t}\) are the posterior mean and covariance of \(\), respectively, and \(G_{t}\) is the outer product of the feature vectors of the taken actions up to round \(t\). These formulas follow from a classic result,that the posterior distribution of a linear model parameter with a Gaussian prior and observations is a Gaussian (Bishop, 2006). The Bayesian UCB of action \(a\) at round \(t\) is \(U_{t,a}=a^{}_{t}+C_{t,a}\), where \(C_{t,a}=\|a\|_{_{t}}\) is the confidence interval width, \((0,1)\) is a failure probability of the confidence interval, and \(\|a\|_{M}=Ma}\).

## 4 Logarithmic Bayes Regret Upper Bounds

In this section, we present finite-time logarithmic Bayes regret bounds for BayesUCB. We derive them for both \(K\)-armed and linear bandits. One bound matches an existing lower bound of Lai (1987) asymptotically and all improve upon prior \(()\) bounds. We discuss this in detail in Section 5.

### BayesUCB in Gaussian Bandit

Our first regret bound is for BayesUCB in a \(K\)-armed Gaussian bandit. It depends on random gaps. To control the gaps, we clip them as \(_{a}^{}=\{_{a},\}\). The bound is stated below.

**Theorem 1**.: _For any \(>0\) and \((0,1)\), the \(n\)-round Bayes regret of BayesUCB in a \(K\)-armed Gaussian bandit is bounded as_

\[R(n)[_{a A_{*}}(1/)}{ _{a}^{}}-_{a}^{}}{_{ 0}^{2}}]+C\,,\]

_where \(C= n+2(+2K)_{0}Kn\) is a low-order term._

The proof is in Appendix A.1. For \(=1/n\) and \(=1/n\), the bound is \(O(c_{} n)\), where \(c_{}\) is a constant depending on the gaps of random bandit instances. The dependence on \(_{0}\) in the low-order term \(C\) can be reduced to \(\{_{0},\}\) by a more elaborate analysis, where the regret of taking each action for the first time is bounded separately. This also applies to Corollary 2.

Now we derive an upper bound on Theorem 1 that eliminates the dependence on random gaps. To state it, we need to introduce additional notation. For any action \(a\), we denote all action parameters except for \(a\) by \(_{-a}=(_{1},,_{a-1},_{a+1},,_{K})\) and the corresponding optimal action in \(_{-a}\) by \(_{a}^{*}=_{j\{a\}}_{j}\). We denote by \(h_{a}\) the prior density of \(_{a}\) and by \(h_{-a}\) the prior density of \(_{-a}\). Since the prior is factored (Section 3.1), note that \(h()=h_{a}(_{a})h_{-a}(_{-a})\) for any \(\) and action \(a\). To keep the result clean, we state it for a "sufficiently" large prior variance. A complete statement for all prior variances is given in Appendix B. We note that the setting of small prior variances favors Bayesian algorithms since their regret decreases with a more informative prior. In fact, we show in Appendix B that the regret of BayesUCB is \(O(1)\) for a sufficiently small \(_{0}\).

**Corollary 2**.: _Let \(_{0}^{2} n}\). Then there exist functions \(_{a}:[,}]\) such that the \(n\)-round Bayes regret of BayesUCB in a \(K\)-armed Gaussian bandit is bounded as_

\[R(n)[8^{2}(1/) n-}{2_{0}^{ 2} n}]_{a}_{_{-a}}h_{a}(_{a}^{*}- _{a}(_{a}^{*}))\,h_{-a}(_{-a})\,_{-a}+C\,,\]

_where \(C=8^{2}K(1/)+2(+2K) _{0}Kn+1\) is a low-order term._

The proof is in Appendix A.2. For \(=1/n\), the bound is \(O(c_{h}^{2}n)\), where \(c_{h}\) depends on prior \(h\) but not on the gaps of random bandit instances. This bound is motivated by Lai (1987). The terms \(_{a}\) arise due to the intermediate value theorem for function \(h_{a}\). Similar terms appear in Lai (1987) but vanish in their final asymptotic claims. The rate \(1/\) in the definition of \(_{a}\) cannot be reduced to \(1/\,n\) without increasing dependence on \(n\) in other parts of the bound.

The complexity term \(_{a}_{_{-a}}h_{a}(_{a}^{*}-_{a}(_{a }^{*}))\,h_{-a}(_{-a})\,_{-a}\) in Corollary 2 is the same as in Lai (1987) and can be interpreted as follows. Consider the asymptotic regime of \(n\). Then, since the range of \(_{a}\) is \([,}]\), the term simplifies to \(_{a}_{_{-a}}h_{a}(_{a}^{*})h_{-a}(_{ -a})\,_{-a}\) and can be viewed as the distance between prior means. In a Gaussian bandit with \(K=2\) actions, it has a closed form of \(^{2}}}[--_{0,2})^{2}}{4 _{0}^{2}}]\). A general upper bound for \(K>2\) actions is given below.

**Lemma 3**.: _In a \(K\)-armed Gaussian bandit with prior \(h()=(;_{0},_{0}^{2}I_{K})\), we have_

\[_{a}_{_{-a}}h_{a}(_{a}^{*})\,h_{-a}(_{-a })\,_{-a}^{2}}}_{a }_{a^{} a}[--_{0,a^{ }})^{2}}{4_{0}^{2}}]\,.\]

The bound is proved in Appendix A.3 and has several interesting properties that capture low-regret regimes. First, as the prior becomes more informative and concentrated, \(_{0} 0\), the bound goes to zero. Second, when the gaps of bandit instances sampled from the prior are large, low regret is also expected. This can happen when the prior means become more separated, \(|_{0,a}-_{0,a^{}}|\), or the prior becomes wider, \(_{0}\). Our bound goes to zero in both of these cases. This also implies that Bayes regret bounds are not necessarily monotone in prior parameters, such as \(_{0}\).

### Ucb1 in Gaussian Bandit

Using a similar approach, we prove a Bayes regret bound for UCB1 (Auer et al., 2002). We view it as BayesUCB (Section 3.1) where \(_{0}=\) and each action \(a\) is initially taken once at round \(t=a\). This generalizes classic UCB1 to \(^{2}\)-sub-Gaussian noise. An asymptotic Bayes regret bound for UCB1 was proved by Lai (1987) (claim (i) in their Theorem 3). We derive a finite-time prior-dependent Bayes regret bound below.

**Theorem 4**.: _There exist functions \(_{a}:[,}]\) such that the \(n\)-round Bayes regret of UCB1 in a \(K\)-armed Gaussian bandit is bounded as_

\[R(n) 8^{2}(1/) n_{a}_{_{-a} }h_{a}(_{a}^{*}-_{a}(_{a}^{*}))\,h_{-a}(_{-a})\,_{-a}+C\,,\]

_where \(C=8^{2}K(1/)+2(+2K) Kn +_{a}[_{a}]+1\)._

The proof is in Appendix A.4. For \(=1/n\), the bound is \(O(c_{h}^{2}n)\) and similar to Corollary 2. The main difference is in the additional factor \(}{2_{0}^{2} n}\) in Corollary 2, which decreases the bound. This means that the regret of BayesUCB improves as \(_{0}\) decreases while that of UCB1 may not change much. In fact, the regret bound of BayesUCB is \(O(1)\) as \(_{0} 0\) (Appendix B) while that of UCB1 remains logarithmic. This is expected because BayesUCB has more information about the random instance \(\) as \(_{0}\) decreases, while the frequentist algorithm is oblivious to the prior.

### BayesUCB in Bernoulli Bandit

Theorem 1 and Corollary 2 can be straightforwardly extended to Bernoulli bandits because

\[(|_{a}-_{t,a}| C_{t,a}\, \,H_{t}) 2\]

holds for any action \(a\) and history \(H_{t}\) (Section 3.2). We state the extension below and prove it in Appendix A.5.

**Theorem 5**.: _For any \(>0\) and \((0,1)\), the \(n\)-round Bayes regret of BayesUCB in a \(K\)-armed Bernoulli bandit is bounded as_

\[R(n)[_{a A_{*}}^{ }}-(_{a}+_{a}+1)_{a}^{}]+C\,,\]

_where \(C= n+2Kn\) is a low-order term._

_Moreover, let \(=_{a}_{a}+_{a}+1\) and \( 2(1/)\,n^{2} n\). Then_

\[R(n)[2(1/) n-]_{a }_{_{-a}}h_{a}(_{a}^{*}-_{a}(_{a}^{*}))\, h_{-a}(_{-a})\,_{-a}+C\,,\]

_where \(C=2K(1/)+2Kn+1\) is a low-order term._

### BayesUCB in Linear Bandit

Now we present a gap-dependent Bayes regret bound for BayesUCB in a linear bandit with a finite number of actions. The bound depends on a random minimum gap. To control the gap, we clip it as \(_{}^{}=\{_{},\}\).

**Theorem 6**.: _Suppose that \(\|\|_{2} L_{*}\) holds with probability at least \(1-_{*}\). Let \(\|a\|_{2} L\) for any action \(a\). Then for any \(>0\) and \((0,1)\), the \(n\)-round Bayes regret of linear BayesUCB is bounded as_

\[R(n) 8[^{}}]^{2}d}{(1+^{2}}{^{2}} )}(1+^{2}n}{^{2}d})(1/ )+ n+4LL_{*}Kn\]

_with probability at least \(1-_{*}\), where \(_{0,}=(_{0})}L\)._

The proof is in Appendix A.6. For \(=1/n\) and \(=1/n\), the bound is \(O(d\,c_{}^{2}n)\), where \(c_{}\) is a constant depending on the gaps of random bandit instances. The bound is remarkably similar to the frequentist \(O(d\,_{}^{-1}^{2}n)\) bound in Theorem 5 of Abbasi-Yadkori et al. (2011), where \(_{}\) is the minimum gap. There are two differences. First, we integrate \(_{}^{-1}\) over the prior. Second, our bound decreases as the prior becomes more informative, \(_{0,} 0\).

In a Gaussian bandit, the bound becomes \(O(K[1/_{}^{}]^{2}n)\). Therefore, it is comparable to Theorem 1 up to an additional logarithmic factor in \(n\). This is due to a more general proof technique, which allows for dependencies between the mean rewards of actions. We also note that Theorem 6 does not assume that the prior is factored, unlike Theorem 1.

## 5 Comparison to Prior Works

This section is organized as follows. In Section 5.1, we show that the bound in Theorem 5 matches an existing lower bound of Lai (1987) asymptotically. In Section 5.2, we compare our logarithmic bounds to prior \(()\) bounds. Finally, in Section 5.3, we outline the key steps in our analyses and how they differ from prior works.

### Matching Lower Bound

In frequentist bandit analyses, it is standard to compare asymptotic lower bounds to finite-time upper bounds because finite-time logarithmic lower bounds do not exist (Lattimore and Szepesvari, 2019). We follow the same approach when arguing that our finite-time upper bounds are order optimal.

The results in Lai (1987) are for single-parameter exponential-family reward distributions, which excludes Gaussian rewards. Therefore, we argue about the tightness of Theorem 5 only. Specifically, we take the second bound in Theorem 5, set \(=1/n\), and let \(n\). In this case, \( 0\) and \(_{a}() 0\), and the bound matches up to constant factors the lower bound in Lai (1987) (claim (ii) in their Theorem 3), which is

\[(^{2}n_{a}_{_{-a}}h_{a}(_{a}^ {*})\,h_{-a}(_{-a})\,_{-a})\,.\] (2)

Lai (1987) also matched this lower bound with an asymptotic upper bound for a frequentist policy.

Our finite-time upper bounds also reveal an interesting difference from the asymptotic lower bound in (2), which may deserve more future attention. More specifically, the regret bound of BayesUCB (Corollary 2) improves with prior information while that of UCB1 (Theorem 4) does not. We observe these improvements empirically as well (Section 6 and Appendix D). However, both bounds are the same asymptotically. This is because the benefit of knowing the prior vanishes in asymptotic analyses, since \(}{2_{0}^{2} n} 0\) in Corollary 2 as \(n\). This motivates the need for finite-time logarithmic Bayes regret lower bounds, which do not exist.

### Prior Bayes Regret Upper Bounds

Theorem 1 and Corollary 2 are major improvements upon existing \(()\) bounds. For instance, take a prior-dependent bound in Lemma 4 of Kveton et al. (2021), which holds for both BayesUCB and Thompson sampling due to a well-known equivalence of their analyses (Russo and Van Roy, 2014; Hong et al., 2020). For \(=1/n\), their leading term becomes

\[4K n}(_{0}^{-2}K}-_{0}^{-2}K})\,.\] (3)

Similarly to Theorem 1 and Corollary 2, (3) decreases as the prior concentrates and becomes more informative, \(_{0} 0\). However, the bound is \(()\). Moreover, it does not depend on prior means \(_{0}\) or the gaps of random bandit instances. Therefore, it cannot capture low-regret regimes due to large random gaps \(_{a}^{}\) in Theorem 1 or a small complexity term in Corollary 2. We demonstrate it empirically in Section 6.

When the random gaps \(_{a}^{}\) in Theorem 1 are small or the complexity term in Corollary 2 is large, our bounds can be worse than \(()\) bounds. This is analogous to the relation of the gap-dependent and gap-free frequentist bounds (Lattimore and Szepesvari, 2019). Specifically, a gap-dependent bound of \(\) in a \(K\)-armed bandit with \(1\)-sub-Gaussian rewards (Theorem 7.1) is \(O(K_{}^{-1} n)\), where \(_{}\) is the minimum gap. A corresponding gap-free bound (Theorem 7.2) is \(O()\). The latter is smaller when the gap is small, \(_{}=o()\). To get the best bound, the minimum of the two should be taken, and the same is true in our Bayesian setting.

No prior-dependent Bayes regret lower bound exists in linear bandits. Thus we treat \([1/_{}^{}]\) in Theorem 6 as the complexity term and do not further bound it as in Corollary 2. To compare our bound fairly to existing \(()\) bounds, we derive an \(()\) bound in Appendix C, by a relatively minor change in the proof of Theorem 6. A similar bound can be obtained by adapting the proofs of Lu and Van Roy (2019) and Hong et al. (2022) to a linear bandit with a finite number of actions. The leading term of the bound is

\[2^{2}dn}{(1+^{2}n}{ ^{2}})}(1+^{2}n}{^{2}d}) (1/)}\,.\] (4)

Similarly to Theorem 6, (4) decreases as the prior becomes more informative, \(_{0,} 0\). However, the bound is \(()\) and does not depend on the gaps of random bandit instances. Hence it cannot capture low-regret regimes due to a large random minimum gap \(_{}\) in Theorem 6. We validate it empirically in Section 6.

### Technical Novelty

All modern Bayesian analyses follow Russo and Van Roy (2014), who derived the first finite-time \(()\) Bayes regret bounds for BayesUCB and Thompson sampling. The key idea in their analyses is that conditioned on history, the optimal and taken actions are identically distributed, and that the upper confidence bounds are deterministic functions of the history. This is where the randomness of instances in Bayesian bandits is used. Using this, the regret at round \(t\) is bounded by the confidence interval width of the taken action, and the usual \(()\) bounds can be obtained by summing up the confidence interval widths over \(n\) rounds.

The main difference in our work is that we first bound the regret in a fixed bandit instance, similarly to frequentist analyses. The bound involves \(_{a}^{-1}\) and is derived using biased Bayesian confidence intervals. The rest of our analysis is Bayesian in two parts: we prove that the confidence intervals fail with a low probability and bound random \(_{a}^{-1}\), following a similar technique to Lai (1987). The resulting logarithmic Bayes regret bounds cannot be derived using the techniques of Russo and Van Roy (2014), as these become loose when the confidence interval widths are introduced.

Asymptotic logarithmic Bayes regret bounds were derived in Lai (1987). From this analysis, we use only the technique for bounding \(_{a}^{-1}\) when proving Corollary 2 and Theorem 5. The central part of our proof is a finite-time per-instance bound on the number of times that a suboptimal action is taken. This quantity is bounded based on the assumption that the action with the highest UCB is taken. Acomparable argument in Theorem 2 of Lai (1987) is asymptotic and on average over random bandit instances.

## 6 Experiments

We experiment with UCB algorithms in two environments: Gaussian bandits (Section 3.1) and linear bandits with Gaussian rewards (Section 3.3). In both experiments, the horizon is \(n=1\,000\) rounds. All results are averaged over \(10\,000\) random runs. Shaded regions in the plots are standard errors of the estimates. They are generally small because the number of runs is high.

### Gaussian Bandit

The first problem is a \(K\)-armed bandit with \(K=10\) actions (Section 3.1). The _prior width_ is \(_{0}=1\). The prior mean is \(_{0}\), where \(_{0,1}=_{0}\) and \(_{0,a}=0\) for \(a>1\). We set \(_{0}=1\) and call it the _prior gap_. We vary \(_{0}\) and \(_{0}\) in our experiments, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.1 and 5.2).

We plot five trends: (a) Bayes regret of BayesUCB. (b) Bayes regret of UCB1 (Section 4.2), which is a comparable frequentist algorithm to BayesUCB. (c) A leading term of the BayesUCB regret bound in Theorem 1, where \(=1/n\) and \(=1/n\). (d) A leading term of the UCB1 regret bound: This is the same as (c) with \(_{0}=\). (e) An existing \(()\) regret bound in (3).

Our results are reported in Figure 1. We observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \(_{0} 0\) or \(_{0}\). It is also lower than that of UCB1, which does not leverage the prior. Second, the regret bound of BayesUCB is tighter than that of UCB1, due to capturing the benefit of the prior. Finally, the logarithmic regret bounds are tighter than the \(()\) bound. In addition, the \(()\) bound depends on the prior only through \(_{0}\) and thus remains constant as the prior gap \(_{0}\) changes.

In Appendix D, we compare BayesUCB to UCB1 more comprehensively for various \(K\), \(\), \(_{0}\), and \(_{0}\). In all experiments, BayesUCB has a lower regret than UCB1. This also happens when the noise is not Gaussian, which a testament to the robustness of Bayesian methods to model misspecification.

### Linear Bandit with Gaussian Rewards

The second problem is a linear bandit with \(K=30\) actions in \(d=10\) dimensions (Section 3.3). The prior covariance is \(_{0}=_{0}^{2}I_{d}\). The prior mean is \(_{0}\), where \(_{0,1}=_{0}\) and \(_{0,i}=-1\) for \(i>1\). As in Section 6.1, we set \(_{0}=1\) and call it the _prior gap_. The action set \(\) is generated as follows. The first \(d\) actions are the canonical basis in \(^{d}\). The remaining \(K-d\) actions are sampled uniformly at random from the positive orthant and scaled to unit length. This ensures that the first action has the highest mean reward, of \(_{0}\), under the prior mean \(_{0}\). We vary \(_{0}\) and \(_{0}\), and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.4 and 5.2). We plot three trends: (a) Bayes regret of BayesUCB. (b) A leading term of the BayesUCB regret bound in Theorem 6, where \(_{0,}=_{0}\), \(=1/n\), and \(=1/n\). (c) An existing \(()\) regret bound in (4).

Figure 1: Gaussian bandit as (a) the prior width \(_{0}\) and (b) the prior gap \(_{0}\) change.

Our results are reported in Figure 2 and we observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \(_{0} 0\) or \(_{0}\). Second, the regret bound of BayesUCB decreases as the problem becomes easier. Finally, our logarithmic regret bound can also be tighter than the \(()\) bound. In particular, the \(()\) bound depends on the prior only through \(_{0}\), and thus remains constant as the prior gap \(_{0}\) changes. We discuss when our bounds could be looser than \(()\) bounds in Section 5.2.

## 7 Conclusions

Finite-time logarithmic frequentist regret bounds are the standard way of analyzing \(K\)-armed bandits (Auer et al., 2002; Garivier and Cappe, 2011; Agrawal and Goyal, 2012). In our work, we prove the first comparable finite-time bounds, logarithmic in \(n\), in the Bayesian setting. This is a major step in theory and a significant improvement upon prior \(()\) Bayes regret bounds that have become standard in the literature. Comparing to frequentist regret bounds, our bounds capture the value of prior information given to the learner. Our proof technique is general and we also apply it to linear bandits.

This work can be extended in many directions. First, our analyses only need closed-form posteriors, which are available for other reward distributions than Gaussian and Bernoulli. Second, our linear bandit analysis (Section 4.4) seems preliminary when compared to our multi-armed bandit analyses. As an example, the complexity term \([1/_{}^{}]\) in Theorem 6 could be bounded as in Corollary 2 and Theorem 5. We do not do this because the main reason for deriving the \(O(c_{h}^{2}n)\) bound in Theorem 5, an upper bound on the corresponding \(O(c_{} n)\) bound, is that it matches the lower bound in (2). No such instance-dependent lower bound exists in Bayesian linear bandits. Third, we believe that our approach can be extended to information-theory bounds (Russo and Van Roy, 2016) and discuss it in Appendix E. Fourth, although we only analyze BayesUCB, we believe that similar guarantees can be obtained for Thompson sampling. Finally, we would like to extend our results to reinforcement learning, for instance by building on the work of Lu and Van Roy (2019).

There have been recent attempts in theory (Wagenmaker and Foster, 2023) to design general adaptive algorithms with finite-time instance-dependent bounds based on optimal allocations. The promise of these methods is a higher statistical efficiency than exploring by optimism, which we adopt in this work. One of their shortcomings is that they are not guaranteed to be computationally efficient, as discussed in Section 2.2 of Wagenmaker and Foster (2023). This work is also frequentist.