# Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation

Abhinav Jain

Department of Computer Science

Rice University

aj70@rice.edu

&Swarat Chaudhuri

Department of Computer Science

UT Austin

swarat@cs.utexas.edu

&Thomas Reps

Department of Computer Science

University of Wisconsin-Madison

reps@cs.wisc.edu

&Chris Jermaine

Department of Computer Science

Rice University

cmj4@rice.edu

###### Abstract

Parameter-Efficient Fine-Tuning (PEFT) has become the standard for customising Foundation Models (FMs) to user-specific downstream tasks. However, typical PEFT methods require storing multiple task-specific adapters, creating scalability issues as these adapters must be housed and run at the FM server. Traditional prompt tuning offers a potential solution by customising them through task-specific input prefixes, but it under-performs compared to other PEFT methods like LoRA. To address this gap, we propose **L**ow-Rank **P**rompt **A**daptation (LoPA), a prompt-tuning-based approach that performs on par with state-of-the-art PEFT methods and full fine-tuning while being more parameter-efficient and not requiring a server-based adapter. LoPA generates soft prompts by balancing between sharing task-specific information across instances and customization for each instance. It uses a low-rank decomposition of the soft-prompt component encoded for each instance to achieve parameter efficiency. We provide a comprehensive evaluation on multiple natural language understanding and code generation and understanding tasks across a wide range of foundation models with varying sizes.

## 1 Introduction

Language models exhibit remarkable few-shot learning capabilities, demonstrating strong performance across tasks, including those unseen during training . Nevertheless, fine-tuning remains crucial for optimised performance on a given downstream task. However, it becomes increasingly challenging with larger models because updating all parameters is impractical. Parameter Efficient Fine-Tuning (PEFT) presents a promising solution, adjusting a limited subset of parameters while leaving the rest unchanged. This approach allows the personalisation of pre-trained Foundation Models (FMs) to multiple users simultaneously.

In recent years, numerous PEFT approaches have been proposed , each varying in how and what they modify within FMs . These variations can be categorized by the position in the FMs where modifications are applied, the functions used for modification, and the methods of integrating the modifications. While effective, these methods require maintaining multiple adapter-like modules for each user-specific task on the FM server and the need to select and assemble a subset of these modules every time a batch of user requests is processed for inference  (see Fig. 1).

Prompt tuning  is a simple approach that has certain advantages. First, it is parameter-efficient, requiring only a small set of vectors (soft prompts) that are prepended at the input layer and learned for a specific task. Second, prompt-based personalization requires no task-specific processing on the server. A task-specific prefix is added before processing, allowing the FM server to perform the same processing regardless of the task. However, despite its advantages, prompt tuning has been shown to under-perform compared to other methods for PEFT . This gap in performance raises concerns about the viability of prompt tuning as a solution. Recently, efforts have been made to enhance the performance of prompt tuning by strategically inserting soft prompts into different layers of the transformer [20; 38; 19; 41]. However, this improvement increases the number of parameters and again necessitates server-side modifications for serving multiple tasks. An interesting recent work explored a simple method to make soft prompts input-dependent by using a lightweight prompt generator for each sample . This approach achieved notable improvements, raising the question: _Can we further improve the performance of prompt tuning while staying parameter-efficient?_

To this end, we propose Low-rank Prompt Adaptation (LoPA), a new instance-aware prompt tuning-based approach 1. LoPA constructs the soft prompt from two components: a task-specific element that shares task information across samples and an instance-specific element that incorporates information from each individual instance. Our extensive analysis reveals that relying solely on either component, as done in previous works [15; 38], is insufficient for outperforming other PEFT baselines. LoPA achieves its high performance by taking a more balanced approach.

LoPA combines the task-specific and instance-specific components using a gating function, which activates task-specific information conditioned on each instance. Additionally, it employs a low-rank decomposition of the instance-level component to enhance parameter efficiency (see Fig. 2). Once trained, users can provide the learned soft prompts as a prefix with their input to the FM, incurring no additional computational cost at the server.

We conducted extensive experiments on various models. These included six benchmark NLU tasks from the GLUE dataset and three Code Understanding and Generation tasks. Our results show that LoPA outperforms existing prompt-tuning methods. It often matches the performance of full fine-tuning and LoRA. In 11 out of 24 test cases, we found LoPA outperformed LoRA.

To summarize, the contributions of this work are as follows:

* We propose LoPA, a parameter-efficient and high-performing prompt-tuning strategy.
* We verify its effectiveness by evaluating it against full fine-tuning and state-of-the-art PEFT methods in nine tasks using seven different transformer backbones.

## 2 Related Work

**Adapter-based.** Several recent approaches have emerged to support parameter-efficient fine-tuning. These methods typically involve incorporating trainable adapter layers or modules into the transformer network [11; 10; 17]. Training such layers has been demonstrated to be computationally more economical than full fine-tuning while maintaining performance. Notably, LoRA  has gained prominence for its low-rank approximation of model updates, effectively capturing task-specific knowledge. Beyond LoRA, Compacter  introduces adapters parameterised by low-rank hyper-complex multiplication (PHM) layers  to achieve a more optimal balance between task

Figure 1: A schematic illustrating how typical PEFT methods like LoRA achieve personalization of a foundation model for multiple tasks, such as Yes/No text classification or code completion, during inference.

performance and the number of trainable parameters. DoRA  is another recent approach that decomposes weights into their magnitude and directional components and employs LoRA for directional updates to minimise the number of trainable parameters efficiently.

**Soft Prompting.** Another line of work advocates for strategically inserting soft prompts into hidden states of the transformer instead of using trainable adapter modules. For example, prefix-tuning  and P-tuning-v2  prepend trainable prefix vectors to keys and values matrices at all transformer layers. Prompt tuning, P-tuning, and DePT [15; 21; 30] are special cases that operate by prepending prompt embeddings to the input at the first layer, with  being a hybrid-approach that further uses LoRA to learn updates for the input's embedding matrix. While these approaches are instance-agnostic and optimise a task-specific prompt, there are also methods to optimise an instance-aware soft prompt. For instance, IDPG  generates a soft prompt for each sample, prepended either at the initial word-embedding layer (version-S) or all layers (version-M). LPT  inserts a prompt into some intermediate layer (i) to eliminate gradient calculation below it for faster training and (ii) to retain more task-related information (which could be lost if it had to be propagated through lower layers). SPT  aims to be more intelligent by learning a gating function to determine whether the soft prompt from the previous layer should be propagated or if a new one should be learned.

With the exception of prompt tuning and S-IDPG, PEFT approaches mostly operate by injecting prefixes and new trainable modules into deeper layers or doing low-rank re-parameterization of existing ones, necessitating the storage of PEFT parameters at the server to update the foundation model. In contrast, LoPA provides the soft prompt as a prefix that is prepended to the input query, overcoming the need to store task-specific parameters on the server. Furthermore, we demonstrate that LoPA achieves a more balanced trade-off between specificity and generalization compared to existing approaches for enhancing learned models, such as prompt tuning , which solely focuses on a general task-specific soft prompt, and IDPG , which emphasizes an instance-specific prompt.

## 3 Proposed Methodology

In this section, we formally define the proposed approach, followed by an explanation of how it affects model learning as compared to existing soft-prompting approaches.

### Preliminaries

**Transformers.** A transformer model consists of multiple stacked layers, where each layer has multiple heads (\(=N_{H}\)), each performing self-attention over the input . Let us consider a single head, \(H\) parameterised by the query, key, and value weights as \(W^{Q},W^{K},W^{V}^{d_{H} d}\), respectively, where \(d\) is the model dimension. In multi-headed attention, \(d_{H}\) is typically set to \(}\). For a given sequence of \(n\) input vectors \(=\{^{1},,^{n}\}\) and a query vector \(^{i}\) with each \(^{d}\), the output of the head at position \(i\) is -

\[^{i}=(W^{Q}^{i},W^{K},W^{V} )=W^{Q}^{i}(W^{K})^{} W^{V}\] (1)

where, we have ignored the constant \(}\) for notational convenience. For the first layer of the transformer network, the input \(\) is the embedding matrix i.e. \(=_{e}^{d n}\).

**Full Fine-Tuning (FFT).** With fine-tuning, the objective is to adapt the model to a new task with data \(=\{(,)\}\). Formally, adaptation is achieved by updating the weights \((W)\) of the model to maximise the log-likelihood of the response \(\), i.e., \(_{W}=_{(,)}[ p _{W}(|)]\).

**Prompt-Tuning (PT).** The objective of prompt-tuning is to achieve task-specific model adaptation by learning a "soft prompt" \(\). Such a soft prompt is prepended to the input word embeddings and trained via back-propagation: \(_{}=[ p_{W}(|( ,_{e}))]\). In PT, the soft prompt \(^{d m}\) is parameterised by a set of \(m\) learnable vectors \(\{^{1},^{m}\}\), where \(m\) denotes its length . Thus, it can be interpreted as an embedding of virtual tokens that enable the model to perform a downstream task without updating its parameters [28; 8].

With prompt tuning, the output of an attention head in the first layer is modified as follows:

\[^{i}_{PT}=(W^{Q}^{i},W^{K}( ,_{e}),W^{V}(,_{e}))\]By using the formulation of Attention from Eq. 1, this equation can be equivalently written as

\[_{PT}^{i} =A_{ik}W^{V}^{k}}_{bias}+A_{ik})}_{scale}^{i}\] (2) \[ A_{ik} =^{k})^{}W^{Q}^{i})}{ _{k}((W^{K}^{k})^{}W^{Q}^{i}))+_{j}((W ^{K}^{j})^{}W^{Q}^{i}))}\] (3)

where, \(A_{ik}\) is the attention transformer gives to the prefix vector \(_{k}\) for a given query vector \(^{i}\). In Eq. 2, we can observe that the soft prompt linearly interpolates the head's position-wise output; where the bias term can be considered in an offset subspace spanned by vectors \(\{W^{V}^{k}\}_{k=1}^{m}\) with dimension \(m\) (or \( m\)) .

### Low-rank Prompt Adaptation (LoPA)

LoPA constructs the soft prompt as \(=_{S} g(_{I})\), where \(_{S}^{d m}\) is the task-specific component and \(_{I}^{d m}\) is the instance-specific component. These are combined using the gating function \(g\), implemented using sigmoid, where \(\) denotes the Hadamard product. Intuitively, by sharing \(_{S}\) across instances, it captures general information, adapting the model to user-defined tasks, while \(_{I}\) fine-tunes the soft prompt for specific instances, acting as activations. Both \(_{S}\) and \(_{I}\) have their own dedicated parameters. Similar to prompt tuning, \(_{S}\) consists of \(m\) learnable vectors. Conversely, \(_{I}\) is obtained from input using the encoding function \(f:^{d n}^{d m}\). However, encoding a matrix of size \(d m\) can be expensive. For example, in , an MLP layer with hidden dimension \(h\) requires \((hdm)\) parameters. To improve parameter efficiency, we assume a low-rank decomposition of \(_{I}\) as \(_{I}=^{}\) and encode the two matrices with rank \(r\), \(^{d r}\) and \(^{m r}\), using separate MLP layers. This design makes \(f\) computationally cheaper, with \((hdm(+))\) parameters, reduced by a factor of \((+)<1\) when \(r\) is chosen to be \((d,m)\). The overall composition of \(\) can be represented as:

\[=_{S} g(_{}(^{})_{}(^{})^{}}_{ _{I}=f(^{})})}_{_{I}=f()}\] (4)

where _ marks the three trainable components. The MLP head consists of a down- and up-projection layer with a non-linear activation in between. It sits atop a smaller language model, referred to as the Encoder model, that gives the input representation \(^{}\). A visual illustration of the framework can be found in Fig. 2.

Next, we provide an intuitive explanation why LoPA could be better than traditional prompt-tuning  and existing instance-specific approaches .

**Offset subspace induced by LoPA.** From Eq.2, we can observe that the bias vector in the offset subspace is a linear combination of \(\{W^{V}^{k}\} k\{1,,m\}\), with \(A_{ik}\) (Eq. 3) representing the scalars . In LoPA, the input influences \(\), causing the vectors \(\{W^{V}^{k}\}\) to vary accordingly. Consequently, distinct offset sub-spaces emerge for different inputs. In contrast, traditional prompt tuning maintains fixed vectors while allowing only the scalars to vary with input. As a result, instance-sensitive approaches can exert greater influence on the attention patterns in deeper layers of the transformer, thereby offering enhanced flexibility and responsiveness to varying inputs.

Figure 2: An illustration of LoPA. No task-specific adapters need to be stored on the server. \(|\) represents the concatenation of the soft prompt \(\) and the input prompt \(_{e}\) i.e. \(=(|_{e})\)

**Coupled learning of \(_{S}\) and \(_{I}\).** Let's examine the partial derivatives of the objective \(\) with respect to \(_{S}\) and \(_{I}\). Using the chain-rule on the formulation in Eq. 4 with \(g(.)=(.)\),

\[_{_{S}} =_{}(_{I})\] \[_{_{I}} =(_{}_{S})( _{I})(1-(_{I}))\]

These expressions suggest a coupled learning process where changes in \(_{I}\) (through the sigmoid function) directly impact the updates to \(_{S}\), and the updates to \(_{I}\) are scaled by both \(_{S}\) and the derivative of the sigmoid function. In contrast, consider IDPG with \(=(^{})\). Here, the bias term of the MLP layer can be viewed as a task-specific element, such that the composition is \(=_{S}+_{I}\). This approach results in updates to \(_{S}\) and \(_{I}\) being independent of each other. Such a linear-sum operation may fail to capture the complex relationships between the two components that LoPA can capture due to LoPA's non-linear factorization.

## 4 Experiments, Results, and Discussion

### Experimental Setup

**Tasks.** We evaluate LoPA on (i) six Natural Language Understanding (NLU) tasks from the GLUE benchmark --namely, SST-2 , MNLI , MRPC , QNLI , QQP, and RTE ; (ii) a code-generation task that requires the model to complete method bodies from MBPP benchmark , and (iii) two code-understanding tasks--namely, CruxEval-I (input prediction) and CruxEval-O (output prediction) from CruxEval benchmark . These tasks assess the model's ability to reason about the execution of simple Python programs by asking it to predict the input given the output of a function and vice-versa.

**Baselines.** We evaluate LoPA against the following baselines that represent different customisation approaches. (1) FFT, (2) LoRA , (3) p-tuning-v2  and (4) prefix-tuning (h=512)  are selected as representatives of methods that customise models on the server side, with FFT representing full fine-tuning and the remainder showcasing parameter-efficient techniques. (5) Standard prompt-tuning , (6) S-IDPG  and (7) DePT  are chosen because they customise models from the user side, focusing on soft prompt insertion with DePT also learning updates to the input embedding matrix. For IDPG, we use DNN layers in MLP-head to encode the soft prompt for the input embedding layer. We chose DNN over PHM layers  because we found them to exhibit better performance across our task set. Refer to Appendix 7.1 for the comparison. Additionally, the proposed LoPA also uses DNN layers, facilitating a fair comparison with S-IDPG-DNN . Lastly, we include results for the model without any fine-tuning to report its (8) zero-shot performance.

   Tuning & Tunable & SST-2 & MNLI & MRPC & QNLI & QQP & RTE & Avg \\  & Parameters & (acc) & (acc) & (acc, F1) & (acc) & (acc, F1) & (acc) & \\  FFT & 355M & 95.99 & **90.40** & 90.81 & 94.60 & **90.39** & **85.92** & **91.35** \\ p-tuning-v2 & 0.49M & 89.91 & 88.13 & 70.18 & 85.21 & 84.63 & 53.43 & 78.58 \\ prefix-tuning & 25.75M & 93.80 & 89.51 & 90.86 & **94.98** & 86.87 & 82.67 & 89.78 \\ LoRA & 2.36M & **96.22** & 90.30 & 90.77 & 94.69 & 89.91 & 85.66 & 91.26 \\ None & 0 & 59.97 & 39.60 & 73.52 & 50.16 & 42.34 & 53.43 & 53.17 \\ PT & 10.2K & 84.40 & 54.67 & 72.38 & 58.74 & 48.20 & 53.07 & 61.91 \\ DePT & 10.2K & 89.68 & 71.51 & 72.97 & 57.09 & 45.81 & 53.79 & 65.14 \\ S-IDPG & 2.89M & 95.30 & 84.50 & 78.60 & 90.48 & 84.88 & 77.26 & 85.17 \\ Ours & 1.60M & 95.99 & 89.22 & **91.09** & 93.74 & 89.72 & 83.39 & 90.53 \\   

Table 1: Performance on GLUE tasks. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. Approaches below the dotted line do not require any modification to the model on the server side. **Bold** denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt tuning methods.

    &  &  &  &  \\   & & & CruxEval-I & CruxEval-O & MBPP \\   & FFT & 350M & 33.0 & **19.5** & 17.49 \\  & LoRA & 1.3M & 31.0 & 18.2 & **21.56** \\   & None & 0 & 20.8 & 15.0 & 15.85 \\  & PT & 10.2K & 32.8 & 15.8 & 15.20 \\  & S-IDPG & 8.5M & 16.9 & 13.2 & 17.04 \\  & Ours & 4.4M & **34.5** & 18.5 & 17.04 \\   & FFT & 1.3B & **45.0** & 34.8 & **44.76** \\  & LoRA & 4.7M & 35.5 & **36.0** & 44.14 \\   & None & 0 & 26.8 & 29.8 & 34.08 \\  & PT & 20.5K & 41.2 & 31.2 & 34.49 \\  & S-IDPG & 16.3M & 26.0 & 28.5 & 42.50 \\  & Ours & 4.2M & 43.0 & 34.5 & 44.66 \\   & FFT & 2.7B & 40.2 & 37.0 & **55.03** \\  & LoRA & 7.9M & 41.5 & **42.5** & 51.54 \\   & None & 0 & 33.5 & 33.0 & 45.17 \\   & PT & 25.6K & 35.0 & 34.0 & 49.69 \\   & S-IDPG & 20.3M & 35.0 & 33.0 & 53.29 \\   & Ours & 4.76M & **43.0** & 37.2 & 52.15 \\   & FFT & 3.8B & 39.2 & 39.5 & **54.00** \\  & LoRA & 6.3M & 38.0 & 41.2 & 42.92 \\   & None & 0 & 33.8 & 39.5 & 8.82 \\   & PT & 30.7K & 33.5 & 31.5 & 34.08 \\   & S-IDPG & 24.2M & 31.0 & 39.5 & 42.29 \\   & Ours & 5.3M & **42.2** & **42.5** & 44.35 \\   & FFT & 7B & _OOM_ & _OOM_ & _OOM_ \\  & LoRA & 11.8M & 47.5 & **49.8** & 53.38 \\   & None & 0 & 39.3 & 44.0 & 50.51 \\   & PT & 41.0K & 45.8 & 44.8 & 37.47 \\   & S-IDPG & 32.1M & 40.5 & 41.5 & **53.59** \\   & Ours & 6.35M & **50.0** & 48.0 & 52.46 \\   & FFT & 8B & _OOM_ & _OOM_ & _OOM_ \\  & LoRA & 9.4M & **45.5** & **40.5** & 44.55 \\   & None & 0 & 27.0 & 31.5 & **45.37** \\   & PT & 41.0K & 37.5 & 32.0 & 26.07 \\   & S-IDPG & 32.1M & 29.2 & 35.2 & 33.05 \\   & Ours & 6.4M & 41.2 & 39.8 & 43.94 \\   

Table 2: Performance comparison on CruxEval and MBPP tasks. We report average _pass\(@1\)_ scores. Approaches below the dotted line are prompt-tuning methods, which do not require any modification to the model on the server side. **Bold** denotes the best-performing tuning method for the given model. Underline marks the best result among all prompt-tuning methods. _OOM_ indicates that the corresponding tuning approach exceeded the available GPU memory and ran out of memory.

**Backbone Architectures.** For NLU tasks, we test all the tuning methods on 355M RoBERTa  similar to the setup opted by prior work [20; 38; 41]. For Code Generation and Understanding, we test a subset of the baselines (FFT, LoRA, PT, S-IDPG) on a range of FM backbones; starting from smaller FMs: 350M CodeGen-mono  and 1.3B Deepseek-Coder ; mid-sized FMs: 2.7B phi-2 , 3.8B phi-3 ; and larger FMs: 7B Deepseek-Coder  and 8B Llama 3 .

**Implementation Details.** For the GLUE tasks, we use the train-test splits pre-defined in the benchmark, while for the MBPP and CruEval tasks, we employ a 50-50 split. Across all tasks and backbone models, the soft prompting baselines are implemented with \(m=10\) virtual tokens representing the soft prompt. For NLU tasks, both IDPG and LoPA use RoBERTa + MLP(h=256) as the encoder, whereas in code tasks, 125M CodeSage + MLP(h=1024) is used. Additionally, for LoPA, the best-performing rank \((r)\) of the low-rank decomposition is chosen from \(\{1,2,4\}\), and the corresponding number of tunable parameters is reported.

**Training Configuration.** For NLU tasks, training with FFT and LoRA was done for 10 epochs, while with prompt-tuning-based approaches it was done for 20 epochs. In MBPP, all foundation model (FM) backbones were trained for 10 epochs across all tuning methods. In CruEval Tasks across all PEFT methods, FM backbones under 7B were trained for 20 epochs, while larger FMs (\(\)7B) were trained for 10 epochs. Lastly, training with FFT on CruEval tasks was done for 5 epochs. The learning rates for LoPA are set to \(1 10^{-5}\) in NLU and \(1 10^{-3}\) in Coding tasks. The baseline tuning methods use the following learning rates across all the tasks: FFT using \(1 10^{-5}\), LoRA and the remainder of soft-prompting approaches using \(1 10^{-4}\). All experiments are conducted on 40GB 2xA100 GPUs.

**Evaluation.** We report binary or multi-class classification accuracies and F1 scores for NLU tasks as provided in the GLUE benchmark. For coding tasks, we report the _pass\(@1\)_ scores computed using the best-performing temperatures: \(0.6\) for MBPP and \(0.2\) for CruEval.

### Baseline Comparison

**Performance on Natural Language Understanding.** In Table 1, we can observe that the LoPA consistently outperforms the traditional prompt-tuning method and DePT by an average margin of \(28.62\) points and \(25.39\) points respectively. This result demonstrates that conditioning the soft prompt on instances enables it to influence the model's output more significantly. Furthermore, LoPA shows an average improvement of \(5.36\) points over IDPG, highlighting that the proposed factorisation captures complex relationships between task-specific (\(_{S}\)) and instance-specific (\(_{I}\)) components. This improvement is particularly notable in limited-data settings, with a \(12.5\)-point increase in MRPC and a \(6.13\)-point increase in RTE.

Additionally, LoPA achieves performance close to FFT and LoRA, within \(1\) point, while using \(760k\) fewer parameters than LoRA owing to LoPA's low-rank decomposition of the soft prompt. This performance profile suggests that the LoPA is a parameter-efficient and high-performing alternative for NLU tasks. It outperforms existing prompt-tuning approaches and performs on par with FFT and LoRA, making it a compelling choice for efficient and effective model tuning.

**Performance on Code Understanding.** In Table 2, LoPA consistently improves the _pass\(@1\)_ score of the baseline with no tuning across all FM backbones. It outperforms prompt-tuning on CruEval tasks, with modest improvements of approximately \(2\) to \(4\) points on smaller FMs like CodeGen

   \(=\) & SST-2 & MNLI & MRPC & QNLI & QQP & RTE & Avg \\  & (acc) & (acc) & (acc, F1) & (acc) & (acc, F1) & (acc) & \\  concat\((_{S},_{I})\) & 94.50 & 78.45 & 76.00 & 91.43 & 76.11 & 84.12 & 83.44 \\ \((_{S},_{I})\) & **95.99** & **89.37** & 88.62 & **93.74** & 78.44 & **85.92** & 88.68 \\ \(_{S} g(_{I})\) & **95.99** & 89.22 & **91.09** & **93.74** & **89.72** & 83.39 & **90.53** \\   

Table 3: Performance of LoPA on GLUE tasks with respect to function encoding \(\). concat\((.)\) represents the concatenation of \(_{S}\) and \(_{I}\). max\((.)\) represents the element-wise max operation. We report the average of accuracy and F1 for both MRPC and QQP. For all the other tasks, we report accuracy. **Bold** denotes the best-performing encoding function for LoPA.

and DeepSeeKoder-1.3B and larger improvements ranging from \(8\) to \(11\) points on larger FMs like LLama-3 and Phi-3 in CruxEval-O. Furthermore, IDPG performs worse than PT for all models except Phi-3 in CruxEval-O. These results suggest that merely encoding an instance-sensitive soft prompt does not guarantee improvements and can even degrade performance (e.g., IDPG on CodeGen and DeepSeekCoder-1.3B in CruxEval tasks). The poor generalization of the learned soft prompt, possibly due to over-parameterization and resulting overfitting, might explain this behaviour. In contrast, LoPA, being more parameter-efficient, explicitly incorporates task and instance information in its design of the soft prompt, leading to better performance.

LoPA also performs on par with LoRA, often within a range of \(1\) to \(4\) points of _pass\(@1\)_, while roughly using two-thirds of the parameters. Notably, LoPA outperforms LoRA across all models in CruxEval-I, except for LLama-3, with improvements approximately ranging from \(2\) points in DeepSeekCoder-7B to \(4\) points in Phi-3. This result might be attributed to the fact that many of the FMs considered here are good knowledge approximators, well-trained on diverse datasets, and demonstrate strong zero-shot generalization. Directly updating a subset of their weights can still lead to catastrophic forgetting, where the models lose previously acquired knowledge . In such cases, soft prompting, as employed by LoPA, can effectively elicit the necessary skills to solve new tasks without compromising existing knowledge .

**Performance on Code Completion.** On the code completion task of MBPP, both IDPG and LoPA improve the performance of the baseline model with nearly equal gains except for LLama-3. However, LoPA achieves this with significantly fewer tunable parameters--approximately half the number used

Figure 3: Performance comparison of baselines as a function of \(m\) on (a)-(c) GLUE benchmark and (d) CruxEval-O (with DeepseeKoder-1.3B as FM). Tunable parameters shown relative to the method with the most. Higher performance and fewer parameters indicate better results.

in CodeGen and only one-fifth of those used by IDPG in DeepseeKCoder-7b. This demonstrates that LoPA scales well with the size of the foundation model, maintaining both performance and parameter efficiency. This efficiency is attributed to the low-rank approximation of the instance-specific matrix employed by LoPA. For LLama-3, all tuning approaches led to a drop in performance, possibly due to over-fitting, suggesting that LLama-3 might already be trained on MBPP.

**Overview of Results.** Averaged over all tasks and foundation models, LoPA showed relative percentage improvements of \(28.52\%\) over PT and \(20.16\%\) over IDPG, while being outperformed by LoRA by only \(0.54\%\). Notably, LoPA outperformed LoRA in 11 out of 24 cases. Thus, in the test cases we considered, there was no clear systematic advantage to LoRA in terms of accuracy. Given that LoPA requires no task-specific processing at the server--the prompt can be computed anywhere, even at the client, before being sent to the server for processing--we believe LoPA may be a useful alternative to LoRA for some tasks.

### Ablation Study

**Performance of LoPA as a function of soft-prompt length.** In Figure 3, we study how the length of the soft prompt impacts the performance of LoPA compared to other prompting methods. Increasing the length of the soft prompt corresponds to adding more vectors to the set that represents the soft prompt, thus expanding the offset subspace (See Eq. 2). Whether the added vectors are mutually independent and provide additional useful information depends on the tuning approach to learning them and the offset subspace of the FM.

For instance, PT and IDPG initially see performance improvements with increased prompt length, but performance eventually plateaus or drops due to over-fitting (See PT on SST-2 Fig. 2(c) and IDPG on CruEval-O Fig. 2(d)). In contrast, LoPA does not exhibit significant performance fluctuations with varying prompt lengths (See Fig. 2(a)-2(d)). This stability is likely due to the shared component \(_{S}\) acting as a regularizer, preventing over-fitting of the instance-specific soft prompts.

Moreover, LoPA with \(m=5\) outperforms PT and IDPG even when they use longer prompts (\(m>5\)) (Refer Fig. 2(a), 2(b)). This result suggests that the dimension of the offset subspace is much smaller, and LoPA can more accurately represent it with its learned vectors.

**Performance of LoPA as a function of rank.** In Figure 4, we examine how the rank of the proposed low-rank decomposition of the instance-specific component affects the performance. For NLU, we consider three tasks: MRPC, RTE, and SST-2, and observe the performance of RoBERTa as the rank increases from 1 to 4. Performance consistently improves with increasing rank, showing gains of \(1\%\) to \(2\%\) for SST-2 and up to \(8\%\) for MRPC.

Figure 4: Performance of LoPA as a function of rank shown for \(m=10\). (a) GLUE Benchmarks and (b) CruEval tasks \((I,O)\) where ds-1.3 denotes DeepseeKCoder-1.3B and phi-2 denotes Phi2-2.7B models. Higher performance and fewer tunable parameters indicate better results.

In contrast, for CruxEval tasks, increasing the rank does not proportionally improve the performance. We attribute this behaviour to the size of the datasets used to approximate the low-rank matrices. NLU tasks provide thousands of samples, allowing for better approximation of higher-order matrices. However, CruxEval has only a few hundred samples, and increasing the rank introduces more parameters, possibly leading to over-fitting.

**Performance of LoPA as a function of encoder network.** In Figure 5, we study the impact on the performance by choosing different transformer backbones for the Encoder network in LoPA. For this experiment, we use \(125M\)\(\), and \(130M\) and \(365M\)\(\) encoder models to generate input encodings, \(^{}\) for the CruxEval tasks. We observe that \(\) models achieve a \(2\)-point improvement over CodeBERT in both tasks. This improvement can be attributed to \(\)'s superior pre-training using Contrastive Learning, which allows for finer distinctions in code representations. Consequently, the soft-prompt vectors \(\{^{k}\}\) in LoPA, as functions of the input \(^{}\), capture instance-specific nuances more effectively and accordingly exert influence on the model's output. We also tried tuning the encoder model while learning soft prompts but did not find any significant improvements in the performance.

**Performance of LoPA with respect to function encoding Z.** In Table 3, we evaluate the effect of different functions encoding \(\) as a combination of \(_{I}\) and \(_{S}\) on RoBERTa's performance in NLU tasks. The results show that simply concatenating \(_{S}\) and \(_{I}\) performs the worst while the non-linear functions, such as \((.)\) and the proposed gating mechanism, yield the best performance. We leave the exploration of other non-linear functions for future work.

## 5 Conclusion

In this paper, we introduced Low-Rank Prompt Adaptation (LoPA), an instance-specific soft-prompting method that outperforms other methods in the prompt-tuning family, and performs on par with full fine-tuning and LoRA, while using fewer tunable parameters. LoPA first uses a low-rank approximation of the instance-specific soft prompt and combines it with a task-specific soft prompt via a gating function. With a more informed way of designing soft prompts, this work aims to position prompt tuning as a powerful alternative to adapter-based methods for user-specific customization of foundation models.

**Limitations and Future Work.** The main limitation of LoPA is that its effectiveness as an alternative to LoRA was demonstrated on a set of benchmark tasks, but this may not hold for obscure real-life user tasks where LoRA or even full fine-tuning might be necessary. Further investigation into its performance on real-world tasks is part of our future work. In this work, we assumed the learned soft prompt to be prepended as a prefix to the input. Future research could explore the effects of positioning it as a suffix or randomly within the input. Finally, the foundation model combined with LoPA can be viewed as a Conditional Auto-Encoder, where soft prompt vectors exist in a latent subspace rather than an offset subspace. This viewpoint raises intriguing questions, such as whether the observed performance improvements result from inferring and compressing task-specific knowledge and providing it as additional information. Investigating this alternate perspective could lead to further performance enhancements by developing more sophisticated auto-encoding systems.

**Broader Impact.** Our contribution to new knowledge is the development of a language-model customization method that delivers strong performance while being parameter-efficient. The significance of our work lies in its potential to reduce training and maintenance costs associated with hosting and customizing foundation models. Furthermore, our method enhances user privacy by enabling task-specific customization on the user end rather than the server end.

## 6 Acknowledgments

This research was supported by the NSF under grant numbers CCF1918651, 2008240, 2131294, and 2212557, NIH CTSA award No. UL1TR003167, and US DOT Tier-1 UTC CYBER-CARE grant #69A3552348332.

Figure 5: Ablation for Encoder in LoPA with DeepseekCoder-1.3B as the foundation model.