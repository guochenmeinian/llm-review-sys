# ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer

Jiawen Zhang

DSA, HKUST(GZ)

Guangzhou, China

jiawe.zh@gmail.com &Shun Zheng

Microsoft Research Asia

Beijing, China

shun.zheng@microsoft.com &Xumeng Wen

Microsoft Research Asia

Beijing, China

xumengwen@microsoft.com &Xiaofang Zhou

CSE, HKUST

Hong Kong SAR, China

zxf@ust.hk &Jiang Bian

Microsoft Research Asia

Beijing, China

jiang.bian@microsoft.com &Jia Li

DSA, HKUST(GZ)

Guangzhou, China

jialee@ust.hk

This work was done during the internship at Microsoft Research Asia.Corresponding Author.

###### Abstract

Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons. Despite the recent strides in crafting specific architectures for time-series forecasting and developing pre-trained universal models, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking. This paper bridges this gap through the design and evaluation of the Elastic Time-Series Transformer (ElasTST). The ElasTST model incorporates a non-autoregressive design with placeholders and structured self-attention masks, warranting future outputs that are invariant to adjustments in inference horizons. A tunable version of rotary position embedding is also integrated into ElasTST to capture time-series-specific periods and enhance adaptability to different horizons. Additionally, ElasTST employs a multi-scale patch design, effectively integrating both fine-grained and coarse-grained information. During the training phase, ElasTST uses a horizon reweighting strategy that approximates the effect of random sampling across multiple horizons with a single fixed horizon setting. Through comprehensive experiments and comparisons with state-of-the-art time-series architectures and contemporary foundation models, we demonstrate the efficacy of ElasTST's unique design elements. Our findings position ElasTST as a robust solution for the practical necessity of varied-horizon forecasting. ElasTST is open-sourced at https://github.com/microsoft/ProbTS/tree/elastst.

## 1 Introduction

Time-series forecasting plays a crucial role in diverse industries, where it is essential to provide forecasts over various time horizons, accommodating both short-term and long-term planning requirements. This includes predicting COVID-19 cases and fatalities one and four weeks ahead to allocate public health resources , estimating future electricity demand on an hourly, weekly, or monthly basis to optimize power management , and projecting both immediate and long-term traffic conditions for efficient road management , among others.

Despite this, a majority of advanced time-series Transformer  variants developed in recent years still necessitate per-horizon training and deployment [37; 40; 33; 32; 22; 20; 39]. These models struggle to handle longer inference horizons once trained for a specific horizon, and may yield sub-optimal performance when assessed for shorter horizons. These constraints lead to the practical inconvenience of maintaining distinct model checkpoints for different forecasting horizons required by real-world applications.

Even though recent studies on pre-training universal time-series foundation models have made some progress in facilitating varied-horizon forecasting [26; 9; 8; 31], they primarily concentrate on assessing the overall transfer performance from pre-training datasets to zero-shot scenarios. However, they lack an in-depth investigation into the challenges of generating robust forecasts for different horizons. To be specific, TimesFM , a decoder-only Transformer, is capable of arbitrary-horizon forecasting, but this approach could potentially lead to substantial error propagation in long-term forecasting scenarios due to autoregressive decoding. DAM , though free from this issue thanks to a novel output design composing sinusoidal functions, cannot effectively capture abrupt changes in time-series data, thereby limiting its utility in critical domains such as energy and traffic. Moreover, while MOIRAI  employs a full-attention encoder-only Transformer architecture and supports arbitrary-horizon forecasting via a non-autoregressive manner by introducing mask tokens into forecasting horizons, it remains uncertain how well MOIRAI adapts to different horizons. For example, its architecture design does not ensure the _horizon-invariant_ property: the model output for a specific future position should be invariant to arbitrary extensions in forecasting horizons beyond that. Besides, its performance could drop significantly for moderate context lengths.

To address this research gap, we introduce a comprehensive study to explore how to construct a time-series Transformer variant that can yield robust forecasts for varied inference horizons once trained. We name the developed model as _Elastic Time-Series Transformer_ (ElasTST). ElasTST adopts a non-autoregressive design by incorporating placeholders into forecasting horizons, which is inspired by diffusion Transformers  and the success of SORA  in video generation. Here we impose structured self-attention masks, only allowing placeholders to attend to observed time-series patches. This design ensures the aforementioned _horizon-invariant_ property by blocking the information exchange across placeholders. Additionally, we devise a tunable version of rotary position embedding (RoPE)  to capture customized period coefficients for time series and to learn the adaptation to varied forecasting horizons. Furthermore, we introduce a multi-patch design to balance fine-grained patches beneficial to short-term forecasting with coarse-grained patches preferred by long-term forecasting, and use a shared Transformer backbone to handle these multi-scale patches. Alongside core model designs, during the training phase, we deploy a horizon reweighting approach that approximates the effects of random sampling across multiple training horizons using just one fixed horizon, eliminating the need for additional sampling efforts. Collectively, these key customizations facilitate ElasTST to produce consistent and accurate forecasts across various horizons.

Our extensive experiments affirm the effectiveness of ElasTST in varied-horizon forecasting. First, we evaluated ElasTST, trained with a fixed horizon and employing a reweighting scheme, against state-of-the-art models trained for specific inference horizons. The results demonstrate that ElasTST delivers competitive performance without requiring per-horizon tuning. Then, we examined varied-horizon forecasting for these models, and the advantages of ElasTST are much more outstanding, demonstrating remarkable extrapolations to longer horizons while preserving robust results for shorter ones. Moreover, we also compared ElasTST with some pre-trained time-series models, such as TimesFM and MOIRAI, and found that dataset-specific tuning still offers prominent advantages over zero-shot inference in challenging datasets, such as Weather and Electricity, and that ElasTST can provide more robust performance across different forecasting horizons. At last, we conducted comprehensive ablation tests to highlight the significance of each unique design element of ElasTST.

In summary, our contributions comprise:

* Conducting a systematic study on varied-horizon forecasting, a critical requirement across various domains, yet an underexplored area in time-series research.
* Developing a novel Transformer variant, ElasTST, which incorporates structured attention masks for horizon-invariance, tunable RoPE for time-series-specific periods, multi-patch representations to balance fine-grained and coarse-grained information, and a horizon reweighting scheme to effectively simulate varied-horizon training.

* Demonstrating the effectiveness of ElasTST through experiments comparing it with state-of-the-art time-series architectures and some up-to-date foundation models. Our ablation tests further reveal the importance of its key design elements.

## 2 Related Work

Traditional Neural Architecture Designs for Time-Series ForecastingThe field of time-series forecasting has witnessed a significant evolution of neural architectures, transitioning from early multi-layer perceptrons , convolutional , and recurrent networks , to a more recent focus on various Transformer variants [37; 40; 33; 32; 22; 20; 39]. However, the challenge of varied-horizon forecasting remains underexplored in these studies, as these models often require specific tuning to optimize performance for each inference horizon. Additionally, many models, including PatchTST , iTransformer , and MTST , utilize horizon-specific projection heads, which inherently complicates the extension of their forecasting horizons.

Developing Foundation Models for Time-Series ForecastingInspired by the remarkable successes in the creation of foundational models in the language and vision domains [4; 25; 3], the trend of pre-training universal foundation models has emerged in time-series forecasting research. Notable works in this area include Lag-Llama , DAM , TimesFM , and MOIRAI . These studies employ unique designs to address the challenges posed by varied variate numbers and forecasting horizons when adapting to new scenarios. Lag-Llama, DAM, and TimesFM adopted the univariate paradigm to circumvent the difficulties associated with handling different variates. In contrast, MOIRAI has taken a different approach by flattening multi-variate time series into a single sequence to facilitate cross-variate learning. While this method has its merits, it is worth noting that it may introduce efficiency issues when handling a substantial number of variates and long forecasting horizons. As a result, this paper also adopts the univariate setup to maintain efficiency. When it comes to varied forecasting horizons, Lag-Llama and TimesFM both utilized the decoder-only Transformer and relied on autoregressive decoding to manage arbitrarily long horizons. DAM introduced a novel output scheme that comprises numerous sinusoidal basis functions, enabling it to project into arbitrary future time points. MOIRAI, on the other hand, used a composite input scheme, combining observed time-series patches with variable placeholders that indicate forecasting horizons, and built a full-attention encoder-only Transformer on top of this. Interestingly, this non-autoregressive generation paradigm originates from diffusion transformers used in video generation [24; 3]. In this paper, we also embrace this paradigm for generating variable-length time-series. Unlike MOIRAI, which has made considerable strides in time-series pre-training using a moderately designed Transformer variant, our focus lies in systematically examining critical architectural enhancements to improve robustness in time-series forecasting across various horizons. We believe that constructing a more robust, resilient, and universal architecture will pave the way for more powerful foundational time-series models to be pre-trained in the future.

Position Encoding in Time-Series TransformersPosition encoding plays a pivotal role in Transformers as both self-attention and feed-forward modules lack inherent position awareness. The majority of existing time-series Transformer variants have roughly adopted absolute position encoding  with minor modifications across different studies. For instance, Informer  and Pyraformer  have combined fixed absolute position embeddings with timestamp embeddings such as day, week, hour, minute, etc. Meanwhile, Autoformer  and Fedformer  have omitted absolute position embeddings and relied solely on timestamp embeddings. Other models like LogTrans  and PatchTST  have explored learnable position embeddings. However, the challenge with absolute position embedding is its inability to extrapolate into unseen horizons, posing a significant challenge for varied-horizon forecasting. To address this issue, MOIRAI has utilized a relative position embedding technique, RoPE , which has been broadly adopted in the language domain to handle variable-length sequences . In our work, we also adopt RoPE to introduce relative position information into self-attention operations. What we uniquely reveal is that the direct application of the RoPE configuration from the language domain to time-series forecasting is not ideal. The reason being that the predefined coefficients do not align well with the typical periodic patterns observed in time-series data. As a solution, we suggest redefining the period range encompassed by the initial RoPE coefficients and making data-driven adjustments to these coefficients.

Input Patches in Time-Series TransformersPatchTST  spearheaded the concept of segmenting time-series data into patches instead of feeding raw time-series values directly into Transformer models. This straightforward yet effective approach has been widely adopted in subsequent studies, including MTST , TSMixer , HDMixer , and MOIRAI. It noteworthy that MOIRAI has been trained with a diverse range of time-series patches with varying patch sizes. When adapting it to a new dataset, practitioners need to search through a range of patch sizes and rely on validation performance to select a single patch size. In our work, however, we have demonstrated that segmenting time series into multiple patch sizes to create multi-scale patch representations is more advantageous. This approach further aids in stabilizing accurate forecasting across various horizons.

## 3 Elastic Time-Series Transformers

In Figure 1, we present an overview of ElasTST. Different from other encoder-only Transformer architectures, ElasTST equipped three core designs to facilitate varied-horizon forecasting: structured self-attention masks for placeholders, tunable rotary position embedding (TRoPE) with customized period coefficients, and a multi-scale patch representation learning. Additionally, we utilize a horizon reweighting scheme to achieve the effects of varied-horizon training.

NotationsWe define a univariate time series as \(_{1:T}=\{x_{t}\}_{t=1}^{T}\), with \(x_{t}\) indicating the value at time index \(t\). The learning objective of a varied-horizon forecasting can be formulated as: \(_{}_{ p(),(t,L,T) p()}  p_{}(_{t+1:t+T}|_{t-L+1:t})\), where \(p()\) is the data distribution from which time series samples are drawn, and \(p()\) is the task distribution, from which the timestamp \(t\), look-back window \(L\), and the prediction horizon \(T\) are sampled.

Model InputsTo accommodate varied forecast horizons, our model combines the historical context series \(_{t-L+1:t}\) with placeholders \(^{T}\) through concatenation, forming the input \(X=(_{t-L+1:t},)\). This approach allows for flexible adjustment of the input and output dimensions to suit different forecasting scenarios. We further segment \(X\) into non-overlapping patches \(X^{p}^{N P}\), where \(P\) is the patch length and \(N=\) represents the number of patches. Each input patch is then transformed into latent space by the encoder \(=(X^{p}),^{N D}\).

Figure 1: Overview of the ElasTST Architecture. ElasTST employs (a) structured attention masks for placeholders to ensure consistent outputs across varied forecasting horizons. It incorporates (b) tunable RoPE customized to time series periodicities, enhancing its robustness. The architecture also integrates a (c) multi-scale patch assembly that merges fine-grained and coarse-grained details for improved forecasting accuracy. Furthermore, we implement (d) training horizon reweighting scheme during the training phase, which effectively simulates random sampling of forecasting horizons, reducing the need for additional sampling efforts.

Masked Self-AttentionA robust varied-horizon forecasting method should deliver consistent outputs across different forecasting horizons while maintaining high accuracy on unseen horizons. Existing time series Transformers, however, typically directly adapt techniques from video generation and natural language processing without considering the unique characteristics of time series. To address this deficiency, ElasTST modifies a standard Transformer Encoder with two crucial enhancements: structured attention masks and a tunable RoPE to encode relative position information effectively. We formulate the attention scores within a masked self-attention as

\[a_{m,n}= f^{}(_{m}^{q},m),f^{}( _{n}^{k},n) M_{m,n},\] (1)

where \(^{q},^{k}^{D d}\) denote the linear mappings for the query and key, respectively. A tunable RoPE \(f^{}\) dynamically adjusts the relative position encoding manner to best suit each dataset, with further details provided in the following subsection. The structured attention mask \(_{,n}\) is set to \(0\) for patches \(X^{p}_{n}\) consisting solely of placeholders and \(1\) otherwise, ensuring that tokens attend only to context-carrying patches. This structured masking, in conjunction with the relative position encoding, prevents the influence of placeholders on prediction outcomes, thus ensuring consistent outputs across varied forecasting horizons.

Tunable Rotary Position EmbeddingPosition embedding is crucial for the attention mechanism to maintain accuracy over unseen horizons. To overcome the limitations of absolute position embedding in extrapolation scenarios, RoPE has been widely adopted in the NLP domain for handling variable-length sequences. It rotates a vector \(^{d}\) onto an embedding curve on a sphere in \(^{d/2}\), with the rotation parameterized by a base frequency \(b\). The function is defined as \(f^{}(,t)_{j}=(x_{2j-1}+ix_{2j})e^{ib^{-2(j-1)/d}t}\), where \(j[1,2,...,d/2]\). Typically in NLP, the base frequency \(b\) is set to a constant, such as 10,000. However, due to the unique characteristics of time series data, specific adaptations of RoPE are necessary. In this paper, we propose to use the period coefficients \(_{j}=}\) for parameterization:

\[f^{}(,t)_{j}=(x_{2j-1}+ix_{2j})e^{i}t}\] (2)

\[_{j}=_{}e^{2(j-1)},= (_{}}{_{}})\] (3)

where \(_{}\) and \(_{}\) represent the predefined minimum and maximum period coefficients, respectively. This formula maintains an exponential distribution but adjusts the range to better align with the periodic characteristics of time series data. By setting \(_{}=2\) and \(_{}=2 b^{1-}\), this approach mirrors the original RoPE setup.

In addition to adjusting the period range, the distinct and varying periodicities inherent in time series data necessitate more flexible period coefficients. Therefore, in ElasTST, we consider period coefficients \(\) as tunable parameters, optimizing it along with varied datasets and forecasting horizons. This adaptive approach allows for more precise and effective forecasting across diverse conditions. We provide a detailed exploration of this design in Section D.4, and illustrate the optimized period coefficients for each dataset in Appendix E.2.

Multi-Scale Patch AssemblyTo ensure robust performance across various forecasting horizons, integrating both fine-grained and coarse-grained features from time series data is essential. Different from earlier multi-patch models that utilize separate processing branches for each patch size , ElasTST features a multi-scale patch design within a shared Transformer backbone, capable of both parallel and sequential processing. We chose sequential processing for our implementation, keeping the memory consumption comparable to baselines such as PatchTST. The implications of this design on memory usage are further discussed in Appendix F. Specifically, we define each patch size as \(=\{p_{1},,p_{S}\}\), with each size corresponding to a dedicated MLP encoder \(f^{}_{p_{i}}:^{p_{i}}^{D}\) and decoder \(f^{}_{p_{i}}:^{D}^{p_{i}}\). The outputs from each size are flattened and then averaged to produce the final forecast \(\). During training, losses under individual patch size are calculated and averaged with the assembled forecast losses, to enhance accuracy and consistency across different scales. Further details on the effectiveness of this design is provided in Section 4.2.

Training Horizon ReweightingTo effectively manage varied forecasting horizons, training models across multiple horizon lengths, rather than using fixed ones, is a practical approach . Inthis study, we propose to use reweighting scheme for loss computation that simulates this process, without the need for additional sampling efforts. Formally, in the conventional implementation, at each training step \(s\), a forecasting horizon \(T_{s}\) is randomly selected from the range \([1,T_{}]\).1 Then the loss \(_{s}\) at step \(s\) is computed as:

\[_{s}=_{=1}^{T_{s}}()(x_{t+}-_{t+}) ^{2},()=}.\] (4)

Theoretically, the expectation of this random sampling process can be represented as a weighted loss over a fixed horizon \(T_{}\). To be specific, the expected value of \(()\) is calculated as: \([()]=}}_{T=1}^{T_{}} \). We further approximate the reweighting function by harmonic series as:

\[()}}((T_{})-()).\] (5)

By employing this weighted loss \(()\) during training, we replicate the effect achieved by randomly sampling horizons at an infinite number of training steps. In addition, the function \(()\) can be adapted to follow any desired distribution family and can be made differentiable.

## 4 Experiments

To validate the effectiveness of ElasTST, we systematically assess its performance across various forecasting scenarios, benchmarking it against established models. The results, detailed in Section 4.1, showcase ElasTSTs adaptability to diverse forecasting horizons. Subsequently, we perform an extensive ablation study in Section 4.2 to examine the impact of its key designs.2

DatasetsOur experiments leverage 8 well-recognized datasets, including 4 from the ETT series (ETTh1, ETTh2, ETTm1, ETTm2), and others include Electricity, Exchange, Traffic, and Weather. These datasets cover a wide array of real-world scenarios and are commonly used as benchmarks in the field. Detailed descriptions of each dataset are provided in Appendix C.1. Following the setup described in , all models use a standard lookback window of 96, except TimesFM  and MOIRAI , which utilize extended lookback windows of 512 and 5000, respectively.

BaselinesFor our comparative analysis, we select 6 representative forecasting models as baselines: (1) Advanced but non-elastic forecasting models, such as iTransformer , PatchTST , and DLinear ; (2) Auttoformer , which supports varied-horizon forecasting but requires horizon-specific tuning; (3) the cutting-edge time series foundation model like TimesFM  and MOIRAI , which are pre-trained for general-purpose forecasting across varied horizons. Our analysis primarily assesses the varied-horizon forecasting capabilities, considering their pre-training on subsets of the datasets used.

ImplementationElasTST is implemented using PyTorch Lightning , with a training regimen of 100 batches per epoch, a batch size of 32, and a total duration of 50 epochs. We use the Adam optimizer with a learning rate of 0.001, and experiments are conducted on NVIDIA Tesla V100 GPUs with CUDA 12.1. To ensure fairness, we conducted an extensive grid search for critical hyperparameters across all models in this study. The range and specifics of these hyperparameters are documented in Appendix C.2. For parameters not mentioned in the table, we adhered to the best practice settings proposed in their respective original papers. For evaluation, we use Normalized Mean Absolute Error (NMAE) and Normalized Root Mean Squared Error (NRMSE) as they are scale-insensitive and widely accepted in recent studies . More details are in Appendix C.3.

### Main Results

Comparing ElasTST with Horizon Reweighting to Neural Architectures Tuned for Specific Inference HorizonsExperimental results demonstrate that ElasTST consistently delivers exceptional performance across all horizons without the need for per-horizon tuning. As evidenced in Table 1, ElasTST outperformed SOTA models on diverse datasets including ETTm1, ETTh1, ETTh2,

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_EMPTY:8]

Multi-Patch DesignThese experiments demonstrate that multi-patch configurations generally outperform single patch sizes across various forecasting horizons. Figure 5 shows that the configuration \(=\{8,16,32\}\) consistently achieves the lowest NMAE values, effectively balancing the capture of short-term dynamics and long-term trends. However, adding larger patches, such as \(=\{8,16,32,64\}\), does not consistently improve performance and can sometimes increase the NMAE. This suggests that more complex configurations may not always provide additional benefits and could even be counterproductive.

Moreover, the patch size selection is particularly critical in the varied-horizon forecasting scenarios. As demonstrated in the Figure 10 (see Appendix D.5), various combinations of training and forecasting horizons exhibit distinct preferences for patch sizes. For instance, when the training forecasting horizon is 720, during the inference stage, longer forecasting horizons prefer larger patch sizes. Conversely, on shorter training horizons, such as 96 and 192, choosing large patch sizes for longer horizons can lead to performance collapse. This difference underscores the complexity and necessity of optimal patch size selection in achieving effective elastic forecasting. Detailed results for four training horizons and further analysis are provided in Appendix D.5.

The Impact of Training HorizonsFurther experiments validate the effectiveness of our proposed training horizon reweighting scheme in enhancing varied-horizon inference. As illustrated in Figure 6, reweighting longer horizons simplifies the training process, yielding better outcomes than

Figure 4: Ablation study for designs in position embedding. A vertical red dashed line distinguishes between seen horizons and unseen horizons.

Figure 5: Performance of patch size selections. Results are averaged across all datasets and training horizons of \(\{96,192,336,720\}\). ‘8_16_32’ represents a multi-patch configuration of \(=\{8,16,32\}\).

selecting a fixed horizon and mitigating the uncertainties associated with random sampling. Crucially, this training approach is model-agnostic and can be applied to different forecasting training scenarios. These results also highlight the advantages of a flexible forecasting architecture, which allows training horizons to be customized to the unique characteristics of each dataset.

We also observe that different datasets have distinct preferences for training horizons. For example, in the Exchange dataset, the longest training horizon led to worse results compared to a shorter horizon of 96, suggesting risks of overfitting or forecast instability with prolonged horizons. Besides, in the ETTh1, employing random sampling for training horizons proved suboptimal. These insights show that tailoring the training horizon selection strategy to the specific dataset can yield improvements. One potential enhancement could involve dynamically optimizing the horizon reweighting scheme alongside model training.

## 5 Conclusion

This study introduces the Elastic Time-Series Transformer (ElasTST), a pioneering model designed to tackle the significant and insufficiently explored challenge of varied-horizon forecasting. ElasTST integrates a non-autoregressive framework with innovative elements such as structured self-attention masks, tunable Rotary Position Embedding (RoPE), and a versatile multi-scale patch system. Additionally, we implement a training horizon reweighting scheme that simulates random sampling of forecasting horizons, thus eliminating the need for extra sampling efforts. Together, these elements enable ElasTST to adapt to a wide range of forecasting horizons, delivering reliable and competitive outcomes even when facing horizons that were not encountered during the training phase.

LimitationsWhile ElasTST demonstrates robust performance across various forecasting tasks, several limitations have been identified that highlight opportunities for future enhancements. First, the current version of ElasTST does not incorporate a pre-training phase, which could significantly improve the models initial grasp of time-series dynamics and boost its efficiency during task-specific fine-tuning. Further exploration is needed to ascertain optimal training methodologies that maximize the architectural benefits of ElasTST. Additionally, while the training horizon reweighting scheme is straightforward and effective in enhancing performance across different inference horizons, it is not the optimal solution for all datasets. Moreover, the evaluation of ElasTST is limited to a select number of datasets, which may not fully represent the broader challenges encountered in more complex or diverse real-world scenarios.

Future WorkIn response to these limitations, our forthcoming research efforts will concentrate on developing and validating pre-training protocols for ElasTST to elevate its foundational performance and extend its applicability across universal forecasting tasks. We aim to incorporate a reasonable training approach that will fine-tune the models ability to seamlessly manage forecasts of varying lengths, thus bolstering its utility in dynamic real-world environments. Furthermore, by broadening the range of datasets used for model evaluations, we intend to rigorously test ElasTSTs effectiveness across an expanded spectrum of industry-specific challenges. This comprehensive approach will not only solidify ElasTSTs standing as a cutting-edge solution for time-series forecasting but also enhance our understanding of its practical implications and potential in diverse industrial applications.

Figure 6: Impact of forecasting horizon selection during the training phase.