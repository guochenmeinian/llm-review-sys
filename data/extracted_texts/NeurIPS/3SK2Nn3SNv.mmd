# Unlearning in- vs. out-of-distribution data

in LLMs under gradient-based methods

 Teodora Baluta 1

teobaluta@gatech.edu

&Pascal Lamblin 2

lamblinp@google.com

&Daniel Tarlow 2

dtarlow@google.com

Fabian Pedregosa 2

pedregosa@google.com

&Gintare Karolina Dziugaite 2

gkd@google.com

###### Abstract

Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.

## 1 Introduction

Training large language models (LLMs) often involves complex data pipelines. These pipelines handle large quantities of data, some of which might be sensitive. Recently, it has been shown that LLMs are susceptible to sentence-level membership inference attacks (Gu et al., 2023) and reconstruction attacks (Carlini et al., 2019), meaning that one may be able to infer which data was part of the training set, or in some cases, even reconstruct partial inputs by interrogating the model. As a result, this raises a prevalent problem of data removal from a trained LLM.

To this end, there has been growing interest in formalizing technical definitions of machine unlearning and designing machine unlearning techniques and evaluation metrics (Triantafillou et al., 2023, 2024). The goal of machine unlearning is to remove the influence of a subset of the original training data, the _forget set_, from a corresponding model. A naive way to achieve it is to retrain the model from scratch on an updated training set (the _retain set_), that does not include the forget set. This approach is resource-intensive, and does not scale to the large models now in development.

Efficient alternatives in LLMs often rely on gradient ascent-based procedures, where one maximizes some loss on the data to be forgotten to reduce the influence of this data on the model predictions (Jang et al., 2022). However, there are a few issues that arise with this approach: (1) inherently, gradient ascent-based unlearning does not come with guarantees, and one needs a way to empirically evaluate the unlearning quality; (2) such unlearning methods do not only affect the forget set examples, but also come at a performance cost on the rest of the data.

Our work touches upon both of these issues. For the first issue, we propose two metrics for evaluating unlearning quality. The first metric, named _generalized exposure_, lower bounds unlearning quality under a particular unlearning definition (Triantafillou et al., 2023), but requires access to a_reference model_, that had never seen the forget set, to compute likelihoods. Another metric, _relative exposure_, is an approximation to the first, further estimating the likelihoods that would be computed by a reference model, only using the current model pre- and post-unlearning.

For the second issue, we present an extensive empirical evaluation, on LLMs, of how unlearning via gradient ascent differs for in- versus out-of-distribution examples. We visualize the trade-offs between unlearning quality as measured per our definitions, and performance on the rest of the data. We capture different patterns of these trade-offs depending on the difficulty of the examples in the forget set, and depending on the degree of memorization of these examples. Our contributions can be summarized as following:

* We propose a new metric for evaluating unlearning in generative models using a reference model that had never seen the unlearning data. Further, we propose an approximation to this metric that does not require having access to the reference model.
* Using our proposed metrics, we evaluate gradient ascent-based unlearning in large language models, and observe that unlearning out-of-distribution samples can be done nearly without affecting the BLEU score Papineni et al. (2002) just like in the reference model. In contrast, unlearning in-distribution samples affects the performance, unlike in the reference model. This indicates a weakness in gradient ascent-based unlearning, and suggests that simultaneous gradient descent on the retain data might be necessary.
* Finally, we evaluate whether measuring unlearning on a data point could be done using similar samples. We observe that similar examples in the training data are unlearned together with the ones on which unlearning is performed. Similar examples outside of the training dataset are almost not affected by this unlearning procedure. This explains why we observe performance degradation for in-distribution examples.

## 2 Preliminaries

Let \(\) be a parameterized space of models (e.g., \(=^{d}\) in the case of neural networks with \(d\) parameters). For our purposes, we care only about the output distribution of learning algorithms. That is, if \(Z^{*}\) denotes the set of finite sequences of input examples, and \(()\) denotes the space of distributions on \(\), a learning algorithm will be viewed as a map \(:Z^{*}()\), and so running the algorithm on a size-\(n\) dataset \(S Z^{n}\) produces the model \((S)\).

In the context of autoregressive sequence models, such as LLMs, the set of training data \(S Z^{*}\) consists of samples \(x Z\), each a sequence of tokens, \(x=(x_{1},,x_{k})\). A model \(\) defines conditional distributions on the next token \(x_{i}\) given all previous tokens \(x_{1:i-1}\), denoted \(f(x_{i}|x_{1:i-1};)\). For a fixed model \(\), we consider its _output_ on a given sequence of tokens \(x=(x_{1},,x_{k}) Z\) to be \(f(x;)=_{i=1}^{k}f(x_{i}|x_{1:i-1};)\), the probability it assigns to that sequence (in other words, the likelihood of \(x\) under the model \(\)).

Let \((,x)=-(f(x;))\) denote the negative log likelihood (NLL) of \(x\). The training objective of language models we consider is based on that loss, averaged over \(x S\). It is minimized using gradient-based iterative algorithms. Although it is immaterial to this work, training can often be viewed as stochastic gradient descent (or some variant) applied to the objective \(\,(,X)\), where the expectation is taken over \(X\) sampled from \(S\).

### Unlearning

Given that a learning algorithm has produced a model \((S)\), the goal of unlearning is to remove the influence of a subset \(F S\) of the training data. We call \(F\) the _forget set_, \(S F\) the _retain set_.

There are many ways one might formalize unlearning. We consider the following definition of unlearning (Sekhari et al., 2021; Gupta et al., 2021; Neel et al., 2021)3:

**Definition 2.1**.: An algorithm \(\) is a _worst-case \(\)-unlearner (for \(\))_ if, for every training set \(S\), forget set \(F S\) of fixed size, and measurable subset \(B\), letting \(_{}((S),F)\) and \(_{-F}(S F)\),

\[e^{-}(_{-F} B|S,F)(_{} B|S,F)  e^{}(_{-F} B|S,F).\] (1)

For any distribution over training data \(S\) and forget sets \(F\), we say \(\) is an _on-average \(\)-unlearner_ if Equation (1) holds when the probabilities are unconditional.

We refer to a sample \(_{-F}(S F)\) as a _reference model_. Definitions of unlearning vary in a number of ways, including in terms of what information is available to the unlearning algorithm. The role of access to (statistics of) the training data is studied by Sekhari et al. (2021).

In this work, we will study unlearning algorithms that operate by performing gradient ascent on the NLL loss, averaged over the forget set \(F\).

## 3 Evaluation of Unlearning in Large Language Models

Fix a pair of algorithms \(\), \(\). Let \(\) be the set of measurable functions from \(\) taking values in \(\). For training data \(S\) and forget set \(F S\), the smallest \(\) satisfying Equation (1) is

\[_{S,F}=_{g}(|[g(_{ })]-[g(_{-F})]|),\] (2)

where \(_{}((S),F)\) and \(_{-F}(S F)\). The supremum \(_{S,F}_{S,F}\) is the tightest parameter for the worst-case notion.

It follows that evaluating the argument in the r.h.s. of Equation (2) with any \(g\) yields a lower bound on the unlearning parameter \(\). Below we construct a function \(g\) that we use to evaluate unlearning.

Let \(F S\) be a set of strings we want to forget. In addition, consider \(n\)_reference strings_\(R=\{r_{i}\}_{i=1}^{n}\), sampled from some given distribution, and that are not part of \(S\) (or \(F\)). Recall that \((,x)=- f(x;)\) is the negative log-likelihood of a sequence \(x\) under model \(\). Let

\[g(x;,R)=_{j=1}^{n}(,x)}{(,x)+(,r_{j})}.\] (3)

Each term \((,x)/((,x)+(,r_{j}))\) can be seen as a relaxation of the hard comparison \(((,x)(,r_{j}))\) (or equivalently \( f(r_{j};) f(x;)\), as the NLL is monotonically decreasing). In aggregate, it represents the fraction of reference strings in \(R\) that have an NLL higher than \(x\). \(g\) can be seen as a soft version (scaled to \(\)) of the rank of \(f(x;)\) among the probabilities of reference strings \(\{f(r_{j})\}_{j=1}^{n}\). A smaller value of \(g\) indicates \(x\) is more likely under \(\) (has a smaller loss) than elements of \(R\), a larger value indicates it is less likely (has a larger loss). If \(g(x;,R)<\), then there are at most \(2n\) elements \(r_{i}\) of \(R\) such that \(f(r_{i};)>f(x;)\) (and \((,r_{i})<(,x)\)). Similarly, if \(g(x;,R)>1-\), then at most \(2n\) elements \(r_{i} R\) satisfy \(f(r_{i};)<f(x;)\) (and \((,r_{i})>f(x;)\)).

We define _Generalized Exposure_ of \(x F\) relative to a set of reference strings \(R\) to be

\[(x;,,F,S)= [g(x;_{-F},R)]-[g(x;_{ },R)].\] (4)

Taking the absolute value of \(\) yields a lower bound on the worst-case epsilon in Equation (2) for a fixed \(g\). One cannot compute the expectations in Equation (4) exactly, since the distributions of \(_{}\) and \(_{-F}\) are not tractable in a standard deep learning setup. In our experiments, we use a Monte Carlo estimate of the expectations in the generalized exposure metric to get an approximate lower bound on the unlearning quality. Such estimates are subject to variance. Alternatively, one could threshold the observed \(g(x;_{},R)\), which would effectively correspond to choosing a different \(g\) in Equation (2), and then use Clopper-Pearson confidence intervals for binomials to compute the confidence intervals of the estimates (Clopper & Pearson, 1934) (also see (Jagielski et al., 2020)).

Exposure and memorization4.Generalized exposure can be seen as an extension of the _exposure_ metric that appeared in the memorization literature, introduced by Carlini et al. (2019). There, the authors inject secret _canaries_ (i.e., strings generated randomly, from a different distribution than the regular data distribution) \(C=\{c_{i}\}_{i=1}^{n}\) in the training set \(S\). In our notation, \(C=F\). In addition, \(n\) reference strings \(\{r_{i}\}_{i=1}^{n}\) are sampled from the same distribution. For each canary \(c_{i}\), letting \((l_{i}|\{l_{j}\}_{j})\) denote the rank of \(l_{i}\) in the set \(\{l_{j}\}_{j}\), Carlini et al. (2019) define exposure as5:

\[(c_{i};)=_{2}(n)-_{2}((( ,c_{i})|\{(,r_{j})\}_{j=1}^{n}))\] (5)

\[(c_{i};)=-_{2}_{j=1:n}[(,r_{j}) (,c_{i})].\] (6)

This metric is meant to capture how much the model memorized the canaries relative to the reference strings that were not seen during training.

Generalized Exposure uses a function \(g\) (Equation (3)) that can be seen as a soft version of the comparison function used in the second formulation of exposure (Equation (6)). The reference strings it uses do not have to come from outside of the distribution of regular data in general, but we can consider \(R=\{r_{i}\}_{i=1}^{n}\), as defined above, as a special case.

For a randomly generated string \(r\) coming from the same distribution as \(R\), and never seen during learning or unlearning (\(\) would be independent of them), \(g(r;,R)\) should be around \(_{2}\), and each term in 4 of the form \(-\,g(r;,R)=(2)\). Similarly, the probability in Equation (6) will tend to \(_{2}\), and exposure to \(_{2}(2)=1\).

When no memorization happens, Generalized Exposure for these randomly generated canary strings \(C\) is zero. To see this, note that \(\,g(c_{i};)=\) under no memorization, and both sides of Generalized Exposure cancel out. For the exposure computation, the outcome of the comparison is \(\), giving \((c_{i};)=-_{2}=1\). Under maximal memorization of \(c_{i}\), the loss would be smaller than for all the reference strings, and thus \((c_{i};)\). Similarly, the first term in the Generalized Exposure, \(-[g(x;_{},R)]\) would tend to \(\) when \(g(x;_{},R)\) gets arbitrarily close to 0 as \(f(c_{i};_{})\) increases with more memorization relative to the reference strings.

Membership inference attacks and differential privacy.Jagielski (2023) connects the exposure metric from (Carlini et al., 2019) to differential privacy and so-called membership inference attacks. Recall that a training algorithm \(\) is \(\)-differentially private (DP) if, for all \(S\) and \(S^{}\) that differ by one data point, and all measurable sets \(B\), \(( B) e^{}(^{} B)\), where \((S)\) and \(^{}(S^{})\). One can interpret DP as a hypothesis test to assess whether the output of the algorithm was obtained by running \(\) on \(S\) versus \(S^{}\). Kairouz et al. (2015) show that a particular computation based on false positive and false negative rates associated with this hypothesis test yields an estimate of \(\) in the differential privacy definition.

Through this hypothesis test view, \(\)-DP can be connected to a version of so-called _membership inference attacks_ (MIAs; see, e.g., Shokri et al. 2017), which attempt to identify whether a data point was or was not in the training set. Probably the most related MIA is a likelihood-ratio test (LiRA) introduced by Carlini et al. (2022). LiRA is motivated by the connections to hypothesis testing, trying to determine whether the observed prediction is more likely to have been sampled from a model that was trained on the sample of interest or without. The authors choose to do a likelihood ratio test (motivated by the Neyman-Pearson lemma), assuming that the predictions for a given sample have a Gaussian distribution.

Inspired by the work by Kairouz et al. (2015) connecting differential privacy and MIAs, Triantafillou et al. (2023, 2024) propose to estimate \(\) in the unlearning definition Equation (1) using false positive and false negative rates from a MIA perspective. In particular, letting \(\{\}\) denote all membership inference attacks, \(\) in the unlearning definition above can be estimated as a supremum over \(\{\}\) of a function of upper and lower bounds of false positive/negative rates for \(\).

### Relative exposure

Generalized exposure requires computing the expected probability of \(x\) under \((S F)\). Practically, having such a reference model may not be possible for computational and memory reasons. Here we introduce an alternative test that only requires access to the original model pre-unlearning.

As above, consider a set of reference strings \(R\), and let \(_{S}(S)\), \(_{-F}(S F)\) and \(_{}((S),F)\). For each given \(x\), we now randomly generate a second set of reference strings \(R_{x}\), such that \([g(x;_{-F},R)][}_ {r R_{x}}[g(r;_{S},R)]]\), where \(}_{r R_{x}}\) denotes an empirical mean over the elements in \(R_{x}\). In theory this is a complex task, and once again requires access to the reference model. In practice, however, we will simply choose \(R_{x}\) so its elements are close to \(x\) under some similarity metric (working in the embedding space), but not part of \(F\); \(R_{x}\) can contain examples from some auxiliary set (public data, held out data, etc.), that do not belong to the training set \(S\). It is also possible to define a common \(R_{x}\) for all \(x F\). By choosing such a set, we ensure that \(_{S}\) does not depend on \(R_{x}\), just like \(_{-F}\) does not depend on \(x F\). Further, when the forget set is small and does not affect the predictions on \(R_{x}\) through \(_{S}\) too much, we can expect our approximation to be more accurate.

Substituting this approximation to Equation (4), we get an alternative to Generalized Exposure that does not use \(_{-F}\). We define _Relative Exposure_ of \(x\) relative to \(R,R_{x}\), as

\[(x;,,F,S,R,R_{x})=_{2}[ }_{r R_{x}}[g(r;_{S},R)]]-_{2} [g(x;_{},R)].\] (7)

### Memorization and example difficulty

When evaluating unlearning, we group examples in the forget set based on measures of the extent to which an example has been memorized and of the example's difficulty.

More carefully, let \(_{S}(S)\) and \(_{-F}(S F)\). For any fixed example \(x F\), we define its _memorization_ as

\[[f(x;_{S})]-[f(x;_{-F})].\]

This is similar to the definition of memorization given by Feldman (2020) for classification tasks, which measures the difference between the probabilities of predicting the right class depending under \(_{-F}\) and \(_{S}\).

We define the _difficulty of any fixed example_\(x F\) as \([- f(x;_{-F})]\), i.e., expected perplexity under the reference model.

Note that all the expectations here are taken over the randomness of the training process (e.g., SGD noise, minibatch noise).

### Unlearning by gradient ascent

Based on Equation (1), one could achieve _exact_ (\(=0\)) unlearning by retraining from scratch without the forget set \(F\). This is not practical for large language models, due to resource constraints. Another common approach is to perform gradient ascent on the loss over \(F\), or/and gradient descent on the loss over \(S F\). Other alternatives have been proposed in the literature (Patil et al., 2023; Meng et al., 2022a,b), but a gradient ascent/descent-type procedure is still a common component in all of them.

While this approach is fairly efficient, and usually implemented with only a small number of gradient updates, it is not guaranteed that the obtained model after unlearning via gradient ascent/descent has truly forgotten the samples. Further, there is no set heuristic for the number of gradient updates to run during unlearning. This technique, thus, hinges on being able to assess how unlearned a set of examples is for a given language model.

### Unlearning in- versus out-of-distribution samples

For a reference model, unlearning, which is equivalent to not training on, out-of-distribution samples should not affect the overall performance, meaning that the models \(_{S}\) and \(_{-F}\) should performsimilarly. When the forget set contains in-distribution samples, then the effect depends on the size of the forget set relative to the training set. We focus on the typical case where the size of the forget set is small enough, and both models, \(_{S}\) and \(_{-F}\), perform similarly under the BLEU score. Thus a good unlearning algorithm should be able to unlearn without any observable trade-offs between unlearning quality and overall performance, as measured by the BLEU score.

## 4 Experiments

We evaluate the trade-offs between unlearning quality and performance on LLMs. We demonstrate that unlearning more memorized or more difficult examples is more damaging for the overall model performance. We also examine how neighbouring examples are affected by unlearning. Finally, we show that our relative exposure metric captures unlearning quality as well as the generalized exposure metric, thus showing a way to assess unlearning without having a reference model.

### Experimental setup

Models and datasets.We train a transformer model (T5-base with \(220\) million parameters (Roberts et al., 2022)) on "WMT14 En-De", a well-known language translation dataset that contains sentence pairs in German and English (Bojar et al., 2014). We train for \(45,000\) training steps with batch size of \(128\) on examples from the training split. We evaluate the task performance of the translation task using the BiLingual Evaluation Understudy (BLEU)) score (Papineni et al., 2002). Our models have a BLEU score of around \(26\), having a clear gist but with grammatical errors.

To avoid training numerous models, we only perform full training of two models:

* A _reference model_, of weights \(_{-F}\) is trained on the full standard training split \(T\) of the dataset mentioned above, without any additional example from a forget set.
* A _subject model_, of weights \(_{S}\), is trained on a dataset \(S\) made of \(T\) and the concatenation of all the potential forget sets \(F_{}\) defined below.

We consider an unlearning method based on gradient ascent (Section 3.3). Following Jang et al. (2022), we use a batch size of \(32\) when unlearning a set of \(512\) examples, giving us \(16\) unlearning steps to go through for the entire forget set considered. During each unlearning experiment, we consider one single forget set \(F\), and perform unlearning only on its examples. We always compare the resulting unlearned model with the same, shared reference model. Even though the reference model is only trained \(T\), which is a subset of the retain set of any given experiment (the full retain set would include the forget sets for the other experiments), we consider it a suitable approximation, as the ignored examples form only a small fraction of the training set.

Out-of-distribution forget sets generation.We generate out-of-distribution (OOD) canaries by sampling alpha-numeric characters uniformly at random, forming a sequence of fixed length (\(10\) characters). We create three disjoint sets \(F^{}_{}\) of 512 OOD canaries each, as well as a set \(R^{}\) of 10,000 reference strings from the same distribution. To study the effect of the number of repetition on unlearning, these sets are incorporated in the training set \(S\) with different frequencies: canaries in \(F^{}_{ 1}\) are seen only once during training, the ones in \(F^{}_{ 10}\) ten times, and \(F^{}_{ 100}\) a hundred times.

In-distribution forget sets generation.We generate sets of in-distribution (InD) examples by randomly selecting examples from the validation split of the dataset, so that we can train the reference model on the full training split \(T\), and these examples do not appear even once in its training set. We create three disjoint sets \(F^{}_{}\) of 512 in-distribution examples each, as well as a set \(R^{}\) of \(3,003\) reference strings formed from the test split. All of these sets of examples are disjoint. Similarly to the OOD canaries, these sets are incorporated in the training set with different frequencies. For the main subject model considered (\(_{S}\)), \(F^{}_{ 1}\) is seen only once, \(F^{}_{ 10}\) ten times, and \(F^{}_{ 100}\) a hundred times. Appendix A.2 also considers a model trained on a different training set \(S^{}\), where different validation examples are used in forget sets, see that section for details.

### Memorization vs. performance trade-offs

Our evaluated method of unlearning modifies the model by performing gradient ascent, as a result it might degrade the model's accuracy on the test set. We first evaluate the trade-off between the effectiveness of unlearning under generalized exposure and the task performance on the unlearned model (Figure 1). At every unlearning step, we measure the _average_ exposure of the canary, and, respectively, forget set. On these checkpoints, we compute the BLEU score on the test set.

Our first observation is that unlearning of canary data in one pass does not degrade the performance as much as unlearning in-distribution samples even when these are repeated as often. The average exposure value of the canaries also does not fall below \(1\) in one pass, meaning the canaries are still twice as less surprising to the model than other random samples unseen in training. The average exposure of the InD samples, however, falls to the minimum value. The reason is that unlearning InD examples affects the perplexities of other similar examples (Section 4.4), whereas for out-of-distribution, unlearning does not affect as much the other canaries' perplexities. This explains why the in-distribution examples have a much faster drop in exposure, as well as task performance.

Different Frequencies.In Figure 1, we observe that the more repeats of the in-distribution sample sets, the higher the (average) generalized exposure is before unlearning (top right point of each orange curve). A similar effect is visible for the exposure of OOD between the OOD \( 1\) and the OOD \( 10\) curves, although it is not visible in the OOD \( 100\) because the estimate of exposure is limited by \(_{2} R^{}\). In Appendix A.2, we also evaluate how a different number of repetition of the _same examples_ of the in-distribution sets affect the trade-off. Despite the three randomly-selected InD sets having a different distribution of perplexities under the reference model (as shown in Figure 2), the qualitative results are not affected by which set is repeated a given number of times.

Distribution of perplexities.We check how the perplexities of the in-distribution samples are affected before and after unlearning with respect to the reference model. We observe that the perplexities of the in-distribution set is reduced, but now the perplexities are skewed, not resembling at all the distribution on the reference model (Figure 2). We also plot the distribution of perplexities when the exposure is below a certain threshold which results in distributions that are closer to

Figure 1: **In- vs. out-of-distribution (canary) trade-off. Trade-off between the generalized exposure (Exposure) and the task performance (BLEU score) when unlearning the subject model (\(_{S}\)) at 45,000 steps, with in-distribution sets and canary sets repeated 100 times (left), 10 times (middle), and 1 time (right) during training.**

Figure 2: **Distributions of perplexities. Perplexities of different sets of in-distribution examples under the subject model (before unlearning, post-unlearning and when exposure is low) and the reference model. Columns left to right: in-distribution example perplexities when the subject model was trained by repeating these examples 100 times (left), 10 times (middle), 1 time (right).**the reference. This suggests an early-stopping strategy for unlearning could benefit in-distribution examples with more evidence of this effect at lower thresholds in Appendix A.4.

### Per-sample difficulty vs. memorization

We empirically evaluate the relationship between the difficulty of in-distribution examples and memorization. In Figure 3, we plot the memorization and difficulty of each example in the forget sets. The per-sample difficulty has a weak correlation with the per-sample memorization when the InD set is repeated once, but the correlation becomes strong with the number of repeats. We also cluster the in-distribution examples into 3 sets of low, medium and high perplexity based on their difficulty (Figure 4), and find that harder examples have slightly better trade-offs (see details in Appendix A.3).

### Unlearning effects on other points

We highlight that unlearning InD examples has an impact on other similar examples. We find similar examples by computing the \(L_{2}\)-distance in the embedding space of each point in the forget set on the reference model (see Appendix A.6). In Figure 5, we plot the memorization vs. performance trade-offs (as we unlearn \(F_{ 100}^{}\)) for both the set \(F_{ 100}^{}\) and a set of similar examples from \(F_{ 1}^{}\). The average exposure of the similar set decreases, without having to do unlearning. This explains why unlearning damages the performance of the model, since the model may forget other examples. Despite this, the effect of unlearning on similar examples outside of the training set is not significant. Thus, unlearning may affect examples that are more memorized as opposed to just similar examples.

## 5 Related Work

Recent work on unlearning in LLMs has focused on developing effective unlearning algorithms and robust evaluation metrics to assess the degree of unlearning achieved. We give a brief overview of the most relevant work here, and point interested readers to Appendix A.7 for more related work.

Unlearning benchmarks and evaluation metrics.Several works propose leveraging evaluation metrics of memorization with the aim to provide better unlearning methods in LLMs (Jang et al., 2022; Barbulescu and Triantafillou, 2024). Our work aims to work with a worst-case \(\)-unlearner (Definition 2.1) and can be seen as complementary to these approaches. Our experiments also point to stark differences between in- and out-of-distribution memorized data. An orthogonal unlearning approach is by removing of training data from the weights (Meng et al., 2022; Patil et al., 2023).

Memorization in LLMs.Whereas our work targets memorized data unlearning, a range of other memorization notions and concerns have been studied in LLMs (Lehman et al., 2021; Ippolito et al., 2022; Carlini et al., 2021; Choquette-Choo et al., 2021; Lukas et al., 2023).

Figure 5: Unlearning affects the average exposure of similar examples.

Conclusion

In this work, we propose a generalized exposure metric for evaluating unlearning. We find instances where gradient ascent-based techniques are insufficient for unlearning without destroying the model's performance. We explain this through the effect of unlearning on similar data.

#### Acknowledgments

We thank Daniel M. Roy and Eleni Triantafillou for feedback on various drafts of this work. This project used computational resources on Google Cloud Platform provided by Google, we would like to thank Danat Pomeranets in particular for support. Teodora was supported in part by the National Research Foundation Singapore under its NRF Fellowship Programme [NRF-NRFFAI1-2019-0004], by the Crystal Centre at National University of Singapore and its sponsors, and a Google PhD fellowship.