# \(K\)-Nearest-Neighbor Local Sampling Based

Conditional Independence Testing

 Shuai Li, Yingjie Zhang

School of Statistics

KLATASDS-MOE

East China Normal University

&Hongtu Zhu

Departments of Biostatistics,

Statistics, Computer Science, and Genetics

The University of North Carolina at Chapel Hill

&Christina Dan Wang

Business Division

New York University Shanghai

&Hai Shu

Department of Biostatistics

School of Global Public Health

New York University

&Ziqi Chen, Zhuoran Sun, Yanfeng Yang

School of Statistics

KLATASDS-MOE

East China Normal University

Corresponding author (zqchen@fem.ecnu.edu.cn).

###### Abstract

Conditional independence (CI) testing is a fundamental task in statistics and machine learning, but its effectiveness is hindered by the challenges posed by high-dimensional conditioning variables and limited data samples. This article introduces a novel testing approach to address these challenges and enhance control of the type I error while achieving high power under alternative hypotheses. The proposed approach incorporates a computationally efficient classifier-based conditional mutual information (CMI) estimator, capable of capturing intricate dependence structures among variables. To approximate a distribution encoding the null hypothesis, a \(k\)-nearest-neighbor local sampling strategy is employed. An important advantage of this approach is its ability to operate without assumptions about distribution forms or feature dependencies. Furthermore, it eliminates the need to derive asymptotic null distributions for the estimated CMI and avoids dataset splitting, making it particularly suitable for small datasets. The method presented in this article demonstrates asymptotic control of the type I error and consistency against all alternative hypotheses. Extensive analyses using both synthetic and real data highlight the computational efficiency of the proposed test. Moreover, it outperforms existing state-of-the-art methods in terms of type I and II errors, even in scenarios with high-dimensional conditioning sets. Additionally, the proposed approach exhibits robustness in the presence of heavy-tailed data.

## 1 Introduction

Testing for conditional independence (CI) is a crucial and challenging task in statistics and machine learning, with wide-ranging applications in graphical models , causal inference , and variable selection . The objective is to determine whether two random variables, \(X\) and \(Y\), are independent given a set of conditioning variables \(Z\), based on observations of the jointdistribution \(p_{X,Y,Z}(x,y,z)\). Specifically, the hypothesis to be tested is:

\[H_{0}:X\!\!\! Y|Z H_{1}:X\!\!\! Y|Z.\] (1)

However, CI testing becomes challenging due to the high-dimensionality of the conditioning variables \(Z\), and the limited availability of data samples [5; 35; 40; 1]. Existing tests may struggle to control the type I error, particularly when handling high-dimensional conditioning variable sets with complex dependency structures [6; 40]. Moreover, even when a test is valid, the limited data availability can make it exceedingly challenging to distinguish between null and alternative hypotheses, resulting in low testing power .

In this article, we present a novel conditional independence (CI) testing method based on a \(k\)-nearest-neighbor local sampling scheme. Our approach incorporates two essential components: a conditional mutual information (CMI) estimator utilizing classification techniques, and a \(k\)-nearest-neighbor local sampling strategy to approximate the conditional distribution \(p_{X|Z}(x|z)\). This approximation enables us to simulate data sets from a distribution that represents the null hypothesis, allowing us to estimate the CMI using these simulated data sets. By comparing the CMI estimator based on real data with those computed from the simulated data sets, we make informed decisions regarding the hypothesis testing. Theoretical analysis demonstrates that our proposed test achieves a valid control of the type I error asymptotically and exhibits consistency against all alternatives in \(H_{1}\). Synthetic and real data analyses showcase that our test outperforms previous methods in terms of both the type I error and power under \(H_{1}\), even when dealing with high-dimensional conditioning sets and/or small datasets. Furthermore, our approach remains computationally efficient as the dimension of the conditioning set and/or sample size increase. Additionally, our method is robust even in the presence of heavy-tailed data.

## 2 Related Work

### CMI estimation

In our work, we utilize CMI as a measure of conditional dependence. CMI offers a strong theoretical guarantee for CI testing, where \(I(X;Y|Z)=0 X\!\!\! Y|Z\). It has the ability to capture complex dependence structures among variables, even in non-linear scenarios. Estimators based on \(k\)-nearest neighbors and kernel methods have been widely employed for CMI estimation [27; 38; 23; 19]. However, these methods may encounter efficiency issues when dealing with high-dimensional conditioning variables, known as the curse of dimensionality [34; 32].  approached the CMI estimation problem by formulating it as a minimax optimization problem and proposed the use of generative adversarial networks (GANs) to optimize and obtain the CMI estimator, which can handle high-dimensional scenarios. However, the training of GANs is often challenging, with the risk of collapse if hyperparameters and regularizers are not carefully chosen . Recently, [34; 30] proposed a classifier-based CMI estimator capable of handling high dimensions. They initially developed an estimator for the Kullback-Leibler (KL) divergence using a classifier and then derived mutual information (MI) estimators based on it. CMI is defined as the difference between two MI values, and the CMI estimator is obtained by computing the difference between two MI estimators. In contrast, our method directly utilizes the classifier-based KL-divergence estimator to obtain the CMI estimator. This approach also handles high dimensions effectively and is computationally more efficient compared to the method employed by [34; 30].

### CI Testing

In recent years, a considerable body of literature on CI testing has emerged. Here, we provide a brief overview of some existing methods, and for a more comprehensive review, we refer readers to .

One important category of CI testing methods involves using different measures for CI [3; 46; 2]. For example,  proposed a CI testing method based on the empirical Hilbert-Schmidt norm of the conditional cross-covariance operator. Another approach, KCIT, was introduced by , which utilizes the partial association of reproducing kernel Hilbert spaces to measure conditional independence.  proposed a novel kernel-based CI testing method using the \(l_{p}\) distance between two well-chosen analytic kernel mean embeddings evaluated at a finite set of locations.  proposed the generalized covariance measure for CI testing based on a regression method. However, obtaining the exact distribution of the test statistic derived from the conditional independence measure under \(H_{0}\) can be challenging. Instead, researchers have employed alternative methods to derive asymptotic distributions of the test statistics under the null hypothesis [47; 46; 45; 40]. Nevertheless, the effectiveness of asymptotic distributions can be compromised when the sample size is small or the dimension of \(Z\) is high [16; 38]. As a result, tests based on asymptotic distributions may exhibit inflated type-I errors or inadequate power when dealing with small sample sizes or high-dimensional \(Z\)[47; 40]. In our work, we choose to utilize CMI as a metric for conditional dependence.

To overcome the reliance on asymptotic null distributions,  proposed the model-X framework, assuming knowledge of the conditional distribution \(p_{X|Z}(x|z)\). Under this assumption, a set of test statistics can be computed that are exchangeable under the null hypothesis, either through direct resampling  or permutation methods . However, in practice, the distribution of \(X|Z\) is rarely available. Therefore, accurately approximating the distribution of \(X|Z\) becomes crucial for maintaining the type I error control of these tests. To address this challenge,  developed the generative conditional independence test (GCIT) using Wasserstein GANs to approximate the distribution of \(X|Z\), while  proposed using Sinkhorn GANs for the same purpose. However, as highlighted in [5; 30], limited data and noise may lead to inaccurate learning of conditional distributions using neural networks, resulting in inflated type I errors for GCIT and the double GANs-based CI test (DGCIT). To mitigate this issue,  proposed using the 1-nearest neighbor method to generate samples from the approximated conditional distribution of \(X\) given \(Z\). However, their method requires splitting the dataset into two parts, with the testing dataset used for calculating the test statistics comprising only one-third of the total samples. This will reduce the statistical power of the test, particularly when working with small datasets.  utilized a Kozachenko-Leonenko estimator for CMI as the test statistic and proposed a permutation scheme based on \(k\)-nearest neighbors to generate samples under the null distribution. However, the curse of dimensionality adversely affects the performance of the Kozachenko-Leonenko CMI estimator, leading to poor performance when the conditioning variable set \(Z\) has high dimensions. Additionally, no theoretical guarantee is provided.  and  employed a binning strategy, discretizing \(Z\) into a finite number of bins based on the proximity of conditioning variables \(Z\), followed by the "permute-within-groups" strategy. However, selecting bins in high-dimensional settings presents a significant challenge , and their methods are limited to handling conditioning variables \(Z\) with very few dimensions.

In our work, we propose the utilization of a \(k\)-nearest-neighbor local sampling strategy as an appealing alternative to the binning strategy. Specifically, our strategy generates samples locally based on the \(k\)-nearest neighbors of the conditioning variables \(Z\). We demonstrate that the distribution of samples generated from this \(k\)-nearest-neighbor local sampling scheme closely approximates the true conditional distribution \(p_{X|Z}(x|z)\) in terms of total variation distance. One significant advantage of our proposed method is that it allows the entire dataset to be used for computing the testing statistics, eliminating the need for dataset splitting. This feature makes our method more effective compared to , which requires dataset splitting, particularly when dealing with small datasets. Moreover, our method does not require the derivation of asymptotic null distributions for the estimated CMI, and it can easily handle high-dimensional conditioning variables \(Z\). We provide theoretical and empirical evidence that our test achieves a valid control of the type I error and attains high power under the alternative hypothesis \(H_{1}\).

## 3 \(K\)-Nearest-Neighbor Local Sampling Based CI Testing

### Classifier-based CMI estimator

The CMI for a triplet of random variables/vectors \((X,Y,Z)\) is defined as:

\[I(X;Y|Z)= p_{X,Y,Z}(x,y,z)(x,y,z)}{p_{X,Z}(x,z)p_{Y|Z} (y|z)}dxdydz,\] (2)

where \(p_{X,Y,Z}(x,y,z)\) is the joint density of \((X,Y,Z)\), \(p_{X,Z}(x,z)\) is the joint density of \((X,Z)\), and \(p_{Y|Z}(y|z)\) is the conditional density of \(Y\) given \(Z=z\). One approach to estimate CMI is by directly estimating the joint and conditional densities from the available data and plugging them into (2). However, accurately estimating the density functions can be challenging, especially in high-dimensional settings where it is more difficult than directly estimating CMI (2) . CMI is a special case of the Kullback-Leibler (KL) divergence, and thus we have:

\[I(X;Y|Z)=D_{KL}(p_{X,Y,Z}(x,y,z)||p_{X,Z}(x,z)p_{Y|Z}(y|z)),\] (3)where \(D_{KL}(f||g)\) denotes the KL divergence between two distribution functions \(F\) and \(G\), with density functions \(f(x)\) and \(g(x)\), respectively. The Donsker-Varadhan (DV) representation of \(D_{KL}(f||g)\) is given by:

\[_{s S}[E_{w f}s(w)- E_{w g}\{s(w)\}],\] (4)

where the function class \(\) includes all functions with finite expectations. The optimal function in (4) is given by \(s^{*}(x)=\{f(x)/g(x)\}\), which leads to:

\[D_{KL}(f||g)=E_{w f}\{f(w)/g(w)\}-[E_{w g}\{f (w)/g(w)\}].\] (5)

Building upon the classifier-based CMI estimation method proposed by , we propose a CMI estimation method using the 1-nearest-neighbor (1-NN) sampling algorithm . The pseudocode for estimating CMI is outlined in Algorithm 2. The main objective is to empirically estimate (5) with \(f=p_{X,Y,Z}(x,y,z)\) and \(g=p_{X,Z}(x,z)p_{Y|Z}(y|z)\), which requires samples from both \(p_{X,Y,Z}(x,y,z)\) and \(p_{X,Z}(x,z)p_{Y|Z}(y|z)\). The available data only consists of samples from \(p_{X,Y,Z}(x,y,z)\). However, generating samples from \(p_{X,Z}(x,z)p_{Y|Z}(y|z)\) requires knowledge of the unknown conditional distribution \(p_{Y|Z}(y|z)\). To address this challenge, we propose utilizing the 1-NN sampling algorithm , which is outlined in Algorithm 1. Empirical and theoretical results presented in  indicate that the 1-NN sampling algorithm can accurately approximate the conditional distribution.

```
0: Datasets \(V_{1}\) and \(V_{2}\), both with sample size \(n\) and \(V=V_{1} V_{2}\) consisting of \(2n\) independently and identically distributed (i.i.d.) samples from \(p_{X,Y,Z}(x,y,z)\).
0: A new data set \(V^{}\) consists of \(n\) samples.
1: Let \(V^{}=\).
2:for\((X,Y,Z)\) in \(V_{2}\)do
3: Go to \(V_{1}\) to find the sample \((X^{},Y^{},Z^{})\) such that \(Z^{}\) is the 1-nearest neighbor of \(Z\) in terms of the \(l_{2}\) norm
4:\(V^{}=V^{}\{(X,Y^{},Z)\}\).
5:endfor
6:return\(V^{}\) ```

**Algorithm 1** 1-Nearest-Neighbor sampling **(1-NN(\(V_{1},V_{2},n\)))**

Next, we formalize the classifier-based CMI estimator. We consider a data set \(V\) consisting of \(2n\) i.i.d. samples \(\{W_{i}:=(X_{i},Y_{i},Z_{i})\}_{i=1}^{2n}\) with \((X_{i},Y_{i},Z_{i}) p_{X,Y,Z}(x,y,z)\). The data set \(V\) is divided into two equally sized parts \(V_{1}\) and \(V_{2}\), where \(|V_{1}|=|V_{2}|=n\). For data sets \(V_{1}\) and \(V_{2}\), we use the 1-NN sampling algorithm 1 to generate a new data set \(V^{}\) with \(n\) samples. We assign labels \(l=1\) for all samples in \(V_{2}\) and \(l=0\) for all samples in \(V^{}\). In this supervised classification task, a binary classifier can be trained using an advanced binary classification model, such as XGBoost  or deep neural networks . The classifier produces predicted probability \(_{m}=P(l=1|W_{m})\) for a given sample \(W_{m}\), leading to an estimator of the likelihood ratio on \(W_{m}\) given by \((W_{m})=_{m}/(1-_{m})\). It follows from (3) and (5) that an estimator of \(I(X;Y|Z)\) is given by

\[(X;Y|Z) :=_{KL}(p_{X,Y,Z}(x,y,z)||p_{X,Z}(x,z)p_{Y|Z}(y|z))\] \[=d^{-1}_{i=1}^{d}(W_{i}^{f})-\{d^{-1} _{j=1}^{d}(W_{j}^{g})\},\] (6)

where \(d= n/3\) with \( t\) being the largest integer not greater than \(t\), \(W_{i}^{f}\) is a sample in \(V_{f}^{}\) and \(W_{j}^{g}\) is a sample in \(V_{g}^{}\), where \(V_{f}^{}\) and \(V_{g}^{}\) are defined in Algorithm 2. According to Theorem 1 in , \((X;Y|Z)\) is a consistent estimator of \(I(X;Y|Z)\).

In contrast to the approach taken by , which estimate CMI by using the difference between two mutual informations (i.e., \(I(X;Y|Z)=I(X;Y,Z)-I(X;Z)\)) and therefore require training two binary classifiers, our method achieves CMI estimation with just a single binary classifier. This leads to significantly improved computational efficiency. As depicted in Algorithm 3, calculating a single \(p\)-value requires the computation of (\(B+1\)) CMIs. Hence, in practical applications involving real data analysis, especially when dealing with large sample sizes or high-dimensional conditional variables, efficient computation of the CMI becomes crucial.

### The Proposed CI Testing Procedure

While we have proposed a consistent estimator for CMI, accurately estimating it as zero under the null hypothesis, \(H_{0}\), is practically unattainable due to random errors in the sample data. These errors lead to deviations of the estimator from the actual value. To enhance the effectiveness of CI testing based on the CMI estimator, we propose a test that takes into account the statistical variation inherent in the CMI estimator.

Under \(H_{0}\), we can express the following representation:

\[p_{X|Y,Z}(x|y,z)=p_{X|Z}(x|z).\] (7)

The null model in (7) preserves the dependence between \(X\) and \(Z\), while breaking any dependence between \(X\) and \(Y\). Therefore, if a direct causal link exists between \(X\) and \(Y\), replacing \(X\) with a null sample \(X^{*} p_{X|Z}(x|z)\) would disrupt this relationship. Thus, we can conclude that \(X^{*}\) and \(Y\) are conditionally independent given \(Z\), i.e., \(I(X^{*};Y|Z)=0\).

Consider \(n\) i.i.d. copies \(=\{(x_{i},y_{i},z_{i}):i=1,,n\}\) of \((X,Y,Z)\). When the distribution of \(X|Z\) is known, conditional on \(=(z_{1},,z_{n})^{T}\), we can independently draw a pseudo sample \(x_{i}^{(b)} p_{X|Z}(x|z_{i})\) for each \(i\) across \(b=1,,B\) such that all \(^{(b)}:=(x_{1}^{(b)},,x_{n}^{(b)})^{T}\) are independent of \(:=(y_{1},,y_{n})^{T}\) and also \(:=(x_{1},,x_{n})^{T}\), where \(B\) is the number of repetitions. Under \(H_{0}\), \((^{(b)},,)}{{=}}(,, )\) for all \(b\), where \(}{{=}}\) denotes equality in distribution. We denote the CMI estimator of \(I(X;Y|Z)\) based on \((^{(b)},,)\) as \(}^{(b)}\) and denote the estimator based on \((,,)\) as \(}\). We can approximate the \(p\)-value by

\[p:=^{B}1(}^{(b)}}) }{1+B},\] (8)

where \(1()\) is the indicator function. To begin with, we demonstrate that the test based on (8) can effectively control the type I error. Specifically, under the null hypothesis \(H_{0}\), the \((B+1)\) triples \((,,),(^{(1)},,),,(^{(B)},,)\) are exchangeable, and thus the \(p\)-value is valid and satisfies \(P(p|H_{0})\) for any given \((0,1)\)[11; 8; 43]. Furthermore, it intuitively suggests that our test can achieve high power under the alternative hypothesis \(H_{1}\). As \(n\) approaches infinity, \(}^{(b)}\) converges to zero in probability. But under \(H_{1}\), we know that \(I(X;Y|Z)>0\). As a result, \(}\) should be positive with high probability. Consequently, the \(p\)-value calculated using (8) is very small with high probability, indicating the consistency of our test against all alternatives stated in \(H_{1}\).

However, \(p_{X|Z}(x|z)\) is rarely known in practice [5; 43]. We propose using the \(k\)-nearest-neighbor local sampling strategy to approximate \(p_{X|Z}(x|z)\). The approximated distribution is denoted as \(_{X|Z}(x|z)\). To generate pseudo samples from \(_{X|Z}(x|z)\), we follow the steps below:1. Obtain the set of indices of the \(k\)-nearest neighbors of \(z_{i}\) based on the \(l_{2}\) norm. Denote this set as \(_{i}=\{j\{1,...,n\}:\|z_{j}-z_{i}\|_{2} d_{i}^{k}\}\), where \(d_{i}^{k}\) is the distance of \(z_{i}\) to its \(k\)-nearest neighbor;
2. Shuffle \(_{i}\) and let \(j\) be the first element of \(_{i}\). Then, define the pseudo sample \(_{i}\) as \(x_{j}\);
3. Repeat the above two steps for all \(i\{1,2,,n\}\) to obtain the resultant pseudo sample vector \(}=(_{1},,_{n})^{T}\).

We establish in Section 4 that the total variation distance between the true distribution of \(X|Z\) and the distribution of samples generated by the \(k\)-nearest-neighbor local sampling strategy tends to zero in probability as \(n\) goes to infinity. This evidence suggests that the latter is a close approximation of the former. Thus, \((},,)\) approximates directly a distribution that encodes the null hypothesis. We then compute the CMI estimator based on it. This process is repeated \(B\) times, resulting in \(B\) realizations of the CMI estimator under the null hypothesis denoted by \((}^{(1)},,}^{(B)})\). We can determine whether to reject the null hypothesis by comparing these estimators to the one of the original sample set \(\). Specifically, we calculate the \(p\)-value using (8), but with \(}^{(b)}\) replaced by \(}^{(b)}\). If the resulting \(p\)-value is smaller than the prespecified significance level, we reject the null hypothesis. The pseudo code is presented in Algorithm 3. In Section 4, we will demonstrate through theoretical analysis that our test asymptotically achieves a valid control of type I error and is consistent against all the alternatives in \(H_{1}\).

```
0: Dataset \((,,)\) consisting of \(n\) i.i.d. samples from \(p_{X,Y,Z}(x,y,z)\).
0: The number of repetitions \(B\); the neighbor order \(k\); the significance level \(\).
0: Accept \(H_{0}:X\!\!\! Y|Z\) or \(H_{1}:X\!\!\! Y|Z\).
1: Use Algorithm 2 to obtain \(}\) based on \((,,)\).
2:for\(i\{1,2,...,n\}\)do
3: Obtain the set of indices of the \(k\)-nearest neighbor of \(z_{i}\): \(_{i}=\{j\{1,2,...,n\}:\|z_{j}-z_{i}\|_{2} d_{i}^{k}\}\), where \(d_{i}^{k}\) denotes the distance of \(z_{i}\) to its \(k\)-nearest neighbor.
4:endfor
5:\(b=1\).
6:while\(b B\)do
7: Initialize empty array \(}\) of length \(n\).
8:for\(i\{1,2,...,n\}\)do
9: Shuffle \(_{i}\)
10:\(j=_{i}\)
11:\(}[i]=[j]\).
12:endfor
13: Use Algorithm 2 to obtain \(}^{(b)}\) based on \((},,)\).
14:\(b=b+1\).
15:endwhile
16: Compute \(p\)-value: \(p:=[1+_{b=1}^{B}1}^{(b)}}}]/(1+B)\).
17:if\(p\)then
18: Accept \(H_{0}:X\!\!\! Y|Z\).
19:else
20: Accept \(H_{1}:X\!\!\! Y|Z\).
21:endif ```

**Algorithm 3**\(K\)-Nearest-neighbor local sampling based CI testing

[FIGURE:S3.F1][ENDFIGURTheoretical Results

In this section, we present our main theoretical results. All the detailed proofs for these results can be found in the Supplementary Materials. Let \(P_{1}\) and \(P_{2}\) be two probability distributions defined on the same probability space. The total variation distance between \(P_{1}\) and \(P_{2}\) is defined as \(d_{TV}(P_{1},P_{2})=_{A}|P_{1}(A)-P_{2}(A)|\), where the supremum is taken over all measurable subsets \(A\) of the sample space \(\). We will show in Theorem 2 that the distribution of \(\) generated by the \(k\)-nearest-neighbor local sampling strategy is very close to the true conditional distribution in terms of the total variation distance.

Lemma 1 plays a crucial role in proving Theorem 2. It demonstrates that the \(k\)-nearest neighbor of \(Z\) has a similar property to the work of , where it has been shown that the nearest neighbor of \(Z\) converges almost surely (a.s.) to \(Z\) as the sample size \(n\) approaches infinity.

**Lemma 1**.: _Let \(Z\) and \(Z_{1},,Z_{n}\) be i.i.d. random vectors from \(p(z)\). For a given positive integer \(k\), let \(Z_{n}^{(k)}\) be the \(k\)-nearest neighbor of \(Z\) from the set \(\{Z_{1},,Z_{n}\}\). Then, \(Z_{n}^{(k)}}{{}}Z\) as \(n\)._

We make the following regularity conditions, which have been introduced in ,  and . To simplify the notation, we may drop the subscripts when referring to probability density functions.

**Assumption 1** (Smoothness on \(p(x|z)\)).: For any \(z^{d_{z}}\) and any \(a\) such that \(\|a-z\|_{2}\), we assume \(0_{}(I_{a}(z))\), where \(d_{z}\) is the dimension of the random vector \(Z\), \(\|\|_{2}\) denotes the \(l_{2}\) norm, \(0<<\), and \(I_{a}(z)\), called the generalized curvature matrix, is defined with entries

\[I_{a}(z)_{ij}=E(- p(X|Z=)}{ _{i}_{j}}_{=a}Z =z)=(}{_{i} _{j}})}p(x|z)dx) _{=a}.\]

**Assumption 2** (Smoothness on \(p(z)\)).: Assume that the probability density function \(p(z)\) is twice continuously differentiable. Let \(H_{p}(z)\) denote the Hessian matrix of \(p(z)\) with respect to \(z\). We assume that \(\|H_{p}(z)\|_{2} c_{d_{z}}\) holds almost everywhere, where \(c_{d_{z}}\) depends only on the dimension \(d_{z}\) of the random vector \(Z\).

Note that Assumption 1 can be viewed as an extension of the requirement that the maximum eigenvalue of the Fisher information matrix for \(x\) with respect to \(z\) is bounded. Assumptions 1 and 2 can be validated when \((X,Z)\) follows a multivariate Gaussian distribution (MVD) and when \(Z\) follows a MVD, respectively.

For \(Z\), we define \(Z_{n}^{(l)}\) as its \(l\)-nearest neighbor for \(l=1,...,k\). According to Algorithm 3, each \(Z_{n}^{(l)}\) is selected with probability \(1/k\). Let \(\) be a random variable with a probability mass function \(P(=l)=1/k\) for \(l=1,...,k\), and \(\) is independent of both \(Y\) and \(Z\). Given \(Z\), we denote by \(\) the sample generated by the \(k\)-nearest-neighbor local sampling mechanism, which follows the distribution \((x|Z)=p(x|Z_{n}^{(l)})^{1\{=1\}} p(x|Z_{n}^{( k)})^{1\{=k\}}\). The sketch proof of Theorem 2 proceeds as follows. We first apply Pinsker's inequality, which relates the total variation distance between \(p(x|Z)\) and \((x|Z)\) to their KL divergence. We then establish a connection between the KL divergence and the discrepancy between \(Z\) and \(Z_{n}^{(l)}\). Finally, by Lemma 1, we derive Theorem 2.

**Theorem 2**.: _Under Assumptions 1 and 2, we have \(d_{TV}\{p(x|Z),(x|Z)\}=o_{p}(1)\) as \(n\)._

**Remark 1**.: _The constant \(\) in Assumption 1 is used to establish an upper bound on the KL divergence between \(p(x|Z)\) and \(p(x|Z_{n}^{(l)})\), which can be used to bound \(d_{TV}\{p(x|Z),(x|Z)\}\) through Pinsker's inequality._

In Theorem 3, we bound the excess type I error conditionally on \(\) and \(\) by the total variation distance between \((|)\) and \(p(|)\).

**Theorem 3**.: _Assume \(H_{0}:X\!\!\! Y|Z\) is true. Under Assumptions 1 and 2, for any significance level \((0,1)\), the \(p\)-value obtained from Algorithm 3 satisfies \(P(p|,)+d_{TV}\{p(|),( |)\}\)._

The type I error rate can be unconditionally controlled based on Theorems 2 and 3, which implies that \(P(p|H_{0})+o(1)\) as \(n\) approaches infinity. Now we turn to analyze the power of our test in asymptotic scenarios in Theorem 4. Some intuitive explanations on both type I error rate and power can be found in the third paragraph of Section 3.2.

**Theorem 4**.: _For any \((0,1)\) and the number of repetitions \(B_{n}\) satisfying \(B_{n}\), let \(p\) be the \(p\)-value calculated in Algorithm 3. Under Assumptions 1 and 2 and the Assumptions stated in the Supplementary Materials, \(P(p|H_{1}) 1\) as \(n\)._

**Remark 2**.: _The condition \(B_{n}\) is mild and has been made in [7; 43]. The consistency of our test heavily relies on the consistency of the CMI estimator, which hinges on whether the joint density of \((X,Y^{},Z)\) generated by the 1-NN sampling (Algorithm 1), denoted as \(\), approximates \(p(x,z)p(y|z)\) (denoted as \(g\)) well, in terms of TV distance. \(\) and \(c_{d_{x}}\) in Assumptions 1 and 2 are used to bound \(d_{TV}(,g)\)._

**Remark 3**.: _According to Theorem 4, our test can achieve consistency against all the alternatives stated in \(H_{1}\). In contrast, the method proposed by  only achieves consistency against a subset of the alternatives in \(H_{1}\), and  does not provide any theoretical results on the testing power._

## 5 Empirical Results

In this section, we present a comparative evaluation of our proposed method against the state-of-the-art (SOTA) methods on synthetic datasets. Our code is publicly available at: https://github.com/LeeShuai-kenwitch/NNLSCIT. We specifically compare our method with six commonly used competitive CI testing methods: KCIT , LPCIT, CCIT , CMIknn , GCIT , and NNSCIT . We set the number of repetitions \(B=200\) and the neighbor order \(k=7\) for our tests. The XGBoost classifier was used in all of our experiments. Further elaboration on the choice of \(k\) is given in Figure 3 in the Supplementary Materials. We set the significance level to \(=0.05\) and report the type I error rate and the testing power under \(H_{1}\) for all methods evaluated in our experiments. All the results are presented as an average over \(200\) independent trials. We provide additional simulation studies and real data analysis in the Supplementary Materials.

**Scenario I: the post-nonlinear model.** The datasets used in our experiments are generated using the post-nonlinear model similar to those in [47; 16; 5; 40]. Specifically, we define \((X,Y,Z)\) under \(H_{0}\) and \(H_{1}\) as follows:

\[H_{0}:X =f_{1}(+0.5_{x}),\;Y=f_{2}(+0.5 _{y}),\] \[H_{1}:X =f_{1}(+0.5_{x})+0.5_{b},Y=f_{2}( +0.5_{y})+0.5_{b},\] (9)

where \(\) represents the sample mean of \(Z=(z_{1},,z_{d_{x}})\), all \(z_{l}\) in \(Z\), \(_{x}\), \(_{y}\) and \(_{b}\) are i.i.d. samples generated from the standard Gaussian or the standard Laplace distribution, and \(f_{1}\) and \(f_{2}\) are randomly sampled from the set \(\{x,x^{2},x^{3},(x),(x)\}\).

We conduct a comparative analysis of various tests from two perspectives. The results are shown in Figure 1. First, we fix the sample size at \(n=500\) and vary the dimension of \(Z\) from \(30\) to \(100\). In both the Gaussian and Laplace cases, our test shows good and stable performance in controlling type I error and achieving high power under \(H_{1}\) as the dimension of \(Z\) increases. LPCIT and NNSCIT have satisfactory performance in controlling type I error, but LPCIT loses power under \(H_{1}\) when the dimension exceeds \(70\), and NNSCIT has inadequate power for all dimensions. Although GCIT, CCIT, and KCIT have adequate power under \(H_{1}\), they inflate the type I error in almost all scenarios. CMIknn shows weak performance on both type I error and testing power. Figure 6 in the Supplementary Materials contains additional results on the performance of various tests in low dimensions of \(Z\) ranging from 5 to 30, further demonstrating the superiority of our test. Second, we vary the sample size from \(300\) to \(2000\) while fixing the dimension of \(Z\) at \(80\). Our test maintains good control of the type I error and achieves high power, with the power approaching \(1\) when the sample size exceeds \(500\). However, LPCIT, NNSCIT, and CMIknn lose power under \(H_{1}\), while GCIT, CCIT, and KCIT either always or sometimes fail to control the type I error well.

In the Supplementary Materials, Figure 7 shows the timing performance of all methods for a single test. Our test is found to be highly computationally efficient even when dealing with large sample sizes and high-dimensional conditioning sets. In contrast, CMIknn and CCIT for sample sizes exceeding \(1000\), and LPCIT for dimension of \(Z\) higher than \(50\) are impractical due to their prohibitively long running time.

**Scenario II: the heavy tailed model.** We compare the performance of our test with the SOTA tests under the heavy tailed error distributions, as described in . The data \((X,Y,Z)\) is generatedaccording to the following model:

\[H_{0}:X=+_{1},\;Y=+_{2},\] \[H_{1}:X=+_{1},Y=+_{1}+ _{2},\] (10)

where \(\) is the sample mean of \(Z=(z_{1},,z_{d_{z}})\), \(z_{l}\)'s are i.i.d. samples generated from the standard Gaussian distribution, and \(_{1}\) and \(_{2}\) are independently generated from the standard Cauchy distribution. We keep the sample size fixed at \(n=500\).

Figure 1: Comparison of the type I error (lower is better) and power (higher is better) of our method with six SOTA methods on the post-nonlinear model under Gaussian or Laplace distributions in Scenario I. **Left:** The results when varying the dimension of \(Z\). **Right:** The results when varying the sample size.

**Scenario III: the chain structure.** We further use the chain structure \(Y Z X\) in  to validate the proposed test. The detailed procedure for generating the simulated data can be found in the Supplementary Materials. The sample size is fixed at \(n=500\).

Figure 2 demonstrates that our test effectively controls the type I error and obtains adequate power in both Scenarios II and III under various dimensions of \(Z\).

## 6 Conclusion

In this paper, we introduce a novel method for conducting the conditional independence testing. Theoretical analysis shows that our test is asymptotically valid and consistent against all alternatives in \(H_{1}\). Extensive experiments on both synthetic and real datasets demonstrate that our method consistently outperforms commonly used SOTA methods. Moreover, our approach maintains computational efficiency even when the sample size and/or dimension of the conditioning set increase. Our method has the potential to enhance the applicability of causal discovery to real-world problems, such as gene regulatory networks or complex social networks, and to facilitate the discovery of relationships and patterns in complex systems. Ethically, we believe that our rather fundamental work has minimal potential for misuse.