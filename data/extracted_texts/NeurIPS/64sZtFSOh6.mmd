# ClevrSkills: Compositional Language and Visual Reasoning in Robotics

Sanjay Haresh

Qualcomm AI Research

sanjayh@qti.qualcomm.com

&Daniel Dijkman

Qualcomm AI Research

ddijkman@qti.qualcomm.com

&Apratim Bhattacharyya

Qualcomm AI Research

aprabhat@qti.qualcomm.com

&Roland Memisevic

Qualcomm AI Research

rmemisevic@qti.qualcomm.com

Qualcomm AI Research

Qualcomm AI Research

###### Abstract

Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2  simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.

## 1 Introduction

Compositional generalization is a hallmark feature of human intelligence. Unlike any other animals, humans can receive instructions in natural language and successfully perform previously unseen tasks with minimal to no task-specific learning or adaptation. Modeling this capability has been a long-standing aspiration in AI, dating back at least to Winograd's influential SHRDLU system  developed more than half a century ago. The architectural underpinnings that enable these capabilities in humans have remained an inspiration as well as puzzle until this day .

A potential steppingstone towards replicating this ability in AI systems is the recent progress in language modeling, based on large models pre-trained using next-token-prediction. These models have shown encouraging compositional reasoning behaviors in response to language-based prompts - an ability that was confined initially to text-based tasks, but that since has been extended to multi-modal, and most recently also to robotics tasks.

Compositional reasoning based on language has evolved hand-in-hand with the introduction of benchmark tasks and challenges. For language-based reasoning tasks, these include, for example, the bAbI AI challenge , GSM8k , and many others. Multi-modal tasks include the popular CLEVR challenge  and its descendants (eg., ), and various intuitive physics datasets (eg., ). Common to these challenges is that they require a model to reason about a scene or situation.

Despite their reliance on some degree of "common sense", these existing challenges do not require any type of actions, behaviors or planning. As such, they are confined to evaluating compositionality in a purely abstract setting, even in the case where the input data is multi-modal. In this work, we propose an environment and corresponding suite of tasks, which instead allow us to study compositional generalization in a highly controlled, but complex robotics context. Our benchmark is based on dexterous manipulation tasks, such as pick, place, throw, touch and push within the ManiSkill2 simulation environment , and it evaluates the ability to generalize to complex tasks based on these low-level capabilities.

Our benchmark allows us to assess a model's capability to perform compositional generalization with respect to the creation and execution of step-by-step execution plans. However, unlike existing benchmarks, such as Vima , our benchmark includes not just the higher-level planning but also the low-level execution layers for a wide variety of end-to-end robotics tasks. This allows us to assess not just a model's ability to perform abstract planning in isolation but a model's ability to plan-and-execute within a closed loop.

Our contributions in detail are as follows:

* We introduce the ClevrSkills2 environment suite, consisting of 33 different tasks spread across 3 different levels which can be used to benchmark compositional reasoning in robotics models. * We introduce an accompanying dataset of 330k ground truth trajectories generated by scripted oracle policies which use motion planning to achieve the tasks that can be used for imitation learning. The dataset also contains many types of annotation, including language, action classes, bounding boxes for objects, visibility annotations, key-steps, rewards (for offline RL), camera parameters and more.
* We benchmark SOTA open-source vision language models and show that they tend to fail on tasks requiring compositional understanding.

## 2 Related Work

### Vision language models for robotics

Large vision language models, including LLaVA  and others, have shown strong zero-shot and few-shot generalization across a wide range of tasks. Unsurprisingly, there has been an increasing effort to get similar results in robotics. For example, CLIPort , Perceiver-Actor , or RT-1  introduce large transformer models for a range of robotics tasks. RT-2  takes this further by co-finetuning a language model on both internet scale text data and large scale robotics data. GATO  and JAT  similarly train a transformer based model that can work across many different tasks and modalities. Octo  is another recent work that proposes a transformer based generalist policy that can be finetuned on downstream tasks. RoboFlamingo  is based on finetuning off-the shelf VLMs on robotics data to show that they are effective at imitation learning. Furthermore, above-mentioned Vima  benchmarks the capability of these models to generalize in highly controlled robotics tasks. Our work is similar in spirit, but goes beyond it in that it tests the ability of these models to generalize to not only new objects/textures/scenes as in Vima but also to totally new tasks given a base set of skills that are sufficient to complete the higher levels tasks.

### Simulators/Benchmarks

There has been a host of simulators introduced in recent years studying various aspects of robot learning. This includes, for example, iGibson , Habitat2.0 , Ai2THOR , Behavior1k , which all support indoor environments with tasks ranging from visual goal navigation, to mobile manipulation, to re-arrangement, etc. ManiSkill2 , ManiPose , DexArt  all introduce different manipulation benchmarks. Meta-world  describes a benchmark for meta-reinforcement learning in table-top environments. CALVIN  and Arnold  present language-conditioned long-horizon table-top tasks that require skill chaining for success.

Our benchmark is similar in spirit to the Vima benchmark , with several important differences. Vima is, to the best of our knowledge, the first robotics benchmark supporting multi-modal task specifications and a controlled probing of agent capabilities. However, the benchmark is limited in that the action space is composed of object poses instead of pose deltas of the robot end-effector and consequently there is no support for assessing compositionality of the agent given a base skill set. We overcome this limitation in ClevrSkills to enable benchmarking of compositional reasoning.

Most of the existing simulators and benchmarks either focus on just the manipulation skills (e.g. ManiSkill2 ), or they abstract the manipulation skills away using oracle policies and only test a model's ability to use the oracle policies to achieve complex tasks (e.g. Vima ). The goal of ClevrSkills is to combine the best of both worlds and to enable training and benchmarking an agent's ability to acquire manipulation skills and to compose them in novel ways to solve higher-level tasks.

## 3 ClevrSkills Environment Suite

ClevrSkills is built within the ManiSkill2 simulator, which allows for realistic physics and graphics. We use a simulated model of the UFACTORY xArm 6 robot with vacuum gripper as our default robot for the environments, with Franka Emika Panda also being available.

Figure 1: The ClevrSkills environment suite includes support for multi-modal prompts as task specification, multi-camera RGB observations, dense hierarchical action labels, action demonstrations in end-effector space and support for RL with dense rewards for all the tasks.

e add support for multi-modal prompts for task specification, add language annotations for the actions of the robot policy, extend the objects and texture databases and add a multitude of tasks that require increasingly higher levels of compositional understanding. A snapshot of the "Sort" task along with the observation and action space, task specification and action labels is shown as an example in Figure 1. A comparison to other simulator and datasets can be seen in Table 1.

### Task Suite

We develop 33 different tasks carefully designed to test compositional generalization of robotics models in a highly controlled setting. Our task set includes simple manipulation tasks (e.g. _moving from A to B, pick, place, push, tracing path_) which allow the model to learn basic manipulation/motor skills, intermediate tasks (e.g. _sorting objects by texture_, stacking_), which test model's ability to compose the manipulation skills learned from simple tasks, and finally complex tasks (e.g. _stacking and toppling structures_, sorting by throwing_, _balancing scales with weights_), which require higher level compositional reasoning. For example, the model needs to make use of the _throwing_ skill learned from the first set of tasks and the _sorting by texture_ capability learned from the second set and then compose these two skills to successfully solve the _sorting by throwing_ task. This design of compositional tasks is shown in Figure 2. We provide specific details of these levels in Section 4.

### Predicates

The reward and success criteria for the individual tasks are specified using _predicates_. There are two main types of predicates: physical and logical.

Physical predicates specify the target state of the robot and/or the objects in the scene, and how the agent achieves these states. _EEAtPos_ and _EEAtPose_ require the end-effector to be at a specified position or pose (within some specified tolerance). _AtPos_ and _AtPose_ require an object to be at a specified position or pose. _OnTop_ and _Inside_ require an object to be on top or inside another object. _Touch_ requires the agent to touch or push an object. _Hit_ requires the agent to drop or topple an object onto another object. _ToppleStructure_ requires a collection of objects to be on the ground.

Logical predicates can be used to combine physical predicates to specify more complex tasks. The logical predicates are _Set_ (all sub-predicates must be completed in any order), _Sequence_ (sub-predicates must be completed in order), and _Once_ (the sub-predicate must be completed once).

The dense reward of physical predicates is designed to allow RL agents to learn tasks. See the plot of the (instantaneous) dense reward shown in Figure 1 for an example. Logical predicates aggregate the rewards of their sub-predicates as appropriate. Note that the decomposition of tasks into predicates allows ClevrSkills to be easily extendable as new tasks can be easily specified as compositions of these predicates.

### Oracle policies

We develop oracle policies for all the tasks in our environment suite. These policies are called _solvers_ as they are designed to solve the predicates that define the tasks. The top-level solver algorithm performs a greedy search for the next predicate to solve, and instantiates a solver policy for the same.

Figure 2: Example task compositions in ClevrSkills. Higher level tasks in ClevrSkills are built on skills acquired from lower level tasks (L0 \(\) L1 \(\) L2).

The mapping from predicate to a specific solver is scripted manually. The available solvers are _Pick_, _Place_, _Move_, _Trace_, _Touch_, _Push_, _Hit_ (throw object towards other object), _ToppleStructure_, _BalanceScale_ (place objects on a scale to balance it). The _Move_ solver internally uses the MPLib  motion planning library, which is a Python wrapper around the implementation of RRT algorithm  found in OMPL .

Higher-level solvers internally use other solvers. For example, the _PickMovePlace_ solver internally uses _Move_ to get the end-effector close to a position where it can pick the object, _Pick_ to pick up the object, _Move_ to carry the object close to the target, and _Place_ to place the object. Solvers will typically let their sub-solvers take actions in the environment until the sub-solver reports that it has completed the action or has failed.

Because the solver policies are stateless, they can be combined with other policies. E.g., one can start collecting oracle solver trajectories from states that were reached by an RL agent (e.g., to perform fine-tuning using an approach like DAgger )

### Observations and action space

Since we extend ManiSkill2, we inherit its flexible observation and action space. However, for the purposes of this benchmark, we constrain the observation space to be RGB images from two cameras: an end-effector mounted camera which gives the robot's "first-person" view of the scene, and a base camera which provides a "third-person" perspective. The action space is restricted to the _delta end-effector pose_ controller from ManiSkill2, which provides for a 6DOF pose delta and 1D gripper scalar value as shown in Figure 1. Note that delta end-effector action demonstrations are convertible to any other controller type supported by ManiSkill2.

### Annotations

The success of recent large vision-language models is largely due to the availability of large amounts of paired vision and language data. However, such data is lacking in the case of robotics. Therefore, we also provide fine-grained language annotations for each step the oracle policy takes to complete any task. We provide three levels of language annotations including task or predicate level (the highest level describing the task, which can also be used as the task specification), sub-task level (a sub-task on a semantic level that needs to be achieved for the high level task to be completed), and step level (a language label for each step that is being taken). The hierarchy of language annotations can be seen in Figure 1 (middle).

We also provide bounding boxes and visibility labels for each object at each time-step of the generated trajectories as seen in Figure 1 (top-right), as well as key-step frames corresponding to the completion of sub-tasks (Figure 1 bottom section).

### Dataset

We generate 10k trajectories for each task using the corresponding oracle policy, resulting in a total of \(\)330k trajectories. We split the set of objects and textures into train and test splits to test the OOD generalization of models to unseen objects and textures. Each of our tasks is used both in training and testing according to the evaluation protocol described in Section 4. The dataset is available at https://www.qualcomm.com/developer/software/clevrskills-dataset.

## 4 Benchmark

Our environment suite consists of 33 tasks across three levels of difficulty:

* **L0: Simple Tasks.** 12 tasks that teach the agent a base set of motor skills like pick, place, throw, touch, push which can then be used to perform more complicated tasks.
* **L1: Intermediate Tasks.** 15 tasks that test the agent's ability to compose the skills learned from the simple tasks to perform simple compositions, such as sorting objects, stacking, swapping, rotating etc.
* **L2: Complex Tasks.** 6 tasks that require long-range compositional understanding which test the models ability to compose skills learned from both the Simple and Intermediatesubsets to achieve more complicated goals, such as balancing a scale with weights, sorting by throwing, swapping by pushing, etc.

The increasing complexity of these tasks can be seen in Figure 3. A full list of tasks along with their specification and success criteria can be found in the Appendix A.

**Task Specification.** We follow Vima  to support multi-modal prompts (interleaved text and images) as task specification as well as with text-only prompts.

**Evaluation.** Our main goal with this benchmark is to test the ability of vision language models to compose simple motor skills in novels ways to perform more complex tasks, both zero-shot and using fine-tuning. We use a three level protocol to systematically test the compositional abilities of the models. At each level, we further evaluate the models on seen and unseen attributes (objects, textures and object placements). The environment also provides partial rewards at each step along with binary success criteria. We report both success rate and average reward achieved for each task.

1. L0: Here, we test the model's ability to pick the base motor skills required to solve higher level tasks. All the prompts are seen at training time.
2. L0 \(\) L1: Here, we test the model's ability to compose skills from L0 tasks to achieve L1 tasks, both zero-shot and using fine-tuning.
3. L0, L1 \(\) L2: Here, we test the models' ability to compose skills from L0 and L1 tasks and perform higher level L2 tasks zero-shot and using fine-tuning.

## 5 Experiments

### Baselines

For baselines, we evaluate open-source vision language policies that can take multi-modal prompts as inputs. We experiment with three different architectures: JAT  and Octo , which accept image

  
**Dataset/Simulator** & **\#Tasks** & **Language** & **Multimodal Prompts** & **Action Granularity** & **Compositionality** & **\#Demonstrations** \\   \\  RoboTurk  & 3 & \(\) & \(\) & Action Deltas & \(\) & 111hrs \\ BridgeData  & 71 & \(\) & \(\) & Action Deltas & \(\) & 7.2k \\ Open-X  & - & \(\) & \(\) & Action Deltas & \(\) & 1M \\ RI20T  & - & \(\) & \(\) & Action Deltas & \(\) & 100k \\ FMB  & 7 & \(\) & \(\) & Action Deltas & \(\) & 22.5k \\    \\  CALVIN  & 34 & \(\) & \(\) & Action Deltas & \(\)\(\) & – \\ Behaviour-IK  & 1000 & \(\) & \(\) & Action Deltas & \(\) & – \\ Maniskil2  & 20 & \(\) & \(\) & Action Deltas & \(\) & \(\)70k \\ VIMA  & 17 & \(\) & \(\) & Poses & \(\) & 650k \\ CleurSkills (our) & 33 & \(\) & \(\) & Action Deltas + Poses & \(\) & 330k \\   

Table 1: Comparison of datasets/simulators. \(\) Compositionality in CALVIN mainly refers to stitching of sub-tasks to achieve long horizon tasks.

Figure 3: _Left_: The median length of an episode across task levels showing significant increase in episode length as we go from lower to higher levels of compositionality. _Right_: The mean number of solvers used by the oracle to complete a task across task levels. Each solver solves for a specific sub-task, showing higher levels have increasingly compositional tasks.

tokens in the context window of a transformer model, RoboFlamingo , which uses cross attention to condition on the image embeddings generated from a vision encoder, and our own StreamRoboLM, which is based on the LRR model  that continuously ingests video input during auto-regressive token generation.

**JAT.** The Jack of All Trades (JAT)  model is an open-source generalist agent trained on a range of reinforcement learning and language and vision tasks. While JAT is trained on a large number of language only, vision-language and RL tasks, it can only perform one task at a time i.e., it can either model language or take image inputs to produce actions for RL tasks which means that none of the RL tasks can be specified using language. We modify JAT by simultaneously feeding text and image tokens so that the RL tasks can be conditioned on multi-modal prompts. We initialize from the pre-trained JAT model and fine-tune it on ClevrSkills tasks.

**Octo.** Octo is another open-source generalist policy trained on Open X-embodiment dataset . The architecture is very similar to JAT with a transformer backbone and readout heads. The model is trained using a diffusion objective. The Octo architecture is geared towards enabling finetuning on tasks with different observation and action spaces. We leverage this to add additional observations for the "first-person" camera and prompt images used in the multimodal prompts. We refer the reader to the Octo  paper for further details on the model. We initialize from the pre-trained Octo model and fine-tune it on ClevrSkills tasks.

**RoboFlamingo.** RoboFlamingo  takes open-source VLMs and augments them with an additional LSTM  based policy head. The base VLM takes language and image inputs and produces an embedding that is then passed to the policy head, which in turn produces the next action. Since the base VLM is frozen, it basically acts as a "prompt-processor" which specifies the task to be performed by the LSTM policy. The model is trained on CALVIN dataset achieving good performance on the long-horizon tasks benchmark. We take the pre-trained RoboFlamingo model and further fine-tune it on ClevrSkills tasks.

**StreamRoboLM.** Different from RoboFlamingo, where the VLM can only reason over a single image at a time, we adapt an LRR  based model that can auto-regressively take videos as input. Inspired by RoboFlamingo, we also attach an LSTM based policy head that takes token embeddings from the language model as input and produces the next action. Concretely, we use OPT1B  or Llama3.2 3B  as the base language model and a ViT  as the vision encoder for the input images. The cross attention layers to condition on images and the LSTM based policy head are randomly initialized. To retain the language capabilities of the model, we use LoRA  to fine-tune

Figure 4: The StreamRoboLM model in contrast to state of the art models, _e.g._, RoboFlamingo (_c.f._, Fig. 1 in ), can auto-regressively process videos as input, which helps for success in long-horizon tasks of ClevrSkills.

the language model while training all the parameters of the vision encoder and the policy head. The architecture diagram can be seen in Figure 4. Further details of the architectures are described in Appendix C.

### Training and evaluation details

We use the open-source implementations for JAT, Octo and RoboFlamingo to evaluate the models on ClevrSkills. We initialize both the models from released checkpoints and fine-tune them on the ClevrSkills data on each task level separately. For StreamRoboLM, we start with OPT1B/Llama3.2 3B weights for the base LLM and ViT trained on ImageNet for the vision encoder. We initialize the cross-attention layers and the LSTM based policy head from scratch. We use LoRA  while fine-tuning the LLM to retain the learned language capabilities while training all the parameters of the other modules. All the models were evaluated on 20 different seeds not seen at training time for each task in all the task levels. All the experiments were carried out on 4 NVIDIA A100 GPUs. Please refer to the Appendix D for further details on training/evaluation hyperparameters.

### Results on L0 tasks.

Results on the L0 tasks can be seen in Table 2. It shows that all the models including JAT, RoboFlamingo, Octo and StreamRoboLM are able to perform some but not all L0 tasks. While the first three models only achieve success rates of \(23.75\%\), \(35.41\%\) and \(34.16\%\), the StreamRoboLM (both, the OPT and Llama version) shows a decent performance overall of \(62.91\%\) and \(62.5\%\) success rate. We also show the task-wise success rate on L0 tasks in Figure 5. As we can see, some of the tasks such as rotate, push, place and pick are particularly difficult for all the models. Note that these tasks still require visual understanding of the scene and objects as the model needs to select the correct object to manipulate in the correct manner. The poor results can also be partially attributed to the strictness of the success criteria, since the models tend to obtain a decent average reward. We also show results of these models on unseen objects and textures to test their

    &  &  \\ 
**Model** & **Success** & **Avg. Reward** & **Reward/Step** & **Success** & **Avg. Reward** & **Reward/Step** \\  Oracle & 100.0 & 320.00 & 3.06 & 100.0 & 175.83 & 3.06 \\ JAT  & 23.75 & 262.92 & 2.29 & 24.16 & 276.86 & 2.42 \\ RoboFlamingo  & 35.41 & 341.61 & 2.53 & 27.91 & 175.07 & 1.78 \\ Octo  & 34.16 & 207.90 & 1.89 & 26.25 & 123.85 & 1.77 \\ StreamRoboLM (Opt) & 62.91 & 223.84 & 2.74 & 41.66 & 329.69 & 2.87 \\ StreamRoboLM (Llama3) & 62.50 & 198.94 & 2.65 & 55.41 & 215.52 & 2.65 \\   

Table 2: Evaluation on L0 tasks.

Figure 5: Per task success rate on L0 tasks.

generalization capabilities on the same task with different objects and textures. As we can see, all models struggle to achieve similar performance as on seen object and textures, which shows that there is room for improvement. Interestingly, JAT achieves similar performance on both seen and unseen objects and textures. This may be attributed to the smaller vision backbone which avoids overfitting.

**Zero-shot generalization to L1 and L2 tasks.** Zero-shot generalization results on L1 and L2 tasks are shown in Table 3. We train the models on L0 tasks and evaluate zero-shot on L1 and then fine-tune the models on L1 tasks and zero-shot evaluate resulting models on L2 tasks. This is the hardest instantiation of our benchmark as the tasks are not seen at training time and the model needs to compose the skills learned from training on L0 and L1 tasks to solve L1 and L2 tasks respectively. Unsurprisingly, we see that all the models struggle to generalize to these tasks in the zero-shot setting and only Octo  achieves non-zero success rate on any of the tasks. This can also be attributed to the significant complexity both in terms of length of episodes and compositional complexity of the tasks (see Fig. 3) in comparison to L0 tasks.

**Fine-tuning on L1 and L2 tasks.** We also test these models by fine-tuning on L1 and L2 tasks. We found that despite training L1 and L2 tasks, these models struggle on both levels. As we can see in Figure 3, all these tasks are significantly longer than L0 tasks and require multiple successful executions of L0 skills (\(\) 9 on average for L1 and \(\) 11 on average for L2 tasks) for successful completion. As we also show in Figure 2, the L1 and L2 tasks require more than simple stitching of L0 skills.

Overall, the results show that even with reasonable performance on the L0 base skills, it is hard for current models to generalize to more complex tasks composed of these skills.

Since ClevrSkills supports both multi-modal and language-only prompts, we also include experiments on language-only task specifications, which can be seen in Appendix F.

## 6 Limitations and Future Work

The main limitation is that our benchmark is fully simulated and building a real-world counterpart is the obvious future work. Another direction for future work is the inclusion of more abstract and free-form tasks, such as playing tic-tac-toe, building structures like pyramids, houses, etc., aimed at evaluating long-range reasoning in robotics models, as well as the addition of multiple different embodiments (e.g. two-fingered grippers, dexterous grippers, or bi-manual robots).

## 7 Conclusion

We present a benchmark for evaluating compositional understanding in robotics. To this end, we develop 33 tasks spread across 3 levels of compositional understanding. We benchmark state-of-art robotics models based on large vision language models (VLMs) and show that even after being trained on large amounts of internet and robotics data, these VLMs are unable to show good compositional generalization to new tasks. Overall, these results show that despite recent progress in both, VLMs and robotics, further research will be required for models to show compositional generalization capabilities in robotics.

    &  &  \\   &  &  &  &  \\ 
**Model** & **Suc.** & **AR** & **R/S** & **Suc.** & **AR** & **R/S** & **Suc.** & **AR** & **R/S** & **Suc.** & **AR** & **R/S** \\  Oracle & 100.0 & 1027 & 5.59 & 100.0 & 1027 & 5.59 & 100.0 & 2583 & 9.12 & 100.0 & 2583 & 9.12 \\ JAT  & 0.0 & 199 & 0.87 & 0.60 & 375 & 1.08 & 0.83 & 1344 & 2.15 & 0.0 & 1461 & 2.33 \\ RoboFlamingo  & 0.0 & 268 & 0.79 & 1.00 & 452 & 1.29 & 0.0 & 1420 & 2.28 & 1.66 & 1493 & 2.39 \\ Octo  & 0.33 & 326 & 0.94 & 5.00 & 469 & 1.45 & 0.83 & 1040 & 1.80 & 5.83 & 2106 & 3.53 \\ StreamRoboLM (Opt) & 0.0 & 248 & 1.06 & 3.33 & 449 & 1.36 & 0.0 & 757 & 1.19 & 4.16 & 1047 & 1.71 \\ StreamRoboLM (Llama3) & 0.0 & 352 & 1.01 & 3.33 & 497 & 1.44 & 0.0 & 1163 & 1.81 & 3.33 & 1566 & 2.57 \\   

Table 3: Zero-shot generalization and fine-tuning results on L1 and L2 tasks. Here, _Suc._ denotes Success rate, _AR_ denotes Avg. reward and _R/S_ denotes Reward per Step.