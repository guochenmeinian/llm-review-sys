# Preference-grounded Token-level Guidance for Language Model Fine-tuning

Shentao Yang1, Shujian Zhang1, Congying Xia2, Yihao Feng2,

**Caiming Xiong2, Mingyuan Zhou1**

1The University of Texas at Austin  2Salesforce Research

shentao.yang@mccombs.utexas.edu, yihao.ac@gmail.com

mingyuan.zhou@mccombs.utexas.edu

###### Abstract

Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the _sequence level_ while LM training and generation both occur at the _token level_. There is, therefore, a _granularity mismatch_ between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and the utilization of the preference among multiple generations. For LM training, based on the amount of supervised data, we present two _minimalist_ learning objectives that utilize the learned guidance. In experiments, our method performs competitively on two distinct representative LM tasks -- discrete-prompt generation and text summarization. Source codes are released at https://github.com/Shentao-YANG/Preference_Grounded_Guidance.

## 1 Introduction

Language models (LMs) have been successfully trained with token-level cross-entropy losses, where each token position has a corresponding term in the overall training losses [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11]. Recent studies have shown that LMs can be further improved by aligning them with preferences from human feedback [12; 13; 14; 15] or automatic evaluation metrics [16; 17; 18]. Typically, the preferences are only provided at the _sequence level, e.g._, "Which of the two generated text sequences is better?" To align LMs with sequence-level preferences, there exist a variety of approaches, such as applying external filters to the training texts , performing supervised learning on some curated/improved datasets [20; 21; 22], and optimizing the LMs based on a learned sequence-level (pairwise-) preference predictor [23; 24; 14; 25].

While these approaches have contributed to the development of several revolutionary products [_e.g._, 18; 15], a mismatch issue has emerged from the perspective of guiding LM fine-tuning. Concretely, the sequence-level preference is not grounded into the token level, where LM training losses occur. This means that there is a _mismatch_ in granularity between the feedback and training losses -- the preference is coarse-grained while the training losses are fine-grained. This issue is similar to the delayed-feedback problem in reinforcement learning (RL) [26; 27; 28], where informative feedback is available only at the end of the trajectory (sequence) and not at any of the intermediate timesteps. Previous studies have noted that this problem could have a negative impact on the empirical performance of the resulting LMs [29; 30], as it introduces a more challenging learning problem characterized by higher gradient variance and lower sample efficiency to achieve the learning goal [31; 32].

To address this granularity mismatch, we focus on the following question: _How can we effectively ground sequence-level preference into token-level guidance for LM fine-tuning?_ We propose an alternate training process that alternates between two stages: learning preference-grounded token-level guidance and improving the LM using the learned guidance. This alternate process reduces the requirement on supervised data and targets the low-data regime, _e.g._, few/zero-shot learning, where task-specific supervised (pre-)training is infeasible and initial LMs have weak zero-shot abilities.

To ground the sequence-level preference into token-level guidance, we propose a framework for learning a token-level "reward" function1, inspired by reward-learning-from-preferences in the imitation learning (IL) literature [33; 34; 35]. Specifically, we train the token-level rewards such that the corresponding evaluation for a generated text sequence reflects the preference among multiple alternative generations, where the preference comes from task-specific evaluation. While _summation_ is classically used in IL to aggregate the learned token-level rewards into the text-sequence evaluation, LM tasks can be different from classical IL tasks. To cater to LM generations, our guidance-learning framework can accommodate more careful choices of the aggregation function beyond the classical summation. For instance, in generating text prompts to steer an LM for text classification, a "key token" in the prompt may be more effective than several mediocre tokens. Hence, using _maximum_ to aggregate the token-level rewards may better reflect the text-sequence quality than _summation_.

To utilize the learned preference-grounded guidance in LM training, we present two _minimalist_ learning objectives that contain only a minimal number of hyperparameters. These two objectives respectively target different amounts of supervised data in the specific LM task. We evaluate our framework on two distinct representative LM tasks: generating discrete text prompts for few-shot text classification and text summarization. On both tasks, our method exhibits competitive performance.

## 2 Main Method

Before diving into technical details, we will first establish the notation, provide some background on classical pairwise-preference learning, and offer an overview of our preference-grounding process.

**Notation.** In most LM tasks, we are given a dataset \(=\{(x^{i},y^{i})\}_{i=1}^{N}\) of \(N\) supervised examples, where \(x\) is the input to the LM, which can be a dummy, and \(y\) is the target text-sequence. We denote the LM parameterized by \(\) as \(_{}\). The \(t^{}\) generated token is denoted as \(a_{t}\), given by \(a_{t}_{}(\,|\,s_{t})\), where the context for token generation at step \(t 0\) is denoted as \(s_{t}\), consisting of the LM input \(x\) and the previously generated tokens \(a_{<t}=(a_{0},,a_{t-1})\). Specifically, \(s_{0}=x\) and \(\,t>0,s_{t}=(x,a_{<t})\). The full generated text-sequence of length \(T\) is denoted as \(=(a_{0},,a_{T-1})\). In most LM tasks, we have a task-specific evaluation metric \((s_{T},y)\) that depends on the final context \(s_{T}\) of the generated sequence and the target sequence \(y\), with \(s_{T}=(x,)\). The objective of LM training is often to maximize the expected task-specific evaluation, which can be expressed as

\[_{}_{(x,y)}_{ _{t=0}^{T-1}\,_{}(a_{t}\,|\,s_{t})}[(s_{T}=( x,),y)].\]

We model the learned token-level guidance as a bounded (reward) function \(r_{}(s_{t},a_{t})\), parametrized by \(\). Unlike the original sequence-level preference or evaluation that is only available at the final step \(T\), the trained \(r_{}\) can densely guide the token selection at each step \(t T\).

**Pairwise Preference Learning.** In reward-learning-from-preferences [_e.g._, 33; 34; 35], a dense reward function is learned such that the _sum-aggregated_ reward for the entire generation trajectory aligns with the pairwise preference between two trajectories. In the context of LM generation, suppose we have two text-generation trajectories \(^{i}\) and \(^{j}\) associated with the same LM input and target \((x,y)\), taking the form \(^{i}=\{(s^{i}_{0},a^{i}_{0}),,(s^{i}_{T-1},a^{i}_{T^{i}-1})\}\) with sequence lengths \(T^{i}\) and \(T^{j}\), respectively. Assume that \(^{i}\) is preferred over \(^{j}\), denoted as \(^{i}^{j}\). A token-level reward function \(r_{}(s_{t},a_{t})\) is learned by requiring \(_{t=0}^{T^{i}-1}r_{}(s^{i}_{t},a^{i}_{t})>_{t=0}^{T^{j}-1}r_{} (s^{j}_{t},a^{j}_{t})\). Following the Bradley-Terry model of preferences , the pairwise-preference loss for reward-function learning is

\[()=-[(_{t=0}^{T^{i}-1}r_{}(s^{i}_{t},a^{i}_{ t}))/_{k\{i,j\}}(_{t=0}^{T^{k}-1}r_{}(s^{k}_{t},a^{k}_{ t}))],\] (1)

which is often interpreted as binary classification in the literature [37; 38; 39]. In Eq. (1), _summation_\(()\) is used to aggregate the learned token-level rewards into a parametrized sequence-level evaluation.

Overview.To ground the _sequence-level_ preference into _token-level_ guidance for LM training and thereby address the granularity mismatch discussed in Section 1, we present an alternate learning process that alternately learns the token-level guidance and trains the LM using the learned guidance.

For learning the preference-grounded guidance, in Section 2.1 we propose a framework that learns a token-level reward function that reflects the preference among multiple generated sequences. To utilize the learned preference-grounded guidance, based on the amount of supervised data in the specific task, in Section 2.2 we present two _minimalist_ LM training approaches that require only minimal tuning. In our framework, we iterate between the above two steps to mitigate the distribution shift between the text sequences used to train the reward function and the text sequences evaluated by the reward function during LM training, taking into account that LMs can evolve during the training process. Our alternate-learning procedure is illustrated in Fig. 1.

### Token-level Guidance Learning for Preference Grounding

Instead of applying the pairwise approach discussed above, we utilize the preference among multiple generated text sequences to learn the reward function \(r_{}(s_{t},a_{t})\). Intuitively, we use more information to train the reward function at each optimization step. Therefore, our approach can be more efficient and effective, especially when the optimization budget is limited.

Concretely, suppose we have \(K 2\) generated text-sequences \((^{1},,^{K})\) for the same LM input and target \((x,y)\), with the associated generation trajectories \((^{1},,^{K})\) and with possibly unequal sequence-lengths \((T^{1},,T^{K})\). Assume that there is a preference ordering among these \(K\) sequences, where by "preference" we mean a ranking of text sequences based on some evaluations of full text-sequences. For description simplicity, let the preference ordering be \(^{1}^{K}^{1}^{K}\). We make no assumption about the source of preference. It may come from human ranking or some task-specific evaluation metric \(\) on the full text-sequences, _e.g._, the descending ordering of the text-sequence evaluations \((s^{1}_{T^{1}},y)>>(s^{K}_{T^{K}},y)\).

For a trajectory \(^{k}=\{(s^{k}_{0},a^{k}_{0}),,(s^{k}_{T^{k}-1},a^{k}_{T^{k}-1})\}\), our desired token-level reward function \(r_{}\) generates a reward for each step as \(\{r_{}(s^{k}_{t},a^{k}_{t})\}_{t=0}^{T^{k}-1}\). A sequence-level evaluation \(e_{}(^{k})\) for trajectory \(^{k}\) can be obtained by \(e_{}(^{k})=f(\{r_{}(s^{k}_{t},a^{k}_{t})\}_{t=0}^{T^{k}-1})\), where \(f()\) is the aggregation function over all per-step rewards, _e.g._, the classical _summation_\(()\). Our goal is to train \(r_{}\) such that these parametrized sequence-level evaluations \(\{e_{}(^{k})\}_{k=1}^{K}\) align with the given preference ordering \(^{1}^{K}\). Through this, the sequence-level preference is grounded into token-level rewards \(r_{}\).

Under the Plackett-Luce choice model [40; 41], the parametrized sequence evaluations \(\{e_{}(^{k})\}_{k=1}^{K}\) induce a probability distribution over all possible permutations of the integers \(\{1,,K\}\). We want to maximize the likelihood of the given preference ordering \(=(1,,K)\), _i.e._,

\[_{}()=:- P(\,|\,\{e_{}(^{ k})\}_{k=1}^{K}),\,P(\,|\,\{e_{}(^{k})\}_{k=1}^{K} )=_{k=1}^{K}\{(e_{}(^{k}))_{i=k}^{K} (e_{}(^{i}))\}.\] (2)

Figure 1: Overview of the proposed framework. “AVG” denotes _average_, which is an example of the aggregation function \(f()\) discussed in Section 2.1. “Seq Eval” refers to the parametrized sequence-level evaluations. The model choice of the reward function and LM depends on the specific task and is discussed in Section 4.

**Algorithm 1** A learning routine for the preference-grounded token-level reward function \(r_{}\).

**Input:** The LM \(_{}\), initialized reward \(r_{}\), aggregation function \(f()\), reward-training steps \(M_{}\).

**for**\(\{1,,M_{}\}\)**do**

Use \(_{}\) to generate \(K\) sequences \(\{^{k}\}_{k=1}^{K}\); and get the preference ordering among \(\{^{k}\}_{k=1}^{K}\).

With \(f()\), get sequence evaluations \(\{e_{}(^{k})\}_{k=1}^{K}\) from \(r_{}\); and optimize \(r_{}\) by Eq. (2).

**end for**

When \(K=2\) and \(f()\) denotes _summation_, Eq. (2) reduces to the classical pairwise-preference loss in Eq. (1). Therefore, our reward-learning loss can be viewed as an extension of the classical pairwise loss. Further, Eq. (2) extends the ListMLE loss  in recommender systems into preference learning under multiple variable-length trajectories.

Algo. 1 summarizes our reward-learning framework by describing an online-learning routine for training \(r_{}\). An offline or hybrid version can be obtained with minor changes.

**The Choice of Aggregation Function \(f()\).** In classical IL tasks such as robotics , the robots are trained to stand or walk as long as possible. In this scenario, _summation_ is a natural choice for the aggregation function \(f()\). However, in many text generation tasks, such as summarization, the generation quality may not be directly associated with the length of the generated text sequence. Nevertheless, suppose the token-level rewards are positive (_i.e._, \(r_{}>0\)), a longer sequence naturally has a higher sum of per-step rewards than a shorter one, which can bias \(r_{}\) towards automatically ranking longer sequences higher. This bias can hinder our reward-learning goal of aligning \(\{e_{}(^{k})\}_{k=1}^{K}\) with the given preference ordering. A naive numeric example is additionally provided in Appendix C.

To mitigate the potential length bias in the classical _summation_, we discuss three alternative choices of the aggregation function \(f()\): _average_, _soft maximum_, and _soft minimum_.

_Average._ We define the _average-aggregated_ sequence-level evaluation \(e_{}^{}(^{k})\) for trajectory \(^{k}\) as

\[e_{}^{}(^{k})=}_{t=0}^{T^{k}-1}r_{ }(s_{t}^{k},a_{t}^{k}), C=_{k=1}^{K}T^{k},\] (3)

where \(C\) is the average length of the \(K\) sequences. Multiplied by the average length \(C\) has the benefit of scaling \(e_{}^{}\) to the scale of \(e_{}^{}\), which ensures numerical-scale consistency with \(e_{}^{}\) and thus reduces hyperparameter tuning when switching from _summation_ to _average_ aggregation.

_Soft Maximum._ We define the _soft-maximum-aggregated_ sequence-level evaluation \(e_{}^{}(^{k})\) as

\[e_{}^{}(^{k})=C[_{t=0}^{T^{k }-1}(r_{}(s_{t}^{k},a_{t}^{k})/)],\] (4)

where \(C\) is the average trajectory-length in Eq. (3) and \(\) is the temperature parameter.

_Soft Minimum._ The _soft-minimum-aggregated_ sequence-level evaluation \(e_{}^{}(^{k})\) follows Eq. (4) except for changing \(\) to \(-\).

### LM Training with Preference-grounded Token-level Guidance

Considering the supervised-data availability, we present two _minimalist_ LM training objectives that utilize the learned preference-grounded guidance: **1)** a REINFORCE-style update when there is no supervised data; **2)** reward-weighted MLE when there are sufficient data. Our LM training directly starts from raw pre-trained checkpoints, without task-specific supervised pre-training. This choice is to keep the algorithm general and consistent in both situations we consider, since task-specific pre-training may not be feasible in the setting of few/zero-shot learning.

As shown in Algo. 1, we train the reward function \(r_{}\) by the sequences sampled from LM \(_{}\). Since task-specific pre-training to \(_{}\) is not assumed, over the course of training, \(_{}\) itself can evolve from a less-preferred distribution to a highly-preferred one. To mitigate the impact of this distribution shift and keep \(r_{}\) as accurate guidance for LM training, we periodically re-estimate \(r_{}\) during the first half of the LM-training process2, motivated by recent works in model-based RL [44; 45; 46; 47].

**Input:** The dataset \(\), initialized LM \(_{}\), initialized reward function \(r_{}\), LM-training steps \(M_{}\), reward-retrain period \(M_{}\), all inputs for training the reward function specified in Algo. 1.

**Initialize \(r_{}\)** by Algo. 1.

**for**\(\{1,,M_{}\}\)**do**

**if**\( M_{}/2\) and \(\;\%\;M_{}\) == 0 **then**

Re-train \(r_{}\) by Algo. 1 without re-initialization.

**end if**

Optimize \(_{}\) by Eq. (5) or Eq. (6) with \(\) and \(r_{}\).

**end for**

**Without Supervised Data.** When the LM \(_{}\) needs to discover good text generations by itself, the learned token-level reward \(r_{}\) can be used to provide dense guidance on generating each token, _i.e._, given the generation context \(s_{t}\), select the next token \(a_{t}\) such that \(r_{}(s_{t},a_{t})\) is high. Intuitively, for a generation trajectory \(\), if \(\,(s_{t},a_{t}),r_{}(s_{t},a_{t})\) is high, then the corresponding sequence-level evaluation \(e_{}()=f(\{r_{}(s_{t},a_{t})\}_{t=0}^{T-1})\) can be also high, _e.g._, the average or summation of token-level rewards. The associated text sequence \(\) will thus be preferable since \(r_{}\) is trained to reflect the sequence-level preference (Section 2.1). Through \(r_{}\), the sequence-level preference is grounded into dense token-level guidance for LM training, without granularity mismatch or feedback delay.

With the learned \(r_{}\), a _minimalist_ implementation of this LM-training idea is the discrete-variable optimization problem

\[_{}_{t\{0,,T-1\}}_{a_{t }_{}(\,\,|\,s_{t})}[r_{}(s_{t},a_{t})]\,,\]

for each timestep \(t\) of which we calculate its gradient by the classical REINFORCE method [48; 49; 50] since it can cope with a large vocabulary size. Here, \(T\) denotes a generic sequence length. Additionally, since we want multiple text generations in typical LM tasks, instead of only one, we relax the convergence of the REINFORCE method by adding a standard max-entropy gradient, which can help capture multiple good behavior-modes [51; 52; 53]. Thus, the LM \(_{}\) is trained by the gradient

\[_{t\{0,,T-1\}}\{_{a_{t} _{}(\,\,|\,s_{t})}[r_{}(s_{t},a_{t})_{} _{}(a_{t}\,|\,s_{t})]+_{}(_{ }(\,|\,s_{t}))\}\,,\] (5)

where \((_{}(\,|\,s_{t}))\) is the Shannon entropy of \(_{}(\,\,|\,s_{t})\) and \(\) is a balancing coefficient.

**With Supervised Data.** With a labelled dataset \(=\{(x^{i},y^{i})\}_{i=1}^{N}\) and with the learned preference-grounded guidance \(r_{}\), a _minimalist_ enhancement of the classical MLE LM-training is the token-level weighted-MLE, where the per-token weight is given by the learned reward-function \(r_{}\). Our intention is to emphasize the important tokens in the given sequence \(y\) and downweight the unimportant ones, where the token importance given by \(r_{}\) grounds the sequence-level preference. Intuitively, this weighting scheme can better utilize the LM capacity and the optimization budget, and may thus improve upon the vanilla supervised loss [54; 16]. Specifically, the LM \(_{}\) is trained by

\[_{}-_{(x,y)}[_{t=0}^{|y|-1}w_{t} _{}(y_{t}\,|\,s_{t})]\,,s_{t}=(x,y_{<t})w_{t}=(s_{t},y_{t})}{_{t  0}^{|y|-1}r_{}(s_{t},y_{t})}\,,\] (6)

where \(|y|\) is the length of the target sequence \(y\) and \(w_{t}\) is the self-normalized token-level reward. The standard self-normalization is used to reduce the gradient variance among the samples in \(\).

Algo. 2 sums up the entire alternate-learning process, with the reward-learning routine in Algo. 1.

## 3 Related Work

**Guiding Signals for LM Training.** One string of works in LM training directly optimizes the LMs against the native sequence-level feedback such as the test-time metric [_e.g._, 3; 55; 56; 57; 58; 59; 32]. This choice, however, may directly suffer from the delayed-feedback issue discussed in Section 1 and the subsequent high gradient variance and low sample efficiency [31; 32]. In the recent trend of RL-based LM training, it has been common to incorporate a token-level KL penalty towards the uniform distribution [31; 60], the initial LM [23; 61], the supervised-fine-tuned model [12; 62; 63; 14], or the base momentum model , to add to the delayed/ungrounded feedback. Although that KL penalty does impact the RL-based LM training at the token level, it is not tailored to the concrete task or the desired sequence-level feedback. When combined with the delayed-feedback issue, itcould distract the LM training from improving the received feedback/evaluation, especially at the beginning of the text-sequence generation, which can however affect all subsequent token selections. By contrast, as seen in Eq. (5), even when added a max-entropy gradient, our preference-grounded token-level guidance can still provide dense, task-specific, and feedback-oriented guidance on the selection of each token. For a more detailed discussion on the RL formulation of LM generation, the delayed-feedback issue in RL-based LM training, and delayed feedback with KL penalty, please refer to Appendix F.

In some relatively "ideal" settings, prior works have attempted to learn task-specific token-level guidance for LM training. For instance, Shi et al.  use inverse RL, Guo et al.  propose a hierarchical approach, and Yang et al.  learn LM discriminators; but these methods require abundant expert data for supervised (pre-)training, making them infeasible for the few/zero-shot settings we consider. Under the same requirement of sufficient expert data, Lin et al.  learn a sequence-level adversarial-ranking reward and Yu et al.  train a GAN structure. They both use Monte-Carlo rollouts to simulate intermediate rewards, which can be computationally expensive and have high variance. Le et al.  use some values related to the sequence evaluation without explicitly learning per-token rewards. Pang et al.  learn a token-level error predictor for machine translation, but they rely on expert error-span annotations for each translation, which is highly demanding.

By contrast, we propose a versatile framework for learning task-specific token-level guidance for LM training that can ground the sequence-level preference. Our approach is not limited to standard LM tasks and is also suitable for the low-data regime, with few assumptions about expert-data availability or preference source. In our experiments, we compare our method to recent RL-based approaches that train LM under delayed/ungrounded feedback with KL penalty. We discuss additional related works on prompt generation, text summarization, and aligning LMs with preferences in Appendix E.

## 4 Experiments

We test our framework on two distinct representative text-sequence generation tasks: **1)** input-agnostic discrete text-prompt generation for few-shot text-classification (Section 4.1), **2)** the classical text summarization (Section 4.2). Our LM training directly starts from raw pre-trained checkpoints from HuggingFace , without task-specific supervised pre-training. Depending on the LM \(_{}\) used in the specific task, our reward function \(r_{}\) can be implemented as either a decoder-only or an encoder-decoder model. Similar to prior works [_e.g._, 32, 25], given a text sequence \(\) and an LM input \(x\), the causal mask in transformers enables us to get the learned guidance \(r_{}(s_{t},a_{t})\) at each step of the sequence in parallel. Source codes have been publicly released.

### Input-agnostic Discrete-prompt Generation

**Overview.** In discrete text-prompt generation [_e.g._, 10, 74], we input a discrete text-prompt \(\) and an observation sequence \(o\) to a large pre-trained downstream LM \(_{}(,o)\) to directly classify text \(o\), without finetuning \(_{}\). We follow the classical setting [_e.g._, 60, 75] to perform classification by selecting tokens corresponding to some predefined class labels. In our input-agnostic setting, the generated prompt is independent of the observation \(o\). During inference time, only the learned prompts are used and the LM \(_{}\) is discarded. The initial input \(x\) to \(_{}\) is a dummy, and the target \(y\) is the class label. We also adopt the standard few-shot setting , where both the training and validation sets have \(16\,(o,y)\)-pairs per class. With a fixed length \(T\), the goal is to find discrete text-prompts \(=(a_{0},,a_{T-1})\) that have high test accuracy. We simulate the sequence-level preference by the stepwise metric in Deng et al. , _i.e._, the higher value the better prompt. This choice ensures a fair comparison and avoids a potential overfitting -- training and testing the LM on the same evaluation metric "accuracy". Appendix D discusses more details about the prompt task.

**LM Training, Implementation, and Datasets.** Since the prompt-generation task does not assume the availability of supervised data -- the ground-truth prompts, the LM \(_{}\) is trained by the REINFORCE-style update in Section 2.2 to discover highly-accurate prompts by itself. We implement our framework on the codebase of RLPrompt , and adopt the standard datasets and most hyperparameter settings in it. Reward training is reconducted every \(1000\) steps during the first \(6000\) steps of the LM training process and has early stopping. Reward function is learned with \(5\) sampled sequences and the temperature in Eq. (4) is set as \(=2\). The coefficient \(\) in Eq. (5) is \(=2^{-3}\). Appendix A.2 discusses the choices of these hyperparameters. The length of the generated prompts is fixed at \(5\). Wetest on three popular few-shot datasets in prior work [_e.g._, 77; 78]: two sentiment binary-classification datasets SST-2 [79; 80] and Yelp Polarity , and a topic four-way-classification dataset AG News [81; 82]. Additional details on the experiment and datasets are provided in Appendix B.1.

**Results.** We compare three variants of our framework with finetuning and with baselines in discrete- and continuous-prompt generation. Since the generated prompts all have length \(5\), in this task, the _average_ aggregation is equivalent to _summation_. Table 1 shows the test accuracy, where we rerun the codebase of RLPrompt  under the same random seeds and evaluation script as our method.3 Other baseline results are from the literature [60; 88].

On all three tested datasets, our method shows competitive and stable results against the strong baselines not only in discrete-prompt generation, but also in heavier continuous-prompt tuning and finetuning the large downstream LM. Based on Section 3, the performance improvement achieved by our method compared to RLPrompt suggests that utilizing the token-level guidance learned by our approach, which grounds the task-specific preference, can be more effective than learning under delayed/ungrounded feedback with KL penalty. Further, on both Yelp P. and AG News, using MAX aggregation is better than the classical _summation_. Table 3 in Appendix A shows examples of good generated prompts and their test accuracy. For instance, high-quality prompts on the AG News dataset often contain a topic classification keyword, such as "Tags" and "Category". This aligns with our intuition that good prompts may be identified by a (few) "key" token(s), as discussed in Sections 1 and 2.1. Thus, the _(soft-)maximum_ aggregation may better reflect prompt quality than _summation_.

### Text Summarization

**Overview.** In the summarization task, we follow the standard setting [_e.g._, 89; 61], where a set of supervised samples is available. The LM input \(x\) is the text to be summarized and the target \(y\) is the given summary. We simulate the sequence-level preference by the classical Meteor score  and report the standard ROUGE scores , to avoid overfitting evaluation metrics as in the prompt task.

**LM Training, Implementation, and Datasets.** Since a supervised dataset \(\) is available in this task, the LM \(_{}\) can be trained by the weighted-MLE objective in Section 2.2. This objective could be more stable and computationally efficient than REINFORCE-style methods in tasks of long-sequence generation. Due to limited computing resources, unless explicitly mentioned, we use the standard T5-small model  for both the LM and reward function. The reward training is simply \(1\) epoch of training on randomly sampled \(10\%\) of the training set and is repeated every \(0.5\) epochs during the first \(2\) epochs of LM training. Reward function is learned with \(3\) sampled sequences and again the temperature \(=2\) in Eq. (4). Additional experiment details are in Appendix B.2. We test on the standard setting of two news summary datasets: CNN/DailyMail (CNN/DM)  and XSum .

**Results.** We compare four variants in our framework with the standard supervised fine-tuning and RL-based methods PPO and NLPO in RL4LMs  under the environmental reward Meteor -- both

    &  &  &  \\  Finetuning & Few-shot Finetuning & 80.6 (3.9) & 88.7 (4.7) & **84.9** (3.6) \\   & Soft Prompt Tuning  & 73.8 (10.9) & 88.6 (2.1) & 82.6 (0.9) \\  & BB Tuning-50  & 89.1 (0.9) & 93.2 (0.5) & 83.5 (0.9) \\  & AutoPrompt  & 75.0 (7.6) & 79.8 (8.3) & 65.7 (1.9) \\   & Manual Prompt  & 82.8 & 83.0 & 76.9 \\  & In-Context Demo  & 85.9 (0.7) & 89.6 (0.4) & 74.9 (0.8) \\  & Instructions  & 89.0 & 84.4 & 54.8 \\  & GrIPS  & 87.1 (1.5) & 88.2 (0.1) & 65.4 (0.8) \\  & RLPPrompt  & 90.5 (1.5) & 94.2 (0.7) & 79.7 (2.1) \\   & Ours (AVG / SUM) & **92.6** (1.7) & 94.7 (0.6) & 82.8 (1.5) \\  & Ours (MIN) & 91.9 (1.8) & 94.4 (0.8) & 82.4 (1.1) \\  & Ours (MAX) & 91.2 (2.5) & **94.8** (0.5) & 83.3 (1.4) \\   

Table 1: Test accuracy on the prompt task. Best overall result is bold and best discrete-prompt is under different. The reported results are mean (standard deviation). We denote “BB Tuning-50” for Black-Box Tuning with mixed discrete and soft prompts that tunes the \(50\) soft tokens; and “AVG”, “SUM”, “MIN”, “MAX” for our method with aggregation function average, summation, soft minimum, and soft maximum (Section 2.1).

with and without task-specific supervised pre-training. For a fair comparison, the baseline results are from our rerunning RL4LMs' codebase with a T5-small model as our method.4 Table 2 shows the mean and standard deviation of ROUGE-1/2/L score across three random seeds.

On both datasets, our method shows favorable and stable performance against the classical and recent baselines. The better results of our method over supervised fine-tuning confirm the improvement of our reward-weighted MLE over the vanilla supervised loss, as discussed in Section 2.2. As in the prompt task, the gain of our method over RL-based baselines may indicate the benefit of utilizing our preference-grounded token-level guidance over learning under delayed feedback with KL penalty. In this task, using _average_ as the aggregation function outperforms the classical _summation_. This confirms our idea in Section 2.1 on avoiding the interference of unequal sequence-lengths in training \(r_{}\). Using MIN is also suitable for this task, since it is not confounded by individual lengths and reflects overall text quality. Unlike the prompt task, using MAX is unsuitable, since good summaries can hardly be identified by a few keywords. Overall, these results show the importance of customizing the aggregation choice for the specific LM task, a key feature of our guidance-learning framework.

Further, to verify the performance of our method under a larger LM, we change the _average_ variant of our method in Table 2 from T5-small to using T5-base LM. Fig. 2**(a)** - **(e)** compares our method on CNN/DM against the baselines, with an additional metric BertScore . The baseline results are directly cited from RL4LMs  and are the per-metric best across their three environmental rewards.5 Table 4 in Appendix A.1 shows the detailed numbers. It is clear that our method performs favorably against these strong baseline methods, especially in the ROUGE-L, BERTScore, Meteor, and ROUGE-2 metrics. To further varify our method, we conducted a human study under the T5-base LM. The results are in Fig. 2**(f)**, with detailed setup and numerics in Table 5 of Appendix A.1. It is clear that this human evaluation on the summarization task supports the improvements in ROUGE, Meteor, and BertScore by our method. Further scaling-up of our method is left as future work.

### Ablation Study

This section discusses the following three research questions to better understand our framework.

**(a):**_What will be the performance if we switch to using preference-based sequence-level guidance?_

To further study the gain of grounding preference into token-level guidance, we change the preference-based _token-level_ reward in our method to the corresponding _sequence-level_ reward. Fig. 3 shows the results when applying this change to our best variants in the prompt and summarization tasks in Sections 4.1 and 4.2, including the T5-base LM in Section 4.2, in comparison to the best corresponding baselines. For summarization, we plot the average ROUGE scores, _i.e._, (ROUGE-1 + ROUGE-2 + ROUGE-L) / 3. Table 6 in Appendix A.1 shows each ROUGE metric with standard deviation.

We see that learning and using preference-based sequence-level guidance does not provide a significant advantage over those baselines that mostly directly work with the task-specific native sequence-level feedback -- the results are even much worse than the baselines in some datasets. Besides, the results of our sequence-level variants are generally less stable. These echo the harm of the delayed-feedback issue discussed in Section 1. Overall, this set of comparisons confirms that the gain of our framework

    &  &  \\  & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-1 & ROUGE-2 & ROUGE-L \\  Lead-3 & 40.10 & 17.50 & 36.30 & 16.30 & 1.60 & 11.95 \\ Supervised & 38.88 (0.02) & 16.22 (0.05) & 32.58 (0.04) & 31.79 (0.02) & 9.68 (0.01) & 24.70 (0.03) \\ PPO & 39.16 (0.51) & 17.37 (0.33) & 33.77 (0.37) & 23.18 (0.31) & 4.46 (0.19) & 16.07 (0.32) \\ Supervised + PPO & 39.17 (0.65) & 17.29 (0.44) & 33.76 (0.53) & 28.24 (0.39) & 7.68 (0.13) & 20.02 (0.23) \\ NLPPO & 38.90 (0.35) & 17.22 (0.35) & 33.51 (0.42) & 22.97 (0.23) & 4.53 (0.13) & 15.62 (0.35) \\ Supervised + NLPPO & 39.27 (0.60) & 17.41 (0.36) & 33.85 (0.42) & 28.08 (0.16) & 7.68 (0.20) & 19.88 (0.16) \\  Ours (AVG) & **40.94** (0.02) & **18.78** (0.03) & **33.87** (0.03) & **33.62** (0.03) & **11.17** (0.02) & **26.33** (0.05) \\ Ours (SUM) & 40.70 (0.06) & 18.48 (0.05) & 37.93 (0.08) & 33.27 (0.09) & 10.83 (0.07) & 25.90 (0.06) \\ Ours (MIN) & 40.78 (0.06) & 18.67 (0.03) & 38.01 (0.04) & 33.57 (0.02) & 11.14 (0.02) & 26.30 (0.03) \\ Ours (MAX) & 39.98 (0.08) & 18.06 (0.03) & 37.26 (0.06) & 32.50 (0.14) & 10.46 (0.12) & 25.58 (0.12) \\   

Table 2: Results on text summarization. We bold the best result of each metric on each dataset. The results of Lead-3 on CNN/DM are from Ramamurthy et al.  and on XSum are from Lewis et al. . Other baseline results are from our rerunning RL4LMs’ codebase  using T5-small. Number reporting formats follow Table 1.

mainly comes from our preference-grounding perspective, _i.e._, learning and using a preference-based _token-level_ guidance, rather than simply learning and using "a preference-based guidance."

**(b):**_How does our method perform if we remove the reward-function retraining scheme?_

To study the effect of guidance re-estimation, we remove the reward-function retraining scheme from our best variants in the prompt and summarization tasks in Sections 4.1 and 4.2, including the T5-base LM in Section 4.2. Fig. 4 compares our methods with the best corresponding baselines. For the summarization task, we again plot the average ROUGE scores. Table 7 in Appendix A.1 shows each ROUGE metric with standard deviation. Appendix G discusses more on this re-estimation scheme.

Without guidance re-estimation, our method still performs competitively against the strong baselines, which corroborates the benefit of our preference-grounded guidance. Fig. 4 also verifies our intuition in Section 2 that the gain of this scheme depends on the zero-shot ability of the initial LMs. Specifically, in the prompt task where the initial LM has little zero-shot ability, reward-function retraining is helpful to both improve performance and reduce variance. In the summarization task where the initial LM does have some zero-shot ability (as shown in Ramamurthy et al. ), guidance re-estimation indeed helps results not as much, since the distribution-shift issue in Section 2 is less significant in this case. In this task, both our variants, with and without reward retraining, outperform the baselines.

**(c):**_What if we learn the token-level guidance by a different number of text sequences?_

To study how the number of sequences used to learn the reward function impacts our method's performance, we vary this number in the AVG variant in Tables 1 and 2. Fig. 5 shows the prompt results on SST-2 and summarization results on CNN/DM and XSum. For the latter, we again plot the average ROUGE scores. The scores of each ROUGE metric are in Tables 8 and 9 of Appendix A.1.

Recall that the best baseline result on SST-2 in Table 1 is \(90.5\), on CNN/DM and XSum in Table 2 is respectively \(31.3\) and \(22.06\). Thus, our method is generally robust to the number of sequences

Figure 3: Performance of our method using sequence-level and token-level preference-based guidance. “Best Baseline” refers to the best result in the baseline discrete-prompt methods for the prompt task, and the best result over all baseline methods for the summarization task. Error bars show one standard deviation.

Figure 2: CNN/DM summarization of our method and baselines under **T5-base** LM. “Sup” denotes “Supervised”. “Ref” denotes the ground-truth reference summary. Except for the human study in **(f)**, baseline results are directly cited from RL4LMs  and are the per-metric best across their three environmental rewards.

used to learn the guidance. Compared with the classical pairwise-preference learning (Section 2), our framework has the flexibility in using multiple sequences. As illustrated in Fig. 5, using three or more sequences to learn the reward function can be generally more beneficial than using only two.

Due to the page limit, we defer additional ablation study to Appendix A.2, where we **(1)** show that our framework is generally robust to the hyperparameter \(\) in Eq. (4) and \(\) in Eq. (5); **(2)** further validate the harm of the delayed-feedback issue to the relevant LM-training methods on longer text-sequence generation; **(3)** show that the efficacy of our framework is not tied to the specific preference sources considered in this section.

## 5 Conclusion

To address the granularity mismatch between the sequence-level preference and the token-level LM training losses, in this paper, we develop an alternate-learning process, where we iterate between grounding sequence-level preference into token-level training guidance, and training the LM with the learned guidance. Our method performs competitively on two distinct representative LM tasks. Future work includes combining our preference-grounded guidance with RL-based LM training, and applying our method to human preference and/or other tasks such as (task-oriented) dialog systems.