# Label Noise: Ignorance Is Bliss

Yilun Zhu

EECS

University of Michigan

allanzhu@umich.edu &Jianxin Zhang

EECS

University of Michigan

jianxinz@umich.edu &Aditya Gangrade

ECE

Boston University

gangrade@bu.edu &Clayton Scott

EECS, Statistics

University of Michigan

clayscot@umich.edu

###### Abstract

We establish a new theoretical framework for learning under multi-class, instance-dependent label noise. This framework casts learning with label noise as a form of domain adaptation, in particular, domain adaptation under posterior drift. We introduce the concept of _relative signal strength_ (RSS), a pointwise measure that quantifies the transferability from noisy to clean posterior. Using RSS, we establish nearly matching upper and lower bounds on the excess risk. Our theoretical findings support the simple _Noise Ignorant Empirical Risk Minimization (NI-ERM)_ principle, which minimizes empirical risk while ignoring label noise. Finally, we translate this theoretical insight into practice: by using NI-ERM to fit a linear classifier on top of a self-supervised feature extractor, we achieve state-of-the-art performance on the CIFAR-N data challenge.

## 1 Introduction

The problem of classification with label noise can be stated in terms of random variables \(X\), \(Y\), and \(\), where \(X\) is the feature vector, \(Y\{1,,K\}\) is the true label associated to \(X\), and \(\{1,,K\}\) is a noisy version of \(Y\). The learner has access to i.i.d. realizations of \((X,)\), and the objective is to learn a classifier that optimizes the risk associated with \((X,Y)\).

In recent years, there has been a surge of interest in the challenging setting of instance (i.e., feature) dependent label noise, in which \(\) can depend on both \(Y\) and \(X\). While several algorithms have been developed, there remains relatively little theory regarding algorithm performance and the fundamental limits of this learning paradigm.

This work develops a theoretical framework for learning under multi-class, instance-dependent label noise. Our framework hinges on the concept of _relative signal strength_, which is a point-wise measure of "noisiness" in a label noise problem. Using relative signal strength to charachterize the difficulty of a label noise problem, we establish nearly matching upper and lower bounds for excess risk. We further identify distributional assumptions that ensure that the lower and upper bounds tend to zero as the sample size \(n\) grows, implying that consistent learning is possible.

Surprisingly, _Noise Ignorant Empirical Risk Minimization (NI-ERM)_ principle, which conducts empirical risk minimization as if no label noise exists, is (nearly) minimax optimal. To translate this insight into practice, we use NI-ERM to fit a linear classifier on top of a self-supervised feature extractor, achieving state-of-the-art performance on the CIFAR-N data challenge.

Literature review

Theory and algorithms for classification with label noise are often based on different probabilistic models. Such models may be categorized according on how \(\) depends on \(Y\) and \(X\). The simplest model is symmetric noise, where the distribution of \(\) is independent of \(Y\) and \(X\)(Angluin and Laird, 1988). In this case, the probability that \(=k\) is the same for all \(k Y\), regardless of \(Y\) and \(X\). In this setting, it is easy to show that minimizing the noisy excess risk (associated to the 0/1 loss) implies minimizing the clean excess risk, a property known as _immunity_. When immunity holds, there is no need to modify the learning algorithm on account of noisy labels. In other words, the learner may be _ignorant_ of the label noise and still learn consistently.

A more general model is classification with label dependent noise, in which the distribution of \(\) depends on \(Y\), but not \(X\). Many practical algorithms have been developed over the years, based on principles including data re-weighting (Liu and Tao, 2015), robust training (Han et al., 2018; Liu et al., 2020; Hu et al., 2020; Foret et al., 2021; Liu et al., 2022) and data cleaning (Brodley and Friedl, 1999; Northcutt et al., 2021). Consistent learning algorithms still exist, such as those based on loss correction (Natarajan et al., 2013; Patrini et al., 2017; Van Rooyen and Williamson, 2018; Liu and Guo, 2020; Zhang et al., 2022). These approaches assume knowledge of the noise transition probabilities, which can be estimated under some identifiability assumptions (Scott et al., 2013; Zhang et al., 2021b).

In the most general setting, that of instance dependent label noise, the distribution of \(\) depends on both \(Y\) and \(X\). While algorithms are emerging (Cheng et al., 2021; Zhu et al., 2021; Wang et al., 2022; Yang et al., 2023), theory has primarily focused on the binary setting. Scott (2019) establishes immunity for a Neyman-Pearson-like performance criterion under a _posterior drift_ model, discussed in more detail below. Cannings et al. (2020) establish an upper bound for excess risk under the strong assumption that the optimal classifiers for the clean and noisy distributions are the same.

Closest to our work, Im and Grigas (2023) derive excess risk upper and lower bounds, and reach a similar conclusion, that noise-ignorant ERM attains the lower bound up to a constant factor. Our results, based on the new concept of relative signal strength, provide a more refined analysis.

Additional connections between our contributions and prior work are made throughout the paper.

## 3 Problem statement

_Notation_. \(\) denotes the feature space and \(=\{1,2,,K\}\) denotes the label space, with \(K\). The \(K\)-simplex is \(^{K}:=\{p^{K}: i,p_{i} 0, p_{i}=1\}\). A \(K K\) matrix is _row stochastic_ if all of its rows are in \(^{K}\). Denote the \(i\)-th element of a vector \(\) as \([]_{i}\), and the \((i,j)\)-th element of a matrix \(\) as \([]_{i,j}\).

In conventional multiclass classification, we observe training data \((X_{1},Y_{1}),,(X_{n},Y_{n})\) drawn i.i.d. from a joint distribution \(P_{XY}\). The marginal distribution of \(X\) is denoted by \(P_{X}\), and the _class posterior_ probabilities \(P_{Y|X=x}\) are captured by a \(K\)-simplex-valued vector \(:^{K}\), where the \(j\)-th component of the vector is \([(x)]_{j}=(Y=j X=x)\). A classifier \(f:\) maps an instance \(x\) to a class \(f(x)\). Denote the risk of a classifier \(f\) with respect to distribution \(P_{XY}\) as \(R(f)=_{(X,Y) P_{XY}}[_{\{f(X) Y\}}]\). The Bayes optimal classifier for \(P_{XY}\) is \(f^{*}(x)*{arg\,max}(x)\). The Bayes risk, which is the minimum achievable risk, is denoted as \(R^{*}=R(f^{*})=_{f}R(f)\).

We consider the setting where, instead of the true class label \(Y\), a noisy label \(\) is observed. The training data \((X_{1},_{1}),,(X_{n},_{n})\) can be viewed as an i.i.d. sample drawn from a "noisy" distribution \(P_{X}\). We define \(P_{|X=x}\), \(}\), \(\) and \(^{*}\) analogously to the "clean" distribution \(P_{XY}\).

The goal of learning from label noise is to find a classifier that is able to minimize the "clean test error," that is, the risk \(R\) defined w.r.t. \(P_{XY}\), even though the learner has access to only corrupted training data \((X_{i},_{i})}}{{}}P_{X }\).

Noise transition perspective.Traditionally, label noise is modeled through the joint distribution of \((X,Y,)\). This joint distribution is governed by \(P_{X}\), the clean class posterior \(P_{Y|X}\), and a matrix-valued function

\[:\{^{K K}:\},\]

known as the _noise transition matrix_. The \((i,j)\)-th entry of the matrix is defined as:

\[[(x)]_{i,j}=(=j Y=i,X=x).\]

This implies that the noisy and clean class posteriors are related by \(}(x)=(x)^{}(x)\), where \({}^{}\) denotes the matrix transpose.

Domain adaptation perspective.Alternatively, label noise learning can be framed as a domain adaptation problem. In this view, \(P_{X}\) represents the source domain, and \(P_{XY}\) represents the target domain. The relationship between the two domains is characterized by "posterior drift," meaning that while the source and target share the same \(X\)-marginal, the class posteriors (i.e., the distribution of labels given \(X\)) may differ (Scott, 2019; Cai and Wei, 2021; Maity et al., 2023; Liu et al., 2024). Thus, a label noise problem can also be described by a triple \((P_{X},,})\).

The two perspectives are equivalent, as discussed in Appendix A.1. In this work, we emphasize the domain adaptation perspective for Sections 4 and 5, and the noise transition perspective for Section 6.

## 4 Relative signal strength

To study label noise, we introduce the concept of _relative signal strength_ (RSS). This is a pointwise measure of how much "signal" (certainty about the label) is contained in the noisy distribution relative to the clean distribution. Previous work (Cannings et al., 2020; Cai and Wei, 2021) has examined a related concept within the context of binary classification, under the restriction that clean and noisy Bayes classifiers are identical. Our definition incorporates multi-class classification and relaxes the requirement that the clean and noisy Bayes classifiers agree.

**Definition 1** (Relative Signal Strength): _For any class probability vectors \(,}\), define the relative signal strength (RSS) at \(x\) as_

\[(x;,})=_{j}\ [}(x)]_{i}-[ }(x)]_{j}}{_{i}[(x)]_{i}-[(x)]_{j}},\] (1)

_where \(0/0:=+\). Furthermore, for \([0,)\), denote the set of points whose RSS exceeds \(\) as_

\[_{}(,})=\{x: (x;,})>\}.\]

\((x;,})\) is a point-wise measure of how much "signal" the noisy posterior contains about the clean posterior. To gain some intuition, first notice that if the noisy Bayes classifier predicts a different class than the clean Bayes classifier, the RSS is 0 by taking \(j=}\) (assuming for simplicity that the \(\) is a singleton set). Now suppose the clean and noisy Bayes classifiers _do_ make the same prediction at \(x\), say \(i^{*}\), and consider a fixed \(j\). If

\[}(x)]_{i^{*}}-[}(x)]_{j}}{[ }(x)]_{i^{*}}-[}(x)]_{j}}\]

is small, it means that the clean Bayes classifier is relatively certain that \(j\) is not the correct clean label, while the noisy Bayes classifier is less certain that \(j\) is not the correct noisy label. Taking the minimum over \(j\) gives the relative signal strength at \(x\). As we formalize in the next section, a large RSS at \(x\) ensures that a small (pointwise) _noisy_ excess risk at \(x\) implies a small (pointwise) _clean_ excess risk. To gain more intuition, consider the following examples.

**Example 1**: _When \((x)=[0\ \ 1\ \ 0]^{}\) and \(}(x)=[0.3\ \ 0.6\ \ 0.1]^{}\),_

\[(x;,})=_{j}\ [ }(x)]_{i}-[}(x)]_{j}}{_{i}[(x)]_{i}-[(x)]_{j}}=}(x)]_{2}-[ }(x)]_{1}}{[(x)]_{2}-[(x)]_{1}}==0.3.\]

_Here, first of all, \(=}=2\), i.e., the clean and noisy Bayes classifier give the same prediction. What's more, \((x;,})<1\) because the clean Bayes classifier is absolutely certain about its prediction, while the noisy Bayes classifier is much less certain._

**Example 2**: _When \((x)=[0\;\;1\;\;0]^{}\) and \(}(x)=[0\;\;0\;\;1]^{}\),_

\[(x;,})=_{j}\;\;[}(x)]_{i}-[}(x)]_{j}}{_{ i}[(x)]_{i}-[(x)]_{j}}=}(x)]_{3}-[ }(x)]_{3}}{[(x)]_{2}-[(x)]_{3}}==0.\]

_The zero signal strength results from \(}\) and \(\) leading to different predictions about \(\)._

**Example 3** (Comparison to KL divergence): _When \((x)=[0.05\;\;0.7\;\;0.25]^{}\), and \(}^{(1)}(x)=[0.25\;\;0.7\;\;0.05]^{}\), \(}^{(2)}(x)=[0.1\;\;0.6\;\;0.3]^{}\),_

\[_{}(\,\, }^{(1)})}<_{}( \,\,}^{(2)})} (x;,}^{(1)})> (x;,}^{(2)}).\]

_Here, \(}^{(2)}\) is "closer" to \(\) in terms of KL divergence, but \(}^{(1)}\) provides more information in terms of predicting the \(\) of \(\). There is no conflict: KL divergence considers the similarity between two (whole) distributions, while the task of classification only focuses on predicting the \(\)._

_This also illustrates why our notion of RSS is better suited for the label noise problem than other general-purpose distance measures between distributions._

A desirable learning scenario would be if \(_{}(,})=\) for some large \(\), indicating that the signal strength is big across the entire space. Unfortunately, this ideal situation is generally not achievable. To gain some insight, consider the following result, proved in Appendix A.2.1.

**Proposition 1**: \(_{0}(,})=x: \,}(x)\,(x)}\)_._

If we assume that both \(\) sets are singletons, this result indicates that \(_{0}\), the region with positive RSS, is the region where the true and noisy Bayes classifiers agree. Accordingly, \(_{0}\), the zero signal region, is the region where the clean and noisy Bayes decision rules differ. The "region of strong signal," \(_{}\), is a subset of \(_{0}\). Since the clean and noisy Bayes classifiers will typically disagree for at least some \(x\), \(_{0}\) in general. We note that the strong assumption that \(_{0}=\) has been made in prior studies (Cannings et al., 2020; Cai and Wei, 2021). Our notion of RSS relaxes this assumption and provides a unified view.

### RSS in binary classification

We can express relative signal strength more explicitly in the binary setup. Let \((x):=[(x)]_{1}=(Y=1 X=x)\) and \((x):=[}(x)]_{1}= {Y}=1 X=x\). In standard binary classification, the _margin_(Tsybakov, 2004; Massart and Nedelec, 2006), defined as \((x)-\), serves as a pointwise measure of signal strength. Our notion of relative signal strength (RSS) can be interpreted as an extension of this concept in the context of label noise learning.

**Proposition 2**: _In the binary setting, for \( 0\),_

\[(x;,)=\{(x)- }{(x)-},\;0\},_{}(,)=\{x:(x)- }{(x)-}>\}.\]

In other words, RSS can be viewed as a "relative" margin.

**Example 4**: _Illustration of relative signal strength in a binary classification setup (Figure 1)._

### Posterior Drift Model Class.

Now putting definitions together, we consider the posterior drift model \(\) defined over the triple \((P_{X},,})\). Let \(,[0,+)\), and define

\[(,):=(P_{X},,} ):P_{X}_{}(,} ) 1-}.\]

This is a set of triples (label noise problems) such that \(_{}\), the region with RSS at least \(\), covers at least \(1-\) of the probability mass. In the next section, we will demonstrate that the performance within \(_{}\) can be guaranteed, whereas learning outside the region \(_{}\) is provably challenging.

## 5 Upper and lower bounds

In this section, we establish both upper and lower bounds for excess risk under multi-class instance-dependent label noise.

### Minimax lower bound

Our first theorem reveals a fundamental limit: no classifier trained using noisy data can surpass the constraints imposed by relative signal strength in a minimax sense. To state the theorem, we employ the following notation and terminology. Denote the noisy training data by \(Z^{n}=\{(X_{i},_{i})\}_{i=1}^{n}P_{X}\). A _learning rule_\(\) is an algorithm that takes \(Z^{n}\) and outputs a classifier. The risk \(R()\) of a learning rule is a random variable, where the randomness is due to the draw \(Z^{n}\).

**Theorem 1** (Minimax Lower Bound): _Let \(,>0\). Then_

\[_{}_{(P_{X},,})(, )}_{Z^{n}}[R()-R(f^{*})] {K-1}{K}+(\,}),\]

_where the \(\) is over all learning rules._

The proof in Appendix A.2.3 offers insights into how label noise impacts the learning process: On the low RSS region \((_{})\), learning is difficult if not impossible, and the learner incurs an irreducible error of \((1-1/K)\). On the high RSS region (\(_{}\)), the standard nonparametric rate [Devroye et al., 1996] is scaled by \(1/\). These aspects determine fundamental limits that no classifier trained only on noisy data can overcome without additional assumptions.

### Upper bound

This subsection establishes an upper bound for _Noise Ignorant Empirical Risk Minimizer (NI-ERM)_, the empirical risk minimizer trained on noisy data. This result implies that NI-ERM is (nearly) minimax optimal, a potentially surprising result given that NI-ERM is arguably the simplest approach one might consider. We begin by presenting a general result on the excess risk of any classifier, which is proved in Appendix A.2.4.

**Lemma 1** (Oracle Inequality): _For any label noise problem \((P_{X},,})\) and any classifier \(f\),_

\[R(f)-R(f^{*})_{>0}\{P_{X}_{}(,})+ ((f)-(^{*}))\}.\]

For \((P_{X},,})(,)\), the first term is bounded by \(\). When \(f\) is selected by ERM over the noisy training data, conventional learning theory implies a bound on the second term. This leads to the following upper bound for NI-ERM, whose proof is in Appendix A.2.5.

**Theorem 2** (Excess Risk Upper Bound of NI-ERM): _Let \(\ \ ,\ >0\). Consider any \((P_{X},,})(,)\), assume function class \(\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(^{*}\) belongs to \(\). Let \(\) be the ERM trained on \(Z^{n}=\{(X_{i},_{i})\}_{i=1}^{n}\), i.e., \(=*{arg\,min}_{f}_{i =1}^{n}_{\{f(X_{i})_{i}\}}\). Then_

\[_{Z^{n}}[R()-R(f^{*})]+ }(}).\]

\(}\) denotes big-\(\) notation ignoring logarithmic factors. The Natarajan dimension is a multiclass analogue of the VC dimension. The upper bound in Theorem 2 aligns with the minimax lower bound (Theorem 1) in both terms. For the irreducible error \(\), there is a small gap of \(1/K\). This gap arises because, in the lower bound construction, the low signal region \(_{}\) is known to the learner, whereas knowledge of \(_{}\) is not provided to NI-ERM. If \(_{}\) were known to the learner (an unrealistic assumption), then a mixed strategy that preforms NI-ERM on \(_{}\) and randomly guesses on \(_{}\) would have an upper bound with first term of \((1-1/K)\), exactly matching the lower bound. Regarding the second term, there is a universal constant and a logarithmic factor between the lower and upper bounds, which is a standard outcome in learning theory.

This result is surprising as it indicates that the simplest possible approach, which ignores the presence of noise, is nearly optimal. No learning rule could perform significantly better in this minimax sense.

### A smooth margin-condition on the relative signal strength

The previous sections have analyzed learning with label noise over the class \((,)=\{(P_{X},,}):P_{X}(_ {}) 1-\}\). Now, the set \(_{}\) equals \((_{0})(_{0}_ {})\). The first part of this decomposition is the region where the Bayes classifiers under the noisy and clean distributions differ, while the second is a region where these match, but the RSS is small. Naturally, while \(P_{X}(_{0})\) must be incurred as an irreducible error, one may question why the class \(\) also limits the mass of \(_{0}_{}\). After all, with enough data, the optimal prediction in this region can be learned.

This issue would be resolved if there existed a \(_{0}>0\) such that \(P_{X}(_{0})=P_{X}(_{_{0}})\), i.e., if \(P_{X}(_{0}_{_{0}})=0\). In fact, our lower bound from Theorem 1 uses precisely such a construction. An interesting point of comparison to this condition lies in Massart's hard-margin condition from standard supervised learning theory, which, for binary problems, demands that \(P_{X}(|[(x)]_{1}-[(x)]_{2}|<h)=0\) for some \(h>0\), under which one obtains minimax excess risk bounds of \(O^{(V/hh)}\)(Massart and Nedelec, 2006). With this lens, we can view the condition \(P_{X}(_{0}_{_{0}})=0\) as a type of _hard-margin condition on the relative signal strength \(\)_. This naturally motivates a smoothened version of this condition, inspired by Tsybakov's soft-margin condition (Tsybakov, 2004).

**Definition 2**: _A triple \((P_{X},,})\) satisfies an \((,,C_{})\)-smooth relative signal margin condition with \(,>0,C_{}>0\) if_

\[>0,\ P_{X}((x;,}) ) C_{}^{}+.\]

_Further, we define \(^{}(,,C_{})\) as the set of triples \((P_{X},,})\) that satisfy an \((,,C_{})\)-smooth relative signal margin condition._

We show in Appendix A.2.7 that the techniques of Section 5.2 yield the following result for \(^{}\).

**Theorem 3**: _Let \(,>0,C_{}>0\). Consider any \((P_{X},,})^{}(,,C_{})\), assume function class \(\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(^{*}\) belongs to \(\). Let \(\) be the ERM trained on \(Z^{n}=\{(X_{i},_{i})\}_{i=1}^{n}\). Then_

\[_{Z^{n}}[R()-R(f^{*})] +_{>0}\{C_{}^{}+} (})\}=+}(n^{-/(2+2)}).\]

Compared to Theorem 2, we see that the rate of the second term is slightly slower, which is consistent with standard learning theory where Massart's hard margin assumption leads to faster rates than Tsybakov's. The advantage of the smooth relative margin is that the irreducible term in the above theorem is exactly \(P_{X}(^{{}^{}}_{0})\), which has a clearer meaning as it measures the mismatch between clean and noisy Bayes classifiers. Further, notice that the NI-ERM algorithm does not need information about \(\), and thus the result is adaptive to both \(\), and to the optimal \(\) for each value of \(\), as a consequence of the oracle inequality of Lemma 1.

More broadly, Theorem 3 illustrates the flexibility of our conceptualization of label noise problems through RSS. The RSS \(\) characterizes the irreducible error in label noise learning, similar to how the regression function \(\) characterizes excess risk in standard learning. Thus, standard theoretical frameworks can be adapted to the noisy label problem via the _relative signal_.

## 6 Conditions that ensure noise immunity

The minimax lower bound in the previous section revealed a negative outcome, indicating that no method can do well in the low signal region. Nevertheless, numerous empirical successes have been observed even under significant label noise. This is not mere coincidence. In this section, we will illustrate that the high signal region \(_{}\) can indeed cover the entire input space \(\) even under massive label noise, albeit with the constraint \(_{}_{0}\) as stated in Proposition 1. This not only explains past empirical successes, but also gives a rigorous condition on the consistency of NI-ERM.

This section will delve into the study of noise transition matrix \(\) and establish precise conditions that lead to \(_{0}=\). These conditions are linear algebraic conditions on \(\) that ensure \(}(x)=(x)\). As a result, we can infer that in a \(10\)-class classification problem, even with up to \(90\%\) of training labels being incorrect, the NI-ERM can still asymptotically achieve Bayes accuracy. In the upcoming definition, we introduce the concept of noise immunity, wherein the optimal classifiers remain unaffected by label noise .

**Definition 3** (Immunity): _We say that a \(K\)-class classification problem \((P_{X},,})\) is immune to label noise if \( x,}(x)=(x)\)._

Notice that due to Proposition 1, if a problem is immune, then \(_{0}=\). We now provide necessary and sufficient conditions on noise transition matrix \(\) that ensure noise immunity. We begin by considering distribution \(P_{XY}\) with zero Bayes risk, that is, where \(\) is one-hot almost surely. A matrix is defined as diagonally dominant if, for each row, the diagonal element is the unique maximum.

**Theorem 4** (Immunity for Zero-error Distribution): _If \(P_{XY}\) has Bayes risk of zero, then immunity holds if and only if for all \(x\), the noise transition matrix \((x)\) is diagonally dominant._

**Remark** : _For a zero-error distribution \(P_{XY}\), even corrupted with instance-dependent label noise, achieving the Bayes risk is still feasible with a noise rate \(( Y)\) up to \(\). This highlights that the task of classification itself is robust to label noise, specially when the clean \(\) is well-separated._

The above result relies on strong assumptions about the distribution \(P_{XY}\). Now, we present a result that applies to any distribution, which, as a trade-off, turns out to impose more requirements on \(\).

**Theorem 5** (Universal Immunity): _For any choice of \(P_{XY}\), immunity holds_

\[\;e(x)>0\;\; x,\;[(x)]_{i,j}=+e(x)&i=j\\ -&i j.\]Previous works (Ghosh and Kumar, 2017; Menon et al., 2018; Oyen et al., 2022) have established that symmetric label noise is sufficient for immunity. Our contribution advances this understanding by demonstrating that such noise conditions are not only sufficient but also necessary. Specifically, under symmetric label noise, learning towards the Bayes classifier is feasible as long as the proportion of wrong labels does not exceed \(\). Furthermore, this transition is abrupt: when \( Y<\), \(_{0}=\), but when \( Y\), \(_{0}=\). Consequently, we expect to see a sudden drop in performance when noise rate passes the threshold.

The rationale behind the necessity of \((x)\) taking this specific form is that it redistributes the probability mass of \(\) in a "uniform" manner. This constraint arises because \((x)\) cannot favor any classes besides the true class. For instance, consider \((x)=+\ -\ ^{}\) for some small \(>0\), a "non-uniform" \((x)\) would alter the \(\).

The above theorems demonstrate that signal strength at \(x\) can still be high even under massive label noise \( Y\), and, in essence, it is the discrete nature of the classification problem that allows robustness to label noise. When immunity holds, the irreducible error in Theorem 2 vanishes, therefore NI-ERM becomes a consistent learning rule. We validate this through data simulations presented in Figure 2, where we systematically flip labels uniformly and observe the corresponding changes in the testing accuracy of NI-ERM. The simulation results align closely with the theoretical expectations: NI-ERM achieves near-Bayes risk performance until a certain noise threshold is reached, beyond which the testing performance sharply deteriorates.

## 7 Practical implication

The modern practice of machine learning often involves training a deep neural network. In complex tasks involving noisy labels, the naive NI-ERM is often outperformed by state-of-the-art methods by a significant extent (Li et al., 2020; Xiao et al., 2023). This is consistent with the finding that directly training a large neural network on noisy data frequently leads to overfitting (Zhang et al., 2021).

Yet this is not grounds for abandoning NI-ERM altogether as a practical strategy. Instead of using NI-ERM for end-to-end training of a deep neural network, we instead propose the following simple, two-step procedure, termed 'feature extraction + NI-ERM'.

1. Perform feature extraction using any method (e.g., transfer learning or self-supervised learning) that does not require labels.
2. Learn a simple classifier (e.g., a linear classifier) on top of these extracted features, using the noisily labelled data, in a noise-ignorant way.

Figure 2: Data simulation that verifies noise immunity. For binary, the turning point is at noise rate \( Y=0.5\). For \(10\)-class, the turning point is at \( Y=0.9\).

This approach has three advantages over full network training. First, it avoids the potentially negative impact of the noisy labels on the extracted features. Second, it enjoys the inherent robustness of fitting a simple model (step 2) on noisy data, which we observed in Figure 2. Third, it avoids the need to tune hyperparameters of the feature extractor using noisy labels. We note that a "self-supervised + simple approach" to learning was previously studied by Bansal et al. (2021), although their focus was on generalisation properties without label noise. We also acknowledge that the practical idea of ignoring label noise is not new (Ghosh and Lan, 2021), but the full power of this approach has not been previously recognized. For example, prior works often combine this approach with additional steps or employ early stopping to mitigate the effects of noise (Zheltonozhskii et al., 2022; Xue et al., 2022).

Remarkably, this two-step approach attains extremely strong performance. We conducted experiments 1 on the CIFAR image data under two scenarios: synthetic label flipping (symmetric noise) and realistic human label errors (Wei et al., 2022), as shown in Figure 3. We examine three different feature extractors: the DINov2 foundation model (Oquab et al., 2023), ResNet-50 features extracted from training on ImageNet (He et al., 2016), and self-supervised ResNet-50 using contrastive loss (Chen et al., 2020). We also compared to a simple linear model trained on the raw pixel intensities, and a ResNet-50 trained end-to-end. We observed that ResNet-50 exhibits degrading performance with increasing noise, consistent with previous findings (Zhang et al., 2021; Mallinar et al., 2022). The linear model demonstrates robustness to noise, but suffers from significant approximation error.

Conversely, the FE+NI-ERM approach enjoys the best of both worlds. Regardless of how the feature extraction is carried out, the resulting models exhibit robustness to label noise, while the overall accuracy depends entirely on the quality of the extracted features. This is illustrated in Figure 3, where the flatness of the accuracy curves as noise increases indicates the robustness, and the intercept at zero label noise is a measure of the feature quality. Importantly, this property holds true even under realistic label noise of CIFAR-N (Wei et al., 2022). In fact, we find that using the DINov2 (Oquab et al., 2023) extracted features in our FE+NI-ERM approach yields state of the art results on the CIFAR-10N and CIFAR-100N benchmarks, across the noise levels, as shown in Table 1.

We emphasize that the only hyperparameters of our model are the hyperparameters of the linear classifier, which are tuned automatically using standard cross-validation on the noisy labels. This contrasts to the implementation of many methods on the CIFAR-N leaderboard (http://noisylabels.com/)

Figure 3: A linear model trained on features obtained from either transfer learning (pretrained ResNet-50 on ImageNet (He et al., 2016) ), self-supervised learning (ResNet-50 trained on CIFAR-10 images with contrastive loss (Chen et al., 2020)), or a pretrained self-supervised foundation model DINov2 (Oquab et al., 2023) significantly boosts the performance of the original linear model. In contrast, full training of a ResNet-50 leads to overfitting.

2, where the hyperparameters are hard-coded. Furthermore, our approach does not rely on data augmentation. Additional experiments, detailed in Appendix A.4, include comparisons with the 'linear probing, then fine-tuning' approach (Kumar et al., 2022), the application of different robust learning strategies on DINov2 features, and results on synthetic instance-dependent label noise.

Overall, the strong performance, the simplicity of the approach and the lack of any untunable hyperparameters highlights the effectiveness of FE+NI-ERM, and indicates the value of further investigation into its properties.

RSS for realistic human label error.To calculate the RSS under realistic human label error, we train two linear classifiers on DINov2 features under clean and noisy labels and use the models' predictions as estimates for the class probabilities \(\) and \(}\). Despite the high overall noise rate in CIFAR-10N "Worst" labels, with \((Y)=40.21\%\), we conjecture that the region where there is no signal, \(_{0}\), covers only a small portion of the probability mass (\( 4\%\)). Furthermore, the cumulative distribution of the estimated RSS can be upper-bounded by a polynomial \(C_{}^{}+\), supporting the validity of the smooth relative signal margin condition introduced in Section 5.3.

## 8 Conclusions

This work presents a rigorous theory for learning under multi-class, instance-dependent label noise. We establish nearly matching upper and lower bounds for excess risk and identify precise conditions for classifier consistency. Our theory reveals the (nearly) minimax optimality of Noise Ignorant Empirical Risk Minimizer (NI-ERM). To make this theory practical, we provide a simple modification leveraging a feature extractor with NI-ERM, demonstrating significant performance enhancements. A limitation of this work is that our methodology warrants more extensive experimental evaluation.

    &  & CIFAR-100N \\   & Aggre & Rand1 & Rand2 & Rand3 & Worst & Noisy \\  ProMix & 97.65 \(\) 0.19 & 97.39 \(\) 0.16 & 97.55 \(\) 0.12 & 97.52 \(\) 0.09 & **96.34 \(\) 0.23** & 73.79 \(\) 0.28 \\ ILL & 96.40 \(\) 0.03 & 96.06 \(\) 0.07 & 95.98 \(\) 0.12 & 96.10 \(\) 0.05 & 93.55 \(\) 0.14 & 68.07 \(\) 0.33 \\ PLS & 96.09 \(\) 0.09 & 95.86 \(\) 0.26 & 95.96 \(\) 0.16 & 96.10 \(\) 0.07 & 93.78 \(\) 0.30 & 73.25 \(\) 0.12 \\ DivideMix & 95.01 \(\) 0.71 & 95.16 \(\) 0.19 & 95.23 \(\) 0.07 & 95.21 \(\) 0.14 & 92.56 \(\) 0.42 & 71.13 \(\) 0.48 \\  FE + NI-ERM & **98.69 \(\) 0.00** & **98.80 \(\) 0.00** & **98.65 \(\) 0.00** & **98.67 \(\) 0.00** & 95.71 \(\) 0.00 & **83.17 \(\) 0.00** \\   

Table 1: Performance comparison with CIFAR-N leaderboard (http://noisylabels.com/) in terms of testing accuracy. “Aggre”, “Rand1”,..., “Noisy” denote various types of human label noise. We compare with four methods that covers the top three performance for all noise categories: ProMix (Xiao et al., 2023), ILL (Chen et al., 2023), PLS (Albert et al., 2023) and DivideMix (Li et al., 2020). Our approach, a Noise Ignorant linear model trained on features extracted by the self-supervised foundation model DINov2 (Qquab et al., 2023) achieves new state-of-the-art results, highlighted in bold. We employed Python’s sklearn logistic regression and cross-validation functions without data augmentation; the results are deterministic and directly reproducible.

Figure 4: Empirical CDF of estimated RSS for CIFAR-10N, evaluated on test data.