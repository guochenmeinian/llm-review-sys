# Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL

Qin-Wen Luo1, Ming-Kun Xie1,2, Ye-Wen Wang1, Sheng-Jun Huang1

1 Nanjing University of Aeronautics and Astronautics, Nanjing, China

2 RIKEN Center for Advanced Intelligence Project, Tokyo, Japan

{luoqw8,linuswangg,huangsj}@nuaa.edu.cn

ming-kun.xie@riken.jp

The authors contribute equally.Correspondence to: Sheng-Jun Huang (huangsj@nuaa.edu.cn).

###### Abstract

Offline-to-online (O2O) reinforcement learning (RL) provides an effective means of leveraging an offline pre-trained policy as initialization to improve performance rapidly with limited online interactions. Recent studies often design fine-tuning strategies for a specific offline RL method and cannot perform general O2O learning from any offline method. To deal with this problem, we disclose that there are evaluation and improvement mismatches between the offline dataset and the online environment, which hinders the direct application of pre-trained policies to online fine-tuning. In this paper, we propose to handle these two mismatches simultaneously, which aims to achieve general O2O learning from any offline method to any online method. Before online fine-tuning, we re-evaluate the pessimistic critic trained on the offline dataset in an optimistic way and then calibrate the misaligned critic with the reliable offline actor to avoid erroneous update. After obtaining an optimistic and and aligned critic, we perform constrained fine-tuning to combat distribution shift during online learning. We show empirically that the proposed method can achieve stable and efficient performance improvement on multiple simulated tasks when compared to the state-of-the-art methods. The implementation is available at https://github.com/QinwenLuo/OCR-CFT.

## 1 Introduction

Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset without additional interactions with the environment. This characteristic makes it particularly promising for critical applications such as healthcare decision-making , human-AI coordination  and autonomous driving . Generally, the performance of the learned policy relies on the quality of the dataset. Given that the offline data is limited, fine-tuning the policy through interactions with the environment is still necessary to achieve favorable performance. Consequently, offline-to-online (O2O) RL tends to achieve faster performance improvements based on better initializations.

To effectively fine-tune offline policies, O2O methods are typically designed based on specific offline RL algorithms. Existing methods can be roughly divided into two groups according to the base offline methods they use. The first group relies on policy constraint methods. These approaches aim to improve online performance by either adaptively adjusting constraints  or directly applying offline algorithms to online fine-tuning . Unfortunately, these methods often suffer from inefficient performance improvement due to restricted action exploration caused by policy constraints. The second group builds on value regularization methods. These methods aim to prevent excessivelylow Q-values resulting from pessimistic evaluations, such as those in CQL . The goal is to enhance the generalization of the value function and mitigate potential performance declines [27; 33]. Some other methods adopt ensemble Q-learning [51; 23; 30; 19] address these issues. However, these methods often face high computational costs due to the need to train multiple Q-networks.

Considering that the aforementioned methods develop online fine-tuning algorithms based on specific offline methods, they often struggle to be applied to other offline methods. To establish a general O2O framework, it is essential to address the core issues associated with transitioning from offline to online environments. Inspired of the recent work , which highlights the misalignment between the actor and the critic in an explicit policy constraint method, we identify two mismatches in general O2O RL: evaluation mismatches and improvement mismatches. Evaluation mismatches primarily occur in value regularization methods. These refer to the differences in policy evaluation methods between online and offline settings, which cause severe fluctuations in Q-value estimation during the initial stages of fine-tuning. Improvement mismatches, on the other hand, are prevalent in policy constraint methods. They are often caused by differences in objectives for updating the policy, leading to a misalignment between the probabilities of actions and their Q-values. Thanks to another recent work by Xu et al , which connects value regularization and policy constraint methods, we bridge these two types of mismatches within a unified framework for general RL-based offline algorithms.

In this paper, we propose a general O2O framework designed to address both evaluation and improvement mismatches simultaneously, aiming for stable and favorable fine-tuning performance from any offline method to representative online methods. To address the evaluation mismatch in value regularization methods, we propose re-evaluating the offline policy in an optimistic manner using an off-policy evaluation method. This approach allows us to obtain optimistic Q-value estimates, preventing the dramatic fluctuations in Q-values that could potentially cause the policy to collapse. Although the re-evaluated critic can estimate Q-values optimistically, it suffers from the misalignment with the offline policy, causing the improvement mismatch. To handle the improvement mismatch in the re-evaluated critics and policy constraint methods, we introduce value alignment to calibrate the critic so that it aligns with the probabilities predicted by the policy. Our approach involves using the Q-value of the most likely action as an anchor and then calibrating other Q-values by either exploiting the correlation between Q-values of different state-action pairs or modeling Q-values as a Gaussian distribution. Finally, we propose a constrained fine-tuning framework to guide the policy update by adding a regularization term, with the target of mitigating the negative impact of data shift. Extensive experimental results on multiple benchmark environments validate that the proposed methods can achieve better or comparable performance when compared to state-of-the-art methods.

Our contributions can be summarized as follows:

* We systematically study that there exist evaluation and improvement mismatches for offline RL methods from the perspective of online RL. We show that resolving these two types of mismatches is essential for achieving general O2O RL.
* We develop two techniques to address these mismatches. Policy re-evaluation aims to achieve optimistic Q-value estimates, preventing instability in Q-value estimation. Value alignment calibrates the critic to align with the policy, ensuring consistency between action probabilities and their corresponding Q-values.
* We introduce a constrained fine-tuning framework that incorporates a regularization term into the policy objective, combating the inevitable distribution shift and ensuring stable and optimal performance when fine-tuning the policy in online environments.

## 2 Preliminaries

In this section, we introduce necessary preliminaries about RL, including Markov decision process, and three target online RL methods.

Markov Decision Process (MDP)A Markov Decision Process \(\) is defined by the tuple (_S,A,R,P,\(\),\(\)_) , where \(S\) is the state space, \(A\) is the action space, \(P:S A(S)\) is the transition function, \(R:S A\) is the reward function, \(\) is the initial state distribution, and \(\) is a discount factor. The goal is to learn a policy that maximizes the expected return as

\[J()=_{}[_{t=0}^{}^{t}r_{ t}]=_{(s_{0},a_{0})}[Q^{}(s_{0},a_{0}) ],(s_{0},a_{0})(s_{0},a_{0}(|s_{0 })).\] (1)

**Online RL** The three most commonly used online RL algorithms are SAC , TD3 , and PPO . Below, we will introduce these three methods one by one.

**SAC** first introduces entropy maximization into the RL scenarios and updates the actor and the critic by minimizing the following objectives:

\[_{}^{}()=}_{s R}}_{_{}(|s )}[_{}(a|s)-Q_{}(s,a) ],\] (2) \[_{Q}^{}(_{i})=}_{(s,a,r,s^{}) R}[(Q_{_{i}}(s,a)-y(r,s^{}))^{2}],\] (3) \[y(r,s^{})=r+_{a^{} _{}(|s^{})}[_{i=1,2}Q_{_{i}} (s,a)-_{}(a^{}|s^{}) ],\] \[_{}^{}()=- }_{s R}}_{_{} (|s)}[_{}(a|s)-} ],\] (4)

where \(>0\), \(_{i}\) are the parameters of the target \(Q\) network, and \(}\) is the target entropy.

**TD3** models a deterministic policy and uses tricks, including clipped double Q-learning and policy smoothing, to address the issue of function approximation error. The deterministic policy gradient used to update policy is defined as

\[_{}J_{}^{}()=}_{s R}[_{a}Q_{}^{}(s,a)|_{a= (s)}_{}_{}(s)].\] (5)

The objective function of updating the critic is defined as

\[_{Q}^{}(_{i})=}_{(s,a,r,s^{}) R}[(Q_{_{i}}( s,a)-y(r,s^{}))^{2}].\] (6)

Here, \(y(r,s^{})=r+_{i=1,2}Q_{_{i}}(s^{ },)\), where \(=_{}(s^{})+\) and \((N(0,),-c,c)\).

**PPO** provides a simple implementation for TRPO , which updates the actor and the critic by minimizing the following objective functions, respectively,

\[_{}^{}()=-}_{s^{_{_{k}}}}}_{a_{ _{k}}(|s)}r()A^{_{ _{k}}}(s,a),(r(),1-,1+ )A^{_{_{k}}}(s,a),\] (7) \[_{V}^{}()=} _{(s,r,s^{}) R}[(A^{_{_{k}}}(s,a )+V_{}(s^{}))-V_{}(s) )^{2}],\] (8)

where \(r()=_{}/_{_{k}}\) and the advantage \(A^{_{_{k}}}\) is computed by GAE .

It is noteworthy that previous works mostly adopted SAC and TD3 for online fine-tuning as they are off-policy methods. In our work, given the widespread use of PPO in online RL, we also employ it for fine-tuning though it is an on-policy method.

## 3 Evaluation and Improvement Mismatches

In this section, we focus on the differences between online and offline RL and study how these differences impact the performance of online fine-tuning. In general, we summarize these differences as two types of mismatches, _evaluation mismatch_ and _improvement mismatch_. The former arises from the changes in policy evaluation functions during the transition from offline to online environments; while the latter represents the inconsistency in the objectives for policy updates between offline and online RL. Most offline RL methods suffer from one or both of these issues, which underscores the importance of addressing these mismatches to achieve stable and effective online fine-tuning.

Evaluation mismatch often occurs in the value regularization methods. For example, in CQL , a representative offline method, the policy evaluation function transitions from a pessimistic estimation inherent to offline learning to a more optimistic estimation during online training. This shift frequently results in a sharp increase in Q-values at the beginning of online fine-tuning, which can hinder stable performance improvements. To address this problem, several attempts have been made to mitigate the excessive underestimation of out-of-the-distribution (OOD) actions during offline learning [33; 27]or to initialize a pessimistic Q-ensemble to maintain pessimism during online fine-tuning . Unfortunately, these approaches are predominantly tailored to the transition from CQL to SAC, limiting their applicability to other offline algorithms.

Improvement mismatch is commonly found in policy constraint methods. Typical examples include TD3+BC  and AWAC , where the objective of actor updates differs significantly from typical online methods. In these models, updates to the actor are not solely reliant on the critic's evaluation. Consequently, actions that have high Q-values may not automatically translate to high probabilities of being selected, and vice versa. This divergence often misguides the update of policy at the beginning stage of online fine-tuning, resulting in unfavourable performance.

Besides, One-step RL and non-iterative methods, _e.g._, IQL, exhibit both evaluation and improvement mismatches. During the policy evaluation stage, these methods estimate the Q-values based on the behavior policy or an unknown policy  instead of the target policy as in online methods; during the improvement stage, because they impose additional constraints on policy updates, they also encounter the same issue as discussed above. This can lead to discrepancies in both the assessment of action values and the subsequent policy optimization, making it hard to achieve effective policy improvement.

Thanks to the recent work , which presents a unified framework for understanding offline RL, we bridge these two mismatches for general RL-based offline algorithms. Formally, the offline RL problem can be defined by the _behavior-regularized_ MDP problem [47; 13] via maximizing the following objective:

\[_{}[_{t=0}^{}^{t}(r(s_{t},a_ {t})- f(|s_{t})}{(a_{t}|s_{t})})) ]=_{(s_{0},a_{0})}[Q^{}(s_{0},a_{0}) - f(|s_{0})}{(a_{0}|s_{0} )})],\] (9)

where \(f()\) is a regularization function and \(\) is the behavior policy.

From Eq. (1) and Eq. (9), we observe a significant divergence in the relation between the actor and the critic in online versus offline scenarios. Unlike in online cases where the policy update is solely dependent on the Q-function, in offline scenarios, it also critically depends on the data distribution, as indicated by the regularization term in Eq. (9). This distinction highlights why offline methods often face evaluation and improvement mismatches when applied to online fine-tuning. Using offline actor and critic trained by Eq. (9) for initialization in online fine-tuning introduces both types of mismatches, resulting in unstable and inefficient updates.

## 4 Method

In this section, we introduce our proposed O2O method for handling the mismatches discussed in Section 3 and the distribution shift problem. To address the pessimistic or unreliable evaluation, such as in CQL and IQL, we develop a policy re-evaluation technique. This technique optimistically re-evaluates the well-trained offline policy using an off-policy evaluation method. Although this re-evaluation helps the critic achieve more optimistic Q-value estimates, unavoidable factors such as function approximation errors and partial data coverage can still lead to a misalignment between the critic and the offline policy. This misalignment means that the action with the highest probability predicted by the policy does not necessarily have the highest Q-value, leading to what we have termed improvement mismatch.

As discussed in in Section 3, both the value regularization (after re-evaluation) and policy constraint methods exhibit improvement mismatch, though the reasons for the mismatch differ between these two approaches. To address this issue, we propose value alignment, which aims to align the critic's estimates with the policy's action probabilities, effectively tackling the improvement mismatch in both types of methods. Finally, to deal with the inevitable distribution shift between offline and online environments, we develop a constrained fine-tuning framework. This framework ensures that the policy consistently updates in the optimal direction by incorporating a regularization term into the policy objective.

We propose methods for various representative online RL algorithms within a unified framework, including SAC , TD3 , and PPO . These algorithms represent the mainstream approaches in online RL, and are targeted respectively for the off-policy approach with stochastic policies, the off-policy approach with deterministic policies, and the on-policy approach.

### Policy Re-evaluation

The critic trained on an offline dataset typically maintains pessimistic estimates of Q-values. When using online evaluation methods to fine-tune this critic without any value regularization, the Q-values can experience a dramatic jump, especially for OOD actions, leading to inaccurate Q-value estimations. To mitigate this problem, we propose re-evaluating the offline policy to acquire a new critic by employing an off-policy evaluation (OPE) method. The goal is to enable the critic to have optimistic estimates of Q-values that more closely approximate the true values.

However, directly applying OPE methods for policy evaluation on offline datasets often leads to large extrapolation errors, as discussed in the previous work . These errors arise due to absent data and training mismatches. Thanks to the pessimism in offline RL, it is reasonable to assume that a well-trained policy is close enough to the behavior policy or even captures the support set of the behavior policy. A common assumption is single-policy concentrability [45; 34], which demonstrates how concentrated a learned policy is within the given dataset and can be defined as follows.

**Assumption 4.1**.: _(\(_{}\)-concentrability ) The behavior policy \(\) and learned policy \(_{}\) satisfy_

\[_{(s,a) S A}}(s,a)}{d^{} (s,a)} C.\]

where \(d^{_{}}(s,a)\) is the occupancy measure of \(_{}\) and \(C\) is a constant. Single-policy concentrability measures the degree to which the state-action distribution induced by the learned policy is covered by the dataset used for training. By ensuring that the learned policy does not deviate significantly from the behavior policy, the extrapolation error can be greatly reduced [22; 29]. When using a representative OPE method called fitted Q-evaluation (FQE), based on Assumption 4.1 and Theorem 4.2 in , the upper bound of extrapolation error can be obtained.

**Corollary 4.2**.: _Under Assumption 4.1, by denoting Q-value function class as \(\), for \((0,1)\), after \(K\) iterations of FQE on the dataset \(\), with probability \(1-\), we have:_

\[|Q^{}-^{}|}{1-}+ ^{K},:=^{2}(||/)}{| |}+20d_{F}^{}.\]

where \(d_{F}^{}\) is inherent Bellman evaluation error (Definition 4.1 in ), and \(\) is the maximum of \(V\), which can be bounded by \(R_{max}/(1-)\).

With a powerful neural network and sufficient data, the inherent Bellman evaluation error could be tiny. Accordingly, with a large training step \(K\), the error will be bounded by an acceptable value. This implies that, given sufficient data, one can achieve a critic with optimistic property and minor extrapolation error through policy re-evaluation. In practical implementation, for off-policy methods SAC and TD3, we can directly use Eq. (3) and Eq. (6) to re-evaluate the offline policy. For the on-policy method PPO, we train a critic by fitting the returns of offline trajectories. Considering that the critic can only approximate \(V^{}(s)\) rather than the true value function, we propose a regularization term in Section 4.2, which ensures the critic's estimates are reliable and conducive to effective policy improvement.

### Value Alignment

Although the critic after policy re-evaluation possesses the optimistic property needed in the online environment, it often does not align well with the offline policy due to factors such as function approximation errors, generalization errors of neural networks, and partial data coverage. Moreover, as discussed in Section 3, policy constraint offline methods also suffer from misalignment between the critic and policy. The misalignment means that the action with the highest Q-value does not necessarily have the highest probability, often leading to misleading updates of the policy. To verify this observation, we trained the policy using SAC and TD3

Figure 1: The results of actors updated with different critics.

with three different critics: the fixed re-evaluated critic, the iterative re-evaluated critic (which updates with the policy), and our aligned critic. From Figure 1, the performance of re-evaluated critics sharply declines at the initial stage and does not recover in the subsequent training; while our aligned critic achieves stable and favorable performance. These results indicates that the misalignment between the re-evaluated critic and the offline policy can make it difficult for the policy to optimize in a correct direction, leading to an irreversible decline in performance. Given that the well-trained offline policy is reliable, the desirable critic should not only have optimistic Q-value estimates but also maintain alignment with the offline policy. To achieve this, we propose performing value alignment to calibrate the critic. The main idea is to use the Q-values of the offline policy actions as anchors, keeping them unaltered, and to suppress any overestimated Q-value that exceed these anchors, thereby aligning the critic with the offline policy. Below, we will discuss how to implement value alignment for different online methods.

O2sacRecall that in SAC , the optimal policy is defined as

\[(a|s)=(Q(s,a))/(V(s))\] (10)

With a simple transformation of Eq. (10), we have

\[Q(s,a)=V(s)+(a|s)\] (11)

One intuition behind our method is that actions with higher probabilities typically have more accurate Q-value estimates because these actions are closer to the dataset, and there is sufficient data nearby to obtain a precise estimate. This motivates us to use the Q-value \(Q_{}(s,)\) of the optimal action \(\) to calibrate any other overestimated action. Assuming that \(_{}\) is the offline policy, we perform value alignment for any state-action pair \((s,a)\) as follows:

\[Q^{}_{}(s,a)=(Q_{}(s,)-(_ {}(|s)-_{}(a|s)),Q_{}(s,a)),\] (12)

where \((,)\) operator is used to maintain the Q-values of actions that are not overestimated and consistent with OPE results.

Formally, we define the objective function of value alignment as follows:

\[_{Q}^{}(_{i}) =}_{s R,a(|s)}[(Q_{ _{i}}(s,a)-Q^{}_{}(s,a))^{2}]\] (13) \[_{Q}^{}(_{i}) =}_{s R,}[(Q_{_{i}}(s, )-Q_{}(s,))^{2}|_{= _{}(s)}]\] \[_{Q}^{}(_{i}) =_{Q}^{}(_{i})+ _{Q}^{}(_{i})\]

Considering that the overall Q-values have decreased, to maintain an optimistic estimate, we use the regularization term \(_{Q}^{}\) to prevent the underestimation of Q-values.

We derive Proposition 4.3 to show that the aligned state values \(V_{}(s)\) can be maintained within an appropriate range, meaning that the estimates remain optimistic while avoiding overestimation.

**Proposition 4.3**.: _After the value alignment process of Eq. (13), the state value function \(V_{}(s)\) satisfies_

\[V_{}(s) V_{}(s) V_{}(s)\] (14)

_where \(Q_{}(s,a)\) are the low Q-values after policy re-evaluation, which do not require calibration, \(V_{}(s)\!=\!Q_{}(s,a)\!-\!(a|s)\) and \(V_{}(s)\!=\!Q(s,)\!-\!(|s)\)._

During value alignment, we update the policy with Eq. (2) simultaneously for sampling the overestimated actions. The whole process iterates Eq. (2) and Eq. (13) to obtain a policy that performs equivalently to the offline policy and a critic aligned with it. We denote the policy as \(_{}\) and the aligned critic as \(Q_{}\) for sequential training. Note that \(Q_{}\), which is modified from the critic obtained in the policy re-evaluation, does not depend on specific offline critics. This flexibility allows us to implement the transition to SAC from different offline algorithms.

O2td3As done in the case of O2SAC, we can use the Q-values of the policy actions \(\) to maintain the optimistic property of policy re-evaluation and calibrate the Q-values of other actions.

Unfortunately, in TD3, the actor is modeled as a deterministic policy, lacking an explicit expression for Q-values, which prevents us from directly aligning Q-values with the policy.

To solve this problem, our main idea is to model the distribution of Q-values around the policy action \(\) as a Gaussian distribution. Specifically, in Eq. (5), when we use the offline policy as \(_{}\), the gradient of the policy is only related to the the gradient of \(Q(s,a)\) with respect to \(a\). It is easy to see that the gradient around \(\) should tend to 0 as \(\) is the optimal action selected by the offline policy. Due to the use of smoothing regularization in the critic's update in TD3, the Q-values of actions near \(\) differ only slightly from the Q-value of \(\) itself. This enables us to assume that normalized Q-values around \(\) follow a Gaussian distribution \(Q(s,a)/Q(s,) N(,)\). Formally, we can calibrate the Q-values of other actions as (see Appendix G.2 for detailed derivation)

\[Q^{}(s,a)=(Q_{}(s,),)}{1+k (d(a,)^{2},^{2})})\] (15)

where \(k\) is a constant, which is set as 1 across all tasks, and \(d(a,)\) is a distance measure, which is defined as the euclidean distance divided by the square root of the action dimension in our implementation.

From Eq. (15), after calibration, the Q-values of the actions around \(\) are only slightly lower than \(Q(s,)\), which ensures that \(\) is the output action of the policy while maintaining a smoothing and optimistic property of Q-values. Moreover, for the actions that differ greatly from \(\), we limit the maximum of the distance measure \(d(a,)\) in Eq. (15) to the policy noise used in Eq. (6) to avoid severe underestimation of their Q-values. Formally, we define the objective loss of value alignment for O2TD3 as

\[_{Q}^{}(_{i} )&=}_{s R}[(Q_{_{i}}( s,)-Q^{}_{}(s,))^{2} ]_{=(s)+}]\\ _{Q}^{}(_{i})& =}_{s R}[(Q_{_{i}}(s, )-Q_{}(s,))^{2} _{=_{}(s)}]\\ _{Q}^{}(_{i})& =_{Q}^{}(_{i})+_{Q}^{ }(_{i})\] (16)

where \(Q^{}_{}(s,)=(Q_{}(s,),Q^{ }_{}(s,))\) and \(\) is a perturbed action defined in Eq (6).

O2ppoIn PPO, only the critic \(V(s)\) is used to estimate the advantages for policy update. During the re-evaluation process, we train a critic by fitting the returns of offline trajectories as mentioned in Section 4.1. This indicates that the re-evaluated critic only approximate \(V^{}(s)\) instead of the true one, misguiding the update of the policy. To mitigate this problem, we propose an auxiliary advantage function to correct erroneous updates.

Generally, a desirable auxiliary advantage function should satisfy the following two conditions: 1) it enables the policy to update in a reliable region; 2) its value must be zero at the beginning of online fine-tuning to enable the policy to transition smoothly from offline to online. Considering that the well-trained offline policy is reliable, we define the auxiliary advantage function as

\[A_{}(s,a)=_{}(a|s)+(_{ }(|s))\] (17)

where \(\) is the entropy of action probabilities predicted by \(_{}\). It is easy to verify the second condition that \(A_{}(s,a)=0\) at the beginning of offline fine-tuning. To verify the first condition, we derive the following proposition. Its proof can be found in Appendix F.

**Proposition 4.4**.: _With \(A_{}(s,a)\) in Eq. (17), the policy update is regularized by the cross-entropy loss about the offline policy, thereby constraining the policy update in a reliable region._

Accordingly, we define the advantage function for policy update as

\[A^{}(s,a)=A(s,a)+ A_{}(s,a)\] (18)

where \(\) anneals to 0 from 1. With the auxiliary advantage, Eq. (18) prevent the update direction from deviating too far from the offline policy.

### Constrained Fine-Tuning

In the previous subsections, we discussed how to address the mismatch issues in the O2O problems. However, due to the distribution shift between the offline dataset and the online environment, along with the optimism in online RL, encountering out-of-distribution (OOD) states and actions becomes inevitable, potentially leading to significant performance fluctuations. Especially for OOD states that are absent in the offline dataset, even though the policy was trained well in the offline phase, it may still fail to output favourable actions, thereby causing erroneous policy update.

Considering that the critic maintains an optimistic nature after undergoing policy re-evaluation and value alignment, with the optimistic update way during online fine-tuning, it typically overestimates the Q-values of OOD state-action pairs, which can mislead the update of the policy. Inspired of CMDP , we develop constrained fine-tuning (CFT) to introduce a regularization term to constrain the current actor and critic, which prevents the policy update from being severely misguided.

Specifically, we impose a constraint term \(f(,_{})\) on the policy objective to ensure that it updates within the credible region of \(_{}\), where \(f(,)\) is a divergence measurement and \(_{}\) is the optimal historical policy during online evaluations. Formally, we define the policy objective of CFT as

\[_{}[_{t=0}^{}_{t}r_{t}(s_{t},a_{t})] \ _{}[f((a_{t}|s_{t}),_{}(a_{t}|s_{t}))]<\] (19)

By incorporating the constraint term into the reward function akin to RCPO , we can solve the problem by minimizing the following objective functions with initializing \(_{}\) as \(_{}\) obtained in Section 4.2 at the beginning of online fine-tuning.

\[& L_{}()=_{_{}}[Q_{} ^{_{}}(s)- f(_{}(a|s),_{}(a|s))]\\ & L_{Q}()=_{(s,a,r,s^{}) R}[(Q_{}^{ _{}}(s,a)-y)^{2}]\\ & y=r+_{^{}_{}(|s^ {})}[Q_{}^{_{}}(s^{},a^{})- f( _{}(a^{}|s^{}),_{}(a^{}|s^{}))] \\ & L()=_{ 0}-[_{_{ }}(f(_{}(a|s),_{}(a|s)))-]\] (20)

We provide a theoretical guarantee for the proposed CFT framework.

**Corollary 4.5**.: _With the penalty \(f(,_{})\) defined before and appropriate learning rates, algorithm of Eq. (20) almost surely to a fixed point \((^{},^{},^{})\), where \(^{}=0\), \(^{}\) and \(^{}\) are corresponded to \(^{}\) and \(Q^{}\), which are optimal in the MDP without constraint._

In our implementation, we use KL divergence and MSE function as \(f(,)\) for the transitions of O2SAC and O2TD3, respectively. For O2PPO, the auxiliary advantage function already has the

Figure 2: Performance curves on D4RL  MuJoCo locomotion tasks during online fine-tuning.

ability to constrain the policy update, we only need to replace \(_{}\) with \(_{}\) during online fine-tuning. Note that in our methods, at the beginning of online fine-tuning, the regularization function \(f(_{}(a|s),_{}(a|s))\) is zero for any state, which guarantees no destruction of the alignment for the actor and the critic obtained in Section 4.2.

## 5 Experiments

In this section, we perform experiments to validate the effectiveness of the proposed method on D4RL  MuJoCo and AntMaze tasks, including HalfCheetah, Hopper, Walker2d and AntMaze environments. Specifically, we compare our methods with AWAC , IQL , PEX , Off2On , Cal-QL  and ACA . For all methods except for O2PPO, we run 100,000 interaction steps with the environment and evaluate the policy per 1000 steps, as they all use off-policy methods for online fine-tuning. For our O2PPO method, due to the low efficiency of the on-policy method, we run 250,000 interaction steps to validate its performance, with 2500 steps as the evaluation interval. We run all methods with five random seeds and report their averaging results. Due to the space limitation, more experimental results can be found in Appendix B and C.

### Comparison with State-of-the-Art Methods

We respectively initialize the policies of O2SAC, O2TD3 and O2PPO from the results of CQL, TD3+BC and IQL. Figure 2 shows the performance curves of our methods and comparing methods on Mujoco tasks. Similar to the previous works [48; 5], for O2SAC and O2TD3, we use CQL and TD3+BC for offline policy pre-training. From the figure, we can see that our method can converge more stably and rapidly than other methods and achieve the optimal performance in most cases. Although the ensemble method Off2On can achieve better final performance than our methods in some cases, it often suffers from a dramatic drop in performance at the beginning of fine-tuning, which is often unacceptable in the O2O problems. We report the results of O2PPO in Appendix A because of its different number of interaction steps. Although PPO is an on-policy method with low efficiency, from the results in Table 1, it shows significant superiority on sparse reward tasks, even though with equal interactions.

### Study on Transferability

In this section, we perform experiments to verify the powerful transferability of the proposed method. As mentioned earlier, one of the advantages of our method is that it imposes no requirements on offline

   Dataset & IQL & PEX & Cal-QL & O2TD3 & O2SAC & O2PPO \\  U-v2 & 80.8\(\)80.8 & 85.0\(\)96.2 & 80.8\(\)97.0 & 92.8\(\)95.8 & 92.8\(\)93.6 & 77.3\(\)98.0 \\ U-D-v2 & 56.6\(\)35.8 & 12.6\(\)16.0 & 23.8\(\)71.2 & 38.4\(\)52.2 & 43.8\(\)79.8 & 56.4\(\)86.3 \\  total & 137.4\(\)116.6 & 97.6\(\)112.2 & 104.6\(\)168.2 & 128.6\(\)142.0 & 131.2\(\)148.0 & 133.7\(\)184.3 \\   

Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=unaze, D=diverse)

Figure 3: The fine-tuning performance achieved by transferring to three online algorithms from their heterogeneous offline algorithms.

algorithms. This means we can achieve the transfer from any offline RL algorithm to three online RL algorithms. Figure 3 illustrates the fine-tuning performance of three online algorithms transferring from their heterogeneous offline algorithms. From Figure 3(a), pre-trained with TD3+BC, O2SAC outperforms SAC trained from scratch with a larger margin. Although Online Decision Transformer (ODT) achieves favourable performance in the offline environment, it converges slowly during online fine-tuning due to the architecture of the transformer. Our O2TD3 and O2PPO significantly enhances its performance through online fine-tuning. These results convincingly verify that our method show the strong transferability from various offline methods.

## 6 Conclusion

In this paper, we disclose there exist two types of mismatches when online fine-tuning offline RL methods. To address these two mismatches in O2O RL, we proposed optimistic critic reconstruction to re-evaluate an optimistic critic and align it with the offline actor before online fine-tuning, ensuring the stable performance improvement at the beginning stage and potentially better efficiency due to the reconstructed optimism consistent with online RL. Furthermore, to combat the inevitable distribution shift that can hinder the stable performance improvement, we introduce constrained fine-tuning to constrain the divergence of current policy and the best foregoing policy to maintain the stability of online fine-tuning. These two components form a versatile O2O framework, allowing the transition from any offline algorithms to three state-of-the-art online algorithms. Experiments show our framework can converge to optimal performance without affecting the aligned critic at the beginning of online fine-tuning and achieve strong empirical performance.