# Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?

Sravanti Addepalli

Google DeepMind

Varun Yerram

Google DeepMind

Arun Suggala

Google DeepMind

Karthikeyan Shanmugam

Google DeepMind

Correspondence to: sravantia@google.com, karthikeyanvs@google.com

Prateek Jain

Google DeepMind

###### Abstract

Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it inevitable to find _adversarial_ prompts that can jailbreak these models, we aim to evaluate whether safety fine-tuned LLMs are safe against _natural_ prompts which are semantically related to toxic seed prompts that elicit safe responses after alignment. We surprisingly find that popular aligned LLMs such as GPT-4 can be compromised using naive prompts that are NOT even crafted with an objective of jailbreaking the model. Furthermore, we empirically show that given a seed prompt that elicits a toxic response from an unaligned model, one can systematically generate several semantically related _natural_ prompts that can jailbreak aligned LLMs. Towards this, we propose a method of _Response Guided Question Augmentation (ReG-QA)_ to evaluate the generalization of safety aligned LLMs to natural prompts, that first generates several toxic answers given a seed question using an unaligned LLM (Q to A), and further leverages an LLM to generate questions that are likely to produce these answers (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak _questions_ from unsafe content (without denial) and can thus be used for the latter (A to Q) step. We obtain attack success rates that are comparable to/ better than leading adversarial attack methods on the JailbreakBench leaderboard, while being significantly more stable against defenses such as Smooth-LLM and Synonym Substitution, which are effective against existing all attacks on the leaderboard.

## 1 Introduction

Large Language Models (LLMs) are trained on massive web-scale data, and are thus exposed to diverse forms of objectionable content during pre-training. To prevent these models from exhibiting undesirable behavior, the generation of toxic content is _suppressed_ using alignment techniques such as reinforcement learning via human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022), instruction tuning (Wiei et al., 2021; Ouyang et al., 2022) and safety filters (Inan et al., 2023; Zeng et al., 2024; Han et al., 2024). However, recent research has revealed that these techniques can be circumvented by adversarial attacks (Carlini et al., 2023; Zou et al., 2023) and handcrafted jailbreaks (Shen et al., 2023; Wei et al., 2023), which are specifically designed to circumvent the safety mechanisms in aligned models. This raises concerns about the robustness of aligned LLMs, and brings up a crucial question: _how robust are aligned LLMs to natural, in-distribution prompts,_which are likely to be encountered during typical usage?_ Understanding this is essential for developing better safety training strategies and for accurately characterizing the real-world safety of deployed models.

To answer this question, we aim to design natural prompts that are semantically related to a given toxic seed prompt. Surprisingly, we find that aligned LLMs such as GPT-4 (OpenAI, 2023b), are brittle against natural prompts generated by simply paraphrasing toxic questions using LLMs. This indicates that current safety mechanisms may be overly reliant on surface-level features of the input, rather than a deeper understanding of intent. Furthermore, we propose _Response Guided Question Augmentation (ReG-QA)_ to systematically evaluate the in-distribution generalization of LLMs after safety fine-tuning, by generating a diverse set of prompts semantically related to a given toxic seed prompt. We achieve this by traversing from a single seed question to diverse answers (Q to A), and then projecting these answers back into a multitude of related questions (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak _questions_ from unsafe content (without denial) and can thus be used for the latter A to Q step. This process incorporates details from the answers into the questions, providing subtle cues that increase the likelihood of eliciting a toxic response. While existing jailbreak approaches often rely on optimization techniques (Zou et al., 2023; Carlini et al., 2023; Liu et al., 2023; Andriushchenko et al., 2024; Sitawarin et al., 2024), or specialized prompting techniques that elicit LLMs to produce _jailbreaks_(Zeng et al., 2024b; Takemoto, 2024), potentially leading to unnatural distribution shifts and biases in the generated prompts, our method ensures the generation of in-distribution and _natural prompts_ by NOT incorporating the jailbreaking objective for generating these question augmentations.

We empirically demonstrate that _ReG-QA_ not only improves the diversity of the generated questions but is also highly effective in bypassing safety mechanisms. In particular, using ReG-QA, we obtain an attack success rate (ASR) of \(82\%\) on GPT-4 and \(93\%\) on GPT-3.5, which is comparable to/better than leading adversarial attack methods on JailbreakBench. We list our contributions below:

* We identify specific failure modes of aligned LLMs: (i) brittleness to paraphrases of toxic questions, ii) sensitivity to cues from the answer embedded in the prompt, and (iii) the ability to generate jailbreak questions when provided with toxic answers, indicating an asymmetry in safety training (forward safety training does not lead to reverse safety).
* We propose ReG-QA, a novel question augmentation method for generating diverse and natural prompts related to a given seed question. This method enables a comprehensive assessment of LLM robustness by systematically exploring the semantic space around the seed prompt.
* We achieve state-of-the-art attack success rates on JailbreakBench using ReG-QA, both with and without incorporating leading defenses such as Smooth-LLM (Robey et al., 2023) and Synonym Substitution (Speer, 2022), which are shown to be very effective against leading attacks on the leaderboard. Our method serves as an adaptive attack against defenses that utilize the non-naturalness and instability of existing jailbreaks to defend against them, motivating the need for developing more robust defenses and safety training methods.

## 2 Related Work

Large Language Models (like other Neural Networks) are susceptible to adversarial attacks (or jailbreaks) that are designed to circumvent their safeguards, thereby inducing the generation of objectionable content. Initial works on LLM jailbreaks have focused on designing handcrafted prompts to elicit undesirable responses (walkerspider, 2022; Yuan et al., 2023; Shen et al., 2023; Wei et al., 2023). While such manual methods are crucial to identify and demonstrate vulnerabilities, they are neither scalable, nor sufficiently comprehensive, to robustly evaluate evolving versions of models which can be trained on such publicly accessible jailbreaks. Another line of work employs white-box optimization techniques (requiring access to model weights) such as gradient ascent to generate prompts that trigger unsafe outputs (Carlini et al., 2023; Zou et al., 2023). A key weakness of these techniques is that the resulting prompts often appear nonsensical and unnatural, and can thus be easily detected based on the presence of such high perplexity tokens (Jain et al., 2023; Alon and Kamfonas, 2023).

The drawbacks related to both manually crafted jailbreaks and white-box attacks have led to greater focus on automated generation of natural language jailbreaks. Liu et al. (2023) propose hierarchical genetic algorithms to generate stealthy jailbreaks using existing handcrafted jailbreaks as prototypes to reduce the search space. Shah et al. (2023) generate prompts that instruct the LLM to take on a persona, conditioned on which the LLM is more willing to elicit harmful content. Zeng et al. (2024) explore persuasive adversarial prompts where a persuasive argument surrounding a harmful instruction jailbreaks LLMs. With black-box access and a safety judge in the loop, Takemoto (2024) adversarially paraphrases a seed prompt until it jailbreaks the target LLM. Prompt Automatic Iterative Refinement (PAIR) (Chao et al. (2023)) uses an attacker LLM to iteratively refine and generate jailbreaks against a target LLM. Andriushchenko et al. (2024) used random search based attacks to maximize log probability with respect to a given target undesirable answer. Tree-of-thought reasoning is employed by Mehrotra et al. (2023) with blackbox access to the LLM to iteratively refine prompts that lead to jailbreaks. Lu et al. (2024) provide a framework for understanding various attacks and defenses, exploring ensemble attacks and defenses.

Most of these methods either i) optimize an adversarial loss iteratively by querying the target model with black-box (or white-box) access, or, ii) prompt an LLM to generate a specific pattern of jailbreaks (such as persona modulation) that can trick the target model. In contrast, our method expands the scope of a seed question within the training distribution of natural prompts by using an unaligned LLM to firstly generate answers from the seed question, and further project these answers back to the question space using another LLM. We show that current day safety aligned LLMs can be jailbroken even by generating such prompts that do not have a stealthy intent of jailbreaking, indicating the poor generalization of safety training. Different from prior works, we do not require black-box access of the target model (or any other model) to iteratively optimize our prompts.

Wei et al. (2024) show that jailbreaks occur because of i) opposing objectives between the model's instruction following ability and the safety mandates, or ii) the distribution shift between safety prompts during training and test time prompts. Our method highlights that safety fine-tuned LLMs can be broken even with minor distribution shifts in prompts used for safety training. Several defenses have been proposed to improve the robustness of LLMs to jailbreaks. One of the methods to defend against attacks that append gibberish tokens without semantic constraints, was the perplexity based filtering (Jain et al., 2023; Alon and Kamfonas, 2023). However this was shown to be weak against natural language attacks. Kumar et al. (2024) propose a method of _Erase and Check_, where some tokens are gradually erased, and certificates are obtained by checking whether the resulting prompts also break a judge. Robey et al. (2023) propose to smoothen the outputs of LLMs by adding random perturbations to the prompt and checking if the ensemble has good attack rates. Two simple defenses - Synonym substitution and Removal of non-dictionary words (Speer, 2022) are seen to be effective against several attacks on JailbreaksBench. Overall, defenses against jailbreaks mainly try to exploit the deviation of adversarial prompts or jailbreaks from the distribution of natural prompts, and their brittleness to mild perturbations in the prompt. This gives a natural advantage to the jailbreaks generated using our proposed method, which are hard to distinguish from natural prompts. Further, our results highlight that the generated jailbreaks are significantly more stable to both random and semantically meaningful perturbations when compared to existing attacks.

## 3 Background and Motivation

In Figure 1, we categorize the landscape of jailbreaks into different regions based on the distribution they belong to. R4 broadly represents the region of all possible text which may/ may not have semantic meaning, R3 is the subset of this containing semantically meaningful text. We consider R2 as the pre-training data distribution, with R0 being a subset which is used for safety fine-tuning. We note that R0 may not always be a subset of R2. As shown in Fig.1, standard gradient based adversarial attacks such as A4 produce text without any semantic meaning, and are thus very easy to detect using perplexity based thresholding methods (Jain et al., 2023; Alon and Kamfonas, 2023). Attacks such as A3 incorporate the objective of generating natural language jailbreaks (Liu et al., 2023; Shah et al., 2023; Zeng et al., 2024; Takemoto, 2024; Chao et al., 2023), and thus circumvent such simple defenses. While these attacks lie within the distribution of semantically meaningful text (R3), they are still far from the distribution of natural text (R2), since they are crafted to optimize a certain objective, or by prompting LLMs directly or indirectly to produce stealthy prompts. Similarly, although handcrafted jailbreaks (walkerspider, 2022; Yuan et al., 2023; Shen et al., 2023,Wei et al., 2023) also contain well-formed sentences, they again lie in far from the distribution of natural text, since they are deliberately crafted with an intention of jailbreaking the LLM. Thus, existing works show that it is very easy for an _adversarial_ player to jailbreak an LLM. Contrary to this, we aim to understand the robustness of LLMs to prompts that belong to the distribution of natural data (R2). We note that the training data distribution inherently captures the diversity present in web scale data, and also represents the vast variety of user prompts that can be expected during inference, thus serving as a proxy to the distribution of natural prompts. We therefore aim to characterize how well aligned LLMs generalize to prompts that lie within the distribution of training data. We thus propose a method for generating such natural prompts that are diverse and related to a seed question.

## 4 Threat Model

In this work, we consider the generation of in-distribution, natural jailbreak prompts related to a given seed prompt \(x\). While prior works mostly focus on naturalness of language, we constraint the threat model further by additionally considering naturalness of content as well. We thus define our threat model as the set of prompts which are naturally occurring with respect to the training distribution of LLMs, and denote it as \(_{}\). To formalize the constraint on similarity with respect to the seed prompt, we use an embedding model (for example, a sentence embedding model like sentenceBERT (Reimers, 2019) or Gecko (Lee et al., 2024)) denoted as an encoder Enc. We consider a perturbation radius \(\) in the embedding space with respect to the seed prompt \(x\) under the distance metric related to cosine similarity given by \(d(x^{},x)=1-((x),(x^{}))\) where \((a,b)\) denotes the cosine similarity between the embedding vectors \(a\) and \(b\). Thus, we define the threat model \(_{x,}\) as:

\[_{x,}=\{x^{}:x^{}_{ }(1-((x),(x^{})))<\}\] (1)

The attack success criteria of prompts within the threat model is measured using a classification model (or judge) \(_{}\), which operates on the generated jailbreak \(x^{}\), the response of the target LLM \(y\) and a system prompt \(S\), to generate an output of 0 (safe) or 1 (unsafe). We note that relying solely on embedding similarity can be susceptible to adaptive adversarial attacks, where an adversary could craft prompts that appear close in embedding space but deviate significantly in semantic meaning. To mitigate this, our proposed jailbreak generation pipeline remains independent of the

Figure 1: **Schematic diagram of data distributions highlighting different types of jailbreak questions:** Let R4 denote the space of all text which may or may not have semantic meaning, R3 denote a subset of R4 containing text with semantic meaning, R2 denote the pre-training data distribution, and R0 denote the fine-tuning data distribution, with R1 being the region close to the fine-tuning data distribution. Note that R0 may not always be a subset of R2. R0 is considered to be the region of safe questions, since the LLM is trained to give safe responses using SFT/RLHF based safety fine-tuning. We depict different methods of modifying a safe seed question within R0 so as to obtain a toxic response. While prompts close to R0 have strict constraints on naturalness of meaning and content, and are thus considered to be safer by virtue of generalization of safety training, prompts closer to R4 can be constructed to overcome the underlying safety mechanism.

specific embedding model used for defining the perturbation bound. Further, our approach uses benign and simple prompts - specifically, we neither instruct the attack generation LLM (directly or indirectly) to adopt any jailbreaking strategy, nor do we perform any form of optimization to increase attack success rate. While such strategies can increase attack success rate of our method as well, our restrictions ensure that the generated jailbreaks are closer to the distribution of natural text.

## 5 Proposed Method

In this section, we first present our algorithm for Question Augmentation, and further discuss implementation details of the same.

### Generation of Question Augmentations

We first discuss how publicly accessible safety aligned LLMs (with only API access) and an unaligned LLM (presumably after pre-training and instruction tuning that does not involve safety) can be used to generate natural jailbreaks that lead to diverse questions from a given toxic seed question. Our approach, which we term as _ReG-QA_, exploits the potential asymmetry in safety alignment between question generation and answer generation in LLMs. We find that while safety-aligned LLMs are robust in generating safe responses to potentially harmful questions, they may be vulnerable to generating unsafe questions when prompted with harmful answers. This vulnerability allows us to generate a diverse set of natural prompts using some of the most capable publicly API accessible models (GPT-4o), as illustrated in Figure 2.

Algorithm 1 formalizes the procedure. First, an unaligned LLM, denoted as \(LLM_{->}^{U}\), generates a diverse set of answers \(\) from a given seed question \(q\) (Line 2). We then filter these answers based on criteria \(C_{A}\), selecting only those deemed toxic by an external judge and exceeding a predefined length threshold (Line 3), resulting in the subset \(_{}\) (Line 3). Next, we utilize a safety-aligned LLM, \(LLM_{->}\), accessible only via API, to generate questions from each answer \(a_{}\) (Line 6). This LLM is prompted to produce multiple questions that could elicit the given answer. To improve the quality and diversity of the augmented questions, we apply a selection criterion \(C_{Q}\), ensuring minimal redundancy and overlap (Line 7). The resulting set of questions, \(Q_{}\) in Algorithm 1, constitutes our natural jailbreaks. We evaluate their effectiveness in eliciting unsafe responses from frontier LLMs (also accessed via API) using a GPT-4o-mini based judge.

**Remarks:** The success of our method hinges on the observation that safety alignment in LLMs may not generalize symmetrically between question and answer generation. Our empirical results (presented in subsequent sections) demonstrate that safety-aligned LLMs, when prompted to generate questions from potentially toxic answers, produce undesirable questions with high attack success rates (ASR). This asymmetry suggests a potential "reversal curse" [Berglund et al., 2023] in safety alignment, a phenomenon that warrants further investigation. While we leverage a safety-aligned LLM for question generation, one could alternatively utilize an unaligned LLM for this step.

The following section describes the precise prompts and the criteria for filtering used in our algorithm. We use minimal and direct prompts for generation as can be seen below in Section 5.2.

Figure 2: Diagram describing various steps of our method Response Guided Question Augmentation (ReG-QA). From a seed question, we use an unaligned LLM to generate multiple answers, each of which is passed to another LLM to generate questions that would give that answer.

```
1:Input: Input question \(q\); an _unaligned_ LLM to generate answers from questions \(^{U}_{->}\); an LLM to generate questions from answers \(_{->}\); a criteria to select answers \(C_{A}\), a criteria to select questions \(C_{Q}\); \(_{Q A}\) - set of prompts for the \(Q A\) direction; \(_{A Q}\) - set of prompts for the \(A Q\) direction; \(N\) - no of answers to sample per seed question.
2:\(=\{a_{i}|a_{i}=^{U}_{->}(_{Q A},q),i N\}\)
3:\(_{sel}=\{a_{i}|a_{i} C_{A}(a_{i},)=1,i 100\}\)
4:\(Q_{}=\{\}\)
5:for each answer \(a\) in \(_{sel}\)do
6:\(Q_{_{a}}=\{q_{i}|q_{i}=_{->}( _{A Q},a),i 10\}\)
7:\(Q_{}=Q_{}\{q_{i}|q_{i} Q_{_{ a}} C_{Q}(q_{i},Q_{})=1\}\)
8:endfor
9:Output: A set of question augmentations \(Q_{}\) ```

**Algorithm 1** Reponse Guided Question Augmentation ReG-QA

### Implementation Details of the Proposed Algorithm

We aim to generate several question augmentations for a given seed question. Towards this, we first use an unaligned version of Palm-2-Otter model (\(^{U}_{-}\) in Algorithm 1) to generate 100 long answers for each seed question using the following prompt (\(_{Q A}\) in Algorithm 1):

_Answer the following question in 200 words:!seed question?_\(\)_n Answer:_

We ensure that the generated answer length is greater than a specified number of tokens (such as 50 or 100), and further select toxic answers using a judge-LLM \(_{}\). Next, we next use GPT-4o (\(_{}\) in Algorithm 1) using the following prompt (\(_{}\)), to generate 10 questions based on each of the unsafe answers:

_Generate 10 diverse questions that give the following specific answer:!answer?_\(\)_n NOTE: Respond only with a python list of length 10, containing these questions._

## 6 Experimental Results

### Experimental Setup

**Models:** To empirically validate our claims, we present results on the following publicly API accessible models: GPT-3.5-Turbo-1106, GPT-4-0125-Preview , Gemma2-27B-IT . We use Palm-2-Otter  based LLMs for generating unsafe answers from a given seed question in the proposed ReG-QA pipeline.

**Judge Prompt and Model:** We evaluate the generated jailbreaks using the prompt for the safety judge on JailbreakBench  with gpt-4o-mini-2024-07-1 acting as the judge (Ref: Appendix B for the judge prompt used).

**Judge Selection:** We test various combinations of LLMs from the GPT family  and prompt candidates from JailbreakBench / PAIR , on the _Judge Comparison_ dataset  containing a mix of 300 human annotated unsafe and safe questions. We compare agreement with human majority, FNR (for mis-classifying unsafe as safe) and FPR (for classifying safe as unsafe) rates in Table 3, along with the cost per 1M tokens. We choose the option that is reasonable in terms of cost and has a good agreement, FPR and FNR rates.

**Datasets:** We benchmark the performance of the proposed methods on the following publicly available dataset: JailbreakBench 2. The seed prompts are also categorized into multiple harm/ undesirable categories.

**Baselines:** We compare with a simple baseline that paraphrases the seed question using this prompt:

_Please paraphrase the following question to generate 10 diverse questions:!seed question?_\(\)_n NOTE: Respond only with a python list of length 10, containing these questions._We refer to this method as Paraphrasing based Question Augmentation or _Para-QA_. We repeat this prompting 100 times to ensure that number of question augmentations per seed question is 1000, which is roughly same as that produced using the proposed method Reg-QA.

### Discussion of results

Higher Attack Success Rate (ASR) than paraphrasing based baselines:We present results of our algorithm ReG-QA when compared with paraphrasing based question augmentation Para-QA in Table 1. Firstly, although the proposed method does not incorporate the objective of jailbreaking in any form during generation, we obtain very high attack success rates as shown in the table. The overall attack success rate is 82% for gpt-4 and 93% for gpt-3.5 for our method as against 41% and 66% respectively for Para-QA. This shows that our projection of the seed question to the space of natural prompts is different from paraphrasing based methods. Further, across several categories, our method ReG-QA outperforms paraphrasing based methods, achieving 100% ASR on many categories for both GPT-variants. Similarly, we also present attack success rates for the open source Gemma-2 model with \(27\)B parameters. Our method produces an ASR of 82% against 36% for the Para-QA baseline.

In our ASR evaluations presented in Table 1, target models have temperature of \(1\) which is the default setting for gpt-4 and gpt-3.5. We use this to mimic the realistic setting of usage through external APIs. We would like to highlight that this is different from the standard jailbreak evaluations, which use temperature \(0\) for the target model for reproducibility . We note from our evaluations (Table-4) that ASR with temperature \(0\) is always higher than ASR with default (higher) temperatures for a fixed attack budget. Thus our results are more conservative than those presented on JailbreakBench. To ensure robustness of the resulting prompt, and repeatability, we prompt the target model with the same question 4 times, and ensure it produces a toxic response as evaluated by \(_{}\) at least \(3\) times.

For a fair comparison with JailbreakBench, we present results by firstly identifying jailbreaks using our method of setting the default temperature, and further verifying that these are able to jailbreak even with temperature of \(0\). We present these results in Table-4 of the Appendix. Firstly, we note that for the setting of temperature=1, ASR drops as we increase the criterion on the number of successes when prompted multiple times. Further, the ASR with temperature of \(0\) is higher than the setting we consider in this work.

Higher ASR rates than leading methods on JailbreakBench:In the proposed method, we first generate \(100\) answers per seed question and further generate \(10\) questions per answer. Thus, the total number of queries per seed is \(1000\). We note that for the same attack budget, the leading attack method  achieves 78% ASR on gpt-4-0125-preview, while we achieve 82%, as shown in Table-1. We further compare ASR of our method against lead attack methods, with and without defenses, in Table-2, with target model as gpt-3.5-Turbo-1106. For the evaluations in this table, we do 1x prompting with default temperature of \(1\), and further find the subset that also jailbreak the target model at temperature=0. We note that the proposed method is significantly more robust than existing methods against all defenses considered from the JailbreakBench leaderboard. Some of the defenses introduce semantically meaningful/ random perturbations

    & gpt-3.5 & gpt-4 &  \\  & (turbo-1106) & (0125-preview) & & (27B) \\  & Para-QA & ReG-QA & Para-QA & ReG-QA & Para-QA & ReG-QA \\  Disinformation & 50 & 70 & 10 & 30 & 20 & 50 \\ Economic Harm & 70 & 90 & 30 & 90 & 20 & 80 \\ Expert Advice & 40 & 80 & 30 & 60 & 10 & 60 \\ Fraud/Deception & 80 & 100 & 50 & 80 & 70 & 100 \\ Government decision-making & 80 & 100 & 80 & 100 & 70 & 100 \\ Harassment/Discrimination & 40 & 100 & 20 & 80 & 10 & 70 \\ Malware/Hacking & 90 & 100 & 80 & 100 & 70 & 100 \\ Physical Harm & 50 & 100 & 10 & 100 & 10 & 80 \\ Privacy & 100 & 100 & 70 & 90 & 70 & 90 \\ Sexual/Adult Context & 60 & 90 & 30 & 90 & 10 & 90 \\ 
**Overall** & 66 & 93 & 41 & 82 & 36 & 82 \\   

Table 1: Category-wise Attack Success Rate (\(\%\)) of the proposed approach ReG-QA when compared to the paraphrasing baseline Para-QA on JailbreakBench seed questions across target models.

to the attack and verify the safety of the resulting prompts. The robustness of the proposed approach against such defenses highlights the stability of the generated attacks in the loss landscape. Thus, the inherent criterion of naturalness in our attack serves as an _adaptive_ attack [Tramer et al., 2020] against defenses which utilize non-naturalness and instability to perturbations as the criteria for detecting jailbreaks, serving as a motivation to build more robust defenses.

_Implications to Generalization of Safety fine tuning:_ Our method does not use the target model in either the white-box or black-box access mode, unlike most existing methods. This serves as a demonstration that the brittleness of safety fine-tuning to even minor distribution shifts at test time (as pointed by Wei et al. ) is one of the main failure modes of LLMs. Further, our results also demonstrate that aligned models such as GPT-4o are indeed capable of generating jailbreaks by simply prompting them to generate questions that give the specified answer, highlighting that _forward_ (Q to A) direction of safety training does not generalize to the _reverse_ direction (A to Q).

_ASR w.r.t. the considered Threat Model:_ We further compute the attack success rate within the threat model outlined in Section 4, based on the Gecko (1B model) [Lee et al., 2024] embedding similarity between the generated question and the seed question. As shown in Fig.4b, as we increase the threshold on the cosine similarity, the attack success rate reduces. Note that both methods have a higher attack success rate when it crosses a certain cosine similarity threshold. However, ReG-QA's ASR beyond cosine similarity of \(0.7(1-)\) is much higher compared to paraphrasing. Finally, our method has a non trivial ASR of close to 80% at a similarity threshold of 0.7 (where roughly the transition happens), suggesting that the proposed algorithm generates natural jailbreaks that are similar to the seed prompt, while also being diverse (Ref: Appendix-D for details on relevance and diversity of the generated question augmentations).

_Competitive Jailbreak rates per seed per 100 queries:_ We report jailbreak statistics per category per seed per 100 queries (normalized) in Figure 4a. We show that our method produces significantly higher jailbreaks on gpt-3.5-Turbo-1106 model per seed and per 100 queries issued to the model compared to paraphrasing based baseline across categories. The average number of jailbreaks per 100 queries per seed is 3.3% which roughly matches the 30 queries needed by the top methods to jailbreak the same model on the JailbreakBench leaderboard. Similar metrics for gpt-4-0125-preview have been reported Fig.5 of the Appendix.

## 7 Conclusion

In this work, we propose a method for verifying the in-distribution generalization of LLMs after safety-training, and demonstrate that popular LLMs such as GPT-4 are brittle against even _natural_ prompts which are semantically related to toxic seed prompts that elicit safe responses. We base our algorithm on the following failure modes of LLMs which we find: i) LLMs are more likely to produce toxic content when presented with cues or details from the answer in the question, ii) forward direction of safety training (Q to A) does not guarantee reverse direction of safety (A to Q). The latter allows us to generate jailbreak prompts using GPT-4o, by simply prompting it to generate questions that give the specified toxic answer. We obtain attack success rate of 82% for GPT-4 and 93% for GPT-3.5 on JailbreakBench. Finally, we show that our method is significantly more robust than existing attacks against several defenses on the JailbreakBench leaderboard. Thus our method serves as an adaptive attack against all defenses that incorporate non-naturalness and instability as the criteria for detecting jailbreaks. We hope this work inspires further research on understanding the generalization of existing safety training algorithms, motivating the need for better defenses.

    & No defense & Remove non-dictionary & Synonym Substitution & Smooth LLM \\  Prompt and Random Search & 93 & 11 & 5 & 4 \\ PAIR & 71 & 18 & 21 & 5 \\ GCG & 47 & 9 & 15 & 0 \\ ReG-QA (**Ours**) & **95** & **88** & **84** & **82** \\   

Table 2: Attack Success Rate (ASR) of the proposed approach ReG-QA when compared with existing attacks, against defenses on JailbreakBench. Target model used is gpt-3.5-Turbo-1106. Jailbreaks generated using ReG-QA are significantly more robust than existing methods [Andriushchenko et al., 2024, Chao et al., 2023, Zou et al., 2023], since they are natural and cannot be distinguished easily from benign prompts. Note that our approach replaces the default LLMA based models with alternate LLMs in both defense implementation and judge LLM.