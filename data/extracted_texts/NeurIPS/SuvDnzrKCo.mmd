# Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations

Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations

Edward Raff

Booz Allen Hamilton

raff_edward@bah.com

&Amol Khanna

Booz Allen Hamilton

khanna_amol@bah.com

&Fred Lu

Booz Allen Hamilton

lu_fred@bah.com

Co-first authors for equal contribution.

###### Abstract

To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for \(L_{1}\) penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from \((TDS+TNS)\) to \((NS+T D+TS^{2})\), where \(T\) is the number of iterations and a sparsity rate \(S\) of a dataset with \(N\) rows and \(D\) features. Our results demonstrate that this procedure can reduce runtime by a factor of up to \(2,200\), depending on the value of the privacy parameter \(\) and the sparsity of the dataset.

## 1 Introduction

Differential Privacy (DP) is currently the most effective tool for machine learning practitioners and researchers to ensure the privacy of the individual data used in model construction. Given parameters \(\) and \(\), on any two datasets \(\) and \(^{}\) differing on one example, an (approximately) differentially private randomized algorithm \(\) satisfies \([() O]\{\}[ (^{}) O]+\) for any \(O()\). Note that lower values of \(\) and \(\) correspond to stronger privacy.

While DP has had many successes in industry and government, DP-based machine learning methods have made little progress for sparse high-dimensional problems [2; 3; 4; 5]. We believe that this issue arises because, to the best of our knowledge, given a dataset with \(D\) features and a training algorithm with \(T\) iterations, all current iterative DP regression algorithms require at least \((TD)\) training complexity, as shown in Table 1. This makes it impractical to use these algorithms on any dataset with a large number of features. _Our solution to this problem is to take already existing algorithms, and remove all redundant computations with mathematically equivalent steps_. This ensures that, by construction, we retain all proofs of correctness -- but end with a faster version of the same method.

   Method & Complexity \\  Frank-Wolfe Methods [6; 7; 8; 9] & \((TND)\) \\ ADMM  & \((TNDM)\) \\ Iterative Gradient Hard Thresholding Methods [9; 11; 12] & \((TND)\) \\ Coordinate Descent  & \((TND)\) \\ Mirror Descent  & \((TNDM)\) \\   

Table 1: A summary of prior methods for solving \(L_{1}\) regularized Logistic Regression, which do not take advantage of sparsity in the input data.

We are interested in creating a differential private machine learning algorithm which scales to sparse datasets with high values of \(D\), so we look toward the LASSO regularized logistic regression model . Specifically, given a dataset \(\{_{1},,_{N}\}^{D}\) which can be represented as a design matrix \(X^{N D}\), \(\{y_{1},,y_{N}\}\{0,1\}\), and a maximum \(L_{1}\) norm \(\), we wish to solve

\[}=*{arg\,min}_{^{D}: \;\|\|_{1}}_{i=1}^{N}(_{i})\] (1)

where \(()\) is the loss function. In this paper, we will use the logistic loss to avoid exploiting any closed-form updates/solutions in the linear case, but our results are still applicable to linear regression. To do so with DP, we use the Frank-Wolfe algorithm as it is well studied for \(L_{1}\) constrained optimization and regularly used in DP literature [15; 16]. Frank-Wolfe is also desirable because when properly initialized, its solutions will have at most \(T\) nonzero coefficients for \(T\) training iterations. Though DP noise can reduce accuracy and cause increased density of solutions through suboptimal updates, when considering problems with more than 1 million features, we are unlikely to perform 1 million iterations, so a benefit is still obtained .

We develop the first sparse-dataset friendly Frank-Wolfe algorithm to remediate the problem of no sparse algorithms for high-dimensional DP regression. Because a sparse-efficient Frank-Wolfe algorithm does not exist today even for non-private problems, our work proceeds in three contributions:

1. We analyze the numerical updates of the Frank-Wolfe algorithm to separate the task into (a) "queue maintenance" for determining the next coordinate to update and (b) sparse updates of the solution and intermediate variables. If the average column sparsity of \(X\) is \(S_{c}<D\) and the average row sparsity of \(X\) is \(S_{r}<N\), we show for the first time that Frank-Wolfe can update its solution in \(S_{r}S_{c}\) work per iteration.
2. We show that in the non-private case, the queue to select the next coordinate can be maintained in \((\|\|_{0} D)\) time, but is cache-unfriendly.
3. Finally, we develop a new Big-Step Little-Step Sampler for DP Frank-Wolfe that can be maintained and sampled from in \(( D)\) time.

We test our algorithm on high-dimensional problems with up to 8.4 million datapoints and 20 million features on a single machine and find speedups ranging from \(10\) to \(2,200\) that of the standard DP Frank-Wolfe method. Critically, our approach is mathematically equivalent, and so retains all prior proofs of correctness.

The remainder of our work is organized as follows. In section 2 we will review related work in more detail and discuss the lack of sparse dataset algorithms for Frank-Wolfe and DP. Next, we develop the nonprivate and DP variants of our sparse-friendly Frank-Wolfe in section 3. In section 4 we demonstrate the empirical correctness of our algorithm, a speedup in runtime for the DP case, and new state-of-the-art accuracy in high-dimensional logistic regression. Finally, we conclude in section 5.

## 2 Related Work

As best as we can find, no works have specifically considered a sparse-dataset efficient Frank-Wolfe algorithm. Additionally, we find that work studying \(L_{1}\)-regularized DP regression has not reached any high-dimensional problems. Our work is the first to show that Frank-Wolfe iterations can be done with complexity sub-linear in \(D\) for sparse datasets. It also produces a sparse weight vector.

For DP regression, the addition of noise to each variable has limited exploration of sparse datasets. Most works study only one regression problem with less than 100 dense variables or are entirely theoretical with no empirical results [11; 10; 6; 18; 19]. To the best of our knowledge, the largest scale attempt for high-dimensional logistic regression with DP is by Iyengar et al., who introduce an improved objective perturbation method for maintaining DP with convex optimization . This required using L-BFGS, which is \((D)\) complexity for sparse data and produces completely dense solution vectors \(\). In addition, Wang & Gu attempted to train an algorithm on the RCV1 dataset but with worse results and similar big-\(\) complexity to Iyengar et al. . While Jain & Thakurta claim to tackle the URL dataset with 20M variables, their solution does so by sub-sampling just 0.29% of the data for training and 0.13% of the data for validation, making the results suspect and non-scalable . Lastly, a larger survey by Jayaraman & Evans shows no prior work considering the sparsity of DP solutions and all other works tackling datasets with less than 5000 features . In contrast, our work does no sub-sampling and directly takes advantage of dataset sparsity in training high-dimensional problems. Our use of the Frank-Wolfe algorithm also means our solution is sparse, with no more than \(T\) non-zero coefficients for \(T\) iterations.

In addition to no DP regression algorithm handling sparse datasets, we cannot find literature that improves upon the \((D)\) dependency of Frank-Wolfe on sparse data. Many prior works have noted this dependence as a limit to its scalability, and column sub-sampling approaches are one method that has been used to mitigate that cost [23; 15; 24]. Others have looked at distributed Map-Reduce implementations  or adding momentum terms  as a means of scaling the Frank-Wolfe algorithm. However, our method is the first to address dataset sparsity directly within the Frank-Wolfe algorithm, and can generally be applied to these prior works with additional derivations for new steps. Other methods that apply to standard regression, use the \(L_{2}\) penalty [27; 28] would require modification to use our approach. Similarly, pruning methods  may require adaption to account for the privacy impact.

Of particular note is  which tackles sub-linear scaling in the number of rows \(N\) when \(N>D\), but is primarily theoretical. Their work is the first to introduce the idea of "queue maintenance" that we similarly leverage in this work. Despite a conceptually similar goal of using a priority queue to accelerate the algorithm,  relies on maximum inner product search, where our queues are of a fundamentally different structure.

To the best of our knowledge, the COPT library is the only Frank-Wolfe implementation that makes any effort to support sparse datasets but is still \((D)\) iteration complexity, so we base our comparison against its approach . No DP regression library we are aware of supports sparse datasets .

## 3 Methods

Throughout this section, sparsity refers to an algorithm's awareness and efficiency of handling input data which contains mostly zero values. We will first review the only current method for using Frank-Wolfe with sparse data and establish its inefficiency. We will then detail how to produce a generic framework for a sparse dataset efficient Frank-Wolfe algorithm using a queuing structure to select the next coordinate update. Having established a sparse friendly framework, we will show how to obtain \((NS_{c}+T\|^{*}\|_{0} D+TS_{r}S_{c})\) complexity for the non-private Frank-Wolfe algorithm by using a Fibonacci heap, where \(^{*}\) is the solution at convergence. Then we replace the Fibonacci heap with a sampling structure to create a DP Frank-Wolfe algorithm with \((NS_{c}+T D+TS_{r}S_{c})\) complexity.

### Frank-Wolfe Iterations in sub-\((D)\) Time

The only work we could find of any sparse-input aware Frank-Wolfe implementation is the COPT library, which contains two simple optimizations: it pre-computes a re-used dense vector (i.e., all values are non-zero) and it uses a sparse matrix format for computing the vector-matrix product \(X\). The details are abstracted into Algorithm 1, where each line has a comment on the algorithmic complexity of each step. Note to make the algorithm DP, we have added a \(+(}{N})\) to draw noise from a zero-mean Laplacian distribution with the specified scale, where \(\) is the constraint parameter and \(L\) is the \(L_{1}-()\). If a non-private Frank-Wolfe implementation is desired, this value can be ignored.

```
1:\(_{0}\)
2:\(} X^{}\) \((NS_{c})\)
3:for\(t=1\) to \(T-1\)do
4:\(}_{t} X_{t}\) \((NS_{c})\)
5:\(}_{t}(}_{t})\) \((N)\)
6:\(}_{t} X^{}}_{t}\) \((NS_{c})\)
7:\(_{t}}_{t}-}\) \((D)\)
8:\(j\)\(\)\(_{j}|_{t}^{(j)}+(}}{N})|\) \((D)\)
9:\(_{t}=-_{t}\) \((D)\)
10:\(d_{t}^{(j)} d_{t}^{(j)}-(_{t}^{(j)})\) \((1)\)
11:\(g_{t}=_{t},_{t}\) \((D)\)
12:\(_{t}=\) \((1)\)
13:\(_{t+1}=_{t}+_{t}_{t}\) \((D)\)
14:endfor
15:Output \(_{T}\) ```

**Algorithm 1** Standard Sparse-Aware Frank-Wolfe

In this and other pseudo-codes, we explicitly write out all intermediate computations as they are important for enabling sparse updates. \(}\) is an intermediate variable for the labels in the gradient of a linear problem that is pre-computed once and reused. \(}\) is a temporary variable. \(}\) and \(\) are the gradients with respect to each row and column respectively. \(}\) is the dot product of each row with the weight vector, and \(\) is the update direction. The iteration subscript \(t\) will be dropped when unnecessary for clarity. The superscript \({}^{(j)}\) denotes updating the \(j\)'th coordinate of a vector and leaving others unaltered.

While lines 2, 4, and 6 of the algorithm exploit the sparsity of the data, this only reduces the complexity to \((NS_{c})\) plus an additional dense \((D)\) work for lines 7 through 13 and \((N)\) work for line 5. This results in a final complexity of \((TNS_{C}+TD)\). For high dimensional problems, especially when \(N D\), this is problematic in scaling up a Frank-Wolfe based solver.

To derive a Frank-Wolfe algorithm that is more efficient on sparse datasets, we will assume there is an abstract priority queuing structure \(Q\) that returns the next coordinate to update \(j\) for each iteration. We will detail how to design a \(Q\) to use this algorithm in non-private and DP cases in the following two sections.

#### Sparse \(_{t}\) Updates

In line 13 of Algorithm 1, if we ignore the change to coordinate \(j\) of \(_{t}\), we can write \(_{t+1}=_{t}-_{t}_{t}\), which can be re-written as \(_{t+1}=(1-_{t})_{t}\). If we represent the weight \(_{t+1}= w_{m}\) with a co-associated multiplicative scalar \(w_{m}\), we can alter \(w_{m} w_{m}(1-_{t})\) to have the same effect as altering all \(D\) variables implicitly. Then the \(j^{}\) coordinate can be updated individually, allowing line 13 of Algorithm 1 to run in \((1)\) time. We will use the same trick to represent \(}_{t}=} w_{m}\) as it has the same multiplicative scale.

#### Sparse \(\) and \(}\) Updates

The dot product scores \(}_{t}\) and column gradients \(_{t}\) are intrinsically connected in their sparsity patterns. When the \(j^{}\) value of \(_{t}\) is altered, multiple values of \(}_{t}\) change, which propagates changes to the gradients \(_{t}\). However, the elements \(\{i\}\) of \(}_{t}\) that change are only those where rows \(\{i\}\) in \(X\) use feature \(j\). Let \(_{j}\) represent a perturbation to the \(j\)'th update direction. Each row \(i\) that uses the \(j\)'th feature will then be alter the variable \(^{(i)}\) by \(-_{t}[i,j]\). This lets us handle line 10 sparsely.

Each row \(\{i\}\) that changes in \(}_{t}\) propagates to the values in \(}\). We can represent the change in gradient value between iterations as \(\), which can then be used to sparsely update the \(\) values by noting that \(X^{}}\) would change only by the non-zero columns of \(X[i,:]\). So we can compute the update \(\) by \( X[i,:]\). By updating \(\) directly, we do not need to account for the contribution of \(}\) after the first iteration.

#### Sparse \(g_{t}\) Updates

The final variable of interest is the Frank-Wolfe convergence gap \(g_{t}=-_{t}^{}_{t}\). Instead of recomputing this every iteration, we can keep a base value \(\) that is altered based on both of the prior two insights. When \(w_{m}\) is updated, we re-scale \(\) by \((1-_{t})\) and add \(_{t}^{(j)}\), the change in the dot product \(-_{t}^{}_{t}\) caused by just the \(j^{}\) coordinate update. After the sparse \(\) updates, \(\) is again updated by \( X[i,:]^{} w_{m}\).

#### Fast Frank-Wolfe Framework

The final procedure that forms the foundation for our results is given in Algorithm 2. The _scale_ variable holds the noise parameter required for DP and can be ignored for non-private training. Lines 6, 13, 15, and 30 require an abstract priority queue that is populated with values proportional each feature's gradient. This mechanism is different in the non-private and private cases, and we will detail them in the following two sections.

The first iteration of Algorithm 2 performs the same calculations as Algorithm 1, but for all subsequent iterations, values will be updated in a sparse fashion.

Lines 16-21 update the multiplicative \(w_{m}\) variables and perform the single coordinate updates to \(\) and \(\), all taking \((1)\) time to complete. Lines 22-29 handle the updates for \(\) and \(}\), which requires looping over the rows that use feature \(j\), which we expect to have \((S_{r})\) complexity. Within the loop, we use one row of the matrix to perform a sparse update which we expect to have \((S_{c})\) complexity. The gradients \(^{(k)}\) that get updated by this loop are updated in the priority queue \(Q\)2 in \((1)\) time per update, so they do not alter the final complexity. The final complexities of our algorithm are now dependent on the complexity of \(Q.()\), which will differ in the non-private and private cases.

### Algorithmically Efficient Non-Private Frank-Wolfe

We first analyze the complexity of the non-private Frank-Wolfe algorithm, though our ultimate goal is to make the private case more efficient. The algorithm we detail in the non-private case will be of superior big-\(\) complexity and perform significantly less FLOPs than the standard Frank-Wolfe implementation but will not be faster in practice due to constant factor overheads which we will explain.

The primary insight in building an algorithmically-faster non-private algorithm is to use a Fibonacci Heap, which allows for \(( D)\) removal complexity and amortized \((1)\) insertion and decreaseKey operations. We use the negative magnitude as the key in the min-heap. Our insight is that we can decrease a key \(j\) whenever \(|^{(j)}|\) increases, and ignore cases where \(|^{(j)}|\) decreases. This means the negative priority is an upper bound on true gradient magnitude. This is favorable because the vast majority of updates to the queue are intrinsically of a magnitude too small to be selected (hence why the solution is sparse) and so even with an inflated magnitude are never top of the queue.

These stale gradients will cause some items to reach the top of the queue incorrectly, which is easy to resolve as detailed in Algorithm 3. The current item \(c\) is popped off the queue, and compared against the current best coordinate \(j\). This loop continues until the top of the queue has a smaller priority than the current gradient magnitude \(^{(j)}\). Because the stale magnitude can only be larger than the true gradient, once we satisfy this condition it must be the case that no item in the queue can have a higher priority. Thus, the procedure is correct.

The number of items we expect to have to consider must be proportional to the number of non-zero coefficients in the weight vector. This gives a final complexity of \(\)\((\|^{*}\|_{0})\) for the number of non-zeros in the final solution, multiplied by the \(((D))\) cost per pop() call, giving a final complexity of \((NS_{c}+T\|^{*}\|_{0} D+TS_{r}S_{c})\).

While this is of superior algorithmic complexity compared to the standard Frank-Wolfe implementation, it has been long known that Fibonacci heaps have high constant-factor overheads that prevent their practical use [33; 34]. We still find the result useful in being the first to demonstrate a faster iteration speed, as well as a deterministic case to verify the correctness of our approach. For this reason we will use Algorithm 3 to show that our method converges at the same rate and with fewer FLOPs compared to the standard Frank-Wolfe implementation, as it does not suffer from the randomness required for differential privacy3. Our concern about this inefficiency is limited, as many faster algorithms exist for non-private LASSO regression that are orders of magnitude faster than using Frank-Wolfe [35; 36; 37], so other tools can suffice. However, no tools for high-dimensional and sparse DP LASSO regression exist except for the Frank-Wolfe method, which we make more efficient in the next section.

### Algorithmically Efficient Differentially Private Frank-Wolfe

For our DP Frank Wolfe algorithm, we convert from the Laplacian mechanism originally used by Talwar et al. to the Exponential Mechanism . Rather than adding noise to each gradient and selecting the maximum, in the exponential mechanism each coordinate \(j\) is given a weight \(()\) where \(u(j)\) is the score of the \(j\)th item and \( u\) is the sensitivity .

This poses two challenges:

1. We need to select a weighted random sample from \(D\) options in sub-\((D)\) time to pick the coordinate \(j\) to maintain the \(DP\) of the exponential mechanism.
2. We need to do this while avoiding the numeric instability of raising a gradient to an exponential, which will overflow for relatively small gradient magnitudes.

We tackle both of these issues by adapting the A-ExpJ algorithm of , and use their naming conventions to make comparison easier. This algorithm works on a stream of items with weights \(w_{i}\), and in \((1)\) space can produce a valid weighted sample from the stream. It does so by computing a randomized threshold \(T_{w}\) and processing samples until the cumulative weights \(_{i}w_{i}>T_{w}\), at which the final item in the sum becomes the new sample. A new value of \(T_{w}\) is computed, and the process continues until the stream is empty. This process requires generating only \(( D)\) random thresholds for a stream of \(D\) items.

We exploit the fact that we have a known and fixed set of \(D\) items to develop a new version of this algorithm that can be updated in constant time, is numerically stable for a wide range of weights, and can draw a new sample in \(( D)\) time. The key idea is to form large groups of variables and keeping track of their collective weight. If the group's weight is smaller than \(T_{w}\), then the entire group can be skipped to perform a "Big-Step". If the group's weight is larger than \(T_{w}\), then the members of the group must be inspected individually to form "Little-Steps". For this reason we term our sampler the "Big-Step Little-Step" sampler, and this procedure is shown in Algorithm 4.

The scale of gradients can change by four or more orders of magnitude due to the evolution of the gradient during training and the exponentiation of the Exponential Mechanism. For this reason, all logic is implemented at log scale, and a total log-sum-weight \(z_{}\) is tracked. Every exponentiation of a log-weight then subtracts this value, performing the log-sum-exp trick to keep the sample weights in a numerically stable range4

Similarly, each group has a group log-sum weight, and we denote the vector of the group weights as \(\). There are \(\) groups so that each group has \(\) members. On lines 34 and 35 of Algorithm 4, a log-sum-exp update is used to update the group sum \(^{(k)}\) and total sum \(z_{}\) (which are already log-scale since there was no exponentiation on line 30 of Algorithm 2). In both cases we always expect the group sum to be larger, and so we use \(^{(k)}\) and \(z_{}\) as the maximal values to normalize by in each update. Lines 31 and 32 select the "Big-Step" group to update for the change in weight of the \(i\)'th item.

Lines 8-12 and 13-17 perform the same loop at two different scales. 8-12 perform big steps over groups, and must handle that the starting position could be in the middle of a group from a previous iteration, making it a partial group. For this reason, there is a "group offset" \(o\) that subtracts the weight of items already visited in the group. Once a Big-Step is made, on line 11 the position is incremented by the group size modulo the current position, so that each step starts at the beginning of the next group regardless of starting position, handling the case of starting from a previous little step's location. Then lines 13-17 perform little steps within a group, and it is known that a new item must be found in the little group, otherwise, lines 8-12 would have repeated due to having a sum smaller than \(T_{w}\).

The remainder of lines 2-5 and 18-30 work as the standard A-ExpJ algorithm, except each calculation, is done at log-scale or exponentiated if an item is needed at a non-log scale, for example on line 215.

Each of the \((D)\) random variates needed by Algorithm 4 corresponds to the selection of a new current sample. In the worst cases, each of these samples will belong to a different group, necessitating exploring \(( D)\) groups each of size \(\) by construction, giving a total sampling complexity of \(( D)\). Just as with the Fibonacci Heap, the update procedure is \((1)\) per update, and so the final DP-FW complexity becomes \((NS_{c}T D+TS_{r}S_{c})\). As we will demonstrate in our results, this provides significant speedups over the standard Frank-Wolfe for sparse datasets. This is because, by design, the Algorithm 4 procedure is very cache friendly, performing linear scans over \(\) items at a time making pre-fetching very easy, and thus has only \(( D)\) cache-misses when performing Little-Step transitions.

## 4 Results

Having derived a sparsity-aware framework for implementing Frank-Wolfe iterations in time proportional to the sparsity of the data, we will now demonstrate the effectiveness of our results. Since our goal is to support faster training with sparse datasets, we focus on high-dimensional problems listed in Table 2 where \(D N\). Note that to the best of our knowledge, the RCV1 dataset at

    & N & D \\  RCV1 & 20,242 & 47,236 \\
20 Newsgroups.Binary “News20” & 19,996 & 1,355,191 \\ Malicious URLs, “URL” & 2,396,130 & 3,231,961 \\ Webb Spam Corpus, “Web” & 350,000 & 16,609,143 \\ KDD2010 (Algebra). “KMDA” & 8,407,752 & 20,216,830 \\   

Table 2: Datasets used for evaluation. We focus on cases that are high-dimensional and sparse.

\(D=47\)k is the highest-dimensional dataset any prior work has used to train a DP logistic regression model , and its \(D\) is 428\(\) smaller than the largest \(D\) we consider.

Our results will first focus on the non-private Frank-Wolfe due to its deterministic behavior and clear convergence criteria via the Frank-Wolfe gap \(g_{t}\)6. This will allow us to show clearly that we reduce the total number of floating point operations per second (FLOPs) required, though we note that in practice the runtime remains similar due to cache inefficiency.

After establishing that Algorithm 2 requires fewer FLOPs, we turn to testing the DP version leveraging our Big-Step Little-Step Sampler Algorithm 4, showing speedups ranging from \(10\) to \(2,200\) that of the standard DP Frank-Wolfe algorithm when training a model on sparse datasets.

Due to the computational requirements of running all tests, we fix the total number of iterations \(T=4,000\) and maximum \(L_{1}\) norm for the Lasso constraint to be \(=50\) in all tests across all datasets. This value produces highly accurate models in all non-private cases, and the goal of our work is not to perform hyper-parameter tuning but to demonstrate that we have taken an already known algorithm with established convergence rates and made each iteration more efficient. All experiments were run on a machine with 12 CPU cores (though only one core was used), and 128GB of RAM. The total runtime for all experiments took approximately 1 week and exploring larger datasets was limited purely by insufficient RAM to load larger datasets in memory. Our code was written in Java due to the need for explicit looping, and implemented using the JSAT library . When comparing Algorithm 1 and our improved Algorithm 2, the latter will be prefixed with "-Fast" in the legend to denote the difference.

### Non-Private Results

We first begin by looking at the convergence gap \(g_{t}\) over each iteration to confirm that we are converging to the same solution, which is shown in Figure 1. Of note, it is often impossible to distinguish between the standard and our fast Frank-Wolfe implementations because they take the exact same steps. Differences that occur are caused by nearly equal gradients between variables and are observable via inspection of \(g_{t}\). In all cases, the solutions returned achieve identical accuracy on the test datasets.

In Algorithm 2, the updating in differences can cause catastrophic cancellation due to the zig-zag behavior of Frank-Wolfe iterates (updating the same coordinate \(j\) multiple times with differing

Figure 1: Convergence gap \(g_{t}\) (y-axis, same scale for both plots) as the number of iterations increases (x-axis), showing that our Algorithm 2 (dotted lines) converges to the same solutions as Algorithm 1, with minor differences due to numerical floating point changes (i.e., both plots look nearly identical, the desired behavior). This shows our new approach maintains solution quality.

signs on each update), resulting in similar magnitude sign changes that result in slightly different results numerically compared to re-computing the entire gradient from scratch. Choosing an adaptive stepsize \(_{t}\) may alleviate this issue in future work.

Next, we empirically validate the \((\|^{*}\|_{0})\) number of times we must query the Fibonacci Heap for selecting the next iterate. The ratio of the number of times we must pop an item from the Heap against the value of \(\|^{*}\|_{0}\) is plotted over training iterations in Appendix Figure 3. We can see in all cases the ratio is \( 3\), and so few calls to the heap are necessary.

Finally, we look at the convergence rate \(g_{t}\) as a function of the number of FLOPs required to obtain it, as shown in Figure 2. It is clear that we avoid orders of magnitude more operations than a naive implementation of Frank-Wolfe would normally require, providing the foundation for our faster DP results in the next section. Unfortunately, the Fibonacci Heap has poor caching behavior, resulting in no meaningful difference in runtime for the sparse case.

### Differentially Private Results

Having established that Algorithm 2 avoids many floating point operations by exploiting sparseness in the training data, we now turn to training DP versions of the normal and our faster Frank-Wolfe algorithms. The total speedup in the runtime of our Algorithm 2 over Algorithm 1 is shown in Table 3. As can be seen, our results range from a \(10\) speedup for the URL dataset at the low end, up to a \(2,200\) speedup for the KDDA and URL datasets at \(=0.1\). In addition we ablated using Algorithm 2 with the brute force noisy-max as an ablation, which shows smaller speedups. This demonstrates that the combination of Algorithms 2 and 4 are necessary to get our results.

We note that the speedup of our method is a function of the sparsity of informative and non-informative features. This is more noticeable on the URL dataset which jumps from a \(10\) speedup to a \(2,400\) speedup when moving from \(=1\) down to \(=0.1\). This is because the URL dataset has 200 dense features that are highly informative, and the remaining features are all sparse. When a feature is dense, there is no advantage to using Alg. 2 & 4, and so no benefit is obtained. At the lower noise level of \(=1\), the denser (and thus slower) informative features are selected more frequently, resulting in longer total runtime. As the noise increase with \(=0.1\), the sparser non-informative features are selected more often, which reduces the average amount of work per update. This phenomena occurs in most datasets as denser features intrinsically have more opportunities to be discriminative, but is uniquely pr

Figure 2: The y-axis (larger is better) shows how many times fewer FLOPs our Alg. 2 + Alg 3 needs compared to the original Frank-Wolf (Alg. 1). The x-axis is how many iterations into training have been performed. Note that in all cases the difference is difficult to differentiate reaching 1,000 iterations as we reduce the number of FLOPs by orders of magnitude per iteration.

    & \)} & \)} \\  Dataset & Alg. 2+4 & Alg. 2 & Alg. 2+4 & Alg. 2 \\  News20 & 81.69 & 17.83 & 93.51 & 19.05 \\ URL & 9.99 & 1.02 & 2451.80 & 95.58 \\ RCV1 & 19.44 & 1.36 & 20.37 & 1.82 \\ Web & 581.25 & 21.24 & 537.65 & 20.79 \\ KDAA & 1239.64 & 206.50 & 2245.56 & 368.96 \\   

Table 3: How many times faster our Algorithm 2+Algorithm 4 over the standard FW implementation. In addition, just Algorithm 2 using the noisy-max sampling is included as an ablation, showing that both Alg. 4 & 2 combined are necessary to obtain maximum speed.

This is ultimately a desirable property of our method, as large values of \(>10\) are effectively not private, and so faster methods of non-private training should be used instead. Our Algorithm 2 with the Big-Step-Little-Step Sampler of Algorithm 4 will increase in its effective utility as the desired amount of privacy increases.

As our final test to highlight the utility and importance of our approach, we re-run each dataset using \(=5000\) with \(T=400,000\) iterations at a real-world useful \(=0.1\). As shown in Table 4 this results in non-trivial accuracy and AUC for all datasets but KDDA, and is only possible by performing hundreds of thousands of training iterations. Iyengar et al.  show the best prior results at \(=0.1\) for RCV1 64.2% accuracy, and in fact, we trail the non-private accuracy of 93.5% by only 3% points (note as well that their solution has 0% sparsity). This is made possible by simply performing far more iterations, which is computationally intractable with prior methods. We also note that we obtain significant sparsity on the higher dimensional datasets News20, URL, Web, and KDDA due to the fact that \(T<D\) for each of them, and the Frank-Wolfe will by construction have a number of non-zero coefficients \( T\).

## 5 Conclusion

We have developed the first DP training procedure that, for a sparse input dataset, obtains a training complexity that is sub-linear in the total number of features \(D\) at \((NS_{c}+T D+TS_{r}S_{c})\). Testing on multiple high-dimensional datasets, we obtain up to \(2,200\) speedup, with increasing efficiency as the value of \(\) decreases. Using this speed efficiency, we show non-trivial accuracy and privacy at \(=0.1\) for \(352\) more features than ever previously attempted and improve accuracy compared to prior work by an absolute 26.3%.