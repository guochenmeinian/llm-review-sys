# Fate: Fairness Attacks on Graph Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study fairness attacks on graph learning to answer the following question: _How can we achieve poisoning attacks on a graph learning model to exacerbate the bias?_ We answer this question via a bi-level optimization problem and propose a meta learning-based attacking framework named Fate. The proposed framework is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate Fate to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that Fate could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.

## 1 Introduction

Algorithmic fairness in graph learning has received much research attention [5; 20; 24]. Despite its substantial progress, existing studies mostly assume the benevolence of input graphs and aim to ensure that the bias would not be perpetuated or amplified in the learning process. However, malicious activities in the real world are commonplace. For example, consider a financial fraud detection system which utilizes a transaction network to classify whether a bank account is fraudulent or not [49; 45]. An adversary may manipulate the transaction network (e.g., malicious banker with access to the transaction data, theft of bank accounts to make malicious transactions), so that the graph-based fraud detection model would exhibit unfair classification results with respect to people of different demographic groups. Consequently, a biased fraud detection model may infringe civil liberty to certain financial activities and impact the well-being of an individual negatively . It would also make the graph learning model fail to provide the same quality of service to people of certain demographic groups, causing the financial institutions to lose business in the communities of the corresponding demographic groups. Thus, it is critical to understand how resilient a graph learning model is with respect to adversarial attacks on fairness, which we term as _fairness attacks_.

To date, fairness attack has not been well studied. Sporadic literature often follows two strategies: (1) adversarial data point injection, which is often designed for tabular data rather than graphs [38; 33; 8; 44] or (2) adversarial edge injection, which only attacks the group fairness of a graph neural network . It is thus crucial to study how to attack different fairness definitions for a variety of graph learning models.

To achieve this goal, we study the Fairness attacks on graph learning (Fate) problem. We formulate it as a bi-level optimization, where the lower-level problem optimizes a task-specific loss function to make the fairness attacks deceptive and the upper-level problem leverages the supervision signal to modify the input graph and maximize the bias function corresponding to a user-defined fairness definition. To solve the bi-level optimization problem, we propose a meta learning-based solver(Fate), whose key idea is to compute the meta-gradient of the upper-level bias function with respect to the input graph to guide the fairness attacks. Compared with existing works, our proposed Fate framework has two major advantages. First, it is capable of attacking _any_ fairness definition on _any_ graph learning model, as long as the corresponding bias function and the task-specific loss function are differentiable. Second, it is equipped with the ability for either continuous or discretized poisoning attacks on the graph topology. We also briefly discuss its ability for poisoning attacks on node features in a later section.

The major contributions of this paper are summarized as follows.

* **Problem definition.** We formally define the problem of fairness attacks on graph learning (the Fate problem). Based on the definition, we formulate it as a bi-level optimization problem, whose key idea is to maximize a bias function in the upper level while minimizing a task-specific loss function for a graph learning task.
* **Attacking framework.** We propose an end-to-end attacking framework named Fate. It learns a perturbed graph topology via meta learning, such that the bias with respect to the learning results trained with the perturbed graph will be amplified.
* **Empirical evaluation.** We conduct experiments on three benchmark datasets to demonstrate the efficacy of our proposed Fate framework in amplifying the bias while being the most deceptive method (i.e., achieving the highest micro F1 score) on semi-supervised node classification.

## 2 Preliminaries and Problem Definition

**A - Notations.** Throughout the paper, we use bold upper-case letter for matrix (e.g., \(\)), bold lower-case letter for vector (e.g., \(\)) and calligraphic letter for set (e.g., \(\)). We use superscript \({}^{T}\) to denote the transpose of a matrix/vector (e.g., \(^{T}\) is the transpose of \(\)). Regarding matrix/vector indexing, we use conventions similar to NumPy in Python. For example, \([i,j]\) is the entry of \(\) at the \(i\)-th row and \(j\)-th column; \([i]\) is the \(i\)-th entry of \(\); \([i,:]\) and \([j,:]\) are the \(i\)-th row and \(j\)-th column of \(\), respectively.

**B - Algorithmic fairness.** The general principle of algorithmic fairness is to ensure the learning results would not favor one side or another.1 Among several fairness definitions that follow this principle, group fairness [16; 18] and individual fairness  are the most widely studied ones. Group fairness splits the entire population into multiple demographic groups by a sensitive attribute (e.g., gender) and ensure the parity of a statistical property among learning results of those groups. For example, statistical parity, a classic group fairness definition, guarantees the statistical independence between the learning results (e.g., predicted labels of a classification algorithm) and the sensitive attribute . Individual fairness suggests that similar individuals should be treated similarly. It is often formulated as a Lipschitz inequality such that distance between the learning results of two data points should be no larger than the difference between these two data points .

**C - Problem definition.** Existing work  for fairness attacks on graphs randomly injects adversarial edges so that the disparity between the learning results of two different demographic groups would be amplified. However, it suffers from three major limitations. (1) First, it only attacks statistical parity while overlooking other fairness definitions (e.g., individual fairness ). (2) Second, it only considers adversarial edge injection, excluding other manipulations like edge deletion or reweighting. Hence, it is essential to investigate the possibility to attack other fairness definitions on real-world graphs with an arbitrary choice of manipulation operations. (3) Third, it does not consider the utility of graph learning models while achieving the fairness attacks, resulting in performance degradation in the downstream tasks. However, an institution that applies the graph learning models are often utility-maximizing [28; 2]. Thus, a performance degradation in the utility would make the fairness attacks not deceptive from the perspective of a utility-maximizing institution.

In this paper, we seek to overcome the aforementioned limitations. To be specific, given an input graph, an optimization-based graph learning model, and a user-defined fairness definition, we aim to learn a modified graph such that a bias function of the corresponding fairness definition would be maximized for _effective_ fairness attacks, while minimizing the task-specific loss function with respect to the graph learning model for _deceptive_ fairness attacks. Formally, we define the problem of fairness attacks on graph learning, which is referred to as the Fate problem.

**Problem 1**Fate_: Fairness Attacks on Graph Learning_

**Given:** (1) An undirected graph \(=\{,\}\); (2) a task-specific loss function \(l(,,,)\) where \(\) is the graph learning results, \(\) is the set of learnable variables and \(\) is the set of hyperparameters; (3) a bias function \(b(,^{*},,)\) where \(^{*}=_{}l(,,,)\) and \(\) is the matrix that contains auxiliary fairness-related information (e.g., sensitive attribute values of all nodes in \(\) for group fairness, pairwise node similarity matrix for individual fairness); (4) an integer budget \(B\).

**Find:** a poisoned graph \(}=\{},}\}\) which satisfies the following properties: (1) \(d(,}) B\) where \(d(,})\) is the distance between the input graph \(\) and the poisoned graph \(}\) (e.g., \(\|,}\|_{1,1}\)); (2) the bias function \(b(,^{*},)\) is maximized for effectiveness; (3) the task-specific loss function \(l(},,,)\) is minimized for deceptiveness.

## 3 Methodology

In this section, we first formulate Problem 1 as a bi-level optimization problem, followed by a generic meta learning-based solver named Fate.

### Problem Formulation

Given an input graph \(=\{,\}\) with adjacency matrix \(\) and node feature matrix \(\), an attacker aims to learn a poisoned graph \(}=\{},}\}\) such that the graph learning model will be maximally biased when trained on \(}\). In this work, we consider the following settings for the attacker.

**The goal of the attacker.** The attacker aims to amplify the bias of the graph learning results output by a victim graph learning model. And the bias to be amplified is a choice made by the attacker based on which fairness definition the attacker aims to attack.

**The knowledge of the attacker.** Following similar settings in , we assume the attacker has access to the adjacency matrix, the feature matrix of the input graph, and the sensitive attribute of all nodes in the graph. For a (semi-)supervised learning problem, we assume that the ground-truth labels of the training nodes are also available to the attacker. For example, for a graph-based financial fraud detection problem, the malicious banker may have access to the demographic information (i.e., sensitive attribute) of the account holders and also know whether some bank accounts are fraudulent or not, which are the ground-truth labels for training nodes. Similar to , the attacker has no knowledge about the parameters of the victim model. Instead, the attacker will perform a gray-box attack by attacking a surrogate graph learning model.

**The capabilitiy of the attacker.** The attacker is able to perturb up to \(B\) edges/features in the graph (i.e., \(\|-}\|_{1,1} B\) or \(\|-}\|_{1,1} B\)).

Based on that, we formulate Problem 1 as a bi-level optimization problem as follows.

\[}=_{}& \;b(,^{*},)\\ &^{*}=_{}l( ,,,),\;d(,}) B\] (1)

where the lower-level problem learns an optimal surrogate graph learning model \(^{*}\) by minimizing \(l(,,,)\), the upper-level problem finds a poisoned graph \(}\) that could maximize a bias function \(b(,^{*},)\) for the victim graph learning model and the distance between the input graph and the poisoned graph \(d(,})\) is constrained to satisfy the setting about the budgeted attack. Note that Eq. (1) is applicable to attack _any_ fairness definition on _any_ graph learning model, as long as the bias function \(b(,^{*},)\) and the loss function \(l(,,,)\) are differentiable.

**A - Lower-level optimization problem.** A wide spectrum of graph learning models are essentially solving an optimization problem. Take the graph convolutional network (GCN)  as an example. It learns the node representation by aggregating information from its neighborhood, i.e., message passing. Mathematically, for an \(L\)-layer GCN, the hidden representation at \(k\)-th layer can be represented as \(^{(k)}=(}^{(k-1)}^ {(k)})\) where \(\) is a nonlinear activation function (e.g., ReLU), \(}=^{-1/2}(+)^{-1/2}\) with \(\) being the degree matrix of \((+)\) and \(^{(k)}\) is the learnable weight matrix of the \(k\)-th layer. Then the lower-level optimization problem aims to learn the setof parameters \(^{*}=\{^{(k)}|k=1,,L\}\) that could minimize a task-specific loss function (e.g., cross-entropy loss for semi-supervised node classification). For more examples of graph learning models from the optimization perspective, please refers to Appendix A.

**B - Upper-level optimization problem.** To attack the fairness aspect of a graph learning model, we aim to maximize a differentiable bias function \(b(,^{*},)\) with respect to a user-defined fairness definition in the upper-level optimization problem. For example, for statistical parity , the fairness-related auxiliary information matrix \(\) can be defined as the one-hot demographic membership matrix, where \([i,j]=1\) if and only if node \(i\) belongs to \(j\)-th demographic group. Then the statistical parity is equivalent to the statistical independence between the learning results \(\) and \(\). Based on that, existing studies propose several differentiable measurements of the statistical dependence between \(\) and \(\) as the bias function. For example, Bose et al.  use mutual information \(I(;)\) as the bias function; Prost et al.  define the bias function as the Maximum Mean Discrepancy _MMD_\((_{0},_{1})\) between the learning results of two different demographic groups \(_{0}\) and \(_{1}\).

### The Fate Framework

To solve Eq. (1), we propose a generic attacking framework named Fate to learn the poisoned graph. The key idea is to view Eq. (1) as a meta learning problem, which aims to find suitable hyperparameter settings for a learning task , and treat the graph \(\) as a hyperparameter. With that, we learn the poisoned graph \(}\) using the meta-gradient of the bias function \(b(,^{*},)\) with respect to \(\). In the following, we introduce two key parts of Fate in details, including meta-gradient computation and graph poisoning with meta-gradient.

**A - Meta-gradient computation.** The key term to learn the poisoned graph is the meta-gradient of the bias function with respect to the graph \(\). Before computing the meta-gradient, we assume that the lower-level optimization problem converges in \(T\) epochs. Thus, we first pre-train the lower-level optimization problem by \(T\) epochs to obtain the optimal model \(^{*}=^{(T)}\) before computing the meta-gradient. The training of the lower-level optimization problem can also be viewed as a dynamic system with the following updating rule

\[^{(t+1)}=^{(t+1)}(,^{(t)},, ),\; t\{1,,T\}\] (2)

where \(^{(1)}\) refers to \(\) at initialization, \(^{(t+1)}()\) is an optimizer that minimizes the lower-level loss function \(l(,,^{(t)},)\) at \((t+1)\)-th epoch. From the perspective of the dynamic system, by applying the chain rule and unrolling the training of lower-level problem with Eq. (2), the meta-gradient \(_{}b\) can be written as

\[_{}b=_{}b(,^{(T)}, )+_{t=0}^{T-2}A_{t}B_{t+1} B_{T-1}_{^{(T )}}b(,^{(T)},)\] (3)

where \(A_{t}=_{}^{(t+1)}\) and \(B_{t}=_{^{(t)}}^{(t+1)}\). However, Eq. (3) is computationally expensive in both time and space. To further speed up the computation, we adopt a first-order approximation of the meta-gradient  and simplify the meta-gradient as

\[_{}b_{^{(T)}}b(,^{(T )},)_{}^{(T)}\] (4)

Since the input graph is undirected, the derivative of the symmetric adjacency matrix \(\) can be computed as follows by applying the chain rule of a symmetric matrix .

\[_{}b_{}b+(_{} b)^{T}-(_{}b)\] (5)

For the node feature matrix \(\), its derivative is equal to the partial derivative \(_{}b\) since it is often an asymmetric matrix.

**B - Graph poisoning with meta-gradient.** After computing the meta-gradient of the bias function \(_{}b\), we aim to poison the input graph guided by \(_{}b\). We introduce two poisoning strategies: (1) continuous poisoning and (2) discretized poisoning.

_Continuous poisoning attack._ The continuous poisoning attack is straightforward by reweighting edges in the graph. We first compute the meta-gradient of the bias function \(_{}b\), then use it to poison the input graph in a gradient descent-based updating rule as follows.

\[-_{}b\] (6)where \(\) is a learning rate to control the magnitude of the poisoning attack. The learning rate should satisfy \(}\|_{1,1}}\) to ensure that constraint on the budgeted attack.

_Discretized poisoning attack._ The discretized poisoning attack aims to select a set of edges to be added/deleted. It is guided by a poisoning preference matrix defined as follows.

\[_{}=(-2)_{}b\] (7)

where \(\) is an all-one matrix with the same dimension as \(\) and \(\) denotes the Hadamard product. A large positive \(_{}[i,j]\) indicates strong preference in adding an edge if nodes \(i\) and \(j\) are not connected (i.e., positive \(_{}b[i,j]\), positive \((-2)[i,j]\)) or deleting an edge if nodes \(i\) and \(j\) are connected (i.e., negative \(_{}b[i,j]\), negative \((-2)[i,j]\)). Then, a greedy selection strategy is applied to find the set of edges \(_{}\) to be added/deleted.

\[_{}=(_{},)\] (8)

where \((_{},)\) selects \(\) entries with highest preference score in \(_{}\). Note that, if we only want to add edges without any deletion, all negative entries in \(_{}b\) should be zeroed out before computing Eq. (7). Likewise, if edges are only expected to be deleted, all positive entries should be zeroed out.

_Remarks._ Poisoning node feature matrix \(\) follows the same steps as poisoning adjacency matrix \(\) without applying Eq. (5).

**C - Overall framework.**Fate generally works as follows. (1) We first pre-train the surrogate graph learning model and get the corresponding learning model \(^{(T)}\) as well as the learning results \(^{(T)}\). (2) Then we compute the meta gradient of the bias function using Eqs. (4) and (5). (3) Finally, we perform the discretized poisoning attack (Eqs. (7) and (8)) or continuous poisoning attack (Eq. (6)). A detailed pseudo-code of Fate is provided in Appendix B.

**D - Limitations.** Since Fate leverages the meta-gradient to poison the input graph, it requires the bias function \(b(,^{(T)},)\) to be differentiable in order to calculate the meta-gradient \(_{}b\). In Sections 4 and 5, we present a carefully chosen bias function for Fate. And we leave it for future work on exploring the ability of Fate in attacking other fairness definitions. Moreover, though the meta-gradient can be efficiently computed via auto-differentiation in many deep learning packages (e.g., PyTorch2, TensorFlow3), it requires \(O(n^{2})\) space complexity to store the meta-gradient when attacking fairness via edge flipping. It is still a challenging open problem on how to efficiently compute the meta-gradient in terms of space. One possible remedy for discretized attack might be a low-rank approximation on the perturbation matrix formed by \(_{}\). Since the difference between the benign graph and poisoned graph are often small and budgeted (\(d(,}) B\)), it is likely that the edge manipulations may be around a few set of nodes, which makes the perturbation matrix to be an (approximately) low-rank matrix.

## 4 Instantiation #1: Statistical Parity on Graph Neural Networks

Here, we instantiate Fate framework by attacking statistical parity on graph neural networks in a binary node classification problem with a binary sensitive attribute. We briefly discuss how to choose (1) the surrogate graph learning model used by the attacker, (2) the task-specific loss function in the lower-level optimization problem and (3) the bias function in the upper-level optimization problem.

**A - Surrogate graph learning model.** We assume that the surrogate model to be used by the attacker is a 2-layer linear GCN  with different hidden dimensions and model parameters at initialization.

**B - Lower-level loss function.** We consider a semi-supervised node classification task for the graph neural network to be attacked. Thus, the lower-level loss function is chosen as the cross entropy between the ground-truth label and the predicted label: \(l(,,,)=_{ }|}_{i_{}}_{j=1}^{c}y_{i,j} _{i,j}\), where \(_{}\) is the set of training nodes with ground-truth labels with \(|_{}|\) being its cardinality, \(c\) is the number of classes, \(y_{i,j}\) is a binary indicator of whether node \(i\) belongs to class \(j\) and \(_{i,j}\) is the prediction probability of node \(i\) belonging to class \(j\).

**C - Upper-level bias function.** We aim to attack statistical parity in the upper-level problem, which asks for \([=1]=[=1|s=1]\). Suppose \(p()\) is the probability density function (PDF) of \(_{i,1}\)for any node \(i\) and \(p(|s=1)\) is the PDF of \(_{i,1}\) for any node \(i\) belong to the demographic group with sensitive attribute value \(s=1\). We observe that \([=1]\) and \([=1|s=1]\) are equivalent to the cumulative distribution functions (CDF) of \(p(<)\) and \(p(<|s=1)\), respectively. To estimate both \([=1]\) and \([=1|s=1]\) with a differentiable function, we first estimate their probability density functions (\(p(<)\) and \(p(<|s=1)\)) with kernel density estimation (KDE, Definition 1).

**Definition 1**: _(Kernel density estimation ) Given a set of n IID samples \(\{x_{1},,x_{n}\}\) drawn from a distribution with an unknown probability density function \(f\), the kernel density estimation of \(f\) at point \(\) is defined as follows._

\[()=_{i=1}^{n}f_{k}(}{a})\] (9)

_where \(\) is the estimated probability density function, \(f_{k}\) is the kernel function and \(a\) is a non-negative bandwidth._

Moreover, we assume the kernel function in KDE is the Gaussian kernel \(f_{k}(x)=}e^{-x^{2}/2}\). However, computing the CDF of a Gaussian distribution is non-trivial. Following , we leverage a tractable approximation of the Gaussian Q-function as follows.

\[Q()=F_{k}()=_{}^{}f_{k}(x)dx e^{- ^{2}--}\] (10)

where \(f_{k}(x)==}e^{-x^{2}/2}\) is a Gaussian distribution with zero mean, \(=0.4920\), \(=0.2887\), \(=1.1893\). The overall workflow of estimating \([=1]\) is as follows.

* For any node \(i\), get its prediction probability \(_{i,1}\) with respect to class \(1\);
* Estimate the CDF \([=1]\) using a Gaussian KDE with bandwidth \(a\) by \([=1]=_{i=1}^{n}(- (_{i,1}}{a})^{2}-(_{i,1}}{a})-)\), where \(=0.4920\), \(=0.2887\), \(=1.1893\) and \((x)=e^{x}\).

Note that \([=1|s=1]\) can be estimated with a similar procedure with minor modifications. The only modifications needed are: (1) get the prediction probability of nodes with \(s=1\) and (2) compute the CDF using the Gaussian Q-function over nodes with \(s=1\) rather than all nodes in the graph.

## 5 Instantiation #2: Individual Fairness on Graph Neural Networks

We provide another instantiation of Fate framework by attacking individual fairness on graph neural networks. Here, we consider the same surrogate graph learning model (i.e., 2-layer linear GCN) and the same lower-level loss function (i.e., cross entropy) as described in Section 4. To attack individual fairness, we define the upper-level bias function following the principles in : the fairness-related auxiliary information matrix \(\) is defined as the oracle symmetric pairwise node similarity matrix \(\) (i.e., \(=\)), where \([i,j]\) measures the similarity between node \(i\) and node \(j\). Kang et al.  define that the overall individual bias to be \((^{T}})\). Assuming that \(\) is the output of an optimization-based graph learning model, \(\) can be viewed as a function with respect to the input graph \(\), which makes \((^{T}})\) differentiable with respect to \(\). Thus, the bias function \(b()\) can be naturally defined as the overall individual bias of the input graph \(\), i.e., \(b(,^{*},)=(^{ T}})\).

## 6 Experiments

### Attacking Statistical Parity on Graph Neural Networks

**Settings.** We compare Fate with 4 baseline methods, i.e., Random, DICE , FA-GNN , under the same setting as in Section 4. That is, (1) the fairness definition to be attacked is statistical parity; (2) the downstream task is binary semi-supervised node classification with binary sensitive attributes. The experiments are conducted on 3 real-world datasets, i.e., Pokec-n, Pokec-z and Bail. Similar to existing works, we use the 50%/25%/25% splits for train/validation/test sets. For all baseline

[MISSING_PAGE_FAIL:7]

**Analysis on the manipulated edges.** Here, we aim to characterize the properties of edges that are flipped by Fate (i.e., Fate-flip) in attacking statistical parity. The reason to only analyze Fate-flip is that the majority of edges manipulated by Fate-flip on all three datasets is by addition (i.e., flipping from non-existing to existing). Figure 0(b) suggests that, if the two endpoints of an manipulated edge share the same class label or same sensitive attribute value, these two endpoints are most likely from the minority class and protected group. Combining Figures 0(a) and 0(b), Fate would significantly increase the number of edges that are incident to nodes in the minority class and/or protected group.

**More experimental results.** Due to the space limitation, we defer more experimental results on attacking statistical parity on graph neural networks in Appendix D. More specifically, we present the performance evaluation under different metrics, i.e., Macro F1 and AUC, as well as the effectiveness of Fate with a different victim model, i.e., FairGNN , which ensures statistical parity.

### Attacking Individual Fairness on Graph Neural Networks

**Settings.** To showcase the ability of Fate on attacking the individual fairness (Section 5), we further compare Fate with the same set of baseline methods (Random, DICE , FA-GNN ) on the same set of datasets (Pokec-n, Pokec-z, Bail). We follow the settings as in Section 5. We use the 50%/25%/25% splits for train/validation/test sets with GCN  being the victim model. For each dataset, we use a fixed random seed to learn the poisoned graph corresponding to each baseline method. Then we train the victim model 5 times with different random seeds. And each entry in the oracle pairwise node similarity matrix is computed by the cosine similarity of the corresponding rows in the adjacency matrix. That is, \([i,j]=[i,:],A[j,:])}\), where \(\) is the function to compute cosine similarity. For fair comparison, we only attack the adjacency matrix in all experiments. Please refer to Appendix C for detailed experimental settings.

**Main results.** Similarly, we test Fate with both edge flipping (Fate-flip in Table 2) and edge addition (Fate-add in Table 2), while all other baseline methods only add edges. From Table 2, we have two key observations. (1) Fate-flip and Fate-add are effective: they are the only methods that could consistently attack individual fairness whereas all other baseline methods mostly fail to attack individual fairness. (2) Fate-flip and Fate-add are deceptive: they achieve comparable or even better utility on all datasets compared with the utility on the benign graph. Hence, Fate framework is able to achieve effective and deceptive attacks to exacerbate individual bias.

**Effect of the perturbation rate.** From Table 2, we obtain similar observations as in Section 6.1 for Bail dataset. While for Pokec-n and Pokec-z, the correlation between the perturbation rate (Ptb.) and the individual bias is weaker. One possible reason is that: for Pokec-n and Pokec-z, the discrepancy between the oracle pairwise node similarity matrix and the benign graph is larger. Since the individual bias is computed using the oracle pairwise node similarity matrix rather than the benign/poisoned adjacency matrix, higher perturbation rate to poison the adjacency matrix may have less impact on the computation of individual bias.

**Analysis on the manipulated edges.** Similarly, since the majority of edges manipulated by Fate-flip is through addition, we only analyze Fate-flip here. From Figure 2, we can find out that Fate will manipulate edges from the same class (especially from the minority class). In this way, Fate would find edges that could increase individual bias and improve the utility of the minority class in order to make the fairness attack deceptive.

**More experimental results.** Due to the space limitation, we defer more experimental results on attacking individual fairness on graph neural networks in Appendix E. More specifically, we present the performance evaluation under different metrics, i.e., Macro F1 and AUC, as well as

Figure 2: Attacking individual fairness with Fate-flip. (a) Ratios of flipped edges that connect two nodes with same/different label. (b) Ratios of flipped edges whose two endpoints are both from the majority/minority class. Majority/minority classes are formed by splitting the training nodes based on their class labels.

[MISSING_PAGE_FAIL:9]