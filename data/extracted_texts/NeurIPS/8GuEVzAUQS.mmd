# Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning

Jialong Wu1, Haoyu Ma2, Chaoyi Deng, Mingsheng Long3

School of Software, BNRist, Tsinghua University, China

wujialong0229@gmail.com, {mhy22,dengcy23}@mails.tsinghua.edu.cn

mingsheng@tsinghua.edu.cn

Equal Contribution

###### Abstract

Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly separate context and dynamics modeling to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is elaborately realized by incorporating a context encoder to retain contextual information and empower the image decoder, which encourages the latent dynamics model to concentrate on essential temporal variations. Our experiments show that in-the-wild video pre-training equipped with ContextWM can significantly improve the sample efficiency of MBRL in various domains, including robotic manipulation, locomotion, and autonomous driving. Code is available at this repository: https://github.com/thuml/ContextWM.

## 1 Introduction

Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning for visual control. Typically, a world model  is learned to approximate state transitions and control signals of the environment to generate imaginary trajectories for planning  or behavior learning . In the wake of revolutionary advances in deep learning, world models have been realized as action-conditional video prediction models  or latent dynamics models . However, given the expressiveness and complexity of deep neural networks, the sample efficiency of MBRL can still be limited by the failure to learn an accurate and generalizable world model efficiently.

_Pre-training and fine-tuning_ paradigm has been highly successful in computer vision  and natural language processing  to fast adapt pre-trained representations for downstream tasks, while learning _tabula rasa_ is still dominant in MBRL. Recent work has taken the first step towards pre-training a world model, named Action-free Pre-training from Videos (APV) . However, it has been conducted by pre-training on domain-specific and carefully simulated video datasets, rather than abundant _in-the-wild_ video data . Prior attempts of leveraging real-world video data result in either underfitting for video pre-training or negligible benefits for downstream visual control tasks . Against this backdrop, we naturally ask the following question:

_Can world models pre-trained on diverse in-the-wild videos benefit sample-efficient learning of downstream visual control tasks?_It opens up a way to utilize the vast amount of videos available on the Internet and thereby fully release the potential of this methodology by applying it at scale. Large and diverse data spanning various scenes and tasks can provide world knowledge that is widely generalizable and applicable to a variety of downstream tasks. For instance, as depicted in Figure 1, world models for robotic manipulation tasks can probably benefit not only from videos of humans interacting with target objects in diverse ways but also from motion videos that embody rigorous physical rules. Analogous to APV , we brand our paradigm as In-the-wild Pre-training from Videos (IPV).

However, in-the-wild videos are of inherent high complexity with various factors, such as intricate backgrounds, appearance, and shapes, as well as complicated dynamics. Coarse and entangled modeling of context and dynamics can waste a significant amount of model capacity on modeling low-level visual details of _what is there_ and prevent world models from capturing essential shared knowledge of _what is happening_. Biological studies reveal that in natural visual systems, \(\)80% retinal ganglion cells are P-type operating on spatial detail and color, while \(\)20% are M-type operating on temporal changes [28; 44]. Partially inspired by this, we suggest that leveraging IPV for world models calls for not only appropriate data and model scaling  but more essentially, dedicated design for context and dynamics modeling.

In this paper, we present the Contextualized World Model (ContextWM), a latent dynamics model with visual observations that explicitly separates context and dynamics modeling to facilitate knowledge transfer of semantically similar dynamics between visually diverse scenes. Concretely, a contextualized latent dynamics model is derived to learn with variational lower bound [33; 38] and elaborately realized by incorporating a context encoder to retain contextual information and a parsimonious latent dynamics model to concentrate on essential temporal variations. Moreover, dual reward predictors are introduced to enable simultaneously enhancing task-relevant representation learning as well as learning an exploratory behavior. The main contributions of this work are three-fold:

* From a data-centric view, we systematically study the paradigm of pre-training world models with in-the-wild videos for sample-efficient learning of downstream tasks, which is much more accessible and thus makes it possible to build a general-purpose world model.
* We propose Contextualized World Models (ContextWM), which explicitly model both the context and dynamics to handle complicated in-the-wild videos during pre-training and also encourage task-relevant representation learning when fine-tuning with MBRL.
* Our experiments show that equipped with our ContextWM, in-the-wild video pre-training can significantly improve the sample efficiency of MBRL on various domains.

## 2 Related Work

Pre-training in RL.Three categories of pre-training exist in RL: unsupervised online pre-training, offline pre-training, and visual representation pre-training. The first two are both domain-specific, which learn initial behaviors, primitive skills, or domain-specific representations by either online environment interaction [55; 39] or offline demonstrations [41; 3]. Recent work has explored general

Figure 1: Illustration of In-the-wild Pre-training from Videos (IPV). In various domains, we pre-train world models with in-the-wild videos by action-free video prediction (_left_) and then fine-tune the pre-trained one on downstream visual control tasks with model-based reinforcement learning (_right_).

purpose visual pre-training using in-the-wild datasets [69; 36; 77; 53; 86; 49; 45] to accelerate policy learning in model-free manners. Plan2Explore , as a model-based method, pre-trains its world model and exploratory policy simultaneously by online interaction. Ye _et al._ and Xu _et al._ build upon EfficientZero  and pre-train their world models using offline experience datasets across Atari games. To the best of our knowledge, APV  is the first to pre-train world models with datasets from different domains, though the datasets are manually simulated demonstrations by scripted policies across tasks from RLBench , which still lacks diversity and scale.

Visual control with in-the-wild videos.While several existing works leverage demonstration videos for learning visual control [43; 84; 62], rare efforts have been made to utilize off-the-shelf video datasets from the Internet. Shao _et al._ and Chen _et al._ learn reward functions using videos from the Something-Something dataset . R3M  leverages diverse Ego4D dataset  to learn reusable visual representations by time contrastive learning. Other applications with such videos include video games [7; 6], visual navigation , and autonomous driving . We instead pre-train world models by video prediction on in-the-wild video datasets.

World models for visual RL.World models that explicitly model the state transitions and reward signals are widely utilized to boost sample efficiency in visual RL. A straightforward method is to learn an action-conditioned video prediction model [50; 34] to generate imaginary trajectories. Ha and Schmidhuber  propose to first use a variational autoencoder to compress visual observation into latent vectors and then use a recurrent neural network to predict transition on compressed representations. Dreamer [22; 21; 23], a series of such latent dynamics models learning via image reconstruction, have demonstrated their effectiveness for both video games and visual robot control. There also have been other methods that learn latent representations by contrastive learning [51; 11] or value prediction [63; 25]. Recent works also use Transformers  as visual encoders  or dynamics models [10; 47; 58] that make world models much more scalable and can be complementary with our effort to pre-train world models with a vast amount of diverse video data.

Concurrent to our work, SWIM  also pre-trains a world model from large-scale human videos and then transfers it to robotic manipulation tasks in the real world. However, it designs a structured action space via visual affordances to connect human videos with robot tasks, incorporating a strong inductive bias and thus remarkably limiting the range of pre-training videos and downstream tasks.

## 3 Background

Problem formulation.We formulate a visual control task as a partially observable Markov decision process (POMDP), defined as a tuple \((,,p,r,)\). \(\) is the observation space, \(\) is the action space, \(p(o_{t}\,|\,o_{<t},a_{<t})\) is the transition dynamics, \(r(o_{ t},a_{<t})\) is the reward function, and \(\) is the discount factor. The goal of MBRL is to learn an agent \(\) that maximizes the expected cumulative rewards \(_{p,}[_{t=1}^{T}^{t-1}r_{t}]\), with a learned world model \((,)\) approximating the unknown environment. In our pre-training and fine-tuning paradigm, we also have access to an in-the-wild video dataset \(=\{(o_{t})_{t=1}^{T}\}\) without actions and rewards to pre-train the world model.

Dreamer.Dreamer [21; 23; 24] is a visual model-based RL method where the world model is formulated as a latent dynamics model [16; 22] with the following four components:

\[& z_{t} q_{ }(z_{t}\,|\,z_{t-1},a_{t-1},o_{t})_{t}  p_{}(_{t}\,|\,z_{t})\\ &_{t} p_{}( _{t}\,|\,z_{t-1},a_{t-1})_{t}  p_{}(_{t}\,|\,z_{t})\] (1)

The representation model, also known as _posterior_ of \(z_{t}\), approximates latent state \(z_{t}\) from previous state \(z_{t-1}\), previous action \(a_{t-1}\) and current observation \(o_{t}\), while the transition model, also known as _prior_ of \(z_{t}\), predicts it directly from \(z_{t-1}\) and \(a_{t-1}\). The overall models are jointly learned by minimizing the negative variational lower bound (ELBO) [33; 38]:

\[()\ _{q_{}(z_{1:T}|a_{1:T},o_{1:T})} &_{t=1}^{T}- p_{}(o_{t}\,|\,z_{t})-  p_{}(r_{t}\,|\,z_{t})\\ &+_{z}\,\,[q_{}(z_{t}\,|\,z_{t-1},a_{t-1},o_{t })\,\|\,p_{}(_{t}\,|\,z_{t-1},a_{t-1})].\] (2)

Behavior learning of Dreamer can be conducted by actor-critic learning purely on imaginary latent trajectories, for which we refer readers to Hafner _et al._ for details.

[MISSING_PAGE_FAIL:4]

As derived in Appendix B, the overall model can be learned with ELBO of conditional log probability \( p_{}(o_{1:T},r_{1:T} a_{1:T},c)\), without the need to model the complex distribution of contexts \(p(c)\):

\[()}{_{q_{}(z_{1:T}|a_{1:T},o_{1:T})}}_{t=1}^{T} - p_{}(o_{t}\,|\,z_{t},\,c)\\ - p_{}(r_{t}\,|\,z_{t}) \\ +_{z}\,\,[q_{}(z_{t}\,|\,z_{t-1},a_{t-1},o_{ t})\,\|\,p_{}(_{t}\,|\,z_{t-1},a_{t-1})].\]

### Contextualized World Model Architectures

We then introduce a concrete implementation of contextualized latent dynamics models, specifically tailored for visual control. Our approach builds upon Dreamer , which utilizes a Recurrent State Space Model (RSSM) . To enable action-free pre-training from videos and action-conditioned fine-tuning for MBRL, we also incorporate the stacked latent model from APV  into our design. Notably, our work distinguishes itself from previous research through the elaborate design of contextualized components and a dual reward predictor structure that enhances task-relevant representation learning. An overview of the overall architecture is illustrated in Figure 3.

Context formulation.There exist various choices of contextual information to be conditioned on, including text descriptions, pre-trained representations, semantic maps, or more sophisticated structured data . As our focus is on end-to-end visual control, we choose the simplest one solely based on visual observations: we consider the context \(c\) as a single frame of observation randomly sampled from the trajectory segment \(o_{1:T}\): \(c o_{},\,\,\{1,2,,T\}\). It is assumed that contextual information lies equally in each frame, and by randomly selecting a context frame, our context encoder should learn to be robust to temporal variations.

Multi-scale cross-attention conditioning.Given the context in the form of a frame of the same size as the reconstruction \(_{t}\), a U-Net  architecture would be one of the most appropriate choices to empower a contextualized image decoder \(p_{}(_{t}\,|\,z_{t},c)\), as its multi-scale shortcuts help to propagate the information directly from context to reconstruction. Moreover, in our case, the image decoder learns by also conditioning on latent dynamics \(z_{t}\), as shown in Figure 3. While a conventional U-Net architecture incorporates shortcut features directly by concatenation or summation and thus forces a spatial alignment between them, temporal variations such as motions or deformations cannot be neglected in our case. Inspired by recent advances in generative models [59; 4], we augment the decoder feature \(X^{c h w}\) with the context feature \(Z^{c h w}\) by a cross-attention mechanism

Figure 3: Architecture of Contextualized World Models. Building upon the stacked latent model from APV , we empower the image decoder by incorporating a context encoder that operates in parallel with the latent dynamics model. The context encoder captures rich contextual information from a randomly selected context frame and enhances the decoder features of each timestep with a multi-scale cross-attention mechanism that enables flexible information shortcuts across spatial positions. This design encourages the latent dynamics model to focus only on essential temporal variations, while also allowing the decoder to reconstruct complex observations more effectively.

, shown as follows (we refer to Appendix C.3 for details):

\[\ Q =(X)^{hw c},\ K=V =(Z)^{hw c}\] (5) \[\ R =(QW^{Q},KW^{K},VW^{V}) ^{hw c}\] \[\ X =(X+( (R)))^{c h  w}.\]

Dual reward predictors.For simplicity of behavior learning, we only feed the latent variable \(s_{t}\) into the actor \((a_{t}|s_{t})\) and critic \(v(s_{t})\), thus introducing no extra computational costs when determining actions. However, this raises the question of whether the latent variable \(s_{t}\) contains sufficient task-relevant information for visual control and avoids taking shortcuts through the U-Net skip connections. For example, in robotics manipulation tasks, the positions of static target objects may not be captured by the time-dependent \(s_{t}\), but by the time-invariant context \(c\). We remark that reward predictors \(p_{}(r_{t}|s_{t})\) play a crucial role in compelling the latent variable \(s_{t}\) to encode task-relevant information, as the reward itself defines the task to be accomplished. Nevertheless, for hard-exploration tasks such as Meta-world , the video-based intrinsic bonus  may distort the exact reward regressed for behavioral learning and constantly drift during the training process , making it difficult for the latent dynamics model to capture task-relevant signals. Therefore, we propose a dual reward predictor structure comprising a _behavioral_ reward predictor that regresses the exploratory reward \(r_{t}+ r_{t}^{}\) for behavior learning, and additionally, a _representative_ reward predictor that regresses the pure reward \(r_{t}\) to enhance task-relevant representation learning .

Overall objective.The model parameters of ContextWM can be jointly optimized as follows:

\[^{}(,,)_{q_{}(s_{t}|s_{t-1},a_{t-1},z_{t}),q_{}(z_{t},  o_{t-1})}}_{}_{t=1}^{T} (o_{t}|s_{t},\,c)}_{}\] (6) \[(r_{t}+ r_{t}^{}|s_{t} )}_{}- p_{}(r_{t}|s_{t} )}_{} [q_{}(z_{t}|z_{t-1},o_{t})\,]\,\|p_{}(_{t}|z_{t- 1})]}_{}\] \[[q_{}(s_{t}|s_{t -1},a_{t-1},z_{t})\,]\,\|p_{}(_{t}|s_{t-1},a_{t-1})]}_{ }.\]

When pre-trained from in-the-wild videos, ContextWM can be optimized by minimizing an objective that drops action-conditioned dynamics and reward predictors from Eq. (6), similar to APV . For behavior learning, we adopt the actor-critic learning scheme of DreamerV2 . For a complete description of pre-training and fine-tuning of ContextWM for MBRL, we refer to Appendix A.

## 5 Experiments

We conduct our experiments on various domains to evaluate In-the-wild Pre-training from Videos (IPV) with Contextualized World Models (ContextWM), in contrast to plain world models (WM) used by DreamerV2 and APV 2. Our experiments investigate the following questions:

* Can IPV improve the sample efficiency of MBRL?
* How does ContextWM compare to a plain WM quantitatively and qualitatively?
* What is the contribution of each of the proposed techniques in ContextWM?
* How do videos from different domains or of different amounts affect IPV with ContextWM?

### Experimental Setup

Visual control tasks.As shown in Figure 4, we formulate our experiments on various visual control domains. Meta-world  is a benchmark of 50 distinct robotic manipulation tasks and we use the same six tasks as APV . DMC Remastered  is a challenging extension of the widely used robotic locomotion benchmark, DeepMind Control Suite , by expanding a complicated graphical variety. We also conduct an autonomous driving task using the CARLA simulator , where an agent needs to drive as far as possible along Town04's highway without collision in 1000 timesteps. See Appendix C.2 for details on visual control tasks.

Pre-training datasets.We utilize multiple in-the-wild video datasets which can potentially benefit visual control. Something-Something-v2 (SSv2)  dataset contains 193K videos of people interacting with objects. Human3.6M  dataset contains videos of human poses over 3M frames under 4 different viewpoints. YouTube Driving  dataset collects 134 real-world driving videos, over 120 hours, with various weather conditions and regions. We also merge the three datasets to construct an assembled dataset for the purpose of pre-training a general world model. See Figure 1 for example video frames and Appendix C.1 for details on data preprocessing.

Implementation details.For the newly introduced hyperparameter, we use \(_{r}=1.0\) in Eq. (6) for all tasks. To ensure a sufficient capacity for both plain WM and ContextWM pre-trained on diverse in-the-wild videos, we implement visual encoders and decoders as 13-layer ResNets . Unless otherwise specified, we use the same hyperparameters with APV. See Appendix C.3 for more details.

Evaluation protocols.Following Agarwal _et al._ and APV, we conduct 8 individual runs for each task and report interquartile mean with bootstrap confidence interval (CI) for individual tasks and with stratified bootstrap CI for aggregate results.

### Meta-world Experiments

SSv2 pre-trained results.Figure 4(a) shows the learning curves on six robotic manipulation tasks from Meta-world. We observe that in-the-wild pre-training on videos from SSv2 dataset consistently improves the sample efficiency and final performance upon the DreamerV2 baseline. Moreover, our proposed ContextWM surpasses its plain counterpart in terms of sample efficiency on five of the six tasks. Notably, the dial-turn task proves challenging, as neither method is able to solve it. These results demonstrate that our method of separating context and dynamics modeling during pre-training and fine-tuning can facilitate knowledge transfer from in-the-wild videos to downstream tasks.

Ablation study.We first investigate the contribution of pre-training in our framework and report the aggregate performance with and without pre-training at the top of Figure 4(b). Our results show that a plain WM seldom benefits from pre-training, indicating that the performance gain of IPV with plain WM is primarily due to the intrinsic exploration bonus. This supports our motivation that the complex contexts of in-the-wild videos can hinder knowledge transfer. In contrast, ContextWM significantly improves its performance with the aid of video pre-training. Additionally, we evaluate the contribution of the proposed techniques in ContextWM, as shown at the bottom of Figure 4(b). We experiment with replacing the cross-attention mechanism (Eq. (5)) with simple concatenation or removing the dual reward predictor structure. Our results demonstrate that all these techniques contribute to the performance of ContextWM, as both variants outperform the plain WM.

Effects of dataset size.To investigate the effects of pre-training dataset size, we subsample 1.5k and 15k videos from SSv2 dataset to pre-train ContextWM. Figure 6(a) illustrates the performance of ContextWM with varying pre-training dataset sizes. We find that pre-training with only a small subset of in-the-wild videos can almost match the performance of pre-training with the full data. This is probably because, despite the diversity of contexts, SSv2 dataset still lacks diversity in dynamics patterns as there are only 174 classes of human-object interaction scenarios, which can be learned by proper models with only a small amount of data. It would be interesting for future work to explore whether there is a favorable scaling property w.r.t. pre-training dataset size when pre-training with more sophisticated video datasets with our in-the-wild video pre-training paradigm.

Effects of dataset domain.In Figure 6(b), we assess pre-training on various video datasets, including RLBench  videos curated by APV  and our assembled data of three in-the-wild datasets. We observe that while pre-training always helps, pre-training from a more similar domain, RLBench, outperforms that from SSv2. Nevertheless, simulated videos lack diversity and scale and make it difficult to learn world models that are broadly applicable. Fortunately, in-the-wild video pre-training

Figure 4: Example image observations of our visual control benchmark tasks: Meta-world, DMControl Remastered, and CARLA (_left to right_).

can continuously improve its performance with a more diverse assembled dataset, suggesting a promising scalable alternative for domain-specific pre-training.

### DMControl Remastered Experiments

In the top row of Figure 5(a), we present the learning curves of IPV from SSv2 dataset and DreamerV2 on the DMC Remastered locomotion tasks. Remarkably, we observe that pre-training with the SSv2 dataset can significantly enhance the performance of ContextWM, even with a large domain gap between pre-training and fine-tuning. This finding suggests that ContextWM effectively transfers shared knowledge of separately modeling context and dynamics. In contrast, a plain WM also benefits from pre-training, but it still struggles to solve certain tasks, such as hopper stand. These results also suggest that our ContextWM could be a valuable practice for situations where visual generalization is critical, as ContextWM trained from scratch also presents a competitive performance in Figure 5(b).

Effects of dataset domain.Figure 5(b) demonstrates the performance of pre-training on human motion videos from the Human3.6M dataset. However, we observe a negative transfer, as pre-training from Human3.6M leads to inferior performance compared to training from scratch. We argue that the Human3.6M dataset is collected in the laboratory environment, rather than truly _in-the-wild_. These results support our motivation that pre-training data lacking in diversity can hardly help learn world models that are generally beneficial. Additionally, we experiment with pre-training on the assembled data of three video datasets but find no significant improvement over SSv2 pre-training.

### CARLA Experiments

The bottom row of Figure 5(a) displays the learning curves of IPV from the SSv2 dataset and DreamerV2 on the CARLA driving task under different weather and sunlight conditions. Similar to the DMC Remastered tasks, we find that MBRL benefits from pre-training on the SSv2 dataset, despite a significant domain gap. We observe that in almost all weather conditions, IPV with ContextWM learns faster than plain WM at the early stages of training and can also outperform plain WM in terms of the final performance. Minor superiority of ContextWM over plain WM also suggests that in autonomous driving scenarios, a single frame may not contain sufficient contextual information, and more sophisticated formulations of contextual information may further enhance performance.

Effects of dataset domain.We then assess the performance of ContextWM on CARLA by pre-training it from alternative domains. As shown in Figure 6(b), we observe that pre-training from

Figure 5: Meta-world results. (a) Learning curves with in-the-wild pre-training on SSv2 dataset, as measured on the success rate, aggregated across eight runs. (b) Performance of ContextWM and plain WM with or without pre-training (_top_) and performance of ContextWM and its variants that replace the cross-attention mechanism with naive concatenation or remove the dual reward predictor structure (_bottom_). We report aggregate results across a total of 48 runs over six tasks.

the YouTube Driving dataset or a combination of three in-the-wild datasets can both improve upon learning from scratch. However, neither can significantly surpass the performance achieved by pre-training from SSv2, despite a narrower domain gap between YouTube Driving and CARLA. We conjecture that this could be attributed to the higher complexity of the YouTube Driving dataset in comparison to SSv2. It would be promising to explore the potential of scaling ContextWM further to capture valuable knowledge from more complex videos.

### Qualitative Analysis

Video prediction.We visually investigate the future frames predicted by a plain WM and ContextWM on SSv2 in Figure 7(a). We find that our model effectively captures the shape and motion of the object, while the plain one fails. Moreover, we also observe that cross-attentions in our model (Eq. (5)) successfully attend to varying spatial positions of the context to facilitate the reconstruction of visual details. This shows our model works with better modeling of context and dynamics.

Video representations.We sample video clips of length 25 with two distinct labels (_push something from right to left_ and _from left to right_) and visualize the averaged model states of the sampled videos using t-SNE  in Figure 7(b). Note that we do not utilize any labels of the videos in pre-training. Video representations of plain WM may entangle with extra contextual information and thus are not sufficiently discriminative to object motions. However, ContextWM which separately models context and dynamics can provide representations well distributed according to different directions of motion.

Figure 6: DMC Remastered (_top_) and CARLA (_bottom_) results. (a) Learning curves with pre-training on SSv2 dataset, as measured on the episode return, aggregated across eight runs. (b) Performance of ContextWM and plain WM with or without pre-training, aggregated across 24 runs over three tasks. Episode returns of each task in CARLA are normalized to comparable ranges.

Figure 7: Analysis on pre-training datasets. We report aggregated results over tasks. (a) Performance of ContextWM on Meta-world tasks, with varying pre-training dataset sizes. (b) Performance of ContextWM with pre-training on videos from various domains.

Compisitional decoding.We conduct a compositional decoding analysis by sampling a random frame from another trajectory to replace the original context and leaving dynamics the same. As shown in Figure (c)c, ContextWM correctly combines the new context with the original dynamics information. These results show that our model, fine-tuned on the DMCR domain, has successfully learned disentangled representations of contexts and dynamics. In contrast, the plain WM suffers from learning entangled representations and thus makes poor predictions about the transitions.

## 6 Discussion

This paper presents Contextualized World Models (ContextWM), a framework for both action-free video prediction and visual model-based RL. We apply ContextWM to the paradigm of In-the-wild Pre-training from Videos (IPV), followed by fine-tuning on downstream tasks to boost learning efficiency. Experiments demonstrate the effectiveness of our method in solving a variety of visual control tasks from Meta-world, DMC Remastered, and CARLA. Our work highlights not only the benefits of leveraging abundant in-the-wild videos but also the importance of innovative world model design that facilitates knowledge transfer and scalable learning.

Limitations and future work.One limitation of our current method is that a randomly selected single frame may not sufficiently capture complete contextual information of scenes in real-world applications, such as autonomous driving. Consequently, selecting and incorporating multiple context frames as well as multimodal information  for better context modeling need further investigation. Our work is also limited by medium-scale sizes in terms of both world models and pre-training data, which hinders learning more broadly applicable knowledge. An important direction is to systematically examine the scalability of our method by leveraging scalable architectures like Transformers [47; 67] and massive-scale video datasets [18; 48]. Lastly, our work focuses on pre-training world models via generative objectives, which utilize the model capacity inefficiently on image reconstruction to overcome intricate contexts. Exploring alternative pre-training objectives, such as contrastive learning [51; 11] or self-prediction [65; 80], could further release the potential of IPV by eliminating heavy components on context modeling and focusing on dynamics modeling.

Figure 8: Qualitative analysis. (a) Future frames predicted on SSv2. Predictions from our model well capture the shape and motion of the water cup. Moreover, our context cross-attentions from the left upper corner (red box) of different frames successfully attend to varying spatial positions of the context frame. (b) t-SNE visualization of average pooled representations of SSv2 videos with two distinct labels, from a pre-trained plain WM and ContextWM, respectively. (c) Compositional decoding analysis on the DMCR domain, where fine-tuned ContextWM is able to combine the new context with dynamics information from the original trajectory.