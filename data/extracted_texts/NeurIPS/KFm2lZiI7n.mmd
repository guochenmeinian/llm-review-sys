# MeCo: Zero-Shot NAS with One Data and Single Forward Pass via Minimum Eigenvalue of Correlation

Tangyu Jiang

School of Artificial Intelligence

Beijing Normal University

jty@mail.bnu.edu.cn

&Haodi Wang

School of Artificial Intelligence

Beijing Normal University

whd@mail.bnu.edu.cn

Corresponding author.

Rongfang Bie

These authors contributed equally to this work.

School of Artificial Intelligence

Beijing Normal University

rfbie@bnu.edu.cn

###### Abstract

Neural Architecture Search (NAS) is a promising paradigm in automatic architecture engineering. Zero-shot NAS can evaluate the network without training via some specific metrics called zero-cost proxies. Though effective, the existing zero-cost proxies either invoke at least one backpropagation or depend highly on the data and labels. To alleviate the above issues, in this paper, we first reveal how the Pearson correlation matrix of the feature maps impacts the convergence rate and the generalization capacity of an over-parameterized neural network. Enlightened by the theoretical analysis, we propose a novel zero-cost proxy called MeCo, which requires only one random data for a single forward pass. We further propose an optimization approach MeCoopt to improve the performance of our method. We design comprehensive experiments and extensively evaluate MeCo on multiple popular benchmarks. MeCo achieves the highest correlation with the ground truth (e.g., 0.89 on NATS-Bench-TSS with CIFAR-10) among all the state-of-the-art proxies, which is also fully independent of the data and labels. Moreover, we integrate MeCo with the existing generation method to comprise a complete NAS. The experimental results illustrate that MeCo-based NAS can select the architecture with the highest accuracy and a low search cost. For instance, the best network searched by MeCo-based NAS achieves 97.31% on CIFAR-10, which is 0.04% higher than the baselines under the same settings. Our code is available at https://github.com/HamsterMimi/MeCo.

## 1 Introduction

Deep Neural Networks (DNNs) have been ubiquitously adopted in various fields due to their ability in hierarchical feature extraction . It is a common consensus that the architecture of DNN is essential in model performance. Unfortunately, the manual trial-and-error method is non-scalable and inefficient, making it infeasible to find the globally optimal structure. Thanks to the development of the Neural Architecture Search (NAS), it is possible to automatically select the network architecture with the best performance.

There are abundant works proposed to improve the performance of NAS. The most fundamental observation is to fully or partially train the candidate networks with specific data and select the network with the highest accuracy. Though effective, these multi-shot NAS methods are inevitably resource-consuming, thus, are non-trivial to be adopted in real-world applications. To address this issue, the one-shot NAS has been proposed in which a supernet is designed and only requires one-time full training . In 2020, Mellor et al.  proposed the zero-shot NAS and realized an efficient approach in which a zero-cost proxy is proposed to evaluate the networks without training.

Despite the effectiveness of the existing zero-cost proxies, there are some critical issues that remain to be solved. (i) Almost all current proxies are established from the network gradients. As a consequence, computing these proxies requires at least one backpropagation, which is resource-consuming. (ii) The majority of the currently used proxies rely highly on the input data and labels. The reliance on the input samples underestimates the evaluation of the network's intrinsic characteristics, and the incorrect labels (which are commonly occurred in real-world datasets) may obfuscate the ranking results. (iii) Although various zero-cost proxies have been proposed, the performance of those proxies can still be promoted in network evaluation.

**Objectives.** The main objective of this paper is to design a novel zero-cost proxy with better performance than the current methods. The proposed proxy should be easily computed by a small number of data without backpropagation. Furthermore, to estimate the intrinsic trait of the architectures and avoid the label influence, our proxy should be label-independent.

**Our Contributions.** To this end, in this paper, we craft a novel zero-cost proxy for zero-shot NAS from a new perspective. The proposed proxy, called \(\), outperforms the State-Of-The-Art (SOTA) zero-cost proxies in NAS and only requires one data sample for a single forward pass. Specifically, we harness a novel observation on the multi-channel convolutional layers and view them as the "multi-sample fully-connected layers with constraints". This insight permits \(\) to be computed by only one data. Moreover, we rigorously analyze how the Pearson correlation matrix of the feature maps impacts the training convergence rate and the generalization capacity of the networks, which theoretically proves the effectiveness of \(\). Unlike the existing methods [7; 8; 9], \(\) is established upon each layer of the feature maps and hence requires only a single forward pass. Based on our design, we further propose an optimization approach \(_{}\) to promote the performance of our method. The experimental results show that both \(\) and \(_{}\) achieve a higher correlation with the network ground truth than the SOTA methods. For example, the Spearman correlation coefficients of \(\) on NATS-Bench-TSS with CIFAR-10 and CIFAR-100 are 0.89 and 0.88, respectively, which are 0.09 and 0.08 higher than the best baseline method. Our \(\)-based zero-shot NAS also acquires the highest accuracy among all the existing approaches, e.g., \(\)-based NAS selects the network with 97.31% accuracy, which is 0.04% higher than the SOTA work. In all, our major contributions are as follows.

* We theoretically analyze how the Pearson correlation matrix of the feature maps impact the training convergence rate and the generalization capacity of the networks, based on the characteristics of the multi-channel convolutional layers.
* We propose \(\), a novel zero-cost proxy for zero-shot NAS that achieves better performance than SOTA methods. Through systematic analysis, we further proposed \(_{}\) as an optimization approach. Our proxy is fully training-free and data-independent, which requires only one data sample with a single forward pass.
* We rigorously implement our \(\) and extensively design the experiments to evaluate its performance on several popular benchmarks (e.g., NAS-Bench-101, NATS-Bench-TSS, NATS-Bench-SSS, NAS-Bench-301, and Transbench-101) with diverse datasets and random data samples. The experimental results show that our zero-cost proxy outperforms the existing methods, and \(\)-based NAS can select the network with the highest accuracy.

## 2 Related work

### Zero-shot NAS and zero-cost proxy

Zero-shot NAS uses specific metrics called zero-cost proxies to evaluate the networks. Unlike the traditional multi-shot and one-shot strategies [10; 6; 11; 12; 13; 14; 15; 16; 17; 18; 19], zero-shot NAS eliminates the training procedure thus significantly improves the efficiency. Various studies have been brought forward to promote the quality of the zero-cost proxies, and most of them are based on the gradients. Mellor et al.  firstly evaluated the performance of the initialized networks using the activation overlaps between data points. Abdelfattah et al.  proposed a series of parameter-pruning based proxies including _snip_, _grasp_, _synftow_, and _fisher_. Recently, Li et al.  proposed _ZiCo_, which firstly works better than the parameter amounts. These approaches need at least one backpropagation and rely on the data labels to compute the proxies. Another line of work [24; 25; 26] is based on the theory of deep learning such as NTK, which is independent of the labels yet still needs backpropagation. For example, Chen et al.  utilized the condition number of NTK to evaluate the trainability of the networks. Finally, there are a few works that do not require backpropagation or data labels. For instance, Lin et al.  proposed Zen-Score, in which they design an efficient zero-cost proxy with Gaussian random inputs. However, Zen-Score requires multiple forward passes while our MeCo only needs a single forward pass.

### Over-parameterized networks

Over-parameterized networks have received a lot of attention due to their outstanding effect and ease of optimization. Jacot et al.  demonstrated that the training dynamic of an infinite-width network follows a kernel called NTK [29; 30]. For finite-width networks, the training dynamic can be illustrated by a gram matrix [31; 32]. Further, Du et al.  proved that the loss of an over-parameterized network could converge to a global minimum [33; 34; 35], and the training convergence rate could be characterized by the minimum eigenvalue of the gram matrix. Some works also discuss the optimization of the training convergence rate  and the topological properties of DNNs .

## 3 MeCo: minimum eigenvalue of correlation on feature maps

### Preliminaries

**Notations.** We define \([n]=\{1,2,,n\}\). The lower and uppercase bold font represent vectors and matrices, respectively, e.g., \(\) is a vector with entry \(x_{i}\), and \(\) is a matrix with entry \([]_{ij}\). The minimum eigenvalue of \(\) is denoted as \(_{}()\). We define \(\) as Hadamard product between two same-sized matrices \(\) and \(\). \(_{m n}\) represents a \(m n\) matrix filled by ones and \(\|\|_{2}\) is used to represent the \(l_{2}\) norm of a vector. \(N(,)\) and \(U\{S\}\) represent the standard Gaussian distribution and uniform distribution over a set \(\), respectively. We denote by \(=\{(_{i},y_{i})|_{i}^{d 1},y_ {i},i[n]\}\) the training set, where \(_{i}\) and \(y_{i}\) represent the \(i\)-th data and label. \(\{\}\) is defined as the indicator function that demonstrates the event occurrence, such that for event \(\), \(\{\}=1\) if and only if \(\) happened, otherwise it equals to 0.

For input \(^{d 1}\), weight vector \(^{d 1}\) in the weight matrix \(^{d m}\), and output weight \(^{m 1}\), we denote \(f(,,)\) as a neural network with a single hidden layer such that

\[f(,,)=}_{r=1}^{m} a_{r}(_{r}^{T})\] (1)

where \(\) is the activation function. In this paper, we mainly consider the ReLU function due to its effectiveness, i.e., \((z)=z\{z>0\}\). Given a training set \(\), the optimization goal is to minimize the empirical risk loss function

\[L(,)=_{i=1}^{n}(f(, ,_{i})-y_{i})^{2}\] (2)

We follow the definitions in  and define the matrices \((t)\) and \(^{}\) as follows.

**Definition 1** (Gram Matrix).: _For a neural network with a single hidden layer, the gram matrix \((t)^{n n}\) induced by the ReLU activation function on a training set \(:=\{(_{i},y_{i})\}_{i=1}^{n}\) with entry \([(t)]_{ij}\) is defined as:_

\[[(t)]_{ij}=_{r=1}^{m}_{i}^{T} _{j}\{_{r}^{T}(t)_{i} 0,_{r}^{T }(t)_{j} 0\}\] (3)

_where \(_{r}(t)\) is a vector that depends on \(t\). We further construct \(^{}\) with entry \([^{}]_{ij}\) such that_

\[[^{}]_{ij}=_{ N(,) }[_{i}^{T}_{j}\{^{T}_{i} 0,^{T}_{j} 0\}]\] (4)

_We denote \(_{0}:=_{}(^{})\)._

**Remark 1**.: _Gram matrix \((t)\) reflects the prediction dynamic at the \(t\)-th iteration of the network training. Du et al.  proved that for all \(t 0\), \(\|(t)-^{}\|_{2} 0\) as \(m\). Moreover, the gram matrix \(^{}\) has the following key property: if \(_{i}_{j}, i j\), then \(_{0}>0\). The proof of this property can be found in Theorem 3.1 in ._

**Definition 2** (Pearson Correlation Matrix).: \(P()\) _is a Pearson correlation matrix with the \((i,j)\)-th entry \([P()]_{ij}\) as_

\[[P()]_{ij}=(_{i},_{j})=[( _{i}-_{_{i}})(_{j}-_{_{j}})]}{ _{_{i}}_{_{j}}}\] (5)

_where \(_{}\) and \(_{}\) are the mean and standard deviation of \(\), respectively._

**Remark 2**.: _Pearson correlation matrix \(P()\) is closely related to \((t)\). For instance, suppose \(_{1},_{2} N(,)\), \(_{i}^{d 1}\) and \(_{r}^{T}(t)_{i} 0\), for \(r[m]\) and \(i\), then \([(t)]_{12}=(d-1)[P()]_{12}\). We will use this characteristic to approximate the training convergence rate and generation capacity of the networks when designing our proxy._

### The construction of \(\)

To present the theoretical analysis, we first approximate the Convolutional Neural Network (CNN) to an over-parameterized Neural Network (NN) and then craft our proxy.

#### 3.2.1 Back to convolution

The convolutional layers are the fundamental parts of CNN. Suppose we have the input \(_{}^{c_{} w h}\) and the filter \(^{c_{} k k}\). We set the stride size to one and retain the size of the feature maps via zero padding, then the convolutional layer can be formally expressed as:

\[[_{}]_{i,j}=_{c=1}^{c_{}} _{a=-p}^{p}_{b=-p}^{p}[^{c}]_{a+p+1,b+p+1} [_{}^{c}]_{a+i,b+j}\] (6)

where \(_{}^{c}\) (_resp._\(^{c}\)) represents the \(c\)-th channel of \(_{}\) (_resp._\(\)), \(p=(k-1)/2\), \(i[w]\), \(j[h]\). In this paper, we observe that \(_{}\) and \(_{}^{c}\) can be flattened to one-dimensional vectors \(}_{}^{d 1}\) and \([}_{}^{c}]^{d 1}\), where \(c[c_{}]\), \(d=w h\). More concretely, we have the following transformations:

\[}_{}=_{c=1}^{c_{}}_{c}((_{c})^{T}}_{ }^{c})\] (7) \[s.t._{c}^{d d_{h}},[ _{c}]_{ij}=\{j=(c-1)d+i\}\] \[_{c}^{d_{h} d},[_{c}]_{ij} =\{(c-1)d<i cd\},\] (8) \[_{c}\]

Figure 1: Conversion between the multi-channel convolution and multi-sample fully-connected operation. The input size is \(3 w h\) and each filter size is \(3 3 3\). Each input channel can be flattened with size \(d 1\), \(d=w h\). We collect the three flattened samples to obtain the output with constrained fully-connected operations (dot lines for zero operations and solid lines for full connection). The final output is computed by a sum operation (dashed lines) from the hidden nodes.

where \(d_{h}=c_{} d\), \(^{d d_{h}}\) is the weight matrix. Hence, the convolutional layer in CNN is equivalent to a fully-connected layer with constraints, i.e., shown in Equation 8. To further simplify the problem, we relax the second constraint to \(_{c}=_{d_{h} d}\) and ignore the last constraint. As a consequence, we obtain a multi-sample fully-connected network in which each flattened channel \(}_{}^{c}\) is regarded as an independent data sample, and the total number of samples is \(c_{}\). To better illustrate our insight, we present an example of the conversion procedure in Figure 1.

#### 3.2.2 Over-parameterized neural networks

The over-parameterized NNs are competitive in hierarchical feature extraction due to a large number of parameters. One of the typical architectures is the networks with wide hidden layers, which is proved to be tractable in training . In the previous subsection, we convert a multi-channel convolutional layer to a multi-sample fully-connected layer with constraints. We further argue that if the number of hidden nodes in the transformed fully-connected layer is large enough, then it can be viewed as an over-parameterized NN layer. Therefore, the characteristics of the over-parameterized NN can be transferred to CNN. To this end, we present the following theorem  about the convergence rate of the fully-connected NN with a single hidden layer as follows.

**Theorem 1**.: _If gram matrix \(^{} 0\), \(\|_{i}\|_{2}=1\), \(|y_{i}|<C\) for some constant \(C\) and \(i[n]\), hidden nodes \(m=(^{6}}{_{0}^{6}^{3}})\), and i.i.d. initialize \(_{r} N(,)\), \(a_{r} U\{[-1,1]\}\) for \(r[m]\), then with probability at least \(1-\) over the initialization, the following inequality holds:_

\[\|f((t),,)-\|_{2}^{2}t)}\|f((0),,)-\|_{2}^{2}\] (9)

**Remark 3**.: _This inequality shows that \(_{0}\) positively affects the training convergence rate of the network. We further point out that the convergence rate of the network is independent of the label \(\), which makes it possible to measure the network performance with only a single forward pass. Based on this observation, we establish our proxy on the intermediate feature maps to measure the performance of the network._

Based on Theorem 1 and Remark 2, we further analyze the relationship between the minimum eigenvalue of the matrix \(P()\) and the training convergence rate of the over-parameterized network. We summarize our results in the following theorem:

**Theorem 2**.: _Suppose \(f\) is an NN with a single hidden layer and ReLU activation function. Assume \(^{d n}\), \((0) N(,)\), \(P() 0\), \(p_{0}:=_{}(P())\), and hidden nodes \(m=(^{6}d^{2}}{_{0}^{6}^{3}})\), then the following formula holds with probability at least \(1-\) over the initialization_

\[\|f((t),,)-\|_{2}^{2}t)}\|f((0),,)-\|_{2}^{2}\] (10)

_where \(c\) is a constant depending on \(m\), and \(d\)._

Proof sketch.: The key of the proof is to find the relationship between \(_{0}\) and \(p_{0}\) by Hoeffding's inequality and Weyl inequalities . We provide the full proof in Appendix A.1. Theorem 2 shows that \(p_{0}\) can reflect the convergence rate of the network to a certain extent. The advantage of \(P()\) over \((t)\) is that its computation only depends on the data. Therefore, it is feasible to estimate the training convergence rate of the network only through the feature maps.

Other than the convergence rate, we also analyze the relationship between \(p_{0}\) and the _generalization capacity_ of the over-parameterized NN. We present the results in the following theorem:

**Theorem 3**.: _For an over-parameterized neural network with the loss on the testing set as \(L()\). Let \(=(y_{1},...,y_{N})^{T}\), and \(\) be the step of SGD, \(= C_{1}^{T}(^{})^{-1}}/(m )\) for some small enough absolute constant \(\). Under the assumption of Theorem 2, for any \((0,e^{-1}]\), there exists \(m^{*}(,N,_{0})\), such that if \(m m^{*}\), then with probability at least \(1-\), we have_

\[[L()](C^{}^{T} }{p_{0}N}})+(})\] (11)

_where \(C,C^{},\) are constants._

Proof sketch.: The proof of the above theorem derives from the Corollary 3.10 of  and Section D.2 of . We present the detailed proof in Appendix A.2.

#### 3.2.3 New zero-cost proxy: \(\)

The above theorem and analysis indicate the following key insights:

* A multi-channel CNN layer can be viewed as a multi-sample over-parameterized NN layer.
* The minimum eigenvalue of \(P()\) positively affects the training convergence rate and the generalization capacity of an NN, which is independent of the labels.

Enlightened by these observations, we harness the Minimum eigenvalue of the Pearson Correlation matrix upon each layer of the feature maps to craft our novel zero-cost proxy called \(\), such that

**Definition 3** (\(\)).: _Assume the NN \(f(;)\) has a total of \(D\) layers, then \(\) is defined as_

\[:=_{i=1}^{D}_{}(P(f^{i}(; )))\] (12)

_where \(f^{i}(;)\) represents the \(i\)-th feature map with the initialized parameters \(\) on dataset \(\)._

**Remark 4**.: _Note that the Pearson correlation matrix contains the process of data normalization, thus our \(\) eliminates the deviation caused by the data, which is more conducive to discovering the essence of the network architectures._

## 4 Experiments and evaluations

We evaluate \(\) from three aspects: (i) The correlation with the ground truth and comparison with the baseline proxies; (ii) The dependency on data/labels; (iii) Performance of \(\)-based NAS scheme. We first describe the experimental configurations and then give the detailed performance.

### Experimental configurations

**Hardware.** We fully implement our \(\) in Python. We evaluate \(\) on a desktop with an Inter Core i7-12700F CPU and GeForce RTX 3090.

**Benchmarks.** We use several popular benchmarks in NAS. (i) _NATS-Bench-TSS_[41; 42] contains 15,625 CNN architectures with different topologies. We demonstrate the performance of the zero-cost proxied on NATS-Bench-TSS with CIFAR-10 , CIFAR-100 , ImageNet16-120, and three extra datasets  in Appendix E.1. (ii) _NATS-Bench-SSS_ contains 32,768 CNN architectures with the same topology but with different numbers of channels. We show the results on NATS-Bench-SSS with CIFAR-10, CIFAR-100, and ImageNet16-120. (iii) _NAS-Bench-301_ is a surrogate NAS benchmark for the DARTS search space. We illustrate the results with CIFAR-10 and the extra three datasets in Appendix E.1. (iv) _TransBench-101_ is a benchmark composed of two subsets, i.e., TransBench-101-Micro and TransBench-101-Macro. The former is evaluated with 10 datasets and the latter with 7 datasets. We present these results in Appendix E.3. (v) _NAS-Bench-101_ includes 423,624 cell-based CNN architectures, which are trained and evaluated on the CIFAR-10 dataset and shown in Appendix E.2. (vi) _AutoFormer_ and _MobileNet OFA_. The description and results are shown in Appendix E.4.

**Baselines.** The baselines for zero-cost proxies include the number of parameters \(\#\), grasp , fisher , snip , synflow , jacov , grad_norm , NTK , zen , NASWOT , KNAS , NASI , GradSign , and ZiCo . Specific forms of these proxies are provided in Appendix B. We adopt the Spearman rank correlation coefficient \(\) between zero-cost proxies and the dataset ground truth to measure their performance.

### Performance of our \(\) and comparisons

We evaluate \(\) between \(\) and the ground truth, and compare it with the baseline zero-cost proxies on various benchmarks. We present the results on NATS-Bench-TSS and NATS-Bench-SSS in Table 1, and the results on NAS-Bench-301 are provided in Table 2. We run \(\) for ten times with different inputs and present the mean and standard deviation.

**NATS-Bench-TSS.** We run our \(\) and all baseline proxies on 15,625 networks of NATS-Bench-TSS with three public datasets. As we can see in Table 1, \(\) achieves 0.894\(\)0.003, 0.883\(\)0.005,and 0.845\(\)0.004 correlation on CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. The variation shows that our method is stable and independent of the input samples. The Spearman correlation coefficients are significantly higher than the baselines, e.g., the SOTA proxy ZiCo  only achieves 0.80, 0.81, 0.79 on three datasets, which is 0.06 to 0.09 lower than ours. Note that ZiCo is the first proxy that is proved to be consistently better than the naive proxy \(\#\) and requires a batch of input data. On the other hand, we surpass all the previous works and only use one data sample. The high correlation coefficients demonstrate the effectiveness of MoCo, which is consistent with the theoretical analysis. To make our results more intuitive, we sample 2,000 networks randomly and present the relationship between MoCo and the ground truth on three public datasets. The results are shown in Figure 4(a) in the Appendix E, where the trend in the scatter charts effectively proves that our zero-cost proxy has a highly strong correlation with the architecture quality.

**NATS-Bench-SSS.** The results of MoCo on 32,768 networks of NATS-Bench-SSS are summarized in Table 1. Among all the datasets, MoCo shows the highest correlation with the ground truth on CIFAR-100, i.e., \(=-0.87 0.01\), which is 0.07 higher than the best baseline synflow. MoCo also achieves stable and competitive results on CIFAR-10 and ImageNet16-120 with one data as input. We find that MoCo is highly _negatively_ correlated with the test accuracy, which is somehow counterintuitive. In fact, MoCo is sensitive to the number of channels (#channels) that cause this phenomenon. Specifically, the channels are viewed as the data samples as described in Section 3.2.1, This leads to the fact that the more channels contained, the larger the Pearson matrix, which will lead to **smaller**\(p_{0}\). On the other hand, larger #channels will normally lead to higher accuracy of the architectures (yet lower convergence rate). Thus, the variation of #channels in NATS-Bench-SSS(chosen from 8 to 64) leads to a negative correlation on this benchmark. In NATS-Bench-TSS, however, all the networks share the same #channels at each stage, thus MoCo does not show a negative correlation. To verify this, we generate random data with different numbers of channels and calculated \(p_{0}\) respectively, and demonstrate the results in Figure 2. We also sample 2,000 networks randomly and present the correlation of MoCo v.s. test accuracy in Figure 4(b) in the Appendix E.

    &  &  \\   & CIFAR-10 & CIFAR-100 & ImageNet16 & CIFAR-10 & CIFAR-100 & ImageNet16 \\  grasp & 0.39 & 0.46 & 0.45 & -0.13 & 0.01 & 0.42 \\ fisher & 0.40 & 0.46 & 0.42 & 0.44 & 0.55 & 0.47 \\ grad\_norm & 0.42 & 0.49 & 0.47 & 0.51 & 0.49 & 0.67 \\ snip & 0.43 & 0.49 & 0.48 & 0.59 & 0.62 & 0.76 \\ synflow & 0.74 & 0.76 & 0.75 & **0.81** & 0.80 & 0.57 \\ \#Param & 0.72 & 0.73 & 0.69 & 0.72 & 0.73 & 0.84 \\ NWOT & 0.77 & 0.80 & 0.77 & 0.45 & 0.43 & 0.42 \\ jacov & 0.73 & 0.70 & 0.70 & 0.30 & 0.13 & 0.30 \\ NTK & 0.76 & 0.75 & 0.72 & 0.34 & 0.29 & 0.28 \\ zen & 0.38 & 0.36 & 0.40 & 0.69 & 0.71 & 0.87 \\ KNAS & 0.20 & 0.35 & 0.42 & 0.25 & 0.12 & 0.32 \\ NASI & 0.44 & 0.43 & 0.63 & 0.17 & 0.04 & 0.20 \\ GradSign & 0.77 & 0.79 & 0.78 & 0.21 & 0.16 & 0.04 \\ ZiCo & 0.80 & 0.81 & 0.79 & 0.73 & 0.75 & **0.88** \\  MoCo **(Ours)** & **0.894\(\)0.003** & **0.883\(\)0.005** & **0.845\(\)0.004** & -0.79\(\)0.01 & **-0.87\(\)0.01** & -0.86\(\)0.02 \\ MoCo\_opt **(Ours)** & **0.901\(\)0.002** & **0.890\(\)0.003** & **0.850\(\)0.003** & **0.89\(\)0.002** & **0.83\(\)0.004** & **0.89\(\)0.003** \\   

Table 1: Spearman correlation coefficients \(\) of proxies on NATS-Bench-TSS and NATS-Bench-SSS

Figure 2: The influence of different channel numbers of data on the minimum eigenvalue.

**NATS-Bench-301**. We demonstrate the results of MeCo and the baseline proxies on NATS-Bench-301 with CIFAR-10 in Table 2. It can be shown from the results that MeCo and MeCo\({}_{}\) also achieve the highest correlation with the testing accuracy on NATS-Bench-301, which is 0.04-0.05 higher than the SOTA approach ZiCo. More experimental results of MeCo can be found in Appendix E

### Dependency on data

MeCo is designed to be fully data-independent due to the characteristics of Pearson correlation matrix (Theorem 2). To further illustrate this attribute, we generate a random dataset and use it as the input of MeCo and NTK , which is also independent of the data and labels. We compute the Spearman correlation coefficients between MeCo and the test accuracy on three public datasets, and compare them with the baseline approach. All the results for MeCo is ran ten times with random inputs.

More concretely, the random data is generated following a Gaussian distribution and fed into the initialized network. Note that the baseline method requires a batch of random data as input, while MeCo only uses one. We present the results for NATS-Bench-TSS and NATS-Bench-SSS in Table 3.

The results show that even if we use random data as the inputs, MeCo is still able to maintain a high correlation with the corresponding ground truth. For example, \(=0.87\) on NATS-Bench-TSS with CIFAR-10 and \(=-0.93\) on NATS-Bench-SSS with ImageNet16-120, which are significantly higher than the data-independent proxy NTK. Note that most existing proxies rely on the gradients and backpropagation. Hence they cannot utilize the random input data for architecture evaluation, such as ZiCo and jacov.

### NAS with MeCo

We integrate our MeCo with the existing search space and generation strategy to comprise a complete NAS method. Specifically, we use NATS-Bench-TSS, DARTS-CNN , and MobileNet V3  as the search spaces to demonstrate the performance. We use the Zero-Cost-PT  as the generation strategy. The algorithm of MeCo-based NAS and the results on MobileNet V3 are presented in Appendix C and F.3, respectively.

**NATS-Bench-TSS.** We directly evaluate all networks in NATS-Bench-TSS using MeCo and baseline proxies. The average test accuracy of the Top-10 networks on three datasets is summarized in Table 12 in Appendix F.1. As shown in the table, the average test accuracy on CIFAR-10 of MeCo is \(0.08\%\) higher than the best baseline ZiCo. Moreover, we further search the networks using Zero-Cost-PT with different proxies and run the search algorithm three times. The networks we searched have the highest average test accuracy on three public datasets, that is, 93.76%, 71.11%, and 41.44% on CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. The results are provided in Table 13 in Appendix F.1.

**DARTS-CNN.** We compare our MeCo-based NAS with multi-shot, one-shot, and zero-shot NAS schemes in DARTS-CNN spaces. All the NAS schemes are executed on CIFAR-10 and CIFAR-100

    & &  & & \\  Dataset & MeCo & MeCo\({}_{}\) & grasp & fisher & grad\_norm & snip & synflow & l2\_norm & \#Param & zen & jacov & mwot & ZiCo \\  CIFAR-10 & **0.7\(\)0.01** & **0.71\(\)0.01** & 0.34 & -0.28 & -0.04 & -0.05 & 0.18 & 0.45 & 0.46 & 0.43 & -0.04 & 0.47 & 0.66 \\   

Table 2: \(\) between zero-cost proxies and test accuracy on NAS-Bench-301 with CIFAR-10

Figure 3: Spearman correlation coefficients of MeCo with different data size on NATS-Bench-TSS and NATS-Bench-SSS, respectively.

    &  &  \\  Dataset & MeCo & NTK & MeCo & NTK \\ 
**CIFAR-10** & 0.87\(\)0.001 & 0.78 & -0.87\(\)0.000 & 0.23 \\
**CIFAR-100** & 0.85\(\)0.001 & 0.79 & -0.88\(\)0.001 & 0.16 \\
**ImageNet16-120** & 0.83\(\)0.001 & 0.76 & -0.94\(\)0.000 & 0.20 \\   

Table 3: Spearman correlation coefficients of MeCo and NTK with random data on NATS-Bench-TSS and NATS-Bench-SSSdatasets, respectively, to compare the Top-1 errors and the search cost. For zero-shot NAS, we use Zero-Cost-PT as the generation strategy due to its effectiveness. The results on CIFAR-10 are summarized in Table 4, where we denote multi-shot, one-shot, and zero-shot as MS, OS, and ZS, respectively. We present the CIFAR-100 results in Appendix F.2. The results demonstrate that our \(\)-based NAS outperforms the existing baseline methods. More concretely, for zero-shot methods, \(\) outperforms all the baselines for 0.04% to 0.43% on Top-1 error with a similar level of search cost under the same generation strategy. For training-based methods, \(\)-based NAS achieves competitive accuracy but with much less search cost. For instance, our approach requires only 0.08 GPU days and obtains 97.31% accuracy, while DARTS-PT costs 10\(\) GPU days than ours with 0.08% higher accuracy.

We remark that the experimental results in this section demonstrate the following properties of \(\): (i) **High correlation with the ground truth; (ii) **Requies only one data without labels for a single forward pass.** That is because \(\) is established upon the observations over the multi-channel CNN layers. Moreover, the minimum eigenvalue of the Pearson correlation matrix is independent of the data labels, and thus we exert \(\) on the feature maps instead of the gradients.

## 5 Discussion and Optimization

### Optimization method \(_{}\)

In the previous sections, we illustrated the theoretical basis of \(\) and evaluated our proposed proxy on various benchmarks. However, as described in Section 4.2, the negative correlation on NATS-Bench-SSS and some other tasks (e.g., Transbench-101 in the appendix) reflect that \(\) is highly sensitive to the number of channels. Although this phenomenon does not contradict the theoretical results or undermine the effectiveness of \(\), it might be problematic when \(p_{0}=0\). To this end, we present an optimization method on top of \(\) to alleviate the channel-sensitive trait.

Specifically, for convolutional layers, \(p_{0}\) is strictly greater than zero if for \( i j\), \(x_{i} x_{j}\), and \(c<w h\), where \(c\) is the number of channels, \(w,h\) are the size of the inputs. Thus, in real-world applications, \(\) might lose efficacy during the down-sampling procedure. To address this issue, instead of flattening all the channels of the feature map as described in Section 3.2.1, we randomly sample a _fixed_ number of channels and flatten them as matrix \(P^{}\). We then compute the final result by multiplying the minimum eigenvalue of \(^{}\) with a channel weight, such that

\[_{}:=_{i=1}^{D}}{n} _{}(P^{})\] (13)

   Approach &  Test Error \\ (\%) \\  &  Search Cost \\ (GPU Days) \\  & 
 Params \\ (M) \\  & Method \\  AmoebaNet-A  & \(3.34 0.06\) & 3150 & 3.2 & MS \\ PNAS  & \(3.41 0.09\) & 225 & 3.2 & MS \\ DARTS (1st)  & \(3.00 0.14\) & \(1.5\) & 3.3 & OS \\ DARTS-PT & \(2.61 0.08\) & 0.8 & 3.0 & OS \\ SDARTS-RS  & \(2.67 0.03\) & 0.4 & 3.4 & OS \\ SGAS (Cri.1) & \(2.66 0.24\) & 0.8 & 3.7 & OS \\ Eigen-NAS & 7.4 & - & - & ZS \\ TE-NAS  & \(2.63 0.064\) & \(0.05\) & 3.8 & ZS \\  Zero-Cost-PT\({}_{}\) & \(2.96 0.11\) & 0.03 & 5.1 & ZS \\ Zero-Cost-PT\({}_{}\) & \(3.12 0.16\) & 0.05 & 2.5 & ZS \\ Zero-Cost-PT\({}_{}\) & \(2.73 0.10\) & 0.1 & 3.3 & ZS \\ Zero-Cost-PT\({}_{}\) & \(2.88 0.15\) & 0.04 & 3.5 & ZS \\ Zero-Cost-PT\({}_{}\) & \(2.90 0.03\) & 0.04 & 4.0 & ZS \\ Zero-Cost-PT\({}_{}\) & \(2.89 0.09\) & 0.21 & 4.1 & ZS \\ Zero-Cost-PT\({}_{}\) & \(2.80 0.03\) & 0.04 & 5.1 & ZS \\
**Zero-Cost-PT\({}_{}\)(Ours)** & **2.69\(\) 0.05** & **0.08** & 4.2 & ZS \\   

Table 4: Comparisons of \(\)-based NAS with baselines using DARTS-CNN and CIFAR-10where \(c^{(i)}\) is the number of channels in the \(i\)-th layer, and \(n\) is the fixed sampling numbers. The high-level idea of this optimization is to limit the dimension of the Pearson correlation matrix by constraining #channels. Instead of computing upon a large matrix, we calculate the minimum eigenvalue upon a fixed-sized matrix and then enlarge it with corresponding constants. Note that the bonus of this optimization is that the time cost can be controlled because the matrix dimension is significantly reduced compared to the original one.

To show the effectiveness of \(_{}\), we calculate the Spearman correlation coefficients of \(_{}\) on NATS-Bench-TSS, NATS-Bench-SSS (Table 1), NATS-Bench-301 (Table 2 and Table 8), and Transbench-101 (Table 10 and Table 11). All the experiments are conducted under \(n=8\). The results demonstrate that \(_{}\) effectively solves the negative correlation issue on multiple benchmarks, e.g., NATS-Bench-SSS and TransBench-101. Meanwhile, the majority of the positive correlations are promoted up to 0.23, and the remaining results are not impacted.

### Ablation study

**Sample size**. We consider the effect of the sample size on the performance of \(\). We select the size of data from \(\{16,32,64,128,256\}\), respectively, and calculate the Spearman rank correlation coefficient \(\) between \(\) and the test accuracy of the networks on three public datasets. The results are summarized in Figure 3. For NATS-Bench-TSS, \(\) increases rapidly before size 64 and grows slowly after that. The maximum value is acquired at size 128, which is slightly higher than size 64. On the other hand, the overall fluctuation is relatively large on NATS-Bench-SSS. The Spearman correlation coefficients are low when the size is smaller than 32, and reach higher at 32. Then \(\) reduces as the sample size grows larger. The ablation results show that the most appropriate size of random data is around 32 and 64, which is also adopted in our settings.

**Sample numbers**. We explore the performance of \(\) regarding the sample numbers that are used as the inputs. We randomly choose a different number of samples to evaluate the architectures in the entire NATS-Bench-TSS and NATS-Bench-SSS, respectively. The results are shown in Figure 5. It can be demonstrated from the figure that \(\) is relatively stable when varying the number of samples. The performance of \(\) fluctuates around 0.01 throughout the experiments. These results support that \(\) only requires one data sample.

**Fixed channel numbers \(n\)**. We conduct an ablation study on \(n\) v.s. \(_{}\) with NATS-Bench-TSS and NATS-Bench-SSS to illustrate the best choice of \(n\). The results are shown in Table 5.

The results show that with \(n\) growing larger, \(_{}\) increases by 0.1 to 0.5. We set \(n=8\) in our experiments to obtain the best performance.

## 6 Conclusion

In this work, we propose a novel feature-based zero-cost proxy called \(\) and its optimization method \(_{}\). Unlike the existing methods, our zero-cost proxies require only one data for a single forward pass. Specifically, we theoretically approximate a multi-channel convolution layer to an over-parameterized NN layer. We then harness the relationship between the theoretical properties and the minimum eigenvalue of the Pearson correlation matrix to craft our new proxies. We rigorously implement our proxies and extensively design the experiments. The experimental results show that \(\) and \(_{}\) significantly outperforms the SOTA zero-cost proxies, which is also independent of the data and labels. Moreover, \(\) can be integrated into a complete NAS and enables us to efficiently find the architecture with the highest accuracy.

    &  &  \\   & CIFAR-10 & CIFAR-100 & ImageNet16 & CIFAR-10 & CIFAR-100 & ImageNet16 \\  \(n=4\) & 0.87\(\) 0.003 & 0.88\(\) 0.002 & 0.84\(\) 0.005 & 0.88\(\)0.005 & 0.82\(\) 0.003 & 0.84\(\) 0.003 \\ \(n=6\) & 0.88\(\) 0.004 & 0.88\(\) 0.001 & 0.84\(\) 0.004 & 0.88\(\) 0.007 & 0.83\(\) 0.002 & 0.85\(\) 0.006 \\ \(n=8\) & 0.90\(\) 0.002 & 0.89\(\) 0.003 & 0.85\(\) 0.003 & 0.89\(\) 0.002 & 0.83\(\) 0.004 & 0.89\(\) 0.003 \\   

Table 5: Spearman correlation coefficients \(\) of proxies on NATS-Bench-TSS and NATS-Bench-SSS