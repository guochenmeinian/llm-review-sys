# Learning the Optimal Policy for Balancing

Short-Term and Long-Term Rewards

 Qinwei Yang\({}^{1}\), Xueqing Liu\({}^{1}\), Yan Zeng\({}^{1}\), Ruocheng Guo\({}^{2}\), Yang Liu\({}^{3}\), Peng Wu\({}^{1}\)

\({}^{1}\)Beijing Technology and Business University \({}^{2}\)ByteDance Research \({}^{3}\)UC Santa Cruz

Corresponding author: pengwu@btbu.edu.cn.

###### Abstract

Learning the optimal policy to balance multiple short-term and long-term rewards has extensive applications across various domains. Yet, there is a noticeable scarcity of research addressing policy learning strategies in this context. In this paper, we aim to learn the optimal policy capable of effectively balancing multiple short-term and long-term rewards, especially in scenarios where the long-term outcomes are often missing due to data collection challenges over extended periods. Towards this goal, the conventional linear weighting method, which aggregates multiple rewards into a single surrogate reward through weighted summation, can only achieve suboptimal policies when multiple rewards are related. Motivated by this, we propose a novel decomposition-based policy learning (DPPL) method that converts the whole problem into subproblems. The DPPL method is capable of obtaining optimal policies even when multiple rewards are interrelated. Nevertheless, the DPPL method requires a set of preference vectors specified in advance, posing challenges in practical applications where selecting suitable preferences is non-trivial. To mitigate this, we further theoretically transform the optimization problem in DPPL into an \(\)-constraint problem, where \(\) represents the minimum acceptable levels of other rewards while maximizing one reward. This transformation provides intuitive into the selection of preference vectors. Extensive experiments are conducted on the proposed method and the results validate the effectiveness of the method.

## 1 Introduction

Learning an optimal policy for balancing multiple short-term and long-term rewards holds extensive applications across various domains. For instance, content providers can optimize recommendations to avoid short-term clickbait strategies, ensuring sustained user engagement and revenue growth . IT companies can design web pages catering to immediate user preferences while enhancing long-term engagement and satisfaction . Economists explore the effects of early childhood interventions on lifetime earnings, seeking optimal policies (e.g., class size) maximizing short-term test scores and long-term earnings simultaneously . Policymakers can improve job training program design, considering both immediate income impacts and subsequent employment status improvements . Medical practitioners can refine drug prescriptions, considering short-term alleviation and long-term outcomes in chronic diseases like Alzheimer's and AIDS . Marketing professionals can optimize incentive strategies to positively influence customer behavior in both short and long terms .

Despite the importance of balancing multiple short-term and long-term rewards, policy learning methods in this area remain largely unexplored. Recent literature  employs a linear weighting method to achieve this goal. It combines multiple rewards into a single surrogate reward by weighted summation, which is optimized to learn the optimal policy. However, this strategy has several limitations. First, it can only find optimal solutions in convex regions of objective space and cannot obtain the optimal solutions in non-convex regions . Second, it achieves the optimal solution onlywhen the rewards are independent of each other. When some of the rewards are interrelated, it can only achieve sub-optimal solutions . Consequently, although the linear weighting method is easy to implement, the optimality of its solution cannot be guaranteed when balancing multiple objectives.

In this article, we propose a principled policy learning approach for balancing multiple long-term and short-term rewards (objectives). Specifically, we first formulate it as a multiple-objective problem (MOP) and aim to seek the Pareto optimal solutions (policies). A solution is Pareto optimal if improving one objective necessitates worsening other objectives. Then, we propose a novel decomposition-based policy learning (DPPL) method, which involves (1) introducing a set of preference vectors, (2) dividing the whole optimization problem into several subproblems based on the preference vectors, and (3) ultimately achieving different Pareto solutions for the objectives by solving these subproblems. Compared with the linear weighting method, it can obtain Pareto optimal solutions in non-convex regions and is applicable to cases where multiple objectives are interrelated.

While the proposed DPPL method can find Pareto optimal policies, it necessitates specifying a set of preference vectors in advance. In practical applications, decision-makers may encounter the challenge of determining which preference vector to choose. To mitigate this concern, we further theoretically transform the optimization problem in DPPL into an \(\)-constraint problem. This transformation can assist decision-makers in better understanding and selecting preference vectors.

The contributions of this paper are summarized as follows.

\(\) We formulate the policy learning problem of balancing multiple long-term and short-term rewards as a multi-objective optimization problem and propose a decomposition-based Pareto policy learning (DPPL) method to obtain a set of Pareto optimal policies.

\(\) We theoretically establish the connection between the DPPL method and the \(\)-constraint problem, offering an intuitive interpretation of preference vectors and guiding their selection.

\(\) We conduct extensive experiments to demonstrate the effectiveness of the proposed method.

## 2 Problem Formulation

Throughout, we employ bold letters for vectors, uppercase letters for random variables, and lowercase letters for their realization values.

### Notation

We introduce notations to delineate short-term and long-term causal effects. Let \(A\) denote the binary treatment indicator, where \(A=1\) represents the treated group and \(A=0\) represents the control group. \(\) represents the features observed, \(=(S_{1},...,S_{I})^{I}\) and \(=(Y_{1},...,Y_{J})^{J}\) represent the vector of short-term and long-term outcomes, respectively. Both short-term and long-term outcomes are observed after the treatment \(A\), and associations among them may exist.

Utilizing the potential outcome framework , we denote \((a)=(S_{1}(a),...,S_{I}(a))\) and \((a)=(Y_{1}(a),...,Y_{J}(a))\) for \(a=0,1\) as the potential short-term and long-term outcomes under treatment \(A=a\), respectively. We assume that larger short-term and long-term outcomes are preferable. The observed short-term and long-term outcomes \(\) and \(\) correspond to the potential outcomes of the actual treatment, that is, \(=(A)\) and \(=(A)\).

In real-world applications, long-term outcomes often suffer from missing due to prolonged follow-up periods and budget constraints. In contrast, collecting short-term outcomes is more manageable. Therefore, we presume that all short-term outcomes \(\) are observable, while long-term outcomes \(\) may be subject to missing. Let \(=(R_{1},...,R_{J})\{0,1\}^{J}\) denote the indicator for observing the long-term outcome \(\), where \(R_{j}=1\) indicates that \(Y_{j}\) is observed and \(R_{j}=0\) indicates that \(Y_{j}\) is missing. The missingness of \(\) would lead to identifiability and estimation problems [12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23].

### Formulation

In this article, we aim to learn the Pareto optimal policy for balancing multiple correlated short-term and long-term rewards, which has a wide range of application scenarios [1; 6; 8; 24]. Let \(:\{0,1\}\) be a policy that maps from the individual context \(=\) to the treatment space \(\{0,1\}\). For a given policy \(()=(,)\) parameterized by \(\), the policy values for the \(i\)-th short-term outcome \(S_{i}\) and the \(j\)-th long-term outcome \(Y_{j}\) are defined as,

\[(;s_{i})&=[()S_{i}(1)+(1-())S_{i}(0)], i=1,...,I\\ (;y_{j})&=[( )Y_{j}(1)+(1-())Y_{j}(0)], j=1,...,J,\]

which are the \(i\)-th short-term reward and the \(j\)-the long-term reward induced by the policy \(()\).

Conventionally, we convert maximization problems to minimization problems. Let \(}(;s_{i})=-(;s_{i}),}(;y_{j})=-(;y_{j})\). The trade-off among multiple correlated long-term and short-term rewards can be formulated as a multi-objective optimization (MOP) problem given by

\[_{}}()& =(}(;s_{1}),,}( ;s_{I}),}(;y_{1}),,}(;y_{J}))\\ &(}_{1}(),} _{2}(),,}_{M}())\] (1)

where \(M=I+J\) and the symbol \(\) means 'denoted as'. Generally, there is no single solution that can simultaneously optimize all objectives in problem (1) and thus we resort to the Pareto optimality. This concept is employed to define the optimal solutions for the MOP problem.

**Definition 1**.: _(Pareto optimality)_

_(a) Pareto dominance. For two points \(},}\). \(}\) dominates \(}\) if and only if \(}_{m}(})}_{m}( }),\ m\{1,...,M\}\) and \(}_{m^{}}(})<}_{m^{} }(}),\ \ m^{}\{1,...,M\}\)_

_(b) Pareto optimality. \(}\) is a Pareto optimal point if there is no other solution \(}\) that dominates \(}\)._

Pareto optimality refers to a condition where improving one objective comes at the expense of worsening other objectives. The collection of Pareto optimal solutions is called the Pareto set. Our goal is to derive the set of Pareto optimal solutions (or Pareto optimal policies), each of them providing a distinct optimal trade-off among all objectives.

### Identification and Estimation of Short-term and Long-term Rewards

The long-term and short-term rewards are causal parameters that cannot be identified without imposing causal assumptions [25; 26; 27]. Therefore, before seeking the Pareto optimal solutions for balancing multiple long-term and short-term rewards, it is necessary to consider the identification and estimation of long-term and short-term rewards. The proposed method is based on Assumptions 1 and 2 below.

**Assumption 1** (Strong Ignorability).:

_(a) \(((a),(a))\!\!\! A\) for \(a=0,1\);_

_(b) \(0<e()(A=1=)<1\) for all \(\)._

Assumption 1(a) suggests that, given the feature \(\), treatment assignment \(A\) is independent of the potential outcomes \((a)\) and \((a)\). This implies that confounding bias between the treatment \(A\) and the short/long-term outcomes \(((a),(a))\) can be eliminated by conditioning on \(\). Assumption 1(b) ensures that for the subpopulation of \(=\), units with both \(A=1\) and \(A=0\) exist. These assumptions are widely used in causal inference [29; 30; 31; 32; 33; 34; 35; 11; 27].

In addition to confounding bias, we also need to address the selection bias induced by the missingness of long-term outcomes . Thus, we further invoke the Assumption 2.

**Assumption 2** (Missing Mechanism of Long-term Outcome).: _For \(a=0,1\) and \(j=1,...,J\)._

_(a) \(R_{j}\!\!\! Y_{j}(a),(a),A=a\);_

_(b) \(0<r_{j}(,a,)(R_{j}=1=,A=a,=)\)._

Assumption 2(a) can be reformulated as \(R_{j}\!\!\! Y_{j}(,,A)\), which means that \(R_{j}\) relies only on the observed variables \((,A,)\). This assumption also ensures that \((Y_{j}=y|,,A,R_{j}=1)=(Y_{j}=y|,,A,R _{j}=0)\). This implies that we can utilize the available data to draw conclusions about the missing long-term outcome. Assumption 2(b) assumes that the long-term outcome for each unit has a non-zero probability of being observed. Assumptions 1 and 2 ensures the identifiability of \((;s_{i})\) and \((;y_{j})\), as shown in Lemma 1.

**Lemma 1** (Identifiability of Short-term and Long-term Rewards).: _For \(i=1,...,I\) and \(j=1,...,J\), (a) under Assumptions 1, the \(i\)-th short-term reward \((;s_{i})\) is identifiable. (b) under Assumptions 1-2, the \(j\)-th long-term reward \((;y_{j})\) is identifiable._

When we have access to only one short-term outcome and one long-term outcome, Lemma 1 reduces to the identifiability result presented in . In this article, our focus is on achieving the Pareto optimal policy for multiple short-term and long-term rewards. Therefore, for the estimation of \((;s_{i})\) and \((;y_{j})\), we defer it to Appendix A.

## 3 Pareto Policy Learning for Balancing Short-Term and Long-Term Rewards

In this section, we aim to learn Pareto optimal policies for the MOP problem (1). Section 3.1 gives the motivation for this work and Section 3.2 introduces the proposed policy learning approach. In Section 3.3, we theoretically establishe the connection between the linear weighting method, the MOP problem for a given preference vector, and the \(\)-constraint problem. This connection offers an intuitive interpretation and guides practitioners in selecting the preference vector.

### Motivation

For seeking the optimal policy for balancing short-term and long-term rewards, previous work  adopted the linear weighting method. Specifically, the authors formulate the goal as

\[_{}}()=_{m=1}^{M}_{m} }_{m}(),\] (2)

where \(_{m}\) is the pre-specified weight for the \(m\)-th objective. The objective function in the optimization problem (2) is merely a linear combination of multiple objectives from the MOP problem (1). Due to its intuitiveness and simplicity, the traditional linear weighting method is commonly used for solving MOP or multi-task learning problems [36; 37; 38].

The linear weighting method simply combines multiple objectives into a single surrogate objective through weighted summation. While simple, it has several limitations. First, the optimal solution is found only in convex regions and not in non-convex regions . Second, an optimal solution can only be achieved if the objectives are independent of each other. That is, if some objectives are interrelated, only a suboptimal solution can be obtained . Thus, it does not guarantee the superiority of the solution or its solution may deviate from the Pareto optimal solution.

To overcome the limitations of the linear weighting method in , we first introduce a decomposition-based multi-objective optimization algorithm to achieve the Pareto optimal policy. However, this algorithm relies on pre-specified preference vectors, which are used to express a decision maker's degree of preference for multiple conflicting objectives. In practice, the explanation and selection of preference vectors is a challenging problem. To further tackle this issue, we establish a theoretical relationship between preference vectors and the \(\)-constraint method . This relationship provides a clear interpretation on preference vectors, assisting in selecting more suitable ones.

### Pareto Policy Learning for the MOP Problem

We introduce the decomposition-based Pareto policy learning (DPPL) method, which can generate the Pareto set containing policies that are optimum from a trade-off perspective. The main idea of the DPPL method is to first decompose the original MOP problem into several constrained subproblems based on a predefined set of preference vectors, and then obtain a set of Pareto optimal policies by solving these subproblems in parallel .

For obtaining the Pareto optimal policy for balancing \(M\) short-term and long-term objectives, first, we are given a set of \(K\) preference vectors \(\{_{1},_{2},...,_{K}\}\) in \(^{M}_{+}\). Each element of a preference vector specifies the importance of the corresponding short-term or long-term reward. For each preference vector \(_{k}\), the corresponding subproblem is given as

\[&_{}}}()=( }_{1}(),}_{2}(),, }_{M}())\\ & s.t.~{}_{k^{}}()=(_{k^{ }}-_{k})^{T}}}() 0,~{}~{}k^{}=1,...,K, \] (3)where \(_{k^{}}(_{t}) 0\) means that objective space2 of the subproblem is restricted in the subregion \(_{k}\), which is defined by \(_{k}=\{ R_{+}^{M}|_{k^{}}^{T}_{k}^{T} ,\;k^{}=1,...,K\}\). Geometrically speaking, \(_{k}\) represents the set of \(\) that forms the smallest acute angle with \(_{k}\), which means that the optimal solution of the subproblem can be obtained by only searching the subregion. The preference vectors divide the objective space into different subregions.

Solving the subproblem (3) involves the following two steps:

* **Step (a)**. Find a reasonable initial solution \(_{0}\). Specifically, we first randomly generate a solution \(_{r}\) in the full decision space3, and then iteratively update it with the rule \(_{r_{t+1}}=_{r_{t}}+_{r}_{r_{t}}\), where \(_{r}\) is the step size. For a given \(_{r_{t}}\), the descent direction \(_{r_{t}}\) is updated by solving (4). * **Step (b)**. Solving the subproblem (3). The descent direction \(_{t}\) for the \(t\)-th iteration is obtained by \[(_{t},_{t})= _{^{n},}+ ||||^{2}\] (5) \[s.t. }_{m}(_{t})^{T} ,m=1,...,M.\] \[_{k^{}}(_{t})^{T} ,k^{}_{}(_{t}),\] where \(_{}()=\{k^{}|_{k^{}}( )-\}\), and the threshold \(\) is a slack variable used to deal with the solutions near the constraint boundary. We further transform it into a dual problem which will greatly reduce the dimension of decision space. Based on the KKT conditions, we have \(_{t}=-(_{m=1}^{M}_{m}}_{m}(_{t})+_{k^{}_{}()}_{k^{ }}_{k^{}}(_{t}))\). Therefore, the dual problem is given as \[_{_{m},_{k^{}}}-||_{m=1}^{M} _{m}}_{m}(_{t})+_{k^{}  I_{}()}_{k^{}}_{k^{} }(_{t})||^{2}\] (6) \[s.t. _{m=1}^{M}_{m}+_{k^{} I_{}()}_{k^{}}=1,_{m} 0,_{k^{}} 0,  m=1,...,M,\;k^{}_{}().\] where \(_{m} 0\) and \(_{k^{}} 0\) are the Lagrange multipliers for the linear inequality constraints.

Step (a) is to find an initial solution \(_{0}\) that is restricted in a subregion of the subproblem (3), and once a feasible solution is found or a predetermined number of iterations is reached, the step stops. For an given initial solution \(_{0}\), Step (b) is to find the optimal solution \(^{*}\) for the subproblem (3). We summarize the proposed policy learning approach in Appendix B.

**Lemma 2** ().: _Let \((_{t},_{t})\) be the solution to the \(t\)-th iteration of problem (5). (a) If \(_{t}\) is Pareto optimal restricted on \(_{k}\), then \(_{t}=0^{m}\) and \(_{t}=0\). (b) If \(_{t}\) is not Pareto optimal restricted on \(_{k}\). then_

\[_{t}-(1/2)||d_{t}||^{2}<0,\] (7) \[}_{m}(_{t})^{T}_{t} _{t},m=1,...,M\] \[_{k^{}}(_{t})^{T}_{t} _{t},k^{}_{}(_{t}).\]

Lemma 2(a) implies that at the \(t\)-th iteration, no direction (\(_{t}=0\)) can simultaneously improve the performance for all objectives, confirming that the solution \(_{t}\) satisfies Pareto optimality. Lemma 2(b) suggests that if \(_{t}\) does not meet Pareto optimality, then the descent direction \(_{t} 0\) serves as the descent direction for all objectives, such that the solution of the next iteration is closer to the Pareto optimal solution. Thus, Lemma 2 demonstrates that we always attain Pareto optimal solutions for each subproblem using the update rule \(_{t+1}=_{t}+_{r}_{t}\). By solving all subproblems, we can acquire a diverse set of Pareto optimal solutions (or policies) confined to different subregions, even when the multiple objectives are correlated.

### Deep Analysis of the Preference Vector

The DPPL method in Section 3.2 requires a set of pre-specified preference vectors, posing challenges in practical applications where selecting suitable preference vectors is non-trivial. To mitigate this problem, we provide a practical method for decision-makers to select appropriate preference vectors by theoretically establishing the connection between the DPPL method and the \(\)-constraint problem.

We first give a brief introduction to the \(\)-constraint problem , which is defined as follows,

\[_{}\ }_{l}(),\ \ \ }_{m}()_{m}\ \ m=1,,M,m l,\] (8)

where \(_{m}\) is pre-specified threshold. Compared to the MOP problem (1) and the linear weighting objective (2), a notable advantage of the \(\)-constraint problem is its interpretation on the threshold \(_{m}\), which represents the maximum acceptable value (i.e., the acceptable worst-case scenario) for the \(m\)-th objective. In contrast, the weights and the preference vectors in problems (1) and (2) are not straightforward for relating the resulting values of objectives. Thus, if we can establish the connection between \(=(_{1},...,_{M})\) and the preference vector \(_{k}\), then we can provide powerful guidance for choosing appropriate preference vectors.

**Theorem 1**.: _For the preference vector \(_{k}=(u_{k1},...,u_{kM})\) in problem (1), the weights \(=(_{1},....,_{M})\) in problem (2), and the thresholds \(\) in problem (8), the following statements hold:_

_(a) the connection between \(\) and \(\) is given as_

\[_{m}=-[(_{l}()+}{ _{l}}_{m}()>0)_{m}()+h_{m}()],\ \ m=1 M,and\ m l,\] (9)

_where \(_{m}()\) is the conditional average causal effects for \(m\)-th short/long-term outcome,_

\[_{m}()=[S_{i}(1)-S_{i}(0)|],\ \ _{m}\ \ }(,s_{i}),\\ [Y_{j}(1)-Y_{j}(0)|],\ \ _{m}\ \ }(,y_{j}),\]

\(()\) _is the indicator function, and_

\[h_{m}()=[S_{i}(0)|],\ \ _{m}\ \ }(,s_{i}),\\ [Y_{j}(0)|,,R_{j}=1],\ \ _{m}\ \ }(,y_{j}).\]

_(b) the connection between \(\) and \(_{k}\) is given as_

\[_{m}=_{m}+_{k^{}_{}() }_{k^{}}(_{k^{}m}-_{km}),\ \ m=1,,M,\] (10)

_where \(_{m}\) and \(_{k^{}}\) are defined in Eq. (6), and \(_{}()=\{k^{}|_{k^{}}( {})-\}\) defined in Eq. (4)._

Theorem 1 (see Appendix C for proofs) establishes a link between the preference vector \(_{k}\) and \(\) through \(\) in scenarios involving multiple long-term and short-term objectives. Specifically, Theorem 1(a) shows how to estimate the threshold \(\) for given weights \(\), and Theorem 1(b) shows how to assign weights \(\) via preference vectors \(_{k}\). This means that for the subproblem determined by preference vectors \(_{k}\), we can ascertain the maximum acceptable threshold \(\) based on Theorem 1, thereby offering an intuitive interpretation of the preference vector \(_{k}\).

There are several practical implications with Theorem 1. On one hand, it assists decision-makers in better understanding and selecting preference vectors in practical applications. In practice, we can initially pre-specify a set of preference vectors \(\{_{1},_{2},...,_{K}\}\) in \(_{+}^{M}\), then derive the weights \(\) corresponding to each preference vector \(_{k}\) through Eq. (10), and finally substitute the obtained weight \(\) into Eq. (9) to calculate the threshold \(\). Leveraging the intuitive interpretability of the threshold \(\), decision-makers can select the appropriate preference vectors according to their specific requirements. On the other hand, it also provides guidance for specifying \(\) in the \(\)-constraint problem (8). Inappropriate selection of \(\) for this problem may result in an empty feasible region, yielding empty solutions. By utilizing a set of preference vectors, we can efficiently screen out some reasonable choices of \(\) and reduce the cumbersome trial-and-error process of testing different \(\).

In conclusion, by establishing the connection between the DPPL method and the \(\)-constraint problem, we can harness the advantages of both methods while mitigating their respective weaknesses.

Experiments

**Datasets.** Following the previous studies , we use two widely used datasets: IHDP and JOBS, for evaluating the performance of the proposed method. The IHDP dataset explores the effectiveness of high-quality home visiting in promoting children's future cognitive development and covers a sample of 747 units, including 139 treated and 608 controlled. In addition, the dataset has 25 characteristics that provide a comprehensive picture of the children and their mothers. The second dataset, JOBS, explores the effects of job training on income and employment status. It consists of 2,570 units (237 treated, 2,333 controlled), with 17 covariates. Note that each unit in both datasets has only one observed outcome from a single treatment, and neither dataset collects long-term outcomes.

**Simulating Outcome.** Consider the case of one long-term reward and one short-term reward. Following the previous data-generation mechanisms , for the \(n\)-th unit (\(n=1,...,N\)), we simulate the potential short-term outcomes \(S(0)\) and \(S(1)\) as follows:

\[S_{n}(0)((w_{0}X_{n}+_{0,n})), S_{n}(1) ((w_{1}X_{n}+_{1,n})),\]

where \(()\) is the sigmoid function, \(w_{0}_{[-1,1]}(0,1)\) follows a truncated normal distribution, \(w_{1}(-1,1)\) follows a uniform distribution, \(_{0,n}(_{0},_{0})\) and \(_{1,n}(_{1},_{1})\). We set \(_{0}=1,_{1}=3\) and \(_{0}=_{1}=1\) for the IHDP dataset, and we set \(_{0}=0,_{1}=2\) and \(_{0}=_{1}=1\) for the JOBS dataset. For generating long-term potential outcomes \(Y(0)\) and \(Y(1)\), we introduce the time step \(t\): we set the initial value at time step 0 as: \(Y_{0,n}(0)=S_{n}(0)\), \(Y_{0,n}(1)=S_{n}(1)\), then generate \(Y_{t,n}(0),Y_{t,n}(1)\) according to the following equation and we eventually regard the outcome at the last time step \(T\) as the long-term outcome, \(Y_{n}(0)=Y_{T,n}(0),Y_{n}(1)=Y_{T,n}(1)\).

\[Y_{t,n}(0)((_{0}X_{n})+C_{t^{}=0}^{t-1}Y _{t^{},n}(0))+_{0,n},\;Y_{t,n}(1)(( _{1}X_{n})+C_{t^{}=0}^{t-1}Y_{t^{},n}(0))+_{1,n},\]

where \(_{0}\) is randomly sampled from \(\{0,1,2,3,4\}\) with probabilities \(\{0.5,0.2,0.15,0.1,0.05\},_{1} 4_{}(0,1)\), and \(C=1/T\) is a scaling factor. For \(_{0,n}\) and \(_{1,n}\), we set \(_{0}=_{1}=0,_{0}=1\) and \(_{1}=3\) for the IHDP dataset and set \(_{0}=_{1}=0,_{0}=1\) and \(_{1}=1\) for the JOBS dataset.

Assumption 2 shows that observing indicator \(R\) depends on the feature \(\), the treatment \(A\), and short-term outcome \(S\). For a given missing rate \(r\), we select the missing indexes for \(Y\) and derive the missing indicator \(R\) according to the following criterion: calculate the \(m_{n}=1/D_{d=1}^{D}(X_{nd}+s_{n}),n=1,,N,\) and choose the index of the row with the smallest \(rN\) values in \(\{m_{n},n=1,,N\}\) as the missing indexes. \(D\) is the feature dimension and \(N\) is the sample size.

**Experimental Details.** In this paper, preference vectors are used to quantify an individual's preference for different objectives in the multi-objective optimization problem. For the case of two-objective, we randomly generate 10 unit preference vectors \((_{1},_{2},,_{10})\), where \(_{k}=(u_{k1},u_{k2})\), \(u_{k1}=cos(t_{k}),u_{k2}=sin(t_{k})\), \(t_{k}(0,1)\), which implies that the \(L_{2}\)-norm of the preference vectors is 1, ensuring the consistency and comparability of the preference measures. \(u_{k1}\) and \(u_{k2}\) are the preferences for the short-term objective and the long-term objective, respectively. Each component of the preference vector \(_{k}\) represents the strength or importance of the decision maker's preference for different objectives. Preference vectors are used as weights in the linear weighting method, whereas our method uses them to divide the original problem (1) into several subproblems.

**Evaluation Metrics.** We measure the performance of our proposed method by three metrics: long and short-term rewards, the variance of long and short-term rewards, and the change in welfare. Formally, the short-term reward of the learned policy \((,)\) is \(}(;s)=_{n=1}^{N}[(X_{n},)S_{ n}(1)+(1-(X_{n},))S_{n}(0)]\), the long-term reward is \(}(;y)=_{n=1}^{N}[(X_{n},)Y_{ n}(1)+(1-(X_{n},))Y_{n}(0)]\). Similar as , the welfare changes are defined as \( W_{s}=_{n=1}^{N}[(S_{n}(1)-S_{n}(0))(X_{n},)]\) for the short-term reward, \( W_{y}=_{n=1}^{N}[(Y_{n}(1)-Y_{n}(0))(X_{n},)]\) for the long-term reward, \( W=0.5 W_{s}+0.5 W_{y}\) for the overall balanced-base reward. Among these metrics, \( W\) is the most critical here, as it directly measures the balance reward achieved by the learned policy.

**Policy learning with short-term and short-term reward.** We choose MLP as the policy model \(()\), and we average over 50 independent trials of policy learning with the short-term and long-term reward in IHDP and JOBS. We fix the missing ratio \(r=0.2\) and the time step \(T=4\). We measure the uncertainty of the model by calculating the variance of the long and short-term reward over 50 experiments, and a smaller variance means a more stable model performance.

**Performance Comparison.** From our previous analyses, the linear weighting method generally achieves the sub-optimal policies. The proposed DPPL method can generate a set of Pareto optimal policies. First, for the long-term reward, the short-term reward, and \( W\), it is not surprising to observe that for most of the preference vectors, DPPL's solutions have better performance. Second, for the variance, our method performs more stable in 50 experiments. Because we will divide the original problem into several subproblems according to preference vectors, and then solve the subproblems in a relatively small subregion to obtain the Pareto optimal solution, whereas the linear weighting method searches the entire objective space. The associated results are displayed in Table 1. More experimental results with missing ratio \(r=0.3\) are given in Appendix D.

**Sensitivity Analysis.** We perform the sensitivity analysis of missing ratio \(r\) and time step \(T\) on JOBS. Our method achieves better performance in all missing rates \(r=[0.2,0.3,0.4,0.5]\) with \(T=4\), and \(r=0.2\) with time step \(T=\). Our method stably outperforms the linear weighting method under varying \(r\) and \(T\), even in scenarios with a high missing ratio or a large time step. This further illustrates the effectiveness of our method. The associated results are displayed in Figure 1.

**Interpretation on preference vectors.** By Theorem 1, for the set of pre-specified preference vectors \((_{1},_{2},,_{10})\), we transform the optimization subproblem corresponding to each preference vector into the \(\)-constraint problem as \(_{}}(;y),s.t.}( {};s)(<0)\) or

    &  &  &  & \)} &  \\  Preference Vector & OURS & LW & OURS & LW & OURS & LW & OURS & LW & OURS & LW \\ 

[MISSING_PAGE_POST]

*1585.800** & **1225.679** & 1225.119 & **156.582** & 143.112 & **59.285** & 88.939 & 95.054 & **85.443** \\   

Table 1: Comparison of our method (OURS) and linear weighting method (LW) on IHDP and JOBS, with Short-Term Reward (S-REWARDS) and Long-Term Reward (L-REWARDS), \( W\) and Variance (S-Var and L-Var) as evaluation metrics. The best result is bolded.

Figure 1: Comparison of two methods with different missing ratios \(\{0.2,0.3,0.4,0.5\}\) on JOBS\(_{}}(;y),s.t.}(;s) -\) and the threshold \(-\) are shown in Table 2. This value of \(-\) is the minimum value of the short-term reward that the decision maker can accept while maximizing the long-term reward. Our results show that as the second component of the preference vector increases, the value of \(-\) shows a decreasing trend. In essence, this signifies that a decision-maker who emphasizes the long-term reward must necessarily loosen constraints on the short-term reward. In practice, decision makers can determine the threshold based on their specific needs for the short-term reward, and then select the most appropriate preference vector from the set of pre-specify preference vectors with the help of the intuitive interpretability of the threshold according to Table 2. More experimental results with different missing ratios \(\{0.3,0.4,0.5\}\) are provided in Appendix D.

## 5 Related Work

**Estimation of long-term causal effects.** Assessing long-term causal effects is challenging due to the delayed long-term outcomes, posing significant difficulties in both identification and estimation. Recently, there has been increasing interest in using short-term surrogates to identify and estimate long-term causal effects, such as . In contrast to these previous works focusing on long-term causal effects, this paper aims to balance multiple short-term and long-term causal effects.

**Trustworthy policy learning.** Trustworthy policy learning ensures that the learned policies or models are reliable and dependable for practical applications. Traditional policy learning aims to identify individuals who would maximize the utility function based on their features if treated . Recently, trustworthy policy learning has focused on ensuring that the learned policy adheres to principles such as benefcence, non-maleficence, autonomy, justice, no-harm, and explicability . Various counterfactual-based metrics have been suggested to assess a policy's trustworthiness . In this paper, we complement this series of work by developing a principled policy learning approach that can effectively balance multiple rewards.

**Multi-objective optimization (MOP)**. MOP aims to find compromises or trade-offs among multiple possibly contrasting objectives. It is widely used in the field of machine learning such as multi-task learning , neural architecture search , and multi-objective reinforcement learning . We extend these works to a new setting by learning the optimal policy for balancing multiple long-term and short-term rewards. Additionally, we provide a practical method for interpreting and selecting preference vectors with theoretical guarantees.

## 6 Conclusion

In this paper, we focus on learning the optimal policy for balancing multiple long-term and short-term rewards. We reveal the limitations of the previous linear weighting method, which usually results in sub-optimal policies in practice. To address these limitations, we formulate the policy learning problem as a multi-objective optimization problem and then propose the novel DPPL method to learn optimal policies. The DPPL method obtains a set of Pareto optimal policies by solving a series of subproblems based on pre-specified preference vectors, effectively balancing multiple objectives. Furthermore, we theoretically establish the connection between the optimization subproblems in the DPPL method and the \(\)-constraint problem. This connection aids decision-makers in better understanding and selecting preference vectors. We conducted extensive experiments on two benchmark datasets which validate the effectiveness of our proposed method. A limitation of this work is that it focuses on discrete treatments in identification and estimation (Section 2.3). In some application scenarios, continuous treatments (e.g., price) are of interest. Further investigation is required to extend the proposed method to accommodate such cases.

   Preference Vector & \(\) \(\) on IHDP & \(\) \(\) on JOBS & Preference Vector & \(\) \(\) on IHDP & \(\) \(\) on JOBS \\  (1.00, 0.00) & 0.820 & 0.878 & (0.00, 1.00) & 0.522 & 0.737 \\ (0.98, 0.17) & 0.827 & 0.875 & (0.17, 0.98) & 0.522 & 0.716 \\ (0.94, 0.34) & 0.826 & 0.868 & (0.34, 0.94) & 0.511 & 0.704 \\ (0.86, 0.50) & 0.833 & 0.868 & (0.50, 0.86) & 0.557 & 0.746 \\ (0.77, 0.64) & 0.741 & 0.865 & (0.64, 0.76) & 0.659 & 0.808 \\   

Table 2: The \(\) values correspond to each preference vector in IHDP and JOBS datasets, where \(T=4\) and \(r=0.2\), obtained according to Theorem 1.