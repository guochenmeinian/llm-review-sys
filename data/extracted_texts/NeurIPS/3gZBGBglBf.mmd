# Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals

Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal correlations. The watermelon EEG dataset collected in this work can be obtained at Zenodo: https://zenodo.org/records/11238929, and all the codes of this work can be obtained in the supplementary materials.

## 1 Introduction and related works

A brain-computer interface (BCI) is a type of human-machine interaction that bridges a pathway from the brain to external devices . Electroencephalogram (EEG) has emerged as a valuable tool for BCI because of its high time resolution, low cost, and good portability , and algorithms of neural decoding from EEG signals play a role in its practical applications. Recently, deep learning methods have been developed widely for various EEG decoding tasks, and high decoding accuracy was reported. For example, in the task of decoding image classes with EEG recordings, when subjects were required to watch images of different classes, a decoding accuracy of 82.90% was reported for the 40-way classification by Spampinato et al. . With their EEG dataset, subsequent studies reported a higher decoding accuracy (98.30%, ), high performance on image retrieval, and even image generation from EEG [5; 6; 7].

However, it remains unclear what kind of EEG features are learned by the DNN-based models. Some researchers have posited that the high decoding accuracy on the image-evoked EEG dataset was attributed to the block-design paradigm during EEG recording [8; 9; 10], in which 50 images with the same class label were presented to the subject continuously in one block, and the 40 image-classes were presented as 40 separate blocks. Due to the existence of temporal autocorrelation of EEG signals, i.e., the temporally nearby data is more similar than the temporally distal [11; 12; 13; 14], the models could learn the block-related features rather than the image-related.

To verify their concerns, Li et al.  recorded EEG with two experimental designs: block design and rapid-event design. For the rapid-event design, images across the 40 classes were presented alternately and randomly. When the same DNN model was used, it was found that the decoding accuracy was close to Spampinato et al.  with the block-design EEG data, but it was dramatically decreased to the chance-level (2.50%) with the rapid-event design data. Subsequent work also confirmed the low decoding accuracy for EEG recorded with rapid-event design [9; 10]. However, Palazzo et al.  proposed that temporal autocorrelations only play a marginal role in EEG decoding tasks because they found that EEG data recorded during rest periods (temporal proximity to adjacent blocks) could not be successfully classified as the preceding block label or the succeeding block label. They also argued that the rapid-event design seemed to weaken the image-related neural responses due to the possible cognitive load and fatigue effect compared to the block design. Some researchers [15; 16; 17; 18] pointed out that block design is essential because humans tend to react more consistently and respond faster when conditions are presented in blocks [19; 20]. Wilson et al.  advised that classification work that decodes from block design datasets is the most suitable approach until advances are made to reduce noise.

Although the pitfall of overestimated decoding accuracy has been mainly discussed in image neural decoding tasks, we noticed that similar pitfalls might also exist in various EEG decoding tasks such as in auditory spatial attention detection (ASAD) tasks [21; 22; 23; 24], which involves decoding the subjects auditory attention locus from neural data, and in emotion recognition task [25; 26; 27], which involves recognizing the subjects emotion type from neural data. Researchers have also found that splitting a continuous EEG from a specific experimental condition into training and test sets would bring higher decoding accuracy in epilepsy detection tasks , motor imagery decoding tasks , and so on. All those high decoding accuracy works share the common characteristic: continuously recorded EEG data of a specific class (condition) label are divided into training and test sets (see the top-left of Figure 1).

Although some studies have mentioned the overestimated decoding accuracy and tried to remind the possible pitfall [8; 30], it is difficult to discriminate the influence of the inherent temporal autocorrelation in EEG signals due to the coupling of stimuli-driven neural responses and the temporal autocorrelations. More importantly, due to the lack of an effective formalization, there is not an adequate explanation of how models utilize temporal autocorrelation features for decoding. Furthermore, their concerns only focused on one specific decoding task, and the results and conclusions cannot be generalized to general BCI decoding tasks.

In this work, the pitfall of various EEG decoding tasks was formulated with a unified framework. To completely decouple the temporal autocorrelation features from stimuli-driven neural responses, EEG data were collected from 10 watermelons in this work to construct "Watermelon EEG". This method is known as phantom EEG in previous studies [31; 32; 33; 34; 35; 36], and the EEG data exclude stimulus-driven neural responses while reserving the temporal autocorrelation features. For comparison, a human EEG dataset was also adopted. The watermelon EEG and human EEG were reorganized into three classic neural decoding EEG datasets following their EEG experimental paradigm: image classification (CVPR, ), emotion classification (DEAP, ), and auditory spatial attention decoding (KUL, ), resulting in six EEG datasets. A sample CNN-based decoding model was used to complete the decoding tasks with the corresponding EEG dataset, and the experimental results revealed that:

1. When the pitfall was formulated with a unique framework, and the temporal autocorrelation was defined as domain features, high decoding accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features.

2. The pitfall exists not only in classification but also widely in EEG-image joint training without explicit labels and even image generation.
3. Splitting a continuous EEG with the same class label into training and test sets should never be used in future BCI decoding works.

## 2 Method

The section is organized by: the pitfall is formulated in Subsection 2.1, and the datasets used are introduced in Subsection 2.2. Then, the methods to finish different classification tasks are introduced in Subsection 2.3, and joint training and image generation from EEG are introduced in Subsection 2.4. Some implementation details and statistical analysis method are described in Subsection 2.5.

### Problem Formulation

In some BCI works on domain generalization , all EEG data from a dataset  or from a subject  are usually regarded as a domain to emphasize EEG pattern distribution differences between datasets or subjects. Adopted from this concept, we regard a period of continuous EEG data with the same class label as a domain. In some BCI works [3; 4; 21; 22; 23; 24; 25; 26; 27], researches segment the EEG data from the same domain into samples and further split the samples into training and test data (as shown in Figure 0(a)) and complete decoding task, such as classification, retrieval and generation (as shown in Figure 0(b)). In these cases, the models used in these works would learn the coupled features containing the class-related feature and domain feature (as shown in the middle of the Figure 0(c)). The underlying assumption of these works is that the domain feature plays only a margin role in EEG decoding tasks as shown in the left of the Figure 0(c). However, we assumed that the domain feature contributes to the high decoding accuracy as shown in the right of the Figure 0(c), which is the pitfall we mentioned in Section 1.

To validate our assumption, we need to formulate the pitfall. Denote \(D\) as the domain set, and each domain \(d D\) contains many samples. We use \(S^{d}\) to denote the sample set of the domain \(d\). The notation \(x_{i}^{d}\) represents the \(i\)-th sample (e.g., a 0.5-second EEG data corresponding to watching a specific image) of domain \(d\), which is associated with class \(y_{i}^{d}\) (e.g., the class label panda of the

Figure 1: Overestimated decoding performance in BCI works. (a) Continuous EEG data in a certain experimental condition (with the same class label) are split into training and test sets for decoder training and evaluation. (b) With the test EEG sample input, the decoder gives output in the forms of classification, retrieval, and generation. (c) Decoders may use both domain features or class-related features for decoding.

watched image). Considering the temporal autocorrelation of the EEG data, the domain features of data within the same domain are more similar, while the domain features of data in different domains are more distinct.

For EEG decoding tasks, we assume the data is generated from a two-stage process. First, each domain is modeled as a latent factor \(z\) sampled from some meta domain distribution \(p()\). Second, each data sample \(x\) is sampled from a sample distribution conditioned on the domain \(z\) and class \(y\):

\[z p(),x p(|z,y)\] (1)

Given the sample \(x\), the aim of a specific EEG decoding task is to uncover its true class label using the posterior \(p(y|x)\). The quantity can be factorized by the domain factor \(z\) as,

\[p(y|x)= p(y,z|x)dz= p(y|x,z)p(z|x)\] (2)

When we use the Watermelon EEG dataset or use a dataset that is completely unrelated to the current task (e.g., decoding images from an auditory EEG dataset), the class-related feature has none possibility to exist in EEG samples. In this condition, \(p(y|x,z)=p(y|z)\) and the equation (2) can be modified as:

\[p(y|x)= p(y,z|x)dz= p(y|z)p(z|x)\] (3)

The assumption of this work is that the model could also deduce \(p(y|x)\) by learning \(p(y|z)\) and \(p(z|x)\) even there is none class-related feature exists. In other words, we assumed that it could also achieve high decoding accuracy on different EEG decoding tasks when using the Watermelons EEG dataset.

### Dataset

**Watermelon EEG Dataset** Ten watermelons were selected as subjects. EEG data were recorded with a NeuroScan SynAmps2 system (Compumedics Limited, Victoria, Australia), using a 64-channel Ag/AgCl electrodes cap with a 10/20 layout. An additional electrode was placed on the lower part of the watermelon as the physiological reference, and the forehead served as the ground site (see Appendix A.1 for photography). The inter-electrode impedances were maintained under 20 kOhm. Data were recorded at a sampling rate of 1000 Hz. EEG recordings for each watermelon lasted for more than 1 hour to ensure sufficient data for the decoding task. We refer to the dataset consisting of EEG recordings of 10 watermelons as the Watermelon EEG Dataset.

**SparrKULee Dataset** SparrKULee dataset is a speech-evoked EEG dataset from the KU Leuven University containing 64-channel EEG recordings from 85 participants, each of whom listened to 90-150 minutes of natural speech. We used this dataset because EEG recordings were longer than 1 hour to ensure a sufficient amount of data for each subject. To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects (ID: Sub7-Sub16) from the SparrKULee Dataset were used.

**Dataset reorganization and dataset segmentation** The term "reorganization" refers to segmenting continuous EEG into samples and assigning each sample a class label and a domain label according to the referenced experimental design. Here, we follow the experimental designs of three classical published EEG datasets to reorganize the Watermelon EEG Dataset and SparrKULee Dataset. These three datasets were collected respectively for image decoding, emotion recognition, and ASAD tasks.

For the image decoding task, we referred to the experimental design of the CVPR dataset . For the CVPR dataset, 40 classes of images were presented in a block-design paradigm. Specifically, 50 different images of the same class were presented continuously in a block, with each image lasting for 0.5 second, resulting in 40 blocks of presentation for each subject. The 0.5-second length EEG data of the same class were split into training, validation, and test sets in a ratio of 8:1:1 [4; 3]. Following this experimental design and dataset segmentation, we segment continuous EEG from the Watermelon EEG Dataset and SparrKULee Dataset into blocks and assign a unique class label and a unique domain label for each block. The interval between adjacent blocks is set to 10 seconds to match the rest time of the subjects during the EEG recording in the CVPR dataset. Then, EEG data in each block are further segmented into 50 0.5-s length samples. Since the EEG data in the CVPR dataset has 128 channels, we replicated our 64-channel EEG in the channel dimension. The reorganized datasets for Watermelon Dataset and SparrKULee Dataset are called WM-CVPR and SK-CVPR, respectively. Here, we use the "A-B" naming format, where the left side of "-" represents the source dataset (WM: watermelon dataset, SK: SparrKULee Dataset), and the right side of "-" represents the dataset of which the experimental design is referenced. For the emotion recognition task and ASAD task, the DEAP dataset and the KUL dataset are used as the referenced dataset, resulting in WM-DEAP, SK-DEAP, WM-KUL, and SK-KUL. More details for reorganization can be found in Appendix A.2.

### Classification tasks

**Model.** To demonstrate that domain features are strong and easy to be learned by the network, we used a simple CNN (or some parts of this CNN) to complete all classification tasks mentioned in this work. The CNN network includes a layer-norm layer, a 2D-convolutional layer (output channel: 100), an averaging pooling layer, and two fully connected layers. The kernel size of the 2D-convolutional layer depends on the channel number and sampling frequency of the input EEG. The node number of the output fully connected layer depends on the number of classes.

**Decoding the domain feature** To demonstrate that the model can predict the domain factor \(z\) from EEG input sample \(x\), which relates to learning posterior \(p(z|x)\), a domain label classification was adopted on the six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP and SK-KUL dataset) with a simple CNN classifier. The splitting strategy leave-samples-out was used, which means that all sample were randomly split into training set, validation set and test set. The outputs after the averaging pooling layer were selected as domain feature representation, and t-SNE was utilized for dimensionality reduction and visualization.

**Decoding the class label from the domain feature** To demonstrate that the model can predict the class label \(y\) from the domain factor \(z\), which relates to learning posterior \(p(y|z)\), a class label classification was adopted on the four datasets (classification on the WM-CVPR dataset and SK-CVPR dataset are unnecessary since domain labels and class labels are one-to-one correspondence) using a single network with two linear layers and an intermediate sigmoid function.

**End-to-end classification** To demonstrate that the model can predict the class label \(y\) from the EEG input sample \(x\) directly when samples in the training set and test set are from common domains, a class label classification was adopted on the six datasets with the simple CNN classifier. The splitting strategy leave samples out was used. Classification on the WM-CVPR dataset and SK-CVPR dataset is the same since domain labels and class labels in the two datasets are one-to-one correspondence. To demonstrate that the model indeed used the domain feature to complete the end-to-end classification, the splitting strategy leave domains out was used on the four datasets (i.e., WM-DEAP, WM-KUL, SK-DEAP, and SK-KUL dataset) in which samples in the same domain only appear in the training set or the test set.

**Zero-shot classification** In a recent work , EEG data from 34 classes within the CVPR2017 dataset were used to train an EEG encoder, and the remaining 6 unseen classes were used for testing. The results showed that features of different unseen classes clustered in distinct groups on the two-dimensional t-SNE plane. Similar analyses were conducted on the SK-CVPR and WM-CVPR datasets. Six classes were selected for testing, and the remaining 34 classes were for training. The simple CNN was used to predict class labels from input EEG samples, and the outputs from the average pooling layer were chosen as the EEG feature representation. Two strategies were employed for selecting the 6 test classes: random selection and first-six selection. For random selection, the 6 test classes are randomly chosen from the 40 classes. For the first-six selections, the first presented 6 classes in the EEG experiment are chosen. During the test stage, since the training set does not include classes corresponding to the test EEG data, the model could not give the corresponding labels and could only output the most probable classes among the 34 seen during training. Therefore, we proposed two evaluation metrics:\(Acc_{near}\) and \(Acc_{7th}\). \(Acc_{near}\) represents the proportion of EEG data classified into temporally adjacent classes, while \(Acc_{7th}\) represents the proportion classified into the category presented seventh in time.

### Joint training and image generation

To demonstrate that the model can utilize domain features to accomplish retrieval and generation besides classification, EEG-image joint training and image generation on WM-CVPR and SK-CVPR were conducted.

**Joint training** In the EEG-image joint training, a pre-trained image encoder was typically utilized to extract image representation, while an EEG encoder was employed to extract EEG features to align with the image representation. During the decoding process, a retrieval task was applied. Specifically, given a test EEG sample and a collection of images containing the target and the non-target. The image representation was reconstructed from the EEG with the EEG encoder. The similarity between the reconstructed image representation and all candidate image representations in the collection is calculated. The decoded output image is selected based on the ranking of these similarities. Usually, the Top-k accuracy and normalized Rank accuracy are used as evaluation metrics. In this work, the simple CNN described in Subsection 2.3 is used as an EEG encoder. The detailed implementation can be found in Appendix A.3.

**Image generation** The image generation aims to generate images seen by the subjects from their EEG data. This task commonly uses a two-stage process: EEG encoding and image generation. In the EEG encoding stage, a model is built to encode EEG data into a latent representation. In the image generation stage, a pre-trained image generator is used. The generator is fine-tuned with EEG representation and corresponding images. In this work, the EEG data are first encoded into image representation with a simple CNN described in Subsection 2.3. Following previous work, a latent diffusion model conditioned on image representation was used. The metric of n-way top-k accuracy was used for evaluating the semantic correctness of generated images . The detailed implementation can be found in Appendix A.4.

### Implement details

The neural networks were implemented with the Pytorch and trained on a single high-performance computing node with 8 A800 GPU. For the classification task, the AdamW  optimizer was employed to minimize the cross-entropy loss function with a learning rate of \(10^{-3}\). For the joint training and image generation, the AdamW optimizer was used with a learning rate of \(10^{-3}\) and \(5 10^{-4}\) for each task respectively. More details can be found in our codes. All the experiments mentioned in this work were trained within the subjects (i.e., models were trained for each subject respectively) except special annotation (unseen subject decoding results were only presented in Appendix A.5). For statistical analysis, the one-sample t-test was used to check whether the reported results were significantly higher than the chance level. Bonferroni correction was used to adjust the \(p\)-value. A \(p\)-value of 0.05 or lower was considered statistically significant.

## 3 Results

### Classification tasks

The results shown in Table 1 present that classification accuracy in domain label classification and class label classification are all significantly above the chance level. This shows that the domain feature can be extracted effectively with a simple CNN, and the label class can be decoded from the extracted domain features or from EEG directly. In contrast, the decoding accuracy drops to the chance level when using the splitting strategy leave-domains-out, further supporting domain feature-induced high decoding accuracy. The standard error of the mean calculated over the subjects level is reported for accuracy in this work.

Figures 1(a) and 1(b) show the t-SNE plot for domain label classification and end-to-end class label classification. As shown in Figure 1(a), 8 distinct clusters exist, each corresponding to one domain. In Figure 1(b), 8 distinct clusters also exist, with four corresponding to class label 1 and the other four corresponding to class label 2. This indicates that the high decoding accuracy results from associating class labels with domain features.

The experimental results for zero-shot classification are displayed in Table 2. It can be observed that the model tended to classify test samples into temporally adjacent classes. Figure 2c shows the t-SNE visualization of the unseen EEG features extracted from the decoder. Despite being unseen, different domains of features clustered in distinct groups. This suggests that the decoder just learned to extract EEG domain features during training and distinguish unseen EEG responses from the domain features.

### Joint training and image generation

For EEG-image joint training, Table 3 displays the accuracy for the retravel task on the test set. The table shows that, for both types of loss functions, decoding accuracy is far above the chance level, demonstrating that the model can utilize domain features to align EEG with image features. Table 3 Result for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS) or InfoNCE.

For image generation, Table 4 displays the n-way top-k accuracy for the generated images on the WM-CVPR and SK-CVPR datasets. The metrics are significantly above the ch

    & WM-CVPR & WM-DEAP & WM-KUL & SK-CVPR & SK-DEAP & SK-KUL \\  DLC & \(88.78 4.95\) & \(96.98 0.76\) & \(99.99 0.01\) & \(69.83 2.98\) & \(72.70 1.36\) & \(100.00 0.00\) \\  DLC (chance level) & 2.50 & 2.50 & 12.50 & 2.50 & 2.50 & **12.50** \\  TLC-DF & - & \(92.77 1.31\) & \(100.00 0.00\) & - & \(76.19 1.80\) & \(100.00 0.00\) \\ TLC-EEG & \(88.78 4.95\) & \(88.74 3.26\) & \(82.74 6.44\) & \(69.83 2.98\) & \(74.44 2.76\) & \(93.34 2.01\) \\ TLC-EEG-woDO & - & \(24.67 2.31\) & \(49.97 4.67\) & - & \(25.34 1.85\) & \(59.32 4.07\) \\  TCL (chance level) & 2.50 & 25.00 & 50.00 & 2.50 & 25.00 & 50.00 \\   

Table 1: Classification accuracy (%) on the six datasets. DLC is for domain label classification. TLC-DF is for class label classification from domain features. TLC-EEG is for end-to-end class label classification. TLC-EEG-woDO is for class label classification direct from EEG when samples in the training set and test set are from different domains.

    & WM-CVPR & WM-CVPR & SK-CVPR & SK-CVPR \\  & first-six & random & first-six & random \\  \(Acc_{near}\) & - & \(79.43 5.61\) & - & \(78.00 5.66\) \\ \(Acc_{7th}\) & \(69.60 10.64\) & \(6.73 3.24\) & \(77.03 11.32\) & \(0.87 0.82\) \\   

Table 2: Zero-shot EEG classification accuracy (%) on WM-CVPR and SK-CVPR datasets.

Figure 2: t-SNE plot for (a) domain label classification, (b) end-to-end class label classification, and (c) zero-shot class label classification

that the generated images have correct semantics. Figure 3 shows some generated images on the WM-CVPR dataset. As shown in the figure, the model can exactly generate the correct images. The results on EEG-image joint training and image generation show that in addition to classification tasks, retrieval, and generation can also achieve high performance by leveraging domain features shared by the test and training sets.

## 4 Discussion

### Relying on the domain features for EEG decoding

While many works on EEG decoding have reported high-performance results, we proposed that some of these high-performance may rely on temporal autocorrelation of EEG data. The pitfall may involve different EEG decoding tasks. To clarify this pitfall, the concept of domain was adopted to describe the temporal autocorrelation of a continuous EEG with the same label. EEG data were collected from watermelon as the phantom to exclude the contribution of stimuli-driven neural responses to decoding results. The results showed that a simple CNN network could well learn domain features from EEG data and could associate class labels with domain features.

To avoid the pitfalls, a feasible approach is to adopt a reasonable data-splitting strategy to avoid training and test sets sharing the common domain features, i.e., a leave-domains-out splitting strategy. For instance, a leave-subjects-out data-splitting strategy can be adopted, which entails designating the data from certain participants for training and data from others for testing. Alternatively, for datasets that do not follow a block design, a leave-trials-out strategy may be applied. Prior research has consistently demonstrated that employing a leave-subjects-out splitting strategy precipitates a notable decline in decoding performance . In some cases, it has been reported that decoding accuracy dropped to the chance level . The prevalent interpretation is that inter-individual variability  hampers the generalizability across different subjects. However, we posit that the observed decrement in decoding accuracy is attributable to model overfitting to domain features. Although the leave-subjects-out partitioning strategy is designed to prevent the leakage of domain features, the presence of these domain features in the training set can still lead the model to inadvertently exploit them to differentiate between categories during the training phase. The methods and results further support the conclusion can be found in Appendix A.5

Palazzo et al.  proposed that the EEG temporal correlation related to baseline drift could be alleviated by high-pass filtering. However, our further experiment proved that the domain feature still exists and that high decoding accuracy could be achieved in any frequency band (see Appendix A.6). We argue that the focus should not be exclusively on the elimination of EEG autocorrelation through

  - & Top-1/50-way & Top-5/50-way & Top-1/100-way & Top-5/100-way \\  WM-CVPR & \(26.77 3.37\) & \(46.44 4.60\) & \(21.64 2.89\) & \(38.11 4.30\) \\ SK-CVPR & \(25.04 0.93\) & \(43.61 0.88\) & \(20.37 0.91\) & \(35.35 0.89\) \\  Chance & 2.00 & 10.00 & 1.00 & 5.00 \\  

Table 4: Accuracy (%) for semantic correctness. The repeated times N was set to 50.

Figure 3: EEG-generated image from a typical watermelon subject, where the first column of each panel represents the real images ”watched” by the watermelon subject, and the following five columns show the images generated by the model.

filtering. Instead, greater emphasis should be placed on the experimental paradigms of EEG recording and the methods employed for dataset splitting. By addressing these aspects, we can proactively prevent the overestimated decoding accuracy arising from EEG temporal autocorrelations.

It is worth noting that we do not want to create an illusion that all BCI works utilize EEG temporal autocorrelation features for decoding. In fact, there are many works that do not rely on EEG temporal autocorrelation features for decoding in image decoding [48; 49; 50] emotion recognition , sleep detection [40; 41] and ASAD . These works demonstrated the feasibility of various BCI tasks.

### Potential sources of domain features

In this work, we have demonstrated the existence of EEG temporal autocorrelation in the watermelon EEG, which consists of no neural activities, and in the human EEG data. Li et al.  believed the model decodes by utilizing the baseline drift in the CVPR2017 dataset. They found that when the EEG data is filtered with a bandpass filter, the decoding accuracy dropped greatly. Palazzo et al.  also claimed that temporal correlation was strong only in low frequency. However, we have demonstrated in Appendix A.4 that the domain feature still exists and that high decoding accuracy can be achieved in any frequency band. In addition to baseline drift, some neuroscience works have shown that temporal autocorrelation existed in neural oscillation, which could be reflected in EEG in various frequency bands. This is referred to as Long-Range Temporal Correlations (LRTC) in neuroscience research [11; 12; 13; 14]. Linkenkaer-Hansen et al.  first calculated the LRTC in resting-state EEG data. They found that spontaneous alpha, mu, and beta oscillations result in significant LRTC for at least several hundred seconds during resting conditions. Subsequent neuroscience research further demonstrated that significant LRTC exists in the theta  and gamma  bands. While baseline drift can be removed through filtering, the frequency range of the LRTC overlaps with the frequency range of stimuli-driven neural responses, making it impossible to remove this domain feature through filtering. Temporal correlation analysis on human EEG in the SparrKULee Dataset showed the existence of strong LRTC in all frequency bands, and the LRTC in a narrowband is sufficient to complete the corresponding decoding task. The methods and results further support the conclusion can be found in Appendix A.7.

### Limitation and future work

Although direct evidence of overestimated decoding accuracy attributable to domain feature across various brain-computer interface (BCI) tasks have been provided in the current work, no solution has been proposed to mitigate overfitting to domain features in the training set. Some works have already used domain adaptation [2; 53; 54] or domain generalization [40; 41] method to improve decoding accuracy under leave-subjects-out data splitting in BCI tasks. This may also help alleviate the adverse effects of domain features on decoding tasks. It is also noteworthy to highlight the remarkable efficacy of large-scale EEG model in various BCI decoding tasks [55; 56; 57]. Given that domain features are pervasive in extensive EEG datasets and do not necessitate manually annotated labels, self-supervised pre-trained large EEG models may be especially adept at discerning and neutralizing domain features, thereby facilitating more robust and generalizable decoding performance.

## 5 Conclusion

In this work, the "overestimated decoding accuracy pitfall" in various EEG decoding tasks is formulated in a unified framework by adopting the concept of "domain". Some typical EEG decoding tasks (image decoding, emotion recognition, and auditory spatial attention detection) are conducted on the self-collected watermelon EEG dataset. The results showed that EEG data from different domains have distinctive domain features induced by EEG temporal autocorrelations. Using the inappropriate data partitioning strategy, high decoding accuracy is achieved by associating class labels with domain features. The results will draw attention to the high decoding performance caused by EEG temporal correlation and guide the development of BCI in a positive direction.