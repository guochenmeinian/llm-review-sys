# Deep Submodular Peripteral Networks

Gantayya Bhatt Arnav M. Das Jeffrey A. Bilmes

University of Washington, Seattle, WA 98195

{gbhatt2, arnavmd2, bilmes}@uw.edu

Equal Contribution

###### Abstract

Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a GPC-based strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripheral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, or RHLF preference ranking, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback. We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function and demonstrate its superiority both for experimental design and online streaming applications.

## 1 Introduction and Background

This paper jointly addresses two seemingly disparate problems presently open in the machine learning community.

The first is that of identifying a useful practical scalable submodular function that can be used for real-world data science tasks. Submodular functions, set functions that exhibit a diminishing returns property (see Appendix B), have received considerable attention in the field of machine learning. This has fostered new algorithms that offer near-optimal solutions for various applications. These applications include detecting disease outbreaks , modeling fine structure in computer vision , summarizing images , active learning , compressed sensing, structured convex norms, and sparsity , fairness , efficient model training  recommendation systems , causal structure , and brain parcellating  (see also the reviews ). Despite these myriad applications, most research on submodularity has been on the algorithmic and theoretical side which assumes the submodular function needing to be optimized is already at hand. While there has been work showing the theoretical challenges associated with learning submodularity , there is relatively little work on practical methods to produce a useful submodular function in the first place (a few exceptions include ). Often, because it works well, one uses the non-parametric submodular facility location (FL) function, i.e., for \(A V\) with \(|V|=n\), \(f(A)=_{i=1}^{n}_{a A}s_{a,i}\) where \(s_{a,i} 0\) is a similarity between items \(a\) and \(i\). On the other hand, the FL function's computational and memory costs grow quadratically with dataset size \(n\) and so FL can become infeasible for large data or online streaming applications. There is a need for practical strategies to obtain useful, scalable, general-purpose, and widely applicable submodular functions.

The second problem we address in this paper is the following: how best, in a modern machine-learning context, can one learn a scaling from oracles that, for a given query, each provide numerically graded pairwise comparisons between two choices? That is, given two choices, \(A\) and \(B\), an oracle provides a score \((A,B)\) which is positive if \(A\) is preferred to \(B\), negative if \(B\) is preferred to \(A\), zero if indifferent, and where the absolute value provides the degree of preference \(A\) or \(B\) has over the other. The oracle could be an individual human annotator, or could be a combinatorially expensive desired target function -- in either case we assume that it is infeasible to optimize over the oracle but it is feasible to query. Learning a "scaling" means learning a scalar-valued function \(f\) that respects these graded pairwise scores in that \(f(A)-f(B)(A,B)\).

A special case of such preference elicitation from human oracles have been studied going back many decades. For example, the psychometric "Law of comparative judgment"  focuses on establishing numeric interval scales based on knowing preferences between pairs of choices (e.g., \(A\) vs \(B\)). The Bradley-Terry  model (generalized to ranking elicitations in the Luce-Shephard model ) also involves preferences among elements of pairs. These preferences, however, are not graded and rather are either only binary (\(A B\) or \(B A\)), ternary (allowing also for indifference), or quaternary (allowing also for incomparability). Thus, each oracle query provides minimal information about a pair. Therefore, multiple preference ratings are often aggregated where a collection of (presumed random i.i.d.) oracles vote on the preference between items of the same pair. This produces a histogram of preference ratings \(C_{ij}\) (a count of how many times \(i\) is preferred to \(j\)) and this can be used in modern reinforcement learning with human feedback (RLHF) systems  for reward learning (i.e., inverse reinforcement learning ). Overall, the above modeling strategies are known in the field of psychometric theory  as "paired comparisons" where it is said that "paired comparison methods generally give much more reliable results"  for models of human preference expression and elicitation.

There are other models of preference elicitation besides paired comparisons. We explore "graded pairwise comparisons" (GPC) which also have been studied for quite some time . Quite recently, it was found that "GPCs are expected to reduce faking compared with Likert-type scales and to produce more reliable, less ipsative trait scores than traditional-binary forced-choice formats"  such as paired comparisons. An intuitive reason for this is that, once we get to the point that an oracle is asked only if \(A B\) or \(B A\), this does not elicit as much information out of the oracle as would asking for \((A,B)\) even though the incremental oracle effort for the latter is often negligible. As far as we know, despite GPC's potential information extraction efficiency advantages, GPC has not been addressed in the modern machine learning community. Also, while histograms \(C_{i,j}\) are appropriate for maximum likelihood estimation (MLE) of non-linear logistic and softmax style regression , MLE with a Bradley-Terry style model is suboptimal to learn scoring from graded preferences \((A,B)\) which can be an arbitrary signed real number.

In the present paper we simultaneously address both of the above concerns via the introduction and training of **deep submodular peripteral networks** (DSPNs). In a nutshell, DSPNs are an expressive theoretically interesting parametric family of submodular functions that can be successfully learnt using a new GPC-style loss that we introduce below. In our work, since the teacher will be an expensive FL function and the learner is a DSPN set function, we can view this as a form of knowledge distillation. Our offering, therefore, is very far from incremental -- rather, we introduce a new provably more expressive model family (DSPNs), and a new learning paradigm (GPC-style learning) for distilling an expensive FL set function down to an expressive parametric learner.

An outline of our paper follows. In SS2 we define the class of DSPN functions, motivating the name "peripteral", and offering theoretical comparisons with the class of deep sets . Next, SS3 describes a new GPC-style "peripteral" loss that is appropriate to train a DSPN but can be used in any GPC-based preference elicitation learning task. For DSPN learning, this happens by producing a list of pairs of subsets \(E,M\) that can be used to transfer information from the oracle target to the DSPN being learnt. SS4 describes several \(E,M\) pair sampling strategy including an active-learning inspired submodular feedback approach. SS5 empirically evaluates DSPN learning based on how effectively information is transferred from the target oracle and performance on downstream tasks such as experimental design. We find that the peripteral loss outperforms other losses for training DSPNs, and show that the DSPN architecture is more effective at learning from the target function than baseline methods such as Deep Sets and SetTransformers. We also find that the grading in GPC indeed improves performance over binary-only comparisons.

While the above introduction situates our research and establishes the setting of our paper in the context of previous related work, our relation to and advancement over **other related work** is fully explored in our Appendix in SSA. The appendices continue (starting at SSB) offering further details of DSPN novelty (SSC) and learning via the GPC-style peripteral loss.

## 2 Deep Submodular Peripteral Networks

Before starting this section, it may assist the reader to consider our brief overview of submodularity, matroids, and weighted matroid rank in SSB, and a brief review of deep submodular functions in SSC, both provided in the appendix.

A deep submodular peripteral network (DSPN) is a new expressive trainable parametric nonsmooth subdifferentiable submodular function. A DSPN has three stages: (1) a "pillar" stage; (2) a submodular-preserving permutation-invariant aggregation stage; and (3) a "roof" stage. When considered together, these three stages resemble an ancient Greek or Roman "peripteral" temple as shown in Figures 1 and 10. In this paper, all DSPNs are also monotone non-decreasing and normalized (see SSB). As a set function \(f:2^{V}_{+}\), a DSPN maps any subset \(A V\) to a non-negative real number. We consider sets as object indices so \(A\) might be a set of image indices while \(\{x_{i}:i A\}\) are the objects being referred to. Hence, w.l.o.g., \(V=[n]\).

The first stage of a DSPN consists of a set of \(|A|\) pillars one for each element in a set \(A\) that is being evaluated. The number of pillars changes depending on \(|A|\) the size of the set \(A\) being evaluated. Each pillar is a many-layered deep neural structure \(_{w_{}}:_{X}_{+}^{d}\), parameterized by the vector \(w_{}\) of real values, that maps from an object (from domain \(_{X}\) which could be an image, string, etc.) to a non-negative embedded representation of that object. For any \(v V\) and object \(x_{v}\), then \(_{w_{}}(x_{v})\) is a \(d\)-dimensional non-negative real-valued vector \((_{w_{}}(x_{v})_{1},_{w_{}}(x_{v})_{2},, _{w_{}}(x_{v})_{d})_{+}^{d}\). The parameters \(w_{}\) are shared for all objects. Also, \(_{w_{}}(x_{})_{j}_{+}^{d}\) refers, for any \(j[d]\), to the \(n\)-dimensional vector of the \(j^{}\) pillar outputs for all \(v V\).

The second stage of a DSPN is a submodular preserving and permutation-invariant aggregation. Before describing this generally, we start with a simple example. Each element \(_{w_{}}(x_{a})_{j}\) of \(_{w_{}}(x_{a})\) is a score for \(x_{a}\) along dimension \(j\) and so a non-negative modular set function can be constructed via \(m_{j}(A)=_{a A}_{w_{}}(x_{a})_{j}\). Such a summation is an aggregator that, along dimension \(j\) simply sums up the \(j^{}\) contribution for every object in \(A\). A simple way to convert this to a monotone non-decreasing submodular function composes it with a non-decreasing concave function \(\) yielding \(h_{j}(A)=(m_{j}(A))\) -- performing this for all \(j\) yields a \(d\)-dimensional vector of submodular functions. More generally, any aggregation function \(\) that preserves the submodularity of \(h_{j}(A)=(_{a A}m_{j}(a))\) would suffice so long as it is also permutation invariant  (see Def. 7). For example, with \(=\) (i.e., max pooling), the function \((_{a A}m_{j}(a))\) is again submodular. We show that there is an expressive infinitely large family of submodular preserving permutation-invariant aggregation operators, taking the form of the weighted matroid rank functions (Def. 6) which are defined based on a matroid \(=(V,)\) and a non-negative vector \(m_{+}^{|V|}\).

**Lemma 1** (Permutation Invariance of Weighted Matroid Rank).: _The weighted matroid rank function \(_{,m}()\) for matroid \(=(V,)\) with any non-negative vector \(m_{+}^{|V|}\) is permutation invariant._

A proof is in Appendix B. We identify the aggregator \(\) as \(_{a A}m_{j}(a)=_{,m_{j}}(A)\) and produce new submodular functions \( j\), \(h_{j}^{w_{}}(A)=(_{,_{w_{}}( x_{})_{j}}(A))\), where the \(n\)-dimensional vector \(_{w_{}}(x_{})_{j}\) constitute the weights for the matroid. Depending on the matroid, this yields summation (using a uniform matroid), max-pooling (using a partition matroid), and an unbounded number of others thanks to the flexibility of matroids . In theory, this means that the aggregator could also be trained via discrete optimization over the space of discrete matroid structures (in SS5, however, our aggregators are fixed). The second stage of the DSPN thus aggregates the modular outputs of the first stage into a resulting \(d\)-dimensional vector for any set \(A V\), namely \(h_{w_{}}(A)=(h_{1}^{w_{}}(A),h_{2}^{w_{}}(A), ,h_{d}^{w_{}}(A))_{+}^{d}\). This can also be viewed as a vector of submodular functions. We presume in this work that each aggregator uses the same matroid and each pillar dimension is used with exactly one matroid, but this can be relaxed leading to a \(d^{}>d\) dimensional space -- in other words, many different discrete matroid structures can be used at the same time if so desired.

The third and final "roof" stage of a DSPN is a deep submodular function (DSF) . Deep submodular functions are a nested series of vectors of monotone non-decreasing concave functions composed with each other, layer-after-layer, leading to a final single scalar output. Each layer contained in a DSF has non-negative weight values which assures the DSF is submodular. It was shown in  that the family of submodular functions expressible by DSFs strictly increase with depth. DSFs are also trainable using gradient-based methods analogous to how any deep neural network can be trained. A DSF is defined as a set function, but we utilize the "root"2 multivariate multi-layered concave function \(_{w_{}}:^{d}_{+}\) of a DSF in this work. That DSPNs significantly extend DSFs, as well as a DSF primer, is further described in SSC. The final stage of the DSPN composes the output of the second stage with this final DSF via \(f_{w}(A)=_{w_{}}(h_{w_{}}(A))\) where \(w=(w_{},w_{})\) constitute all the learnable continuous parameters of the DSPN (assuming the discrete parameters, such as the matroid structure, is fixed). While the roof parameters \(w_{}\) must be positive, the pillar parameters \(w_{}\) are free, but the pillar must produce non-negative outputs.

**Theorem 2** (A DSPN is monotone non-decreasing submodular).: _Any DSPN \(f_{w}(A)=_{w_{}}(h_{w_{}}(A))\) so defined is guaranteed to be monotone non-decreasing submodular for all valid values of \(w\)._

The proof of Theorem 2 is found in Appendix SSD. We immediately have the following corollary.

**Corollary 3** (Submodular Preservation of Weighted Matroid Rank Aggregators).: _Any weighted matroid rank function for matroid \(=(V,)\) with any non-negative vector \(m_{+}^{|V|}\), when used as an aggregator in a DSPN, will be submodularity preserving._

Clearly there is a strong relationship between DSPNs and Deep Sets . In fact any DSPNs is a special case of a Deep Set, but there is an important distinction, namely that a DSPN is assuredly submodular. It may seem like this would restrict the family of aggregation functions available but, as shown above, there are in theory an unbounded number of aggregators to study. Training a DSPN also preserves submodularity simply by ensuring \(w_{}\) remains positive which can be achieved using projected gradient descent (the positive orthant is one of the easiest of the constraints to maintain). A deep set, when trained, can easily lose the submodular property, something we demonstrate in SS5, rendering the deep set inapplicable when submodularity is required. Also, as was shown in , DSFs are already quite expressive, the only known limitation being their inability to express certain matroid rank functions. With a DSPN, however, the aggregator functions immediately eliminate this inability.

**Corollary 4** (DSPN Family).: _Let \(_{}\) (resp. \(_{}\)) be the family of DSPN (resp. DSF) submodular functions. Then \(_{}_{}\)._

It is an open theoretical problem if a DSPN, by varying in parameter \(w\) and combinatorial space \(\), can represent all possible monotone non-decreasing submodular functions.

## 3 Peripteral Loss

We next describe a new "peripteral" loss to train a DSPN and simultaneously address how to learn from numerically graded pairwise comparisons (GPCs) queryable from a target oracle.

We are given two objects \(E\) and \(M\) and we assume access to an oracle that when queried provides a numerical score \((E,M)\). As shorthand, we define \((E|M)=(E,M)\). We do not presume it is always possible to optimize over \((E|M)\). This score, as mentioned above, could come from human annotators giving graded preferences for \(E\) vs. \(M\), or could be an expensive process of training a machine learning system separately on data subsets \(E\) and \(M\) and returning the difference in accuracy on a development set. In the context of DSPN learning, our score comes from the difference of a computationally challenging target submodular function \(f_{}\). As an example, \(f_{}\) could be a facility location function that requires \(O(n^{2})\) time and memory and is not streaming friendly, where we have \((E|M)=f_{}(E)-f_{}(M)\), this being a positive if \(E\) is more diverse than \(M\) and negative otherwise. Given a collection of \(E,M\) pairs (SS4), we wish to efficiently transfer knowledge from the oracle target's graded preferences \((E|M)\) to the learners graded preferences \(_{f_{w}}(E|M)=f_{w}(E)-f_{w}(M)\) by adjusting the parameters \(w\) of the learner \(f_{w}\).

We are inspired by simple binary linear SVMs that learn a binary label \(y\{+1,-1\}\) using a linear model \(,x\) for input \(x\) and a hinge loss \((z)=(1-z,0)\). We express the loss of a given prediction for \(x\) as _hinge\((y,x)\)_ where \(y,x\) is positive for a correct prediction and negative otherwise, regardless of the sign of \(y\).

Addressing how to measure \(_{f_{w}}(E|M)\)'s divergence from \((E|M)\), multiplying the two (as above) would have the same sign benefit as the linear hinge SVM -- a positive product indicates the preference for \(E\) vs. \(M\) is aligned between the oracle and learner. We want more than this, however, since we wish to learn from GPC. Instead, therefore, we measure mismatch via the ratio \(_{f_{w}}(E|M)/(E|M)\). Immediately, we again have a positive quantity if preferences between oracle and target are aligned, but here grading also matters. A good match makes \(_{f_{w}}(E|M)/(E|M)\) not only positive but also large. If \((E|M)\) is large and positive, \(_{f_{w}}(E|M)\) must also be large and positive to make the ratio large and positive, while if \((E|M)\) is small and positive, \(_{f_{w}}(E|M)\) must also be positive but need not be too large to make the ratio large. A similar property holds when \((E|M)\) is negative. That is, if \((E|M)\) is large and negative, then so would \(_{f_{w}}(E|M)\) need to be to make the ratio large and positive, while \((E|M)\) being small and negative allows for \(_{f_{w}}(E|M)\) to be small and negative while ensuring the ratio is large and positive.

On the other hand, a ratio alone is not a good objective to maximize, one big reason being that small changes in the denominator can have disproportionately large effects on the value of the ratio, which can amplify noise and lead to erratic optimization behavior.3 Ideally, we could find a hinge-like loss that will penalize small ratios and reward large ratios. Consider the following:

\[_{}()=||(1+(1-))\] (1)

Here, if \(\) is (very) positive, then \(\) must also be (very) positive to produce a small loss. If \(\) is (very) negative then \(\) also must be small to produce a small loss. The absolute value of \(\) determines how non-smooth the ratio is so we include an outer pre-multiplication by \(||\) to ensure the slope of the loss, as a function of \(\), asymptotes to negative one (or one when \(\) is negative) in the penalty region, analogous to hinge loss, and thus produces numerically stable gradients. Also analogous to hinge, we add an extra "1" within the \(()\) function as a form of margin confidence, analogous to hinge loss that, as with SVMs, expresses a distance between the decision boundary and the support vectors. Plots of \(_{}()\) are shown in Fig. 7 in Appendix F -- this appendix also discusses the smoothness of \(_{}()\) via its gradients, describes a few additional useful hyperparameters, and provides a numerically stable solution to the case when \(=0\) (thereby solving any \(/0\) issues).

Comparing with standard contrastive losses and deep metric learning , a peripteral loss on \(_{f_{w}}(E|M)=f_{w}(E)-f_{w}(M)\) can be seen as a high-order self-normalizing by \(\) generalization of triplet loss  if \(E\) is a heterogeneous and \(M\) a homogeneous set of objects. We recover aspects of a triplet loss if we set \(E=\{v,v^{-}\}\) and \(M=\{v,v^{+}\}\) where \(M\) is the positive pair (close and homogeneous) while \(E\) is the negative pair (distant from each other and thus a diverse pair). With \(|E|>2\) and \(|M|>2\), we naturally represent high order relationships amongst **both** the positive group \(M\) and the disperse group \(E\) (which is apparently rare, see SSA.1). We remind the reader that while our peripteral loss could be used for GPC-style contrastive training for representation learning or for RLHF-based transformer alignment, our experiments in this paper in SS5 focus on the submodular learning problem.

### Augmented Loss for Augmented Data

Data augmentation is frequently used to improve the generalization of neural networks. Since augmented images represent the same knowledge in principle, they should have the same submodular valuation and also should be considered redundant by any learnt submodular function. Given a set \(E\) of images, let \(E^{}\) represent a same-size set of augmented images generated after augmenting each image \(e E\). This means any learnt model \(f_{w}\) should have \(f_{w}(E)=f_{w}(E^{})\) and \( e E\), \(f_{w}(e)=f_{w}(e^{})\). To ensure this is the case, we use an augmentation regularizer \(_{}(f_{w};E,E^{})\) defined as follows:

\[_{}(f_{w};E,E^{})=_{1}(f_{w}(E)-f_{w}(E^{ }))^{2}+}{|E|}_{e E}(f_{w}(e)-f_{w}(e^{}))^ {2}\] (2)

Since augmented images represent the same information, the submodular gain of augmented images relative to non-augmented images should be small or zero meaning \(f_{w}(E|E^{})=f_{w}(E^{}|E)=0\)and \( e E\), \(f_{w}(e|e^{})=f_{w}(e^{}|e)=0\). To encourage this, we use a redundancy regularizer \(_{}(f_{w};E,E^{})\) defined as follows:

\[_{}(f_{w};E,E^{})=_{3}(f_{w}(E|E^{})^ {2}+f_{w}(E^{}|E)^{2})+}{|E|}_{e E}(f_{w}(e|e^{ })^{2}+f_{w}(e^{}|e)^{2})\] (3)

In the above, we square all quantities to provide an extra penalty to larger values, while as quantities such as \(f_{w}(E|E^{})\) approach zero, we diminish the penalty.

### Final Loss

With all of the above individual loss components now well described, our final total loss \(_{}(f_{w};(E|M),_{f_{w}}(E|M))\) for an \(E,M\) pair becomes:

\[_{}(f_{w};(E|M),_{f_{w}}(E|M)) =_{(E|M)}(_{f_{w}}(E|M))+_{ }(f_{w};E M,E^{} M^{})\] \[+_{}(f_{w};E,E^{})+_{ }(f_{w};M,M^{})\] (4)

With a dataset \(=\{(E_{i},M_{i})\}_{i=1}^{N}\) of pairs, the total risk is:

\[}(w;)=_{i[N]}_{}(f_{w};(E_{i}|M_{i}),_{f_{w}}(E_{i}|M_{i})).\] (5)

Fig. 1 shows the structure of a DSPN and its learning control flow. Please see appendix G for more details.

## 4 Sampling (E,M) Pairs

DSPN training via the peripteral loss requires a dataset \(=\{(E_{i},M_{i})\}_{i=1}^{N}\) of pairs of sets where each \(E_{i},M_{i} V\). Since the DSPN starts with the pillar stage, the learnt model can then be tested on samples that are outside of \(V\) as we do in SS5. Still, exhaustively training on all pairs of subsets from \(V\) is infeasible. Therefore, the pairing strategy used to incorporate the sampled sets is a critical component in our dataset construction. We thus propose several practical, both passive and active, strategies to efficiently sample from this combinatorial space. The passive strategies are heuristic-based, while the active strategies are dependent on either the DSPN as it is being learnt or optionally the target function if it is possible to optimize (fortunately, Table 1 shows target sampling is not necessary). Our description here of these strategies is augmented with further discussion and analysis in appendix E.

**Passive Sampling:** If we assume the maximum set size to be \(K\) so that \(|E| K\) and \(|M| K\) and a training dataset to have \(C\) classes or clusters, we may generate \(E,M\) pairs in two ways. In **Style-I**, we sample a homogeneous set of size \(K\) from a randomly chosen single class and a heterogeneous

Figure 1: The structure of a DSPN and the control flow of how a DSPN is trained; parameters are shared between the DSPNs processing E and M sets.

set of size \(K\) randomly over the full dataset. In **Style-II**, we first choose a random set of \(C^{} C\) classes, where \(C^{}\) is chosen uniformly at random over \([C]\), and (if \(C^{}<C\)) subselect from the ground set a set \(V_{C^{}} V\) containing only those samples corresponding to those \(C^{}\) classes. To choose an \(E\) set, we perform K-Means clustering on \(V_{C^{}}\), and pick the sample closest to each cluster mean to construct our set. To build an \(M\) set, we choose one sample for each class/cluster in \(V_{C^{}}\) and then add its \( K/C^{}-1\) nearest neighbors. Importantly, the \(E\) and \(M\) sets in each pair sampled with **Style-II** are much more difficult to distinguish than **Style-I**. We also note that these sampling strategies are imperfect, so the supposedly heterogeneous \(E\) set may not truly be heterogeneous in relation to its homogeneous counterpart \(M\). However, as discussed in SSE, the peripteral loss allows us to train a DSPN even if the set pairs are imperfectly labeled since the oracle just flips the sign of its preference. Simplified passive 2D sets are shown in Fig. 2 and a set-pairing illustration is seen in Fig. 3.

**Active Sampling:** Sets selected by the passive strategies are independent of both the learner and the target function. Thus, due to the combinatorial nature of the problem, there may be a discrepancy between what sets are valued highly by the DSPN and/or the target. This leads us to employ _actively sampled sets_ obtained through two approaches. We refer to the first approach as **DSPN Feedback**, which obtains sets by maximizing or minimizing the DSPN as it is being learnt. As DSPN is submodular, we may optimize it over either full ground set \(V\) or a class-restricted ground set to get high-valued subsets, where the greedy algorithm can be used for maximization  and submodular minimization  can be used to find the homogeneous sets (although in practice, submodular minimization heuristics suffice). We refer to the second approach as **Target Feedback**, where we obtain sets by optimizing the target. The latter approach is only applicable when the target is optimizable (e.g., submodular), which would not be the case with a human oracle. While we do test "Target Feedback" below, our experiments in SS5.3 show that it is not necessary for good performance, which portends well for our framework to be used in other GPC contexts (e.g., RLHF learning via human feedback). Thus, while the \(E,M\) pair generation strategies mentioned here are specific to DSPN learning, our peripteral loss could be used to train any deep set or other neural network given access to graded oracle preferences, thus yielding a general purpose GPC-style learning procedure.

## 5 Experiments

We evaluate the effectiveness of our framework by training a DSPN to emulate a costly target oracle FL function. We assert that DSPN training is deemed successful if the subsets recovered by maximizing the learnt DSPN are **(1)** assigned high values by the target function (SS5.1); and **(2)** are usable for a real downstream task such as training a predictive model (SS5.2).

Figure 3: _Set Pairing._ We actively sample sets by optimizing the DSPN as it is being learnt or optionally the target function. The depicted graph demonstrates how the actively sampled sets are integrated into \(=\{(E,M_{i})\}^{N}\). In red/blue vertices refer to passively/actually sampled sets. An edge between two vertices indicates that sets from the categories denoted by the vertices are used to create \((E,M)\) pairs to train the DSPN. Dashed edges represent non-critical pairs.

Figure 2: _Passive Sets._ We consider a simple 2D ground set with 5 clusters/classes, as indicated by the colors. The various types of passively sampled sets are depicted (discussed in section 4). Type-1 Homogeneous sets are randomly sampled from a single class, while Type-1 heterogeneous sets are sampled from the full ground set. Meanwhile, Type-11 restricts the ground set to a subset of classes and samples “clumps” from each of the sampled classes to construct the homogeneous set, and diverse sets from each class to create the heterogeneous sets. Intuitively, using Type-III allows the learnt DSPN to model intraclass/interclass respectively.

**Datasets:** We consider four image classification datasets: Imagenette, Imagewoof, CIFAR100, and Imagenet100. Imagenette, Imagewoof, and Imagenet100 are subsets of Imagenet-1k  where a subset of the original classes is used. Imagenet100 (resp. CIFAR100) have 100 classes and 130,000 (resp. 60,000) images, while Imagenette and Imagewoof have 10 classes (and 13,000 images).

**DSPN Architecture:** We use CLIP ViT encoder  for all datasets to get the embeddings that are used as inputs to the DSPN which can be seen as a fixed part of the pillar. We use an additional 2-layer network as the trainable part of the pillar (\(_{w_{p}}\)); for DSPN roof (\(_{w_{t}}\)) we use a 2-layered DSF for Imagenette/Imagewoof and 1-layered DSF for CIFAR100/Imagenet100. We use multiple concave activation functions, i.e., all of \((x)\{x,x^{0.5},(1+x),(x),1-e^{-x}\}\) in every layer of the roof (ensuring \(w_{} 0\) during training) and thus allow the training procedure to decide between them. See SSH for hyperparameter details.

**Target:** We use Facility Location (FL) as a target whose construction is rather expensive (quadratic) and also does not generalize to held-out data, but this is a key point. That is, we show how we distill from this expensive target FL to a cheaper and generalizable DSPN. Each target FL is created by computing a real-valued feature vector for each sample and then computing a non-negative similarity between each such vector. We used a CLIP ViT encoder to encode input images, and a tuned RBF kernel to construct similarities for our target FLs.

**Training and Evaluation:** We train up to sets of size \(|E|,|M| 100\) for all datasets. We show that we generalize beyond this size not just on held-out subsets but even on held-out data. Given ground set \(V\), we construct pairs \((E,M)\) both passively and actively, as discussed earlier. Please refer to SSG for the details on training. For the target oracle, we use a facility location function with similarity instantiated using a radial basis function kernel with a tuned kernel width. During the evaluation, both learned set functions and target oracle use a held-out ground set \(V^{}\) of images, that is, \(V^{} V=\).

### Transfer from Target to Learner

We demonstrate that the target FL assigns high values to summaries that are generated by the DSPN in Figure 4. Specifically, we determine if \(S\) is a good quality set if it has a high normalized FL evaluation \(}(S)}{f_{}(S_{FL})}\), where \(f_{}\) is the target FL and \(S_{}\) is the set generated by applying cardinality constrained greedy maximization to the target FL.

Therefore, we compare different DSPN models based on the normalized FL evaluation that their maximizers attain. We compare DSPN models trained with three different losses: the peripteral loss, the contrastive max-margin style structured prediction loss , and a regression loss (shown as _DSPN (peripteral)_, _DSPN (margin)_ and _DSPN (regression)_ respectively in Figure 4). Please refer appendix H for the definition of baseline losses (Equations 30 and 31). We also compare against randomly generated sets, which are denoted as _Random_. From this set of experiments, we find that the DSPN trained with the peripteral loss consistently outperforms all other baselines across different **summary sizes** (budgets) and datasets. Interestingly, the DSPN outperforms random even on budgets greater than 100, despite never having encountered such subsets during training. This provides evidence that our GPC-based peripteral loss is the loss of choice for training DSPNs.

### Application to Experimental Design

_Experimental design_ involves selecting a subset of unlabeled examples from a large pool for labeling to create a training set . In this section, we evaluate the performance of a learnt DSPN based on how well a simple linear model generalizes when trained on a training set that was chosen by a DSPN.

Figure 4: _Transfer._ We compare different loss functions in terms of their effectiveness at training a DSPN.

We consider an _offline setting_ where we assume the full unlabeled pool is available and an _online setting_ where the unlabeled pool is presented to the model in a non-iid stream. To test the robustness of our framework to real-world challenges, we add duplicates such that the ground set is heavily class-imbalanced (the procedure to generate the class-imbalanced set is shown in Appendix H). Importantly, in these experiments we train with one set \(V\) and test on a completely held-out set \(V^{}\) with \(V V^{}=\); that is, the testing DSPN is on an entirely different ground set than the training DSPN.

**Offline Setting:** The offline setting assumes that the full unlabeled pool is accessible by the summarization procedure. The results of these experiments are presented in Figure 5. Our approach, denoted as _DSPN_ in the figure, applies greedy maximization to the learnt DSPN to generate the summary. We also consider a _cheating_ experiment by applying greedy to the target FL after removing duplicates from the ground set; we refer to this approach as _Target FL_. Finally, we consider three baseline summarization procedures CRAIG , K-centers , and random subset selection . The learnt DSPN achieves performance comparable to the target FL in all datasets up to a budget of 100. Beyond that, the difference between DSPN and the Target FL grows but the DSPN continues to generate higher quality than any of the baselines, showing DSPNs generalize to set sizes beyond the training set sizes.

**Online Setting:** We consider an online setting where the data is presented to the model in a non-iid, class-incremental stream, as done in the continual learning literature . Unlike the facility location function which requires the full ground set for evaluation, the DSPN can be maximized in a streaming manner. Thus, we employ the streaming maximization algorithm proposed by  to maximize the DSPN, due to its effectiveness in real-world settings and its lack of additional memory requirements. This approach is referred to as _DSPN-online_ in Figure 6.

We compare our approach against two streaming summarization baselines. The first approach, _reservoir sampling_, is an algorithm that enables random sampling of a fixed number of items from a data stream as if the sample was drawn from the entire population at once. The second approach, _VeSSAL_, seeks to select samples such that the summary maximizes the determinant of the gram matrix constructed from the model embeddings. We also apply (offline) greedy maximization to the target facility location (_Target FL-offline_) and the learned DSPN (_DSPN-offline_), which serve as upper bounds to what is attainable by streaming DSPN maximization.

In Figure 6, we present quantitative results where a linear model is trained on a given streaming summary after labels have been obtained. We find that _DSPN-online_ significantly outperforms the other streaming baselines on **all** datasets. Surprisingly, _DSPN-offline_ and _DSPN-online_ achieve similar

Figure 5: _Offline Experimental Design._ We compare different summarization procedures, and assess them based on the test accuracy that a linear model attains upon being trained on the summary. We find that maximizing the learnt DSPN generates high quality datasets, outperforming existing summarization techniques such as CRAIG and k-centers. In many cases, the DSPN approaches the Target FL even though the latter is aware of the duplicates present in the data. **Takeaway: DSPN effectively chooses training samples for labeling from an unlabeled pool.**

Figure 6: _Online Experimental Design._ The performance of a linear probe, trained based on a subset selected by an algorithm that does not use labels, is reported at varying budgets. _DSPN-online_ achieves far higher performance than other streaming algorithms such as _reservoir sampling_ and _VeSSAL_ and is performs comparably to offline algorithms. **Takeaway: DSPN can effectively select training samples for labeling from a _stream_ of unlabeled data.**

performance, demonstrating the flexibility of DSPNs. _FL-offline_ tends to generate the highest quality summaries, but requires access to the full ground set and assumes duplicates are manually removed.

### Ablations

Table 1, provides Imagenet100 results on sets of size 100 that explore the importance of various components of the DSPN. We report three numbers for each ablation: (1) _FL Eval_ which corresponds to the normalized FL evaluation discussed in SS 5.1 (2) _Offline Acc_ which denotes the final accuracy of a linear model in the offline experimental design setting and (3) _Online Acc_ denotes the final accuracy of a linear model in the online experimental design setting.

**Removing (FL) Feedback:** Active selection is obtained by maximizing the DSPN or Target FL during training (denoted as _DSPN feedback_ and _Target feedback_ respectively). We find that only removing the sets obtained from target feedback has little to no impact on the final performance of the DSPN, suggesting that it is not necessary to optimize the target function. _This shows that our framework can be applied to GPC learning even when it is not possible to optimize the target, such as with human oracles_.

However, we find that completely removing Active selection significantly hurts the final DSPN.

**Removing Learnable Pillar:** We assess the utility of a learnable pillar, by training a DSPN with a fully frozen pillar. In this case, we attempt to learn the weights of a DSF on random projections of CLIP features. We discover that the DSPN without learnable pillars significantly underperforms its counterpart. We discuss this further in appendix I.

**Removing Submodularity:** We study the utility of submodular constraints to learn the target. To this end, we compare DSPN against a Deep Set  and SetTransformer  architecture in the third section of table 1. we discover that the absence of submodularity **significantly hampers** the performance for all the metrics, since the greedy algorithm has no guarantees on an arbitrary set function.

## 6 Conclusion and Future Work

We have proposed DSPNs and methods for training/distilling them. Since the computational cost for querying a DSPN is independent of dataset size, and the training scheme leverages automatically generated \((E,M)\) pairs, our methods learn practical submodular functions scalably. It may ultimately be possible to scale our training framework to massive heterogeneous datasets to develop foundation DSPNs having strong _zero-shot summarization_ capabilities on a wide array of domains. We also propose the _peripteral loss_ which leverages numerically graded relationships between pairs of objects. The graded pairwise preferences are provided from an oracle and are used to learn a submodular function that captures these relationships. However, the scope of the peripteral loss extends far beyond training DSPNs and has many applications such as new contrastive representation learning procedures, and learning a reward model in the context of RLHF with real human oracles.

## 7 Acknowledgments

We thank Suraj Kothawade, Krishnateja Killamsetty and Rishabh Iyer for several early discussions. This work is supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and by NSF Grant Nos. IIS-2106937 and IIS-2148367.

   Method & FL Eval & Offline Acc & Online Acc \\  DSPN & 0.92 & **67.8** & 65.9 \\  w/o target feedback & **0.93** & 67.7 & **66.9** \\ w/o feedback & 0.83 & 47.7 & 30.1 \\ w/o type-II sampling & 0.64 & 13.1 & 10.7 \\  PC rather than GPC & 0.90 & 62.8 & 63.4 \\  w/o learnt pillar & 0.71 & 39.1 & 38.0 \\  Deep Set & 0.56 & 9.2 & 3.5 \\ Set Transformer & 0.34 & 13.8 & – \\   

Table 1: _Ablations on Imagenet100_. The importance of feedback, learnt features, enforcing submodularity, and GPC (in each column, **bold** is best, underline is 2nd best). “w/o feedback” indicates a DSPN model trained without feedback, while “w/o target feedback” is when the oracle is only queried. For Set Transformer, streaming maximization fails to produce a summary of the required size (therefore it is omitted). “PC rather than GPC” uses non-graded pairwise comparisons, showing the benefits of grading, consistent with . All results are for a size-100 set.