# Mean-Field Langevin Dynamics for Signed Measures

via a Bilevel Approach

 Guillaume Wang1

1Ecole polytechnique federale de Lausanne

Alireza Mousavi-Hosseini1

1University of Toronto and Vector Institute

guillaume.wang@epfl.ch, mousavi@cs.toronto.edu, lenaic.chizat@epfl.ch

Lenaic Chizat1

1Ecole polytechnique federale de Lausanne

###### Abstract

Mean-field Langevin dynamics (MLFD) is a class of interacting particle methods that tackle convex optimization over probability measures on a manifold, which are scalable, versatile, and enjoy computational guarantees. However, some important problems - such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution - are originally defined over the set of signed, rather than probability, measures. In this paper, we investigate how to extend the MFLD framework to convex optimization problems over signed measures. Among two known reductions from signed to probability measures - the _lifting_ and the _bilevel_ approaches - we show that the bilevel reduction leads to stronger guarantees and faster rates (at the price of a higher per-iteration complexity). In particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results. First, this dynamics is amenable to an annealing schedule, adapted from , that results in improved convergence rates to a fixed multiplicative accuracy. Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses).

## 1 Introduction

Let \(()\) be the set of finite signed measures on a compact Riemannian manifold without boundaries \(\) and let \(G:()\) be a convex function, assumed smooth in the sense of Assumption 1 below. In this paper, we investigate optimization methods to solve

\[_{()}G_{}(), G_{}() G()+\|\|_{TV}^{2},\] (1.1)

where \(\|\|_{TV}\) is the total variation norm and \(>0\) the regularization level.2 This covers for instance risk minimization for infinite-width 2-layer neural networks (2NN)  by taking \(=^{d}\) the unit sphere in \(^{d+1}\) or \(=^{d+1}\) and

\[G()=_{(x,y)}(h(,x),y)  h(,x)=_{}( x,w )(w).\] (1.2)

Here \(:\) is the activation function, \(h(,)\) is the predictor parameterized by \(\), \(G\) is the (population or empirical) risk under the data distribution \((^{d+1})\), and \(\) is smooth (uniformlyin \(y\)) and convex in its first argument. These 2NNs will be our guiding examples throughout, but note that the class of problems covered by Eq. (1.1) is more general and includes for instance sparse deconvolution via the Beurling-LASSO estimator  or optimal design .

To tackle such problems, interacting particle methods use the parameterization \(=_{i=1}^{m}r_{i}_{w_{i}}\) and apply gradient methods in a well-chosen geometry . They have recently gained traction thanks to their scalability and flexibility, and in the context of 2NNs, the usual gradient descent algorithm is an instance of such a method. On the downside, _global_ convergence guarantees remain difficult to obtain due to the nonconvex nature of the reparameterized problem and existing positive results require either very specific settings , or modifications of the dynamics which often limit their scalability2.

In a related, but slightly different context, mean-field Langevin dynamics (MFLD) solve entropy-regularized problems of the form

\[_{(^{})}F_{}(), F_{}() F()+^{-1}H(),\] (1.3)

where \((^{})\) is the space of probability measures on a manifold \(^{}\) (typically \(^{d}\)), \(F:(^{})\) is a (sufficiently regular) convex functional, \(H()=(/)\) is the negative differential entropy and \(>0\). These dynamics are obtained as the mean-field limit of _noisy_ interacting particles dynamics  and converge globally at an exponential rate , under two key conditions on \(F\): (i) a notion of regularity, which we refer to as _displacement smoothness_ (see P1 below) and (ii) a _uniform log-Sobolev inequality (LSI)_ condition (see P2 below). These mean-field, continuous-time guarantees have been further refined into computational guarantees for fully discrete algorithms . The favorable properties of MFLD naturally lead to the following question:

_Can we efficiently solve problems of the form Eq. (1.1) using MFLD?_

At first, it is not obvious that MFLD can be applied at all since it is originally defined only for problems over probability measures. However, we can find in the literature two general recipes to reduce a problem over \(()\) to a problem over \((^{})\), thus amenable to MFLD. The first one is a _lifting_ reduction, that takes \(^{}=\) where the extra dimension serves to encode the signed mass of particles [14, Section A.2]. The second one, that takes \(^{}=\), is a _bilevel_ reduction  that uses a variational representation of the regularizer \(\|\|_{TV}^{2}\), common in the multiple kernel learning literature . A first task is thus to compare the behavior of MFLD on these two approaches. Furthermore, MFLD involves an entropic regularization which is absent from Eq. (1.1). A second task is thus to analyze the behavior of MFLD in the large \(\) regime, when the regularization vanishes.

In this work, we tackle these two tasks and make the following contributions:

* In Sec. 3, we introduce the lifting and bilevel reductions and compare the "displacement smoothness" (P1) and "uniform LSI" (P2) properties of the resulting problems. These properties play a central role in the global convergence analysis of MFLD. Specifically, we consider a large class of lifting reductions and show that none satisfies simultaneously (P1) and (P2) unless \(\) is large. In contrast, the bilevel reduction satisfies both under mild assumptions. So in the sequel we focus on MFLD applied to the bilevel reduction.
* In Sec. 4, we investigate what convergence rates can be obtained for the problem (1.1) by using MFLD on the bilevel formulation. While a classical simulated annealing technique yields convergence in \(O( t/ t)\), we show that the structure of the bilevel objective is in fact amenable to a more efficient annealing schedule, adapted from , that reaches a fixed multiplicative accuracy, say \(1.01 G_{}\), in time \(e^{O(^{-1}^{-1})}\) instead of \(e^{O(^{-2})}\) for the classical schedule.
* In Sec. 5, to obtain a more complete picture, we investigate the problem of learning a single neuron. Here, using a Lyapunov type argument, we show that the _local_ convergence rate of MFLD applied to the bilevel formulation scales polynomially in \(\) and \(d\), at odds with all previous MFLD analyses which had exponential dependencies.

All proofs are deferred to the Appendix.

### Related work

Particle methods and mean-field limits.Interacting particle systems have been studied for decades in various fields, see e.g. . Their more recent connection with the standard training of 2NNs  has suggested new settings of analysis, where convexity of the functional plays a key role, and has led to many developments. In particular, the case of MFLD (under study here) quickly progressed from nonquantitative guarantees , to mean-field convergence rates  and fully discrete computational guarantees  in the span of a few years. Recent progress also address its accelerated (underdamped) version , which could also be of interest in our setting.

Multiple kernel learning and bilevel training of NNs.The lifting reductions we consider are inspired by the unbalanced optimal transport literature , while the bilevel reduction comes from the Multiple Kernel Learning (MKL) literature  (see  for an account). While the latter is usually studied with a discrete domain \(\) (see also  for recent computational considerations), it was suggested for the training of large width 2NN in  and used in conjonction with MFLD in  (more details below). Relatedly, a recent line of work studies the (noiseless) training of 2NN in a two-timescale regime, where the outer layer is trained at a much faster rate than the inner layer . This implicitly corresponds to optimizing the bilevel objective and leads to improved convergence guarantees.

The work that is closest to ours is , which considers the MFLD on a 2NN with weight decay where the outer layer is optimized at each step. They interpret the resulting dynamics as a kernel learning dynamics and study properties of the learnt kernel and its associated RKHS. While they do not formulate explicitly the problem Eq. (1.1), it can be shown that our approaches are equivalent when considering \(=^{d+1}\) in Eq. (1.2) (and adding an extra regularization). The details are given in Sec. A.2. Key advantages of our formulation with \(=^{d}\) are that we cover the case of unbounded homogeneous activation functions (such as ReLU), and can obtain improved LSI.

## 2 Background on guarantees for mean-field Langevin dynamics

The MFLD is defined as the Wasserstein gradient flow \((_{t})_{t_{+}}\) in \(()\) of an objective of the form Eq. (1.3). It is characterized as the solution to the partial differential equation (PDE)

\[_{t}_{t}=(_{t} F^{}[_{t}])+^{-1} _{t},_{0}().\] (2.1)

where \(F^{}[]:\) is the _first variation_ of \(F\) at \(\)[11, Sec. 7.2], defined by \(_{ 0}(F(+(^{}-))-F( ))= F^{}[](^{}-)\) for any \(^{}()\). This PDE corresponds to the mean-field limit (\(N\)) of the noisy particle gradient flow \(_{t}^{N}\):

\[ i N,\;_{t}^{i}=-N_{_{t}^{i}}F^{(N)} (_{t}^{1},...,_{t}^{N})t+} B_{t}^{i},_{0}^{i}}}{{}}_{0}\]

where \(F^{(N)}(^{1},...,^{N})=F(_{i=1}^{N} _{^{i}})\) and the \(B_{t}^{i}\) are \(N\) independent Brownian motions on \(\). The convergence guarantees for MFLD rely on three key properties:

1. **(Convexity)**\(F\) is convex and is such that \(F_{}\) admits a minimizer \(_{}^{*}\).
2. **(Displacement smoothness)**\(F\) is \(L\)-displacement smooth, in the sense that3 3. **(Uniform LSI)** There exists \(>0\) such that \( t 0\), \(F_{}\) satisfies local \(\)-LSI at \(_{t}\), as in Def. 2.1.

**Definition 2.1** (Local LSI).: We say that a functional \(F_{}=F+^{-1}H\) satisfies local \(\)-LSI at \(()\) if \(Z_{}(- F^{}[])<\) and the _proximal Gibbs measure_\( Z^{-1}(- F^{}[])()\) satisfies \(\)-LSI, that is

\[^{}(),\ H(^{}| )I(^{}|),\]

where the relative entropy and relative Fisher Information are respectively defined as

\[H(^{}|)_{}(^{}}{})^{},  I(^{}|)_{}\| ^{}}{}()\|_{}^ {2}^{}(),\]

and \(\|\|_{}\) denotes the Riemannian metric.

We review some useful criteria for LSI in App. B. In particular, the uniform LSI property (P2) holds for example when training two-layer neural networks with a frozen second layer, under some technical assumptions such as bounded activation function. In fact in that case, the proximal Gibbs measures \(\) even satisfy LSI uniformly for _all_\(()\).

Note that the Riemannian gradient \(\) and the Laplace-Beltrami operator \(\) appearing in (2.1), as well as the definition of Brownian motion, depend on the Riemannian metric of \(\). This dependency is reflected in (P1) and (P2).

The global convergence of MFLD is guaranteed by the following theorem, with a rate.

**Theorem 2.1** ([10, Thm. 3.2][11, Thm. 1]).: _Consider \(F:()\) and \((_{t})\) as in (2.1). If (P0), (P1) and (P2) are satisfied then for \(t 0\) it holds_

\[^{-1}H(_{t}|_{}^{*}) F_{}(_{t})-F_{}(_{ }^{*})(-2^{-1}\ t)F_{}(_{0})-F_{} (_{}^{*}).\]

Note that although the \(L\)-smoothness constant does not appear in Thm. 2.1, it does appear in the discrete-time guarantees of , and is thus an important quantity in practice. In this paper, we limit our analysis to the mean-field dynamics (2.1) because its time-discretization has not yet been studied on Riemannian manifolds. In continuous time, the proof of Thm. 2.1 translates directly to Riemannian manifolds thanks to our definition of (P1), see App. B.

## 3 Reductions from signed measures to probability measures

In order to apply the MFLD framework to solve our initial problem over signed measures (1.1), we must first recast it as an optimization problem over probability measures. In this section we build two such reductions, and discuss the properties (P0, P1 and P2) of the resulting problems.

### Reduction by lifting

Reductions by lifting consist in representing signed measures as projections of probability measures in the higher dimensional space \(=\). This construction involves the \(1\)-homogeneous projection operator4\(:_{1}()()\) characterized by

\[(,),\ _{} (w)()(w)=_{}r(w)(r, w),\]

where \(_{p}()\) is the subset of \(()\) for which \(|r|^{p}(r,w)<+\). For instance, it acts on discrete measures as \((_{j=1}^{m}_{(r_{j},w_{j})})=_{j=1}^{m}r_{j}_{w_{j}}\). We also define, for \(b\) and \(_{b}()\), \(_{b}()(_{}|r|^{b}(r,w))^{2/b}\). The objective functional of the lifted problem is then defined, for \(_{b}()\), as

\[F_{,b}() G()+_{b}().\] (3.1)

It is equivalent to minimize \(G_{}\) or \(F_{,b}\), as shown in the following statement.

**Proposition 3.1**.: _Let \(()\). For any \(_{b}()\) such that \(=\), it holds \(F_{,b}() G_{}()\), and equality holds for \((r,w)=_{f(w)}(r)w )}{\|\|_{TV}}\) where \(f(w)=\|\|_{TV}}{||}(w)\) (and only for this \(\) when \(b>1\)). In particular, if \(G_{}\) admits a minimizer then \(F_{,b}\) does too, and it holds_

\[_{_{b}()}F_{,b}()=_{ ()}G_{}().\]

It is not difficult to see that \(F_{,b}\) satisfies (P0) as long as \(G_{}\) admits a minimizer. In order to study (P1) and (P2), we need to define a Riemannian metric on \(\). Following , we consider a general class of Riemannian metrics on \(^{*}^{*}\), parameterized by \(q_{r},q_{w}\) and \(>0\), defined by

\[ r_{1}\\  w_{1}, r_{2}\\  w_{2}_{(r,w)}=^{-1}|r|^{q_{r} } r_{2}}{r^{2}}+|r|^{q_{w}}  w_{1}, w_{2}_{w}.\] (3.2)

This indeed defines an inner product on \(T_{(r,w)}^{*} T_{w}\) that varies smoothly, and so equips \(^{*}\) with a (disconnected) Riemannian manifold structure . Intuitively, the parameter \(\) will govern the relative speed of the weight or position variables along gradient flows; larger \(\) means faster weight updates.

Two particular cases of this construction appear (sometimes implicitly) in the literature on 2NN:

1. when \(q_{r}=2\) and \(q_{w}=0\), the metric (3.2) extends to the product metric on \(=\). With \(=^{d+1}\), this corresponds to the usual parameterization of 2NNs and is the setting of most previous works applying MFLD to 2NN (with a weight decay regularization on the second layer for \(b=2\) and \(>0\)).
2. when \(q_{r}=q_{w}=1\), \(^{*}\) is isometric to the union of two copies of the (tipless) metric cone over \(\) (via the mapping \((r,)((r),,)\)). This is the natural setting for optimization over signed measures; and with \(=^{d}\), is equivalent to the parameterization of 2NNs with ReLU activation and balanced initialization [11, App. H].

Issues caused by the disconnectedness of \(^{*}\).On the level of the equivalence of variational problems, one can check that the statement of Prop. 3.1 also holds if \(=\) is replaced by \(^{*}=^{*}\). However, when the manifold \(^{*}\) is truly disconnected,5 then \(()\) is not connected in the sense of absolutely continuous curves in Wasserstein space. More precisely, \(^{*}\) is the disjoint union of \(^{*}_{-}=^{*}_{+}\) and \(^{*}_{-}=^{*}_{-}\), and one can show that (for certain choices of \(q_{r},q_{w}\)), if \((_{t})_{t}\) is a Wasserstein gradient flow (or any other absolutely continuous curve), then \(_{t}(^{*}_{+})=_{0}(^{*}_{+})\) for all \(t\).

Moreover, supposing for simplicity that \(G_{}\) has a unique minimizer \(\) and that \(b>1\), then \(F_{,b}\) has a unique minimizer \(^{*}\), and \(^{*}(^{*}_{+})=_{+}()/\|\|_{TV}\) where \(=_{+}-_{-}\) is the Jordan decomposition of \(\). Therefore, Wasserstein gradient flow for \(F_{,b}\) can only converge to \(^{*}\) if it was initialized such that \(_{0}(^{*}_{+})=^{*}(^{*}_{+})\). In terms of particle methods, this means that the fraction of the particles \((r_{i},w_{i})\) initialized with \(r_{i}>0\) must be precisely \(^{*}(^{*}_{+})\). A similar problem arises if we apply MFLD to \(F_{,b}\), since it is nothing else than Wasserstein gradient flow for \(F_{,b}+^{-1}H\); but it is more tedious to discuss formally, as \(F_{,b}+^{-1}H\) does not have a minimizer in general.

In order to bypass this limitation, one may focus on settings where the ratio \(_{+}()/\|\|_{TV}\) for the optimal \(\) is known in advance, e.g., the problem (1.1) constrained to non-negative measures, or on choices of \(q_{r},q_{w}\) for which \(^{*}\) can be extended into a connected manifold, such as the product metric \(q_{r}=2,q_{w}=0\). However, even in those cases, MFLD on \(F_{,b}\) presents other limitations.

Incompatibility with MFLD.We now show that, in spite of the degrees of freedom given by the parameters \(q_{r},q_{w}\) and \(b\), satisfying both (P1) and (P2) requires restrictive assumptions. This suggests that the lifting approach is fundamentally incompatible with MFLD.

**Proposition 3.2**.: _Consider \(F_{,b}\) from Eq. (3.1) and \(^{*}\) equipped with the metric (3.2). Suppose \(G^{}[]\) is continuous for all \(\) and that there exists \(\) such that \(^{2}G^{}[]\) is not constant equal to \(0\). Then_

* _If_ \(q_{r} 1\) _or_ \(q_{w} 1\) _or_ \(b 1\)_, then_ (_P_1_) _does not hold._
* _If_ \(q_{r}=q_{w}=b=1\)_, then for any_ \(_{1}()\)_, there exists_ \(_{0}>0\) _such that_ \(F_{,b}+^{-1}H\) _does not satisfy local LSI at_ \(\) _for any_ \(<_{0}\) _(in particular (_P_2_) does not hold unless_ \(\) _is large enough)._

When \(q_{r}=q_{w}=b=1\) and \(\) is large enough, then it can indeed be shown that Thm. 2.1 applies under natural conditions, see for instance [14, Sec. 5.1].

_Remark 3.1_.: For functionals of the form \(G_{,s}=G()+\|\|_{TV}^{s}\), instead of (1.1) which corresponds to \(s=2\), one can formulate a similar reduction by posing \(_{b,s}()=(_{}|r|^{b}(r,w))^{s/b}\) and \(F_{,b,s}()=G()+_{b,s}()\). The statements of Prop. 3.1 and Prop. 3.2 hold true with \(G_{}\) replaced by \(G_{,s}\), and \(F_{,b}\) by \(F_{,b,s}\), for any \(1 b s\), as can be shown by very simple adaptations of the proofs (only the second inequality in the proof of Lem. C.1, and the definition of \(^{}\) in (C.2), need to be adapted). Note that the problem considered in  is of the form \(G()+\|\|_{TV}\), and they analyzed Wasserstein gradient flow on \(F_{,1,1}\) with \(q_{r}=q_{w}=1\) (in particular the issues caused by the disconnectedness of \(^{*}\) are bypassed thanks to the choice \(b=1\)). The above discussion shows that applying MFLD to that problem would only yield convergence guarantees for \(\) large enough.

### Reduction by bilevel optimization

We define the bilevel objective functional \(J_{}\) for \(()\) as6

\[J_{}()_{()}G()+_{}}{}.\] (3.3)

It can be derived using the variational representation of the squared TV-norm : for any \(()\), one has \(\|\|_{TV}^{2}=_{()}_{ }}{}\). By exchanging infima, it thus holds \(_{()}G_{}()=_{ (),()}G()+ }{}=_{()}J_{ }()\). Moreover, the objective minimized in (3.3) is jointly convex in \((,)\) and partial minimization preserves convexity, so \(J_{}\) is convex. Let us gather these crucial remarks in a formal statement.

**Proposition 3.3**.: _The bilevel objective \(J_{}\) is convex and \(_{()}J_{}=_{()}G_ {}\). Moreover, if \(G_{}\) admits a minimizer \(()\), then \( J_{}= }, G_{}}\)._

Link between the lifted and bilevel reductions.The equality case in the statement of Prop. 3.1 shows that we can restrict the lifted reduction to measures \(_{b}()\) of the form \((r,w)=_{f(w)}(r)(w)\) for some \(f:\) and \(()\). Since they satisfy \((w)=f(w)(w)\), the lifted reduction with \(b=2\) thus rewrites

\[_{()}_{f L^{2}()}G(f)+_{}f(w)^{2}(w).\]

After the change of variable \((,)=(f,)\), the outer objective is precisely \(J_{}()\). Thus, Wasserstein gradient flow on \(J_{}\) can be seen as a two-timescale optimization dynamics: it is the Wasserstein gradient flow on \(F_{,2}\) in the limit where \(\). In the context of 2NN training with the parametrization (i), this amounts to training the output layer infinitely faster than the input layer, as done in . This remark allows to implement the bilevel MFLD numerically by discretizing in time the system of SDEs, for fixed large \(N\) and \(\),

\[ i N, r_{t}^{i}=-\ _{r^{i}}F_{,2}^{}[_{t}](r_{t}^{i},w_{t}^{i}) t =-\ G^{}[_{t}](w_{t}^{i})+ r_{t}^{i} \,t\] (3.4) \[w_{t}^{i}=-_{w^{i}}F_{,2}^{}[_{t} ](r_{t}^{i},w_{t}^{i})t+}B_{t}^{i} =-r_{t}^{i} G^{}[_{t}](w_{t}^{i})t+}B_{t}^{i}\]where \(_{t}=_{i=1}^{N}_{ r^{i}_{t},w^{i}_{t}}\) and \(_{t}=_{i=1}^{N}r^{i}_{}_{w^{i}_{t}}\), and taking \(_{t}=_{i=1}^{n}_{w^{i}_{t}}\). Notice the absence of noise term on the weight variables \(r\); it reflects the fact that MFLD for the bilevel objective is _not_ a limit case of MFLD for the lifted objective, as the noise would prevent to reach optimality in the inner problem.

Compability with MFLD.We now show that, in contrast to the lifting reduction, the bilevel reduction is amenable to MFLD. The main assumption on (1.1) is as follows.

**Assumption 1**.: \(G:()\) is non-negative and admits second variations, and for each \(i\{0,1,2\}\), there exist \(L_{i},B_{i}<\) such that \(\|^{i}G^{}[](w,w^{})\|_{w} L_{i}\) and \(\|^{i}G^{}[]\|_{w} L_{i}\|\|_{TV} +B_{i}\) for all \(()\) and \(w,w^{}\). Moreover there exists \(_{2}<\) such that \(\|_{w}_{w^{}}G^{}[](w,w^{}) \|_{2}\) for all \(,w,w^{}\). Furthermore, \(\) is compact and the uniform probability measure \(\) on \(\) satisfies LSI with constant \(_{}\).

Concrete settings that satisfy Assumption 1 are discussed in Sec. 5. The following proposition confirms the compatibility with MFLD and gives quantitative bounds on the LSI constant.

**Proposition 3.4**.: _Under Assumption 1, \(J_{}\) satisfies (P0), (P1) and (P2). More precisely, for any \(()\), \(J_{}+^{-1}H\) satisfies local LSI at \(\) with the constant \(_{}=_{}(-L_{0} J_{ }())\). Further, \(J_{}+^{-1}H\) satisfies \(\)-LSI uniformly along the MFLD trajectory \((_{t})_{t}\) with the constant \(=_{}(-L_{0}\{G(0),J_ {}(_{0})+^{-1}H(_{0}|)\})\)._

In view of the negative result of Prop. 3.2 for the lifting reduction, and the positive result of Prop. 3.4 for the bilevel reduction, in the sequel we focus on MFLD applied on \(J_{}\), which we will refer to as MFLD-Bilevel.

## 4 Global convergence and annealing for MFLD-Bilevel

While the bounds from Prop. 3.4 along with Thm. 2.1 allow to establish global convergence to minimizers of \(J_{}+^{-1}H\), our aim is to minimize the unregularized bilevel objective \(J_{}\). This can be achieved by annealing the temperature parameter \(^{-1}\) along the dynamics. Namely, Theorem 4.1 of  guarantees that by choosing \(_{t}=c(t)\) for an appropriate constant \(c\), the annealed MFLD trajectory

\[_{t}_{t}=(_{t} J_{}^{*}[_{ t}])+_{t}^{-1}_{t}\]

satisfies \(J_{}(_{t})- J_{}=O()\). This is a very slow rate however.

In this section, we show that the structure of \(J_{}\) originating from the bilevel reduction can be exploited to go beyond the generic guarantees from [10, Thm. 4.1]. Namely, we study in detail an alternative temperature annealing strategy, and we show that it improves upon the classical one \(_{t}(t)\) in terms of convergence to a fixed multiplicative accuracy.

### Faster convergence to a fixed multiplicative accuracy

**Definition 4.1**.: Suppose \(0 G\), so that \(J_{}^{*} J_{}>0\). We will say that MFLD-Bilevel with a given temperature annealing schedule \((_{t})_{ 0}\)_converges to \((1+)\)-multiplicative accuracy in time-complexity \(T_{}\)_, for a fixed positive constant \(\) (say \(=0.01\)), if \(J_{}(_{T_{}})(1+)J_{}^{*}\).

Note that in machine learning settings where the problem (1.1) corresponds to learning with overparameterized models, it is realistic to assume \(J_{}^{*}\) to be small (as long as the regularization \(\) is small), and \(T_{}\) is the time it takes for the annealed MFLD to achieve a suboptimality of at most \( J_{}^{*}\).

For ease of comparison, let us report the time-complexity \(T_{}\) that can be achieved by simply running MFLD-Bilevel with a constant but well-chosen \(\), based on the bounds from Prop. 3.4 and Thm. 2.1.

**Proposition 4.1** (Baseline "annealing" schedule: constant \(_{t}\)).: _Under Assumption 1, let \(>0\) and assume that \(L_{1}G(0)}{^{2}J_{}^{*}}\). Then, MFLD-Bilevel with the temperature schedule \( t,_{t}=^{*}}(^{*}})\) converges to \((1+)\)-multiplicative accuracy in time_

\[T_{}}{ J_{}^{*}}(^{*}})(L_{0}G(0)}{ \; J_{}^{*}}(^{*}} ))(^{*}}+C^{}H (_{0}|))\]

_where \(B=(L_{0},L_{1},B_{1},G(0),^{-1})\) and \(C,C^{}\) are constants dependent on \(\) (and \(d\) and \(_{}\))._For the annealing schedule \(_{t}(t)\), the time-complexity \(T_{}\) that can be guaranteed from inspecting the proof of [16, Thm. 4.1] has the same dependency on \(d,\) and \(J_{}^{*}\) as for the baseline \(_{t}=\).

Improved annealing schedule.Recall the result of Prop. 3.4: for any \(>0\), \(J_{}+^{-1}H\) satisfies local \(_{}\)-LSI at \(\) with \(_{}=_{}(-}{} J_{}( ))\). Informally, if we manage to control \(J_{}(_{t})\) along the annealed MFLD trajectory and show that it decreases, then we can increase \(_{t}\) at the same rate, while retaining the same local LSI constant. This observation and the resulting annealing procedure were introduced in , in a 2NN classification setting with the logistic loss. There the optimal value of the loss functional, corresponding to our \(J_{}^{*}\), is \(0\), and the annealing procedure yields favorable rates for global convergence. Here we show that this procedure is also applicable for MFLD-Bilevel, as soon as \(G\) satisfies the mild Assumption 1, yielding favorable rates for convergence to a fixed multiplicative accuracy.7

**Theorem 4.2**.: _Under Assumption 1, there exist constants \(B=(L_{i},B_{i},G(0),^{-1})\) and \(C_{i}\) dependent only on \(G(0)\), \(H(_{0})\), \(\) (and \(d\) and \(_{}\)) such that the following holds. For any \(^{*}}\),_

_MFLD-Bilevel with the temperature schedule \((_{t})_{t 0}\) defined by \( k K, t[t_{k},t_{k+1}],_{t}=2^{k}d\) where \(t_{0}=0\) and \(K= 2_{2}(B/( J_{}^{*}))\) and_

\[t_{k+1}-t_{k}=C_{1}2^{k}\ k(d}{}(}{}(^{*}})+C_{2}) ),\]

_achieves \((1+)\)-multiplicative accuracy, with time-complexity_

\[T_{} t_{K+1}}{ J_{}^{*}}( {B}{ J_{}^{*}})^{2}(d}{} (}{}(^{*}})+C_ {2})).\]

Note that assuming that \(G\) admits a minimizer \(_{0}\) and that \( G=0\), as is typically the case in over-parametrized machine learning settings, then by the envelope theorem \(J_{}^{*}=(G+_{TV }^{2})=_{TV}^{2}}{2^{2}}+o()\). So in the regime of small \(\), ignoring the subexponential factors, the time complexity bound achieved by the annealing schedule of Thm. 4.2 scales as \((c^{-1}^{-1})\) for a constant \(c\). This improves upon the time complexity bound of the classical annealing procedure \(_{t}(t)\) (the same as in Prop. 4.1), which scales as \((c^{}^{-2})\).

## 5 Local LSI constant at optimality for learning a single neuron

Devising temperature annealing schemes for global convergence, as illustrated in the previous section, relies on bounds on the local LSI constant at every iterate \(_{t}\) of the (annealed) MFLD. Such bounds are readily provided by the widely applicable Holley-Stroock perturbation argument, on which for example our Prop. 3.4 is based, but may be overly pessimistic. Indeed in this section, we demonstrate that for MFLD-Bilevel, _the LSI constant at convergence can be independent of \(\), \(\) and \(d\), instead of exponential in \(\)_ as a global analysis would suggest.

More precisely, we are interested in \(^{*}\), the best local LSI constant of \(J_{,} J_{}+^{-1}H(|)\), at \(_{,} J_{,}\). In fact the proximal Gibbs measure of the optimum is the optimum itself: \(}=_{,}\), so \(^{*}\) is precisely the LSI constant of \(_{,}\). A bound on \(^{*}\) is of interest, especially in the regime of large \(\) (low entropic regularization), for two reasons. Firstly, it directly implies a local convergence bound on MFLD-Bilevel, as shown in the proposition below. Secondly, characterizing the dependency of \(^{*}\) on \(\) may open the way to more efficient temperature annealing strategies; but this is out of the scope of this paper.

**Proposition 5.1**.: _Under Assumption 1, suppose \(_{,}\) satisfies LSI with some constant \(_{}^{*}\). For any \(>0\), there exists a sublevel set of \(J_{,}\) such that, for any initialization \(_{0}\) in this sublevel set, \(J_{,}(_{t})- J_{,}(J_{,}(_ {0})- J_{,})\,e^{-(_{}^{*}^{-1}- )t}\)._For the local LSI analysis, we focus on a specific setting of (1.1), namely, least-squares regression using a 2NN with a normalization constraint on the first-layer weights, and a single-neuron teacher network. See Fig. 1 for an illustrative numerical experiment. Note that Assumption 2, with additional bounded-moment assumptions on \(\) and \(\), is a special case of Assumption 1, as shown in Prop. F.4.

**Assumption 2**.: \(=^{d}\) is the Euclidean sphere in \(^{d+1}\) and there exist \(\) a covariate distribution over \(^{d+1}\), \(y L_{}^{2}(^{d+1})\) a fixed target function, and \(:\) a \(^{2}\) activation function such that \(G()=_{x}|_{}(x)-y(x)|^{2}\) where \(_{}(x)=_{}( w,x)(w)\).

Under the above assumption, we show in Prop. F.1 a simplified expression for the bilevel objective and its first variation,

\[J_{}()= y,(K_{}+\,)^ {-1}y_{L_{}^{2}}, J_{}^{}[](w)=-( w,),(K_{}+\, )^{-1}y_{L_{}^{2}}^{2},\]

where \(K_{}\) is the integral operator in \(L_{}^{2}\) of the kernel \(k_{}(x,x^{})=( w,x)( w,x^{ })(w)\) and \(\) is the identity operator on \(L_{}^{2}\). Additionally, we make the following assumption on the data distribution \(\) and on the response \(y\).

**Assumption 3**.: \(\) is rotationally invariant and the labels come from a single-index model: \(y=( v,x)\) for some fixed \(v\).

With the above assumptions, we can state the main theorem of this section.

**Theorem 5.2**.: _Under Assumptions 2 and 3, there exists a function \(g:[-1,+1]_{+}\) such that \(J_{}^{}[_{v}](w)=- g( w,v)\) for any \(w^{d}\). Suppose that \( 1\) and that there exist constants \(c_{i},C_{i}>0\) such that for all \(r[-1,+1]\),_

\[c_{1} g^{}(r) C_{1}, g^{}(r)-C_{2}, |g^{}(r)(1-r^{2})^{1/2}| C_{3},|g^{ }(r)(1-r^{2})^{3/2}| C_{4}.\]

_Then there exist constants \(_{v}\), \(D_{0}\) (dependent only on the \(c_{i},C_{i}\)) such that for any \( D_{0}d^{-1}\), \(} e^{- J_{}^{}[_{v}]}\) satisfies \(_{v}\)-LSI. Furthermore, if additionally \(}_{x}\|x\|^{4},\|^{(i) }\|_{L^{4}()}<\) for \(i\{0,1,2\}\) where \(\|\|_{L^{p}()}^{p}|( w,x )|^{p}(x)\) (independent of \(w\) as \(\) is rotationally invariant), then there exists a constant \(^{*}\) dependent only on those constants and on the \(c_{i},C_{i}\) such that, provided that \((d,^{-1})\), \(_{,}\) satisfies \(^{*}\)-LSI._

The proof is based on the observation that \(_{,} J_{}=_{v}\) the Dirac measure at \(v\), for certain regimes of \(\) and \(\), in the Wasserstein metric. Thus we show that \(J_{}^{}[_{v}]\) is amenable to a Lyapunov type argument inspired from , and then transfer its properties to \(J_{}^{}[_{,}]\).

Figure 1: The regularized training loss \(G_{}()\) (1.1) of a 2NN with the ReLU activation, learning a teacher 2NN with the 4th degree Hermite polynomial as its activation. In both plots, \(d=10\) and \(=^{-1}=10^{-3}\). The implementation details are provided in Sec. F.4. Plots are averaged over 5 experiments. \(G_{}^{*}\) is the best value achieved at each experiment. In Fig. ((b)), “Conic” refers to using the metric (3.2) with \(q_{r}=1,q_{w}=1\), while “Canonical” refers to the choice of \(q_{r}=2,q_{w}=0\).

We now verify the assumptions of Thm. 5.2 for a class of smooth, non-negative, and monotone activations which includes some popular practical choices such as the Softplus \((z)=(1+e^{z})\) and sigmoid \((z)=1/(1+e^{-z})\). While we only consider smooth activations here for simplicity, certain non-smooth activations such as a leaky version of ReLU can also satisfy the conditions of Thm. 5.2.

**Proposition 5.3**.: _Suppose Assumptions 2 and 3 hold, and \(b_{1}(d+1)[ x^{2}][ x ^{12}]^{1/6} b_{2}(d+1)\) for constants \(b_{1},b_{2}>0\). Let \(m 2b_{2}^{3/2}/b_{1}\). Suppose \(\) and \(^{}\) are non-negative, \(_{|z| m}(z)^{}(z)>0\) and \(^{(i)}_{L^{4}()}<\) for \(i 3\). Then, \(\) satisfies the assumptions of Thm. 5.2 with constants that only depend on \(b_{1}\), \(b_{2}\), and \(\)._

## 6 Conclusion

In this paper, we investigated how mean-field Langevin dynamics (MFLD), an optimization dynamics over probability measures with global convergence guarantees, can be leveraged to solve convex optimization problems over signed measures of the form (1.1). For a large class of objectives \(G\), we highlighted that MFLD with a lifting approach necessarily runs into some issues, whereas the bilevel approach always inherits the guarantees of MFLD, leading to convergence guarantees for \(G_{}\) via annealing. Finally, turning to a 2-layer NN learning task which can be stated as an instance of (1.1), we showed that the local LSI constant of MFLD-Bilevel can scale much more favorably with \(d\) and \(\) than a generic analysis would suggest.

Another approach to tackle (1.1) could be to build noisy particle dynamics directly in the space of signed measures, complementing the MFLD updates with, for instance, a birth-death process. A challenge then is to build such dynamics that can be efficiently discretized. It is also an interesting question for future works to find other settings to which MFLD can be extended, beyond signed measures.