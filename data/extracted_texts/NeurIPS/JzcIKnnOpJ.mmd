# Proof

[MISSING_PAGE_FAIL:1]

is an existing classifier for the dataset, it must be discarded. A recent approach proposes to learn a post-hoc rejector on top of a pretrained classifier via surrogate loss functions .

In this work, to learn rejectors we shift from a loss function perspective to a _distributional_ perspective. Given a model and a corresponding loss function, we find a distribution where the model and loss _performs "best"_ and compare it against the data input distribution to make rejection decisions (see Fig. 1 and Algorithm 1 with equations boxed). As such, the set of rejectors that we propose creates a rejection decision by considering the _density ratio_ between a "best" case (idealized) distribution and the data distribution, which can be thresholded by different values \(\) to provide different accuracy vs rejection percentage trade-offs. To learn these density ratios for rejection, we consider a risk minimization problem which is regularized by \(\)-divergences . We study a particular type of \(\)-divergences as our regularizer: the family of \(\)-divergences which generalizes the KL-divergence. To this end, one of our core contributions in this work is providing various methods for constructing and approximating idealized distributions, in particular those constructed by \(\)-divergences.

The idealized distributions that we consider are connected to adversarial distributions examined in _Distributionally Robust Optimization_ (DRO)  and the distributions learned in _Generalized Variational Inference_ (GVI) ; and, as such, the closed formed solutions found for our idealized distribution have utility outside of rejection. Furthermore, when utilizing the KL-divergence and Bayes optimal models, we recover the well known optimal rejection policies, _i.e._, Chow's rule . Our rejectors are then examined empirically on 6 different datasets and under label noise corruption.

In summary, our contributions are the following:

* We present a new framework for learning with rejection involving the density ratios of idealized distributions which mirrors the distributions learned in DRO and GVI;
* We show that rejection policies learned in our framework can theoretically recover optimal rejection policies, _i.e._, Chow's rule;
* We derive optimal idealized distributions generated from \(\)-divergences;
* We present a set of simplifying assumptions such that our framework can be utilized in practice for post-hoc rejection and verify this empirically.

## 2 Preliminaries

NotationLet \(\) be a domain of inputs and \(\) be output targets. We primarily consider the case where \(\) is a finite set of \(N\) labels \([N]\), where \([N]\{0,,N-1\}\). We also consider output domains \(^{}\) which is not necessarily the same as the output data \(\), _e.g._, class probabilities estimates in classification. Denote the underlying (usually unknown) input distribution as \(_{,}()\). The marginals of a joint distribution are denoted by subscript, _e.g._, \(_{}()\) for the marginal on the label space. We denote conditional distributions, _e.g._, \(_{|}\). For marginals on \(()\), the subscripting of \(\) is implicit with \(=_{}\). The empirical distributions of \(\) are denoted by \(}_{N}\), with \(N\) denoting the number of samples used to generate it. Denote the Iverson bracket \([\![p]\!]=1\) if the predicate \(p\) is true and \([\![p]\!]=0\) otherwise . The maximum \([z]_{+}\{0,z\}\) is shorthanded.

```
0: Model \(h\); Divergence \( 1\); \(\ >0\); Threshold \(\).
1: Calibrate \(h\) if needed.
2: Calculate density ratio \(^{}_{}\) (either Corollary 4.1 or 4.6).
3: Normalize \(^{}_{}\) with Monte-Carlo or Eq. (17).
4: Calculate rejector \(r_{}\) via Def. 3.2.
4:\(r_{}\)
```

**Algorithm 1** Density-Ratio Rejection

Learning with RejectionWe first recall the standard _risk minimization_ setting for learning. Suppose that we are given a (point-wise) loss function \(^{}_{ 0}\) which measures the level of

Figure 1: An _idealized distribution_\(\) is learned to _minimizes_ the loss of a model. We then compare \(\) with the original data distribution \(\) via a density ratio \(=/\). A rejection criteria is defined via threshold value \(\).

[MISSING_PAGE_FAIL:3]

For GVI, we generalize a number of quantities. For instance, the 'loss' considered can be altered from the log-likelihood \( p(z)\) to an alternative loss function over samples \(\{z_{i}\}\). The divergence function \(()\) can also be altered to change the notion of distributional distance. Furthermore, the set of distributions being minimized \(()\) can also be altered to, _e.g._, reduce the computational cost of the minimization problem. One can thus alter Eq. (4) to give the following:

\[*{argmin}_{} L()+  D(), \]

where \(>0\) and \(L\), \(D\), and \(\) denote the generalized loss, divergence, and set of distribution, respectively. Here \(\) seeks to act as a regularization constant which can be tuned.

In our work, we will consider a GVI problem which changes the loss function and divergence to define an entropy regularized risk minimization problem. Our loss function corresponds to the learning setting. For the change in divergence, we consider \(\)-divergence [1; 19], which are otherwise referred to as \(f\)-divergences  or the Csiszar divergence .

**Definition 2.2**.: Let \((-,]\) be a convex lower semi-continuous function with \((1)=0\) then the corresponding \(\)-divergence over non-negative measures for \(,\) is defined as:

\[D_{}()_{} (}{}),;, D_{}( )=+. \]

We note that for \(\)-divergences, the regularization constant in Eq. (5) can be absorbed into the \(\)-divergence generator, _i.e._, \( D_{}()=D_{ }()\).

Distributionally Robust OptimizationA related piece of literature is Distributionally Robust Optimization (DRO) , where the goal is to find a distribution that maximizes (or minimizes) the expectation of a function from a prescribed _uncertainty_ set. Popular candidates for these uncertainty sets include all distributions that are a certain radius away from a fixed distribution by some divergence. \(\)-divergences have been used to define such uncertainty set [8; 22; 45]. Given radius \(>0\), define \(B^{}_{}()\{( ):D_{}()<\}\). Given a point-wise loss function \(^{}\), DRO alters risk minimization, as per Eq. (1), to solve the following optimization problem:

\[_{h}\;_{,x B_{}()}\; _{}_{x,y}}[(,h())]. \]

The max over \(}\) is typically over the target space \(\). That is, \(}(x,y)=(y)(x y)\) and the max is adversarial over the marginal label distribution . Note that converting the \(\)-ball constraint into a Lagrange multiplier, the inner optimization over \(\) in Eq. (7) mirrors Eq. (5). The connection between GVI and adversarial robustness has been previously noted .

Typically in DRO and related learning settings, the construction of the adversarial distribution defined by the inner maximization problem is implicitly solved. For example, when \(\) is twice differentiable, it has been shown that the inner maximization can be reduced to a variance regularization expression [22; 21]; whereas other choices of divergences such as kernel Maximum Mean Discrepancy (MMD) yields kernel regularization  and Integral Probability Metrics (IPM) correspond to general regularizers . Another popular choice is the Wasserstein distance which has shown strong connections to point-wise adversarial robustness [10; 11; 64].

The aforementioned work, however, seek only to find the value of the inner maximization in Eq. (7) without considering the form the optimal adversarial distribution takes.

## 3 Rejection via Idealized Distributions

We propose learning rejection functions \(r\{0,1\}\) by comparing data distributions \(\) to a learned idealized distribution \(\) (as per Fig. 1). An idealized distribution (w.r.t. model \(h\)) is a distribution which when taken as data results in low risk (per Eq. (1)). \(\) are idealized rather than 'ideal' as they do not solely rely on a model's performance, but are also regularized by their distance from the data distribution \(\). Formally, we define our idealized distribution via a GVI minimization problem.

**Definition 3.1**.: Given a data distribution \(_{,}()\) and a \(\)-divergence, an _idealized distribution_\(()\) for a fixed model \(h\) and loss \(\) is a distribution given by

\[*{arginf}_{()}\;L() + D_{}(), \]

where \(L()_{}[L^{}()]\) and \(L^{}(x)_{_{|x}}[ the objective of Eq. (8), an idealized distribution \(Q\) will have high mass when \(L^{}(x)\) is small and low mass when \(L^{}(x)\) is large. The \(\)-divergence regularization term prevents the idealized distributions from collapsing to a point mass. Indeed, without regularization the distance from \(P\), idealized distributions would simply be Dirac deltas at values of \(x\) which minimize \(L^{}(x)\).

With an idealized distribution \(Q\), a rejection can be made via the density ratio  w.r.t. \(P\).

**Definition 3.2**.: Given a data distribution \(P\) and an idealized distribution \(Q P\), the \(\)-ratio-rejector w.r.t. \(Q\) is given by \(r_{}(x)(x)\), where \((x)Q/P(x)\).

Definition 3.2 aims to reject inputs where the idealized rejection distribution has lower mass than the original data distribution. Given Definition 3.1 for idealized distribution, small values of \((x)\) corresponds to regions of the input space \(\) where having lower data probability would decrease the expected risk of the model. Note that we do not reject on regions with high density ratio \(\) as \(L^{}(x)\) would be necessarily small or the likelihood of occurrence \(P(x)\) would be relatively small. We restrict the value of \(\) to \((0,1]\) to ensure that rejection is focused regions where \(L^{}(x)\) is high with high probability w.r.t. \(P(x)\) -- further noting that \(=0\) always rejects.

Although Definitions 3.1 and 3.2 suggests that we should learn distributions \(Q\) (and \(P\)) separately to make our rejection decision, in practice, we can learn the density ratio \(Q/P\) directly. Indeed, through the definition of Definition 2.2, we have that equivalent minimization problem:

\[*{argmin}_{_{+}}\ \ _{P_{,}}[(X)(Y,h(X))+ ((X))];\ \ \ _{P}[(X)]=1. \]

Such an equivalence has been utilized in DRO previously [23, Proof of Theorem 1]. Notice that the learning density ratio \(\) in Eq. (9) is analogous to the acceptor \(1-r(x)\) in Eq. (2). Indeed, ignoring the normalization constraint \(_{P}[(X)]=1\), by restricting \((x)\{0,1\}\), letting \(=c\), and letting \((z)= z=0\), the objective function of Eq. (9) can be reduced to:

\[_{P_{,}}[(X)(Y,h(X))]+c  P((X)=0). \]

Given the restriction of \(\) to binary outputs \(\{0,1\}\) (and Definition 3.2), we have that \(r_{}(x)=1-(x)\). As such, Eq. (10) in this setting is equivalent to the minimization of \(r\) in Eq. (2) (with \(\) as all possible functions \(\{0,1\}\)). Through the specific selection of \(\) and restriction of \(r\), we have shown that rejection via idealized distributions generalizes the typical learning with rejection objective Eq. (2).

By utilizing form of \(\)-divergences, we find the form of the idealized rejection distributions \(Q\) and their corresponding density ratio \(\) used for rejection.

**Theorem 3.3**.: _Given Definition 3.1, the optimal density ratio function \(\) of Eq. (9) is of the form,_

\[_{}^{}(x)=(^{})^{-1}( (x)+b}{}), \]

_where \(a(x)\) are Lagrange multipliers to ensure non-negativity \(_{}^{}() 0\); and \(b\) is a Lagrange multiplier to ensure the constraint \(_{P}[_{}^{}(X)]=1\) is satisfied. Furthermore, the optimal idealized rejection distribution is given by: \(Q^{}(x)=P(x)_{}^{}(x)\)._

Taking \(h\) as the Bayes posterior \(h^{}\), \(L^{}\) becomes a function of the ground truth posterior \((} X=x)\). Hence taking the output \(h\) as a neural network plugin estimate of the underlying true posterior \((} X=x)\) (see Section 4.3) yields an approach similar to softmax response, _i.e._, rejection based on the output of \(h\) when it outputs softmax probabilities . Theorem 3.3 presents a general approach to generating rejectors from these plugin estimates, as a function of \(\) and \(\).

Connections to GVI and DROThe formulation and solutions to the optimization of idealized distributions has several connections to GVI and DRO. In contrast to the setting of GVI, Eqs. (4) and (5), the support of the idealized distributions being learned in Definition 3.1 is w.r.t. inputs \(\) instead of parameters. Furthermore, the inner maximization of the DRO optimization problem, Eq. (7), seeks to solve a similar form of optimization. For idealized distributions, the maximization is switched to minimization and we consider a distribution over inputs \(\) instead of targets \(\). Indeed, notice that the explicit inner optimization of DRO (in Eq. (7)) over \(Q\) can be expressed as the following via the Fan's minimax Theorem [25, Theorem 2]:

\[_{>0}\ _{Q_{}()}\ \ -L(Q_{ })+(D_{}(Q_{} P)-). \]Notably, the loss \(L()\) in Eq. (7) can be simply negated to make the optimization over \(\) in Eq. (12) equivalent to DRO Eq. (8) (noting the only requirement for Eq. (7) to Eq. (12) is that \(L()\) is a linear functional of \(\)). This shows that switching the sign of the loss function changes idealized distributions of Definition 3.1 to DRO adversarial distributions. Indeed, through the connection between our idealized distributions and DRO adversarial distributions, the distributions \(^{}(x)\) will have the same form as the optimal rejection distributions implicitly learned in DRO.

**Corollary 3.4**.: _Suppose \(_{}^{}\) denotes the optimal idealized distribution in Theorem 3.3 (switching \(L()\) to \(-L()\)) for a fixed \(>0\). Further let \(^{}*{arginf}_{>0}\{-L(_{ }^{})+(D_{}(_{}^{} )-)\}\). Then the optimal adversarial distribution in the inner minimization for DRO (Eq. (7)) is \(_{^{}}^{}\)._

As such, the various optimal idealized distributions (w.r.t. Definition 3.1) for rejection we will present in the sequel can be directly used to obtain the optimal adversarial distributions for DRO.

## 4 Learning Idealized Distributions

In the following section, we explore optimal closed-form density ratio rejectors. We first examine the easiest example -- the KL-divergence -- and then consider the more general \(\)-divergences.

### KL-Divergence

Let us first consider the KL-divergence  for constructing density ratio rejectors via Theorem 3.3.

**Corollary 4.1**.: _Let \((z)=z z-z+1\) and \(>0\). The optimal density ratio of Definition 3.1 is,_

\[_{}^{}(x)=((x) }{}),Z=[( (x)}{})]. \]

One will notice that \(_{}^{}\) corresponds to an exponential tilt  of \(\) to yield a _Gibbs distribution_\(^{}\). The KL density ratio rejectors are significant in a few ways. First, we obtain a closed-form solution due to the properties of '\(\)' and complementary slackness, _i.e._, \(a()=0\). Secondly, due to the properties of \(\), the normalization term \(b\) has a closed form solution given by the typical log-normalizer term of exponential families.

Another notable property of utilizing a KL idealized distribution is that it recovers the previously mentioned optimal rejection policies for classical modelling with rejection via cost penalty.

**Theorem 4.2** (Informal).: _Given the CPE setting Theorem 2.1, if \(h=h^{}\) is optimal, then there exists a \(r_{}^{}\) which is equivalent to the optimal rejectors in Theorem 2.1._

Theorem 4.2 states that the KL density rejectors (with correctly specified \(\) and \(\)) with optimal predictors \(h^{}\) recovers the optimal rejectors of the typical rejection setting, _i.e._, Chow's rule.

Until now, we have implicitly assumed that the true data distribution \(\) is accessible. In practice, we only have access empirical \(}_{N}\), defining subsequent rejectors \(_{,N}\). We show that for the KL rejector, \(}_{N}\) is enough given the following generalization bound.

**Theorem 4.3**.: _Assume we have bounded loss \(|(,)| B\) for \(B>0\), \(}_{N}\) with h.p., and \(()\). Suppose \(M=||<+\), then with probability \(1-\), we have that_

\[_{x}|_{}^{}(x)-_{ ,N}^{}(x)| C( {2M}{})},\]

_where \(C=(B/)^{3}(B/)\)._

Looking at Theorem 4.3, essentially we pay a price in generalization \(M\) for each element \(x\) we are testing for rejection. For generalization, it is useful to consider how \(N,M\) changes our rate in Theorem 4.3. If we assume that the test set \(\) is small in comparison to the \(N\) samples used to generate empirical distribution \(}\), then the \((1/)\) rate will dominate. A concrete example of this case is when \(||\) is finite. A less advantaged scenario is when \(M N\), yielding \(((N)/)\) -- the scenario where we test approximately the same number of data points as that used to learn the rejector. This rate will still decrease with \(N\), although with a \( N\) price.

### Alpha-Divergences

Although the general case of finding idealized rejection distributions for \(\)-divergences is difficult, we examine a specific generalization of the KL case, the \(\)-divergences .

**Definition 4.4**.: For \(\), the \(\)-divergence \(D_{}\) is defined as the \(_{}\)-divergence, where

\[_{}(z)} (1-z^{})-(z-1)&  1\\ - z+(z-1)&=-1\\ z z-(z-1)&=1.\]

We further define \(_{}\), where \(_{}(z)=z^{}\) when \( 1\) and \(_{}(z)= z\) when \(=1\).

Note that taking \(=1\) recover the KL-divergence. The \(\)-divergence covers a wide range of divergences including the Pearson \(^{2}\) divergence \((=3)\). For the density ratio, \(\)-divergences with \( 1\) (_i.e._ not KL) can be characterized as the following.

**Theorem 4.5**.: _Let \( 1\) and \(>0\). For \(_{}\), the optimal density ratio rejector \(_{}^{}(x)\) is,_

\[_{}^{}(x)=_{}^{-1}( (a(x)-(x)}{}+b)^{-1}) \]

\(a(x)\) _are Lagrange multipliers for positivity; and \(b\) is a Lagrange multiplier for normalization._

One major downside of using general \(\)-divergences is that solving the Lagrange multipliers for the idealized rejection distribution is often difficult. Indeed, the "\(\)" and "\(\)" ensures non-negativity of the idealized distribution when the input data \(\) is in the interior of the simplex; and also provides a convenient normalization calculation. For \(\)-divergences, the non-negative Lagrange multipliers \(a()\) can be directly solved given certain conditions.

**Corollary 4.6**.: _Suppose \(>1\) and \(>0\), then Eq. (14) simplifies to,_

\[_{}^{}(x)=[((b-(x)}{}))^{}]_{+}, \]

_where we take non-integer powers of negative values as 0._

On the other hand, for \(-1\), \(D_{}()=\) whenever \(\) is on the boundary whenever \(\) is not on the boundary [3, Section 3.4.1]. As such, we can partially simplify Eq. (14) for \(-1\).

**Corollary 4.7**.: _Suppose \(-1\), \(>0\), and \(\) lies in the simplex interior, then \(a()=0\) in Eq. (14)._

Both Corollaries 4.6 and 4.7 can provide a unique rejector policy than the KL-divergence variant. Corollary 4.7 can provide a similar effect when \(a(x) 0\) for all \(x\). Nevertheless, having to determine which inputs \(a(x) 0\) and solving these values are difficult in practice. As such, we focus on \(>0\). If there are values of \(L^{}(x)\) with high risk, the \(\) will flatten these inputs to \(0\) in Corollary 4.6. However, if the original model \(h\) performs well and \(L^{}(x)\) is relatively small for all \(x\), then it is possible that the \(\) is not utilized. In such a case, the \(\)-divergence rejectors can end up being similar -- this follows the fact that (as \(_{}^{}(1)>0\)) locally all \(\)-divergences will be similar to the \(^{2}\) / \((=3)\)-divergence [56, Theorem 7.20]. This ultimately results in \(\)-divergences being similar to, _e.g._, Chow's rule when \(h_{y}(x)(=y=x)\) in classification via Theorem 2.1.

Among the \(>0\) cases, we examine the \(^{2}\)-divergence (\(=3\)) which results in closed form for bounded loss functions and sufficient large regularizing parameter \(\).

**Corollary 4.8**.: _Suppose that \((,) B\) and \(>_{x}L^{}(x)-_{}[L^{}(x)]\). Then,_

\[_{}^{=3}(x)=1+_{}[L^{}(x)]-L ^{}(x)}{}. \]

The condition on \(\) for Corollary 4.8 is equivalent to rescaling bounded loss functions \(\). Indeed, by fixing \(=1\), we can achieve a similar Theorem with suitable rescaling of \(\). Nevertheless, Eq. (16) provides a convenient form to allow for generalization bounds to be established.

**Theorem 4.9**.: _Assume we have bounded loss \(|(,)| B\) for \(B>0\), \(>2B\), \(}_{N}\) with h.p., and \(()\). Suppose \(M=||<+\), then with probability \(1-\), we have that_

\[_{x}|_{}^{=3}(x)-_{}^ {=3}(x)|( {2M}{})}.\]

Notice that Theorem 4.9's sample complexity is equivalent to Theorem 4.3 up to constant multiplication. Hence, the analysis of Theorem 4.3 regarding the scales of \(N,M\) hold for Theorem 4.9.

A question pertaining to DRO is what would be the generalization capabilities of the corresponding adversarial distributions \(}_{,N}}_{N}_{,N}\) (through Corollary 3.4). On a finite domain, via Theorems 4.3 and 4.9 and a simple triangle inequality, one can immediately bound the total variation \((}_{,N},_{}) (1/)\), see Appendix M for further details.

### Practical Rejection

We consider practical concerns for utilizing the KL-or (\(\) > 1)-divergence case, Eqs. (13) and (15), for post-hoc rejection. To do so, we need to estimate the loss \(L^{}(x)\) and reject normalizer \(Z\) or \(b\).

**Loss**: The former is tricky, we require an estimate to evaluate \(L^{}(x)=_{_{|}}[( ,h(x))]\) over any possible \(x\) to allow us to make a rejection decision. Implicitly, this requires us to have a high quality estimate of \(_{|}\). In a general learning setting, this can be difficult to obtain -- in fact it is just the overall objective that we are trying to learning, _i.e._, predicting a target \(y\) given an input \(x\). However, in the case of CPE (classification), it is not unreasonable to obtain an _calibrated estimate_ of \(_{|}\) via the classifier \(h()\). In Section 5, we utilize temperature scaling to calibrate the neural networks we learn to provide such an estimate . Hence, we set \(L^{}(x)=_{ h(x)}[(,h(x))]\). For proper CPE loss functions, \(_{ h(x)}[(,h(x))]\) acts as a generalized entropy function. As such, the rejectors Eqs. (13) and (15) act as functions over said generalized entropy functions. It should be noted that simply considering the softmax outputs of neural networks for probability estimation have seen prior success [29; 39]. This is equivalent to taking the 0-1-loss function [15; 35; 28] and taking a plugin estimate of probabilities via the neural network's output. The study of using plugin estimates have also been explored in the model cascade literature .

**Normalization**: For the latter, we utilize a sample based estimate (over the dataset used to train the rejector) of \(_{}[_{}^{}()]\) is utilized to solve the normalizers \(Z\) and \(b\). In the case of the KL-divergence rejector, this is all that is required due to the convenient function form of the Gibbs distribution, _i.e._, the normalizer \(Z\) can be simply estimated by a sample mean. However, for \(>1\)-divergences \(b\) needs to be found to determine the normalization. Practically, we find \(b\) through bisection search of

\[_{}[((b-(x)}{}))^{}]_{+}-1=0. \]

This practically works as the optimization \(b\) is over a single dimension. Furthermore, we can have that \(b>_{x}\{L^{}(x)/\}\). As an upper bound over the possible values of \(b\), we utilize a heuristic where we multiple the corresponding maximum of \(L^{}(x)/\) with a constant.

**Threshold \(\)**: In addition to learning the density ratio, an additional consideration is how to tune \(\) in the rejection decision, Definition 3.2. Given a fixed density estimator \(\), change \(\) amounts to changing the rate of rejection. We note that this problem is not limited to our density ratio rejectors, where approaches with rejectors \(r\) via a (surrogate) minimization of Eq. (2) may be required multiple rounds of training with different rejection costs \(c\) to find an accept rate of rejection. In our case, we have a fixed \(\) which allows easy tuning of \(\) given a validation dataset, similar to other confidence based rejection approaches, _e.g._, tuning a threshold for the margin of a classifier .

## 5 Experiments

In this section, we evaluate our distributional approach to rejection across a number of datasets. In particular, we consider the standard classification setting with an addition setting where uniform label noise is introduced [4; 30]. For our density ratio rejectors, we evaluate the KL-divergence basedrejector (Corollary 4.1) and (\(\)=3)-based rejectors (Corollary 4.6) with 50 equidistant \((0,1]\) values. For our tests, we fix \(=1\). Throughout our evaluation, we assume that a neural network (NN) model without rejection is accessible for all (applicable) approaches. For our density ratio rejectors, we utilize the log-loss, practical considerations in Section 4.3, and Algorithm 1.2

To evaluate our density ratio rejectors and baselines, we compare accuracy and acceptance coverage. Accuracy corresponds to the 0-1 loss in Eq. (1) and the acceptance coverage is the percentage of non-rejections in the test set. Ideally, a rejector smoothly trade-offs between accuracy and acceptance, _i.e._, a higher accuracy can be achieved by decreasing the acceptance coverage by rejecting more data. We study this trade-off by examining multiple cut-off values \(\) and rejection costs \(c\).

Dataset and BaselinesWe consider 6 multiclass classification datasets. For tabular datasets, we consider the gas drift dataset  and the human activity recognition (HAR) dataset . Each of these datasets consists of 6 classes to predict. Furthermore, we utilize a two hidden layer MLP NN model for these datasets. We consider the MNIST image dataset  (10 classes), where we utilize a convolutional NN. Additionally, we consider 3 larger image datasets with ResNet-18 architecture : CIFAR-10  (10 classes); and OrgMNIST / OrganSMNIST (11 classes) and OctMNIST (4 classes) from the MedMNIST collection . These prediction models are trained utilizing the standard logistic / log-loss without rejection and then are calibrated via temperature scaling . For each of these datasets, we utilize both clean and noisy variants. For the noisy variant, we flip the class labels of the train set with a rate of 25%. We note that the test set is clean in both cases. All evaluation uses 5-fold cross validation. All implementation use PyTorch and training was done on a p3.2xlarge AWS instance.

We consider 4 different baseline for comparison. Each is trained with 50 equidistant costs \(c,[0,0.5)\), except on OctMNIST which uses 10 equidistant costs (selecting \(c,\) discussed in Appendix P). One baseline used corresponds to a modification of 's cross-entropy surrogate approach (DEFER) originally used for the learning to defer literature (see [14, Appendix A.2]). This approach treats the rejection option as a separate class and solves a \(||+1\) classification problem. A generalization of DEFER is considered which utilizes generalized cross-entropy  as its surrogate loss (GCE) . We also consider a cost-sensitive classification reduction (CSS) of the classification with rejection problem  utilizing the sigmoid loss function. The aforementioned 3 baselines all learn a model with rejection simultaneously, _i.e._, a pretrained model cannot be utilized. We also consider a two stage predictor-rejector approach (PredRej) which learns a rejector from a pretrained classifier .

ResultsTable 1 presents a tabular summary of accuracy and coverage values when targeting 80% coverage values; and Fig. 2 presents a summary plot of the acceptance coverage versus model accuracy after rejection, focused around the 60% to 100% coverage region. This plot is limited to HAR, Gas Drift, and MNIST due to space limitations however the deferred datasets show curves where the density ratio rejector dominate with better accuracy and coverage in the plotted region, with corresponding extended plots for all datasets in Appendix Q. Over all folds for MNIST our density ratio rejectors take approximately \( 1/2\) hour to fit. A single baseline (fixed \(c\)) takes upwards of 2 hour for a single fold. Overall, given that the underlying model is calibrated, we find that our

    & & Base & KL-Rej & (\(\)=3)-Rej & PredRej & CSS & DEFER & GCE \\   & HAR & 97.38  & 99.93  & **99.93** & 98.86  & 99.58  & 99.44  & 99.31  \\  & Gas Drift & 94.10  & **99.16** & **99.16** & 98.12  & 98.68  & 98.06  & 97.62  \\  & MNIST & 98.55  & 99.93  & 99.93  & 99.18  & **99.95** & 99.93  & 99.85  \\  & CIFAR-10 & 90.20  & **97.22** & 97.71  & 91.40  & 95.45  & 93.72  & 94.25  \\  & OrganMNIST & 89.10  & **96.55** & 96.52  & 93.79  & 94.49  & 93.47  & 93.68  \\  & OctMNIST & 91.93  & 97.08  & **97.18** & 93.43  & 95.40  & 94.66  & 94.91  \\   & HAR & 96.51  & 98.56  & 98.56  & 97.22  & 97.82  & 97.78  & **98.85** \\  & Gas Drift & 93.84  & 97.30  & 97.28  & 95.87  & 98.71  & **99.02** & 97.52  \\   & MNIST & 97.88  & 99.89  & 99.89  & 98.00  & 99.94  & 99.93  & **99.95** \\   & CIFAR-10 & 85.31  & 92.25  & **92.50** & 85.84  & 89.58  & 90.93  & 92.22  \\   & OrganMNIST & 89.10  & 95.86  & **96.29** & 93.40  & 96.74  & 94.67  & 94.48  \\   & OctMNIST & 91.89  & **97.17** & 97.10  & 93.42  & 95.49  & 94.08  & 94.63  \\   

Table 1: Summary of rejection methods over all baselines and datasets targeting 80% coverage. Each cell reports the “accuracy [coverage]” values, **bold** for most accurate, and s.t.d. reported in Appendix.

density ratio rejector are either competitive or superior to the baselines. One might notice that the aforementioned baselines do not or provide poor trade-offs for coverage values \(>95\%\) (as per Fig. 2). Indeed, to achieve rejection with high coverage (without architecture tuning), approaches which 'wrap' a base classifier seem preferable, _i.e._, PredRej and our density ratios rejectors. Even at lower coverage targets (80%), Table 1 shows that density-ratio methods are comparable or superior in the more complex datasets of CIFAR-10 and the MedMNIST collection. If large models are allowed to be used for the rejector -- as per the MNIST case -- CSS, DEFER, and GCE can provide superior accuracy vs acceptance coverage trade-offs (noisy MNIST). However, this is not always true as per CIFAR-10 where all approaches are similarly effected by noise; or OctMNIST and OrgMNIST where approaches only slightly change with noise. The latter appears to be a consequence of label noise not effecting the Base classifier's accuracy (using the larger ResNet-18 architecture), as per Table 1.

Among the approaches which 'wrap' the base classifier \(h\), we find that these approaches have higher variance ranges than the other approaches. In particular, the randomness of the base model potentially magnifies the randomness after rejection. The variance range of the base model tends to increase as the noise increases (additional ranges of noise for HAR and Gas Drift are presented in the Appendix). The influence on rejection is unsurprising as these 'wrapping' approaches predict via a composition of the original model (and hence inherits its randomness across folds). In general, our density ratio rejector outperforms PredRej. However, it should be noted that PredRej does not require a calibrated classifier. Among the density ratio rejectors, between KL and \(=3\), the only variation is in coverage region that the \((0,1]\) threshold covers. This follows from the fact that for similar distributions, \(\)-divergences act similarly (56, Theorem 7.20). We find this pattern holds for other values of \(\).

## 6 Limitations and Conclusions

We propose a new framework for rejection by learning idealized density ratios. Our proposed rejection framework links typically explored classification with rejection to generalized variational inference and distributionally robust optimization. It should be noted that although we have focused on classification, \(L^{}()\) could in theory be replaced by any other loss functions. In this sense, one could adapt this approach to other learning tasks such as regression, discussed in Appendix N. Furthermore, although we have focused on \(\)-divergences, there are many alternative ways idealized distribution can be constructed, _e.g._, integral probability metrics (51, 9). One limitation of our distributional way of rejecting is the reliance on approximating \([]\) with model \(h\). In future work, one may seek to approximate the density ratio \(\) by explicitly learning densities \(\) and \(\) or via gradient based methods (for the latter, see Appendix O).

Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \((0,1]\) and \(c[0,0.5)\) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \( 1\) s.t.d. region.