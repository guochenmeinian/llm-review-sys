# Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration

Mahdi Morafah\({}^{*1}\), Vyacheslav Kungurtsev\({}^{2}\), Hojin Chang\({}^{1}\), Chen Chen\({}^{3}\), Bill Lin\({}^{1}\)

\({}^{1}\)University of California San Diego (UCSD), \({}^{2}\)Czech Technical University in Prague,

\({}^{3}\)University of Central Florida (UCF)

\({}^{*}\)Correspondence: mmorafah@ucsd.edu

###### Abstract

Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes--from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive _task arithmetic_ knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL.

+
Footnote †: Project Website: https://mmorafah.github.io/takflpage.

## 1 Introduction

Federated Learning (FL) has rapidly gained traction as a promising approach to train machine learning models collaboratively across multiple devices, while preserving the privacy of user data. Standard federated learning methods, such as FedAvg , however, are primarily designed for unrealistic _device-homogeneous_ scenarios, where all devices are assumed to have identical compute resource and can train the same neural network architecture . Therefore, standard FL cannot support the participation of _heterogeneous devices_, all of which could significantly contribute to model training due to their unique and invaluable local datasets. To address this gap, knowledge distillation (KD) techniques have emerged as a promising approach to establish federation among heterogeneous device prototypes and facilitate knowledge transfer between them. In this approach, locally updated client models from different device prototypes, collectively termed as ensembles, serve as teachers to distill their knowledge into each device prototype's server student model using an unlabeled public dataset.

Despite their success, however, existing KD-based methods for device heterogeneous FL are primarily designed for scenarios where device prototypes are in the same-size with similar capabilities, i.e. same model and dataset sizes. However, in practice, _device capabilities vary widely_, ranging from small devices like IoTs with small models and small datasets to large devices like workstations with large models and large datasets. This diversity, often overlooked in the existing literature, results in device prototypes with varying strengths and information qualities. Unfortunately, existing methods struggle to establish effective knowledge transfer in these challenging, real-world device heterogeneous settings, primarily due to two reasons: 1 Existing methods often disregard the individual strengths and information quality of each device prototype's ensembles and integrate their logits into a single distillation target. _This approach dilutes the richer, more informative logits from larger, more capable devices with less informative logits from smaller, less capable ones. 2_ Additionally, these methods employ this single integrated distillation target to transfer knowledge across all different size student models. _This one-size-fits-all approach fails to provide customized knowledge integration based on the unique learning capacities of each student and the specific helpfulness of each device prototype's ensembles_.

Moreover, the heterogeneous ensemble distillation process can inadvertently lead student models into erroneous learning directions, causing them to forget their self-knowledge acquired through averaged locally updated parameters. This issue arises primarily due to two reasons: 1 The distillation process introduces noise, _as the ensembles' logits are inferred on an unfamiliar public dataset_, distinct from their original training data. Additionally, the presence of data heterogeneity and the insufficient training of some ensembles, due to computational constraints, can further exacerbate this noise. 2 The distillation process _lacks supervision from the actual private datasets, which are the ultimate learning objectives_. Consequently, these factors, combined with the limitations outlined earlier, result in _suboptimal knowledge transfer_ in device heterogeneous settings. This underscores the urgent need for a more effective knowledge transfer framework.

In this paper, we introduce TAKFL, a novel _"Task Arithmetic Knowledge Transfer Integration for Federated Learning"_ framework, designed to overcome the fundamental limitations in the existing methods and improve knowledge transfer in scenarios where device prototypes vary in size--both model and dataset--and consequently, in strength. TAKFL treats knowledge transfer from each device prototype's ensembles as separate tasks, distilling them independently to ensure that each prototype's unique contributions are accurately distilled without interference. To tackle the challenges associated with noisy and unsupervised ensemble distillation, we incorporate a KD-based self-regularization technique into this individual knowledge transfer process. Subsequently, to selectively integrate the separately distilled knowledge from heterogeneous prototypes' ensembles, we introduce an adaptive _task arithmetic_ knowledge integration method by extending the notion of task vectors from centralized learning to federated learning. Our approach enables the student model to strategically customize the knowledge integration process based on the quality of knowledge from each prototype's ensembles and its intrinsic capacity, aiming to achieve optimal performance. We present theoretical results, grounded on the established theoretical learning properties of over-parametrized neural networks, that conceptualize knowledge distillation as the allocation of device prototypes' capacities to accurately fit the chosen logits. These results demonstrate the advantages of employing task arithmetic for knowledge transfer in terms of overall accuracy, coverage, and efficiency, as well as the adaptive knowledge integration based on the capacity of the student prototype. Furthermore, we comprehensively evaluate our method across both computer vision (CV) and natural language processing (NLP) tasks, utilizing various datasets and architectures, and demonstrate that TAKFL consistently achieves state-of-the-art (SOTA) performance.

The contribution of our paper is as follows:

1. We formalize and review the important considerations of the problem statement of federated learning with heterogeneous device prototypes.
2. We introduce TAKFL, a novel KD-based method designed to overcome the fundamental limitations of existing approaches, effectively facilitating knowledge transfer across diverse heterogeneous device prototypes with varying capabilities.
3. We present the first theoretical model for device heterogeneous KD, and demonstrate the effectiveness and efficiency of TAKFL compared to the standard alternatives that do not adapt to the student's self-knowledge quality and available learning capacity.
4. Our comprehensive experimental evaluations on both CV and NLP tasks, spanning various datasets and architectures, reveal that TAKFL consistently achieves SOTA performance, outperforming existing KD-based methods.

## 2 Related Works

**Device Heterogeneous FL.** Prior works on device heterogeneous FL have considered two distinct approaches with different objectives and settings. The first array of studies focuses on accommodating devices with varying compute resources, aiming to train a single global model. Techniques such as static and rolling-based partial model training allow devices to train a sub-model of the global model tailored to their compute resources [11; 18; 3; 1]. However, this approach does not fully reflect real-world scenarios. In practice, device prototypes such as IoTs and smartphones have unique neural network architectures designed for their specific configurations and underlying tasks, which may not support training varying neural architectures. This highlights a significant limitation in accommodating the full spectrum of device heterogeneity in this approach. The second array of studies addresses a more practical scenario where device prototypes with heterogeneous model architectures participate in FL to enhance their global model performance through mutual knowledge sharing [30; 43; 6]. In this context, KD techniques are used to transfer knowledge among prototypes, where locally updated client models, termed as ensembles, serve as teachers to distill their knowledge into each server's student model using an unlabeled public dataset. For example, FedDF  uses vanilla logit averaging, while Fed-ET  applies an uncertainty-weighted logit averaging, enhanced by a diversity regularization technique. _However, existing works typically focus on settings where prototypes have similar capabilities--both model and dataset sizes--and thus neglecting the challenges in more diverse settings with varying capabilities. This oversight leaves their effectiveness in such settings largely unexplored. In this paper, we aim to study the underexplored diverse heterogeneous device settings._ See Appendix A for a more detailed discussion on the related works.

**Model Editing via Task Arithmetic.** Traditional methods for model editing often involve expensive joint fine-tuning across multiple tasks, which can limit scalability and democratization . Recently, a promising technique called task arithmetic has emerged as a cost-effective and scalable method for updating pre-trained models with new information or refining undesired behavior [53; 37; 32]. The concept of "task vectors" introduced by Wortsman et al.  plays a pivotal role in these techniques. For any given task \(t\), a task vector is derived by subtracting the model's pre-trained weights \(_{pre}\) from its fine-tuned weights \(_{ft}^{t}\) on task \(t\), i.e. \(_{t}=_{ft}-_{pre}\). These task vectors act as unique representations for specific tasks. Furthermore, researchers have demonstrated that by summing multiple task vectors \(\{_{t}\}_{t=1}^{T}\), and integrating them into a pre-trained model via \(=_{pre}+_{t=1}^{T}_{t}\), one can effectively create a model capable of handling multiple tasks [53; 57]. _To the best of our knowledge, this work is the first to extend the notion of task vectors to the federated learning setting, introducing a task arithmetic for knowledge distillation across diverse heterogeneous device prototypes._

Figure 1: **Overview of our approach and its distinction from prior works. (a) This figure illustrates the vanilla ensemble distillation process, where logits from ensembles of various sizes are averaged and used as the distillation target across all prototypes. This approach leads to the dilution of information and suboptimal knowledge transfer (refer to Sections 6 and 7 for details). (b) This figure depicts our approach, TAKFL, which treats knowledge transfer from each prototype’s ensemble as a separate task and distills them independently. Additionally, a KD-based self-regularization technique is introduced to mitigate issues related to the noisy and unsupervised ensemble distillation. Finally, the heterogeneously distilled knowledge is strategically integrated using an adaptive task arithmetic operation, allowing for customized knowledge integration based on each student prototype’s needs.**

## 3 Problem Statement: FL with Heterogeneous Device Prototypes

Consider a cross-device FL setup with a set of \(M\) distinct device prototypes \(\), i.e., \(M=||\). Each device prototype \(m_{j}\) has a distinct neural network architecture \(f^{j}(;^{j})\) parameterized by \(^{j}^{n_{j}}\) and a set of clients \(^{j}\), with \(N^{j}=|^{j}|\) clients in total. Each client \(c_{k}^{j}\) has a local private dataset \(^{j}_{k}=\{(_{i},y_{i})\}_{i=1}^{n_{j},k}\), where \(n_{j,k}=|^{j}_{k}|\), and locally trains the parameters \(^{j}\) of the neural network architecture \(f^{j}\) on its local dataset. Furthermore, denote \(^{j}=_{k^{j}}^{j}_{k}\) to be the union of the private datasets for device prototype \(j\). We assume \(^{j}^{j}\), that is a subsample from the population distribution \(^{j}\) and similarly \(^{j}_{k}^{j}_{k}\). The union of the private datasets, i.e. \(=_{j}^{j}\), is sampled from the entire population \(\), which is defined as an unknown mixture of the distributions each device prototype sampled its data from, i.e. generically non-i.i.d. We formalize this as a mixture of local clients data population, i.e., \(=_{j}_{j,:}^{j}=_{j}_{k}_{j,k} ^{j}_{k}\), where \(0_{j,k} 1\) and \(_{jk}_{j,k}=1\), and \(_{j,k}\) is unknown.

The ultimate objective is to minimize the test error and thus enable accurate inference for each device prototype \(j\), aiming to obtain the optimal parameters for the population dataset:

\[*{argmin}_{^{j}}_{(,y)} [(f^{j}(;^{j}),y)]=*{argmin}_{^{ j}}\ _{j=1}^{M}_{k=1}^{N^{j}}_{i,k}_{(,y)^{j}_{ k}}[(f^{j}(;^{j}),y)]\] (1)

where \((,)\) is the sample-wise loss function (e.g. cross entropy for image classification) and we decompose by total population loss with the linearity of expectation in the mixture. See Fig (b)b for a visual illustration of heterogeneous device prototype FL.

## 4 Background: Federated Ensemble Distillation

To address the limitations of standard FL in device heterogeneous settings, Lin et al.  proposed ensemble knowledge distillation to transfer knowledge between heterogeneous device prototypes in FL. This procedure consists of two stages: (1) local per-prototype FL, and (2) server-side vanilla ensemble distillation. The details of each stage discussed in the following paragraphs.

**Local Per-Prototype FL.** In this context, at each round \(r\) a subset of clients \(^{j}_{r}\) from each device prototype \(j\) is randomly selected by the server and download their corresponding model initialization \(^{j}_{r}\). Each client \(c^{j}_{k}^{j}_{r}\), starting from this model initialization, locally train the model \(f^{j}\) on its local private data \(^{j}_{k}\) by taking multiple steps of stochastic gradient descent. Then, they send back their updated parameters \(\{^{j}_{k}\}_{k^{j}_{r}}\) to the server. The server aggregates the received clients parameters, and computes \(^{j}_{avg}=_{k^{j}_{r}}_{k}|}{ _{k^{j}_{r}}|_{k}|}}^{j}_{k}\). In classic federated learning formalism, the parameters \(^{j}_{avg}\) satisfy,

\[^{j}_{avg}*{argmin}_{^{j}}\ _{k=1}^{N^{j}} _{(,y)^{j}_{k}}[(f^{j}(;^{j}),y)]\] (2)

**Vanilla Ensemble Distillation.** In this stage, each server model \(f^{j}\) gets initialized with \(^{j}\), and undergoes updates using ensemble knowledge distillation. Here, heterogeneous client models from heterogeneous device prototypes, collectively termed as ensembles, serve as teachers, i.e. \(:=\{f^{i}(,^{i}_{k})\,|\,i,k^{i}\}\), transferring their knowledge to each server student model, i.e. \(_{i}:=f^{i}(,^{i})\). For simplicity, we drop the index for each server student model, denoting it as \(\). The ensemble distillation loss using a mini-batch of data from an unlabeled public dataset, i.e \(^{public}\), can be defined by the following equation:

\[_{}=|} _{}()\,,\,\ (())\,,\] (3)

where \(()\) is the softmax function. As illustrated in Eq. 3, vanilla ensemble distillation treats all heterogeneous device prototypes' ensembles equally by uniformly averaging their logits. This way of knowledge integration overlooks the individual strengths and informational value of each prototype's ensembles. As a result, the richer, more informative logits from stronger ensembles are diluted by less informative logits from weaker ensembles, leading to information loss. Furthermore,this averaged logits is used as the distillation target across different-sized student models, irrespective of their intrinsic capacity and the helpfulness of each prototype's ensembles. Consequently, this leads to suboptimal knowledge transfer in device heterogeneous FL. See Section 6 for theoretical analysis and Section 7 for experimental observations.

## 5 Task Arithmetic Knowledge Transfer and Integration

In this section, we introduce TAKFL, designed to overcome the fundamental limitations of previous approaches and enhance knowledge transfer across diverse heterogeneous device prototypes, which vary in size--in terms of both model and dataset size. TAKFL consists of two main components: (1) individually transferring knowledge from each device prototype's ensembles, and (2) adaptively integrating knowledge via task arithmetic. Detailed descriptions of each component are provided in Section 5.1 and 5.2, respectively. An illustrative overview of TAKFL is presented in Figure 0(b), and the full algorithm is detailed in Appendix B, Algorithm 1.

### Knowledge Transfer from Individual Device Prototype

We begin by discussing our proposed knowledge transfer framework from each individual device prototype's ensembles. This process consists of two main components: ensemble knowledge transfer and self-regularization, each detailed in the subsequent paragraphs.

**Ensemble Knowledge Transfer.** Vanilla ensemble distillation integrates the knowledge of varying strength ensembles by uniformly averaging their logits. This approach can potentially transform or even degrade the overall quality of the knowledge being transferred, leading to suboptimal knowledge transfer. To effectively distill the unique knowledge and contributions of each prototype's ensembles, and to avoid dilution, information loss, and interference from other prototypes' ensembles, we propose transferring the knowledge from each prototype's ensembles separately and independently.

Specifically, let's consider \(_{i}:=\{f^{i}(,}_{i}^{i})|\,k^{i}\}\) denotes the ensembles of device prototype \(i\) as teacher and \(_{j}\) denotes the server student model of the device prototype \(j\). Without loss of generality, we refer to each device prototype's server student model as just student denoted as \(\). Therefore, the knowledge distillation loss between the teacher ensembles \(_{i}\) and server student \(\) (\(_{i}\)) is defined below:

\[_{KD}^{_{i}}= _{i}|}_{_{i}} ()\,,\,(()).\] (4)

**Scaffolding Student from Noisy Ensemble Distillation.** The ensemble distillation process may adversely impact the student, causing it to forget its own knowledge acquired through averaged locally updated parameters and be drifted into erroneous directions. This is primarily due to two key factors: (1) The ensemble distillation process introduces noise, mainly because the ensembles' logits are inferred on an unfamiliar public dataset they have not been trained on. These ensembles are originally trained on local private datasets, which usually differ from the unlabeled public dataset used for distillation. Moreover, other factors such as the presence of data heterogeneity within FL and insufficient training of some ensembles due to limited computational resources can exacerbate this noise, particularly in the initial rounds of federation. (2) The ensemble distillation process lacks supervision from the actual private datasets, which is the ultimate learning objective.

To scaffold the student models from the noisy and unsupervised distillation process, which may cause them to drift into erroneous directions and forget their invaluable self-knowledge, we introduce a KD-based self-regularization technique. Our self-regularization technique mitigates these issues by enforcing similarity between the logits of the student and its initial logits (when the student is initialized with averaged parameters) using KL divergence loss defined below:

\[_{}^{}=((;_{avg}))\,,\,(() ).\] (5)

**Overall Knowledge Transfer Objective.** The overall knowledge transfer objective from teacher ensembles \(_{i}\) of device prototype \(i\) to the student \(\) is the combination of the ensemble knowledge distillation loss \(_{KD}^{_{i}}\) (Eq. 4) and the self-regularization loss \(_{}^{}\) (Eq. 5) defined in the following:

\[_{}^{_{i}}=_{KD}^{_{i} }+_{}^{}.\] (6)

Here, \(\) is a hyperparameter controlling the effect of self-regularization term. We associate the knowledge transfer from each device prototype \(i\) to a task \(T_{i}\) with the loss \(_{}^{_{i}}\).

### Task Arithmetic Knowledge Integration

Herein, we delve into the details of our proposed method for customized integration of the separately distilled knowledge from heterogeneous ensembles. Drawing inspiration from recent advances in model editing via task arithmetic , where a pre-trained model's knowledge can be edited via task-specific vectors using arithmetic operation, we propose a novel customizable knowledge integration method via task arithmetic. To do so we extend the notion of task vector from centralized learning to federated learning. We conceptualize the averaged locally updated parameters, i.e. \(_{avg}\), as a "pre-trained", similar to those in centralized learning, and the parameters of the distilled model via knowledge transfer objective (Eq. 4), denoted as \(_{distilled}\), as a "fine-tuned" version of the model (see Fig. 2). Consequently, the task vector \(_{i}\) associated with the knowledge transfer task \(_{}^{_{i}}\) can be defined by subtracting the distilled parameters from the averaged locally updated parameters as follows:

\[_{i}=_{distilled}^{_{i}} -_{avg}.\] (7)

Essentially, task vectors serve as unique representations for the transferred knowledge from each prototype's ensembles to the student and encapsulate the distinct contributions of each prototype's ensembles to the student model. To selectively merge the knowledge of each prototype' ensembles into the student, we employ an adaptive task arithmetic operation as follows:

\[_{merged}=_{avg}+_{i}_{i}_{i},\] (8)

where \(_{i}\) denotes the merging coefficient associated with task vector \(_{i}\), and they sum to one, i.e. \(_{i}_{i}=1\). The merging coefficients determine the extent of knowledge integration from each prototype's ensembles. Essentially, they enable the student to have customized knowledge integration to achieve maximum performance. The student can determine these merging coefficients based on its own learning capacity and the relative knowledge and helpfulness of other device prototypes' ensembles. This approach provides an effective, low-cost, and scalable knowledge integration strategy in settings with diverse device heterogeneity. In our experiments, we considered this as a hyperparameter and tuned it manually or determined it using held-out validation sets which achieves similar results. More details can be found in Appendix F.3.

## 6 Theoretical Results

We present a theoretical understanding on the efficacy of knowledge distillation in device heterogeneous FL. We argue that vanilla ensemble distillation (VED) diffuses the information from logits, which presents a notable disadvantage for solving (1). This effect is particularly pronounced when the teacher ensembles are from a device prototype of small capacity, and the student model is from a device prototype of large capacity. By contrast, our proposed method of task arithmetic knowledge integration, mitigates the drawbacks of VED and is able to simultaneously incorporate information from differently sized heterogeneous ensembles, efficiently filling up the capacity of each student with the most informative knowledge, achieving optimal knowledge transfer.

**Assumptions and Preliminaries.** Standard practice, including the setting in consideration as well as the numerical experiments here, involves _overparametrized_ neural networks, that is, the total number of weights far exceeds the training sample size. This implies that the set of weights that minimize the loss is non-unique, and moreover, it has been argued that they form a submanifold . This submanifold structure of solution sets will provide the critical source of understanding the subsequent results. In particular, we shall consider knowledge distillation as filling up the capacity of device prototypes' models with basis vectors corresponding to submanifolds that minimize as many device prototypes' data distributions as possible.

Since we are interested in server-side distillation across heterogeneous device prototypes, we assume optimal conditions at the local per-prototype FL level, meaning that the perfect solution for local per-prototype FL is achieved. The formal details of the assumptions and statements are presented in Appendix C. In these Propositions we assume that the prototype \(i\) and \(j\) datasets are disjoint, i.e.

Figure 2: **Analogy between task vector in centralized learning and federated learning.**\(^{i}^{j}=\), and \(_{i=1}^{M}^{i}=\). We show that the other cases are trivial and uninteresting in the appendix.

**Proposition 1**.: _(information loss in VED, informal). Consider the VED procedure in the form of solving (3). Consider two device prototypes with a device capacity and solution dimension of \(Q^{1},Q^{2}\) and \(W^{1},W^{2}\), respectively, and with associated eigenbases \(^{i},^{i}\). Denote \(W^{i,j},i,j=1,2\) as the capacity allocated by student \(i\) in order to distill knowledge from teacher \(j\)'s logits._

1. _[leftmargin=*]_
2. _Case 1: When the capacities are the same, that is_ \(Q^{1}=Q^{2}\) _and_ \(W^{1}=W^{2}=W^{1,2}=W^{2,1}\)_, then with VED, there will be some capacity, in the sense of eigenspace, of student prototypes that will be allocated with parameters that do not minimize the student's its own data distribution._
3. _Case 2: Assume that_ \(Q^{1}>Q^{2}\) _and_ \(W^{1}=W^{1,2}>W^{2}\)_. Then the phenomenon as for Case 1 holds. Moreover, there will be some capacity of student 1's model that will be allocated with parameters that do not minimize either of the teacher or student prototype's data distribution._

An interesting key mechanism of the proof is that when VED is applied in distilling logits from a small device prototype to a large one, the modeling capacity of \(W^{1,2}\) is structurally reduced to that of \(W^{2}<W^{1,2}\), i.e., it is an operation wasteful of the potential model capacity.

**Remark 1**.: _This proposition proves that in general, VED is prone to diffuse knowledge already present in students, and leads to inefficient and inaccurate use of model capacity. Furthermore, under the case that device prototypes have different capacities, VED ends up leading to more erroneous models entirely as the small information within the small teacher is transferred onto a larger capacity target._

**Proposition 2**.: _(improve knowledge transfer with task arithmetic, informal). Consider the TAKFL procedure as in the form of computing (8). Consider two device prototypes with a device capacity and solution dimension of \(Q^{1},Q^{2}\) and \(W^{1},W^{2}\), respectively, and with associated eigenbases \(^{i},^{i}\)._

1. _[leftmargin=*]_
2. _Case 1: In the case that that_ \(Q^{1} Q^{2}\) _and_ \(W^{1} W^{2}\)_, it holds that the TAKFL with prototype 1 as student preserves the eigenbasis associated to the parameters used to accurately fit the data_ \(^{1}\)_._
3. _Case 2: Assume that_ \(Q^{1}=Q^{2}\) _and_ \(W^{1}=W^{2}\)_. TAKFL yields a solution for the student that is at the intersection of the subspaces corresponding to minimizing the two data distributions._
4. _Case 3: Assume that_ \(Q^{1}>Q^{2}\) _and_ \(W^{1}>W^{2}\)_. In the case of prototype_ \(1\) _being the student, TAKFL yields a solution that:_ 1. _[leftmargin=*]_ 2. _retains the approximation accuracy on device 1's data distribution,_ 3. _ensures approximation accuracy to the level of device_ \(2\)_'s relative capacity_ 3. _fills the remaining local capacity device_ \(1\) _has allocated for device_ \(2\)_'s logits with no informative new knowledge, unless enforced otherwise._

**Remark 2**.: _This proposition proves that in general, TAKFL promotes the most efficient allocation of the devices' capacity in order to accurately fit a diverse set of data distributions. With TAKFL, the previously acquired knowledge is entirely preserved. Even under the case that device prototypes have different capacities, TAKFL smartly transfers the most informative knowledge to each prototype's student model based on its own intrinsic capacity. Still, the final statement indicates that in the case that there are many different teachers, while a small device prototype serving as teacher will not be necessarily compromise information, it would still be preferable to allocate that capacity to a more informative, larger, teacher model._

**Comments.** We comment on the complexity and convergence aspects of TAKFL briefly:

1. [leftmargin=*]
2. **Computation Time**: TAKFL's computation time is \(O(1)\) (constant) due to parallelization, as all distillation processes occur simultaneously.
3. **Computation Load**: The overall computational load scales as \(O(M)\) (linear) since the distillation tasks are performed independently for each prototype in parallel and merged into a singe task arithmetic operation (Eq. 8).
4. **Resource Usage (Memory)**: The resource usage scales as \(O(M^{2})\) (quadratic) because of the need to store and process multiple task vectors concurrently.
5. **Optimization Convergence Rate**: The convergence rate of the Knowledge Distillation procedure is as standard for training a nonconvex loss function, e.g. .

Experiments

### Main Experimental Setup

**Dataset and Architecture.** We evaluate our method on computer vision (CV) and natural language processing (NLP) tasks. For CV, we train image classification using CIFAR10/100 , CINIC-10 , and TinyImagenet . For NLP, we fine-tune pre-trained models for text classification on MNLI , SST-2 , MARC , and AG News . Our architectures include ResNet , VGG , and ViT  for CV, and small BERT variants  (-Tiny, -Mini, -Small) for NLP. We simulate a federated non-i.i.d setting using a Dirichlet distribution \(Dir()\), where a lower \(\) indicates higher heterogeneity . Further details can be found in Appendix F.1 and F.2.

**Implementation Details.** We use the Adam optimizer for both CV and NLP tasks. For CV, local training involves 20 epochs with a learning rate of 0.001, weight decay of 5e-5, and a batch size of 64. NLP training is conducted over 1 epoch with a learning rate of 3e-5, no weight decay, and a batch size of 32. For distillation, Adam is used with a learning rate of 1e-5 and weight decay of 5e-4 for CV, and 3e-5 with no weight decay for NLP. Batch sizes for distillation are 128 for CV and 32 for NLP. The softmax temperature is set at 3 for both tasks, with a temperature of 20 for self-regularization. Further details are provided in Appendix F.1 and F.2.

**Baselines and Evaluation Metric.** We compare our method against standard FL, i.e. FedAvg  and SOTA KD-based methods designed for heterogeneous device prototypes FL, including FedDF  and FedET . The evaluation metric is the final top-1 classification accuracy of each device prototype's global model on the test dataset, as per the methodology described in . We report the average results and the standard deviation over three independent runs, each with a different random seed.

_A more detailed version of the experiments, alongside additional experiments and ablation studies, is presented in Appendix D and E._

### Main Experimental Results

In this section, we evaluate the performance of our method, TAKFL, in a federated learning environment that mirrors real-world scenarios with diverse, heterogeneous device prototypes, as illustrated in Fig. 3(b). Our experimental setup includes three different device prototype sizes: Small (S) with a small model and small dataset, Medium (M) with a medium-sized model and medium-sized dataset, and large (L) with a large model and large dataset.

**Performance on CV Task.** Table 1 presents the performance of TAKFL in the homo-family architecture setting on the CIFAR-10 and CIFAR-100  datasets (for hetero-family architecture results, see Appendix D.1, Table 4). TAKFL consistently enhances performance across all device prototypes in various scenarios, achieving SOTA results. Notably, in the Dir(0.3) setting on CIFAR-10, TAKFL improves average performance across all prototypes by 8%, and by 4% on CIFAR-100. From Table 1, inconsistent performance improvements are observed with prior KD-based methods, especially for the L prototype. While S and M prototypes achieve gains, the L prototype suffers up to a 10% degradation compared to vanilla FedAvg, highlighting the dilution issue where valuable information from larger, more capable device prototypes is diluted by less informative outputs from smaller devices. Moreover, the significant performance improvements TAKFL achieves for each device prototype, particularly for S and M prototypes, illustrate the ineffectiveness of the one-size-fits-all approach used in the existing KD methods. These observations confirm the shortcomings of vanilla ensemble distillation and corroborate our theoretical findings in Remark 1 and 2. The effectiveness of our self-regularization technique is further supported by these experimental results. For more detailed and insightful analysis see Appendix D.1.1.

**Performance on NLP Task.** Table 2 presents the results on MNLI  and SST-2  datasets (see Appendix D.3 for further experiments). Similar to the CV task, TAKFL has consistently improved performance across all device prototypes of varying sizes, achieving SOTA results: a 3% average increase on MNLI and 2% on SST-2. The suboptimality of existing KD methods, is evident from the results presented here as well. Notably, FedET suffers from a significant performance degradation compared to vanilla FedAvg. This issue stems from FedET's reliance on the confidence scores of neural networks for uncertainty estimates. However, neural networks, especially pretrained language models (PLMs), tend to be poorly calibrated and overconfident, undermining reliable uncertainty estimates [50; 15; 5; 55].

### Scalability Evaluation

We evaluate the scalability of TAKFL across a spectrum of device prototypes, from extremely small (XXS) to extremely large (XXL), to see how well our method adapts from a uniform array of small-size prototypes to a diverse mix of sizes. Each prototype is equipped with appropriately scaled model and dataset sizes, simulating real-world variations in device capabilities.

Figure 3 illustrates TAKFL's ability to effectively scale from 3 to 7 device prototypes. In scenarios where all devices are similarly small, i.e. 3-device setup, TAKFL's performance is slightly better than FedDF. This is because when devices are homogeneously small and similar in capability, they

    &  &  \\   & & S & M & L & Average & S & M & L & Average \\   & Fed/Avg & \(36.21_{ 0.24}\) & \(46.41_{ 0.23}\) & \(59.64_{ 0.17}\) & \(47.36\) & \(22.01_{ 0.78}\) & \(26.26_{ 0.39}\) & \(51.51_{ 3.21}\) & \(32.93\) \\  & FedDF & \(49.31_{ 0.15}\) & \(50.63_{ 0.73}\) & \(49.82_{ 0.98}\) & \(49.92\) & \(34.71_{ 1.48}\) & \(35.27_{ 0.74}\) & \(51.08_{ 0.40}\) & \(40.35\) \\  & FedF & \(49.21_{ 0.72}\) & \(55.01_{ 0.81}\) & \(50.64_{ 0.47}\) & \(52.60\) & \(25.84_{ 0.30}\) & \(30.96_{ 0.70}\) & \(45.53_{ 0.66}\) & \(45.36\) \\  & TAKFL & \(55.90_{ 0.17}\) & \(57.93_{ 0.49}\) & \(60.58_{ 23.55}\) & \(58.14_{ 0.76}\) & \(37.41_{ 0.88}\) & \(38.96_{ 0.74}\) & \(51.49_{ 0.15}\) & \(42.62\) \\  & TAKFL+Reg & \(56.37_{ 0.56}\) & \(58.60_{ 0.43}\) & \(65.69_{ 1.28}\) & \(60.22\) & \(40.51_{ 0.16}\) & \(40.12_{ 1.24}\) & \(53.24_{ 2.51}\) & \(44.62\) \\   & Fed/Avg & \(13.22_{ 0.11}\) & \(29.17_{ 0.78}\) & \(21.36\) & \(11.86_{ 0.98}\) & \(14.63_{ 0.86}\) & \(26.25_{ 0.61}\) & \(17.58\) \\  & FedF & \(19.54_{ 0.20}\) & \(24.32_{ 0.45}\) & \(29.21_{ 0.45}\) & \(24.38\) & \(16.69_{ 0.32}\) & \(19.50_{ 0.17}\) & \(20.59_{ 0.25}\) & \(20.83\) \\   & FedF & \(19.67_{ 0.30}\) & \(25.72_{ 0.30}\) & \(10.31_{ 0.13}\) & \(25.35\) & \(11.18_{ 0.16}\) & \(18.22_{ 0.35}\) & \(26.00_{ 0.25}\) & \(18.60\) \\   & TAKFL & \(24.48_{ 0.42}\) & \(27.60_{ 0.25}\) & \(29.84_{ 0.94}\) & \(27.31\) & \(22.90_{ 0.18}\) & \(26.33_{ 0.26}\) & \(26.98_{ 0.13}\) & \(24.50\) \\   & TAKFL+Reg & \(27.18_{ 0.27}\) & \(29.14_{ 0.20}\) & \(31.15_{ 0.09}\) & \(29.15\) & \(22.88_{ 0.37}\) & \(23.92_{ 0.97}\) & \(28.01_{ 0.34}\) & \(24.94\) \\   

Table 1: **Performance Results for CV task on CIFAR-10 and CIFAR-100.** Training data is distributed among S, M, and L device prototypes in a 1:3:6 ratio, subdivided among clients using Dirichlet distribution. Public datasets are CIFAR-100  for CIFAR-10  and ImageNet-100  for CIFAR-100. Client configurations include 100, 20, and 4 clients for S, M, and L, with sampling rates of 0.1, 0.2, and 0.5. Architectures are ResNet-8, ResNet-14, and ResNet-18  for S, M and L, respectively. All models are trained from scratch for 60 rounds. See Appendix D.1 for additional experiments using hetero-family architecture and more details.

    &  &  \\   & S & M & L & Average & S & M & L & Average \\  FedAvg & \(36.15_{ 0.46}\) & \(54.47_{ 2.48}\) & \(57.51_{ 2.79}\) & \(49.37\) & \(54.98_{ 1.81}\) & \(74.71_{ 8.22}\) & \(86.69_{ 0.06}\) & \(72.13\) \\ FedDF & \(54.21_{ 0.15}\) & \(60.44_{ 1.91}\) & \(66.71_{ 1.09}\) & \(60.45\) & \(74.41_{ 2.62}\) & \(80.71_{ 1.63}\) & \(84.35_{ 1.66}\) & \(79.82\) \\ FedF & \(48.03_{ 6.32}\) & \(50.33_{ 7.87}\) & \(53.80_{ 6.18}\) & \(50.72\) & \(66.63_{ 9.14}\) & \(65.89_{ 16.53}\) & \(70.05_{ 15.83}\) & \(67.52\) \\ TAKFL & \(57.43_{ 0.21}\) & \(63.58_{ 0.31}\) & \(68.74_{ 0.12}\) & \(63.25\) & \(74.73_{ 0.55}\) & \(82.17_{ 0.31}\) & \(86.93_{ 0.42}\) & \(81.28\) \\
**TAKFL+Reg** & \(}\) & \(}\) & \(}\) & \(\) & \(}\) & \(}\) & \(}\) & \(\) \\   

Table 2: **Performance Results for NLP Task on MNLI and SST-2.** Training data distribution is similar to the CV task using only Dir(0.5) here. Public datasets are SNLI  for MNLI  and Sentiment140  for SST-2 . Client configurations are 8, 4, and 2 clients for S, M, and L, with sample rates of 0.3, 0.5, and 1.0, respectively. Architectures include Bert-Tiny, Bert-Mini, and Bert-Small  for S, M, and L, initialized from pre-trained parameters and fine-tuned for 20 communication rounds. See Appendix F.2 for more details.

Figure 3: **Scalability Evaluation of TAKFL.** Image classification on CINIC-10  dataset is used to evaluate TAKFL’s scalability across device prototypes ranging from XXS to XXL. Training data is distributed among prototypes in a 1:2:3:4:5:6:7 ratio, further subdivided using Dir(0.5). Client configurations range from 35 for XXS to 5 for XXL. Architectures span from ResNet10-XXS to ResNet50 for XXL prototype, all initialized from scratch and trained over 30 communication rounds. The public dataset is CIFAR-100 . See Appendix D.4 for more details.

do not offer unique contributions that could benefit from more complex distillation strategies. However, as the scenario expands to include larger devices like XL and XXL in the 5- and 7-device configurations, TAKFL significantly outperforms existing KD-based methods. This improvement is driven by the larger devices' ability to offer more significant and higher-quality knowledge, which TAKFL effectively distills across all prototypes, contrasting sharply with existing methods that fail to utilize this potential. These experimental observations, corroborated by our theoretical insights in Remark 2, demonstrate TAKFL's superior scalability and effectiveness.

## 8 Conclusion and Discussion

In this work, we addressed a fundamental issue in standard federated learning: the lack of support for heterogeneous device prototypes. Existing KD-based methods often fall short in real-world scenarios, where device capabilities vary widely. To address this, we introduced TAKFL, a novel KD-based method that treats knowledge transfer from each prototype's ensembles as separate tasks and distills them independently. TAKFL subsequently integrates the knowledge using an adaptive task arithmetic technique for optimized performance. We also introduced a KD-based self-regulation technique to mitigate issues arising from noisy and unsupervised ensemble distillation. The effectiveness of our method is substantiated by both theoretical results and extensive experimentation across CV and NLP tasks, using various datasets and models.

Limitations remain, notably in real-world applicability. While TAKFL's effectiveness in an approximated real-world setup has been demonstrated, actual deployment on physical devices and in environments with extremely large models remains untested due to resource constraints. Experiencing TAKFL in genuine real-world settings could unveil additional challenges or limitations, providing further insights into its scalability and efficiency.

## 9 Acknowledgment

This work was partially supported by a research grant from Cisco Systems, Inc., Project Number 49790. V.K. acknowledges support from the Czech National Science Foundation under Project 24-11664S. We also gratefully acknowledge the use of the computational infrastructure provided by the OP VVV funded project CZ.02.1.01/0.0/0.0/16_019/0000765, "Research Center for Informatics," which enabled us to conduct the experiments presented in this work.

We would like to express our sincere gratitude to Ang Li, Matias Mendieta, and Guangyu Sun for their invaluable feedback and insightful discussions, which significantly contributed to the development and refinement of this work. Their thoughtful suggestions and careful review of earlier drafts were instrumental in enhancing the quality of this paper.