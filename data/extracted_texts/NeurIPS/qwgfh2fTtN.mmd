# Easy-to-Hard Generalization:

Scalable Alignment Beyond Human Supervision

 Zhiqing Sun\({}^{1}\), Longhui Yu\({}^{2}\), Yikang Shen\({}^{3}\), Weiyang Liu\({}^{4,5}\),

**Yiming Yang\({}^{1}\)\({}^{}\), Sean Welleck\({}^{1}\)\({}^{}\), Chuang Gan\({}^{3,6}\)\({}^{}\)**

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)Peking University, \({}^{3}\)MIT-IBM Watson AI Lab

\({}^{4}\)University of Cambridge, \({}^{5}\)Max Planck Institute for Intelligent Systems, \({}^{6}\)UMass Amherst

Code: Edward-Sun/easy-to-hard

###### Abstract

Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (_e.g._, level 4-5 MATH problems) via learning from human annotations on easier tasks (_e.g._, level 1-3 MATH problems), which we term as _easy-to-hard generalization_. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the (process-supervised) reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such _easy-to-hard generalization from evaluators_ can enable _easy-to-hard generalizations in generators_ either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model and 34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500, respectively, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.

## 1 Introduction

Rapid advancements in large language models (LLMs) indicate that in the near future, highly sophisticated AI systems could surpass human capabilities in certain areas, significantly enhancing our capabilities in solving harder problems beyond the levels we can currently solve . Since the current AI alignment methods mostly rely on either supervised fine-tuning (SFT) with human-provided demonstrations  or reinforcement learning from human feedback (RLHF) , their capabilities would be inherently limited as humans cannot always provide helpful demonstrations or supervision on the hard tasks beyond their expertise .

In order to build future AI systems for tackling complex challenges, such as advancing scientific knowledge, it is crucial to develop new approaches for _scalable oversight_ challenge, i.e., to supervise the AI systems that can potentially outperform humans in most skills . The key question is:

* _Can we limit human supervision to easier tasks, yet enable the model to excel in harder tasks?_We refer to this scenario as _Easy-to-Hard Generalization_. This setting requires no human supervision on the harder tasks, which differs from existing work that either enhances humans' ability to verify the outputs of AI systems  or enables weak-to-strong generalization via a teacher that only offers unreliable or noisy supervision .

The most basic form of easy-to-hard generalization can be achieved by training the policy models (i.e., generator) using supervised fine-tuning (SFT) or in-context learning (ICL) on easy tasks , and expect this will unlock the ability to perform well on hard tasks. However, it has been observed that SFT or ICL training of generators on easy tasks often fails to generalize to hard tasks . We hypothesize and show that methods beyond these can enable stronger degrees of easy-to-hard generalization. Our intuition is guided by the observation that _evaluation is easier than generation_, so an evaluator may offer a degree of easy-to-hard generalization that is useful for improving a generator. If that is true, we can first train a verifier on easy tasks, then make use of its generalization ability to supervise the generator on hard tasks.

Complex tasks can often be broken down into smaller steps  and verified by validating the individual steps - a strategy that is commonly employed in solving mathematical problems . Inspired by this, we train outcome-supervised and process-supervised reward models  as our easy-to-hard evaluators. The training dataset is often comprised of a set of labeled easy tasks, each with a question and a high-quality solution1, paired with a set of unlabeled hard tasks that are represented only by their questions. This simulates the practical setting of having numerous problems with known solutions, as well as significant unresolved challenges, such as the Millennium Prize Problems , which present challenging open problems. The pivotal aspect of easy-to-hard generalization thus lies in how we effectively leverage the capabilities of easier-level models in solving harder problems.

Our investigation includes to training policy and reward models on the easy (i.e., level 1-3) portion of the PRM800K  dataset, and comparing the performance of majority vot

Figure 1: Illustration of different alignment scenarios: **traditional alignment** relies on human demonstrations or judgements ; **scalable alignment** assumes that humans cannot reliably supervise smarter-than-human models; **weak-to-strong generalization** focuses on using weak models with unreliable labels to supervise strong models; Our proposed **easier-to-general generalization** focuses on the transfer of rewarding policies from weak models to harder tasks.

only and weighted majority voting with the policy model and PRMs (Process-supervised Reward Models). We also introduce the _Outcome & Process Reward Model (OPRM)_, which harnesses the complementary strengths of outcome reward models (ORMs) and process reward models (PRMs): judging if each step in reasoning is correct (like PRMs do) and deciding if the final answer is right (like ORMs do). Our findings reveal a marked performance improvement with the inclusion of reward models, especially on the hard (i.e., level 4-5) portion of the MATH500 test set. This improvement indicates that easier-level evaluators can maintain their effectiveness on harder tasks. We have similar observations in our experiments on the MetaMath dataset  and the Math-Shepherd dataset .

We further investigate the use of the easy-to-hard evaluator as a reward model in reinforcement learning, where the evaluator provides targeted, step-by-step guidance in solving hard problems. We have an intriguing finding that _training with human supervision only on the easy tasks (i.e., training with Level 1-3 problems and answers) can outperform both SFT and Final-Answer RL training on the full dataset (Level 1-5)_. This finding underscores the potential of using easy-to-hard evaluation to improve easy-to-hard generators, particularly when dealing with varied levels of task complexity.

## 2 Related Work

### Scalable Oversight

While present-day models operate within the scope of human assessment, future, more advanced models may engage in tasks that are beyond human evaluation capabilities. This raises a concern that such models might prioritize objectives other than maintaining accuracy (Andreas 3, Perez et al. 53, Sharma et al. 64, Wei et al. 80). To address this, a branch of research develops techniques to enhance the human capacity to supervise such models, such as via using AI to evaluate the work of other AIs . Our setting differs from enhancing human oversight; instead, we focus on enabling models to excel in hard tasks where human supervision may not be available. This also differs from weak-to-strong generalization , where human supervision may be available, but not reliable, on hard tasks. However, our framework aligns with the "sandwiching" concept proposed for measuring progress in scalable oversight, which involves domain experts evaluating the outputs of AI-assisted non-experts .

Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.

### Compositional Generalization

Compositional generalization is a fundamental aspect of how language works . It refers to the ability to understand and utilize novel combinations based on the understanding of basic concepts and a limited number of their combinations . Recently, least-to-most prompting [95; 20] teaches language models how to solve a complex problem by reducing it to a series of easier sub-problems, achieving easy-to-hard generalization on semantic parsing tasks like SCAN  and CFQ  with perfect generalization accuracy. In addition, least-to-most prompting has also been successful in mathematical reasoning tasks, specifically in datasets like GSM8K  and DROP , by teaching language models to solve problems more difficult than those seen in the prompts. This success not only underscores the capacity of language models to effectively break down complex tasks into simpler sub-tasks Perez et al. , but also demonstrates their generalization capability in solving these sub-problems.

### Easy-to-Hard Generalization

Past work has evaluated easy-to-hard generalization by training easy-to-hard generators on easy tasks using supervised finetune-tuning (SFT) or in-context learning (ICL) [55; 10]. Nevertheless, Swayamdipta et al.  showed that the BERT model performs poorly on common-sense reasoning when only trained on easy data. Fu et al.  showed similar results for ICL on reasoning tasks like GSM8K . In concurrent work, Hase et al.  evaluate the performance of easy-to-hard generators on more datasets and models, and find that ICL or SFT on easy tasks is a strong baseline for multiple-choice tasks like ARC  and MMLU . In contrast, we evaluate the easy-to-hard generation performance on the more challenging MATH dataset , and show that easy-to-hard evaluation can improve a generator's easy-to-hard generalization beyond ICL and SFT. Iterative machine teaching  gives theoretical justification to show that training classifiers from easy to hard examples yield better generalization.

## 3 Methodology

We study the easy-to-hard generalization problem: how can we enable capabilities beyond human supervision? Specifically, we explore the efficacy and scalability of various easy-to-hard methodologies on competition-level mathematical problem-solving problems (MATH; Hendrycks et al. 32). This dataset is suitable for our study since it explicitly categorizes problems across five difficulty levels. We consider levels 1-3 as "easy" tasks, encompassing both the problems and their respective solution demonstrations, along with the correct answers. Conversely, levels 4-5, characterized by their more complex nature, are treated as "hard" tasks and are represented solely by their questions. The MATH

Figure 3: The overview diagram of our methods: the different components of modeling and training and how they are interconnected.

dataset's difficulty distribution roughly follows a \(1:2:2:3:3\) ratio across levels 1 to 5. So our division maintains a balanced number of easy and hard tasks.

The remainder of the paper aims to answer following research questions:

**RQ1:** How do generators generalize from easy to hard?

**RQ2:** How do evaluators generalize from easy to hard?

**RQ3:** If evaluators generalize better than generators, how can we take advantage of this to enable stronger easy-to-hard generalization in generators?

### Setup

**Dataset** MATH  is a dataset of 12,500 challenging competition mathematics problems, where 7,500 of them are training problems and 5,000 are originally used for testing. Following Lightman et al. , Wang et al. , we use the identical subset of 500 representative problems (i.e., MATH500) as our test set, uniformly sample another 500 problems for validation, across all five difficulty levels, and leave the rest 4,000 MATH test split problems combined with the original 7,500 MATH training split problems as our training set.

**Simulated Human Demonstrations** While the original MATH dataset provides full step-by-step solutions, these solutions typically skip many chain-of-thought steps , which can be hard for language models to directly imitate2. Instead, we consider filtered PRM800K  and MetaMATH  as our SFT training data: the former is generated by a Minerva-style base GPT-4 model using few-shot prompting after filtering the correct answers [39; 48], while the latter is generated by ChatGPT . We keep all the GSM8K data in the MetaMATH dataset since they are typically easier than the problems in MATH. PRM800K comes with human annotated process labels, while for MetaMath, we use Math-Shepherd as the corresponding process labels .

### Generators

For a given dataset (e.g., a variant of MATH), we consider the following generator models:

**Full & Hard ICL** Full in-context learning (ICL) is a base model prompted with exemplars sampled from all difficulty levels, or only from the level 5 .

**Easy-to-Hard ICL** This model is prompted with exemplars from easy problems. This baseline evaluates the degree to which a model can solve problems more difficult than those seen in the prompts .

**Full SFT** As prior work suggests that finetuning should outperform prompting alone [68; 52; 50], the full supervised fine-tuning (SFT) model is typically considered as a ceiling that a model can achieve on a type of task [11; 29].

**Easy-to-Hard SFT** This generator model is trained only on the easy tasks. Prior work suggests that it can generalize to hard tasks but with some degeneration in performance .

The generator models are evaluated in greedy decoding and self-consistency (also known as majority voting) settings .

### Evaluators

Similarly, we consider the following evaluator models that can be trained either on the easy tasks only, or on the full dataset. Notably, unlike final-answer rewards, reward models trained on easy tasks can be applied to evaluate solutions to hard problems.

**Final-Answer Reward** is a symbolic reward that provides a binary reward based on the accuracy of the model's final answer. The matching is performed after normalization3.

**Outcome Reward Model (ORM)** is trained on the Final-Answer rewards. Following Cobbe et al. , Uesato et al. , Lightman et al. , we train the reward head to predict on every token whether the solution is correct, in a similar sense to a value model . At inference time, we use the ORM's prediction at the final token as the reward of the solution.

**Process Reward Model (PRM)** is trained to predict whether each step (delimited by newlines) in the chain-of-thought reasoning path is correct. The labels are usually labeled by humans [74; 40] or estimated with rollouts [65; 75].

**Outcome & Process Reward Model (OPRM)** Building on the distinct advantages of ORMs and PRMs, we introduce the _Outcome & Process Reward Model (OPRM)_, which harnesses the complementary strengths of both. OPRM is trained on the mixed data of ORMs and PRMs. Specifically, it evaluates the correctness of each intermediate reasoning step, akin to PRMs, while also assesses the overall solution's accuracy at the final answer stage, mirroring the functionality of ORMs.

### Optimizing Generators Against Evaluators

Finally, given a generator model (i.e., policy model) and a evaluator model (i.e., reward model; RM), we optimize the generator against the evaluator using either re-ranking or reinforcement learning.

**Best-of-\(n\) (BoN)**, also known as rejection sampling, is a reranking approach that sample multiple solutions from the generator and selects one with the highest RM score.

**Weighted Voting** is similar to majority voting or self-consistency , but weights each solution according to its RM score .

**Reinforcement Learning (RL)** We consider three online/offline RL variants, Reinforced Self-Training (ReST) [28; 67], Direct Policy Optimization (DPO) , and Proximal Policy Optimization (PPO) . Due to the space limit, please find their detailed description in Appendix B.

### Evaluation Metrics

In this study, we have chosen not to establish terms analogous to the weak-to-strong performance gap recovery (PGR) as discussed in Burns et al.  or the easy-to-hard supervision gap recovery (SGR) highlighted by Hase et al. . This decision is based on our observations that sometimes, models trained exclusively on simpler tasks--particularly when employing RL training--can outperform those trained across the entire spectrum of problem difficulties. Therefore, we mainly focus on the absolute and relative performance of generators (optionally optimized by the evaluator) on the MATH500 test set .

### Implementation Details

**Base Language Model** Llemma is a large language model for mathematics , which is continue pre-trained from Code Llama  / LlaMA-2 . We use both 7b and 34b variants in our experiments.

    & & &  &  \\  & Greedy & Maj@16 & Maj@256 & Greedy & Maj@16 & Maj@256 \\    } & Full ICL & 12.8 & 15.6 & 20.8 & 16.4 & 18.4 & 25.6 \\  & Hard ICL & 12.6 & 18.0 & 27.0 & 16.6 & 19.0 & 27.0 \\  & Easy-to-Hard ICL & 14.0 & 17.6 & 24.4 & 14.2 & 17.4 & 26.8 \\  & Full SFT & 20.6 & 32.0 & 36.2 & 31.4 & 40.2 & 41.6 \\  & Easy-to-Hard SFT & 19.8 & 31.6 & 36.0 & 30.0 & 38.6 & 42.4 \\    } & Full ICL & 18.6 & 23.6 & 36.0 & 20.6 & 28.8 & 39.2 \\  & Hard ICL & 15.8 & 21.4 & 34.2 & 21.8 & 26.4 & 38.6 \\   & Easy-to-Hard ICL & 18.2 & 25.2 & 36.8 & 19.8 & 26.8 & 37.2 \\   & Full SFT & 25.6 & 41.8 & 46.4 & 35.4 & 44.2 & 45.6 \\   & Easy-to-Hard SFT & 24.8 & 40.8 & 46.0 & 32.2 & 42.6 & 43.4 \\   

Table 1: Easy-to-hard generalization of generators. We compare generator performance under various decoding settings. PRM800K and MetaMath indicate the SFT training data and ICL exemplars. Evaluations are performed on the same MATH500 test set.

**SFT / RL / Reward Model** We fine-tune all models in full fine-tuning with frozen input-output embedding layers and normalization layers. RMs are initialized from the base model, and have an added scalar head to output the reward. In PPO training, we initialize the value model from the reward model.

**Hyper-parameters** Due to the space limit, our training hyper-parameters can be found in Appendix. C.

## 4 Main Results

### Easy-to-Hard Generalization of Generators

In Table 1, we compare the easy-to-hard generalization performance of the generators under various decoding settings:

Supervised Fine-Tuning (SFT) outperforms In-Context Learning (ICL):This is consistent with prior work . We also find that the performance of ICL has larger variance than SFT with respect to data ordering (or random seeds) .

SFT data quality impacts easy-to-hard generalization:PRM800K data is generated by a base (unaligned) GPT-4 model through few-shot prompting and is thus of lower quality than well-aligned ChatGPT-generated MetaMATH data. We find that only MetaMath-trained models have certain easy-to-hard gaps (e.g., 16.6 v.s. 14.2 in MetaMath-7b-ICL), while such gaps in PRM800K-trained models are very small (less than 1%), or even inverted in the ICL setting. We hypothesize that low-quality SFT data may only teach the model the format of the task , while high-quality (imitation) SFT data can teach the model the principles of solving the task . Nevertheless, the strongest performance is achieved by full SFT on the high-quality MetaMath data (35.4), showing an unignorable difference, with a gap of up to 3.2, compared to its easy-to-hard SFT counterpart (32.2).

### Easy-to-Hard Generalization of Evaluators

The primary metric we use to assess the effectiveness of our process reward model is not the average accuracy of verifying each step in a solution but rather the overall performance achieved through re-ranking methods (See discussion in Sec. 3.5). We first use re-ranking to evaluate the easy-to-hard generalization performance of evaluators.

Figure 4: Easy-to-hard generalization of 7(upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.

[MISSING_PAGE_FAIL:8]

observation leads to the conclusion that _evaluators demonstrate better easy-to-hard generalization capabilities in comparison to generators_. This motivates us to explore RL approaches that optimize the generator against the evaluator to further improve the performance of easy-to-hard generation.

#### 4.2.2 Reinforcement Learning (RL)

Given the conclusion above, an important question arises: how can evaluators once again assist generators in achieving enhanced easy-to-hard generalization capabilities? We further investigate the enhancement of policy models through RL, utilizing easy-to-hard evaluators as reward models. Similar to re-ranking, SFT and PRM are only trained on easy data. For a fair comparison between PRM800K and MetaMath, we only use vanilla PRMs in the RL training. All the RL methods use the validation accuracy for selecting the best checkpoint5. Our comparison spans offline (ReST & DPO) and online (PPO) RL algorithms under two training conditions:

**Easy Questions & Easy Final Answers.** The SFT model samples from easy questions and receives the corresponding Final-Answer and optional PRM rewards.

**All Questions & Easy Final Answers.** This assumes access to a range of easy and hard problems for RL training, with rewards for hard tasks solely provided by the easy-to-hard evaluator.

Based on the results reported in Table 2, we have the following findings:

DPO and PPO excel over ReST.Among the RL algorithms trained on the PRM800K dataset, PPO emerges as the most effective, significantly surpassing both ReST and DPO. On the MetaMATH dataset, PPO and DPO achieve top performance, while ReST shows only marginal improvements over the SFT baseline. The comparative analysis between DPO and PPO across the PRM800K and MetaMATH datasets indicates that while DPO's efficacy is on par with PPO given a high-quality SFT model as initialization, PPO's effectiveness is less contingent on the quality of the underlying SFT model .

PRM rewards are more beneficial than Final-Answer rewards for hard tasks.Notably, models trained with PRM rewards with human supervision on the easy tasks (achieving a top performance of 34.0) outperform the previous state-of-the-art model trained across all task levels (33.0). This highlights the effectiveness of leveraging easy-to-hard evaluations to improve generator performance across varying task difficulties.

    &  &  \\  & RL Data & Final-Answer & Process RM & Easy (level 1-3) & Hard (level 4-5) & All \\   &  &  &  \\  & SFT & & & & 28.2 & 12.2 & 19.8 \\  & Easy & Easy & \(\) & 33.2 & 12.6 & 22.4 \\  & Easy & Easy & \(\) & 42.0 & 12.2 & 26.4 \\  & PPO & Easy & Easy & \(\) & 42.0 & 14.1 & 27.4 \\  & All & Easy & \(\) & **45.4** & **14.9** & **29.4** \\   \\  &  &  &  &  \\  & Previous RL SoTA  & & & & - & 33.0 \\   \\  &  &  &  &  \\  & SFT & & & & \\  & Easy & & 50.4 & 14.5 & 31.6 \\  & Iterative DPO & Easy & Easy & \(\) & **53.8** & **16.0** & **34.0** \\  & Iterative DPO & All & Easy & \(\) & 49.6 & 10.7 & 29.2 \\  & PPO & Easy & Easy & \(\) & 50.8 & 15.3 & 32.2 \\  & All & Easy & \(\) & **53.8** & **16.0** & **34.0** \\   

Table 2: Comparing reinforcement learning (RL) approaches for easy-to-hard generalization. All methods are of 7b size and evaluated with greedy decoding.

### Easy-to-Hard Generalization on the Coding Domain

We conduct further experiments in the coding domain with the APPS dataset . Similarly to Lightman et al. , we sub-sampled 500 questions from the original test set of APPS as our test set. Specifically, we sub-sampled 100 Introductory questions, 300 Interview questions, and 100 Competition questions, following the original distribution in the test set.

In Table 3, we compare the performance of SFT-trained Code Llama  (7b & 34b) with greedy decoding and best-of-N approach. In the latter, an Outcome Reward Model (ORM) of the same model size is trained to select the best coding one from N sampled solutions.

We found that while the reward model is only trained on the outcome supervision of easy (Introductory) data, it significantly improves the model performance on hard (Interview & Competition) data. These findings extend the premise of easy-to-hard generalization beyond the confines of mathematical reasoning, suggesting its applicability across diverse domains.

## 5 Conclusion

Our study advances the field of AI alignment by demonstrating the potential of easy-to-hard generalization, where models trained on simpler tasks can be guided to solve more complex problems without direct human supervision on these harder tasks. Through the use of (process-supervised) reward models for evaluating and enhancing policy models, we show that evaluators can facilitate this form of generalization, outperforming traditional training methods. Our findings highlight the effectiveness of re-ranking strategies and reinforcement learning (RL) in leveraging evaluators for performance gains on difficult tasks. This approach presents a promising direction for developing AI systems capable of surpassing human problem-solving capabilities, suggesting a scalable alignment method that could enable AI to independently advance knowledge in complex domains.

While our study provides valuable insights into easy-to-hard generalization and the potential of process-supervised reward models, there are limitations to consider. These include the focus on specific model sizes and datasets, the domain specificity of reasoning tasks, and the need for further research on the long-term implications and robustness of the method.

## 6 Acknowledgement

This work is supported by OpenAI Superalignment Fast Grants and Microsoft Accelerate Foundation Models Research (AFMR) Initiative. Additionally, ZS thanks Google PhD Fellowship; SW thanks NSF SCALE (NSF DMS 2134012) and Convergent Research.