# BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection

Saket Sanjeev Chaturvedi\({}^{1}\), Lan Zhang\({}^{1}\), Wenbin Zhang\({}^{2}\), Pan He\({}^{3}\), Xiaoyong Yuan\({}^{1}\)

\({}^{1}\)Michigan Technological University

\({}^{2}\)Florida International University \({}^{3}\)Auburn University

###### Abstract

3D object detection plays an important role in autonomous driving; however, its vulnerability to backdoor attacks has become evident. By injecting "triggers" to poison the training dataset, backdoor attacks manipulate the detector's prediction for inputs containing these triggers. Existing backdoor attacks against 3D object detection primarily poison 3D LiDAR signals, where large-sized 3D triggers are injected to ensure their visibility within the sparse 3D space, rendering them easy to detect and impractical in real-world scenarios. In this paper, we delve into the robustness of 3D object detection, exploring a new backdoor attack surface through 2D cameras. Given the prevalent adoption of camera and LiDAR signal fusion for high-fidelity 3D perception, we investigate the latent potential of camera signals to disrupt the process. Although the dense nature of camera signals enables the use of nearly imperceptible small-sized triggers to mislead 2D object detection, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. The primary challenge emerges from the fusion process that transforms camera signals into a 3D space, thereby compromising the association with the 2D trigger to the target output. To tackle this issue, we propose an innovative 2D-oriented backdoor attack against LiDAR-camera fusion methods for 3D object detection, named BadFusion, aiming to uphold trigger effectiveness throughout the entire fusion process. Extensive experiments validate the effectiveness of BadFusion, achieving a significantly higher attack success rate compared to existing 2D-oriented attacks.

## 1 Introduction

3D object detection has become a core component for many state-of-the-art autonomous driving systems Qian et al. (2022). By accurately recognizing and localizing objects like vehicles, pedestrians, and cyclists, 3D object detection enhances the ability of driving systems to perceive and understand surroundings, enabling them to make responsible decisions. Despite the significant progress achieved by deep neural networks in 3D object detection, it has been demonstrated that neural network-based object detectors are susceptible to backdoor attacks Xiang et al. (2021), Li et al. (2021), Zhang et al. (2022). Backdoor attackers contaminate the detector's training dataset by injecting "triggers," which consequently mislead predictions during inference. The prevalence of backdoor attacks poses significant safety hazards, particularly in safety-critical driving scenarios.

Existing backdoor attacks against 3D object detection mainly inject triggers to LiDAR signals because the spatial information provided by LiDAR offers critical 3D detection evidence. However, due to the sparsity of LiDAR signals in most commercialized LiDAR sensors, backdoor attacks require adding large-size triggers to the target vehicle to ensure that the trigger information can be effectively captured. For example, Zhang et al. (2022) used a cargo carrier bag with a size of \(1.1\)m \( 0.8\)m \( 0.5\)m or an exercise ball with a radius of \(0.4\)m as a trigger, which is mounted on the roof of the target vehicle for backdoor attacks. Such large 3D triggers can significantly change the vehicle's shape and appearance and thus be easily detected, making 3D backdoor attacks impractical to implement in real-world scenarios. Therefore, to thoroughly investigate the robustness of 3D object detection, in this paper, we explore a new potential attack surface through 2D camera signals.

In addition to LiDAR signals, camera signals have been another prominent source of input for 3D object detection. Compared to 3D spatial yet low-resolution signals from LiDAR, cameras capture high-resolution color features, yielding robust fusion outcomes that significantly enhance the quality of 3D perception Wang et al. (2021); Yin et al. (2021); Li et al. (2022). However, the popularity of these multi-modal systems leads to a new backdoor attack surface against 3D object detection through cameras. Due to the dense nature of camera signals, attackers can add 2D triggers with a small size into camera signals, making the attack nearly imperceptible and easy to deploy in practice. Such 2D-oriented backdoor attacks have shown their effectiveness in many 2D object detection tasks Chan et al. (2022); Luo et al. (2023). Nevertheless, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. As illustrated in Figure 1, state-of-the-art LiDAR and camera fusion systems first transform camera signals to align with 2D LiDAR projection, which are then fused with 3D LiDAR features to make detection decisions in a 3D space. Although the transformation of camera signals bridges the gap between 2D and 3D feature spaces, it compromises the association with the injected 2D triggers to the target output. Due to the sparsity of LiDAR points, the resulting transformed camera features are also sparse, causing a limited number of trigger pixels to be observed effectively, thereby substantially diminishing the impact of 2D triggers in 3D object detection. Moreover, due to the dynamicity of LiDAR signals, the applicable trigger pixels after 2D to 3D transformation may not remain consistent across different training samples, further weakening the association between the 2D trigger and target labels. In view of these, it is critical to delve into the potential threats posed by 2D camera-oriented backdoor attacks in influencing 3D object detection.

In this paper, we identify a novel 2D-oriented backdoor attack against the multi-modal 3D object detection system, named BadFusion. BadFusion aims to insert backdoors into the camera and LiDAR fusion-based 3D object detector by only compromising camera inputs with 2D triggers. To obtain effective 2D triggers against the modality transformation in the fusion system, BadFusion develops fusion-aware 2D triggers, which preserve the density of individual triggers while maintaining the trigger pattern consistency across different camera signals. Moreover, considering the inaccessibility of LiDAR signals that are synchronized with camera signals during inference, _i.e._ when deploying the designed triggers to fool the backdoored detector, BadFusion develops LiDAR-free attack approaches, which predicts the 2D LiDAR projection based on camera signals. To the best of our knowledge, this is the first effort in examining 2D-oriented backdoor attacks against fusion-based 3D object detection. We intend to raise community awareness of new backdoor threats in emerging multi-modal fusion systems. Our contributions to this paper are summarized below:

1. We investigate the existing 2D-oriented backdoor attacks against LiDAR and camera fusion systems for 3D object detection. Our research indicates that the fusion system offers effective protection, weakening existing attacks.
2. We propose a new 2D-oriented backdoor attack, named BadFusion, which effectively preserves the 2D backdoor patterns throughout the fusion process and eventually manipulates the 3D predictions.
3. We consider the unavailability of synchronous LiDAR signals when compromising the camera inputs, where a LiDAR-free attack approach is developed to generate LiDAR projection based on camera observations.
4. We extensively evaluate BadFusion against state-of-the-art LiDAR-camera fusion methods with two goals: resizing the bounding boxes and disappearing the objects. BadFusion successfully achieves the two attack goals and outperforms existing 2D-oriented backdoor attacks with a much higher Attack Success Rate (ASR).

Figure 1: The pipeline of 2D (camera) and 3D (LiDAR) data fusion for 3D object detection in autonomous driving.

## 2 2D-Oriented Backdoor Attacks

This paper explores the potential of 2D-oriented backdoor attacks in influencing the fusion-based multi-modal 3D perception. This section first introduces the existing backdoor attacks for 2D object detection and then presents the fusion-based 3D object detection systems that involve both 2D and 3D inputs. Our threat model is finally elaborated.

### Backdoor Attacks for 2D Object Detection

The mainstream backdoor attack research for object detection centers on 2D perception. An attacker's goal is to use predefined 2D triggers to mislead the target model's predictions. During training, the attacker first poisons \(n\) samples of the training dataset \(_{train}=\{_{i},_{i}\}_{i=1}^{N}\), where \(N\) is the number of all training samples, \(n N\). Specifically, for a clean sample \((,)\), the poisoned input \(^{}\) can be given by

\[^{}=+(1-), \]

where \(\) is the injected trigger; \(\) is a binary mask, using \(1\) to represent the location of the trigger and \(0\) everywhere else; \(\) denotes the element-wise product. Meanwhile, target label \(^{}\) (different from the original label \(\)) is associated with the poisoned input \(^{}\). The poisoned samples consist of the backdoor dataset \(_{back}\), which is mixed with rest of clean data \(_{clean}\) to train the target model \(f\). This produces a backdoored model, which misclassifies any poisoned input to the target label while not affecting the prediction of clean samples. The backdoor attack objective is formulated as

\[_{(^{},^{})_{back}}-14.226378pt (f(^{}),^{})-14.226378pt+ -14.226378pt_{(,)_{clean}}-14.226378pt (f(),), \]

where the first and second terms calculate the loss for poisoned and clean samples, respectively. The above problem considers a single modality object detection, which modifies the 2D inputs to mislead 2D predictions, _e.g._, 2D bounding boxes Chan et al. (2022); Luo et al. (2023). Instead, this paper targets a multi-modal object detection system with both 2D and 3D inputs for 3D perception, _e.g._, 3D bounding boxes.

### Fusion Pipeline for 3D Object Detection

There are two main categories of research on fusing 2D and 3D inputs for 3D perception. The first projects 3D inputs to a 2D space, which unfortunately often results in severe geometric distortion Chen et al. (2017); Yang et al. (2018) and thus becomes ineffective for geometric tasks, such as 3D object detection. Therefore, this paper focuses on the second category, which maps 2D inputs to a 3D space for using camera inputs to augment 3D signals. Such fusion has been a promising solution for 3D object detection by preserving critical geometric information Sindagi et al. (2019); Wang et al. (2021); Yin et al. (2021); Li et al. (2022). As illustrated in Figure 1, one key component of this fusion is the transformation module to map the 2D signal into 3D measurements, which mainly includes three steps. First, the 3D LiDAR signals are projected to a 2D space, such as in the field-of-view (FoV), to derive 2D-LiDAR projection. Then, the 2D camera signals are processed by a camera feature extractor, _e.g._, 2D CNN, to extract high-level features with semantic information. Finally, the extracted camera features are aligned with 2D LiDAR projection to obtain the camera-based information for each LiDAR point. The transformed camera features will be combined with the LiDAR signals to perform 3D objection detection.

### Our Threat Model

This paper focuses on a fusion-based 3D object detection system with both 2D camera and 3D LiDAR inputs. We consider a practical but challenging attack setup: _the objective of the attacker is to launch

Figure 2: Comparison between existing 2D-oriented backdoor attacks and the proposed BadFusion.

backdoor attacks for fusion-based 3D object detection by only compromising the camera inputs with 2D triggers_. This attack is more feasible and imperceptible in practice than creating 3D triggers that significantly change the shape and appearance of vehicles. Besides, we consider standard backdoor attack settings: 1) the attacker injects only a small number of poisoned samples into the training dataset; 2) the attacker has no control of the model training process; 3) the attacker has no knowledge about the target model's parameters or architecture.

## 3 The Proposed BadFusion

In order to achieve the aforementioned attack objective, this paper proposes BadFusion, an innovative 2D-oriented backdoor attack against fusion-based 3D object detection. Similar to the attack procedure described in Section 2.1, BadFusion first creates a poisoned dataset. Define the two modality data, 2D camera and 3D LiDAR signals, by \(_{c}\) and \(_{l}\), respectively. The poisoned 2D camera data \(^{}_{c}\) is created by injecting the 2D trigger \(\) to \(_{c}\) based on (1). Meanwhile, the target label \(y^{}\) is associated with the poisoned camera input \(^{}_{c}\). After that, the poisoned camera inputs \(^{}_{c}\), remaining clean camera inputs \(_{c}\), and LiDAR inputs \(_{l}\) are jointly used to train the backdoored fusion model \(f\). This optimization problem is formulated as

\[_{(_{l},^{}_{c},^{})_{back}} (f(_{l},^{}_{c}),^{})+ _{(_{l},_{c},)_{clean}}(f (_{l},_{c}),). \]

### Design Challenges

Although existing backdoor attacks against single-modality systems, _i.e_., camera-only inputs, can successfully mislead 2D object detection, BadFusion cannot directly follow their attack procedure. As discussed in Section 2.2, the target fusion model \(f\) in (3) needs to transform the poisoned camera signal \(^{}_{c}\) from 2D to 3D measurement for data fusion purposes. Unfortunately, this transformation breaks the association with the injected 2D trigger to the target output. Specifically, we identify the following two key challenges: (1) _Trigger sparsity_. Due to the sparsity of 3D LiDAR points, only a few camera pixels are transformed into LiDAR features and subsequently used for object detection. Thus, most pixels of the 2D triggers are ignored in the fusion-based object detection system, making it hard to mislead the prediction of the target model. (2) _Trigger inconsistency_. Due to the dynamicity of LiDAR data, the same LiDAR point may correspond to different pixels of the transformed camera signal. Thus, the effective trigger pixels become inconsistent among inputs after transformation. Consequently, the effective trigger pixels during inference are inconsistent with those during training, weakening the association between trigger patterns and target labels. Figure 2 illustrates the challenges of existing attacks. The triggers injected by existing attacks are both sparse and inconsistent due to the transformation, thus ineffective in misleading the fusion model.

### Fusion-Aware 2D Trigger Design

To address these challenges, BadFusion employs the fusion-aware 2D triggers tailored for multi-modal fusion systems. These triggers aim to preserve dense and consistent patterns against the transformation module of the fusion pipeline. To enhance _trigger density_, BadFusion intends to maximize the effective pixels in 2D triggers after transformation. Recall that the 2D camera trigger aligns with the 2D LiDAR projection to extract applicable camera features for multi-modal fusion, as shown in Figure 1. Hence, we propose to identify the dense region of the 2D LiDAR projection for trigger placement, where only contiguous dense regions are considered to make 2D triggers easy to implement in reality. Besides, we identify another challenge from the availability of LiDAR signals. Although the LiDAR signals of training samples are accessible to the attacker, LiDAR signals in the inference phase are usually unavailable. Therefore, we introduce a LiDAR-free method for BadFusion by predicting the dense regions of the 2D LiDAR projection, which is detailed in Section 3.3.

Additionally, to enhance _trigger consistency_, BadFusion intends to maximize the consistent trigger patterns among different inputs. Conventional 2D backdoor attacks optimize triggers with various colors of pixels towards different goals, such as high attack success rate, clean data accuracy, and high stealthiness Liu et al. (2018); Zhao et al. (2020); Zhong et al. (2020); Garg et al. (2020). The impact of these colorful pixels will be diminished in the fusion system after the 2D to 3D transformation, as the effective pixels of an optimized trigger after the transformation vary among different inputs. Thus, instead of generating complex and imperceptible triggers, we introduce a simple yet effective approach to create 2D triggers with (almost) solid colors for all pixels. These triggers with a solid color remain consistent after transformation across different inputs, thereby largely reducing the discrepancy between different backdoor samples.

### LiDAR-Free Attack

In many real-world scenarios, the attacker does not have access to the LiDAR signal that is synchronized with the camera signal, especially during inference, _i.e_., when deploying the designed 2D trigger to fool the backdoored fusion model. Hence, the absence of LiDAR signals poses challenges to identifying the densely populated regions of the 2D LiDAR projection where the 2D trigger should be implemented. To address this issue, we propose a LiDAR-free BadFusion approach by predicting dense regions of the 2D LiDAR projection based on camera signals. We convert this region prediction task to an object detection task, where the object becomes the densest region in the 2D LiDAR projection. To achieve this, we create a training dataset containing camera signals and the bounding boxes of the densest areas, denoted by \((x,y,w,h)\), where \(x\) and \(y\) are the center coordinates, and \(w\) and \(h\) are the width and height of bounding box, respectively. Here, we set \(w\) and \(h\) the same as the width and height of the injected trigger \(\). For each vehicle, we annotate a bounding box that contains most points in 2D LiDAR projection. Then, we train a dense region detector \(f_{2d-lidar}\) to predict the bounding boxes based on the Faster R-CNN framework Ren et al. (2015) with a VGG backbone. Our evaluation results show that the detector can successfully identify the dense areas and facilitate the backdoor attacks even without knowing the LiDAR signals, which achieves comparable performance to the LiDAR-aware attack.

### Overall Algorithm Design

Algorithm 1 outlines the overall procedure of BadFusion as shown in Appendix A.

## 4 Evaluation

In this section, we first detail our experimental framework (dataset, implementation & training details, evaluation metrics) and then present the evaluation results of the proposed BadFusion. We further demonstrate the effectiveness of BadFusion against mainly Point-line Camera-to-LiDAR fusion methods in 3D object detection and also benchmark our approach against three state-of-the-art backdoor detection methods. Lastly, we conduct an ablation study to elucidate the internal mechanics of the BadFusion.

### Evaluation Settings

**Dataset.** We use the KITTI dataset Geiger et al. (2013) in the evaluation. The dataset collects real traffic environments from Europe Street for 3D detection tasks, comprising \(7,481\) labeled training frames and \(7,518\) unlabeled test samples. Additional details are presented in Appendix C.

**LiDAR-camera fusion methods.** In this work, we evaluate backdoor attacks against widely used LiDAR-camera fusion methods. In the main paper, we report the evaluation results on MVX-Net, a single-stage fusion model Sindagi et al. (2019). The results for other LiDAR-camera fusion methods are presented in Appendix. To train the fusion model, we adopt common data augmentation techniques, including resizing, rotation, scaling, translation, and flip1. We use FocalLoss Lin et al. (2017) for classification and SmoothL1Loss Huber (1992) for bounding box regression, respectively. The fusion models are trained using an AdamW optimizer with a learning rate of \(0.002\) and a weight decay parameter of \(0.01\) for \(70\) epochs.

**Attack goals.** To manipulate the prediction of vehicles (_Car_ class in the KITTI dataset), we consider two attack goals. 1) Resizing attack: the attacker aims to reduce the sizes of target bounding boxes to mislead the prediction as a smaller vehicle, 2) Disappear attack: the attacker aims to make the vehicle disappear from detection. Figure 3 illustrates an example of the two goals of attacks. Additional details on Attack goals are presented in Appendix C.1. In this section, we report most evaluation results based on the resizing attacks. The effectiveness of the disappear attacks is presented in the ablation study (Section 4.3).

**Baseline attacks and attack setup.** We compare the proposed BadFusion with three state-of-art 2D-oriented backdoor attacks, including OptimizedTrigger Liu et al. (2018), BadDet Chan et al. (2022) and UntarOD Luo et al. (2023). To make a fair comparison, for all attacks, we poison \(15\)% training data using a trigger with the size of \(15 15\) and maintain consistent training or experimental settings. Additional details on Baseline attacks and their attack setup are presented in Appendix C.1.

**Evaluation metrics.** We evaluate the effectiveness of the backdoor attacks based on three well-established metrics: Clean data mAP, Attack Success Rate (ASR), and Poisoned data mAP. Aneffective backdoor attack should achieve high clean data mAP, high ASR, and low poisoned data mAP. Additional details on the definitions of the Evaluation metrics are presented in Appendix C.1.

### Main Evaluation Results

Table 1 compares our proposed BadFusion attack with existing backdoor attacks. The results show that the existing backdoor attacks (OptimizedTrigger, BadDet, UntarOD) are ineffective in manipulating the fusion detector's predictions. All the attacks result in less than 50% ASR and relatively high poisoned data mAP. This is mainly due to the sparse and inconsistent trigger patterns during the fusion process as discussed in Section 3.1. Our proposed BadFusion attack addresses the problem and successfully performs backdoor attacks achieving over 95% ASR and around 3% poisoned data mAP. In the meanwhile, BadFusion can still provide accurate predictions on the clean samples without triggers and achieves much higher clean data mAP compared with the baseline attacks. Additionally, in BadFusion, we assume the attacker has no information about LiDAR signals and trains a model to predict the dense LiDAR region. To investigate the effectiveness of the dense region detector, we compare BadFusion with a LiDAR-ware version of BadFusion, where we assume LiDAR signals are accessible and the dense region can be directly calculated. We find that although LiDAR-ware BadFusion achieves a better attack performance. However, the gap between BadFusion and LiDAR-ware BadFusion is marginal, which suggests the effectiveness of the dense region detector.

### Ablation Study

In this section, we conduct ablation studies to demonstrate the effectiveness of BadFusion with different attack goals and trigger patterns. Additional experiments are presented in Appendix C.5.

**Effectiveness of BadFusion with different attack goals**. We first investigate the attack performance with two goals in BadFusion: resizing bounding boxes and disappearing objects. In disappearing attack, we use two poisoning strategies: moving the center coordinates of bounding boxes farther or closer in the poisoned data. As shown in Appendix Table 2, all the attacks achieve good performance with high ASR and low poisoned data mAP. Additionally, we find that, compared with disappear attack (farther), moving bounding boxes closer is more effective for disappearing attacks.

**Effectiveness of BadFusion with different trigger patterns.** In the evaluation, we consider two trigger patterns. 1) Solid pattern: using a solid color for all pixels in the trigger and 2) almost solid pattern: using a solid color for most pixels while only a few pixels are applied with other colors. The almost solid pattern applies to many real-world scenarios, _e.g._, emojis or decals used in vehicle stickers, which makes the trigger more stealthy. Figure 6 in the Appendix shows the two patterns used in the evaluation. As shown in Appendix Table 9, using almost solid patter, BadFusion can still achieve an ASR of 79.51%. This indicates the severe security risks of BadFusion in the real world.

## 5 Conclusion

This paper presents the first analysis of 2D-oriented backdoor attacks against LiDAR-camera fusion for 3D object detection. By analyzing the existing 2D-oriented backdoor attacks, we find that these attacks are ineffective against fusion models due to the sparsity and inconsistency of backdoor triggers introduced during the fusion process. To address these challenges, we propose BadFusion, an innovative fusion-aware backdoor attack against 3D object detection. By maximizing both effective trigger pixels and consistent trigger patterns among different inputs, BadFusion successfully performs backdoor attacks against state-of-the-art LiDAR-camera fusion methods and realizes two attack goals: resizing the bounding boxes and disappearing the objects. Compared with existing 2D-oriented backdoor attacks, BadFusion achieves a much higher attack success rate and low Poisoned data mAP. We hope our analysis will enhance safety awareness for autonomous driving and promote further research in this field.

   Backdoor attack & Clean data mAP (\%) \(\) & Poisoned data mAP (\%) \(\) & ASR (\%) \(\) \\  Clean model & 93.75 & - & - \\ OptimizedTrigger & 18.90 & 21.12 & 49.49 \\ BadDet & 36.14 & 48.21 & 39.32 \\ UntarOD & 64.98 & 37.57 & 46.74 \\ (LiDAR-aware) BadFusion & 88.65 & 1.61 & 96.74 \\ BadFusion & 88.65 & 3.05 & 95.28 \\   

Table 1: Comparison between existing backdoor attacks and proposed BadFusion against MVX-Net. We perform resizing attacks to reduce the bounding boxes of predicted vehicles. Clean model shows the performance of the fusion model without backdoor attacks.