# 4M-21: An Any-to-Any Vision Model

for Tens of Tasks and Modalities

 Roman Bachmann\({}^{1}\)1  Oguzhan Fatih Kar\({}^{1}\)2  David Mizrahi\({}^{2}\)3  Ali Garjani\({}^{1}\)

**Mingfei Gao\({}^{2}\)**  David Griffiths\({}^{2}\)**  Jiaming Hu\({}^{2}\)**  Afshin Dehghan\({}^{2}\)**  Amir Zamir\({}^{1}\)

\({}^{1}\)Swiss Federal Institute of Technology Lausanne (EPFL) \({}^{2}\)Apple

https://4m.epfl.ch

###### Abstract

Current multimodal and multitask foundation models, like 4M  or UnifiedIO , show promising results. However, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like DINov2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes. A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text.

Through this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so _without a loss in performance_. In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model. We scale the training to a three billion parameter and different datasets. The multimodal models and training code are open sourced at https://4m.epfl.ch.

Figure 1: We demonstrate training a single model on tens of highly diverse modalities _without a loss in performance_ compared to specialized single/few task models. The modalities are mapped to discrete tokens using modality-specific tokenizers. The model can generate _any_ of the modalities from _any subset_ of them.

Introduction

Having _a single neural network_ to handle a wide and varied range of tasks and modalities has been a longstanding goal. Such a model, especially when capable of any-to-any predictions, brings notable advantages, such as test-time computational efficiency, model size, and enabling modality fusion.

However, multitask learning has commonly faced significant challenges. For example, the training often suffers from negative transfer, leads to reduction in performance compared to single-task models, and typically requires careful strategies for balancing losses or gradients . Moreover, training a single network on tasks and modalities that vary greatly in terms of dimensionality, data type, and value ranges presents additional complexities2. Recent notable efforts in the space of multimodal and multitask training, such as Pix2Seq , OFA , 4M , or Unified-IO  have made significant strides in unifying the representation space for conceptually different inputs and targets. A large part of their success can be attributed to transforming different modalities into a common representation, namely sequences of discrete tokens, and training relatively standard Transformer architectures on them. While these works show promising results, they are typically trained on a small set of modalities. This raises the question if increasing the set of tasks/modalities the models can solve will lead to a degradation of performance.

We build upon the multimodal masking pre-training scheme  and increase its capabilities by training on tens of highly diverse modalities. Concretely, we add SAM segments , 3D human poses and shapes from 4DHumans , Canny edges extracted from RGB and SAM instances, color palettes, multiple types of image, semantic and geometric metadata, as well as T5-XXL  text embeddings, in addition to 7 more common modalities. On top of that, we include dense feature maps of the recent state of the art models DINOc2  and ImageBind , as well as their global embedding vectors to enable multimodal retrieval abilities. Please see fig. 1 for an overview.

We are able to train a single unified model on diverse modalities by encoding them with modality-specific discrete tokenizers (see fig. 3). For image-like modalities, e.g. RGB or edges, we train ViT-based  VQ-VAE  tokenizers to map the inputs into a small grid of discrete tokens. For modalities like 3D human poses or image embeddings, we train MLP-based discrete VAEs to compress them into a small set of discrete tokens. All other modalities that can be mapped to a text representation, such as captions or metadata, are encoded using a WordPiece tokenizer .

The resulting model demonstrates the possibility of training a single model on a large number of diverse modalities/tasks without any degradation in performance and significantly expands the out-of-the-box capabilities compared to existing models. Adding all these modalities enables new potential for multimodal interaction, such as retrieval from and across multiple modalities, or highly steerable generation of any of the training modalities, all by a single model.

In short, we expand the capabilities of existing models across several key axes:

* **Modalities**: Increase from 7 in the existing best any-to-any models  to 21 diverse modalities, enabling new capabilities like cross-modal retrieval, controllable generation, and strong out-of-the-box performance. This is one of the first times in the vision community that a single model can solve **tens of diverse tasks in an any-to-any manner** (see fig. 2), without sacrificing performance and especially do so without any of the conventional multitask learning difficulties .
* **Diversity**: Add support for more structured data, such as human poses, SAM instances, metadata, and color palettes for controllable generation.
* **Tokenization**: Investigate discrete tokenization of diverse modalities such as global image embeddings, human poses, and semantic instances using modality-specific approaches.
* **Scale**: Scale the model size to 3B parameters and dataset to 0.5B samples using .
* **Co-Training**: Demonstrate co-training on vision and language modeling simultaneously.

## 2 Method

We adopt the 4M pre-training scheme  as it has been shown to be a versatile approach that can be efficiently scaled to a diverse set of modalities. We keep the architecture and the multimodal masked training objective the same, but expand upon the model and dataset size, the types and number of modalities with which we train the model, and train jointly on multiple datasets. All modalities are first transformed into sequences of discrete tokens using modality-specific tokenizers (See fig. 3). During training, random subsets of these tokens are selected from all modalities as inputs and targets, and the objective is to predict one subset from the other. We rely on pseudo labeling to create a large pre-training dataset with multiple aligned modalities. See appendix I.1 for a discussion on different architecture choices.

### Modalities

We train on a large and diverse set of modalities that we group into the following categories: RGB, geometric, semantic, edges, feature maps, metadata, and text modalities. Below we provide a summary of them (See fig. 1 and appendices D and E for details, and fig. 2 for generation examples).

**RGB:** We include both tokenized and pixel versions of RGB images to facilitate transfer learning. In particular, discrete tokens enable iterative sampling, making them useful for generative tasks [96; 15].

Figure 2: **One-to-all generation.** 4M-21 can generate all modalities from any given input modality and can benefit from chained generation . Notice the _high consistency_ among the predictions of all modalities for one input. Each row starts from a different modality coming from the same scene. Highlighted in green are new input/output pairs that 4M  cannot predict nor accept as input. Note that, while this figure shows predictions from a single input, 4M-21 can generate any modality from _any subset of all modalities_.

On the other hand, using RGB pixels as input is more suitable for visual perception tasks. By avoiding the discrete bottleneck, there is no information loss during the tokenization step, and the projection layer can be more lightweight. Given these tradeoffs, we follow 4M by training on both and treating them as separate modalities, with RGB pixels as an input-only modality. We also extracted _color palettes_ from RGB images using PyPalette , at varying number of colors. This enables us to perform conditional generation using desired colors for better artistic control.

**Geometric modalities:** These contain _surface normals_, _depth_, and _3D human poses & shape_ which provide important information about the scene geometry. For the first two, we used Omnidata models from  for pseudo labeling due to their strong generalization performance. For 3D human poses and shape, we leverage a recent state-of-the-art model, 4D-Humans .

**Semantic modalities:** We include _semantic segmentation_ and _bounding boxes_ to capture the scene semantics and leverage Mask2Former  and ViTDet  models for pseudo labeling. Next to these, we also incorporated pseudo labels extracted from Segment Anything Model  (SAM) as _SAM instances_ for its strong object representation.

**Edges:** As recent generative methods such as ControlNet  showed, edges carry important information about the scene layout and semantics that are also useful for conditioning, abstraction, and sketching. We consider two types of edges, specifically _Canny edges_ and _SAM edges_. The former is extracted from the RGB images with OpenCV . As Canny edges may contain low-level information, e.g. shading edges, we also include edges extracted from SAM instances to get a more semantic boundary map. We tokenize Canny and SAM edges with a shared tokenizer.

**Feature maps:** We extract embeddings from _CLIP_, _DINOV2_ and _ImageBind_ as they demonstrated strong transfer learning and retrieval capabilities. Previously, tokenized CLIP features were shown to be an effective target for masked image modelling  that enables distilling a useful semantic representation of the scene. We follow a similar approach and tokenize the feature maps from pre-trained CLIP-B16, DINOV2-B14 and ImageBind-H14 models. We also included the _global embeddings_ of DINOV2 and ImageBind models and tokenized them separately.

**Metadata:** We extract several useful pieces of information from the RGB images and other modalities, that can be categorized into _semantic metadata_, _geometric metadata_, and _image processing metadata_. For this, we use functionalities from Pillow  OpenCV , and Omnidata .

The following semantic metadata are extracted from bounding boxes, poses, and segmentation maps:

* _Crowedness score_: number of humans (extracted from 4DHumans instances)
* _SAM clutter score_: number of SAM instances
* _COCO clutter score_: number of COCO  instances
* _COCO instance diversity_: number of unique COCO instance classes
* _Objectness score_: % of pixels that belong to countable COCO semantic classes
* _Walkability score_: % of pixels belonging to walkable COCO semantic classes such as 'road'
* _Semantic diversity_: number of unique COCO semantic classes
* _Caption length_: length of the caption in characters, words, and sentences

These are aimed to capture the semantic regularities of the scene at a more holistic level as opposed to pixel-based representations.

Similarly, geometric metadata captures the scene geometry more globally. They are extracted from surface normals and depth maps:

* _Geometric complexity_: angular variance of surface normals
* _Occlusion score_: % of occlusion edges over a fixed threshold

Finally, image processing metadata contains several aspects of images such as _original image height and width_ before cropping, which can be used as conditioning to generate higher quality images , _brightness_, _contrast_, _saturation_, _entropy_, and _colorfulness_. Similar to color palette, these help with encoding low-level image representations into the model and enable more steerable generation.

**Text:** Large language models (LLMs) trained on large text corpora learn strong representations as shown by several works . We include _captions_ from CC12M  and COYO700M datasets, as well as web text from C4  for language modeling. Next, we employ both a standard WordPiece  tokenizer for captions as  as well as _caption embeddings_ obtained from a T5-XXL  encoder to capture better text representations, which have been shown to improve text-to-image generation fidelity [80; 14] (See fig. 4).

### Tokenization

Tokenization consists of converting modalities and tasks into sequences or sets of _discrete tokens_, thereby unifying their representation space. This is critical for training large multimodal models as it confers the following key benefits: 1) It enables training multimodal and multitask models with a single pre-training objective. After tokenization, all tasks are formulated as a per-token classification problem using the cross-entropy loss. This improves training stability, enables full parameter sharing, and removes the need for task-specific heads, loss functions, and loss balancing. 2) It makes generative tasks more tractable by allowing the model to iteratively predict tokens, either autoregressively [72; 96] or through progressive unmasking [15; 14]. 3) It reduces computational complexity by compressing dense modalities like images into a sparse sequence of tokens. This decreases memory and compute requirements, which is crucial when scaling up to larger dataset and model sizes.

We use different tokenization approaches to discretize modalities with different characteristics. See fig. 3 for an overview. To summarize, we mainly use three different types of tokenizers, as explained below. Please see appendices D and H for more details and insights on tokenizer design choices.

**ViT tokenizer (with optional diffusion decoder):** We trained modality-specific ViT  based VQ-VAE  tokenizers for image-like modalities such as edges and feature maps. The resulting tokens form a small grid of size \(14 14\) or \(16 16\), according to the pseudo-labeler patch size. The edge tokenizers use a diffusion decoder [82; 65] to get visually more plausible reconstructions.

**MLP tokenizer:** For human poses and global embeddings from DINOv2 and ImageBind, we use Bottleneck MLP  based discrete VAEs with Memcodes quantization  to tokenize them into a small number of tokens, e.g. 16.

**Text tokenizer:** We leverage a WordPiece  tokenizer which is used to encode not only text, but also other modalities such as bounding boxes, color palettes and metadata using a shared set of special tokens to encode their type and values (See appendix D.6 for details).

### Training details

**Datasets:** We perform the training in two stages, namely a 4M pre-training stage on a significantly larger image dataset, followed by a fine-tuning phase on a smaller dataset containing a larger number of modalities. Since the 4M-XL model showed signs of overfitting on sequence modalities when trained on CC12M , we re-trained the models on COYO700M , containing 50 times more samples. COYO700M was pseudo labeled with the same modalities used for 4M. To cut down

Figure 3: **Tokenization overview. We employ suitable tokenization schemes for different modalities based on their format and performance. For image-like modalities and feature maps, we use spatial VQ-VAEs  with optional diffusion decoders for detail rich modalities like RGB. For non-spatial modalities like global tokens or parameterized poses, we compress them to a fixed number of discrete tokens using Memcodes  with MLP encoders and decoders. All sequence modalities are encoded as text using WordPiece . The shown examples are real tokenizer reconstructions. Notice the low reconstruction error. See appendix D for more details and Fig. 13 for visualizations.**

on pseudo labeling cost when expanding the number of modalities, we decided to pseudo label CC12M instead of COYO700M, and fine-tune the models with both new and old modalities. To avoid overfitting the larger models, we co-train them with samples from COYO700M. In addition to the previously mentioned multimodal datasets, we also included the C4  text corpus in training. We perform the training by randomly sampling elements of each batch from any of these datasets, given a pre-determined set of sampling weights, and perform language modeling on them. Exact details on the training mixture are given in appendix E.2.

**Architecture:** We adopt 4M's encoder-decoder based transformer architecture with additional modality embeddings to accommodate new modalities. Similar to 4M, besides RGB tokens, the encoder directly accepts RGB pixels with a learnable patch-wise projection to enable use as a ViT  backbone for transfer learning.

**Masking strategy:** We used both multimodal random [7; 65] and span masking  strategies that mask input and target tokens. We invoke dataset mixing ratios and Dirichlet sampling parameters, \(\), to ensure stable training on multiple modalities and datasets, as detailed in appendix E.2.

## 3 Multimodal capabilities

We demonstrate a broad range of capabilities unlocked by 4M-21, including steerable multimodal generation (Sec. 3.1), multimodal retrieval (Sec. 3.2) and strong out-of-the-box capabilities (Sec. 3.3). Please see the project website for more visualizations demonstrating these capabilities.

### Steerable multimodal generation

4M-21 can predict any training modality by iteratively decoding tokens [65; 15; 14]. This is shown in fig. 2 where we can generate all modalities from a given input modality in a consistent manner. Furthermore, as we can generate _any_ of the training modalities from _any_ subset of other modalities, both conditionally and unconditionally, it enables several ways to perform fine-grained and multimodal generation, as shown in fig. 4. This includes diverse capabilities such as performing multimodal edits, probing the learned representations, and steering multimodal data generation.

Figure 4: **Fine-grained & steerable multimodal generation.****Top left: 4M-21 can generate variants of images that are grounded in any input modality, here human poses. **Bottom left:** This enables us to perform multimodal edits (e.g. editing the shape of a polygon or grounding generation with edges) and probe the learned representation. For example, by only changing the shape of the ellipse, 4M-21 renders the bowl from different angles. **Top right:** By pre-training on 21 types of modalities, including T5-XXL embeddings, and _co-training with language modeling_ on a large text corpus, we show improved text understanding capabilities (even when the input is captions instead of language model embeddings). **Bottom right:** Compared to generating images from captions only, metadata provides a more direct and steerable way of controlling the multimodal data generation process, enabling exciting further research into generative dataset design.

Moreover, 4M-21 exhibits improved text understanding capabilities leading to geometrically and semantically plausible generations, both when conditioning on T5-XXL embeddings and on regular captions (fig. 4, top right). Please see appendix I.2 for additional results.

### Multimodal retrieval

Our model can also perform multimodal retrievals by predicting global embeddings of DINov2 and ImageBind _from any (subset) of the input modalities_. Once the global embeddings are obtained, the retrieval is done by finding the retrieval set samples with the smallest cosine distance to the query . As shown in fig. 5, this unlocks retrieval capabilities that were not possible with the original DINov2 and ImageBind models such as retrieving RGB images or any other modality via using any other modality as the query. Furthermore, one can combine multiple modalities to predict the global embedding, resulting in better control over retrievals, as shown on the right. Please see appendix 1.3 for additional results.

### Evaluating out-of-the-box capabilities

4M-21 is capable of performing a range of common vision tasks out-of-the-box, as demonstrated visually in fig. 6. In table 1, we evaluate the performance on DIODE  surface normal and depth estimation, COCO  semantic and instance segmentation, 3DPW  3D human pose estimation, and do ImageNet-1K  kNN retrieval using predicted DINov2 global tokens. We compare against the pseudo labeling networks, strong baselines, and the 4M model from  trained on 7 modalities. For surface normal estimation and semantic segmentation, we observed that ensembling multiple predictions significantly improves performance, see appendix F for more details and results.

Our model consistently achieves strong out-of-the-box performance, and often matches or even outperforms the pseudo labelers and other specialist baselines, _while being a single model for all tasks_. Notice the large performance gap with other multitask models like Unified-IO  and Unified-IO-2 . For kNN retrieval, 4M-21 XL performance approaches the tokenizer bound, i.e. the retrieval performance using the DINov2 tokenizer reconstructions. While the smaller models lag behind 4M models, we observe that 4M-21 XL is able to match the performance of 4M-7 XL, while being trained to solve three times more tasks. The trend over the model size needing to be larger is expected as the number of tasks increase.

Figure 5: **Different modes of multimodal retrieval.** We perform _multimodal_ retrievals by predicting global embeddings (here shown for DINov2) from a given input (of any modality) using 4M-21 and comparing the cosine distances between the query and retrieval set embeddings. **Left:** Retrieving RGB images from distinctly different query modalities (here RGB, segmentation map, edges, depth map, color palette, and caption). **Middle:** Retrieving any modality using any other modality as the query input. Each query modality constrains the retrievals differently, e.g. here the RGB image and caption queries always yield Neuschwastein castle retrievals. In contrast, for depth and semantic queries, the scene is more ambiguous, thus they retrieve other buildings with similar characteristics. **Right:** We can also _combine any subset of modalities_ to define the query input, e.g. surface normals and a color palette, to better control the retrieval. See appendix B.2 for more results.

## 4 Transfer experiments

To study the scaling characteristics of pre-training any-to-any models on a larger set of modalities, we train models across three different sizes: B, L, and XL. We then transfer their encoders to downstream tasks and evaluate on both unimodal (RGB) and multimodal (RGB + Depth) settings. The decoders are discarded for all transfer experiments, and we instead train task-specific heads. We perform self-comparisons in a similar manner to [65; 7], as well as comparing to a set of strong baselines.

**Unimodal transfers.** For unimodal transfers we leverage the RGB patch embeddings learned during the pre-training, as RGB pixel inputs are used alongside the tokenized modalities. For the XL models

    & Method & Normals \(\) & Depth \(\) & Sem. seg. \(\) & Inst. seg. \(\) & IN1K kNN \(\) & 3D human KP \(\) \\    } & Omnidata  & 22.5 & **0.68** & ✗ & ✗ & ✗ & ✗ \\  & MZF-B  & ✗ & ✗ & 45.7 & ✗ & ✗ & ✗ \\  & SAM  & ✗ & ✗ & ✗ & **32.9** & ✗ & ✗ \\  & DIMO-2B14  & ✗ & ✗ & ✗ & ✗ & **82.1**/ 93.9 & ✗ \\  & ImageBind-H14  & ✗ & ✗ & ✗ & ✗ & 81.1 / **94.4** & ✗ \\  & 4D-Humans  & ✗ & ✗ & ✗ & ✗ & ✗ & **81.3** \\    } & OASIS  & 34.3 & ✗ & ✗ & ✗ & ✗ \\  & MiDs DPT  & ✗ & 0.73 & ✗ & ✗ & ✗ \\  & MZF-S  & ✗ & ✗ & 44.6 & ✗ & ✗ & ✗ \\  & MZF-L  & ✗ & ✗ & 48.0 & ✗ & ✗ & ✗ \\  & HMR  & ✗ & ✗ & ✗ & ✗ & 130.0 \\  & Unified-B  & 35.7 & 1.00 & 32.9 & ✗ & ✗ & ✗ \\  & UnifiedIO-L  & 33.9 & 0.87 & 41.6 & ✗ & ✗ & ✗ \\  & UnifiedIO-XL  & 31.0 & 0.82 & 44.3 & ✗ & ✗ & ✗ \\  & UnifiedIO 2-L  & 37.1 & 0.96 & 38.9 & ✗ & ✗ & ✗ \\  & UnifiedIO 2-XL  & 34.8 & 0.86 & 39.7 & ✗ & ✗ & ✗ \\  & UnifiedIO 2-XXL  & 37.4 & 0.84 & 41.7 & ✗ & ✗ & ✗ \\    } & 4M-7 B  & 21.9 & 0.71 & 43.3 & ✗ & ✗ & ✗ \\  & 21.7 & 0.71 & 42.5 & 15.9 & 73.1 / 89.7 & 108.3 \\   & 4M-7 L  & 21.5 & 0.69 & 47.2 & ✗ & ✗ & ✗ \\  & 21.1 & 0.69 & 46.4 & 31.2 & 77.0 / 91.9 & 97.4 \\    } & **20.6** & 0.69 & **48.1** & ✗ & ✗ & ✗ \\  & 20.8 & **0.68** & **48.1** & 32.0 & 78.3 / 92.4 & 92.0 \\   

Table 1: **Out-of-the-box (zero-shot) performance.** We show the performance for a common subset of tasks: surface normals and depth estimation on DIODE , semantic and instance segmentation on COCO , k-NN retrieval on ImageNet-1K , and 3D human keypoint estimation on 3DPW . We compare to a set of strong baselines and specialist models, including our pseudo labelers. The model learned to solve all the tasks without a loss of performance, is significantly better than the baselines, and is competitive with pseudo labelers, _while being a single model for all tasks_. Compared to 4M-7, the 4M-21 model preserved its performance while solving 3x more tasks. ✗ denotes that a given model cannot solve the task out-of-the-box. * shows the tokenizer reconstruction quality and provides an estimate on the performance upper bound due to tokenization. See fig. 14 for qualitative comparisons. Best results are **bolded**, second best underlined.

Figure 6: **Out-of-the-box vision tasks.** Given an RGB image, 4M-21 can predict all tasks successfully, as can be seen from their high consistency with the pseudo labels. See fig. 7 for more results.

and DINOV2 g, we perform parameter-efficient fine-tuning using LoRA  instead of full fine-tuning, which significantly improves results for XL models. We did not observe similar performance gains for the smaller models. Further training details are described in appendix G.

We evaluate on ImageNet-1K classification [23; 79], ADE20K semantic segmentation , NYUv2 depth estimation , and ARKitScenes  3D object detection tasks. Some transfer tasks are completely unseen during pre-training, e.g. object classification or 3D object detection, while others are included as different instantiations, e.g. absolute depth instead of relative depth, or using ADE20K instead of COCO classes. We follow the best practices and commonly used settings from other papers .

The results are shown in table 2. We make the following observations: **1)** for the transfer tasks that are similar to the seven modalities of 4M, e.g. semantic segmentation or depth, 4M-21 does not lose performance due to being trained on many more modalities, **2)** for novel transfer tasks like 3D object detection that are sufficiently different from 4M modalities, we observe an improved performance. Moreover, the performance improves with larger model sizes, showing promising scaling trends. These trends can be further seen in the multimodal transfer results, which we explain next.

**Multimodal transfers.** We perform multimodal transfers on NYUv2, Hypersim  semantic segmentation, and 3D object detection on ARKitScenes. We compare transfers using RGB images only, and RGB pixels + tokenized sensory depth as inputs. As table 3 shows, 4M-21 makes strong use of optionally available depth inputs and significantly improves upon the baselines.

## 5 Related Work

Multitask learning in vision involves training a single model to perform multiple visual tasks efficiently [13; 78]. Earlier methods [28; 64; 50] combined multiple dense vision tasks into a single model but faced challenges scaling to a larger variety

    & Pre-training & Enc. & IN1K & ADE20K & NYUv2-D & ARKS \\  & data & param. & Acc.\(\) & mIoU\(\) & \(_{1}\) acc.\(\) & AP\({}^{3}\)\(\) \\  MAE B  & IN1K & & 84.2 & 46.1 & 89.1 & 30.9 \\ DeiT III B  & IN21K & & 85.4 & 49.0 & 87.4 & 36.1 \\ MultiMAE B  & IN1K & & 84.0 & 46.2 & 89.0 & 34.2 \\ DINOV2 B  & LVD142M & 86M & 85.3 & 51.6 & 92.2 & 38.1 \\
4M-7 B  & CC12M & & 84.5 & 50.1 & 92.0 & 40.3 \\
4M-7 B [0urs] & COYO & & 84.4 & 49.4 & 91.4 & 38.6 \\
4M-21 B & CC12M+COYO+C4 & & 84.5 & 50.1 & 90.8 & 42.4 \\  MAE L  & IN1K & & 86.8 & 51.8 & 93.6 & 36.2 \\ DeiT III L  & IN21K & & 87.0 & 52.0 & 89.6 & 40.3 \\ DINOV2 L  & LVD142M & & 86.7 & 53.4 & 94.1 & 42.8 \\
4M-7 L  & CC12M & & 86.6 & 53.4 & 94.4 & 46.8 \\
4M-7 L (Ours) & COYO & & 86.7 & 53.5 & 94.3 & 45.2 \\
4M-21 L & CC12M+COYO+C4 & & 86.5 & 53.4 & 93.7 & 47.0 \\  DINOV2 g  & LVD142M & 1.1B & **88.0** & **58.7** & 92.5 & 45.3 \\
4M-7 XL  & CC12M & & 87.0 & 55.0 & 96.1 & 48.1 \\
4M-7 XL (Ours) & COYO & 1.2B & 87.1 & 56.1 & **96.5** & 47.3 \\
4M-21 XL & CC12M+COYO+C4 & & 87.1 & 56.0 & **96.5** & **48.4** \\   

Table 2: **Unimodal transfer study. We transfer 4M-21 and baselines to ImageNet-1K  classification, ADE20K  semantic segmentation, NYUv2  depth estimation, and ARKitScenes  (ARKS) 3D object detection. We observe that 4M-21 **1)** does not lose performance for the transfer tasks that are similar to the seven modalities of 4M, i.e. first three columns of the results, while being able to solve many more, and **2)** leads to improved performance for novel tasks that are more different from 4M modalities, e.g. 3D object detection (last column). The improvements are further verified in the multimodal transfer results (Table 3) showing the usefulness of new modalities. Best results per task are **bolded**, second best underlined.

    &  &  &  \\  &  &  & ^{3}\)\(\)} \\   & RGB & RGB-D & RGB & RGB-D & RGB & RGB-D \\ 
4M-7 B & 56.6 & 57.5 & 40.2 & 43.9 & 40.3 & 46.5 \\
4M-21 B & 58.7 & 59.7 & 38.6 & 46.4 & 42.4 & 48.1 \\
4M-7 L & 61.2 & 61.4 & **48.7** & 50.5 & 46.8 & 49.5 \\
4M-21 L & 61.8 & 61.8 & 47.3 & 50.7 & 47.0 & 50.1 \\
4M-7 XL & 62.1 & 61.2 & 48.6 & 51.0 & 48.1 & 50.1 \\
4M-21 XL & **63.9** & **63.9** & 48.6 & **52.5** & **48.4** & **51.3** \\   

Table 3: **Multimodal transfer study. We transfer both 4M-21 and 4M (pre-trained on CC12M) to NYUv2 and Hypersim segmentation, and 3D object detection on ARKitScenes. All models are able to use optionally available depth when it is of high quality (Hypersim & ARKitScenes), while our model achieves the best results. Best results are **bolded**, second best underlined.

of tasks and modalities, limited by training instabilities and the need for careful task selection and loss balancing to reduce negative transfer .

Recently, discrete tokenization has enabled a shift towards integrating numerous vision tasks into unified multimodal and multitask models such as Gato , OFA , Pix2Seq , UnifiedIO , 4M , and more . These methods first transform various modalities and tasks into sequences or sets of discrete tokens , and then train a single Transformer on these tokens using either a sequence modeling  or masked modeling objective . Some methods (e.g. Gato , UnifiedIO ) perform co-training on multiple disjoint datasets and are capable of performing a wide range of tasks, but not jointly. In contrast, methods like 4M  train on a single aligned dataset through the use of pseudo labeling, enabling any-to-any modality prediction but on a typically more limited set of modalities. We significantly expand upon them by adding the ability to use this framework for an even greater amount of modalities and capabilities.

Furthermore, masked modeling has proven effective for learning useful representations in both NLP  and vision . Extending it to multimodal domains  enables strong cross-modal representations which is critical for multimodal learning. When combined with tokenization, masked modeling also enables generative applications . Our work highlights the ability of masked modeling to expand to a much greater set of modalities than previously shown, improving upon the out-of-the-box and multimodal generation capabilities of previous works.

## 6 Limitations and Discussion

We developed an any-to-any model on tens of diverse modalities and tasks. This was achieved by mapping all modalities to discrete sets of tokens via modality-specific tokenizers and using a multimodal masked training objective . We successfully scaled the training to 3 billion parameters and to 21 modalities and different datasets, without a degradation in performance compared to the existing expert single/few task models. This results in strong out-of-the-box capabilities as well new potential for multimodal interaction, generation, and retrieval, all by a single unified model. Below, we discuss limitations and future work.

_Transfer/emergent capabilities:_ One hope from training a single network on several tasks is leading to a model that can solve novel tasks, often referred to as "transfer" or "emergent" capabilities. While, as we showed, a multitask model brings several key advantages even without transfer/emergence (e.g., efficiency, using a single model for broad out-of-the-box capabilities, modality fusion, etc.), we observe that **the potential for transfer/emergence remains largely untapped**. In general, compared to LLMs, vision/multimodal models have not shown exciting results in terms of transfer/emergence yet. We find this to be an important point to address in the future, e.g., via designing multitask architectures that have emergence, in contrast to out-of-the-box capabilities, as their main objective.

_Better tokenization:_ Like any token-based model, ours can directly benefit from progress on tokenizers, e.g. higher reconstruction fidelity.

_Co-training on partially aligned datasets:_ We showed the possibility of training on partially aligned datasets, e.g. text data from C4 and other modalities from CC12M, yet further investigations and a larger mixture of datasets are expected to bring stronger capabilities.

**Acknowledgement:** This work was supported as part of the Swiss AI initiative by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a08 on Alps. We also acknowledge the support of the ETH4D and EPFL EssentialTech Centre Humanitarian Action Challenge Grant.