# Adam with model exponential moving average is effective for nonconvex optimization

Kwangjun Ahn

Microsoft Research

Cambridge, MA

kwangjunahn@microsoft.com &Ashok Cutkosky

Boston University

Boston, MA

ashok@cutkosky.com

###### Abstract

In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements--momentum and discounting factors--as well as model EMA, motivating their wide applications in practice.

## 1 Introduction

In neural network training, the training loss \(F:^{d}\) is often optimized using an iterative optimization algorithm which starts with the initial iterate \(_{0}\) and then updates during each iteration \(t=1,2,\) as follows:

\[_{t}=_{t-1}+_{t}\,,\]

where \(_{t}\) denotes the increment chosen by the algorithm during the \(t\)-th iteration. One of the most popular optimization algorithms is **Adam**Kingma and Ba (2014). Adam has gained significant attention due to its effectiveness in training Transformer-based language models (Zhang et al., 2020; Kunstner et al., 2023; Jiang et al., 2023; Pan and Li, 2023; Ahn et al., 2024; Kunstner et al., 2024; Zhang et al., 2024).

The **model exponential moving average** (EMA) (Polyak and Juditsky, 1992; Ruppert, 1988) is an optimization technique that has gained popularity in conjunction with Adam for various recent applications. EMA maintains an exponential moving average of the model iterates, \(_{t}\), which contributes to the stabilization of these iterates. There has been a resurgence of interest in this technique due to its effectiveness in training high-quality generative models (Yaz et al., 2018; Karras et al., 2019; Song et al., 2021; Dhariwal and Nichol, 2021; Nichol and Dhariwal, 2021; Song et al., 2021; Balaji et al., 2022; Karras et al., 2022; Rombach et al., 2022; Kang et al., 2023; Karras et al., 2023). Moreover, a recent work by Block et al. (2024) demonstrates the effectiveness of EMA for both language modeling and imitation learning applications.

In this work, we theoretically study the effectiveness of these two modern optimization techniques. Our main results can be informally summarized as follows.

**Theorem 1** (Informal).: clipped-Adam _with the EMA on its iterates achieves the optimal convergence rate for nonconvex optimization both for smooth and nonsmooth settings (Section 5). The coordinate-wise adaptivity of Adam is particularly effective when the scale varies across different coordinates (Section 6)._Our main results are based on the online-to-nonconvex conversion framework of Cutkosky et al. (2023), which chooses the increment \(_{t}\) based on an online learner of choice. In particular, our approach is quite different than the previous analyses of Adam (see Section 1.1 below). Notably, our analysis relies on the key components of Adam (momentum and adaptive learning rate) as well as EMA of the iterates, offering new, theoretical insight into their success. See Section 7 for a more detailed discussion.

At a high level, our analysis combines the main insights from the two recent works: Zhang and Cutkosky (2024) and Ahn et al. (2024). We first carefully modify the discounted-to-nonconvex conversion framework (Lemma 7) of Zhang and Cutkosky (2024) which converts an online learner that achieves a good discounted regret (Definition 6) into a good nonconvex optimizer. We then combine it with the main insight of Ahn et al. (2024) that an effective discounted online learner can be designed based on scale-free Follow-the-Regularized-Leader (FTRL) (Orabona and Pal, 2018). In particular, the way we arrive at Adam is similar to Ahn et al. (2024): choosing a discounted version of FTRL in the discounted-to-nonconvex conversion leads to Adam.

### Related work

Even though Adam is widely used in deep learning, our theoretical understanding of its inner workings, especially the importance of its core components--momentum and discounting factors--remains incomplete, as pointed out by Ahn et al. (2024). Most theoretical work on Adam and its variations focus on characterizing the convergence rate for convex or smooth nonconvex functions, where methods like SGD already achieve the minimax optimal convergence rate. (Reddi et al., 2018; Zhou et al., 2019; Chen et al., 2019; Zou et al., 2019; Alacaoglu et al., 2020; Guo et al., 2021; Defossez et al., 2022; Zhang et al., 2022; Li et al., 2023; Wang et al., 2023). In fact, even the most recent results (Li et al., 2023; Wang et al., 2023) are not reflective of practice, in the sense that Adam's convergence rate worsens with momentum (Wang et al., 2023; Zhang et al., 2023) or is no better than that of SGD (Li et al., 2023, SS7). A notable exception is Crawshaw et al. (2022), which demonstrates the advantages of momentum under the generalized smoothness conditions of Zhang et al. (2020). However, the algorithm they analyze is signSGD, which differs significantly from the original Adam. In contrast, as we will explore in the subsequent sections, momentum and discounting factors are important in our analysis. See Section 7 for more details.

We also highlight that our analysis relies on model EMA, a technique widely used in practice as mentioned above (also see a recent work by Block et al. (2024)). It is worth noting that EMA (or model averaging in general) has shown to have generalization benefits in practice (Tarvainen and Valpola, 2017; Izmailov et al., 2018). In this paper, we study EMA from an optimization perspective, and show that the use of EMA leads to optimal guarantees for nonconvex optimization. Interestingly, EMA naturally derives from the discounted-to-online conversion (see Algorithm 1), which, we believe, provides new theoretical insights into this practical method.

The use of EMA also represents a significant departure from most non-convex optimization analyses. While EMA is a classical technique in the _convex_ setting, theoretical analyses in the non-convex setting typically randomly select an iterate as the "final output" of the optimizer, rather than using EMA. This random selection is intuitively extremely impractical (indeed, on average it actually wastes half of the computation), and is never performed in real implementations.

Our analysis follows a line of work studying convergence guarantees for non-smooth non-convex optimization. Our particular convergence criterion is similar to finding the Goldstein stationary points (Goldstein, 1977) that were first studied in the context of modern machine learning by (Zhang et al., 2020), and has seen much subsequent interest (Tian and So, 2022; Jordan et al., 2023; Davis et al., 2020). Other notions of convergence are also reasonable--common alternatives involve the Moreau envelope, or imposing a weak convexity condition (Davis et al., 2018, 2022).

## 2 Setting for nonconvex and nonsmooth optimization

Throughout this paper, unless specified otherwise, \(\) denotes the \(L_{2}\) norm. Following (Cutkosky et al., 2023), we consider optimizing a loss function \(F\) that satisfies the following conditions, accessing information about \(F\) through a _stochastic gradient oracle_\(:^{d}^{d}\), for the set of randomness \(\).

**Assumption 2**.: _Let \(F:^{d}\) be a differentiable function with the following properties:_

* _Let_ \( F(_{0})-_{}F()\)_._
* _For any two points_ \(\) _and_ \(\)_,_ \(F()-F()=_{0}^{1} F(+t( -)),-t\)_._
* **Lipschitzness.**__\(F\) _is_ \(G\)_-Lipschitz, i.e., for any point_ \(\)_,_ \(\| F()\| G\)_._
* **Stochastic gradient variance.** _For any point_ \(\)_, the stochastic gradient_ \((,r)\) _for randomness_ \(r\) _satisfies_ \([]= F()\) _and_ \(\|- F()\|^{2}^{2}\)_._

The Lipschitz continuity condition is a standard assumption in nonconvex nonsmooth settings. However, as we will discuss in Section 6, one of the key insights from our results is that Adam enables us to **adapt** to the Lipschitz constants **coordinate-wise** without requiring prior knowledge of these constants. Note that we almost certainly need some form of structural assumption on the "difficulty" of the loss function; thus, relaxing the Lipschitz assumption would likely come at the cost of another assumption, such as smoothness.

The second condition, called _well-behavedess_ in (Cutkosky et al., 2023, Definition 1), is a mild regularity condition. For any locally Lipschitz function \(F\), applying an arbitrarily small perturbation to the function is sufficient to ensure this condition (Cutkosky et al., 2023, Proposition 2).

For the notion of optimality, we follow Zhang and Cutkosky (2024) and consider the following notion of stationarity for nonconvex and nonsmooth functions. This notion is a slight relaxation of the notion of a Goldstein stationarity point (Goldstein, 1977), which was further studied by recent works (Zhang et al., 2020; Davis et al., 2022; Tian et al., 2022; Jordan et al., 2023).

**Definition 3** (\((,)\)-stationary point).: _Suppose \(F:^{d}\) is differentiable. We say \(\) is a \((,)\)-stationary point of \(F\) if \(\| F()\|^{[]}\), where_

\[\| F()\|^{[]}_{ p(^{d}),\\ _{ p}[]=}\{ \|[ F()]\|+\| -\|^{2}\}\;.\]

To further motivate this definition, we remark that \((,)\)-stationary points retain the desirable properties of Goldstein stationary points. Specifically, the following result (Zhang and Cutkosky, 2024, Lemma 2.3) demonstrates that, akin to Goldstein stationary points, \((,)\)-stationary points can be reduced to first-order stationary points with appropriate choices of \(\) when the objective function is smooth or second-order smooth.

**Lemma 4**.: _If \(F\) is \(L\)-smooth, then an \((L^{2}^{-1},)\)-stationary point \(\) of \(F\) satisfies \(\| F()\| 2\). Moreover, if \(F\) is \(H\)-second-order-smooth, then an \((H/2,)\)-stationary point \(\) of \(F\) satisfies \(\| F()\| 2\)._

Moreover, as shown by (Zhang and Cutkosky, 2024, Lemma 2.4), \((,)\)-stationary points can also be reduced to Goldstein stationary points when \(F\) is Lipschitz.

**Lemma 5**.: _Suppose \(F\) is \(G\)-Lipschitz. For any \(,,>0\), a \((,)\)-stationary point is a \((,^{})\)-Goldstein stationary point, where \(^{}=(1+})\)._

Now we design algorithms that find \((,)\)-stationary points efficiently.

## 3 Discounted-to-nonconvex conversion: online learning of increments

Our main results are built on the online-to-nonconvex conversion framework of Cutkosky et al. (2023). At its core, this framework involves selecting the increment \(_{t}\) using an online learner, as discussed by Ahn et al. (2024). Specifically, we follow a variant developed by Zhang and Cutkosky (2024), which carefully incorporates the discounting factor in the conversion process. Note that we make slight modifications to the version proposed by Zhang and Cutkosky (2024) as follows. Here \((1)\) denotes the exponential random variable with mean \(1\).

Given Algorithm 1, it turns out we need to design an online learner that minimizes the discounted regret, formally defined below. It is worth noting that discounted regret has been recently studied with the goal of better adapting online learners to dynamic environments (Ahn et al., 2024; Zhang et al., 2024; Jacobsen and Cutkosky, 2024).

**Definition 6** (Discounted regret).: _For a comparator \(\), the \(\)-discounted regret is defined as_

\[_{t}^{[]}()^{t}_{s=1}^{t}( _{s}^{[]}(_{s})-_{s}^{[]}())=_{s=1}^ {t}^{t-s}_{s},_{s}-\,.\]

The discounted regret of an online learner \(\) can be used to upper bound the norm of averaged gradients, as shown in the following result.

**Lemma 7** (**Discounted-to-nonconvex conversion)**.: _Suppose that \(F\) satisfies Assumption 2. Then for the comparator sequence chosen as \(_{t}-D^{t}^{-s} F(_{s} )}{\|_{s=1}^{t}^{-s} F(_{s})\|}\), Algorithm 1 gives_

\[*{}_{t[T]}*{} \|*{}_{_{t}} F(_{t})\|\] \[+[*{}[ _{T}^{[]}(_{T})]+(1-)_{t=1}^{ T}*{}[_{t}^{[]}(_{t}) ]]\,,\]

_where \(_{t}\) is distributed over \(\{_{s}\}_{s=1}^{t}\) as \(*{}(_{t}=_{s})=^{t-s} }\) for \(s=1,2,,t\)._

The proof combines the techniques of (Cutkosky et al., 2023, Theorem 7) and (Zhang and Cutkosky, 2024, Theorem 3.3). See Appendix A for details.

We briefly explain how Lemma 7 can be used to find a \((,)\)-stationary point (Definition 3). Recall that \((,)\)-stationarity essentially requires producing a point \(=*{}[]\) such that both \(\|*{}[ F()]\|\) and \(*{}\|-\|^{2}\) are small.

Given this context, Lemma 7 states that as long as the discounted regret of the online learner \(\) is small, we can ensure that the EMA iterates \(}_{t}=*{}[_{t}]\) serve as good candidates for \((,)\)-stationarity, since the term \(*{}\|*{}_{_{t}} F (_{t})\|\) can be kept small. The remaining task is to bound the variance term, \(*{}\|_{t}-}_{t}\|^{2}\), which will be addressed later in Lemma 10.

Moreover, the comparator \(_{t}\) roughly models the _update direction that an oracle algorithm with perfect knowledge of the loss would select_. In the proof of Lemma 7, we demonstrate that moving along the \(_{t}\) direction effectively decreases the loss value, which forms the basis for establishing our convergence guarantee.

Thanks to the discounted-to-nonconvex conversion, the task now reduces to designing an online learner that achieves low discounted regret.

## 4 Scale-free Follow-the-Regularized-Leader (FTRL)

In this section, we introduce an algorithmic component, called the Followed-The-Regularized-Leader (FTRL), a powerful online learning technique with various applications (Gordon, 1999, Kalai and Vempala, 2005, Shalev-Shwartz and Singer, 2006, Abernethy et al., 2008, Nesterov, 2009, Hazan and Kale, 2010).

For the setting, consider the online linear optimization (OLO) setting, where during each round \(t=1,,T\), and online learner chooses \(_{t}\), and then the linear loss \(_{t}()=_{t},\) is revealed by the environment. Here the goal of the online learner is to minimize the regret defined as \(_{t}_{t},_{t}-\), where \(\) is the comparator in hindsight. For this setting, FTRL is presented in Algorithm 2.

```
0: Regularizers \(\{_{t}\}:^{d}\), the domain \(^{d}\)
1:for\(t=1,2,,T\)do
2: Update \(_{t}_{}\;[_{ t}()+_{s=1}^{t-1}_{s}()]\)
3: Receive the next loss \(_{t}()=_{t},\)
4:endfor ```

**Algorithm 2** Follow-the-Regularized-Leader (FTRL)

The key insight of Ahn et al. (2024) and Zhang et al. (2024) is that in order to design an online learner for discounted regret, it is important that the online learner is _scale-free_ as described below. In particular, following Ahn et al. (2024), we consider a gradient adaptive scale-free FTRL algorithm called _scale-free FTRL_Orabona and Pal (2018).

We will focus on the case where \(=_{D}\), the \(d\)-dimensional \(L_{2}\)-ball of radius \(D>0\). Scale-free FTRL is given by Algorithm 2 with the following choie:

\[_{t}()=}\|\|^{2} _{t}=^{t-1}\|_{s}\|^{2}}}.\]

Then using the clipping operator \(_{D}():=(D/\|\|,1)\), we can write down the update rule more explicitly as follows:

\[_{t}_{D}}{} [}\|\|^{2}+_{s=1}^{t-1} _{s},]=-_{D}(D ^{t-1}_{s}}{^{t-1}\| _{s}\|^{2}}})\;.\;\;\]

Here, if the denominator is zero, _i.e._, \(_{1}==_{t-1}=\), then we set \(_{t} 0\). Note that this algorithm is scale-free in the sense that when the loss sequence is scaled by a scalar \(c>0\), the updates remain the same.

Let us now present the regret bound of scale-free FTRL.

**Lemma 8** (Gradient-adaptive regret bound).: _For any \(T>0\), loss sequence \(_{1:T}\) and comparator \(^{d}\) s.t. \(\|\| D\), scale-free FTRL guarantees the following regret bound:_

\[_{t=1}^{T}_{t},_{t}-  4D^{T}\|_{t}\|^{2}}\,.\]

We note that Lemma 8 follows (with a slightly worse constant) from (Orabona and Pal, 2018, Theorem 1), and the version we invoke is here due to (Ahn et al., 2024, Theorem A.1).

Recall from Lemma 7 that an online learner for the discounted-to-nonconvex conversion (Algorithm 1) needs to have a low discounted regret. To achieve this, following Ahn et al. (2024) and Zhang et al. (2024), we simply substitute \(_{t}^{-t}_{t}\) into scale-free FTRL, resulting in the update

\[_{t}-_{D}(D^{t-1}^ {-s}_{s}}{^{t-1}^{-2s}\|_{s} \|^{2}}})\;.\] ( \[\] -FTRL)

Here again, if the denominator is zero, _i.e._, \(_{1}==_{t-1}=\), then we set \(_{t} 0\). Then, the following result characterizes the discounted regret guarantee of \(\)-FTRL.

**Theorem 9** (**Discounted regret bound)**.: _Let \((0,1]\). For any \(T>0\), loss sequence \(_{1:T}\) and comparator \(^{d}\) s.t. \(\|\| D\), \(\)-FTRL guarantees the following static regret bound_

\[_{T}^{[]}() 4D^{T}^{2(T-t)} \|_{t}\|^{2}}\,.\]

We next use this result to design an algorithm for nonconvex optimization.

Discounted-FTRL leads to adaptive nonconvex optimization

In this section, as a warm-up, let us see the implications of choosing \(=\)-FTRL in Algorithm 1. First, let us obtain a bound on the expected discounted regret. By Theorem 9 together with Jensen's inequality, we have the following regret bound for any \(t=1,2,,T\):

\[[_{t}^{[]}(_{t})] 4D \,\,^{T}^{2(T-t)}\|_{t}\| ^{2}} 4D^{T}^{2(T-t)}\,\|_{t} \|^{2}}\,.\]

Since \(\|_{t}\|^{2} G^{2}+^{2}\) and \(}}}\), it follows that

\[[_{t}^{[]}(_{t})]}}}\,.\] (1)

### From gradient-adaptive regret to nonconvex optimization

In order to obtain nonconvex optimization guarantees in terms of the \((,)\)-stationarity (Definition 3), we need to handle the variance term. Following (Zhang and Cutkosky, 2024, Lemma 3.2), the variance term can be bounded as follows.

**Lemma 10** (Variance bound).: _Using the notations of Lemma 7, for any \(t=1,2,,T\), \(\)-FTRL satisfies_

\[}_{t[T]}\|_{t}-}_{t}\|^{2} 12}{(1-)^{2}}\,.\]

Proof.: From (Zhang and Cutkosky, 2024, Lemma 3.2), it follows that \(_{t=1}^{T}\|_{t}-}_{t} \|^{2}}\,_{t=1}^{T}\| _{t}\|^{2}\). Now since \(\|_{t}\| D\) for all \(t=1,2,,T\), after dividing each side by \(T\), we get the desired inequality. 

Plugging the regret bound (1) into Lemma 7 and combining it with Lemma 10, we arrive at the following optimization guarantee in terms of the \((,)\)-stationarity. See Section B.1 for a proof.

**Theorem 11**.: _Suppose that \(F\) satisfies Assumption 2 and consider any \(>0\). For \(C>0\), choose \(=\)-FTRL in Algorithm 1 with the following parameters:_

\[=1-()^{2},\ \ D=}{4^{1/2}},\ \ \ \ T=\{}{^{3/2}},\ \}\,.\]

_Then we have \(_{t[T]}\| F(}_{t}) \|^{}(1+)\). In other words, a randomly chosen **model EMA**\(}_{t}\) is a \((,(1+))\)-stationary point, in expectation._

### Optimality and gradient adaptivity

Here, we discuss several notable aspects of the guarantee provided in Theorem 11.

#### 5.2.1 Optimality

As shown in (Zhang and Cutkosky, 2024, Corollary 5.1), the lower bound on the iteration complexity for finding a \((,)\)-stationary point is \(((G+)^{2}^{1/2}^{-7/2})\), provided that \(}{^{2}}^{-1}\). Theorem 11 implies that setting \(C=G+\) achieves this optimal iteration complexity.

**Corollary 12**.: _In Theorem 11, choosing \(C=G+\) leads to the following iteration complexity for finding a \((,)\)-stationary point:_

\[O(\{^{1/2}}{^{7/2}}, \ }{^{3}}\})\,.\]

_In particular, treating \(G\), \(\), and \(\) as constants, as long as \(\), this leads to the optimal complexity of \(O((G+)^{2}^{1/2}^{-7/2})\)._In light of Lemma 4, the above optimal complexity can be converted into the optimal complexities for smooth settings.

**Corollary 13** (Smooth settings).: _Corollary 12 implies the following optimal iteration complexity for smooth settings. Choosing \(=O(^{-1})\), it implies the optimal complexity of \(O(^{-4})\) for smooth loss functions . Similarly, with \(=O(1)\), it achieves the optimal iteration complexity of \(O(^{-7/2})\) for second-order smooth loss functions ._

We next discuss the benefits of using the gradient-adaptive regret bound (Theorem 9) by considering the case where we do not have knowledge of \(G,\).

#### 5.2.2 Gradient adaptivity

A remarkable consequence of Theorem 11 is that, due to the gradient-adaptive regret bound of Theorem 9, the final convergence guarantee has a better dependence on \(G,\) in the case when we do not have knowledge of them. For concreteness, in the following discussion, we treat \(G,,\) as constants, and focus on the regime \(\).

First, our Theorem 11 with \(C=1\) (since we do not know \(G,\)) leads to the following iteration complexity for finding a \((,)\)-stationary point:

\[O((G+)^{7/2}^{1/2}^{-7/2})\]

The price we pay for not knowing \(G,\) relative to the lower bound is a multiplicative factor of \((G+)^{3/2}\). To see the benefit of this adaptive regret approach, let us consider the guarantees given by Zhang and Cutkosky (2024). Their approach is based on choosing online gradient descent for \(\) in Algorithm 1, when the learning rate is not properly tuned with the knowledge of \(G\) and \(\), it would lead to the following (suboptimal) discounted regret bound:

\[[_{t}^{[]}(_{t})] O( }{})\,.\]

Then, the resulting iteration complexity becomes \(O(^{1/2}(})^{-7/2})\), which is equal to \(O((G+)^{7}^{1/2}^{-7/2})\). This is larger than the complexity due to our adaptive approach by a multiplicative factor of \((G+)^{7/2}\).

Next, we build on the results from this section and consider a better approach to design an adaptive nonconvex optimizer.

## 6 Coordinate-wise adaptivity via (clipped-)Adam

In this section, we consider the setting where the Lipschitzness constants vary across different coordinates, which is empirically observed to be reflective of practical neural network training (see, _e.g._(Crawshaw et al., 2022; Zhuang et al., 2022)). Formally, we consider the following setting.

**Assumption 14**.: _Under the same setting as Assumption 2, we replace the last two conditions with the following coordinate-wise version:_

* _For each coordinate_ \(i=1,2,,d\)_, there is a Lipschitzness constant_ \(G_{i}>0\) _and a variance constant_ \(_{i}>0\) _such that_ \(\)_,_ \(|_{i}F()| G_{i}\) _and the stochastic gradient_ \((,r)\) _satisfies_ \([[i]]=_{i}F(_{i})\) _and_ \(|[i]-_{i}F()|^{2}_ {i}^{2}\)_. (Here,_ \(_{i}F\) _denotes the partial derivative of_ \(F\) _w.r.t. the_ \(i\)_-th coordinate.)_

_Let \((G_{1},,G_{d})\) and \((_{1},,_{d})\). Then, the above condition implies the last two conditions in Assumption 2 with \(G=\|\|_{2}\) and \(=\|\|_{2}\)._

As we mentioned before, the previous approaches (Cutkosky et al., 2023; Zhang and Cutkosky, 2024) choose the online learner \(\) to be online gradient descent, and hence choosing the learning rate requires the knowledge of \(G_{i},_{i}\) for all \(i\). However, for neural network training, \(d\) is equal to the number of parameters in the network, so tuning them individually is computationally infeasible. We instead consider running \(\)-\(\)**coordinate-wise** in Algorithm 1, which will automatically adapt to each coordinate. We begin with an important observation that such an approach in fact leads to a popular optimizer widely used in practice.

### Coordinate-wise discounted FTRL corresponds to (clipped-)Adam

For notational simplicity, fix a coordinate among \(i=1,2,,d\), and let us denote the iterate by \(x_{t}\), the stochastic gradient by \(g_{t}\), and the update by \(z_{t}\). Then the resulting optimizer becomes:

\[=-_{D}(D^{t}_{1}^{t-s}g_{ s}}{^{t}_{2}^{t-s}g_{s}^{2}}})\,}\] (clipped-Adam)

where \(_{1}=\) and \(_{2}=^{2}\). Here, again if the denominator is zero, _i.e._, if \(g_{1}==g_{t}=0\), then we set the update to be zero, _i.e._, \(z_{t+1}=0\). Note that **this almost exactly the Adam optimizer**(Kingma and Ba, 2014), except that now we add clipping to control the variance of the iterates relative to their EMA. Notably, clipped-Adam retains one of the most important properties of Adam: it is _scale-invariant_. The scale invariance causes the optimizer to make updates of the same magnitude on each coordinate even when the scale differs across different coordinates.

In practice, we expect that the clipping operation will effectively be a no-op. This is because, when the algorithm is converging (even if the convergence is somewhat slow), the gradients are likely to behave as approximately mean-zero random variables (due to factors such as stochastic noise, unstable training trajectories, etc.). In such cases, standard concentration inequalities imply that \(_{s=1}^{t}^{t-s}g_{s}^{t}(^{t-s}g_{s})^{2}}\), and hence, the clipping has no effect.

We also remark that clipped-Adam does not consider the "bias correction" terms in the original updates of Adam (Kingma and Ba, 2014). However, note that the bias correction terms are coordinate-independent, and they can be merged into the scalar \(D\).

### Nonconvex optimization guarantees of clipped-Adam

We next discuss the theoretical guarantees of clipped-Adam for nonconvex and nonsmooth optimizaton. Inspired by (Duchi et al., 2010; McMahan and Streeter, 2010), where the coordinate-wise online learners lead to regret bounds with respect to the \(L_{1}\) norms of stochastic gradients, we consider the following variant of Definition 3, in the same vein as (Cutkosky et al., 2023, Section 4).

**Definition 15** (\((,)\)-\(L_{1}\)-stationary point).: _Suppose \(F:^{d}\) is differentiable. We say \(\) is a \((,)\)-\(L_{1}\)-stationary point of \(F\) if \(\| F()\|_{1}^{[]}\), where_

\[\| F()\|_{1}^{[]}_{p  P(^{d}),\\ _{y p}[]=}\{ \|[ F()]\|_{1}+ \|-\|_{2}^{2}\}\,.\]

Using the fact \(\|\|_{1}\|\|_{2}\), one can connect the two notions of \((,)\)-stationary points.

**Lemma 16**.: _A \((/,/)\)-stationary point is a \((,)\)-\(L_{1}\)-stationary point._

In order to obtain the guarantee in terms of \(L_{1}\)-norm, we consider the coordinate-wise version of discounted-to-online conversion, in the same vein as (Cutkosky et al., 2023, Appendix G). See Section A.1 for details.

**Lemma 17** (\(L_{1}\)-variant of Lemma 7).: _Suppose that \(F\) satisfies Assumption 14. Consider the comparator sequence chosen as \(_{t}\) defined as \(_{t}[i]-D^{t}^{-s}_{i}F( _{s})}{|_{s=1}^{t}^{-s}_{i}F(_{s}) |}\) for \(i=1,2,,d\). Then, Algorithm 1 gives_

\[}_{t[T]}\| }_{_{t}} F(_{t})\|_{1} \ ++\|_{1}}{(1- )T}+\|\|_{1}\] \[+[[_{T}^{[ ]}(_{T})]+(1-)_{t=1}^{T}[ _{t}^{[]}(_{t})]]\,,\]

_where \(_{t}\) is distributed over \(\{_{s}\}_{s=1}^{t}\) as \((_{t}=_{s})=^{t-s}}\) for \(s=1,2,,t\)._Next, let us consider the (expected) regret bound. Fix a coordinate \(i=1,,d\). Then, by the one-dimensional version of Theorem 9 together with Jensen's inequality, we have the following regret bound for any \(t=1,2,,T\):

\[[_{t}^{[]}(_{t}[i])] 4D ^{T}^{2(T-t)}\,|_{t}[i]|^{2 }}+_{i})}{}\,.\]

Hence, taking the sum over all coordinates \(i=1,,d\), we obtain

\[[_{t}^{[]}(_{t})]+\|_{1}}{}\,.\] (2)

Combining these together, one get the following guarantee in terms of the \(L_{1}\) norm. See Section B.2 for a proof.

**Theorem 18**.: _Suppose that \(F\) satisfies Assumption 14 and consider any \(>0\). For \(C>0\), choose the coordinate-wise optimizer clipped-Adam in Algorithm 1 with the following parameters:_

\[=1-()^{2},\;\;D=}{4d^{1/2}^{1/2}},\;\;\;\;T=\{^{1/2}}{^{3/2}}, \;\}\,.\]

_Then we have \(_{t[T]}\,\| F(}_{t}) \|_{1}^{[]}(1++ \|_{1}}{C})\). In other words, a randomly chosen **model EMA**\(}_{t}\) is a \((,(1++\|_{1}}{C}) )\)-\(L_{1}\)-stationary point, in expectation._

### Benefits of coordinate-wise adaptivity of clipped-Adam

In this section, we discuss the benefits of coordinate-wise adaptivity by examining the guarantee from Theorem 18 and compare it with that of Theorem 11. We begin with the \((,)\)-\(L_{1}\)-stationary point guarantee due to Theorem 18. We consider the scenario where \(\) is carefully tuned by making the optimal choice of \(C\).

**Corollary 19**.: _In Theorem 18, choosing \(C=\|+\|_{1}\) leads to the following iteration complexity for finding a \((,)\)-\(L_{1}\)-stationary point:_

\[O(\{+\|_{1 }^{2} d^{1/2}^{1/2}}{^{7/2}},\;+\|_{1}^{3}}{^{3}}\} )\,.\]

In order to better appreciate the benefits of coordinate-wise adaptivity, let us compare the above iteration complexity with that of Theorem 11.

For concreteness, we treat \(G=\|\|_{2}\) and \(=\|\|_{2}\) as constants throughout, and more importantly, we assume that the **coordinates are heterogeneous** in the sense that

\[\|+\|_{1}\| {G}+\|_{2}\,.\] (3)

The assumption (3) roughly says that a few coordinates of \(+\) take much larger values than the rest; if all the coordinates of \(+\) have similar magnitudes, then \(\|+\|_{1}\| +\|_{2}\). In the case \(\), Corollary 19 implies that the iteration complexity is

\[O(\|+\|_{1}^{2} d^{1/2} ^{1/2}^{-7/2})\,.\] (4)

Next, let us consider the counterpart that does not adapt to each coordinate separately. In this case, we apply Lemma 16, which tells us that to find a \((,)\)-\(L_{1}\) stationary point it suffices to find a \((/,/)\)-stationary point. Then, from Corollary 12, the iteration complexity is \(O(\|+\|_{2}^{2}(/ )^{1/2}(/)^{-7/2})\), _i.e._,

\[O(\|+\|_{2}^{2} d^{3/2} ^{1/2}^{-7/2})\,.\] (5)

Hence, when (3) holds, (4) can be lower than (5) by a multiplicative factor of \(d\), showing the benefits of coordinate-wise adaptivity.

Discussion

Our analyses of Adam based on the discounted-to-online conversion is quite different than the previous ones. As discussed in Section 1.1, the previous analyses often result in guarantees that are not quite reflective of practice--_e.g._, the rates get better without momentum and the rates are no better than that of non-adaptive methods. In contrast, our analyses and results highlight the role of the practical components as highlighted below.

* **Momentum.** In order to obtain a low discounted regret, any sensible online learner should integrate the past history of stochastic gradients \(_{1:t}\) to make the decision \(_{t+1}\). Such online learners under the discounted-to-online conversion lead to momentum methods that integrate \(_{1:t}\) to obtain the next increment \(_{t+1}\). In particular, non-momentum methods would correspond to aggressive online learners that only use the last gradient \(_{t}\) to make the decision \(_{t+1}\). This perspective provides new insights into understanding the role of momentum, as echoed by Ahn et al. (2024).
* **Adaptive learning rates.** The adaptive learning rate due to \(\)-\(\) leads to a gradient-adaptive regret bound (Theorem 9), which is important to obtain a better Lipshitzness dependence (Section 5.2) as well as the coordinate-wise adaptivity for high-dimension settings (Section 6.3). Our analysis offers theoretical benefits of adaptive learning rate from a discounted regret perspective.
* **Model EMA.** Lastly, the discounted-to-nonconvex conversion (Algorithm 1) naturally leads to guarantees in terms of the model EMA, \(}_{t}\). At a high level (see Appendix A precise details), this is because for a dynamic environment, it is important to discount the losses such that online learners adapt to changing environments. The appearance of model EMA in the discounted-to-nonconvex conversion provides a new perspective on its role.

Our analyses and results have several limitations and raise several interesting questions. Firstly, clipped-Adam does not precisely match the original Adam algorithm, warranting further investigation into the original Adam update. Specifically, our analysis suggests choosing \(_{1}=\) and \(_{2}=^{2}\), which does not align with the commonly used practical choices. Understanding the exact roles of these practical choices for \(_{1}\) and \(_{2}\) would be valuable.

In Section 5.2, we observed that our iteration complexity for finding a \((,)\)-stationary point is \(O((G+)^{7/2}^{1/2}^{-7/2})\) when \(G\) and \(\) are unknown. Investigating whether this complexity is optimal presents another intriguing direction for future research.

Lastly, from a practical standpoint, developing a more advanced online learner for discounted regret and designing an algorithm that surpasses Adam in practicality would have significant practical implications.

#### Funding Acknowledgments

AC is supported by NSF grant number CCF-2211718.