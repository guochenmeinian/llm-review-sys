# WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks

Jun Xia\({}^{1,}\), Zhihao Yue\({}^{1,}\), Yingbo Zhou\({}^{1}\), Zhiwei Ling\({}^{1}\),

**Yiyu Shi\({}^{2}\), Xian Wei\({}^{1}\), Mingsong Chen\({}^{1}\)1**

###### Abstract

Due to the increasing popularity of Artificial Intelligence (AI), more and more backdoor attacks are designed to mislead Deep Neural Network (DNN) predictions by manipulating training samples or processes. Although backdoor attacks have been investigated in various scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily identified by existing backdoor detection algorithms. To overcome this weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains high-frequency image features through Discrete Wavelet Transform (DWT) to generate highly stealthy backdoor triggers. By introducing an asymmetric frequency obfuscation method, our approach adds an adaptive residual to the training and inference stages to improve the impact of triggers, thus further enhancing the effectiveness of WaveAttack. Comprehensive experimental results show that, WaveAttack can not only achieve higher effectiveness than state-of-the-art backdoor attack methods, but also outperform them in the fidelity of images (i.e., by up to 28.27% improvement in PSNR, 1.61% improvement in SSIM, and 70.59% reduction in IS). Our code is available at https://github.com/BililiCode/WaveAttack.

## 1 Introduction

Along with the prosperity of Artificial Intelligence (AI), Deep Neural Networks (DNNs) have become increasingly prevalent in numerous safety-critical domains for precise perception and real-time control, such as autonomous vehicles , medical diagnosis, and industrial automation . However, the trustworthiness of DNNs faces significant threats due to various notorious adversarial and backdoor attacks. Typically, adversarial attacks [3; 4] manipulate input data during the inference stage to induce incorrect predictions by a trained DNN, while backdoor attacks  tamper with training samples or processes to embed concealed triggers during training, which can be exploited to generate malicious outputs. Although adversarial attacks on DNNs frequently appear in various scenarios, backdoor attacks have attracted more attention because of their stealthiness and effectiveness. Generally, the performance of backdoor attacks can be evaluated by the following three objectives of an adversary: i) _efficacy_ that refers to the effectiveness of an attack in causing the target model to produce incorrect outputs or exhibit unintended behavior; ii) _specificity_ that denotes the precision of the attack in targeting a specific class; and iii) _fidelity_ that represents the degree to which adversarial examples or poisoned training samples are indistinguishable from their benign counterparts . Note that efficacy and specificity represent the effectiveness of backdoor attacks, while fidelity denotes the stealthiness of backdoor attacks.

In order to achieve higher stealthiness and effectiveness, existing backdoor attack methods (e.g. IAD , WaNet , BppAttack , and FTrojan ) are built based on various optimizations, which can be mainly classified into two categories. The former is the _sample minimal impact_ method that can optimize the size of the trigger and minimize its pixel value, making the backdoor trigger difficult to detect in training samples for the purpose of achieving the high stealthiness of a backdoor attacker. Although these methods are promising in backdoor attacks, due to the explicit trigger influence on training samples, they cannot fully evade existing backdoor detection methods based on training samples. The latter is the _latent space obfuscation-based_ methods, which can be integrated into any existing backdoor attack methods. Using asymmetric samples, these methods can obfuscate the latent space between benign samples and poisoned samples . Although these methods can bypass latent space detection techniques, they suffer greatly from low image quality, making them extremely difficult to apply in practice. Therefore, _how to improve both the effectiveness and stealthiness of backdoor attacks while minimally impacting the quality of training samples is becoming a significant challenge in the development of backdoor attacks._

According to the work in , wavelet transform techniques have been widely investigated in various image-processing tasks [13; 14; 15], where high-frequency features can be utilized to enhance the generalization ability of DNNs and remain imperceptible to humans. Inspired by this finding, this paper introduces a novel backdoor attack method named WaveAttack, which adopts Discrete Wavelet Transform (DWT) to extract high-frequency components for highly stealthy backdoor trigger generation. To improve the impact of triggers and further enhance the effectiveness of our approach, we employ _asymmetric frequency obfuscation_ that utilizes an asymmetric coefficient of the trigger in the high-frequency domain during the training and inference stages. This paper makes the following three contributions:

* We introduce a promising frequency-based backdoor trigger generation method, which can effectively generate the backdoor residuals for the high-frequency component based on DWT, thus ensuring the high fidelity of poisoned samples.
* We propose a novel asymmetric frequency-based obfuscation backdoor attack method to enhance the stealthiness and effectiveness of WaveAttack, which can increase stealthiness in latent spaces and improve the Attack Success Rate in training samples.
* We conduct comprehensive experiments on four public benchmarks to demonstrate that WaveAttack outperforms state-of-the-art (SOTA) backdoor attack methods from the perspectives of both stealthiness and effectiveness.

## 2 Related Work

**Backdoor Attack.** Typically, backdoor attacks try to embed backdoors into DNNs by manipulating their input samples and training processes. In this way, adversaries can control DNN output through concealed triggers, which results in manipulated predictions . Depending on whether the training process is manipulated, existing backdoor attacks can be categorized into two types, i.e., _training-umanipulated_ and _training-manipulated_ attacks. Specifically, training-umanipulated attacks only inject a visible or invisible trigger into the training samples of some DNN, leading to its recognition errors . For example, Chen et al.  introduced a Blend attack that generates poisoned data by merging benign training samples with specific key visible triggers. Moreover, there exists a large number of invisible trigger-based backdoor attack methods, such as natural reflection , human imperceptible noise , and image perturbation , which exploit the changes induced by real-world physical environments. Although these training-umanipulated attacks are promising, due to their substantial impacts on training sample quality, most of them still can be easily identified somehow. As an alternative, training-manipulated attacks [8; 9] assume that adversaries from some malicious third party can control the key steps of the training process, thus achieving a stealthier attack. Although the above two categories of backdoor attacks are promising, most of them struggle with coarse-grained optimization of effectiveness and stealthiness, complicating the acquisition of superior backdoor triggers. Due to the significant difference in latent space and low poisoned sample fidelity, they cannot evade the latest backdoor detection methods.

**Backdoor Defense.** There are two major types of backdoor defense methods, i.e., the _detection-based defense_ and _erasure-based defense_. The detection-based defenses can be further classified into two categories, i.e., sample-based and latent space-based detection methods. Specifically, sample based detection methods can identify the differences in the distribution between poisoned samples and benign samples , while latent space-based detection methods aim to find the disparity between the latent spaces of poisoned samples and benign samples . Unlike the detection strategies described above that aim to prevent the injection of backdoors into DNNs by identifying poisoned samples during the training stages, erasure-based defenses can eradicate the backdoors from DNNs. So far, the erasure-based defenses can be classified into three categories, i.e., poison suppression-based, model reconstruction-based, and trigger generation-based defenses. The poison suppression-based methods  utilize the differential learning speed between poisoned and benign samples during training to mitigate the influence of backdoor triggers on DNNs. The model reconstruction-based methods [23; 24] use a selected set of benign data to rebuild DNN models, aiming to mitigate the impact of backdoor triggers. The trigger generation-based methods [25; 26] reverse engineer backdoor triggers by capitalizing on the effects of backdoor attacks on training samples.

To the best of our knowledge, WaveAttack is the first attempt to generate backdoor triggers for the high-frequency component obtained through DWT. Unlike existing backdoor attack methods, WaveAttack first considers both the fidelity of poisoned samples and latent space obfuscation simultaneously. By using asymmetric frequency obfuscation, WaveAttack can not only acquire backdoor attack effectiveness but also achieve high stealthiness regarding both image quality and latent space.

## 3 Our Method

In this section, we first present the preliminaries for the problem notations, threat model, and adversarial goal. Then, we visualize our motivations for adding triggers to the high-frequency components. Finally, we celebrate the attack process of our method, WaveAttack.

### Preliminaries

**Notations.** We follow the training scheme of Adapt-Blend . Let \(=\{(_{i},y_{i})\}_{i=1}^{N}\) be a clean training dataset, where \(_{i}=\{0,1,...,255\}^{C W H}\) is an image, and \(y_{i}=\{1,2,...,K\}\) is its corresponding label. Note that \(K\) represents the number of labels. For a given training dataset, we select a subset of \(\) with a poisoning rate \(p_{a}\) as the _payload samples_\(_{a}=\{(}_{i},y_{t})|}_{i}=T(_{i}),_{i}\}\), where \(T()\) is a backdoor transformation function, and \(y_{t}\) is an adversary-specified target label. We use a subset of \(\) with poisoning rate \(p_{r}\) as the _regularization samples_\(_{r}=\{(}_{i},y_{i})|}_{i}=T( _{i}),_{i}\}\). For a given dataset, a backdoor attack adversary tries to train a backdoored model \(f\) that predicts \(\) as its corresponding label, where \(_{a}_{r}\).

**Threat Model.** Similar to existing backdoor attack methods [7; 8; 9], we assume that adversaries have complete control over the training datasets, and model implementation. They can embed backdoors into the DNNs by poisoning the given training dataset. Moreover, in the inference stage, we assume that adversaries can only query backdoored models using any samples.

**Adversarial Goal.** Throughout the attack process, adversaries strive to achieve two core goals, i.e., effectiveness and stealthiness. Effectiveness indicates that adversaries try to train backdoored models with a high ASR while ensuring that the decrease in Benign Accuracy (BA) remains imperceptible. Stealthiness indicates that samples with triggers have high fidelity and that there is no latent separation between poisoned and clean samples in the latent space.

### Motivation

Unlike humans who are not sensitive to high-frequency features, DNNs can effectively learn high-frequency features of images , which can be used for the generation of backdoor triggers. In other words, the poisoned samples generated by high-frequency features can easily escape various examination methods by humans. Based on this observation, if we can design backdoor triggers on top of high-frequency features, the stealthiness of corresponding backdoored attacks can be ensured. To obtain high-frequency components from the training samples, we resort to Discrete Wavelet Transform (DWT) to capture characteristics from both the time and frequency domains , allowing the extraction of multiple frequency components from the training samples. The reason why we adopt DWT rather than Discrete Cosine Transform (DCT) is that DWT can better capture high-frequency features from training samples (i.e., edges and textures) and allows superior reverse operations during both encoding and decoding phases, thus minimizing the impact on the fidelity of poisoned samples. In our approach, we adopt a classic and effective biorthogonal wavelet transform method (i.e., Haar wavelet ), which mainly contains four kernel operations, i.e., \(LL^{T}\), \(LH^{T}\), \(HL^{T}\), and \(HH^{T}\). Here \(L\) and \(H\) denote the low and high pass filters, respectively, where \(L^{T}=}[1\ \ ],H^{T}=}[-1\ \ ]\). Note that, based on the four operations, the Haar wavelet can decompose an image into four frequency components (i.e., \(LL\), \(LH\), \(HL\), \(HH\)) using DWT, where \(HH\) only contains the high-frequency information of a sample. Meanwhile, the Haar wavelet can reconstruct the image from the four frequency components via the Inverse Discrete Wavelet Transform (IDWT). To verify the motivation of our approach, Figure 1 illustrates the impact of adding the same noises to different frequency components on an image, i.e., Figure 1(a). We can find that, compared to the other three poisoned images, i.e., Figure 1(b) to 1(d), it is much more difficult to determine the difference between the original image and the poisoned counterpart in HH, i.e., Figure 1(e). Therefore, it is more suitable to inject triggers into the high-frequency component (i.e., HH) for backdoor attack purposes.

### Implementation of WaveAttack

In this subsection, we detail the design of our WaveAttack approach. As shown in Figure 2, we give an overview of our attack method WaveAttack. To be concrete, we first make samples poisoned into payload and regularization samples using our trigger design, which is implemented with frequency transformation. Then, we use benign samples, payload samples, and regularization samples to train a classifier to achieve the core goals of WaveAttack.

**Trigger Design.** As mentioned above, our WaveAttack approach aims to achieve a stealthier backdoor attack, introducing triggers into the \(HH\) frequency component. Figure 2 contains the process of generating triggers using WaveAttack. First, we obtain the four components of the samples through DWT. Then, to generate imperceptible sample-specific triggers, we employ an encoder-decoder network as a generator \(g\). These generated triggers are imperceptible additive residuals. Next, to achieve asymmetric frequency obfuscation, we multiply the residuals by a coefficient \(\), and generate the poisoned \(HH^{}\) component with the triggers as follows:

\[}=+ g(;_{g}),\] (1)

where \(_{g}\) is the generator parameters. Finally, we can utilize IDWT to reconstruct four frequency components of poisoned samples. Specifically, we use a U-Net-like  generator to obtain residuals, although other methods (e.g., VAE ) can also be used by the adversary. This is because the skip connections of U-Net can effectively preserve the features of inputs with minimal impacts .

**Optimization Objective.** Our WaveAttack method has two networks to optimize. We aim to optimize a generator \(g\) to generate small residuals with minimal impact on the samples. Furthermore, our objective is to optimize a backdoored classifier \(c\), enabling the effectiveness and stealthiness of WaveAttack. For the first optimization objective, we use the \(L_{}\) norm to optimize small residuals. The optimization objective is defined as follows:

\[_{r}=||g(HH;_{g})||_{}.\] (2)

Figure 1: A motivating example for the backdoor trigger design on high-frequency components.

Figure 2: Overview of our attack method WaveAttack.

For the second optimization objective, we train the classifier using the cross-entropy loss function in \(\), \(_{a}\), and \(_{r}\) dataset. The optimization objective is defined as follows:

\[_{c}=(_{p},y_{t};_{f})+(_{r},;_{c})+(_{b},;_{c}),\] (3)

where \(()\) is the cross-entropy loss function, \(_{f}\) is the classifier parameters, \(_{b},_{p}_{a}\), and \(_{r}_{r}\). The total loss function is as follows:

\[_{total}=_{c}+_{r}.\] (4)

Algorithm Description.Algorithm 1 details the training process of our WaveAttack approach. At the beginning of WaveAttack training (Line \(2\)), the adversary randomly selects a minibatch data \((,)\) from \(\), which has \(b\) training samples. Lines \(4\)-\(6\) calculate the number of poisoned samples, payload samples, and regulation samples, respectively. Lines \(7\)-\(11\) denote the process of modifying samples by injecting triggers into the high-frequency component. After acquiring the modified samples in Line \(7\), Line \(8\) decomposes the samples into four frequency components (i.e., \(LL\), \(LH\), \(HL\) and \(HH\)) by DWT. Then, in Lines \(9\)-\(10\), we add the residual to the frequency component \(HH\) by Equation (1) and obtain the frequency component \(HH^{}\). Line \(11\) reconstructs the samples from the four frequency components via IDWT. Lines \(12\)-\(15\) compute the optimization object using Equations (2) to (4). In Lines \(16\)-\(17\), we can use an optimizer (e.g., SGD optimizer) to update the parameters of the generator model and classifier model. Line \(20\) returns the well-trained generator model parameters \(_{g}\) and the classifier model parameters \(_{}\).

Asymmetric Frequency Obfuscation.According to , regularization samples \(_{r}\) can make DNNs learn the semantic feature of each class and the trigger feature, which can make the backdoor attack stealthy in the latent space. However, using the same trigger in samples during the inference process may diminish the fidelity of poisoned samples. Hence, it is crucial to devise an asymmetric frequency obfuscation method to enhance the effectiveness of backdoor attack methods. In our approach, we employ a coefficient \(\) with a small value (i.e., \(\)=1.0) to improve the stealthiness of triggers during the training process, while a larger value (i.e., \(\)=100.0) is used to enhance the impact of triggers and further improve the effectiveness of WaveAttack. This method ensures that, during the inference process, the backdoored samples have sufficient "power" to activate the DNN backdoor, thus achieving a high ASR.

```
0: i) \(\), benign training dataset. ii) \(_{}\), randomly initialized generator parameters. iii) \(_{c}\), randomly initialized classifier parameters. iv) \(p_{a}\), payload sample rate. v) \(p_{r}\), rate of regularization samples. \(j_{t}\), target label. vi) \(E\), # of epochs in training process.
0:\(_{g}\), well-trained generator model. ii) \(_{c}\), well-trained classifier model.
1:for\(e=1,,E\)do
2:for\((,)\) in \(\)do
3:\(b\)
4:\(m_{}(p_{a}+p_{r}) b\)
5:\(n_{a} p_{a} b\)
6:\(n_{r} p_{r} b\)
7:\(_{m}[:n_{m}]\)
8:\((,,,) DWT(_{m})\)
9:\( g(;_{g})\)
10:\(^{}+\)
11:\(_{m} IDWT(,,,^{})\)
12:\(_{1}(_{m}[n_{a}];y_{t};_{c})\)
13:\(_{2}(_{m}[:n_{a}];y_{t}[n_{a}:n_{t}];_{c})\)
14:\(_{3}([n_{m}];y[n_{m}:];_{c})\)
15:\(_{1}+_{2}+_{3}+||||_{}\)
16:\(\)-backward0
17:update(\(_{g},_{c}\))
18:endfor
19:endfor
20:Return\(_{g},_{}\) ```

**Algorithm 1** Training of WaveAttack

## 4 Experiments

To demonstrate the effectiveness and stealthiness of our approach, we implemented WaveAttack using Pytorch and compared its performance with seven existing backdoor attack methods. We conducted all experiments on a workstation with a 3.6GHz Intel i9 CPU, 32GB of memory, an NVIDIA GeForce RTX3090 GPU, and a Ubuntu operating system. We designed comprehensive experiments to address the following three research questions:

**RQ1 (Effectiveness of WaveAttack)**: Can WaveAttack successfully inject backdoors into DNNs?

**RQ2 (Stealthiness of WaveAttack)**: How stealthy are the poisoned samples generated by WaveAttack compared to those generated by SOTA backdoor attack methods?

**RQ3 (Resistance to Existing Defenses)**: Can WaveAttack resist existing defense methods?

### Experimental Settings

**Datasets and DNNs.** We evaluated all the attack methods on four well-known benchmark datasets, i.e., CIFAR-10 , CIFAR-100 , GTSRB  and a subset of ImageNet (with the first 20 categories) . The statistics of the datasets adopted in the experiments are presented in Table 6 (see Appendix 7.1). We used ResNet18  as the base DNN for the effectiveness and stealthiness evaluation. In addition, we used VGG16 , SENet18 , ResNeXt29 , and DenseNet121  to evaluate the generalizability of WaveAttack.

**Attack Configurations.** To compare the performance of WaveAttack with SOTA attack methods, we considered nine SOTA backdoor attacks, i.e., BadNets , Blend , IAD , WaNet , BppAttack , Adapt-Blend , FTrojan , LIRA , and Fiba . Note that, similar to our work, Adapt-Blend has asymmetric triggers, and FTrojan and Fiba are also frequency domain-based attack methods. We performed the attack methods using the default hyperparameters described in their original papers. Specifically, the poisoning rate is set to 10% with a target label of 0 to ensure a fair comparison. See the Appendix for more details on both data and attack settings.

**Evaluation Metrics.** Similar to the existing work in , we evaluated the effectiveness of all attack methods using two metrics, i.e., Attack Success Rate (ASR) and Benign Accuracy (BA). To evaluate the stealthiness of all attack methods, we used three metrics, i.e., Peak Signal-to-Noise Ratio (PSNR) , Structure Similarity Index Measure (SSIM) , and Inception Score (IS) .

### Effectiveness Evaluation (RQ1)

**Effectiveness Comparison with SOTA Attack Methods.** To evaluate the effectiveness of WaveAttack, we compared the ASR and BA of WaveAttack with nine SOTA attack methods. Since IAD  cannot attack the ImageNet dataset based on its open-source code, we do not provide its comparison result. Table 1 shows the attack performance of different attack methods. From this table, we can find that WaveAttack can acquire a high ASR without obviously degrading the BA. Especially for the datasets CIFAR-10 and GTSRB, our WaveAttack achieves the best ASR and BA compared to other SOTA attack methods. Compared to frequency domain-based attack methods (i.e., FTrojan and Fiba), WaveAttack outperforms FTrojan and Fiba in BA for CIFAR-10, CIFAR-100, GTSRB, and ImageNet datasets. Moreover, compared to the asymmetric-based method Adapt-Blend, WaveAttack can also obtain superior performance in terms of ASR and BA for all datasets.

**Effectiveness on Different Networks.** To evaluate the effectiveness of WaveAttack on various networks, we conducted experiments on CIFAR-10 using different networks (i.e., VGG16 , SENet18 , ResNeXt29 , and DenseNet121 ). Table 2 shows the attack performance of WaveAttack on these networks. From this table, we can find that our WaveAttack approach can successfully embed the backdoor into different networks. WaveAttack can not only cause malicious impacts of backdoor

    &  &  &  &  \\   & BA \(\) & ASR \(\) & BA \(\) & ASR \(\) & BA \(\) & ASR \(\) & BA \(\) & ASR \(\) \\  No attack & 94.59 & - & 75.55 & - & 99.00 & - & 87.00 & - \\  BadNets  & 94.36 & **100** & 74.90 & **100** & 98.97 & **100** & 85.80 & **100** \\ Blend  & 94.51 & 99.91 & 75.10 & 99.84 & 98.26 & **100** & 86.40 & **100** \\ IAD  & 94.32 & 99.12 & 75.14 & 99.28 & 99.26 & 98.37 & - & - \\ WaNet  & 94.23 & 99.57 & 73.18 & 98.52 & 99.21 & 99.58 & **86.60** & 89.20 \\ BppAttack  & 94.10 & **100** & 74.68 & **100** & 98.93 & 99.91 & 85.90 & 99.50 \\ Adapt-Blend  & 94.31 & 71.57 & 74.53 & 81.66 & 98.76 & 60.25 & 86.40 & 90.10 \\ FTrojan  & 94.29 & **100** & 75.37 & **100** & 98.83 & **100** & 85.10 & **100** \\ LIRA  & 93.57 & 99.96 & 73.09 & 99.98 & 10.74 & 99.03 & - & - \\ Fiba  & 93.80 & 75.40 & 74.87 & 80.36 & 99.12 & 85.18 & - & - \\ 
**WaveAttack (Ours)** & **94.55** & **100** & **75.41** & **100** & **99.30** & **100** & **86.60** & **100** \\   

Table 1: Attack performance comparison between WaveAttack and seven SOTA attack methods. The best and the second-best results are **highlighted** and underlined, respectively.

    & No Attack & WaveAttack \\   & BA \(\) & BA \(\) & ASR \(\) \\  VGG16  & 93.62 & 93.70 & 99.76 \\ SENet18  & 94.51 & 94.63 & 100 \\ ResNeXt29  & 94.79 & 95.08 & 100 \\ DenseNet121  & 95.29 & 95.10 & 99.78 \\   

Table 2: Attack performance on different DNNs.

[MISSING_PAGE_FAIL:7]

### Resistance to Existing Defenses (RQ3)

To evaluate the robustness of WaveAttack against existing backdoor defenses, we implemented representative backdoor defenses (i.e., GradCAM , STRIP , Fine-Pruning , ANP  and Neural Cleanse ) and evaluated the resistance to them. We also show the robustness of WaveAttack against Spectral Signature  and other frequency detection methods  in the appendix.

**Resistance to STRIP.** STRIP  is a representative sample-based defense method. When entering a potentially poisoned sample into a model, STRIP will perturb it through a random set of clean samples and monitor the entropy of the prediction output. If the entropy of an input sample is low, STRIP will consider it poisoned. Figure 5 shows the entropies of benign and poisoned samples. From this figure, we can see that the entropies of the poisoned samples are larger than those of the benign samples, and STRIP fails to detect the poisoned samples generated by WaveAttack.

**Resistance to GradCAM**. As an effective visualization mechanism, GradCAM  has been used to visualize intermediate feature maps of DNN, interpreting the predictions of DNN.

    &  &  &  &  \\   & PSNR \(\) & SSIM \(\) & IS \(\) & PSNR \(\) & SSIM \(\) & IS \(\) & PSNR \(\) & SSIM \(\) & IS \(\) & PSNR \(\) & SSIM \(\) & IS \(\) \\  No Attack & INF & 1.0000 & 0.000 & INF & 1.0000 & 0.000 & INF & 1.0000 & 0.000 & INF & 1.0000 & 0.000 \\  BadNets  & 25.77 & 0.9942 & 0.136 & 25.48 & 0.9943 & 0.137 & 25.33 & **0.993** & 0.180 & 21.88 & 0.9678 & 0.025 \\ Blend  & 20.40 & 0.8181 & 2.037 & 0.8031 & 1.600 & 18.58 & 0.6840 & 2.118 & 13.72 & 0.1871 & - \\ IAD  & 24.35 & 0.9180 & 0.472 & 23.98 & 0.9138 & 0.490 & 23.84 & 0.9404 & 0.309 & - & - \\ WaNet  & 39.91 & 0.9724 & 0.326 & 31.62 & 0.9762 & 0.327 & 33.26 & 0.969 & 0.170 & 35.18 & 0.9756 & 0.029 \\ BgpAttack  & 27.79 & 0.9285 & 0.895 & 27.93 & 0.9207 & 0.779 & 27.79 & 0.8462 & 0.714 & 27.34 & 0.8009 & 0.273 \\ Adapt-Blend  & 25.97 & 0.9231 & 0.519 & 26.00 & 0.9133 & 0.495 & 24.14 & 0.8103 & 1.136 & 18.96 & 0.6065 & 1.150 \\ FTFlow  & 44.07 & 0.9976 & 0.019 & 44.09 & 0.9972 & 0.017 & 0.423 & 0.9813 & 0.065 & 35.55 & 0.9440 & 0.013 \\ LIRA  & 46.77 & **0.9979** & 0.019 & 47.77 & **0.9995** & 0.018 & 40.44 & 0.9879 & 0.089 & - & - \\ Friba  & 26.08 & 0.9734 & 0.061 & 26.24 & 0.9688 & 0.055 & 23.41 & 0.9130 & 0.079 & - & - \\ 
**WaveAttack (Ours)** & **47.49** & **0.9979** & **0.011** & **50.12** & 0.9992 & **0.005** & **40.67** & 0.9877 & **0.058** & **45.60** & **0.9913** & **0.007** \\   

Table 4: Stealthiness comparison with existing attacks. Larger PSNR, SSIM, and smaller IS indicate better performance. The best and the second-best results are **highlighted** and underlined, respectively.

Figure 5: STRIP normalized entropy of WaveAttack.

Existing defense methods [50; 51] exploit GradCAM to analyze the heatmap of input samples. Specifically, a clean model correctly predicts the class label, whereas a backdoored model predicts the target label. Based on this phenomenon, the backdoored model can induce an abnormal GradCAM heatmap compared to the clean model. If the heatmaps of poisoned samples are similar to those of benign sample counterparts, the attack method is robust and can withstand defense methods based on GradCAM. Figure 6 shows the visualization heatmaps of a clean model and a backdoored model attacked by WaveAttack. Please note that here "clean" denotes a clean model trained using benign training datasets. From this figure, we can find that the heatmaps of these models are similar and that WaveAttack can resist defense methods based on GradCAM.

**Resistance to Fine-Pruning.** As a representative model reconstruction defense method, Fine-Pruning (FP)  is based on the assumption that the backdoor can activate a few dormant neurons in DNNs. Therefore, pruning these dormant neurons can eliminate the backdoors in DNNs. To evaluate the resistance to FP, we gradually pruned the neurons of the last convolutional and fully connected layers. Figure 7 shows the performance comparison between WaveAttack and seven SOTA attack methods on CIFAR-10 by resisting FP. We find that along with more neurons being pruned, WaveAttack can acquire superior performance than other SOTA attack methods in terms of both ASR and BA. In other words, Fine-Pruning cannot eliminate the backdoor generated by WaveAttack. Note that, though the ASR and BA of WaveAttack are similar to those of Adapt-Blend at the final stage of pruning, the initial ASR (i.e., 71.57%) of Adapt-Blend is much lower than that (i.e., 100%) of WaveAttack.

**Resistance to ANP.** Figure 8 compares the attack performance between WaveAttack and SOTA attack methods on the dataset CIFAR-10 against the defense method, i.e., ANP , where we use the threshold to denote the pruning rate of neurons. We find that as more neurons are pruned, WaveAttack consistently outperforms the other SOTA attack methods in ASR and BA.

**Resistance to Neural Cleanse.** As a representative defense method for trigger generation, Neural Cleanse (NC)  assumes that the trigger designed by the adversary is small. Initially, NC optimizes a trigger pattern for each class label via an optimization process. Then, NC uses the Anomaly Index (i.e., Median Absolute Deviation ) to detect whether a DNN is backdoored. Similar to the work , we think the DNN is backdoored if the anomaly index is larger than 2. To evaluate the resistance to NC, we conducted experiments to evaluate our WaveAttack approach by resisting NC. Figure 9 shows the defense results against NC. Please note that here, "clean" denotes clean models trained by using benign training datasets, and "backdoored" denotes backdoored models by WaveAttack that are from the Subsection 4.2. From this figure, we

Figure 8: Attack performance comparison against ANP.

Figure 6: GradCAM visualization results for both clean and backdoored models.

Figure 7: ASR comparison against Fine-Pruning. Figure 9: Defense performance against NC.

can see that the abnormal index of WaveAttack is smaller than 2 for all datasets, and WaveAttack can bypass NC detection.

**Resistance to Different Frequency Filtering Methods.** From Table 10, we find that WaveAttack outperforms FTrojan in both BA and ASR under two frequency filtering methods. This is mainly because FTrojan only swaps the values of two random pixels of the samples after DCT transformation, while the quality (i.e., PSNR, SSIM, and IS) of training samples after attacks is neglected.

**Resistance to Frequency Detection Methods.** Table 5 compares performance between different attack methods against the same defense method, i.e., the frequency detection method . From this table, we can find that our method achieves a lower BDR than FTrojan, BppAttack, IAD, BadNets, and Blend. Note that, as studied in the experiment section, WaNet, and Adapt-Blend can be more easily detected by the latent space-based and sample-based detection methods, respectively.

**Resistance to Spectral Signature** Spectral Signature  is a representative latent space-based detection defense method. Given a set of benign and poisoned samples, Spectral Signature first collects their latent features and computes the top singular value of the covariance matrix. Then, for each sample, the correlation score is calculated between its features and the top singular value used as the outlier score. If the samples have high outlier scores, they will be evaluated as poisoned. We randomly selected 9000 benign samples and 1000 poisoned samples. Figure 11 shows the histograms of the correlations between latent features of the samples and the top right singular vector of the covariance matrix. From this figure, we can find that the histograms of the poisoned data are similar to those of the benign data. Therefore, Spectral Signature fails to detect the poisoned data generated by WaveAttack.

## 5 Conclusion

Although backdoor attacks on DNNs have attracted increasing attention from adversaries, few of them simultaneously consider both the fidelity of poisoned samples and latent space to enhance the stealthiness of their attack methods. To establish an effective and stealthy backdoor attack against various backdoor detection techniques, this paper proposed a novel frequency-based method called WaveAttack, which employs DWT to extract high-frequency features from samples to generate stealthier backdoor triggers. Furthermore, we introduced an asymmetric frequency obfuscation method to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that, compared with various SOTA backdoor attack methods, WaveAttack not only can achieve higher stealthiness and effectiveness but also can minimize the impact of image quality on well-known datasets.

Figure 11: The correlation with top right singular vector on different datasets.

  
**Method** & **BadNets** & **Blend** & **IAD** & **WaNet** & **BppAttack** & **Adapt-Blend** & **FTrojan** & **WaveAttack** \\  BDR (\%) & 100 & 97.91 & 96.18 & 0.12 & 96.32 & 1.25 & 78.11 & **5.71** \\   

Table 5: Backdoor Detection Rate (BDR) comparison against the frequency detection method.

Figure 10: Performance comparison considering different frequency filtering methods.

## 6 Acknowledgements

This work was supported by the Natural Science Foundation of China (62272170), "Digital Silk Road" Shanghai International Joint Lab of Trustworthy Intelligent Software (22510750100), and Shanghai Trusted Industry Internet Software Collaborative Innovation Center.