# EM Distillation for One-step Diffusion Models

Sirui Xie\({}^{1,2,3}\)   Zhisheng Xiao\({}^{2}\)   Diederik P. Kingma\({}^{1}\)   Tingbo Hou\({}^{2}\)

**Ying Nian Wu\({}^{3}\)   Kevin Murphy\({}^{1}\)   Tim Salimans\({}^{1}\)   Ben Poole\({}^{1}\)   Ruiqi Gao\({}^{1}\)**

\({}^{1}\)Google DeepMind  \({}^{2}\)Google Research  \({}^{3}\)UCLA

###### Abstract

While diffusion models can learn complex distributions, sampling requires a computationally expensive iterative process. Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution. We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality. Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents. We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilize the distillation process. We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL. EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models.

## 1 Introduction

Diffusion models [1; 2; 3] have enabled high-quality generation of images [4; 5; 6], videos [7; 8], and other modalities [9; 10; 11]. Diffusion models use a forward process to create a sequence of distributions that transform the complex data distribution into a Gaussian distribution, and learn the score function for each of these intermediate distributions. Sampling from a diffusion model reverses this forward process to create data from random noise by solving an SDE, or an equivalent probability flow ODE . Typically, solving this differential equation requires a significant number of evaluations of the score function, resulting in a high computational cost. Reducing this cost to single function evaluation would enable applications in real-time generation.

To enable efficient sampling from diffusion models, two distinct approaches have emerged: (1) trajectory distillation methods [13; 14; 15; 16; 17; 18] that accelerate solving the differential equation, and (2) distribution matching approaches [19; 20; 21; 22; 23] that learn implicit generators to match the marginals learned by the diffusion model. Trajectory distillation-based approaches have greatly reduced the number of steps required to produce samples, but continue to face challenges in the 1-step generation regime. Distribution matching approaches can enable the use of arbitrary generators and produce more compelling results in the 1-step regime, but often fail to capture the full distribution due to the _mode-seeking_ nature of the divergences they minimize.

In this paper, we propose EM Distillation (EMD), a diffusion distillation method that minimizes an approximation of the _mode-covering_ divergence between a pre-trained diffusion teacher model and a latent-variable student model. The student enables efficient generation by mapping from noise to data in just one step. To achieve Maximum Likelihood Estimation (MLE) of the marginal teacher distribution for the student, we propose a method similar to the Expectation-Maximization (EM) framework , which alternates between an Expectation-step (E-step) that estimates the learning gradients with Monte Carlo samples, and a Maximization-step (M-step) that updates the studentthrough gradient ascent. As the target distribution is represented by the pre-trained score function, the E-step in the original EM that first samples a datapoint and then infers its implied latent variable would be expensive. We introduce an alternative MCMC sampling scheme that jointly updates the data and latent pairs initialized from student samples, and develop a reparameterized approach that simplifies hyperparameter tuning and improves performance for short-run MCMC . For the optimization in the M-step given these joint samples, we discover a tractable linear noise term in the learning gradient, whose removal significantly reduces variances. Additionally, we identify a connection to Variational Score Distillation [9; 26] and Diff-Instruct , and show how the strength of the MCMC sampling scheme can interpolate between mode-seeking and mode-covering divergences. Empirically, we first demonstrate that a special case of EMD, which is equivalent to the Diff-Instruct  baseline, can be readily scaled and improved to achieve strong performance. We further show that the general formulation of EMD that leverages multi-step MCMC can achieve even more competitive results. For ImageNet-64 and ImageNet-128 conditional generation, EMD outperforms existing one-step generation approaches with FID scores of 2.20 and 6.0. EMD also performs favorably on one-step text-to-image generation by distilling from Stable Diffusion models.

## 2 Preliminary

### Diffusion models and score matching

Diffusion models [1; 2], also known as score-based generative models [27; 3], consist of a forward process that gradually injects noise to the data distribution and a reverse process that progressively denoises the observations to recover the original data distribution. This results in a sequence of noise levels with conditional distributions, whose marginals are. We use a variance-preserving forward process [3; 28; 29] such that. Song et al.  showed that the reverse process can be simulated with a reverse-time Stochastic Differential Equation (SDE) that depends only on the time-dependent score function  of the marginal distribution of the noisy observations. This score function can be estimated by a neural network through (weighted) denoising score matching [30; 31]:

(1)

where is the weighting function and is the noise schedule.

### MCMC with Langevin dynamics

While solving the reverse-time SDE results in a sampling process that traverses noise levels, simulating Langevin dynamics  results in a sampler that converges to and remains at the data manifold of a target distribution. As a particularly useful Markov Chain Monte Carlo (MCMC) sampling method for continuous random variables, Langevin dynamics generate samples from a target distribution by iterating through

(2)

where is the stepsize,, and indexes the sampling timestep. Langevin dynamics has been widely adopted for sampling from diffusion models [27; 3] and energy-based models [33; 34; 35]. Convergence of Langevin dynamics requires a large number of sampling steps, especially for high-dimensional data. In practice, short-run variants with early termination have been succesfully used for learning of EBMs [25; 37; 38].

### Maximum Likelihood and Expectation-Maximization

Expectation-Maximization (EM)  is a maximum likelihood estimation framework to learn latent variable models:, such that the marginal distribution approximates the target distribution. It originates from the generic training objective of _maximizing_ the log-likelihood function over parameters:, which is equivalent to _minimizing_ the _forward_ KL divergence . Since the marginal distribution is usually analytically intractable, EM involves an E-step that expresses the gradients over the model parameters with an expectation formula

(3)where \(p_{}(|)=}(|)p ()}{p_{}()}\) is the posterior distribution of \(\) given \(\). See Appendix A for a detailed derivation. The expectation can be approximated by Monte Carlo samples drawn from the posterior using e.g. MCMC sampling techniques. The estimated gradients are then used in an M-step to optimize the parameters. Han et al.  learned generator networks with an instantiation of this EM framework where E-steps leverage Langevin dynamics for drawing samples.

### Variational Score Distillation and Diff-Instruct

Our method is also closely related to Score Distillation Sampling (SDS) , Variational Score Distillation (VSD)  and Diff-Instruct , which have been used for distilling diffusion models into a single-step generator [23; 41]. The generator produces clean images \(_{0}=g_{}()\) with \(p()=(,)\), and can be diffused to noise level \(t\) to form a latent variable model \(p_{,t}(_{t},)=p_{,t}(_{t }|)p()\), \(p_{,t}(_{t}|)=(_{t}g_{}(),_{t}^{2})\). This model is trained to match the marginal distributions \(p_{,t}(_{t})\) and \(q_{t}(_{t})\) by minimizing their _reverse_ KL divergence. Integrating over all noise levels, the objective is to _minimize_\(()\) where

\[()=_{p(t)}[(t)D_{}(p_{ {},t}(_{t})||q_{t}(_{t}))]=_{p(t)}[ (t) p_{,t}(_{t}),t}( _{t})}{q_{t}(_{t})}d_{t}].\] (4)

When parametrizing \(_{t}=_{t}g_{}()+_{t}\), the gradient for this objective in Eq. (4) can be written as

\[_{}()=_{p(t),p( ),p()}[-(t)(_{t}} q_{t} (_{t})}_{}-_{t}}  p_{,t}(_{t})}_{(_{t},t)$}})_{t}_{}g_{}( )],\] (5)

where \(p()=(,)\), the teacher score is provided by the pre-trained diffusion model. In SDS, \(_{_{t}} p_{,t}(_{t})\) is the known analytic score function of the Gaussian generator. In VSD and Diff-Instruct, an auxiliary score network \(s_{}(_{t},t)\) is learned to estimate it. The training alternates between learning the generator network \(g_{}\) with the gradient update in Eq. (5) and learning the score network \(s_{}\) with the denoising score matching loss in Eq. (1).

## 3 Method

### EM Distillation

We consider formulating the problem of distilling a pre-trained diffusion model to a deep latent-variable model \(p_{,t}(_{t},)\) defined in Section 2.4 using the EM framework introduced in Section 2.3. For simplicity, we begin with discussing the framework at a single noise level and drop the subscript \(t\). We will revisit the integration over all noise levels in Section 3.3. Assume the target distribution \(q()\) is represented by the diffusion model where we can access the score function \(_{} q()\). Theoretically speaking, the generator network \(g_{}()\) can employ any architecture including ones where the dimensionality of the latents differs from the data dimensionality. In this work, we reuse the diffusion denoiser parameterization as in other work on one-step distillation: \(g_{}()=}_{}(,t^{*})\), where \(}_{}\) is the \(\)-prediction function inherited from the teacher diffusion model, and \(t^{*}\) remains a hyper-parameter.

Figure 1: **Before and after MCMC correction.** In (a)(b), the left columns are \(=g_{}()\), the right columns are updated \(\) after 300 steps of MCMC sampling jointly on \(\) and \(\). (a) illustrates the effect of correction in ImageNet. Note that the off-manifold images are corrected. (b) illustrates the correction in the embedding space of Stable Diffusion v1.5, which are decoded to image space in (c). Note the disentanglement of the cats and sharpness of the sofa. Zoom in for better viewing.

A naive implementation of the E-step involves two steps: (1) draw samples from the target diffusion model \(q()\) and (2) sample the latent variable \(\) from \(p_{}(|)\) with _e.g._ MCMC techniques. Both steps can be highly non-trivial and computationally expensive, so here we present an alternative approach to sampling the same target distribution that avoids directly sampling from the pretrained diffusion model, by instead running MCMC from the joint distribution of \((,)\). We initialize this sampling process using a joint sample from the student: drawing \( p()\) and \( p_{}(|)\). This sampled \(\) is no longer drawn from \(q()\), but \(\) is guaranteed to be a valid sample from the posterior \(p_{}(|)\). We then run MCMC to correct the sampled pair towards the desired distribution: \(_{}(,):=q()p_{}( |)=p_{}(,))}{p_{}()}\) (see Fig. 1 for a visualization of this process). If \(q()\) and \(p_{}()\) are close to each other, \(_{}(,)\) is close to \(p_{}(,)\). In that case, initializing the _joint_ sampling of \(_{}(,)\) with pairs of \((,)\) from \(p_{}(,)\) could significantly accelerate both sampling of \(\) and inference of \(\). Assuming MCMC converges, we can use the resulting samples to estimate the learning gradients for EM:

\[_{}()=_{_{}( ,)}[_{} p_{}( ,)]=_{_{}(, )}[-}\|- g_{}()\|_{2}^{2}}{2^{2}}].\] (6)

We abbreviate our method as EMD hereafter. To successfully learn the student network with EMD, we need to identify efficient approaches to sample from \(_{}(,)\).

### Reparametrized sampling and noise cancellation

As an initial strategy, we consider Langevin dynamics which only requires the score functions:

\[_{}_{}(,) =} q()}_{}-} p_{}()}_{ \ s_{}()}+} p_{ }(|)}_{-- g_{ }()}{^{2}}}\] (7) \[_{}_{}(,) =_{} p_{}(|)+ _{} p_{}()=-- g _{}()}{^{2}}_{}g_{}()-.\]

While we do not have access to the score of the student, \(_{} p_{}()\), we can approximate it with a learned score network \(s_{}\) estimated with denoising score matching as in VSD  and Diff-Instruct . As will be covered in Section 3.3, this score network is estimated at all noise levels. The Langevin dynamics defined in Eq. (7) can therefore be simulated at any noise level.

Running Langevin MCMC is expensive and requires careful tuning, and we found this challenging in the context of diffusion model distillation where different noise levels have different optimal step sizes. We leverage a reparametrization of \(\) and \(\) to accelerate the joint MCMC sampling and simplify step size tuning, similar to Nijkamp et al. , Xiao et al. . Specifically, the parametrization \(= g_{}()+\) defines a deterministic transformation from the pair of \((,)\) to the pair of \((,)\), which enables us to push back the joint distribution \(_{}(,)\) to the \((,)\)-space. The reparameterized distribution is

\[_{}(,)=}( )+)}{p_{}( g_{}( )+)}p()p().\] (8)

The score functions become

\[_{}(, )&=(_{} q()-_{ } p_{}())-,\\ _{}(,)& =(_{} q()-_{} p_{}())_{}g_{}()-. \] (9)

``` Input: Teacher score functions \(_{_{t}} q_{t}(_{t})\), generator network \(g_{}\), prior \(p()\), score network \(s_{}\), noise scheduler \(p(t)\), weighting functions \(w(t)\) and \((t)\), # of MCMC steps \(K\), MCMC step size \(\). Output: Generator network \(g_{}\), score network \(s_{}\). while not converged do  Sampling a batch of \(t\), \(\), \(\) from \(p(t)\), \(p()\), \((,)\) to obtain \(_{t}\)  Updating \(s_{}\) via Stochastic Gradient Descent with the batch estimate of Eq. (12)  Sampling \(_{t}^{K}\) and \(^{K}\) with \((,)\)-corrector\((_{0},,,t,_{_{t}} q_{t}( _{t}),g_{},s_{},K,)\)  Updating \(g_{}\) via Stochastic Gradient Ascent with the batch estimate of Eq. (11) endwhile ```

**Algorithm 1**EM DistillationSee Appendix B for a detailed derivation. We found that this parameterization admits the same step sizes across noise levels and results in better performance empirically (Table 1).

Still, learning the student with these samples continued to present challenges. When visualizing samples \(\) produced by MCMC (see Fig. 1(a)), we found that samples contained substantial noise. While this makes sense given the level of noise in the marginal distributions, we found that this inhibited learning of the student. We identify that, due to the structure of Langevin dynamics, there is noise added to \(\) at each step that can be linearly accumulated across iterations. By removing this accumulated noise along with the temporally decayed initial \(\), we recover cleaner \(\) samples (Fig. 1(b)). Since \(\) is effectively a regression target in Eq. (6), and the expected value of the noises is \(\), canceling these noises reduces variance of the gradient without introducing bias. Empirically, we find bookkeeping the sampled noises in the MCMC chain and canceling these noises after the loop significantly stabilize the training of the generator network. This noise cancellation was critical to the success of EMD, and is detailed in Appendix B and ablated in experiments (Fig. 2(b)).

### Maximum Likelihood across all noise levels

The derivation above assumes smoothing the data distribution with a single noise level. In practice, the diffusion teachers always employ multiple noise levels \(t\), coordinated by a noise schedule \(p(t)\). Therefore, we optimize a weighted loss over all noise levels of the diffusion model, to encourage that the marginals of the student network match the marginals of the diffusion process at all noise levels:

\[_{}()=_{ }}_{p(t),q_{t}(_{t})} [(t) p_{,t}(_{t})]= }_{p(t),_{t}(_{t},)}[ (t)_{} p_{,t}( _{t},)],\] (10)

where \(p_{,t}(_{t},)\) are a series of latent-variable models as defined in Section 2.4, with a shared generator \(g_{}()\) across all noise levels. Empirically, we find \((t)=_{t}^{2}/_{t}\) or \((t)=_{t}^{2}/_{t}^{2}\) perform well.

Denote the resulted distribution after \(K\) steps of MCMC sampling with noise cancellation as \(_{t}^{K}(_{t}^{K},^{K})\), the final gradient for the generator network \(g_{}\) is

\[_{}()=}_{p(t),_{t}^{K}(_{t}^{K},^{K})}[- (t)}\|_{t}^{K}-_{t} g_{}(^{K})\|_{2}^{2}}{2_{t}^{2}}].\] (11)

The final gradient for the score network \(s_{}(_{t},t)\) is

\[_{}()=}_{p(t),p_{,t}(_{t},)}[ w(t)_{}\|s_{}(_{t},t)-_{ _{t}} p_{t}(_{t}|g_{}() )\|_{2}^{2}].\] (12)

Similar to VSD [26; 22], we employ alternating update for the generator network \(g_{}\) and the score network \(s_{}(_{t},t)\). See summarization in Algorithm 1.

Figure 2: Images after 8-step Langevin updates with and without accumulated noise.

### Connection with VSD and Diff-Instruct

In this subsection, we reveal an interesting connection between EMD and Variational Score Distillation (VSD) [26; 22], _i.e._, although motivated by optimizing different types of divergences, VSD [26; 22] is equivalent to EMD with a special sampling scheme.

To see this, consider the 1-step EMD with noise cancellation, stepsize \(=1\) in \(\), and no update on \(\)

\[_{t}^{0}=_{t}g_{}()+_{t} ,_{t}^{1}=_{t}g_{}( )+_{t}^{2}_{}_{t}^{0})}{p_{ ,t}(_{t}^{0})}[t]{2^{}}.\] (13)

Substitute it into Eq. (11), we have

\[_{}() =_{p(t),p(),p()}[- (t)}\|_{t}^{1}-_{t}g _{}()\|_{2}^{2}}{2_{t}^{2}}]\] (14) \[=_{p(t),p(),p()}[ (t)(_{_{t}} q_{t}(_{t})-_{ _{t}} p_{,t}(_{t}))_{t} _{}g_{}()],\]

which is exactly the gradient for VSD (Eq. (5)), up to a sign difference. This insight demonstrates that, EMD framework can flexibly interpolate between mode-seeking and mode-covering divergences, by leveraging different sampling schemes from 1-step sampling in only \(\) (a likely biased sampler) to many-step joint sampling in \((,)\) (closer to a mixing sampler). Notably, for image generation, some believe that _forward_ KL divergence may fail to achieve better fidelity compared to _reverse_ KL divergence. The interpolation enabled by EMD can thus be very useful in practice.

If we further assume the marginal \(p_{}()\) is a Gaussian, then EMD update in Eq. 14 would resemble Score Distillation Sampling (SDS) .

## 4 Related Work

**Diffusion acceleration.** Diffusion models have the notable issue of slowness in inference, which motivates many research efforts to accelerate the sampling process. One line of work focuses on developing numerical solvers [43; 44; 12; 45; 46; 47] for the PF-ODE. Another line of work leverages the concept of knowledge distillation  to condense the sampling trajectory of PF-ODE into fewer steps [13; 49; 15; 50; 51; 18; 52; 53; 54; 55]. However, both approaches have significant limitations and have difficulty in substantially reducing the sampling steps to the single-step regime without significant loss in perceptual quality.

**Single-step diffusion models.** Recently, several methods for one-step diffusion sampling have been proposed, sharing the same goal as our approach. Some methods fine-tune the pre-trained diffusion model into a single-step generator via adversarial training [20; 21; 56], where the adversarial loss enhances the sharpness of the diffusion model's single-step output. Adversarial training can also be combined with trajectory distillation techniques to improve performance in few or single-step regimes [52; 57; 58]. Score distillation techniques [9; 26] have been adopted to match the distribution of the one-step generator's output with that of the teacher diffusion model, enabling single-step generation [41; 22]. Additionally, Yin et al.  introduces a regression loss to further enhance performance. These methods achieve more impressive 1-step generation, some of which enjoy additional merits of being data-free or flexible in the selection of generator architecture. However, they often minimizes over mode-seeking divergences that can fail to capture the full distribution and therefore causes mode collapse issues. We discuss the connection between our method and this line of work in Section 3.4. Concurrent with our work, Zhou et al.  adopt Fisher divergence as the distillation objective and propose a novel decomposition that alleviates the dependency on the approximation accuracy of the auxiliary score network. Although the adopted Fisher divergence is similar to _reverse_ KL in terms of the reparametrization and hence the risk of mode collapse, Zhou et al.  demonstrate impressive performance gain.

## 5 Experiments

We employ EMD to learn one-step image generators on ImageNet 64\(\)64, ImageNet 128\(\)128  and text-to-image generation. The student generators are initialized with the teacher model weights. Results are compared according to Frechet Inception Distance (FID) , Inception Score (IS) ,Recall (Rec.)  and CLIP Score . Throughout this section, we will refer to the proposed EMD with \(K\) steps of Langevin updates on \((,)\) as EMD-\(K\), and we use EMD-1 to describe the DiffInstruct/VSD-equivalent formulation with only one update in \(\) as presented in Section 3.4.

### ImageNet

We start from showcasing the effect of the key components of EMD, namely noise cancellation, multi-step joint sampling, and reparametrized sampling. We then summarize results on ImageNet 64\(\)64 with Karras et al.  as teacher, and ImageNet 128\(\)128 with Kingma and Gao  as teacher.

Noise cancellationDuring our development, we observed the vital importance of canceling the noise after the Langevin update. Even though theoretically speaking our noise cancellation technique does not guarantee reducing the variance of the gradients for learning, we find removing the accumulated noise term from the samples (including the initial diffusion noise \(\)) does give us seemingly clean images empirically. See Fig. 2 for a comparison. These updated \(^{K}\) can be seen as regression targets in Eq. (11). Intuitively speaking, regressing a generator towards clean images should result in more stable training than towards noisy images. Reflected in the training process, canceling the noise significantly decreases the variance in the gradient (Fig. 2(a)) and boosts the speed of convergence (Fig. 2(b)). We also compare with another setting where only the noise in the last step gets canceled, which is only marginally helpful.

Multi-step joint samplingWe scrutinize the effect of multi-step joint update on \((,)\). Empirically, we find a constant step size of Langevin dynamics across all noise levels in the \((,)\)-space works well: \(=(_{},_{})=(0.4 ^{2},0.004^{2})\), which simplifies the process of step size tuning. Fig. 1 shows results of running this \((,)\)-corrector for 300 steps. We can see that the \((,)\)-corrector removes visual artifacts and improves structure. Fig. 2(c)d illustrates the relation between the distilled generator's performance and the number of Langevin steps per distillation iteration, measured by FID and Recall respectively. Both metrics show clear improvement monotonically as the number of Langevin steps increases. Recall is designed for measuring mode coverage , and has been widely adopted in the GAN literature. A larger number of Langevin steps encourages better mode coverage, likely because it approximates the mode-covering _forward_ KL better. Sampling \(\) is more expensive than sampling \(\), requiring back-propagation through the generator \(g_{}\). An alternative is to only sample \(\) while keeping \(\) fixed, with the hope that if \(\) does not change dramatically with a finite number of MCMC updates, the initial \(\) remains a good approximation of samples from \(_{}(|)\). As shown in Fig. 2(d)d, sampling \(\) performs similarly to the joint sampling of \((,)\) when the number of sampling steps is small, but starts to fall behind with more sampling steps.

Reparametrized samplingAs shown in Appendix B, the noise cancellation technique does not depend on the reparametrization. One can start from either the score functions of \((,)\) in Eq. (7) or the score functions of \((,)\) in Eq. (9) to derive something similar. We conduct a comparison between the two parameterizations for joint sampling, \((,)\)-corrector and \((,)\)-corrector.

For the \((,)\)-corrector, we set the step size of \(\) as \(_{}^{2}_{}\) to align the magnitude of update with the one of the \((,)\)-corrector, and keep

    & FID (\(\)) & IS (\(\)) \\  \((,)/(,)\) & 2.829 & 62.31 \\ \((,)\) & 3.11 & 61.08 \\ \((,)\) & **2.77** & **62.98** \\   

Table 1: EMD-8 on ImageNet 64\(\)64, 100k steps of training

Figure 3: (a)(b) Gradient norms and FIDs for complete noise cancellation, last-step noise cancellation and no noise cancellation. (c)(d) FIDs and Recalls of EMD with different numbers of Langevin steps.

[MISSING_PAGE_FAIL:8]

### Text-to-image generation

We further test the potential of EMD on text-to-image models at scale by distilling the Stable Diffusion v1.5  model. Note that the training is image-free and we only use text prompts from the LAION-Aesthetics-6.25+ dataset . On this task, DMD  is a strong baseline, which introduced an additional regression loss to VSD or Diff-Instruct to avoid mode collapse. However, we find the baseline without regression loss, or equivalently EMD-1, can be improved by simply tuning the hyperparameter \(t^{*}\). Empirically, we find it is better to set \(t^{*}\) to intermediate noise levels, consistent with the observation from Luo et al. . In Appendix C.4 we discuss the selection of \(t^{*}\). The intuition is that by choosing the value of \(t^{*}\), we choose a specific denoiser at that noise level for initialization. Other hyperparameters can be found in Appendix C.3.

We evaluate the distilled one-step generator for text-to-image generation with zero-shot generalization on MSCOCO  and report the FID-30k in Table 4 and CLIP Score in Table 5. Yin et al.  uses the guidance scale of 3.0 to compose the classifier-free guided teacher score (we refer to this guidance scale of teacher as tCFG) in the learning gradient of DMD, for it achieves the best FID for DDIM sampler. However, we find EMD achieves a lower FID at the tCFG of 2.0. Our method, EMD-8, trained on 256 TPU-v5e for 5 hours (5000 steps), achieves the FID=9.66 for one-step text-to-image generation. Using a higher tCFG, similar to DMD, produces a model with competitive CLIP Score. In Fig. 5, we include some samples for qualitative evaluation. Additional qualitative results (Tables 14 and 15), as well as side-by-side comparisons (Tables 10 to 13) with trajectory-based distillation baselines  and adversarial distillation baselines  can be found in Appendix D.2.

Figure 4: ImageNet samples from the distilled 1-step generator. Models are trained class-conditionally with all classes. We provide single-class samples in (c) to demonstrate good mode coverage.

Figure 5: Text-to-image samples from the 1-step student model distilled from Stable Diffusion v1.5.

### Computation overhead in training

Despite EMD being more expensive per training iteration compared to the baseline approach Diff-Instruct, we find the performance gain of EMD cannot be realized by simply running Diff-Instruct for the same amount of time or even longer than EMD. In fact, the additional computational cost that EMD introduced is moderate even with the most expensive EMD-16 setting. In Table 6 we report some quantitative measurement of the computation overhead. Since it is challenging to time each python method's wall-clock time in our infrastructure, we instead logged the sec/step for experiments with various algorithmic ablations on ImageNet 64\(\)64. EMD-16 only doubles the wall-clock time of Diff-Instruct when taking all other overheads into account.

## 6 Discussion and limitation

We present EMD, a maximum likelihood-based method that leverages EM framework with novel sampling and optimization techniques to learn a one-step student model whose marginal distributions match the marginals of a pretrained diffusion model. EMD demonstrates strong performance in class-conditional generation on ImageNet and text-to-image generation. Despite exhibiting compelling results, EMD has a few limitations that call for future work. Empirically, we find that EMD still requires the student model to be initialized from the teacher model to perform competitively, and is sensitive to the choice of \(t^{*}\) (fixed timestep conditioning that repurposes the diffusion denoiser to become a one-step generator) at initialization. While training a student model entirely from scratch is supported theoretically by our framework, empirically we were unable to achieve competitive results. Improving methods to enable generation from randomly initialized generator networks with distinct architectures and lower-dimensional latent variables is an exciting direction of future work. Although being efficient in inference, EMD introduces additional computational cost in training by running multiple sampling steps per iteration, and the step size of MCMC sampling can require careful tuning. There remains a fundamental trade-off between training cost and model performance. Analysis and further improving on the Pareto frontier of this trade-off would be interesting for future work.