# GuardT2I: Defending Text-to-Image Models

from Adversarial Prompts

 Yijun Yang\({}^{1,2}\), Ruiyuan Gao\({}^{1}\), Xiao Yang\({}^{2}\), Jianyuan Zhong\({}^{1}\), Qiang Xu\({}^{1}\)

\({}^{1}\)The Chinese University of Hong Kong, \({}^{2}\)Tsinghua University

{yjyang,rygao,jyzhong,qxu}@cse.cuhk.edu.hk,{yangyj16,yangxiao19}@tsinghua.org.cn

This work was carried out as part of Yijun Yang's internship at Tsinghua University.Corresponding authors.

###### Abstract

Recent advancements in Text-to-Image models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance Text-to-Image models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a large language model to conditionally transform text guidance embeddings within the Text-to-Image models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. Our framework is available at [https://github.com/cure-lab/GuardT2I](https://github.com/cure-lab/GuardT2I).

## 1 Introduction

The recent advancements in Text-to-Image (T2I) models, such as Midjourney, Leonardo.Ai, DALL-E 3, and others, have significantly facilitated the generation of high-quality images from textual prompts, as demonstrated in Fig. 1 (a). As the widespread application of T2I models continues, concerns about their misuse have become increasingly prominent . In response, T2I service providers have implemented defensive strategies. However, sophisticated adversarial prompts that appear innocuous to humans can manipulate these models to produce explicit Not-Safe-for-Work (NSFW) content, such as pornography, violence, and political sensitivity , raising significant safety challenges, as illustrated in Fig. 1 (b).

Existing defensive methods for T2I models can be broadly classified into two categories: _training interference_ and post-hoc content moderation. _Training interference_ focuses on removing inappropriate concepts during the training process through techniques like dataset filtering  or fine-tuning to forget NSFW concepts . While effective in suppressing NSFW generation, these methods often compromise image quality in normal use cases and remain vulnerable to adversarial attacks . On the other hand, _post-hoc content moderation methods_, such as OpenAI-Moderation and Safety-Checker, maintain the synthesis quality therefore being widely used in T2I services . These methods rely on text or image classifiers to identify and block malicious prompts or generated content. However, they struggle to effectively defend against adversarial prompts, as reported in .

In this paper, we introduce a new defensive framework called GuardT2I, specifically designed to protect T2I models from adversarial prompts. Our key observation is that although adversarial prompts (as shown in Fig. 1 (b)) may have noticeable visual differences compared to explicitprompts, they still contain the same underlying semantic information within the T2I model's latent space. Therefore, we approach the defense against adversarial prompts as a generative task and harness the power of the large language model (LLM) to effectively handle the semantic meaning embedded in implicit adversarial prompts. Specifically, we modify LLM to a conditional LLM, c-LLM, and fine-tune the c-LLM to "translate" the latent representation of prompts back to plain texts, which can reveal the real intention of the user. For legitimate prompts, as shown in Fig. 1 (c), GuardT2I tries to reconstruct the input prompt, as shown in Fig. 1 (c)'s _Prompt Interpretation_. For adversarial prompts, instead of reconstructing the input prompt, GuardT2I would generate the prompt interpretation conform to the underlying semantic meaning of the adversarial prompt whenever possible, as demonstrated in Fig. 1 (d). Consequently, by estimating the similarity between the input and the synthetic prompt interpretation, we can identify adversarial prompts. GuardT2I accomplishes defense without altering the original T2I models. This ensures that the performance and generation qualities of the T2I models remain intact. Additionally, GuardT2I operates in parallel with the T2I models, thereby imposing no additional inference latency during normal usage. Moreover, GuardT2I has the capability to halt the diffusion steps of malicious prompts at an early stage, which helps to reduce computational costs.

Overall, the **contributions** of this work include:

* To the best of our knowledge, GuardT2I is the first generative paradigm defensive framework specifically designed for T2I models. Through the transformation of latent variables from T2I models into natural language, our defensive framework not only demonstrates exceptional generalizability across various adversarial prompts, but also provide decision-making interpretation.
* We propose a conditional LLM (_c-LLM_) to "translate" the latent back to plain text, coupled with bi-level parsing methods for prompt moderation.
* We perform extensive evaluations for GuardT2I against various malicious attacks, including rigorous adaptive attacks, where attackers have full knowledge of GuardT2I and try to deceive it for NSFW syntheses.

Experimental results demonstrate that GuardT2I outperforms baselines, such as Microsoft Azure , Amazon AWS Comprehend , and OpenAI-Moderation , by a large margin, particularly when facing adaptive attacks. Furthermore, our in-depth analysis reveals that the adaptive adversarial prompts that can bypass GuardT2I tend to have much-weakened synthesis quality.

## 2 Related Work

### Adversarial Prompts

Diffusion-based T2I models, trained on extensive internet-sourced datasets, are adept at producing vibrant and creative imagery . However, the lack of curation in these datasets leads to

Figure 1: **Overview of GuardT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.**

generations of NSFW content by the models [38; 27]. Such content may encompass depictions of _violence_, _pornography_, _bullying_, _gore_, _political sensitivity_, _racism_. Currently, such risk mainly comes from two types of adversarial prompts, _i.e_., manually and automatically generated ones.

**Manually Crafted Attacking Prompts.** Schramowski _et al._ amass a collection of handwritten adversarial prompts, referred to as _I2P_, from various online communities. These prompts not only lead to the generation of NSFW content but also eschew explicit NSFW keywords. Furthermore, Rando _et al._ reverse-engineer the safety filters of a popular T2I model, Stable Diffusion . By adding extraneous text, which effectively deceived the model's safety mechanisms.

**Automatically Generated Adversarial Prompts**. Researchers propose adversarial attack algorithms to automatically construct adversarial prompts for T2I models to induce NSFW contents [38; 45; 46; 41] or functionally disable the T2I models . For instance, by considering the existence of safety prompt filters, SneakyPrompt  "jailbreak" T2I models for NSFW images with reinforcement learning strategies. MMA-Diffusion  presents a gradient-based attacking method, and showcases current defensive measures in commercial T2I services, such as Midjourney  and Leonardo.Ai , can be bypassed in the black-box attack way.

### Defensive Methods

**Model Fine-tuning** techniques target at developing harmless T2I models. Typically, they involve concept-erasing solutions [12; 15; 38], which change the weights of existing T2I models [12; 15] or the inference guidance [12; 38] to eliminate the generation capability of inappropriate content. Although their concepts are meaningful, currently, their methods are not practical. For one thing, the deleterious effects they are capable of mitigating are not comprehensive, because they can only eliminate harmful content that has clear definitions or is exemplified by enough images, and their methods lack scalability. For another, their methods inadvertently affect the quality of benign image generation [48; 16; 38]. Due to these drawbacks, current T2I online services [6; 3] and open-sourced models [33; 26] seldom consider this kind of method.

**Post-hoc Content Moderators** refer to content moderators applied on top of T2I systems. The moderation can be applied to _images_ or _prompts_. _Image-based moderators_, like safety checkers in SD [1; 30], operate on the syntheses to detect and censor NSFW elements. They suffer from significant inference costs because they take the output from T2I models as input. _Prompt-based moderators_ refer to prompt filters to prevent the generation of harmful content. Due to its lower cost and higher accuracy compared to image-based ones, currently, these technologies are extensively employed by online services, such as Midjourney  and Leonardo.Ai . More examples in this category include OpenAI's Moderation API , Detoxify  and NSFW-Text-Classifier .

Note that most existing content moderators treat content moderation as a classification task, which necessitates extensive amounts of meticulously labeled data and operate in a black-box manner . Therefore, they fail to adapt to unseen/customized NSFW concepts, as summarized in Tab. 1 and lack interpretability of the decision-making process, not to mention advanced adversarial prompt threats [45; 46; 38]. By contrast, in this paper, we take a generative perspective to build GuardT2I, which is more generalizable to various NSFW content and provides interpretation.

## 3 Method

Overview.As illustrated in Fig. 2 (a), T2I models rely on a text encoder, \(()\), to convert a user's prompt \(\) into a guidance embedding \(\), defined by \(=()^{d}\). This embedding effectively dictates the semantic content of the image produced by the diffusion model . We have observed that an adversarial prompt, denoted as \(_{}\), which may appear benign or nonsensical to humans, can contain the same underlying semantic information within the T2I model's latent space as an explicit prompt does, leading the diffusion model to generate NSFW content.

This observation has motivated us to introduce the concept of _Prompt Interpretation_ (see Fig. 2 (b)) in order to convert the implicit guidance embedding \(\) into plain text. By moderating the _Prompt

    &  **Open** \\ **Source** \\  } &  \\   & & **Paatqin** & **Label** & **Inter-** & **Custom-** \\  OpenAI & ✗ & Classifier & ✗ & ✗ \\ Microsoft & ✗ & Classifier & ✗ & ✗ \\ AWS & ✗ & Classifier & ✗ & ✗ \\ SafetyChecker & ✗ & Classifier & ✗ & ✗ \\ NSFW cls. & ✔ & Classifier & ✗ & ✗ \\ Detoxify & ✔ & Classifier & ✗ & ✗ \\ Perplexities & Classifier & ✔ & ✗ & ✗ \\
**GARDT2I** & ✔ & **Generator** & ✔ & ✔ \\   

Table 1: Comparison of our generative defensive approach with existing classification-based ones.

_Interpretation_, we can easily identify adversarial prompts (see Fig. 2 (c)). To be specific, when given a guidance embedding for a normal prompt, as depicted in Fig. 1 (c), the GuardT2I model accurately reconstructs the input prompt with slight variations. However, when encountering an adversarial prompt's guidance embedding, like the one shown in Fig. 2 (b), the generated prompt interpretation will differ significantly from the original input and may contain explicit NSFW words, _e.g._ "sex", and "fuck", which can be easily distinguished. Furthermore, the generated prompt interpretation enhances decision-making transparency, as illustrated in Fig. 2 (d).

Text Generation with cLLM.Translating the latent representation \(\) back to plain text presents a significant challenge due to the implicitness of latents. To resolve this issue, we approach it as a conditional generation problem and incorporate cross-attention modules to pre-trained LLMs, resulting in a conditional LLM (_c-LLM_) to fulfill this conditional generation task. To be specific, we employ a decoder-only architecture, comprising of \(L\) stacked transformer layers, as outlined in Fig. 3, and insert cross-attention layers in each transformer block. These cross-attention layers receive the guidance embedding \(\) as the query and utilize the scaled dot product attention mechanism to calculate the _attention score_, as follows:

\[(=,,)= (^{T}}{}) \]

Finally, the output from the final layer of the c-LLM is projected through a linear projection layer into the token space and translated to text.

To fine-tune c-LLM, we curate a sub-dataset sourced from the LAION-COCO dataset , as the training set, denoted as \(\). It is important to note that the source dataset \(\) should be unfiltered, meaning it naturally contains both Safe-For-Work (SFW) and NSFW prompts. This deliberate inclusion enables the resulting c-LLM, trained on this dataset, to acquire potentially generate NSFW prompts in natural language.3 We input the prompt \(\) from \(\) into the text encoder of T2I models, yielding the corresponding guidance embedding, expressed as \(=()^{d}\)(see Fig. 3). The resulting dataset, comprising pairs of guidance embeddings and

Figure 3: Architecture of c-LLM. T2I’s text guidance embedding \(\) is fed to c-LLM through the multi-head cross attention layer’s query entry. \(\) indicates the total number of transformer blocks.

Figure 2: **The Workflow of GuardT2I against Adversarial Prompts.****(a)** GuardT2I halts the generation process of adversarial prompts. **(b)** Within GuardT2I, the c-LLM translates the latent guidance embedding \(\) into natural language, accurately reflecting the user’s intent. **(c)** A double-folded generation parse detects adversarial prompts. The Verbalizer identifies NSFW content through sensitive word analysis, and the Sentence Similarity Checker flags prompts with interpretations that significantly dissimilar to the inputs. **(d)** Documentation of prompt interpretations ensures transparency in decision-making transparency, as illustrated in Fig. 2 (d).

their corresponding prompts \((,)\), is named the _Mapped Guidance Embedding Dataset_, \(_{e}\), and serves in the training of cLLM.

For a given training sample \((_{i},_{i})\) from \(_{e}\), c-LLM is tasked with generating a sequence of interpreted prompt tokens \(}=(_{1},_{2},...,_{n})\) conditioned on the T2I's guidance embedding \(\). The challenges arise from potential information loss during the compression of \(\), and the discrepancy between the LLM's pre-training tasks and the current conditional generation task. These challenges may hinder the decoder's ability to accurately reconstruct the target prompt \(\) using only \(\), as illustrated in Fig. 3. To address this issue, we employ _teacher forcing_ training technique, wherein the c-LLM is fine-tuned with both \(\) and the ground truth prompt \(\). We parameterize the c-LLM by \(\), and our optimization goal focuses on minimizing the cross-entropy (CE) loss at each prompt token position \(t\), conditioned upon the guidance embedding \(\). By denoting the token sequence of prompt \(\) as \(=(y_{1},y_{2},...,y_{n})\) the loss function can be depicted as:

\[_{CE}()=-_{t=1}^{n}log(p_{}(}|y_{0},y_{1},...,y_{t-1};)), \]

where \(y_{0}\) indicates the special \(<BOS>\) begin of sentence token. The underlying concept of the aforementioned objective Eq. (2) aims to tune c-LLM to minimize the discrepancy between the predicted token sequence \(}\) and the target token sequence \(\). Teacher forcing ensures that the model is exposed to the ground truth prompt \(\) at each step of the generation, thereby conditioning the model to predict the next token in the sequence more accurately [44; 9; 43]. The approach is grounded in the concept that a well-optimized model, through minimizing \(_{CE}()\), will produce an output probability distribution \(p_{}(|y_{0},y_{1},...,y_{t-1};)^{|V|}\), where \(|V|\) represents the size of the vocabulary codebook, which closely matches the one-hot encoded target token \(y_{t}\), thereby enhancing the fidelity and coherence of the generated prompt interpretations [44; 9; 43; 17].

A Double-folded Generation Parse Detects Adversarial Prompts.After revealing the true intent of input prompts with plain text, in this step, we introduce a bi-level parsing mechanism including _Verbalizer_ and _Sentence Similarity Checker_ to detect malicious prompts.

Firstly, _**Verbalizer**_, \(V(,)\), as a simple and direct moderation method, is used to check either the _Prompt Interpretation_ contains any explicit words, _e.g._ "fuck", as illustrated in Fig. 2 (c). Here, \(\) denotes a developer-defined NSFW word list. Notably, \(\) is adaptable, allowing real-time updates to include emerging NSFW words, while maintaining the system's effectiveness against evolving threats.

In addition, we utilize the _**Sentence Similarity Checker**_ to examine the similarity in text space. For a benign prompt, its _Prompt Interpretation_ is expected to be identical to the itself, indicating high similarity during inference. In contrast, adversarial prompts reveal the obscured intent of the attacker, resulting in significant discrepancy with the original prompt. We measure this discrepancy using an established sentence similarity model , flagging low similarity ones as potentially malicious.

**Resistance to Adaptive Attacks.** GuardT2I demonstrates considerable robustness even under adaptive attacks. To deceive both T2I and GuardT2I simultaneously, the adversarial prompts must appear nonsensical yet retain similar semantic content in T2I's latent space, while also resembling their prompt interpretation to bypass GuardT2I. This requirement creates conflicting optimization directions: while adaptive attacks aim for prompts that differ visually from explicit ones, GuardT2I requires similarity in prompt interpretation and absence of explicit NSFW words. Consequently, increasing GuardT2I's bypass rate leads to a reduced NSFW generation rate by the T2I model, making it challenging for adaptive attackers to circumvent GuardT2I effectively.

Figure 4: Workflow of _Sentence Similarity Checker_. **(a)** Normal Prompt: In the case of a normal prompt, its prompt interpretation closely aligns with the original prompt, resulting in a SFW decision. **(b)** Adversarial Prompt: Conversely, for an adversarial prompt, its prompt interpretation significantly differs from the original prompt both, therefore be identified.

## 4 Experiments

### Experimental Settings

Training Dataset.LAION-COCO  represents a substantial dataset comprising 600M high-quality captions that are paired with publicly sourced web images. This dataset encompasses a diverse range of prompts, including both standard and NSFW content, mirroring real-world scenarios. We use a subset of LAION-COCO consisting of 10M randomly sampled prompts to fine-tune our c-LLM.

**Test Adversarial Prompt Datasets.** I2P  comprises 4.7k hand-crafted adversarial prompts. These prompts can guide T2Is towards NSFW syntheses, including self-harm, violence, shocking content, hate, harassment, sexual content, and illegal activities. We further extract 200 sexual-themed prompts from I2P to form the I2P-sexual adversarial prompt dataset. SneakyPrompt , Ring-A-Bell , P4D , and MMA-Diffusion  generate adversarial prompts automatically, we directly employ their released benchmark for evaluation.

**Target Model.** We employ Stable Diffusion v1.5 , a popular open-source T2I model, as the target model of our evaluation. This model has been selected due to its extensive adoption within the community and its foundational influence on subsequent commercial T2I models .

**Implementation.** Our GuardT2I comprises three components: Verbalizer, Sentence Similarity Checker, and c-LLM. Verbalizer operates based on predefined 25 NSFW words. We utilize the off-the-shelf _Sentence-transformer_, to function as the Sentence Similarity Checker. We implement c-LLM with 24 transformer blocks. Its initial weights are sourced from . Please refer to Appendix for more detailed implementation. Note that GuardT2I as an LLM-based solution, also follows the scaling law , one can implement GuardT2I with other types of pre-trained LLMs and text similarity models, based on real scenarios.

**Baselines.** We employ both commercial moderation API models and popular open-source moderators as baselines. OpenAI Moderation  classifies five type NSFW themes, including sexual content, hateful content, violence, self-harm, and harassment. If any of these categories are flagged, the prompt is rejected . Microsoft Azure Content Moderator , as a classifier-based API moderator, focuses on sexually explicit and offensive NSFW themes. AWS Comprehend  treats NSFW prompt detection as a binary classification task. If the model classifies the prompt as toxic, it is rejected. NSFW-text-classifier  is an open-source binary NSFW classifier. Detoxity  is capable of detecting four types of inappropriate prompts, including pornography content, threats, insults, and identity-based hate.

    &  &  \\   & &  &  & ^{}\)**} & ^{}\)**} & ^{}\)**} &  &  &  \\    } & OpenAI-Moderation  & **98.50** & 73.02 & 97.93 & 84.60 & 99.35 & 95.68 & _91.51_ & _+11.59_ \\  & Microsoft Azure  & 81.89 & 90.66 & 55.04 & 54.25 & _99.42_ & 81.90 & 77.19 & +18.64 \\  & AWS Comprehend  & 97.09 & 97.33 & 69.67 & 70.50 & 98.76 & 91.51 & 87.48 & \(\)13.70 \\  & NSFW-text-classifier  & 85.80 & 97.78 & 66.98 & 65.39 & 64.34 & 57.97 & 73.04 & \(\)15.32 \\  & Cloudity  & 75.10 & 79.27 & 54.63 & 51.83 & 96.27 & 82.22 & 73.22 & \(\)17.06 \\  & **GardT2I (Ours)** & 97.86 & **98.86** & **93.05** & **92.56** & **99.91** & **98.36** & **96.77** & **\(\)3.15** \\    } & OpenAI-Moderation  & **98.48** & 58.99 & **92.14** & _83.39_ & _98.21_ & _94.87_ & 87.68 & \(\)15.10 \\  & Microsoft Azure  & 82.83 & 91.58 & 54.97 & 60.12 & 99.56 & 90.38 & 79.91 & \(\)18.19 \\  & AWS Comprehend  & 97.24 & 97.30 & 77.47 & 73.25 & 98.80 & 91.73 & \(\)93.90 & \(\)11.14 \\  & NSFW-text-classifier  & 66.46 & 67.33 & 53.62 & 51.54 & 53.86 & 51.06 & 57.31 & \(\)_7.51_ \\  & Dediotry  & 85.97 & _97.51_ & 67.02 & 64.44 & 95.52 & 80.98 & 81.91 & \(\)13.95 \\  & **GardT2I (Ours)** & 98.28 & **98.95** & _89.64_ & **91.66** & _99.92_ & **88.51** & **96.16** & **\(\)_4.35_ \\    } & OpenAI-Moderation  & **4.40** & 40.20 & 35.50 & 99.09 & _0.70_ & **25.42** & _27.35_ & _22.27_ \\  & Microsoft Azure  & 61.53 & 57.60 & 77.50 & 98.32 & 1.05 & 80.00 & 62.67 & \(\)33.51 \\  & AWS Comprehend  & 19.78 & **4.95** & 90.50 & 95.56 & 6.32 & 80.42 & 49.59 & \(\)43.57 \\  & NSFW-text-classifier  & 84.61 & 48.10 & 92.50 & 94.45 & 68.42 & 87.92 & 79.33 & \(\)_17.88_ \\  & D detoxify  & 51.64 & 13.70 & 76.00 & 79.20 & 15.09 & 90.83 & 54.41 & \(\)33.52 \\  & **GardT2I (Ours)** & 6.50 & 6.59 & **25.50** & **34.96** & **0.35** & _41.67_ & **19.26** & \(\)_17.14_ \\    } & ESD  & 28.57 & 66.7 & 36.25 & - & 98.60 & 79.16 & _6.86_ & \(\)_9.31_ \\  & SLD-medium  & 58.24 & 85.00 & 39.10 & - & 98.95 & 80.51 & 72.36 & \(\)_23.66_ \\   & SLD-strong  & 41.76 & 80.82 & _30.12_ & - & _97.19_ & _73.75_ & 64.73 & \(\)_27.93_ \\   & **GardT2I (Ours)** & **9.89** & **10.20** & **26.4** & - & **3.16** & **8.75** & **11.68** & \(\)_8.71_ \\   

Table 2: Comparison with baselines. **Bolded** values are the highest performance. The _underlined italicized_ values are the second highest performance. * indicates human-written adversarial prompts.

SLD  and ESD  are concept-erasing methods, which are designed to reduce the probability of NSFW generation. Therefore, we use the Attack Success Rate (ASR) as our evaluation metric. For GuardT2I, we set the threshold at FPR@5%, a common adaptation. As a concept-erasing method, ESD  only removes a single NSFW concept, "nudity", by fine-tuning the T2I model. This limitation means it fails to mitigate other NSFW themes such as violence, self-harm, and illegal content. Consequently, our evaluation focuses solely on "adult content". All implementations of the baseline models and the tested adversarial prompts are released by their original papers.

**Evaluation Metrics.** Rejecting adversarial prompts is a detection task, for which we employ standard metrics including AUROC, AUPRC, and FPR@TPR95. These metrics are used to evaluate GuardT2I and baseline models, in line with established practices in . Higher values of AUROC and AUPRC signify superior performance, whereas a lower FPR@TPR95 value is preferable. Due to space limitation, detailed explanations of these metrics are provided in Appendix.

### Main Results

Tab 2 presents a comprehensive evaluation of the proposed GuardT2I moderator in comparison with several baseline methods across multiple adversarial prompt datasets. The results demonstrate that GuardT2I consistently outperforms existing approaches in key performance metrics. Specifically, GuardT2I achieves the highest average AUROC of **98.36%** and the highest average AUPRC of **98.51%**, surpassing all baseline methods, including OpenAI-Moderation, Microsoft Azure, AWS Comprehend, NSFW-text-classifier, and Detoxify. Furthermore, GuardT2I exhibits superior effectiveness in minimizing false positives and attack success rates, attaining an average FPR@TPR95 of **19.26%** and an average ASR of **8.75%**, both of which are significantly lower than those of the compared baselines. The reduced standard deviations across these metrics (**\(\)3.15** for AUROC, **\(\)4.35** for AUPRC, and **\(\)17.14** for FPR@TPR95) further indicate the robustness and consistency of GuardT2I's performance. These findings collectively highlight the superior capability of GuardT2I in effectively moderating adversarial prompts, ensuring both high detection accuracy and resilience against various attack strategies.

GuardT2I causes little impact on normal use cases.Tab. 2's FPR@TPR95 results corroborate GuardT2I is harmless to normal prompts, demonstrating a significantly lower FPR of 18.39%, which is 89.23% lower than the top-performing baseline average. This metric is critical in practical scenarios where high FPR can frustrate user experience. Moreover, we evaluate the performance of GuardT2I using the FID  and CLIP-Score  metrics to assess image quality and text alignment in Tab. 3. We compared our approach to the concept-erasing defense methods ESD  and SLD , which aim to reduce the probability of generating NSFW images. Additionally, we reported the average Attack Success Rate (ASR) to indicate the effectiveness of the defense methods.

Figure 5: ROC curves of our GuardT2I and baselines against various adversarial prompts. The black line represents the GuardT2I model’s consistent and high AUROC scores across different thresholds.

    & **Image Fidelity** & **Text Alignment** & **Defense Effectiveness** \\  & FID  (\(\)) & CLIP-Score  (\(\)) & ASR (Avg.)(\(\)) \\  ESDu1  & **49.24** & _0.1501_ & _61.86_ \\ SLD-Medium  & 54.15 & 0.1476 & 72.36 \\ SLD-Strong  & 56.44 & 0.1455 & 64.73 \\
**GuardT2I(Ours)** & 52.10 & **0.1502** & **11.68** \\   

Table 3: Normal Use Case Results. **Bolded** values are the highest performance. The _underlined italicized_ values are the second highest performance.

[MISSING_PAGE_FAIL:8]

sarial prompts, the significance of prompt interpretations becomes even more pronounced. As illustrated in Tab. 4's lower section, GuardT2I interprets adversarial prompts' corresponding text guidance embedding into readable sentences. These sentences, which serve as prompt interpretations, can reveal the actual intention of the attacker. As analyzed in Fig. 7, the original adversarial prompts' prominent words seem safe for work, while after being parsed by our GuardT2I we can get their actual intentions. The ability to provide interpretability is a distinctive feature of GuardT2I, distinguishing it from classifier-based methods that typically lack such transparency. This capability not only differentiates GuardT2I but also adds significant value by shedding light on the decision-making process, offering developers of T2I a deeper understanding.

### Evaluation on Adaptive Attacks

Considering attackers have complete knowledge of both T2I and GuardT2I, we modify the most recent MMA-Diffusion adversarial attack , which provides a flexible gradient-based optimization flow to attack T2I models, by adding an additional term to attack GuardT2I, as depicted in Eq. (3), to perform adaptive attacks.

\[L_{adaptive}=(1-) L_{T2I}+ L_{GuardT2I}, \]

where \(L_{T2I}\) is the original attack loss proposed by MMA-Diffusion, which steers T2I model towards generating NSFW content. Besides, \(L_{GuardT2I}\) is the loss function from GuardT2I's _Sentence Similarity Checker_, which can attack GuardT2I by optimizing with gradients, and \(\) is a hyperparameter to trade off two items.

The experiments are performed on a NVIDIA-A800-(80G) GPU with the default attack settings of MMA-Diffusion. We sample 100 NSFW prompts from MMA-Diffusion's dataset, and report the results with various \(\) in Tab. 5, where"GuardT2I bypass Rate" indicates the percentage of adaptive prompts that bypass GuardT2I. "T2I NSFW Content Rate" represents the percentage of bypassed prompts that result in the T2I generating NSFW content. Therefore, the "Adaptive Attack Success Rate" is calculated as "GuardT2I Bypass Rate" \(\) "T2I NSFW Content Rate". Following , a synthesis is considered NSFW, once it can trigger the NSFW detector nested in Stable Diffusion .

The results show that adaptive attacks on the entire system are challenging due to conflicting optimization directions. Specifically, \(L_{T2I}\) aims to find prompts that appear different and malicious semantic according to the embeddings of T2I. On the other hand, GuardT2I requires any bypassed prompts to stay close to their semantics according to the embeddings of T2I models. As a result, an increase in the "GuardT2I Bypass Rate" leads to a decrease in the "T2I NSFW Generation Rate", and vice versa. Therefore, even for adaptive attackers, evading GuardT2I becomes difficult, with an overall "Attack Success Rate" no higher than 16%. In a sanity check with doubled attack iterations (1000, \(\)30 minutes per adv. prompt), the highest "Adaptive Attack Success Rate" observed is 24%. By contrast, that of Safety Checker is higher than 85.48% as reported by . Moreover, qualitative results show that the successful adversarial prompts trend to degrade the synthesis quality, as illustrated in Fig. 8, weakening the threat posed by adaptive attacks. To strengthen GuardT2I's robustness, developers can set a more strict threshold. If some users are still concerned about moving to GuardT2I from the alternative moderators then they can use both in parallel.

### Ablation Study

Tab. 6 explores the roles of two key components in GuardT2I: _Verbalizer_ and _Sentence Similarity Checker_. _Verbalizer_ shows variable effectiveness across different adversarial prompts, indicating its limited capacity to handle complex cases independently. As a complementary, _Sentence Similarity Checker_ consistently achieves high AUROC scores above 91%, demonstrating its ability to discern subtle differences between prompts effectively. Combining both components results in the highest performance, highlighting a synergistic effect. The _Verbalizer_ analyzes the linguistic structure, while the _Sentence Similarity Checker_ assesses semantic coherence, together providing a comprehensive defense against adversarial prompts.

Figure 8: Syntheses generated by successful adaptive attack prompts. Adaptive adversarial prompts that can bypass GuardT2I tend to have much-weakened synthesis quality.

## 5 Discussion

**Failure Case Analysis.** We analyze two types of failure cases involving both false negatives and false positives. As shown in Fig. 9 (a), a false negative occurred when an adversarial prompt  led to the generation of unauthorized T2I content about Trump, mistakenly classified as normal. To prevent such errors, we can enrich Verbalizer by including specific keywords like "Donald Trump." In addition, we have observed that GuardT2I occasionally suffers from false alarms due to the rare appearance of certain terminologies. However, the rare terminology is either difficult for T2I model to depict, as demonstrated in Fig. 9 (b), making the false alarm less harmful.

**Computational Cost.** Tab. 7 compares the computational costs of GuardT2I and the image classifier-based post-hoc SafetyChecker . GuardT2I operates in parallel with T2I, allowing for an immediate cessation of the generation process upon detection of harmful messages. As long as GuardT2I's inference speed is faster than the image generation speed of the T2I model, it does not introduce additional latency from the user's perspective. In contrast, SafetyChecker requires a full diffusion process of 50 iterations to classify NSFW content, making it significantly less efficient. Particularly in the presence of an adversarial prompt, GuardT2I responds approximately 300 times faster than SafetyChecker.

## 6 Conclusion

By adopting a generative approach, GuardT2I enhances the robustness of T2I models against adversarial prompts, mitigating the potential misuse for generating NSFW content. Our proposed GuardT2I offers the capability to track and measure the prompts of T2I models, ensuring compliance with safety standards. Furthermore, it provides fine-grained control that accommodates diverse adversarial prompt threats. Unlike traditional classification methods, GuardT2I leverages the c-LLM to transform text guidance embeddings within T2I models into natural language, enabling effective detection of adversarial prompts without compromising T2I models' inherent performance. Through extensive experiments, we have demonstrated that GuardT2I outperforms leading commercial solutions such as OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. And show decent robustness against adaptive attacks. We firmly believe that our interpretable GuardT2I model can contribute to the development of safer T2I models, promoting responsible behavior in real-world scenarios.