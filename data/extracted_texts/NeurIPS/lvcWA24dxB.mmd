# MotionCraft:

Physics-based Zero-Shot Video Generation

 Antonio Montanaro

indicates equal contribution.

Project page: https://mezzelfo.github.io/MotionCraft/.

Luca Savant Aira

**Emanuele Aiello,**

**Diego Valsesia**

**Enrico Magli**

Politecnico di Torino

{name.surname}@polito.it

indicates equal contribution.

Project page: https://mezzelfo.github.io/MotionCraft/.

###### Abstract

Generating videos with realistic and physically plausible motion is one of the main recent challenges in computer vision. While diffusion models are achieving compelling results in image generation, video diffusion models are limited by heavy training and huge models, resulting in videos that are still biased to the training dataset. In this work we propose MotionCraft, a new zero-shot video generator to craft physics-based and realistic videos. MotionCraft is able to warp the noise latent space of an image diffusion model, such as Stable Diffusion, by applying an optical flow derived from a physics simulation. We show that warping the noise latent space results in coherent application of the desired motion while allowing the model to generate missing elements consistent with the scene evolution, which would otherwise result in artefacts or missing content if the flow was applied in the pixel space. We compare our method with the state-of-the-art Text2Video-Zero reporting qualitative and quantitative improvements, demonstrating the effectiveness of our approach to generate videos with finely-prescribed complex motion dynamics.

## 1 Introduction

As human beings, we have always exploited our creativity to generate art, in different forms such as visual art, music or poetry. In vision, we are often inspired by the natural world since our visual system continuously acquire images perceived as a video sequence. Indeed, videos or movies are one of the best visual stimuli since they contain images, motion and audio.

Recent generative models for still images based on diffusion models  achieved remarkable results with quality almost indistinguishable from real images. It is therefore clear that the next big goal is video generation. However, it seems that including the dimension of time remains challenging. Some works such as Sora  achieve astonishing temporal consistency and photorealism at the expense of enormous computational and data requirements. Moreover, we argue that fine-grained control over the motion dynamics is impossible with a simple text prompt. If one wants to synthesize a video according to some precise physical dynamics, they would not be able to do it with current models. Interestingly, explicitly controlling the motion dynamics also allows to decouple temporal evolution from content generation. Indeed, explicitly injecting the physics of the real world as motion dynamics allows to develop more parsimonious models, that do not need to brute-force learn them from data.

For this reason, in this paper, we investigate the possibility to create a zero-shot video generation model that only requires a pretrained still image generator and knowledge of physical laws regarding motion. Indeed, since videos are temporal sequences of images correlated by physical laws, we only need to devise a way to include physical laws in the diffusion prior to animate a starting image. We thus advocate for physics simulators as appropriate sources of motion, output as a sequence of optical flows, while also being completely user-controllable, plausible, and explainable.

We propose MotionCraft, a physics-based zero-shot video generator that uses optical flow extracted from a physical simulation to warp the noise latent space of a pretrained image diffusion model to generate videos with complex dynamics without the need to train anything. While using a projection of motion onto the camera plane as a pixelwise displacement field (optical flow) may seem limiting due to the fact that, if applied in the pixel space, it would not be able to synthesise novel coherent content but only displace pixels, the trick lies in its application in the noise latent domain. Backed by evidence that motion vectors correlate between pixel and noise space, warping of the latter by means that MotionCraft allows to simultaneously apply the desired motion and exploit the powerful image prior of the generative model. This is capable of adapting the scene to the prescribed motion without significant artefacts, generate novel content and shows impressive global consistency (reflections, illumination, etc., consistent with the desired evolution).

We present quantitative and qualitative experimental results where we show that our zero-shot MotionCraft is capable of synthesising realistic videos with finely controlled temporal evolution governed by fluid-dynamics equations, rigid body physics, and multi-agent interaction models, while zero-shot state-of-art techniques cannot.

## 2 Related work

Diffusion Based Video GenerationVideo Generation  is a longstanding problem in computer vision aiming to learn the distribution of and synthesise realistic videos. Recently, text-based Denoising Diffusion Probabilistic Models (DDPM) [28; 31] have been studied to tackle this challenge delivering impressive results. These approaches include Sora , Video Diffusion models , Imagen-video  and Align your Latents . They require sophisticated spatio-temporal denoising architectures at the expense of huge computational requirements and large amounts of paired text-video data for training. To reduce the data requirements, different approaches investigate few-shot and unsupervised learning techniques. Make-a-Video  proposes an unsupervised training with only videos, coupled with a retrieval strategy to sample using text. On the other hand, Ni et al.  train a diffusion-based optical flow generator that outputs a flow conditioned on a reference image and a textual prompt, that reduces the computational burden of generating videos by training the diffusion process on small flow fields. Differently from them, our approach is zero-shot and we do not train anything.

To the best of our knowledge, Text-to-video-Zero  and Generative Rendering  are the only zero-shot video generators. However, Generative Rendering (concurrent work, with no code available)

Figure 1: Melting man simulation. Top: MotionCraft; Bottom: T2V0 . MotionCraft uses a fluid dynamics simulation to warp noise latents and synthetize video frames. T2V0 is unable to simulate the evolution of the melting statue and simply moves the object towards the bottom of the frame.

has significant extra requirements beyond Stable Diffusion (SD) as image generator, in the form of a depth-conditioned ControlNet , and a 3D mesh manually animated, leveraging UV maps to render the scene. Moreover, Generative Rendering cannot render fluids, since they are difficult to represent as 3D meshes.

In this paper, we compare our method to Text-to-video-Zero (T2V0), as zero-shot video generator baseline. T2V0 applies a constant shift (with a fixed direction) to the initial latent noise of SD, sampling each frame sequentially by means of DDPM. As shown in our work, since the motion in the noise latent space directly translates into the motion of the pixel space, the generated videos result in a overall shift in the same fixed direction. The largest part of the motion is caused by the stochastic fluctuations of the DDPM sampling strategy leading to unnatural motion and inconsistency of the objects in the different frames. On the contrary, in this work, we avoid the use of a constant warping operation derived from physics simulation flows in the latent space in order to incorporate complex motion dynamics.

Diffusion Based Video and Image EditingRecently, different methods exploit the prior of text-to-image diffusion models for video editing. In particular, Tune-A-Video  finetunes a text-to-image diffusion model to edit a video. They start from the inverted frames in the latent space and use the text prompt as an editing tool. Pix2Video  employs a self-attention injection mechanism to edit videos using a pretrained image diffusion model.

Other methods use the optical flow to edit reference images or videos. Motion Guidance  leverages a user defined optical flow that allow zero-shot image editing. It works by guiding the diffusion sampling process with the gradient from a pretrained optical flow network via a guidance loss. LatentWarp  and TokenFlow , use an optical flow estimated from a reference video to warp the latent space of the diffusion model to achieve consistent editing. These methods leverage both diffusion models priors and other components such as ControlNet for structural control, and trained flow estimators such as RAFT . Alternatively, we propose a zero-shot video generation method, using only vanilla SD. This means that MotionCraft does not require a reference video but it can animate an image, generated by the SD model or obtained by inverting a real one. Moreover, the physics simulations allow to generate different videos from the same starting image.

## 3 Method

This section describes MotionCraft, a zero-shot video generation method, where the meaning of "zero-shot" is twofold: we do not train or finetune any component of the text-to-image diffusion model, nor we do not use reference video or optical flow estimators as starting point. In the following, we used used Stable Diffusion as pretrained text-to-image model.

### Optical Flow is preserved in the Latent Space of Stable Diffusion

Our proposed method stems from a key observation: the optical flow estimated between two frames in the pixel space is correlated with the flow estimated between the corresponding noise latent representations of SD. We conjecture that this is related to the specific design of the SD variational auto-encoder and denoiser architectures. In fact, by largely using convolution operations, they enforce a locality prior which preserves spatial information to some extent.

Figure 2: A qualitative example of the image and latent flows correlation. This figure shows, from left to right, (a) the first RGB frame, (b) the second RGB frame superimposed with the estimated flow in the RGB domain, (c) the first latent frame, (d) the second latent frame superimposed with the estimated flow in the latent domain and (e) the correlation map of the two non-zero flows.

In order to empirically investigate this phenomenon, we conducted a quantitative experiment using the MSU Video Frame Interpolation Benchmark dataset , considering only real videos. For each pair of consecutive video frames, the following steps have been taken. We first estimate the optical flow in the RGB space by using a well-established method, based on the Gunnar Farneback's algorithm, provided by OpenCV . Then, we compute the noise latent representations of the two frames, first encoding the image in the variational autoencoder (VAE) of SD at timestep \(=0\), followed by DDIM inversion  up to timestep \(=400\) (same value for all experiments in this work, empirically determined). Finally, a correlation coefficient based on cosine similarity is computed between the optical flows estimated in the RGB and noise latent spaces. The resulting correlations are then averaged across all pairs of consecutive frames in the dataset, obtaining an average value of 0.727, which indicates a strong correlation between the optical field in the RGB and noise latent domains. An example of this experiment is presented in Fig. 2, showcasing the two estimated flows in the image and latent space and their correlation.

### Physics-based zero shot video generation

Based on the analysis presented in the previous section, we propose a novel zero-shot video generation method, named MotionCraft, where an image (real or generated), serving as a starting frame \(I^{0}\), is animated according to a physical simulation, by means of a (possibly time-varying) optical-flow generator \(\) in the noise latent space. The outcome is a video made of \(N\) frames \(I^{0},,I^{N-1}\) that follows the motion prescribed by the physical simulation and evolves the content of the first frame coherently. Inspired by the previous observation, this animation is obtained by warping the noisy latent representation of an image in the latent diffusion space. Regarding the physics simulation for the optical flow generation, we use different libraries to simulate different physics, as explained in the experimental section, such as fluid dynamics, rigid motion and multi-agent systems. It is also possible, albeit not shown in this paper, to use animation software to generate the required optical flows.

Fig. 3 illustrates an overview of MotionCraft highlighting the autoregressive generation of the video. At each iteration \(f 1\), the frame \(I^{f}\) is generated using only the information contained in the first frame \(I^{0}\) and the previous frame \(I^{f-1}\). Given this Markovian structure, MotionCraft is characterized by \((1)\) space complexity and \((N)\) time complexity with respect to the total number \(N\) of frames to be generated. More in detail, first, the two RGB frames \(I^{0},I^{f-1}\) are encoded into the latent space and they are independently inverted with the reversed DDIM sampling scheme up to a fixed diffusion timestep \(\), obtaining \(z_{}^{0}\), and \(z_{}^{f-1}\), respectively. Then, the optical flow warping operator \(^{f-1 f}\) prescribed by the physical simulation is applied to \(z_{}^{f-1}\), obtaining \(_{}^{f}\). Finally, the next RGB frame \(I^{f}\) is generated by performing \(\) steps of reverse diffusion using the DDIM sampling scheme with a novel cross-frame attention mechanism and a novel spatial noise map \(^{f}\) weighting technique, explained below. Furthermore, we exploit the classifier-free guidance (CFG)

Figure 3: MotionCraft overview. A video is generated from a starting image using a pretrained still image generative model by warping noise latents according to an optical flow description of the motion to be synthesised.

technique for generation proposed in , with \(\) and \(_{}\) being the positive and negative prompt, respectively, and \(>1\) being the strength of the CFG. More details can be found in the Appendix A.

Algorithm 1 reports the pseudocode of MotionCraft. Lines \(2-6\) include the DDIM inversion up to timestep \(\). Starting current frame \(I^{f-1}\) that was previously generated, in line \(2\) we embed it with the VAE encoder \(\), obtaining \(z_{0}^{f-1}\). Then we apply DDIM inversion on \(z_{0}^{f-1}\) for \(\) timesteps (line \(3-6\)). This involves the UNet with the standard self-attention (note the repetition of the noisy latent \(z_{t}^{f-1}\)) and the positive prompt \(\). As briefly reported in , we have also experienced that DDIM inversion is not compatible with CFG; hence, during the inversion, we do not use the negative prompt \(_{}\). The resulting estimated noise is used in line \(5\) for applying the DDIM inversion step (note that the \(=0\), so pure DDIM is performed). Upon completion of the DDIM inversion process, we obtain \(z_{}^{f-1}\), the noisy latent corresponding to the frame \(I^{f-1}\).

In line \(7\), the optical flow warping operator \(^{f-1 f}\) is applied to the noisy latent of the current frame \(z_{}^{f-1}\) to obtain a new noisy latent \(_{}^{f}\) that will generate the successive frame. Finally, in lines \(8-14\), the frame is generated. During this generation phase we use CFG to increase the quality of the generated images, hence also the negative prompt \(_{}\) is used. To create new content while preserving the original image, we propose two direct generalization of two known techniques: the multiple cross-frame attention (MCFA) mechanism and a spatial noise map weighting (Spatial-\(\)).

The MCFA technique generalizes the Cross Frame Attention (CFA) , as it enables the to-be-generated frame to attend to an arbitrary number of frames. We choose to attend to the first frame and the previous frame (as shown in lines \(9-10\) of Alg. 1 and Fig. 3) to ensure long-range and short-range temporal consistency, respectively. MCFA intervenes in all the self-attention blocks of the SD UNet, by replacing the keys and values, that are originally computed from projections of the generating frame features, with the ones computed from the attended frames.

We also propose Spatial-\(\) (line \(12\)), that is a novel technique that enables to choose, on a pixel-by-pixel basis, whether to use DDIM or DDPM as a sampling scheme. This enables the usage of DDPM in regions of the images where novel content should be created (for example, when a new part of an object is entering the scene), while using DDIM in the other regions to ensure consistency and determinism where the already-present content is just moving. Note that this spatial map \(^{f}\) can be obtained in multiple ways from the physical simulation. For example, \(^{f}\) can be set to \(1\) in regions of the image where the flow is not well-defined (pointing outside of the image boundaries) or in regions where the optical flow field has discontinuities.

## 4 Experimental results

### Experimental setting

In this section, we show examples of video generation based on different physics simulations: rigid body motion, fluid dynamics and multi-agent systems. Given an optical flow, we apply it on the SD latent space using MotionCraft (code is available at https://mezzelfo.github.io/MotionCraft/). Then, we compare our method to Text2Video-Zero  that, to the best of our knowledge, is the only diffusion-based zero-shot method for video generation.

We show qualitative results in Figs. 1, 4, 5, 6, 7, which we separately describe in the following sections. Table 1 reports two metrics to evaluate the quality of the generated videos. As done in previous works, we use the _Frame Consistency_ metric, defined as the average cosine similarity of the CLIP embeddings of consecutive frames. However, this metric presents some limitations, as CLIP focuses on high-level semantic features and not on low-level details, resulting in high correlations even if the content changes but its semantics do not (as an example, see the video generated by T2V0 of the dragon in Fig. 6, which has a _Frame Consistency_ of 0.97 even if the dragons are not the same dragons in each frame). To overcome some of these limitations, we propose a novel metric, named _Motion Consistency_, that measures how similar two frames are while accounting for the motion between them. We start from the observation that, if an object moves through the scene, its textures should remain almost the same, and, if we know its flow, we can bring back that object to overlap with its starting position. Then we can apply a similarity distance between the initial image and the next frame brought back by the reversed flow. Given two consecutive frames, we use a high-quality flow estimator (RAFT ) to estimate the optical flow between them and apply it on the second frame to reverse the motion. Then we compute the SSIM metric  on the first frame and the registered one.

### Rigid Body flows

Fig. 4 shows a pivotal example where MotionCraft can be directly compared to the state-of-the-art T2V0, as in this case we use an optical flow equivalent to a their proposed shift along the vertical axis. This example shows a video generated starting from a satellite view of a city, and, by simulating the

   &  &  \\   & T2V0  & **MotionCraft** & T2V0  & **MotionCraft** \\   Fluid & Dragons & 0.9664 & **0.9991** & 0.6846 & **0.9637** \\   & Melting Man & 0.9463 & **0.9566** & 0.7817 & **0.8252** \\   & Satellite Scan & 0.9588 & **0.9875** & 0.2852 & **0.9219** \\   & Revolving Earth & **0.9812** & 0.9696 & **0.7213** & 0.6783 \\  Agents & Birds & 0.9765 & **0.9968** & 0.8973 & **0.9385** \\    & 0.9658 \(\) 0.01 & **0.9819**\(\) 0.02 & 0.6740 \(\) 0.23 & **0.8655**\(\) 0.12 \\  

Table 1: Quantitative results.

Figure 4: Rigid motion simulation: satellite orbit. Top: MotionCraft; Bottom: T2V0 .

rectilinear motion of the satellite, new portions of the city appear from the top of the image. While T2V0 struggles with keeping temporal consistency, even with large structural elements (e.g., the river), MotionCraft is able to coherently scroll down the already present part of the city, while also generating new plausible content in the top part of the frames.

A similar case study is the Earth rotation in Fig. 5. Here, the optical flow is obtained by simulating a rotating sphere that was fitted to the first frame while keeping track of the starting and ending position of each point. As the Earth rotates, a slice disappears from one side and a new one needs to be generated on the opposite side. Thanks to the powerful natural image prior of SD, MotionCraft is able to autonomously generate other continents in the correct position, even if the text prompt contains no reference about them (see Appendix D for all the text prompts used in this paper). On the contrary, T2V0 is not able to rotate the Earth consistently while creating new content, as visible in the same Fig. 5.

### Fluids dynamics

In this set of experiments, we use the \(\)-flow  library to simulate fluid dynamics (by numerically solving Navier-Stokes equations) with the shape and position provided by the first frame \(I^{0}\). Moreover, we can set up the simulation in different ways, depending on the numerical solvers, i.e. _Eulerian_ (particle-based) or _Lagrangian_ (grid-based), we can add rigid obstacles to the fluid or we can define a initial velocity and force fields. All these different options result in videos that can have the same starting frame but differ in their evolution according to the simulation constraints. We extract the velocity field of the simulation as a proxy for the optical flow. Examples of the velocity field can be seen in Appendix C.

Fig. 6 shows a fluid simulation of two dragons breathing fire. We can approximate the two initial fire breaths with two centered smoke balls, obtaining a binary mask that will be fed to the simulation. At this point, we run the simulation, solving the Navier-Stokes equations by sequentially evaluating advection, diffusion and pressure. The vorticity and the expansion of the smoke is due to the buoyancy

Figure 5: Rigid motion simulation: revolving Earth. Top: MotionCraft; Bottom: T2V0 .

Figure 6: Fluid simulation: dragon fire. Top: MotionCraft; Bottom: T2V0 .

force set in the desired direction, that in this case is such that the two balls cross near the middle of the image.

The figure shows that MotionCraft produces a consistent scene with a realistic animation of the fire breaths. Moreover, the global scene illumination seems to change accordingly, and a realistic occlusion of a dragon due to smoke gradually appears. This is mainly due to the MCFA mechanism, as we ablate in Sec. 4.5. In T2V0, the scene is not temporally consistent and shows increasingly more artefacts, such as color shifts or the fact that the right dragon changes with time, while the left one even disappears.

A similar analysis can be conducted for Fig. 1, where a simulation of a melting statue is shown. We can see that the generated video includes bouncing of parts on the ground before the fluid settles.

### Multi-agent systems

Multi-agent systems are another interesting family of simulated dynamics. A simple agents model is the _Boids_ model , consisting of a set of point-like agents (named boids) that move according to three steering behaviour rules: separation, as boids avoid collisions with nearby agents by steering away from them, alignment, as boids align their direction with that of nearby agents, and cohesion, as boids move towards the average position of nearby agents to stay together as a group. To simulate this system we used the agentpy  library, in which the number of agents, the simulation time-steps and different physical parameters related to the steering rules can be chosen.

An example is shown in Fig. 7, generating the temporal evolution of a flock of birds. As SD is not able to generate images with a controllable number of agents in specified positions, we start from an image where there is a single agent (a bird in the example). Then, we extract the corresponding latent vector patch with the attention map  related to the CLIP token containing the word "bird", and clone it to the simulated positions of the other agents. At this point, we evolve the frames according to the optical flow derived from the simulation velocity field.

While MotionCraft produces a realistic flock motion, T2V0 motion is not consistent and the number of birds changes in each frame.

### Ablations

In this section, we ablate the contribution of the most important components/hyperparameters in the proposed pipeline. First, we start from investigating the impact of the cross-attention mechanism by comparing four different variants: i) each frame attends to itself (no MCFA); ii) each frame attends to the previous frame; iii) each frame attends to the first frame; iv) each frame attends to both the previous frame and the first frame (proposed MCFA). Visual results are shown in Fig. 8. As can be seen, the MCFA mechanism is necessary to generate plausible frames; moreover, attending only to the first frame reduces the overall motion, (e.g., always showing Africa as in the first frame), while only attending to the previous frame reduces color consistency. Overall, we demonstrate that the proposed MCFA, attending to both the first and the previous frame, represents the optimal solution to keep global consistency with the initial image and local consistency with the preceding frame.

Figure 7: Multi-agent system simulation: bird flock. Top: MotionCraft; Bottom: T2V0 .

Fig. 9 shows the ablation of the Spatial-\(\) weighting technique. As shown, being able to sample with DDPM in some parts of the image is crucial in order to generate novel plausible content. Indeed, DDPM adds, during each reverse diffusion step, random white noise to the latent. We suppose that this allows to better sample from the real distribution, avoiding artefacts other components of the method, such as the warping operator or the MCFA, would otherwise introduce.

Finally, we ablated the partial inversion process, i.e., lines 2-6 in Alg. 1. Without the DDIM inversion, textures and details generated by SD cannot be brought into the next frame, resulting in corrupted videos. Visual results can be found in the Appendix B.3.

Figure 8: Ablation - Cross-Frame attention. First row: no cross frame attention; Second Row: Attend only to the initial frame; Third Row: Attend only to the previous frame; Fourth Row: Attend to the initial and preceding frame (ours).

Figure 9: Ablation - Spatial-\(\). First Row: \(=0\); Second Row: Spatial-\(\) on.

### Additional Qualitative Results

Fig. 10 shows some additional results of MotionCraft. The first row shows a tree growing. This video was obtained using a simple constant outward-facing radial optical flow applied only on the foliage. Note that while the tree grows, its shadow evolution is coherent. As the input flow is zero in this part of the image, the shadow consistency is recovered only by Stable Diffusion. The last row show a video obtained by applying MotionCraft to SDXL. This shows the generalizability of MotionCraft to different diffusion models, with also different resolutions. Hence, MotionCraft is able to produce high-resolution videos with a high level of detail.

## 5 Conclusions

In this work, we have presented MotionCraft, a novel zero-shot approach for video generation. Our method allows to generate realistic videos with the image prior of Stable Diffusion and a physically-derived optical flow, without any additional training. MotionCraft warps the noise latent space according to the prescribed flow, and with a modified sampling process exploiting multi-frame cross-attention and the spatial-\(\) variable sampling scheme generates novel plausible contents following the prescribed motion and tempoirally consistent. For the evaluations of the results, we relied on a standard metric and a proposed one, showing that our method is not only qualitatively but also quantitatively superior to the state-of-the-art of zero-shot video generation.

Figure 10: Additional videos from MotionCraft. The last row is obtained by applying MotionCraft to SDXL