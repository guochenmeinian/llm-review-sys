# On the Robustness of Spectral Algorithms for

Semirandom Stochastic Block Models

 Aditya Bhaskara

University of Utah. Email: bhaskaraaditya@gmail.com.

Agastya Vibhuti Jha

University of Chicago. Email: agastyavjha.28@gmail.com.

Michael Kapralov

Ecole Polytechnique Federale de Lausanne. Email: michael.kapralov@epfl.ch.

Naren Sarayu Manoj

Toyota Technological Institute at Chicago. Email: nsm@ttic.edu.

Davide Mazzali

Ecole Polytechnique Federale de Lausanne. Email: davide.mazzali@epfl.ch.

Weronika Wrzos-Kaminska

Ecole Polytechnique Federale de Lausanne. Email: weronika.wrzos-kaminska@epfl.ch.

###### Abstract

In a graph bisection problem, we are given a graph \(G\) with two equally-sized unlabeled communities, and the goal is to recover the vertices in these communities. A popular heuristic, known as spectral clustering, is to output an estimated community assignment based on the eigenvector corresponding to the second smallest eigenvalue of the Laplacian of \(G\). Spectral algorithms can be shown to provably recover the cluster structure for graphs generated from certain probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clustering is known to be non-robust to model mis-specification. Techniques based on semidefinite programming have been shown to be more robust, but they incur significant computational overheads.

In this work, we study the robustness of spectral algorithms against semirandom adversaries. Informally, a semirandom adversary is allowed to "helpfully" change the specification of the model in a way that is consistent with the ground-truth solution. Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster. Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input.

On the positive side, we identify classes of semirandom adversaries under which spectral bisection using the _unnormalized_ Laplacian is strongly consistent, i.e., it exactly recovers the planted partitioning. On the negative side, we show that in these classes spectral bisection with the _normalized_ Laplacian outputs a partitioning that makes a classification mistake on a constant fraction of the vertices. Finally, we demonstrate numerical experiments that complement our theoretical findings.

## 1 Introduction

Graph partitioning or clustering is a fundamental unsupervised learning primitive. In a graph partitioning problem, one seeks to identify clusters of vertices that are highly internally connected and sparsely connected to the outside. This task is of particular significance when the given graph presents a latent community structure. In this setting, the goal is to recover the communities as accurately as possible. Various statistical models that attempt to capture this situation have been proposed and studied in the literature. Perhaps the most popular of these is the Symmetric Stochastic Block Model (SSBM) .

Following the notation of previous works , in this paper we describe an SSBM with specifications \(n,P_{1},P_{2},p,q\), where \(n\) is an even positive integer, \(P_{1}\) and \(P_{2}\) are a partitioning of the vertex set \(V=\{1,,n\}\) into subsets of equal size, and \(p\) and \(q\) are probabilities. Without loss of generality, we may assume that the partitions \(P_{1}\) and \(P_{2}\) consist of vertices \(1,,n/2\) and \(n/2+1,,n\), respectively. Hence, with a mild abuse of notation, we write an SSBM with parameters \(n,p,q\) only and write it as \((n,p,q)\). Now, let \((n,p,q)\) be a distribution over random undirected graphs \(G=(V,E)\) where each edge \((v,w) P_{1} P_{1}\) and \((v,w) P_{2} P_{2}\) (which we refer to as "internal edges") appears independently with probability \(p\), and each edge \((v,w) P_{1} P_{2}\) (which we refer to as "crossing edges") appears independently with probability \(q\). When \(p q\), there should be many more internal edges than crossing edges. Hence, we expect the community structure to become more evident as \(p\) tends away from \(q\).

In such scenarios, our general algorithmic goal is to efficiently identify \(P_{1}\) and \(P_{2}\) when given \(G\) without any community labels. This task is hereafter referred to as the _graph bisection problem_. In this work, we will be interested in _exact recovery_, also known as _strong consistency_, in which we want an algorithm that, with probability at least \(1-1/n\) over the randomness of the instance, exactly returns the partition \(\{P_{1},P_{2}\}\) for all \(n\) sufficiently large. Other approximate notions of recovery (such as almost exact, partial, and weak recovery) are also well-studied but are beyond the scope of this work.

Although the \((n,p,q)\) distribution over graphs is a useful starting point for algorithm design and has led to a deep theory about when recovery is possible and of what nature , it may not be representative of all scenarios in which we should expect our algorithms to succeed. To remedy this, researchers have proposed several different random graph models that may be more reflective of properties satisfied by real-world networks. These include the geometric block model , the Gaussian mixture block model , and others.

In this paper, we take a different perspective to graph generation by considering various _semirandom models_. At a high level, a semirandom model for a statistical problem interpolates between an average-case input (for example produced by a model such as the SSBM) and a worst-case input, in a way that still allows for a meaningful notion of ground-truth solution. In our context of graph bisection, this can be achieved by an adversary adding internal edges or by the distribution of internal edges itself being nonhomogeneous (i.e., every internal edge \((v,w)\) appears independently with probability \(p_{vw} p\), where the \(p_{vw}\) may be chosen adversarially for each internal edge). Researchers have studied similar semirandom models for graph bisection  and other statistical problems such as classification under Massart noise , detecting a planted clique in a random graph , sparse recovery , and top-\(K\) ranking .

These modeling modifications are not necessarily meant to capture a real-world data generation process. Rather, they are a useful testbed with which we can determine whether commonly used algorithms have overfit to statistical assumptions present in the model. In particular, observe that these changes in model specification are ostensibly helpful, in that increasing the number of internal edges should only enhance the community structure. Perhaps surprisingly, it is known that a number of natural algorithms that succeed in the SSBM setting no longer work under such helpful modifications . Therefore, it is natural to ask which algorithms for graph bisection are robust in semirandom models.

At this point, the performance of approaches based on convex programming is well-understood in various semirandom models . However, in practice, it is impractical to run such an algorithm due to computational costs. Another class of algorithms, that we call _spectral algorithms_, is more widely used in practice. Loosely speaking, a spectral algorithm constructs a matrix \(\) that is a function of the graph \(G\) and outputs a clustering arising from the embedding of the vertices determined by the eigenvectors of \(\). Popular choices of matrices include the unnormalized Laplacian \(_{G}\) and the normalized Laplacian \(_{G}\) (we will formally define and intuit these notions in the sequel) . This is because structural properties of both \(_{G}\) and \(_{G}\) imply that the second smallest eigenvalue of each, denoted as \(_{2}(_{G})\) and \(_{2}(_{G})\), serves as a continuous proxy for connectivity, and the corresponding eigenvector, \(_{2}(_{G})\) and \(_{2}(_{G})\), has entries whose signs reveal a lot of information about the underlying community structure. This motivates Algorithm 1. It can be run, for example, with \((G)_{G}\) or \((G)_{G}\). Following this discussion, we arrive at the question we study in this paper.

**Question 1**.: _Under which semirandom models do the Laplacian-based spectral algorithms, using the second eigenvector of \(_{G}\) or \(_{G}\), exactly recover the ground-truth communities \(P_{1}\) and \(P_{2}\)?_

Main contributions.Our results show a surprising difference in the robustness of spectral bisection when considering the normalized versus the unnormalized Laplacian. We summarize our results below:

* Consider a nonhomogeneous symmetric stochastic block model with parameters \(q<p<\), where every internal edge appears independently with probability \(p_{uv}[p,]\) and every crossing edge appears independently with probability \(q\). We show that under an appropriate spectral gap condition, the spectral algorithm with the unnormalized Laplacian exactly recovers the communities \(P_{1}\) and \(P_{2}\). Moreover, this holds even if an adversary plants \( np\) internal edges per vertex prior to the edge sampling phase.
* Consider a stronger semirandom model where the subgraphs on the two communities \(P_{1}\) and \(P_{2}\) are adversarially chosen and the crossing edges are sampled independently with probability \(q\). We show that if the graph is sufficiently dense and satisfies a spectral gap condition, then the spectral algorithm with the unnormalized Laplacian exactly recovers the communities \(P_{1}\) and \(P_{2}\).
* We show that there is a family of instances from a nonhomogeneous symmetric stochastic block model in which the spectral algorithm achieves exact recovery with the unnormalized Laplacian, but incurs a constant error rate with the normalized Laplacian. This is surprising because it contradicts conventional wisdom that normalized spectral clustering should be favored over unnormalized spectral clustering .

We also numerically complement our findings via experiments on various parameter settings.

Outline.The rest of this paper is organized as follows. In Section 2, we more formally define our semirandom models, the Laplacians \(\) and \(\), and formally state our results. In Section 3, we give sketches of the proofs of our results. In Section 4, we show results from numerical trials suggested by our theory. In Appendices A.1 and A.5 we prove important auxiliary lemmas we need for our results. In Appendix A.6, we prove our robustness results for the unnormalized Laplacian. In Appendix A.8, we prove our inconsistency result for the normalized Laplacian. In Appendix B, we give additional numerical trials and discussion.

## 2 Models and main results

In this paper, we study unnormalized and normalized spectral clustering in several semirandom SSBMs. These models permit a richer family of graphs than the SSBM alone.

**Matrices related to graphs.** Throughout this paper, all graphs are to be interpreted as being undirected, and we assume that the vertices of an \(n\)-vertex graph coincide with the set \(\{1,,n\}\). With this in mind, we begin with defining various matrices associated with graphs, building up to the unnormalized and normalized Laplacians, which are central to the family of algorithms we analyze (Algorithm 1).

**Definition 2.1** (Adjacency matrix).: _Let \(G=(V,E)\) be a graph. The adjacency matrix \(_{G}^{V V}\) of \(G\) is the matrix with entries defined as \(_{G}[v,w]=\{(v,w) E\}\)._

**Definition 2.2** (Degree matrix).: _Let \(G=(V,E)\) be a graph. The degree matrix \(_{G}^{V V}\) of \(G\) is the diagonal matrix with entries defined as \(_{G}[v,v]=_{G}[v]\), where \(_{G}[v]\) is the degree of \(v\)._

**Definition 2.3** (Unnormalized Laplacian).: _Let \(G=(V,E)\) be a graph. The unnormalized Laplacian \(_{G}^{V V}\) of \(G\) is the matrix defined as \(_{G}_{G}-_{G}=_{(v,w) E}(_{ v}-_{w})(_{v}-_{w})^{}\), where \(_{i}\) denotes the \(i\)-th standard basis vector._

**Definition 2.4** (Normalized Laplacians).: _Let \(G=(V,E)\) be a graph. The symmetric normalized Laplacian \(_{G,}^{V V}\) and the random walk Laplacian \(_{G,}^{V V}\) of \(G\) are defined as_

\[_{G,} -_{G}^{-1/2}_{G}_{G}^{-1/2}\,, _{G,} -_{G}^{-1}_{G}.\]

For all notions above, when the graph \(G\) is clear from context, we omit the subscript \(G\). Furthermore, when we discuss normalized Laplacians, we intend its symmetric version \(_{}\) unless otherwise stated. So, we omit this subscript as well and simply write \(\).

Next, we define the spectral bisection algorithms. We will discuss some intuition for why these algorithms are reasonable heuristics in Section 3.

**Definition 2.5** (Unnormalized and normalized spectral bisection).: _Let \(G=(V,E)\) be a graph, and let its unnormalized and normalized Laplacians be \(\) and \(\), respectively. We refer to the algorithm resulting from running Algorithm 1 on \(G\) with \((G)_{G}\) as unnormalized spectral bisection. We refer to the algorithm resulting from running Algorithm 1 on \(G\) with \((G)=_{G}\) as normalized spectral bisection._

Our goal is to understand when the above algorithms, applied to a graph with a latent community structure, achieve _exact recovery_ or _strong consistency_, defined as follows.

**Definition 2.6**.: _Let \(\{P_{1},P_{2}\}\) be a partitioning of \(V=\{1,,n\}\), and let \((\{P_{1},P_{2}\})\) be a distribution over \(n\)-vertex graphs \(G=(V,E)\). We say that an algorithm is strongly consistent or achieves exact recovery on \(\) if given a graph \(G\) it outputs the correct partitioning \(\{P_{1},P_{2}\}\) with probability at least \(1-1/n\) over the randomness of \(G\)._

### Nonhomogeneous symmetric stochastic block model

Our first model is a family of nonhomogeneous symmetric stochastic block models, defined below.

**Model 1** (Nonhomogeneous symmetric stochastic block model).: _Let \(n\) be an even positive integer, \(V=\{1,,n\}\), \(\{P_{1},P_{2}\}\) be a partitioning of \(V\) into two equally-sized subsets, and \(q<p\) be probabilities. Let \(\) be any probability distribution over graphs \(G=(V,E)\) such that for every \((v,w) P_{1} P_{1}\) and \((v,w) P_{2} P_{2}\), the edge \((v,w)\) appears in \(E\) independently with some probability \(p_{vw}[p,]\), and for every \((v,w) P_{1} P_{2}\), the edge \((v,w)\) appears in \(E\) independently with probability \(q\). We call such \(\) a nonhomogeneous symmetric stochastic block model (which we will abbreviate as NSSBM). We call the set of all such \(\) the family of nonhomogeneous stochastic block models with parameters \(p,,q\), written as \((n,p,,q)\)._

To visualize Model 1, consider the expected adjacency matrix of some NSSBM distribution. We then have the relations

\[[_{n/2}}{q_{n/ 2}}&_{n/2}}{p_{n/2}}\\  q_{n/2}&_{n/2}}{q _{n/2}}][ _{n/2}}{q_{n/2}}&_{n/2}}{ p_{n/2}}\\  q_{n/2}&_{n/2}}{p _{n/2}}]\,,\]

where the leftmost matrix denotes the expected adjacency matrix of \((n,p,q)\), the rightmost matrix denotes the expected adjacency matrix of \((n,,q)\), \(_{k}\) denotes the \(k k\) all-ones matrix, and \(_{P_{1}}\) and \(_{P_{2}}\) denote the edge probability matrices for edges internal to \(P_{1}\) and \(P_{2}\), respectively.

The above also shows that the rank of the expected adjacency matrix for \((n,p,q)\) is \(2\). However, the rank for the expected adjacency matrix for some NSSBM distribution may be as large as \((n)\). Perhaps surprisingly, this will turn out to be unimportant for our entrywise eigenvector perturbation analysis. In particular, the tools we use were originally designed for low-rank signal matrices or spiked low-rank signal matrices , but we will see that they can be adapted to the signal matrices we consider.

The NSSBM family generalizes the symmetric stochastic block model described in the previous section - this is attained by setting \(p_{vw}=p\) for all internal edges \((v,w)\). However, it can also encode biases for certain graph properties. For instance, a distribution from the NSSBM family may encode the idea that certain subsets of \(P_{1}\) are expected to be denser than \(P_{1}\) as a whole.

With this definition in hand, we are ready to formally state our first technical result in Theorem 1.

**Theorem 1**.: _Let \(p,,q\) be probabilities such that \(q<p\) and such that \(/(p-q)\) is an arbitrary constant. Let \((n,p,,q)\). Let \(n N()\) where the function \(N()\) only depends on \(\). There exists a universal constant \(C>0\) such that if_

\[n(p-q) C( n}+ n),\] (gap condition)

_then unnormalized spectral bisection is strongly consistent on \(\)._

We prove Theorem 1 in Appendix A.7.1. In fact, we show a somewhat stronger statement - in addition to the process described above, we also allow the adversary to, before sampling the graph, set a small number of the \(p_{vw}\) to \(1\) (at most \(n/ n\) edges per vertex). We detail this further in Appendix A.7.1.

We now remark on the tightness of our gap condition in Theorem 1. A work of Abbe, Bandeira, and Hall  identifies an exact information-theoretic threshold above which exact recovery with high probability is possible and below which no algorithm can be strongly consistent. In particular, the threshold states that for any \(p\) and \(q\) satisfying \(->\), exact recovery is possible, and when \(p\) and \(q\) do not satisfy this, exact recovery is information-theoretically impossible. Furthermore, Feige and Kilian  prove that the information-theoretic threshold does not change in a somewhat stronger semirandom model that includes the NSSBM family. Additionally, Deng, Ling, and Strohmer  show that unnormalized spectral bisection is strongly consistent all the way to this threshold in the special case where the graph is drawn from \((n,p,q)\). By contrast, our gap condition holds in the same critical degree regime as in the information-theoretic threshold (namely, \(p=( n/n)\)) but our constant is not optimal. We incur this constant loss because for the sake of presentation, we opt for a cleaner argument that can handle the nonhomogeneity and generalizes more readily across degree regimes. To our knowledge, none of these features are present in prior work analyzing spectral methods in an SSBM setting .

### Deterministic clusters model

Given Theorem 1, it is natural to ask what happens if we allow the adversary full control over the structure of the graphs in \(P_{1}\) and \(P_{2}\) instead of simply allowing the adversary to perturb the edge probabilities. In this section, we answer this question. We first describe a more adversarial semirandom model than the NSSBM family. We call this model the _deterministic clusters_ model, defined as follows.

**Model 2** (Deterministic clusters model).: _Let \(n\) be an even positive integer, \(V=\{1,,n\}\), \(\{P_{1},P_{2}\}\) be a partitioning of \(V\) into two equally-sized subsets, \(q\) be a probability, and \(d_{}\) be an integer degree lower bound. Consider a graph \(G=(V,E)\) generated according to the following process._

1. _The adversary chooses arbitrarily graphs_ \(G[P_{1}]\) _and_ \(G[P_{2}]\) _with minimum degree_ \(d_{}\)_;_
2. _Nature samples every edge_ \((v,w) P_{1} P_{2}\) _to be in_ \(E\) _independently with probability_ \(q\)_._
3. _The adversary arbitrarily adds edges_ \((v,w) P_{1} P_{1}\) _and_ \((v,w) P_{2} P_{2}\) _to_ \(E\) _after observing the edges sampled by nature._

_We call a distribution \(\) of graphs generated according to the above process a deterministic clusters model (DCM). We call the set of all such \(\) the family of deterministic clusters models with parameters \(d_{}\) and \(q\), written as \((n,d_{},q)\)._

The DCM graph generation process is heavily motivated by the one studied by Makarychev, Makarychev, and Vijayaraghavan . This model is much more flexible than the SSBM and NSSBM settings in that the graphs the adversary draws on \(P_{1}\) and \(P_{2}\) are allowed to look very far from random graphs. This means the DCM is a particularly good benchmark for algorithms to ensure they are not implicitly using properties of random graphs that might not hold in the worst case.

Within the DCM setting, we have Theorem 2.

**Theorem 2**.: _Let \(q\) be a probability and \(d_{}\) be an integer, and let \((n,d_{},q)\). For \(G\), let \(}\) denote the expectation of \(\) after step (2) but before step (3) in Model 2. There exists constants\(C_{1},C_{2},C_{3}>0\) such that for all \(n\) sufficiently large, if_

\[d_{} C_{1}(+) _{3}(})-_{2}(}) +C_{2}nq+C_{3}(+ n)\,,\]

_then unnormalized spectral bisection is strongly consistent on \(\)._

We prove Theorem 2 in Appendix A.7.2. We remark that, as in Theorem 1, the constants that appear in Theorem 2 are somewhat arbitrary. They are chosen to make our proofs cleaner and can likely be optimized.

As a basic application of Theorem 2, note that in the SSBM, if \(p=(1/)\) and \(q=1/\), then for \(n\) sufficiently large, with high probability, the resulting graph satisfies the conditions needed to apply Theorem 2. For a more interesting example, let \(P_{1}\) and \(P_{2}\) be two \(d\)-regular spectral expanders with \(d=()\) and let \(q 1/\). On top of both of these two graph classes, one can further allow arbitrary edge insertions inside \(P_{1}\) and \(P_{2}\) while still being guaranteed exact recovery from unnormalized spectral bisection.

### Inconsistency of normalized spectral clustering

Notice that in Theorem 1 and Theorem 2, we only address the strong consistency of the unnormalized Laplacian in our nonhomogeneous and semirandom models. But what happens when we run spectral bisection with the _normalized_ Laplacian?

In Theorem 3, we prove that there is a subfamily of instances belonging to \((n,p,,q)\) with \(=6p,q=p/2\) on which unnormalized spectral bisection is strongly consistent (following from Theorem 1) but normalized spectral clustering is inconsistent in a rather strong sense. Thus, one cannot obtain results similar to Theorem 1 and Theorem 2 for normalized spectral bisection.

**Theorem 3**.: _For all \(n\) sufficiently large, there exists a nonhomogeneous stochastic block model such that unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection (both symmetric and random-walk) incurs a misclassification rate of at least \(24\%\) with probability \(1-1/n\)._

We prove Theorem 3 in Appendix A.8. Furthermore, we expect that it is straightforward to adapt the example in Theorem 3 to prove an analogous result for our DCM setting.

The result of Theorem 3 may run counter to conventional wisdom, which suggests that normalized spectral clustering should be favored over the unnormalized variant . Perhaps a more nuanced view in light of Theorem 1 and Theorem 2 is to acknowledge that the normalized Laplacian and its eigenvectors enjoy stronger concentration guarantees , but the unnormalized Laplacian's second eigenvector is more robust to monotone adversarial changes.

### Open problems

Perhaps the most natural follow-up question inspired by our results is to determine whether the restriction that every internal edge probability \(p_{vw}\) can be lifted entirely while still maintaining strong consistency of the unnormalized Laplacian (Theorem 2). Another exciting direction for future work is to lower the degree and/or spectral gap requirement present in our results in the DCM setting (Theorem 2). Finally, we only study insertion-only monotone adversaries, as crossing edge deletions change the second eigenvector of the expected Laplacian. It would be illuminating to understand the robustness of Laplacian-based spectral algorithms against a monotone adversary that is also allowed to delete crossing edges. We are optimistic that the answers to one or more of these questions will further improve our understanding of the robustness of spectral clustering to "helpful" model misspecification.

## 3 Analysis sketch

First, let us give some intuition as to why one may expect that unnormalized spectral bisection is robust against our monotone adversaries. Here and in the sequel, let \(_{2}^{}=[_{n/2}-_{n/2}]/\), where \(_{k}\) denotes the all-\(1\)s vector in \(k\) dimensions and \(\) denotes vector concatenation. Let \(\) be the unnormalized Laplacian of the graph we want to partition, \(^{}[]\), \(-^{}\), and \(_{i}^{}_{i}(^{})\) for \(1 i n\). For an edge \((v,w)\), let \(_{vw}_{v}-_{w}\), so that \(_{vw}\) is an edge incidence vector corresponding to the edge \((v,w)\). Let \(p_{vw}\) be the probability that the edge \((v,w)\)appears in \(G\) and observe that \(^{}\) can be written as

\[^{}=_{(v,w) E_{}}p_{vw}_{vw} _{vw}^{T}+_{(v,w) E_{}}q_{vw}_{vw}^{T}\,,\]

where \(E_{}=(P_{1} P_{1})(P_{2} P_{2})\) and \(E_{}=P_{1} P_{2}\). We can verify that \(_{2}^{}\) is an eigenvector of \(^{}\) - indeed, we do so in Lemma A.14. And, for now, assume that \(_{2}^{}\) does correspond to the second smallest eigenvalue of \(^{}\) (in our NSSBM family, this is easily ensured by enforcing \(p>q\)). Moreover, for every internal edge \((v,w) E_{}\), we have \(_{vw},_{2}^{}=0\). Hence, any changes in internal edges do not change the fact that \(_{2}^{}\) is an eigenvector of the perturbed matrix. Thus, if the sampled \(\) is close enough to \(^{}\), then it is plausible that the second eigenvector of \(\), denoted as \(_{2}\), is pretty close to \(_{2}^{}\). In fact, the following conceptually stronger statement holds. If the subgraph formed by selecting just the crossing edges of \(G\) is regular, then \(_{2}^{}\) is an eigenvector of \(\). This follows from the fact that \(_{2}^{}\) is an eigenvector of the unnormalized Laplacian of any regular bipartite graph where both sides have size \(n/2\) and the previous observation that every internal edge is orthogonal to \(_{2}^{}\).

To make this perturbation idea more formal, we recall the Davis-Kahan Theorem. Loosely, it states that \(\|_{2}-_{2}^{}\|_{2}\|(- ^{})_{2}^{}\|_{2}/_{3}^{}- _{2}^{}\) (we give a more formal statement in Lemma A.15). Expanding the entrywise absolute value \(|(-^{})_{2}^{}|\) reveals that its entries can be expressed as \(2|_{}[v]-[_{}[v]] |/\), where \(_{}[v]\) denotes the number of edges incident to \(v\) crossing to the opposite community as \(v\). This is unaffected by any increase in the number of edges incident to \(v\) that stay within the same community as \(v\), denoted as \(_{}[v]\). Hence, regardless of how many internal edges we add before sampling or what substructures they encourage/create, if we have \(_{2}^{}_{3}^{}\), then we get \(\|_{2}-_{2}^{}\|_{2} o(1)\). This immediately implies that \(_{2}\) is a correct classifier on all but an \(o(1)\) fraction of the vertices.

**Entrywise analysis of \(_{2}\) and NSSBM strong consistency.**  In order to achieve strong consistency, we need that for all \(n\) sufficiently large, \(_{2}\) is a perfect classifier. Unfortunately, the above argument does not immediately give that. In particular, in the density and spectral gap regimes we consider, the bound of \(o(1)\) yielded by the Davis-Kahan theorem is not sufficiently small to directly yield \(\|_{2}^{}-_{2}\|_{2} 1/\). Instead, we carry out an entrywise analysis of \(_{2}\). A general framework for doing so is given by Abbe, Fan, Wang, and Zhong  and is adapted to the unnormalized and normalized Laplacians by Deng, Ling, and Strohmer .

At a high level, we adapt the analysis of Deng, Ling, and Strohmer  to our setting. We consider the intermediate estimator vector \((-_{2})^{-1}_{2}^{}\). This is a natural choice because we can verify \((-_{2})^{-1}_{2}=_{2}\). We will see that it is enough to show that this intermediate estimator correctly classifies all the vertices while satisfying \(|(-_{2})^{-1}(_{2}^{}- _{2})||(-_{2})^{-1} _{2}^{}|\) (again, the absolute value is taken entrywise). With this in mind, taking some entry indexed by \(v V\) and multiplying both sides by \([v]-_{2}\) (which we will show is positive with high probability), we see that it is enough to show

\[|_{v},_{2}^{}-_{2}|| _{v},_{2}^{}|=_{}[v]-_{}[v]|}{},\] (1)

where \(_{v}\) denotes the \(v\)-th row of \(\). The advantage of this rewrite is that the right hand side can be uniformly bounded, so it is enough to control the left hand side.

To argue about the left hand side of (1), it may be tempting to use the fact that \(_{v}\) is a Bernoulli random vector and use Bernstein's inequality to argue about the sum of rescalings of these Bernoulli random variables. Unfortunately, we cannot do this since \(_{2}\) and \(_{v}\) are dependent. To resolve this, we use a leave-one-out trick . We can think of this as leaving out the vertex \(v\) corresponding to the entry we want to analyze and sampling the edges incident to the rest of the vertices. The second eigenvector of the resulting \(^{(v)}\), denoted as \(_{2}^{(v)}\), is a very good proxy for \(_{2}\) and is independent from \(_{v}\). Hence, we may complete the proof of Theorem 1.

One of our main observations is that although this style of analysis was originally built for low-rank signal matrices , it can be adapted to handle the nonhomogeneity inside \(P_{1}\) and \(P_{2}\). In particular, the nonhomogeneity we permit in the NSSBM family may make \(^{}\) look very far from a spiked low-rank signal matrix. Furthermore, our entrywise analysis of eigenvectors under perturbations is one of the first that we are aware of that moves beyond analyzing low-rank signal matrices or spiked low-rank signal matrices.

**Extension to deterministic clusters.**  To prove Theorem 2, we start again at (1). An alternate way to upper bound the left hand side is to use the Cauchy-Schwarz inequality. A variant of the Davis-Kahan theorem gives us control over \(_{2}-_{2}^{}_{2}\) while \(_{v}_{2}=[v]}\). The advantage of this is that we get a worst-case upper bound on the left hand side of (1) - it holds no matter what edges orthogonal to \(_{2}^{}\) are inserted before or after nature samples the crossing edges (which are precisely the internal edges). Combining these and using the fact that the right hand side of (1) is increasing in \(_{}[v]\) (and increases faster than \(_{v}_{2}=[v]}\)) allows us to complete the proof of Theorem 2.

**Inconsistency of normalized spectral bisection.**  Finally, we describe the family of hard instances we use to prove Theorem 3. To motivate this family of instances, recall that by the graph version of Cheeger's inequality, the second eigenvalue of \(\) and the corresponding eigenvector can be used to find a sparse cut in \(G\). Thus, if we create sparse cuts inside \(P_{1}\) that are sparser than the cut formed by separating \(P_{1}\) and \(P_{2}\), then conceivably the normalized Laplacian's second eigenvector may return the new sparser cut.

To make this formal, consider the following graph structure. Let \(n\) be a multiple of \(4\). Let \(L_{1}\) consist of indices \(1,,n/4\), \(L_{2}\) consist of indices \(n/4+1,,n/2\), and \(R\) consist of indices \(n/2+1,,n\). Consider the block structure induced by the matrix \(^{}=[]\) shown in Table 1.

Intuitively, as \(K\) gets larger, the cut separating \(L_{1}\) from \(V L_{1}\) becomes sparser. From Cheeger's inequality, this witnesses a small \(_{2}()\) and therefore the corresponding \(_{2}()\) may return the cut \(L_{1},V L_{1}\). We formally prove that this is indeed what happens when \(K\) is a sufficiently large constant and then Theorem 3 follows.

## 4 Numerical trials

We programmatically generate synthetic graphs that help illustrate our theoretical findings using the libraries NetworkX 3.3 (BSD 3-Clause license), SciPy 1.13.0 (BSD 3-Clause License), and NumPy 1.26.4 (modified BSD license) . We ran all our experiments on a free Google Colab instance with the CPU runtime, and each experiment takes under one hour to run. In this section we focus on a setting that allows relating Theorem 1 and Theorem 3, and defer more experiments that investigate both NSSBM and DCM graphs to Appendix B.

To put Theorem 1 and Theorem 3 in perspective, we consider graphs generated following the process outlined in the proof of Theorem 3, which gives rise to the following benchmark distribution.

**Benchmark distribution.**  Let \(n\) be divisible by \(4\) and let \(\{P_{1},P-2\}\) be a partitioning of \(V=[n]\) into two equally-sized subsets. Let \(\{L_{1},L_{2}\}\) be a bipartition of \(P_{1}\) such that \(|L_{1}|=|L_{2}|=n/4\) and call \(L=P_{1},\,R=P_{2}\) for convenience as in the proof of Theorem 3. Then, for some \(p,,q\) such that \(q p\), consider the distribution \(_{p,,q}\) over graphs \(G=(V,E)\) obtained by sampling every edge \((u,v)(L_{1} L_{1})(L_{2} L_{2})\) independently with probability \(\), every edge \((u,v)(L_{1} L_{2})(R R)\) independently with probability \(p\), and every edge \((u,v) L R\) independently with probability \(q\). One can see that \(_{p,,q}\) is in fact in the set \((n,p,,q)\).

**Setup.**  Let us fix \(n=2000\), \(p=24 n/n\), \(q=8 n/n\). For varying values of \(\) in the range \([p,1]\), we sample \(t=10\) independent draws \(G\) from \(_{p,,q}\). For each of them, we run spectral bisection (i.e. Algorithm 1) with matrices \(,_{},_{},\). Then, we compute the _agreement_ of the bipartition hence obtained (with respect to the planted bisection), that is the fraction of correctly classified vertices. We average the agreement across the \(t\) independent draws. The results are shown in the top left plot of Fig. 1. Another natural way to get a bipartition of \(V\) from the eigenvector is a _sweep cut_. In a sweep cut, we sort the entries of \(_{2}\) and take the vertices corresponding to the smallest \(n/2\) entries to be on one side of the bisection and put the remaining on the other side. The average agreement obtained in this other fashion is shown in the bottom left plot of Fig. 1.

  & \(L_{1}\) & \(L_{2}\) & \(R\) \\  \(L_{1}\) & \(Kp_{n/4 n/4}\) & \(p_{n/4 n/4}\) & \(q_{n/2 n/2}\) \\  \(R\) & \(q_{n/2 n/2}\) & \(p_{n/2 n/2}\) \\ 

Table 1: \(^{}\) for Theorem 3 is defined to have the above block structure.

Theoretical framing.As per Theorem 1, we expect unnormalized spectral bisection to achieve exact recovery (i.e. agreement equal to \(1\)) whenever \(_{}\), where

\[_{}=}{n n}\] (2)

is obtained by rearranging the precondition of Theorem 1, ignoring the constants and disregarding the fact that \(\) should be \(O(1)\). On the contrary, the proof of Theorem 3 shows that normalized spectral bisection misclassifies a constant fraction of vertices provided that \(p/q 2\) (which our choice of parameters satisfies) and \(_{}\), where

\[_{}=3 p^{2}/q\,.\] (3)

In Fig. 1, the solid vertical line corresponds to the value of \(_{}\) on the \(x\)-axis, and the dashed vertical line corresponds to the value of \(_{}\) on the \(x\)-axis. In particular, observe that in our setting \(_{}<_{}\), so there is an interval of values for \(\) where we expect Theorem 1 and Theorem 3 to apply simultaneously.

Empirical evidence: consistency.One can see from the top left plot in Fig. 1 that the agreement of unnormalized spectral bisection is \(100\%\) for all values of \(\), even beyond \(_{}\) and \(_{}\). On the other hand, the agreement of the bipartition obtained from all other matrices (hence including normalized spectral bisection) drops below \(70\%\) well before the threshold \(_{}\) predicted by Theorem 3. From the right plot in Fig. 1, we see that computing the bipartition by taking a sweep cut of \(n/2\) vertices does not change the results - \(_{2}\) of the unnormalized Laplacian continues to achieve \(100\%\) agreement, while for all other matrices the corresponding \(_{2}\) remains inconsistent.

Empirical evidence: embedding variance.From the setting of the experiment we just illustrated, observe that as we increase \(\), we expect the subgraph \(G[L]\) to have increasing volume. As illustrated in Fig. 1, this seems to correlate with a decrease in the "variance" of the second eigenvector \(_{2}\) of the unnormalized Laplacian with respect to the ideal second eigenvector \(_{2}^{}\). More precisely, we compute the average distance squared of the embedding of a vertex in \(_{2}\) from its ideal embedding in \(_{2}^{}\), i.e. the quantity

\[_{s\{ 1\}}\|_{2}-s_{2}^{} \|_{2}^{2}\,.\] (4)

This suggests that not only does the second eigenvector of the unnormalized Laplacian remain robust to monotone adversaries, but it actually concentrates more strongly around the ideal embedding \(_{2}^{}\).

Empirical evidence: example embedding.Let us fix the value \(=_{}\), for which we see in Fig. 3 that all matrices except the unnormalized Laplacian fail to recover the planted bisection. We generate a graph from \(_{p,,q}\), and plot how the vertices are embedded in the real line by the second eigenvector of all the matrices we consider. The result is shown in Fig. 1, where the three horizontal dashed lines, from top to bottom, respectively correspond to the value of \(1/,0,-1/\) on the \(y\)-axis.

### Related work

Community detection.Community detection has garnered significant attention in theoretical computer science, statistics, and data science. For a general overview of recent progress and related literature, see the survey by Abbe . In what follows, we discuss the works we believe are most related to what we study in this paper.

As mentioned in the introduction, perhaps the most fundamental and well-studied model is the symmetric stochastic block model (SSBM), due to . The celebrated work of Abbe, Bandeira, and Hall  gives sharp bounds on the threshold for exact recovery for the SSBM setting. They complement their result by showing that SDP based methods can achieve the information theoretic lower bound for the planted bisection problem, even with a monotone adversary . A line of work  demonstrates that natural spectral algorithms achieve exact recovery for the SSBM all the way to the information-theoretic threshold.

Generalizations of the symmetric stochastic block model.Since the introduction of SBMs , numerous variants have been proposed that are designed to better reflect real-world graph properties. For instance, real-life social networks are likely to contain triangles. To address this, Sankararaman and Baccelli  introduced a spatial stochastic block model, sometimes known as the geometric stochastic block model (GSBM). Other variations were introduced in the works of . Subsequent work studies the performance of spectral algorithms on certain Gaussian or Geometric Mixture block models .

Studying community detection with a semirandom model approaches this modeling question differently. Rather than implicitly encouraging a particular structure within the clusters like the models just mentioned, a semirandom adversary (including the ones we study in this paper) can more directly test the robustness of the algorithm to specially designed substructures.

**Semirandom and monotone adversaries.** As far as we are aware, Blum and Spencer  were the first to introduce a semirandom model. Within this model, they studied graph coloring problems. Feige and Kilian  demonstrated that semidefinite programming methods can accurately recover communities up to a certain threshold, even in the semi-random setting. Other problems, such as detecting a planted clique , have also been studied in the semi-random model of . In the setting of planted clique, a natural spectral algorithm fails against monotone adversaries . Monotone adversaries and semirandom models have also been extensively studied for other statistical and algorithmic problems . Finally,  shows that a spectral heuristic due to Boppana  is robust under a monotone adversary that is allowed to both insert internal edges and delete crossing edges. However, as far as we are aware, this algorithm does not fit in the framework of Algorithm 1.

We remark that the models we study in this paper are most closely related to models studied by  and . In particular, allowing increased internal edge probabilities is analogous to Massart noise in classification problems, and our model with adversarially chosen internal edges can be seen as the same model as that studied in  (although without allowing crossing edge deletions). Finally, note that Cohen-Addad, d'Orsi, and Mousavifar  give a near-linear time algorithm for graph clustering in the model of , though they do not explicitly show their algorithm is strongly consistent on instances that are information-theoretically exactly recoverable.

Figure 1: **Top left, bottom left**: Agreement with the planted bisection of the bipartition obtained from several matrices associated with an input graph generated from a distribution in \((n,p,,q)\) for fixed values of \(n,p,q\) and varying values of \(\). In the top left plot, the bipartition is the \(0\)-cut of the second eigenvector, as in Algorithm 1. In the bottom left plot, the bipartition is the sweep cut of the first \(n/2\) vertices in the second eigenvector. The dashed vertical line corresponds to \(_{}=_{}(n,p,q)\) (see (2)), and the solid vertical line corresponds to \(_{}=_{}(n,p,q)\) (see (3)). **Top middle, top right, bottom middle**: Embedding of the vertices given by the second eigenvector \(_{2}\) of several matrices associated with a graph sampled from \(_{p,,q}\) with \(=_{}\). Horizontal dashed lines, from top to bottom, correspond to \(1/,0,-1/\) respectively. **Bottom right**: Variance of the embedding in the second eigenvector \(_{2}\) of the unnormalized Laplacian with respect to the ideal eigenvector \(_{2}^{*}\) (see (4)), for input graphs generated from a distribution in \((n,p,,q)\) with fixed values of \(n,p,q\) and varying values of \(\).

Acknowledgments. AB was partially supported by the National Science Foundation under Grant Nos. CCF-2008688 and CCF-2047288. NSM was supported by a National Science Foundation Graduate Research Fellowship. We thank Avrim Blum and Yury Makarychev for helpful discussions. We thank Nirmit Joshi for pointing us to the reference .