# FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations

Chanakya Ekbote

Microsoft Research India

chanakyaekbote@gmail.com

&Ajinkya Pankaj Deshpande*

Microsoft Research India

ajinkya.deshpande56@gmail.com

&Arun Iyer

Microsoft Research India

ariy@microsoft.com

&Ramakrishna Bairi

Microsoft Research India

rkbairi@gmail.com

&Sundararajan Sellamanickam

Microsoft Research India

ssrajan@microsoft.com

Both authors contributed equally to this work. Work done while the authors were Research Fellows at Microsoft Research India.

###### Abstract

Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe, achieves an average gain of up to 4.4%, compared to the state-of-the-art unsupervised models, across all datasets in consideration, both homophilic and heterophilic. Our code can be found at: https://github.com/microsoft/figure.

## 1 Introduction

Contrastive learning is a powerful method for unsupervised graph representation learning, achieving notable success in various applications [44; 15]. However, these evaluations typically focus on tasks exhibiting homophily, where task labels strongly correlate with the graph's structure. An existing edge suggests the connected nodes likely share similar labels in these scenarios. However, these representations often struggle when dealing with heterophilic tasks, where edges tend to connect nodes with different labels. Several papers [7; 17; 4; 28] have tackled the problem of heterophily by leveraging information from both low and high-frequency components. However, these methods operate in the semi-supervised setting, and the extension of these ideas in unsupervised learning still needs to be explored. Inspired by the insights in these papers, we propose a simple method incorporating these principles.

Our approach introduces filter banks as additional views and learns separate representations for each filter bank. However, this approach faces two main challenges: Firstly, storing representations from each view can become prohibitively expensive for large graphs; secondly, contrastive learningmethods typically demand high-dimensional representations, which increase both the computational cost of training and the storage burden. We employ a shared encoder for all filter banks to tackle the first challenge. Our results confirm that a shared encoder performs on par with independent encoders for each filter bank. This strategy enables us to reconstruct filter-specific representations as needed, drastically reducing the storage requirement. For the second challenge, we train our models with low-dimensional embeddings. Then, we use random Fourier feature projection  to lift these low-dimensional embeddings into a higher-dimensional space. Kernel tricks  were typically used in classical machine learning to project low-dimensional representation to high dimensions where the labels can become linearly separable. However, constructing and leveraging the kernels in large dataset scenarios could be expensive. To avoid this issue, several papers [40; 41; 20; 36; 26] proposed to approximate the map associated with the kernel. For our scenario, we use the map associated with Gaussian kernel . We empirically demonstrate that using such a simple approach preserves high performance for downstream tasks, even in the contrastive learning setting. Consequently, our solution offers a more efficient approach to unsupervised graph representation learning in computation and storage, especially concerning heterophilic tasks. The proposed method exhibits simplicity not only in the augmentation of filters but also in its ability to learn and capture information in a low-dimensional space, while still benefiting from the advantages of large-dimensional embeddings through Random Fourier Feature projections.

Our contributions in this work are, 1] We propose a simple scheme of using filter banks for learning representations that can cater to both heterophily and homophily tasks, 2] We address the computational and storage burden associated with this simple strategy by sharing the encoder across these various filter views, 3] By learning a low-dimensional representation and later projecting it to high dimensions using random Fourier Features, we further reduce the burden, 4] We study the performance of our approach on four homophilic and seven heterophilic datasets. Our method, FiGURe, achieves an average gain of up to 4.4%, compared to the state-of-the-art unsupervised models, across all datasets in consideration, both homophilic and heterophilic. Notably, even without access to task-specific labels, FiGURe performs competitively with supervised methods like GCN .

## 2 Related Work

Several unsupervised representation learning methods have been proposed in prior literature. Random walk-based methods like Node2Vec  and DeepWalk  preserve node proximity but tend to neglect structural information and node features. Contrastive methods, such as Deep Graph InfoMax (DGI) , maximize the mutual information (MI) between local and global representations while minimizing the MI between corrupted representations. Methods like MVGRL  and GRACE  expand on this, by integrating additional views into the MI maximization objective. However, most of these methods focus on the low frequency components, overlooking critical insights from other parts. Semi-supervised methods like GPRGNN , BernNet , and PPGNN  address this by exploring the entire eigenspectrum, but these concepts are yet to be applied in the unsupervised domain. This work proposes the use of a filter bank to capture information across the full eigenspectrum while sharing an encoder across filters. Given the high-dimensional representation demand of contrastive learning methods, we propose using Random Fourier Features (RFF) to project lower-dimensional embeddings into higher-dimensional spaces, reducing computational load without sacrificing performance. The ensuing sections define our problem, describe filter banks and random feature maps, and explain our model and experimental results.

## 3 Problem Setting

In the domain of unsupervised representation learning, our focus lies on graph data, denoted as \(=(,)\), where \(\) is the set of vertices and \(\) the set of edges (\(\)). We associate an adjacency matrix with \(\), referred to as \(:\{0,1\}^{n n}\), where \(n=||\) corresponds to the number of nodes. Let \(^{n d}\) be the feature matrix. We use \(}\) to represent \(+\) with \(\) is the identity matrix, while \(}}\) signifies the degree matrix of \(}\). We also define \(}\) as \(}^{-1/2}}}}^{-1/2}}\). No additional information is provided during training. The goal is to learn a parameterized encoder, \(E_{}:^{n n}^{n d}^ {n d^{}}\), where \(d^{} d\). This encoder produces a set of node representations \(E_{}(,})=\{h_{1},h_{2},...,h_{n}\}\) where each \(h_{i}^{d^{}}\) represents a rich representation for node \(i\). The subsequent section will provide preliminary details about filter banks and random feature maps before we discuss the specifics of the proposed approach.

## 4 Preliminaries

Our approach relies on filter banks and random feature maps. In this section, we briefly introduce these components, paving the way for a detailed explanation of our approach.

### Filter Banks

Graph Fourier Transform (GFT) forms the basis of Graph Neural Networks (GNNs). A GFT is defined using a reference operator \(\) which admits a spectral decomposition. Traditionally, in the case of GNNs, this reference operator has been the symmetric normalized laplacian \(_{}=-_{}\) or the \(_{}\) as simplified in . A graph filter is an operator that acts independently on the entire eigenspace of a diagonalisable and symmetric reference operator \(\), by modulating their corresponding eigenvalues. [43; 42]. Thus, a graph filter \(\) is defined via the graph filter function \(g(.)\) operating on the reference operator as \(=g()=g()^{T}\). Here, \(=diag([_{1},_{2},...,_{n}])\), where \(_{i}\) denotes the eigenvalues of the reference operator. We describe a filter bank as a set of filters, denoted as \(=\{_{1},_{2},...,_{K}\}\). Both GPRGNN  and BernNet employ filter banks, comprising of polynomial filters, and amalgamate the representations from each filter bank to enhance the performance across heterophilic datasets. GPRGNN uses a filter bank defined as \(_{}=\{,_{},...,_{}^{K-1}\}\), while \(_{}=\{_{0},_{1},...,_ {K-1}\}\) characterizes the filter bank utilized by BernNet. Here, \(_{i}=}{{K-1}}(2-_{ })^{K-i-1}(_{})^{i}\). Each filter in these banks highlights different parts of the eigenspectrum. By tuning the combination on downstream tasks, it offers the choice to select and leverage the right spectrum to enhance performance. Notably, unlike traditional GNNs, which primarily emphasize low-frequency components, higher frequency components have proved useful for heterophily [4; 7; 17; 28]. Consequently, a vital takeaway is that **for comprehensive representations, we must aggregate information from different parts of the eigenspectrum and fine-tune it for specific downstream tasks**.

### Random Feature Maps for Kernel Approximations

Before the emergence of deep learning models, the kernel trick was instrumental in learning non-linear models. A kernel function, \(k:^{d}^{d}\), accepts two input features and returns a real-valued score. Given a positive-definite kernel, Mercer's Theorem  assures the existence of a feature map \(()\), such that \(k(x,y)=(x),(y)\). Leveraging the kernel trick, researchers combined Mercer's theorem with the representer theorem , enabling the construction of non-linear models that remain linear in \(k\). These models created directly using \(k\) instead of the potentially complex \(\), outperformed traditional linear models. The implicit maps linked with these kernels projected the features into a significantly high-dimensional space, where targets were presumed to be linearly separable. However, computational challenges arose when dealing with large datasets. Addressing these issues, subsequent works [41; 20; 36; 40] introduced approximations of the map associated with individual kernels through random projections into higher-dimensional spaces (\(^{}(.)\)). This approach ensures that \(^{}(),^{}() k(x,y)\). These random feature maps are inexpensive to compute and affirm that simple projections to higher-dimensional spaces can achieve linear separability. The critical insight is that **computationally efficient random feature maps, such as Random Fourier features (RFF) , exist. These maps project lower-dimensional representations into higher dimensions, enhancing their adaptability for downstream tasks..**

## 5 Proposed Approach

The following section delineates the process of unsupervised representation learning. Then, we detail the use of filter bank representations in downstream tasks with random feature maps.

### Unsupervised Representation Learning

Our method FiGURe (**F**ilter-based **G**raph **U**nsupervised **R**epresentation Learning) builds on concepts introduced in [18; 44], extending the maximization of mutual information between node and global filter representations for each filter in the filter bank \(=\{_{1},_{2},..._{K}\}\). In this approach, we employ filter-based augmentations, treating filter banks as "additional views" within the context of contrastive learning schemes. In the traditional approach, alternative baselines like DGI have employed a single filter in the GPRNN filter bank. Additionally, MVGRL attempted to use the diffusion kernel; nevertheless, they only learned a single representation per node. We believe that this approach is insufficient for accommodating a wide range of downstream tasks. We construct an encoder for each filter to maximize the mutual information between the input data and encoder output. For the \(i^{}\) filter, we learn an encoder, \(E_{}:_{i}_{i}^{}\), denoted by learnable parameters \(\). In this context, \(_{i}\) represents a set of examples, where each example \([_{ij}},_{ij}}]_{i}\) consists of a filter \(_{i}\), its corresponding nodes and node features, drawn from an empirical probability distribution \(_{i}\). That is, \(_{i}\) captures the joint distribution of a filter \(_{i}\), its corresponding nodes and node features (\([,_{i}]\)). Note that \([_{ij}},_{ij}}]\) denote nodes, node features and edges sampled from the \(i^{th}\) filter (\([,_{i}]\)) basis the probability distribution \(_{i}\). Note that \(_{ij}}^{N^{} d}\) and \(_{ij}}^{N^{} N^{}}\). \(_{i}^{}\) defines the set of representations learnt by the encoder on utilizing feature information as well as topological information from the samples, sampled from the joint distribution \(_{i}\). The goal, aligned with , is to identify \(\) that maximizes mutual information between \([,_{i}]\) and \(E_{}(,_{i})\), or \(_{i}([,_{i}],E_{}(,_ {i}))\). While exact mutual information (MI) computation is infeasible due to unavailable exact data and learned representations distributions, we can estimate the MI using the Jensen-Shannon MI estimator , defined as:

\[_{i,,}^{}([,_{i}],E_{}(,_{i})):=_{_{i}}[-(-T_{,}([_{ij}},_{ij}}],E_{}(_{ij}},_{ ij}}))]-\\ _{_{i}}_{i}}[ (T_{,}([_{ij}},_{ ij}}],E_{}[_{ij}},_{ij}}])])\] (1)

Here, \(T_{}:_{i}^{}{}_{i}\) represents a discriminator function with learnable parameters \(\). Note that \([_{ij}},_{ij}}]\) is an input sampled from \(}_{i}\), which denotes a distribution over the corrupted input data (more details given below). The function sp(.) corresponds to the softplus function . Additionally, \(T_{,}([h_{ij}]_{1},[h_{ij}]_{2})=D_{w}(([h_{ij}]_{1 }),[h_{ij}]_{2})\), where \(\) denotes the readout function responsible for summarizing all node representations by aggregating and distilling information into a global filter representation. We introduce a learnable discriminator \(D_{}\), where \(D_{}(.,.)\) represents the joint probability score between the global representation and the node-specific patch representation. Note that \([h_{ij}]_{1}\), denotes representations obtained after passing samples sampled from the original distribution \(_{i}\), to the encoder, and \([h_{ij}]_{2}\), denotes representations obtained after passing samples sampled from the original distribution \(_{i}\) or samples sampled from the corrupted distribution \(}_{i}\) to the encoder. Intuitively, Eq. 1 implies that we would want to maximize the mutual information between the local (patch) representations of the nodes and their corresponding global graph representation, while minimising the mutual information between a global graph representation and the local representations of corrupted input data. Note that: \([_{ij}},_{ij}}]\) denotes a corrupted version of input features and given filter. More details with regards to this will be given below. In our approach, we first obtain node representations by feeding the filter-specific topology and associated node features into the encoder: \(_{ij}=E_{}(_{ij},_{ij})=\{h_{1}^{ _{ij}},h_{2}^{_{ij}},...,h_{N^{}}^{_{ij}}\}\). Note that \(_{ij}\) has the following dimensions \(^{N^{} d^{}}\).To obtain global representations, we employ a readout function \(:^{N^{} d^{}}^{d^{ }}\), which combines and distills information into a global representation \(h_{g}^{F_{ij}}=(_{ij})=(E_{}(_{ ij},_{ij}))\). Instead of directly maximizing the mutual information between the local and global representations, we maximize \(D_{}(.,.)\). This joint score should be

Figure 1: Unsupervised learning of node embeddings by maximizing mutual information between node and graph representations over the graphs from the filter bank. Note that the parameter \(\) is shared across all the filters.

higher when considering global and local representations obtained from the same filter, as opposed to the joint score between the global representation from one filter and the local representation from corrupted \((_{ij},_{ij})\). To generate negative samples for contrastive learning, we employ a corruption function \(:^{N^{} d}^{N^{} N ^{}}^{N^{} d}^{N^{ } N^{}}\), which yields corrupted samples denoted as \([_{ij}},_{ij}}]=( _{ij},_{ij})\). The designed corruption function generates data decorrelated with the input data. Note that \([_{ij}},_{ij}}]\) denote nodes, node features and edges sampled from the corrupted version of \(i^{th}\) filter, basis the probability distribution \(}_{i}\). The corruption function can be designed basis the task at hand. We employ a simple corruption function, whose details are present in the experimental section. Let the corrupted node representations be as follows: \(}_{ij}=E_{}((_{ij},_{ij}))=E_{}(_{ij}},_{ij}})= \{h_{1}^{_{ij}}},h_{2}^{_{ij}}},...,h_{N^{}}^{_{ij}}}\}\). In order to learn representations across all filters in the filter bank, we aim to maximise the average estimate of mutual information (MI) across all filters, considering \(K\) filters, defined by \(_{}\).

\[_{}=_{i=1}^{K}_{i,, }^{JSD}([,_{i}],E_{}(,_{ i}))\] (2)

Maximising the Jenson-Shannon MI estimator can be approximately optimized by reducing the binary cross entropy loss defined between positive samples (sampled from the \(_{i}\)) and the negative samples (sampled from \(}_{i}\)). Therefore, for each filter, the loss for is defined as follows:

\[_{_{i}1}=-}_{([_ {ij},_{ij}]_{i})}(_{k=1}^{N^{}}[(D_{}(h_{k}^{_{ij}},h_{g}^{_{ij}}))+(1-D_{}(^{_{ij}}},h_{g}^{_{ij}}))])\] (3)

Note that for Eq. 3, the global representation \((h_{g}^{_{ij}})\) is generated by \((E_{}(_{ij},_{ij}))\). The local representations \((^{_{ij}}}\;\;k)\) are constructed by passing the sampled graph and features through a corruption function (see \(}_{ij}\)). Therefore to learn meaningful representations across all filters the following objective is minimised:

\[=_{i=1}^{K}_{_{i}}\] (4)

Managing the computational cost and storage demands for large graphs with distinct node representations for each filter poses a challenge, especially when contrastive learning methods require high dimensions. To address this, we employ parameter sharing, inspired by studies like  and . This approach involves sharing the encoder's parameters \(\) and the discriminator's parameters \(\) across all filters. Instead of storing dense filter-specific node representations, we only store the shared encoder's parameters and the first-hop neighborhood information for each node per filter, reducing storage requirements. To obtain embeddings for downstream tasks, we reconstruct filter-specific representations using a simple one-layer GNN. This on-demand reconstruction significantly reduces computational and storage needs associated with individual node representations. Fig 1 illustrates such a simple encoder's mutual information-based learning process. To address the second challenge, we initially train our models to produce low-dimensional embeddings that capture latent classes, as discussed in . These embeddings, while informative, lack linear separability. To enhance separability, we project these low-dimensional embeddings into a higher-dimensional space using random Fourier feature (RFF) projections, inspired by kernel methods (see Section 4.2). This approach improves the linear separability of latent classes, as confirmed by our experimental results in Section 6.2, demonstrating the retention of latent class information in these embeddings.

### Supervised Representation Learning

After obtaining representations for each filter post the reconstruction of the node representations, learning an aggregation mechanism to combine information from representations that capture different parts of the eigenspectrum for the given task is necessary. We follow learning schemes from [7; 17; 28],learning a weighted combination of filter-specific representations. The combined representations for downstream tasks, considering \(K\) filters from filter bank \(\), are as follows:

\[Z=_{i=1}^{K}_{i}^{}(E_{}(,_{i}))\] (5)

The parameters \(_{i}\)'s are learnable. Additionally, the function \(^{}(.)\) represents either the RFF projection or an identity transformation, depending on whether \(E_{}(,_{i})\) is low-dimensional or not. A classifier model (e.g. logistic regression) consumes these embeddings, where we train both the \(_{i}\)'s and the weights of the classifier. Fig 2 illustrates this process. Notably, semi-supervised methods like [28; 7; 17] differ as they learn both encoder and coefficients from labeled data, while our method pre-trains the encoder and learns task-specific combinations of filter-specific representations.

## 6 Experimental Results

**Training Details**: We define a single-layer graph convolutional network (GCN) with shared weights (\(\)) across all filters in the filter bank (\(\)) as our encoder. Therefore, the encoder can be expressed as follows: \(E_{}(,_{i})=(_{i})\). It is important to note that \(_{i}\) represents a normalized filter with self-loops, which ensures that its eigenvalues are within the range of [0; 2]. The non-linearity function \(\) refers to the parametric rectified linear unit . As we work with a single graph, we obtain the positive samples by sampling nodes from the graph. Using these sampled nodes, we construct a new adjacency list that only includes the edges between these sampled nodes in filter \(_{i}\). On the other hand, the corruption function \(\) operates on the same sampled nodes. However, it randomly shuffles the node features instead of perturbing the adjacency list. In essence, this involves permuting the node features while maintaining a consistent graph structure. This action introduces a form of corruption, altering each node's feature to differ from its original representation in the data. Similar to , we employ a straightforward readout function that involves averaging the representations across all nodes for a specific filter \(_{i}\): \((_{i})=(_{j=0}^{N}{h_{j}^{ _{i}}})\) where \(\) denotes the sigmoid non-linearity. We utilize a bilinear scoring function \(D_{}(.,.)\), whose parameters are also shared across all filters, where \(D_{}({h_{j}^{_{i}}},{h_{q}^{_{i}}})=({h_{j}^{ _{i}}}^{T}^{_{i}}})\). We learn the encoder and discriminator parameters by optimising Eq. 4. While we could use various filter banks, we specifically employ the filter bank corresponding to GPRCNN (\(_{}\)) for all our experiments. However, we conduct an ablation study (see 6.5) comparing \(_{}\) with \(_{}\). Additional training details are available in 8.3.

We conducted a series of comprehensive experiments to evaluate the effectiveness and competitiveness of our proposed model compared to SOTA models and methods. These experiments address the following research questions: **[RQ1]** How does FiGURe, perform compared to SOTA unsupervised models? **[RQ2]** Can we perform satisfactorily even with lower dimensional representations using projections such as RFF? **[RQ3]** Does shared encoder decrease performance? **[RQ4]** What is the computational efficiency gained by using lower dimensional representations compared to methods that rely on higher dimensional representations? **[RQ5]** Can alternative filter banks be employed to

Figure 2: Supervised Learning: Using the trained parameter \(\), we generate the node embeddings by encoding the filtered graphs that get consumed in the classification task.

[MISSING_PAGE_FAIL:7]

average performance drops of approximately \(5\)% and \(10\)%, respectively. This result indicates that FiGURe\({}_{512}\) learns representations that exhibit high generalization and task-agnostic capabilities. Another important observation is the effectiveness of RFF projections in improving lower dimensional representations. We compared FiGURe at different dimensions, including FiGURe\({}_{32}^{}\) and FiGURe\({}_{128}^{}\), corresponding to learning \(32\) and \(128\)-dimensional embeddings, respectively, in addition to the baseline representation size of \(512\) dimensions. Remarkably, even at lower dimensions, FiGURe with RFF projections demonstrates competitive performance across datasets, surpassing the \(512\)-dimensional baselines in several cases. This result highlights the effectiveness of RFF projections in enhancing the quality of lower dimensional representations. Using lower-dimensional embeddings reduces the computation time and makes FiGURe faster than the baselines. The computational efficiency of reducing dimension size becomes more significant with larger datasets, as evidenced in Table 3. On arXiv-Year, a large graph with \(169,343\) nodes, 128-dimensional embeddings yield a 1.6x speedup, and 32-dimensional embeddings yield a 1.7x speedup. Similar results are observed in OGBN-arXiv. For further insights into the effectiveness of RFFprojections, see Section 6.2, and for computational efficiency gains, refer to Section 6.4. In Table 2, we include GCN as a benchmark for comparison. Remarkably, FiGURe\({}_{512}\) remains competitive across most datasets, sometimes even surpassing GCN. This highlights that FiGURe\({}_{512}\) can capture task-specific information required by downstream tasks, typically handled by GCN, through unsupervised means. When considering a downstream task, using FiGURe\({}_{512}\) embeddings allows for the use of computationally efficient models like Logistic Regression, as opposed to training resource-intensive end-to-end graph neural networks. There are, however, works such as , , and , that explore methods to speed up end-to-end graph neural network training. In summary, FiGURe offers significant computational efficiency advantages over end-to-end supervised graph neural networks. Performance can potentially improve further by incorporating non-linear models like MLP. For detailed comparisons with other supervised methods, please refer to 8.4. It's noteworthy that both OGBN-arXiv and arXiv-Year use the arXiv citation network but differ in label prediction tasks (subject area and publication year, respectively). FiGURe demonstrates improvements in both cases, showcasing its task-agnostic and multi-task capabilities due to its flexible node representation. This flexibility enables diverse tasks to extract the most relevant information (see Section 5.2), resulting in strong overall performance. Note that in 8.5, we have also performed an ablation study where the depth of the encoder is increased.

### RQ2: RFF Projections on Lower Dimensional Representations

In this section, we analyse the performance of unsupervised baselines using \(32\)-dimensional embeddings with and without RFF projections (see Table 4). Despite extensive hyperparameter tuning, we could not replicate the results reported by SUGRL, so we present the best results we obtained. Two noteworthy observations emerge from these tables. Firstly, it is evident that lower dimensional embeddings can yield meaningful and linearly separable representations when combined with simple RFF projections. Utilising RFF projections enhances performance in almost all cases, highlighting the value captured by MI-based methods even with lower-dimensional embeddings. Secondly, FiGURe\({}_{32}^{}\) consistently achieves superior or comparable performance to the baselines, even in lower dimensions. Notably, this includes SUGRL, purported to excel in such settings. However, there is a

   & rFF & cora & citeseer & squirrel & chameleon \\  DGI & \(\) & **81.65 (1.90)** & 65.62 (2.39) & 31.60 (2.19) & 45.48 (3.02) \\  & ✓ & 81.49 (1.96) & **66.50 (2.44)** & **38.19 (1.52)** & **56.01 (2.66)** \\  MVGRL & \(\) & **81.03 (1.29)** & 72.38 (1.68) & 37.20 (1.22) & 49.65 (2.08) \\  & ✓ & 80.48 (1.71) & **72.54 (1.89)** & **39.53 (1.04)** & **56.73 (2.52)** \\  SUGRL & \(\) & 65.35 (2.41) & 42.84 (2.57) & 31.62 (1.47) & 43.20 (1.79) \\  & ✓ & **70.06** (1.24) & **47.03** (3.02) & **38.50 (2.19)** & **51.01 (2.26)** \\  GRACE & \(\) & 76.84 (1.09) & 58.40 (3.05) & 38.20 (1.38) & 53.25 (1.58) \\  & ✓ & **79.15 (1.44)** & **63.66 (2.96)** & **51.56 (1.39)** & **67.39 (2.23)** \\  FiGURe\({}_{32}\) & \(\) & **82.88 (1.42)** & 70.32 (1.98) & 39.38 (1.35) & 53.27 (2.40) \\  & ✓ & 82.56 (0.87) & **71.25 (2.20)** & **48.89 (1.55)** & **65.66 (2.52)** \\  

Table 4: Node classification accuracy percentages with and without using Random Fourier Feature projections (on 32 dimensions). A higher number means better performance. The performance is improved by using RFF in almost all cases, indicating the usefulness of this transformation \(3\%\) performance gap between GRACE and our method for the squirrel and chameleon datasets. While GRACE handles heterophily well at lower dimensions, its performance deteriorates with homophilic graphs, unlike FiGURe\({}^{}_{32}\) which captures lower frequency information effectively. Additionally, our method exhibits computational efficiency advantages for specific datasets in lower dimensions. Please refer to 8.6: for discussions with regards to the RFF algorithm, 8.7: for analysing the RFF behaviour and community structure, 8.8: for experiments using other random projection methods, 8.9 and 8.10: for ablation studies with regards to projecting to higher dimensional spaces via RFF, and 8.11: for issues related to including RFF in training. Overall, these findings highlight the potential of RFF projections in extracting useful information from lower dimensional embeddings and reaffirm the competitiveness of FiGURe over the baselines.

### RQ3: Sharing Weights Across Filter Specific Encoders

Our method proposes to reduce the computational load by sharing the encoder weights across all filters. It stands to reason whether sharing these weights causes any degradation in performance. We present the results with shared and independent encoders across the filters in Table 5 to verify this. We hypothesize that, sharing encoder weights embeds diverse filter representations in a common space, improving suitability for combined representation learning. This enhances features for downstream tasks, in some cases boosting performance. Experimental results confirm that shared weights do not significantly reduce performance; sometimes, they even enhance it, highlighting shared encoders' effectiveness in reducing computational load without sacrificing performance.

### RQ4: Computational Efficiency

To assess the computational efficiency of the different methods, we analyzed the computation time and summarized the results in Table 6. The key metric used in this analysis is the mean epoch time: the average time taken to complete one epoch of training. We compared our method with other MI based methods such as DGI and MVGRL. Due to the increase in the number of augmentation views, there is an expected increase in computation time from DGI to MVGRL to FiGURe. However, as demonstrated in 6.2, using RFF projections allows us to achieve competitive performance even at lower dimensions. Therefore, we also included comparisons with our method at \(128\) and \(32\) dimensions in the table. It is evident from the results that our method, both at \(128\) and \(32\) dimensions, exhibits faster computation times compared to both DGI and MVGRL, which rely on higher-dimensional representations to achieve good performance. This result indicates that FiGURe is computationally efficient due to its ability to work with lower-dimensional representations. During training, our method, FiGURe\({}^{}_{32}\), is \( 3\)x faster than DGI and \( 6\)x times faster than MVGRL. Despite the faster computation, FiGURe\({}^{}_{32}\) also exhibits an average performance improvement of around \(2\)% across the datasets over all methods considered in our experiments. Please refer to 8.12 and 8.13 for further discussions.

   & DGI & MVGRL & FiGURe\({}_{512}\) & FiGURe\({}^{}_{128}\) & FiGURe\({}^{}_{32}\) \\  cora & 38.53 (0.77) & 75.29 (0.56) & 114.38 (0.51) & 20.10 (0.46) & 11.54 (0.34) \\ citeseer & 52.98 (1.15) & 102.41 (0.99) & 156.24 (0.56) & 30.30 (0.60) & 17.16 (0.51) \\ squirrel & 87.06 (2.07) & 168.24 (2.08) & 257.65 (0.76) & 47.72 (1.40) & 23.52 (1.14) \\ chameleon & 33.08 (0.49) & 64.71 (1.05) & 98.36 (0.64) & 18.56 (0.39) & 11.63 (0.48) \\  

Table 6: Mean epoch time (in milliseconds) averaged across 20 trials with different hyperparameters. A lower number means the method is faster. Even though our method is slower at 512 dimensions, using \(128\) and \(32\) dimensional embeddings significantly reduces the mean epoch time. Using RFF as described in 6.2 we are able to prevent the performance drops experienced by DGI and MVGRL.

   & cora & citeseer & squirrel & chameleon \\  Independent & 86.92 (1.10) \(\%\) & 75.03 (1.75) \(\%\) & 50.52 (1.51) \(\%\) & 66.86 (1.85) \(\%\) \\ Shared & 87.00 (1.24) \(\%\) & 74.77 (2.00) \(\%\) & 52.23 (1.19) \(\%\) & 68.55 (1.87) \(\%\) \\  

Table 5: A comparison of the performance on the downstream node classification task using independently trained encoders and weight sharing across encoders is shown. The reported metric is accuracy. In both cases, the embeddings are combined using the method described in 5.2

### RQ5: Experiments on Other Filter Banks

To showcase the versatility of our proposed framework, we conducted an experiment using Bernstein and Chebyshev filters, as detailed in Table 7. The results indicate that using \(_{}\) leads to better performance than BernNet and ChebNet filters. We believe this is happening is due to the latent characteristics of the dataset. [17; 28] have shown that datasets like chameleon and squirrel need frequency response functions that give more prominence to the tail-end spectrum. GPRGNN filters are more amenable to these needs, as demonstrated in . However, different datasets may require other frequency response shapes, where BernNet and ChebNet filters may excel, and give better performance. For instance, \(_{}\) may better approximate comb filters, as their basis gives uniform prominence to the entire spectrum. Our framework is designed to accommodate any filter bank, catering to diverse dataset needs. Further discussions are in 8.14.

### RQ6: Combining information from different filters (in \(_{}\))

To analyze how FiGURe combines representations from different filters, we present alpha coefficients for the highest-performing split in Table 8, utilizing \(_{}\) on: cora, citeseer, squirrel, and chameleon (coefficients may vary for different splits within the same dataset) . GPRGNN filters adapt well to heterophilic datasets like chameleon and squirrel, emphasizing spectral tails with significant weightage on the \(^{2}\) filter. In contrast, homophilic datasets require low-pass filters, achieved by assigning higher weightage to the \(\) and \(^{3}\) filters. Achieving these filter shapes with other methods like BernNet and ChebNet is possible but more challenging for the model to learn. We believe that this is why GPRGNN filters consistently outperform other filter banks on the datasets we've examined. However, it's important to note that some datasets may benefit from different response shapes, where BernNet and ChebNet filters might be more suitable. This table validates our hypothesis about the efficacy of GPRGNN filter coefficients, with \(^{3}\) dominating in homophilic datasets (cora, citeseer) and \(^{2}\) in heterophilic datasets (chameleon, squirrel).

## 7 Conclusion and Future Work

Our work demonstrates the benefits of enhancing contrastive learning methods with filter views and learning filter-specific representations to cater to diverse tasks from homophily to heterophily. We have effectively alleviated computational and storage burdens by sharing the encoder across these filters and focusing on low-dimensional embeddings that utilize high-dimensional projections, a technique inspired by random feature maps developed for kernel approximations. Future directions involve expanding the analysis from  to graph contrastive learning and investigating linear separability in lower dimensions, which could strengthen the connection to the random feature maps approach.

   & \(\) & citeseer & squirrel & chameleon \\  \(_{}^{3}\) & 85.13 (1.26) & 73.38 (1.81) & 37.07 (1.29) & 53.95 (2.78) \\ \(_{}^{11}\) & 86.62 (1.59) & 73.97 (1.43) & 43.48 (3.80) & 62.13 (3.66) \\ \(_{}^{3}\) & 83.84 (1.36) & 71.92 (2.29) & 40.23 (1.58) & 60.61 (2.03) \\ \(_{}^{11}\) & 76.14 (6.80) & 59.89 (8.94) & 52.46 (1.10) & 67.37 (1.60) \\  \(_{}\) & 87.00 (1.24) & 74.77 (2.00) & 52.23 (1.19) & 68.55 (1.87) \\  

Table 7: Accuracy percentages for various filter banks in conjunction with FiGURe. Specifically, \(_{}^{3}\) and \(_{}^{11}\) refer to the \(_{}\) filter bank with \(K\) set to \(3\) and \(11\), respectively. Similarly, \(_{}^{}\) and \(_{}^{}\) represent the \(_{}\) filter bank with \(K\) set to \(3\) and \(11\), respectively.

   & 1 & \(\) & \(^{2}\) & \(^{3}\) \\  cora & 18.2 & 0 & 0 & 35.95 \\ citeseer & 0 & 0 & 0 & 0.48 \\ squirrel & 0 & 0 & 15.3 & 0 \\ chameleon & 0 & 0 & 8.93 & 0.1 \\  

Table 8: We present the alpha coefficients obtained from the best-performing split utilizing GPRGNN filters for cora, citeseer, squirrel, and chameleon datasets