# Passive learning of active causal strategies in agents and language models

Andrew K. Lampinen

Google DeepMind

London, UK

lampinen@deepmind.com &Stephanie C. Y. Chan

Google DeepMind

London, UK

scychan@deepmind.com &Ishita Dasgupta

Google DeepMind

London, UK

idg@deepmind.com &Andrew J. Nam

Stanford University

Stanford, CA

ajhnam@stanford.edu &Jane X. Wang

Google DeepMind

London, UK

wangjane@deepmind.com

###### Abstract

What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that, under certain assumptions, learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from otherwise perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.

## 1 Introduction

Learning from passive observational data only allows learning correlational, not causal, structure. This observation is sometimes cited as a fundamental limitation of current machine learning research . However, reinforcement learning (RL) agents can intervene on their environment, and are therefore not entirely limited. Indeed, various works have shown that RL agents can (meta-)learn to intervene on the environment to discover and exploit its causal structure .

However, these prior works leave open the possibility that an agent could _passively_ learn a generalizable strategy for discovering and exploiting causal structure. While it is certainly necessary for an agent to intervene on the world _at test time_ to discover causal structure, it may be possible for the agent to learn such a strategy _from purely passive, offline data_. Metaphorically, we ask "could anagent learn to be a scientist by only reading a textbook detailing previous experiments and discoveries, and then successfully perform new experiments to discover and exploit new causal structures?"

The answer to this question is interesting for several reasons. From the perspective of causal learning, it is useful to investigate what limitations passive learning actually induces with respect to the knowledge an agenttic system can later gain of the world's causal structure. Moreover, the answer is particularly salient at present because language models (LMs) -- even when trained on purely passive imitation -- show emergent capabilities such as adapting or reasoning in-context [71; 55; 77], or using tools to interact with the world . These abilities are presumably driven by distributional properties of the passive data on which the LMs are trained [cf. 10; 64]. While LMs are often fine-tuned in active settings, e.g. using RL from Human Feedback , it is unclear whether this step is necessary to unlock causal abilities; the vast majority of learning occurs during the passive pretraining phase [e.g. 76]. Thus, understanding whether passive learning can lead to generalizable causal strategies can contribute to understanding the capabilities and limitations of language models.

In this work, we make the following contributions:

* We formally show that it is possible for an agent to learn a generalizable strategy for exploiting certain causal structures from passive data, as long it can intervene at test time.
* We demonstrate empirically that an agent trained via imitation (behavioral cloning) on expert data from a simple causal intervention task indeed can generalize at test time to infer and use causal links that are never present in the training data.
* We show that passively-trained agents can also generalize strategies for adaptive experimentation to experiment optimally in a novel situation.
* We show that strategies for causal intervention and exploitation can be learned from passive data even in a more complex environment with pixel observations and relational structure.
* We further show that natural language explanations support learning and generalizing causal strategies from passive data, and can enable generalization out-of-distribution, even from otherwise totally confounded training data.
* Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations.

## 2 Related work

Here, we attempt to briefly situate our contributions within the extensive prior literature on causal structure induction, language models, and online and offline RL agents.

A variety of works have considered the problem of causal graph structure induction from data [21; 54; 63]. While in a few cases the causal graph is identifiable from observational data alone , observations are generally not sufficient [e.g. 73], so most works also incorporate at least some interventional data. However, there is some flexibility in the interventional information required; for example, Faria et al.  show that a deep learning system can infer causal relations from data in which interventions are present, but latent (not directly observed). Perhaps most relevantly, Ke et al.  show that a transformer-based model can learn to induce causal structures from observational and interventional data. Our work moves beyond theirs in demonstrating that an agent can passively learn both to actively collect its own interventional data, and to use implicit inferences about causal structure to achieve goals -- and by connecting these observations to recent progress in LMs.

Many perspectives on the limitations of LMs correspond to causal arguments. For example, the "octopus test" of Bender and Koller  is an argument about causal structure in the world that is latent in language. Other works have expressed skepticism about the possibility of LMs (or other models) learning causal strategies from passive data [e.g. 56; 60]. Even works that take more optimistic perspectives tend to focus on memorization of observed causal structure -- e.g. Kuciman et al.  study LM knowledge about real-world causal structure, and note their potential utility as causal priors. Willig et al.  make some overlapping observations, but argue that these are simply examples of discovering correlations between causal structures in the real world and their descriptions in language, and that therefore LLMs "simply recite the causal knowledge embedded in the data" and thus "we cannot expect any sort of generalization in terms of causal prowess." However, some works have suggested that LMs can learn implicit world models from the training data [43; 44], suggesting that there is potential for more than purely superficial memorization. Correspondingly, we will show that it is possible to learn generalizable causal strategies from passive data, especially when given explanations that link observations to causal structure (which is particularly relevant for LMs).

Work in offline RL has also studied the ability of agents to generalize from passive data [20; 38; 1], but has generally used more complex offline algorithms rather than pure imitation. Some work on pure imitation has considered the causal challenges for learning an optimal policy [65; 15; 74], but generally under a fixed causal structure. Other work has explored the generalization capabilities of agents trained from pure imitation when conditioned on information about the trajectories (such as returns) [67; 68; 3; 11]. Laskin et al.  show that passive offline imitation can learn a meta-RL algorithm that can improve an RL policy at test time. However, none of these works focused on tasks of causal experimentation and exploitation.

Instead, work on learning causal strategies has generally relied on online RL agents. Dasgupta et al.  showed that causal reasoning can emerge implicitly from meta-RL; other approaches learn to infer causal structures explicitly as part of an meta-RL process [50; 30]. Lampinen et al.  demonstrate that explanations can support meta-learning of causal strategies. Other works have proposed benchmarks for meta-learning causal experimentation . However, all of these works relied on active RL. Kosoy et al.  recently considered learning of causal overhypotheses, and in addition to online RL agents evaluated behavioral cloning, but found it performed poorly; a decision transformer performed somewhat better, but only if forced to explore. Thus, past works leave open the question of whether agents can generalize causal strategies from passive data.

## 3 Passive learning can suffice

The key idea is that an agent that can intervene at test time could correctly infer and use the causal structure, as long as it has acquired an appropriate strategy during training. One way to enable this is to teach the agent this strategy in two stages: first, explore a system using an _experimentation strategy_ that is guaranteed to identify the causal structure; second, learn an _exploitation strategy_ to use this identified causal structure to achieve a goal. If the agent is successfully able to execute the experimentation strategy in the first part, it will necessarily have acquired the information required. Generalizing the _exploitation strategy_ is directly amenable to passive learning - given that all the relevant information has been uncovered, there is no in-principle limitation on its success.

More formally, suppose an agent \(A\) receives observations \(o_{t}\) at time \(t\), and is capable of taking actions \(a_{t}\) that intervene on a system. Assume that the system in question consists of a finite set of \(n\) random variables \(\{X_{0},...,X_{n}\}\) that are connected to form a causal Directed Acyclic Graph (DAG; see e.g. Glymour et al. , Ch. 2). We will assume for simplicity that the variables are real-valued and that edges in the graph correspond to linear effects. We assume that independent variables (i.e., those without ancestors) are sampled from a normal distribution with mean 0 and a random variance, followed by a nonlinearity; nodes with ancestors are sampled from a normal distribution with mean corresponding to the linear effects of its inputs and again a random variance, and final nonlinearity. It is easy to adapt our argument in order to change many of these assumptions.1 The agent's observations correspond to current values of the variables and (at some time steps) a goal \(g\), that is, \(o_{t}^{n}\). The agent's actions correspond to interventions that set the value of a variable \(a_{t}=do(X_{i}=x)\) for some \(i\{1,...,n\}\) and \(x\).

Now suppose we have a dataset of expert demonstrations on varied causal structures, in which the expert first intervenes on every variable to determine the causal structure; then uses that structure to achieve some goal. For example, the trajectories of the expert might have the following form:

\[=do(X_{0}\!=\!x_{0}),...,a_{n}=do(X_{n}\!= \!x_{n})}_{}a_{n+1}=do(X_{i}\!=\!x_{n+1}^{g}),...,a_{n+k}=do(X_{j}\!=\!x_{n+k}^{g})\]

In this example, the first phase -- the _intervention strategy_ -- is easy to learn, since it consists of a fixed sequence of actions.2 Thus, as long as the agent successfully learns the training data, it should reproduce this sequence on new causal structures. Once the agent has completed these interventions, it will have acquired all the information necessary to correctly infer the causal structure. With this information, the agent can in principle behave optimally in the goal-seeking remainder of the episode, i.e. implementing the _exploitation strategy_. Thus, the agent is not subject to any in-principle limitations. The two core tenets are whether a) the agent is able to follow the learned _exploration strategy_ in new unobserved causal structures at test time, and b) the agent is able to generalize the _exploitation strategy_ to account for the novel outcomes of those interventions. Below, we show empirically that an agent can indeed exhibit this kind of generalization from passive data.

## 4 Methods

**Agent:** We used a standard agent architecture: an input encoder that processes the observations, a memory that tracks and integrates information over time, and a policy that predicts actions. In analogy to the language model setting, we used a TransformerXL  for the agent memory, with a sufficiently long memory to cover the expected episode length. For the encoder in the causal DAG environments we concatenated the input observations and projected through a single linear layer. For the odd-one-out environments we used a CNN for visual observation encoding and an LSTM for language (following prior work on these environments ). For the agent policy, we used a single linear layer which receives as input the state output from the Transformer memory, and produces logits over the set of possible actions. For explanation prediction output (where applicable) we used a single-layer LSTM which received the same input as the policy.

**Expert policies & datasets:** We trained the agent on a dataset of trajectories generated by an expert policy, which behaves (near-)optimally. The specific implementation of the expert policy depends on the task, see below and Appendix A. We place our agent in the infinite-data regime (a common assumption in language model training [e.g., 27]), that is, the number of episodes in the dataset is larger than the number of episodes the agent trains on, so the agent does not repeat any episode.

**Language model:** For experiments with language models, we used the 70 billion parameter Chinchilla LM ; a decoder-only transformer that is trained purely on language modelling, without any downstream fine-tuning. When evaluating the LM's choice among multiple answers, we calculated the log-likelihood of each as a continuation of the current prompt, and selected the one with the highest likelihood. When generating from the model, to produce explanations and reasoning traces at test time, we used nucleus sampling  with \(p=0.8\) and \(T=1\).

Figure 1: The causal DAG environment and experimental results. (a) The constraints imposed on the causal DAG structures when creating the training dataset, and when evaluating the agent on test structures. During training, D is not allowed to be an ancestor of E (even indirectly), though they may be correlated due to confounding variables. In evaluation environments, D is the most impactful ancestor of E (see text). (b) Rewards obtained by the agent when evaluated in interactive settings, as a percentage of the optimal rewards. In both evaluation settings, the agent still achieves close to optimal reward. (c) Analyzing the agent’s behavior in more detail, by plotting the proportion of the agent’s actions that match the optimal behavior, or baselines based on heuristics from the interventions or correlational statistics. The agent matches the optimal strategy significantly more closely than it matches the heuristic baselines. (Error bars are 95%-CIs; 5 seeds per condition.)

Experiments

### A simple causal DAG environment

Our first experiments take place in a simple causal DAG setting, inspired by the argument above and prior work on RL agents learning causal strategies . The environment consists of a causal DAG over \(n=5\) variables (A, B, C, D, E), with the causal graph, edge weights, and noise variance for each node resampled on each episode. Each episode consists of several segments: first, the agent is given \(n\) steps to explore the graph -- enough to intervene on each variable and observe the consequences -- and a second phase in which the agent is cued to try to maximize a particular variable. During this second phase, the agent is rewarded according to the value of the cued variable after its actions. In both phases, the agent observes its previous intervention and the value of all variables before and after. In the second phase, it also observes the goal cue. For details see Appx. A.1.

The training dataset is generated by an optimal expert policy that first explores (by choosing a random node to start with, then iterating through subsequent nodes in order, looping back to the first node after the last, and intervening on each to determine its effect). Then, the expert plans using the inferred causal structure to identify the intervention that will optimally achieve the given goal, and takes that action. The learning agent is trained to imitate this expert dataset via behavioral cloning -- that is, by passively learning to maximize the probability of the expert actions.

The training data are generated from a subset of the possible causal DAGs, holding out some causal dependency links entirely for testing. We train the agent on expert data from DAGs in which node E is never descended (even indirectly) of node D. We then test the agent (interactively) on DAGs in which node E _is_ descended from node D, with sufficiently large edge weights along paths from D to E (and sufficiently small from other ancestors) that node E is most strongly influenced via node D. We evaluate the agent's ability to determine this dependency through its experiments, and then, when cued to maximize node E, to optimally intervene. Specifically, we consider two test conditions: "eval. target," where node D is the optimal target to intervene on, and "eval. path," where node D is somewhere on the path from the optimal target to node E (but an ancestor of node D might be a better intervention). The "eval. target" condition is perhaps the most difficult, as in the training data the agent never sees any structure in which intervening on node D affects node E at all. Note that this generalization split is challenging because neither the environment observations nor the agent architecture encode permutation-invariance of the variables; their values are presented in a fixed order in distinct observation entries, and the agent uses distinct actions for intervening on each variable.

Thus, the question is whether the agent can learn to infer this novel causal dependency path from its experiments, and to use it to achieve the goal -- i.e. can an agent generalize its exploitation strategy to novel causal structures? (Generalization of exploration strategy is investigated below in Sec 5.1.1). Indeed, in Fig. 1b we show that the agent rapidly learns to achieve high reward under interactive evaluation in all conditions (both training and evaluation structures).

However, simpler heuristic strategies could achieve some rewards. We therefore compare the agent's actions _during the exploitation phase_ to the optimal strategy, and four baselines. The change and value baseline strategies use simpler heuristics, rather than relying on the full causal structure to achieve the goal of maximizing a given node. The value baseline simply repeats the action that during the exploration phase resulted in the highest value for the goal node. The change baseline is slightly more sophisticated; it chooses the intervention that changed the value of the goal node the most, and then flips its sign if the change was negative. We also consider two strategies that observe all the node values that the agents do, but consider only correlational statistics -- either the total correlation between two nodes, or the correlation when controlling for all other nodes. Note that these baselines and the optimal expert policy are not mutually exclusive; these heuristics are effective _because_ they often are match the expert. However, in Fig. 1c we show that the agent's actions match the expert policy significantly better than any baseline, across both training and evaluation. These observations suggest that the agent is learning to use more complex causal strategies, rather than simpler heuristics or correlational statistics.

In the supplement we perform a number of follow-up experiments, showing that _at test time_, it is necessary to intervene rather than just observe (Appx. B.2); how causal strategy generalization depends on the variety of causal DAG structures in training (Appx. B.3); and that agents can generalize causal strategies to DAGs with different functional structures, such as linear DAGs in training and nonlinear in test (Appx. B.4).

#### 5.1.1 Adaptive experimentation strategies

It is limiting to assume that the optimal policy requires intervening on every causal factor; that does not scale to realistic scenarios. Scientists are guided by domain knowledge that suggests which variables may be relevant to a given phenomenon. We therefore explored whether the passive agent can learn to adapt its experimentation procedure based on cues about which variables might matter.

To do so, we increased the variables to \(n=10\), but restricted so that only 5 are relevant in any given episode. The other 5 are independent from everything, and the goal node to maximize is always one of the 5 relevant variables. The agent is given a many-hot cue that marks which variables are relevant currently, and the expert policy which the agent learns to imitate only intervenes on the relevant variables. In the training data, we hold out some subsets of variables such that the agent never sees experiments on these subsets, as well as holding out certain ancestor structures as above. We test the agent in settings where it encounters both a held-out set of variables, and new relations among them.

In Fig. 2, we plot over training the proportion of evaluation episodes in which the agent correctly uses an optimal exploration strategy; that is, in which it intervenes on each relevant variable once, without intervening on any irrelevant variable. The agent rapidly learns to generalize the experimentation strategy from the data to correctly experiment when cued with sets of variables never encountered in training. It then goes on to successfully use the results of these experiments to achieve the goal, and again aligns with the optimal causal policy more closely than the heuristic baselines (Appx. B.5).

### The odd-one-out environments

The above experiments used simple environments, where the agent directly observes the variables and the outcomes of its interventions. Here, we extend to a more complex set of environments involving

Figure 3: The odd-one-out meta-learning environments, where the goal is to pick up an object that is unique along the “correct” dimension, which cannot be directly observed. (a) An example of the observations and task structure. At top is an example experimentation trial. The agent is the white square (the view is egocentric, i.e. the agent is always in the center); it transforms one object to a unique shape, then it will “pick up” that object to check whether shape is the relevant dimension for this episode. After three experimentation trials, the agent proceeds to an exploit trial (bottom), where it can no longer transform objects, but where each object is unique along a different dimension; it must pick up an object according to the latent causal dimension discovered through experimentation. In training, the environment provides explanations of the objects and the results of experiments, which support learning. (b) Generalization to novel feature combinations. (c) Generalization from training data where 0-2 dimensions have a hard initialization (see text) to the case when all 3 dimensions do. (Panel (a) adapted with permission from Lampinen et al. . 3 seeds per condition.)

Figure 2: When the agent is passively trained on expert actions that explore (intervene on) only a subset of the variables, it can generalize to explore correctly on held-out variable subsets at test time.

a causal structure that can only be understood by interpreting relations among objects in high-dimensional pixel observations. Specifically, we experiment on the causal intervention/meta-learning versions of the odd-one-out environments . See Fig. 2(a) for some example observations.

In these environments, objects vary along multiple dimensions (color, shape, and texture). In each episode there is one dimension that is "correct" -- this "correct" dimension is a latent causal factor that the agent cannot directly observe. The agent is rewarded for picking up an object that is unique (the odd one out) along the "correct" feature dimension. The agent is first given a series of trials in which to experiment and determine which dimension is currently "correct." Specifically, the agent is given a magic wand that allows it to transform features of objects, thus allowing it to make an object unique along one feature dimension. It can then pick up this object, and it can infer from whether it receives reward whether the dimension it tried is correct. These experiments allow the agent to succeed on a final exploit trial, where it no longer has access to a magic wand, but instead is faced with multiple objects that are each unique along a different dimension. To achieve a reward, the agent must use the results of its earlier experiments to choose the object that is unique in the correct way.

We evaluated two types of generalization. First, we simply split possible feature combinations into train and test sets. We evaluate whether the agent can learn, from passive data, an experimentation and exploitation strategy that generalizes to novel combinations. In Fig. 2(b), we show that the agent is able to learn these tasks well, and to generalize equally well to the evaluation conditions. Note that the tasks are designed such that simpler strategies -- such as relying on feature values -- will not yield above chance performance.

We then evaluated a more challenging generalization condition. The original paper  considered harder object initializations that make the experiments more difficult, by forcing the agent to transform one object, and then choose a _different_ object that is made unique by that transformation. We create a more challenging relational generalization split by training the agent in settings where 0-2 of the feature dimensions (randomly sampled in each training episode in the dataset) have this harder initialization. We then evaluate the agent's ability to generalize to evaluation in a harder setting where all 3 feature dimensions have this difficult initialization. This condition tests the agent's generalization to settings more challenging than those observed in the data. In Fig. 2(c), we show that the agent is able to learn and generalize these more difficult tasks.

**Explanations support learning:** In these experiments, we followed the method of Lampinen et al. ; the agent is trained to predict natural language explanations (provided by the environment during training) as an auxiliary loss. For example, when the agent performs a successful experiment that identifies the correct dimension as shape, during training this will be accompanied by an explanation like "correct, the latent feature is shape, and this object is uniquely square." In Supp. Fig. 12 we show that these explanations are beneficial even when learning from expert data; without predicting explanations, the agents do not learn or generalize these tasks as readily. Furthermore, explanations can enable other kinds of generalization from passive learning, as we explore in the next section.

#### 5.2.1 Explanations can deconfound for passive learners too

The original odd-one-out work also explored a setting of learning from perfectly confounded experiences, in which a rewarding object is unique along three feature dimensions, so it is unclear which feature(s) are relevant. The authors showed that RL agents can generalize out of distribution to follow any particular dimension in a deconfounded test (where different objects are unique along each dimension), if they are trained to predict language explanations that focus on that particular feature dimension. Although these experiments do not directly involve causal intervention, they are relevant to the broad question of what can be learned from confounded data -- and are particularly relevant to LMs, because explanations are a common feature of the human language from which these models learn. However, the prior work only experimented with active RL agents. Can passive learners also generalize out-of-distribution from predicting explanations?

Figure 4: Explanations can deconfound for passive learners trained on confounded data — the agents generalize OOD in accordance with the explanations they were trained to predict.

[MISSING_PAGE_FAIL:8]

strategies for exploiting the results of their own experiments to achieve goals under those novel structures. Agents can even generalize in complex environments where they receive high-dimensional observations; likewise, passively-trained language models can generalize from a high-quality few-shot prompt. Furthermore, our results highlight the role that natural language explanations can play in drawing out latent causal structure to enable more generalizable learning. Explanations can even enable passive learners to generalize nearly perfectly from otherwise perfectly-confounded data. Passive learning can be surprisingly powerful under the right circumstances.

These observations provide perspective on the kinds of understanding of causality and experiments that can be acquired by language models. For example, consider the following, from Pearl :

"This hierarchy, and the formal restrictions it entails, explains why statistics-based machine learning systems are prevented from reasoning about actions, experiments and explanations. [...] the hierarchy denegrades [_sic_] the impressive achievements of deep learning to the level of Association [...] Unfortunately, the theoretical barriers that separate the three layers in the hierarchy tell us that the [...] objective function does not matter. As long as our system optimizes some property of the observed data, however noble or sophisticated, while making no reference to the world outside the data, we are back to level-1 of the hierarchy with all the limitations that this level entails."

Naively, it might seem that these arguments doom language models to never acquire real knowledge of causality [cf. 72]. Our results provide useful formal and empirical nuance. While there are certainly restrictions to learning from passive data (see below for further discussion), it is possible to learn generalizable strategies for causal experimentation and intervention from passive data alone -- at least if the data include examples of an expert intervening, and perhaps explanations.

Other work has engaged with these issues. In particular, Ortega et al.  provide a theoretical account of the effects of confounding on passively trained sequence models, and propose a counterfactual training method to enable an adaptive system to learn effective intervention strategies in the presence of confounders. However, this method relies on having active access to an expert teacher rather than simply offline data; the authors note that "in general, we cannot meta-train an agent only from expert demonstrations to imitate the expert at deployment time" and "[t]o address the problem, it is necessary to incorporate assumptions (e.g. a set of hypotheses) about the confounding variable, but currently it is unclear how to incorporate this seamlessly into a meta-learning scheme." Our work complements theirs, by illustrating some of the cases in which an agent can successfully generalize causal experimentation and goal-seeking strategies from passive data alone -- even in the presence of confounders -- and by highlighting the role that natural language explanation can play in incorporating expert knowledge to allow active generalization from passive learning.

**Explanations & causal structure:** Explanations were important for effective learning or generalization in the odd-one-out tasks; explanations can even allow generalizing from otherwise totally-confounded data. These results fit with prior work showing that explanations and reasoning traces help agents  and LMs . More broadly, explanations are key for human learning because they explicitly highlight generalizable, causal structures . For example, explanations can constrain the hypothesis space of causal graphs that a system needs to consider [cf. 51]. Alternatively, language3 may obviate the need for explicit causal graphs. That is, explanations could serve as the "reference to the world outside the data" that Pearl  sought in the quote above.

**Language models:** Our experiments have obvious implications for the causal abilities of language models. Prior works have suggested that LMs can only parrot causal structures observed in training . However, our experiments illustrate that this should not be assumed; the implications of passive learning for causal strategies can be subtle. Passive data is not necessarily strictly observational. In particular, observing other's interventions, especially with explanations, can enable generalization. LM training data will contain many examples describing interventions and outcomes together with explanations thereof -- whether scientific articles, or just descriptions of a debugging process. Just as the data in a scientific paper do not become observational if we read the paper rather than writing it, the data on the internet do not become observational just because an LM consumes it passively. Thus, sequences of interventions, outcomes, and explanations may in some cases be sufficient to allow an LM to generalize to proposing experiments and using the results to draw appropriate conclusions, at least if given an appropriate prompt.

**Tool use & agency:** These findings are of particular interest given the recent focus on language models using tools [e.g. 66]. Tool use is a clear case of active interaction in the world, which one might assume requires some knowledge of causality. Our results may help explain the successes of these methods with models trained purely from passive supervised learning (though note that many applications of LMs are also actively fine-tuned, which may also contribute, see below). A related question is whether LMs, despite passive learning, can acquire an ability to act as goal-directed agents. Andreas  notes that effective language modeling requires modeling the goals of the communicator that produced the language, and thus suggests that through this process LMs learn to at least _model_ some aspects of agency. Our results may provide some support for these arguments, as generalizing goal-directed behavior to new causal structures is a key aspect of agency.

**Experimental limitations:** Our experiments are limited in a number of important ways. First, the causal structures we used are small, finite acyclic graphs. While this limitation (i.e., that the causal graph is a DAG over a handful of variables) is assumed by much of the prior causal literature, many real world causal structures are much more complex. For example, they may include cycles, at least over time.4 Changing these assumptions changes the nature of the causal discovery problem ; thus it is an open question whether agents could similarly learn causal strategies in these more complex settings. More generally, even our most complex environments have relatively simple and constrained observation and action spaces compared to the real world, and the space of possible experiments is generally small and heavily constrained. Our agents are also limited to fit with their environments (e.g. having the right number of observation inputs and action outputs for the number of variables in the DAG environment). It is an open question whether agents could in practice learn causal strategies from passive data in environments that relaxed these simplifying assumptions, though our results suggest it would be possible in principle.

**What this paper does _not_ imply:** We do not intend to suggest that passive learning is as beneficial as active learning, or that confounding is not a challenge for passive learners. Active learning can be substantially more effective or efficient than passive learning in humans , animals , and RL agents . It can be difficult to learn from purely positive examples, without some active correction of off-optimal-policy behavior [e.g. 65, 15]. Indeed, while the LM we evaluated was only trained passively, many deployments of LMs incorporate interactive learning through RL from Human Feedback [12, e.g. 59]; one key benefit of this may be the value of training the model on active, on-policy data. We therefore expect that agents or LMs that are fine-tuned in an active setting, or with more sophisticated causal corrections , will generally develop more robust and generalizable causal strategies than those trained in purely passive settings.

Furthermore, confounding is clearly a challenge for any system that attempts to discover and use causal structure. The ongoing debates about confounding in the medical literature [e.g. 37, 75] illustrate that this is a challenge even for causal experts such as human scientists. However, our work contributes to understanding the regimes in which confounding during passive learning can be overcome by experimentation at test time. We consider a range of confounds; even our train-test splits effectively correspond to strong confounds which are not directly observable, and for which the agent/LM only experiences training data sampled under one value, and then is tested on situations sampled under another. Nevertheless, agents can in some cases generalize despite these confounds, particularly when supported by explanations. These results highlight the role that language and explicit descriptions of the quest for causal structure can play in acquiring causal strategies.

**Conclusions:** We illustrate the potential for purely passive learning of active causal strategies across various settings, particularly with the support of explanations. These results have implications for understanding the causal limitations of passive learning for a system that can only interact at test time, and have particular relevance to understanding the capabilities and behaviour of language models.