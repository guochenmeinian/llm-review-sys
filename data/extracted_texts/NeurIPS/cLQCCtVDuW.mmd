# HIQL: Offline Goal-Conditioned RL

with Latent States as Actions

Seohong Park\({}^{1}\)  Dibya Ghosh\({}^{1}\)  Benjamin Eysenbach\({}^{2}\)  Sergey Levine\({}^{1}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)Princeton University

seohong@berkeley.edu

###### Abstract

Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/

## 1 Introduction

Many of the most successful machine learning systems for computer vision [15; 36] and natural language processing [10; 18] leverage large amounts of unlabeled or weakly-labeled data. In the reinforcement learning (RL) setting, offline goal-conditioned RL provides an analogous way to potentially leverage large amounts of multi-task data without reward labels or video data without action labels: offline learning [54; 55] enables leveraging previously collected and passively observed data, and goal-conditioned RL [44; 79] enables learning from unlabeled, reward-free data. However, offline goal-conditioned RL poses major challenges. First, learning an accurate goal-conditioned value function for any state and goal pair is challenging when considering very broad and long-horizon goal-reaching tasks. This often results in a noisy value function and thus potentially an erroneous policy. Second, while the offline setting unlocks the potential for using previously collected data, it is not straightforward to incorporate vast quantities of existing action-free video data into standard RL methods. In this work, we aim to address these challenges by developing an effective offline goal-conditioned RL method that can learn to reach distant goals, readily make use of data without reward labels, and even utilize data without actions.

One straightforward approach to offline goal-conditioned RL is to first train a goal-conditioned value function and then train a policy that leads to states with high values. However, many prior papers have observed that goal-conditioned RL is very difficult, particularly when combined with offline training and distant goals [35; 38; 102]. We observe that part of this difficulty stems from the "signal-to-noise"ratio in value functions for faraway goals: when the goal is far away, the optimal action may be only slightly better than suboptimal actions, because a transition in the wrong direction can simply be corrected at the next time step. Thus, when the value function is learned imperfectly and has small errors, these errors can down out the signal for distant goals, potentially leading to an erroneous policy. This issue is further exacerbated with the offline RL setting, as erroneous predictions from the value function are not corrected when those actions are taken and their consequences observed.

To address this challenge, we separate policy extraction into two levels. We first train a goal-conditioned value function from offline data with implicit Q-learning (IQL)  and then we extract two-level policies from it. Our high-level policy produces intermediate waypoint states, or _subgoals_, as actions. Because predicting high-dimensional states can be challenging, we will propose a method that only requires the high-level policy to product _representations_ of the subgoals, with the representations learned end-to-end from the value function. Our low-level policy takes this subgoal representation as input and produces actions to reach the subgoal (Figure 0(a)). Here, in contrast to previous hierarchical methods [56; 65], we extract both policies from the _same_ value function. Nonetheless, this hierarchical decomposition enables the value function to provide clearer learning signals for both policies (Figure 0(b)). For the high-level policy, the value difference between various subgoals is much larger than that between different low-level actions. For the low-level policy, the value difference between actions becomes relatively larger because the low-level policy only needs to reach nearby subgoals. Moreover, the value function and high-level policy do not require action labels, so this hierarchical scheme provides a way to leverage a potentially large amount of passive, action-free data. Training the low-level policy does require some data labeled with actions.

To summarize, our main contribution in this paper is to propose **Hierarchical Implicit Q-Learning (HIQL)**, a simple hierarchical method for offline goal-conditioned RL. HIQL extracts all the necessary components--a representation function, a high-level policy, and a low-level policy--from a single goal-conditioned value function. Through our experiments on six types of state-based and pixel-based offline goal-conditioned RL benchmarks, we demonstrate that HIQL significantly outperforms previous offline goal-conditioned RL methods, especially in complex, long-horizon tasks, scales to high-dimensional observations, and is capable of incorporating action-free data.

## 2 Related work

Our method draws on concepts from offline RL [54; 55], goal-conditioned RL [4; 44; 79], hierarchical RL [6; 62; 77; 85; 86; 96], and action-free RL [7; 12; 34; 80; 88; 105], providing a way to effectively train general-purpose goal-conditioned policies from previously collected offline data. Prior work on goal-conditioned RL has introduced algorithms based on a variety of techniques, such as hindsight relabeling [4; 13; 27; 56; 57; 75; 100], contrastive learning [23; 24; 102], and state-occupancy matching [20; 60].

Figure 1: _(left)_ We train a value function parameterized as \(V(s,(g))\), where \((g)\) corresponds to the subgoal representation. The high-level policy predicts the representation of a subgoal \(z_{t+k}=(s_{t+k})\). The low-level policy takes this representation as input to produce actions to reach the subgoal. _(right)_ In contrast to many prior works on hierarchical RL, we extract both policies from the _same_ value function. Nonetheless, this hierarchical structure yields a better “signal-to-noise” ratio than a flat, non-hierarchical policy, due to the improved relative differences between values.

However, directly solving goal-reaching tasks is often challenging in complex, long-horizon environments [35; 65; 65]. To address this issue, several goal-conditioned RL methods have been proposed based on hierarchical RL [11; 17; 51; 56; 65; 66; 81; 91; 103] or graph-based subgoal planning [22; 38; 40; 45; 46; 69; 78; 101]. Like these prior methods, our algorithm will use higher-level subgoals in a hierarchical policy structure, but we will focus on solving goal-reaching tasks from _offline_ data. We use an offline RL algorithm  to train a goal-conditioned value function from the dataset, which allows us to simply _extract_ the hierarchical policies in a decoupled manner with no need for potentially complex graph-based planning procedures. Another important difference from prior work is that we only train a _single_ goal-conditioned value function, unlike previous hierarchical methods that train multiple hierarchical value functions [56; 65]. Perhaps surprisingly, we show that this can still significantly improve the performance of the hierarchical policies, due to an improved "signal-to-noise" ratio (Section 4).

Our method is most closely related to previous works on hierarchical offline skill extraction and hierarchical offline (goal-conditioned) RL. Offline skill extraction methods [2; 43; 50; 72; 76; 84] encode trajectory segments into a latent skill space, and learn to combine these skills to solve downstream tasks. The primary challenge in this setting is deciding how trajectories should be decomposed hierarchically, which can be sidestepped in our goal-conditioned setting since subgoals provide a natural decomposition. Among goal-conditioned approaches, hierarchical imitation learning [35; 59] jointly learns subgoals and low-level controllers from optimal demonstrations. These methods have two drawbacks: they predict subgoals in the raw observation space, and they require expert trajectories; our observation is that a value function can alleviate both challenges, as it provides a way to use suboptimal data and stitch across trajectories, as well as providing a latent goal representation in which subgoals may be predicted. Another class of methods plans through a graph or model to generate subgoals [25; 26; 58; 83]; our method simply extracts all levels of the hierarchy from a single unified value function, avoiding the high computational overhead of planning. Finally, our method is closely related to POR , which predicts the immediate next state as a subgoal; this can be seen as one extreme of our method without representations, although we show that more long-horizon subgoal prediction can be advantageous both in theory and practice.

## 3 Preliminaries

**Problem setting.** We consider the problem of offline goal-conditioned RL, defined by a Markov decision process \(=(,,,p,r)\) and a dataset \(\), where \(\) denotes the state space, \(\) denotes the action space, \(()\) denotes an initial state distribution, \(p()\) denotes a transition dynamics distribution, and \(r(s,g)\) denotes a goal-conditioned reward function. The dataset \(\) consists of trajectories \(=(s_{0},a_{0},s_{1},a_{1},,s_{T})\). In some experiments, we assume that we have an additional action-free dataset \(_{}\) that consists of state-only trajectories \(_{s}=(s_{0},s_{1},,s_{T})\). Unlike some prior work [4; 40; 46; 65; 101], we assume that the goal space \(\) is the same as the state space (_i.e._, \(=\)). Our goal is to learn from \(_{}\) an optimal goal-conditioned policy \((a|s,g)\) that maximizes \(J()=_{g p(g),r p^{}()}[_{t=0}^{T}^{t}r(s _{t},g)]\) with \(p^{}()=(s_{0})_{t=0}^{T-1}\,(a_{t} s_{t},g)p(s_{t+1} s _{t},a_{t})\), where \(\) is a discount factor and \(p(g)\) is a goal distribution.

**Implicit Q-learning (IQL).** One of the main challenges with offline RL is that a policy can exploit overestimated values for out-of-distribution actions , as we cannot correct erroneous policies and values via environment interactions, unlike in online RL. To tackle this issue, Kostrikov et al.  proposed implicit Q-learning (IQL), which avoids querying out-of-sample actions by converting the \(\) operator in the Bellman optimal equation into expectile regression. Specifically, IQL trains an action-value function \(Q_{_{Q}}(s,a)\) and a state-value function \(V_{_{V}}(s)\) with the following loss:

\[_{V}(_{V}) =_{(s,a)}[L_{}^{}(Q_{ _{Q}}(s,a)-V_{_{V}}(s))],\] (1) \[_{Q}(_{Q}) =_{(s,a,s^{})}[(r_{}(s, a)+ V_{_{V}}(s^{})-Q_{_{Q}}(s,a))^{2}],\] (2)

where \(r_{}(s,a)\) denotes the task reward function, \(_{Q}\) denotes the parameters of the target Q network , and \(L_{2}^{}\) is the expectile loss with a parameter \([0.5,1)\): \(L_{2}^{}(x)=|-(x<0)|x^{2}\). Intuitively, expectile regression can be interpreted as an asymmetric square loss that penalizes positive values more than negative ones. As a result, when \(\) tends to \(1\), \(V_{_{V}}(s)\) gets closer to \(_{a}Q_{_{Q}}(s,a)\) (Equation (1)). Thus, we can use the value function to estimate the TD target (\(r_{}(s,a)+_{a^{}}Q_{_{Q}}(s^{},a^{ })\)) as \((r_{}(s,a)+ V_{_{V}}(s^{}))\) without having to sample actions \(a^{}\).

After training the value function with Equations (1) and (2), IQL extracts the policy with advantage-weighted regression (AWR) [67; 70; 71; 73; 74; 94]:

\[J_{}(_{})=_{(s,a,s^{})}[( (Q_{_{Q}}(s,a)-V_{_{V}}(s)))_{_{}}(a s )],\] (3)

where \(_{0}^{+}\) denotes an inverse temperature parameter. Intuitively, Equation (3) encourages the policy to select actions that lead to large \(Q\) values while not deviating far from the data collection policy .

**Action-free goal-conditioned IQL.** The original IQL method described above requires both reward and action labels in the offline data to train the value functions via Equations (1) and (2). However, in real-world scenarios, offline data might not contain task information or action labels, as in the case of task-agnostic demonstrations or videos. As such, we focus on the setting of offline goal-conditioned RL, which does not require task rewards, and provides us with a way to incorporate state-only trajectories into value learning. We can use the following action-free variant [34; 97] of IQL to learn an offline goal-conditioned value function \(V_{_{V}}(s,g)\):

\[_{V}(_{V})=_{(s,s^{})_{S},g  p(g|)}[L_{2}^{}(r(s,g)+ V_{_{V}}(s^{},g)- V_{_{V}}(s,g))].\] (4)

Unlike Equations (1) and (2), this objective does not require actions when fitting the value function, as it directly takes backups from the values of the next states.

Action-labeled data is only needed when extracting the policy. With the goal-conditioned value function learned by Equation (4), we can extract the policy with the following variant of AWR:

\[J_{}(_{})=_{(s,a,s^{})_{S},g p (g|)}[( A(s,a,g))_{_{}}(a s,g)],\] (5)

where we approximate \(A(s,a,g)\) as \( V_{_{V}}(s^{},g)+r(s,g)-V_{_{V}}(s,g)\). Intuitively, Equation (5) encourages the policy to select the actions that lead to the states having high values. With this action-free variant of IQL, we can train an optimal goal-conditioned value function only using action-free data and extract the policy from action-labeled data that may be different from the passive dataset.

We note that this action-free variant of IQL is unbiased when the environment dynamics are deterministic , but it may overestimate values in stochastic environments. This deterministic environment assumption is inevitable for learning an unbiased value function solely from state trajectories. The reason is subtle but important: in stochastic environments, it is impossible to tell whether a good outcome was caused by taking a good action or because of noise in the environment. As a result, applying action-free IQL to stochastic environments will typically result in overestimating the value function, implicitly assuming that all noise is controllable. While we will build our method upon Equation (4) in this work for simplicity, in line with many prior works on offline RL that employ similar assumptions [14; 33; 34; 41; 93; 97], we believe correctly handling stochastic environments with advanced techniques (_e.g._, by identifying controllable parts of the environment [99; 92]) is an interesting direction for future work.

## 4 Hierarchical policy structure for offline goal-conditioned RL

Goal-conditioned offline RL provides a general framework for learning flexible policies from data, but the goal-conditioned setting also presents an especially difficult multi-task learning problem for RL algorithms, particularly for long-horizon tasks where the goal is far away. In Section 4.1, we discuss some possible reasons for this difficulty, from the perspective of the "signal-to-noise" ratio in the learned goal-conditioned value function. We then propose hierarchical policy extraction as a solution (Section 4.2) and compare the performances of hierarchical and flat policies in a didactic environment, based on our theoretical analysis (Section 4.3).

### Motivation: why non-hierarchical policies might struggle

One common strategy in offline RL is to first fit a value function and then extract a policy that takes actions leading to high values [3; 8; 29; 30; 49; 52; 67; 71; 95; 97; 98; 100]. This strategy can be directly applied to offline goal-conditioned RL by learning a goal-conditioned policy \((a s_{t},g)\) that aims to maximize the learned goal-conditioned value function \(V(s_{t+1},g)\), as in Equation (5). However, when the goal \(g\) is far from the current state \(s\), the learned goal-conditioned value function may not provide a clear learning signal for a flat, non-hierarchical policy. There are two reasons for this. First, the differences between the values of different next states (\(V(s_{t+1},g)\)) may be small, as bad outcomes by taking suboptimal actions may be simply corrected in the next few steps, causing only relatively minor costs. Second, these small differences can be overshadowed by the noise present in the learned value function (due to, for example, sampling error or approximation error), especially when the goal is distant from the current state, in which case the magnitude of the goal-conditioned value (and thus the magnitude of its noise or errors) is large. In other words, the "signal-to-noise" ratio in the next time step values \(V(s_{t+1},g)\) can be small, not providing sufficiently clear learning signals for the flat policy. Figure 2 illustrates this problem. Figure 2 shows the ground-truth optimal value function \(V^{*}(s,g)\) for a given goal at each state, which can guide the agent to reach the goal. However, when noise is present in the learned value function \((s,g)\) (Figure 2), the flat policy \((a s,g)\) becomes erroneous, especially at states far from the goal (Figure 2).

### Our hierarchical policy structure

To address this issue, our main idea in this work, which we present fully in Section 5, is to separate policy extraction into two levels. Instead of directly learning a single, flat, goal-conditioned policy \((a s_{t},g)\) that aims to maximize \(V(s_{t+1},g)\), we extract both a high-level policy \(^{h}(s_{t+k} s_{t},g)\) and a low-level policy \(^{}(a s_{t},s_{t+k})\), which aims to maximize \(V(s_{t+k},g)\) and \(V(s_{t+1},s_{t+k})\), respectively. Here, \(s_{t+k}\) can be viewed as a waypoint or _subgoal_. The high-level policy outputs intermediate subgoal states that are \(k\) steps away from \(s\), while the low-level policy produces primitive actions to reach these subgoals. Although we extract both policies from the _same_ learned value function in this way, this hierarchical scheme provides clearer learning signals for both policies. Intuitively, the high-level policy receives a more reliable learning signal because different subgoals lead to more dissimilar values than primitive actions. The low-level policy also gets a clear signal (from the same value function) since it queries the value function with only nearby states, for which the value function is relatively more accurate (Figure 1). As a result, the overall hierarchical policy can be more robust to noise and errors in the value function (Figure 2).

### Didactic example: hierarchical policies mitigate the signal-to-noise ratio challenge

To further understand the benefits of hierarchical policies, we study a toy example with one-dimensional state space (Figure 3). In this environment, the agent can move one unit to the left or right at each time step. The agent gets a reward of \(0\) when it reaches the goal; otherwise, it always gets \(-1\). The optimal goal-conditioned value function is hence given as \(V^{*}(s,g)=-|s-g|\) (assuming \(=1\)). We assume that the noise in the learned value function \((s,g)\) is proportional to the optimal value: _i.e._, \((s,g)=V^{*}(s,g)+ z_{s,g}V^{*}(s,g)\), where \(z_{s,g}\) is sampled independently from the standard normal distribution and \(\) is its standard deviation. This indicates that as the goal becomes more distant, the noise generally increases, a trend we observed in our experiments (see Figure 8).

In this scenario, we compare the probabilities of choosing incorrect actions under the flat and hierarchical policies. We assume that the distance between \(s\) and \(g\) is \(T\) (_i.e._, \(g=s+T\) and \(T>1\)). Both the flat policy and the low-level policy of the hierarchical approach consider the goal-conditioned values at \(s 1\). The high-level policy evaluates the values at \(s k\), using \(k\)-step away subgoals. For the hierarchical approach, we query both the high- and low-level policies at every step. Given these settings, we can bound the error probabilities of both approaches as follows:

Figure 3: **1-D toy environment.**

Figure 2: **Hierarchies allow us to better make use of noisy value estimates.**_(a)_ In this gridworld environment, the optimal value function predicts higher values for states \(s\) that are closer to the goal \(g\) (**). _(b, c)_ However, a noisy value function results in selecting incorrect actions (\(\)). _(d)_ Our method uses this _same_ noisy value function to first predict an intermediate subgoal, and then select an action for reaching this subgoal. Actions selected in this way correctly lead to the goal.

**Proposition 4.1**.: _In the environment described in Figure 3, the probability of the flat policy \(\) selecting an incorrect action is given as \(()=(-}{+1}})\) and the probability of the hierarchical policy \(^{}^{h}\) selecting an incorrect action is bounded as \((^{}^{h})(-}{ {(T/k)^{2}+1}})+(-}{+1}})\), where \(\) denotes the cumulative distribution function of the standard normal distribution, \((x)=[z x]=}_{-}^{x}e^{-t^{2}/2 }t\)._

The proof can be found in Appendix E.1. We first note that each of the error terms in the hierarchical policy bound is always no larger than the error in the flat policy, implying that both the high- and low-level policies are more accurate than the flat policy. To compare the total errors, \(()\) and \((^{}^{h})\), we perform a numerical analysis. Figure 4 shows the hierarchical policy's error bound for varying subgoal steps in five different \((T,)\) settings. The results indicate that the flat policy's error can be significantly reduced by employing a hierarchical policy with an appropriate choice of \(k\), suggesting that splitting policy extraction into two levels can be beneficial.

## 5 Hierarchical Implicit Q-Learning (HIQL)

Based on the hierarchical policy structure in Section 4, we now present a practical algorithm, which we call **Hierarchical Implicit Q-Learning (HIQL)**, to extract hierarchical policies that are robust to the noise present in the learned goal-conditioned value function. We first explain how to train a subgoal policy (Section 5.1) and then extend this policy to predict representations (learned via the value function), which will enable HIQL to scale to image-based environments (Section 5.2).

### Hierarchical policy extraction

As motivated in Section 4.2, we split policy learning into two levels, with a high-level policy generating intermediate subgoals and a low-level policy producing primitive actions to reach the subgoals. In this way, the learned goal-conditioned value function can provide clearer signals for both policies, effectively reducing the total policy error. Our method, HIQL, extracts the hierarchical policies from the _same_ value function learned by action-free IQL (Equation (4)) using AWR-style objectives. While we choose to use action-free IQL in this work, we note that our hierarchical policy extraction scheme is orthogonal to the choice of the underlying offline RL algorithm used to train a goal-conditioned value function.

HIQL trains both a high-level policy \(^{h}_{_{h}}(s_{t+k} s_{t},g)\), which produces optimal \(k\)-step subgoals \(s_{t+k}\), and a low-level policy \(^{}_{_{t}}(a s_{t},s_{t+k})\), which outputs primitive actions, with the following objectives:

\[J_{^{h}}(_{h}) =_{(s_{t},s_{t+k},g)}[(^{h}(s_{ t},s_{t+k},g))^{h}_{_{h}}(s_{t+k} s_{t},g)],\] (6) \[J_{^{}}(_{}) =_{(s_{t},a_{t},s_{t+1},s_{t+k})}[( ^{}(s_{t},a_{t},s_{t+k}))^{}_{_{}}(a_{t} s _{t},s_{t+k})],\] (7)

where \(\) denotes the inverse temperature hyperparameter and we approximate \(^{h}(s_{t},s_{t+k},g)\) as \(V_{_{V}}(s_{t+k},g)-V_{_{V}}(s_{t},g)\) and \(^{}(s_{t},a_{t},s_{t+k})\) as \(V_{_{V}}(s_{t+1},s_{t+k})-V_{_{V}}(s_{t},s_{t+k})\). We do not include rewards and discount factors in these advantage estimates for simplicity, as they are (mostly) constants or can be subsumed into the temperature \(\) (see Appendix A for further discussion). Similarly to vanilla AWR (Equation (5)), our high-level objective (Equation (6)) performs a weighted regression over subgoals to reach the goal, and the low-level objective (Equation (7)) carries out a weighted regression over primitive actions to reach the subgoals.

Figure 4: **Comparison of policy errors in flat vs. hierarchical policies in didactic environments. The hierarchical policy, with an appropriate subgoal step, often yields significantly lower errors than the flat policy.**We note that Equation (6) and Equation (7) are completely separated from one another, and only the low-level objective requires action labels. As a result, we can leverage action-free data for both the value function and high-level policy of HIQL, by further training them with a potentially large amount of additional passive data. Moreover, the low-level policy is relatively easy to learn compared to the other components, as it only needs to reach local subgoals without the need for learning the complete global structure. This enables HIQL to work well even with a limited amount of action information, as we will demonstrate in Section 6.4.

### Representations for subgoals

In high-dimensional domains, such as pixel-based environments, directly predicting subgoals can be prohibitive or infeasible for the high-level policy. To resolve this issue, we incorporate representation learning into HIQL, letting the high-level policy produce more compact _representations_ of subgoals. While one can employ existing action-free representation learning methods [34; 61; 68; 82] to learn state representations, HIQL simply uses an intermediate layer of the value function as a goal representation, which can be proven to be sufficient for control. Specifically, we parameterize the goal-conditioned value function \(V(s,g)\) with \(V(s,(g))\), and use \((g)\) as the representation of the goal. Using this representation, the high-level policy \(^{h}(z_{t+k} s_{t},g)\) produces \(z_{t+k}=(s_{t+k})\) instead of \(s_{t+k}\), which the low-level policy \(^{t}(a s_{t},z_{t+k})\) takes as input to output actions (Figure 0(a)). In this way, we can simply learn compact goal representations that are sufficient for control with no separate training objectives or components. Formally, we prove that the representations from the value function are sufficient for action selection:

**Proposition 5.1** (Goal representations from the value function are sufficient for action selection).: _Let \(V^{*}(s,g)\) be the value function for the optimal reward-maximizing policy \(^{*}(a s,g)\) in a deterministic MDP. Let a representation function \((g)\) be given. If this same value function can be represented in terms of goal representations \((g)\), then the reward-maximizing policy can also be represented in terms of goal representations \((g)\):_

\[\ V_{}(s,(g))\ s.t.\ V_{}(s,(g))=V^{*}(s,g) s,g\] \[\ _{}(a s,(g))\ s.t.\ _{}(a s,(g))=^{*}(a s,g) s,g.\]

While Proposition 5.1 shows that the parameterized value function \(V(s,(g))\) provides a sufficient goal representation \(\), we found that additionally concatenating \(s\) to the input to \(\) (_i.e._, using \(([g,s])\) instead of \((g)\))  leads to better empirical performance (see Appendix A for details), and thus we use the concatenated variant of value function parameterization in our experiments. We provide a pseudocode for HIQL in Algorithm 1 and the full training details in Appendices A and D.

```
1:Input: offline dataset \(\), action-free dataset \(_{}\) (optional, \(_{}=\) otherwise)
2:Initialize value function \(V_{_{V}}(s,(g))\) with built-in representation \((g)\), high-level policy \(^{h}_{_{h}}(z_{t+k} s_{t},g)\), low-level policy \(^{a}_{_{c}}(a s_{t},z_{t+k})\), learning rates \(_{V},_{h},_{}\)
3:while not converged do
4:\(_{V}_{V}-_{V}_{_{V}}_{V} (_{V})\) with \((s_{t},s_{t+1},g)_{}\)# Train value function, Equation (4)
5:endwhile
6:while not converged do
7:\(_{h}_{h}+_{h}_{_{h}}J_{^{h}}( _{h})\) with \((s_{t},s_{t+k},g)_{}\)# Extract high-level policy, Equation (6)
8:endwhile
9:while not converged do
10:\(_{}_{}+_{}_{_{}}J_{^{ }}(_{})\) with \((s_{t},a_{t},s_{t+1},s_{t+k})\)# Extract low-level policy, Equation (7)
11:endwhile ```

**Algorithm 1** Hierarchical Implicit Q-Learning (HIQL)

## 6 Experiments

Our experiments will use six offline goal-conditioned tasks, aiming to answer the following questions:

1. How well does HIQL perform on a variety of goal-conditioned tasks, compared to prior methods?
2. Can HIQL solve image-based tasks, and are goal representations important for good performance?
3. Can HIQL utilize action-free data to accelerate learning?
4. Does HIQL mitigate policy errors caused by noisy and imperfect value functions in practice?

### Experimental setup

We first describe our evaluation environments, shown in Figure 5 (state-based) and Figure \(6\) (pixel-based). **AntMaze**[9; 87] is a class of challenging long-horizon navigation tasks, where the goal is to control an \(8\)-DoF Ant robot to reach a given goal location from the initial position. We use the four medium and large maze datasets from the original D4RL benchmark . While the large mazes already present a significant challenge for long-horizon reasoning, we also include two even larger mazes (AntMaze-Ultra) proposed by Jiang et al. . **Kitchen** is a long-horizon manipulation domain, in which the goal is to complete four subtasks (_e.g._, open the microwave or move the kettle) with a \(9\)-DoF Franka robot. We employ two datasets consisting of diverse behaviors ('-partial' and '-mixed') from the D4RL benchmark . **CALVIN**, another long-horizon manipulation environment, also features four target subtasks similar to Kitchen. However, the dataset accompanying CALVIN  consists of a much larger number of task-agnostic trajectories from \(34\) different subtasks, which makes it challenging for the agent to learn relevant behaviors for the goal. **Procgen Maze** is a pixel-based maze navigation environment. We train agents on an offline dataset consisting of \(500\) or \(1000\) different maze levels with a variety of sizes, colors, and difficulties, and test them on both the same and different sets of levels to evaluate their generalization capabilities. **Visual AntMaze** is a vision-based variant of the AntMaze-Large environment . We provide only a \(64 64 3\) camera image (as shown in the bottom row of Figure 5(b)) and the agent's proprioceptive states, excluding the global coordinates. As such, the agent must learn to navigate the maze based on the wall structure and floor color from the image. **Roboverse**[25; 104] is a pixel-based, goal-conditioned robotic manipulation environment. The dataset consists of \(48 48 3\) images of diverse sequential manipulation behaviors, starting from randomized initial object poses. We evaluate the agent's performance across five unseen goal-reaching tasks that require multi-stage reasoning and generalization. To train goal-conditioned policies in these benchmark environments, during training, we replace the original rewards with a sparse goal-conditioned reward function, \(r(s,g)=0\) (if \(s=g),\ -1\ ()\).

We compare the performance of HIQL with six previous behavioral cloning and offline RL methods. For behavioral cloning methods, we consider flat goal-conditioned behavioral cloning (GCBC) [19; 33] and hierarchical goal-conditioned behavioral cloning (HGCBC) with two-level policies [35; 59]. For offline goal-conditioned RL methods, we evaluate a goal-conditioned variant of IQL  ("GC-IQL") (Section 3), which does not use hierarchy, and POR  ("GC-POR"), which uses hierarchy but does not use temporal abstraction (_i.e._, similar to \(k=1\) in HIQL) nor representation learning. In AntMaze, we additionally compare HIQL with two model-based approaches that studied this domain in prior work: Trajectory Transformer (TT) , which models entire trajectories with a Transformer , and TAP , which encodes trajectory segments with VQ-VAE  and performs model-based planning over latent vectors in a hierarchical manner. We use the performance reported by Jiang et al.  for comparisons with TT and TAP. In our experiments, we use \(8\) random seeds and represent \(95\%\) confidence intervals with shaded regions (in figures) or standard deviations (in tables), unless otherwise stated. We provide full details of environments and baselines in Appendix D.

### Results on state-based environments

We first evaluate HIQL in the five state-based environments (AntMaze-{Medium, Large, Ultra}, Kitchen, and CALVIN) using nine offline datasets. We evaluate the performance of the learned policies by commanding them with the evaluation goal state \(g\) (_i.e._, the benchmark task target position in AntMaze, or the state that corresponds to completing all four subtasks in Kitchen and CALVIN), and measuring the average return with respect to the original benchmark task reward function. We test two versions of HIQL (without and with representations) in state-based environments. Table 1 and Figure 6(a) show the results on the nine offline datasets, indicating that HIQL mostly achieves the best performance in our experiments. Notably, HIQL attains an \(88\%\) success rate on AntMaze-Large and \(53\%\) on AntMaze-Ultra, which is, to the best of our knowledge, better than any previously reported

Figure 5: **State-based benchmark environments. Figure 6: Pixel-based benchmark environments.**result on these datasets. In manipulation domains, we find that having latent subgoal representations in HIQL is important for enabling good performance. In CALVIN, while other methods often fail to achieve any of the subtasks due to the high diversity in the data, HIQL completes approximately two subtasks on average.

### Results on pixel-based environments

Next, to verify whether HIQL can scale to high-dimensional environments using goal representations, we evaluate our method on three pixel-based domains (Procgen Maze, Visual AntMaze, and Roboverse) with image observations. For the prior hierarchical approaches that generate raw subgoals (HGCBC and GC-POR), we apply HIQL's value-based representation learning scheme to enable them to handle the high-dimensional observation space. Table 2 and Figure 6(b) present the results, showing that our hierarchical policy extraction scheme, combined with representation learning, improves performance in these image-based environments as well. Notably, in Procgen Maze, HIQL exhibits larger gaps compared to the previous methods on the test sets. This is likely because the high-level policy can generalize better than the flat policy, as it can focus on the long-term direction toward the goal rather than the maze's detailed layout. In Roboverse, HIQL is capable of generalizing to solve unseen robotic manipulation tasks purely from images, achieving an average success rate of \(62\%\).

### Results with action-free data

As mentioned in Section 5.1, one of the advantages of HIQL is its ability to leverage a potentially large amount of passive (action-free) data. To empirically verify this capability, we train HIQL on action-limited datasets, where we provide action labels for just \(25\%\) of the trajectories and use state-only trajectories for the remaining \(75\%\). Table 3 shows the results from six different tasks, demonstrating

  
**Dataset** & **GCBC** & **HGCBC** & **GC-IQL** & **GC-POR** & **TAP** & **TT** & **HIQL (ours)** & **HIQL (w/o rep.)** \\  antmaze-medium-diverse & \(67.3 0.1\) & \(71.6 0.9\) & \(63.5 0.14\) & \(74.8 1.1\) & \(85.0\) & \(\) & \(86.8 4.6\) & \(89.9 1.5\) \\ antmaze-medium-play & \(71.9 16.2\) & \(66.3 0.2\) & \(70.9 11.2\) & \(71.4 0.9\) & \(78.0\) & \(\) & \(84.1 10.8\) & \(87.0 8.4\) \\ antmaze-large-diverse & \(20.2 1.9\) & \(63.9 0.9\) & \(50.7 11.8\) & \(90.9 11.7\) & \(82.0\) & \(60.0\) & \( 2.5\) & \(87.3 13.7\) \\ antmaze-large-play & \(23.1 1.6\) & \(64.7 11.4\) & \(65.5 11.4\) & \(63.2 16.1\) & \(74.0\) & \(66.7\) & \( 17.5\) & \(81.2 14.6\) \\ antmaze-ultra-diverse & \(14.4 19.7\) & \(39.4 39.6\) & \(21.6 15.2\) & \(29.8 11.6\) & \(26.0\) & \(33.3\) & \( 17.4\) & \(52.6 18.7\) \\ antmaze-ultra-play & \(20.7 19.7\) & \(38.2 13.2\) & \(29.8 12.4\) & \(31.0 19.4\) & \(22.0\) & \(20.0\) & \(39.2 14.8\) & \( 12.4\) \\  kitchen-partial & \(38.5 11.8\) & \(32.0 16.7\) & \(39.2 11.3\) & \(18.4 14.3\) & - & - & \( 9.2\) & \(46.3 36.6\) \\ kitchen-mixed & \(46.7 20.1\) & \(46.8 17.6\) & \(31.5 12.8\) & \(27.9 17.9\) & - & - & \( 4.6\) & \(36.8 20.1\) \\  calvin & \(17.3 11.8\) & \(3.1 9.8\) & \(7.8 17.6\) & \(12.4 16.6\) & - & - & \( 59.5\) & \(23.4 27.1\) \\   

Table 1: **Evaluating HIQL on state-based offline goal-conditioned RL.** HIQL mostly outperforms six baselines on a variety of benchmark tasks, including on different types of data. We show the standard deviations across 8 random seeds and refer to Appendix B for the full training curves. Baselines: GCBC , HGCBC , GC-POR , TAP , TT .

  
**Dataset** & **GCBC** & **HGCBCBC** (+-----) & **GC-IQL** & **GC-POR** (+--) & **HIQL (ours)** \\  procgen-maze-500-train & \(16.8 2.8\) & \(14.3 4.1\) & \(72.5 0.0\) & \(75.8 12.1\) & \( 0.0\) \\ procgen-maze-500-test & \(14.5 5.6\) & \(11.2 3.7\) & \(94.5 5.8\) & \(53.8 14.5\) & \( 13.2\) \\ procgen-maze-1000-train & \(27.2 2.9\) & \(15.0 8.7\) & \(78.2 27.2\) &  HIQL, even with a limited amount of action information, can mostly maintain its original performance. Notably, action-limited HIQL still outperforms previous offline RL methods (GC-IQL and GC-POR) trained with the full action-labeled data. We believe this is because HIQL learns a majority of the knowledge through hierarchical subgoal prediction from state-only trajectories.

### Does HIQL mitigate policy errors caused by noisy value functions?

To empirically verify whether our two-level policy architecture is more robust to errors in the learned value function (_i.e_., the "signal-to-noise" ratio argument in Section 4), we compare the policy accuracies of GC-IQL (flat policy), GC-POR (hierarchy without temporal abstraction), and HIQL (ours) in Procgen Maze, by evaluating the ratio at which the ground-truth actions match the learned actions. We also measure the noisiness (_i.e_., standard deviation) of the learned value function with respect to the ground-truth distance between the state and the goal. Figure 8 shows the results. We first observe that the noise in the value function generally becomes larger as the state-goal distance increases. Consequently, HIQL achieves the best policy accuracy, especially for distant goals (\((s,g) 50\)), as its hierarchical policy extraction scheme provides the policies with clearer learning signals (Section 4.2).

We refer to Appendix C for further analyses, including **subgoal visualizations** and an **ablation study** on subgoal steps and design choices for representations.

## 7 Conclusion

We proposed HIQL as a simple yet effective hierarchical algorithm for offline goal-conditioned RL. While hierarchical RL methods tend to be complex, involving many different components and objectives, HIQL shows that it is possible to build a method where a single value function simultaneously drives the learning of the low-level policy, the high-level policy, and the representations in a relatively simple and easy-to-train framework. We showed that HIQL not only exhibits strong performance in various challenging goal-conditioned tasks, but also can leverage action-free data and enjoy the benefits of built-in representation learning for image-based tasks.

**Limitations.** One limitation of HIQL is that the objective for its action-free value function (Equation (4)) is unbiased only when the environment dynamics are deterministic. As discussed in Section 3, HIQL (and other prior methods that use action-free videos) may overestimate the value function in partially observed or stochastic settings. To mitigate the optimism bias of HIQL in stochastic environments, we believe disentangling controllable parts from uncontrollable parts of the environment can be one possible solution [92; 99], which we leave for future work. Another limitation of our work is that we assume the noise for each \(V(s,g)\) is independent in our theoretical analysis (Proposition 4.1). While Figure 8 shows that the "signal-to-noise" argument empirically holds in our experiments, the independence assumption in our theorem might not hold in environments with continuous state spaces, especially when the value function is modeled by a smooth function approximator.

  
**Dataset** & **GC-IQL (full)** & **GC-POR (full)** & **HIQL (full)** & **HIQL (action-limited)** & vs. HIQL (full) & vs. Prev. best (full) \\  antrance-large-diverse & \(50.7 1.8\) & \(49.0 7.9\) & \(88.2 5.3\) & \(89.9 6.4\) & \(+0.7\) & \(+38.2\) \\ antrance-ultra-diverse & \(21.6 0.2\) & \(29.8 1.8\) & \(52.9 1.4\) & \(38.2 1.4\) & \(-14.7\) & \(+8.4\) \\ kitchen-mixed & \(51.3 0.2\) & \(27.9 7.9\) & \(67.4 6.8\) & \(59.1 6.6\) & \(-8.6\) & \(+7.8\) \\ calvin & \(7.8 0.4\) & \(12.4 1.8\) & \(43.8 0.5\) & \(35.8 0.7\) & \(-8.0\) & \(+23.4\) \\ procepgn-maze-500-train & \(7.5 0.0\) & \(75.8 11.3\) & \(82.5 0.6\) & \(77.0 11.5\) & \(-5.5\) & \(+1.2\) \\ procepgn-maze-500-test & \(49.5 9.8\) & \(53.8 11.5\) & \(64.5 11.2\) & \(63.5 16.4\) & \(+1.0\) & \(+11.7\) \\   

Table 3: **HIQL can leverage passive, action-free data.** Since our method requires action information only for the low-level policy, which is relatively easier to learn, HIQL mostly achieves comparable performance with just \(25\%\) of action-labeled data, outperforming even baselines trained on full datasets.

Figure 8: **Value and policy errors in Procgen Maze: _(left)_ As the distance between the state and the goal increases, the learned value function becomes noisier. _(middle)_ We measure the accuracies of learned policies. _(right)_ Thanks to our hierarchical policy extraction scheme (Section 4.2), HIQL exhibits the best policy accuracy, especially when the goal is far away from the state. The blue numbers denote the accuracy differences between HIQL and the second-best methods.**