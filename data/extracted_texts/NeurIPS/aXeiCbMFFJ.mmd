# Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning

Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning

 Hao Shao\({}^{1,2}\) Shengju Qian\({}^{1}\) Han Xiao\({}^{1}\) Guanglu Song\({}^{2}\)

Zhuofan Zong\({}^{1}\) Letian Wang\({}^{3}\) Yu Liu\({}^{2\) Hongsheng Li\({}^{1,4\)

\({}^{1}\)The Chinese University of Hong Kong \({}^{2}\)SenseTime Research

\({}^{3}\)University of Toronto \({}^{4}\)HKGAI under InnoHK

###### Abstract

Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this webpage to support further research in this area.

## 1 Introduction

With the success of large language models (LLMs) like GPT-4  and Gemini , researchers are enhancing these models by incorporating visual understanding capabilities. This enthusiasm has led to the emergence of multi-modal large language models (MLLM), such as LLAVA , SPHINX , and Qwen-VL . Involving the extraction of visual tokens from input images, these MLLMs mostly follow a two-stage schedule: first the alignment of these tokens with linguistic modalities, and then the joint processing in LLMs. MLLMs have demonstrated viability in various scenarios, such as image captioning, visual question answering, and optical character recognition, owing to their ability to generate plausible outputs and leverage the extensive knowledge of LLMs.

However, many popular MLLMs  and related benchmarks  are primarily trained to respond to instructions based on visual inputs, employing a decoder-only autoregressive design as a single black box. While these models exhibit impressive generation capabilities, they suffer from inaccurate information  and even hallucinations . Moreover, the black-box design hinders the interpretability of visual-language models. Additionally, the potential of multi-turn in-context capability and the advantages of chain-of-thought  for LLMs have not been extensively explored in MLLMs. Some recent works, such as multimodal-CoT and [80; 79], have shown improvements by incorporating text-level chain-of-thought reasoning or in-context learning. However, it remains uncharted whether existing MLLMs can benefit from chain-of-thought reasoning in the visual understanding process, along with their interpretability remains largely unexplored.

Furthermore, humans comprehend intricate visual information differently, often by focusing on specific image regions or details within a given sample. For instance, when asked for a detailed regional description, humans tend to scan the entire image first, locate the references, and then focus on the targets. In contrast, most MLLMs process aligned image contexts in a fixed-grain manner with a large amount of computation (_e.g.,_ CLIP , EVA2-CLIP , InternVL ). To mimic human-like efficient reasoning behaviors, models need to identify image regions containing essential visual details and dynamically zoom in to capture adjusted context, which current MLLMs struggle with, leading them to seek information primarily from the text domain.

Therefore, there is a pressing need to develop methods that can handle multi-turn, dynamic focused visual inputs, while providing more interpretable stages of reasoning to enhance the efficacy and applicability of MLLMs. However, two significant challenges hinder the design of such pipelines: the lack of intermediate visual chain-of-thought supervision in existing visual question-answering (VQA) datasets, and the reliance of popular MLLM pipelines on static image context inputs.

To address these challenges, we develop and release a 438k visual chain-of-thought dataset by annotating each visual question-answer pair with a bounding box. The bounding box highlights the key image region essential for answering the question. We suppose that accurately locating and comprehending this key region will significantly improve MLLM's response accuracy and relevance. Notably, about 98k question-answer pairs include extra detailed reasoning steps. These annotations are designed to instruct the MLLM in a logical, step-by-step process to identify the final bbox and generate the answer. Building on the dataset, we propose a novel pipeline that unleashes the visual CoT reasoning capability of MLLMs, which is designed to identify and output key regions in an image that provides detailed information relevant to the given question. It integrates the understanding of both the original image and detailed local image to generate the final answer. Besides, we provide the corresponding visual CoT benchmark and pre-trained models for reproducibility, aiming to foster further research in the visual chain-of-thought for MLLMs.

To summarize, this paper makes the following contributions:

* We present a visual chain-of-thought dataset comprising 438k data items, each consisting of a question, an answer, and an intermediate bounding box as CoT contexts. Some items also contain detailed reasoning steps. The dataset spans across five distinct domains.
* We propose a novel multi-turn processing pipeline for MLLMs that can dynamically focus on visual inputs and provide intermediate interpretable thoughts.
* We introduce the visual chain-of-thought benchmark for evaluating MLLMs in scenarios where they need to focus on specific local regions or reasons to identify objects.

## 2 Related Works

**Multi-modal LLMs.** Since the advent of large language models (LLMs), their success in various language applications has paved the way for the development of multi-modal large language models (MLLMs), which integrate vision and language modalities. Initially, MLLMs were treated as dispatch schedulers to connect vision expert models, such as VisualChatGPT , HuggingGPT , and MM-REACT , in order to extend language models to other tasks and modalities. More recently, MLLMs have focused on aligning these modalities through extensive training on image-caption pairs or image-question conversations. Notable methods like LLaVA  train a projector that maps image tokens to aligned representations of pre-trained LLMs. Other approaches, such as BLIP-2 [32; 31], adopt a query transformer (Q-Former) to learn image embeddings using learnable queries after obtaining image features. MoVA  designs an adaptive router to fuse task-specific vision experts with a coarse-to-fine mechanism. In terms of training strategy, recent works [40; 3; 68; 94; 10; 44] commonly employ a 2-stage framework. The first stage involves pre-training on image-caption pairs, while the second stage focuses on alignment by using question-answering triplets. MLLMs have also been extended to various applications, including fine-grained localization [69; 29] such as object detection , video understanding [84; 34; 11], and image generation [25; 56].

**Reasoning Capability of LLMs and MLLMs.** LLMs have demonstrated impressive reasoning capabilities, enabled by in-context learning (ICL) , which allows feeding prompted samples and context. This capability has been further enhanced by chain-of-thought (CoT)  prompting, which enables LLMs to generate coherent intermediate reasoning steps toward the final answer. Previous studies have shown that LLMs benefit from manually written demonstrations  as well as zero-shot prompting outputs . Trar  proposes a routing module to dynamically select informative regions based on the attention map. However, due to the domain gap between vision and text data, MLLMs fail to naturally inherit this reasoning capability. To address this limitation, researchers have focused on enhancing the reasoning capability of MLLMs in both the training and prompting paradigms. For instance, Flamingo  bridges the gap between these two modalities by pre-training on interleaved visual and textual data. Similarly, other works leverage visual grounded-reasoning  data in training, such as Shikra  and KOSMOS-2 . More recently, V\({}^{*}\) and CogCoM modify the general mechanism in MLLMs and collect a series of visual reasoning steps as training data. On the other hand, studies have also explored prompting models  to understand complex visual scenes and tasks, focusing on the details of prompting techniques in MLLMs.

## 3 Visual CoT Dataset

There is a shortage of multimodal datasets for training multi-modal large language models (MLLMs) that require to identify specific regions in an image for additional attention to improve response performance. This type of dataset with grounding bbox annotations could possibly help the MLLM output intermediate interpretable attention area and enhance performance. To fill the gap, we curate a visual CoT dataset, as illustrated in Fig. 1 and Tab. 1. This dataset specifically focuses on identifying critical regions within images, a feature essential for models to concentrate on relevant visual elements

Figure 1: Examples of five domains covered in the visual CoT dataset, with corresponding question-answer annotations and visual CoT bboxes: chart, text/doc, general VQA, fine-grained understanding, and relation reasoning. The red bounding boxes in the images highlight the critical image regions that provide necessary and related information for answering the questions.

to improve response accuracy. Each data sample consists of a question, answer, and a corresponding visual bounding box across five domains, as shown in Tab. 2. Some data samples also include extra detailed reasoning steps.

To ensure a robust foundation for detailed visual and textual analysis, our dataset deliberately integrates a diverse selection of data including text/doc, fine-grained understanding, charts, general VQA, and relation reasoning. These data domains are deliberately chosen to cultivate a comprehensive skill set across varied analytical tasks: 1) Text/doc enhances MLLM's capabilities on OCR and contextual understanding, crucial for applications requiring text interpretation in complex environments. 2) Fine-grained understanding aids in identifying and distinguishing subtle differences in visual appearance and patterns. 3) Charts foster the ability to interpret graphical data, which are essential for business and scientific applications. 4) General VQA exposes models to a wide array of visual queries, improving their general usability. 5) Relation reasoning data develops spatial and contextual awareness of MLLMs, vital for interactive and navigational tasks. Together, these modalities ensure the dataset not only fills existing gaps but also enhances the versatility and contextual awareness of MLLMs across varied scenarios.

### Data Generation

To collect and build a diverse and comprehensive Visual CoT dataset, we select twelve source datasets across five distinct domains, primarily consisting of Visual Question Answering (VQA) and Image Captioning datasets. We reuse their images and useful annotations, such as question-answer pairs, image captions, and object relations, to aid in building our dataset. The data construction process involves both linguistic and visual annotators to create question-answer pairs, and provide intermediate chain-of-thought bounding boxes indicating the crucial image region for answering the question. For the linguistic annotations, we employ GPT-4 , known for its robust language

  
**An example of detailed reasoning steps in GQA dataset** \\
**Question:** What appliance is to the right of the cabinet? \\
**H#** \\ Please think step by step and provide the bounding box coordinate of the region that can help you answer the question better. \\
**H#** \\
**Reasoning steps:** 1. Identify the cabinet in the image. \\
2. Observe the area to the right of the identified cabinet. \\
3. Look for any appliance located to the right side of the cabinet. \\
4. Determine the name of the appliance found in this location \\
**CoT BBox:** \\ 
**Answer** \\ The appliance is a microwave. \\   

Table 1: One data example with detailed reasoning steps, of which we have collected about 98k of this type. The red bounding box shows the important image region for answering the question.

  
**Domain** & **Source Dataset** & **Size** & **Used GPT-4?** & **Dataset Description** \\   & TextVQA  & 16k & No & Images with text \\  & TextCaps  & 32k & Yes & Images with text \\  & DocVQA  & 33k & No & Doc Images \\  & DUE  & 15k & No & Doc Images \\  & SROIE  & 4k & No & Invoice Images \\ 
**Fine-Grained Understanding** & Birds-200-2011  & 10k & No & Images of birds \\   & Flickr30k  & 136k & Yes & Images \\  & Visual7W  & 43k & No & Images \\ 
**Charts** & InfographicsVQA  & 15k & No & Infographic \\   & VSR  & 3k & No & Images \\  & GQA  & 88k & Yes & **(with detailed reasoning steps)** \\  & Open images  & 43k & No & Images \\   

Table 2: The overview of the visual CoT dataset. The dataset spans five distinct domains and includes various source datasets, ensuring a broad representation of visual data styles.

understanding and generation capabilities. For the visual annotations, we choose PaddleOCR , an efficient and accurate tool for optical character recognition. In the following sections, we elaborate on the generation methods employed for each domain-specific dataset.

**Text/Doc.** We choose five text-related datasets to create data in this domain: TextVQA , DocVQA , DUDE , TextCaps , SROIE . The five datasets focus on text recognition and comprehension in a variety of images and documents. TextVQA, DocVQA, DUDE and SROIE have already provided question-answer pairs, which we directly adopt. TextCaps, providing only captions and OCR tokens, required us to employ a linguistic annotator to create corresponding questions and answers (see further details in Appendix E.1). For the visual CoT bboxes, we then apply PaddleOCR to detect OCR-identified regions in the image, and specify the CoT bounding boxes as the region that consists of words and sentences aligning with the answer. Furthermore, we also design a filtering pipeline to improve content quality. This process ensures that the areas highlighted by the bounding boxes are directly relevant to the questions.

**Fine-Grained Understanding.** For this domain, we use Birds-200-2011 , which is a widely-used dataset for fine-grained visual categorization. This dataset is not only rich in visual data but also includes detailed annotations about various bird parts and their attributes, along with bird bounding boxes in each picture. To leverage this dataset for our MLLM, we have formulated questions that challenge the model to identify specific characteristics or features present in the birds. These questions are designed to test the MLLM's ability to discern and recognize fine-grained details in the images.

**General VQA.** We use Flickr30k  and Visual7W  as the dataset for general VQA tasks. In Flickr30k, each image encompassed five captions and the bounding boxes of most objects mentioned in the captions. Employing a similar approach to TextCaps, we use GPT-4 to generate questions that require focusing on small objects in the images. The visual CoT bounding boxes in our proposed dataset correspond to the bboxes of objects identified and annotated in the official dataset. Visual7W has already provided the question-answer pairs with object-level grounding annotations.

**Charts.** We select the InfographicsVQA  dataset for its high-resolution infographics, which are advantageous for training MLLMs to pinpoint answer locations. Like in our Text/Doc data, we apply OCR techniques to identify regions containing the answers, using these identified areas as the CoT bounding boxes for more precise model training.

Figure 2: Statistics of the proposed visual CoT dataset. We visualize the CoT bbox distribution, average bbox size, and average relative size of bbox area \(R\) for each source dataset.

**Relation Reasoning.** We select the Visual Spatial Reasoning (VSR) , GQA , and Open Images  datasets to construct data focusing on relation-reasoning. These datasets are rich in spatial relational information among objects in images. For our chain-of-thought (CoT) bounding boxes, we use the bounding boxes surrounding the objects relevant to the question. For instance, if the question is _"What is the material of the desk left to the woman?"_, the bounding box of the desk to the woman's left is designated as the visual CoT bounding box, providing more visual context for the MLLM's reasoning process. In GQA  each image is associated with a scene graph of objects and relations. Each question comes with a structured representation of its semantics. With these annotations, we utilize GPT-4 to generate detailed reasoning steps, as illustrated in Tab. 1. The related prompt is available in Appendix E.3.

### Dataset Analysis

We provide a visualization of the data statistics in Fig. 2. We partition the bboxes in each dataset into three groups (large, medium, small) based on the relative bounding box size \(R\), which is the ratio of the CoT bbox size relative to the total image size. The visualization reveals that the majority of the annotated key regions, particularly in text-oriented datasets, occupy only a small portion of the entire image, highlighting the importance of identifying these crucial areas to enhance performance. Specifically, the average bounding box size is \(247.8^{2}\) pixels, which well aligns with the common input resolution for a vision encoder ranges between 224 and 336 pixels, while the original image size is usually too large and needs down-sampling that loses information. These regions account for only about 13.2% of the image area. This highlights the necessity for MLLMs to accurately pinpoint these crucial areas to enhance processing efficiency and effectiveness. If the model fails to correctly identify and focus on these key regions, the majority of the image processed could be irrelevant, leading to inefficient computation, hallucination, and potential degradation in performance.

## 4 Enhancing MLLMs with Chain-of-Thought Capabilities

Along with the visual CoT dataset, we also propose a visual CoT MLLM framework named VisCoT, which employs standard models without specialized modifications, serving as a baseline to enhance MLLMs with visual CoT capabilities. In this section, we briefly introduce the framework, and illustrate the pipeline in Fig. 3. Readers are referred to Appendix B for more details.

**VisCoT Pipeline.** To train the MLLM baseline with visual CoT data, we add a CoT prompt (_"Please provide the bounding box coordinate of the region that can help you answer the question better."_) to the question, asking the model to identify the most informative region of the image. VisCoT then determines this region and generates its bounding box. During the training phase, we utilize the ground truth bounding box to extract visual information rather than a predicted one in the following steps. With the original image \(X_{0}\) and the bbox, a visual sampler extracts the localized image \(X_{1}\) containing detailed information. The same vision encoder and projector are then used to extract visual tokens \(H_{1}\). The MLLM then integrates visual tokens from both the original and localized images \(\{H_{0},H_{1}\}\) to provide more precise and comprehensive answers. For data without visual CoT annotations, this procedure is omitted as indicated by the dashed box in Fig. 3. Here, the MLLM directly answers based on the input image alone. Our VisCoT baseline is thus adaptable to data in both annotated and non-annotated formats simultaneously.

**Visual Sampler.** Given the original image and the predicted bbox, the visual sampler's role is to accurately select the relevant region that considers the visual encoder requirement and bbox corner cases. We first calculate the center point \([x_{0},y_{0}]\), half-width \(w_{half}\), and half-height \(h_{half}\) of the bounding box predicted by VisCoT. To capture more context and meet the square receptive field requirement of the CLIP model, \(\{\{w_{half},h_{half}\},_{half}\}\) is chosen as the sample size \(s\). \(_{half}\) is the half input size of the vision encoder. Consequently, the visual sampler crops the region \([x_{0}-s,y_{0}-s,x_{0}+s,y_{0}+s]\) for further processing. During inference, if the calculated cropped box extends beyond the image boundaries, the center point is adjusted towards the center of the image to ensure the box remains within the image frame. This adjustment is important for improving the overall performance, as it can mitigate the impact of any detection inaccuracies.

**Inference.** VisCoT offers two options to generate answers: with or without the visual CoT process. If the CoT feature is not needed, users can simply provide the MLLM with the image and question. To engage the CoT feature, users can append the additional visual CoT prompt after the question.

**Model Training** VisCoT baseline is trained in two stages. In the first stage, consistent with LLaVA-1.5, we freeze the weights of the vision encoder and LLM, and utilize image-text caption data for training. In the second stage, all weights are trainable. For more details, see Appendix B.

## 5 Experiments

Firstly, we provide an overview of the construction and evaluation of the CoT benchmark. Subsequently, in the evaluation phase, we begin by accessing VisCoT on the proposed benchmark (refer to Sec. 5.2). Additionally, we conduct further experiments to analyze the impact of essential components within VisCoT through an ablation study in Sec. 5.3. Finally, we showcase the capabilities of VisCoT in engaging complex multimodal conversations in Sec. 5.4. The training details and detection performance of the visual CoT bboxes can be found in Appendix B & C.

### Visual CoT Benchmark

In this section, we provide an overview of our visual CoT benchmark, which primarily focuses on scenarios where the MLLM needs to concentrate on specific regions within a complete image. We utilize 12 source datasets, as shown in Fig. 1, and when an official training/evaluation split exists, we adopt it. In cases where such a split does not exist, we randomly divide the dataset. Additionally, we incorporate the test split of SROIE, DUDE, and Visual7W to evaluate the model's zero-shot visual CoT capabilities. Following the methodology of previous MLLM studies [33; 46], we employ

    &  &  \\  MLLM & Res. & DocVQA & TextCaps & TextVQA & DUDE & SROIE & InfographicsVQA \\  LLaVA-1.5-7B  & \(336^{2}\) & 0.244 & 0.597 & 0.588 & 0.290 & 0.136 & 0.400 \\ LLaVA-1.5-13B  & \(336^{2}\) & 0.268 & 0.615 & 0.617 & 0.287 & 0.164 & **0.426** \\ SPHINX-13B  & \(224^{2}\) & 0.198 & 0.551 & 0.532 & 0.000 & 0.071 & 0.352 \\  VisCoT-7B & \(224^{2}\) & 0.355 & 0.610 & 0.719 & 0.279 & 0.341 & 0.356 \\ VisCoT-7B & \(336^{2}\) & **0.476** & **0.675** & **0.775** & **0.386** & **0.470** & 0.324 \\    &  &  &  &  \\  MLLM & Res. & Flickr30k & Visual7W & GQA & Open images & VSR & Birds-200-2011 & \\  LLaVA-1.5-7B  & \(336^{2}\) & 0.581 & 0.575 & 0.534 & 0.412 & 0.572 & 0.530 & 0.454 \\ LLaVA-1.5-13B  & \(336^{2}\) & 0.620 & **0.580** & 0.571 & 0.413 & 0.590 & **0.573** & 0.478 \\ SPHINX-13B  & \(224^{2}\) & 0.607 & 0.558 & 0.584 & 0.467 & 0.613 & 0.505 & 0.419 \\  VisCoT-7B & \(224^{2}\) & **0.671** & **0.580** & 0.616 & **0.833** & **0.682** & 0.556 & 0.550 \\ VisCoT-7B & \(336^{2}\) & 0.668 & 0.558 & **0.631** & 0.822 & 0.614 & 0.559 & **0.580** \\   

Table 3: Performance on the Visual CoT benchmark. Datasets highlighted in grey indicate their training splits were not used in our model’s training phase. Res indicates input image resolution.

Figure 3: VisCoT first extracts visual tokens from an image and pinpoints the key region relevant to the question. Then, it processes the localized visual information. Finally, the MLLM integrates the information from the overall and localized images to construct a comprehensive and accurate answer.

ChatGPT  and ask it to assign a numerical score between 0 and 1, where a higher score indicates better prediction accuracy. For detailed information on the prompt used for ChatGPT-based evaluation, please refer to Appendix E.4.

### Performance Evaluation

In this section, we comprehensively evaluate VisCoT across various multi-modal tasks to thoroughly assess our model's visual understanding ability. Tab. 3 highlights the enhancements through the visual CoT benchmark. We also showcase the baseline performance of our model on other benchmarks in Appendix D, where it directly answers questions without employing the visual CoT process.

In Tab. 3, we test our model and LLaVA-1.5 on the proposed visual CoT benchmark as detailed in Sec. 5.1. To demonstrate the impact of the chain-of-thought process, we also include the ablation study that removes this reasoning process and directly generates the response in a standard, direct manner. Notably, our pipeline shows significant improvement in the doc/text-related tasks and high-resolution image processing, even when the training splits from corresponding datasets are not utilized for the model training. For instance, SROIE  is a dataset that involves extracting key information from scanned receipts, such as the company name and the total price. Our model achieves 8\(\) performance compared to the standard pipeline without a chain-of-thought process. Furthermore, the visual CoT pipeline also shows superior results in other benchmark tasks, showing its efficacy in enhancing the model's comprehensive visual and textual interpretation abilities.

### Ablation Study

In the ablation studies below, in default, we ablate VisCoT-7B with a resolution of 224 and mainly evaluate in the proposed visual CoT benchmark.

**Visual CoT BBox Selection Strategies.** Tab. 4 showcases the performance of our model on the visual CoT benchmark using different strategies for bbox selection. As anticipated, employing ground truth annotated bounding boxes instead of model predictions yields the highest performance, surpassing the baseline by a significant margin. This can be considered the upper bound of our model's potential.

    &  &  \\   & DocVQA & TextCaps & TextVQA & DUDE & SROIE & InfographicsVQA \\  Baseline & 0.355 & 0.610 & 0.719 & 0.279 & 0.341 & 0.356 \\  w/o CoT & 0.170 & 0.502 & 0.463 & 0.175 & 0.044 & 0.332 \\ GT BBox & **0.774** & **0.827** & **0.840** & **0.718** & **0.633** & **0.778** \\ Random & 0.208 & 0.463 & 0.495 & 0.157 & 0.146 & 0.378 \\ Center & 0.220 & 0.533 & 0.558 & 0.204 & 0.205 & 0.366 \\    &  &  &  &  \\    & Flickr30k & Visual7W & GQA & & & & \\  Baseline & 0.671 & 0.580 & 0.616 & 0.833 & 0.682 & 0.556 & 0.550 \\  w/o CoT & 0.610 & 0.554 & 0.600 & 0.656 & 0.634 & 0.534 & 0.443 \\ GT BBox & **0.692** & **0.699** & **0.796** & **0.896** & **0.792** & 0.577 & **0.752** \\ Random & 0.627 & 0.458 & 0.477 & 0.763 & 0.585 & **0.683** & 0.453 \\ Center & 0.653 & 0.529 & 0.547 & 0.803 & 0.657 & 0.609 & 0.490 \\   

Table 4: Ablation study on the different BBox selection strategies. ‘w/o CoT’ indicates a standard, non-CoT-based inference process. ‘GT BBox’ uses annotated ground truth bboxes. ‘Random’ and ‘Center’ refer to using random and center bboxes instead of model predictions.

   Expanded Cropping & Centered Cropping & Doc/Text & Chart & General VQA & Relation Reasoning & Fine-grained & Average \\   & & 0.399 & 0.321 & 0.621 & 0.668 & 0.509 & 0.496 \\ ✓ & & 0.410 & 0.328 & 0.625 & 0.678 & 0.531 & 0.506 \\  & ✓ & 0.434 & 0.331 & 0.641 & 0.677 & 0.521 & 0.518 \\ ✓ & ✓ & **0.461** & **0.356** & **0.626** & **0.710** & **0.556** & **0.550** \\   

Table 5: Ablation study on the visual sampler design.

**Token Efficiency.** The visual CoT pipeline utilizes double the visual tokens for answer generation, leading us to assess its performance at various resolutions: 224, 336, and 448. As depicted in Fig. 4, the visual CoT pipeline exhibits improved token efficiency in our model. For instance, when equipped with the visual CoT, our model's accuracy at 224 resolution surpasses that of the standard pipeline at 448 resolution, while only using half the visual tokens.

Interestingly, random box selection demonstrates similar performance to the 'w/o CoT' approach, suggesting limited impact when the box selection is arbitrary or the prediction is incorrect. However, selecting the 'Center' box exhibits an improvement over the "Random" strategy, indicating that the central region of an image often contains more relevant information. This ablation study provides two key insights: firstly, our model excels at accurately predicting visual bounding boxes, and secondly, the precision of these box predictions significantly influences overall performance.

**Visual Sampler.** We ablate the visual sampler design in Tab. 5. Expanded Cropping refers to enlarging the cropped region if the region is smaller than the vision encoder's input size. Centered Cropping denotes moving the cropped region toward the center if the region extends beyond the image. The results reveal that more image context can bring better performance, and we suppose that it mitigates the problem of detection inaccuracies.

Figure 4: Trade-offs between visual token numbers and average accuracy on the visual CoT benchmark.

Figure 5: Visualization results of visual CoT to illustrate the difference between various inference modes. Model-generated bounding boxes are shown in red, while ground truth (GT) bounding boxes are in blue. The scores are evaluated by the ChatGPT. Best viewed in color and zoomed in.

### Visualization

This section displays VisCoT's qualitative performance through Fig. 5, highlighting its visual CoT ability to identify critical regions in images that aid in answering questions and synthesizing the combined contexts of both original and zoomed-in images. We also provide comparative results with different configurations: VisCoT (GT BBox), and VisCoT (w/o CoT). The accuracy of detection and depth of understanding directly contribute to the quality of the generated answers.

## 6 Conclusion

In this paper, we introduced VisCoT, a pioneering approach that enhances multi-modal large language models with visual chain-of-thought reasoning. This methodology addresses critical gaps in MLLMs, particularly in interpretability and processing dynamic visual inputs. Our visual CoT dataset offers 438k annotated question-answer pairs for detailed visual analysis. Our novel multi-turn processing pipeline allows MLLMs to dynamically focus and interpret visual data, mirroring human cognition. VisCoT provides more interpretable reasoning stages, and the visual CoT benchmark advances the evaluation of MLLMs' focus on specific image areas. Extensive experiments validate the framework's effectiveness, offering a promising starting point for further exploration in visual CoT.

**Acknowledgement.** This project is funded in part by National Key RD Program of China Project 2022ZD0161100, by the Hong Kong Generative AI Research and Development Center (HKGAI) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of HKGAI under the InnoHK.