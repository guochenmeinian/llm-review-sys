# MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot Compressive Imaging

Zhenghao Pan\({}^{1,}\)

Equal contribution. \(\) Corresponding Author: Yongyong Chen (YongyongChen.cn@gmail.com)

 Haijin Zeng\({}^{2,}\)1

Jiezhang Cao\({}^{3}\)

 Yongyong Chen\({}^{1,}\)

 Kai Zhang\({}^{4}\)

 Yong Xu\({}^{1}\)

\({}^{1}\) Harbin Institute of Technology (Shenzhen), \({}^{2}\) Ghent University,

\({}^{3}\) Harvard University, \({}^{4}\) Nanjing University

###### Abstract

Color video snapshot compressive imaging (SCI) employs computational imaging techniques to capture multiple sequential video frames in a single Bayer-patterned measurement. With the increasing popularity of quad-Bayer pattern in mainstream smartphone cameras for capturing high-resolution videos, mobile photography has become more accessible to a wider audience. However, existing color video SCI reconstruction algorithms are designed based on the traditional Bayer pattern. When applied to videos captured by quad-Bayer cameras, these algorithms often result in color distortion and ineffective demosaicing, rendering them impractical for primary equipment. To address this challenge, we propose the MambaSCI method, which leverages the Mamba and UNet architectures for efficient reconstruction of quad-Bayer patterned color video SCI. To the best of our knowledge, our work presents the first algorithm for quad-Bayer patterned SCI reconstruction, and also the initial application of the Mamba model to this task. Specifically, we customize Residual-Mamba-Blocks, which residually connect the Spatial-Temporal Mamba (STMamba), Edge-Detail-Reconstruction (EDR) module, and Channel Attention (CA) module. Respectively, STMamba is used to model long-range spatial-temporal dependencies with linear complexity, EDR is for better edge-detail reconstruction, and CA is used to compensate for the missing channel information interaction in Mamba model. Experiments demonstrate that MambaSCI surpasses state-of-the-art methods with lower computational and memory costs. PyTorch style pseudo-code for the core modules is provided in the supplementary materials. Code is at https://github.com/PAN083/MambaSCI.

## 1 Introduction

In recent years, there has been significant progress in enhancing imaging quality in smartphone image sensors. One notable trend is the adoption of the quad-Bayer Color Filter Array (CFA) pattern . Smartphones such as the iPhone 14 Pro/Max, vivo X90 Pro+, Xiaomi 13S Ultra, and OPPO Find X6 Pro utilize quad-Bayer array to enhance image quality in low-light conditions and offer higher resolution for mobile photography [2; 3]. Unlike the traditional RGBB Bayer CFA pattern, the quad-Bayer pattern expands each pixel into four sub-pixels and arranges them periodically [4; 5], as depicted in Fig. 1(a). This arrangement allows for larger pixels by

Figure 1: (a) Bayer CFA vs. Quad-Bayer CFA. (b) PSNR and FLOPS on color simulation videos (larger size means more parameters).

combining neighboring pixels of the same color, resulting in more light intake compared to the Bayer pattern. Consequently, quad-Bayer sensors offer enhanced sensitivity and resolution for imaging tasks [6; 7; 8]. As illustrated in Fig. 2(b), quad-Bayer technology effectively mitigates resolution loss and enables the capture of low-noise photos in low-light environments. In summary, quad-Bayer sensors provide HDR capability  and improved color accuracy  while effectively mitigating resolution loss and capturing low-noise photos in low-light environments.

Color videos captured by traditional high-speed cameras incur high transmission and storage costs. To solve this issue, color video snapshot compressive imaging (SCI) , which comprises both hardware encoder and software decodes components, has been proposed. During the encoding phase, multiple raw video frames undergo modulation and compression using various masks to generate 2D measurements . Subsequently, in the decoding stage, the desired high-speed color video is reconstructed from the acquired measurements and predefined masks.

So far, all color video SCI encoding and reconstruction algorithms have been designed based on the Bayer pattern [13; 14; 15; 16; 17]. As smartphone cameras continue to improve in pixel count and performance, the majority of videos are now captured using smartphones equipped with quad-Bayer patterns. However, our observation reveals that existing methods struggle to effectively reconstruct videos based on quad-Bayer patterns, often resulting in artifacts, color distortions, and incomplete demosaicing. This ineffectiveness poses a challenge for processing videos captured by smartphone cameras. Thus, in this paper, we aim to break through these challenges and introduce the first quad-Bayer-based color video SCI reconstruction method, as illustrated in Fig. 2(a).

When incorporating quad-Bayer in color video SCI reconstruction tasks, two challenges arise. One is _how to efficiently manage quad-Bayer color video processing with reduced computational complexity_. Current color video SCI reconstruction methods encompass model-based [13; 18; 19], iteration-based [14; 15], and End-to-End (E2E) approaches [20; 21]. However, model-based and iterative methods necessitate corresponding demosaicing models [6; 22; 23; 24], which have not been extensively explored for quad-Bayer fields regarding performance and model size trade-offs, leading to suboptimal reconstruction outcomes and inefficiencies. On the other hand, existing E2E methods, primarily transformer-based  and hybrid CNN-transformer  approaches, typically require significant parameters and computational resources, making it difficult to process long video sequences effectively. The high computational demands of transformers and the absence of a global attention mechanism in CNNs hinder their scalability to modern, lightweight architectures. Leveraging the advancements in State Space Models (SSMs) [25; 26; 27], modern SSMs like Mamba  have demonstrated the ability to effectively capture long-range dependencies while maintaining linear complexity relative to the input size. Furthermore, numerous experiments have illustrated that image-based Mamba [29; 30] achieves promising results and can match the performance of existing transformer [31; 32; 33] and CNN [34; 35; 36] models with smaller parameters. Therefore, we focus on exploring the multi-scale reconstruction capabilities of Mamba-UNet for quad-Bayer processing. This approach aims to create a lightweight design suitable for deployment on mobile devices. By leveraging the Mamba-UNet framework [37; 38; 39; 40], we can reduce parameters and FLOPS while still achieving state-of-the-art performance.

Figure 2: (a) Schematic diagram of the comparison between color video SCI based on the proposed quad-Bayer-based method and the previous Bayer-based method. (b) Photo taken by quad-Bayer CFA pattern (Sony IMX689) (top) and Bayer CFA pattern (bottom). One can see that the upper image is sharper with less noise.

The second challenge is _how to eliminate motion artifacts to ensure clarity in dynamic video_. Motion artifacts may arise from handheld camera instability and pixel merging in quad-Bayer patterns. To preserve scene clarity and edge details, we have customized the Residual-Mamba-Block, which integrates three key components: the Spatial-Temporal Mamba (STMamba), the Edge-Detail-Reconstruction module (EDR), and the Channel Attention module (CA) . The Residual-Mamba-Block further enhances long-range spatiotemporal dependence by leveraging residual connections and learning scales. STMamba replaces the self-attention module by performing parallel scanning of spatial and temporal dimensions, ensuring spatiotemporal consistency and enabling visual reconstruction of videos in a more lightweight manner. The EDR module enhances boundary sharpness perception and restores edge details lost due to compression and motion artifacts. By combining linear transformation with DWConv features, it integrates both local and global information. This approach strengthens the global features extracted by STMamba, fusing them with local details to more effectively capture complex edge structures. As a result, the module enables high-quality video reconstruction with improved edge clarity. The CA module compensates for the overlooked channel interactions in Mamba by weighting features based on channel importance, reducing the impact of artifact noise on reconstruction and thus improving the clarity of reconstruction results.

Building upon the foundation modules discussed earlier, we introduce a novel model called MambaSCI, which serves as the reconstruction algorithm for our proposed quad-Bayer-based SCI method as illustrated in Fig. 2(a). MambaSCI adopts a non-symmetric U-shaped encoder-decoder architecture with skip connections to enhance model efficiency. To incorporate spatial-temporal consistency at multiple scales, we introduce Residual-Mamba-Blocks at each encoding stage. Furthermore, we employ residual convolution in the decoding stage to reduce both parameters and computational complexity. The effectiveness of the MambaSCI model is demonstrated in Fig. 1(b), where it achieves superior performance compared to the SOTA methods while requiring fewer parameters and FLOPS. Pseudo-code detailing the core modules is provided in the supplementary materials.

In short, our contributions can be summarized as follows:

**(i)** We are the **first** to introduce Quad-Bayer CFA pattern into color video SCI to accommodate that most videos are captured by mobile photographers using quad-Bayer patterned smartphone cameras.

**(ii)** We are the **first** to use Mamba model for video SCI reconstruction. By integrating Mamba with a non-symmetric UNet, we employ a hierarchical encoder to capture spatial-temporal correlations at various scales, thus accelerating the model and enhancing reconstruction quality.

**(iii)** We customize the Residual-Mamba-Block, integrating STMamba, EDR, and CA modules with residual connections and learnable scales to enhance reconstruction quality and edge details.

**(iv)** MambaSCI outperforms other SOTA methods, requiring fewer parameters and FLOPS, and delivers superior visual results on 6 simulation color videos and 4 large-scale simulation color videos.

Figure 3: The proposed MambaSCI network architecture and overall process for color video reconstruction. (a) Quad-Bayer patterned color video SCI reconstruction process. It feeds quad-Bayer pattern measurement \(\) and masks \(\) into the initialization block to get \(_{in}\) and inputs it into MambaSCI network to get the reconstructed RGB color video \(_{out}\). (b) The overall network architecture of the proposed MambaSCI network. (c) Structure of Residual-Mamba-Block (RSTMamba) with STMamba, EDR, and CA modules connected via residuals. The detailed design of EDR and CA is shown in Fig. 4. (d) STMamba. It captures spatial-temporal consistency via structured SSMs that enable parallel scanning in the spatial forward-backward and temporal dimensions.

## 2 Related Work

### Mathematical Model for Color Video SCI

For color video SCI systems, the original \(T\)-frame input video \(^{H W 3 T}\) is given, along with the mask \(^{H W T}\), where \(H,W\) and \(T\) denote color video's height, width and number of frames, respectively. As in previous Bayer-based approaches, since each pixel captures only the red (R), green (G) or blue (B) channel of the raw data in a spatial layout, both \(\) and \(\) are divided into four parts to obtain four sub-measurements \(\{^{r},^{g_{1}},^{g_{2}},^{b}\} ^{}\).

\[^{r}=^{r}^{r},^{g_{1}}= ^{g_{1}}^{g_{1}},^{g_{2}}= ^{g_{2}}^{g_{2}},^{b}=^{b} ^{b}.\] (1)

Some methods [14; 15] process the four sub-measurements individually, restore them to RGB color by an off-the-shelf demosaicing algorithm and finally combine them to get the final reconstructed video, which is inefficient and cannot make good use of channel correlation. Thus others [16; 17] input all sub-measurements into their proposed reconstruction network simultaneously, and finally obtain the final RGB color video by convolutional network. Similarly, we first obtain four sub-measurements based on the spatial layout of the quad-Bayer array shown in Fig. 1(a), and then feed them into the network simultaneously.

Following [11; 43], we denote the vectorized measurement \(^{HW}\). Then given vectorized color video \(^{HWT}\) and mask \(^{HW HWT}\), the degradation model can be formulated as:

\[=+,\] (2)

where \(^{HW}\) represents noise on measurement. SCI reconstruction is to obtain \(\) from captured \(\) and the pre-set \(\) using a reconstruction algorithm [14; 15; 16; 44]. However, for quad-Bayer color videos, demosaicing algorithms bring artifacts, and Bayer pattern-based convolutional reconstruction causes color distortion, significantly affecting quality.

### State Space Models (SSMs)

SSMs are common treated as linear time-invariant systems that map a 1D sequence \(x(t)\) to \(y(t)\) through a hidden state \(h(t)^{N}\), the process can be expressed as follows:

\[ h^{}(t)&=h(t)+ x(t),\\ y(t)&=h(t),\] (3)

where \(^{N N}\), \(^{N 1}\), \(^{1 N}\). It is common to use the zero-order hold (ZOH) method to discretize the continuous system, thus Eq. (3) can be discretized as following:

\[ h_{k}&=}h_{k-1}+}x_{k},}=e^{},\\ y_{k}&=h_{k},}=(e^{}-I)^{-1},\] (4)

which uses timescale parameter \(\) to convert continuous \(\) and \(\) to discrete \(}\) and \(}\).

### SSMs for Visual Applications

The 1D S4 model  is extented to handle multidimensional data, while TranS4mer model  optimizes movie scene detection by combining S4 with self-attention. Vision Mamba and MambaIR [30; 47] introduce SSMs into the vision domain as generic backbones. U-Mamba  addresses long-range dependencies in biomedical images. Efficient medical image segmentation is achieved with lightM-UNet and UltraLight VM-UNet [40; 41]. ViVim  is proposed for effective and efficient medical video object segmentation.

These methods focus on (i) image restoration (spatial information), (ii) video understanding (global features), and (iii) medical image or video segmentation (small resolution and frame count). However, they do not apply SSMs to video SCI, a task requiring spatial-temporal consistency and detailed feature reconstruction for high-resolution, long-frame videos. Therefore, there is an urgent need to explore SSMs' performance and efficiency in long-sequence problems such as video SCI.

The Proposed Method

In this section, we introduce our proposed MambaSCI network framework, and detail our customized Residual-Mamba-Block, which is capable of capturing long-range spatial-temporal consistency along with edge detail sharpness reconstruction and channel information interaction, thus being able to outperform SOTA results with fewer parameters and FLOPS. Fig. 3 illustrates the reconstruction process, network framework and details of core modules.

### Architecture Overview

Given the 2D measurement \(^{H W}\) and mask \(^{H W T}\), through the general initialization module of SCI, the general initialization module of SCI provides an initial raw quad-Bayer reconstruction video \(_{in}^{H W 1 T}\). This serves as the input to the MambaSCI reconstruction network, which outputs the final color reconstruction video \(_{out}^{H W 3 T}\). MambaSCI comprises five main components: **(i)** shallow feature extraction block, **(ii)** encoder layer, **(iii)** bottleneck layer, **(iv)** decoder layer, **(v)** color video reconstruction block. When \(_{in}\) is feed into MambaSCI, it first goes through shallow feature extraction block, which includes a depthwise separable convolution (DWConv), producing the shallow feature \(^{H W C T}\). Next \(\) sequentially passes through three encoder layers, each composed of a Residual-Mamba-Block and Max-Pooling operation, resulting in the feature \(_{ei}^{(H/2^{i})(W/2^{i})(C 2^{i})  T}\), where \(i\{1,2,3\}\). After these encoding layers, the deep feature \(}^{ T}\) is obtained, with \(=\), \(=\) and \(=8 C\). The bottleneck layer, composed of Residual-Mamba-Blocks, keeps the feature's shape unchanged. Then, through each decoding layer, which includes residual convolutions and upsampling operations, the feature transforms into \(_{di}^{( 2^{i})( 2^{i}) (/2^{i}) T}\). Eventually, feature \(_{d3}\) is fed into the color video reconstruction block to obtain the reconstructed color video \(_{out}^{H W 3 T}\). See Fig. 3(b) for an overall view.

### Residual-Mamba-Block

Within each encoder layer, we incorporate \(N\) Residual-Mamba-Blocks, specifically designed to capture and enhance temporal-spatial coherence across multiple scales, resulting in more accurate and comprehensive feature representations. As depicted in Fig. 3(c), each Residual-Mamba-Block consists of three key components: **(i)** the STMamba module, **(ii)** the EDR module, and **(iii)** the CA module. These components are detailed below, and the overall process can be mathematically described as follows:

\[_{1}^{l} =((^{l}))+^{l} s_{ 1},\] (5) \[_{2}^{l} =Projection(((_{1}^{l}))+_{ 1}^{l} s_{2}),\] \[_{out}^{l} =((_{2}^{l}))+_{2}^{l} s _{3},\]

where LN represents LayerNorm and \(l[1,N]\), \(^{l}\) is the \(l_{th}\) block's input feature and \(_{out}^{l}\) can be treated as \(^{l+1}\) block's input, \(s_{1}\), \(s_{2}\), \(s_{3}\) represent the learnable scales in the residual connection.

**(i) STMamba module.** Previous video SCI reconstruction algorithms typically compute attention separately for the temporal and spatial dimensions, then fuse them using a residual network . However, this approach may lack temporal-spatial consistency. To address this issue, we employ the STMamba model , which integrates spatial-temporal information through structured SSMs. Specifically, As illustrated in Fig. 3(d), the input \(^{H W C T}\) is processed in two parallel branches. In the first branch, \(\) is expanded to \(=S C\) channels via a linear layer, resulting in \(^{H W T}\), where \(S\) is expansion scale. Then, \(\) is unfolded along frames \(T\) to form \(_{s}^{T(HW)}\). Forward and backward scanned features, \(sf\) and \(_{sb}\), are obtained by scanning \(_{s}\) in both directions, efficiently capturing spatial dependencies. Simultaneously, sequence \(_{t}^{(HW)T}\) is generated to explore temporal dependencies by forward scanning each pixel across the \(T\) frames.

STMamba utilizes parallel SSMs to capture intra-frame and inter-frame correlations, enforcing time-space consistency constraints. This approach enables STMamba to capture both the spatial features within frames and the temporal dependencies between frames, accurately modeling dynamic changes in video data, facilitating efficient spatial-temporal feature extraction and preservation of consistency. As shown in Fig. 3(d), the process can be formulated as:

\[&=SiLU(Linear()),\\ &=SiLU(Linear()),\\ &_{s},_{t}=(),\\ &_{sf}=LN((Conv1d(_{s}))),\\ &_{sb}=LN((Conv1d(_{s}))), \\ &_{t}=LN((Conv1d(_{t}))),\\ &_{ssm}=Linear(_{sf}+_{sb}+_{t}),\] (6)

where \(\) represents Hadamard product, LN represents LayerNorm and \(SiLU\) is an activation function.

**(ii) EDR and CA module.**

To achieve the fine reconstruction of edge details and to compensate for the missing inter-channel interaction capability in the Mamba model, we introduce the EDR and CA modules. The EDR module consists of linear layers and depthwise separable convolution (DWConv). By combining linear transformation with DWConv features, the EDR module gains the ability to perform multi-scale feature fusion, allowing the network to extract global information from local features and process both global and local information simultaneously. This enhances the model's capacity to understand complex edge structures. Additionally, through adaptive weight initialization, the model can capture finer details more effectively in the early stages of training, enabling efficient edge detail reconstruction in a lightweight manner while enhancing image edge information.

On the other hand, the CA module compresses the spatial dimension of the feature map to \(1 1 1\) through an average pooling operation, which in turn maps the compressed features to the interval \((0,1)\) through a convolutional layer with an activation function to form channel attention weights. These weights are subsequently multiplied channel-by-channel with the original feature map to implement the channel attention mechanism. In addition, the CA module utilizes the convolution operation to further facilitate the fusion and exchange of information between channels, thereby enhancing the interaction effect between channels.

### Bottleneck and Decoder Layer.

Like transformer, Mamba encounters severe optimization and convergence challenges as network depth increases . In the bottleneck layer, multiple Residual-Mamba-Blocks are concatenated, keeping the same number of feature channels and resolution. This maintains feature richness and clarity, enhances the model's spatial-temporal dependency capture, and improves MambaSCI's performance without significantly increasing computational burden.

Decoding layer decodes features and recover image resolution. It receives two inputs: \(_{e}^{ C T}\) from the skip connection, which retains original spatial information, and \(_{d}^{ T}\) from the previous decoding layer, containing higher-level spatial-temporal information. The decoding layer fuses features using element-wise addition to enhance expressiveness, applies DWConv with residual concatenation, an activation function and an upsampling operation. This process produces output features with richer semantics and higher spatial resolution.

### Color Video Reconstruction Block.

The color video reconstruction block reconstructs the desired video \(_{out}^{H W 3 T}\). Instead of the computationally intensive remosaicing and demosaicing of raw quad-Bayer images, we use a

  Method & \#Channel & Block & PSNR (dB) & SSIM & Params (M) & FLOPS (G) \\  MambaSCI-T & 8 &  & 32.13 & 0.919 & 1.61 & 165.47 \\ MambaSCI-S & 10 &  & 34.53 & 0.950 & 2.47 & 247.53 \\ MambaSCI-B & 16 &  & 35.70 & 0.959 & 6.11 & 556.89 \\  

Table 1: Reconstruction quality and computational complexity for different versions of MambaSCI.

Figure 4: Detailed design of EDR and CA module.

three-layer convolution (kernel sizes 3\(\)3\(\)3, 3\(\)3\(\)3, and 1\(\)1\(\)1) to process the decoding layer's output and obtain the final RGB color video.

### Network Variants and Computational Complexity

To balance size and performance, we propose three versions of the MambaSCI model: MambaSCI-T (_tiny_), MambaSCI-S (_small_), and MambaSCI-B (_base_). Tab. 1 shows the network hyperparameters, model parameters, and computational complexity (FLOPS). By varying the number of channels from the initial DWConv, our method achieves significantly lower complexity than EfficientSCI  and STFormer .

We also calculate the computational complexity of the attention module n MambaSCI compared to other SOTA methods, as shown in Tab. 3, where \(C\) is the number of input features, \(K\) represents the kernel size, \(G_{h}\) and \(G_{w}\) are the spatial size of local window in Swin-transformer , \(N\) is a fixed parameter in Mamba set to 16. In MambaSCI, input features are unfolded into a sequence \(^{HW C T}\). As seen in Tab. 3, while STFormer and EfficientSCI scale linearly with the spatial size (\(HW\)), their complexity grows quadratically with video frames \(T\) and \(C\), which is typically 64 or larger, resulting in high computational costs. MambaSCI scales linearly with the entire video sequence \((HWT)\) and \(C\), which is capped at 64 in MambaSCI-B, enabling efficient reconstruction of longer video sequences. Inference time comparisons are shown under various methods in Tab. 5.

## 4 Experiment

In this section, we evaluate MambaSCI against SOTA video reconstruction methods on multiple simulation datasets using PSNR, SSIM metrics, and visual comparisons.

### Experimental Setup

Following STFormer and EfficientSCI, we use DAVIS2017  with resolution 480\(\)894 (480p) as the model training dataset. To verify model performance, we test our MambaSCI on several

   Method & Params (M) & FLOPS (G) & Beauty & Bosphours & Runner & ShakeNDry & Traffic & Jockey & Avg \\  GAP-TV  & - & - & 33.38 & 29.53 & 29.61 & 29.70 & 19.64 & 29.32 & 28.53 \\  & - & - & 0.965 & 0.904 & 0.872 & 0.884 & 0.625 & 0.885 & 0.856 \\  PnP-FFDnet-gray  & - & - & 32.47 & 27.45 & 28.66 & 26.93 & 20.56 & 31.07 & 27.86 \\  & - & - & 0.958 & 0.883 & 0.864 & 0.832 & 0.686 & 0.96 & 0.855 \\  PnP-FastDVD-gray  & - & - & 34.29 & 33.07 & 34.18 & 30.11 & 23.74 & 32.70 & 31.35 \\  & - & - & 0.967 & 0.947 & 0.928 & 0.883 & 0.811 & 0.921 & 0.909 \\  EfficientSCI-S  & 2.21 & 1434.18 & 19.47 & 26.88 & 34.26 & 24.13 & 25.98 & 30.41 & 26.86 \\  & - & 0.402 & 0.642 & 0.906 & 0.639 & 0.761 & 0.788 & 0.690 \\  EfficientSCI-B  & 8.83 & 5701.50 & 36.40 & 24.52 & 36.34 & 34.73 & 26.63 & 35.52 & 32.35 \\  & - & 0.980 & 0.497 & 0.919 & 0.955 & 0.774 & 0.945 & 0.845 \\  STFormer-S  & **1.23** & 769.23 & 23.15 & 23.75 & 34.36 & 24.78 & 26.17 & 30.13 & 27.06 \\  & - & 0.679 & 0.435 & 0.885 & 0.659 & 0.771 & 0.785 & 0.703 \\  STFormer-B  & 19.49 & 12155.47 & 36.69 & 23.84 & 37.13 & **34.83** & 26.62 & 35.80 & 32.48 \\  & - & **0.981** & 0.446 & 0.927 & **0.955** & 0.791 & 0.952 & 0.842 \\ 
**MambaSCI-T** & 1.61 & **165.47** & 33.45 & 35.07 & 35.03 & 31.81 & 25.31 & 32.09 & 32.13 \\  & - & 0.965 & 0.963 & 0.926 & 0.912 & 0.843 & 0.909 & 0.919 \\ 
**MambaSCI-S** & 2.47 & 247.53 & 36.12 & 37.33 & 38.35 & 33.72 & 26.70 & 34.98 & 34.53 \\  & - & 0.978 & 0.976 & 0.968 & 0.943 & 0.886 & 0.951 & 0.950 \\ 
**MambaSCI-B** & 6.11 & 556.89 & **36.95** & **38.62** & **40.02** & 34.55 & **27.52** & **36.54** & **35.70** \\  & - & 0.979 & **0.982** & **0.977** & 0.950 & **0.904** & **0.960** & **0.959** \\   

Table 2: Comparisons between MambaSCI and SOTA methods on 6 simulation videos. PSNR (upper entry in each cell), and SSIM (lower entry in each cell) are reported. The best and second-best results are highlighted in bold and underlined, respectively.

   Method & Computational Complexity \\  STFormer & \(6HWTC^{2}+2G_{h}G_{w}HWTC+HWT^{2}C\) \\ EfficientSCI & \(HWTK^{2}C^{2}+HWTC^{2}+HWT^{2}C\) \\
**MambaSCI** & \(8HWTCN+2HWTC^{2}\) \\   

Table 3: Computational complexity of several SOTAs.

simulated datasets, six benchmark mid-scale color datasets  (Beauty, Bosphorus, Jockey, Runner, ShakeNDry and Traffic of size 512\(\)512\(\)3\(\)32), and four benchmark large-scale color datasets  (Messi, Hummingbird, Swinger and Football). Since there is currently no real color video SCI dataset based on quad-Bayer pattern, our method is not tested on real datasets.

### Implementation Details

We use PyTorch framework training on 4 NVIDIA RTX4090 GPUs and use random flipping, scaling, and cropping on DAVIS2017 for data augmentation. We use randomly generated masks as training input to enhance model robustness and optimize the model using the Adam  optimizer. Since MambaSCI is flexible in input size, we first train for 100 epochs at a learning rate of 0.0005 on data with a spatial size of 128\(\)128. Then, we train for 50 epochs at learning rate of 0.0001, followed by fine-tuning on 256\(\)256 data at learning rate of 0.00001 for an additional 50 epochs.

### Results on Middle-scale Simulation Color Video

To test the performance of our method for color video reconstruction, we perform experiments on a 32-frame simulation color RGB video dataset with size of 512 \(\) 512 \(\) 3 \(\) 32. We compress the color video with compression rate of \(B\) = 8 and capture quad-Bayer pattern measurements using a camera with quad-Bayer CFA pattern.

Since all current color video SCI reconstruction algorithms are designed based on Bayer pattern, which cannot be directly applied to quad-Bayer pattern. Meanwhile, re-training an E2E model requires much training time and memory. Thus we only re-train two of the latest SOTA E2E models (STFormer , EfficientSCI ) with quad-Bayer pattern. We compare with iterative optimization algorithm (GAP-TV ), PnP algorithms (PnP-FFDnet  and PnP-FastDVD ) and E2E algorithms (STFormer  and EfficientSCI ). The number of parameters, FLOPS, and reconstruction results are shown in Tab. 2.

Notably, there is no readily available quad-Bayer demosaicing package. Therefore, for model-based and PnP algorithms, we first upsample the reconstructed raw quad-Bayer video

Figure 5: Visual reconstruction results of different algorithms on middle-scale simulation color video dataset (Bosphorus #10, Runner #11, Traffic #32 and Jockey #24 in order from top to bottom). PSNR/SSIM is shown in the upper left corner of each picture.

according to the quad-Bayer CFA pattern to obtain a three-channel video \(}^{H W 3 T}\). We then demosaic it using the BJDD  algorithm to get the final RGB color video. See supplementary materials for details. Visual comparisons of the reconstruction results are shown in Fig. 5.

We summarize our observations: **(i)** Our MambaSCI model significantly outperforms SOTA methods with lower computational and memory resources. For instance, MambaSCI-B surpasses STFormer-B by 3.22 dB, using only **31%** (6.11 / 19.49) of the Params and **4.5%** (556.89 / 12155.47) of the FLOPS. It also outperforms EfficientSCI-B by 3.35 dB with just **69%** Params and **9.8%** FLOPS. Additionally, both MambaSCI-S and MambaSCI-T achieve better results than STFormer-S and EfficientSCI-S with fewer Params and FLOPS. Fig. 1(b) shows MambaSCI's superior PSNR-FLOPS performance with fewer resources. **(ii)** In visual comparisons, GAP-TV and PnP-based methods exhibit artifacts, while STFormer and EfficientSCI suffer from color distortions likely because their reconstruction modules being designed for Bayer pattern and not compatible with Quad-Bayer pattern. None of these methods achieve high-quality reconstruction. MambaSCI, however, eliminates artifacts and achieves high-quality reconstruction with accurate color fidelity.

In addition, our proposed model demonstrates SOTA reconstruction performance even at higher frame rates and compression ratios. We tested it on _beauty_ data across various compression ratios (B = 8, 16, 32), with results detailed in the Tab. 4. Notably, our method requires only 9.8% of the FLOPS needed by EfficientSCI, while also significantly outperforming it in the SSIM.

### Results on Large-scale Simulation Color Video

Similar to previous studies, we conduct experiments on a large-scale color video dataset. Due to the significant time and memory required to retrain existing E2E models for Bayer patterns, we only compare with SOTA model-based methods (GAP-TV, PnP-FFDnet, PnP-FastDVDnet) and retrain STFormer-S and EfficientSCI-S for the quad-Bayer pattern. Tab. 5 shows the comparisons on PSNR and SSIM, and Fig. 6 provides visual comparisons.

We summarize the observations: **(i)** MambaSCI-S outperforms other methods in PSNR and SSIM on football and swinger, achieving over 1.5 dB higher PSNR on football. **(ii)** In visual comparisons, GAP-TV and PnP algorithms exhibit artifacts, while STFormer and EfficientSCI suffer from color distortions. MambaSCI excels in reconstructing detailed information, resulting in superior reconstruction quality. **(iii)** The poorer performance on Messi and Hummingbird may be due to faster, more detailed motions that strained by limited parameters and FLOPS. However, the visual results remain superior to other SOTA methods.

   B & Methods & Params (M) & FLOPS (G) & PSNR (dB) & SSIM \\   & PnP-FFDnet & - & - & 24.85 & 0.767 \\  & STFormer & 19.49 & 2431.16 & 25.21 & 0.655 \\  & EfficientSCI & 8.83 & 11406.23 & 25.35 & 0.656 \\  & MambaNSCI & **6.11** & **1113.78** & **25.39** & **0.817** \\   & PnP-FFDnet & - & - & 1.82 & 0.496 \\  & STFormer & 19.49 & OOM & - \\  & EfficientSCI & 8.83 & 22825.34 & **23.24** & 0.653 \\  & MambaSCI & **6.11** & **2272.57** & 22.44 & **0.785** \\  

Table 4: Performance analysis at \(B\)=16 and 32 cases.

Figure 6: Visual reconstruction results of different algorithms on large-scale simulated color video dataset (Footbal #11, Swinger #1, and Hummingbird #40 from top to bottom). PSNR/SSIM is under each image.

### Ablation Study

We conduct ablation experiments to evaluate the effectiveness of each module in MambaSCI. Tab. 6 presents the results, comparing reconstruction quality, Params, and FLOPS across different models. All experiments are performed on six color benchmark datasets.

STMamaba Block:We verify the impact of STMamba blocks on reconstruction quality. As indicated in Tab. 6, replacing vanilla Mamba with STMamba boosts PSNR by 6dB, while Params and FLOPS remain unchanged. STMamba's linear scanning enables satisfying spatial-temporal consistency without a notable increase in computational complexity.

**CAD Block:** Tab. 6 demonstrates that the EDR module can improve PSNR by approximately 4.3dB, dramatically improving the quality of the reconstruction. However, it will result in the increase of Params and FLOPS.

**CA Block:** CA block compensates for the lack of channel information interaction in Mamba model, achieves the modelling of channel importance and improves the reconstruction quality through channel attention mechanism. Tab. 6 shows CA block can significantly improve reconstruction quality. However, the CA module's multiple \(Conv3d\) operations result in a notable increase in parameters and FLOPS, which is an aspect to optimize in future work.

**Number of Channels:** Tab. 1 illustrates that the only distinction among different MambaSCI versions is the varying number of channels. Through experimentation, we observed that Params and FLOPS are significantly influenced by the number of channels, rather than the number of Residual-Mamba-Blocks. Moreover, an excess of Residual-Mamba-Blocks prolongs both training and inference times, highlighting the need for a trade-off in the current setup.

**Residual Connection and Learnable Scales:** The Residual-STMamba-Block is the core customised module of our MambaSCI. Tab. 7 is experimentally demonstrated that the residual connections and the learnable scales are effective in improving the reconstruction quality enhancement.

## 5 Conclusion

In this paper, we introduced quad-Bayer pattern into video SCI for the first time, enabling SCI to align with the fact that most current videos are captured by mobile phones with quad-Bayer cameras, thus avoiding artifacts and color distortions caused by existing algorithms. Specifically, we integrate Mamba model with an asymmetric UNet in video SCI, leveraging Mamba's linear complexity and the speed improvements from the non-symmetric architecture for efficient SCI reconstruction. Moreover, we customized Residual-Mamba-Blocks to connect STMamba, EDR, and CA modules through residual connections, ensuring efficient spatial-temporal consistency and detailed reconstruction. Experimental results on simulated color video datasets highlighted that MambaSCI outperformed SOTA methods with fewer parameters, lower computational complexity and better visual effects.

   Baseline & STMamba & EDR & CA & PSNR & SSIM & Params(M) & FLOPS(G) \\  ✓ & & & & 24.18 & 0.811 & 0.28 & 35.36 \\ ✓ & ✓ & & & 30.31 & 0.897 & 0.28 & 35.36 \\ ✓ & ✓ & ✓ & & 34.66 & 0.953 & 2.53 & 235.47 \\ ✓ & ✓ & ✓ & ✓ & **35.70** & **0.959** & 6.11 & 556.89 \\   

Table 6: Ablation study on each major module.

   Model & PSNR (dB) & SSIM & Params (M) & FLOPS (G) \\  w/o learnable scale & 35.33 & 0.955 & 6.11 & 556.89 \\ w/o residual connections & 34.71 & 0.953 & 6.11 & 556.89 \\ 
**Residual-STMamba-Block** & **35.70** & **0.959** & 6.11 & 556.89 \\   

Table 7: Ablation study on Residual-Mamba-Block

   Dataset & Pixel resolution & GAP-TV & Pn-FPFNFN & Pn-FNFNDD & STFormS-S & EfficientSciC1S & **MambaSCLS** \\  & (17.03) & (17.97) & (50.03) & (**2.12**) & (5.47) & (4.98) \\   &  & 25.00 & 28.62 & **29.17** & 17.77 & 18.45 & 26.36 \\  & & 0.868 & 0.939 & **0.939** & 0.639 & 0.685 & 0.874 \\   & 29.33 & 29.72 & **32.11** & 31.96 & 30.15 & 30.73 \\  & & 0.856 & **0.924** & 0.867 & 0.886 & 0.811 & 0.815 \\   &  & 24.92 & 26.72 & 28.60 & 20.10 & 20.90 & **29.78** \\  & & 0.833 & 0.883 & 0.887 & 0.556 & 0.589 & **0.920** \\   &  & 31.19 & 33.82 & 34.76 & 30.61 & 27.10 & **36.31** \\  & & 0.939 & 0.963 & 0.959 & 0.815 & 0.754 & **0.976** \\   

Table 5: Comparisons between MambaSCI and SOTA methods on 4 large-scale simulation videos. PSNR (upper), and SSIM (lower) are reported. The total time (minutes) taken to reconstruct 4 videos is under each method. The best and second-best results are highlighted in bold and underlined.