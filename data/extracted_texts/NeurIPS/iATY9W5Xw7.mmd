# CAST: Cross-Attention in Space and Time

for Video Action Recognition

 Dongho Lee1

Jongseo Lee1

Jinwoo Choi2

Kyung Hee University, Republic of Korea

{kide004, jong980812, jinwoochi}@khu.ac.kr

###### Abstract

Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics. The code is available at https://github.com/KHU-VLL/CAST.

## 1 Introduction

To accurately recognize human actions in videos, a model must understand both the spatial and temporal contexts. A model that lacks fine-grained spatial understanding is likely to fail in predicting the correct action. For example, as shown in Figure 1 (a), a model that understands temporal context such as hand motion across frames but not the fine-grained spatial context may confuse whether an object in the hand a _ketchup_, or a _cheese_, or a _milk carton_. Consequently, the model fails to predict the correct action, _Put down a cheese_. Similarly, a model that lacks temporal context understanding may also fail to predict the correct action. In Figure 1 (b), let us suppose a model understands spatial context but does not understand temporal context, e.g., the model is confused about whether the hand is moving from outside the fridge to the inside or vice versa. Then the model fails to predict the correct action of _Take out a sauce_. Therefore, for accurate action recognition, models need to comprehend both the spatial and temporal contexts of videos.

Despite the recent progress in action recognition through the use of Transformers , achieving a balanced spatio-temporal understanding remains a challenging problem. Compared to images, the additional temporal dimension in videos makes spatio-temporal representation learning computationally intensive and requires a significant amount of training data . Consequently, most action recognition models lack a balanced spatio-temporal understanding of videos. Notably, models that perform well on static-biased  datasets, such as Kinetics-400, may not perform as well on temporal-biased  datasets, such as Something-Something-V2, and vice versa. For instance, as shown in Figure 4 (a), on the EPIC-KITCHENS-100 dataset, VideoMAE  outperforms ST-Adapter  on the verb prediction task, while ST-Adapter outperforms VideoMAE on the noun prediction task. Similarly, BEVT  outperforms AIM  on the Something-Something-V2dataset, while BEVT underperforms AIM on the Kinetics-400 dataset. We observe a similar trend for other methods as reported in Table 2.

One possible solution to the challenge of balanced spatio-temporal understanding is to use multimodal learning. For example, two-stream networks [51; 14] employ both RGB and optical flow streams to learn both spatial and temporal contexts. However, this approach can be computationally expensive due to optical flow estimation.

In this work, we introduce a two-stream architecture, Cross-Attention in Space and Time (CAST), to address the challenge of balanced spatio-temporal understanding using only RGB input. In Figure 2, we show a high-level illustration of the proposed method. Our architecture employs two expert models - a spatial expert model and a temporal expert model - which exchange information to make a synergistic collective prediction. We realize the information exchange by cross-attention between the two experts. We empirically validate that placing cross-attention in a bottleneck architecture facilitates more effective learning. To validate the effectiveness of the proposed method, we conduct extensive experiments on multiple datasets with distinct characteristics, including the temporal-biased Something-Something-V2, static-biased Kinetics-400, and fine-grained EPIC-KITCHENS-100. Our results demonstrate that CAST achieves balanced spatio-temporal understanding and shows favorable performance across these different datasets.

In this work, we make the following significant contributions.

* We introduce a two-stream architecture, CAST, which addresses the challenge of _balanced spatio-temporal understanding_ that has been largely overlooked by previous works.
* We conduct extensive experiments on multiple datasets with distinct characteristics to demonstrate the effectiveness of CAST. In terms of _balanced_ spatio-temporal understanding, CAST shows favorable performance, while existing methods show more imbalanced performance.
* We conduct an extensive ablation study and analysis to validate the design choices of the proposed method. We show that employing _spatial expert_ and _temporal expert_ and placing _cross-attention in a bottleneck_ architecture is crucial for achieving effective spatio-temporal representation learning.

## 2 Related Work

Video Action Recognition.CNN-based approaches have been widely used for action recognition, including 2D CNNs [61; 74; 33; 52; 27], 3D CNNs [58; 5; 59; 64; 13], 2D and 1D separable CNNs [59; 70], or two-stream CNNs [14; 15]. These methods have achieved great progress thanks to the strong inductive biases. Recently, Transformer-based approaches [1; 3; 21; 43; 68; 12; 71] become popular in the community due to the long-term context modeling capabilities. Similar to the two-stream CNNs, we propose a two-stream transformer architecture consisting of two expert models: a spatial expert and a temporal expert. However, unlike traditional two-stream CNNs, we use RGB input only, instead of RGB and flow.

Figure 1: **The importance of spatio-temporal understanding. If a model lacks fine-grained spatial understanding, the model may predict an incorrect action. E.g., the model fails to predict _Put down a cheese_ in (a) due to subtle appearance differences between the objects. On the other hand, if a model lacks temporal context understanding, the model may predict an incorrect action. E.g., the model fails to predict _Take out a sauce_ in (b) due to the ambiguity of the action. Therefore, both spatial and temporal understanding are crucial in action recognition. Best viewed with zoom and color.**

Cross-attention.Cross-attention has been widely utilized in multi-modal learning to facilitate information exchange between different modalities such as audio, visual, and text [34; 67; 40; 30; 18]. Recently, cross-attention between different views of the same video has shown impressive results [71; 75; 6; 26]. Similar to these, we propose a cross-attention method using a single RGB input, but with two distinct expert models: a spatial expert and a temporal expert. The two experts attend to each other through cross-attention to achieve a balanced spatio-temporal understanding.

Foundation model.Trained on web-scale datasets using self-supervised learning, foundation models [25; 4; 48; 45; 44] are highly adaptable and versatile. Foundation models show impressive performance on various tasks in computer vision [65; 62], natural language processing [49; 57], and audio recognition . In this work, we employ CLIP  as our spatial expert as it shows impressive performance on more than 30 computer vision tasks.

Parameter-efficient transfer learning.Although the "pre-training and fine-tuning" paradigm with strong foundation models has demonstrated impressive performance on several computer vision tasks, it is computationally expensive and often unnecessary to fine-tune the full model . Several works have demonstrated that learning only a small subset of parameters and keeping the remaining parameters frozen is effective for NLP tasks [23; 29] and computer vision tasks [36; 66; 54; 46; 47]. Extending image foundation models by adding adapter architectures has shown favorable performance on action recognition [35; 72; 42]. The proposed method also employs adapter architecture with cross-attention between two experts. We empirically demonstrate that the proposed method outperforms existing adapter-based video models in terms of achieving balanced spatio-temporal understanding.

## 3 Method: Cross-Attention in Space and Time

We introduce CAST, a method for balanced spatio-temporal representation learning for action recognition, as shown in Figure 3. We employ frozen spatial and temporal expert models that can be any vision transformer, consisting of 12 transformer blocks each. To facilitate information exchange between the experts, we introduce the bottleneck cross-attention in space and time (B-CAST) module on top of the frozen layers. This module enables the experts to exchange information and learn more balanced spatio-temporal contexts than separate experts. To improve adaptation to downstream tasks, we use adapter layers with a small number of learnable parameters, following AIM . In the following subsections, we provide a detailed description of each component of our proposed CAST.

### Input embeddings

CAST takes only RGB videos as inputs. The input is a mini-batch of videos, \(^{B 2T H W C}\), consisting of \(B\) videos of \(2T\) frames, \(H W\) spatial dimensions, and \(C\) channels. We apply patch tokenization to the input videos for the spatial expert and the temporal expert. For the spatial expert, we decompose every even frame of each video in \(\) into \(N\) non-overlapping patches of \(p p\) pixels .

Figure 2: **High-level illustration of the proposed method. In this work, we employ spatial and temporal expert models. The two experts exchange information with each other using cross-attention. Initially, the experts may predict incorrect actions due to the lack of information. For example, the temporal expert may predict _reach out to something_ while the ground truth is _Pick up a fork_. Similarly, the spatial expert may predict _utensil holder_ instead of _fork_ in the shallower layers. However, after using cross-attention to exchange information multiple times, the proposed method can collectively predict the correct action _Pick up a fork_. Best viewed with zoom and color.**

Then we pass the patches through a frozen linear layer and add position embeddings to obtain spatial embeddings, \(_{s}^{BT N D}\). For the temporal expert, we decompose every two frames of each video in **I** into \(2 p p\) pixels non-overlapping tubes . Then we pass the tubes through a frozen linear layer and add position embeddings to obtain temporal embeddings, \(_{t}^{B TN D}\).

### CAST architecture

The model architecture of each expert is the same as the ViT except for adapters and the B-CAST module. All the other parameters are frozen, while the adapter and B-CAST parameters are learnable.

For completeness, we first define the operations used and then describe the entire model architecture. Given an input **X**, we define Multi-Head Self Attention (MHSA) operation as follows:

\[()=((_{Q})( _{K})^{})(_{V}),\] (1)

\(_{Q}\), \(_{K}\), and \(_{V}\) are the query, key, and value projection matrices, respectively. We also define the adapter operation with linear down and up projection matrices \(_{D}\) and \(_{U}\) as follows:

\[()=(_{D})_{U},\] (2)

where \(()\) is the GELU activation function .

For each attention block \(l\), we apply independent Multi-Head Self Attention (MHSA) for each expert along with a skip connection as follows:

\[^{(l)}=^{(l)}+((( ^{(l)})))+((^{(l)})),\] (3)

where \(()\) denotes the Layer Normalization operation. The spatial path undergoes spatial attention, while the temporal path undergoes space-time attention following TimeSformer .

Figure 3: **Overview of CAST.** (a) CAST employs frozen spatial and temporal expert models. On top of the experts, we add a cross-attention module B-CAST to enable the exchange of information between the two experts. Additionally, we employ adapters with a small number of learnable parameters to the experts for better adaptation. (b) The proposed B-CAST consists of temporal-to-spatial (T2S) and spatial-to-temporal (S2T) cross-attentions to allow for a better understanding of the spatio-temporal features in the video data. For efficient and effective learning, we incorporate cross-attention into the bottleneck adapter. We employ separate position embedding for each expert. (c) We visualize T2S and S2T cross-attentions. Given a query, the model attends along the temporal axis only in T2S while the model attends along the spatial axes only in S2T.

As shown in Figure 3 (b), to exchange information between the two experts, we apply the B-CAST operation \(()\) to \(_{e_{1}}\) and \(_{e_{2}}\) from the expert \(e_{1}\) and \(e_{2}\) as follows along with a skip connection:

\[^{(l)}=^{(l)}+(_{e_{1}}^{(l)},_{e_{ 2}}^{(l)}).\] (4)

We describe the B-CAST operation \(()\) in detail in Section 3.3.

Finally, we pass the output, denoted as \(^{(l)}\), through a two-layer feed forward network (FFN)  with the GELU activation function in between the layers and another adapter to obtain the next layer input \(^{(l+1)}\) as follows along with a skip connection:

\[^{(l+1)}=^{(l)}+((^{(l)}))+ ((^{(l)})).\] (5)

Classification head.To produce the final prediction, we need to aggregate the outputs of both spatial and temporal experts. For the spatial expert, we average the frame-level class tokens from the last attention block, \(_{s}^{(12)}\), to obtain a single class token. We denote this operation as \(()\). To obtain temporal expert features, we aggregate all the tokens from the last attention block of the temporal expert, \(_{t}^{(12)}\), using the global average pooling \(()\) operation. Then we add the adapter output of the CLS token and the adapter output of the GAP token to produce a fused token \(\):

\[=((_{s}^{(12)}))+((_{t}^{(12)})).\] (6)

Finally, we feed the fused token \(\) a classification layer followed by a softmax function to obtain the predicted class probabilities. We train the model using the standard cross-entropy loss.

### B-CAST module architecture

Multi-Head Cross-Attention.Multi-Head Cross-Attention (MHCA) is a variant of the MHSA operation (1), where query tokens come from one expert (\(e_{1}\)) and key and value tokens come from another expert (\(e_{2}\)). This allows the experts to exchange information and benefit from the strengths of each other. We define the MHCA operation as follows:

\[(_{e_{1}},_{e_{2}})=((_{e_{1}}_{Q})(_{e_{2}}_{K})^{})(_{e_{2}}_{V}),\] (7)

where \(_{Q}\), \(_{K}\), and \(_{V}\) are learnable query, key, and value parameter matrices respectively.

Temporal-to-Spatial Cross-Attention.In Temporal-to-Spatial (T2S) cross-attention, query tokens come from the spatial expert \(s\), and key and value tokens come from the temporal expert \(t\): \((_{s}^{(l)},_{t}^{(l)})\). We depict the attention window in Figure 3 (c). Given a query, the model attends along the temporal dimension only. By using T2S cross-attention, the spatial expert can learn to attend to temporal features from the temporal expert. T2S MHCA leads to capturing spatio-temporal dependencies and improves the model performance in action recognition.

Spatial-to-Temporal Cross-Attention.In Spatial-to-Temporal (S2T) cross-attention, query tokens come from the temporal expert \(t\), and key and value tokens come from the spatial expert \(s\): \((_{t}^{(l)},_{s}^{(l)})\). We illustrate the attention window in Figure 3 (c). Given a query, the model attends along the spatial dimension only. By using S2T cross-attention, the temporal expert can attend to fine-grained spatial features from the spatial expert. S2T MHCA leads to a more balanced spatio-temporal understanding and improves the performance in fine-grained action recognition.

## 4 Experimental Results

In this section, we present the experimental results that answer the following research questions: (1) Do existing methods show a balanced spatio-temporal understanding of videos? (Section 4.3) (2) What are the ingredients for a balanced spatio-temporal understanding? (Section 4.3) (3) Is the proposed method effective? (Section 4.3, Section 4.4) (4) How can we effectively combine spatial and temporal models to achieve such balance? (Section 4.5) (5) Does the proposed method outperform state-of-the-art methods in terms of balanced spatio-temporal understanding? (Section 4.6) To this end, we first provide details about the datasets and implementation in Section 4.1 and Section 4.2, respectively.

### Datasets

Action recognition.We evaluate the CAST on two public datasets for conventional action recognition: Something-Something-V2 (SSV2)  and Kinetics-400 (K400) . The SSV2 requires more temporal reasoning [3; 28] while the K400 is relatively static biased [32; 8; 50].

Fine-grained action recognition.We evaluate the CAST on the fine-grained action recognition task: EPIC-KITCHENS-100 (EK100) . In contrast to conventional action recognition, EK100 defines an action as a combination of a verb and a noun. Therefore, we refer to the action recognition in EK100 as _fine-grained action recognition_. Since fine-grained action recognition requires correctly predicting both the verb and the noun to recognize an action it is more challenging than conventional action recognition, which requires predicting a single action label: e.g., K400 or SSV2.

### Implementation details

In this section, we briefly provide our experimental setup and implementation details. Please refer to the Appendix SS B for complete implementation details. We conduct all the experiments with 16 NVIDIA GeForce RTX 3090 GPUs. We implement CAST using PyTorch and build upon the existing codebase of VideoMAE .

Training.We sample 16 frames from each video to construct an input clip. For the K400 dataset, we apply dense sampling , while for SSV2 and EK100, we use uniform sampling . We then perform random cropping and resizing every frame into \(224 224\) pixels. We use the AdamW  optimizer with momentum betas of (0.9, 0.999)  and a weight decay of 0.05. By default, we train the model for 50 epochs, with the cosine annealing learning rate scheduling  and a warm-up period of 5 epochs. The default base learning rate, layer decay , and drop path are set to 0.001, 0.8, and 0.2, respectively. We freeze all the parameters of each expert, except for the B-CAST layer, adapters, and the last layer normalization. We set the batch size per GPU as 6 with update frequency of 2.

Inference.Given an input video, we randomly sample frames multiple times to construct input clips with multiple temporal views with multiple spatial crops. After the temporal frame sampling, we resize every frame so that the shorter side has 224 pixels. Then we perform spatial cropping to get multiple \(224 224\) crops for each clip. We get the final prediction by averaging the predictions on (temporal views) \(\) (spatial crops). For the K400 dataset, we use (5 clips) \(\) (3 crops) views, while for the other datasets, we use (2 clips) \(\) (3 crops) views for the inference.

Figure 4: **Balanced spatio-temporal understanding performance. We visualize the action recognition accuracies of existing methods and the proposed method. (a) We show the Top-1 accuracies of ST-Adapter and VideoMAE on the EK100 verb and noun prediction tasks. (b) We show the Top-1 accuracies of AIM and BEVT on the SSV2, and K400. (c) For each method, we show the harmonic mean of Top-1 accuracies on the EK100 noun, EK100 verb, SSV2, and K400. CAST shows a more balanced spatio-temporal understanding capability compared to the existing methods. Best viewed with zoom and color.**

### Balanced spatio-temporal understanding

In Figure 4 (a), we present the top-1 accuracies of several existing models. In the EK100 verb prediction task, VideoMAE outperforms ST-Adapter with a margin of 2.9 points (70.5% vs. 67.6%), while in the EK100 noun prediction task, ST-Adapter  outperforms VideoMAE  with a margin of 3.6 points (55.0% vs. 51.4%). As shown in Figure 4 (b), BEVT  outperforms AIM  with a margin of 2.5 points (70.6% vs. 68.1%) on the SSV2 dataset, while on the K400 dataset, AIM outperforms BEVT with a margin of 3.9 points (84.5% vs. 80.6%). We observe similar trends for other methods as well. Please refer to Section 4.6 for a detailed comparison. Our findings indicate that the performance of many existing models is significantly imbalanced toward either spatial or temporal understanding.

Ingredients for balanced spatio-temporal understanding.To achieve a more balanced spatio-temporal understanding, we can employ two expert models: a spatial expert and a temporal expert. For the spatial expert, we use CLIP , which has demonstrated impressive performance on various computer vision tasks. For the temporal expert, we use VideoMAE , which has shown favorable performance on temporal-biased tasks such as SSV2 and EK100 verb prediction tasks. (Please refer to Section 4.6 for the accuracy details.) While each expert is highly specialized in its own domain, we aim to create synergy between them by exchanging information to improve the balanced spatio-temporal understanding performance.

Effect of CAST.In Figure 4 (c), we gauge the balanced spatio-temporal understanding performance of our spatial expert, temporal expert, and CAST. For each method, we calculate the harmonic mean of top-1 accuracies for EK100 noun, EK100 verb, SSV2, and K400. The harmonic mean is an effective metric for gauging balanced performance because it gives more weight to lower-performing tasks. A higher harmonic mean value indicates that the performance over the different tasks is more balanced. Our spatial expert achieves an accuracy of 56.5%, while the temporal expert achieves an accuracy of 66.6%, and our CAST achieves an accuracy of 71.6%. These results validate the effectiveness of our proposed method, CAST, which allows our spatial and temporal experts to make synergistic predictions by exchanging information with each other through cross-attention.

### Analysis on fine-grained action recognition

In this section, we provide a detailed analysis of how the proposed CAST improves the balanced spatio-temporal understanding in the fine-grained action recognition task: EK100.

Category-level performance analysis.In Figure 5, We present the EK100 noun super-category-wise weighted average F1 score improvement of CAST over our spatial expert (CLIP) and temporal expert (VideoMAE). In Figure 5 left, we observe that CAST significantly improves upon the spatial expert, CLIP, in several super-categories such as _cutlery_, _utensils_, and _vegetables_. These results indicate that the spatial expert achieves a more accurate understanding of fine-grained small objects interacting with the actors by leveraging the temporal context from the temporal expert. Similarly, in Figure 5 right, we observe that CAST significantly improves upon the temporal expert, VideoMAE, in several categories such as _vegetables_ and _cutlery_. The trend is similar to the comparison with the spatial expert: CAST achieves more accurate understanding of fine-grained small objects by leveraging the fine-grained spatial context from CLIP.

Qualitative analysis.To better understand the effectiveness of CAST, we provide qualitative analysis on a few sample frames from the EK100 dataset in Figure 6. We show the predictions of CLIP, VideoMAE, and CAST. As expected, each expert model provides more accurate prediction in their respective tasks of expertise but shows weaker performance in the other task. In contrast,

Figure 5: **Improvements of CAST over each expert on EK100 noun classes. We show the super-category-wise weighted average F1 score improvement of CAST over each expert. (Left) Improvement over CLIP. CAST outperforms CLIP for every super-category except _meat and substitute_. (Right) Improvement over VideoMAE. CAST outperforms VideoMAE for every super-category except _furniture_ and _prepared food_. Best viewed with zoom and color.**CAST consistently shows correct predictions for both noun and verb prediction tasks, such as _spoon_ and _open_. The qualitative examples demonstrate the effectiveness of CAST in achieving balanced spatio-temporal understanding, which is essential for fine-grained action recognition.

### Ablation study on CAST architecture

We conduct comprehensive ablation studies to examine the design choices for the proposed CAST architecture. Here we conduct all experiments on the EK100  dataset with 16-frame input videos and report the top-1 accuracy on the validation set. We employ CLIP  as a spatial expert model and VideoMAE  as a temporal expert model. For a fair ablation study, we use the same hyperparameters for each experiment unless explicitly mentioned.

Effect of information exchange.We investigate whether CAST effectively achieves a synergistic effect by exchanging information between the two expert models. In Table 1 (a), we compare CAST with three baselines. i) A baseline using two independent expert models without any information exchange (fully fine-tuned). ii) The same baseline as i), but we add adapters and fine-tune the adapters and head only, iii) A test-time ensemble of two independent experts (with adapters and heads fine-tuning only). The baselines predict nouns using the spatial model and verbs using the temporal model. We observe that the two expert models using ensembling achieve an improvement in Action accuracy by at least 1.2 points compared to the baselines without any information exchange. Furthermore, CAST achieves a best Action accuracy of 48.7%. These results suggest that information exchange is crucial for achieving balanced spatio-temporal understanding.

Comparison with simple information exchange baselines.We compare CAST with simple information exchange baselines: i) late fusion with addition, ii) late fusion with concatenation, iii) layer-wise fusion using the bidirectional lateral connection (element-wise addition) with linear projection. We add adapters and fine-tune the adapters and head only in all three baselines, using our training recipe in Section 4.2 For the details of baseline fusion methods, please see Figure 7. We show the results in Table 1 (b). It is worth noting that both the late fusion and layer-wise lateral connection baselines result in a significant performance drop. Furthermore, we observe that layer-wise fusion without cross-attention yields inferior performance compared to the simple late fusion baselines. The results indicate that cross-attention in the bottleneck architecture is crucial for effective information exchange between spatial and temporal experts.

Design of B-CAST module.To explore the most effective and efficient way to integrate adapters for information exchange between the two expert models, we conduct an ablation study and present the results in Table 1 (c). For the details of the baselines, please see Figure 8. The first row of the table represents a baseline without the B-CAST module, which is equivalent to the identity function. Compared to this baseline, B-CAST achieves a significant improvement of 7.0 points in Action accuracy. The second row shows the performance of a baseline with cross-attention but without the

Table 1: **Ablation study. To validate the effect of each component, we show experimental results on the EPIC-Kitchens-100 dataset. In every experiment, we use the ViT-B/16 backbone for every expert. The best numbers are highlighted in gray.**

(a) Effect of information exchange.

bottleneck adapters. The 9.3-point gap between this baseline and B-CAST highlights the importance of bottleneck adapters for effective information exchange between the two expert models. The third row (_X-attn_\(\)_adapter_) is a baseline with the adapters after cross-attention. Compared to B-CAST, this baseline shows a 0.8 points drop in Action accuracy while having more than double the number of learnable parameters (\(44.9M\) vs. \(93.0M\)). The results indicate that cross-attention in bottleneck is more effective and more efficient than the baseline. In summary, by placing cross-attention in the middle of the bottleneck adapter, B-CAST facilitates effective information exchange between the two experts and achieves a synergistic effect.

Effect of projection ratio in bottleneck.In this study, we investigate the impact of the down projection ratio in the bottleneck architecture presented in Table 1 (d). The results demonstrate that a ratio of \(1/2\) yields the best performance. Notably, a ratio of 1 results in inferior performance, which we attribute to overfitting caused by the addition of more parameters.

Effect of cross-attention window shape.We investigate the impact of the window shape in the cross-attention mechanism in the T2S and S2T modules in Table 1 (e). Please refer to Figure 3 (c) for the details of the window size. We maintain the same model capacity across different methods. Using space-time attention for both T2S and S2T modules results in the worst performance. We conjecture that learning joint space-time attention is challenging with the given model capacity . On the other hand, using time attention in T2S and space attention in S2T yields the best performance. Consequently, we adopt this configuration throughout the paper.

Effect of the number of cross-attention layers.We investigate the impact of the number of cross-attention layers used. To this end, we gradually increase the number of cross-attention layers starting from the top three layers, and report the results in Table 1 (f). As we increase the number of cross-attention layers, we observe a corresponding improvement in performance, as expected.

Effect of bi-directional cross-attention.To validate the effectiveness of bi-directional information exchange, we ablate each cross attention at a time. We compare CAST with unidirectional information exchange baselines equipped with S2T or T2S cross-attention only. Each unidirectional information exchange baseline still has both experts. In Table 1 (g), compared to our CAST (48.7%), the S2T only baseline shows 5.0 points drop (43.7%) and the T2S only baseline shows 2.0 points drop (46.7%) in accuracy. The results validate the effectiveness of the proposed bi-directional cross-attention.

Role of each expert.In Table 1 (h), we investigate the role of experts within CAST by controlling the assignment of models to each expert. We observe that we can achieve the best performance of 48.7% when we employ CLIP as our temporal expert and VideoMAE as our spatial expert. When we employ one VideoMAE as our spatial expert and another VideoMAE as our temporal expert, we obtain 40.3% accuracy. When we employ one CLIP as our spatial expert and another CLIP as our temporal expert, we obtain 46.0% accuracy.

Interestingly, when we revert the role of CLIP and VideoMAE, i.e., we employ VideoMAE as the spatial and CLIP as the temporal expert, we achieve a good performance of 47.8%. The results demonstrate that the B-CAST architecture facilitates effective information exchange between the two experts. Through the stacked B-CAST, the experts can learn high-quality spatio-temporal representations by exchanging information, even when the roles are reverted.

In summary, these findings suggest that CAST achieves optimal performance when models are assigned to expert roles that align with their strengths. CLIP serves as an effective spatial expert, whereas VideoMAE is more effective as a temporal expert. The B-CAST architecture encourages these experts to leverage their respective strengths through information exchange, resulting in enhanced spatio-temporal balanced understanding.

Figure 6: **Qualitative examples from EK100 comparing CLIP, VideoMAE, and the proposed CAST. Each expert model shows more accurate predictions in their expertise but shows weaker performance on the other task. However, CAST consistently shows correct predictions for both tasks, demonstrating the effectiveness of the proposed spatio-temporal cross-attention mechanism.**

Comparison with state-of-the-arts on the EK100, SSV2 and K400 datasets. We show the Top-1 accuracy on each dataset and the harmonic mean (H.M.) of the Top-1 accuracies. The best performance is in **bold** and the second best is underscored.

### Comparison with state-of-the-art

In this section, we evaluate the performance of CAST and state-of-the-art methods in terms of balanced spatio-temporal understanding on multiple datasets, as shown in Table 2. For each method, in addition to reporting the top-1 accuracy of each task, we report the harmonic mean of top-1 accuracies for i) SSV2, and K400, and ii ) EK100 verb, EK100 noun, SSV2, and K400. For comparison with state-of-the-art models, we have set different hyperparameters than those used in our ablation study. Please refer to Table 4 for the details. For fair comparisons of the computation complexity, we show the GFLOPs/View. In cases where a compared method shows various GFLOPs/View depending on the dataset, we specifically note the lowest GFLOPs/View value for reference. For more detailed comparison of computation complexity, please refer to the Appendix SS E.

We observe that among the CLIP-based methods (the second group in Table 2), AIM  achieves favorable performance on the static-biased K400 dataset, with 84.5% accuracy. However, AIM shows a relatively lower performance of 68.1% on the temporal-biased SSV2. On the other hand, VideoMAE , one of the state-of-the-art methods, shows 70.8% accuracy on the SSV2 dataset, which is more competitive than AIM. However, VideoMAE shows a lower accuracy of 81.5% on the K400 dataset, less competitive than AIM. Our proposed method, CAST, demonstrates favorable performance on both the SSV2 (71.6%) and K400 (85.3%) datasets, resulting in a harmonic mean of 77.9%, which is higher than that of AIM (75.4%) and VideoMAE (75.8%). CAST shows a more balanced spatio-temporal understanding than the existing methods. Additionally, CAST shows favorable performance in fine-grained action recognition on the EK100 dataset. CAST achieves a competitive Action accuracy of 49.3%, which is the second best among the compared methods.

In terms of the overall harmonic mean of EK100 verb, EK100 noun, SSV2, and K400 accuracies, CAST shows the best performance of 71.6%. The results highlight the effectiveness of CAST. By exchanging information between spatial and temporal experts, our CAST shows a favorable balanced spatio-temporal understanding performance.

## 5 Conclusions

In this paper, we present a solution to the problem of action recognition models lacking a balanced spatio-temporal understanding of videos. The proposed method, CAST, incorporates a spatial expert and a temporal expert that exchange information through cross-attention to achieve synergistic predictions. Our extensive experiments on datasets with varying characteristics demonstrate that CAST outperforms both individual expert models and existing methods in terms of a balanced spatio-temporal understanding measure: the harmonic mean of accuracies on the datasets. The results highlight the effectiveness of CAST in achieving a balanced spatio-temporal understanding of videos, and suggest that CAST could have broad applicability in the field of video understanding.

    & GFLOPs/ &  &  & All \\  Method & View & Verb & Noun & Act. & SSV2 & K400 & H.M. & H.M \\  CLIP\({}^{*}\) & 140 & 54.9 & 52.7 & 33.8 & 47.8 & 78.9 & 59.5 & 56.5 \\ EVL  & 592 & - & - & - & 62.4 & 82.9 & 71.2 & - \\ ST-Adapter  & 607 & 67.6 & 55.0 & - & 69.5 & 82.7 & 75.5 & 67.3 \\ AIM  & 404 & 64.8 & 55.5 & 41.3 & 68.1 & 84.5 & 75.4 & 66.7 \\  MBT  & 936 & 64.8 & 58.0 & 43.4 & - & 80.8 & - & - \\ ViViT FE  & 990 & 66.4 & 56.8 & 44.0 & 65.9 & 81.7 & 73.0 & 66.6 \\ Timesformer  & 2380 & - & - & - & 62.4 & 80.7 & 70.4 & - \\ MViT  & 170 & - & - & - & 67.7 & 80.2 & 73.4 & - \\ MFormer  & 1185 & 67.1 & 57.6 & 44.1 & 68.1 & 80.2 & 73.7 & 67.3 \\ ORViT MF  & - & 68.4 & 58.7 & 45.7 & 67.9 & - & - & - \\ Video Swin  & 282 & - & - & - & 69.6 & 82.7 & 75.8 & - \\ BEVT  & 282 & - & - & - & 70.6 & 80.6 & 75.3 & - \\ VideoMAE  & 180 & 70.5 & 51.4 & 41.7\({}^{*}\) & 70.8 & 81.5 & 75.8 & 66.6 \\ MeMVViT  & 59 & 70.6 & 58.5 & 46.2 & - & - & - & - \\ OMNVORE  & - & 69.5 & 61.7 & **49.9** & 71.4 & 84.0 & 77.2 & 70.8 \\ MTV-HR  & 930 & 68.0 & 63.1 & 48.6 & 68.5 & 82.4 & 74.8 & 69.8 \\  CAST & 391 & 72.5 & 60.9 & 49.3 & 71.6 & 85.3 & **77.9** & **71.6** \\    \({}^{*}\)We conduct experiments with our own implementation.

Table 2: **Comparison with the state-of-the-arts on the EK100, SSV2 and K400 datasets. We show the Top-1 accuracy on each dataset and the harmonic mean (H.M.) of the Top-1 accuracies. The best performance is in **bold** and the second best is underscored.

Acknowledgment.This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2022-00155911, Artificial Intelligence Convergence Innovation Human Resources Development (Kyung Hee University)); by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea Government (MSIT) (Artificial Intelligence Innovation Hub) under Grant 2021-0-02068; by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1F1A1070997).