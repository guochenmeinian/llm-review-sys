# Derivative-enhanced Deep Operator Network

Yuan Qiu, Nolan Bridges, Peng Chen

Georgia Institute of Technology, Atlanta, GA 30332

{yuan.qiu, bridges, pchen402}@gatech.edu

###### Abstract

The deep operator networks (DeepONet), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited. DE-DeepONet explicitly incorporates linear dimension reduction of high dimensional parameter input into DeepONet to reduce training cost and adds derivative loss in the loss function to reduce the number of required parameter-solution pairs. We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO). Numerical experiments validate the effectiveness of our approach.

## 1 Introduction

Using neural networks to approximate the maps between functions spaces governed by parametric PDEs can be very beneficial in solving many-query problems, typically arising from Bayesian inference, optimization under uncertainty, and Bayesian optimal experimental design. Indeed, once pre-trained on a dataset, neural networks are extremely fast to evaluate given unseen inputs, compared to traditional numerical methods like the finite element method. Recently various neural operators are proposed to enhance the learning capacity, with two prominent examples deep operator network (DeepONet)  and Fourier neural operator (FNO) , which are shown to be inclusive of each other in their more general settings [3; 4], see also their variants and other related neural operator architectures in [5; 6; 7; 8; 9; 10]. Though these work demonstrate to be successful in approximating the output function, they do not necessarily provide accurate approximation of the derivative of the output with respect to the input, which are often needed for many downstream tasks such as PDE-constrained optimization problems for control, inference, and experimental design [11; 12; 13; 14].

In this paper, we propose to enhance the performance of DeepONet through derivative-based dimension reduction for the function input inspired by [15; 16; 17; 18] and the incorporation of derivative information in the training to learn both the output and its derivative with respect to the input inspired by [19; 20]. These two derivative-enhanced approaches can significantly improve DeepONet's approximation accuracy for the output function and its directional derivative with respect to the input function, especially when the training samples are limited. We provide details on the computation of derivative labels of the solution of PDEs in a general form as well as the derivative-based dimension reduction to largely reduce the computational cost. We demonstrate the effectiveness of our proposed method (DE-DeepONet) compared to three other neural operators, including DeepONet, FNO, and derivative-informed neural operator (DINO) , in terms of both test errors and computational cost. In addition, we apply derivative learning to train the FNO and also compare its performance with other methods. The code for data generation, model training and inference, as well as configurations to reproduce the results in this paper can be found at https://github.com/qy849/DE-DeepONet.

Preliminaries

This section presents the problem setup, high-fidelity approximation using finite element for finite dimensional discretization, and the DeepONet architecture in learning the solution operator.

### Problem setup

Let \(^{d}\) denote an open and bounded domain with boundary \(^{d-1}\), where the dimension \(d=1,2,3\). We consider a PDE of the general form defined in \(\) as

\[(m,u)=0,\] (1)

prescribed with proper boundary conditions. Here \(m V^{}\) is an input parameter function defined in a separable Banach space \(V^{}\) with probability measure \(\) and \(u V^{}\) is the output as the solution of the PDE defined in a separable Banach space \(V^{}\). Our goal is to construct a parametric model \((m;)\) to approximate the solution operator that maps the parameter \(m\) to the solution \(u\).

Once constructed, the parametric model \((m;)\) should be much more computationally efficient to evaluate compared to solving the PDE with high fidelity approximation.

### High fidelity approximation

For the high fidelity approximation of the solution, we consider using a _finite element method_ in this work. We partition the domain \(\) into a finite set of subregions, called cells or elements. Collectively, these cells form a _mesh_ of the domain \(\). Let \(h\) represent the diameter of the largest cell. We denote \(V^{}_{h}\) indexed by \(h\) as the finite element space for the approximation of the input space \(V^{}\) with Lagrange basis \(\{^{}_{1},,^{}_{N^{}_{h}}\}\) of dimension \(N^{}_{h}\) such that \(^{}_{i}(x^{(j)})=_{ij}\) at the finite element node \(x^{(j)}\), with \(_{ij}\) being the Kronecker delta function. Similarly, we denote \(V^{}_{h}\) as the finite element space for the approximation of the solution space \(V^{}\) with basis \(\{^{}_{1},,^{}_{N^{}_{h}}\}\) of dimension \(N^{}_{h}\). Note that for the approximation to be high fidelity, \(N^{}_{h}\) and \(N^{}_{h}\) are often very large. To this end, we can write the high fidelity approximation of the input and output functions as

\[m_{h}(x)=_{i=1}^{N^{}_{h}}m_{i}^{}_{i}(x)\ \ \ \ u_{h}(x)=_{i=1}^{N^{}_{h}}u_{i}^{}_{i}(x),\]

with coefficient vectors \(=(m_{1},,m_{N^{}_{h}})^{T}^{N^{}_{ h}}\) and \(=(u_{1},,u_{N^{}_{h}})^{T}^{N^{}_{ h}}\), whose entries are the _nodal values_ of \(m\) and \(u\) at the corresponding nodes.

### DeepONet

We briefly review the DeepONet architecture  with a focus on learning the solution operator of the PDE in Equation (1). To predict the evaluation of solution function \(u\) at any point \(x\) for any given input function \(m\),  design a network architecture that comprises two separate neural networks: a trunk net \(t(\ ;_{t})\), which takes the coordinate values of the point \(x\) at which we want to evaluate the solution function as inputs, and a branch net \(b(\ ;_{b})\), which takes the vector \(\) encoding the parameter function \(m\) as inputs. In , the vector \(\) is the function evaluations at a finite set of fixed points \(\{x^{(j)}\}_{j=1}^{N^{}_{h}}\), that is, \(=(m(x^{(1)}),,m(x^{(N^{}_{h})}))^{T}\), which corresponds to coefficient vector in the finite element approximation with Lagrange basis at the same nodes. If the solution function is scalar-valued, then both neural networks output vectors of the same dimensions. The prediction is obtained by taking the standard inner product between these two vectors and (optionally) adding a real-valued bias parameter, i.e,

\[(;)(x)= b(;_{b}),t(x;_{t}) +_{},\]

with \(=(_{b},_{t},_{})\). If the solution \(u\) is vector-valued of \(N_{u}\) components, i.e., \(u=(u^{(1)},,u^{(N_{u})})\), as in our experiments, we can use one of the four approaches in  to construct the DeepONet. Specifically, for each solution component, we use the same outputs of branch net with dimension \(N_{b}\) and different corresponding groups of outputs of trunk net to compute the inner product. More precisely, the solution \(u^{(i)}\) of component \(i=1,,N_{u}\), is approximated by

\[^{(i)}(;)(x)= b(;_{b}),t^{(i)}(x;_ {t})+^{(i)}_{},\] (2)where \(t^{(i)}(x;_{t})=t(x;_{t})[(i-1)N_{b}+1:iN_{b}]\), the vector slice of \(t(x;_{t})\) with indices ranging from \((i-1)N_{b}+1\) to \(iN_{b}\), \(i=1,,N_{u}\).

For this construction, the outputs of the branch net can be interpreted as the coefficients of the basis learned through the trunk net. By partitioning the outputs of the trunk net into different groups, we essentially partition the basis functions used for predicting different components of the solution. The DeepONet is trained using dataset \(=\{(m^{(i)},u^{(i)})\}_{i=1}^{N_{}}\) with \(N_{}\) samples, where \(m^{(i)}\) are random samples independently drawn from \(\) and \(u^{(i)}\) are the solution of the PDE (with slight abuse of notation from the vector-valued solution) at \(m^{(i)}\), \(i=1,,N_{}\).

## 3 DE-DeepONet

The DeepONet uses the input parameter and output solution pairs as the labels for the model training. The approximation for the derivative of the solution with respect to the parameter (and the coordinate) are not necessarily accurate. However, in many downstream tasks such as Bayesian inference and experimental design, the derivatives are required. We consider incorporating the the Frechet derivative \(du(m;)\) for the supervised training of the DeepONet, which we call derivative-enhanced DeepONet (DE-DeepONet). By doing this we hope the optimization process can improve the neural network's ability to predict the derivative of the output function with respect to the input function. Let \(\) denote the trainable parameters in the DE-DeepONet. We propose to minimize the loss function

\[L()=_{1}_{m}\|u(m)-(m)\|_{L^{2}() }^{2}+_{2}_{m}\|du(m;)-d(m;)\|_{ {HS}}^{2},\] (3)

where the \(L^{2}()\) norm of a square integrable function \(f\) is defined as \(\|f\|_{L^{2}()}^{2}=(_{}\|f(x)\|_{2}^{2}\ x)^{1/2}\), the Hilbert-Schmidt norm of an operator \(T:H H\) that acts on a Hilbert space \(H\) is defined as \(\|T\|_{}^{2}=_{i I}\|Te_{i}\|_{H}^{2}\), where \(\{e_{i}\}_{i I}\) is an orthonormal basis of \(H\). Here, \(_{1},_{2}>0\) are hyperparameters that balance different loss terms.

The main challenge of minimizing the loss function (3) in practice is that with high fidelity approximation of the functions \(m\) and \(u\) using high dimensional vectors \(\) and \(\), the term \(\|du(m;)-d(m;)\|_{}^{2}\) (approximately) becomes \(\|_{}-_{}}\|_{F}^{2}\), where the Frobenius norm of a matrix \(M^{m n}\) is defined as \(\|M\|_{F}^{2}=_{i=1}^{m}_{j=1}^{n}M_{ij}^{2}\). It is a critical challenge to both compute and store the Jacobian matrix at each sample as well as to load and use it for training since it is often very large with size \(N_{h}^{} N_{h}^{}\), with \(N_{h}^{},N_{h}^{} 1\).

To tackle this challenge, we employ dimension reduction for the high dimensional input vector \(\). The reduced representation of \(\) is given by projecting \(\) into a low dimensional linear space spanned by a basis \(_{1}^{},,_{r}^{}^{N_{h }^{}}\), with \(r N_{h}^{}\), that is,

\[}=(_{1}^{},,, _{r}^{},)^{T}^{r}.\]

To better leverage the information of both the input probability measure \(\) and the map \(u:m u(m)\), we consider the basis generated by active subspace method (ASM) using derivatives . ASM identifies directions in the input space that significantly affect the output variance, or in which the output is most sensitive. See Section 3.3 for the detailed construction. In this case, the term \(_{m}\|du(m;)-d(m;)\|_{}^{2}\) can be approximated by \(N_{2}}_{i=1}^{N_{1}}_{j=1}^{N_{2}}\|_{}}u(m^{(i)})(x^{(j)})-_{}}(m^{(i)})(x^{(j) })\|_{2}^{2}\) with a small amount of functions \(m^{(i)}\) sampled from input probability measure \(\) and points \(x^{(j)}\) in the domain \(\). Note that \(_{}(m^{(i)})(x^{(j)})\) is vector of size \(r\), which is computationally feasible. For comparison, we also conduct experiments if the basis is the most commonly used Karhunen-Loeve Expansion (KLE) basis.

We next formally introduces our DE-DeepONet, which uses dimension reduction for the input of the branch net and incorporates its output-input directional derivative labels as additional soft constraints into the loss function.

### Model architecture

We incorporate dimension reduction into DeepONet to construct a parametric model for approximating the solution operator. Specifically, if the solution is scalar-valued (the vector-valued case can be constructed similar to (2)), the prediction is given by

\[(;)(x)= b(};_{b}),t(x;_{t })+_{},\] (4)where \(=(_{b},_{t},_{})\) are the parameters to be learned. The branch net \(b(;_{b})\) and trunk net \(t(;_{t})\) can be chosen as an MLP, ResNet, etc. Note that the branch net takes a small vector of the projected parameter as input. We also apply the Fourier feature embeddings , defined as \((x)=[(Bx),(Bx)]\), to the trunk net, where each entry in \(B^{m d}\) is sampled from a Gaussian distribution \((0,^{2})\) and \(m^{+},^{+}\) are hyper-parameters.

### Loss function

In practical training of the DE-DeepONet, we formulate the loss function as follows

\[L() =}{N_{}}_{i=1}^{N_{}} (\{((^{(i)};)(x^{(j)}),u(m^{(i)})(x^{(j)}))\}_{j= 1}^{N_{x}})\] (5) \[+}{N_{}}_{i=1}^{N_{}} (\{(_{}(^{(i)};)(x^{(j)})^{ {in}},^{}(x^{(j)})(_{}(^{(i)})^{ })\}_{j=1}^{N_{x}}),\]

where \(\{x^{(j)}\}_{j=1}^{N_{x}}\) are the nodes of the mesh, \(^{}=(^{}_{1}||^{}_{r})\) is the matrix collecting the nodal values of the reduced basis of the input function space, and \(^{}(x)=(^{}_{1}(x),,^{}_{N^{ }_{r}}(x))\) is the vector-valued function consisting of the finite element basis functions of the output function space. The \((\{(^{(i)},^{(i)})\}_{i=1}^{n})\) denotes the relative error between any two groups of vectors \(^{(i)}\) and \(^{(i)}\), computed as

\[(\{(^{(i)},^{(i)})\}_{i=1}^{n})=^{n}\| ^{(i)}-^{(i)}\|_{2}^{2})^{1/2}}{+(_{i=1}^{n}\|^{(i)}\|_{2}^{2})^{1/2}},\]

where \(>0\) is some small positive number to prevent the fraction dividing by zero.

In the following, we explain how to compute different loss terms in Equation (5)

* The first term is for matching the prediction of the parametric model \((^{(i)};)\) evaluated at any set of points \(\{x^{(j)}\}_{j=1}^{N_{x}}\) with the high fidelity solution \(u(m^{(i)})\) evaluated at the same points. The prediction \((^{(i)};)(x^{(j)})\) is straightforward to compute using Equation (4). This involves passing the reduced branch inputs \(^{(i)}\) and the coordinates of point \(x^{(j)}\) into the branch net and trunk net, respectively. The label \(u(m^{(i)})(x^{(j)})\) is obtained using finite element method solvers.
* The second term is for learning the directional derivative of the evaluation \(u(x^{(j)})\) with respect to the input function \(m\), in the direction of the reduced basis \(^{}_{1},,^{}_{r}\). It can be shown that \[_{}(^{(i)};)(x^{(j)})^{}=_{ }}(^{(i)};)(x^{(j)}).\] Thus, the derivative of the outputs of the parametric model can be computed as the partial derivatives of the output with respect to the input of the branch net via automatic differentiation. On the other hand, the derivative labels \[^{}(x^{(j)})(_{}(m^{(i)})^{})=(du (m^{(i)};^{}_{1})(x^{(j)}),,du(m^{(i)};^{}_{r} )(x^{(j)}))\] are obtained by first computing the Gateaux derivatives \(du(m^{(i)};^{}_{1}),,du(m^{(i)};^{}_{r})\) and then evaluating them at \(x^{(j)}\). See Appendix B.3 for details about the computation of the Gateaux derivative \(du(m;)\).
* We initialize the loss weights \(_{1}=_{2}=1\) and choose a loss balancing algorithm called the self-adaptive learning rate annealing algorithm  to update them at a certain frequency. This ensures that the gradient of each loss term have similar magnitudes, thereby enabling the neural network to learn all these labels simultaneously.

**Remark**.: _For the training, the computational cost of the second term of the loss function (and its gradients) largely depends on the number of points used in each iteration. To reduce the computational and especially memory cost, we can use a subset of points, \(N^{}_{x}= N_{x}\), where \(\) is small number between \(0\) and \(1\) (e.g., \(=0.1\) in our experiments), in a batch of functions, though their locations could vary among different batches in one epoch. We find that this approach has little impact on the prediction accuracy of the model when \(N_{x}\) is large enough._

### Dimension reduction

Throughout the paper, we assume that the parameter functions \(m\) are independently drawn from some Gaussian random field . In particular, we consider the case where the covariance function is the Whittle-Matern covariance function, that is, \(m}{{}}(,)\), where \(\) is the (deterministic) mean function and \(=( I-)^{-2}\) (\(I\) is the identity and \(\) is the Laplacian) is an operator such that the square root of its inverse, \(^{-}\), maps random function \((m-)\) to Gaussian white noise with unit variance . The parameters \(,^{+}\) jointly control the marginal variance and correlation length.

We consider two linear projection bases for dimension reduction of the parameter function.

Karhunen-Loeve Expansion (KLE) basis.The KLE basis is optimal in the sense that the mean-square error resulting from a finite representation of the random field \(m\) is minimized . It consists of eigenfunctions determined by the covariance function of the random field. Specifically, an eigenfunction \(\) of the covariance operator \(=( I-)^{-2}\) satisfies the differential equation

\[=.\] (6)

When solved using the finite element method, Equation (6) is equivalent to the following linear system (See Appendix A.2 for the derivation)

\[M^{}(A^{})^{-1}M^{}(A^{})^{-1}M^{}= M^{},\] (7)

where the \((i,j)\)-entries of matrices \(A^{}\) and \(M^{}\) are given by

\[A^{}_{ij}=^{}_{j},^{}_{i} +^{}_{j},^{}_{i} , M^{}_{ij}=^{}_{j},^{} _{i}.\]

Here, we recall that \(V^{}_{h}\) is the finite element function space of input function, \(^{}_{i},i=1,,N^{}_{h}\) for the finite element basis of \(V^{}_{h}\), and \(,\) for the \(L^{2}()\)-inner product. We select the first \(r\) (typically \(r N^{}_{h}\)) eigenfunctions \(^{}_{1},,^{}_{r}\) corresponding to the \(r\) largest eigenvalues for the dimension reduction. Let \(^{}=(^{}_{1}][^{}_{r})\) denote the corresponding nodal values of these eigenfunctions. Since the eigenvectors \(^{}_{i}\) are \(M^{}\)-orthogonal (or equivalently, eigenfunctions \(^{}_{i}\) are \(L^{2}()\)-orthogonal), the reduced representation of \(\) can be computed as \(}=(^{})^{T}M^{}\), that is, the coefficients of the low rank approximation of \(m\) in the linear subspace spanned by the eigenfunctions \(^{}_{1},,^{}_{r}\).

Active Subspace Method (ASM) basis.The active subspace method is a gradient-based dimension reduction method that looks for directions in the input space contributing most significantly to the output variability . In contrast to the KLE basis, the ASM basis is more computationally expensive. However, since the ASM basis captures sensitivity information in the input-output map rather than solely the variability of the input space, it typically achieves higher accuracy in predicting the output than KLE basis. We consider the case where the output is a multidimensional vector , representing the nodal values of the output function \(u\). The ASM basis \(_{i}\), \(i=1,,r\), are the eigenfunctions corresponding to the \(r\) largest eigenvalues of the generalized eigenvalue problem

\[=^{-1},\] (8)

where the action of operator \(\) on function \(\) is given by

\[=_{m(m)}[d^{*}u(m;du(m;))].\] (9)

Here, \(du(m;)\) is the Gateaux derivative of \(u\) at \(m V^{}_{h}\) in the direction of \( V^{}_{h}\), defined as \(_{ 0}(u(m+)-u(m))/\), and \(d^{*}u(m;)\) is the adjoint of the operator \(du(m;)\). When solved using finite element method, Equation (8) is equivalent to the following linear system (See Appendix A.3 for the derivation)

\[H= C^{-1},\] (10)

where the action of matrix \(H\) on vector \(\) is given by

\[H=_{()}[(_{})^{T}M^{ }(_{})],\] (11)

and the action of matrix \(C^{-1}\) on vector \(\) is given by

\[C^{-1}=A^{}(M^{})^{-1}A^{}.\] (12)Here, \(M^{}\) denotes the mass matrix of the output function space, i.e., \(M^{}_{ij}=^{}_{j},^{}_{i}\). In practice, when solving Equation (10), we obtain its left hand side through computing

\[(,^{}_{1},, ,^{}_{N^{}_{h}})^{T}\]

and its right hand side through the matrix-vector multiplication in Equation (12) (See A.3 for details). Similar to the KLE case, let \(^{}\) denote the nodal values of \(r\) dominant eigenfunctions. Since the eigenvectors \(^{}_{i}\) are \(C^{-1}\)-orthogonal, the reduced representation of \(\) can be computed as \(}=(^{})^{T}C^{-1}\). We use a scalable _double pass randomized algorithm_ implemented in hIPPYlib to solve the generalized eigenproblems Equation (7) and Equation (10).

To this end, we present the computation of the derivative label and the action of \(\) as follows.

**Theorem 1**.: _Suppose the PDE in the general form of (1) is well-posed with a unique solution map from the input function \(m V^{}\) to the output function \(u V^{out}\) with dual \((V^{out})^{}\). Suppose the PDE operator \(:V^{} V^{out}(V^{out})^{}\) is differentiable with derivatives \(_{m}:V^{}(V^{out})^{}\) and \(_{u}:V^{out}(V^{out})^{}\), and in addition \(_{u}\) is invertible with invertible adjoint \((_{u})^{*}\). Then the directional derivative \(p=du(m;)\) for any function \( V^{}\), and an auxiliary variable \(q\) such that \(d^{*}u(m;p)=-(_{m})^{*}q\) can be obtained as the solution of the linearized PDEs_

\[(_{u})p+(_{m}) =0,\] (13) \[(_{u})^{*}q =p.\] (14)

Proof sketch.: We perturb \(m\) with \(\) for any small \(>0\) and obtain \((m+,u(m+))=0\). Using Taylor expansion to expand it to the first order and letting \(\) approach \(0\), we obtain Equation (13), where \(p=du(m;)\). Next we compute \(d^{*}u(m;p)\). By Equation (13), we have \(du(m;)=-(_{u})^{-1}(_{m})\). Thus, \(d^{*}u(m;)=-(_{m})^{*}(_{u})^{-*}p\). Then we can first solve for \(q=(_{u})^{-*}p\) and then compute \(-(_{m})^{*}q\). See A.1 for a full proof.

## 4 Experiments

In this section, we present experiment results to demonstrate the derivative-enhanced accuracy and cost of our method on two test problems of nonlinear vector-valued PDEs in comparison with DeepONet, FNO, and DINO. Details about the data generation and training can be found in B.

### Input probability measure

In all test cases, we assume that the input functions \(m^{(i)}\) are i.i.d. samples drawn from a Gaussian random field with mean function \(\) and covariance operator \(( I-)^{-}\). We take \(=2\) in two space dimensions so the covariance operator is of trace class . It is worth noting that the parameters \(\) and \(\) jointly control the marginal variance and correlation length, for which we take values to have large variation of the input samples that lead to large variation of the output PDE solutions. In such cases, especially when the mapping from the input to output is highly nonlinear as in our test examples, a vanilla neural network approximations tend to result in relatively large errors, particularly with a limited number of training data. To generate samples from this Gaussian random field, we use a scalable (in mesh resolution) sampling algorithm implemented in hIPPYlib .

### Governing equations

We consider two nonlinear PDE examples, including a nonlinear vector-valued hyperelasticity equation with one state (displacement), and a nonlinear vector-valued Navier-Stokes equations with multiple states (velocity and pressure). For the hyperelasticity equation, we consider an experimental scenario where a square of hyperelasticity material is secured along its left edge while a fixed upward-right force is applied to its right edge . Our goal is to learn the map between (the logarithm of) the material's Young's modulus and its displacement. We also consider the Navier-Stokes equations that describes viscous, incompressible creeping flow. In particular, we consider the lid-driven cavity case where a square cavity consisting of three rigid walls with no-slip conditions and a lid moving with a tangential unit velocity. We consider that the uncertainty comes from the viscosity term and aim to predict the velocity field. See B.1 for details.

### Evaluation metrics

We use the following three metrics to evaluate the performance of different methods. For the approximations of the solution \(u(m)\), we compute the relative error in the \(L^{2}()\) norm and \(H^{1}()\) norm on the test dataset \(_{}=\{(m^{(i)},u^{(i)})\}_{i=1}^{N_{}}\), that is,

\[}}_{m^{(i)}_{}}(m^{(i)};)-u(m^{(i)})\|_{L^{2}()}}{\|u(m^{(i)})\|_{L^{2}()}}, }}_{m^{(i)}_{}}(m^{(i)};)-u(m^{(i)})\|_{H^{1}()}}{\|u(m^{(i)})\|_{H^{1}( )}},\]

where

\[\|u\|_{L^{2}()}=_{}\|u(x)\|_{2}^{2}\;x=^{T}M^ {},\|u\|_{H^{1}()}=(\|u\|_{L^{2}()}^{2}+\|  u\|_{L^{2}()}^{2})^{1/2}.\]

For the approximation of the Jacobian \(du(m;)\), we compute the relative error (for the discrete Jacobian) in the Frobenius norm on \(_{}\) along random directions \(=\{_{i}\}_{i=1}^{N_{}}\), that is,

\[_{}|}_{m^{(i)}_{} }(m^{(i)};)-du(m^{(i)};)\|_{F}}{\|du(m^{(i)}; )\|_{F}},\]

where \(_{i}\) are samples drawn from the same distribution as \(m\).

### Main results

We compare the prediction errors measured in the above three evaluation metrics and computational cost in data generation and neural network training for four neural operator architectures, including DeepONet , FNO , DINO , and our DE-DeepONet. We also add experiments to demonstrate the performance of the FNO trained with the derivative loss (DE-FNO) and the DeepONet trained with input dimension reduction but without the derivative loss (DeepONet (ASM) or DeepONet (KLE)). For FNO, we use additional position embedding  that improves its approximation accuracy in our test cases. For DINO, we use ASM basis (v.s. KLE basis) as the input reduced basis and POD basis (v.s. output ASM basis ) as the output reduced basis, which gives the best approximation accuracy. For the input reduced basis of DE-DeepONet, we also test and present the results for both KLE basis and ASM basis.

Figure 1: Mean relative errors (\(\) standard deviation) over 5 random seeds of neural network training for a varying number of training samples for the [top: hyperelasticity; bottom: Navier–Stokes] equation using different methods. Relative errors in the \(L^{2}()\) norm (left) and \(H^{1}()\) norm (middle) for the prediction of \(u=(u_{1},u_{2})\). Right: Relative error in the Frobenius (Fro) norm for the prediction of \(du(m;)=(du_{1}(m;),du_{2}(m;))\).

Test errors.In Figure 1, we show a comparison of different methods for the hyperelasticity and Navier-Stokes problem in predicting the solution (in \(L^{2}\) norm, left, and \(H^{1}\) norm, middle) and its derivative (in Frobenius norm, right) with respect to parameter in the direction of \(N_{}=128\) test samples \(\{_{i}\}_{i=1}^{N_{}}\). First, we can observe significant improvement of the approximation accuracy of DE-DeepONet compared to the vanilla DeepONet for all three metrics in all cases of training samples. Second, we can see that FNO leads to larger approximation errors than DINO and DE-DeepONet for all metrics except when number of training samples is 1024. We believe that the reason why FNO performs better than DE-DeepONet and DINO when training samples are large enough is mainly due to the use of input dimensionality reduction in DE-DeepONet and DINO (where the linear reduction error cannot be eliminated by increasing training samples) whereas in FNO we use full inputs. We also see that DE-FNO performs the best among all models when the training samples are sufficient (256 or 1024), although in the compensation of much longer training time shown in Figure 2. Third, we can see that the approximation accuracy of DINO is similar to DE-DeepONet (ASM) but requires much longer inference time as shown in Table 5 (see Section 5 and Appendix B.3.2 for reasons). In DINO, the output reduced basis dimension is set to be smaller than or equal to the number of training samples as the output POD basis are computed from these samples, i.e., 16 for 16 samples and 64 for \( 64\) samples. Increasing the output dimension beyond 64 does not lead to smaller errors in our test. Finally, we can observe that DE-DeepONet using ASM basis leads to smaller errors than using KLE basis, especially for the Navier-Stokes problem.

Moreover, we present the output reconstruction error due to the input dimension reduction using KLE or ASM basis in Table 1. The errors provide the lower bound of the relative errors in \(L^{2}()\) norm of DINO and DE-DeepONet. We can see that using the ASM basis results in a lower output reconstruction error than the KLE basis (more significant difference observed in the more nonlinear Navier-Stokes equations). See Appendix B.6 for the decay of the reconstruction error with increasing number of reduced basis.

    & Relative \(L^{2}\) error \\  Dataset & KLE & ASM \\  Hyperelasticity & 3.8\% & 2.7\% \\ Navier-Stokes & 17.4\% & 5.8\% \\   

Table 1: Output reconstruction error with 16 input reduced bases

Figure 2: Mean relative errors (\(\) standard deviation) over 5 random seeds versus model training time for the Navier–Stokes equations when the number of training samples is [top: 16; bottom: 256].

Data generation computational cost.We use MPI and the finite element library FEniCS  to distribute the computational load of offline data generation to 64 processes for the PDE models considered in this work. See Table 2 for the wall clock time in generating the samples of Gaussian random fields (GRFs), solving the PDEs, computing the \(r=16\) reduced basis functions (KLE or ASM) corresponding to the \(16\) largest eigenvalues, generating the derivative labels, respectively. In Table 3, We also provide the total wall clock time of data generation of DE-DeepONet (ASM) (we only includes the major parts - computing high fidelity solution, ASM basis and dm labels [16 directions]) when \(N_{}=16,64,256,1024\) using 16 CPU processors.

Model training computational cost.We present comparisons of the wall clock time of each optimization iteration (with batch size 8) of different methods in Table 4 and convergence plot (error versus training time) in Figure 2 and the figures in Appendix B.5. We find that incorporating derivative loss leads to longer training time as expected. However, when the training data are limited, the increased computation cost is compensated for a significant reduction of errors. We note that there are potential ways to further reduce the training cost, e.g., by training the model with additional derivative loss only during the later stage of training, or by using fewer points for computing the derivative losses in each iteration. Additionally, thanks to the dimension reduction of the input, we can define a relatively small neural network and thus are able to efficiently compute the derivatives using automatic differentiation.

## 5 Related work

Our work is related to Sobolev training for neural networks , which was found to be effective in their application to model distillation/compression and meta-optimization. In the domain of surrogate

    &  \\   & GRFs (\(N_{}=2000\)) & PDEs (\(N_{}=2000\)) & (\(r=16\)) & ASM (\(r=16\)) & dm labels (\(N_{}=2000,r=16\)) \\  & (\(64\) procs) & (\(64\) procs) & (\(1\) procs) & (\(16\) procs) & (\(64\) procs) \\  Hyperelasticity & \(1.1\) & \(9.7\) & \(0.4\) & \(1.4\) & \(19.5\) \\ Navier–Stokes & \(1.9\) & \(99.1\) & \(1.3\) & \(9.7\) & \(125.5\) \\   

Table 2: Wall clock time (in seconds) for data generation on 2 \(\) AMD EPYC 7543 32-Core Processors

    &  \\   & DeepONet & FNO/DE-FNO & DINO\({}^{1}\) & DE-DeepONet & Numerical solver \\  & & & 1 GPU + 1 CPU/16 CPUs & & 0 GPU + 16 CPUs \\  Hyperelasticity & 3 & 33 & 69/7 & 10 & 166 \\ Navier–Stokes & 3 & 33 & 2152/151 & 18 & 1103 \\    \({}^{1}\) The inference time of DINO is dominated by the time required to compute evaluations of all finite element basis functions at the grid points using FEniCS (which may not be the most efficient, see Appendix B.3.2). Even though these grid points overlap with parts of the finite element nodes—allowing us to skip evaluations by extracting the relevant nodes—for a fairer comparison with DE-DeepONet (in terms of its ability to evaluate at any arbitrary point), we assume they are arbitrary points requiring explicit evaluation.

Table 4: Wall clock time (seconds/iteration with batch size 8) for training on a single NVIDIA RTX A6000 GPU

    &  \\   & GRFs (\(N_{}=2000\)) & PDEs (\(N_{}=2000\)) & (\(r=16\)) & (\(r=16\)) & (\(N_{}=2000,r=16\)) \\  & (\(64\) procs) & (\(64\) procs) & (\(1\) procs) & (\(16\) procs) & (\(64\) procs) \\  Hyperelasticity & \(1.1\) & \(9.7\) & \(0.4\) & \(1.4\) & \(19.5\) \\ Navier–Stokes & \(1.9\) & \(99.1\) & \(1.3\) & \(9.7\) & \(125.5\) \\   

Table 3: Wall clock time (in seconds) for data generation with different number of training samples using 16 CPU processorsmodels for parametric partial differential equations, our work is more closely related to derivative-informed neural operator (DINO)  which is based on a derivative-informed projected neural network (DIPNet) , and presents an extension to enhance the performance of the DeepONet. Compared to DINO, although the DeepONet architecture (and its formulation of dm loss) requires longer training time, it offers the following advantages: (1) Potentially shorter inference time. The additional trunk net (which receives spatial coordinates) allows us to quickly query the sensitivity of output function at any point when input function is perturbed in any direction. While DINO can only provide the derivative of the output coefficients respect to the input coefficients (we call reduced dm), in order to compute the sensitivity at a batch of points, we need to post process the reduced dm by querying the finite element basis on these points and computing large matrix multiplications; (2) Greater flexibility and potential for improvements. Although both DeepONet and DINO approximate solution by a linear combination of a small set of functions, these functions together in DeepONet is essentially the trunk net, which is "optimized" via model training, whereas in DINO, they are POD or derivative-informed basis precomputed on training samples. When using DINO, if we encounter a case where the training samples not enough to accurately compute the output basis, the large approximation error between the linear subspace and solution manifold will greatly restrict the model prediction accuracy (see Figure 2 when \(N_{}=16\)). And the reduced dm labels only supports linear reduction of output. However, it is possible that we can further improve DeepONet by, e.g., adding physical losses (to enhance generalization performance) and Fourier feature embeddings (to learn high-frequency components more effectively) on the trunk net  and replacing the inner product of the outputs of two networks by more flexible operations [33; 9] (to enhance expressive power). The dm loss formulation of our work is broadly suitable any network architecture that has multiple subnetworks, where at least one of them receives high-dimensional inputs.

## 6 Discussion

In this work, we proposed a new neural operator-Derivative-enhanced Deep Operator Network (DE-DeepONet) to address the limited accuracy of DeepONet in both function and derivative approximations. Specifically, DE-DeepONet employs a derivative-informed reduced representation of input function and incorporates additional loss into the loss function for the supervised learning of the derivative of the output with respect to the inputs of the branch net. Our experiments for nonlinear PDE problems with high variations in both input and output functions demonstrate that adding this loss term to the loss function greatly enhances the accuracy of both function and derivative approximations, especially when the training data are limited. We also demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator.

We presented matrix-free computation of the derivative label and the derivative-informed dimension reduction for a general form of PDE problems by using randomized algorithms and linearized PDE solves. Thanks to this scalable approach, the computational cost in generating the derivative label data is shown to be only marginally higher than generating the input-output function pairs for the test problems, especially for the more complex Navier-Stokes equations which require more iterations in the nonlinear solves than the hyperelasticity equation.

Limitations:We require the derivative information in the training and dimension reduction using ASM, which may not be available if the explicit form of the PDE is unknown or if the simulation only provides input-output pairs from some legacy code. Another limitation is that dimension reduction of the input function plays a key role in scalable data generation and training, which may not be feasible or accurate for intrinsically very high-dimensional problems such as high frequency wave equations. Such problems are also very challenging and remain unsolved by other methods to our knowledge.