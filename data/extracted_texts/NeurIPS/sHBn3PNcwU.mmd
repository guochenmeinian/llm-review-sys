# EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography

Jehan Yang

Carnegie Mellon University

Pittsburgh, PA 15213

jehan@cmu.edu

&Maxwell Soh

Carnegie Mellon University

Pittsburgh, PA 15213

msoh@andrew.cmu.edu

&Vivianna Lieu

Carnegie Mellon University

Pittsburgh, PA 15213

vlieu@andrew.cmu.edu

&Douglas J Weber

Carnegie Mellon University

Pittsburgh, PA 15213

dweber2@andrew.cmu.edu

&Zackory Erickson

Carnegie Mellon University

Pittsburgh, PA 15213

zerickso@andrew.cmu.edu

These authors contributed equally to this work.

###### Abstract

This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user's intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification, and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets, the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.

## 1 Introduction

Electromyography (EMG) sensors detect muscle and motor neuron activity from the body, allowing for wearable gesture-based control of robots or devices. Particularly, EMG sensors can be used to sense intended hand or other body movements from people who are unable to move parts of their body due to injury or neurodegenerative disease . For people with upper or lower limb amputations, EMG-based prosthetic arms or legs can be controlled using the residual muscles from the remaining limb after amputation . Additionally, for people with paralysis from stroke or spinal cord injury (SCI), EMG sensors can detect motor intent based on residual muscle fiber activity .

Several EMG datasets have been made publicly available, although many of them differ in regards to the hardware used and the placements of the sensors . Due to these common differences between EMG control interfaces, it is important to evaluate multiple EMG datasets to assess the classification accuracy and hardware-agnostic nature of machine learning-based techniques .

However, there is not yet a standardized benchmark for evaluating machine learning algorithm-based classification for EMG. This gap significantly impacts the standardization of results achieved from learning-based classification of EMG datasets, making results from machine learning papers that test on EMG difficult to compare. Furthermore, the absence of benchmarks designed to assess practical generalization and adaptation tasks, particularly those involving inter-subject performance evaluation, represents a notable gap in the current research landscape. In this work, we define and benchmark _generalization_ as the ability of a model to classify the gestures of a subject without using any of their data for training, and _adaptation_ as the ability of a model to personalize by fine-tuning using initial data from a subject after being pretrained on data from other subjects.

Improving performance on out-of-distribution subject generalization and adaptation-based tasks could significantly streamline the setup process for EMG interfaces, making them more accessible and easier to use for new users. For example, by benchmarking generalization tasks such as intersubject classification [13; 14], we can evaluate the performance of algorithms trained on large datasets to generalize to new subjects, enabling new users to control an interface without requiring additional data collection. In addition, by evaluating adaptation by fine-tuning using an initial subset of data from a new user [15; 14], we can determine the minimum amount of labeled data needed from a new user to personalize a model and achieve high gesture recognition accuracy . For the aim of creating a benchmark for out-of-distribution subject generalization and adaptation for EMG, while including some of the most popular EMG datasets, we have also curated a number of datasets that have been demonstrated to have strong performance for out-of-distribution generalization and adaptation [17; 18; 7]. In all, we present the following contributions:

The first open-source EMG benchmarkThis benchmark presents a codebase for standardized evaluation of machine learning models on 9 curated EMG datasets for out-of-domain generalization and adaptation tasks. The codebase is available at https://github.com/jehanyang/emgbench.

New dataset with wearable EMG sensorWe present data using an easy-to-wear, reusable, high-density EMG sensor. Results evaluating the generalizability between subjects in this dataset show high classification accuracy.

Benchmarking results across tasksBenchmarking results for a range of machine learning models and data preprocessing techniques across multiple generalization and adaptation tasks.

Figure 1: **Electrode configurations for various datasets and generalization task.** (Left) We show the main categories of electrode configurations used: dry electrodes and wet electrodes. These categories can be further separated into individually placed electrodes and electrode arrays. (Middle) The electrode configurations and placements used are shown for each dataset. We include nine total EMG datasets. Ninapro DB3 includes subjects with transradial amputations (TA). (Right) We show that voltage signals from the arm are detected using EMG sensors and illustrate that gesture-specific patterns of muscle activity can occur.

Background and Related Work

### EMG signals

Although non-invasive EMG sensors are placed on the skin, these sensors can readily record significant voltage signals caused by the changes in voltage that occur following motor neuron action potentials. Muscles can amplify the biological voltage changes initiated by motor neurons . This is because of the high number of moving ions during muscle fiber action potentials compared to neuronal action potentials, and the high number of muscle fibers activated per motor neuron, ranging from around 100 to as many as 1000 . These changes in voltage signals can be detected by skin electrodes, with both dry and wet electrodes used in common non-invasive EMG devices, as shown in Figure 1. More details about EMG signals are in Appendix A.1.

### EMG as a control interface

EMG sensors have demonstrated the capability to detect signals from the muscles in an amputated forearm , enabling high-dimensional control of prosthetic arms by leveraging residual muscle activity . Furthermore, as an alternative to motion-based interfaces [22; 23], in individuals with clinically motor complete cervical spinal cord injuries resulting in hand paralysis, machine-learning algorithms have shown promise in predicting voluntary hand gesture intentions at the individual finger level, given EMG signals from seven subjects with paralysis .

EMG often faces non-stationary signals that historically have made generalization difficult for learning-based methods. However, these instances present a unique opportunity for domain adaptation methods. EMG signals experience several phenomena that cause **concept shifts**, altering the conditional probability of labels given the inputs from the training set to the test set . The mechanisms causing this phenomenon include variations in muscle locations , arm sizes , skin impedances , and electrode placements . By accounting for some of these mechanisms through fine-tuning or other methods, adaptation can potentially maintain or improve classification performance between subjects.

### Classification over EMG datasets

Extensive research has focused on training machine learning models for EMG-based gesture classification, utilizing both publicly available datasets [18; 29; 30; 31] and novel datasets collected by the researchers [17; 32; 33; 34; 35; 36; 1]. While many studies report results based on randomized train-test split accuracy [37; 31; 36] and k-fold cross-validation (KFCV) [38; 39; 40; 41], where data from the training, validation, and test sets may be randomly sampled from the same subjects, such approaches may not accurately reflect real-world scenarios. In practice, it is often desirable for the validation and test sets to comprise data collected either temporally after the training data from the same subject (termed train-test splits for time series, or TSTS), or from a subject entirely excluded from the training set, as evaluated using leave-one-subject-out cross-validation (LOSO-CV). These data splitting strategies provide more robust assessments of model performance by introducing out-of-distribution generalization challenges. We present a categorization on how several other EMG classification papers split their data in Appendix Table 10.

In the case of TSTS, we test with data collected after the training set, which introduces potential distribution shifts due to factors such as variations in gesture execution, fatigue [42; 43], perspiration , electrode displacement on the skin , drying or changes in ionic concentrations of hydrogel or electrolyte gels , and variations in electrode adherence on the skin . Similarly, LOSO-CV introduces variability stemming from inter-individual differences in body size, muscle morphology , and differences in skin impedance and adipose tissue distribution . Studies employing randomized or mixed data splits, where evaluation data may precede training data, risk reporting artificially inflated accuracies that fail to reflect true generalization capabilities in practical EMG classifier deployments. To facilitate benchmarking, a standardized approach to EMG sample window extraction from raw data can ensure consistent data preprocessing across studies [11; 48]. This methodology enables more reliable comparisons of classification performance. A detailed analysis of prior work on EMG generalization and adaptation is provided in Appendix A.5.

## 3 Datasets

Several datasets have been released for classification of gestures using EMG. Between them, there are a large variety of experimental conditions and data collection protocols: some of these variations include the amount of time and repetitions used in cues for the participant to perform gestures [2; 49; 32; 30], different numbers of participants, and different devices used to collect data [2; 49; 8]. Figure 1 illustrates the sensor configurations and the number of subjects for each dataset, while Table 9 summarizes the amount of time the participant is cued to perform gestures. Further detail about each dataset is included in Appendix A.7 as well as in Table 6.

NinaproOne of the most popular EMG datasets, Ninapro includes over 180 data acquisition sessions and is subdivided into 10 sub-datasets [2; 49]. We include some of the most popular sub-datasets used for benchmarking machine learning algorithms: Ninapro DB2, DB3, and DB5 [49; 11]. The Ninapro DB5 dataset uses two sets of a low-cost wireless wearable EMG device called the Myo Armband  worn on the same arm. Ninapro DB2 and DB3 use individually placed dry electrodes. All users in Ninapro DB3 have transradial amputations .

CappMyoIntroduced in Geng et al.  and further described in Du et al. , the CappMyo dataset includes 3 sub-datasets: DB-a, DB-b, and DB-c. We include DB-b in our benchmark dataset, which has multi-session data. In total, CappMyo DB-b includes 10 subjects performing 8 gestures and data recorded by 128 high-density electrodes, separated into 8 acquisition modules with 16 electrodes per module.

Myo DatasetDescribed in Cote-Allard et al. , the Myo dataset uses the wireless wearable EMG device called the Myo Armband. Like in , we include the evaluation dataset, which has 18 subjects. In this dataset, 7 different gestures are recorded and the methodology for placing the Myo Armband on the arm is specified in detail.

EMG Data for Gestures DatasetThe EMG Data for Gestures (UCI EMG) dataset , hosted on the UC Irvine Machine Learning Repository, has been used to benchmark leave-subject-out tests Lu et al. . The device used is the Myo Armband. This dataset includes 36 subjects and includes 2 sessions, with only 1 gesture performance for each of 6 or 7 gestures performed per session. Although the timespan between the two sessions is not specified, both sessions for each participant are collected on the same day, as indicated by date-based timestamps in the filenames.

Multi-channel sEMG DatasetThe multi-channel sEMG (MCS) dataset presented in Ozdemir et al.  uses 4 bipolar Ag/AgCl electrode channels individually placed on 40 subjects' arms, placed by approximate locations of the specific muscles. Electrolyte gel is placed on the arm under the Ag/AgCl electrodes. The muscles measured from are the extensor carpi radialis, flexor carpi radialis, extensor carpi ulnaris, and flexor carpi ulnaris.

Hyser DatasetThe Hyser dataset, with details specified in Jiang et al. , includes data from 20 subjects and 34 gestures while recording from 256 gelled electrodes separated into 4 grids of high density electrodes, with two grids on the flexor side and two grids on the extensor side. Although there does not seem to be details on the exact materials used in the device, the most common gelled electrode materials used are Ag/AgCl along with a Cl electrolyte gel.

FlexWear-HD DatasetWe present a 13-person EMG dataset using a reusable electrode array called the FlexWear-HD dataset. This array uses a flexible printed circuit board (FPCB) with 64 hydrogel electrodes placed onto gold-plated copper pads on the FPCB. Two sessions are presented per subject, with the time between each session being about 1 hour. The wearable array is also kept on between the two sessions, allowing for the evaluation of typical changes that occur over time on EMG signals without the effects that can occur from replacing the electrode array on the arm.

## 4 Methods

This section outlines our methods for developing and evaluating gesture classification for EMG. We describe preprocessing techniques to convert time-series EMG data into 2D activity maps, the gesture classification models tested, and the generalization tasks for cross-subject and cross-session performance. Additionally, we detail the classification metrics and hardware setup used in our experiments. We note that we always use a constant numbers of epochs for training the classification models.

### Preprocessing Methods

A variety of preprocessing methods have been proposed for model training on EMG data. Given the time-series nature of EMG data, feature extraction methods include root-mean-square (RMS), number of zero-crossings, and mean absolute value . By converting raw time series data or other manual features over time to heatmaps, we are able to create spatiotemporal patterns from time-series data, which can be given as input into 2D CNNs. We convert time series data for each electrode into separate rows in the activity map. Another approach to convert time-series data into a 2D format involves time-frequency transforms, such as spectrograms and continuous wavelet transforms (CWT). In this benchmark, we evaluate preprocessing using 1) heatmaps from raw data, 2) heatmaps from RMS windows, 3) spectrograms, and 4) CWTs. We show examples of these preprocessing methods resulting in activity maps in Figure 5.1. Classification of EMG data using 2D CNNs after preprocessing has achieved high accuracy in prior studies [50; 32]. We review ways that EMG has been processed into activity maps in Appendix A.3.

### State-of-the-Art Image Classifier Algorithms

Several image classifier models have been successfully applied to EMG data [32; 50; 53; 54]. Following the examples of prior EMG classification work by Ozdemir et al.  and Dere and Lee , we evaluate on the ImageNet-pretrained ResNet18, and Visual Transformer (ViT) models [55; 56]. Additionally, we test the EfficientNet and EfficientViT models, which have shown strong performance on ImageNet classification while using fewer model parameters or achieving faster inference compared to other state-of-the-art visual models [57; 58]. The number of parameters used for each model is 2 million for EfficientViT, 4 million for EfficientNet, 6 million for ViT, and 11 million for Resnet18. By utilizing the PyTorch Image Models library in our benchmarking code, we enable other researchers to easily benchmark different visual classifier models by simply modifying the configuration file, with support for both pretrained and untrained models.

Figure 2: **Training pipeline for testing generalization and adaptation. We show the training and evaluation pipeline for our benchmarking script to evaluate generalization across subjects and over time. For _leave-one-subject-out cross validation_ (LOSO-CV), the training set involves all subjects other than the test subject, with the left-out subject \(i\) changing from \(1\) to \(n\) between training runs. In addition, we test for _adaptation for time-series_, where data from the beginning of subject \(i\) is used for fine-tuning after pretraining a model on data from other subjects. In our benchmark, both _few-shot fine-tuning_ and _intersession fine-tuning_ are used to evaluate adaptation for time-series.**

### Different Generalization and Adaptation Tasks

Generalization and adaptation across different setups is essential for robust EMG-based models for real-world deployment. We evaluate two tasks: 1) generalization on a left-out subject, and 2) adaptation using initial data from a subject. For this second task, we fine-tune a pretrained model using initial data from the evaluation subject or the first session from the evaluation subject, respectively.

In task 1, we use LOSO-CV to assess how well an EMG gesture classifier and interface may work for a new user whose data was not included in training. In task 2, we perform TSTS on the data data to assess few-shot learning through fine-tuning, determining the minimal data required from a new subject for robust classification. We also test intersession accuracy to evaluate how well an EMG interface works for a user in a new session without recalibration. Train-test splits for generalization and adaptation are illustrated in Figure 3. More details on the classification metrics and data splits are provided in Appendix A.6.

### Hardware Used for Benchmarking

In our benchmarking experiments, we use GPU nodes from the Pittsburgh Supercomputer Center, which have eight NVIDIA Tesla V100-32GB SXM2 GPUs each. Each node uses two Intel Xeon Gold 6248 "Cascade Lake" CPUs, and 512GB of DDR4-2933 RAM. In total, our tests took around 10,000 GPU hours. Much of the compute required comes from training individual models for each individual subject in order to evaluate average classification metrics across subjects.

## 5 Results

In this section, we present the outcomes of our experiments, highlighting the generalization and adaptation performance of gesture classification models using EMG. We evaluate the effectiveness of various data preprocessing techniques, followed by benchmarking different machine learning architectures. We then vary the amount of data used for fine-tuning, and present results on generalizing and adapting to data that comes from multiple sessions. The results provide insights into the generalization and adaptation tasks across different datasets, offering insight into the robustness and applicability of gesture classifiers in real-world scenarios.

### Data Representation Benchmarking

Generalization and few-shot fine-tuning adaptation results for raw heatmap images, RMS heatmap images, spectrograms, and CWT preprocessing methods are summarized in Table 1. Each value represents the mean gesture recognition performance averaged over all \(N\) models and train-test splits from LOSO-CV, where \(N\) is the number of subjects in a given dataset. Leveraging insights from prior studies [32; 59], which highlighted the effectiveness of an ImageNet pre-trained ResNet model for gesture recognition, we adopted the ImageNet pre-trained ResNet-18 model to benchmark these preprocessing methods.

The experimental workflow involved pretraining the model using data from all training subjects, followed by fine-tuning with the first 20% of data from the left-out evaluation subject. Data splits were stratified by gesture, ensuring balanced representation across the fine-tuning process. A flowchart illustrating this process is provided in Figure 3. Fine-tuning significantly improved test accuracy across all four preprocessing methods, demonstrating the effectiveness of incorporating even small amounts of subject-specific data into the training process.

Performance varied across preprocessing methods and datasets. Raw heatmaps, spectrograms, and CWTs performed well overall, but their relative effectiveness depended on the dataset. For instance, time-frequency transforms significantly outperformed raw heatmaps for the MCS and Hyser datasets. Prior work has not comprehensively compared the performance of raw heatmaps and time-frequency transforms on the Hyser dataset, with Li et al.  focusing solely on raw heatmaps. Similarly, earlier studies on the MCS dataset  evaluated only CWT and STFT preprocessing, neglecting raw and RMS-based methods.

One notable gap in prior research is the exploration of phase information from time-frequency transforms, such as the STFT and Hilbert-Huang transform, for EMG-based gesture classification. Our findings, detailed in Appendix A.8, reveal that phase-based preprocessing methods underperformcompared to magnitude-based approaches like STFT or CWT. This performance disparity may be attributed to the absence of magnitude information, which effectively captures variations in EMG activity across electrodes, as illustrated in Figure 5.1. For all experiments reported in Table 1, models were pretrained over 100 epochs and fine-tuned over 750 epochs.

### Machine Learning Architecture Benchmarking

Evaluating diverse machine learning architectures is crucial for identifying the most effective models for EMG gesture classification, particularly when using heatmaps, spectrograms, or CWTs. We assess both convolutional neural networks (CNNs) and vision transformers (ViTs) due to their distinct strengths: CNNs leverage convolutional and pooling layers to learn locally shift-invariant features , while transformers utilize self-attention mechanisms to capture complex spatial relationships across entire images . Our benchmarks include architectures known for strong performance in EMG classification [32; 53; 59], specifically CNNs [8; 59; 50] and ViTs , pretrained on the ImageNet

    & **Myo Dataset** & **UCI** & **NinaproBBS** & **Capsaps** & **NinaproDB2** & **NinaproDB3** & **MCS** & **Huser** & **FlexWear-HD** \\   \\  Raw & **74.594.8** & 76.434.7 & **41.379.4** & 42.481.0 & 18.462.0 & 11.2572.0 & 67.793.2 & 44.482.1 & **77.7997.4** \\ RMS & 68.792.5 & 75.494.5 & 39.276.1 & 41.881.4 & 17.260.1 & 11.2572.8 & 58.28.0 & 48.5783.5 & 75.6597.2 \\ STFT & 72.894.3 & **78.295.6** & 41.377.81 & 41.980.4 & 19.262.6 & **12.652.8** & 74.495.6 & 54.4889.9 & 75.396.8 \\ CVT & 69.792.6 & 75.941.4 & 38.277.2 & **42.981.5** & **20.183.2** & 10.951.6 & **77.996.7** & **58.090.4** & 75.1097.2 \\   \\  Raw & **95.199.5** & 91.6699.2 & **63.394.1** & **92.899.5** & 52.286.0 & 44.280.0 & 89.599.8 & 79.1960.0 & 95.209.6 \\ RMS & 94.199.3 & 91.289.2 & 67.593.4 & 89.499.0 & 49.484.5 & 40.677.5 & 78.095.6 & 84.8599.1 & 95.7599.7 \\ STFT & 95.099.5 & **91.799.1** & 66.693.1 & 92.299.5 & **53.186.5** & **44.780.2** & 90.999.9 & 89.499.6 & **95.8599.7** \\ CVT & 92.599.0 & **91.795.8** & 62.191.0 & 89.799.2 & 51.985.9 & 43.47/9.6 & **92.199.1** & **90.395.8** & 95.6599.8 \\   

Table 1: **Benchmark of EMG Preprocessing Techniques. LOSO-CV average test accuracy (Acc) / area under the receiver operating characteristic (AUROC) for all the datasets and for four common EMG-to-image preprocessing methods: original temporal EMG signals (Raw), root-mean-square (RMS), short-time fourier transform (STFT), and continuous wavelet transform (CWT), as mentioned in Section 4.1. RMS consists of taking consecutive windows of length 16-20 timesteps and applying the RMS transform.**

Figure 3: **Varying preprocessing methods for generating heatmaps. Samples from different preprocessing methods are shown for the 128 electrode CapsMyo dataset, the 12 electrode NinaPro DB2, and the 8 electrode Myo Dataset. Values on the heatmaps correspond to the index of the electrode for the sub-image shown on the grid. All samples correspond to the closed-hand gesture. The closed-hand gesture primarily activates deep muscles of the flexor side of the forearm, such as the flexor digitorum superficialis.**

dataset  with all weights unfrozen during training. Pretraining is conducted over 50 epochs, followed by fine-tuning over 375 epochs.

For each dataset, we select the best-performing preprocessing method from Table 1 (highlighted in bold) and evaluate leave-one-subject-out cross-validation (LOSO-CV) and few-shot transfer learning using four image classifiers: 1) ResNet18, 2) EfficientNet, 3) ViT, and 4) EfficientViT. All models are initialized from an ImageNet-1k pretrained baseline .

Results reveal that ResNet18 achieves the best performance on Ninapro DB5, Ninapro DB2, and MCS. EfficientNet outperforms others on CapsMyo, UCI, Ninapro DB3, and FlexWear-HD, while EfficientViT performs best on the Myo Dataset and Hyser. Notably, EfficientNet and EfficientViT have not previously been applied to EMG signal classification but are recognized for their state-of-the-art performance and inference speed in computer vision tasks [58; 57].

Tables 1 and 2 indicate that datasets utilizing array-based wearable electrode devices, such as the Myo Dataset, UCI EMG dataset, and FlexWear-HD, generally exhibit superior performance in LOSO-CV compared to datasets requiring individually placed electrodes or uniform electrode grids, as seen in CapsMyo and Hyser. Exceptions include the Ninapro DB5 and MCS datasets. For Ninapro DB5, despite its use of two Myo Armband devices, classification performance is lower, potentially due to noisy labels , which may artificially suppress generalization performance. Conversely, the MCS dataset achieves relatively high LOSO-CV performance despite having the fewest electrodes. This may be attributed to its electrode placement methodology, where expert researchers position individual electrodes on a few specific muscles, potentially reducing concept shifts across subjects . However, this approach is challenging to replicate for end-users who must independently don the devices. Additionally, it significantly increases the donning time compared to using a single, integrated device.

### Varying Amount of Data Available for Training

Using the best-performing preprocessing method identified in Table 1 and the corresponding best classifier architecture from Table 2 for each dataset, we evaluate adaptation methods, as presented in Table 3. For the FT-\(X\%\) rows, fine-tuning is performed using the first \(X\%\) of data from the left-out subject after pretraining the model on data from all other subjects. Gesture stratification ensures a balanced fine-tuning set. By varying the amount of adaptation data, we assess the data requirements for achieving significant performance gains.

Our findings indicate that performance generally improves with more fine-tuning data from the left-out subject. Notably, for the Myo Dataset and FlexWear-HD, performance reaches 93.1% for 7 gestures and 94.2% for 10 gestures, respectively, with just 5% of the subject's initial data. This corresponds to approximately eight seconds of data for the Myo Dataset and 30 seconds for FlexWear-HD. These results highlight that minimal data collection from a new subject is sufficient for achieving high classification accuracy when fine-tuning a model pretrained on a large dataset of training subjects. On average, accuracy improves by 44% compared to the pretrained model after fine-tuning with just 5% of the subject's data. This aligns with findings from Sussillo et al. , who reported a 30% performance improvement on EMG tasks after model personalization via fine-tuning. For Table 3,

    & **Myo Dataset** & **UCI** & **NinaproDB5** & **CapsMyo** & **NinaproDB2** & **NinaproDB3** & **MCS** & **Hyser** & **FlexWear-HD** \\   \\  RN18 & 73.794.9 & 76.695.0 & **42.178.9** & 41.482.9 & **19.963.0** & 11.151.6 & **77.896.6** & 56.689.1 & 77.097.4 \\ EN & 73.594.2 & **76.695.3** & 39.978.6 & **46.682.4** & 19.563.0 & **11.665.3** & 76.964.4 & 58.690.7 & **83.998.5** \\ ViT & 70.893.2 & 77.395.1 & 38.977.7 & 39.873.9 & 18.061.1 & 11.151.3 & 73.966.0 & 53.638.8 & 70.209.63 \\ EVT & 73.8**93.3** & 77.294.4 & 40.978.7 & 45.082.8 & 18.860.2 & 11.151.3 & 76.296.2 & **61.892.2** & 74.996.3 \\   \\  RN18 & 94.699.5 & 91.499.0 & 68.593.6 & **91.299.3** & 52.485.8 & **45.281.3** & 91.059.0 & **87.498.4** & 95.959.8 \\ EN & **94.999.69** & **91.799.1** & 67.779.3 & 89.796.6 & 53.837.1 & 45.181.0 & 91.059.0 & 55.497.8 & **96.999.8** \\ ViT & 94.999.4 & 90.198.1 & 68.794.2 & 85.189.5 & 51.089.61 & 41.278.5 & 91.498.9 & 70.693.3 & 93.699.5 \\ EVT & 94.999.4 & **91.799.1** & **69.294.1** & 87.198.3 & **54.086.9** & 44.881.4 & **92.098.8** & 83.296.6 & 95.399.6 \\   

Table 2: **Benchmarking of Machine Learning Architectures. Performance of gesture recognition models on each dataset (Acc/AUROC). The models used are the ImageNet-pretrained ResNet18 (RN18), EfficientNet (EN), ViT, and EfficientViT (EViT).**

[MISSING_PAGE_FAIL:9]

### Domain Generalization Algorithms

We test additional training algorithms, which have been tested in previous work on domain generalization, to compare how well they work compared to standard supervised learning [62; 63]. In these experiments, we again use the best performing preprocessing method and model architecture for the dataset, but include the use of invariant risk minimization (IRM), and correlation alignment (CORAL) [62; 63]. We test these algorithms that take into account the different domains during training: 1) IRM minimizes a loss that penalizes models where the optimal classifier differs across domains, and 2) CORAL aligns the covariances of the feature representations between domains through a loss term. The resulting generalization and few-shot fine-tuning results are shown in Appendix A.9. Overall performance is comparable to with training using standard cross-entropy loss, which is a similar result found in Gulrajani and Lopez-Paz  and Koh et al. .

## 6 Limitations and Future Work

The current benchmarking studies in this work have not evaluated EMG classification performance with activity maps exceeding a resolution of 224x224 pixels. Higher resolution activity maps may enhance inter-subject performance when employing spectrogram or continuous wavelet transform (CWT) image preprocessing techniques, particularly for datasets with a greater number of electrodes. This is due to the potential reduction in downsampling of the resulting time-frequency transformed images for each electrode, thereby preserving specific features within the transformed data.

Among the datasets analyzed (Table 2), the FlexWear-HD dataset achieved the highest LOSO-CV accuracy, registering an 83.9% test accuracy for 10-class classification. While robust generalization to new subjects remains a significant challenge for EMG-based control interfaces , future work leveraging larger datasets and models holds promise for developing a universal EMG classifier. This classifier could rapidly generalize or adapt to EMG data from new subjects, enabling a more reliable deployment of EMG-based control systems.

Datasets such as CappMay, Hyser, Myo Dataset, and FlexWear-HD do not present transition data between gestures, likely due to the complexity of labeling short, dynamic transitions in gesture-based classification systems. Notably, in prior work with FlexWear-HD , users successfully controlled robots in real time using classified EMG data streams, even without transition data in the training process. For datasets that do include transition data, such as Ninapro, MCS, and UCI EMG, Table 5 presents results where transition windows are also classified.

Currently, none of these datasets capture gesture data performed during real-world activities, where users interact with computers or robots outside controlled, cue-based environments. Developing a dataset with ground-truth labels for spontaneously performed gestures in natural settings would address this gap. Such a dataset could better represent the variability of real-world gestures, ultimately enhancing the adaptability and performance of EMG-based gesture classification systems.

## 7 Conclusion

This study introduces a new tool for benchmarking EMG datasets, providing insights into the effectiveness of various classification methods, EMG preprocessing techniques, potential for real-world applications, and open research problems in learning-based EMG gesture classification for the research community. The performance of these methods shows promising results for enhancing the generalization of EMG-based systems, especially through a standardized format to compare performances metrics. While benchmarking, we find that adaptation using data from the validation subject can significantly enhance performance, requiring only a small amount of data from the subject.