# Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem

Jincheng Cao

ECE Department

UT Austin

jinchengcao@utexas.edu

&Ruichen Jiang

ECE Department

UT Austin

rjiang@utexas.edu

&Nazanin Abolfazli

SIE Department

The University of Arizona

nazaninabolfazli@arizona.edu

&Erfan Yazdandoost Hamedani

SIE Department

The University of Arizona

erfany@arizona.edu

&Aryan Mokhtari

ECE Department

UT Austin

mokhtari@austin.utexas.edu

###### Abstract

In this paper, we study a class of stochastic bilevel optimization problems, also known as stochastic simple bilevel optimization, where we minimize a smooth stochastic objective function over the optimal solution set of another stochastic convex optimization problem. We introduce novel stochastic bilevel optimization methods that locally approximate the solution set of the lower-level problem via a stochastic cutting plane, and then run a conditional gradient update with variance reduction techniques to control the error induced by using stochastic gradients. For the case that the upper-level function is convex, our method requires stochastic oracle queries to obtain a solution that is \(_{f}\)-optimal for the upper-level and \(_{g}\)-optimal for the lower-level. This guarantee improves the previous best-known complexity of \((\{1/_{f}^{4},1/_{g}^{4}\})\). Moreover, for the case that the upper-level function is non-convex, our method requires at most \(}(\{1/_{f}^{3},1/_{g}^{3}\})\) stochastic oracle queries to find an \((_{f},_{g})\)-stationary point. In the finite-sum setting, we show that the number of stochastic oracle calls required by our method are \(}(/)\) and \(}(/^{2})\) for the convex and non-convex settings, respectively, where \(=\{_{f},_{g}\}\).

## 1 Introduction

An important class of bilevel optimization problems is simple bilevel optimization in which we aim to minimize an upper-level objective function over the solution set of a lower-level problem . Recently this class of problems has attracted great attention in machine learning society due to their applications in continual learning , hyper-parameter optimization , meta-learning , and reinforcement learning . Motivated by large-scale learning problems, in this paper, we are particularly interested in the stochastic variant of the simple bilevel optimization where the upper and lower-level objective functions are the expectations of some random functions with unknown distributions and are accessible only through their samples. Hence, the computation of the objective function values or their gradients is not computationally tractable. Specifically, we focus on the _stochastic simple bilevel problem_ defined as

\[_{^{d}} f()=[( ,)]*{arg\, min}_{}g()=[(, )],\] (1)where \(\) is compact and convex and \(,:^{d}\) are continuously differentiable functions on an open set containing \(\), and \(\) and \(\) are some independent random variables drawn from some possibly unknown probability distributions. As a result, the functions \(f,g:^{d}\) are also continuously differentiable functions on \(\). We assume that \(g\) is convex but not necessarily strongly convex, and hence the solution set of the lower-level problem in (1) is in general not a singleton. We also study the finite sum version of the above problem where both functions can be written as the average of \(n\) component functions, i.e., \(f()=(1/n)_{i=1}^{n}(,_{i})\) and \(g()=(1/n)_{i=1}^{n}(,_{i})\).

The main challenge in solving problem (1), which is inherited from its deterministic variant, is the absence of access to the feasible set, i.e., the lower-level solution set. This issue eliminates the possibility of using any projection-based or projection-free methods. There have been some efforts to overcome this issue in the deterministic setting (where access to \(f\) and \(g\) and their gradients is possible), including , however, there is little done on the stochastic setting described above. In fact, the only work that addresses the stochastic problem in (1) is , where the authors present an iterative regularization-based stochastic extra gradient algorithm and show that it requires \((1/_{f}^{4})\) and \((1/_{g}^{4})\) queries to the stochastic gradient of the upper-level and lower-level function, respectively, to obtain a solution that is \(_{f}\)-optimal for the upper-level and \(_{g}\)-optimal for the lower-level. We improve these bounds and also extend our results to nonconvex settings.

**Contributions.** In this paper, we present novel projection-free stochastic bilevel optimization methods with tight non-asymptotic guarantees for both upper and lower-level problems. At each iteration, the algorithms use a small number of samples to build unbiased and low variance estimates and construct a cutting plane to locally approximate the solution set of the lower-level problem and then combine it with a Frank-Wolfe-type update on the upper-level objective. Our methods require careful construction of the cutting plane so that with high probability it contains the solution set of the lower-level problem, which is obtained by selecting proper function and gradient estimators to achieve the obtained optimal convergence guarantees. Next, we summarize our main theoretical results for the proposed Stochastic Bilevel Conditional Gradient methods for Infinite and Finite sample settings denoted by SBCGI and SBCGF, respectively.

* (Stochastic setting) We show that SBCGF (Algorithm 1), in the convex setting, finds a solution \(}\) that satisfies \(f(})-f^{*}_{f}\) and \(g(})-g^{*}_{g}\) with probability \(1-\) within \(((d/)/^{2})\) stochastic oracle queries, where \(=\{_{f},_{g}\}\), \(f^{*}\) is the optimal value of problem (1) and \(g^{*}\) is the optimal value of the lower-level problem. Moreover, in the non-convex setting, it finds \(}\) satisfying \((})_{f}\) and \(g(})-g^{*}_{g}\) with probability \(1-\) within \((((d/))^{3/2}/^{3})\) stochastic oracle queries, where \((})\) is the Frank-Wolfe (FW) gap.
* (Finite-sum setting) We show that SBCGF (Algorithm 2), in the convex setting, finds \(}\) that satisfies \(f(})-f^{*}_{f}\) and \(g(})-g^{*}_{g}\) with probability \(1-\) within \((((1/))^{3/2}/)\) stochastic oracle queries, where \(n\) is the number of samples of finite-sum problem. Moreover, in the nonconvex setting, it finds \(}\) that satisfies \((})_{f}\) and \(g(})-g^{*}_{g}\) with probability \(1-\) within \(((1/)/^{2})\) stochastic oracle queries.

    &  &  &  &  &  \\    & & Objective & & & & & \\  aR-IP-SeG  & Stochastic & C, Lipschitz & C, Lipschitz & Closed & \((\{1/_{f}^{4},1/_{g}^{4}\})\) & \((1/^{4})\) \\ 
**Algorithm1** & Stochastic & C, smooth & C, smooth & Compact & \(}(\{1/_{f}^{2},1/_{g}^{2}\})\) & \(}(1/^{2})\) \\ 
**Algorithm1** & Stochastic & NC, smooth & C, smooth & Compact & \(}(\{1/_{f}^{3},1/_{g}^{3}\})\) & \(}(1/^{3})\) \\ 
**Algorithm2** & Finite-sum & C, smooth & C, smooth & Compact & \(}(\{1/_{f},1/_{g}\})\) & \(}(/)\) \\ 
**Algorithm2** & Finite-sum & NC, smooth & C, smooth & Compact & \(}(\{1/_{f}^{2},1/_{g}^{2}\})\) & \(}(/^{2})\) \\   

Table 1: Results on stochastic simple bilevel optimization. The abbreviations “SC”, “C”, and “NC” stand for “strongly convex”, “convex”, and “non-convex”, respectively. Note that \(=\{_{f},_{g}\}\)

### Related work

**General stochastic bilevel.** In a general format of stochastic bilevel problems, the upper-level function \(f\) also depends on an extra variable \(y^{p}\) which also affects the lower-level objective,

\[_{^{d},^{p}}\ f( ,)=[(,,)] *{arg\,min}_{ }g(,)=[(,,)].\] (2)

There have been several works including [10; 17; 18; 19; 20; 21] on solving the general stochastic bilevel problem (2). However, they only focus on the setting where the lower-level problem is strongly convex, i.e., \(g(,)\) is strongly convex with respect to \(\) for any value of \(\). In fact, (2) with a convex lower-level problem is known to be NP-hard . Hence, the results of these works are not directly comparable with our work as we focus on a simpler setting, but our assumption on the lower-level objective function is weaker and it only requires the function to be convex.

**Deterministic simple bilevel.** There have been some recent results on non-asymptotic guarantees for the deterministic variant of problem (1). The BiG-SAM algorithm was presented in , and it was shown that its lower-level objective error converges to zero at a rate of \((1/t)\), while the upper-level error asymptotically converges to zero. In , the authors achieved the first non-asymptotic rate for both upper- and lower-level problems by introducing an iterative regularization-based method which achieves an \((_{f},_{g})\)-optimal solution after \((\{1/_{f}^{4},1/_{g}^{4}\})\) iterations. In , the authors proposed a projection-free method for deterministic simple bilevel problems that has a complexity of \((\{1/_{f},1/_{g}\})\) for convex upper-level and complexity of \((\{1/_{f}^{2},1/(_{f}_{g})\})\) for non-convex upper-level. Moreover, in  the authors presented a switching gradient method to solve simple bilevel problems with convex smooth functions for both upper- and lower-level problems with complexity \((1/)\). However, all the above results are limited to the deterministic setting.

**General bilevel without lower-level strong convexity.** Recently, there are several recent works on general bilevel optimization problems without lower-level strong convexity including [24; 25; 26; 27; 23]. However, they either have a weaker theoretical results like asymptotic convergence rate in  or have some additional assumptions. Specifically, in , the authors reformulated the problem as a constrained optimization problem and further assumes such problem to be a convex program. Moreover, in [26; 27], the authors in both papers assumed that the lower-level objective satisfies the PL inequality, while we assumed that the lower-level objective is convex. In , the authors used a looser convergence criterion that only guarantees convergence to a Goldstein stationary point. Since these works consider a more general class of problems, we argue that their theoretical results when applied to our setting are necessarily weaker.

## 2 Preliminaries

### Motivating examples

_Example 1: Over-parameterized regression._ A general form of problem (1) is when the lower-level problem represents training loss and the upper-level represents test loss. The goal is to minimize the test loss by selecting one of the optimal solutions for the training loss . An instance of that is the constrained regression problem, where we intend to find an optimal parameter vector \(^{d}\) that minimizes the loss \(_{}()\) over the training dataset \(_{}\). To represent some prior knowledge, we usually constrain \(\) to be in some subsets \(^{d}\), e.g., \(=\{\ |\ \|\|_{1}\}\) for some \(>0\) to induce sparsity. To handle multiple global minima, we adopt the over-parameterized approach, where the number of samples is less than the parameters. Although achieving one of these global minima is possible, not all optimal solutions perform equally on other datasets. Hence, we introduce an upper-level objective: the loss on a validation set \(_{}\,.\) This helps select a training loss optimizer that performs well on both training and validation sets. It leads to the following bilevel problem:

\[_{^{d}} f()_{}()\ *{arg\,min}_{}\,g( )_{}()\] (3)

In this case, both the upper- and lower-level losses are smooth and convex if \(\) is smooth and convex.

_Example 2: Dictionary learning._ Problem (1) also appears in lifelong learning, where the learner takes a series of tasks sequentially and tries to accumulate knowledge from past tasks to improve performance in new tasks. Here we focus on continual dictionary learning. The aim of dictionarylearning is to obtain a compact representation of the input data. Let \(=\{_{1},,_{n}\}^{m n}\) denote a dataset of \(n\) points. We seek a dictionary \(=[_{1},,_{p}]^{m p}\) such that all data points \(_{i}\) can be represented by a linear combination of basis vectors in \(\) which can be cast as [28; 29; 30; 31]:

\[_{^{m p}}_{^{p  n}}_{i}\|_{i}- _{i}\|_{2}^{2}\|_{j}\|_{2} 1,j=1, ,p;\|_{i}\|_{1},i.\] (4)

Moreover, we denote \(=[_{1},,_{n}]^{p n}\) as the coefficient matrix. In practice, data points usually arrive sequentially and the representation evolves gradually. Hence, the dictionary must be updated sequentially as well. Assume that we already have learned a dictionary \(}^{m p}\) and the corresponding coefficient matrix \(}^{p n}\) for the dataset \(\). As a new dataset \(^{}=\{^{}_{1},,^{}_{n^ {}}\}\) arrives, we intend to enrich our dictionary by learning \(}^{m q}(q>p)\) and the coefficient matrix \(}^{q n^{}}\) for the new dataset while maintaining good performance of \(}\) on the old dataset \(\) as well as the learned coefficient matrix \(}\). This leads to the following stochastic bilevel problem:

\[_{^{m q}}_{^{q  n^{}}}f(},})\|}_{k}\|_{1},k=1, ,n^{};\ }*{argmin}_{ \|}_{j}\|_{2} 1}g(}),\] (5)

where \(f(},})}_{ k=1}^{n^{}}\|^{}_{k}-}}_{k}\|_{2}^{2}\) represents the average reconstruction error on the new dataset \(^{}\), and \(g(})_{i=1}^{n}\|_{i }-}}_{i}\|_{2}^{2}\) represents the error on the old dataset \(\). Note that we denote \(}_{i}\) as the prolonged vector in \(^{q}\) by appending zeros at the end. In problem (5), the upper-level objective is non-convex, while the lower-level loss is convex with multiple minima.

### Assumptions and definitions

Next, we formally state the assumptions required in this work.

**Assumption 2.1**.: \(\) _is convex and compact with diameter \(D\), i.e., \(,,\) we have \(\|-\| D.\)_

**Assumption 2.2**.: _The upper-level stochastic function \(\) satisfies the following conditions:_

1. \(\) _is Lipschitz with constant_ \(L_{f}\)_, i.e.,_ \(,,\)_,_ \(\|(,)-(,)\| L _{f}\|-\|.\)__
2. _The stochastic gradients noise is sub-Gaussian,_ \([\{\|(,)- f()\|^ {2}/_{f}^{2}\}]\{1\}.\)__

**Assumption 2.3**.: _The lower-level stochastic function \(\) satisfies the following conditions:_

1. \(g\) _is convex and_ \(\) _is_ \(L_{g}\)_-Lipschitz, i.e.,_ \(,,\)_,_ \(\|(,)-(,)\| L_{g} \|-\|.\)__
2. _The stochastic gradients noise is sub-Gaussian,_ \([\{\|(,)- g()\|^{2} /_{g}^{2}\}]\{1\}.\)__
3. _The stochastic functions noise is sub-Gaussian,_ \([\{\|(,)-g()\|^{2}/_{f}^{2} \}]\{1\}.\)__

_Remark 2.1_.: Assumptions 2.1 and 2.3(i) imply that \(\) is Lipschitz continuous on an open set containing \(\) with some constant \(L_{l}\), i.e., for all \(,,\) we have \(|()-()| L_{l}\|-\|.\)

In the paper, we denote \(g^{*}_{}g()\) and \(^{*}_{g}_{}g()\) as the optimal value and the optimal solution set of the lower-level problem, respectively. Note that by Assumption 2.3, the set \(^{*}_{g}\) is nonempty, convex, and compact, but typically not a singleton as \(g\) potentially has multiple minima on \(\). Furthermore, we denote \(f^{*}\) and \(^{*}\) as the optimal value and an optimal solution of problem (1), which are assured to exist since \(f\) is continuous and \(^{*}_{g}\) is compact.

**Definition 2.1**.: _When \(f\) is convex, a point \(}\) is \((_{f},_{g})\)-optimal if \(f(})-f^{*}_{f}\) and \(g(})-g^{*}_{g}\). When \(f\) is non-convex, \(}\) is \((_{f},_{g})\)-optimal if \((})_{f}\) and \(g(})-g^{*}_{g}\), where \((})\) is the FW gap [32; 33] defined as \((})_{^{*}_{g}}\{  f(}),}-\}.\)_

## 3 Algorithms

**Conditional gradient for simple bilevel optimization.** A variant of the conditional gradient (CG) method for solving bilevel problems has been introduced in  which uses a cutting plane idea to approximate the solution set of the lower-level problem denoted by \(_{g}^{*}\). More precisely, if one has access to \(_{g}^{*}\), it is possible to run the FW update with stepsize \(_{t}\) as

\[_{t+1}=(1-_{t})_{t}+_{t}_{t}, _{t}=*{arg\,min}_{ _{g}^{*}} f(_{t}),s\] (6)

However, the set \(_{g}^{*}\) is not explicitly given and the above method is not implementable. In , the authors suggested the use of the following set: \(_{t}=\{: g(_{t}), -_{t} g(_{0})-g(_{t})\}\) instead of the set \(_{g}^{*}\) in the FW update given in (6). Note that \(_{0}\) is selected in a way that \(g(_{0})-g^{*}\) is smaller than \(_{g}/2\), and such a point can be efficiently computed. A crucial property of the above set is that it always contains the solution set of the lower-level problem denoted by \(_{g}^{*}\). This can be easily verified by the fact that for any \(_{g}^{*}\) in \(_{g}^{*}\) we have \(_{g}^{*}\) and

\[ g(_{t}),_{g}^{*}-_{t} g (_{g}^{*})-g(_{t}) g(_{0})-g(_{t}),\] (7)

where the second inequality holds as \(g(_{0}) g(_{g}^{*})\). As shown in , this condition is sufficient to show that if one follows the update in (6) with \(}_{t}\) instead of \(_{g}^{*}\), the iterates will converge to the optimal solution. However, this framework is not applicable to the stochastic setting as we cannot access the functions or their gradients. Next, we present our main idea to address this delicate issue.

**Random set for the subproblem.** A natural idea to address stochasticity is to replace all gradients and functions with their stochastic estimators for both the subproblem in (6), i.e., \( f(_{t})\), as well as the construction of the cutting plane \(_{t}\), i.e., \(g(_{t})\), and \( g(_{t})\). However, this simple idea fails since the set \(_{t}\) may no longer contain the solution set \(_{g}^{*}\). More precisely, if \(_{t}\) and \(_{t}\) are unbiased estimators of \(g(_{t})\) and \( g(_{t})\), respectively, for the following approximation set

\[_{t}^{}=\{:_{t},-_{t} g(_{0})-_{t}\}\] (8)

we can not argue that it contains \(_{g}^{*}\), as the second inequality in (7) does not hold, i.e., \((_{t}),_{g}^{*}-_{t} (_{g}^{*})-(_{t}) g( _{0})-(_{t})\). In the appendix, we numerically illustrate this point.

To address this issue, we tune the cutting plane by only moving it but not rotating it, i.e., adding another term to tolerate the noise from stochastic estimates. We introduce the stochastic cutting plane

\[}_{t}=\{:_{ t},-_{t} g(_{0})-_{t}+K_{t}\},\] (9)

where \(_{t}\) and \(_{t}\) are gradient and function value estimators, respectively, that we formally define later. In the above expression, the addition of the term \(K_{t}\), which is a sequence of constants converging to zero as \(t\), allows us to ensure that with high probability the random set \(}_{t}\) contains all optimal solutions of the lower-level problem. Choosing suitable values for the sequence \(K_{t}\) is a crucial task. If we select a large value for \(K_{t}\) then the probability of \(}_{t}\) containing \(_{g}^{*}\) goes up at the price allowing points with larger values \(g\) in the set. As a result, once we perform an update similar to the one in (6), the lower-level function value could increase significantly. On the other hand, selecting small values for \(K_{t}\) would allow us to show that the lower level objective function is not growing, while the probability of \(}_{t}\) containing \(_{g}^{*}\) becomes smaller which could even lead to a case that the set becomes empty and the bilevel problem becomes infeasible.

_Remark 3.1_.: How to compute \(g(_{0})\)? In the finite sum setting, we can accurately compute \(g(_{0})\), and the additional cost of \(n\) function evaluations will be dominated by the overall complexity. In the stochastic setting, we could use a large batch of samples to compute \(g(_{0})\) with high precision at the beginning of the process. This additional operation will not affect the overall sample complexity of the proposed method, as the additional cost is negligible compared to the overall sample complexity. Specifically, we need to take a batch size of \(b=}(^{-2})\) to estimate \((_{0})\). Using the Hoeffding inequality for subgaussian random variables, we have the following bound: \(|(_{0})-g(_{0})|_{l}(T+1)^{- /2}\), with a probability of at least \(1-\), where \(T\) is the maximum number of iterations. Comparing this with Lemma 4.1.3, we can further derive: \(|(_{0})-g(_{0})|_{l}(T+1)^{- /2}(2L_{l}D+}{3^{}- 1}_{l})(t+1)^{-/2}\), with a probability of at least \(1-\) for all \(0 t T\). Consequently, the introduced error term would be absorbed in \(K_{0,t}\) and will not affect any parts of the analysis.

**Variance reduced estimators.** As mentioned above, a key point in the design of our stochastic bilevel algorithms is to select \(K_{t}\) properly such that \(}_{t}\) contains \(_{g}^{*}\) with high probability, for all \(t 0\). Toachieve such a guarantee, we first need to characterize the error of our gradient and function value estimators. More precisely, suppose that for our function estimator we have that \(P(|_{t}-g(_{t})| K_{0,t}) 1-^{{}^{}}\) and for the gradient estimator we have \(P(\|g_{t}- g(_{t})\| K_{1,t}) 1-^{{}^{ }}\), for some \(^{{}^{}}(0,1)\). Then, by setting \(K_{t}=K_{0,t}+DK_{1,t}\), we can guarantee that the conditions required for the inequalities in (7) hold with probability at least \((1-2^{{}^{}})\).

Using simple sample average estimators would not allow for the selection of a diminishing \(K_{t}\), as the variance is not vanishing, but by using proper variance-reduced estimators the variance of the estimators vanishes over time and eventually, we can send \(K_{t}\) to zero. In this section, we focus on two different variance reduction estimators. For the stochastic setting in (1) we use the STOchastic Recursive Momentum estimator (STORM), proposed in , and for the finite-sum setting, we utilize the Stochastic Path-Integrated Differential EstimatoR (SPIDER) proposed in . If \(_{t-1}\) is the gradient estimator of STORM at time \(t-1\), the next estimator is computed as

\[_{t}=(1-_{t})_{t-1}+(_{t}, _{t})-(1-_{t})(_{t-1},_{t}),\] (10)

where \((,)\) is the stochastic gradient evaluated at \(\) with sample \(\). The main advantage of the above estimator is that it can be implemented even with one sample per iteration. Unlike STORM, for the SPIDER estimator, we need a larger batch of samples per update. More precisely, if we consider \(_{t-1}\) as the estimator of SPIDER for \( f(_{t})\), it is updated according to

\[_{t}= f_{}(_{t})- f_{}( _{t-1})+_{t-1},\] (11)

where \( f_{}()=(1/S)_{i} f( ,_{i})\) is the average sub-sampled stochastic gradient computed using samples that are in the set \(\). As we will discuss later, in the finite sum case that we use SPIDER, the size of batch \(S\) depends on \(n\) which is the number of component functions. We delay establishing a high probability error bound for these estimators to section 4.1.

### Conditional gradient algorithms with random sets: stochastic and finite-sum

Next, we present our Stochastic Bilevel Conditional Gradient method for Infinite sample case abbreviated by SBCGI for solving (1) and its finite sum variant denoted by SBCGF. In both cases, we first find a point \(_{0}\) that satisfies \(g(_{0})-g^{*}_{g}/2\), for some accuracy \(_{g}\). The cost of finding such a point is negligible compared to the cost of the main algorithm as we discuss later. At each iteration \(t\), we first update the gradient estimator of the upper-level and the function and gradient estimators of the lower-level problem. In SBCGI, we follow the STORM idea as described in steps 4-6 of Algorithm 1, while in SBCGF, we use the SPIDER technique as presented in steps 7-10 of Algorithm 2.In the case of SBCGF, we need to compute the exact gradient and function values once every \(q\) iteration as presented in steps 4-6 of Algorithm 2. Once the estimators are updated, we can define the random set \(}_{t}\) as in (9) and solve the following subproblem over the set \(}_{t}\),

\[_{t}=*{arg\,min}_{}_{t}} _{t},,\] (12)where \(_{t}\) is the unbiased estimator of \( f(_{t})\). Note that we implicitly assume that we have access to a linear optimization oracle that returns a solution of the subproblem in (12), which is standard for projection-free methods [32; 33]. In particular, if \(\) can be described by a system of linear inequalities, then problem (12) corresponds to a linear program and can be solved by a standard solver as we show in our experiments. Once, \(_{t}\) is calculated we simply update the iterate

\[_{t+1}=(1-_{t+1})_{t}+_{t+1}_{t}\] (13)

with stepsize \(_{t+1}\). The only missing part for the implementation of our methods is the choice of \(K_{t}\) in the random set and the stepsize parameters. We address these points in the next section.

_Remark 3.2_.: SBCGI can be implemented with a batch size as small as \(S=1\). However, this does not imply that the batch size "has to be" \(S=1\). In other words, the main advantage of SBCGI, compared to SBCGF, is its capability to be implemented with any mini-batch size, even as small as \(S=1\). Therefore, for SBCGI, the batch size can be set arbitrarily, whereas for SBCGF, it must be \(\).

_Remark 3.3_.: In the finite-sum setting, if the numbers of functions in the upper- and lower-level losses are different, we could simply modify SBCGF 2 by choosing \(S_{u}=q_{u}=}\) and \(S_{l}=q_{l}=}\), where \(n_{u}\) and \(n_{l}\) are the number of functions in the upper- and lower-level, respectively.

## 4 Convergence analysis

In this section, we characterize the sample complexity of our methods for stochastic and finite-sum settings. Before stating our results, we first characterize a high probability bound for the estimators of our algorithms, which are crucial in the selection of parameter \(K_{t}\) and the overall sample complexity.

### High probability bound for the error terms

To achieve a high probability bound, it is common to assume that the noise of gradient or function is uniformly bounded as in [36; 37], but such assumptions may not be realistic for most machine learning applications. Hence, in our analysis, we consider a milder assumption and assume the noise of function and gradient are sub-Gaussian as in Assumptions 2.2 and 2.3, respectively. Given these assumptions, we next establish a high probability error bound for the estimators in SBCGI.

**Lemma 4.1**.: _Consider SBCGI in Algorithm 1 with parameters \(_{t}=_{t}=_{t}=_{t}=1/(t+1)^{}\) where \((0,1]\). If Assumptions 2.1, 2.2, and 2.3 are satisfied, for any \(t 1\) and \((0,1)\), with probability at least \(1-\), for some absolute constant \(\), (\(d\) is the number of dimension), we have_

\[_{t}- f(_{t})  c(2L_{f}D+}{3^{}-1}_{f})( t+1)^{-/2},\] (14) \[\|_{t}- g(_{t})\|  c(2L_{g}D+}{3^{}-1}_{g}) (t+1)^{-/2},\] (15) \[|_{t}-g(_{t})|  c(2L_{l}D+}{3^{}-1}_{l}) (t+1)^{-/2}.\] (16)Lemma 4.1 shows that for any \((0,1]\), if we set \(_{t}=_{t}=_{t}=_{t}=1/(t+1)^{}\), then with high probability the gradient and function approximation errors converge to zero at a sublinear rate of \(}(1/t^{/2})\). Moreover, the above result characterizes the choice of \(K_{t}\). Specifically, if we define \(K_{1,t}\) as the upper bound in (15) and \(K_{0,t}\) as the upper bound in (16), by setting \(K_{t}=K_{0,t}+DK_{1,t}\), then with probability \((1-)\) the random set \(}_{t}\) contains \(_{g}^{*}\). Later, we show that \(=1\) leads to the best complexity bound for the convex setting and \(=2/3\) is the best choice for the nonconvex setting.

Next, we establish a similar result for the estimators in SBCGF.

**Lemma 4.2**.: _Consider SBCGF with stepsize \(\) and \(S=q=\). If Assumptions 2.1-2.3 hold, for any \(t 1\) and \((0,1)\), with probability \(1-\) we have \(\|_{t}- f(_{t})\| 4L_{g}D,\)\(\|_{t}- g(_{t})\| 4L_{g}D,\) and \(|_{t}-g(_{t})| 4L_{l}D.\)_

Similarly, for SBCGF, we set \(K_{1,t}=4L_{g}D\) and \(K_{0,t}=4L_{l}D\) and choose \(K_{t}=K_{0,t}+DK_{1,t}\), then the random set \(}_{t}\) contains \(_{g}^{*}\) with probability \(1-\).

Next, we formalize our claim about the random set with the above choice of \(K_{t}\).

**Lemma 4.3**.: _If \(_{g}^{*}\) is the solution set of the lower-level problem and \(}_{t}\) is the feasible set constructed by cutting plane at iteration \(t\), then for any \(t 0\) and \((0,1)\), we have \((_{g}^{*}}_{t}) 1-\)._

This lemma shows all \(_{g}^{*}\) is a subset of the constructed feasible set \(}_{t}\) with a high probability of \(1-\). Indeed, using a union bound one can show that the above statement holds for all iterations up to time \(t\) with probability \(1-t\).

### Convergence and complexity results for the stochastic setting

Next, we characterize the iteration and sample complexity of the proposed method in SBCGI for the stochastic setting. First, we present the result for the case that \(f\) is convex.

**Theorem 4.4** (Stochastic setting with convex upper-level).: _Consider SBCGI in Algorithm 1 for solving problem (1). Suppose Assumptions2.1, 2.2, and 2.3 hold and \(f\) is convex. If the stepsizes of SBCGI are selected as \(_{t}=_{t}=_{t}=_{t}=(t+1)^{-1}\), and the cutting plane parameter is \(K_{t}=c((2L_{l}D+_{l})+D(2L_{g}D+ {3}{2}_{g}))(t+1)^{-1/2}\), then after \(T\) iterations,_

\[g(_{T})-g^{*}}{}+D^{2} T }{T}+}{2}, f(_{T})-f^{*} }{}+_{0})-f^{*}+L_{f}D^{2} T}{T}.\]

_with probability \(1-\) for some absolute constants \(C_{1}\) and \(C_{2}\) and \(:=\)._

Theorem 4.4 shows a convergence rate of \((/)\). As a corollary, SBCGI returns an \((_{f},_{g})\)-optimal solution with probability \(1-\) after \(((d/)/^{2})\) iterations, where \(=\{_{f},_{g}\}\). Since we use one sample per iteration, the overall sample complexity is also \(((d/)/^{2})\). Note that the iteration complexity and sample complexity of our method outperform the ones in , as they require \((1/^{4})\) iterations and sample to achieve the same guarantee.

_Remark 4.1_.: The task of finding \(_{0}\) which is equivalent to a single-level stochastic optimization problem requires \((1/_{g}^{2})\) iterations and samples. As a result, this additional cost does not affect the overall complexity of our method. The same argument also holds in the non-convex case.

**Theorem 4.5** (Stochastic setting with non-convex upper level).: _Consider SBCGI for solving problem (1). Suppose Assumptions 2.1-2.3 hold, \(f\) is nonconvex, and define \(=_{}f()\). If the stepsizes of SBCGI are selected as \(_{t}=_{t}=_{t}=(t+1)^{-2/3}\), \(_{t}=(T+1)^{-2/3}\), and the cutting plane parameter is \(K_{t}=c((2L_{l}D+}{3^{2/3}-1}_{l})+D (2L_{g}D+}{3^{2/3}-1}_{g}))(t+1)^{- 1/3}\), then after \(T\) iterations, there exists an iterate \(_{t^{*}}\) in the set \(\{_{0},_{1},,_{T-1}\}\) for which_

\[g(_{t^{*}})\!-\!g^{*}+L_{g}D^{2}}{(T+1)^{1/3}}+ }{2},(_{t^{*}})_{0})\!-\!+C_{4}+L_{f}D^{2}}{(T+1)^{1/3}}\]

_with probability \(1-\) for some absolute constants \(C_{3}\) and \(C_{4}\) and \(:=\)._As a corollary of Theorem 4.5, the number of iterations required to find an \((_{f},_{g})\)-optimal solution can be upper bounded by \(((d/)^{3/2}/^{3})\), where \(=\{_{f},_{g}\}\). We note that the dependence on the upper-level accuracy \(_{f}\) also matches that in the standard CG method for a single-level non-convex problem [33; 38]. Moreover, as we only need one stochastic oracle query per iteration, SBCGI only requires \((}/^{3})\) stochastic oracle queries to find an \((_{f},_{g})\)-optimal.

### Convergence and complexity results for the finite-sum setting

Similarly, we present iteration and sample complexity for algorithm 2 under the finite-sum setting.

**Theorem 4.6** (Finite-sum setting with convex upper-level).: _Consider SBCGF presented in Algorithm 2 for solving the finite-sum version of (1). Suppose Assumptions 2.1, 2.2, and 2.3 hold and \(f\) is convex. If we set the stepsizes of SBCGF as \(= T/T\), \(S=q=\), and the cutting plane parameter as \(K_{t}=4D(L_{l}+L_{g}D) T/T\), then after T iterations,_

\[g(_{T})-g^{*}^{}+L_{g}D^{2}) T}{T}+ }{2}, f(_{T})-f^{*}_{0 })-f^{*}+C_{6}^{} T}{T},\]

_with probability at least \(1-\), for some absolute constant \(C_{5}\) and \(C_{6}\), and \(^{}=\)._

Theorem 4.6 implies the number of stochastic oracle queries is \(((1/)^{3/2}/)\), where \(=\{_{f},_{g}\}\), which matches the optimal sample complexity of single-level problems .

**Theorem 4.7** (Finite-sum setting with non-convex upper-level).: _Consider SBCGF presented in Algorithm 2 for solving the finite-sum version of (1). Suppose Assumptions2.1-2.3 hold, and \(f\) is non-convex. Define \(=_{}f()\). If the parameters of SBCGF are selected as \(=1/\), \(S=q=\), and the cutting plane parameter is \(K_{t}=4D(L_{l}+L_{g}D)/\), then after \(T\) iterations, there exists an iterate \(_{t^{*}}\{_{0},_{1},,_{T-1}\}\) for which,_

\[g(_{t^{*}})-g^{*}^{}+L_{g}D^{2}}{T^{1/2}} +}{2},(_{t^{*}})_{0})\!-\!f+C_{8}^{}}{T^{1/2}}\]

_with probability at least \(1-\), for some absolute constants \(C_{7}\) and \(C_{8}\), and \(^{}=\)._

As a corollary of Theorem 4.7, the number of stochastic oracle queries is \(((1/)/^{2})\), where \(=\{_{f},_{g}\}\), which matches the state-of-the-art single-level result \((/^{2})\) in . SBCGF also improves the number of linear minimization oracle queries of SBCGI from \((1/^{2})\) to \((1/)\) for convex upper-level and from \((1/^{3})\) to \((1/^{2})\) for non-convex upper-level.

## 5 Numerical experiments

In this section, we test our methods on two different stochastic bilevel optimization problems with real and synthetic datasets and compare them with other existing stochastic methods in  and .

**Over-parameterized regression.** We consider the bilevel problem corresponding to sparse linear regression introduced in (3). We apply the Wikipedia Math Essential dataset  which composes of a data matrix \(^{n d}\) with \(n=1068\) samples and \(d=730\) features and an output vector \(^{n}\). To ensure the problem is over-parameterized, we assign \(1/3\) of the dataset as the training set \((_{},_{})\), \(1/3\) as the validation set \((_{}\:,_{}\:)\) and the remaining \(1/3\) as the test set \((_{}\:,_{}\:)\). For both upper- and lower-level loss functions we use the least squared loss, and we set \(=10\). We compare the performance of our methods with the aR-IP-SeG method by  and the stochastic version of DBGD introduced by . We employ CVX [41; 42] to solve the lower-level problem and the reformulation of the bilevel problem to obtain \(g^{*}\) and \(f^{*}\), respectively. We also include the additional cost of finding \(_{0}\) in SBCGI and SBCGF in our comparisons.

In Figure 1(a)(b), we observe that SBCGF maintains a smaller lower-level gap than other methods and converges faster than the rest in terms of upper-level error. SBCGI has the second-best performance in terms of lower- and upper-level gaps, while aR-IP-SeG performs poorly in terms of both lower- and upper-level objectives. The performance of DBGD-sto for the upper-level objective is well, however, it underperforms in terms of lower-level error. In Figure 1(c), SBCGF, SBCGI, and DBGD-sto achieve almost equally small test errors, while aR-IP-SeG fails to achieve a low test error. Notethat after the initial stage, SBCGI increases slightly in terms of all the performance criteria, because SBCGI (1) only takes one sample per iteration and uses a decreasing step-size while SBCGF takes \(\) samples per iteration and uses a small constant stepsize, demonstrating a more robust performance.

**Dictionary learning.** To test our methods on problems with non-convex upper-level we consider problem (5) on a synthetic dataset with a similar setup to . We first construct the true dictionary \(}^{*}^{25 50}\) comprising of 50 basis vectors in \(^{25}\). All entries of these basis vectors are drawn from the standard Gaussian distribution and then normalized to have unit \(_{2}\)-norm. We also generate two more dictionaries \(^{*}\) and \(^{*}\) consisting of 40 and 20 basis vectors in \(}^{*}\), respectively (thus they share at least 10 bases). These two datasets \(=\{_{1},,_{250}\}\) and \(^{}=\{^{}_{1},,^{}_{250}\}\) are constructed as \(_{i}=^{*}_{i}+_{i},\) for \(i=1,,250\), and \(^{}_{k}=^{*}^{}_{k}+^{}_{k},\) for \(k=1,,250\), where \(\{_{i}\}_{i=1}^{250},\{^{}_{k}\}_{k=1}^{250}\) are coefficient vectors and \(\{_{i}\}_{i=1}^{250},\{^{}_{k}\}_{k=1}^{250}\) are random Gaussian noises. As neither \(\) nor \(^{}\) includes all the elements of \(}^{*}\), it is important to renew our dictionary by using the new dataset \(^{}\) while maintaining the knowledge from the old dataset \(\).

In our experiment, we initially solve the standard dictionary learning problem employing dataset \(\), achieving the initial dictionary \(}\) and coefficient vectors \(\{}\}_{i=1}^{250}\). We define the lower-level objective as the reconstruction error on \(\) using \(\{}\}_{i=1}^{250}\), and the upper-level objective as the error on new dataset \(^{}\). We compare our algorithms with aR-IP-SeG and DBGD (stochastic version), measuring performance with the recovery rate of true basis vectors. Note that a basis vector \(}^{*}_{i}\) in \(}^{*}\) is considered as successfully recovered if there exists \(}_{j}\) in \(}\) such that \(|}^{*}_{i},}_{j}|>0.9\) (for more details of the experiment setup see Appendix F). In Figure 2(a), we observe SBCGF converges faster than any other method regarding the lower-level objective. While SBCGI has the second-best performance in terms of the lower-level gap, aR-IP-SeG and DBGD-sto perform poorly compared with SBCGI and SBCGF. In Figures 2(b) and (c), we see that SBCGI, SBCGF, and DBGD-sto achieve good results in terms of the upper-level objective and the recovery rate. However, aR-IP-SeG still performs poorly in terms of both criteria, which matches the theoretical results in Table 1.