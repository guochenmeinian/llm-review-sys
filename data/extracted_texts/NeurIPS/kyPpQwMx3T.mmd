# Random Propagations in GNNs

Thu Bui1, Anugunj Naman1, Carola-Bibiane Schonlieb2,

**Bruno Ribeiro1, Beatrice Bevilacqua1, Moshe Eliasof2**

1Department of Computer Science, Purdue University, West Lafayette, IN, USA

2Department of Applied Mathematics, University of Cambridge, Cambridge, UK

{bui35, anaman, ribeirob, bbevilac}@purdue.edu

{cbs31, me532}@cam.ac.uk

###### Abstract

Graph learning benefits many fields. However, Graph Neural Networks (GNNs) often struggle with scalability, especially on large graphs. At the same time, many tasks seem to be simple in terms of learning, e.g., simple diffusion yields favorable performance. In this paper, we present **R**andom **P**ropagation **GNN** (RAP-GNN), a framework that addresses two main research questions: (i) can random propagations in GNNs be as effective as end-to-end optimized GNNs? and (ii) can they reduce the computational burden required by traditional GNNs? Our empirical findings indicate that RAP-GNN reduces training time by up to 58%, while maintaining strong accuracy for node and graph classification tasks.

## 1 Introduction

Graph learning has become crucial across fields like biology and recommendation systems, with Graph Neural Networks (GNNs) at the core . While GNNs perform well on various tasks, their computational demands can be high due to backpropagation through every layer, making large-scale training costly. Early studies introduced lightweight alternatives, such as diffusion [2; 3], linear GNNs , and neighborhood sampling . More recently, Forward-Forward learning  bypassed backpropagation by training layers independently, including in GNNs [7; 8]. However, this still requires training each layer and computing outputs for all classes. Random propagation in GNNs, which uses random weights instead of backpropagation, offers a more efficient approach, significantly reducing training time and computational costs. This method allows the processing of deeper, larger graphs without significant performance loss [9; 10; 11], though it has been mostly limited to simpler tasks and GNN architectures.

In this work, we address the following questions:

1. How effective is random propagation across various datasets and tasks, from small to large graphs?
2. Can random weights, requiring only the final classifier to be trained, serve as a viable alternative to current methods?

To explore this, we introduce **R**andom **P**ropagation **GNN** (RAP-GNN), a framework utilizing randomly sampled weights in GNN layers and a pretrained feature embedding, requiring only the classifier to be trained. Our experiments on node and graph level tasks show that RAP-GNN cuts training time by up to 58% while maintaining competitive accuracy compared to end-to-end trained GNNs.

Related Work

Several methods have been proposed to improve GNN training efficiency, as outlined below.

**Backpropagation-Free Training Methods.** The Forward-Forward approach simplifies training by avoiding backpropagation and training each layer independently. Initially applied to computer vision , it was later adapted to GNNs [7; 8]. However, training all layers and computing outputs for all classes is still required. In contrast, our method only trains the classifier, using random propagation through the GNN layers, which reduces computational overhead.

**Graph Lottery-Ticket Hypothesis.** The Graph Lottery-Ticket approach proposed recently [12; 13] suggests that sparse sub-networks can perform as well as fully trained models. However, this method requires pretraining the entire network to find these sub-networks, which remains computationally intensive.

**Random Models.** The most closely related work comes from Reservoir Computing (RC), where fixed, randomly sampled reservoirs capture graph dynamics without extensive training [14; 9; 15]. Traditional RC methods [14; 9] rely on recurrent forward passes until convergence or a set number of iterations, which can be computationally demanding. FDGNN  introduced a GNN framework with random, fixed weights for graph classification, using stability constraints in a recurrent setting. MRGNN  extended this by "unrolling" the recurrent hidden layer, reducing time complexity. These methods rely on static random weights and are primarily suited for graph classification, yet demonstrate strong performance at lower computational cost. GCN-RW  further improved efficiency with random filters optimized through least squares, enabling faster training in node classification tasks without sacrificing accuracy. Moreover, randomness in node features [16; 17; 18; 19] and its propagation has proven effective as a positional encoding technique  and within the normalization layer . Similarly, Yu et al.  explored how adding noise to graph features can improve performance, paralleling the use of random initialization as a form of augmentation. Although the method does not focus on improving training efficiency, it underscores the versatility of randomness as a tool for improving efficiency and performance in GNN tasks.

Additionally, recent work [23; 24] questions the necessity of graph convolution during training, suggesting that alternative methods, such as post-training modifications (e.g., Graph-ODE), can achieve strong results. They highlight the potential for bypassing training of GNN layers, aligning with the random weight techniques used in our method. Our RAP-GNN, which dynamically samples random weights in each hidden GNN layer during every forward pass, similarly demonstrates that full reliance on GNN may not be essential for achieving strong performance. This method enhances representation flexibility while reducing computational overhead.

## 3 RAP-GNN

In this section, we introduce RAP-GNN, which uses randomness in GNN layers while only training the final classifier. Random weights for all GNN layers are sampled from a uniform distribution at each forward pass, eliminating backpropagation and significantly reducing computational costs. We also utilize a pretrained feature embedding to process input node features. The necessity of this pretrained embedding is discussed in Appendix C. The full architecture of RAP-GNN variants using randomly sampled GNN weights is shown in Figure 1, with other variants detailed in Section 4.

The model RAP-GNN consists of three components: (i) an embedding layer \(h^{}_{}\), (ii) a stack of GNN layers with non-linearities \(g_{}\), and (iii) a classifier \(c_{}\). The embedding layer \(h^{}_{}:^{}^{}\) maps input feature dimension, p, into a hidden-dimensional space, d, using a multi-layer perceptron (MLP) or a single-layer

Figure 1: RAP-GNN Framework: We employ an L-layer GNN with randomly sampled GNN weights as defined in Equation (1) instead of learnable GNN weights, paired with either a pre-trained or learnable embedding.

GNN. The GNN layers \(g_{}:^{n d}^{n n}^{n  d}\) process these representations, with weights \(=[^{(1)},^{(2)},,^{(L)}]\) sampled uniformly at each forward pass. If \(h_{}^{}\) is learnable, backpropagation through the GNN layers would be needed, but this is avoided with a pretrained embedding. The classifier \(c_{}:^{d}\) maps representations to the target space \(\), with only the classifier parameters \(\) learned during training. The model is trained in two phases:

**Pretraining Phase.** The embedding layer, \(h_{}^{}\), is pretrained on the dataset for the downstream task using a simplified network, \(f^{}=c_{^{}}^{} h_{}^{}\). This step optimizes the embedding layer to extract meaningful features from the input data. By pretraining the embedding layer, the model captures relevant features early on, eliminating the need to update \(\) during the main training phase.

Appendix C empirically demonstrates that using dynamic random embeddings at each call introduces excessive noise, while fixed random embeddings lack adaptability during training, both of which degrade accuracy. Therefore, this pretraining step is essential in scenarios where extensive training is undesirable or impractical.

**Training Phase.** In the training phase, the key innovation is introducing randomness into the GNN network, \(g_{}\), which consists of \(L\) hidden layers. During each forward pass, a new random diagonal weight matrix \(^{(l)}\) is sampled for each GNN layer. In Appendix A, we detail how \(^{(l)}\) is applied across different GNN backbones, but it is generated as follows:

\[^{(l)}=(_{1}^{(l)}&}\\ &&\\ &}&_{}^{(l)})\] (1)

where \(^{(l)}=[_{1}^{(l)},,_{d}^{(l)}]\) is a vector randomly sampled from the uniform distribution, with \(=\). The diagonal entries \(_{i}^{(l)}\) are constrained to the interval \(\). Thus, \(^{(l)}\) is sampled from a uniform distribution \(U(0,1)\). The diagonal structure ensures that each feature is propagated independently by the corresponding \(_{i}^{(l)}\), thereby preventing full random weights from mixing up the features. Limiting each \(^{(l)}\) to include values in , combined with GCN , renders diffusion propagations, as shown in Eliasof et al. . The main difference is that instead of learning \(^{(l)}\), it is randomly sampled at each forward pass of the network. This controlled propagation type allows the model to explore a rich variety of feature representations while avoiding excessive disruption of the learned structure.

During backpropagation in this training phase, only the parameters \(\) of the classifier \(c_{}\) are updated based on the loss function, while \(\) and \(\) remain unchanged. This eliminates the need for backpropagation through the GNN layers, significantly reducing computational costs.

**Inference with Majority Voting.** During inference, majority voting is employed to enhance robustness and generalization, with the number of votes, M, treated as a hyperparameter. For each vote \(j\{1,,\}\), a new random vector \(^{(l)}\) is sampled to generate a corresponding \(^{(l)}\), \( l[L]\), as outlined in Equation (1). The model then computes the output \(_{j}=f(^{})\) for each vote, where \(^{}\) is the testing data. The final prediction \(\) is determined by majority voting across all voters' outputs \(_{j}\). A similar scheme was used by Bevilacqua et al.  to reduce stochasticity in subgraph sub-sampling.

Our approach leverages on-the-fly random sampling of diagonal weights for all GNN layers, combined with a fixed pretrained embedding layer and a trainable classifier. This enhances generalization across diverse graph structures while improving robustness and efficiency, as empirically validated in Section 4 and Appendix D. The pseudocode for the algorithm is provided in Appendix A.

## 4 Experiments

We empirically address our key questions by first evaluating whether training the entire network is necessary or if randomness in GNN layers can serve as an effective alternative. We assess the impact of different randomness strategies on downstream performance using the Cora , CiteSeer ,PubMed  and ogbn-arxiv  datasets. Additionally, we compare the training and evaluation times of these strategies with those of end-to-end backpropagation, using a GCN backbone .

For further evaluation across tasks and backbones, Appendix D provides results for graph classification on TUDatasets  with a GIN backbone , and additional results on ogbn-arxiv using GraphSage . We also assess the performance of RAP-GNN using different randomness strategies in the embedding layer versus a pretrained embedding on Cora, CiteSeer, and PubMed. Full implementation details are in the Appendix E.

### Impact of Randomness in GNN layers

We evaluate the impact of three randomness strategies in GNN weights: (i) Identity Propagation Weights (IPW): All \(^{(l)}\) in \(\) are identity matrices, passing input representations unchanged, without using randomly sampled weights. (ii) Fixed Propagation Weights (FPW): All \(^{(l)}\) are randomly initialized and fixed throughout. (iii) Dynamic Propagation Weights (DPW): All \(^{(l)}\) are randomly sampled at each forward pass. Each strategy is tested with and without random features (RNF). To optimize accuracy, we combine these strategies with a learnable embedding (LearnEmb) trained via backpropagation, comparing against a standard GCN with fully learnable weights (LW). We further evaluate a RAP-GNN variant that combines a pretrained embedding (PreEMb) with DPW and RNF. Additionally, we introduce another trained end-to-end baseline combining LW, LearnEmb and RNF. We use majority voting with voter count M = 5 for the Cora, Citeseer, and PubMed datasets, and M = 3 for ogbn-arxiv in the RAP-GNN variants.

Firstly, without prioritizing runtime optimization, we focus on evaluating how different randomized sampling methods combined with LearnEmb performs compared to fully end-to-end trained networks. The results in Table 1 reveal consistent trends across the Cora, CiteSeer, and PubMed datasets. Among the end-to-end baselines, the LearnEmb setup slightly underperforms compared to baseline without LearnEmb, likely due to the less expressive MLP backbone used in LearnEmb compared to GCN. However, within the variations of RAP-GNN combined with LearnEmb, IPW achieves accuracy comparable to fully end-to-end models on all three datasets. Even FPW, which limits random sampling in GNN, reaches the same accuracy levels as the end-to-end baselines. Notably, DPW combined with LearnEmb consistently improves over the end-to-end networks across these datasets, highlighting the strength of leveraging randomness in GNN layers, which can even surpass traditional fully learned networks.

On the more challenging ogbn-arxiv dataset, all variations of RAP-GNN combined with LearnEmb exhibit only a slight decrease in accuracy compared to the two end-to-end baselines, further demonstrating that randomized GNN layers can still deliver competitive results.

We also notice that DPW with a pretrained embedding show a slight accuracy drop compared to a learnable embedding across all datasets (since the embeddings aren't updated during training), they still outperform the end-to-end trained baselines on Cora. Although these are early-stage results,

   Method \(\) / Dataset \(\) & Cora & CiteSeer & PubMed & ogbn-arxiv \\ 
**Natural Baselines** & & & & \\ LW & 81.50 \(\) 0.8 & 71.10 \(\) 0.7 & 79.00 \(\) 0.6 & **73.41 \(\) 0.2** \\ LW + LearnEmb + RNF & 81.25 \(\) 0.6 & 70.34 \(\) 0.2 & 78.83\(\)0.7 & 73.28 \(\) 0.3 \\ 
**RAP-GNN Variants** & & & & \\ IPW + LearnEmb & 82.28 \(\) 0.6 & **73.58 \(\) 0.2** & 78.86 \(\) 0.3 & 70.57 \(\) 0.1 \\ IPW + LearnEmb + RNF & 81.86 \(\) 0.5 & 73.64 \(\) 0.3 & 79.14 \(\) 0.2 & 70.59 \(\) 0.2 \\ FPW + LearnEmb & 81.92 \(\) 0.9 & 70.98 \(\) 0.5 & 78.34 \(\) 0.5 & 70.28 \(\) 0.6 \\ FPW + LearnEmb + RNF & 82.34 \(\) 0.8 & 71.56 \(\) 0.3 & 79.13 \(\) 0.2 & 70.15 \(\) 0.4 \\ DPW + LearnEmb & 82.82 \(\) 0.9 & 71.48 \(\) 0.7 & 78.76 \(\) 0.6 & 70.58 \(\) 0.3 \\ DPW + LearnEmb + RNF & **84.36 \(\) 0.3** & 72.16 \(\) 0.6 & **79.32 \(\) 0.3** & 71.23 \(\) 0.2 \\ DPW + PreEMb + RNF & 83.33 \(\) 0.2 & 70.80 \(\) 0.3 & 78.70 \(\) 0.4 & 69.84 \(\) 0.4 \\   

Table 1: RAP-GNN accuracy performance (%)\(\) with a GCN backbone on node classification is evaluated against two end-to-end networks. The results show that RAP-GNN achieves _competitive accuracy_, even can _suppass_ the baselines across various datasets with LearnEmb.

they underscore the potential of our approach for real-world applications where both accuracy and efficiency are critical.

### Time Analysis

To assess the efficiency of RAP-GNN, we report the average runtime for a single training epoch (Train) and on the whole test (Test) on the small Cora dataset (with M = 5 votes) and the larger ogbn-arxiv dataset (with M = 3 votes). For a fair comparison, all models are configured with the same number of hidden channels, layers, and random features (for RNF models).

In Table 2, we demonstrate that RAP-GNN offers a significant advantage in training efficiency. On Cora, using PreEmb and DPW reduces training time by 58% compared to end-to-end training, as we only need to backpropagate through the classifier. Similarly, on ogbn-arxiv, using PreEmb and DPW reduces training time by 47%. The pretraining times, shown in Appendix D.4, reveal that even when combined with pretraining, the total training time for PreEmb and DPW remains substantially lower than methods with fully learnable weights. Variants using LearnEmb also show reduced training times, particularly with IPW and FPW, where layer weights are fixed.

However, during evaluation, all RAP-GNN variants experience slower performance due to the need for majority voting (M>1). When M=1 (i.e., without majority voting), the evaluation time is comparable, although DPW lags slightly due to random sampling at each forward pass. Accuracy comparisons for both with and without majority voting on the Cora and ogbn-arxiv datasets are provided in Appendix D.3, suggesting that further investigation is needed into the impact of majority voting.

## 5 Conclusion

We demonstrate that RAP-GNN achieves competitive accuracy in node and graph classification tasks across various small- and large-scale datasets with different GNN backbones while notably reducing training time. These findings highlight that the key component of RAP-GNN, random sampling GNN weights, offers an effective and efficient alternative to end-to-end trained models.