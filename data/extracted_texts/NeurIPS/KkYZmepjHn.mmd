# Fast Sampling via Discrete Non-Markov Diffusion

Models with Predetermined Transition Time

Zixiang Chen Huizhuo Yuan Yongqian Li Yiwen Kou Junkai Zhang Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{chenzx19,hzyuan,yongqianl,evankou,jkzhang,qgu}@cs.ucla.edu

###### Abstract

Discrete diffusion models have emerged as powerful tools for high-quality data generation. Despite their success in discrete spaces, such as text generation tasks, the acceleration of discrete diffusion models remains under-explored. In this paper, we propose discrete non-Markov diffusion models (DNDM), which naturally induce the predetermined transition time set. This enables a training-free sampling algorithm that significantly reduces the number of function evaluations (i.e., calls to the neural network), making the sampling process much faster. Furthermore, we study the transition from finite to infinite step sampling, offering new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models. Codes are available at https://github.com/uclaml/DNDM.

## 1 Introduction

Diffusion-based generative models, as first introduced by Sohl-Dickstein et al. (2015), have shown remarkable capabilities in generating high-quality samples across various domains, including images (Ho et al., 2020; Song and Ermon, 2020), audio (Chen et al., 2020; Kong et al., 2020), and videos (Ho et al., 2022). The diffusion model utilizes an innovative approach comprising a forward process that gradually transforms training data into pure noise and a reverse process that reconstructs clean data from the noise. Throughout the training phase, the model optimizes a neural network by minimizing an objective derived from maximum likelihood estimation. Once trained, the model can generate samples using various decoding strategies, including implicit dynamics (Song et al., 2020), analytical processes (Bao et al., 2022), or differential equation solvers (Song et al., 2020; Liu et al., 2022; Lu et al., 2022). In particular, Song et al. (2020) introduced the denoising diffusion implicit model (DDIM), providing a non-Markov and de-randomized version of the Denoising Diffusion Probabilistic Model (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020), which enables faster generation of high-quality samples.

Although diffusion models were initially introduced for both discrete and continuous-state spaces (Sohl-Dickstein et al., 2015), these studies have largely focused on Gaussian diffusion processes in continuous-state spaces. Recently, Discrete Denoising Diffusion Probabilistic Models (D3PMs) (Austin et al., 2021) working in discrete-state spaces have gained increasing interest due to their applications in diverse areas such as text generation (Hoogeboom et al., 2021), medical record generation (Ceritli et al., 2023), and protein design (Gruver et al., 2024). These models, which are distinct from their Gaussian counterparts, employ discrete noises, such as the multinomial distribution, for diffusion processes. Very recently, Zheng et al. (2023) introduced a reparameterized diffusionmodel (RDM) that can improve sampling speed and sample quality in text generation tasks. However, their proposed algorithm is a training-based approach. Compared with diffusion models using Gaussian noise, discrete diffusion models remain under-studied, especially regarding training-free sampling acceleration.

In this work, we introduce a training-free approach aiming at enhancing the sampling speed of discrete diffusion models. This approach stems from a unique characteristic of discrete diffusion models: unlike continuous diffusion models, which typically employ Gaussian noise for data corruption (Ho et al., 2020; Song and Ermon, 2020; Song et al., 2020, 2020), discrete diffusion models often use categorical white noises (Hoogeboom et al., 2021; Austin et al., 2021; Zheng et al., 2023).

By delving into this special property, we develop a discrete non-Markov diffusion model, together with a design of accelerated algorithm. Notably, this new sampling technique does not require any modifications to the training objective of diffusion models and is, therefore, training-free. Our contributions are summarized as follows:

* We propose discrete non-Markov diffusion models (DNDM), which naturally induces a set of latent variables \(\), termed as the _transition time set_. This key feature enables us to develop a training-free sampling algorithm that can accelerate a large family of discrete diffusion models. Importantly, DNDM preserves the essential properties of the original discrete diffusion model: for any diffusion trajectory \(\{_{t}\}\) starting from real data \(_{0}\), it provably maintains both the marginal distribution \(q(_{t})\) and the conditional distribution \(q(_{0}|_{t})\). Our method can accelerate the two most widely used discrete diffusion models: multinomial diffusion (Hoogeboom et al., 2021) and absorbing diffusions (Austin et al., 2021). Similar to how DDIM introduces a de-randomized, faster sampling algorithm compared to DDPM in continuous space, DNDM achieves acceleration through a predetermined transition time set in discrete space (See Table 1).
* Based on the predetermined transition time set \(\) in DNDM, we design an accelerated sampling algorithm that reduces the required number of neural network function evaluations. In a standard \(T\) time-step discrete diffusion process, while D3PM, including Multinomial (Ho et al., 2020) and absorbing state discrete sampling (Austin et al., 2021), requires evaluating the neural network function \(T\) times, our approach only requires \(||\) function evaluations, where \(||\) is the cardinality of the transition set \(\). Moreover, \(||\) is provably less than \(T\) and approaches \(O(1)\) as \(T\) goes to infinity. We provide both theoretical analysis and empirical experiments showing that the improvement in the number of function evaluations (NFE) is significant. Notably, our algorithm is about \(3\) faster than baselines for \(T=50\) and about \(30\) faster for \(T=1000\) while preserving the sample quality.
* To further illustrate the effectiveness of DNDM, we explore the limit as \(T\) and introduce an infinite-step sampling algorithm. With a pretrained neural network, we can generate an initial noise \(_{T}\) and a transition time set \(\) with infinitesimal spacing, such that \(||=O(1)\). This enables the generation of the real data distribution with only \(||\) neural network evaluations. This study offers new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.

**Notation.** We use \(||\) to denote the cardinality of the set \(\) (excluding repeated elements). We use lowercase letters to denote scalars, boldface lowercase letters to denote vectors, and boldface uppercase letters to denote matrices. The notation \(1:N\) indicates the sequence from \(1\) through \(N\). The symbol \(\) designates the real distribution in a diffusion process, while \(\) represents the distribution during sampling. With its success probability inside the parentheses, the Bernoulli distribution is denoted by \(()\). We further use \((;)\) to denote a categorical distribution over a one-hot row vector \(\) with probabilities given by the row vector \(\).

## 2 Background

In this section, we provide the background of discrete diffusion models. We begin by introducing the discrete Markov diffusion model, designed for handling categorical random variables. Specifically,

    & **Continuous** & **Discrete** \\   & DDPM & D3PM \\  & (Sohl-Dickstein et al., 2015) & Austin et al. (2021) \\   & DDIM & DNDM \\  & (Song et al., 2020) & (Ours) \\   

Table 1: Cross Comparison of Diffusion Models.

consider a diffusion model trying to generate distributions over a discrete random variable \(^{K}\) that is one-hot encoded with \(K\) categories, i.e., \(\) can be chosen as one of \(K\) categories, and for any \(k[K]\), \(\) is categorized as \(k\) if \(\) aligns with the standard basis vector \(_{k}\). The sequence \(\{_{t}\}_{t=0}^{T}\) represents how this random variable changes over time \(0 t T\), starting from an \(_{0}^{K}\) drawn from the real distribution \(_{}\). In this paper, we focus on the two most widely used D3PMs: multinomial diffusion (Hoogeboom et al., 2021) and absorbing diffusions (Austin et al., 2021).

**Forward Process.** During the forward process, the real distribution \(_{}\) is gradually transformed into a noise distribution named \(_{}\). The transformation occurs through \(T\) steps, with \(T\) intermediate latent variables \(_{1},_{T}\) and update rules given by:

\[_{t}=b_{t}_{t-1}+(1-b_{t})_{t}, t=1, ,T\] (1)

Here \(b_{t}\) is randomly drawn from a Bernoulli distribution with parameter \(_{t}\), denoted by \(b_{t}(_{t})\), and \(_{t}\) is randomly drawn from the noise distribution \(_{}\), while for different \(t\) the samples are independent. In this work, we focus on cases where the noise \(_{}\) can be either a uniform distribution over the vocabulary \(\{1,2,,K\}\)(Hoogeboom et al., 2021), or a point mass with all of the probability mass lying on an absorbing state (Austin et al., 2021). Following this notation, the process in (1) defines a Markov process characterized by the transition kernel

\[q(_{t}|_{t-1})=_{t};=_{t}_{t-1}+(1-_{t})_{}.\] (2)

Moreover, the Markov chain property allows us to get samples \(_{0:t}\) from \(_{0}\) by multiplying the transition probabilities at each step as \(p(_{1:t}|_{0})=_{i=1}^{t}q(_{t}| _{t-1})\). It further leads to the following marginal distribution.

\[q(_{t}|_{0})=_{t};=_{t}_{0}+(1-_{t})_{},\] (3)

where \(_{t}:=_{s=1}^{t}_{s}\) is determined by the sequence of \(_{t}\) of our choice and decreases from \(1\) to \(0\).

**Reverse Process.** Given the forward Markov process, the reverse process can be derived by Bayes' rule (Hoogeboom et al., 2021; Austin et al., 2021; Zheng et al., 2023). The conditional probability \(q(_{t-1}|_{0},_{t})\) can be determined by \(q(_{t-1}|_{0},_{t})=q(_{t}| _{t-1})q(_{t-1}|_{0})/q(_{t}|_{0})\). The reverse process can be used for synthetic data generation by sampling from the noise distribution \(q_{}\) and repeatedly applying a learned predictor (neural network) \(p_{}(|_{t})\) parameterized by \(\):

\[p_{}(_{T})=q_{}(_{T}),  q_{}(_{t-1}|_{t})=_{}_{0}}q(_{t-1}|_{t},}_{0}) p_{}(}_{0}|_{t})d}_{0}.\] (4)

We note that the reverse process \(q(_{t-1}|_{t},}_{0})\) is stochastic and thus requires function evaluation at every step.

**Training the Neural Network.** The neural network \(p_{}(|_{t})\) that predicts \(}_{0}\) is trained by maximizing the evidence lower bound (ELBO) (Sohl-Dickstein et al., 2015),

\[ p_{}(_{0}) _{q(_{1:T}|_{0})} }(_{0:T})}{q(_{1:T}|_ {0})}d_{1:T}\] \[=_{q(_{1}|_{0})}[ p_{ }(_{0}|_{1})]-_{t=2}^{T}_ {q(_{t}|_{0})}[(q(_{t-1}| _{t},_{0})\|p_{}(_{t-1}|_{t }))\] \[-_{q(_{T}|_{0})}(q( _{T}|_{0})\|p_{}(_{T})),\] (5)

Here KL denotes Kullback-Liebler divergence and the last term \(_{q(_{T}|_{0})}(q(_{T}| _{0})\|q_{}(_{T}))\) equals zero. Building on this foundation, Austin et al. (2021) introduced an auxiliary denoising objective, which refines the data predictions \(_{0}\) at each time step. Since this paper primarily focuses on reverse sampling, we leave detailed discussions of these losses to Appendix B.

## 3 Discrete Non-Markov Diffusion Models (DNDM)

### Forward and Reverse Process

In this section, we introduce a non-Markov process such that the joint distribution of \((_{0},_{t})\) remains the same as the one defined with Markov process in Section 2. The new process aims to gradually transform input data \(_{}\) to the noise distribution \(_{}\) through \(T\) intermediate latent variables \(_{1},_{T}\) with the following process:

\[_{t}=b_{t}_{t-1}+(1-b_{t}),\] (6)

where \(b_{t}\) is independently drawn from the Bernoulli distribution \((_{t})\) and \(\) is drawn from the noise distribution \(_{}\). The only difference between (6) and (1) is that we replace \(_{t}\) in (1) by \(\), which is time-invariant during the diffusion. Therefore, the process in (6) becomes non-Markov since \(q(_{t}|_{t-1},,_{0})\) doesn't necessarily equals \(q(_{t}|_{t-1})\). The following theorem shows that the conditional distribution \(q(_{t}|_{0})\) remains unchanged.

**Theorem 3.1**.: _For the non-Markov process in (6), we have_

\[q(_{t}|_{0})=_{t}; =_{t}_{0}+(1-_{t})_{},\]

_where \(_{t}:=_{i=1}^{s}_{s}\) is specified to decrease from \(1\) to \(0\)._

Using the Bayes' rule, we have \(q(_{0}|_{t}) q(_{t}|_{0})q( _{0})\). Consequently, the condtional distribution \(q(_{0}|_{t})\) remains consistent with the one induced by the process process in (1). Therefore, neural network \(p_{}(|_{t})\) trained by the Markov process in (1), remains applicable to our non-Markov process (6) (see Appendix B for detail).

Based on the discrete non-Markov diffusion model, we can give a simple characterization of the reverse process by introducing the transition time.

**Definition 3.2**.: Transition time \(\) is the time that the token \(_{t}\) transition from \(_{0}\) to noise, i.e., \(:=_{t}\{t|b_{t}=0\}\).

_Remark 3.3_.: The concept of transition time has also been introduced in Hoogeboom et al. (2021a). However, Hoogeboom et al. (2021a) restricts the transition time to be the first time of entering the absorbing state, which is only applicable to absorbing diffusion. Our definition is more general and applicable to discrete diffusion with various noise including multinomial diffusion.

Given the transition time \(\), the forward process reduces to:

\[_{t}=(>t)_{0}+(  t),\] (7)

which shows that the token will be a real token \(_{0}\) before the time \(\) and will be the noise \(\) after the transition time. Since token only get changed at the transition time \(\), we can derive a reverse process based on (7),

\[_{t-1}=(=t)_{0}+(  t)_{t}.\] (8)

Therefore, the process in (8) is de-randomized given transition time \(\). Specifically, after independently sampled transition times \(\), \(_{t-1}\) becomes deterministically known and fixed if we observe \(_{0}\) and \(_{t}\). It is also worth noting that given \(_{0}\) and \(\), the exact reverse process (8) is Markovian, since \(_{t-1}\) solely depends on \(_{0},,_{t}\). Plugging (8) into (4) gives the generation process. We can prove the ELBO of the DNDM is equivalent to the ELBO of the original process (5) up to some constant, which further supports the neural network \(p_{}(|_{t})\) trained by the Markov process in (1), remains applicable to DNDM. (See Appendix B.3 for details).

_Remark 3.4_.: (7) and (8) suggest that even though there are \(T\) distinct time steps, not every time in the range \(1:T\) is crucial for capturing the process. Therefore, our primary focus should be on the most significant time step, i.e., the transition time \(\), enabling faster reverse sampling. We further note that although transition happens only at time \(\), the transition time is random, differs across runs, and covers the full range from \(1\) to \(T\) on average.

_Remark 3.5_.: While Song et al. (2020a) proposed a non-Markov multinomial diffusion model in Appendix A, DDIM and DNDM are fundamentally different models when specialized to multinomial diffusion. DDIM's discrete process remains stochastic at every step, even with deterministic noise scheduling. In contrast, DNDM achieves full de-randomization by pre-determined transition time \(\) (Equation 8 in our paper). By sampling these transition times upfront, DNDM establishes a predetermined transition time set that guides the sampling process, enabling deterministic evolution and faster sampling speed even under the same number of sampling steps, which is not reported under DDIM framework. For detailed technical comparison, see Appendix B.1.

### Accelerated Reverse Sampling

In this section, we demonstrate that sampling from DNDM can lead to accelerated reverse sampling. Although our algorithm is quite general, we focus on text generation in the presentation.

In Section 3.1, we only consider the case of a single token \(^{K}\) being one hot encoding of \(K\) categories. In real applications, we are interested in generating a sentence with multiple tokens. So, we extend the terminology in Section 3.1, and we denote the sequence of tokens at \(t\)-th time step to be \(_{t,1:N}=[_{t,1},,_{t,N}]\) where \(_{t,n}\) is the \(n\)-th token and \(N\) is the sequence length. The noise will be added to each token in a sequence independently. Therefore, each token will have its own transition time defined in Definition 3.2. We denote the transition time for each token \(_{n}\) to be \(_{n}\) and further denote the transition time set \(:=\{_{n}\}_{n=1}^{N}\). Given the transition times \(_{n}\), our DNDM can now be extended to the sequence with multiple tokens

\[_{t-1,n}=(_{n}=t)_{0,n}+(_{n}  t)_{t,n}, n[N].\] (9)

**Learning the Reverse Process.** We first generate the transition times \(_{n}\) for \(n[N]\), then we follow (9) to generate the learned reverse process. Since \(_{0,n}\) is unknown in the process, we use the neural network evaluation \(p_{}(|_{t})\) obtained in Section 3.1 to predict \(_{0,n}\). In detail, the noisy sequence \(_{t,1:N}\) is fed into \(p_{}(|_{t,1:N})\) and the prediction tokens \(}_{0,1:N} p_{}(|_{t,1 :N})\) are collected.

**Transition time.** Transition time, denoted by \(\), is crucial in our reverse process. This is because the reverse sampling becomes deterministic upon using (9). Each instance of transition time \(\) is a random variable within the set \(\{1,2,,T\}\). Let's assume it follows the distribution \(_{}\). Given the schedule \(\{_{t}\}_{t=0}^{T}\), we can derive the distribution for \(_{}\).

**Theorem 3.6**.: _Each specific transition time \(_{n}\) in Definition 3.2 is independent. Furthermore, they collectively adhere to the distribution \(_{}\), which obeys the rule \((_{n}=t)=_{t-1}-_{t}\)._

From Theorem 3.6, we discern that the nature of the diffusion model scheduler, \(_{t}\), clarifies the distribution of \(\). Take the linear schedule as an example, as given by Austin et al. (2021), the relationship is \(_{t}=1-t/T\). This translates to \((_{n}=t)=1/T\) for every \(t\) in the range \(1\) to \(T\). As a result, transition time distributes uniformly across each moment in the set \(\{1,,T\}\). Generally, if we express \(_{t}\) as \(g(t/T)\), then we can simplify to \((_{n}=t)=g((t-1)/T)-g(t/T)\), which further refines to \((1/T)|g^{}(t/T)|+o(1/T)\). This indicates that transitions are more likely where \(|g^{}|\) is large.

In practice, we observed that the shape of the transition time does not need to exactly match the theoretically predicted schedule \(\) in Theorem 3.6. Algorithm 1 works even if \(\) is unknown. In particular, we can approximate the schedule with a Beta distribution by first sampling a time \(t\) from a Beta distribution, then adjusting these samples to fit by multiplying by \(T\) and rounding the result to obtain an integer.

**Accelerated Sampling.** According to (9), a token \(_{t-1,n}\) is updated only if step \(t\) is the transition time for the \(n\)-th token. If step \(t\) is not the transition time for any token, the sentence from the previous step can be directly copied: \(_{t-1,1:N}=_{t,1:N}\). As a result, there is no need to do a function evaluation for the current step. Our attention, therefore, can be solely centered on the transition set \(\), necessitating function evaluations only for \(t\) within \(\). For our method, when \(N\) is fixed while \(T\), the total NFE \(||\) will reach \(N\). On the other hand, when \(T\) is fixed and \(N\), the NFE \(\) will reach \(T\) (See Theorem D.1 for detail). It is worth noting that the auto-regressive diffusion model (ARDM) (Hoogeboom et al., 2021) can also achieve at most \(N\) NFE when \(T=\). However, ARDM only focuses on infinite time steps, while our method here isable to accelerate sampling for finite time steps. More detailed discussion and theoretical analysis can be found in Section D, where additional experiments also demonstrate that our DNDM achieves an NFE that is less than half of the original Markov sampling method for discrete diffusion.

By incorporating the forward process with different noises, we can develop DNDM-Multi and DNDM-Absorb, which accelerate the Multinomial and Absorbing sampling methods respectively. Recent works have demonstrated that the quality of samples can be enhanced by utilizing supplementary information derived from the neural network, (Ghazvininejad et al., 2019; Savinov et al., 2021; Chang et al., 2022; He et al., 2022; Zheng et al., 2023). Our DNDM can also be improved using this idea. We call it a discrete non-Markov Diffusion Model with Top-k Transition Time (DNDM-\(k\)). Due to the limit of the pages, we leave the detailed Algorithm and discussion to Appendix E.

### Continous-time (Infinite Step) Reverse Sampling

In the context of continuous state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021; Zhang and Chen, 2022; Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020; Dockhorn et al., 2021). However, the application of continuous-time schemes to discrete-state spaces remains largely unexplored. Campbell et al. (2022) first developed a continuous framework for discrete-time diffusion for the Markovian process and randomized sampling, but not in our non-Markovian setting. In this section, we investigate the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.

**Continuous-time Forward and Backward process.** Recall that the forward process described in (6) can be sampled from \(_{0,n}\) through the following process:

\[_{t,n}=_{t}_{0,n}+(1-_{t})_{},_{t}=_{i=1}^{t}_{i}.\] (10)

In the previous section, we are constrained to discrete time steps, where we must define a maximum step, denoted by \(T\). The values of \(_{t}\) are computed only for \(t=1,,T\). As a result, during the training process, it is only possible to predict \(_{0}\) at these predetermined time steps. This constraint confines the computation of our reverse process exclusively to these fixed time stamps. To derive the continuous limit of (10), for each \(T\) we rescale (10) to a diffusion process on \(\), e.g., \(_{T,n}=}_{1,n},_{0,n}=}_{0,n}\), and \(_{t,n}=}_{t/T,n}\). Therefore, when \(T\), \(}_{t,n}\) represents the continuous process that has values at arbitrary \(t\). If the choice of \(_{t}\) for each \(T\) is scale-invariant, we can define a continuous function \((t)\) as the continuous \(\) schedule of the discrete counterpart1. More specifically, we obtain

\[}_{t,n}=(t)}_{0,n}+(1-(t)) _{}, t.\] (11)

For the reverse-time process, we define the transition time set \(:=\{_{n}\}_{n=1}^{N}\) consistent with Theorem 3.6 and sample it from \((_{n}=t)=-^{}(t)\) (we always use decreasing \((t)\)). With \(\) defined, the updates to \(_{t,n}\) only occur at \(\{_{n}\}\). Consequently, we arrange \(_{n}\) to obtain an ordered sequence \(_{n_{k}}\), where \(_{n_{1}}<_{n_{2}}<<_{n_{N}}\). When omitting the infinitely many time steps between \(_{n_{k}}\) and \(_{n_{k-1}}\), the resulting reverse process is then given by:

\[_{_{n_{k-1}},n}=(_{n}=_{n_{k-1}})_{ 0,n}+(_{n}_{n_{k-1}})_{_{n_{k}},n},.\] (12)

for all \(n[N]\). The detailed algorithm named DNDM-C is shown in Algorithm 2.

```
0: Trained prediction function \(p_{}\), \(_{}\), \(_{}\)
1:for\(n=1 N\)do
2: Initiate each token \(_{T,n}_{}\)
3: Initiate the transition time \(_{n}_{}\) and order them as \(_{n_{1}}<<_{n_{N}}\)
4:endfor
5:for\(k=N 1\)do
6: Generate \(}_{0,1:N}\) from \(p_{}(|_{_{n_{k}},1:N},_{n_{k}})\)
7:for\(n=1 N\)do
8: Update \(_{_{n_{k-1}},n}\) based on condition of \(_{n}\)
9:endfor
10:endfor
11:Return\(_{0,1:N}\) ```

**Algorithm 2** Sampling from DNDM-C

_Remark 3.7_.: Autoregressive Diffusion Model (ARDM) (Hoogeboom et al., 2021a) is a discrete diffusion model built upon the autoregressive nature of data. ARDM is shown to be equivalent to a continuous-time absorbing diffusion model and thus provides a unique perspective for discrete diffusion. For continuous-time (\(T=\)) reverse sampling, both ARDM and our method achieve \(N\) NFEs. Unlike ARDM which is limited to absorbing-state transitions, our method provides a unified framework including both absorbing and multinomial diffusions, applicable to both finite time and continuous time diffusions. For infinite timesteps, Hoogeboom et al. (2021a) also proposed an advanced parallelizing technique that can reduce NFE according to the log-likelihood, which we have not considered in DNDM-C.

## 4 Experiments

In this section, we evaluate DNDM and demonstrate its superior performance on two types of tasks: conditional sequence-to-sequence text generation (i.e., machine translation) and unconditional text generation. For the fairness of comparison, all the experiments are conducted using a single NVIDIA RTX A6000 GPU with 48 GB memory. Additional experiment details are provided in Appendix F.

### Conditional Text Generation

We evaluate DNDM's effectiveness on conditional text generation through machine translation tasks. Following Zheng et al. (2023), we use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to create a shared vocabulary of words and subwords from both source and target languages. We implement our experiments using FairSeq (Ott et al., 2019), which employs an encoder-decoder architecture. The model uses bi-directional self-attention blocks without causal masking, allowing tokens to attend to both past and future positions during training and inference. The encoder processes the source text, while the decoder generates the target translation.

**Datasets.** We use the following three datasets to compare with the baselines for machine translation tasks: (1) IWSLT14 DE-EN (Cettolo et al., 2014), a dataset with German as the source language and English as the target language. It consists of \(174272\) examples (sentence pairs), and each of the validation set and the testing set accounts for \(7283\) and \(6750\) of the dataset; (2) WMT14 EN-DE (Bojar et al., 2014), which is an English-to-German translation dataset consisting of \(3967182\) examples. Each of the validation set and the testing set accounts for \(3000\) and \(3003\) of the dataset; and (3) WMT16 EN-RO (Bojar et al., 2016), which is an English-to-Russian translation dataset consisting of \(612317\) examples. Each of the validation sets and the testing set accounts for \(1999\) and \(1999\) of the dataset. The train-validation-test split is fixed across all experiments for all machine translation datasets to ensure fair comparison.

**Performance Metrics.** We use the BLEU score (Papineni et al., 2002) to evaluate the machine translation quality, where the BLEU score is calculated based on the similarity between the actual target sequence and the predicted target sequence. The sampling speed is measured by wall-clock time (in second).

**Baselines.** The main baselines we are comparing with are RDM and RDM-\(k\) from Zheng et al. (2023). Here, we use RDM-\(k\) and RDM to denote the sampling method proposed in their paper with and without the usage of top-\(k\) selection for the token generation technique (see Appendix E for more details), respectively. RDM and RDM-\(k\) are applied to two previously proposed state-of-the-art discrete diffusion models: Multinomial Diffusion (Hoogeboom et al., 2021b) and Absorbing Diffusion (Austin et al., 2021).

**Results and Discussion.** Tables 2 and 3 present the performance evaluations of our algorithms in machine translation tasks. Table 2 presents results for multinomial diffusion, while Table 3 displays results for absorbing diffusion. Our reported time and BLEU scores are averaged over 5 repeated experiments, except for the baseline RDM experiment2.

From Tables 2 and 3, we observe that methods based on DNDM significantly accelerate the sampling process compared to baseline diffusion models. This acceleration allows for greater flexibility in increasing the number of steps (up to infinity) without imposing a significant computational burden.

In particular, more sampling steps lead to better generation quality (BLEU) at the expense of longer sampling time, as indicated in each column of Tables 2 and 3. For RDM-based methods, generation time increases linearly with the number of sampling steps. On the contrary, for our DNDM-based method, generation time only increases marginally (See Figure 4 in Section G). As a result of the difference in the growing speed of sampling time with respect to sampling steps, the more sampling steps, the more speedup DNDM can obtain.

Continuous-time results, as the ultimate limit of increasing sampling steps, are presented in the last row of each dataset with the tag \(\). Given that the results with 1000 steps consistently outperform those with 50 steps, we compare \(\) with 1000 steps in Table 2 and 3. For IWSLT14 and WMT16, where the generation BLEU score is relatively high, we observe a consistent performance improvement of up to \(0.3\) in BLEU score when utilizing the DNDM-C algorithm, with the exception of a single case in the absorbing diffusion setting for WMT16 without the use of top-\(k\) selection. The performance gain of the continuous-time method on WMT14 is less significant, with both drops and gains. However, WMT14 itself has not reached a high level of performance, with a BLEU score significantly lower than other datasets. In general, training WMT14 poses challenges across all diffusion models, including multinomial diffusion (Hoogeboom et al., 2021), absorbing diffusion (Austin et al., 2021), and RDM diffusion (Zheng et al., 2023), etc. We defer a more detailed discussion on WMT14 to Appendix F.1. Finally, when compared with the results obtained with 50 steps, the performance of DNDM-C demonstrates improvement consistently. Furthermore, we note that regardless of the dataset or the method (i.e., RDM or DNDM) employed, top-\(k\) token generation consistently outperforms vanilla methods. This approach enhances the BLEU score by approximately \(1\)-\(2\) points without introducing significant increases in sampling time.

Scaling Law in Sampling Speed.For illustrative purposes, we use the example of IWSLT14 to visualize how the sample quality scales regarding sampling speed for different methods. In Figure 1, we observe the trend of the BLEU score in relation to computational time. Each line in the legend represents a different sampling algorithm, and a steeper slope indicates a larger marginal gain when sampling for longer periods. Figure 1 demonstrates that our algorithm displays nearly linear growth in BLEU score over the log of time, which is remarkable in contrast with the flat curve of the baseline. Particularly, for multinomial diffusion, the BLEU score increases by 1 in less than 60 seconds of additional sampling time. For absorbing diffusion, DNDM outperforms RDM before RDM samples 50 steps. In Tables 7 and 8 in Appendix D, we further use the average number of function evaluations (NFE) to measure the improved speed within the specified number of sampling steps. Additionally, in Figure 2, we visualize how the BLEU score and the generated text change throughout the sampling process.

    &  &  &  &  &  \\   & & **BLEU** & **Time (s)** & **BLEU** & **Time (s)** & **BLEU** & **Time(s)** & **BLEU** & **Time (s)** \\   IWSLT14 \\ (6.75k) \\  } & 25 & **31.26** & 166.9 & 30.95 & **52.9** & **32.82** & 161.9 & 32.30 & **52.6** \\  & 50 & **31.50** & 328.6 & 31.45 & **83.9** & **32.82** & 321.2 & 32.80 & **93.2** \\  & 1000 & 31.69 & 6308.9 & **31.82** & **191.3** & 32.64 & 6321.3 & **33.15** & **191.5** \\  & \(\) & - & - & **31.89** & **225.2** & - & - & **33.44** & **228.1** \\   WMT14 \\ (3k) \\  } & 25 & **25.25** & 237.3 & 25.01 & **90.7** & **26.03** & 230.9 & 25.98 & **90.5** \\  & 50 & **25.75** & 466.1 & 25.33 & **138.4** & 26.14 & 500.2 & **26.37** & **138.3** \\  & 1000 & 25.66 & 8996.7 & **25.71** & **265.4** & 25.82 & 8991.7 & **26.88** & **265.5** \\  & \(\) & - & - & **24.79** & **307.5** & - & - & **26.39** & **307.3** \\   WMT16 \\ (2k) \\  } & 25 & **32.29** & 145.2 & 31.97 & **36.4** & **33.12** & 143.5 & 32.94 & **36.4** \\  & 50 & **32.53** & 286.1 & 32.50 & **63.2** & **33.41** & 312.4 & 33.26 & **62.7** \\   & 1000 & 32.63 & 5588.9 & **32.86** & **171.4** & 33.67 & 5601.0 & **33.79** & **171.2** \\   & \(\) & - & - & **32.91** & **196.4** & - & - & **33.86** & **196.3** \\   

Table 2: BLEU score comparison of multinomial diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k).

### Unconditional Text Generation

For unconditional text generation, we evaluate our approach on language modeling tasks, where the model learns to generate text that matches the statistical patterns of the training data. Unlike conditional generation, this task involves directly learning \(q(_{0}|_{t})\) without conditioning on any input text. We conduct experiments on the text8 and enwik8 datasets using a decoder-only architecture similar to GPT models. Since unconditional generation does not require encoding input sequences, we employ a 12-layer Transformer decoder without an encoder component.

**Datasets.** The natural language generation task is evaluated on two language datasets following Hoogeboom et al. (2021): text8 and enwik8. Both datasets are from Wikipedia, but their contents are highly distinct. In text8, the plain text consists of English words (all the letters are in lower case) and spaces, and it is tokenized into 26 characters and one blank space, resulting in 27 categories. In contrast to the cleanness of text8, enwik8 preserves the original XML dump contents, and there exist various special symbols in its raw text, so its text is tokenized into 1 Byte, resulting in 256 categories. We utilize text8 dataset with sequence length 256 and enwik8 dataset with sequence length 320. The train/val/test splits are \(97/56/55\) for both text8 and enwik8.

**Performance Metrics.** Our evaluation of text generation quality relies on the perplexity score. When generating text8 data, we calculate perplexity scores using the GPT2 model, while for enwik8 data generation, we employ the GPT2-large model. The sampling speed is measured in seconds.

    &  &  &  &  &  \\   & & **BLEU** & **Time (s)** & **BLEU** & **Time (s)** & **BLEU** & **Time(s)** & **BLEU** & **Time (s)** \\   & 25 & 31.58 & 116.3 & **32.43** & **67.2** & **34.50** & 108.9 & 34.14 & **67.3** \\  & 50 & 31.80 & 227.2 & **32.63** & **95.9** & **34.58** & 213.9 & 34.34 & **96.2** \\  & 1000 & 31.91 & 4197.4 & **32.93** & **161.1** & **34.60** & 4205.9 & 34.56 & **162.3** \\  & \(\) & - & - & **33.03** & **174.6** & - & - & **34.65** & **180.7** \\   & 25 & 24.97 & 116.4 & **25.79** & **68.1** & **27.50** & 107.5 & 27.18 & **68.0** \\  & 50 & 24.95 & 231.1 & **26.10** & **102.0** & **27.73** & 255.2 & 27.66 & **102.5** \\  & 1000 & 25.22 & 4169.4 & **26.43** & **178.3** & 27.75 & 4167.4 & **27.82** & **179.1** \\  & \(\) & - & - & **26.50** & **180.1** & - & - & **27.50** & **181.2** \\   & 25 & 32.86 & 75.5 & **33.20** & **41.2** & 33.92 & 69.9 & **33.96** & **41.4** \\  & 50 & 32.93 & 148.4 & **33.30** & **62.5** & 34.10 & 166.1 & **34.20** & **62.7** \\   & 1000 & 33.25 & 2951.7 & **33.60** & **121.3** & **34.44** & 2718.7 & 34.38 & **122.7** \\   & \(\) & - & - & **33.42** & **121.8** & - & - & **34.41** & **121.9** \\   

Table 3: BLEU score comparison of absorbing diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k).

Figure 1: Generation quality to generation time comparison on IWSLT14. \(x\)-axis: computational time in seconds; \(y\)-axis: BLEU score.

**Baselines.** We compare our proposed DNDM on unconditional text generation task with the vanilla Multinomial Diffusion (Hoogeboom et al., 2021). **Results and Discussion.** Table 4 displays the performance of our algorithms in text generation tasks. We run the multinomial diffusion model on the text8 dataset for 1000 diffusion steps and on the enwik8 dataset for 4000 diffusion steps. Our DNDM-based algorithms outperform the vanilla sampling algorithm used in Hoogeboom et al. (2021) in terms of both sampling time and perplexity score. Specifically, for the text8 dataset, DNDM-based algorithms are \(5\) times faster than the vanilla algorithm. For the enwik8 dataset, DNDM-based algorithms are 14 times faster than the vanilla algorithm.

## 5 Conclusion and Future Work

This paper presents a novel discrete non-Markov diffusion model (DNDM) accompanied by an accelerated sampling algorithm designed to boost sampling speed in a discrete-state space. Our discrete diffusion model incorporates "transition time set" latent variables, establishing itself as an efficacious diffusion and data generation method. Thanks to our acceleration technique, we significantly decrease the number of neural network function evaluations without sacrificing sample quality. We also introduce an infinite-step sampling algorithm, DNDM-C, which provides new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. While this study focuses on text generation using non-autoregressive models, a promising direction for future exploration is applying our method to other tasks, such as audio and image generation.