# LLM and Simulation as Bilevel Optimizers:

A New Paradigm to Advance Physical Scientific Discovery

 Pingchuan Ma\({}^{1}\), Tsun-Hsuan Wang\({}^{1}\), Minghao Guo\({}^{1}\), Zhiqing Sun\({}^{2}\),

**Joshua B. Tenenbaum\({}^{1}\)\({}^{3}\)**\({}^{4}\), Daniela Rus\({}^{1}\), Chuang Gan\({}^{5}\)\({}^{6}\), Wojciech Matusik\({}^{1}\)

\({}^{1}\)MIT CSAIL, \({}^{2}\)CMU LTI, \({}^{3}\)MIT BCS, \({}^{4}\)CBMM, \({}^{5}\)UMass Amherst, \({}^{6}\)MIT-IBM Watson AI Lab

###### Abstract

Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities. However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery. Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis. Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations. We introduce _Scientific Generative Agent_ (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters. We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.

## 1 Introduction

Physical science automation aims to accelerate discovery . Key aspects of human scientific process include: iterative hypothesis testing , discrete and continuous solution components , knowledge exploitation with occasional exploration , and universal principles with discipline-specific nuances . LLMs excel as generalist tools with vast knowledge , aiding scientific discovery through reasoning and natural language interfaces. However, they lack computational capabilities crucial for physical sciences that requires specific domain knowledge.

To this end, inspired by the overarching philosophy of human scientists, we introduce **Scientific Generative Agent (SGA)**, a bilevel optimization approach wherein the outer-level engages LLMs as knowledgeable and versatile thinkers for generating and revising scientific hypothesis, while the inner-level involves simulations as experimental platforms for providing observational feedback. Overall, our contributions are concluded as:

* We present a generic framework for physical scientific discovery that combines LLMs with physical simulations.
* We propose a bilevel optimization with LLMs for discrete-space search-based optimization and differentiable simulations for continuous-space gradient-based optimization.
* We conduct extensive experiments to demonstrate the effectiveness and generality of the proposed framework in physics law discovery and molecular design.

## 2 Scientific Generative Agent

SGA is a bilevel optimization framework where the upper level features LLMs as proposers of scientific solutions, and the lower level utilizes simulations as experimental platforms for validation. We illustrate the overall pipeline in Fig. 1.

### Bilevel Optimization Pipeline

First, we describe the simulation as a process where a simulator takes in a scientific expression and continuous components as inputs and gives simulated physical phenomenon and additional observational feedback as outputs. Next, the LLM acts as a thinker to propose expressions based on past experimental results from simulation. This process involves the LLM taking in a set of past simulation results containing an evaluation of the scientific problem, other physical feedback, and past proposals, along with a prompt. The LLM then outputs proposed expressions and continuous parameterization for the decision variables. With these elements, we define a bilevel optimization problem: the objective is to minimize the evaluation of the simulated physical phenomenon, which depends on the proposed expression, continuous parameterization, and optimal continuous parameters. The optimization problem has two levels: (i) the outer optimization searches for an expression that defines what experiments to be conducted and continuous parameterization that defines the search space of the inner continuous optimization; (ii) the inner optimization, which depends on the outer-level variables, searches for the optimal continuous parameters given the proposed expression via differentiable simulation. We detail the complete algorithm with a python-like pseudo-code in Alg. 1.

### LLM-Driven Outer-Level Search

LLM-driven OptimizationLLMs are effective for generic optimization through prompting and context [60; 43]. Inspired by , we use evolutionary search with multiple offspring per iteration. Our approach selects several high-performing candidates, enhancing hypothesis feasibility and facilitating crossover, with LLMs generating new hypotheses from past experiments .

Figure 1: **The overall pipeline of Scientific Generative Agent (SGA).** Taking the constitutive law searching problem as an example, the input is an initial guess (a purely elastic material), and the output is another constitutive law optimized towards the ground truth (weakly compressible fluid).

#### 2.2.2 Interfacing with Simulation

Integrating LLMs with simulation requires efficient, structured communication. We use equation searching and entity searching for LLM-to-simulation communication, unified as an abstraction. Equation searching allows LLMs to propose equations and search spaces, while entity searching focuses on structural descriptions. For simulation-to-LLM communication, we use expert knowledge to extract relevant information as feedback, similar to a senior scientist guiding a junior colleague. The inner optimization results also serve as feedback from simulation to LLMs, detailed in the next section.

#### 2.2.3 Exploitation and Exploration

We employ an exploit-and-explore strategy by adjusting LLMs' decoding temperature , mimicking human scientists' approach to breakthroughs. When generating offspring, we divide them into two groups: cautious followers (exploit) and daring adventurers (explore). We observed that the exploit group often repeats previous solutions, while the explore group tends to yield overly random or invalid solutions. A 1:3 ratio between exploit and explore groups has proven effective emperically based our experiments.

### Differentiable Inner-Level Optimization

Inner optimization uses gradient-based methods to find optimal parameters within the search space defined by the outer level. Domain-specific knowledge is distilled through gradients from the simulation to intermediate optimization results. These results, along with the final output, are fed back to LLMs for solution refinement. The feedback may include loss curves and auxiliary recordings, providing information on various aspects of improvement.

## 3 Experiments

### Problem Definitions

#### 3.1.1 Constitutive Law Discovery

Identifying the constitutive law from motion observations stands as one of the most difficult challenges in fields such as physics, material science, and mechanical engineering. Here we follow the recent advances in physical simulation and formulate the constitutive law discovery task as an optimization problem .

#### 3.1.2 Molecule Design

We focus on a prevalent task in molecule design: discovering molecules with specific quantum mechanical properties. Our objective is to determine the optimal molecular structure and its 3D conformation to match a predefined target quantum mechanical property. The design process involves both the discrete expression - the molecular structure represented by SMILES strings , and the continuous parameters - the 3D coordinates of each atom in the molecule.

### Experiment Setup

We design a diverse set of challenging tasks for evaluation. For constitutive law discovery, we propose 4 tasks including: **(a)** fitting the non-linear elastic material starting from a linear elastic material, **(b)** fitting the von Mises plastic material starting from a purely elastic material, **(c)** fitting the granular material starting from a purely elastic material, and **(d)** fitting the weakly compressible fluid starting

    &  &  &  &  &  \\   & & & &  &  &  &  &  &  &  &  \\ 
**CoT** & 1 & 5 & N/A & ✗ & 298.5 & 1462.3 & 150 & 384.1 & 3.0 & 32.1 & 18.6 & 6.0 \\
**FunsSearch** & 20 & 2 & 0/4 & ✗ & 210.3 & 872.2 & 82.8 & 139.5 & 1.1 & 7.1 & 8.3 & 1.1 \\
**Eureka** & 5 & 1 & 0/16 & ✗ & 128.0 & 531.0 & 101.7 & 150.1 & 4.3 & 9.8 & 3.3 & 9.7e-1 \\
**OPRO** & 5 & 5 & 0/16 & ✗ & 136.2 & 508.3 & 99.2 & 128.8 & 2.4 & 9.4 & 3.1 & 1.3 \\ 
**Ours (no bilevel)** & 5 & 5 & 4 /12 & ✗ & 90.2 & 517.0 & 83.6 & 68.4 & 8.6e-1 & 9.1 & 1.8 & 1.4 \\
**Ours (no exploit)** & 5 & 5 & 0/16 & ✓ & 30.3-3 & 3.9e-1 & 6.6e-2 & **1.4e-12** & 4.0-4 & 1.5e-1 & 6.1e-1 & **2.8e-5** \\ 
**Ours** & 5 & 5 & 4 /12 & ✓ & **5.2e-5** & **2.1e-1** & **6.0e-2** & **1.4e-12** & **1.3e-4** & **1.1e-1** & **5.4e-1** & 3.6e-5 \\   

Table 1: **Benchmark. We use column #Iter. as the number of iterations, #Hist. as the \(K\) value for the top-k retrieval. #Exploit as the number of offspring for exploitation versus exploration, Bilevel as if bilevel optimization is enabled. The best method with the lowest loss is highlighted in bold text.**from a purely elastic material. For molecular design task, we consider 4 popular tasks, centering on 3 commonly evaluated quantum mechanical properties [13; 65]: **(e)** HOMO (Highest Occupied Molecular Orbital) set to 0, **(f)** LUMO (Lowest Unoccupied Molecular Orbital) set to 0, **(g)** the HOMO-LUMO energy gap set to 0, and **(h)** the HOMO-LUMO energy gap set to -2.

### Physical Scientific Discovery

We consider 6 strong baselines for evaluation: (i) **Chain-of-Thoughts (CoT)** prompting  solves the problem by looking at step-by-step solutions from examples. We provide 5 examples with an explanation to CoT as the initial solution. (ii) **FunSearch** utilizes evolutionary strategy to avoid local optimum. We adopt the given hyperparameters from the original implementation with 2 optimization histories and 4 explorers. We set the number of iterations to 20, yielding the same number of solutions evaluated, for a fair comparison to other methods. (iii) **Eureka** generates multiple solutions in each iteration to improve the success rate of the generated code. We keep the hyperparameters from the original implementation. (iv) **Optimization by PROmpting (OPRO)** highlights the advantages of involving a sorted optimization trajectory. We set the hyperparameters to be equal to **Eureka** except for the number of historical optimization steps. In all these works (i-iv), we notice the temperatures for LLM inference are all 1.0, which is equal to the exploring temperature in our method, so we denote them with 0 exploiter. We also consider 2 variants of our method: (v) **Ours (no bilevel)** removes the bilevel optimization by only searching with LLM. (vi) **Ours (no exploit)** removes the exploitation by setting the temperature to 1.0 all the time.

We present our experiments against the 8 designed tasks and show the results in Table 1. Compared to baselines (i-iv), our method is significantly better by a number of magnitudes. When the bilevel optimization is removed from our method, the performance drops dramatically, but still statistically better than baselines (i-iv), indicating the choice of hyperparameters and the integration of exploitation is helpful for the task. When we remove the exploitation but restore the bilevel optimization, we notice the performance grows back. It has comparable performance compared to our method in **(d)** or even better results in **(h)**. However, in some tasks, especially hard ones (e.g., **(b)** and **(f)**) that we care more in reality, the performance gap is over \(50\%\), indicating the effectiveness of our exploit-and-explore strategy. We also present the loss trend in task **(a)** in Figure 1(a), our method outstands with a much lower loss and a converging trend. We present more experiments in Sec. C.

### Bilevel Optimization

Here we evaluate the importance of bilevel optimization in Figure 1(b) using the task **(h)**. Comparing the blue triangle curve and the red dot curve, which represent the LLM-driven outer-level optimization and the simulation-driven inner-level optimization, it is easy to conclude that the loss performance with bilevel optimization is better. Nevertheless, we are also interested in how bilevel optimization works inside each optimization step and how much LLMs and simulations help respectively. As shown as a zigzag curve, we found that LLMs and simulations help each other over all optimization steps: the next proposal from LLMs will be better with simulation-optimized results, and vice versa. We argue that LLMs and simulations have different expertise: LLMs are generalist scientists who have cross-discipline knowledge, while simulations are domain experts who have specialized knowledge.