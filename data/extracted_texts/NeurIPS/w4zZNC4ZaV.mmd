# How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources

Yizhong Wang\({}^{*}\)\({}^{}\)   Hamish Ivison\({}^{*}\)\({}^{}\)   Pradeep Dasigi\({}^{}\)   Jack Hessel\({}^{}\)

**Tushar Khot\({}^{}\)   Khyathi Raghavi Chandu\({}^{}\)   David Wadden\({}^{}\)   Kelsey MacMillan\({}^{}\)   Noah A. Smith\({}^{\)   Iz Beltagy\({}^{}\)   Hannaneh Hajishirzi\({}^{}\)**

\({}^{}\)Allen Institute for AI  \({}^{}\)University of Washington

{yizhongw,hamishi}@allenai.org

Equal contribution.

###### Abstract

In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tulu\({}^{}\), our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.

Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tulu\({}^{}\), along with our code, data, and evaluation framework to facilitate future research.2

## 1 Introduction

The latest generation of large language models has brought unprecedented attention to the potential of language technologies. To support imperative user requests and a chat interface, these models often undergo an _instruction-tuning_ step which involves training on supervised input/output pairs. Recent instruction tuning corpora are often gathered via crowdsourcing (Dolly , Open Assistant ) or via distillation from another model (Alpaca , Vicuna ). However, while some public, instruction-tuned models are advertised as comparable to powerful closed-source proprietary models such as ChatGPT, most experiments that support such claims only cover a small set of tasks, and mostly rely on model-based evaluation metrics [8; 56]. We contend that the evaluation setup should

[MISSING_PAGE_FAIL:2]

### Instruction Datasets

We attempt to collect a representative sample of different styles of datasets (listed in Table 1), including datasets: (1) created by researchers from existing NLP datasets (SuperNI , Flan V2 ); (2) written by humans from scratch for the purpose of instruction tuning (Dolly , Open Assistant 1 ); (3) generated by proprietary models (Self-Instruct , Unnatural Instructions , Alpaca , Baize , GPT4-Alpaca ); (4) comprised of user-shared prompts accompanied by model-generated completions (ShareGPT3); (5) built for specific skills (CoT  for chain-of-thought, Code-Alpaca  for code generation). See Appendix C for further details.

### Pretrained Models

We primarily use the LLaMa suite [44; 45], a series of pretrained models ranging in size from 6.7B to 65B parameters. We initially experimented with the LLaMa-1 models for the first version of this paper and added LLaMa-2 in our camera ready, which use similar numbers of parameters but were trained over significantly more tokens. These models represent the largest, highest-quality pretrained models available to the community (albeit under restrictive licensing). We also consider OPT  and Pythia  models with a size comparable to the LLaMa 6.7B model, to examine the effect of different base models. For simplicity, we will round all the sizes to the nearest integer number. We note several ongoing efforts to pre-train similar- or better-quality models [18; 33; 1]. We believe our findings should hold for these models and future stronger open base models.

## 3 Training Models with Various Datasets

### Unifying the Format

We format all datasets to follow a chatbot-style schema to unify the varied styles and formats of the instruction datasets, shown in Figure 1. This allows us to fit arbitrary rounds of interactions between the user and the language model (a.k.a. "assistant") into one input sequence and encode them together with a causal language model. We add special tokens <|user|> and <|assistant|> before user utterances and target assistant responses respectively, and an end-of-text marker </s> at the end of each assistant output, which, at inference time, will stop the model's response for each round.

  
**Datasets** & **Sourced from** & **\# Instances** & \(_{rounds}\) & \(L_{p_{}}\) & \(L_{}\) \\  SuperNI  & NLP datasets + Human-written Instructions & 96,913 & 1.0 & 291.1 & 38.7 \\ ColT  & NLP datasets + Human-written CoIs & 100,000 & 1.0 & 266.0 & 53.2 \\ Flan V2  & NLP datasets + Human-written Instructions & 100,000 & 1.0 & 355.7 & 31.2 \\ Dolly  & Human-written from scratch & 15,011 & 1.0 & 118.1 & 91.3 \\ Open Assistant 1  & Human-written from scratch & 34,795 & 1.6 & 34.8 & 212.5 \\ Self-instruct  & Generated w/ vanilla GPT3 LM & 82,439 & 1.0 & 41.5 & 29.3 \\ Unnatural Instructions  & Generated w/ Davinci-002 & 68,478 & 1.0 & 107.8 & 23.6 \\ Alpaca  & Generated w/ Davinci-003 & 52,002 & 1.0 & 27.8 & 64.6 \\ Code-Alpaca  & Generated w/ Davinci-003 & 20,022 & 1.0 & 35.6 & 67.8 \\ GPT4-Alpaca  & Generated w/ Davinci-003 + GPT4 & 52,002 & 1.0 & 28.0 & 161.8 \\ Baize  & Generated w/ ChatGPT & 210,311 & 3.1 & 17.6 & 52.8 \\ ShareGPT3 [FOOT 3]

  
**Base LMs** & **\# Params** & **\# Tokens** \\   & 6.7B & 1.0T \\  & 13.0B & 1.0T \\ LLaMa  & 32.5B & 1.4T \\  & 65.2B & 1.4T \\  LLaMa-2  & 6.7B & 2.0T \\  & 13.0B & 2.0T \\  OPT  & 6.7B & 180B \\  Pythia  & 6.9B & 300B \\   

Table 2: Base models that we finetuned in this work.

### Model Training Details

During training, we compute loss only on tokens after <|assistant|> and before the next <|user|> token. More formally, we consider an instruction dataset as consisting of \(N\) tuples, each with \(i\) turns, \(\{(x_{1}^{j},y_{1}^{j},x_{2}^{j},y_{2}^{j},...x_{i}^{j},y_{i}^{j})\}_{j=1}^{N}\), where \(x_{i}\) is a user prompt and \(y_{i}\) the desired output. For most instances, \(i=1\), and we train the model to output \(y^{j}\) given \(x^{j}\). However, in the case of conversation datasets, we train the model to predict \(y_{i}^{j}\) given some conversation history \(x_{1}^{j},y_{1}^{j},x_{2}^{j},...,x_{i}^{j}\). We train decoder-only models, and use teacher-forcing with loss masking to train the models, where we mask all tokens belonging to the input sequence(s) \(x_{i}\). Given \(X\) as the tokens belonging to the input, and \(Y\) as the target tokens, the loss function is:

\[L=-_{j} p_{}(t_{j} t_{<j})1&t_{j}  Y\\ 0&\]

where \(t_{j}\) is the \(j\)th input token (belonging to \(X\) or \(Y\)). See Appendix SSD for further training details.

### Tulu\(\): a Better Instruction-Tuned Model by Combining Resources

Existing studies [48; 31] (and our own evaluation below) have shown that increasing the diversity of instructions can effectively improve the performance of instruction tuning. Following this motivation, we create two mixtures of datasets:

**Human data mixture**, which comprises the best human-authored datasets, including FLAN V2, CoT, Dolly, and Open Assistant 1 (we exclude SuperNI as FLAN V2 includes most tasks in SuperNI);

**Human+GPT data mixture**, which comprises the human mixture and three additional datasets that have generations by OpenAI GPT models, including GPT4-Alpaca, Code-Alpaca, and ShareGPT.

For both mixtures, we concatenate datasets and leave exploring more complex sampling mixtures to future work. We name LLaMa models trained on the Human+GPT data mixture \(^{}\), after a hybrid camel resulting from interbreeding between different species. We differentiate the Tulu models trained from the LLaMa-2 base models by versioning them as **Tulu-1.1**.

## 4 Evaluation Setup

Evaluation of instruction-following models remains a challenging problem due to the enormous scope of "generality" and its open-ended nature. However, we argue that general-purpose models should be able to perform some core tasks before they can generalize to satisfy various practical needs. As such, we set up a multi-faceted evaluation to cover several key aspects of capabilities covering core abilities and open-ended instruction following. Our evaluations closely follow prior work on evaluating instruction-tuned models [9; 2; 47; 8; 16], but serve as the first one to compile them together for systematic evaluation.

### Facets of Evaluation

**Factual knowledge** is essential for language models to serve users' information needs. We use the Massive Multitask Language Understanding dataset (MMLU ) for measuring models' factual knowledge. MMLU consists of a set of questions about 57 subjects ranging in difficulty from elementary levels to professional levels, and its multiple-choice format makes it suitable for probing models' knowledge without worrying about the open-endedness of generations.

Figure 1: An example from ShareGPT data. We use <|role|> to set the boundary between messages. The entire sequence is encoded together, and loss is computed on the assistant parts (colored in blue).

**Reasoning** is another fundamental ability for models, especially for solving complex tasks. We use the test split of Grade School Math dataset (GSM ) to evaluate models' mathematical reasoning capabilities. We also adopt Big-Bench-Hard (BBH ), which contains 23 challenging tasks from Big-Bench , to evaluate models' general reasoning capabilities.

**Multilinguality** acts as an important perspective of models for serving people from different backgrounds. We use TyDiQA , a multilingual question answering benchmark covering 11 typologically diverse languages for testing how much models can process non-English text. We use the gold-passage setup where one passage containing the reference answer is given.

**Coding** is a particular application that people have used language models for and might be important for integrating these models with external tools . We use the HumanEval dataset  to evaluate the models' capability to generate functionally correct programs from docstrings. To avoid ambiguity with our human evaluation, we call this dataset Codex-Eval in this paper.

**Open-ended instruction following.** While the performance on the benchmarks above quantifies the models' ability at specific skills, it may not reflect how well the models can handle instructions from real users, which cover highly diverse requests and are often open-ended. For example, the popular ShareGPT dataset contains instances of users asking for programming help, resume formatting tips, educational role-playing, pronunciation suggestion, fanfiction writing, and more. We evaluate such open-ended instructability of models using both model-based evaluation (SS4.2) and human evaluation (SS4.3), both of which consist of multiple test sets from existing studies [47; 8; 26; 3; 19].

**Safety** is of particular concern regarding the fast-developing language models to ensure the ethical and proper use of them. Following LLAMa-2 , we employ ToxiGen  to measure the amount of toxic language and hate speech generation across different groups when the models are prompted to do so. We also adopt TruthfulQA  to measure how well models can avoid generating known falsehoods due to misconceptions or false beliefs while providing useful information.

For all the benchmark-based evaluations, we follow their standard metrics, while we subsample some benchmarks to a reasonable size to improve the efficiency of doing chain-of-thought reasoning. We refer the reader to Appendix SSA for the setup details.

### Model-Based Evaluation using GPT-4

To evaluate the open-ended instructability, we first adopt a model-based approach introduced in AlpacaEval . The test set consists of 805 instructions, with 252 instructions from the Self-Instruct evaluation , 188 from the Open Assistant evaluation , 129 from the helpful evaluation by Anthropic , 80 from the Vicuna evaluation , and 156 from the Koala evaluation .

We use their simulated GPT-4 annotator, which computes the win rate of the testing model as judged by GPT-4 when compared to the outputs produced by Davinci-003. We use the AlpacaEval codebase and prompts  to make our scores directly comparable to those on the AlpacaEval leaderboard4 When doing pairwise comparisons with GPT-4, the orders of model outputs are randomized to avoid position bias during evaluation . We do not evaluate vanilla LLAMA models due to them having little instruction-following ability without further prompt engineering.

### Human Evaluation

To further test the quality of the open-ended generations, we conduct a human evaluation based on 332 instructions that combine the Self-Instruct evaluation set  and Vicuna evaluation set . Inspired by Bai et al. , we design a similar interface (Figure 5) for gathering human judgments of model outputs along the following dimensions. We note that we evaluated based on our fine-tuned LLAMa-1 models, as LLAMA-2 was not available at the time of this experiment.

**Individual acceptability.** We ask human raters to assess whether each system's responses were acceptable in isolation. This is a binary decision, and we ask the raters to mark a response as acceptable if and only if the response answered the request in the query, had no significant errors, and did not have repetitive information.

**Pairwise preference.** We then ask humans to compare the outputs of two systems and select which one they think is more helpful. This is a 5-way decision, and the raters could select if one of the responses is "clearly" or "slightly" better than the other or if it is a tie implying that both responses were equally good or bad.

To get a more reliable evaluation, we recruited a group of 18 expert annotators who are researchers at AI2 or students at UW. All of them are fluent English speakers, holding bachelor's degrees or above.

## 5 Results

### Analysis of Instruction Tuning Datasets and Base Models

To understand how the instruction datasets listed in Table 1 contribute to model abilities, we evaluated LLaMa 13B models trained on these datasets using our evaluation suite. Table 3 shows the results on our benchmark evaluation set, with more extensive results in App. F. We find that:

**There is not a single best instruction tuning dataset across all tasks**. Different datasets enable different capabilities in the model. Noteworthy examples include training on CoT being particularly helpful for mathematical reasoning in GSM and Code-Alpaca being helpful for Codex-Eval. We hypothesize that success on these tasks, which are significantly different from the rest of the evaluation tasks, calls for training sets where these tasks are well-represented. Apart from constructing task-specific datasets manually, distilling task-specific data from large models also appears to be an effective way to ensure this (e.g., CodeAlpaca is distilled from Davinci-003).

**Combining datasets results in the best overall performance on the benchmark tasks.** While models trained on our combination datasets are often not the best model for a single task (being the best only in 2 out of 6 evaluation settings), they are the best when measuring average performance across tasks. This suggests that future work into better dataset mixing or instruction-tuning modular

    & **MMLU** & **GSM** & **BBH** & **TydQA** & **Codes-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\   & **EM** & **EM** & **EM** & **F1** & **P@10** & **Win \%** vs** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & & \\  Pythia 6.9B & 34.8 & 16.0 & 29.2 & 32.8 & 20.9 & 23.5 & 26.2 \\ OPT 6.7B & 32.6 & 13.5 & 27.9 & 24.1 & 8.9 & 25.9 & 22.2 \\ LLAMA 7B & 44.8 & 25.0 & 38.5 & 43.5 & 29.1 & 48.6 & 38.3 \\ LLAMA-2 7B & **49.2** & **37.0** & **44.2** & **52.8** & **33.9** & **57.3** & **45.7** \\   

Table 4: Performance of different base models after training on the Human+GPT data mixture.

    & **MMLU** & **GSM** & **BBH** & **TydQA** & **Code-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\  & **EM** & **EM** & **EM** & **F1** & **P@10** & **Win \%** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & & \\  Vanilla LLaMa 13B & 42.3 & 14.5 & 39.3 & 43.2 & 28.6 & - & - \\ +SuperNI & 49.7 & 40.0 & 43.5 & **50.2** & 12.9 & 4.2 & 20.9 \\ +CoT & 44.2 & 40.0 & 41.9 & 47.8 & 23.7 & 6.0 & 33.9 \\ +Flan V2 & **50.6** & 20.0 & 40.8 & 47.2 & 16.8 & 3.2 & 29.8 \\ +Dolly & 45.6 & 18.0 & 28.4 & 46.5 & 31.0 & 13.7 & 30.5 \\ +Open Assistant 1 & 43.3 & 15.0 & 39.6 & 33.4 & 31.9 & 58.1 & 36.9 \\ +Self-instruct & 30.4 & 11.0 & 30.7 & 41.3 & 12.5 & 5.0 & 21.8 \\ +Unatural Instructions & 46.4 & 8.0 & 33.7 & 40.9 & 23.9 & 8.4 & 26.9 \\ +Alpaca & 45.0 & 9.5 & 36.6 & 31.1 & 29.9 & 21.9 & 29.0 \\ +Code-Alpaca & 42.5 & 13.5 & 35.6 & 38.9 & 34.2 & 15.8 & 30.1 \\ +GPT4-Alpaca & 46.9 & 16.5 & 38.8 & **23.5** & **36.6** & 63.1 & 37.6 \\ +Baze & 43.7 & 10.0 & 38.7 & 33.6 & 28.7 & 21.9 & 29.4 \\ +ShareGPT & 49.3 & 27.0 & 40.4 & 30.5 & 34.1 & **70.5** & 42.0 \\ +Human data mix. & 50.2 & 38.5 & 39.6 & 47.0 & 25.0 & 35.0 & 39.2 \\ +Human+GPT data mix. & 49.3 & **40.5** & **43.3** & 45.6 & 35.9 & 56.5 & **45.2** \\   

Table 3: Comparison of different instruction tuning datasets, showing that different instruction-tuning datasets can excel in different aspects, and mixtures perform best on average. Cells are blue if the finetuning boosts the vanilla LLAMA performance, and orange if the finetuning hurts the performance.

models (e.g., mixture-of-experts ) is a promising direction for developing models that retain strong performance across all evaluation settings.

**Base model quality is extremely important for downstream performance.** We examine the impact of using different base models in Table 4, comparing LLaMa, OPT , and Pythia  models of comparable size trained on the Human+GPT data mix. Across all evaluation settings, we find that using LLaMa performs best by a significant margin, likely due to the fact that LLaMa is pretrained on significantly more tokens than the other models (see Table 2). This suggests that models pretrained on larger (or potentially higher-quality) corpora are preferable as base models for instruction tuning. The later addition of LLaMa-2 confirms this finding by showing a significant improvement can come from only the base model upgrade.

**Some datasets degrade vanilla model performance.** Notably, most datasets we evaluate cause degradation in performance on GSM and TydiQA over the vanilla base model. We hypothesise this is due to data style and quality. Many of the datasets we examine contain little to no examples of chain-of-thought-style reasoning and contain little to no multilingual data. As such, training on these datasets likely results in some forgetting of the CoT or multilingual abilities previously held by the model, resulting in degraded performance. Additionally, we note that self-instruct appears to cause degradations across most tasks, which we hypothesise is due to the relatively poor quality of the original self-instruct data, being generated by a weaker model (base GPT-3) than the other GPT-distilled datasets.

### Pushing the Limits of Open Models

Having established that (a) using a broad mix of data is best, and (b) using LLaMa as the base model is preferable to other open alternatives, we compare the performance of models trained on the Human+GPT data mix (Tulu models) across all LLaMa sizes in Table 5. We find that:

**Instruction tuning brings large benefits on top of LLaMa models at all sizes.** On average, all LLaMa models improve considerably after instruction tuning.

    & **MMLU** & **GSM** & **BBH** & **TydiQA** & **Codex-Eval** & **AlpacaEval** & **Average** \\  & **(factuality)** & **(reasoning)** & **(reasoning)** & **(multilinguality)** & **(coding)** & **(open-ended)** & **Average** \\   & **EM** & **EM** & **EM** & **F1** & **P@0** & **Win \% vs** & \\  & **(0-shot)** & **(8-shot, CoT)** & **(3-shot, CoT)** & **(1-shot, GP)** & **(0-shot)** & **Davinci-003** & \\   \\  LLaMa 7B & 31.5 & 10.0 & 33.0 & 38.4 & 20.5 & - & - \\ LLaMa 13B & 42.3 & 14.5 & 39.3 & 43.2 & 28.6 & - & - \\ LLaMa 30B & 54.6 & 36.0 & 49.5 & 55.3 & 42.8 & - & - \\ LLaMa 65B & 58.7 & 50.0 & 58.1 & 56.8 & 46.9 & - & - \\ LLaMa-2 7B & 41.8 & 12.0 & 39.3 & 51.2 & 26.8 & - & - \\ LLaMa-2 13B & 52.0 & 25.0 & 48.9 & 56.5 & 32.5 & - & - \\   \\  ShareGPT 65B & 61.3 (+2.6) & 59.0 (+9.0) & 55.8 (-2.3) & 31.6 (-25.2) & 56.2 (+9.3) & 73.6 & 56.3 \\ Human mix. 65B & 60.4 (+1.7) & 60.0 (+10.0) & 54.8 (-3.3) & 58.3 (+1.7) & 44.6 (-2.3) & 43.4 & 53.6 \\   \\  Tulu \(\)B & 44.8 (+13.3) & 25.0 (+15.0) & 38.5 (+5.5) & 43.5 (+5.1) & 29.1 (+8.6) & 48.6 & 38.3 \\ Tulu \(\)B & 49.3 (+7.0) & 40.5 (+26.0) & 43.3 (+4.0) & 45.6 (+2.4) & 35.9 (+7.3) & 56.5 & 45.2 \\ Tulu \(\) & 57.7 (+3.1) & 53.0 (+17.0) & 51.9 (+2.4) & 51.9 (-3.4) & 48.0 (+5.2) & 62.3 & 54.1 \\ Tulu \(\) & 59.2 (+0.5) & 59.0 (+9.0) & 54.4 (+3.7) & 56.6 (+0.2) & 49.4 (+2.5) & 61.8 & 56.7 \\   \\  Tulu-1.1 \(\)B & 49.2 (+7.4) & 37.0 (+25.0) & 44.2 (+4.9) & 52.8 (+1.6) & 33.9 (+7.1) & 57.3 & 45.7 \\ Tulu-1.1 \(\) & 52.3 (+0.3) & 53.0 (+28.0) & 50.6 (+1.7) & 58.8 (+2.3) & 38.9 (+7.4) & 64.0 & 52.9 \\   \\  ChatGPT & 67.9 & 76.0 & 66.1 & 51.9 & 88.4 & 83.6 & 72.3 \\ GPT-4 & 82.4 & 92.5 & 88.0 & 70.8 & 94.1 & 93.5 & 86.9 \\   

Table 5: Performance of Tulu and other of our trained models to vanilla LLaMa models and the state-of-the-art proprietary models across evaluation settings. See Table 8 for a complete list.

**Smaller models benefit most from instruction tuning.** We find that relative improvements from instruction tuning are largest for the smallest models, and shrink as models get larger. Notably, the 65B LLaMA model performs comparably or better than the 65B Tulu model on MMLU, BBH, and TydiQA. This suggests that **instruction-tuning does not help to enhance strong capabilities already present in the original model**, and also highlights that care must be taken during finetuning to avoid forgetting the base model's original capabilities.

**Tulu still lags behind state-of-the-art proprietary models.** Despite the impressive performance of Tulu 65B, it lags behind ChatGPT and GPT-4 in all evaluation settings, contrary to prior claims that models trained on these open resources can match ChatGPT . We note **we cannot discount the possibility that either ChatGPT or GPT-4 was trained on significant portions of our evaluation suite.** However, the presence of a significant gap between Tulu models and ChatGPT matches our findings in the model and human-based evaluations, which are less likely to be compromised.

### Evaluation of Potential Risks and Harms

We evaluate our models on ToxiGen and TruthfulQA to measure the degree to which different datasets are likely to yield models that generate toxic language or misinformation. We find that:

**Trends remain similar to capability-focused benchmarks.** Similarly to the results in Sec. 4.1, we find that GPT-distilled datasets yield the best overall performance and that there is a large variance in performance across datasets.

**Models trained on GPT-sourced data yield less toxic generations than GPT.** Larger models trained on GPT-distilled data appear to refuse to produce toxic generations almost entirely, despite the fact that ChatGPT and GPT-4 produce toxic generations a non-trivial amount of the time. We hypothesise this is due to our models overfitting on refusal-style behaviour, refusing to generate anything moderately toxic, while GPT models balance refusal behaviour with helpfulness to a greater extent.

**TruthfulQA performance does not scale.** Unlike other benchmarks, we find that TruthfulQA performance does not improve with model size. Further examining this, we find that larger models do output more correct facts, but also tend to provide large improvements to the model capabilities tested in Table 3.

**Datasets that encourage long, diverse generations perform best**. Intrigued by ShareGPT's performance, we plot the average number of unique tokens in model generations against the AlpacaEval win-rate in Figure 2. We find that the evaluation is **strongly correlated with the average number of unique tokens** (Pearson correlation of 0.96, \(p 0.05\)). Given GPT-4's strong performance on other tasks, we do not believe that GPT-4 evaluation is merely counting unique tokens, but this result highlights how model preference scores do not necessarily reward only model capabilities.

    &  &  \\ 
**Model \(\)** & **7B** & **13B** & **7B** & **13B** \\  LLaMA & 85.4 & 82.6 & 26.2 & 23.6 \\ + SuperNI & 85.3 & 77.3 & 26.7 & 26.2 \\ + CoT & 63.0 & 43.9 & 35.1 & 35.5 \\ + Flan V2 & 77.5 & 61.4 & 33.2 & 33.4 \\ + Dolly & 72.1 & 78.9 & 30.1 & 32.9 \\ + Open Assistant 1 & 39.2 & 5.2 & 40.9 & 48.6 \\ + Self-instruct & 89.0 & 89.3 & 22.4 & 22.4 \\ + Unnatural Inst. & 35.8 & 55.7 & 27.3 & 31.7 \\ + Alpaca & 63.2 & 58.1 & 33.5 & 39.8 \\ + Code-Alpaca & 84.3 & 92.0 & 25.1 & 26.7 \\ + GPT-Alpaca & **3.9** & 1.2 & **51.2** & 56.7 \\ + Baize & 77.2 & 41.2 & 42.4 & 43.9 \\ + ShareGPT & 5.5 & 2.5 & 45.3 & **60.0** \\  + Human mix. & 51.8 & 76.9 & 34.1 & 32.1 \\ + Tulu\(\)\(\) & 10.6 & **0.1** & 44.6 & 41.6 \\  ChatGPT & 27.7 & & 75.2 \\ GPT-4 & 10.6 & & 82.3 \\   

Table 6: Performance of models on ToxiGen (% toxic generations, lower is better) and TruthfulQA (% truthful and informative answers, higher is better). See Table 9 and Table 10 for the full breakdown of these two evaluations.

[MISSING_PAGE_FAIL:9]

Related Work

Instruction Tuning of LMsFinetuning language models on diverse instruction sets alongside regular samples has been shown to greatly improve zero-shot performance on unseen tasks [39; 51; 49; 32; 9; 48], and serves as a good base for further finetuning in supervised settings . Increasing the number of diverse prompts , the number of tasks [48; 9], and diversity of data  have all been shown to be important to performance. More recently, a growing number of models have made use of model-generated instruction-augmented data [47; 23; 25; 53], most often generated or collected from larger proprietary models such as ChatGPT or GPT-4 [8; 15; 43; 52; 36; inter alia]. Despite the explosion of models and datasets, evaluation remains inconsistent and difficult, with different evaluation setups used across models. Prior work has examined models trained on varying dataset sources with the aim of identifying 'the best mixture' [31; 24], but is often limited to examining only benchmark performance, and covers a smaller number of instruction sources than in this work. QLoRA  also explores (quantized and parameter-efficient) instruction-tuning of recent models and datasets, but explores a smaller range of models, datasets, and evaluations than this work.

Evaluation of LMsGiven the success of LMs on NLP and instruction-following tasks, many evaluation frameworks have been proposed. Frameworks such as HELM  and LM Evaluation Harness  cover a broad range of NLP tasks but are often focused on evaluating the base models as opposed to instruction-tuned ones. Similar to our work, Chung et al.  focus on a series of benchmark evaluations focused around factuality and reasoning, but largely neglect open-ended instruction following abilities. Releases of large (closed) proprietary models such as GPT-4  and PaLM v2  are often accompanied by comprehensive evaluations over a wide variety of benchmarks, although both similarly neglect evaluation of open-ended instruction following, and without open releases of pretraining or instruction tuning data there is no way to test for evaluation data contamination.

Recently, evaluation frameworks such as AlpacaEval  and Chatbot Arena  have been proposed to evaluate the open-ended instruction following ability of LMs, moving beyond benchmark-based evaluations. These either make use of other models (in the case of AlpacaEval) or humans (in the case of Chatbot Arena) as annotators for judging model generations. We make use of this recent work and evaluate our models on traditional benchmarks, model-based evaluation, and human-based evaluation. Concurrent to this work, Gudibande et al.  examine models trained on GPT model outputs and argue that such models learn to mimic only the style, not the content, of their teacher GPT models. While we similarly find that existing datasets fail to train models close to strong proprietary models, the diversity of performance we observe across datasets suggests that significant performance improvements can be achieved through imitation data, so long as it contains a diverse and wide-ranging set of skills and domains.

## 7 Conclusion

In this work, we provide an extensive evaluation of a wide variety of publicly-available resources for instruction-tuning models, and compare them to the strongest proprietary models currently available. We find that using strong base models is vital to performance, combining datasets works best on average (but does result in slight performance drops compared to best performance in specific tasks), and our strongest open models do not yet match ChatGPT or GPT-4. Furthermore, we believe that our evaluation highlights the need for the continued development of strong base models and broader, diverse datasets. Finally, we hope that our evaluation and released code and models enable more comprehensive evaluations and spur research to close these gaps and shed insights on all large language models, closed or open.