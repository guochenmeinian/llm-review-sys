# Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification

Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification

Zhaorui Tan\({}^{1,2}\), Xi Yang\({}^{1}\), Qiufeng Wang\({}^{1}\), Anh Nguyen\({}^{2}\), Kaizhu Huang\({}^{3}\)

\({}^{1}\) Xi'an-Jiaotong Liverpool University

\({}^{2}\) University of Liverpool

\({}^{3}\)Duke Kunshan University

Corresponding authors. <Xi.Yang01@xjtlu.edu.cn; kaizhu.huang@dukekunshan.edu.cn>

###### Abstract

Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.

## 1 Introduction

One critical challenge in visual classification models is their ability to generalize effectively to unseen samples or unknown classes. For instance, a model trained on real images of various animals should ideally classify animal sketches accurately (referred to as multi-domain generalization classification ) or discover novel categories not present in the training set (referred to as generalized category discovery ). These problems are prevalent in real-world scenarios, where training data-target pairs are usually insufficient, and labeling is time-consuming so that not every data is paired with a label. Meanwhile, test data is likely to contain shifts in both data and targets, making it essential to propose methods that generalize to border scenarios.

Regularization terms, such as \(L_{2}\) regularization leading to weight decay, are commonly employed during training to improve a model's generalization capabilities. However, the \(L_{2}\) regularization

Figure 1: GradCAM  visualizations for the unknown class ‘person’ across seen and unseen domains of the GMDG baseline with \(L_{2}\) regularization that is trained without and with L-Reg, respectively. Both experiments share the same hyper-parameters, except the latter uses the L-Reg.

is _parametric-based_ rather than _sample-based_, which may lead to ambiguous interpretability . As illustrated in Fig. 1, the model trained solely with \(L_{2}\) regularization exhibits low interpretability. Other regularization terms [57; 58; 59] attempt to improve the interpretability of deep learning models for sequential signals rather than vision, whereas  proposes a regularization term to enhance interpretability for robustness in visual classification models rather than generalization. Drawing inspiration from logical reasoning has shown promise for better generalization and interpretability in various tasks. Current work unveils the effectiveness of logical reasoning in generalization tasks, such as boosting performance in length generalization [1; 3; 2; 60] and abstract symbol relational reasoning [10; 36] (e.g., mathematical solving and psychological tests). Several efforts, such as , explore the explicit entropy-based logical explanations of neural networks for image classification, confirming the presence and interpretability of logical reasoning within visual tasks. Yet, there are limited studies tackling the generalization of visual classification tasks through the lens of logical reasoning.

This paper studies two pivotal questions corresponding to the above: _1) How does logical reasoning relate to visual tasks such as image classification? 2) How can we derive a logical reasoning-based regularization term to benefit generalization?_ To achieve these, we correlate the image classification procedure in computer vision with the framework of logic studies , positing that training an image classifier involves learning a _good general_ logical relationship between images and labels via an encoder. This good general logic is attained when the semantics generated by the encoder and classifier can be combined to form atomic formulas. Our exploration leads to the introduction of a sample-based Logical regularization term named L-Reg. We reveal that L-Reg efficiently reduces the _complexity_ of the model from two aspects: 1) L-Reg leads to a balanced feature distribution in the semantic space; 2) L-Reg reduces the number of weights with extreme values in the classifier.

Intuitively, the complexity reduction achieved by L-Reg stems from its ability to filter out redundant features or semantics, focusing instead on the minimal yet sufficient semantics for classification - defined as semantic support in Definition 3.2, where the interpretability also emerges. This filtering feature benefits the generalization when there is a domain shift in data where the domain-dependent features are ignored for classification. Moreover, it further promotes generalization when unlabeled data from the unknown classes is present. If such data lacks the semantic support associated with known classes, it is then classified as belonging to an unknown class, and its corresponding semantic supports are extracted. These capabilities equip L-Reg with explicit interpretability. As Fig. 1 shows, with L-Reg, the model can identify the unknown class 'person', and pinpoint faces which are the crucial features for classifying this category. In contrast, the model trained solely with \(L_{2}\) (without L-Reg) focuses on the ambiguous features for classification.

Rigorous theoretical analysis and experimental results validate that L-Reg yields better generalization across diverse scenarios. Specifically, L-Reg facilitates better performance under the aforementioned multi-domain generalization and generalized category discovery tasks, whose settings are presented in Fig. 2 (a)(b). Furthermore, to evaluate L-Reg's robustness, we introduce a more complex real-world scenario, as shown in Fig. 2 (c), where unlabeled images may not only belong to unknown classes, but also originate from unseen domains. Even in this challenging context, L-Reg is still able to consistently demonstrate notable improvements in generalization, underscoring its practical utility and effectiveness. Our code is available at https://github.com/zhaorui-tan/L-Reg_NeurIPS24.

## 2 Preliminaries and generalization settings for visual classification

Consider paired \((X,Y)(,)\), \((X_{s},Y_{s})(_{s},_{s})\), and \((X_{u},Y_{u})(_{u},_{u})\) denote all sets of inputs and labels, seen paired subsets of \((X,Y)\), and unseen paired subsets of \((X,Y)\), respectively. Note that \(X_{u},Y_{u}\) may be accessible for the model separately, but their pairing relationships are not accessible. Let \(D\) denote the possible domains, with \(D_{s},D_{u} D\) representing the seen and unseen domains. In classification tasks, an encoding function \(g(x) Z^{M}\) is commonly introduced to map \(X\) into the latent feature set \(Z\), where each latent feature has \(M\) dimensions. A predictor \(h(Z)^{K}\) maps \(Z\) to predictions \(\), where \(K\) denotes

Figure 2: Diagrams of different generalization settings in visual classification tasks.

the number of classes and the dimensions of predictions. \(P()\) and \(H()\) symbolize probability and entropy, respectively. This paper discusses two typical cases for generalization in image classification tasks: (1) _Data-shift generalization:_\(X_{s}\) and \(X_{u}\) have distribution shifts, such as multi-domain generalization (mDG); and (2) _Target-shift generalization:_\(Y_{s}\) and \(Y_{u}\) have distribution shifts, which stands for tasks like generalized category discovery (GCD). We additionally explore a challenging scenario called _All-shift generalization:_ both \(X_{s}\) and \(X_{u}\), \(Y_{s}\) and \(Y_{u}\) have distribution shifts, which is a combination of mDG and GCD tasks (mDG + GCD). The following lists the detailed settings for generalization. Please refer to Fig. 2 for brief diagrams.

**Data-shift generalization: Problem setting for mDG.** Illustrated in Fig. 2 (a), mDG  intends to generalize well to unseen domains having the objective of \( H(X_{s},Y_{s} D_{s})\) and expecting the model to be generalized to \(X_{u}\) when predicting \(Y_{u}\) from the unseen domain \(D_{u}\). In such cases, \(Y_{u}\) is fully accessible to the model since \(Y_{s}\) and \(Y_{u}\) share the same domain: \(_{s}=_{u}\) but there are shifts in \(X\) where \(_{s}_{u}\).

**Target-shift generalization: Problem setting for GCD.** GCD  (Fig. 2 (b)) aims to discover possible unseen labels among unlabeled datasets \(X_{u}\). The challenge is that the samples in \(X_{u}\) may belong to known classes or unknown classes: \(_{s}_{u}\) and probably \(_{s}_{u}\). The model should be able to distinguish the samples from the known classes and cluster the samples for unknown classes simultaneously. Note that \(X_{u}\) is used for model training, but the relationship between \(X_{u}\) and \(Y_{u}\) is unseen for the model. In summary, shifts exist between \(Y_{s}\) and \(Y_{u}\) but not between \(X_{s}\) and \(X_{u}\).

**All-shift generalization: Problem setting for mDG + GCD.** To explore the generalization problem further, we introduce a setting that is the combination of mDG and GCD as shown in Fig. 2 (c). Specifically, the model is trained on the labeled pairs \((X_{s},Y_{s})\) and unlabeled set \(X_{u}\) from the seen domains \(D_{s}\); \(X_{u}\) may belong to known and unknown classes. Furthermore, the model is tested on \(X_{u}\) from the unseen domain \(D_{u}\), where \(X_{u}\) may also come from the known and unknown classes. In this setting, the model is expected to 1) classify samples to the seen classes and discover the unseen classes among unlabeled samples from seen domains and 2) generalize this ability to the samples from the unseen domain. In this scenario, \(X_{s}\) and \(X_{u}\) have shifts, and so do \(Y_{s}\) and \(Y_{u}\).

For all aforementioned generalization settings, the objective can be summarized as minimizing the _generalization loss_:

**Definition 2.1** (Generalization loss).: Let the target model \(f^{*}:f^{*}(X,Y):X Y\), can generalize across both seen and unseen sets \(X,Y\). Denote its trainable form \(f\), which is only trained on the seen sets. The generalization loss for the unseen sets is defined as:

\[GL(f,f^{*},(X_{u},Y_{u}))=_{(x,y)(X_{u},Y_{u})}||f(x,y)-f^{*}(x, y)||_{2}.\] (1)

## 3 Logical regularization for generalization in image classification

Under the problem settings defined in Section 2, we introduce Logic regularization (L-Reg) targeting the objective:

\[_{h,g}_{z_{i} z,z Z}[H(|z_{i},D)]-_{z Z }[H(|Z,D)],\] (2)

Figure 3: Visualizations of classifiers’ weights form models trained using GMDG on PACS dataset without and with L-Reg under mDG+GCD setting, respectively. Both experiments share the same hyper-parameters using Regnety-16g backbone, except the latter uses additional L-Reg.

where \(^{K}=h g(X)\) is the prediction set. The corresponding Logic regularization loss (L-Reg) is defined as:

\[ L_{L-Reg}=&-_{i=1}^{M} [_{j=1}^{K}_{j,i}(^{T}Z)_{j,i}(^{T}Z) ]\\ &+_{j=1}^{K}[_{i=1}^{M}_{j,i}(^{T}Z)(_{i=1}^{M}_{j,i}(^{T}Z))],\] (3)

where \(_{j,i}(^{T}Z)\) denotes the value at the \(i,j\) position of \(softmax(^{T}Z)\) and the soft-max function is applied at the last dimension. By incorporating other existing methods' losses denoted by \(L_{main}\), the overall loss is formulated as:

\[L_{all}=L_{main}+ L_{L-Reg},\] (4)

with a weight \(\) applied to balance two losses. As depicted in Fig. 1, L-Reg plays a pivotal role in extracting crucial features for image classification, thus enhancing generalization capabilities. This beneficial outcome can be attributed to two primary factors:

**Reducing classifier complexity:** L-Reg streamlines the complexity of the classifier itself, as depicted in Fig. 3 (a). Notably, the heat map of the model with L-Reg displays fewer extremely valued weights, evidenced by the diminished presence of intense blue and red colors. This reduction implies that the classifier focuses on leveraging semantically rich and relevant features for decision-making (classification), sidelining the less relevant ones. Additionally, Fig. 3 (b) reveals a reduction in the number of semantic features used to classify each class.

**Balancing feature complexity:** L-Reg results in a more balanced distribution of features compared to the baseline, as illustrated in Fig. 4. This balanced distribution suggests the elimination of certain extracted semantics characterized by dominant frequencies across all samples. Semantics that occur frequently across samples often lack decisiveness for classification. Hence, reducing their prominence contributes to more expressive feature space and less complex feature distributions. Coupled with the reduced classifier complexity, a simplified classifier achieved through L-Reg facilitates improved generalization across various settings. Specifically, the top row also indicates the distance between the feature distributions of the known and unknown classes, which is enlarged; thus, they are more dividable, leading to classification improvements.

We present a logical-based theoretical analysis in Section 3.1 and provide the derivation details of L-Reg in Section 3.2. In addition, we discuss the efficacy of L-Reg under various generalization settings in Section 4. Furthermore, L-Reg serves as a plug-and-play loss function that is compatible with most existing frameworks. We conduct experiments applying L-Reg to various established approaches across different generalization settings, as outlined in Section 5.

### Logical framework for visual classification

This part provides the connections between logical reasoning and visual classification tasks. We would like to remind readers of the framework for studying logics and link it with our practical scenarios.

**Definition 3.1**.: Following , a logic \(\) is defined as a five-tuple in the form:

\[= F_{},M_{},_{},mng_{},_{},\] (5)

where 1) \(F_{}\) denotes the set of formulas formed by images and labels (\(X,Y\)); 2) \(M_{}\) represents different domains \(D\) of \(X\); 3) \(_{}\) is a binary relation relating the truth of whether the formulas are true or false, which has \(_{} M_{} F_{}\); 4) \(mng_{}:F_{} M_{}\) Sets defines the meaning of \(X\) as determined by classifiers, where Sets indicate the class of all sets. (5) \(_{}\) symbolizes the provability relation of \(\), evaluating formulas formed by \(mng_{}\) is true or false in one possible world, such as the estimation criteria. More details of \(\) can be seen in Appendix B.

Figure 4: Visualizations of latent features form models trained using GMDG on PACS dataset without and with L-Reg under mDD+GCD setting using RegNetY-16G backbone, respectively.

For clarity, we specify \(_{(X_{s},Y_{s})}= F_{(X_{s},Y_{s})},D,_{(X_{s},Y_{s}) },h,_{(h(X),Y)}\) as the logic formed on the given \(X,Y\) sets. With the goal for logic to generalize across a broader scenario and provide extrapolation across all possible formulas in \(\), a good general logic \(^{*}\) should be derived from \(\) through the feature extractor \(g\):

\[^{*}= F_{(g(X_{s}),Y_{s})},D,_{(g(X_{s}),Y_{s}) },h,_{(hog(X),Y)},s.t.,_{(hog(X),Y)}=_{(g(X_{s} ),Y_{s})}.\] (6)

Importantly, as a good general logic, \(F_{(g(X_{s}),Y_{s})}\) and \(h\) in \(^{*}\) should form the _atomic formulas_, i.e., the tuple of terms with a predicate: \(h g(x)\) belongs/not belongs to class \(y\) in domain \(d\ \)\(Ture/False,\ \ \ x,y,d X,Y,D\), which makes that \(_{(hog(X_{u}),Y_{u})}=_{(g(X_{s}),Y_{s})}\) still holds. We simply denote one atomic formula in the form of \(h(g(x),y,d)\) mapping to binary values. Additionally, \(_{(hog(X),Y)}=_{(g(X_{s}),Y_{s})}\) in Eq. (6) can be safely omitted in the rest of the paper. Please see more details about the conditions of the good general logic in Appendix B.

An additional tool is necessary to convert the logic problem into a continuous form, enabling the application of machine learning algorithms. The conditional entropy-based method enables a logically sound derivation of knowledge from the provided dataset with constraints . Specifically, the probabilistic inference process adheres to a probabilistic version of Modus Ponens: \(A B,A B\) (if \(A\) then \(B\); not \(A\) therefore not \(B\)). It is important to note that the logical propositions in probabilistic Modus Ponens are uncertain, with the conditional probability replacing the material implication \(A B\). This framework allows us to interpret logical deduction through the lens of entropy. Therefore, for Eq. (6) which implies

\[ h g,\ (x,y)(X,Y),\  d D,\ h g(x) y,\] (7)

finding \(h g\) through optimization is equivalent to

\[_{h,g}_{(x,y)(X,Y),d D}P(y|g(x),d)- _{h,g}_{(x,y)(X,Y),d D}H(y|g(x),d)+,\] (8)

where \(\) denotes any other possible regularization.

As the logical framework for image classification takes shape, it becomes evident that the unresolved question of identifying an appropriate function \(g\) to generate suitable atomic formulas emerges as a critical factor in ensuring the effectiveness of the overarching logic \(^{*}\). This paper proposes L-Reg as the regularization to ensure \(F_{(g(X_{s}),Y_{s})}\) are formed by atomic formulas in Section 3.2.

### Constructing atomic formulas using L-Reg

In this part, we show the derivation details of L-Reg the aims to ensure the formation of suitable atomic formulas, as depicted in Eq. (6). As highlighted in , current algorithms may induce implicit biases towards unseen data, resulting in varied solutions for such data. However, expecting an algorithm to generalize effectively to unseen data domains without appropriate incentivization, such as specifically designed regularization, is unreasonable. Therefore, we aim to enhance the generalization capability of models by employing a logic-based regularization approach. To this end, we introduce the concept of _semantic support_ for image classification.

**Definition 3.2** (Semantic support).: We denote \(z=g(x)\), where \(z Z\), as a set of compositions of these semantics: \(z:=\{z^{i}\}_{i=1}^{M}\), where \(M\) is the number of dimensions or semantics. Notably, not all semantics in \(z\) may be useful for deduction or inference. We define the subset \(\) of \(z\), extracted from the sample \(x\), as the semantic support of \(x\) if \(\) is sufficient for deducing the relationship between \(x\) and a \(y\).

For instance, if the subset \(\{z^{1},z^{2}\} z\) is sufficient for accurate inference, the values of other semantics \(\{z^{i}\}_{i=3}^{M}\) will not impact the inference process. When \(\{z^{1},z^{2}\}\) constitutes the minimal combination of semantics required for inference, it is termed the semantic support. We denote \(\) as the set of semantic supports of \(X\) for deducing each individual class.

**Derivation of L-Reg.** Regarding Eq. (6), if the semantic supports and their relationship with \(Y\) form atomic formulas, Eq. (6) holds as a good general logic, and the generalization would be improved. Thus, we aim to learn the latent features \(Z\), which contain sufficient semantic supports for the deduction of \(Y\):

\[, z,\ (z,y)(Z,Y), d D, \ h(|d) y.\] (9)

Specifically, \(g()\) should meet the following:

\[(_{i},y_{i}),(_{j},y_{j})(Z,Y), d D,\ y_{i}  y_{j}_{i}_{j},\] (10)i.e., the semantic support set for each class should be distinct. The multiple-class classification task has that \(,|| M\). Under the constraints demonstrated in Eq. (9) and Eq. (10), we need to achieve the following through optimization:

\[_{h,g}H(Y|g(),D),_{h,g}H(Y|g(),D) _{h,g}H(Y|g(),D)-H(Y|g(),D),\] (11)

where \(\) denotes the negation of \(\), i.e., the set of semantics which does not include semantic support.

Intuitively, Eq. (11) regularizes that the model should be able to judge whether a sample belongs to a class by using a minimal set of semantic supports; simultaneously, the semantic support sets are also implicitly disentangled for each class, not only for maintaining rich and useful semantics but also for enhancing the independence of deduction of each class. The actual collection of \(\) appears to be intractable during optimization. Hence, we resort to deriving its bounds. Regarding Eq. (11), its former term can be elaborated as follows:

\[H(Y|g(),D) H(Y|h(z_{i}),D)_{z_{i} z}[H(Y|g(z_{i}), D)],\] (12)

where \(z_{i}\) is minimal semantics form \(z\), and \(_{i=1}^{M}H(Y|g(z_{i}),D)\) is the upper-bound for \(_{h,g}H(Y|g(),D)\). Therefore, minimizing \(_{i=1}^{M}H(Y|g(z_{i}),D)\) is equivalent to minimizing \(H(Y|g(),D)\). Meanwhile, for the latter in Eq. (11), we have:

\[H(Y|g(),D) H(Y|g(z),D),\] (13)

where \(H(Y|h(z)),D)\) is the lower-bound for \(_{h,g}H(Y|g(),D)\). Combining the aforementioned bounds, we have the L-Reg objective as Eq. (2).

**Interpretability of semantic supports roots in forming atomic formulas.** The atomic formula \(^{y}\) is of the form \(h(g(x),y,d)\). Our aim is to find the good (most) general \(^{y*}^{y}\) for \(y\) class from which the interpretability of L-Reg is derived. Consider \(_{1}^{y},_{2}^{y}^{y}\), if \(_{1}^{y}\) is more general than \(_{2}^{y}\), there will be a substitution \(\) such that \(_{1}^{y}=_{2}^{y}\). \(^{y*}\) should meet \(^{y*}=_{1}^{y}^{y}\), which infers that \(^{y}=z^{y}\) (cf. Eq. (9)) for predication of \(y\) where \(^{y}\) is the semantic support. Note here that the form of \(^{y}\) is constructed for \(y Y\), i.e., predicate whether the sample belongs to the \(y\) class. Considering multiple classes \(y_{i},\,y_{j} Y,i j\), it has \(^{y_{i}*}^{y_{j}*}\) thus \(^{y_{i}}^{y_{j}}\) (cf. Eq. (10)), which constrains that different minimal semantic supports should be used for predicting different classes. The interpretability of L-Reg is based on \(^{y*}\), compelling the model to use distinct minimal semantic supports for each class. These minimal semantic supports can be interpreted as the most critical features for efficient prediction. For example, as shown in Fig. 1, the model with L-Reg has learned the facial features of the person class (see more examples in Appendix Figs. 7 to 12), forming the (informal) atomic formula \(h(,,d D)\). Similarly, it also leads to \(h(,,d D)\).

## 4 L-Reg under different generalization settings

**L-Reg under data-shift generalization.** The task mDG endeavors to facilitate a model's ability to generalize to unseen domains by fostering invariance across seen domains . In the context of mDG, the term \(|D| 2\) in Eq. (8) typically denotes multiple domains. Traditionally, existing methods focus on minimizing domain gaps, leading to remarkable results [25; 50]. However, it is noteworthy that even when the domain gap is effectively minimized, and \(|D|=1\) for the latent features can be considered, L-Reg still demonstrates its efficacy in promoting the generalization of \(X_{u}\) from \(D_{u}\).

**Proposition 4.1** (Effectiveness of L-Reg in enhancing data-shift generalization.).: _Assume the gap across all domains is well minimized. Let \(f^{*}\) denote the target model that generalizes to the data \(X_{u}\) from the unseen domain with the lowest complexity. For a model \(f^{R}_{(X_{s},Y_{s})},f_{(X_{s},Y_{s})}\) trained under the data-shift generalization setting (i.e., \((X_{s},Y_{s})\) is accessible and \(_{s}=_{u}\)). We have:_

\[GL(f^{R}_{(X_{s},Y_{s})},f^{*},X_{u}) GL(f_{(X_{s},Y_{s})},f^{*},X_{u}).\] (14)

Please see proof details in Proposition C.1. To illustrate Proposition 4.1, consider the following intuitive example: In the seen domains, all cats are either black or white, while all dogs are brown. Now, imagine encountering a sample labeled 'a brown cat' from an unseen domain. Without the application of L-Reg, the model might erroneously classify it as a dog. However, with L-Reg in 

[MISSING_PAGE_FAIL:7]

DomainNet . Following MIRO  and GMDG , the RegNetY-16GF backbone with SWAG pre-training ) is used. Specifically, we train the backbone using GMDG with L-Reg. Accuracy is adopted as the evaluation metric, and the results of the averages from three trials of each experiment, with standard deviations, are presented. See Supplementary H for more experimental details.

**Results.** The experimental results presented in Table 1 demonstrate the efficacy of L-Reg in improving the performance of GMDG across all datasets in mDG classification tasks. Notably, more substantial improvements are observed when the GMDG baseline achieves relatively low accuracy. These observed enhancements provide empirical support for Proposition 4.1. Please see using L-Reg with basic ERM in Appendix E. For detailed insights into each domain within each dataset, please refer to Appendix H.1.

### Experiments on GCD

**Experimental settings.** We validate our approach through training PIM additionally with L-Reg. Six image datasets are adopted to validate the feasibility of our proposed L-Reg with PIM compared to other competitors, including three generic object recognition datasets, CIFAR10 , CIFAR100  and ImageNet-100 ; two fine-grained datasets CUB  and Stanford Cars ; and the long-tail dataset Herbarium19 . Following prior works [54; 16], we use the proposed accuracy metric from  of all classes, known classes, and unknown classes for evaluation. Please see a detailed description of the experimental setup in Appendix H.2.

**Results.** The average results across all datasets for utilizing L-Reg with PIM are presented in Table 2, while detailed dataset-specific information is available in Appendix H Table 17. The results highlight that L-Reg consistently increases the accuracy of all unknown classes across all datasets, thus confirming the validity of Proposition 4.2. However, it is notable that L-Reg may marginally compromise the performance of known classes, as it reduces the size of semantic support for deducing \(Y\), thereby reducing the information available for known classification. Nevertheless, this compromise is deemed acceptable given the significant improvements observed for the unknown classes.

### Experiments on mDG + GCD

**Experimental settings.** We utilize datasets designed for mDG tasks to conduct mDG + GCD experiments. During the training stage, only samples from seen domains are available, with half of the classes masked as unknown, and only their unlabeled data are utilized. Notably, even though all the unlabeled data originates from unknown classes during training, this prior knowledge is not assumed or constrained, aligning the setting with GCD. Similar to mDG, we adopt the leave-one-out cross-validation method. This entails testing each domain in each dataset as the unseen domain. The performance is tested on unseen domains by employing GCD metrics. To validate L-Reg's efficacy comprehensively, we re-implement four methods under the mDG + GCD setting, testing them both with and without L-Reg. The four methods include ERM, PIM, MIRO, and GMDG. ERM serves as the baseline approach without additional regularization, while PIM maximizes information without minimizing domain gaps. MIRO and GMDG focus on minimizing domain gaps, with GMDG offering a comprehensive approach in this regard. It is worth noting that PIM has been re-implemented. For further experimental details, please refer to Appendix H.3.

**Results.** The averaged results across all unseen domains of all datasets are summarized in Table 3. For a detailed breakdown of results for each domain in each dataset, please refer to Appendix H.3. As discussed in Proposition 4.1 and Proposition 4.2, a noticeable trend is observed wherein, as the

   Average & All & Known & Unknown \\  K-means  & 44.7 & 46.0 & 43.9 \\ RankRankRankRankRank (2) (TPAMI-2) & 88.6 & 54.6 & 25.6 \\ U(ND+ 19) (ConvC-2) & 51.2 & 74.5 & 36.7 \\ ORCA 13(1) (Ck-2) & 46.3 & 51.3 & 41.2 \\ ORCA - VINTG & 56.7 & 65.6 & 49.9 \\ GCD 13 (CKP-22) & 60.4 & 71.8 & 52.9 \\ RGB  (ConvC-10) & 62.0 & 72.5 & 53.4 \\ TIM  (NemiP-20) & 62.7 & 72.6 & 56.4 \\  PIM (16) (ICCV-2) & 67.4 & **79.3** & 59.9 \\  PIM (**R+Reg**) & **68.8** & **79.0** & **56.7** \\   

Table 2: GCD results: Average results across all datasets of PIM with L-Reg. Improvements and degradation are highlighted in red and blue, respectively.

   Method & Domain gap & All & Known & Unknown \\  ERM & Not & 46.9 & 53.3 & 23.54 \\
**+LR-Reg** & minimized & 45.0 & 61.3 & 21.63 \\  Img. & **0.81** & **2.09** & -1.91 \\ PIM & Not & 46.95 & 0.35 & 25.90 \\
**+LR-Reg** & minimized & 47.27 & 60.83 & 26.34 \\  Img. & 0.32 & 0.48 & 0.57 \\  MIRO & Not sufficiently & 49.67 & 68.86 & 25.79 \\
**+LR-Reg** & minimized & 52.11 & 71.26 & 26.49 \\ Img. & 2.44 & 23.90 & 0.71 \\  GMDG & 47.94 & 68.75 & 30.68 \\
**+LR-Reg** & minimized & 51.94 & 69.87 & 27.68 \\ Img. & 4.00 & 1.12 & 7.01 \\   

Table 3: MDG+GCD results: Averaged accuracy scores for all, known and unknown classes across all five datasets. Improvements and degradation are highlighted in red and blue, respectively.

domain gap is gradually minimized, the improvements for unknown classes increase, with the best results achieved using GMDG with L-Reg.

**L-Reg forms atomic formulas and improves interpretability.** Furthermore, Fig. 5 provides visual insights into the behavior of models trained with L-Reg. Evidently, these models tend to focus on minimal semantics sufficient for class distinctions. For the known classes, the efficacy of L-Reg can be intuitively understood as extracting the minimal semantic supports for a given class label. For instance, the presence of a guitar's fingerboard, even in unseen domains, helps classify a sample as belonging to the guitar category, whose informal forms can be denoted as \(h(,,d D)\) and \(h(,,d D)\). For all known classes, samples with these minimal semantic supports are recognized accordingly. In contrast, if a sample lacks these minimal supports for any known class, it is very likely categorized as an unknown class. This behavior stems from Paper Eq.10 which ensures \(^{y_{i}*}^{y_{i}*}\) through constraining \(^{y_{i}}^{y_{j}}\). L-Reg further enhances the model's ability to identify minimal supports for unknown classes by filtering out co-covariant features associated with other classes and thus generalizing to unseen domains. Therefore, the very interpretable features for unknown classes from unseen domains can be extracted using L-Reg. Fig. 5 (right side) demonstrates that the model with L-Reg can even extract facial features for the unknown person class and can generalize this to the unseen domain. Similarly, here we obtain (informal) atomic formulas as \(h(,,d D)\), \(h(,,d D)\).

However, as shown in Row 3, significant domain shifts, such as those between the sketch domain and other domains, pose challenges. Specifically, the differences between the stick-figure style of sketches of persons and figures from other domains can hinder the model's ability to cluster sketches with other domains' figures when the class label is unknown. Thus, under this circumstance, the model may fail to extract meaningful features from those sketches. We acknowledge this limitation and will explore solutions in future work.

**L-Reg should be applied to features from deep layers.** One crucial precondition highlighted in the theoretical analysis is that L-Reg operates effectively with a representation \(Z\), where each dimension represents independent semantics. The semantic features usually come from the deeper layers of the model architecture . However, Table 4 shows that applying L-Reg to features from earlier layers, which may not necessarily represent semantics, leads to a degradation in performance for known classes, albeit improving performance for unknown classes. This phenomenon arises due to the potential interdependence among features from earlier layers, resulting in penalization that may hinder the capture of semantic supports essential for known classes. To ensure generalization improvements without significant compromise to the performance of known classes, we advocate for applying L-Reg specifically to features extracted from deeper layers, such as the bottleneck layer. These suggest that the compromised results observed in Table 2 could be attributed to the less depth of the model structure, which fails to provide the expected semantic features.

### Apply L-Reg to congestion prediction for circuit design.

**Experimental settings.** We also test L-Reg in Congestion prediction on the CircuitNet  dataset by using CircuitFormer  backbone. The congestion prediction is for circuit design and benefits from logical reasoning-based approaches. All parameters, except for L-Reg, remain consistent with CircuitFormer, and we follow its metrics.

   & All & Known & Unknown \\  GMDG & 58.33 & 91.46 & 10.18 \\  L-Reg: Deep layer & **67.82** & **91.86** & 31.33 \\ L-Reg: Earlier and the deep layers & 58.97 & 80.73 & **35.05** \\  

Table 4: Averaged results of applying L-Reg to different layers across domains in PACS.

Figure 5: GradCAM visualizations of GMDG trained without and with L-Reg. The seen, unseen domains and known, unknown classes are denoted.

**Results.** Table 5 shows the results of prediction results on the CircuitNet dataset. We also include the results of Gpdl with UNet++ and CircuitFormer for better comparison. Notably, the improvements brought by CircuitFormer with L-Reg across all metrics, especially for the person metric can be observed. The consistent improvement with L-Reg across all metrics indicates L-Reg's feasibility.

## 6 Related work

**Logical reasoning for deep learning.** Current studies focus on length generalization or symbolic reasoning in the logic-based scope. For length generalization,  proposes the generalization to the unseen setting, theoretically verifying that commonly used models can generalize to the unseen and degree curriculum promotes the generalization ability of the transformer, followed by [3; 2; 60]. Another branch is to improve the logical reasoning ability for abstract symbols, such as learning the logical-based temples and expecting the model to generalize to unseen samples [10; 36]. These studies are closely related to languages, such as generating longer answering sequences or solving mathematical problems in large language models, lacking explicit connections to visual tasks.  delves into the logical explanations in image classification by explicitly extracting logical relationships. While this logical-based approach sheds light on the interpretability of image classification models, its specific benefits for visual generalization remain relatively unexplored.

**Multi-domain generalization.** Current approaches for mDG in image classification focus on learning invariant representation across domains. Previous approaches like DANN  minimize feature divergences between source domains. CDANN , CIDG , and MDA  consider conditions for learning conditionally invariant features. MIRO  and GMDG  take advantage of pre-trained models to improve generalization. Specifically, in comparison to MIRO, GMDG proposes a general entropy-based learning objective for mDG and sufficiently minimizes the domain gaps, yielding better generalization results.

**Generalized category discovery.** Generalized category discovery, pioneered by , addresses unlabeled samples with both known and unknown classes. Furthermore, PIM  integrates InfoMax into generalized category discovery, effectively handling imbalanced datasets and surpassing GCD on both short- and long-tailed datasets.

## 7 Conclusion

This paper presents L-Reg, a logical regularization approach tailored for image classification tasks using logic analysis frameworks. L-Reg yields better generalization across different settings by fostering balanced feature distributions and streamlining the classification model's complexity. Rigorous theoretical analyses and empirical validations underscore its efficacy, as L-reg consistently improves generalization performance with different frameworks under various scenarios.

**Limitation.** L-Reg narrows the extent of semantic supports, potentially diminishing the amount of information available for classification and leading to certain trade-offs in the performance of seen datasets. This effect is evidenced by the slight decline in the accuracy of known classes when L-Reg is applied, as shown in Table 2. A similar phenomenon is observed in Fig. 5, where the model fails to recognize a person in the sketch domain lacking facial features. Analysis from Table 4 suggests that these compromises may result from improper \(Z\). Future work should focus on mitigating potential compromises on seen datasets by exploring strategies for better capturing \(Z\) through improved model architecture design. We offer more experimental results of possible solutions to this limitation in Appendix G, such as further constraining the independence of each dimension in \(Z\). Those results may suggest a direction for future work.