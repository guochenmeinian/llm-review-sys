# Likelihood Ratio Confidence Sets for

Sequential Decision Making

 Nicolas Emmenegger

ETH Zurich

&Mojmir Mutny

ETH Zurich

&Andreas Krause

ETH Zurich

Equal contribution.

###### Abstract

Certifiable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use _likelihood ratios_ to construct _any-time valid_ confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a _non-asymptotic_ analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.

## 1 Introduction

One of the main issues addressed by machine learning and statistics is the estimation of an unknown _model_ from noisy observations. For example, in supervised learning, this might concern learning the dependence between an input (covariate) \(x\) and a random variable (observation) \(y\). In many cases, we are not only interested in an estimate \(\) of the true model parameter \(_{}\), but instead in a set of plausible values that \(_{}\) could take. Such confidence sets are of tremendous importance in sequential decision-making tasks, where uncertainty is used to drive exploration or risk-aversion needs to be implemented, and covariates are iteratively chosen based on previous observations. This setting includes problems such as bandit optimization, reinforcement learning, or active learning. In the former two, the confidence sets are often used to solve the _exploration-exploitation_ dilemma and more generally influence the selection rule (Mukherjee et al., 2022), termination rule (Katz-Samuels and Jamieson, 2020), exploration (Auer, 2002) and/or risk-aversion (Makarova et al., 2021).

When we interact with the environment by gathering data sequentially based on previous confidence sets, we introduce correlations between past noisy observations and future covariates. Data collected in this manner is referred to as _adaptively gathered_(Wasserman et al., 2020). Constructing estimators, confidence sets, and hypothesis tests for such non-i.i.d. data comes with added difficulty. Accordingly, and also for its importance in light of the reproducibility crisis (Baker, 2016), the task has attracted significant attention in the statistics community in recent years (Ramdas et al., 2022).

Instead of deriving explicit concentration inequalities around an online estimator, we construct confidence sets _implicitly_ defined by an inclusion criterion that is easy to evaluate in a computationally efficient manner and requires little statistical knowledge to implement. Roughly speaking, given a model \(p_{}(y\,|\,x)\) that describes the conditional dependence of the observation \(y\) given the covariate \(x\) under parameter \(\), we will build sets based on a _weighted_ modification of the sequential likelihood ratio statistic (Robbins et al., 1972; Wasserman et al., 2020)

\[R_{t}():=_{t}(\{_{s}\}_{s=1}^{t})}{ _{t}()}:=^{t}p_{_{s}}^{w_{s}}(y_ {s}\,|\,x_{s})}{_{s=1}^{t}p_{}^{w_{s}}(y_{s}\,|\,x_{s})},\] (1)

where \(\{_{s}\}_{s}\) is a running estimator sequence that we are free to choose, but which may only depend on _previously_ collected data. Parameters \(\) for which this statistic is small, i.e., for which \(R_{t}() 1/\) will be included in the set (and considered _plausible_). Examples of sets in a parametric and non-parametric setting are shown in Figure 1. The weighting terms \(w_{s}(0,1]\) are crucial for dealing with inherent irregularities of many conditional observation models but can be flexibly chosen. Classically, these are set to \(w_{s}=1\). The full exposition of our method with choice of estimators and weights is given in Section 2. Apart from being easy to use and implement, our approach also comes with performance guarantees. These sets maintain a provable \(1-\) coverage - a fact we establish using Ville's inequality for supermartingales (Ville, 1939), which is known to be essentially tight for martingales (see Howard et al., 2018, for a discussion). Therefore, in stark contrast to alternate methods, our confidence sequence is _fully data-dependent_, making it empirically tighter than competing approaches. Despite the rich history of sequential testing and related confidence sets going back to Wald (1945) and Robbins et al. (1972), these sets have found little use in the interactive machine learning community, which is a gap we fill in the present paper.

ContributionsIn this work, we revisit the idea of using likelihood ratios to generate anytime-valid confidence sets. The main insight is that whenever the likelihood of the noise process is known, the likelihood ratio confidence sets are fully specified. They inherit their geometry from the likelihood function, and their size depends on the quality of our estimator sequence. We critically evaluate the likelihood ratio confidence sets and, in particular, we shed light on the following aspects: **Firstly**, for generalized linear models, we _theoretically_ analyze the geometry of the LR confidence sets under mild assumptions. We show their geometry is dictated by Bregman divergences of exponential families (Chowdhury et al., 2022). **Secondly**, we show that the size of the confidence set is dictated by an online prediction game. The size of these sets depends on a sequence of estimators \(\{_{s}\}_{s=1}^{t}\) that one uses to estimate the unknown parameter \(_{}\). We discuss how to pick the estimator sequence in order to yield a provably small radius of the sets, by using the Follow-the-Regularized-Leader algorithm, which implements a regularized maximum-likelihood estimator. We prove that the radius of the confidence sets is nearly-worst-case optimal, and accordingly, they yield nearly-worst-case regret bounds when used in generalized linear bandit applications. However, due to their data-dependent nature, they can be much tighter than this theory suggests. **Thirdly**, we analyze the limitations of classical (unweighted) LR sets when the underlying conditional observation model is not identifiable. In this case, the resulting (inevitable) estimation bias unnecessarily increases the size of the confidence sets. To mitigate this, we propose an adaptive reweighting scheme that decreases the effect of uninformed early

Figure 1: (a) and (b) show examples of confidence sets defined via level sets of the log-likelihood function in 2D at two dataset sizes, for Gaussian (a) and Laplace (b) likelihoods respectively. The sets inherit the geometry of the likelihood, and are not always ellipsoidal. (c) shows confidence bands on an RKHS function in a bandit game searching for the optimum. We compare prior work on confidence sets (Abbasi-Yadkori et al., 2011), our LR sets, and a common heuristic (orange). Our sets are nearly as small as the commonly used heuristic, but have provable coverage and can vastly improve sequential decision-making tasks such as bandits by quickly eliminating hypotheses.

bias of the estimator sequence on the size of the sets downstream. The reweighting does not affect the coverage guarantees of our sets and utilizes an elegant connection to (robust) powered likelihoods (Wasserman et al., 2020). **Finally**, thanks to the adaptive reweighting scheme, our sets are very practical as we showcase experimentally. We demonstrate that our method works well with exponential and non-exponential family likelihoods, and in parametric as well as in kernelized settings. We attribute their practical benefits to the fact that they _do not depend on (possibly loose) worst-case parameters_.

## 2 The Likelihood Method

The sequential likelihood ratio process (LRP) in (1) is a statistic that compares the likelihood of a given model parameter, with the performance of an adaptively chosen estimator sequence. As noted above, we generalize the traditional definition, which would have \(w_{s} 1\), and define a corresponding confidence set as

\[_{t}=\{ R_{t}() 1/\}.\] (2)

The rationale is that the better a parameter \(\) is at explaining the data \(\{(x_{s},y_{s})\}_{s}^{t}\) from the true model \(_{}\), the smaller this statistic will be, thereby increasing its chances to be included in \(_{t}\). When we construct \(R_{t}\), the sequence of \(x_{s}\), \(w_{s}\) and \(_{s}\) cannot depend on the noisy observation \(y_{s}\). Formally, consider the filtration \((_{s})_{s=0}^{}\) with sub-\(\)-algebras \(_{s}=(x_{1},,y_{1}, x_{s},y_{s},x_{s+1})\). We require that \(_{s}\) and \(w_{s}\) are \(_{s-1}\)-measurable. Under these very mild assumptions and with arbitrary weights \(w_{s}(0,1]\), we can show coverage, i.e., our (weighted) confidence sets uniformly track the true parameter with probability \(1-\).

**Theorem 1**.: _The stochastic process \(R_{t}(_{})\) in (1) is a non-negative supermartingale with respect to the filtration \((_{t})\) and satisfies \(R_{0}(_{}) 1\). In addition, the sequence \(_{t}\) from (2) satisfies \(_{_{}}\), \(( t\,:\,_{}_{t})\)._

The last statement follows by applying Ville's inequality for super-martingales on \(R_{t}(_{})\). The proof closely follows Wasserman et al. (2020). While coverage is always guaranteed irrespective of the estimator sequence \(\{_{s}\}\), we would like to make the sets as small as possible at fixed coverage, which we do by picking a well-predicting estimator sequence.

### The Estimator Sequence Game

The specification of the LR process (LRP) allows us to choose an arbitrary estimator sequence \(\{_{s}\}_{s}\). To understand the importance of the sequence, let us introduce \(_{}\) to the definition of \(R_{t}\) in (1), and divide by \(_{t}(\{_{s}\}_{s=1}^{t})\). This gives the equivalent formulation

\[_{t}:=\{_{t}(_{})}{ _{t}()}_{t}(_{ })}{_{t}(\{_{s}\}_{s=1}^{t})}\}.\]

We see that the predictor sequence does not influence the geometry of the confidence set, which is fully specified by the likelihood function. We also observe that the ratio on the right-hand side serves as a confidence parameter controlling the size (radius) of the confidence sets measured under the likelihood ratio distance to \(_{}\). If the confidence parameter goes to zero, only \(_{}\) is in the set. The better the estimator sequence is at predicting the data, the smaller the inclusion threshold, and hence the smaller the sets will ultimately be. Specifically, taking the \(\), we would like to _minimize_

\[_{t}:=_{t}(_{})}{_{t}( \{_{s}\}_{s=1}^{t})}=_{s=1}^{t}-(p_{_{s}}^{w_{ s}}(y_{s}\,|\,x_{s}))-_{s=1}^{t}-(p_{_{s}}^{w_{s}}(y_{s}\,|\,x_{s})).\] (3)

The quantity \(_{t}\) corresponds to a regret in an online prediction game, as will become apparent below.

Online Prediction GameOnline optimization is a mature field in interactive learning (Cesa-Bianchi and Lugosi, 2006; Orabona, 2019). The general goal is to minimize a sequence of loss functions as in Eq. (3) and compete against a baseline, which typically is the best-in-hindsight prediction, or - in our case - given by the performance of the fixed parameter \(_{}\). Specifically, at every timestep \(s\), iteratively, the agent chooses an action \(_{s}\) based on \(_{s-1}\), and a loss function \(f_{s}()\) is revealed. In most of the online optimization literature, \(f_{s}\) can be chosen adversarially. In our prediction game, we know the whole form of loss function \(f_{s}()=-(p_{}^{w_{s}}(y_{s}\,|\,x_{s}))\), as can be seen in (3), and not just \(f_{s}(_{s})\). Opposed to traditional assumptions in online prediction, in our case, \(f_{s}\) are non-adversarial, but have a stochastic component due to \(y_{s}\). Also, contrary to most instances of online prediction, we do not compare against the best-in-hindsight predictor, but \(_{}\) instead, as this is more meaningful in our setting.

Online Optimization AlgorithmsGenerally, we seek an algorithm that incurs low regret. Here, we focus on _Follow-the-Regularized Leader (FTRL)_, which corresponds exactly to using regularized maximum likelihood estimation, making it a natural and computationally practical choice. The update rule is defined in Alg. 1 (Line 3). While other algorithms could be considered, FTRL enjoys the optimal regret rate for generalized linear regression as we show later, and is easily implemented. In order to run the algorithm, one requires a sequence of strongly convex regularizers. For now, let us think of it as \(_{s}()=||||_{2}^{2}\), which we use in practice. However, one can derive a tighter analysis for a slightly modified, time-dependent regularization strategy for generalized linear models as we show in Sec. 3.3.

### Adaptive Reweighting: Choosing the Right Loss

There is yet more freedom in the construction of the LR, via the selection of the _loss function_. Not only do we select the predictor sequence, but also the _weights_ of the losses via \(w_{t}\). This idea allows controlling the influence of a particular data point \((x_{t},y_{t})\) on the cumulative loss based on the value of \(x_{t}\). For example, if we know a priori that for a given \(x_{t}\) our prediction will be most likely bad, we can opt out of using the pair \((x_{t},y_{t})\) by setting \(w_{t}=0\). Below we will propose a weighting scheme that depends on a notion of _bias_, which captures how much of the error in predicting \(y_{t}\) is due to our uncertainty about \(_{t}\) (compared to the uncertainty we still would have _knowing_\(_{}\)). Sometimes this _bias_ is referred to as _epistemic_ uncertainty in the literature, while the residual part of the error is referred to as _aleatoric_. Putting large weight on a data point heavily affected by this bias might unnecessarily increase the regret of our learner (and hence blow up the size of the confidence set). Note that, conveniently, even if we put low weight (zero) on a data point, nothing stops us from using this sample point to improve the estimator sequence in the next prediction round. As we will show below, our reweighting scheme is crucial in defining a practical algorithm for Reproducing Kernel Hilbert Space (RKHS) models and in high signal-to-noise ratio scenarios. Since we do not know \(_{}\), our strategy is to compute an _estimate of the bias of the estimator_\(_{t}\) and its effect on the value of the likelihood function for a specific \(x\) that we played. We use the value of the bias to rescale the loss via \(w_{t}\) such that its effect is of the same magnitude as the statistical error (see Algorithm 1; we call this step bias-weighting).

IntuitionTo give a bit more intuition, suppose we have a Gaussian likelihood. Then the negative log-likelihood of \((x_{t},y_{t})\) with weighting is proportional to \(}{^{2}}(y_{t}-x_{t}^{}_{t})^{2}\). Now, if \(x_{t}\) does not lie in the span of the data points \(\{x_{s}\}_{s=1}^{t-1}\) used to compute \(_{t}\), it is in general unavoidable to incur large error, inversely proportional to \(^{2}\). To see this, let us decompose the projection onto \(x_{t}\) as

\[x_{t}^{}(_{t}-_{})=^{}(_{t}-[_{t}])}_{}+ ^{}([_{t}]-_{})}_{ {bias}_{_{t}}(_{t})},\] (4)

where the first term represents the statistical error up to time \(t\), while the second, bias, is deterministic, and independent of the actual realization \(y\), depending only \(_{}\). Estimators with non-zero bias are _biased_. Plugging this into the likelihood function, we see that in expectation \(}[(y_{t}-x_{t}^{}_{t})^{2}| _{t-1}]}_{x_{t}}^{2}(_{t})+^{2}+\), where \(^{2}\) is the unavoidable predictive error in expectation (due to a noisy objective) and is a constant independent of \(^{2}\). \(\) is the statistical error, and \(C\) is independent of \(^{2}\). Note that the bias term scales inversely with the variance, and leads to unnecessarily big confidence parameters for small \(^{2}\).

In fact, the problem is that we use the likelihood to measure the distance between two parameters, but this is only a "good" distance once the deterministic source of the error (bias) vanishes. For this reason, without weighting, the incurred regret blows up severely in low-noise settings. To counter this, we balance the deterministic estimation bias and noise variance via proper selection of \(w_{t}\). In this case, it turns out that \(w_{t}=}{^{2}+_{_{t}}^{2}(_{t})}\) ensures that the overall the scaling is independent of \(^{2}\). While the choice of weights \(\{w_{s}\}_{s}^{t}\) influences the geometry of the confidence sets, with a _good data collection and estimation strategy_ the bias asymptotically decreases to zero, and hence the weights converge to \(1\).

Bias estimationIn order to generalize this rule beyond Gaussian likelihoods, we need a proper generalization of the bias. Our generalization is motivated by our analysis of generalized linear models, but the method can be applied more broadly. The role of the squared statistical error (variance) is played by the inverse of the smoothness constant of the negative log-likelihood functions \(f_{s}\), denoted by \(L\). This is the usual smoothness, commonly seen in the convex optimization literature.

We consider penalized likelihood estimators with strongly convex regularizers (Alg. 1, line 3). For this estimator class, we define the bias via a hypothetical stochastic-error-free estimate \(_{t}^{}\), had we access to the expected values of the gradient loss functions (a.k.a. score). We use the first-order optimality conditions and the indicator function of the set \(\), \(i_{}\), to define the error-free-estimate \(_{t}^{}\), and the bias of the estimator \(_{t}\) as

\[_{x_{t}}^{2}(_{t})=(x_{t}^{}(_{}- {}_{t}^{}))^{2}[_{s=1}^{t- 1} p_{_{t}^{}}(y_{s}|x_{s})]-_{t}( _{t}^{})+i_{}(_{t}^{})=0,\] (5)

where the expectation denotes a sequence of expectations conditioned on the prior filtration. This notion of bias coincides with the definition of bias in Eq. (4) for the Gaussian likelihood. This quantity cannot be evaluated in general, however, we prove a computable upper bound.

**Theorem 2** (Bias estimate).: _Let the negative log-likelihood have the form, \(- p_{}(y_{s}|x_{s})=g(x_{s}^{})\), where \(g:\) is \(\) strongly-convex and let the regularizer be \(_{t}()=||||_{2}^{2}\) making the overall objective strongly convex. Then, defining \(_{t}^{;}=_{s=1}^{t} x_{s}x_{s}^{}+ \), we can bound_

\[_{x}^{2}(_{t}) 2\|_{}\|_{2}^{2}x ^{}(_{t}^{;})^{-1}x.\] (6)

The proof is deferred to App. A.3, and requires elementary convex analysis.

This leads us to propose the weighting scheme \(w_{t}=_{x_{t}}^{2}(_{t})+1/L}\). We justify that this is a sensible choice by analyzing the confidence set on the GLM class in Section 3, which satisfies the smoothness and strong-convexity conditions. We show that this rule properly balances the stochastic and bias components of the error in the regret as in (3). However, this rule is more broadly applicable beyond the canonical representation of GLM or the GLM family altogether.

```
1:Input: convex set \(^{d}\), confidence level \(>0\), likelihood \(p_{}(y|x)\), regularizers \(\{_{t}\}_{t}\)
2:for\(t_{0}\)do
3:\(_{t}=_{}_{s=1}^{t-1}- p_{}(y _{s}\,|\,x_{s})+_{t}()\)\(\) FTRL
4:\(w_{t}=_{x_{t}}^{2}(_{t})}& \\ 1&\\ \)\(\) bias-weighting \(_{x_{t}}(_{t})\) in Eq. (5) or Eq.(6)
5:\(_{t}=\{\,|\,_{s=1}^{t}_{s}}^{w_{s}}(y_{s}\,|\,x_{s})}{p_{}^{w_{s}}(y_{s}\,|\,x_{s })}\}.\)\(\) Confidence set
6:endfor ```

**Algorithm 1** Constructing the LR Confidence Sequence

## 3 Theory: Linear Models

While the _coverage_ (i.e., "correctness") of the likelihood ratio confidence sets is always guaranteed, their worst-case _size_ (affecting the "performance") cannot be easily bounded in general. We analyze the size and the geometry of the LR confidence sequence in the special but versatile case of generalized linear models.

### Generalized Linear Models

We assume knowledge of the conditional probability model \(p_{}(y|x)\), where the covariates \(x^{d}\), and the true underlying model parameter lies in a set \(^{d}\). If \(t\) is indexing (discrete) time, then \(x_{t}\) is acquired sequentially, and the - subsequently observed - \(y_{t}\) is sampled from an exponential family distribution parametrized as

\[p_{}(y\,|\,x_{t})=h(y)(T(y) x_{t}^{}-A(x_{t}^{ })).\] (7)

Here, \(A\) is referred to as the _log-partition function_ of the conditional distribution, and \(T(y)\) is the sufficient statistic. The function \(h\) is the base measure, and has little effect on our further developments, as it cancels out in the LR. Examples of commonly used exponential families (Gaussian, Binomial, Poisson or Weibull) with their link functions can be found in Table 1 in App. A.1.

In order to facilitate theoretical analysis for online algorithms, we make the following assumptions about the covariates \(x\) and the set of plausible parameters \(\).

**Assumption 1**.: _The covariates are bounded, i.e., \(_{x}\|x\|_{2} 1\), and the set \(\) is contained in an \(_{2}\)-ball of radius \(B\). We will also assume that the log-partition function is strongly convex, that is, that there exists \(:=_{z[-B,B]}A^{}(z)\), and that \(A\) is L-smooth, i.e. \(L:=_{z[-B,B]}A^{}(z)\)._

These assumptions are common in other works addressing the confidence sets of GLMs (Filippi et al., 2010; Faury et al., 2020), who remark that the dependence on \(\) is undesirable. However, in contrast to these works, our confidence sets _do not_ use these assumptions in the construction of the sets. We only require these for our theoretical analysis. As these are worst-case parameters, the practical performance can be much better for our sets.

### Geometry and Concentration

Before stating our results, we need to define a distance notion that the convex negative log-likelihoods induce. For a continuously differentiable convex function \(f\), we denote the Bregman divergence as \(D_{f}(a,b):=f(a)-f(b)- f(b)^{}(a-b).\) The \(\)-regularized sum of log-partition functions is defined as

\[Z_{t}^{}():=_{s=1}^{t}w_{s}A(x_{s}^{})+|| ||_{2}^{2}.\] (8)

This function will capture the geometry of the LR confidence sets. The confidence set size depends mainly on two terms. One refers to a notion of complexity of the space referred to as _Bregman information gain_: \(_{t}^{}(_{t})=(}(- ||||_{2}^{2})d}{_{}a^{}(-D_{ Z_{t}^{}}(,_{t}))d})\), first defined by Chowdhury et al. (2022) as a generalization of the _information gain_ of Srinivas et al. (2009), \(_{t}^{}=((_{i=1}x_{i}x_{i}^{}+ ))\) for Gaussian likelihoods. We will drop the superscript whenever the regularization is clear from context and simply refer to \(_{t}\). This term appears because one can relate the decay of the likelihood as a function of the Bregman Divergence from \(_{}\) with the performance of a (regularized) maximum likelihood estimator via convex (Fenchel) duality. In particular, if \(_{t}\) is a regularized MLE, \(_{t}^{}:=_{t}^{}(_{t})\) will asymptotically scale as \((d t)\)(cf. Chowdhury et al., 2022, for further discussion). For Gaussian likelihoods and \(w_{s} 1\), it coincides with the classical information gain independent of \(_{t}\). The second term that affects the size is the regret \(_{t}\) of the online prediction game over \(t\) rounds we introduced previously in (3). These two parts together yield the following result:

**Theorem 3**.: _Let \(>0\) and \(,(0,1)\). For the level \(1-\) confidence set \(_{t}\) defined in (2) under the GLM in (7), with probability \(1-\), for all \(t 1\), any \(_{t}\) satisfies_

\[D_{Z_{t}^{}}(,_{})_{t}+2( )+2_{t},\] (9)

_where \(_{t}=(()+ B^{2}+_{t}^{})\) and \(L,\) are defined as above and finally \(_{t}\) is the regret of the game in Eq. (3)._

The set defined via the above divergence does not coincide with the LR confidence set. It is slightly larger due to a term involving \(\) (as in Eq. (8)). This is a technical consequence of our proof technique, where the gradient of \(Z_{t}^{}\) needs to be invertible, and regularization is added to this end. We note that this \(>0\) can be chosen freely. Note that the theorem involves two confidence levels, \(\) and \(\): \(\) is a bound on the Type I error - coverage of the confidence sets - while \(\) upper bounds the probability of a large radius - and is therefore related to the power and Type II error of a corresponding hypothesis test. The proof of the theorem is deferred to App. B.2.

To give more intuition on these quantities, let us instantiate them for the Gaussian likelihood case with \(w_{s} 1\). In this scenario, \(Z_{t}^{}()=_{s=1}^{t}}||||_{x_{s}x_{s} ^{}}^{2}+||||_{2}^{2}\), and the (in this case symmetric) Bregman divergence is equal to \(D_{Z_{t}^{}}(_{},)=||-_{}||_{ _{t}^{^{2},}}^{2}\), where \(_{t}^{;}=_{s=1}^{t} x_{s}x_{s}^{}+\), which means that our confidence sets are upper bounded by a ball in the same norm as those in the seminal work on linear bandits (Abbasi-Yadkori et al., 2011).

### Online Optimization in GLMs: Follow the Regularized Leader

The size of the confidence sets in Theorem 3 depends on the regret of the online prediction game involving the estimator sequence. We now bound this regret when using the Follow-the-Regularized-Leader (FTRL) algorithm in this setting. This high probability bound is novel to the best of ourknowledge and may be of independent interest. We state in a weight-agnostic manner first, and then with our particular choice. The latter variant uses a specifically chosen regularizer. In this case, we can track the contribution of each time-step towards the regret separately.

**Theorem 4**.: _Let \(_{t}()=\|\|_{2}^{2}\). Assume Assumption 1, and additionally that \(A\) is \(L\)-smooth everywhere in \(^{d}\), and let \(w_{t}\) be arbitrary. Then, with probability \(1-\) the regret of FTRL (Alg. 1) satisfies for all \(t 1\)_

\[_{t} B^{2}+(_{t}^{}+2(1/ ))+B^{2}}{}_{t}^{}.\] (10)

The regret bounds are optimal in the orders of \(_{t}^{}\), matching lower bounds of Ouhamma et al. (2021), as for linear models \(_{t}=(d t)\). Combining results of Thm. 4 with Thm. 3, we get a confidence parameter that scales with \((})\), for confidence sets of the form \(\|-_{*}\|_{_{t}}\), which coincides with the best-known confidence sets in this setting in the worst-case (Abbasi-Yadkori et al., 2012). The requirement of global \(L-\)smoothness can be relaxed to \(L-\)smoothness over \(\). With a more elaborate (but less insightful) analysis, we can show that we achieve a \(}(_{t})\) bound even in this case. The proofs of these results are deferred to App. C.4, App. C.5 and App. C.6 respectively.

Regret, Weighting and Estimation BiasInterestingly, the term in Thm. 4 involving the (crude) proxy to the bias - the bound \(B\) - is not scaled by the same \(L/\) factors as the other terms in the regret bound (10) and in Theorem 3. Namely, the prefactor is \(L^{2}/\) instead of \(L/\). This extra dependence manifests itself in the unnecessary penalization through the estimation bias we introduced in Sec. 2.2, particularly in low-noise settings. We addressed this issue by picking the weights \(\{w_{t}\}\). While the above theorem holds for any valid weighting, it does not exhibit the possible improvement from using specific weights.

We argued earlier that the error in prediction should not be measured by the likelihood function if there is deterministic error, since initially, we are fully uncertain about the value of \(_{*}{}^{}()\) outside the span of previous observations. Of course, if our goal would be to purely pick weights to minimize \(_{t}\), then \(w_{s}=0\) would lead to zero regret and hence be optimal. However, the likelihood ratio would then be constant, and uninformative. In other words, the associated log-partition Bregman divergence in Theorem 3 would be trivial and not filter out any hypotheses. Clearly, some balance has to be met. With this motivation in mind, we proposed a _nonzero_ weighting that decreases the regret contribution of the bias, namely \(w_{t}=_{s,t}^{2}(_{t})}\). The advantage of this choice becomes more apparent when we use the regularizer \(_{t}()=||||^{2}+A(x_{t}^{})\) to obtain the following result.

**Theorem 5**.: _Let \(_{s}()=||||^{2}+A(x_{s}^{})\). Assume Assumption 1, and additionally that \(A\) is \(L\)-smooth everywhere in \(^{d}\), and choose \(w_{s}=_{s,t}(_{s})^{2}}\). Additionally, let the sequence of \(x_{s}\) be such that, \(_{s}(1-w_{s})(f_{s}(_{*})-f_{s}(_{s+1})) L/ _{t}^{}\), where \(_{s}\) is the FTRL optimizer with the regularizer \(||||_{2}^{2}\) from Theorem 42. Then, with probability \(1-\) the regret of FTRL (Alg. 1) satisfies for all \(t 1\)_

\[_{t} B^{2}+(_{t}^{}+ ())+_{s=1}^{t}}{1/ L+_{x_{s}}^{2}(_{s})}_{s}^{},\]

_where \(_{s}^{}=_{s+1}^{}-_{s}^{}\)._

One can see that for points where the information gain \(_{s}\) is large (corresponding to more unexplored regions of the space, where the deterministic source of error is then large), the weighting scheme will make sure that the multiplicative contribution of \(B^{2}\) is mitigated, along with having the correct prefactor \(L/\). The reader may wonder how this result is useful when we replace \(_{x_{s}}^{2}(_{s})\) with the upper bound from Thm. 2. While instructive, our bound still only makes the bias proxy \(B^{2}\) appear in front of the information gain \(_{t}\), instead of the more desireable bias itself. In the latter case, we could also directly make use of the upper bound and get an explicit result only using an upper bound on the bias. We leave this for future work.

We point out that this choice of \(_{s}()\) in Theorem 5 corresponds to the Vovk-Azoury-Warmuth predictor (Vovk, 2001; Azoury and Warmuth, 1999) in the online learning literature. This choice is helpful in order to track the bias contribution more precisely in our proof.

Application: Linear and Kernelized Bandits

Our main motivation to construct confidence sets is bandit optimization. A prototypical bandit algorithm - the Upper Confidence Bound (UCB) (Auer, 2002) - sequentially chooses covariates \(x_{s}\) in order to maximize the reward \(_{s=1}^{t}r_{_{}}(x_{s})\), where \(r_{_{}}\) is the unknown pay-off function parametrized by \(_{}\). UCB chooses the action \(x_{s}\) which maximizes the optimistic estimate of the reward in each round, namely

\[x_{s}=*{arg\,max}_{x}_{_{s -1}}r_{}(x),\] (11)

where \(_{s-1}\) is some confidence set for \(_{}\), and can be constructed with Algorithm 1 from the first \(s-1\) data points. An important special case is when \(r_{_{}}\) is linear (Abe and Long, 1999) or modelled by a generalized linear model (Filippi et al., 2010). In that case, the inner optimization problem is convex as long as \(_{s-1}\) is convex. The outer optimization is tractable for finite \(\). In the applications we consider, our confidence sets are convex, and we easily solve the UCB oracle using convex optimization toolboxes.

Extension to RKHSWe introduced the framework of LR confidence sets only for finite-dimensional Euclidean spaces. However, it can be easily extended to Reproducing Kernel Hilbert Spaces (RKHS) (Cucker and Smale, 2002). The definition of the LR process in (1) is still well-posed, but now the sets are subsets of the RKHS, containing functions \(f_{k}\). An outstanding issue is how to use these sets in downstream applications, and represent them tractably as in Figure 1. Conveniently, even with infinite-dimensional RKHSs, the inner-optimization in (11) admits a Lagrangian formulation, and the generalized representer theorem applies (Scholkopf et al., 2001; Mutny and Krause, 2021). In other words, we can still derive a pointwise upper confidence band as \((x)=_{f_{k},\|f\|_{k} B,f C_{s}}  f,k(x,)\) in terms of \(\{x_{j}\}_{j=1}^{*}\{x\}\), leading to a \(s+1\)-dimensional, tractable optimization problem.

We also point out that the weighting is even more paramount in the RKHS setting, as the bias never vanishes for many infinite dimensional Hilbert spaces (Mutny and Krause, 2022). For this purpose, our weighting is of paramount practical importance, as we can see in Figure 1(a)), where the gray arrow represents the significant improvement from reweighting.

### Instantiation of the Theory for Linear Bandits

Before going to the experiments, we instantiate our theoretical results from Sec. 3 to the important and well-studied special case of linear payoffs. In that case, \(r_{}(x)= x,\,\) and the agent observes \(y_{s}= x_{s},\,_{}+_{s}\) upon playing action \(x_{s}\), where \(_{s}(0,^{2})\). We are interested in minimizing the so-called cumulative pseudo-regret, namely, \(_{t}=_{s=1}^{t}[ x_{s},\,_{}- x _{s},\,_{}]\), where \(x_{}\) refers to the optimal action. Using the set from (2) along with Theorem 3 and the FTRL result of Theorem 4 we can get a regret bound for the choice \(w_{s} 1\).

**Theorem 6**.: _Let \(w_{s} 1\). For any \(}\), with probability at least \(1-3\), for all \(t\) we have_

\[_{t} 6^{}}(^{}}+^{1/2}B+B^{ }}).\]

Our results are optimal in both \(d\) and \(t\) up to constant and logarithmic factors. The proof is deferred to App. D, but is an instantiation of the aforementioned theorems, along with a standard analysis. There, we also compare to the seminal result of Abbasi-Yadkori et al. (2011), which does not suffer from the dependence on \(B}\). We attribute this to the incurred bias in the absence of the reweighting scheme.

For the weighted likelihood ratio, we can obtain a result similar to the above, but multiplied by an upper bound on \(_{s 1}w_{s}^{-1}\). This is undesirable, as our experiments will show that the reweighting scheme vastly improves performance. While this could be somewhat mitigated by using the Theorem 5 instead of Theorem 4 to bound the FTRL regret, a better result should be achievable using our weighting scheme that improves upon Theorem 6 and possibly even matches Abbasi-Yadkori et al. (2011) exactly in the worst-case. We leave this for future work.

### Experimental Evaluation

In this subsection, we demonstrate that the practical applicability goes well beyond the Gaussian theoretical result from the previous subsection. In the examples below, we always use the UCBalgorithm but employ different confidence sets. In particular, we compare our LR confidence sets for different likelihood families with alternatives from the literature, notably classical sub-family confidence sets (Abbasi-Yadkori et al., 2011; Mutny and Krause, 2021), and the robust confidence set of Neiswanger and Ramdas (2021). In practice, however, the radius of these confidence sets is often tuned heuristically. We include such sets as a baseline _without_ provable coverage as well. The main take-home message from the experiments is that among all the estimators and confidence sets that enjoy _provable_ coverage, our confidence sets perform the best, on par with successful heuristics. For all our numerical experiments in Figure 2, the true payoff function is assumed to be an infinite dimensional RKHS element. For further details and experiments, please refer to App. E.

Additive Noise ModelsSuppose that \(r_{_{}}\) is linear and we observe \(y_{s}=x_{s}^{}_{}+_{s}\), where \(_{s}\) is additive noise, and \(_{}\) is an element of a Hilbert space. We consider classical Gaussian noise as well as Laplace noise in Fig. 2(a), b)]. Notice that in both cases our confidence sets yield lower regret than any other provably valid method. In both cases they are performing as good as _heuristic_ confidence sets with confidence parameter \(_{t} 2(1/)\). The sub-Gaussian confidence sets of Abbasi-Yadkori et al. (2011) (AY 2011) are invalid for the Laplace distribution as it is not sub-Gaussian but only sub-Exponential. For this reason, we compare also with sub-exponential confidence sets derived similarly to those of (Faury et al., 2020). The confidence sets of (Neiswanger and Ramdas, 2021) (NR 2021) perform similarly on Gaussian likelihood, but are only applicable to this setting, as their generalization to other likelihood families involves intractable posterior inference. We note also the difference between the unweighted LR and the weighted one. The examples in Fig. 2 use the true payoff functions \(r(x)=-(1.4-3x)(18x)\), which we model as an element of a RKHS with squared exponential kernel lengthscale \(=6 10^{-2}\) on \([0,1.2]\), which is the baseline function no. 4 in the global optimization benchmark database _infinity77_(Gavana, 2021). Additional experiments can be found in App. E.

Poisson BanditsA prominent example of generalized linear bandits (GLB) are Poisson bandits, where the linear payoff is scaled by an exponential function. We instantiate our results on a common benchmark problem, and report the results in Fig. 2d). We improve the regret of UCB for GLBs compared to two alternative confidence sets: one that uses a Laplace approximation with a heuristic confidence parameter, and one inspired by considerations in Mutny and Krause (2021) (MK 2021), also with a heuristic confidence parameter. Note that we cannot compare directly to their provable results in their original form as they do not state them in the canonical form of the exponential family.

Figure 2: Bandit experiments: On the \(y\)-axis we report cumulative regret, while the \(x\)-axis shows the number of iterations. In a) and b) we report the results for linear models with different parametric additive noise. In c) we report the results on a survival analysis with a log-Weibull distribution (\(p=2\)) and in d) we showcase Poisson bandits. See App. E for more details. Heuristic methods are _dashed_, while provable are _solid_. Our sets perform the best among all provable methods. Notice in a) the difference in gray and black represents the improvement due to adaptive weighting over \(w_{s}=1\) for all \(s[t]\). For each experiment we did 10 reruns, median values are plotted.

Survival AnalysisSurvival analysis is a branch of statistics with a rich history that models the lifespan of a service or product (Breslow, 1975; Cox, 1997; Kleinbaum and Klein, 2010). The classical approach postulates a well-informed likelihood model. Here, we use a specific hazard model, where the survival time \(T\) is distributed with a Weibull distribution, parametrized by \(\) and \(p\). The _rate_\(_{}(x)=(x^{})\) differs for each configuration \(x\), and \(p\) - which defines the shape of the survival distribution - is fixed and known. We assume that the unknown part is due to the parameter \(\) which is the quantity we build a confidence set around to use within the UCB Algorithm. In particular, the probability density of the Weibull distribution is \(P(T=t|x)=_{}(x)pt^{p-1}(-t^{p}_{}(x))\). In fact, with \(p=2\), the confidence sets are convex and the UCB rule can be implemented efficiently.

Interestingly, this model admits an alternate linear regression formulation. Namely upon using the transformation \(Y= T\), the transformed variables \(Y|x\) follow a Gumbel-type distribution, with the following likelihood that can be obtained by the change of variables \(P(Y=y|x)=_{}(x)p(y)^{p}(-(y)^{p}_{}(x))\). The expectation over \(Y\) allows us to express it as a linear regression problem since \([Y|x]=-(^{}x+)/p\), where \(\) is the Euler-Mascheroni constant. More importantly, \(Y|x\) is sub-exponential. Hence, this allows us to use confidence sets for sub-exponential variables constructed with the pseudo-maximization technique inspired by Faury et al. (2020). More details on how these sets are derived can be found in App. E. However, these approaches necessarily employ crude worst-case bounds and as can be seen in Figure 1(c)) the use of our LR-based confidence sequences substantially reduces the regret of the bandit learner.

## 5 Related Work and Conclusion

Related WorkThe adaptive confidence sequences stem from the seminal work of Robbins et al. (1972), who note that these sets have \(\)-bounded Type I error. The likelihood ratio framework has been recently popularized by Wasserman et al. (2020) for likelihood families without known test statistics under the name _universal inference_. This approach, although long established, is surprisingly uncommon in sequential decision-making tasks like bandits. This might be due to the absence of an analysis deriving the size of the confidence sets (Mutny and Krause, 2021), a necessary ingredient to obtain regret bounds. We address this gap for generalized linear models. Another reason might be that practitioners might be interested in non-parametric _sub_-families - a scenario our method does not cover. That being said, many fields such as survival analysis (Cox, 1997)_do_ have well-informed likelihoods. However, most importantly, if used naively, this method tends to fail when one departs from assumptions that our probabilistic model is identifiable (i.e., \(p_{}(\,|\,x)=p_{}(\,|\,x)\) even if \(\)). We mitigate this problem by introducing the scaling parameters \(w_{t}\) in Eq. (1) to deal with it.

Prevalent constructions of anytime-valid confidence intervals rely on carefully derived concentration results and for a specific estimator such as the least-squares estimator and noise sub-families such as sub-Gaussian, sub-Bernoulli and sub-Poisson Abbasi-Yadkori et al. (2011); Faury et al. (2020); Mutny and Krause (2021). Their constructions involve bounding the suprema of collections of self-normalized stochastic processes (Faury et al., 2020; Mutny and Krause, 2021; Chowdhury et al., 2022). To facilitate closed-form expressions, worst-case parameters are introduced that prohibitively affect the size of the sets - making them much larger than they need to be.

Chowdhury et al. (2022) use the exact form of the likelihood to build confidence sets for parameters of exponential families. However, their approach is restricted to exponential family distributions. They use self-normalization and mixing techniques to explicitly determine the size of the confidence set and do not use an online learning subroutine as we do here. Neiswanger and Ramdas (2021) use likelihood ratios for bandit optimization with possibly misspecified Gaussian processes but is not tractable beyond Gaussian likelihoods. The relation between online convex optimization and confidence sets has been noted in so-called online-to-confidence conversions (Abbasi-Yadkori et al., 2012; Jun et al., 2017; Orabona and Jun, 2021; Zhao et al., 2022), where the existence of a low-regret learner implies a small confidence set. However, these sets still use potentially loose regret bounds to define confidence sets. Our definition is _implicit_. We do not necessarily need a regret bound to run our method, as the radius will depend on the actual, instance-dependent performance of the learner.

ConclusionIn this work, we generalized and analyzed sequential likelihood ratio confidence sets for adaptive inference. We showed that with well-specified likelihoods, this procedure gives small, any-time valid confidence sets with model-agnostic and precise coverage. For generalized linear models, we quantitatively analyzed their size and shape. We invite practitioners to explore and use this very versatile and practical methodology for sequential decision-making tasks.