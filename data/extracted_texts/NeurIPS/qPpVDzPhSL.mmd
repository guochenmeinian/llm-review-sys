# Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases

Zian Su\({}^{1}\) Xiangzhe Xu\({}^{1}\)   Ziyang Huang\({}^{2}\)   Kaiyuan Zhang\({}^{1}\)   Xiangyu Zhang\({}^{1}\)

\({}^{1}\) Purdue university  \({}^{2}\) Johns Hopkins University

{su284,xu1415,zhan4057}@purdue.edu, zhuang66@jhu.edu

xyzhang@cs.purdue.edu

Corresponding author.

###### Abstract

Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.

## 1 Introduction

In recent years, we see two trends of uni-modal code model pre-training. On one hand, there is a remarkable surge in the development of generative Source Code Foundation Models (SCFMs) [12; 65; 55; 20; 43], along with advancements in general Large Language Models (LLMs) [4; 60; 46]. Driven by a growing interest in automating software development, these powerful models are trained on billions of tokens from diverse codebases, covering a wide spectrum of programming languages . They possess the capability to complete, infill , and refine code , as well as generate code from natural language instructions . On the other hand, there is a stream of research focusing on binary understanding models [61; 58], which target learning nuanced code semantics with structures of low-level code, which is critical for software security. Both fields are evolving through continuous pre-training on expansive datasets of uni-modal data, setting new benchmarks in both effectiveness and complexity.

Human-Oriented Binary Reverse Engineering (HOBRE), which involves automatically lifting binary to human understandable contents [14; 69], typically source-code related, occupies a uniqueintersection between these two fields. Existing decompilers for binary reverse engineering are able to translate binary code to C-style code that is functionally equivalent to its original source code. However, a significant semantic gap remains between the decompiled code and its original source, primarily due to the absence of meaningful symbolic information in binary code. Hence, human expertise is still indispensable in the reverse engineering process. HOBRE aims to bridge this semantic gap, which traditionally requires substantial human effort, by leveraging cross-modal deep learning models. Existing approaches either train task-specific small expert models in a supervised manner , which lack generalizability as shown in later evaluations , or require extensive continual pre-training of uni-modal SCFMs  which is undesirable considering cost and the risk of forgetting previously acquired source code knowledge . There are also attempts in directly prompting LLMs for HOBRE, which, even though demonstrates better generalizability than small supervised models, also face challenges in understanding stripped decompiled code that lacks symbolic information .

Our insight is that this semantic gap between binary and source code is analogous to the gap between low-level pixels in images and high-level concepts in natural language, which can be bridged with sufficient understanding of both. Inspired by the achievements of multi-modal models that seamlessly integrate vision, audio, or other signals with language to facilitate reasoning , we hypothesize that HOBRE could similarly benefit from leveraging uni-modal models developed for both source code and binary code. Such integration would enhance our ability to bridge the semantic gap and enable more effective semantic lifting.

In this paper, we validate this idea by proposing a novel probe-and-recover framework ProRec that incorporates a binary-source encoder-decoder model and black-box LLMs for HOBRE, featuring a compute-efficient cross-modal alignment approach of a binary function encoder and a frozen SCFM for the binary-source model. The workflow of ProRec is shown in Figure 1. The aligned binary-source model acts as a _cross-modal-knowledge-proper_ that can synthesize symbol-rich, diverse source code fragments condition on binary input, denoted as _probed contexts_. The black-box LLM functions as _recoverer_ that takes as input the binary function together with the probed contexts for tasks such as binary summarization. Intuitively, the conditional source code synthesis by the aligned binary-source code model can be viewed as probing the base SCFM as a parametric knowledge base  with a binary function as query, given that the SCFM's weights remains unchanged before and after the

Figure 1: The ProRec Framework for human-oriented binary reverse engineering. The figure shows a simple example of lifting a cumsum function from binary to human readable summarization. The probed contexts synthesized by the cross-modal knowledge proper, while not identical to the oracle source code of the query binary, exhibit informativeness in terms of symbol names and correct loop structure. These contexts help the black-box LLMs to successfully recover the high-level functionality of binary function in the summary that is consistent with the source code summary, moving beyond merely describing its low-level operations.

alignment. A black-box LLM analyzes and aggregates these knowledgable contexts with the binary function for recovery. This way, ProRec leverages both cross-modal aligned knowledge and strong reasoning ability of LLMs and can outperform directly letting the LLM to reason. ProRec is general and can be applied to different base architectures, continually evolve with base models.

We demonstrate the effectiveness of ProRec on two core tasks in reverse engineering [9; 10]: binary summarization and binary function name recovery. The former aims to generate natural language descriptions for a binary function, and the later aims to recover the function name of a decompiled function. We evaluate ProRec on a diversified dataset compiled from GitHub repositories, demonstrating improvements of 3.1% (10.3% relative gain) in CHRF and 12% (16.7% relative gain) in a GPT4-based metric that has high correlation with human judgement on the summarization task over zero-shot baseline. We conduct human study to show the effectiveness of the newly proposed GPT4-based metric. On name recovery tasks, ProRec significantly improves over zero-shot baseline by 6.7% and 7.4% for token-level precision and recall, respectively. For both tasks, ProRec also consistently show advantage over a retrieval-augmented baseline with a strong cross-modal dense retriever. 2

## 2 ProRec: Reverse Binary by Probing Source Code Foundation Models

In this section, we first present the ProRec framework in SS2.1. Next, we describe the neural architecture used for the cross-modal knowledge proper and recoverer in SS2.2. The training for the proper in is detailed in SS2.3, followed by the comprehensive explanation of the knowledge probing stage in SS2.4.

FormulationGiven a binary file, we can leverage binary analysis tools 3 to obtain each binary function \(x\). Specifically, \(x\) can either be in its disassembled code form which we denote as \(x_{}\), or its stripped decompiled code form, denoted as \(x_{}\). \(x_{}\) and \(x_{}\) are semantically equivalent and similarly unreadable. The goal is to recover human readable information \(y\) given \(x_{}\) and \(x_{}\).

### The Probe-and-Recover Framework

ProRec assumes a binary understanding model parameterized by \(\), an open-source SCFM parameterized by \(\), and a black-box LLMs by \(\). As illustrated in Figure 1, the binary model together with the SCFM form the cross-modal knowledge proper. The black-box LLM serves as a recoverer. The cross-modal proper can synthesize source code fragments given binary input. The recoverer takes in augmented context with binary code to analyze and perform final recovery.

Conceptually, the ProRec framework decomposes the probability to generate \(y\) into three parts, the probability of a set of \(k\) source code fragments \(_{k}=\{s_{1},,s_{k}\}\) being relevant to input \(P(_{k}|x)\), the probability of LLM's relevance analysis of the source code fragments \(P(|_{k},x)\), and the probability of generating the recovery results conditioned on the analysis and source code fragments.

\[P(y|x)=_{_{k} P_{,}(|x), P_{ }(|_{k},x)}P_{}(y|,_{k}, x) P_{}(|_{k},x) P (_{k}|x)\] (1)

The decomposition is similar to that of retrieval-augmented generation [36; 68], where \(p(y|x)=_{s-k(S^{*})}P(y|s,x)P(s|x)\), given a document pool \(S^{*}\). However, there are two major differences. First, the source code fragments \(_{k}\) are not retrieved from \(S^{*}\), instead, they are sampled from the conditional distribution of the proper \(P_{,}(|x)\). Due to the alignment strategy (discussed in SS2.3), source code fragments sampled from the proper's distribution have more flexibility than those retrieved a fixed document pool in binary reverse engineering scenario, potentially less noisy. We empirically demonstrate the superiority of probing over retrieval for augmentation in SS4.

Second, we stress the internal analysis from the LLM denoted as \(P_{}(|_{k},x)\) in the decomposition. The insight is that, even though high-level recovery requires additional domain information to hint black-box LLMs for further induction, that doesn't necessarily mean LLMs totally lacks of such knowledge (since proprietary LLMs can be significantly larger than open-source code language models in size and may have more training data, just different mixtures), it might be some long-tail knowledge that requires better prompting to exploit [35; 76]. On the other hand, the analysis help LLMs to be less influenced by the noisy contexts. This is beneficial for both retrieved and probed contexts.

Note that, in Equation 1, the final probability is marginalized over all possible \(_{k}\) and \(\). In practice, we take the most probable \(_{k}\) and keep the analysis with in the response before final result, without heavy sampling. We will discuss the sampling of each \(s\) within \(_{k}\) in SS2.4.

### Model Architecture and Instantiation

ProRec is a general framework and is not bounded to existing models and architectures. This section is our current implementation of the proper and recoverer that provide the best performance in our experiments.

Cross-Modal Knowledge ProberThe core of ProRec is the cross-modal proper, which is an encoder-decoder model aligned in the token embedding space of the SCFM, as illustrated in Figure 2. We would like to benefit from both pretrained binary function encoders that possesses binary domain knowledge and the strong SCFMs for generalizable probing. We choose the state-of-the-art CodeArt as our structure-aware binary function encoder \(g()\). CodeArt is a BERT-like transformer encoder that takes as input a disassembled binary function \(x_{}\) along with its dependency graph \(G_{}\) obtained by program analysis, and outputs the embeddings for all assembly code tokens and graph node tokens (each graph node token corresponds to one instruction, e.g., mov rax, [rbp+var_40], in the assembly code). We choose the Code-Llama  family as our base SCFM 4.

For the final proper architecture, we apply a simple two-layer MLP to project the _node token embeddings_\(_{}\), with indices N_IDX in all token embeddings, to source code token embeddings space.

\[_{}=(x_{},G_{})[,:]^{l_{n} d_{b}},\ _{}=(_{})^{l_{n}  d_{s}}\] (2)

where \(l_{n}\) denotes the number of node tokens, \(d_{b}\) is the dimension of the binary encoder and \(d_{s}\) is the dimension of the SCFM. The projected embeddings are fed into the SCLM as an additional prefix before regular subtoken embeddings for conditional generation.

We only use node token embeddings as binary features due to their significantly smaller quantity compared to all token embeddings (approximately one eighth) since assembly functions tend to be long. These embeddings also already capture some structural abstraction of binary code which is meaningful in HOBRE tasks.

RecovererWe leverage proprietary black-box LLMs (GPT3.5, Claude-3, and Gemini-Pro) as our recoverer, since they have strong reasoning ability and support long contexts. Specifically, the LLMs are prompted with \(x_{}\) as zero-shot baseline. For retrieval-augmented baseline and ProRec, we append the additional context to the original input and instruct LLMs to analyze relevance and then generate recovery. Detailed prompts can be found in Appendix D.

### Proper Training

The training of proper contains two stages: the pre-alignment of the binary encoder to a source code encoder, and the binary encoder-SCFM alignment. Both utilize data in the form of paired binary-source functions. The goal is to gradually align the binary encoder with the base SCFM with minimum knowledge loss.

Contrastive Assembly-Source Code Pre-AlignmentSince CodeArt is exclusively pre-trained on binary code corpus , we first align it with a pre-trained source code encoder coder5bp-embedding-110m  in the function-level embedding space as a pre-alignment stage in order to facilitate the later encoder-decoder alignment. To achieve this, we add a projection head for each encoder to project their [CLS] token embeddings to the same dimension \(d_{}\), forming a standard dual-encoder . This dual-encoder can encode \((x_{},G_{})\) into \(_{}^{d_{}}\) and \(x_{}\) into \(_{}^{d_{}}\). We train the dual-encoder in a CLIP-like symmetric contrastive fashion . Since the implementation is relatively standard, we refer readers to Appendix A for details.

The dual-encoder can function as a dense retriever to score and retrieve the top-\(k\) source functions for a query binary function from the source function pool of the training set, based on the similarity measure \((x_{},x_{})=_{},_{ })}\). It achieves 84% recall@1 on the validation set with a pool of 10k examples, demonstrating strong performance as a retriever. We utilize this dual-encoder to set up a retrieval-augmented HOBRE baseline to compare with ProRec in SS4.

Compute-Efficient Cross-Modal Prober AlignmentFor encoder-decoder alignment, we freeze all the parameters within the SCFM because we intend to explore the extreme end of probing knowledge from it. We freeze CodeArt from the first stage except for the last layer which is a transformer block for fast convergence and avoid too much change in the representation. The MLP is fully trainable. The objective of the alignment is to maximize

\[P(x_{}|x_{},G_{})=_{i}^{|x_{}|}P _{,}(x_{i}|_{},x_{<i})\] (3)

The limited amount of trainable parameters results in efficient training. For memory efficiency, we apply quantization (4bit or 8bit) [17; 18] to the base SCFM during alignment.

One evidence that the knowledge of the aligned proper is mainly from the SCFM pre-training instead of learned during alignment is shown in Figure 3. We sampled 500 \((x_{},x_{})\) pairs from the validation set and find that the negative log-likelihood \(- P_{}(x_{})\) for \(x_{}\) provided by the base SCFM and \(- P_{,}(x_{}|x_{},G_{})\) for \(x_{}\) conditioned on the \(x_{}\) provided by the aligned proper are highly correlated, indicating that the proper's ability is consistent with the base SCFM. Another interesting observation is that, instruction-tuned SCFMs typically show higher losses during alignment than their original models, which also implies the significance of pre-trained knowledge of source code for cross-modal ability as instruction-tuning may cause forgetting.

### Cross-Modal Knowledge Probing

For the probing process, i.e., sampling \(_{k}\) with the aligned \(P_{,}(|x_{},G_{})\), we want to cover a diverse yet relevant set of candidates. We leverage nucleus sampling  to first let the proper generate a relatively large set of source function signatures with high randomness (top-\(p=0.75\)). We use idea similar to retrieval by training a binary-signature dual-encoder to rank generated signatures and filter out the noisy ones. Ultimately, we use the proper to further complete the remaining signatures with smaller randomness (top-\(p=0.5\)). Since signature is short and important for HOBRE, our strategy achieves both better relevance compared to using a fixed small \(p\) for full function generation and better efficiency compared to sampling a large set of functions with a large \(p\).

## 3 Experiment Setup

We evaluate ProRec on two binary reverse engineering tasks: summarizing function semantics from decompiled code (SS3.1), and recovering function names from decompiled code (SS3.2). In this section, we first introduce our dataset, and the setups of each task.

DatasetThe training and evaluation of ProRec requires pair-wise data between a binary function and its corresponding source code. To the best of our knowledge, there is no publicly available dataset that contains matched source code with the binary program. Therefore, we follow a widely adapted practice in the reverse engineering domain [13; 34; 7], using GHCC 5 to automatically clone and compile repositories from GitHub. After the compilation, we map the resulting binary programs with the corresponding source code functions leveraging the debug information in binary programs. In total, our data consists of 270k pairs of binary and source code functions. We split 260k data samples for training and 10k data samples for test. We use 5% of the training data as the validation dataset. To make the evaluation cost tractable, we randomly sample 1k samples from the test dataset. For details in data processing and quality assurance, please see Appendix B.1.

### Binary Summarization

A binary summarization tool takes as input a snippet of decompiled code, and outputs natural language descriptions. It facilitates two key processes in the reverse engineering practice : understanding the purpose and domain of a program (referred to as _context relevance_), and understanding the functionality of a program (_functionality_). Please see Appendix F for a detailed example.

SetupWe instruct an LLM to summarize decompiled code with three setups: (1) providing the model with only the decompiled code; (2) additionally providing the relevant source code snippets retrieved from the datastore consisting of all source functions in proper's training set by the cross-modal retriever; (3) additionally providing and the source code snippets generated by ProRec. The first two setups are considered baseline approaches for comparison. We further instruct each LLM to summarize the source code corresponding to the test samples as reference summaries.

MetricsGiven the reverse engineering nature of binary summarization, the automatic evaluation metrics should reflect context relevance and functionality of the summary, different from text summarization. For final results, we report CHRF , which our meta-evaluation (described next) identified as the most aligned with human preferences among popular existing metrics such as BLEU . Additionally, we introduce and report two GPT4-based metrics for context relevance and functionality judgement respectively, following _LLM as a Judge_, which demonstrate strong correlation with human judgments. The GPT4-based metrics range from 1 (worst) to 5 (best) based on corresponding criteria. Further details (e.g., prompts and rationale) about the GPT4-based metrics can be found in Appendix B.2 and Appendix D.

User StudyWe conduct a user study 6 to gather human judgments on the quality of binary summarization, which serves as the gold standard for this task. The study aims to (1) perform a meta-evaluation of automatic metrics and (2) accurately assess the performance of different summarization approaches. Participants are asked to score a summary based on decompiled code, corresponding source code, and the reference summary. The scoring is done on two criteria--context relevance and functionality--on a scale from 1 (worst) to 5 (best). The method used to generate each summary is not disclosed to the participants. For the meta-evaluation of automatic metrics, we calculate the Spearman correlation between human judgments and automatic metric scores. For more details, we refer readers to Appendix E.

### Binary Function Name Recovery

Different from generating summary for a decompiled function, recovering function name requires more accurate understanding about program contexts and more concise abstraction for program semantics. This assists reverse engineers in efficiently navigating numerous functions in a real world binary program. A detailed example is provided in Appendix F.

SetupWe use the source code function names as ground truth for name recovery. Similar to the binary summarization task, we conduct experiments with three setups: prompting recoverers with only the decompiled code, with decompiled code and source code snippets obtained by a retriever, with decompiled code and source code snippets generated by ProRec. The first two setups are considered baselines for comparison.

MetricsWe evaluate the performance of a tool for the binary function name recovery task at different levels of granularity.

Token-level Metrics.In line with existing work in reverse engineering , we tokenize both the predicted function name and the corresponding ground truth, then compute precision, recall, and F1 score at the token level. For each metric, we first calculate the scores for individual function name predictions and then average them across all functions.

Character-level Metrics.We adapt BLEU , METEOR , ROUGE-L  for the function name by tokenizing function names into characters and computing these metrics on character level, similar to [40; 57]. They provide a fine-grained evaluation of the function names and can avoid some limitations of tokenization.

## 4 Results

In all the following experiments, we report ProRec results based on CodeLlama-34b (4bit quantized). For both the retrieval-augmented baseline (+retrieval) and ProRec (+ProRec), we use their top-5 contexts as augmentation. The versions of the black-box LLM recoverers are gpt-3.5-turbo-1106 for GPT3.5-turbo, claude-3-haiku-20240307 for Claude-3, gemini-1.0-pro for Gemini-Pro, and gpt-4-turbo-2024-04-09 for GPT4 Evaluator.

    &  &  \\   & & CHRF & G4-F & G4-C & P\({}_{}\) & R\({}_{}\) & F\({}_{}\) & cBLEU & cRoL & cMETEOR \\  GPT-3.5-turbo & - & 30.4 & 3.6 & 3.8 & 16.3 & 20.9 & 17.2 & 11.9 & 45.1 & 38.1 \\ GPT-3.5-turbo & +retrieval & 31.7 & 3.7 & 3.9 & 15.5 & 21.3 & 17.0 & 10.8 & 45.5 & 38.3 \\ GPT-3.5-turbo & +ProRec & **33.5** & **4.2** & **4.0** & **22.2** & **28.3** & **23.5** & **14.4** & **47.6** & **41.1** \\  Gemini-Pro & - & 27.1 & 3.7 & 3.5 & 25.3 & 28.7 & 25.3 & 16.8 & 48.1 & 39.5 \\ Gemini-Pro & +retrieval & 26.4 & 3.4 & 3.2 & 22.7 & 23.3 & 21.6 & 16.2 & 45.8 & 36.5 \\ Gemini-Pro & +ProRec & **27.6** & **3.8** & **3.6** & **32.6** & **30.5** & **29.9** & **22.4** & **50.9** & **40.9** \\  Claude-3 & - & 33.5 & 3.7 & 3.9 & 16.5 & 22.3 & 17.9 & 13.2 & 40.7 & 33.9 \\ Claude-3 & +retrieval & 33.9 & 3.8 & 3.9 & 19.9 & 23.9 & 20.5 & 14.0 & 43.3 & 36.0 \\ Claude-3 & +ProRec & **34.9** & **4.0** & **4.1** & **25.9** & **29.4** & **26.0** & **17.8** & **47.7** & **40.1** \\   

Table 1: Main results of binary summarization and binary function name recovery. “G4-F” and “G4-C” denote GPT4Evaluator for functionality and context relevance, respectively. P\({}_{}\), R\({}_{}\), F\({}_{}\) denote token-level precision, recall, F-1 score as in SymLM . “cBLEU”, “cRoL”, and “cMETEOR” stands for character-level BLEU, ROUGE-L, and METEOR scores.

### Binary Summarization Results

We show the results for binary summarization in Table 1. Observe that ProRec helps all models generate better summary in terms of CHRF. A retriever, on the other hand, may introduce noise (irrelevant source functions) to a model and even makes the results worse (e.g., for the Gemini-Pro model). Moreover, we can see that ProRec achieves higher scores when evaluated with the GPT4Evaluator on functionality (G4-F) and context relevance (G4-C), indicating the summary of ProRec is more helpful to a human reverse engineer.

We further analyze the results of GPT4Evaluator to illustrate the advantage of ProRec. The results for summaries generated by GPT-3.5 are visualized in Figure 4. It is worth-noting that we define the score 3 as a "neutral" score, meaning that a summary does not contain specific context (for the context relevance question) or contains only correct but low-level operations without high-level abstractions (for functionality question). We can see that for most cases, GPT-3.5 achieves a score with at least 3. That indicates the LLM can largely understand the low-level behaviors of decompiled code. That is because decompiled code is in the C-syntax.

On the other hand, we can see that for the context relevance question, both retrieval-augmented baseline and ProRec introduces more useful context information to the model, and thus the resulting summaries have closer relevance to the ground truth source code. Especially, queries with the code snippets generated by ProRec achieve more scores 4 and 5 than queries enhanced with a retriever's results. That illustrates ProRec indeed generates code snippets with better context relevance than a retriever.

For the functionality question, we can observe similar patterns. That indicates better contexts introduced by ProRec help the LLM to understand code functionality. We show a detailed example in Appendix F.

Human EvaluationTable 2 presents the human evaluation results from our user study for 50 randomly sampled summaries of each method (using GPT-3.5-turbo as the recoverer). The human evaluation aligns with the automatic metrics: ProRec consistently outperforms the other approaches in terms of both context relevance and functionality, according to human judgment.

### Binary Function Name Recovery Results

We show results for binary function name recovery in Table 1. We can see that the code snippets generated by ProRec helps all three LLMs predict better names in terms of token-level precision, recall, and F-1 score designed for reverse engineering task . Especially, ProRec outperforms a retriever by a large margin, indicating that ProRec generates more relevant code than a retriever. For character-level metrics, ProRec shows similar improvements, not biasing towards certain metrics. Note that different LLM recovers can have different performance on the two tasks, e.g., Gemini-Pro is better at binary function name recovery than summarization compared to other two models, yet this does not influence the improvement ProRec brings to both tasks and all recoverers.

    & **Cont. Rel.** & **Func.** \\  - & 4.29 & 4.22 \\ +retrieval & 4.49 & 4.43 \\ +ProRec & **4.76** & **4.62** \\   

Table 2: Human evaluation of binary summarization results w.r.t. context relevance and functionality.

Figure 4: Scores from our proposed GPT4 evaluator for summaries generated based on GPT3.5-turbo. The x-axes denote context relevance (left) and functionality (right), respectively. Larger scores are better. Bars denote the number of summaries with the corresponding score, and dashed lines denote the number of summaries with _at least_ the corresponding score.

## 5 Analysis

How does black-box LLM's internal analysis \(\) help robust recovery?We study the influence of LLM's internal analysis by evaluating retrieval-augmented recovery and ProRec with different number of additional contexts, since we believe this kind of internal analysis is crucial for LLM-based reconveers to perform robust binary recovery with additional contexts, especially when the provided contexts are noisy. We run binary function name recovery for 100 randomly sampled examples, 3 times, for zero-shot recovery (dec-only), retrieval-augmented recovery (recovery), and ProRec. As shown in Figure 5, the internal analysis consistently reduce the variance of function name recovery performance of both retrieval-augmented recovery and ProRec. This is particularly true for retrieval when \(k\) gets large. We deem that it may due to a lack of function-level similar source code in the data store. On the other hand, we observe sometimes LLM tend to be too conservative without leveraging the extra contexts with the internal analysis, potentially because of our not specifically optimized prompts which can be fixed by making some adjustments. Moreover, we argue that _misleading is worse than not informative_, and reverse engineers can further interact with LLMs for more aggressive induction after obtaining a conservative response.

Ablation Study on Base Source Code Foundation Model SizeProRec's performance relies on the ability of the base SCFM, where size is a crucial indicator since knowledge is represented by model parameters. Therefore, we study the influence of base SCFM size. We train three probers based on CodeLlama-7b, CodeLlama-13b, and CodeLlama-34b, all in 4bit quantization for fair comparison. We report statistics of these three probers in Table 3. As shown in the table, with growing number of base model size, the proper achieve a lower loss on validation set, which leads to an increase in average n-gram overlap of probed source code fragments and the oracle source function, which we run 3 times on 100 examples for each row. However, n-gram overlap with oracle source function seems not to significantly influence downstream task performance like CHRF for binary summarization. We hypothesize that this is potentially due to the tasks like binary summarization is not very sensitive to subtle symbolic difference, which means we can leverage modest size SCFM for probing instead of large ones, being economic in real practice.

Case StudyWe examine three specific cases to illustrate the performance and limitation of ProRec in Appendix F.

## 6 Related Work

Large Multimodal ModelsRecent advancements in vision-language models have demonstrated their efficacy across a range of practical applications such as image captioning , visual question

Figure 5: Binary function name recovery results with and without LLM’s internal analysis by using top-\(k\) additional contexts on 100 examples.

  
**Base SCFM** & **Trainable Params** & **Ratio (\%)** & **Eval Loss** & **N-gram Recall (1-4)** & **CHRF** \\  CodeLlama-7b & 27M & 0.393 & 0.6756 & 27.22 / 13.69 / 8.21 / 5.14 & 31.52 (\(\)0.642) \\ CodeLlama-13b & 37M & 0.283 & 0.6387 & 27.51 / 13.88 / 8.40 / 5.32 & 32.01 (\(\)0.886) \\ CodeLlama-34b & 80M & 0.237 & 0.5786 & 27.58 / 14.06 / 8.55 / 5.45 & 31.54 (\(\)0.163) \\   

Table 3: Statistics of proper with different base SCFM sizes.

answering [5; 16; 3], and image-text matching . While the limited availability of datasets that align different modalities was perceived as a major impediment to scalability, recent works leverage the knowledge embedded within pre-trained large language models [2; 6; 37; 15; 41; 80]. Beyond their capacity to interpret diverse information modalities such as images  and audio , LLMs have increasingly been aligned with graph structures [11; 59] and gained widespread attention. In particular, there have been successful attempts that leverage LLMs for graph data involves the Graph2Text strategy, which transforms graph data into textual representations. This technique has been effectively utilized in several studies [62; 21; 74]. The designs in the proper of ProRec share some similarity with recent LMMs, with a modality-specific encoder, and a SCFM decoder. However, we tackle the binary-source code multi-modality which is largely unexplored compared to popular modalities. Also, the multi-modal project is used in a larger probe-and-recover framework instead of end-to-end training.

Retrieval-Augmented GenerationRetrieval-augmented generation is widely applied in knowledge-intensive scenarios, such as question answering [22; 26; 31], molecule generation , and source code generation . By leveraging a non-parametric datastore, retrieval-augmented approaches decompose knowledge and LMs, can complement some long-tail knowledge or keep the knowledge up-to-date without heavy tuning the model which is costly. ProRec, on the other head, tries to exploit knowledge within a parametric SCLM for black-box LLM-based binary recovery. A closely related work is GenRead that prompts InstructGPT to generate context instead of retrieval for knowledge intensive tasks. ProRec differs from this work in that binary recovery requires cross-modal understanding and informative contexts cannot be obtained by directly prompting LLMs We introduce specially designed cross-modal alignment to allow informative context generation.

Binary Reverse EngineeringAdvances in machine learning models have been widely used to solve challenging tasks in binary program analysis [51; 49; 61; 71; 58; 50]. However, most work focuses on reasoning binary program, and is not human-oriented. Another stream of work trained smaller end-to-end models for individual human oriented tasks, such as variable name prediction [47; 13; 72; 73], and function name recovery . Nonetheless, these models are not benefiting from pretraining efforts and thus have sub-optimal performance . Preliminary study shows HOBRE remains a challenge for state-of-the-art LLMs [29; 56]. Our efforts attempt to address this challenge, leveraging pretraining knowledge of SCFMs to help HOBRE.

## 7 Conclusion

In this paper, we introduced a novel probe-and-recover framework, ProRec, designed to bridge the semantic gap between binary code and human-understandable source code. By integrating an aligned binary-source encoder-decoder model with black-box large language models, our approach effectively synthesizes symbol-rich code fragments from binary input, providing valuable context for improving binary analysis tasks. Our extensive evaluations demonstrate that ProRec significantly enhances performance in both binary summarization and binary function name recovery tasks.

Limitations & Future WorkWe experiment with a simple achitecture and straightforward alignment of the binary-source proper in this paper, which might not be optimal for ProRec. Future work can explore better proper architecture and alignment objectives. Moreover, currently we only focus on intra-procedure analysis, similar to most existing work. In practice, HOBRE needs to deal with full binary with multiple functions. An important direction will be extending ProRec to inter-procedure scenarios, where additional information from the whole program such as call-graph can be leveraged, building program-level binary reverse engineering agents.

## 8 Acknowledgement

We thank the anonymous reviewers for their valuable comments and suggestions. We thank all the participants in our user study. We are grateful to the Center for AI Safety for providing computational resources. This research was supported in part by DARPA VSPELLS - HR001120S0058, IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.