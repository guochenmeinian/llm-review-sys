# Generating Compositional Scenes via Text-to-image RGBA Instance Generation

Alessandro Fontanella

University of Edinburgh

&Petru-Daniel Tudosiu

Huawei Noah's Ark Lab

&Yongxin Yang

Huawei Noah's Ark Lab

Shifeng Zhang

Huawei Noah's Ark Lab

&Sarah Parisot

Microsoft Research

Work done while at Huawei Noah's Ark Lab

###### Abstract

Text-to-image diffusion generative models can generate high quality images at the cost of tedious prompt engineering. Controllability can be improved by introducing layout conditioning, however existing methods lack layout editing ability and fine-grained control over object attributes. The concept of multi-layer generation holds great potential to address these limitations, however generating image instances concurrently to scene composition limits control over fine-grained object attributes, relative positioning in 3D space and scene manipulation abilities. In this work, we propose a novel multi-stage generation paradigm that is designed for fine-grained control, flexibility and interactivity. To ensure control over instance attributes, we devise a novel training paradigm to adapt a diffusion model to generate isolated scene components as RGBA images with transparency information. To build complex images, we employ these pre-generated instances and introduce a multi-layer composite generation process that smoothly assembles components in realistic scenes. Our experiments show that our RGBA diffusion model is capable of generating diverse and high quality instances with precise control over object attributes. Through multi-layer composition, we demonstrate that our approach allows to build and manipulate images from highly complex prompts with fine-grained control over object appearance and location, granting a higher degree of control than competing methods.

## 1 Introduction

The development of text-to-image generative diffusion models has allowed the generation of high-quality synthetic images guided by textual prompts. However, achieving desired image quality and properties inherently requires tedious and meticulous prompt engineering . As image content is solely described using a textual description, prompt content has to be carefully crafted, notably taking into account the models' struggles to understand subtleties of language (e.g. counting, object attributes, negation or spatial relationships) . Additionally, minor prompt modifications can lead to substantial modifications of generated image content, further increasing prompt crafting tediousness.

An intuitive way of improving controllability of generated content is by providing alternative image descriptors, e.g. using image layouts which describe the location of specific objects in an image as bounding boxes. Several approaches have been proposed to inject layout information in the generative process. Training based methods  adapt or introduce new model weights to introduce object coordinates as conditioning, while  leverage cross-attention manipulationand inference time optimisation. While substantially increasing control over image structure, these solutions still generate all components as a single denoised image. This limits the user's ability to finely control object attributes, and layout adjustments requires re-generation with limited content preservation guarantees. These issues can potentially be addressed with image editing techniques. This, however, can require expensive data construction and training , while the displacement or resizing of objects (essential for layout manipulation) remains a very challenging type of edit that few have explored [9; 31]. This challenge can be attributed to the fact that objects and backgrounds have to be regenerated, while at the same time ensuring appearance preservation with the original image.

The emerging layer-based generation paradigm [38; 49; 40] has shown promise in addressing these limitations. The idea is to represent images as multi-layer stacks, where different image components are generated on separate layers. This has two key benefits: 1) instances can be generated more precisely as separate images with individual prompts, 2) separating image components simplifies image manipulation tasks, as only the relevant layers will be modified. Nonetheless, layer-based methods typically suffer from the fact that layers are not fully generated, as they are collapsed to a single image in the later stages of a diffusion process. Generating all components jointly can affect the ability to achieve fine-grained control of layers attributes, control their relative positioning in 3D space (as layers are collapse with no hierarchical structure), ensure a smooth composition of all image component, and achieve layout manipulation abilities with strong content preservation (as layers and composite image influence each other).

In this work, we seek to address current limitations through a multi-stage generation process that is designed for fine-grained control, flexibility and interactivity. We propose to build complex scenes by first generating individual instances as RGBA images, then iteratively integrating them in a multi-layer composite image according to a specific layout. The ability to generate individual instances allows to finely control instance appearance and attributes (e.g. colours, patterns and pose), while their layered composition allows to easily control and modify positioning, scale and ordering. In particular, our two-stage approach provides intrinsic layout and attribute manipulation abilities with strong content preservation. To achieve our goal, we firstly train a diffusion model capable of generating RGBA images, i.e. images with alpha transparency information. Directly generating isolated instances, c.f. generating and extracting them with a segmentation model, ensures better transparency masks and more fine-grained control of instance attributes. Our RGBA generator is obtained by fine-tuning a latent diffusion model (LDM) using RGBA instance data from the recently released MuLAn dataset . In order to incorporate transparency information in the generative process, we devise a transparency-aware training procedure for both VAE and diffusion model. In contrast with the contemporary transparent generation method of , which implicitly encodes transparency information in the null space of the VAE to carry out standard LDM fine-tuning, we explicitly integrate transparency in our generation and training process. Our VAE is trained so as to disentangle RGB and alpha channels, ensuring colour and detail preservation in RGB reconstruction. Our LDM is then fine-tuned with a novel training paradigm leveraging our disentangled latent space, that allows a conditional sampling-driven inference where alpha and RGB latents are sequentially denoised with mutual conditioning. Finally, we leverage our RGBA generator to build composite images with fine-grained control over object attributes and scene layout. We build multi-instance scenes via a multi-layer noise blending solution, where each instance is associated to a specific image layer. Crucially, each instance is integrated in the scene one layer at a time, yielding increasingly complex image layers so as to ensure scene coherence and accurate relative positioning. This contrasts with pre-existing multi-layer works which assemble all instances at once [38; 49], and is uniquely afforded by our RGBA generation process. By manipulating latent representations in early stages of the denoising process, we are able to achieve high degrees of precision and control, while at the same time generating smooth and realistic scenes. An overview of our complete methodological process is shown in Fig. 1.

We provide a thorough evaluation of our RGBA generator's capabilities, showing that we are able to generate highly diverse objects and precisely control their attributes and style, outperforming alternative solutions. In addition, our scene composition experiments show that we consistently outperform state of the art layout controlled methods, while highlighting our unique ability to generate and manipulate complex scenes with strongly overlapping objects. In summary, our main contributions are the following:* We propose a multi-layer generation framework designed for fine-grained controllable generation that grants user control over instance appearance and location, with intrinsic scene manipulation capabilities.
* We introduce a novel fine-tuning paradigm for pre-trained diffusion models to generate individual objects as RGBA images with transparency information. We propose a disentangled training strategy that relies on mutual conditioning of RGB and alpha channel latents.
* We develop a multi-layer scene compositing strategy based on noise blending that allows to build and manipulate complex scenes from multiple instance images. By combining RGBA generation with scene compositing, we are able to generate images with accurate attribute assignment and relative object positions.

## 2 Related Work

**Text-to-image and transparent generation.** GLIDE  first proposed text-to-image generation with diffusion models by adopting classifier-free guidance through text. While GLIDE trains the text encoder jointly with the diffusion prior using paired image and text data, Imagen  employs a frozen large language model as the text encoder. More recently, Latent Diffusion Models (LDM) trained in latent space have increased in popularity, given their lower computational requirements. Stable Diffusion incorporates an adversarial objective to learn the latent representation and enhance the authenticity of generated images and introduces cross-attention for text conditioning. While the U-Net  architecture was originally the most popular for latent diffusion models, recent works are converging towards the use of Diffusion Transformers (DiT) , which operate on latent patches. For example, Pixart-\(\) has demonstrated remarkable image generation capabilities, while reducing the computational requirements to \(10.8\%\) of Stable Diffusion-v1.5's training time. Leveraging these models to generate instances with transparency information has been rarely explored. Text2Layer  generates foreground and background images separately, defining foreground as the ensemble of all salient objects and predicting the foreground alpha mask separately from RGB pixels. Developed concurrently, LayerDiff  generates image components as separate images, providing the transparency masks as input. This technique tends to generate partial instances, as occluded areas are not generated. Also contemporary, LayerDiffusion generates transparent images by encoding transparency in the null space of the pre-trained VAE, allowing to fine-tune a diffusion model using standard techniques. While able to generate high quality instances, this implicit transparent modelling can lead to inconsistent results, without guarantees that transparency will be achieved. In contrast, our explicit disentangled latent space allows for more control and guarantees transparent images outputs.

**Image editing and scene controllability** The increasing popularity of diffusion models has highlighted the limitations of working solely with text based control. We identify two main areas of research seeking to increase generative controllability: image editing and layout controlled generation. Image editing can be achieved by fine-tuning on dedicated dataset , constrained sampling with CLIP  losses [20; 21; 45; 17; 50], mask guided inpainting [30; 59; 11; 52; 7], training-free cross attention manipulation [12; 34; 5; 48], and inference-time optimisation . Despite achieving impressive results for replacement and local modification tasks, image editing often struggles with more complex manipulations such as moving, rescaling and removing, especially on non isolated instances. By modifying pre-generated content, editing methods additionally have limited layout control capabilities. Alternatively, precise layout control during generation has been explored by introducing additional conditioning. GLIGEN , ReCo  and Boxdiff  achieve image generation from layouts given as bounding boxes specified by the user. ControlNet  allows to incorporate several forms of image based conditioning, such as sketches, depth maps, or canny edges. These techniques afford high generative controllability, but lack layout editing capability. Composite  and layer-based generation techniques [38; 49; 40] offer a promising avenue, where individual scene components are associated with unique prompts and assembled in a scene with bounding boxes or instance specific masks, leveraging the diffusion model's intrinsic priors to build coherent scenes. However, generating scenes and all individual components jointly requires compromising between instance quality, attribute accuracy and scene coherence, reducing flexibility and control. We build from layered approaches, and use multi-layer paradigms to iteratively integrate pre-generated RGBA instances in increasingly complex images, affording us precise control over layout, instance attributes and allowing us to focus on scene composition quality solely during the composition step.

## 3 Methods

### Preliminaries

**Diffusion models.** Diffusion models learn to reverse a _forward diffusion process_, where images are iteratively converted to near isotropic Gaussian noise over \(T\) timesteps as \(y_{t}=)}y_{t-1}+_{t}\), where \((0,I)\) and \(\) describes a noise schedule. Diffusion models are trained to reverse the process by learning to predict the noise \(_{}\) that was added at a specific timestep:

\[=_{y,,t}=\|-_{}(y_{t},t,C)\|\] (1)

Where \(C\) is a conditioning variable, typically a text prompt embedding. Starting from isotropic Gaussian noise \(y_{T}\) at inference time, diffusion models generate images through the reverse iterative denoising process, where \(y_{0}\) is the final denoised output. At a given timestep \(t\) of this _backwards diffusion process_, the pre-trained model predicts the noise \(_{}\) that updates current noisy image \(y_{t}\) to move on to the next timestep \(t-1\). In this work, we consider _latent diffusion models_ (LDMs), where the diffusion process is carried out in the latent space of a Variational AutoEncoder (VAE) .

Alternatively, the pre-trained model can be leveraged to convert an image \(x_{0}\) to Gaussian noise by iteratively adding predicted noise \(_{}(x_{t},t)\) according to the same noise schedule . This _inversion_ process provides an estimation of the input noise \(x_{T}\) that allows to obtain \(x_{0}\) via the backwards diffusion process. This technique is particularly relevant to image editing tasks and noise blending methods like ours. To differentiate between both processes, we use \(y_{t}\) to refer to _generated_ latents and \(x_{t}\) to refer to _inverted_ latents in the remainder of this paper.

**RGBA representation.** An RGBA instance image is modelled as a four channel image \(I^{w,h,4}\), where the additional _Alpha_ channel describes the level of transparency of RGB pixels. The alpha channel takes values in \(\), where 0 correspond to full transparency. Transparent pixels for which we do not have RGB information are set to black (\(RGB=(0,0,0)\)).

### RGBA Instance Generation

A simple method for creating instances with transparency involves using a text-to-image model to generate an initial image, then applying image matting techniques  to extract the transparency alpha mask. This reliance on automated matting can yield inaccurate masks, particularly when dealing with complex object structures and image backgrounds. In addition, this approach reduces controllability, as object attributes can bleed in the background content rather than the instance itself

Figure 1: Overview of key components of our proposed methodology.

. Alternatively, we propose to natively generate RGBA instances using a generative diffusion model. To this end, we developed a transparency aware training procedure for a pre-trained LDM that focuses on the interaction between RGB and alpha channels. This section will first introduce our training dataset, then detail our fine-tuning approach for the LDM's VAE and diffusion model.

**Training data.** We employed 87989 instances from the MuLAn dataset , a novel dataset consisting of automatically generated RGBA decompositions with a diverse array of scenes, styles and object categories, and 15791 instances extracted from a variety of image matting datasets with high quality masks. More details about the datasets are provided in Appendix A.1.

**RGBA VAE.** We train our RGBA VAE following the training procedure of , comprising a \(L_{1}\) reconstruction loss \(_{recon}(I,((I)))\), a perceptual loss \(_{LPIPS}(I,((I)))\), a discriminator based adversarial loss \(_{adv}(((I))))\), and a Kullback Liebler (KL) loss between the estimated latent distribution and a normal distribution \((x:0,1)\). However, we make key modelling and training adjustments in order to accommodate for the new alpha transparency information.

Firstly, the additional channel is simply taken into account by replacing and retraining the input and output layers of the model. Secondly, we observed that learning a joint RGBA latent space leads to entanglement of RGB and alpha channels, affecting generation capability of diffusion models trained in this latent space. We address this challenge by disentangling representations in the latent space: our VAE predicts two separate distributions \((x:_{RGB},_{RGB})\) and \((x:_{},_{})\), each associated with a separate KL loss. While we do not explicitly model the separate distributions to encode RGB and alpha channels respectively, it is a natural disentanglement of the data.

Lastly, the VAE in  is trained with very small KL regularisation (e.g. a weight factor of \(w_{KL}=10^{-6}\) on the KL loss), so as to focus on reconstruction quality. In our setting, we observed that this deviation from standard VAE training was harmful to generative ability, yielding dark and highly contrasted images. In contrast, training our VAE with large regularisation (\(w_{KL}=1\)) noticeably improved image quality, while at the same time enforcing disentanglement more strongly (see Appendix A.3 for visual comparisons). The perceptual loss is also computed separately for RGB and alpha channels and the two components \(_{LPIPS}(I_{RGB},((I))_{RGB})\) and \(_{LPIPS}(I_{},((I))_{})\) are then averaged.

**RGBA Diffusion Model.** In the second stage, we fine-tune the LDM on our instances datasets. Our VAE fine-tuning keeps the dimension of the original LDM latent space (4 channels and \(64 64\) spatial dimension), allowing us to directly fine-tune without architecture adaptation. However, in our case, the latent space also encodes information on the transparency layer. When sampling from our model, we seek to exploit the mutual dependency between RGB and alpha channels. In particular, given noised latents \(y_{t}^{RGB}\) and \(y_{t}^{}\) for RGB and alpha channels respectively at timestep \(t\), information contained in \(y_{t-1}^{RGB}\) could be employed to inform the update from \(y_{t}^{}\) to \(y_{t-1}^{}\) and vice-versa. In order to train the network to leverage this conditional information, we modify the training procedure of the LDM. Given \(y_{0}^{RGB}\) and \(y_{0}^{}\), we sample two different Gaussian noise vectors \(^{RGB}\) and \(^{}\) and use them to compute \(y_{t}^{RGB}\), \(y_{t-1}^{RGB}\),\(y_{t}^{}\), \(y_{t-1}^{}\) with the forward process of the diffusion model and \(t\) randomly sampled in \([0,T]\), with \(T\) number of training steps. Our network is trained to predict \((^{RGB},^{})\) jointly given one of the following as input: \((y_{t}^{RGB},y_{t}^{})\), \((y_{t}^{RGB},y_{t-1}^{})\), or \((y_{t-1}^{RGB},y_{t}^{})\).

This training procedure unlocks the ability to perform conditional sampling. Given \((y_{t}^{RGB},y_{t}^{})\), we can alternate between updating the alpha component \(y_{t-1}^{}\) and then use \((y_{t}^{RGB},y_{t-1}^{})\) to update the RGB component, obtaining \((y_{t-1}^{RGB},y_{t-1}^{})\). Alternatively, we can update the RGB component first and use it to condition the alpha update, but observed that the former approach worked best in practice. When sampling from diffusion models, it is common to use fewer sampling steps than at training time for faster image generation . Therefore, in order to make our training regime more flexible and applicable to a variety of sampling strategies, we additionally use pairs \((y_{t}^{RGB},y_{k}^{})\), and \((y_{k}^{RGB},y_{t}^{})\) as conditioning input in the second half of training iterations, with \(k\) randomly sampled in \([0,t-1]\).

### Multi-Layer Noise Blending for Scene Composition

We consider that we have an image layout available, and generated \(K\) instances \(\{I_{k},M_{k}\}\) using our RGBA generator, where \(I_{k}\) refers to RGB values and \(M_{k}\) is the transparency alpha mask. We represent a layout as a collection of bounding boxes, where each box is associated with a specific instance: \(=\{[cx_{k},cy_{k},w_{k},h_{k}],k 1,,K\}\). We design our scene composition approach as a multi-layer noise blending process, where instances are sequentially integrated in intermediate layered representations. As a result, we concurrently generate \(K+1\) images (sorted as background, and \(K\) composite images with increasing number of instances). While more costly, we observed that generating layered images affords more flexibility, better control over relative positions of instances and yields more natural compositions. An algorithmic overview is provided in Appendix B.

We consider the latent representation \(x_{0}^{k}\) of instance image \(I_{k}\), and compute its noisy representation \(x_{t}^{k}\) at all diffusion timesteps \(t[1,T]\) using DDIM inversion . To generate our layered composite image, we first initialise our \(K+1\) images with the same noise vector \(y_{T}\). Inspired from compositing  and layered  image generation methods, we combine noisy image representations for the first \(n\) timesteps of the diffusion denoising process. After carrying out the denoising update at time \(t\), and before moving on to the next time step, we sequentially update noisy images as follows:

\[y_{t}^{k}=y_{t}^{k-1}(1-m_{k})+x_{t}^{k} m_{k}k[1,K]\] (2)

where \(m_{k}\) is the instance alpha mask \(M_{k}\) downsampled to the latent space and \(y_{t}^{0}\) is the background image generated concurrently. Equation 2 builds layered images by iteratively injecting new instances in the noisy images at the desired locations. Carrying this process only at the first \(n\) generative timesteps enables to precisely control scene composition and instance appearance, while at the same time allowing for a smooth and realistic composition. The smaller \(n\) is, the more freedom is given to the generative model to adapt scene content. We further detail two optional additional mechanisms.

**Background blending.** In the noise blending procedure described above, the background is generated independently from the rest of the image. That can reduce blending quality as generated background elements can be incompatible with instance locations. We propose to address this limitation by introducing background blending. We inject appearance information from the composite image to the background as:

\[y_{t}^{0}=y_{t}^{0} m^{*}+(y_{t}^{0}+y_{t}^{K})(1-m^{*})\] (3)

where \(m^{*}\) is the union of all instances alpha masks. The background area of the composite image gets adjusted throughout the generation timesteps to accommodate instances neighbourhood. Equation 3 has two benefits: 1) it ensures stronger consistency between the background layer and final composite image, and 2) allows the background image to be generated with stronger awareness of instances. This blending process can be carried out for the first \(b\) steps of the composition process.

**Increasing cross-layer consistency.** In situation where intermediate layers are needed, and \(n\) is small, consistency can be enforced on the composite image by applying Eq. 2 to \(y^{K}\) for \(n_{s}\) subsequent timesteps: \(y_{t}^{K}=y_{t}^{K}(1-m_{k})+y_{t}^{k-1} m_{k}\) for \(k[1,K-1]\).

**Scene editing.** Our design intrinsically allows scene manipulation and editing easily. Once instances are pre-generated, scene content can easily altered locally by replacing instances, or modifying instance location by providing new bounding boxes. The modified image can easily be generated following our scene composition process with a fixed seed, without any additional constraints.

## 4 Experiments

### RGBA generation

Our RGBA generator is fine-tuned from a pre-trained PixArt-\(\) model . As baselines, we compare quantitatively and qualitatively to Stable Diffusion 1.5 (SD-1.5) and PixArt-\(\), adding 'on a black background' to the captions to replicate instance generation. We additionally compare to both models combined with the Matte Anything (MA) matting algorithm , our reimplementation of Text2Layer , and LayerDiffusion . In all approaches we used 100 steps. Our RGBA generator leverages the conditional sampling approach described in Sec. 3.2.

In Fig. 2 and 3 we show qualitative examples of instances obtained with our RGBA generator. Fig. 2 shows that we are able to generate instances across different styles and to follow fine-grained attributes in prompts. Fig. 3 provides visual comparisons to our transparent baselines. We can observe that our approach is able to generate realistic instances following the instructions given. Text2Layer shows lower image quality and excessive transparency, while LayerDiffusion struggles to follow prompt details, such as image style. Combining SD with Matting allows to achieve reasonable segmentation of the instances generated, while when applied to PixArt-\(\) it can sometimes struggle to correctly identify and segment the main object of the image, especially when dealing with artwork content. On top of attributes bleeding in the background, this highlights how unreliable matting can be for instance generation purposes.

We quantitatively evaluate the quality of the generated instances with the Kernel Inception Distance (KID) , a metric that was proposed to reduce the bias of the Frechet Inception Distance, especially when evaluated on a low number of samples. We compute it using features from the last convolutional layer of the Inception v3 model and considering only the RGB channels of the generated images. In order to evaluate the alpha mask generated by the RGBA generator, following , we employ masks

Figure 3: Instances generated with the captions: ‘a maigestic brown bear with dark brown fur, its **head slightly tilted** to the left and its **mouth slightly open’**, ‘an **Impressionist** portrait of a woman’, ‘a portrait of a young man, depicted in a **blend of blue and red tones’**.

Figure 2: Our model can generalise to different styles and to follow detailed instructions. Top row: ‘a **cartoon style** frog’, ‘a digital artwork of an **anime-style** character with **long, flowing white hair** and **large and expressive purple eyes** in a **white attire**’, ‘a **stylised** character with a traditional Asian **hat, with a red and green pattern**’, ‘a man with a **contemplative expression** and a **nearly trimmed beard**’, Bottom row: ‘a woman with a **classic, vintage style, curly hair, red lipstick, fair skin in a dark attire’, ‘a bird **mid-flight** with **brown and white feathers** and **orange** head’, ‘a **hand-painted ceramic vase** in **blue and yellow colours** and with a ** floral pattern**’, ‘a woman with **short, blonde hair**, vivid **green eyes**, in a **white blouse**, with a **gold necklace** featuring a pendant with a **gemstone**’.

obtained with ICON with PVT backbone  as ground truth, and compute the Intersection-Over-Union (IOU) with the predicted alpha masks. Lastly, to evaluate the correlation between the captions and the content of the generated instances, we employ the CLIP Score , which computes the cosine similarity between image and text features using the CLIP model (ViT-B/16 variant) . The results obtained are shown in Table 1. We can see that our approach obtains the best results for all the metrics, with KID of 0.0150, IoU of 0.892 and CLIP score of 18.49. PixArt-\(\) has the second-best KID of 0.0447. However, the result is worsened when applying MA. Pixart + MA also has the second best IoU of 0.8111. SD-v1.5 has KID of 0.0839, result which is improved to 0.0711 by adding MA. The same model also achieves IoU of 0.649. Text2Layer has KID of 0.0500 and IoU of 0.326, while LayerDiffusion obtains KID of 0.0772 and IoU of 0.320. The latter's lower performance could be attributed to unreliable transparent generation, as instances can be generated without transparency due to the modelling strategy, affecting average performance. With regards to CLIP scores, they are relatively close to each other, with SD-v1.5 + MA  obtaining the second-best score of 18.40.

**Ablation studies.** In order to explore the impact of our novel training procedure, we train two RGBA LDMs 1) without our conditioned training procedure (i.e. predicting noise \(\) without conditioning from different timesteps) and 2) without our conditional sampling inference where RGB and alpha are mutually conditioned for generation. Performance of these models is reported in Table 1, bottom rows. We can see that the largest gains in performance are obtained with our novel training procedure, while conditional generation results in more subtle improvement. As shown in Fig. 4, our conditional sampling allows to correct small details and critical areas of the alpha masks.

### Scene Compositing results

**Generation details.** We build composite scenes in three steps: 1) RGBA instance generation, 2) layout building: we draw one bounding box per generated instance and rescale instances accordingly, 3) noise blending of generated instances according to a global prompt. We keep parameters consistent across all scene compositions unless specified otherwise: guidance scale \(2.5\) (RGBA) and \(4.5\) (Blending), guidance rescale \(0.25\), noise blending steps \(n=30\), background blending \(b=20\) steps and consistency regularisation \(n_{s}=10\) steps. Both instances and composite image are generated over \(50\) steps. We use the Pixart-\(\) model for compositing.

**Baselines.** We compare our approach to state of the art scene composition methods focusing on layout control and interactivity. Our first baseline is Pixart-\(\) to provide intuition into the complexity of requested prompts and limitation of relying solely on text controls. Next, we compare to GLIGEN , which learns additional cross attention layer to integrate bounding boxes and bounding box text descriptors as generation conditioning. Finally, we compare our approach to multi-layer methods MultiDiffusion  and contemporary approach Instance Diffusion  for completeness. Similarly to ours, these approaches rely on noise blending techniques to build composite scenes. Besides our

   & KID \(\) & IoU \(\) & CLIP Score \(\) \\  SD-v1.5 & 0.0839 & N.A. & 18.33 \\ PixArt-\(\) & 0.0447 & N.A. & 18.10 \\ SD-v1.5 + MA & 0.0711 & 0.649 & 18.40 \\ PixArt-\(\) + MA & 0.0558 & 0.811 & 18.03 \\ Text2Layer \(\) & 0.0500 & 0.326 & 18.21 \\ LayerDiffusion & 0.0772 & 0.320 & 18.39 \\ Ours & **0.0150** & **0.892** & **18.49** \\   \\  No paired training & 0.0181 & 0.814 & 18.41 \\ No conditional sampling & 0.0152 & 0.887 & **18.49** \\  

Table 1: Quantitative evaluation of our RGBA generator. We measure KID for instance generation quality, IoU (Jaccard) between the alpha masks and ICON segmentation, and CLIP Score for the caption/image similarity. \(\): our reimplementation, best results are highlighted in **bold**.

Figure 4: Our proposed training and sampling approaches (c) improve results obtained with standard training (a) and standard sampling (b).

RGBA instance pre-generation, a crucial difference, in contrast to our multi-layer approach, is the use of noise averaging for overlapping objects. All methods use the same global prompt, GLIGEN and Instance diffusion leverage our bounding box layout and RGBA instance prompts as descriptors, while MultiDiffusion uses our instances alpha masks and captions as well.

Due to the inherently interactive nature of our and competing approach (layout design, instance ordering), we provide visual results in Fig. 5, comparing all methods across different scene compositions. For a fair comparison, we generate competing methods with multiple seeds and select the best result. We provide results for simple scenes with unusual attributes (a) and complex scenes with overlapping objects (b,c,d) (Additional results and ablations are available in Appendix E.2 and F.2). While GLIGEN is capable of accurately reproducing the desired layout, it often fails to assign the right attributes to objects (e.g. blue apple, orange plane, swan painting) and struggles with highly overlapping objects (fox and unicorn). In contrast, Multidiffusion is more accurate in terms of attribute assignments, but struggles to handle overlapping objects. This can be attributed to the noise averaging process, which fails to integrate a notion of instance ordering like our multi-layer approach. Instance diffusion achieves performance closest to ours, but still struggles with complex patterns, attributes and relative positioning (swan painting behind the couch, white bus with a red flower pattern, woman walking away). With our RGBA instance generation and multi-layer noise blending, we are able to accurately assign object attributes and follow the required layout, while successfully building smooth and realistic scenes.

**Scene manipulation results.** Finally, we evaluate our method's potential for scene manipulation. To optimise cross scene consistency, we set \(b=0\) (remove background blending) and set a common generation seed across all versions of the same scene. The manipulations we consider here are: attribute modification, instance replacement, and layout adjustment. We note that the first two tasks

Figure 5: Visual examples of scene composition results. RGBA instances are highlighted in **bold**.

require RGBA generation of new instances. We highlight that we _do not introduce any new explicit scene preservation or image editing technology_ in this experiment, therefore evaluating our method's potential for scene manipulation and controllability. We compare our results to contemporary method Instance Diffusion, which has been highlighted to possess similar editing capabilities . Results are reported in Fig. 6, showing that we are able to control and modify image content easily while maintaining strong consistency across different versions of the scene, without explicitly enforcing content preservation. This highlights the strong potential of multi-layer approaches to facilitate the development of image editing methods. We can see that we achieve substantially stronger scene preservation compared to instance diffusion, which can generate entirely different images and instances when modifications are too strong.

## 5 Conclusion

In this work, we introduced a novel multi-layer strategy to scene composition, that focuses on interactivity and fine-grained control. To achieve this, we proposed a new training paradigm that adapts diffusion models to generate transparent images, through channel disentanglement and conditional sampling. To build composite scenes from RGBA instances, we further present a multi-layer compositing strategy that concurrently generates increasingly complex scenes through cross-layer noise blending. Extensive experiments show that we are able to generate diverse, fine-grained RGBA instances, and assemble them in complex scenes, achieving precise control and high flexibility over scene structure. One key limitation of our approach is the independent generation of instances, increasing the challenge of assembling them in a coherent scene. Future work will explore conditioned RGBA generation to intrinsically generate coherent scenes, as well as RGBA editing methods to further improve fine-grained control over scene content.

Figure 6: Visual examples of scene manipulations compared to Instance Diffusion. Our layer-based approach allows to replace instances or modify their positions.