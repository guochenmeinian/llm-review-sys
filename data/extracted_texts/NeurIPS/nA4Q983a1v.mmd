# Recurrent Reinforcement Learning with Memoroids

Steven Morad\({}^{1,2}\), Chris Lu\({}^{3}\), Ryan Kortvelesy\({}^{2}\), Stephan Liwicki\({}^{4}\), Jakob Foerster\({}^{3}\), Amanda Prorok\({}^{2}\)

\({}^{1}\)Faculty of Science and Technology, University of Macau, China

\({}^{2}\)Computer Science and Technology, University of Cambridge, UK

\({}^{3}\)Engineering Science, University of Oxford, UK

\({}^{4}\)Toshiba Europe, UK

smorad@um.edu.mo, christopher.lu@exeter.ox.ac.uk, rk627@cst.cam.ac.uk, Stephan.Liwicki@toshiba.eu, jakob.foerster@eng.ox.ac.uk, asp45@cam.ac.uk

###### Abstract

Memory models such as Recurrent Neural Networks (RNNs) and Transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models called Linear Recurrent Models. We discover that the recurrent update of these models resembles a _monoid_, leading us to reformulate existing models using a novel monoid-based framework that we call _memoroids_. We revisit the traditional approach to batching in recurrent reinforcement learning, highlighting theoretical and empirical deficiencies. We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning.

## 1 Introduction

Reinforcement learning (RL) traditionally focuses on solving Markov Decision Processes (MDPs), although for many interesting problems the Markov state is hidden. Instead, we receive noisy or ambiguous _observations_, resulting in Partially Observable MDPs. The standard approach to RL under partial observability involves summarizing a sequence of observations into a latent Markov state using a _memory model_ or _sequence model_. Commonly used models include RNNs and Transformers.

Training Transformers or RNNs over long sequences is computationally expensive. Instead, prior work often splits these sequences into shorter fixed-length subsequences called _segments_ (Figure 1). Using segments adds implementation complexity, reduces efficiency, and introduces theoretical issues. Despite these drawbacks, most prior work and virtually all existing RL libraries follow this segment-based approach. A new class of sequence models, sometimes called Linear Recurrent Models, offers much greater efficiency over long sequences than Transformers or RNNs. We posit that we can utilize these efficient models to do away with segments and their associated drawbacks.

ContributionsWe aim to remove the need for segments in RL. First, we discover that many efficient memory models share an underlying structure reminiscent of _monoids_, a concept from category theory. We propose to extend the monoid into a _memoroid_, a mathematical framework which can represent a large class of efficient memory models. Armed with the memoroid, we propose a new batching method that eliminates the need for segments.

In particular, we

* Derive memoroids for existing sequence models, as well as the discounted return and advantage
* Introduce a method for inline resets, enabling any memoroid to efficiently process multiple episodes
* Demonstrate that using segments degrades recurrent value functions
* Propose a new memoroid-based batching method that eliminates the need for segments
* Use this batching method to improve sample efficiency and simplify recurrent RL loss functions

## 2 Preliminaries

Consider an MDP \((S,A,R,,)\), where at each timestep \(t\), an agent produces a transition \(T=(s,a,r,s^{})\) from interaction with the environment. We let \(s,s^{} S\) denote the current and next states and state space, \(a A\) denote the action and action space, \(R:S A S\) denote the reward function, and \(:S A S\) denote the state transition matrix (\(\) denotes a distribution). In RL, our goal is to learn a policy parameterized by \(\) that maps states to action distributions \(_{}:S A\). The agent samples an action from the policy given the current state \(a_{}(s)\), and stochastically transitions to the next state \(s^{}(s,a)\), receiving a reward \(r=R(s,a,s^{})\). The optimization objective is to find the parameters \(\) that maximize the expected return, discounted by \(\): \(_{}[_{t=0}^{}^{t}R(s_{t},a_{t},s_{t+1})]\).

### Rollouts, Causality, and Episode Boundaries

It is often practical to model terminal states in MDPs, such as a game over screen in a video game. In a terminal state, all actions lead back to the terminal state and the discounted return after entering the terminal state is always zero. We mark whether a state is terminal using the _done flag_\(d\{0,1\}\). The done flag is stored in the transition \(T=(s,a,r,s^{},d)\). Transitions are often used in loss functions to train the policy. However, while navigating the MDP we do not have access to the full transition - just the state. We receive the done flag, reward, and next state \((r,d,s^{})\) at the _next_ timestep. This distinction between current and next timestep becomes important when we execute memoroids over multiple episodes.

We find that our paper is more clear if we introduce a _begin flag_\(b\{0,1\}\) that is emitted alongside each observation, available during both training and rollouts. The begin flag is \(1\) at the initial timestep of an episode and \(0\) otherwise. We differentiate between a transition \(T=(s,a,r,s^{},b,d)\) available only in hindsight, and a partial transition \(=(s,b)\) as emitted while navigating the MDP. To reiterate, we can access \(\) at any time, but we can only access \(T\) during training.

Figure 1: We visualize the Segment-Based Batching approach often used in prior literature. A worker collects a rollout of episodes, denoted by color. Each episode is split and zero-padded to produce a batch of segments, each with a constant, user-specified segment length \(L\). Episodes exceeding the specified length are broken into multiple segments, preventing backpropagation through time from reaching earlier segments. Segments contain zero padding, reducing efficiency, biasing normalization methods, and necessitating padding-aware recurrent loss functions.

### Partial Observability

In partially observable settings, we cannot directly measure the Markov state \(s\). Instead, we indirectly measure \(s\) via the observation \(o(s)\), following the observation function \(:S O\). With the observation replacing the state, interaction with the environment now produces a transition \(P=(o,a,r,o^{},d,b)\) and partial transition \(=(o,b)\). For certain tasks, the action from the previous timestep is also necessary, and is implicitly included in the observation.

A sequence of transitions starting where \(b=1\) and continuing until a terminal state is known as an episode \(E\). We use a memory model \(M\) to summarize the corresponding sequence of partial transitions into a latent Markov state.

\[M:^{n} S^{n}.\] (1)

If \(M\) is recurrent, we may alternatively write \(M\) as a single update or batched update respectively

\[M:H H S, M:H^{n} H^{n} S^{n},\] (2)

where \(H\) is the set of recurrent states.

## 3 Background and Related Work

In deep learning, we often wish to train memory models over batches of sequences. For a single sequence, we can use Backpropagation Through Time (BPTT) (Werbos, 1990). If the sequences differ in length, it is not immediately clear how to efficiently combine them into a batch. Williams and Peng (1990) propose Truncated BPTT (T-BPTT), which enables backpropagation over fixed-length sequences that we call _segments_. T-BPTT is the defacto standard for training memory models in both supervised learning and RL (Hausknecht and Stone, 2015; Kapturowski et al., 2019; Hafner et al., 2023; Bauer et al., 2023; Liang et al., 2018; Raffin et al., 2021; Huang et al., 2021; Serrano-Munoz et al., 2023; Lu et al., 2022; Ni et al., 2024).

In _Segment-Based Batching_ (SBB), we split and zero pad episodes so that they can be stacked into a tensor with batch and sequence length dimensions \(B L\). Each row in this tensor is a segment \(\) containing exactly \(L\) transitions. Episodes longer than \(L\) transitions will be split into multiple _fragments_, such that each is at most \(L\) transitions. Fragments shorter than \(L\) transitions will be zero padded from the right, such that they become exactly length \(L\). We call these padded length \(L\) fragments _segments_. We must also store a mask \(m\) denoting which elements are zero-padding and which are data. The segments and masks are stacked along the batch dimension, creating \(B L\) matrices for storage and training (Figure 1). We formally define SBB in Appendix C.

The Shortcomings of SegmentsSBB introduces a number of shortcomings. (1) The zero padding and associated masks must be stored, taking up additional space. (2) The zero padding is fed to the memory model, wasting computation on zeros that are discarded during gradient computation. (3) The zero padding also prevents the use of BatchNorm (Ioffe and Szegedy, 2015) and other normalization methods by shifting the mean and variance of input data. (4) The extra time dimension and padding complicates RL loss functions. (5) Most importantly, when SBB splits episodes into distinct segments, we approximate the true BPTT gradient with the T-BPTT gradient.

Let us demonstrate the effect of SBB on the memory model gradient. Assume we have a loss function \(\) defined over a model parameterized by \(\). We define the true gradient of the loss over an episode of length \(n\) as \(\). In SBB, we split an episode into length \(L\) segments. We approximate the gradient over these segments as \(_{}\)

\[=(,(P_{0},P_{1},  P_{n-1}))}{},_{}=_{j=0}^{  n/L-1}(,(P_{jL}, P_{((j+ 1)L-1,n-1))})}{}.\] (3)

Under SBB, we compute the gradient independently for each segment. The gradient across segment boundaries is therefore always zero. With zero gradient, it is unlikely that temporal dependencies greater than the segment length \(L\) can be learned. In fact, our experiments show that \(_{}\) is often a poor approximation of \(\).

Alternatives to SegmentsWe are not the first to realize the drawbacks of SBB. Hausknecht and Stone (2015) store recurrent states in the replay buffer, while Kapturowski et al. (2019) replay the previous segment to generate a "warm" initial recurrent state for the current segment. These methods improve the return, highlighting issues with zero-initialized states, but do not fix the underlying gradient truncation issue. Real Time Recurrent Learning (RTRL) is an alternative to BPTT, but it has \(O(n^{4})\) time complexity and is thus much slower (Williams and Zipser, 1989). Irie et al. (2024) propose a faster version of RTRL for RL, but the model must be at most one layer deep. Similar to our work, Lu et al. (2024) avoids truncating backpropagation entirely. They find that this results in greater returns, but do not explore _why_ this occurs. Furthermore, their method is restricted to on-policy methods and the S5 memory model. Our method extends Lu et al. (2024) to off-policy algorithms and a large majority of efficient memory models.

### On the Efficiency of Sequence Models

SBB evolved alongside RNNs in RL (Hausknecht and Stone, 2015), and Transformers to a lesser extent. Such models are only tractable when the sequence length \(L\) is small. RNNs rely on the previous recurrent state to compute the following recurrent state, prohibiting parallelism over the time dimension. Thus, RNNs are unable to exploit the parallelism of modern GPUs over the time dimension. Transformers use pairwise attention on the sequence elements, scaling quadratically in space on the length of the sequence.

A recent class of models espouse time-parallel execution while being either linear or subquadratic in space complexity. These models, such as State Space Models, Linear Transformers, Fast Weight Programmers, RetNet, RWKV, Linear Recurrent Units, Gated Impulse Linear Recurrent Networks, and Fast and Forgetful Memory (Gu et al., 2021; Smith et al., 2022; Schlag et al., 2021; Anonymous, 2023; Peng et al., 2023; Orvieto et al., 2023; Martin and Cundy, 2018; Morad et al., 2023b) are sometimes called _Linear Recurrent Models_ because they usually (but not always) employ a Linear Time-Invariant (LTI) recurrent state update, which can be computed in parallel over the time axis (Gu and Dao, 2023).

MonoidsPrior work on efficient sequence modeling primarily updates the recurrent state using linear functions (Schlag et al., 2021; Gu et al., 2021; Smith et al., 2022; Orvieto et al., 2023). However, works like Blelloch (1990); Martin and Cundy (2018); Morad et al. (2023b) show that it is possible to create efficient models using nonlinear recurrent updates. The key to efficiency is not that updates are linear, as stated in Gu and Dao (2023), but rather that the recurrent update obeys the associative property. More formally, the recurrent update must be a _monoid_(Bourbaki, 1965). Hinze (2004) shows that _all_ monoids have time-parallel implementations.

**Definition 3.1**.: A tuple \((H,,e_{I})\) is a monoid if:

\[(a b)=c\] \[(a b) c=a(b c)\] \[(e_{I} a)=(a e_{I})=a\] \[(e_{I} a)=(a e_{I})=a\]

where \(\) for a single input \(a\) is defined as \((\ a)=(e_{I} a)\).

Any monoid operator \(\) can be computed in parallel across the time dimension using a parallel scan (Appendix I) (Hinze, 2004; Dhulipala et al., 2021). Given a sequence of length \(n\), a work-efficient parallel scan known as the Blelloch Scan executes \(O(n)\) calls to \(\) in \(O(n)\) space to produce \(n\) outputs (Blelloch, 1990). With \(p\) parallel processors, the parallel time complexity of the scan is \(O(n/p+ p)\). For large GPUs where \(n=p\), the parallel time complexity becomes \(O( n)\).

## 4 Approach

While powerful, standard monoids are restrictive and cannot represent most Linear Recurrent Models in their entirety. Monoids require that the input, output, and recurrent space be identical. In memory models, we often decouple the input space, from the recurrent state space \(H\), from the Markov state space \(S\) (Equation 2). Consider, for example, a navigation task where the input is an image, the recurrent state \(H\) is a latent map representation, and the Markov state \(S\) is a set of \(x,y\) coordinates of the agent. In search of a more general memory model framework, we extend the monoid into a memory monoid, or _memoroid_ for short.

**Definition 4.1**.: \(((H,,e_{I}),f,g)\) constitute a memoroid if \((H,,e_{I})\) defines a monoid and functions \(f,g\) are:

\[f: H\] Mapping from a partial transition to the right argument of \[\] (7) \[g:H S\] Mapping a recurrent state and a partial transition to a Markov state (8)

Recall that a partial transition consists of the observation and begin flag \(=(o,b)\). The memoroid defines a recurrent memory model (Equation 2) over a sequence of partial transitions to produce recurrent states (\(h_{0},h_{1}, H\)) and then compute Markov states (\(s_{0},s_{1}, S\))

\[h_{0}&h_{1}&h_{2}&\\ s_{0}&s_{1}&s_{2}&=e_{I} f(_{0})&e_{I} f(_{0}) f(_{1})&e_{I}  f(_{0}) f(_{1}) f(_ {2})&\\ g(h_{0},_{0})&g(h_{1},_{1})&g(h_{2},_{2}) &.\] (9)

Given \(n\) inputs, functions \(f\) and \(g\) can each be split into \(n\) concurrent threads. Recall that monoids have \(O( n)\) parallel time and \(O(n)\) space complexity. Consequently, _all memoroids have \(O( n)\) parallel time complexity and \(O(n)\) space complexity_ on the length of the sequence1.

Reformulating Existing Sequence ModelsAs an exercise in the flexibility of our memoroid, we rewrite the Linear Transformer, the Simplified State Space Model, the Linear Recurrent Unit, and Fast and Forgetful Memory (Katharopoulos et al., 2020; Lu et al., 2024; Orvieto et al., 2023; Morad et al., 2023b) as memoroids in Appendix G. We note that our monoid reformulation of Morad et al. (2023b) improves upon the original, exhibiting better numerically stability by replacing exponentiated cumulative sums with a Blelloch scan.

Accelerated Discounted ReturnsMemoroids can model other recurrences as well. For example, we can rewrite the discounted return and Generalized Advantage Estimate (GAE) (Schulman et al., 2016) as a memoroids. Reformulating the discounted return and GAE targets as memoroids enables us to compute them in a GPU-efficient fashion, using a high-level framework like JAX (Bradbury et al., 2018). We find that we can compute such quantities orders of magnitude more quickly than the standard approach. We provide the definitions and proofs of these formulations in Theorem D.1 and Theorem E.1.

Inline Recurrent State ResetsSo far, we have assumed that we operate memoroids over a single episode using the Blelloch Scan. To scan over a batch of variable-length episodes, we could truncate and zero pad sequences such that each is a fixed length (i.e., SBB). However, this introduces the issues explained in Section 3.

Since memoroids are efficient over long sequences, we could consider concatenating individual episodes into one very long sequence, removing the need for padding and truncation. Unfortunately, as the scan crosses episode boundaries, it feeds information from all prior episodes into future episodes, and information from future episodes into preceding episodes.

To resolve this issue, we propose a _resettable monoid transformation_, which prevents information from leaking across episode boundaries. We can apply this transformation to any monoid (or memoroid), to produce a new monoid that respects episode boundaries.

**Theorem 4.2**.: _All monoids \((H,,e_{I})\) can be transformed into a resettable monoid \((G,,g_{I})\) defined as_

\[G=\{(A,b)\ |\ A H,b\{0,1\}\}\] (10) \[g_{I}=(e_{I},0)\] (11) \[(A,b)(A^{},b^{})=((A(1-b^{})+e_{I}  b^{}) A^{},b b^{})\] (12)

_For a single episode, the \(A\) term output by the operator \(\) is equivalent to the output of \(\). Over multiple contiguous episodes, \(\) prevents information flow across episode boundaries._

Proof.: See Appendix F.

By transforming the monoid \((H,,e_{I})\) within a memoroid, we no longer require separate time and batch dimensions during training. _Now, memoroids can process a long sequence comprised of many distinct episodes_. Unlike Blelloch (1990); Lu et al. (2024) which provide a reset operator for a specific model, our resettable transformation works for _any_ monoid.

### Tape-Based Batching

Training over the concatenation of episodes would be intractable using Transformers or RNNs due to poor sequence length scaling, while Linear Recurrent Models leak information between episodes. By combining the efficiency of memoroids with our resettable transform, we resolve these issues, enabling us to fold the batch and time dimensions into a single dimension. We call this approach _Tape-Based Batching_ (TBB), which consists of two operations: _insertion_ and _sampling_. We provide operations for both on-policy and off-policy RL algorithms. Furthermore, we design TBB in a manner that greatly simplifies the implementation of recurrent loss functions.

InsertionIn RL, we must store the training data (transitions) that we collect during a rollout. TBB stores them following Algorithm 1. We maintain ordered lists of transitions \(\), and begin indices \(\), corresponding to episode boundaries in \(\). Upon reaching the maximum capacity of \(\), we discard old transitions by popping the episode begin index from the left of \(\), and discarding the resulting episode in \(\). This is guaranteed to discard the oldest episode in \(\).

This method works both for rollouts that contain complete episodes (\(d_{n-1}=1\)), and those that contain incomplete episodes (\(d_{n-1} 1\)), where a rollout might stop before finishing an episode. When combining incomplete episodes with multiple rollout workers, we can experience race conditions. In this scenario, it is easiest to keep one \(,\) per worker to prevent race conditions.

SamplingOnce we have constructed \(,\), we are ready to train a policy or compute returns. We sample transitions from \(,\) following Algorithm 2. If we are training on-policy, we can simply train on \(\). If we are training off-policy, we randomly sample a training batch \(\) from our dataset by slicing \(\) using randomly-sampled sequential pairs of episode indices from \(\). One could extend our sampling approach to implement Prioritized Experience Replay (Schaul et al., 2015) by assigning each episode or index in \(\) a priority.

Simplified Loss FunctionsWith TBB, we utilize unmodified, non-recurrent loss functions to train recurrent policies, reducing the implementation complexity of recurrent RL algorithms. Unlike SBB, there is no need to mask outputs or handle additional time dimensions like in SBB. With TBB, the only difference between a recurrent and nonrecurrent update is precomputing the Markov states \(s,s^{}\) before calling the loss function. We demonstrate this by writing the TBB Q learning update in Algorithm 3, highlighting departures from the standard, non-recurrent Q learning update in red. For posterity, we define the standard SBB Q learning update in Algorithm 4. Note that the SBB update equation has an additional time dimension \(k\) and requires a padding mask \(m_{i,j}\).

Figure 2: A visualization of sampling in TBB, with a batch size of \(B=4\). Transitions from rollouts are stored in-order in \(\), with each color denoting a separate episodes. Associated episode begin indices are stored in \(\). We sample a train batch by randomly selecting from \(\). For example, we might sample \(4\) from \(\), corresponding to \(E_{1}\) in red. Next, we sample \(7\) from \(\), corresponding to \(E_{2}\) in red. We concatenate \(=(E_{1},E_{2})\) and return the result as a train batch.

``` Input: List of transitions \(\), list of indices \(\), buffer size \(D\) Output: List of transitions \(\), list of indices \(\) \((P_{0},P_{1},,P_{n-1})\)\(\) Collect rollout from env ifon_policy then \(\) \((b_{0}, b_{n-1})\)\(\) Indices of new episodes else while\((+())>D\)do \([1:]\)\(\) Buffer full, pop oldest index \([I:]\)\(\) Pop transitions for the oldest episode endwhile \((,()+(b_{0}, b_{n-1}))\)\(\) Update replay buffer indices \((,)\)\(\) Add new transitions to buffer endif ```

**Algorithm 2** Sampling transitions using TBB

``` Input: List of transitions \(\), list of indices \(\), batch size \(B\) Output: Batch of transitions \(\) \(\) Empty list \(()\) while\(()<B\)do\(i(0,()-1)\)\(\) Randomly sample an index in \(\) \((,D[[i]:[i+1]])\)\(\) Append episode to batch endwhile \(=[:B]\)\(\) Make batch exactly \(B\) transitions ```

**Algorithm 3** TBB deep Q update

``` Input: params \(\), target params \(\), Q function \(Q\), sequence model \(M\), train batch \(\), discount \(\), update rate \(\) Output: params \(\), \(\) \(\) Estimate Markov state \((s_{1},s_{2}, s_{B}) M_{}(P_{1},,P_{B})\)\(\) Estimate next Markov state \(_{j}=r_{j}+_{a A} Q_{}(s^{}_{j},a), [j]\)\(\) Compute target \(_{} Q_{}(s_{j},a_{j})-_{j}, [j]\)\(\) Compute loss and update parameters \(+(1-)\)\(\) Update target network params ```

**Algorithm 4** SBB deep Q update

``` Input: params \(\), target params \(\), Q function \(Q\), sequence model \(M\), train batch \(\), discount \(\), update rate \(\) Output: params \(,\) \(\) \(+(1-)\)\(\) Update target network parameters ```

**Algorithm 5** TBB deep Q update

## 5 Experiments and Discussion

We begin our experiments by investigating the shortcomings of SBB, specifically the theoretical issues stemming from truncated BPTT. We then compare TBB to SBB across a variety of tasks and models. Finally, we examine the wall-clock efficiency of memoroids.

Our experiments utilize tasks from the POPGym benchmark (Morad et al., 2023), and all TBB to SBB comparisons use identical hyperparameters and random seeds. We validate our findings across Simplified State Space Models (S5), Linear Recurrent Units (LRU), Fast and Forgetful Memory (FFM), and the Linear Transformer (LinAttn) memoroids. We train our policies using Double DuelingDQN (Van Hasselt et al., 2016; Wang et al., 2016). See Appendix H for architecture and training details.

What are the Consequences of Truncating BPTT?In Section 3, we discussed how the estimated (truncated) gradient used in SBB differs from the true gradient. We aim to determine whether the estimated gradient used in SBB is a sufficient approximation of the true gradient. We note that if all episodes are a fixed length, and we set \(L\) to be this length, both SBB and TBB produce identical results - although this is rare in practice.

We utilize the _Reward Memory Length_ (RML) and _Value Memory Length_ (VML) metrics from Ni et al. (2024). The RML refers to the maximum temporal dependency \(j\) required to predict the expected reward, while the VML determines at which point \(k\) prior observations stop affecting the Q value. In other words, the environment defines the RML while the memory model defines the VML. We hope to find that \(=\); that the Q value only depends on the necessary history.

\[[R(s,a,s^{}) o_{0:n}] =[R(s,a,s^{}) o_{j:n}] \] (13) \[Q(M(o_{0:n}),a) =Q(M(o_{k:n}),a) \] (14)

We examine the VML for the Repeat Previous task with a known RML of ten timesteps. We measure the VML as the impact each observation has on the terminal Q value of an episode (i.e., \(Q(s_{n},a_{n})=R(s_{n},a_{n},s_{n+1})\)). Any observations older than ten timesteps are not necessary to predict the reward, and given a relative-time policy, should have little to no impact on the terminal Q value. Recall that we can write a memory model as \(s_{n}=M(o_{0}, o_{n})\). We explicitly compute

\[|,a_{n})}{ o_{i}}|=|,a_{n})}{ s_{n}}}{ o_{i} }|,\] (15)

and plot the results for FFM, S5, LinAttn, and LRU models in Figure 3. We examine the VML and RML of other model and task combinations in Appendix B.

Surprisingly, the VML differs significantly from the RML. The RML is fixed at ten, while the VML appears to span the entire episode. This means that the recurrent models are unable to ignore uninformative prior observations, suggesting that _truncated BPTT degrades recurrent value estimators_. Learned recurrent Q functions do not generalize well over time, although we find that policies trained with TBB tend to generalize better.

In the case of FFM, roughly 90% of the Q value is produced outside the segment boundaries, where truncated BPTT cannot reach. This means that these unnecessary prior observations have a significant impact on our Q values, yet we are unable to forget or ignore such observations. Our findings suggest that SBB could be a major contributor to the increased difficulty and reduced sample efficiency of recurrent RL, as we demonstrate in the next experiment.

Is TBB More Sample Efficient?For our second experiment, we measure the difference in sample efficiency between TBB and SBB. There are two reasons that TBB could improve upon SBB sample

Figure 3: We demonstrate that SBB can hurt Q learning through truncated BPTT. We examine the Repeat Previous task, with \(=10\), comparing SBB (left) to TBB (right). For SBB, we set \(L==10\) to capture all necessary information. After training, we plot the cumulative partial derivative with respect to the observations on the y-axis. _This partial derivative determines the VML â€“ how much each prior observation contributes to the Q value._ We draw a vertical red line at \(L==10\). We see that across models, a majority of the Q value is not learnable when using SBB. Even when we set \(L=\) using TBB, we see that the VML still spans far beyond the RML. This surprising finding shows that _truncated BPTT degrades recurrent value estimators._

efficiency: (1) As previously discovered, the truncated gradient used by SBB is often a poor estimate of the true gradient (2) SBB decreases the effective batch size through zero padding. We note that the cost of zero padding in SBB is equivalent to the cost of real data - it takes up equivalent space in the replay buffer and takes just as much compute to process as real data. We report some combinations of model and task in Figure 4 and present the full results in Appendix A.

We find that TBB produces a noticeable improvement in sample efficiency over SBB, across nearly all configurations of memory model and environment. Even for large segments lengths \(L=100\), we find a significant gap between SBB and TBB. SBB must make an inherent tradeoff - it can use long segments to improve gradient estimates at the cost of smaller effective batch sizes, or shorter segments to improve the effective batch size at the expense of a worse gradient estimate. TBB does not need to make this tradeoff. In our experiments, SBB with larger \(L\) always outperforms shorter \(L\), suggesting that the gradient estimation error is a larger contributor to SBB's lackluster performance than reduced effective batch sizes.

Wall-Clock EfficiencyIn Figure 5, we investigate the wall-clock efficiency of memoroids. We find that memoroids compute the discounted return and GAE roughly three orders of magnitude faster than a standard implementation. Next, we compare the wall clock time TBB and SBB take to train a policy from start to finish. For SBB, the parallel time complexity is \(O( L)\) while TBB has \(O( B)\) complexity where \(B>L\), but in practice there is no perceivable difference in wall-clock time. One possible reason for this discrepancy is that SBB applies expensive split-and-pad operations to the trajectories, while TBB does not. Each fragment contains a varying number of transitions, which corresponds to a varying amount of padding we need to add. Variable-size operations are generally slow and difficult to batch efficiently.

Limitations and Future WorkAccording to our sensitivity analysis, old observations unexpectedly impacted the Q value across models and tasks. Moving forward, we suggest testing newly-designed memory models to see whether \(=\), to determine whether such models truly generalize over time.

In our experiments, we focused on long-term memory tasks from the POPGym benchmark, each of which tests a specific aspect of long-term memory. We did not experiment on environments like Atari, primarily because it is unclear to what extent Atari tasks require long-term memory.

Figure 4: We compare TBB (ours) to SBB across POPGym tasks and memory models, reporting the mean and 95% bootstrapped confidence interval of the evaluation return over ten seeds. We find that TBB significantly improves sample efficiency. See Appendix A for more experiments.

Figure 5: (Left) We compare how long it takes to compute the discounted return using our memoroid, compared to the standard way of iterating through a batch. Computing the discounted return is orders of magnitude faster when using our memoroid implementation. (Right) we compare the total time to train a policy on Repeat First. For both experiments, we evaluate ten random seeds on a RTX 2080Ti GPU.

Although memoroids scale well to long sequences, TBB still pays an increased \( B\) time cost compared with SBB's \( L\) cost. There was no perceptible difference in our experiments, but very long sequences such as those used for in-context RL could incur more noticeable training costs. TBB does not strictly require memoroids, but would likely be intractable for RNN or Transformer-based memory models.

## 6 Conclusion

We introduced memoroids as a unifying framework for efficient sequence modeling. We found that memoroids can represent a large number of efficient recurrent models, as well as the discounted return and the advantage. Using our resettable transformation, we extended our approach to encompass batching across variable length sequences. Given the efficiency of memoroids over long sequences, we questioned whether the standard split-and-pad approach to POMDPs was still necessary. We found that said approach causes issues, with shorter segment lengths hampering sample efficiency and ultimately converging to lower returns. We proposed a simple change to batching methodology, that when combined with memoroids, improves sample efficiency at a negligible cost.