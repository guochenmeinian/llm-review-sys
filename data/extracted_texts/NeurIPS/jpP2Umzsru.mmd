# Modern Hopfield Networks meet Encoded Neural Representations - Addressing Practical Considerations

Satyananda Kashyap

IBM Research - Almaden

satyananda.kashyap@ibm.com

&Niharika S. D'Souza

IBM Research - Almaden

&Luyao Shi

IBM Research - Almaden

&Ken C. L. Wong

IBM Research - Almaden&Hongzhi Wang

IBM Research - Almaden&Tanveer Syeda-Mahmood

IBM Research - Almaden

###### Abstract

Content-addressable memories such as Modern Hopfield Networks (MHN) have been studied as mathematical models of auto-association and storage/retrieval in the human declarative memory, yet their practical use for large-scale content storage faces challenges. Chief among them is the occurrence of meta-stable states, particularly when handling large amounts of high dimensional content. This paper introduces Hopfield Encoding Networks (HEN), a framework that integrates encoded neural representations into MHNs to improve pattern separability and reduce meta-stable states. We show that HEN can also be used for retrieval in the context of hetero association of images with natural language queries, thus removing the limitation of requiring access to partial content in the same domain. Experimental results demonstrate substantial reduction in meta-stable states and increased storage capacity while still enabling perfect recall of a significantly larger number of inputs advancing the practical utility of associative memory networks for real-world tasks.

## 1 Introduction

The hippocampal system in the brain plays a central role in long-term memory, responsible for storing and recalling facts and events. Its structure, particularly the auto-associative networks in the CA3 region, enables efficient memory retrieval based on partial input, a process that has inspired several mathematical models of memory networks (Almeida et al., 2007; Whittington et al., 2020; Gillett et al., 2020; Burns et al., 2022). Classical Hopfield network models (Hopfield, 1982; Amari, 1972) are a type of associative memory architecture that stores memories as fixed point attractor states in an energy landscape, using Hebbian learning to recall patterns form partial input cues using a recurrent network. The Modern Hopfield network (MHN) introduced a continuous relaxation of the original method and in theory enable exponential storage capacity growth with respect to the number of neurons (Krotov and Hopfield, 2016; Demircigil et al., 2017; Ramsauer et al., 2020). MHNs have been applied to tasks such as immune repertoire classification (Vaswani et al., 2017; Widrich et al., 2020) and graph anomaly detection (Hoover et al., 2023).

Extending these ideas, heteroassociative memories provide a framework for hippocampal memory storage and retrieval, enabling pattern recall from different input modalities. Hetero-association has also been interpreted in various ways, including for modeling sequence associations (Chaudhry et al., 2023; Karuvally et al., 2023; Gutfreund and Mezard, 1988), where the process begins with a given pattern in a sequence, adjusting the energy weights to transition from one pattern to the next. Similarly, (Tyulmankov et al., 2021) introduces a key-value memory model where sequential patterns are used to predict the next item in the sequence. Further, (Millidge et al., 2022) demonstratedhetero-association by reconstructing missing portions of an image based on other parts of the same image. Most research on content addressable memories focus on dense associative memory theory, and use simplistic scenarios or simulations for validation (Kang & Toyoizumi, 2023; Iatropoulos et al., 2022). Translating to practical large-scale content storage systems has been difficult, as associative memories like MHNs tend to enter spurious meta-stable states when handling large volumes of content (Martins et al., 2023). The metastable states are closely related to the separability of the input patterns (Ramsauer et al., 2020). Additionally, MHNs rely on partial content for recall, which limits their utility in cases where such cues are unavailable.

In this paper we address the issue of meta-stable states in Modern Hopfield Networks (MHN) by improving the separability of input patterns through neural encoding representations. Our approach termed HEN (modern Hopfield networks with Encoded Neural representations or in short Hopfield Encoding Networks) encodes inputs into a latent representational space using a pre-trained neural encoder-decoder model before storage, and decodes them upon recall. While prior work has explored encoding content for associative memory (Kang & Toyoizumi, 2023), our method uniquely combines pre-encoding and post-decoding to specifically tackle metastable states in MHNs. Additionally, HEN supports hetero-association, allowing retrieval through free text queries, thus eliminating the need for users to provide partial content for recall. Comprehensive ablation experiments demonstrate that HEN significantly increases storage capacity, reduces metastable states across modalities, and enables perfect recall of a significantly larger number of stored elements using natural language input.

Our approach begins by surveying different representations of energy-based formulations, demonstrating how they form a unifying framework that connects support vector-based kernel memories (Iatropoulos et al., 2022), Modern Hopfield networks (Ramsauer et al., 2020; Krotov & Hopfield, 2020), and the transformer attention mechanism (Vaswani et al., 2017). By establishing these theoretical connections, we shed light on how similar representations emerge in distinct neural models.

Despite the theoretical promise of MHNs, we identify their practical limitations, notably the emergence of spurious states due to weak input pattern separability. **To address this issue, we propose HEN, which enhance pattern separation by encoding input representations into a latent space before storage.** This method delays the onset of metastability and significantly increases storage capacity, leading to improved recall stability compared to alternative representation learning strategies.

## 2 Modern Hopfield Networks: A Representational Perspective

In this section, we review the basic framework of Modern Hopfield Networks (MHNs), focusing on their representational properties, equivalence with other models, and inherent limitations. This discussion motivates our enhancements to MHNs using neural encoding strategies aimed at improving pattern separability and storage capacity.

The MHN framework provides a framework for dense associative memory using continuous dynamics. It can be described in terms of its energy function and the resulting attractor dynamics. Let \(N\) be the number of memories and \(K\) be the data dimensionality (number of neurons in the MHN). Defining a similarity metric between the memories \(\{_{n}^{K 1}\}_{n=1}^{N}\) or matrix \(^{N K}\) and the state vector \(^{K 1}\) the generalized objective function is expressed as the following energy minimization:

\[^{*}=*{arg\,min}_{}E(, )=*{arg\,min}_{}E_{1}(, ;)+E_{2}()\] (1) \[E_{1}(,;)=F_{}(f_{}(\{_{n}\},))=F_{}\{_{k=1}^{K}_{n}(k)(k) \}=-_{n=1}^{N}_{k=1}^ {K}(k)(k)\] \[E_{2}()=^{T}+\]

The function \(f_{}(\{_{n}\},;)\) is a measure of similarity between the state vector and each memory in the bank. A common choice for this similarity metric is the dot product in \(K\) dimensional vector space, which is both efficient and widely adapted in practical implementations of MHNs (Ramsauer et al., 2020; Krotov & Hopfield, 2020). The energy function \(F_{}()\), defined as the log-sum-exponential (LSE), approximates the \(*{arg\,max}\) function to select the most relevant memory. The inverse temperature parameter \(\) controls the sharpness of this selection. The energy minimization equation treats memories as attractors of a dynamical system (Krotov and Hopfield, 2016), recoverable from partial cues through an iterative optimization process. The state vector recurrence is given by:

\[^{(t+1)}=(^{T} ^{(t)})=_{n}(_{n}^{T}^{(t)})} {_{n}(_{n}^{T}^{(t)})}\] (2)

These updates progressively reduce the energy of the system monotonically (Millidge et al., 2022) and are guaranteed to converge under specific conditions (Ramsauer et al., 2020). Starting with an initial state \(^{(0)}\), which could be a partial or noisy memory cue, the iterations \(\{^{(t)}\}\) aims to reconstruct a full pattern \(^{(T_{f})}\) that corresponds to one of the stored memories \(\{_{n}\}\). However, in practice, the process may lead to local minima or saddle points, resulting in meta-stable configurations.

### Equivalence with Kernel Memory Networks (KMN)

The formulation of the MHNs can be viewed as a special case of Kernel Memory Networks (KMMs) (Iatropoulos et al., 2022), where each neuron performs kernel-based classification or regression. The MHN update rule is analogous to that in KMMs and can be computed in closed form as a recurrence. Let \(K(,^{(t)})\) denote the (symmetric positive definite) kernel function that defines pairwise similarities, and \(^{}\) be the Moore-Penrose pseudoinverse for \((,)\). For continuous valued memories, the radial translation-invariant exponential kernel (infinite dimensional basis) with a fixed spatial scale \(r\) and temperature \(\) is proposed. This parameterization allows for an analysis of memory capacity and storage limits by interpreting the MHN optimization as a feature transformation operating in a Reproducing Kernel Hilbert Space (RKHS). The kernel and state update are as follows:

\[K_{(,r)}(,)=- ||-||_{2}^{}^{(t+1)}=^{}K(, ^{(t)})\] (3)

While KMMs provide strong theoretical storage guarantees (Iatropoulos et al., 2022), their real-world performance is known to be sensitive to data distributions and parameter choices Wu et al. (2024).

### Equivalence with Transformers

Alternatively, the update rule in Eq. (2) has been shown (Ramsauer et al., 2020) to be equivalent to the key-query _self_-attentional framework used in transformer models (Vaswani et al., 2017).

\[=}}^{T }^{T}=^{T}}}^{T}\] (4)

with the keys being related to the memories as \(_{K}=\), and queries/values being related to the intermediate state vectors \(^{t+1}=^{T}\), \(_{Q}^{t}=^{T}\), \(_{V}^{t}=^{T}\) and the dispersion parameter relating to the temperature \(}}=\). The matrices \(_{Q},_{K},_{V}:^{K}^{D}\) are linear transformations associated with the query-key-value triplet, which when substituted with the identity matrix \(_{K}\) gives us the form in Eq. (2)

Under the kernel memory networks framework, the MHN equations (1-2,4) do not involve a symmetric positive definite kernel, as the energy objective is inherently non-convex (Iatropoulos et al., 2022; Wright and Gonzalez, 2021). However, the system still permits a bilinear reproducing form for the kernel \(K(,)\), where input patterns are mapped into higher-dimensional feature spaces. This bilinear kernel is equivalent to the transformer's key-query attention mechanism, where inputs are projected into higher dimensional feature spaces. The transformer kernel can be written as: \(K(,)=}}(_{Q} )^{T}(_{K})\). Overall, this highlights the connection between the MHNs and transformer attention mechanisms, showing that both rely on projecting input representations into a shared space for similarity-based comparison. Further details on the KMN and transformer equivalence are provided in Appendix Section 1.

## 3 HEN: Modern Hopfield Networks with Encoded Neural Representations

Although the results of modern Hopfield networks and its various equivalent forms imply that the formulation has theoretically exponential capacity to store memories, _our experimental results_demonstrate that the system of updates can be brittle in practice and highly sensitive to real-world data distributions across each of these data representations_.

Specifically, MHN often struggle with spurious attractor basins (Bruck & Roychowdhury, 1990; Ramsauer et al., 2020; Barra et al., 2018), which manifest as erroneous memory patterns due to overlapping or similar inputs. This issue is particularly evident in large datasets, where poor pattern separability results in meta-stable states, limiting retrieval accuracy and scalability. The key insight from the unified representations discussed in Sections 2.1 and 2.2 is that improving the separability of input memories significantly enhances retrieval accuracy. This can be achieved by mapping input memories \(\) and partial queries \(^{(0)}\) from their original \(K\)-dimensional space (where memories may overlap or be less distinct) into a higher dimensional embedding space, the stored patterns become better separable. The increased separability directly reduces the occurrence of spurious attractors and leads to more reliable memory retrieval.

Going one step further, a key idea we put forward here is to see if we can bolster the separability of the input patterns before they enter the Modern Hopfield network in order to reduce the spurious attractor states problem via large pre-trained encoded-decoder models and their latent space representations (i.e. generalizing the linear transforms in Eq. 4). Following observations of the phenomenon of input encoding in the dentate gyrus (DG) region of the trisynaptic circuit prior to memorization (Bernier et al., 2017), we propose to store these latent-space neural encodings, i.e. transformation computed on the memories \(}\) and partial query \(}^{(0)}\), in the MHN memory bank using the encoder transformation \(_{}()\). Recovery of such patterns can be performed by unrolling the recurrence relation in the latent-space (i.e. Eq.(2)) followed by applying the associated decoder transformation \(_{}()\) to the latent space representation \(}^{(T_{f})}\). Mathematically, this procedure can be expressed as follows:

\[}=_{}()}^{(0) }=_{}(^{(0)})\] (5)

\[}^{(t+1)}=}(}^{T }}^{(t)})=_{n}(_{n}^{T }}^{(t)})}{_{n}(_{n}^{T}} ^{(t)})}\] (6)

\[^{(T_{f})}=_{}(}^{(T_{f})})\] (7)

_Hypothesis 1: The spurious attractors can be reduced by encoding inputs prior to storing them in the Modern Hopfield network and decoding them after recall due to increased separability in latent space_

Our proposed HEN combines an auto-encoder with the Modern Hopfield network (MHN). Specifically, the encodings are generated by a pre-trained auto-encoder. The raw content is then recovered through chaining MHN with the decoder portion of the auto-encoder. We hypothesize that the encodings produced by an auto-encoder contain discriminative information that is not only compact but can improve the separability in the energy landscape to significantly delay the meta-stable states even with increased content. That is, by leveraging a well-trained auto-encoder for feature extraction, we posit that the most significant and discernible features between images can be easily identified, leading to less spurious patterns emerging during recall and allowing more content to be stored, thereby increasing storage capacity.

### Experimental Evaluation and Results:

We provide practical insights drawn from experiments on the MS-COCO dataset, which contains 110,000 images (Lin et al., 2015). This dataset offers a more realistic distribution of high-dimensional, real-world data compared to smaller, curated datasets like MNIST. Its unique associative captions also make it ideal for illustrating hetero-associations within our proposed framework.

To evaluate this hypothesis, we conducted studies that examined the effectiveness of various pre-trained encoder-decoder architectures to produce encoded representations that can lead to successful recall of dense associative memories by comparing them against the native data representations and KMNs. We also analyzed the parameter choices for the energy formulation of MHNs in affecting the identity of the recalled memory items when using their encoded representations.

Specifically, we evaluated various pre-trained encoder-decoder architectures known for their state-of-the-art performance in deep learning-based image encoding and decoding ranging from vanilla-transformer based to variational auto-encoder based models. In particular, we utilized the Discrete Variational Autoencoder (D-VAE) from (Ramesh et al., 2021) and other architectures from (Rombach et al., 2021) and explored two transformer variants from the diffusion library: one trained with codebook-based (Vector Quantized - VQ) criteria and the other using Kullback-Leibler (KL) divergence-based criteria. Our empirical analysis revealed that Vector Quantized VAE (VQ-VAE) methods outperformed others in our setup. Consequently, we selected D-VAE and variants of VQ from (Rombach et al., 2021) for further analysis. To maintain consistency in representation, we downsampled all the images to a resolution of \(28 28 3\) to match the number of features that the encoded representations produced.

**Evaluation Metrics**: This study tested the image-based dense MHNs (Eq. 1) and KMNs with the exponential kernel against pre-trained Discrete VAE (Ramesh et al., 2021) and VQ-VAEs encoding equipped Hopfield encoding network (Rombach et al., 2021) (Eq. 7). The test was conducted on a memory bank (\(\{_{n}^{1 K}\}\)) and query size (\(N\)) of 6000 images from the MS-COCO dataset. To examine the effect of different choices of \(f_{}(,)\) in Eq. (1), both dot product and \(_{2}\) based similarity measures were utilized. The performance of different encoder-decoder architectures was evaluated (see Fig. 2) by varying the dimensionality \(K\). The Mean Squared Error (\(MSE\)) and Structural Similarity Index (\(1-SSIM\)) metrics were used to compute the similarities between the encoder reconstructions stored in the \(\{_{n}\}\) memory bank and the reconstructed ones.

Note that as this evaluation was conducted to assess the performance on metastable states, _the focus was on recovering the correct identity rather than the quality of reconstruction_. Hence, a \(MSE=1-SSIM=0\) indicated that the dense associative memory could retrieve the full encoded representation of the image from which the pre-trained decoder could reconstruct the image. 1

**Results:** Figure 2 shows the result of our analysis using six different encoding methods including raw image store and two similarity types (dot product and \(_{2}\)). The encoded representations uniformly perform well above a certain \(\) value. In comparison, we note that the baseline image-based MHN (dot-image & L2-image) persists in meta-stable states irrespective of the \(\) value or the similarity metric. Additionally, we also assessed metastable states by recording the relative reduction in the rank of the update matrix with the collapse of the pattern recovery process. The result is available in the Appendix, Section 2 and shows that for a judiciously chosen value of \(\), the iterates of HEN stabilize to provide near perfect retrieval, consistent with the trends observed in Figure 2.

**KMRNs Sensitivity to Real-World Conditions**: KMNs have strong theoretical guarantees, but in our experiments, they faced significant challenges with real-world, large-scale image retrieval tasks. Despite an extensive parameter sweep over hyperparameters like \(r\) (spatial scale) and \(\) (inverse temperature), KMNs were highly sensitive to these settings and consistently underperformed. For

Figure 1: Progression of the **(Top Row)** Modern Hopfield Network (MHN) run on image inputs and the **(Bottom Row)** HEN at different intermediate steps. The sequence from left to right is as follows: the original image, the query image with half of it occluded, an intermediate update at iteration 11, and the final reconstruction at iteration 150. This uses an \(_{2}\) similarity and discrete Variational Autoencoder (D-VAE) encoder for the HEN. We set \(=150\) in both experiments

example, with a memory bank of 6000 images, KMNs yielded an MSE of 0.2427 and 1-SSIM of 0.972, indicating poor retrieval accuracy. While KMNs excel in controlled environments, their performance degrades in high-dimensional, non-linearly separable data, limiting their practical use. This is suspected to be a direct consequence of the restrictive assumptions made on the underlying data-distributions in KMNs (Wu et al., 2024). In contrast, our HEN shows more robust performance, mitigating meta-stable states and improving retrieval accuracy.

**Quality of recovery**: Fig. 1 shows the result of using D-VAE encoding for perfect memory recall for the same set of images for which raw image storage in the Hopfield network failed. While the reconstruction quality is not as clear as the original, the identity of the recovered images is preserved one-to-one. In comparison, the recall using the raw images for the same dataset using the Modern Hopfield network shows the metastable states.

**Scale-out Performance and Ablation Study on Encoder-Decoder Pairs**: We evaluated HEN's scalability by testing it's retrieval performance as the number of stored images increased in the memory bank. Table 1 presents the performance metrics across different encoder-decoder approaches as the memory bank scaled from 6,000 to 15,000 images. Our results show that all the encoder-based approaches robustly recovered the image representations without a noticeable drop in the reconstruction quality, supporting HEN's stability and scalability in retrieval performance at this scale. Additionally, our ablation studies included five pre-trained encoder-decoder architectures - dVAE (Ramesh et al., 2021) and VQ-F8, VQ-F16, KL-F8, KL-F16 from (Rombach et al., 2021) to assess whether HEN's retrieval stability holds across varied configurations. Section 2 of the Appendix contain a more detailed breakdown of the recovery performance as a function of encoder-decoder methods, and the similarity metric for different values of \(\). _By contrast, KMNs consistently failed to scale with the increase in images, showing degraded performance as the number of images increased._

**Probing the separability of HEN encodings:** To study separability of various encoding strategies, we examine the strength of association patterns in the HEN memory bank, i.e. the latent-space vectors. Extending the notation in Section 3, let \(}_{i}^{(0)}^{K 1}=_{}( _{i}^{(0)})\) denote the encoded query for example \(i\) in the dataset. This encoding is generated by occluding a portion of the image fed

   NUM IMAGES & vq8 & vqf16 & Image & D-VAE \\ 
6000 & 0.021, 0.000 & 0.019, 0.004 & 0.836, 0.064 & **0.000, 0.000** \\ 
8000 & 0.019, 0.000 & 0.046, 0.004 & 0.835, 0.067 & **0.000, 0.000** \\ 
10000 & 0.019, 0.000 & 0.047, 0.004 & 0.835, 0.064 & **0.000, 0.000** \\ 
15000 & 0.019, 0.000 & 0.048, 0.004 & 0.836, 0.066 & **0.000, 0.000** \\   

Table 1: All neural encoders can recover images without quality loss as more data is stored in \(\{_{n}\}\).

Figure 2: Memory recall performance of various encoder methods and the image-based Modern Hopfield network, each color-coded differently. The **(Left)** figure plots the MSE while the **(Right)** depicts the 1-SSIM as a function of \(\). The encoder-based HEN methods outperform the raw image-based method over a very large range of choices of hyperparameters. Dot or L2 in the legend denote the dot product or \(_{2}\), followed by the type of representation used including Original input-Image, dVAE-Discrete Variational Auto Encoder, Kullback-Leibler (KL)-based variants- KLf8, KLf16, and Vector quantized VAE methods-VQf8, VQf16 per the convention in (Rombach et al., 2021)

through the encoder (or just the occluded image for the raw image MHN). We expect that the major contributor to poor recovery performance is the lack of separation between the attractor basins in in Eq. (1), due to which the dynamics of state evolution \(}^{(t)}_{i}\) in Eq. (2) are meta-stable configurations. To quantify this separation, we compute the cosine similarity between pairs of query and memory vectors, i.e. \(c_{ij}=(}^{(0)}_{i},_{j})=_{j}^{T}}^{(0)}_{i}/||}^{(0)}_{i}||_{2}||_{j}||_{2}\). If the patterns are well separated, each query \(}_{i}\) in the encoding space (or in the native space for raw images) is close to its own memory \(_{i}\) but far apart from others \(_{j}, j i\). We test this in Fig. 3 by plotting the distribution of values as histograms for \(c_{ij} j i\) colored in blue and for \(c_{ii}\) colored in red, for different \(_{}()\)s in HEN.

This separation is poor for the raw image case, with the two histograms having a high overlap in values. This overlap substantially reduces across the neural encoder-based models, with the Vector Quantized variants providing improved separability and a relatively higher magnitude of self-similarity values compared to their KL counterparts. Finally, we notice that the D-VAE encoder, besides providing separable encodings, also results in the tightest fit around the mean for the self and cross-similarity value distributions. This is likely why the D-VAE provided the best performance (Fig. 2 and Table 1).

### Natural language-based Hetero-associations

We now extend the HEN framework in a hetero-associative setting as a practical application. Specifically, we explore the use of cross-stimuli coming from language and vision, as language-based queries are often used as cues for recall, for example, in practical storage/retrieval contexts. Cross-associative features have been previously demonstrated for the classical Hopfield networks model, albeit under the limited setting of carefully curated binary patterns (Shriwas et al., 2019).

_Hypothesis 2: Hopfield encoding networks serve as content-addressable memories even with cross-stimuli associations as long as they are unique associations._

We conducted three separate experiments. First, we use a native textual embedding for the language cue and associate it with the content to be stored as a practical way of enabling recall. Next, we explore the paradigm of stimuli type-conversion to render the language cue into a convenient image form to allow for content-based access. Finally, we show that if the uniqueness of association is lost, spurious memory states could again emerge even if the inputs are encoded.

Fig. 4 illustrates the overall methodology for the Hopfield Encoding Network (HEN) under hetero-associations. Here, the memory bank vectors are formed by concatenating image and text embeddings

Figure 3: Illustrating separability in various embeddings used in Subsection 3. We plot the histogram of the distribution of cosine similarity values between the queries and memories, i.e. \((}^{(0)}_{i},_{j})\). Distributions colored indicate self-similarity (i.e. \(i=j\)) across paired examples, while distributions colored in blue indicate cross similarities (i.e. \(i j\)). We generate these distributions for (a) Raw Images in the Black Box, Diffusion models trained (b) on KL Divergence in the Orange Box, (c) trained using Vector Quantization in the Brown Box, and (d) Discrete-VAE (D-VAE) in the Red Box.

   NUM IMAGES &  &  &  &  \\
1-SSIM, MSE & & & & \\ 
6000-CLIP & 0.016, 0.000 & 0.024, 0.000 & 0.681, 0.118 & **0.000, 0.000** \\ 
6000 & 0.016, 0.000 & 0.024, 0.000 & 0.952, 0.214 & **0.000, 0.000** \\ 
8000 & 0.016, 0.000 & 0.023, 0.000 & 0.952, 0.215 & **0.000, 0.000** \\ 
10000 & 0.015, 0.000 & 0.024, 0.000 & 0.952, 0.215 & **0.000, 0.000** \\ 
15000 & 0.015, 0.000 & 0.024, 0.000 & 0.952, 0.215 & **0.000, 0.000** \\   

Table 2: Performance of encoded cross-modal HEN compared to image-based MHNs as the memory bank \(\{_{n}\}\) increases. The first row shows the CLIP-encoded cross-modal representations, while the rest present pixelized text-encoded representations for increasing dataset sizes.

Figure 4: HEN architecture for Natural language based hetero-associations. **Orange Box:** Paired text and image inputs are fed to text (\(^{T}}()\)) and image (\(^{I}}()\)) encoders respectively to generate the memories of the HEN memory bank. **Grey Box:** At query time, a partial query \(}^{(0)}\) is generated by feeding the query text into \(^{T}}()\). After convergence, the image encoding is extracted from \(}^{(T_{J})}\), and decoded through the Image decoder (\(^{I}}()\)) to retrieve the corresponding image. Using full image representations instead of image encodings implies \(^{I}}()=^{I}}()=_{K}\), the identity transformation. In the experiment where the text captions are pixelized as input, \(^{T}}()=^{I}}()\).

Figure 5: Step-by-step progression of a cross-modal query using two different encodings for associated visual and language cues. **Top Row:** The text stimulus is encoded via the CLIP (Radford et al., 2021) text encoder and associated with the image represented by a D-VAE encoded vector. **Bottom Row:** The reconstruction process for heteroassociation. **(L-R)** Ground Truth, Iteration \(t=0\) starting with a blank canvas with the provided CLIP Encoded text inputs as query prompts, an intermediate update, and the full reconstruction. This demonstrates the networkâ€™s ability to accurately reconstruct the image from a text-only input from a completely different stimulus space as the image content.

\(_{n}=[_{}^{}(_{n});_{}^{}(_{n})]\). During retrieval, we construct a query vector \(}^{(0)}=[;}_{}]\) constructed using the encoded text vector \(}_{T}=_{}^{}(_{ }^{(0)})\) and zeros in the location of the image encodings. Finally, after convergence \(}^{(T_{f})}=[}_{}^{(T_{f})};}_{}^{(T_{f})}]\), we decode the image embedding \(_{}^{(T_{f})}=_{}^{}( }_{}^{(T_{f})})\) to retrieve the image content.

To test language-image associations, we utilized the unique set of captions associated with each image in the COCO dataset. In our experiment, we allowed image and text to be encoded with different encoder decoder architectures. Specifically, we retained the best performing encoder (D-VAE) for image encoding (See Fig. 5) but the textual associative stimulus was encoded using the CLIP foundational model (Radford et al., 2021).

To create a more meaningful embedding, we concatenated the set of caption sentences per image into a single long sentence. This sentence was then encoded using the pre-trained CLIP model. The resulting text and image encodings were then ingested into HEN, as illustrated in Fig. 5 (Top). The experiment yielded promising results. The performance of the 6,000 images tested was on par with that of the discrete VAE in the same embedding space. _We found this to be significant, as it suggests that text and image encoders can operate in disjoint spaces while still achieving accurate reconstructions_, provided the Hopfield energy landscapes are appropriately normalized. Further, the top row of Table. 2 shows robust performance across all CLIP combinations.

We also explored an alternate strategy for representing cross-stimulus cues in a single input space by pixelizing the text representations into a unique 'image' representation (See example in Fig. 6) instead of a text-encoder. Converting this text into an image allows us to re-use the image encoder-decoder for both image and text portions of the queries and memory bank latent-space transformations. To our surprise, this schema provided comparable recovery performance to using dedicated image and text encoding strategies in Table. 2. See Section 4 in the Appendix for details on the pixelization.

Finally, to examine the uniqueness of association hypothesis, we designed an experiment in which two different images to be stored in HEN were selected at random and associated with the same textual pattern. We rendered the text in a pixelized form and used the same encoding as for image to remove the effect of separate encodings for image and text in testing the uniqueness aspect. We queried the system using the pixelized text to observe the type of images that would be reconstructed. Fig. 6 indicates that violating the uniqueness constraint led to spurious recall where the reconstructed image appeared to be a mixture of two different images. Thus HEN can support cross-stimuli associations and the recall is accurate if the associative text pattern is distinct per image.

## 4 Conclusions

In this paper, we unified and explored the diverse energy formulations used across associative memory models, highlighting their theoretical interconnections and the shared focus on pattern separability. This foundational analysis informed our development of key enhancements for Modern Hopfield Networks (MHNs), specifically by integrating pattern encoders and decoders to enhance separability and reduce metastable states. Additionally, we demonstrated how this approach supports cross-stimuli associations using different encodings, as long as the uniqueness of association is maintained

Figure 6: In each example, **(Left)** the upper and lower image cases depict the disruption of unique associations in the \(\{_{n}\}\) memory bank. **(Right)** A single text input corresponding to these disrupted associations is used during the query phase. The top blue image represents an empty image as a zero-encoded vector at initialization. The reconstruction appears to be a meta-stable state. Neither of the original images is accurately recovered, supporting the hypothesis of unique text-image associations.

suggesting promising adaptability for applications that require cross-modal recall, like multimedia search or multi-sensor data fusion. These advancements mark a step toward improving the practicality and performance of Modern Hopfield Networks for real-world retrieval and storage tasks.

Additionally, HEN's architecture shows potential for other associative tasks, such as temporal sequences or visual variations (e.g., images of the same entity from different angles). Prior studies on associative memory Gutfreund & Mezard (1988); Chaudhry et al. (2023); Shriwas et al. (2019); Millidge et al. (2022) suggest that with minor modifications, HEN could be extended to sequential or contextual retrieval, supporting broader applications in dynamic and context-rich environments.

While HEN has shown consistent performance, we recognize that its convergence and stability depend on factors such as the update rule, encoder-decoder selection, and time step \(T_{f}\). Our results indicate that after a set number of iterations (\(T_{f}=100\) in our case), retrieval dynamics reach a stable plateau; however, the optimal \(T_{f}\) may vary by dataset. Thus, while our approach delays metastable states, further tuning may be necessary in different contexts.

Looking forward, future work could investigate HEN's scalability with even larger datasets by implementing fine-tuning steps to optimize VAE parameters, potentially further reducing metastable behavior. This approach could enhance HEN's memory capacity, enabling it to support practical storage and retrieval use cases at scale. These extensions position HEN as a versatile framework, adaptable to diverse associative memory tasks and scalable to meet the demands of large-scale, heterogeneous data environments.