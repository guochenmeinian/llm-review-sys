# Similarity-based cooperative equilibrium

Caspar Oesterheld\({}^{1*}\) Johannes Treutlein\({}^{2}\) Roger Grosse\({}^{3,4,5}\)

Vincent Conitzer\({}^{1,6}\) Jakob Foerster\({}^{7}\)

\({}^{*}\)oesterheld@cmu.edu \({}^{1}\)FOCAL, Carnegie Mellon University \({}^{2}\)CHAI, UC Berkeley

\({}^{3}\)Anthropic \({}^{4}\)Vector Institute \({}^{5}\)University of Toronto

\({}^{6}\)Institute for Ethics in AI, University of Oxford \({}^{7}\)FLAIR, University of Oxford

###### Abstract

As machine learning agents act more autonomously in the world, they will increasingly interact with each other. Unfortunately, in many social dilemmas like the one-shot Prisoner's Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another's source code (Rubinstein, 1998; Tennenholtz, 2004) - or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is challenging for agents to learn their way to cooperation in the full transparency setting. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods.

## 1 Introduction

As AI systems start to autonomously interact with the world, they will also increasingly interact with each other. We already see this in contexts such as trading agents (CFTC & SEC, 2010), but the number of domains where separate AI agents interact with each other in the world is sure to grow; for example, consider autonomous vehicles. In the language of game theory, AI systems will play general-sum games with each other. For example, autonomous vehicles may find themselves in Game-of-Chicken-like dynamics with each other (cf. Fox et al., 2018). In many of these interactions, cooperative or even peaceful outcomes are not a given. For example, standard game theory famously predicts and recommends defecting in the one-shot Prisoner's Dilemma. Even when cooperative equilibria exist, there are typically many equilibria, including uncooperative and asymmetric ones. For instance, in the infinitely repeated Prisoner's Dilemma, mutual cooperation is played in some equilibria, but so is mutual defection, and so is the strategy profile in which one player cooperates 70% of the time while the other cooperates 100% of the time. Moreover, the strategies from different equilibria typically do not cooperate with each other. A recent line of work at the intersection of AI/(multi-agent) ML and game theory aims to increase AI/ML systems' ability to cooperate with each other (Stastny et al., 2021; Dafoe et al., 2020; Conitzer & Oesterheld, 2023).

Prior work has proposed to make AI agents _mutually transparent_ to allow for cooperation in equilibrium (McAfee 1984; Howard 1988; Rubinstein 1998, Section 10.4; Tennenholtz 2004; Barasz et al. 2014; Critch 2019; Oesterheld 2019b). Roughly, this literature considers for any given 2-player normal-form game \(\) the following _program meta game_: Both players submit a computer program, e.g., some neural net, to choose actions in \(\) on their behalf. The computer program then receives as input the computer program submitted by the other player. The aforecited works have shown that the program meta game has cooperative equilibria in the Prisoner's Dilemma.

Unfortunately, there are multiple obstacles to cooperation based on full mutual transparency. 1) Settings of _full_ transparency are rare in the real world. 2) Games played with full transparency ingeneral have many equilibria, including ones that are much worse for some or all players than the Nash equilibria of the underlying game (see the folk theorems given by Rubinstein 1998, Section 10.4, and Tennenholtz 2004). In particular, full mutual transparency can make the problem of equilibrium selection very difficult. 3) The full transparency setting poses challenges to modern ML methods. In particular, it requires at least one of the models to receive as input a model that has at least as many parameters as itself. Meanwhile, most modern successes of ML use models that are orders of magnitudes larger than the input. Consequently, we are not aware of successful projects on learning general-purpose models such as neural nets in the full transparency setting.

**Contributions.** In this paper we introduce a novel variant of program meta games called _difference (diff) meta games_ that enables cooperation in equilibrium while also addressing obstacles 1-3. As in the program meta game, we imagine that two players each submit a program or _policy_ to instruct an _agent_ to play a given game, such as the Prisoner's Dilemma. The main idea is that before choosing an action, the agents receive credible information about how _similar_ the two players' policies are to each w.r.t. how they make the present decision. In the real world, we might imagine that this information is provided by a mediator (cf. Monderer & Tennenholtz, 2009; Ivanov et al., 2023; Christoffersen et al., 2023) who wants to enable cooperation. We may also imagine that this signal is obtained more organically. For example, we might imagine that the agents can see that their policies were generated using the same code base. We formally introduce this setup in Section 3. Because it requires a much lower degree of mutual transparency, we find the diff meta game setup more realistic than the full mutual transparency setting. Thus, it addresses Obstacle 1 to cooperation based on full mutual transparency.

Diff meta games can still have cooperative equilibria when the underlying base game does not. Specifically, in Prisoner's Dilemma-like games, there are equilibria in which both players submit policies that cooperate with similar policies and thus with each other. We call this phenomenon _similarity-based cooperation (SBC)_. For example, consider the Prisoner's Dilemma as given in Table 1 for \(G=3\). (We study such examples in more detail in Section 3.) Imagine that the players can only submit _threshold policies_ that are parameterized only by a single real-valued threshold \(_{i}\) and cooperate if and only if the perceived difference to the opponent is at most \(_{i}\). As a measure of difference, the policies observe \((_{1},_{2})=|_{1}-_{2}|+Z\), where \(Z\) is sampled independently for each player according to the uniform distribution over \(\). For instance, if Player 1 submits a threshold of \(}{{2}}\) and Player 2 submits a threshold of \(}{{4}}\), then the perceived difference is \(}{{4}}+Z\). Hence, Player 1 cooperates with probability \(P(}{{4}}+Z}{{2}})=}{{4}}\) and Player 2 cooperates with probability \(P(}{{4}}+Z}{{4}})=}{{2}}\). It turns out that \((_{1}=1,_{2}=1)\), which leads to mutual cooperation with probability \(1\), is a Nash equilibrium of the meta game. Intuitively, the only way for either player to defect more is to lower their threshold. But then \(|_{1}-_{2}|\) will increase, which will cause the opponent to defect more (at a rate of \(}{{2}}\)). This outweighs the benefit of defecting more oneself.

In Section 4, we prove a folk theorem for diff meta games. Roughly speaking, this result shows that observing a diff value is sufficient for enabling all the cooperative outcomes that full mutual

Figure 1: A graphical representation of diff meta games (Definition 1). Nodes with two incoming nodes are determined by applying one of the parent nodes to the other.

transparency enables. Specifically, we show that for every _individually rational_ strategy profile \(\) (i.e., every strategy profile that is better for each player than their minimax payoff), there is a function \(\) such that \(\) is played in an equilibrium of the resulting diff meta game.

Next, we address Obstacle 2 to full mutual transparency - the multiplicity of equilibria. First, note that any given measure of similarity will typically only enable a specific set of equilibria, much smaller than the set of individually rational strategy profiles. For instance, in the above example, all equilibria are symmetric. In general, one would hope that similarity-based cooperation will result in symmetric outcomes in symmetric games. After all, the new equilibria of the diff game are based on submitting similar policies and if two policies play different strategies against each other, they cannot be similar. In Section 5, we substantiate this intuition. Specifically, we prove, roughly speaking, that in symmetric, additively decomposable games, the Pareto-optimal equilibrium of the meta game is unique and gives both players the same utility, if the measure of difference between the agents satisfies a few intuitive requirements (Section 5). For example, in the Prisoner's Dilemma, the unique Pareto-optimal equilibrium of the meta game must be one in which both players cooperate with the same probability.

Finally we show that diff meta games address Obstacle 3: we demonstrate that in games with higher-dimensional action spaces, we can find cooperative equilibria of diff meta games with ML methods. In Section 6.4, we show that, if we initialize the two policies randomly and then let each of them learn to be a best response to the other, they generally converge to the Defect-Defect equilibrium. This is expected based on results in similar contexts, such as in the Iterated Prisoner's Dilemma. However, in Section 6.1, we introduce a novel, general pretraining method that trains policies to cooperate against copies and defect (i.e., best respond) against randomly generated policies. Our experiments show that policies pretrained in this way find partially cooperative equilibria of the diff game when trained against each other via alternating best response training.

We discuss how the present paper relates to prior work in Section 7. We conclude in Section 8 with some ideas for further work.

## 2 Background

**Elementary game theory definitions.** We assume familiarity with game theory. For an introduction, see Osborne (2004). A _(two-player, normal-form) game_\(=(A_{1},A_{2},)\) consists of sets of actions or pure strategies \(A_{1}\) and \(A_{2}\) for the two players and a utility function \( A_{1} A_{2}^{2}\). Table 1 gives the Prisoner's Dilemma as a classic example of a game. A mixed strategy for Player \(i\) is a distribution over \(A_{i}\). We denote the set of such distributions by \((A_{i})\). We can extend \(\) to mixed strategies by taking expectations, i.e., \((_{1},_{2})_{a_{1} A_{1},a_{2} A_{2} }_{1}(a_{1})_{2}(a_{2})(a_{1},a_{2})\). For any player \(i\), we use \(-i\) to denote the other player. We call \(_{i}\) a _best response_ to a strategy \(_{-i}(A_{-i})\), if \((_{i})_{a_{i} A_ {i}}u_{i}(a_{i},_{-i})\), where \(\) denotes the support. A strategy profile \((A_{1})(A_{2})\) is a vector of strategies, one for each player. We call a strategy profile \((_{1},_{2})\) a _(strict) Nash equilibrium_ if \(_{1}\) is a (unique) best response to \(_{2}\) and _vice versa_. As first noted by Nash (1950), each game has at least one Nash equilibrium. We say that a strategy profile \(\) is _individually rational_ if each player's payoff is at least her minimax payoff, i.e., if \(u_{i}()_{_{-i}(A_{-i})}_{a_{i} A _{i}}u_{i}(a_{i},_{-i})\) for \(i=1,2\). We say that \(\) is _Pareto-optimal_ if there exists no \(^{}\) s.t. \(u_{i}(^{}) u_{i}()\) for \(i=1,2\) and \(u_{i}(^{})>u_{i}()\) for at least one \(i\).

**Symmetric games and additively decomposable games.** We say that a game is _(player) symmetric_ if \(A_{1}=A_{2}\) and for all \(a_{1},a_{2}\) for \(i=1,2\), we have that \(u_{i}(a_{1},a_{2})=u_{-i}(a_{2},a_{1})\). The Prisoner's Dilemma in Table 1 is symmetric. We say that a game _additively decomposes into_\((u_{i,j} A_{j})_{i,j\{1,2\}}\) if \(u_{i}(a_{1},a_{2})=u_{i,1}(a_{1})+u_{i,2}(a_{2})\) for all \(i=\{1,2\}\) and all \(a_{1} A_{1},a_{2} A_{2}\). Intuitively, this means that each action \(a_{j}\) of Player \(j\) generates some amount of utility \(u_{i,j}(a_{j})\) for Player \(i\)_independently_ of what Player \(-j\) plays. For example, the Prisoner's Dilemma in Table 1 is additively decomposable, where \(u_{i,i} 0, 1\) and \(u_{i,-i} 0\).

  &  \\  &   } &  &  \\   & & \(G,G\) & \(0,G+1\) \\   & & Defect & \(G+1,0\) & \(1,1\) \\   & & & \\ 

Table 1: The Prisoner’s Dilemma, parameterized by some number \(G>1\).

\(G, 0\) for \(i=1,2\). Intuitively, \(\) generates \(G\) for the opponent and \(0\) for oneself, while \(\) generates \(1\) for oneself and \(0\) for the opponent.

**Alternating best response learning.** The orthodox approach to learning in games is to learn to best respond to the opponent, essentially ignoring that the opponent is also a learning agent. In this paper, we specifically consider alternating best response (ABR) learning. In ABR, the players take turns. In each turn, one of the two players updates the parameters \(_{i}\) of her strategy to optimize \(u_{i}(_{i},_{-i})\), i.e., updates her model to be a best response to the opponent's current model (Brown cf. 1951; Zhang et al. 2022; Heinrich et al. 2023). Since learning an exact best response is generally intractable, we will specifically consider the use of gradient ascent in each turn to optimize \(u_{i}(_{i},_{-i})\) over \(_{i}\). In continuous games if ABR with _exact_ (locally) best response updates converges to \((_{1},_{2})\), then \((_{1},_{2})\) is a (local) Nash equilibrium. Note, however, that ABR may fail to converge (e.g., in the face of Rock-Paper-Scissors dynamics). Moreover, if the best response updates of \(_{i}\) are only approximated, ABR may converge to non-equilibria (Mazumdar et al., 2020, Proposition 6).

## 3 Diff Meta Games

We now formally introduce diff meta games, the novel setup we consider throughout this paper. Given some base game \(\), we consider a new _meta game_ played by two players whom we will call _principals_. Each principal \(i\) submits a _policy_. The two players' policies each observe a real-valued measure of how similar they are to each other. Based on this, the policies then output a (potentially mixed) strategy for the base game. Finally, the utility is realized as per the base game. Below we define this new game formally. This model is illustrated in Figure 1.

**Definition 1**.: _Let \(=(A_{1},A_{2},)\) be a game. A (diff-based) policy for Player \(i\) for \(\) is a function \((A_{i})\) mapping the perceived real-valued difference between the diff-based policies to a mixed strategy of \(\). For \(i=1,2\) let \(_{i}(A_{i})^{}\) be a set of difference-based policies for Player \(i\). Then a policy difference (diff) function for \((_{1},_{2})\) is a stochastic function \(_{1}_{2}^{2}\). For any two policies \(_{1},_{2}\) and difference function \(\), we say that \((_{1},_{2})\) plays the strategy profile \((A_{1})(A_{2})\) of \(\) if \(_{i}=[_{i}(_{i}(_{1},_{2}))]\) for \(i=1,2\). For sets of policies \(_{1},_{2}\) and difference function \(\) we then define the diff meta game \((,_{1},_{2},)\) to be the normal-form game \((_{1},_{2},V)\), where \(V(_{1},_{2})[((_{i}(_ {1},_{2})))_{i=1,2}]\) for all \(_{1}_{1}\), \(_{2}_{2}\)._

Note that Definition 1 does not put any restrictions on \(\). For example, the above definition allows \(((_{i},_{-i}))_{i}\) to be a real number whose binary representation uniquely specifies \(_{-i}\). This paper is dedicated to situations in which \(\) specifically represents some intuitive notion of how different the policies are, thus excluding such \(\) functions. Unfortunately, there are many different ways in which one could formalize this constraint, especially in asymmetric games. In Section 5 we will impose some restrictions along these lines, including symmetry. Our folk theorem (Theorem 3 in Section 4) will similarly impose constraints on \(\) to avoid \(\) functions like the above.

The rest of this section will study concrete examples of Definition 1. First, we define a particularly simple type of diff-based policy. Almost all of our theoretical analysis will be based on this class of policies.

**Definition 2**.: _Let \(\{-,\}\) and \(_{i}^{},_{i}^{>}(A_{i})\) be strategies for Player \(i\) for \(i=1,2\). Then we define \((_{i}^{},,_{i}^{>})\) to be the policy \(\) s.t. \((d)=_{i}^{}\) if \(d\) and \((d)=_{i}^{>}\) otherwise. We call policies of this form threshold policies. Let \(}_{i}\) denote the set of such threshold policies._

Throughout the rest of this section, we analyze the Prisoner's Dilemma as a specific example. We limit attention to threshold agents of the form \((C,,D)\), i.e., policies that cooperate against similar opponents (diff below threshold \(\)) and defect against dissimilar opponents. This is because such policies can be used to form cooperative equilibria, while policies that always cooperate (say, \((C,1,C)\)) or policies that are more cooperative against _less_ similar opponent policies (e.g., \((D,1,C)\)) cannot be used to form cooperative equilibria in the PD with a natural diff function. Policies of the form \((C,,D)\) are uniquely specified by a single real number \(\). A natural measure of the similarity between two policies \(_{1},_{2}\) is then the absolute difference \(|_{1}-_{2}|\). We allow \(\) to be noisy, however. We summarize this in the following.

**Example 1**.: _Let \(\) be the Prisoner's Dilemma as per Table 1. Then consider the \((,_{1},_{2},)\) meta game where \(_{i}=\{(C,_{i},D)_{i}\}\) and \(_{i}((C,_{1},D),(C,_{2},D)))=|_{1}-_{2}|+Z_ {i}\) for \(i=1,2\) where \(Z_{i}\) is some real-valued random variable._The only open parameters of Example 1 are \(G\) (the parameter used in our definition of the Prisoner's Dilemma) and the noise distribution. Nevertheless, Example 1 is a rich setting that allows for nontrivial results. We leave a detailed analysis for Appendix B and only give two specific results about equilibria here.

In the first result, we imagine that the noise \(Z_{i}\) is distributed uniformly between \(0\) and \(>0\) and that \(G\) is at least \(2\). Then, roughly, there are two kinds of equilibria. First, there are equilibria in which both players always defect, because their threshold for cooperation is at most \(0\) (such that they defect with probability \(1\) even against exact copies). Second, and more interestingly, there are equilibria in which both players submit _the same_ threshold strictly between \(0\) and \(\). Note that this means that if both players submit a threshold of \(\), they both cooperate with probability \(1\).

**Proposition 1**.: _Consider Example 1 with \(Z_{i}([0,])\) i.i.d. for some \(>0\) and with \(G 2\). Then \(((C,_{1},D),(C,_{2},D))\) is a Nash equilibrium if and only if \(_{1},_{2} 0\) or \(0<_{1}=_{2}\). In case of the latter, the equilibrium is strict if \(G>2\)._

What happens if, instead of the uniform distribution, we let the \(Z_{i}\) be, say, normally distributed? It turns out that for all unimodal distributions (which includes the normal distribution) and \(G=2\), we get an especially simple result: in equilibrium, both players submit the same threshold and that threshold must be left of the mode.

**Proposition 2**.: _Consider Example 1 with \(G=2\). Assume \(Z_{i}\) is i.i.d. for \(i=1,2\) according some unimodal distribution with mode \(\) with positive measure on every interval. Then \(((C,_{1},D),(C,_{2},D))\) is a Nash equilibrium if and only if \(_{1}=_{2}\)._

## 4 A folk theorem for diff meta games

What are the Nash equilibria of a diff meta game on \(\)? A first answer is that Nash equilibria of \(\) carry over to the diff meta game regardless of what \(\) function is used (assuming that at least all constant policies are available); see Proposition 16 in Appendix C.1. Any other equilibria of the diff meta game hinge on the use of the right \(\) function. In fact, if \(\) is constant and thus uninformative, the Nash equilibria of the diff meta game are exactly the Nash equilibria of \(\); see Proposition 17 in Appendix C.1. So the next question to ask is for what strategy profiles \(\)_there exists_ some \(\) function s.t. \(\) is played in an equilibrium of the resulting diff meta game. The following result answers this question. In particular, a folk theorem similar to the folk theorems for infinitely repeated games (e.g., Osborne 2004, Ch. 15) and for program equilibrium (see Section 7).

**Theorem 3** (folk theorem for diff meta games).: _Let \(\) be a game and \(\) be a strategy profile for \(\). Let \(_{i}}_{i}\) for \(i=1,2\). Then the following two statements are equivalent:_

1. _There is a_ \(\) _function such that there is a Nash equilibrium_ \((_{1},_{2})\) _of the diff meta game_ \((,,_{1},_{2})\) _s.t._ \((_{1},_{2})\) _play_ \(\)_._
2. _The strategy profile_ \(\) _is individually rational (i.e., better than everyone's minimax payoff)._

_The result continues to hold true if we restrict attention to deterministic \(\) functions with \(_{1}=_{2}\) and \(_{i}(_{1},_{2})\{0,1\}\) for \(i=1,2\)._

We leave the full proof to Appendix C.2, but give a short sketch of the construction for 2\(\)1 here. For any \(\), we construct the desired equilibrium from policies \(_{i}^{*}=(_{i},}{{2}},_{i})\) for \(i=1,2\), where \(_{i}\) is Player \(i\)'s minimax strategy against Player \(-i\). We then take any \(\) function s.t. \((_{i}^{*},_{-i})=(0,0)\) if \(_{-i}=_{-i}^{*}\) and \((_{i}^{*},_{-i})=(1,1)\) otherwise.

## 5 A uniqueness theorem

Theorem 3 allows for highly asymmetric similarity-based cooperation. For example, in the PD with, say, \(G=2\), Theorem 3 shows that with the right \(\) function, the strategy profile \((C,}{{3}}*C+}{{3}}*D)\) is played in an equilibrium of the diff meta game of the PD. This seems odd, as one would expect SBC to result in playing symmetric strategy profiles. Note that, for example, all equilibria of Propositions 1 and 2 are symmetric. In this section, we show that under some restrictions on \(\) and the base game \(\), we can recover the symmetry intuition. This is good because in symmetric games the symmetric outcomes are the fair and otherwise desirable ones (Harsanyi et al., 1988, Sect. 3.4) and because SBC thus avoids equilibrium selection problems of other forms of cooperation (including cooperation based on full mutual transparency and cooperation in the iterated Prisoner's Dilemma).

We first need a few definitions of properties of \(\). Let \(\) be a symmetric game. We say that \(\) is _minimized by copies_ if for all policies \(,^{}\), all \(y\) and \(i=1,2\), \(P(_{i}(,^{}){<}y) P(_{i}(,){<}y)\). For example, the \(\) function in Example 1 is minimized by copies. The \(\) functions in the proof of Theorem 3 are not in general minimized by copies when the given base game is symmetric. For example, to achieve \((C,2/3*C+}{{3}}*D)\) in equilibrium, the proof of Theorem 3 (as sketched above) uses the policies \(_{1}^{*}=(C,}{{2}},D)\) and \(_{2}^{*}=(}{{3}}*C+}{{3}}*D,}{{2}},D)\) and a \(\) function with \((_{1}^{*},_{2}^{*})=(0,0)\) but \((_{1}^{*},_{1}^{*})=(1,1)\). If the base game is symmetric, we call \(\)_symmetric_ if for all \(_{1},_{2},\,(_{1},_{2})\) is distributed the same as \((_{2},_{1})\) and \((_{1}(_{1},_{2}),_{2}(_{1},_{2}))\) is distributed the same as \((_{2}(_{1},_{2}),_{1}(_{1},_{2}))\).

Finally, we need a more complicated but nonetheless intuitive property of \(\) functions. In this paper, we generally imagine that _low_ values of \(\) are informative about the other player's policy. In contrast, we will her assume that _high_ values of \(\) are uninformative. That is, for any \(_{i}\) and \(_{-i}\), we will assume that there is a policy \(_{i}\) that plays \(_{i}\) against \(_{-i}\) and triggers the above-threshold policy of \(_{-i}\) with the highest-possible probability. Formally, let \(_{-i}=(_{-i}^{},_{-i},_{-i}^{})\) be any threshold policy. Let \(p\) be the supremum of numbers \(p^{}\) for which there is \(_{i}\) s.t. in \((_{i},_{-i})\), Player \(-i\) plays \((1-p^{})_{-i}^{}+p^{}_{-i}^{}\). Let \(_{_{-i}}^{}=(1-p)_{-i}^{}+p_{-i}^{}\). Intuitively, \(_{_{-i}}^{}\) is the strategy played by \(_{-i}\) against the most different opponent policies. For the examples of Section 3 we have \(p=1\) and thus simply \(_{_{-i}}^{}=_{-i}^{}\). But if \(\) is bounded, then we might even have \(p=0\) or anything in between.

**Definition 3**.: _We call \(}_{1}}_{2} ^{2}\) high value uninformative if for each threshold policy \(_{-i}\), \(_{i}\) and \(>0\) there is a threshold policy \(_{i}\) such that in \((_{i},_{-i})\), a strategy profile within \(\) of \((_{i},_{_{-i}}^{})\) is played._

We are now ready to state a uniqueness result for the Nash equilibria of diff meta games.

**Theorem 4**.: _Let \(\) be a player-symmetric, additively decomposable game. Let \(\) be symmetric, high-value uninformative, and minimized by copies. Then if \((_{1},_{2})\) is a Nash equilibrium that is not Pareto-dominated by another Nash equilibrium, we have that \(V_{1}(_{1},_{2})=V_{2}(_{1},_{2})\). Hence, if there exists a Pareto-optimal Nash equilibrium, its payoffs are unique, Pareto-dominant among Nash equilibria and equal across the two players._

We prove Theorem 4 in Appendix D.3. Roughly, we prove that under the given assumptions, equilibrium policies are more beneficial to the opponent when observing a diff value below the threshold than if they observe a diff value above the threshold. Second, we show that if in a given strategy profile Principal \(i\) receives a lower utility than Principal \(-i\), then Principal \(i\) can increase her utility by submitting a copy of Principal \(-i\)'s policy. Appendix D.1 shows why the assumptions (additive decomposability of the game and and high-value uninformativeness and symmetry of \(\)) are necessary.

## 6 Machine learning for similarity-based cooperation in complex games

Our results so far demonstrate the theoretical viability of similarity-based cooperation, but leave open questions regarding its practicality. In complex environments, where cooperating and defecting are by themselves complex operations, can we find the cooperative equilibria for a given \(\) function with machine learning methods?

### A novel pretraining method for similarity-based cooperation

We now describe _Cooperate against Copies and Defect against Random (CCDR)_, a simple ML method to find cooperative equilibria in complex games. To use this method, we consider neural net policies \(_{}\) parameterized by a real vector \(\). First, for any given diff game, let \(V^{d}(^{m})^{(^{n+1})} (^{m})^{(^{n+1})}^{2}\) be the utility of a version of the game in which \(\) is non-noisy. CCDR trains a model \(_{_{i}}\) to maximize \(V^{d}(_{_{i}},_{_{i}})+V^{d} (_{_{i}},_{_{-i}^{}})\) for randomly sampled \(_{-i}^{}\). That is, each player \(i\) pretrains their policy \(_{_{i}}\) to do well in both of the following scenarios: principal \(-i\) copies principal \(i\)'s model; and principal \(-i\) generates a random model. The method is named for its intended effect in Prisoner's Dilemma-like games. Note, however, that it is well-defined in all symmetric games, not just Prisoner's Dilemma-like games.

CCDR pretraining is motivated by two considerations. First, in games like the Prisoner's Dilemma, there exist cooperative equilibria of policies that cooperate at a diff value of \(0\) and defect as the perceived diff value increases. We give a toy model of this in Appendix E. CCDR puts in place the rudimentary structure of these equilibria. Note, however, that CCDR does not directly optimize for the model's ability to form an equilibrium. Second, CCDR can be thought of as a form of curriculum training. Before trying to play diff games against other (different but similar) learned agents, we might first train a policy to solve two (conceptually and technically) easier related problems.

### A high-dimensional one-shot Prisoner's Dilemma

To study similarity-based cooperation in an ML context, we need a more complex version of the Prisoner's Dilemma. The complex Prisoner's Dilemma-like games studied in the multi-agent learning community generally offer other mechanisms that establish cooperative equilibria (e.g., playing a game repeatedly). For our experiments, however, we specifically need SBC to be the only mechanism to establish cooperation.

We therefore introduce a new game, the High-Dimensional (one-shot) Prisoner's Dilemma (HDPD). The goal is to give a variant of the one-shot Prisoner's Dilemma that is conceptually simple but introduces scalable complexity that makes finding, for example, exact best responses in the diff meta game intractable. In addition to \(G\), the HDPD is parameterized by two functions \(f_{C},f_{D}^{n}^{m}\) representing the two actions Cooperate and Defect, respectively, as well as a probability measure \(\) over \(^{n}\). Each player's action is also a function \(f_{i}^{n}^{m}\). This is illustrated in Figure 2 for the case of \(n=1\) and \(m=1\). For any pair of actions \(f_{1},f_{2}\), payoffs are then determined as follows. First, we sample some \(\) according to \(\) from \(^{n}\). Then to determine how much Player \(i\) cooperates, we consider the distance \(d(f_{i}(),f_{C}())\) to determine, roughly speaking, how much Player \(1\) cooperates. The larger the distance the less cooperative is \(f_{i}\). In the case of \(n=m=1\) and \(\) uniform, the expected distance between \(f_{i}(x)\) and \(f_{D}(x)\) is simply the area between the curves of \(f_{i}\) and \(f_{D}\), as visualized in Figure 2. We analogously determine how much the players defect. Formally, we define \(u_{i}(f_{1},f_{2})=-_{}[d(f_{i}( ),f_{D}())+Gd(f_{-i}(),f_{C}())]/ _{}[d(f_{C}(),f_{D}())]\). Thus, the action \(f_{i}=f_{D}\) corresponds to defeating and the action \(f_{C}\) corresponds to cooperating, e.g., \((f_{C},f_{C})=(-1,-1)\) and \((f_{D},f_{D})=(-G,-G)\). The unique equilibrium of this game is \((f_{D},f_{D})\). In our experiments, we specifically used \(G=5\).

We consider a diff meta game on the HDPD. Formally, a diff-based policy for the HDPD is a function \((^{m})^{(^{n})}\). For notational convenience, we will instead write policies as functions \(^{n+1}^{m}\). We then define our diff function by \(_{i}(_{1},_{2})=_{(y,)} [d(_{1}(y,),_{2}(y,))]+Z_{i}\), where \(\) is some probability distribution over \(^{n+1}\) and \(Z_{i}\) is some real-valued noise.

### Experiments

**Experimental setup.** We trained on the environment from Section 6.2. We selected a fixed set of hyperparameters based on prior exploratory experiments and the theoretical considerations in Appendix E. We then randomly initialized \(_{1}\) and \(_{2}\), CCDR-pretrained them (independently), and then trained \(_{1}\) and \(_{2}\) against each other using ABR. We repeated the experiment with 28 random seeds. As control, we also ran the experiment _without_ CCDR on 26 seeds. We also ran experiments with Learning with Opponent-Learning Awareness (LOLA) (Foerster et al., 2018), which we report in Appendix G.

**Results.** First, we observe that in the runs without CCDR pretraining, the players generally converge to mutual defection during alternating best response learning. In particular, in all 26 runs, at least one

Figure 2: The figure illustrates how utilities are calculated in the HDPD. The function \(f_{i}\) is an action chosen by Player \(i\). The area between the curves \(f_{i}\) and \(f_{D}\) determines, intuitively, how much the agent defects; the area between the curves \(f_{i}\) and \(f_{C}\) determines how much it cooperates. The \(f_{i}\) shown in the figure is much closer to defecting than to cooperation.

player's utility was below \(-5\). Only two runs had a utility above \(-5\) for one of the players (\(-4.997\) and \(-4.554\)). The average utility across the 26 runs and across the two players was \(-5.257\) with a standard deviation of \(0.1978\). Anecdotally, these results are robust - ABR without pretraining practically never finds cooperative equilibria in the HDPD.

Second, we observe that in all 28 runs, CCDR pretraining qualitatively yields the desired policy models, i.e., a policy that cooperates at low values of diff and gradually comes closer to defecting at high values of diff. Figure 2(a) shows a representative example.

Our main positive experimental result is that after CCDR pretraining, the models converged in alternating best response learning to a partially cooperative equilibrium in 26 out of 28 runs. Thus, the cooperative equilibria postulated in general by Theorem 3 and in simplified examples by Propositions 1 and 2 (as well as Proposition 25), do indeed exist and can be found with simple methods. The minimum utility of either player across the 26 successful runs was -4.854. The average utility across all runs and the two players was about -2.77 and thus a little closer to \(u(f_{C},f_{C})=-1\) than to \(u(f_{D},f_{D})=-5\). The standard deviation was about 1.19. Figure 2(b) shows the losses (i.e., the negated utilities) across ABR learning. Generally, the policies also converge to receiving approximately the same utility (cf. Section 5). The average of the absolute differences in utility between the two players at the end of the 28 runs is about 0.04 with a standard deviation of 0.05. We see that in line with Theorem 4, we tend to learn egalitarian equilibria in this symmetric, additively decomposable setting. After alternating best response learning, the models generally have a similar structure as the model in Figure 2(a), though often they cooperate only a little at low diff values. Based on prior exploratory experiments, CCDR's success is moderately robust.

### Discussion

Without pretraining, ABR learning unsurprisingly converges to mutual defection. This is due to a bootstrapping problem. Submitting a policy of the form "cooperate with similar policies, defect against different policies" is a unique best response against itself. If the opponent model \(_{-i}\) is not of this form, then any policy \(_{i}\) that defects, i.e., that satisfies \(_{i}((_{i},_{-i}))=f_{D}\), is a best response. Because \(f_{C}\) is complex, learning a model that cooperates at all is unlikely. (Even if \(f_{C}\) was simple, the appropriate use of the perceived \(\) value would still be specific and thus unlikely to be found by chance.) Similar failures to find the more complicated cooperative equilibria by default have also been observed in the iterated PD (Sandholm & Crites 1996; Foerster et al. 2018; Letcher et al. 2019) and in the open-source PD (Hutter, 2020). Opponent shaping methods have been used successfully to learn to cooperate both in the iterated Prisoner's Dilemma (Foerster et al. 2018; Letcher et al. 2019) and the open-source Prisoner's Dilemma (Hutter, 2020). Our experiments in Appendix G show that LOLA can also learn SBC, but unfortunately not as robustly as CCDR pretraining.

CCDR pretraining reliably finds models that cooperate with each other and that continue to partially cooperate with each other throughout ABR training. This shows that when given some guidance, ABR can find SBC equilibria - SBC equilibria have at least some "basin of attraction". Our experiments therefore suggest that SBC is a promising means of establishing cooperation between ML agents.

Figure 3: (a) The behavior of a CCDR-pretrained policy. For each perceived value of diff to the opponent \(y\), the green line shows the expected distance of the learned policy’s choice to \(f_{C}\) (smaller means more cooperative) and the read line shows the expected distance to \(f_{D}\). (b) Losses of Player 1 in 10 runs through the ABR phase.

That said, CCDR has many limitations that we hope can be addressed in future work. For one, in many games the best response against a randomly generated opponents does poorly against a rational opponent. Second, our experiments show that while the two policies almost fully cooperate after CCDR pretraining, they quickly partially unlearn to cooperate in the ABR phase. We would prefer a method that preserves closer to full cooperation throughout ABR-style training. Third, while CCDR seems to often work, it can certainly fail in games in which SBC is possible. Learning to distinguish randomly sampled opponent policies from copies will in many settings not prepare an agent to distinguish cooperative/SBC opponents from uncooperative but trained (not randomly sampled) opponents. Consequently, CCDR may sometimes result in insufficiently steep incentive curves, cooperating with too dissimilar opponents. We suspect that to make progress on the latter issues we need training procedures that more explicitly reason about incentives _a la_ opponent shaping (cf. our experiments with LOLA Appendix G).

## 7 Related work

We here relate our project to the two most closely related lines of work. In Appendix H we discuss more distantly related lines of work.

**Program equilibrium.** We already discussed in Section 1 the literature on program meta games in which players submit computer programs as policies and the programs fully observe each other's code (McAfee 1984; Howard 1988; Rubinstein 1998, Section 10.4; Tennenholtz 2004). Interestingly, some constructions for equilibria in program meta games are similarity based. For example, the earliest cooperative program equilibrium for the Prisoner's Dilemma, described in all four of the above-cited papers, is the program "Cooperate if the opponent's program is equal to this program; else Defect". The program "cooperate if my cooperation implies cooperation from the opponent" proposed by Critch et al. (2022) is also similarity-based. Other approaches to program equilibrium cannot be interpreted as similarity based, however (see, e.g., Barasz et al., 2014; Critch, 2019; Oesterheld, 2019b). To our knowledge, the only published work on ML in program equilibrium is due to Hutter (2020). It assumes the programs to have the structure proposed by Oesterheld (2019b) on simple normal-form games, thus leaving only a few parameters open. Similar to our experiments, Hutter shows that best response learning fails to converge to the cooperative equilibria. In Hutter's experiments, the opponent shaping methods LOLA (Foerster et al., 2018) and SOS (Letcher et al., 2019) converge to mutual cooperation.

**Decision theory and Newcomb's problem.**Brams (1975) and Lewis (1979) have pointed out that the Prisoner's Dilemma against a similar opponent closely resembles _Newcomb's problem_, a problem first introduced to the decision-theoretical literature by Nozick (1969). Most of the literature on Newcomb's problem is about the normative, philosophical question of whether one should cooperate or defect in a Prisoner's Dilemma against an exact copy. Our work is inspired by the idea that in some circumstances one should cooperate with similar opponents. However, this literature only informally discusses the question of whether to also cooperate with agents other than exact copies (Hofstadter e.g., 1983; Drescher 2006, Ch. 7; Ahmed 2014, Sect. 4.6.3). We address this question formally.

One idea behind the present project, as well as the program game literature, is to analyze a decision situation from the perspective of (actual or hypothetical) _principals_ who design policies. The principals find themselves in an ordinary strategic situation. This is how our analysis avoids the philosophical issues arising in the _agent's_ perspective. Similar changes in perspective have been discussed in the literature on Newcomb's problem (e.g., Gauthier 1989; Oesterheld & Conitzer 2022).

## 8 Conclusion and future work

We make a strong case for the promise of similarity-based cooperation as a means of improving outcomes from interactions between ML agents. At the same time, there are many avenues for future work. On the theoretical side, we would be especially interested in generalizations of Theorem 4, that is, theorems that tell us what outcomes we should expect in diff meta games. Is it true more generally that under reasonable assumptions about the diff function, we can expect SBC to result in fairly specific, symmetric, Pareto-optimal outcomes? We are also interested in further experimental investigations of SBC. We hope that future work can improve on our results in the HDPD in terms of robustness and degree of cooperation. Besides that, we think a natural next step is to study settings in which the agents observe their similarity to one another in a more realistic fashion. For example, we conjecture that SBC can occur when the agents can determine that their policies were generated by similar learning procedures.