# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

which lead to two novel strategies, respectively. The first is inspired by the commonly used _inverted_ implementation of dropout regularization [37; 24], and searches for the solution in the family of unbiased estimators. The second one takes into account all the (participating in a given round) client specific sub-models, and aims to reconstruct the full weight matrix from all these observations (instead of considering each client separately). Notably, the solutions for both strategies can be presented in closed form. Furthermore, since the sampling of sub-models takes place on the server side, the computation of the optimal probabilities in these settings does not require any additional calculation on potentially weak clients. Finally, we employ such procedures for sampling without replacement which preserve given inclusion probabilities .

## 2 Related Works

Heterogeneity in its different aspects is ubiquitous in real-life setups for federated learning and is therefore a long-standing topic of research [11; 45]. Probably the most well-studied issue is the non-i.i.d. distributed data samples, available on clients' devices . However, the deployment of federated systems also faces the variety of devices which often have different computational constraints . This makes training the same model for each client infeasible and motivates the search for new solutions.

Many popular approaches to tackle this kind of heterogeneity follow one of two directions. Methods based on knowledge distillation in most cases use client models to train a new predictor on the server side [27; 14]. However, these techniques typically require a separate dataset on the server, which may be a serious limitation in practice. In contrast, partial training-based approaches do all the training solely on the client side. To fit the clients' constraints, they sample sub-models which have a size that allows them to be trained on the corresponding device. After local training, these sub-models are aggregated into a global model. Notably, this allows for a final model size that is otherwise infeasible to train in any of the available clients.

Many such methods conduct the sub-sampling in the original model space, e.g., by selecting specific output dimensions on each layer. While HeteroFL  simply selected the top-left corner of the weight matrix of the required size, methods such as Federated Dropout  and FjORD  argued for randomized selection. FedRolex  replaced randomness with rolling windows, with the aim of covering all possible sub-models in a shorter period of time.

Recent advances in using factorized low-rank models, both for training from scratch [22; 21; 38; 43] and for fine-tuning , found applications in the realm of federated learning [2; 46]. E.g., FLANC used this idea to address heterogeneous computational constraints by representing a weight matrix as a product of two factors, one fixed for all clients and the other having a varying size based on the capacity of each client . Such approaches for model sub-sampling can be generally useful, given the fact that the (stable) rank of the model weights tends, in some cases, to decrease during training [41; 43]. In a similar spirit, FedHM  applied truncated SVD to create the sub-models, while PriSM [32; 33] employed a random procedure for choosing the SVD factors. In our work, we propose novel strategies for the selection of sub-models and their training in such scenarios and demonstrate their advantages.

## 3 Method

### Preliminaries

In the case of cross-device federated learning, low-end clients may not be able to perform the computations with the full weight matrix \(W\) in all the layers due to various constraints. A possible remedy for this problem can be to use an approximation for \(W\). In this work we consider a particular case of such approximations, namely sub-sampling the terms from the singular decomposition (SVD) \(W=_{i=1}^{N}_{i}u_{i}v_{i}^{T}\), where both sets of column vectors \(\{u_{i}^{c_{out}}\}_{i}\) and \(\{v_{i}^{c_{in}}\}_{i}\) are orthonormal, and the singular values are sorted in a non-increasing order \(_{1}_{2}_{N}>0\). Due to the usage of singular decomposition, we name this type of sub-model sampling as _spectral model sharing_.

More formally, let \(0<r 1\) be the _keep ratio_ which depends on the client's capabilities, then the goal is to sample the \(n= Nr\) terms from the full sum. Following prior works, keep ratio \(r\) is sharedacross layers [33; 32; 30]. With a sufficiently low value of \(r\), this reduces both communication costs and computational complexity. We introduce a vector of indicators \(z=(z_{1},,z_{N})\{0,\;1\}^{N}\) such that \(_{i=1}^{N}z_{i}=n,\) and the corresponding _sub-layer_ is created with a weight matrix \(,\)

\[=_{i=1}^{N}z_{i}_{i}_{i}u_{i}v_{i}^{T}=_{j=1}^{n} _{_{j}}_{_{j}}u_{_{j}}v_{ _{j}}^{T},\] (1)

where \(\{_{i}\}_{i}\) are some scalar _auxiliary multipliers_, and \(\) is the set of selected indices, \(=\{i:\;z_{i}=1\}.\)

If the sampling of indicators \(z_{i}\) is random, we denote the corresponding marginal inclusion probabilities as \(_{i}=(z_{i}=1).\) The joint distribution of indicator variables is interchangeably referred to as \(p(z)\) or \(p().\) Wherever the expectation symbol \(\) is used in the text, expectation w.r.t. \(p(z)\) is assumed. If it is necessary to emphasize that the particular estimator, multiplier, vector, etc. is used by the client \(c\), this is shown with the client index in parentheses, e.g. \(^{(c)}.\)

Based on the previous models of FedHM  and PriSM [32; 33], we consider the following pipeline of sharded model training in the case of federated learning on heterogeneous devices:

1. In the beginning of each communication round, the server performs SVD for the weight matrices of the affine layers. Then, for each client \(c\) it randomly samples a subset of indices \(^{(c)}\) from the decomposition with keep ratio \(r^{(c)},\)\(^{(c)}=_{j=1}^{n^{(c)}}_{_{j}^{(c)}}u_{_{j} ^{(c)}}^{}v_{_{j}^{(c)}}^{ T}.\) The singular values are absorbed into the vectors, namely \(u_{i}^{}=}u_{i},\)\(v_{i}^{}=}v_{i}.\) Note that in both FedHM and PriSM, the auxiliary multipliers \(_{i}\) (which can be used to 'compensate' for the dropped terms) were set to 1. However, since they are actively used in our method, we include them in the description.
2. Each participating client \(c\) receives the matrices \(U^{(c)}=(u_{_{1}^{(c)}}^{},,u_{_{n}^{( c)}}^{})^{c_{out} n}\) and \(V^{(c)}=(v_{_{1}^{(c)}}^{},,v_{_{n}^{ (c)}}^{})^{c_{in} n}\) as well as the vector \(^{(c)}=(_{_{1}^{(c)}},,_{_ {n}^{(c)}})^{n}\) from the server and performs local training with the factorized weight matrix \(U^{(c)}^{(c)}V^{(c)T}\) for some predefined number of epochs. During training \(U^{(c)}\) and \(V^{(c)}\) are updated while the diagonal matrix \(^{(c)}\) which has values of \(^{(c)}\) on the diagonal is kept frozen. When the local training is done, the updated matrices \(U^{(c)}\) and \(V^{(c)}\) are sent back to the server.
3. The server aggregates each vector from the decomposition separately, i.e. \(u_{i}^{}=1}{D^{(c)}}u_{i}^{(c)}}{ _{c:s^{(c)}=1}{D^{(c)}}-},\) where \(D^{(c)}\) is the size of the local dataset of the client \(c\) and \(u_{i}^{(c)}\) is the updated value of the \(i\)-th vector received from the client \(c\). If the \(i\)-th term was not selected by any client in the current round, it remains the same. Vectors \(\{v_{i}^{}\}_{i}\) are aggregated in the same way. Afterward, the updated full weight matrix is calculated as \(W_{i=1}^{N}u_{i}^{}v_{i}^{ T}\), and a new communication round begins.

FedHM proposed to use the \(n\) terms corresponding to the largest singular values, a deterministic selection instead of random sampling. In contrast, PriSM presented a randomized algorithm, which samples the terms according to the magnitudes of the singular values. It employs the NumPy  method numpy.random.choice with the option replace=False, and the associated probabilities are set proportional to \(_{i}^{k},\) where \(k\) is a hyperparameter depending on the keep ratio \(r\). Sampling in this manner results in a probability law similar to Wallenius' noncentral hypergeometric distribution [42; 5; 10; 9], see Appendix C for details. Such sampling procedures make the analysis of the resulting marginal inclusion probabilities \(\{_{i}\}_{i}\) complicated . Thus, the statistical properties of the obtained matrix approximation are opaque.

### Sampling Distribution

As opposed to the prior works, we consider the search of an optimal sampling distribution to be the central task. We introduce two assumptions which we use to derive our two different strategies.

#### 3.2.1 Unbiased Estimator

The bias and variance are inherent trade-offs for an estimator. To keep the bias in check, we propose an assumption of the unbiasedness of the sampled estimator \(\) of the original weight matrix \(W\), i.e. \(=W\), where the expectation is taken over the sampling distribution \(p()\) with the marginal inclusion probabilities \((_{1},,_{N}).\) As shown in Appendix B.1, this requirement necessarily leads to the following values of auxiliary multipliers \(_{i}=_{i}^{-1}.\) This selection of multipliers is known in literature as a Horvitz-Thompson estimator  and resembles the common implementation of _inverted dropout_ which assumes dividing the non-zeroed activations by the inclusion probability at training time.

With the bias of the estimator taken under control, we aim to reduce the approximation error of the resulting layer. For any input vector \(x^{c_{in}},\) the following inequality holds

\[\|Wx-x\|_{2}^{2}\|W-\|_{2}^{2} \|x\|_{2}^{2}\|W-\|_{F}^{2}\|x \|_{2}^{2},\] (2)

where \(\|\|_{F}^{2}\) is a squared Frobenius norm which equals the sum of the squared singular values. Thus, we can control the upper-bound of the expected error by searching for sampling distribution \(p()\) that minimizes the _Frobenius discrepancy_

\[_{p()}\ \ _{p()}\|W-\|_{F}^ {2}.\] (3)

**Theorem 3.1**.: _For an unbiased estimator \(\) of the type specified in Eq. (1) and consisting of \(n\) terms, the Frobenius discrepancy can be expressed in terms of the marginal inclusion probabilities_

\[\|W-\|_{F}^{2}=_{i=1}^{N}_{i}^{2}( _{i}^{-1}-1),\] (4)

_and the optimal set of inclusion probabilities has the following form_

\[_{i}=1,&i t,\\ }{_{k=t+1}^{N}_{k}},&i>t\] (5)

_for \(i=1,,N,\) where \(t\{0,,n-1\}.\)_

Proof.: See Appendix B.1. 

As follows from Theorem 3.1, to find the true arguments of the minima of Eq. (4), one needs to sweep over \(n\) possible values of \(t,\) evaluate the Frobenius discrepancy and select the \(t\) with the minimal discrepancy. Note that this procedure takes place on the server side and therefore does not require extra computation on clients' devices.

#### 3.2.2 Collective Estimator

Due to the well-known bias-variance trade-off, unbiased estimators in practice can have too large variance. This motivates us to consider another perspective. Consider the simplifying assumption where \(C\) clients participating in the current communication round share the same number of terms \(n\) in their respective estimators. We can target'reconstructing' the full matrix \(W\) from the whole set of i.i.d. observations \(\{^{(c)}\}_{1 c C}.\) The simplest way of obtaining such reconstruction is averaging, i.e. \(=_{c=1}^{C}^{(c)},\) and we refer to the matrix \(\) as the _collective estimator_. To ensure that our reconstruction is accurate, we aim to find the optimal sampling distribution as well as the set of auxiliary multipliers,

\[_{p(),\{_{i}\}_{i}}\ \ _{p( )}\|W-\|_{F}^{2}.\] (6)

**Theorem 3.2**.: _For a collective estimator \(\), the average value of the Frobenius discrepancy can be expressed in terms of just the marginal inclusion probabilities_

\[\|W-\|_{F}^{2}=_{i=1}^{N}_{i}^{2} _{i}_{i}(-2+}{C}+_{i}(C -1)}{C})+_{i=1}^{N}_{i}^{2},\] (7)_and the optimal set of inclusion probabilities and auxiliary weights has the following form in the case of \(C>1\)_

\[_{i}=(C-1)},_{i}= 1,&i t,\\ }}{C-1})_{i}}{_{k=t+1}^{t+ 1}_{k}}-,&t<i t+u,\\ 0,&i>t+u,\] (8)

_for \(i=1,,N,\) where \(t\) and \(u\) are integer constants such that \(0 t n\), and \(0 u N-t\). For \(C=1\) the optimal values are \(_{i}=_{i}=(i n)\)._

Proof.: For \(C=1\) the proof immediately follows from the Eckart-Young-Mirsky theorem  which states that truncated SVD provides the best low-rank approximation of the original matrix in terms of Frobenius discrepancy. Since all the estimators defined by Eq. (1) are low-rank approximations, it is impossible to obtain an average error which is strictly lower than the error of truncated SVD. For the case \(C>1\), see Appendix B.2. 

Note that for a group consisting of a single client (i.e. \(C=1\)) the optimal solution coincides with top-\(n\) sampling used in FedHM method. For larger groups, similarly to the unbiased estimator, sweeping across all possible values of \(t\) and \(u\) is required on the server side. Nevertheless, this search can be performed in vectorized form.

In practice, to apply Theorem 3.2, the server can cluster the heterogeneous clients into homogeneous groups which share that same keep ratio \(r\), and compute a specific sampling distribution for each group. Such clustering is often employed by methods targeting diverse clients, cf. .

#### 3.2.3 Joint Distribution and Sampling

The strategies derived in Secs. 3.2.1 and 3.2.2 provide the values of marginal inclusion probabilities but do not specify the joint distribution, i.e. the co-occurrences of sampled terms. Tille  presented a systematic survey of multiple sampling procedures which preserve the given inclusion probabilities. Notably, each of the methods, while keeping the mean vector \((z_{1},,z_{N})=(_{1},,_{N})\) fixed, has different covariance matrices. We follow the authors' recommendation and stick with the Conditional Poisson scheme (CPS) for our experiments. The joint distribution achieved with this method has the maximum entropy among all distributions over samples of a size of exactly \(n\) with the same marginals. In theory, such a design should allow the model to better explore the interaction between the singular vectors during training. The reference implementation of this sampling algorithm is provided in the accompanying package2 for .

### Local Training

While the derivations in Sec. 3.2 provide an optimal (in a certain sense) estimator \(^{(c)}\) of the full weight matrix \(W\) for the client \(c\)_before_ the local training starts on that client, they do not provide any guidance on how to optimize the received sub-model.

**Auxiliary multipliers.** As indicated in Sec. 3.1, we follow Khodak et al.  and Niu et al.  and absorb the square root of the singular value \(_{i}\) into both left and right singular vectors, \(u_{i}^{}=}u_{i},\;v_{i}^{}=}v_{i}\). During early stages of our experiments we explored the absorption of the auxiliary multipliers \(_{i}\) in the same manner. However, this led to unsatisfactory results. Note that, except for the corner cases \(_{i}\{0,1\}\), the multiplier \(_{i}\) is in inverse proportion to \(_{i}\) and therefore the magnitude of their product is almost independent of \(i\). Therefore, after the product \(}}\) is directly 'baked' into the sub-model weights, it is harder for the clients to distinguish between important and uninformative terms.

Instead, we treat the auxiliary multipliers as scaling factors, frozen for the current round of local training. Hence, the effective weight matrix of the sub-model is trained locally in the factorized form \(=U V^{T}\) with learnable terms \(U\) and \(V\). This could introduce an issue of stale multipliers: during the course of local training the original meaning, along with the guarantees on the desired estimator properties, of the auxiliary multipliers is gradually fading away. However, this was not something we observe in practice and any attempts to mitigate this (e.g., with gradual decrease of the scaling factors) did not provide consistent improvement for the model performance.

**Effective learning rate.** Nevertheless, the chosen way of incorporating the auxiliary multipliers into the model still has a drawback. Namely, the gradient of the scalar loss function \(L\) w.r.t. the factors is expressed as \(=}V, =(})^ {T}U\). This means that the auxiliary multipliers \(_{i}\) also affect the effective learning rate of the corresponding column vectors \(u^{}_{i}\) and \(v^{}_{i}\) when first-order optimization methods like SGD are used. For the collective estimator it follows from Eq. (8) that the values of the multipliers are bounded by the size of the group, \(1_{i} C\). However, for the unbiased estimators there is no such upper bound as \(_{i}=_{i}^{-1}\). Moreover, for small values of the keep ratio \(r\) there necessarily should exist relatively large values of \(_{i}^{-1}\). This is because

\[_{j=1}^{n}_{_{j}}=_{j=1}^{n}_ {_{j}}^{-1}=_{i=1}^{N}z_{i}_{i}^{-1}=N,\] (9)

and one needs to approximate \(N\) with \(n rN\) randomly chosen terms. This can lead to excessively high values of the effective learning rate. Based on that, we adjust the nominal learning rate for each vector or, equivalently, override the value of the gradient before conducting the optimization step, \(_{i}}_{i}}1,}, _{i}}_{i}}1,}\) for some threshold \( 1\). With this modification, the effective learning rate cannot be more than \(\) times higher than the nominal. In all reported experiments we use \(=10\). Worth noting that equalizing the efficient learning rate with \(=1\) turned out very detrimental for the performance, as we found early in our experiments. This coincides with the intuition behind some of adaptive optimization methods like AdaGrad  that perform larger updates for the parameters which are in charge of 'less frequent' features.

**Frobenius decay and momentum.** As proposed in other works [22; 33; 44] we use Frobenius weight decay (FD) during local training, i.e. we apply an additional loss function proportional to the \(\|\|_{F}^{2}\) for each effective weight matrix. The weight of FD in the resulting loss function is set to \(1 10^{-4}\). Additionally, confirming findings of Niu et al. , we found it necessary to use plain SGD with momentum weight of \(0.9\) during local optimization of the sub-model weights.

## 4 Experiments

### Experimental Setup

**Datasets and models.** To evaluate the proposed strategies, we conducted experiments on several datasets which are usually employed to test federated systems. In case of artificial simulation of clients, we follow  and sample the prior probabilities of classes for each client from the Dirichlet distribution \(( p)\) where \(p\) is the vector of class proportions in the original dataset and \(\) is a hyperparameter that controls the amount of non-i.i.d.-ness in the data splits. Note that this notation is different from the one used in some of the recent papers, e.g. [44; 32; 33].

Since in our work we do not aim to reach new state-of-the-art in the field and instead focus on analyzing the _relative_ performance of the proposed strategies of spectral sharding, we have chosen to test all the methods in the presence of high data non-i.i.d.-ness between clients.

For _CIFAR-10_ we split the data with \(=1\) and conduct experiments with a ResNet-18 model  with the kernel size of the first convolutional block equal to \(3 3\) and the normalization layer replaced with GroupNorm . For _TinyImagenet_ we use \(=10\) and a compact transformer (CCT) model , namely CCT-6/3x2. For _CIFAR-100_ the two-staged Pachinko allocation method (PAM)  is used: for each client, at first parameters of the multinomial distribution over the twenty coarse labels are sampled with \(=1\), and afterwards the distribution over the coarse-to-fine labels with \(=10\). On this dataset we train both the ResNet and CCT models described above. We select _Shakespeare_ as an example of a dataset with a natural data split over clients. We train a tiny transformer model with three GPT-2  blocks on the task of next character prediction and report the performance in terms of accuracy in accordance with prior works .

For all the architectures we do not decompose the very first and very last layers of the model [22; 33]. However, unlike PriSM, we decompose all the affine layers inside the attention blocks, not just the fully connected ones . Also, we follow Niu et al. , Wang et al.  instead of Khodaket al.  and decompose a convolutional layer to a sequence of a regular convolution with \(n\) output channels followed by a \(1 1\) convolution. For image datasets we employ per-image pixel value standardization similarly to  but do not apply random cropping.

**Training.** We train all models from scratch with cosine annealing learning rate scheduler . The initial value for learning rate is \(0.1\) for CIFAR-10, \(0.05\) for CIFAR-100 and TinyImagenet, and \(0.1\) for the Shakespeare data. The client's batch size equals 32, 64, 128, and 10 respectively. All experiments are run with three random seeds which also affect data splitting between clients, if applicable. Standard deviation is reported in all tables and plots based on those runs.

**Federated setup.** While Shakespeare dataset naturally contains 715 clients, we simulate 100 clients for all other datasets. We randomly sample 10 clients for each communication rounds which results in participation ratio of \(10\%\) on image datasets and about \(1.4\%\) for Shakespeare. In each communication round all participating clients train their sub-models for two local epochs. The total number of local epochs (e.g., number of local epochs per round \(\) number of communication rounds) equals \(2{,}000\) for CIFAR-10, \(3{,}000\) for CIFAR-100, \(5{,}000\) for TinyImagenet and \(3{,}000\) for Shakespeare.

**Baselines.** As stated above, we focus on exploring different strategies of sampling sub-models when a particular approach of training on weak and, possibly, heterogeneous devices was chosen, namely the approach described in Sec. 3.1. Therefore, we compare our presented strategies, denoted as _Unbiased_ and _Collective_, with the _Top-\(n\)_ sampling proposed by  and the _PriSM_ method introduced in [32; 33]. We copy the value of the hyperparameter \(k\) required for sampling in PriSM from the original implementation. In detail, \(k\) depends on the keep ratio \(r\): \(k=4\) if \(r 0.2\) and \(k=2.5\) otherwise.

To decouple the sampling strategies themselves from other training details discussed in Sec. 3 we additionally introduce modifications to the PriSM strategy: motivated from our unbiased method, we train it with auxiliary multipliers to allow for (approximate) unbiasedness of the estimators, i.e. \(_{i}=_{i}^{-1}\) (this strategy is dubbed as _PriSM + Wallenius_), and clip the effective learning rate (_PriSM + Wallenius + ClipLR_). Exact computation of the mean vector for the Wallenius' distribution occurring in these two strategies is very time-consuming for the ranks \(N\) of weight matrices used in practice. For that reason we use the approximate algorithm3 from .

Finally, we explore the simplest option of compensating for the missing terms of the estimator, namely, introducing a scaling factor that keeps the Frobenius norm of the estimator equal to that of the full matrix, i.e. \(_{i}^{(c)}=^{N}_{k}^{2}}{_{k=1}^{N}z_{ k}^{(c)}_{k}^{2}}}\) for all \(i\). This modification is marked as _+Scaled_.

All the considered methods require equal communication costs and equal amount of computation on the client side per round, since the overhead caused by the vector of auxiliary weights introduced by novel strategies is negligible.

### Results

#### 4.2.1 Main Results

We report the accuracy of different strategies in all the datasets considered in Tab. 1. In our experiments we mostly explore weak clients with low keep ratio. The results are provided for two setups with homogeneous clients (\(r=0.1\) and \(0.2\)) and one setup with two groups, namely \(60\%\) clients having a keep ratio of \(0.2\) and \(40\%\) of \(0.4\). Also, for reference purposes we train a conventional full model on all the clients without sharding, as well as an over-parameterized model with keep ratio \(r=1\). Similarly to what was reported by Khodak et al. , we observe that overparameterization can provide a better performance than usual training.

As the results demonstrate, the preservation of the Frobenius norm (_+Scaled_) is the least effective and stable among the considered modifications. It is often detrimental for the Top-\(n\) strategy, and although it sometimes improves the performance of PriSM, the PriSM modification based on unbiasedness (_+Wallenius+ClipLR_) is generally more successful. In certain cases, it even achieves the best accuracy among all models, e.g. see the results of ResNet on CIFAR-100. However, overall the 'unbiasedness' has an inconsistent effect on PriSM: it is usually beneficial in case of ResNet but is less helpful or even harmful for attention-based architectures. As has already been noted, deeper analysis of such behavior is complicated due to unclear statistical properties of the PriSM sampling procedure and we leave this fo future work. One possible reason is that the approximation given by Wallenius' distribution may be less precise in some cases.

Our strategies presented in Sec. 3 in most cases outperform the baselines for all datasets but Shakespeare where Top-\(n\) demonstrates superior results, although the difference in performance is not significant. As the results imply, the Collective strategy is more suitable if the keep ratio is small, while for larger \(r\) the Unbiased strategy seems more preferable. This may be explained by the negative influence of too large auxiliary multipliers which also amplify the impact of less informative terms during the forward pass; see Sec. 3.3.

Interestingly, in all experiments, the novel sampling strategies significantly improve the results of baselines if those had a large accuracy gap compared to the full model. For the Shakespeare dataset, in contrast, all the low-rank methods achieve performance close to the full model even for small values of the keep ratio. Therefore, it seems that the performance on Shakespeare is determined more by the properties of the data split and the corresponding federated setup than by the training strategy.

#### 4.2.2 Discussion

**Explorative and exploitative strategies.** To understand the behavior of the strategies better, we propose a tool named _average normalized marginal entropy_ (ANME). For each decomposed layer with

   Strategy &  \\  Top-n & \(1.500\) & \(5.000\) \\ Top-n + Scaled & \(21.13_{2.01}\) & \(32.22_{1.74}\) \\ Top-n + Scaled & \(14.13_{2.36}\) & \(39.63_{2.82}\) \\ PRSM + Scaled & \(18.65_{2.51}\) & \(31.58_{1.12}\) \\ PRSM + Scaled & \(28.85_{5.57}\) & \(57.08_{1.08}\) \\ PRSM + Walkenius & \(37.75_{1.04}\) & \(59.81_{1.07}\) \\ PRSM + Walkenius + CliffR & \(40.96_{0.60}\) & \(60.02_{0.02}\) \\ Unbiased & \(35.12_{2.01}\) & \(59.44_{1.09}\) \\ Collective & \(37.83_{5.12}\) & \(460.26_{0.33}\) \\   

Table 2: **Longer training on CIFAR-100.** Despite our ‘unbiased’ modification of PriSM demonstrated the best accuracy in case of limited computation budget, the more explorative Collective strategy closes the gap in performance if the number of communication rounds is increased.

   Strategy &  \\  Top-n & \(21.500\) & \(5.000\) \\ Top-n & \(21.13_{2.01}\) & \(32.22_{1.74}\) \\ Top-n + Scaled & \(14.13_{2.36}\) & \(39.63_{2.82}\) \\ PRSM & \(18.65_{2.51}\) & \(31.58_{1.12}\) \\ PRSM + Scaled & \(28.85_{5.57}\) & \(57.08_{1.08}\) \\ PRSM + Walkenius & \(37.75_{1.04}\) & \(59.81_{1.07}\) \\ PRSM + Walkenius + CliffR & \(40.96_{0.60}\) & \(60.02_{0.02}\) \\ Unbiased & \(35.12_{2.01}\) & \(59.44_{1.09}\) \\ Collective & \(37.83_{5.12}\) & \(460.26_{0.33}\) \\   

Table 1: **Accuracy achieved by different strategies under limited computational budget.** Our _Unbiased_ and _Collective_ strategies outperform the vanilla _Top-\(n\)_ and _PriSM_ baselines on all datasets except for Shakespeare, although the gap is not significant for that dataset. In addition, the modifications proposed for local training allow PriSM to significantly improve its performance for ResNet architecture, and sometimes even surpass other methods in the current setting.

   Strategy & ResNet & CCT \\  _Keep ratio_\(r=0.1\) & & \\ Unbiased & \(35.12_{2.11}\) & \(45.01_{1.07}\) \\ Unbiased w/o CliplR & \(30.14_{1.15}\) & \(32.46_{0.71}\) \\ _Zero ratio_\(r=0.2\) & \(46.34_{0.02}\) & \(53.39_{0.81}\) \\ Unbiased & \(46.31_{1.12}\) & \(46.67_{0.85}\) \\ Unbiased & \(46.67_{0.85}\) & \(46.31_{1.12}\) & \(46.67_{0.85}\) \\   

Table 3: **Influence of the clipped learning rate.** In our experiments, we find that clipping the effective learning rate is beneficial for the Unbiased strategy in case of all architectures and values of the keep ratio \(r\). Without clipping the performance drops consistently.

inclusion probabilities \((_{1},,_{N})\) we calculate the mean entropy of the corresponding marginal Bernoulli distributions, \(_{:\,0<_{i}<1}(-_{i}_{i}-(1-_{i}) (1-_{i})).\) It is easy to show that the minimum value of this quantity equals 0 and is achieved by the deterministic Top-\(n\) strategy. The maximum value is achieved when all probabilities are equal, i.e. \(_{i}=\). Therefore, we normalize the mean entropy from above by dividing it by the mean entropy of the uniform inclusion probabilities. To finish the computation of ANME, we average this normalized entropy across all sharded layers in the network.

Intuitively, low values of ANME mean that the server tends to send the same terms to all the clients, and we name such strategies _exploitative_. On the contrary, high value of ANME shows that the strategy is _explorative_ and the selection of terms for each client is'more random'. We observe the same qualitative behaviour in all our experiments (see Fig. 0(b)): PriSM is the most explorative strategy, while the modification based on unbiasedness turns it into the most exploitative among randomized methods. The Unbiased and Collective strategies are between these two extreme types of behaviour, and the latter is usually more exploitative than the former one. Based on this observation, we check if training with more communication rounds can help the explorative strategies to perform better and report the results in Tab. 2. Indeed, all methods improve with more training and the largest improvement is for PriSM, which is the most explorative. We also observe that our collective strategy overtakes our modified PriSM baseline and achieves the best performance.

**Influence of learning rate clipping.** In all the experiments we found that clipping the effective learning rate is necessary for our strategies. Tab. 3 illustrates this observation by using models trained on CIFAR-100. Notably, Tab. 1 demonstrates that, empirically, such clipping is not always beneficial for the modified PriSM method. This may be explained by the fact that 'unbiased' PriSM becomes extremely exploitative. In this case, terms which could produce too large auxiliary multipliers are very unlikely to be sampled in practice, resulting in the large variance of the left-hand side of Eq. (9).

**More results.** Please refer to the Appendix A for additional results on model convergence, post-client updates and influence of data heterogeneity.

## 5 Conclusion

We presented two novel sampling strategies for spectral model sharding, grounded as solutions to specific optimization problems. Alongside them, we presented techniques that facilitate local training with such methods. As shown, in a number of cases these techniques can also significantly improve the performance of the strategies proposed earlier in the literature. Nonetheless, our strategies demonstrate superior performance on a number of commonly used datasets in the presence of high data heterogeneity between clients. As a downside, in certain cases their learning curve is less steep than that of the baselines due to their more explorative nature. We leave the improvement of the convergence speed for future work.

Figure 1: **Communication efficiency. The original PriSM method is too explorative (high ANME), while our ‘unbiased’ modification (+_Wallenius_) makes it the most exploitative strategy and allows to achieve the best performance in some experiments with limited computational budget.**