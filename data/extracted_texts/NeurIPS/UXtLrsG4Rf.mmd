# Patrick Rubin-Delanchy\({}^{2}\)

Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks

 Alexander Modell\({}^{1}\) Ian Gallagher\({}^{2}\) Emma Ceccherini\({}^{2}\) Nick Whiteley\({}^{2}\)\({}^{1}\)Imperial College London, U.K. \({}^{2}\)University of Bristol, U.K.

a.modell@imperial.ac.uk, {ian.gallagher,emma.ceccherini,

nick.whiteley,patrick.rubin-delanchy}@bristol.ac.uk

###### Abstract

We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples \((i,j,t)\), each representing a time-stamped (\(t\)) interaction between two entities (\(i,j\)), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm 'borrowing strength' across the network.

## 1 Introduction

Making sense of patterns of connections occurring over time is a common theme of modern data analysis and is often approached in one of two ways. On the one hand, we may see dynamic network data as a _graph_, in which connections between the same entities over time are somehow treated as one, e.g. through weighting. This view evokes methodological ideas such as community detection , topological data analysis , or manifold learning . On the other, we may see the data as a set of _point processes_, each modelling the event times of connections between two entities. This view evokes temporal notions such as trend, changepoints and periodicity.

In this paper, we develop a representation learning framework for continuous-time dynamic network data in which ideas from both the graph and temporal domains can be combined. The framework obtains a continuously evolving trajectory for each node which represents its continuously evolving behaviour in the network. These trajectories can be used for data exploration, inference and prediction tasks such as spatio-temporal clustering, behavioural analytics, detecting network trends and periodicities, and forecasting. Our framework provides two basic guarantees which could reasonably be viewed as minimum requirements for these tasks:

1. _structural coherence_: when two nodes exhibit similar behaviour at a point in time, their representations at that time are close;2. _temporal coherence_: when a node exhibits similar behaviours at two points in time, its representations at those times are close.

Despite this, almost all existing embedding algorithms fail to satisfy both properties, and there is a perception that some trade-off between the two is necessary [7; 8]. To our knowledge, unfolded spectral embedding [9; 10] is the only exception, but the method is limited to discrete time.

At its core, our procedure is a spectral method and can be implemented at scale using a sparse singular value decomposition. We give a preliminary motivation for the algorithm as minimising a notion of reconstruction error, before developing more rigorous estimation theory: a non-asymptotic, uniform error bound under minimal assumptions on the data generating process, which draws on recent developments in entrywise eigenvector estimation for random matrices [10; 11; 12; 13; 14]. This tight control of the behaviour of estimated trajectories means that even quite noise-sensitive follow-on analyses, such as topological data analysis, could plausibly be shown to be consistent.

With appropriate development, the ideas of this paper could be brought to bear on several problems. The first is building a coherent narrative about the nodes based on an embedding. In contact-tracing data for interactions between children at school (Section 5), we see trajectories mixing during lunch hours and then _returning_ to their earlier positions, which represent physical classrooms. Both types of coherence are necessary for this description to be possible. We cannot see this, for example, just by looking at the pattern of clustering over time. Given richer and larger datasets it could be possible to automate this "story-telling" process. Among many possible applications, such a technology could transform how we use contact-tracing data in the future. Second, the jump from discrete to continuous time has substantial implications for mathematical modelling, since it makes notions such as continuity and smoothness possible. This work could pave the way to describing structural dynamics in formal mathematical and quantitative terms, for example, what we mean by "communities splitting" , including the _exact time_ at which this occurs, or by "network polarisation" , and how it could be measured, e.g. as a type of derivative. Finally, in criminal investigations, dynamic networks are often analysed to locate an entity, such as a victim of human-trafficking, whose pseudonym or other identifier is changing . Similarly, in dynamic networks of corporate contracting, it is useful to detect when one company is acting like another in the past, e.g. when a shell company takes over the illegal operations of a sanctioned company . Since two nodes acting the same way, at different epochs, are still embedded to the same position, our framework could lead to novel matching and tracking technologies.

Related work.While modelling and performing inference on continuous-time dynamic networks has a well-established literature [6; 19; 20; 21], the majority of existing methods for representation learning obtain a single, static representation of each node [22; 23; 24; 25; 26; 27; 28; 29; 30], and we only aware of two existing methods which learn continuously evolving node representations from the data we consider [31; 32]. Representation learning for discrete-time dynamic networks has a more established literature, and algorithms broadly fall under community detection methods [33; 34; 35; 36], fitting latent position models [37; 38; 39], spectral methods [9; 10; 40; 41; 42] and word-embedding-based methods [43; 44]. For a specific choice of intensity estimator (the histogram), our method can be viewed as a weighted graph analogue of unfolded spectral embedding [9; 10], a spectral method for discrete-time and multilayer networks, but those papers consider different data and models. Given how limited the options are for handling continuous time, in our method comparison we also include some discrete-time methods which could reasonably be used as alternatives.

## 2 Intensity Profile Projection

**Data.** We consider dynamic network data, denoted \(\), representing instantaneous undirected interactions between nodes over time, which we define formally as \(=(,)\) on a time domain \(=(0,T]\), containing a vertex set \(=[n]\) and a set of triples \(=\{(i_{e},j_{e},t_{e})\}_{e 1}\), each corresponding to an undirected interaction event, where \(i_{e}<j_{e},t_{e}\). We let \(_{ij}:=\{t:(i,j,t)\}\) denote the interaction events between nodes \(i\) and \(j\).

**Model.** We assume the interaction events \(_{ij}\) are driven by an independent inhomogeneous Poisson process with intensity \(_{ij}(t)\). Informally:

\[_{ij}(t)t=\{t]$}\}.\]We represent these intensities in a symmetric time-varying matrix \(():_{+}^{n n}\).

**Procedure.** Intensity Profile Projection can be summarised as follows.

1. **Intensity estimation.** Construct intensity estimates \(_{ij}()\) of \(_{ij}()\) from \(_{ij}\) for all \(i<j\).
2. **Subspace learning.** Compute the top \(d\) eigenvectors \(}_{d}=(_{1},,_{d})\) of \[}:=_{0}^{T}}^{2}(t) t,\] (1) where \(}(t)\) has symmetric entries \(_{ij}(t)\), and rows denoted \(_{i}(t)\) called _intensity profiles_.
3. **Projection.** For a query node \(i\) at time \(t\), project the intensity profile \(_{i}(t)\) onto the subspace spanned by \(_{1},,_{d}\), to obtain \(_{i}(t)=}_{d}^{}_{i}(t)\).

While we develop more principled statistical justifications for the procedure in future sections, it is inspired by a simple reconstruction argument. For an arbitrary \(d\)-dimensional subspace spanned by the orthonormal columns of a matrix \(_{d}^{n d}\), let

\[_{i}(t;_{d}):=\|_{d}_{d}^{} _{i}(t)-_{i}(t)\|_{2}\]

denote the reconstruction error of node \(i\) at time \(t\), and define the _integrated residual sum of squares_ as

\[^{2}(_{d}):=_{0}^{T}_{i=1}^{n}_{i}^{2 }(t;_{d})\,t.\]

**Lemma 1**.: _Among all \(d\)-dimensional subspaces of \(^{n}\), the column span of \(}_{d}\) minimises the integrated residual sum of squares criterion \(^{2}\)._

Lemma 1 may be viewed as a dynamic analogue to the classical Eckart-Young theorem on low-rank matrix approximation . A proof is given in Section E of the appendix.

### Intensity estimation

The choice of intensity estimator is left fully open, but our theory makes two important recommendations. First, there are computational gains to be made using sparse estimators for subspace learning. Second, the procedure borrows strength across the network, and can give precise representations even when the individual intensity estimates are noisy (e.g. inconsistent). In our experiments, we focus on standard non-parametric estimators such as the histogram or kernel smoothers, and choose kernels with finite support to induce sparse estimates.

### Subspace learning

The subspace learning step of our procedure involves the computation of an integral, and computing the eigendecomposition of the resulting dense matrix \(}\), both of which may be infeasible for large networks. If a sparse intensity estimator is employed in step 1 of the procedure and we approximate the integral (1) using a numerical quadrature scheme, then step 2 can be rephrased as a single sparse, truncated singular value decomposition, which can be computed quickly for very large networks using a efficient solver [46; 47].

Consider the numerical approximation

\[}_{b=1}^{B}}^{2}(t_{b})\] (2)

where \(t_{1}<<t_{B}\) are equally spaced points on \((0,T]\). The top \(d\) eigenvectors of the right-hand-side of (2) are then equal1 to the top \(d\) left singular vectors of the matrix

\[[}(t_{1})\ \ }(t_{2}) \ \ \ }(t_{B})],\] (3)

the row concatenation of \(}(t_{1}),,}(t_{B})\). This procedure is presented in Algorithm 1, and we discuss its computational complexity in Section D of the appendix.

### Projection

The inductive nature of the Intensity Profile Projection allows us to obtain representations \(_{i}(t)\) on demand, for example, the full trajectory for a particular node, or the representations of the entire graph at a point in time. It is possible to obtain representations for intensity profiles outside the training sample, corresponding to new nodes or times outside the training domain, allowing online inference. In practice, one will need to retrain occasionally, i.e. return to step 2, although we leave the discussion of this computational and statistical trade-off for future work (see, for example,  in the context of static networks).

## 3 Estimation theory

In this section, we develop estimation theory showing the sense in which \(_{i}(t)\) is a "good" estimator of \(X_{i}(t):=_{d}^{}_{i}(t)\) where \(_{d}=(u_{1},,u_{d})^{n d}\) is the matrix containing the top-\(d\) orthonormal eigenvectors of

\[:=_{0}^{T}^{2}(t)t.\]

In this section, we assume, without loss of generality, that \(=(0,1]\). We now introduce some quantities which appear in our main theorem. Firstly, we assume that each \(_{ij}()\) is Lipschitz with constant \(L\), and is upper bounded by \(_{}\). Secondly, we define the (reduced) condition number and the eigengap,

\[:=}{_{d}},:=_{d }-_{d+1},\]

respectively, where \(_{1}^{2}_{n}^{2}\) are eigenvalues of \(\). Finally, we introduce the subspace coherence parameter

\[:=}\|_{d}\|_{2,},\]

which is small when, informally, information about a single entry of \(\) is "spread out" across the matrix .

Rather than attempt to develop a theoretical framework encompassing all intensity estimators, we choose arguably the most rudimentary, the histogram, and we expect more powerful estimators will only improve matters. This choice of estimator is also attractive because it allows us pinpoint the crucial practical considerations at play.

Notation.We say an event \(E\) occurs _with overwhelming probability_ if \((E) 1-n^{-c}\) for any constant \(c>0\). We use \(a b\) to denote that \(a Cb\) where \(C>0\) is a universal constant which, when qualified with the prior probabilistic statement, may depend on the constant \(c\). Additionally, we write \(a b\) if \(a b\) and \(a b\).

We now state the assumptions we require for our theorem. Our first assumption is that the intensities are bounded.

**Assumption 1** (Bounded intensities).: The intensities are upper bounded by a constant which doesn't depend on the other quantities in the problem; i.e. \(_{} 1\).

Our second assumption is on the population integrated residuals. It ensures that the intensity profiles \(_{1}(t),,_{n}(t)\) do not deviate "too much" from a common low-dimensional subspace.

**Assumption 2** (Small population residuals).: The population residuals satisfy

\[r_{1}(t),,r_{n}(t)}^{5/2}n\]

for all \(t\).

Our third assumption is a technical condition on the eigengap which, broadly speaking, ensures that there is "enough signal".

**Assumption 3** (Enough signal).: The eigengap satisfies \((/}) n_{}\).

Our final assumption is on the bin size, and ensures that the bins are not chosen "too small".

**Assumption 4** (Large enough bins).: The number of bins satisfies \(M n_{}/^{3}n\).

These assumptions are weaker than those typically required in the literature (e.g. on stochastic block models and random dot product graphs ). To emphasise this point, consider the following stronger alternative assumptions which imply Assumptions 1, 2 and 3:

**Assumption 1a.** The intensities \(_{ij}(t)\) are of comparable order, i.e. \(_{ij}(t)\) for some \( 1\) and all \(i,j[n],t(0,1]\).

**Assumption 2a.** The matrix \(\) has rank \(d 1\); is incoherent, i.e. \( 1\); and its non-zero eigenvectors are of comparable order, i.e. \(_{1}_{d}>_{d+1}=0\).

It is immediate that Assumption 1a implies Assumption 1 and under Assumption 2a, the population residuals are all exactly zero, \( 1\) and \( n\), which implies Assumptions 2 and 3.

Assumption 4 requires that the expected number of events involving each node in each bin is at least of the order \(^{3}n\). This is analogous to the \( n\) degree growth required for perfect clustering under the binary stochastic block model. Since the latter is an information-theoretic bound  and the additional logarithmic powers in our work stem from the sub-exponential tails of the Poisson distribution, we do not think this assumption can be weakened.

We now state our main theorem, which under Assumptions 1-4, provides a non-asymptotic bound on the error between the learned representations and their population counterparts, which holds uniformly over the whole node-set and the time domain.

**Theorem 1**.: _Suppose that \(_{ij}(t)\) are histogram estimates with \(M\) equally-spaced bins and that Assumptions 1-4 hold. Then with overwhelming probability, there exists an orthogonal matrix \(\) such that_

\[_{i[n]}_{t(0,1]}\|_{i}(t)-X_{i}(t) \|_{2}L_{}}{M}+}d^{5/2}n.\] (4)

The proof of Theorem 1 is given in Section F of the appendix. As a corollary to Theorem 1, we state a simplified version of this result in which we replace Assumptions 1, 2 and 3 with the stronger Assumptions 1a and 2a. Since the Lipschitz constant \(L\) scales with the order of the intensities, and we define the quantity \(L_{0}\) satisfying \(L= L_{0}\) which is invariant to the rescaling of intensities.

**Corollary 1**.: _Suppose that \(_{ij}(t)\) are histogram estimates with \(M\) equally-spaced bins and that Assumptions 1a, 2a and 4 hold. Then with overwhelming probability, there exists an orthogonal matrix \(\) such that_

\[_{i[n]}_{t(0,1]}\|_{i}(t)-X_{i}(t) \|_{2} L_{0}}{M}+^{5/2}n.\] (5)

### A bias-variance trade-off

The first term in the bound corresponds to the bias between \(_{i}(t)\) and \(X_{i}(t)\), where \(_{i}(t)\) is a histogram approximation to \(X_{i}(t)\) (modulo orthogonal transformation, see Section F of the appendix). The second term corresponds to the variance of the estimate.

Theorem 1 gives some theoretical guidance on how to select the number of bins in the histogram estimator. For simplicity, we consider the setting of Corollary 1. Ignoring logarithmic terms in \(n\), the bound in (5) is optimised by choosing

\[M(n L_{0}^{2})^{1/3}.\]

Figure 1 illustrates this bias-variance trade-off with an example. We simulate a dynamic network with 100 nodes with common intensities \(_{ij}(t)=0.7\{2+(t)\}\), for all \(i,j\), on the time domain \((0,4]\).

The top row shows the population representation \(X_{i}(t)\) of a single node (gray) and its histogram approximation \(_{i}(t)\) (blue) for a variety of bin sizes. The more bins that are chosen, the smaller the bias and the more \(_{i}(t)\) resembles \(X_{i}(t)\). The bottom rows shows the histogram approximation \(_{i}(t)\), and the estimate \(_{i}(t)\) (orange) obtained using Intensity Profile Projection. The fewer bins that are chosen, the smaller the variance and the more that \(_{i}(t)\) resembles \(_{i}(t)\).

## 4 Structural and temporal coherence

For many practical inference tasks, it is desirable for a representation learning procedure to possess the following two properties:

* **Structural coherence.** If two nodes exhibit statistically indistinguishable behaviour at a given time, then their representations at that time are close. That is, if \(_{i}(t)=_{j}(t)\), then \(_{i}(t)_{j}(t)\);
* **Temporal coherence.** If a node exhibits statistically indistinguishable behaviour at two distinct points in time, then its representations at both these times are close. That is, if \(_{i}(s)=_{i}(t)\), then \(_{i}(s)_{i}(t)\).

Figure 1: A bias-variance trade-off. We simulate a network with common intensities \(_{ij}(t)=0.7\{2+(t)\}\) for all \(i,j\), and apply Intensity Profile Projection with a histogram intensity estimator with 5, 20, and 200 bins. In the ‘bias’ plots, the gray lines shows an estimand \(X_{i}(t)\), while the blue lines shows its histogram approximation. The discrepancy between the gray line and the blue line corresponds the bias of the Intensity Profile Projection estimator. In the ‘variance’ plots, the blues lines are as in the ‘bias’ plots and the orange line show the estimate obtains using Intensity Profile Projection into one dimension. The discrepancy between the blue line and the orange line corresponds the variance of the Intensity Profile Projection estimator.

It has been observed in a recent survey of  that almost all existing dynamic network embedding procedures possess only one of these properties, but not both.

In the following lemma, we formally define \(_{i}(s)_{j}(t)\) to mean that \(X_{i}(s)=X_{j}(t)\), referring to equality "up to statistical noise" in the sense of Theorem 1.

**Lemma 2**.: _Intensity Profile Projection is both structurally and temporally coherent._

This follows from the simple observation that \(X_{i}(t)\) is a fixed function of \(_{i}(t)\) for all \(i\) and \(t\). To the best of our knowledge, Intensity Profile Projection is the only existing continuous-time procedure which satisfies these desiderata.

### Simulated example: a bifurcating block model

To illustrate these properties, we simulate a two-community dynamic stochastic block model (i.e. where \((t)\) is block structured) in which the intra-community intensities and inter-community intensities are initially distinct, they then gradually merge, remain indistinguishable for some time, and finally diverge. We refer to this model as a _bifurcating block model_ and provide full details of the simulation in Section A of the appendix.

We apply Intensity Profile Projection to the simulated network, using both a histogram intensity estimator, and a kernel smoother, to produce two-dimensional representations. For visualisation, we reduce the dimension from two to one using a dynamic adaptation of principal component analysis (see Section C of the appendix), and the resulting representations are shown in Figures 2(a) and (b).

In both cases, the estimated trajectories mirror the underlying dynamics of the network: the two communities are in well separated to begin with, gradually merge, remain relatively constant before returning to the positions in which they started.

We now illustrate the potential pitfalls of some more naive approaches for embedding dynamic networks. We find that most existing methodology can be viewed as some combination of the two techniques:

* **Alignment**. Obtain a sequence of static snapshots of the network, embed each of the networks snapshots separately and subsequently align the embedding from window \(t+1\) with the embedding from window \(t\).
* **Averaging**. Obtain a static summary of the network by averaging it over time, and to embed this to obtain constant node representations.

Figure 2: One-dimensional PCA visualisation of the two-dimensional node representations, obtained using a collection of methods, for a network simulated from a bifurcating block model. Colours correspond to the community membership of the node.

Alignment is structurally coherent, however can fail to be temporally coherent. Averaging is temporally coherent, but can fail to be structurally coherent. To illustrate this point, we apply both approaches, using adjacency spectral embedding into two dimensions, orthogonal Procrustes alignment and linear interpolation, to a network simulated from the bifurcating block model. Figures 2(c) and (d) show visualisations of the trajectories obtained from each approach.

### Method comparison

In this section, we demonstrate how our procedure compares to some existing methods on the simulated data described above. Due to the limited number of continuous-time methods, we include a number of discrete-time methods (Omnibus, PisCSE and MultiNeSS) which we give as an input a discrete sequence of snapshots \((1),,(M)\) of our simulated continuous-time networks. We compare the following methods:

* **IPP (kernel smoothing)**. Algorithm 1 applied with intensities estimated using kernel smoothing.
* **IPP (histogram) / USE**. Algorithm 1 applied with intensities estimated using a histogram estimator. Equivalent to a weighted extension of the Unfolded Spectral Embedding algorithm of .
* **CLPM**. Fits a continuous latent position model \(_{ij}(t)=-\|Z_{i}(t)-Z_{j}(t)\|^{2}\) with a penalty on large velocities in the latent space.
* **Omnibus**. Approximately factorises the matrix \(\) with blocks \([k,l]=((k)+(l))\), using a spectral decomposition.
* **PisCES**. Minimises the objective function \[_{k=1}^{M}\|(k)-^{}(k)\|_{F}^{2}+_{k= 1}^{M-1}\|^{}(k)-^{}(k+1)\|_{}^{2}\,,\] for \(^{}(1),...,^{}(M)\), where \(\) and \((k)\) are the Laplacian normalisations of \((k)\). Then, approximately factorises each \(^{}(1),...,^{}(M)\) using spectral decompositions.
* **MultiNeSS**. Fits a latent position model \(_{ij}(k) Q\{;f(Z_{i}(k),Z_{j}(k)),\}\), where \(Q(;,)\) is a parametric distribution.

We use an embedding dimension of \(d=2\) for all methods, and for visualisation we reduce this to one using PCA. Additional details such as hyperparameter selection, where applicable, are given in the Section A of the appendix.

The CLPM and Omnibus methods produce representations which are temporally coherent, however both fail to capture the complete merging of the communities, shown by Figures 2(e) and (f), and are therefore not structurally coherent. The PisCES and MultiNeSS methods produce representations which are structurally coherent, however both are unstable when the communities are indistinguishable, shown by Figures 2(g) and (h), and are therefore not temporally coherent.

## 5 Real data

We demonstrate Intensity Profile Projection on a dataset containing the face-to-face interactions of the pupils of a primary school in Lyon over two days in October 2009 . During the study, discreet radio-frequency identification devices were worn by 232 pupils and 10 teachers which recorded their face-to-face interactions. When two participants were in close proximity over an interval of 20 seconds, the timestamped interaction event was recorded. The school contains five year groups, each divided into two classes, and each class has an assigned room and an assigned teacher. The school day runs from 8:30am to 4:30pm, with a lunch break from 12:00pm to 2:00pm, and no data was gathered on contacts taking place outside the school or during sports activities. For more details about the study and dataset, we refer the reader to .

We apply Intensity Profile Projection to the data corresponding to each day of the study using a kernel smoother with an Epanechnikov kernel, choosing a bandwidth of 5 minutes and computing 30 dimensional trajectories.

Figure 4: Two-dimensional t-SNE visualisation of the 30-dimensional node representations of all pupils and teachers evaluated at 9:30am on Day 1, and 9:30am, 12:30pm and 3:30pm on Day 2.

Figure 3: One-dimensional PCA visualisation of the 30-dimensional node representations for pairs of classes in the same year group. The solid lines show the average trajectory for each class, and the dashed line show one standard deviation above and below.

To visualise the node trajectories, we first rescale them to have unit norm, which has the effect of removing information about the "activeness" of a node from its representation (see, for example, ), and apply two dimension reduction techniques. The first is principal component analysis (PCA), which we adapt to our dynamic setting by projecting the (centered) representations onto the the direction of maximum average variance over the time domain. This visualisation gives us a temporally coherent view on the trajectories (more details are given in Section C of the appendix). In Figure 3, we visualise the trajectories of each pair of classes in each year group using PCA, and for clarity, we just plot the average trajectory for each class, along with one standard deviation above and below.

The second is t-Distributed Stochastic Neighbor Embedding (t-SNE), a popular non-linear dimension-reduction tool which provides enough flexibility to visualise the whole set of representations at each point in time. Figure 4 shows t-SNE visualisations of the node representations at a collection of times throughout the study. In Section B of the appendix, we include analogous figures for aligned spectral embedding and Omnibus embedding for comparison.

Figure 3 clearly shows the mixing of classes during the lunch hours, and from Figures 4, we see that the the representations are much more fragmented during the lunch hour (12:30pm, Day 2) than they are during lessons at the other times, where they form tighter clusters corresponding to classes.

While it is reassuring that the geometry of the trajectories reflects the _known_ class and timetable structures of the school, it also allows us to uncover structure in the data that _was not known_ from the report on the study. For example, classes 5A and 5B (olive and cyan, respectively) merge into a single cluster at approximately 9:30am on Day 1, and classes 3A and 3B (brown and pink, respectively) do the same at approximately 9:30am on Day 2. One might conjecture that this corresponds to a joint lesson, which is taken by the students of both classes in a year group.

## 6 Discussion

We have presented an algorithmic framework to learn continuous-time, low-dimensional trajectories representing the evolving behaviours of nodes in a dynamic network. We view our framework as providing a platform on which novel inference procedures can be developed, particularly combining graph and temporal concepts. For example, in dynamic networks with continuously evolving community structure, it might be interesting to develop procedures for detecting branching points (see bifurcating block model example, Section 4), or measures of polarisation and cohesion in the network via the velocities of the trajectories. More generally, we believe there is much left to understand and exploit in the time-evolving topology and geometry of these representations.

A limitation of our framework is the need for bandwidth and dimension selection. These decisions are difficult because they are trade-offs, bias versus variance in the case of bandwidth selection (as seen here), and statistical versus computational in the case of dimension selection (see e.g. ). In the presence of a specific supervised downstream task, both decisions could be assisted by cross-validation. In unsupervised settings with reasonably-sized networks, our method is very fast, allowing expedient exploration of different choices.

Our theory suggests selecting a dimension which corresponds to an "eigengap" in the spectrum. In practice, it is possible than the spectrum does not decay quickly and no eigengap is present. This likely corresponds to the violation of Assumption 3. Some possible solutions are to take an entrywise transformation of the intensity profiles, such as the square root, to temper heavy tails , or to employ a robust subspace estimator, such as robust PCA .

Our method might be viewed as a dynamic analogue of adjacency spectral embedding for static graphs  and, as a result, in future research it could be profitable to find dynamic analogues of other variants of spectral embedding, e.g. applying Laplacian normalisation [59; 60; 61] or regularisation [54; 62].

We believe improved dynamic network analysis can be used for societal good, in applications such as cyber-security, or combating human-trafficking, fraud, and corruption. However, one should also be aware of the risks, particularly to individual privacy and targeted influence.