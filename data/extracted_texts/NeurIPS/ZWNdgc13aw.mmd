# NeoRL: Efficient Exploration for Nonepisodic RL

Bhavya Sukhija, Lenart Treven, Florian Dorfler, Stelian Coros, Andreas Krause

ETH Zurich, Switzerland

Correspondence to sukhijab@ethz.ch

###### Abstract

We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., adapt online and without resets. This setting is ubiquitous in the real world, where resetting is impossible or requires human intervention. We propose _Nonepisodic Optimistic RL_ (NeoRL), an approach based on the principle of optimism in the face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy assumptions on the system, we provide a first-of-its-kind regret bound of \((_{T}})\) for general nonlinear systems with Gaussian process dynamics. We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.

## 1 Introduction

In recent years, data-driven control approaches, such as reinforcement learning (RL), have demonstrated remarkable achievements. However, most RL algorithms are devised for an episodic setting, where during each episode, the agent interacts in the environment for a predetermined episode length or until a termination condition is met. After the episode, the agent is reset back to an initial state from where the next episode commences. Episodes prevent the system from blowing up, i.e., maintain stability, while also restricting exploration to states that are relevant to the task at hand. Moreover, resets ensure that the agent explores close to the initial states and does not end up at undesirable parts of the state space that exhibit low reward. In simulation, resetting is typically straightforward. However, if we wish to enable agents to learn and adapt by interacting online with the real world, resets are often prohibitive since they typically involve manual intervention. Instead, agents should be able to learn autonomously (Sharma et al., 2021) i.e., from a single trajectory. This problem is extensively studied in adaptive control (Astrom and Wittenmark, 2013), where classical works focus on controller design (Lai and Wei, 1982, 1987; Krstic et al., 1992, 1995; Annaswamy, 2023) and not on the exploration/learning aspect of the problem. Only a few works consider these two aspects jointly (Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz and Foster, 2020; Zhao et al., 2024). However, these works study linear systems with quadratic costs, i.e., the LQR setting. While several works in the Deep RL community have also studied this problem, (c.f., Section 5), the theoretical results for this setting are fairly limited. In particular, theoretical results mostly exist for the finite state and action spaces (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Jaksch et al., 2010) and the extension to nonlinear systems with continuous spaces is much less understood. In our work, we address this gap and propose a practical RL algorithm that is grounded in theory. In particular, we make the following contributions.

**Contributions**

1. We propose, NeoRL, a novel model-based RL algorithm based on the principle of optimism in the face of uncertainty. NeoRL operates in a nonepisodic setting and picks average cost optimal policies optimistically w.r.t. to the model's epistemic uncertainty.

2. We show that when the dynamics lies in a reproducing kernel Hilbert space (RKHS) of kernel \(k\), NeoRL exhibits a regret of \((_{T}_{T}})\), where the regret, akin to prior work, is measured w.r.t to the optimal average cost under known dynamics, \(T\) is the number of environment steps, \(_{T}\) the calibration coefficient (Chowdhury and Gopalan, 2017; Srinivas et al., 2012) and \(_{T}\) the maximum information gain of kernel \(k\)(Srinivas et al., 2012). Our regret bound is similar to the ones obtained in the episodic setting (Kakade et al., 2020; Curi et al., 2020; Sukhjia et al., 2024; Treven et al., 2024) and Gaussian process (GP) bandit optimization (Srinivas et al., 2012; Chowdhury and Gopalan, 2017; Scarlett et al., 2017) and is sublinear for common kernel such as the exponential kernel. To the best of our knowledge, we are the first to obtain regret bounds for the setting.
3. We evaluate NeoRL on several RL benchmarks against common model-based RL baselines. Our experimental results demonstrate that NeoRL consistently achieves sublinear regret, also when neural networks are employed instead of GPs for modeling dynamics. Moreover, in all our experiments, NeoRL converges to the optimal average cost.

## 2 Problem Setting

We consider a discrete-time dynamical system with running costs \(c\).

\[_{t+1}=^{*}(_{t},_{t})+_{t}, (_{t},_{t}),\ (0)=_{0}\] (1) \[c(,)_{ 0}\] (Running cost)

Here \(_{t}^{d_{}}\) is the state, \(_{t}^{d_{}}\) the control input, and \(_{t}^{}\) the process noise. The dynamics \(^{*}\) are unknown and the cost \(c\) is assumed to be known.

TaskIn this work, we study the average cost RL problem (Puterman, 2014), i.e., we want to learn the solution to the following minimization problem

\[A(^{*},_{0})=_{}A(,_{0})=_{}_{T}_{}[_{t=0}^{T-1}c(_{t},_{t})].\] (2)

Moreover, we consider the nonepisodic RL setting where the system starts at an initial state \(_{0}\) but never resets back during learning, that is, we seek to learn online from a single trajectory. After each step \(t\) in the environment, the RL system receives a transition tuple \((_{t},_{t},_{t+1})\) and updates its policy based on the data \(_{t}\) collected thus far during learning. The average cost formulation is common for the nonepisodic setting (Jaksch et al., 2010; Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz and Foster, 2020), and the cumulative regret for the learning algorithm in this case is defined as

\[R_{T}=_{t=0}^{T-1}_{_{t},_{t}|_{0}}[c(_{ t},_{t})-A(^{*},_{0})].\] (3)

Studying the average cost criterion for general continuous state-action spaces is challenging even when the dynamics are known, since the average cost exists only for special classes of nonlinear systems (Arapostathis et al., 1993). In the following, we impose assumptions on the dynamics and policy class \(\) that enable our theoretical analysis.

### Assumptions

Imposing continuity on \(^{*}\) is quite common in the control theory (Khalil, 2015) and reinforcement learning literature (Curi et al., 2020; Sussex et al., 2023; Sukhjia et al., 2024). To this end, for our analysis, we make the following assumption.

**Assumption 2.1** (Continuity of \(^{*}\) and \(\)).: The dynamics model \(^{*}\) and all \(\) are continuous.

Next, we make an assumption on the system's stochastic disturbances.

**Assumption 2.2** (Process noise distribution).: The process noise is i.i.d. Gaussian with variance \(^{2}\), i.e., \(_{t}(,^{2})\).

Our analysis can be extended for the more general heteroscedastic case, where \(\) depends on \(\). However, for simplicity, we focus on the homoscedastic setting. In the following, we make assumptions on our policy class. To this end, we first introduce the class of \(_{}\) functions.

**Definition 2.3** (\(_{}\)-functions).: The function \(:_{ 0}_{ 0}\) is of class \(_{}\), if it is continuous, strictly increasing, \((0)=0\) and \((s)\) for \(s\).

**Assumption 2.4** (Policies with bounded energy).: We assume there exists \(,_{}\), positive constants \(K,C_{u},C_{l}\) with \(C_{u}>C_{l}\), and \((0,1)\) such that for each \(\) we have,

_Bounded energy:_ There exists a Lyapunov function \(V^{}:[0,)\) for which \(,^{}\),

\[|V^{}()-V^{}(^{})|& (\|-^{}\|)&\\ C_{l}(\|\|)& V^{}()  C_{u}(\|\|)&\\ _{_{+}|,}[V^{}(_{+})]&  V^{}()+K&\]

where \(_{+}=^{*}(,())+\).

_Bounded norm of cost:_

\[_{},())}{1+V^{}( )}<\]

_Boundedness of the noise with respect to \(\):_

\[_{}[(\|\|)]<,\ _{} [^{2}(\|\|)]<\]

The drift condition states that the energy between two timesteps can increase at most by \(K\). In particular, the Lyapunov function \(V^{}\) can be viewed as an energy function for the dynamical system, and the bounded energy condition above ensures that the system is not "blowing up". We do not perceive this as restrictive for real-world engineered systems. Other works that study learning nonlinear dynamics (Foster et al., 2020; Sattar and Oymak, 2022; Lale et al., 2021) in the nonepisodic setting also make stability assumptions such as global exponential stability for their analysis. In similar spirit, we make the bounded energy assumption for our policy class. The drift condition on the Lyapunov function is also used to study the ergodicity of Markov chains for continuous state spaces (Meyn and Tweedie, 2012; Hairer and Mattingly, 2011), which is crucial for our analysis of the infinite horizon behavior of the system. Moreover, for a very rich class of problems, the drift condition is satisfied. We highlight this in the corollary below.

**Lemma 2.5**.: _Assume \(^{*}\) is uniformly continuous and for all \(\), \(\), \(\|()\| u_{}\). Further assume, there exists \(_{s}\) such that we have constants \(K,C_{u},C_{l}\) with \(C_{u}>C_{l}\), \((0,1)\), \(,_{}\) and a Lyapunov function \(V:[0,)\) for which \(,^{}\),_

\[|V()-V(^{})|& (\|-^{}\|)\\ C_{l}(\|\|)& V() C_{u}( \|\|)\\ _{_{+}|,_{s}}[V(_{+})]&  V()+K,\]

_where \(_{+}=^{*}(,())+\). Then, \(V\) also satisfies the drift condition for all \(\), i.e., is a Lyapunov function for all policies._

We prove this lemma in Appendix A. Intuitively, if the inputs are bounded, the energy inserted into the system by another policy is also bounded. Nearly all real-world systems have bounded inputs due to the physical limitations of actuators. For these systems, it suffices if only one policy in \(\) satisfies the drift condition.

The boundedness assumptions for the cost and the noise in Assumption 2.4 are satisfied for a rich class of cost and \(_{}\) functions.

Under these assumptions, we can show the existence of the average cost solution.

**Theorem 2.6** (Existence of Average Cost Solution).: _Let Assumption 2.1 - 2.4 hold. Consider any \(\) and let \(P^{}\) denote its transition kernel, i.e., \(P^{}(,)=(_{+}|, ())\) for \(\). Then \(P^{}\) admits a unique invariant measure \(^{}\), and there exists \(C_{2},C_{3}(0,)\), \((0,1)\) such that_

Average Cost;

\[A()=_{T}_{}[_{t=0}^{ T-1}c(_{t},_{t})]=_{^{}} [c(,(x))]\]Bias Cost; _Letting \(B(,_{0})=_{T}_{}[_{t=0}^{T-1 }c(_{t},_{t})-A()]\) denote the bias, we have_

\[|B(,_{0})| C_{2}(1+V^{}(_{0}))\]

_for all \(_{0}\)._

Theorem 2.6 is a crucial result for our analysis since it implies that the average cost is bounded and _independent of the initial state_\(_{0}\). Furthermore, it also shows that the bias is bounded. The average cost criterion satisfies the following Bellman equation (Puterman, 2014) below

\[B(,)+A()=c(,())+_{_{ +}}[B(,_{+})|,]\] (4)

Accordingly, the bias term plays an important role in the regret analysis (also notice its similarity to our regret term in Equation (3)).

Thus far, we have only made assumptions that make the average cost problem tractable. In the following, we make an assumption on the dynamics that allow us to learn it from data. Moreover, we assume that at each step \(n\) we learn a mean estimate \(_{n}\) of \(^{*}\) and can quantify our uncertainty \(_{n}\) over the estimate. More formally, we learn a well-calibrated statistical model of \(^{*}\) as defined below.

**Definition 2.7** (Well-calibrated statistical model of \(^{*}\), Rothfuss et al. (2023)).: Let \(}}{{=}}\). An all-time well-calibrated statistical model of the function \(^{*}\) is a sequence \(\{_{n}()\}_{n 0}\), where

\[_{n}()}}{{=}}\{ :^{d_{x}}, j  1,,d_{x}:|_{n,j}()-f_{j}()|_{n}()_{n, j}()\},\]

if, with probability at least \(1-\), we have \(^{*}_{n 0}_{n}()\). Here, \(f_{j}\), \(_{n,j}\) and \(_{n,j}\) denote the \(j\)-th element in the vector-valued functions \(\), \(_{n}\) and \(_{n}\) respectively, and \(_{n}()_{ 0}\) is a scalar function that depends on the confidence level \((0,1]\) and which is monotonically increasing in \(n\).

Next, we assume that \(^{*}\) resides in a Reproducing Kernel Hilbert Space (RKHS) of vector-valued functions and show that this is sufficient for us to obtain a well-calibrated model.

**Assumption 2.8**.: We assume that the functions \(f_{j}^{*}\), \(j 1,,d_{x}\) lie in a RKHS with kernel \(k\) and have a bounded norm \(B\), that is \(^{*}_{k,B}^{d_{x}}\), with \(_{k,B}^{d_{x}}=\{\|f_{j}\|_{k} B,j=1, ,d_{x}\}\). Moreover, we assume that \(k(,)_{}\) for all \(\).

Assumption 2.8 allows us to model \(^{*}\) with GPs for which the mean and epistemic uncertainty (\(_{n}()=[_{n,j}()]_{j d_{x}}\), and \(_{n}()=[_{n,j}()]_{j d_{x}}\)) have an analytical formula

\[_{n,j}() =_{n}^{}()(_{n}+^{2})^{-1}_{1:n}^{j},\] (5) \[_{n,j}^{2}() =k(,)-_{n}^{}()(_{n}+^{2} )^{-1}_{n}(),\]

Here, \(_{1:n}^{j}\) corresponds to the noisy measurements of \(f_{j}^{*}\), i.e., the observed next state from the transitions dataset \(_{1:n}\), \(_{n}=[k(,_{i})]_{i nT}\), \(_{i}_{1:n}\), and \(_{n}=[k(_{i},_{l})]_{i,l nT}\), \(_{i},_{l}_{1:n}\) is the data kernel matrix. The restriction on the kernel \(k(,)_{}\) implies boundedness of \(^{*}\) and has also appeared in works studying the episodic setting for nonlinear systems (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Wagenmaker et al., 2023). We can also define \(^{*}\) such that \(_{k}=_{k-1}+^{*}(_{k-1},_{k-1})+_{k-1}\) in which case the boundedness of \(^{*}\) captures many real-world systems.

**Lemma 2.9** (Well calibrated confidence intervals for RKHS, Rothfuss et al. (2023)).: _Let \(^{*}_{k,B}^{d_{x}}\). Suppose \(_{n}\) and \(_{n}\) are the posterior mean and variance of a GP with kernel \(k\), c.f., Equation (5). There exists \(_{n}()}\), for which the tuple \((_{n},_{n},_{n}())\) is a well-calibrated statistical model of \(^{*}\)._

In summary, in the RKHS setting, a GP is a well-calibrated model. For more general models like Bayesian neural networks (BNNs), methods such as Kuleshov et al. (2018) can be used for calibration. Our results can also be extended beyond the RKHS setting to other classes of well-calibrated models similar to Curi et al. (2020).

## 3 NeoRL

In the following, we present our algorithm: **N**onepisodic **O**ptimistic **RL** (NeoRL) for efficient nonepisodic exploration in continuous state-action spaces. NeoRL builds on recent advances in episodic RL (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Treven et al., 2024) and leverages the optimism in the face of uncertainty paradigm to pick policies that are optimistic w.r.t. the dynamics within our calibrated statistical model as follows

\[(_{n},_{n})*{}}{{=}}}*{}_{,\, _{n-1}_{0}}A(,).\] (6)

Here, \(_{n}\) is a dynamical system such that the cost by controlling \(_{n}\) with its optimal policy \(_{n}\) is the lowest among all the plausible systems from \(_{n-1}_{0}\). Note, from Lemma 2.9 we have that \(^{*}_{n-1}_{0}\) (with high probability) and therefore the solution to Equation (6) gives an optimistic estimate for the average cost. We take the intersection of \(_{n-1}\) with \(_{0}\) to ensure that we maintain at least the same confidence about our model as at the beginning, i.e., \(n=0\), during learning. NeoRL proceeds in the following manner. Similar to Jaksch et al. (2010), we bin the total time \(T\) the agent spends interacting in the environment into \(N\) "artificial" episodes. At each episode, we pick a policy according to Equation (6) and roll it out for \(H_{n}\) steps on the system. Next, we use the data collected during the rollout to update our statistical model. Finally, we double the horizon \(H_{n+1}=2H_{n}\), akin to Simchowitz and Foster (2020), and continue to the next episode _without resetting_ the system back to the initial state \(_{0}\). Intuitively, in the beginning, when our model estimate is not accurate, we update our model more frequently, and with more episodes as our model gets better we reduce the frequency of updates. The algorithm is summarized in Algorithm 1.

``` Init: Aleatoric uncertainty \(\), Probability \(\), Statistical model \((_{0},_{0},_{0}())\), \(H_{0}\) for\(n=1,,N\)do \(_{n}=*{}_{}\,_{ _{n-1}_{0}}A(,)\) \(H_{n}=2H_{n-1}\) \(_{n}(_{n})\) \(\) Select measurements for horizon \(H_{n}\)  Update \((_{n},_{n},_{n})_{n}\) \(\) Update statistical model \(_{n}\) endfor ```

**Algorithm 1**NeoRL:**N**onepisodic Optimistic **RL**

### Theoretical Results

In the following, we study the theoretical properties for NeoRL and provide a first-of-its-kind bound on the cumulative regret for the average cost criterion for general nonlinear dynamical systems. Our bound depends on the _maximum information gain_ of kernel \(k\)(Srinivas et al., 2012), defined as

\[_{T}(k)=_{;| | T}|+^{-2}_{T}|.\]

\(_{T}\) represents the complexity of learning \(^{*}\) from \(T\) data points and is sublinear for a very rich class of kernels (e.g., \((^{d_{x}+d_{u}+1}(T))\) for the exponential (RBF) kernel, \(((d_{x}+d_{u})(T))\) for the linear kernel). In Appendix A, we report the dependence of \(_{T}\) on \(T\) in Table 1.

**Theorem 3.1** (Cumulative Regret of NeoRL).: _Let Assumption 2.1 - 2.8 hold, and define \(H_{0}\) as the smallest integer such that_

\[H_{0}>/C_{t})}{(1/)}.\]

_Then with probability at least \(1-\), we have the following regret for NeoRL_

\[R_{T} D_{4}(_{0},K,)_{T}}+D_{5}(_{ 0},K,)_{2}(}+1).\] (7)

_with \(D_{4}(_{0},K,)\), \(D_{5}(_{0},K,)\) being bounded constants for bounded \(\|_{0}\|\), \(K\), and \(<1\)._From Lemma 2.9 we have that \(_{T}}\) and therefore Theorem 3.1 gives sublinear regret for a rich class of RKHS functions. Moreover, it also gives a minimal horizon \(H_{0}\) that we need to maintain before switching to the next policy. Even for the linear case, fast switching between stable controllers can destabilize the closed-loop system. We ensure this does not happen in our case by having a minimal horizon of \(H_{0}\). Theorem 3.1 can also be derived beyond the RKHS setting for a more general class of well-calibrated models. In this case, the maximum information gain is replaced by the model complexity from Curi et al. (2020) (c.f., Curi et al. (2020); Sukhija et al. (2024) for further detail).

In the following, we give an intuitive proof sketch for Theorem 3.1. The detailed proof is provided in Appendix A.

Proof sketchThe proof can be split into three main steps. First, we show the ergodicity of the closed-loop system, a sufficient condition for showing the existence of the average cost and bias term, i.e., Theorem 2.6, for every policy \(\) under Assumption 2.1 - 2.4. For this, we use elementary results on Markov chains in measurable spaces from Meyn and Tweedie (2012); Hairer and Mattingly (2011). Second, we show that under Assumption 2.8, the optimistic system selected in Equation (6), retains the same properties as the true system \(^{*}\), e.g., stability, and therefore also is ergodic. Crucial to show this is that the true system \(^{*}\) and the optimistic system \(_{n}\) are at most \(_{n}_{n}\) apart. Finally, in the third step, we show that as we update our model and policy every \(H_{n}\) steps, the doubling of the horizon retains the system properties from above, and our accumulated model uncertainties across \(T\) environment steps grow with the rate \(_{T}\). For the latter, we use the analysis from Kakade et al. (2020) for the episodic case, to bound the deviation between the optimistic average cost and the true average cost.

### Practical Modifications

For testing NeoRL, we make three modifications that simplify its deployment in practice in terms of implementation and computation time. First, instead of doubling the horizon \(H_{n}\) we pick a fixed horizon \(H\) during the experiment. This makes the planning and training of the agent easier. Next, we use a receding horizon controller, i.e., model predictive control (MPC) (Garcia et al., 1989), instead of directly optimizing for the average cost in Equation (6). MPC is widely used to obtain a feedback controller for the infinite horizon setting. Moreover, while for linear systems, the Riccati equations (Anderson and Moore, 2007) provide an analytical solution to Equation (2), no such solution exists for the nonlinear case and MPC is commonly used as an approximation. Further, under additional assumptions on the cost and dynamics, MPC also obtains a policy with bounded average cost, which is crucial for the nonepisodic case (c.f., Assumption 2.4). We use the iCEM optimizer for planning (Pinneri et al., 2021). Finally, instead of optimizing over \(_{n}_{0}\), we optimize directly over \(_{n}\). This allows us to use the reparameterization trick from Curi et al. (2020) and obtain a simple and tractable optimization problem. In summary, for each step \(t\) in the environment, we solve the following optimization problem

\[_{_{0:H_{^{-1}}},_{0:H_{^ {-1}}}}[_{h=0}^{H_{^{-1}}}c(}_{h}, {u}_{h})],\] (8) \[\ }_{h+1}=_{n-1}(}_{h}, {u}_{h})+_{n-1}()_{n-1}(}_{h},_{h})_{h}+_{h}\ }_{0}=_{t}.\]

Here \(H_{}\) is the MPC horizon. We take the first input from the solution of the problem above, i.e., \(_{0}^{*}\), and execute this in the system. We then repeat this procedure for \(H\) steps and then update our statistical model \(_{n}\). The resulting optimization above considers a larger action space as it includes the hallucinated controls \(\) as additional input variables. The hallucinated controls are introduced through the reparameterization trick from (Curi et al., 2020) and are used to directly optimize over models in \(_{n-1}\). Moreover, the final algorithm can be seen as a natural extension to H-UCRL (Curi et al., 2020) for the nonepisodic setting. We summarize the algorithm in Appendix B Algorithm 2. Note while these modifications deviate from our theoretical analysis, empirically they work well for GP and BNN models, c.f., Section 4.

## 4 Experiments

We evaluate NeoRL on the Pendulum-v1 and MountainCar environment from the OpenAI gym benchmark suite (Brockman et al., 2016), Cartpole, Reacher, and Swimmer from the DeepMind control suite (Tassa et al., 2018), the racecar simulator from Kabzan et al. (2020), and a soft robotic arm from Teknialp et al. (2024). The swimmer and the soft robotic arm are fairly high-dimensional systems - the swimmer has a 28-dimensional state and 5-dimensional action space, and the soft arm is represented by a 58-dimensional state and has a 12-dimensional action space. All environments are never reset during learning. Moreover, the Pendulum-v1, MountainCar, CartPole, and Reacher environments operate within a bounded domain and thus inherently satisfy Assumption 2.4. The swimmer, racecar, and soft arm can operate in an unbounded domain but have a cost function that penalizes the distance between the system's state \(_{t}\) and a target state \(^{*}\). Therefore, the cost encourages the system to move towards the target and remain within a bounded domain.

**Baselines** In the episodic setting, resets can be used to control the exploration space for the agent. However, in the absence of resets, the agent can explore arbitrarily and end up in states that are irrelevant to the task at hand. Moreover, the agent has to follow an uninterrupted chain of experience, which makes the nonepisodic setting the most challenging one in RL (Kakade, 2003). Accordingly, there are only a few algorithms that consider this setting (c.f., Section 5). In this work, we focus on model-based RL (MBRL) algorithms due to their sample efficiency. In particular, we adopt common MBRL methods for our setting. MBRL algorithms typically differentiate in three ways; (\(t\)) propagating dynamics for planning (Chua et al., 2018; Osband and Van Roy, 2017; Kakade et al., 2020; Curi et al., 2020), (\(ii\)) representation of the dynamics model (Ha and Schmidhuber, 2018; Hafner et al., 2019; Kipf et al., 2019), and (\(iii\)) types of planners (Williams et al., 2017; Hafner et al., 2020; Pinneri et al., 2021). NeoRL is independent to the choice of representation or planners. Therefore, we focus on (\(i\)) and use probabilistic ensembles (Lakshminarayanan et al., 2017) and GPs for modeling our dynamics and MPC with iCEM (Pinneri et al., 2021) as the planner. Common techniques to propagate the dynamics for planning are using the mean, trajectory sampling (Chua et al., 2018), and Thompson

Figure 1: Average reward \(A()\) and cumulative regret \(R_{T}\) over ten different seeds for all environments. We report the mean performance with one standard error as shaded regions. During all experiments, the environment is never reset. For all baselines, we model the dynamics with probabilistic ensembles, except in the Pendulum-GP experiment, where GPs are used instead. NeoRL significantly outperforms all baselines and converges to the optimal average reward, \(A(^{*})=0\), showing sublinear cumulative regret \(R_{T}\) for all environments.

sampling (Osband and Van Roy, 2017). We adapt these three for our setting similar to as discussed in Section 3.2. For all experiments with probabilistic ensembles, we consider TS1 from Chua et al. (2018) for trajectory sampling, and for the GP experiment, we use distribution sampling from Chua et al. (2018). We call the three baselines NeMean (nonepisodic mean), NePETS (nonepisodic PETS), and NeTS (nonepisodic Thompson sampling). NeMean and NePETS are greedy w.r.t. the current estimate of the dynamics, i.e., do not explicitly encourage exploration. In our experiments, we show that being greedy does not suffice to converge to the optimal average cost, that is, obtain sublinear regret. The code for our experiments is available online.2

Convergence to the optimal average costIn Figure 1 we report the normalized average cost and cumulative regret of NeoRL, NeMean, NePETS, and NeTS. The normalized average cost is defined such that \(A(^{*})=0\) for all environments. We observe that NeMean fails to converge to the optimal average cost for the Pendulum-v1 environment for both probabilistic ensembles and a GP model. It also fails to solve the MountainCar environment and is unstable for the Reacher and CartPole. In general, NeMean performs the worst among all methods. This is similar to the episodic case, where using the mean model often leads to the policy "overfitting" to the model inaccuracies (Chua et al., 2018). NePETS performs better than the mean, however still significantly worse than NeoRL. Even in the episodic setting, PETS tends to underexplore (Curi et al., 2020). We observe the same for the nonepisodic case, especially for the MountainCar task, which is a challenging RL environment with a sparse cost. Here NePETS is also not able to achieve the optimal average cost and thus does not have sublinear cumulative regret. NeTS performs similarly to NePETS and is also not able to solve the MountainCar task.

NeoRL performs the best among the baselines for all experiments and converges to the optimal average cost achieving sublinear cumulative regret using only \( 10^{3}\) environment interactions. Moreover, this observation is consistent between different dynamics models (GPs and probabilistic ensembles) and environments. Even in environments that are unbounded, i.e., Swimmer, SoftArm, and RaceCar, we observe that NeoRL converges to the optimal average cost the fastest. We believe this is due to the feedback control from MPC, which has a stabilizing effect.

Calling reset when neededAll the experiments in Figure 1 considered the nonepisodic setting where the system was never reset during learning. A special case of our theoretical analysis is the class of policies \(\) that may call for a reset / "ask for help" whenever they end up in an undesirable part of the state space. In this setting, the system is typically restricted to a compact subset of the state space \(\), and the policy class satisfies Assumption 2.4. For many real-world applications, such a policy class can be derived. To simulate this experiment, we consider the CartPoleBalance task in Figure 2, where the goal is to balance the pole in the upright position. A reset is triggered whenever the pole drops. We again observe that NeoRL achieves the best performance, i.e., lowest cumulative regret and thus learns to solve the task the fastest. Moreover, it also requires fewer resets than NeMean, NePETS, and NeTS.

## 5 Related Work

Average cost RL for finite state-action spacesA significant amount of work studies the average cost/reward RL setting for finite-state action spaces. Moreover, seminal algorithms such as \(^{3}\)(Kearns and Singh, 2002) and \(\)(Brafman and Tennenholtz, 2002) have established PAC bounds for the nonepisodic setting. These bounds are further improved for communicating MDPs by the UCRL2 (Jaksch et al., 2010) algorithm, which, similar to NeoRL, is based on the optimism in the face of uncertainty paradigm and picks policies that are optimistic w.r.t. to the estimated dynamics. Their result is extended for weakly-communicating MDPs by REGAL (Bartlett and Tewari, 2012), similar results are derived for Thompson sampling based exploration (Ouyang et al., 2017), and for factored-MDP (Xu and Tewari, 2020). Albeit the significant amount of work for the finite case, progress for continuous state-action spaces has mostly been limited to linear dynamical systems.

Nonepisodic RL for linear systemsThere is a large body of work for nonepisodic learning with linear systems (Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Simchowitz and Foster, 2020; Dean et al., 2020; Lale et al., 2020; Faradonbeh et al., 2020; Abeille and Lazaric, 2020; Treven et al., 2021). For linear systems with quadratic costs, the average reward problem, also known as the linear quadratic-Gaussian (LQG), has a closed-form solution which is obtained via the Riccati equations (Anderson and Moore, 2007). Moreover, for LQG, stability and optimality are intertwined,making studying linear systems much easier than their nonlinear counterpart. For studying nonlinear systems, additional assumptions on their stability are usually made.

**Episodic RL for nonlinear systems** In the case of nonlinear systems, guarantees have mostly been established for the episodic setting (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Wagenmaker et al., 2023; Sukhija et al., 2024; Treven et al., 2024). In this setting, the agent begins each episode from an initial state \(_{0}\) (or initial state distribution) and interacts with the environment for a fixed horizon \(H\). It uses the data collected from the interactions to update its model. After each episode, the agent is reset back to \(_{0}\). The works mentioned above theoretically study this setting for finite-horizon MDPs and establish regret bounds for general nonlinear systems. Particularly Kakade et al. (2020); Curi et al. (2020); Sukhija et al. (2024); Treven et al. (2024) also use an optimism-based approach similar to ours. Compared to the nonepisodic case, the analysis of episodic RL methods is simpler as resets restrict the agent's exploration around the initial state \(_{0}\) and prevent the system from blowing up or visiting states from which the agent cannot recover. However, as discussed in Section 1, resets are often prohibitive and RL agents that learn non-episodically are preferred for many real-world applications.

**Nonepisodic RL beyond linear systems** Only a few works consider the nonepisodic/single-trajectory case. For instance, a line of work studies data-driven MPC approaches focusing mostly on establishing system-theoretic guarantees such as closed-loop stability and robustness (Berberich and Allgower, 2024). From the learning side, Foster et al. (2020); Sattar and Oymak (2022) study the problem of system identification of a closed-loop globally exponentially stable dynamical system from a single trajectory. Lale et al. (2021) study the nonepisodic setting for nonlinear systems with MPC. Moreover, they consider finite-order or exponentially fading NARX systems that lie in the RKHS of infinitely smooth functions, which they further approximate with random Fourier features (Rahimi and Recht, 2007)\(\) with feature size \(D\). Further, they assume access to bounded persistently exciting inputs w.r.t. the feature matrix \(_{t}_{t}^{}\). This assumption is generally tough to verify and common excitation strategies such as random exploration often don't perform well for nonlinear systems (Sukhija et al., 2024). The algorithm also operates in two stages, where in the first stage it performs pure exploration for system identification and in the second stage exploitation, i.e., acting greedily w.r.t. the estimated dynamics, akin to NeMean. Additionally, the algorithm requires the feature size \(D\) to increase with the horizon \(T\). They give a regret bound of \((T^{2/s})\) where the regret is measured w.r.t. to the oracle MPC with access to the true dynamics. Lale et al. (2021) also assume exponential input-to-output stability of the system to avoid blow-up during exploration. Our work considers more general RKHS, naturally trades-off exploration and exploitation, does not require apriori knowledge of persistently exciting inputs and gives a regret bound of \((_{T}})\) w.r.t. the optimal average cost criterion. Moreover, our regret bound is similar to the ones obtained for nonlinear systems in the episodic case and Gaussian process bandits (Srinivas et al., 2012; Chowdhury and Gopalan, 2017; Scarlett et al., 2017). To the best of our knowledge, we are the first to give such a regret bound for nonlinear systems.

Figure 2: Total number of resets and cumulative regret \(R_{T}\) for the cart pole balancing task over ten different seeds. We report the mean performance with one standard errors as the shaded region. The environment is automatically reset whenever the agent drops the pole. All baselines solve the task, but NeoRL converges the fastest requiring fewer resets and suffering smaller regret.

Nonepisodic Deep RLStandard deep RL approaches often fail in the nonepisodic setting (Sharma et al., 2021). To this end, deep RL algorithms have also been developed for the nonepisodic case. Mostly, these works focus on learning to reset and formulate it from the perspective of safety (Eysenbach et al., 2018) (avoiding undesirable states), chaining multiple controllers (Han et al., 2015), skill discovery/intrinsic exploration (Zhu et al., 2020; Xu et al., 2020), curriculum learning (Sharma et al., 2021), and learning initial state distributions from demonstrations (Sharma et al., 2022). However, in contrast to us, none of the works above provide any theoretical guarantees.

There are several extensions of model-free deep RL algorithms to the average reward setting (TRPO (Zhang and Ross, 2021), PPO (Ma et al., 2021), and DDPG (Saxena et al., 2023)). However, they mostly focus on maximizing the long-term behavior of the RL agent and allow for resets during learning. Overall, extending RL algorithms for the discounted case to the average one is still an open problem (Dewanto et al., 2020). However, future work in this direction will benefit NeoRL. Since average-reward optimizers can be used in combination with NeoRL to directly minimize the average cost in a model-based policy optimization (Janner et al., 2019) manner.

## 6 Conclusion

We propose, NeoRL, a novel model-based RL algorithm for the nonepisodic setting with nonlinear dynamics and continuous state and action spaces. NeoRL seeks for average-cost optimal policies and leverages the model's epistemic uncertainty to perform optimistic exploration. Similar to the episodic case (Kakade et al., 2020; Curi et al., 2020), we provide a regret bound for NeoRL of \((_{T}})\) for Gaussian process dynamics. To our knowledge, we are the first to obtain this result in the nonepisodic setting. We compare NeoRL to other model-based RL methods on standard deep RL benchmarks. Our experiments demonstrate that NeoRL, converges to the optimal average cost of \(A(^{*})=0\) across all environments, suffering sublinear regret even when Bayesian neural networks are used to model the dynamics. Moreover, NeoRL outperforms all our baselines across all environments requiring only \( 10^{3}\) samples for learning.

Future work may consider deriving lower bounds on the regret of NeoRL, studying different assumptions on \(^{*}\) and \(\), and investigating different notions of optimality such as bias optimality in the nonepisodic setting (Mahadevan, 1996).