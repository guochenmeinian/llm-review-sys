# C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory

Tianjiao Luo\({}^{1}\), Tim Pearce\({}^{2}\), Huayu Chen\({}^{1}\), Jianfei Chen\({}^{1}\), Jun Zhu\({}^{1*}\)

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center,

THBI Lab, BNRist Center, Tsinghua University, Beijing 100084, China

\({}^{2}\)Microsoft Research

{luotj21, chenhuay21}@mails.tsinghua.edu.cn

{jianfeic, dcszj}@tsinghua.edu.cn

###### Abstract

Generative Adversarial Imitation Learning (GAIL) provides a promising approach to training a generative policy to imitate a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from an adversarial discriminator. However, optimizing GAIL is difficult in practise, with the training loss oscillating during training, slowing convergence. This optimization instability can prevent GAIL from finding a good policy, harming its final performance. In this paper, we study GAIL's optimization from a control-theoretic perspective. We show that GAIL cannot converge to the desired equilibrium. In response, we analyze the training dynamics of GAIL in function space and design a novel controller that not only pushes GAIL to the desired equilibrium but also achieves _asymptotic stability_ in a simplified "one-step" setting. Going from theory to practice, we propose Controlled-GAIL (C-GAIL), which adds a differentiable regularization term on the GAIL objective to stabilize training. Empirically, the C-GAIL regularizer improves the training of various existing GAIL methods, including the popular GAIL-DAC, by speeding up the convergence, reducing the range of oscillation, and matching the expert distribution more closely.

+
Footnote â€ : * Corresponding author.

## 1 Introduction

Generative Adversarial Imitation Learning (GAIL)  aims to learn a decision-making policy in a sequential environment by imitating trajectories collected from an expert demonstrator. Inspired by Generative Adversarial Networks (GANs) , GAIL consists of a learned policy serving as a generator, and a discriminator distinguishing expert trajectories from generated ones. The learned policy is optimized through Reinforcement Learning (RL) with a reward signal derived from the discriminator. This paradigm offers distinct advantages over other imitation learning strategies such as Inverse Reinforcement Learning (IRL), which requires an explicit model of the reward function , and Behavior Cloning (BC), which suffers from a distribution mismatch during roll-outs .

Meanwhile, GAIL does bring certain challenges. One key issue it inherits from GANs is _instability_ during training . GAIL presents a difficult minimax optimization problem, where the convergence of the discriminator and the policy generator towards their optimal points is not guaranteed in general. This problem manifests in practice as oscillating training curves and an inconsistency in matching the expert's performance (Fig. 1). However, recent empirical works  on GAIL mostly focus on improving the sample efficiency and final return of the learned policy, without directly resolving the problem of unstable training. On the other hand, theoretical works  on GAIL's convergence are based on strong assumptions and do not yield a practical algorithm for stabilizing training.

In this paper, we study GAIL's training stability using control theory. We describe how the generative policy and discriminator evolve over training at timestep \(t\) with a dynamical system of differential equations. We study whether the system can converge to the _desired state_ where the generator perfectly matches with the expert policy and the discriminator cannot distinguish generated from expert trajectories. This surprisingly reveals that the desirable state is not an equilibrium of the system, indicating that existing algorithms do not converge to the expert policy, even with unlimited data and model capacity. In response, we study a "one-step GAIL" setting, and design a controller that does create an equilibrium at the desired state. We theoretically prove that this controller achieves _asymptotic stability_ around this desired state, which means that if initialized within a sufficiently close radius, the generator and discriminator will indeed converge to it.

Motivated by our theoretical analysis, we propose C-GAIL, which incorporates a pragmatic controller that can be added as a regularization term to the loss function to stabilize training in practice. Empirically we find that our method speeds up the convergence, reduces the range of oscillation in the return curves (shown on Fig. 1), and matches the expert's distribution more closely on GAIL-DAC and other imitation learning methods for a suite of MuJoCo control tasks.

### Related Work

**Adversarial imitation learning.** Inspired by GANs and IRL, Adversarial Imitation Learning (AIL) has emerged as a popular technique to learn from demonstrations. GAIL  formulated the problem as matching an occupancy measure under the maximum entropy RL framework, with a discriminator providing the policy reward signal, bypassing the need to recover the expert's reward function. Several advancements were subsequently proposed to enhance performance and stability. For instance, AIRL  replaced the Shannon-Jensen divergence of GAIL by KL divergence. Baram et al.  explored combining GAIL with model-based reinforcement learning. DAC  utilized a replay buffer to remove the need for importance sampling and address the issue of absorbing states. Other empirical works such as  helped improve the sample efficiency and final return of GAIL. In contrast, our work focuses on the orthogonal direction of training stability.

Meanwhile, the convergence behaviors of AIL have also been investigated theoretically. Chen et al.  proved that GAIL convergences to a stationary point (not necessarily the desired state). The convergence to the desired state has only been established under strong assumptions such as i.i.d. samples and linear MDP  and strongly concave objective functions . However, existing theory has not analyzed the convergence behavior to the desired state in a general setting, and has so far not presented practically useful algorithms to improve GAIL's convergence. Our analysis in Sec. 3.2 show that GAIL actually _cannot_ converge to the desired state under general settings.

Figure 1: Normalized return curves for controlled GAIL-DAC with four expert demonstrations on five MuJoCo environments averaged over five random seeds. The x-axis represents the number of gradient step updates in millions and the y-axis represents the normalized environment reward, where 1 stands for the expert policy return and 0 stands for the random policy return

Additionally, our proposed controller achieves not only a theoretical convergence guarantee, but also empirical improvements in terms of convergence speed and range of oscillation.

**Control theory in GANs.** Control theory has recently emerged as a promising technique for studying the convergence of GANs. Xu et al.  designed a linear controller which offers GANs local stability. Luo et al.  utilized a Brownian motion controller which was shown to offer GANs global exponential stability. However, for GAIL, the policy generator involves an MDP transition, which results in a much more complicated dynamical system induced by a policy acting in an MDP rather than a static data generating distribution. Prior theoretical analysis and controllers are therefore inapplicable. We adopt different analysis and controlling techniques, to present new stability guarantee, controller, and theoretical results for the different dynamical system of GAIL.

## 2 Preliminaries

We start by formally introducing our problem setting, as well as necessary definitions and theorems relating to the stability of dynamical systems represented by Ordinary Differential Equations (ODEs).

### Problem Setting

Consider a Markov Decision Process (MDP), described by the tuple \(,,,r,p_{0},\), where \(\) is the state space, \(\) is the action space, \((s^{}|s,a)\) is the transition probability function, \(r(s,a)\) is the reward function, \(p_{0}\) is the probability distribution of the initial state \(s_{0}\), and \(\) is the discount factor. We work on the \(\)-discounted infinite horizon setting, and define the expectation with respect to a policy \(\) as the (discounted) expectation over the trajectory it generates. For some arbitrary function \(g\) we have \(_{}[g(s,a)][_{n=0}^{}^{n} g(s_{n},a_{n})]\), where \(a_{n}(a_{n}|s_{n})\), \(s_{0} p_{0}\), \(s_{n+1}(s_{n+1}|s_{n},a_{n})\). Note that we use \(n\) to represent the **environment timestep**, reserving \(t\) to denote the **training timestep** of GAIL. For a policy \(\), We define its (unnormalized) state occupancy \(_{}(s)=_{n=0}^{}^{n}P(s_{n}=s|)\). We denote \(Q^{}(s,a)=_{}[ D(,)+(| )|s_{0}=s,a_{0}=a]\) and the advantage function \(A^{}(s,a)=Q^{}(s,a)-_{}[Q^{}(s,a)]\). We assume the setting where we are given a dataset of trajectories \(_{E}\) consisting of state-action tuples, collected from an expert policy \(_{E}\). We assume access to interact in the environment in order to learn a policy \(\), but do not make use of any external reward signal (except during evaluation).

### Dynamical Systems and Control Theory

In this paper, we consider dynamical systems represented by an ODE of the form

\[=f(x(t)),\] (1)

where \(x\) represents some property of the system, \(t\) refers to the timestep of the system and \(f\) is a function. The necessary condition for a solution trajectory \(\{x(t)\}_{t 0}\) converging to some steady state value is the existence of an 'equilibrium'.

**Definition 2.1**.: **(Equilibrium)** _[_19_]_ _A point \(\) is an equilibrium of system (1) if \(f()=0\). Such an equilibrium is also called a fixed point, critical point, or steady state._

Note that a dynamical system is unable to converge if an equilibrium does not exist. A second important property of dynamical systems is'stability'. The stability of a dynamical system can be described with Lyapunov stability criteria. More formally, suppose \(\{x(t)\}_{t 0}\) is a solution trajectory of the above system (1) with equilibrium \(\), we define two types of stability.

**Definition 2.2**.: **(Lyapunov Stability)** _[_20_]_ _System (1) is Lyapunov Stable if given any \(>0\), there exists a \(>0\) such that whenever \(\|x(0)-\|\), we have \(\|x(t)-\|<\) for \(0 t\)._

**Definition 2.3**.: **(Asymptotic Stability)** _[_20_]_ _System (1) is asymptotic stable if it is Lyapunov stable, and there exists a \(>0\) such that whenever \(\|x(0)-\|\), we have \(_{t}\|x(t)-\|=0\)._

Note that a dynamical system can be Lyapnuov stable but not asymptotic stable. However, every asymptotic stable dynamical system is Lyapnuov stable.

The field of control theory has studied how to drive dynamical systems to desired states. This can be achieved through the addition of a 'controller' to allow influence over the dynamical system'sevolution, for example creating an equilibrium at some desired state, and making the dynamical system stable around it.

**Definition 2.4**.: **(Controller)** A _controller_ of a dynamical system is a function \(u(t)\) such that

\[=f(x(t))+u(t).\] (2)

The equilibrium and stability criteria introduced for dynamical system (1), equally apply to this controlled dynamical system (2). In order to analyze the stability of a controller \(u(t)\) of the controlled dynamical system given an equilibrium \(\), the following result will be useful.

**Theorem 2.5**.: _(Principle of Linearized Stability)  A controlled dynamical system (2) with equilibrium \(\) is asymptotically stable if all eigenvalues of \((f()+u(t))\) have negative real parts, where \((f()+u(t))\) represents the Jacobian of \(f(x(t))+u(t)\) evaluated at \(\)._

**Corollary 2.6**.: _If \((f()+u(t))\) has positive determinant and negative trace, all its eigenvalues have negative real parts and the system is asymptotically stable._

## 3 Analyzing GAIL as a Dynamical System

In this section, we study the training stability of GAIL through the lens of control theory. We derive the differential equations governing the training process of GAIL, framing it as a dynamical system. Then, we analyze the convergence of GAIL and find that it cannot converge to the desired equilibrium due to the entropy term. For simplicity, we limit the theoretical analysis to the original GAIL among many variants [6; 7; 8; 9; 10], while the controller proposed in the next section is general.

### GAIL Dynamics

GAIL consists of a learned generative policy \(_{}:\) and a discriminator \(D_{}:(0,1)\). The discriminator estimates the probability that an input state-action pair is from the expert policy, rather than the learned policy. GAIL alternatively updates the policy and discriminator parameters, \(\) and \(\). (The parameter subscripts are subsequently dropped for clarity.) The GAIL objective  is \(_{}[(D(s,a))]+_{_{E}}[(1-D(s,a))]- H ()\), where \(_{E}\) is the expert demonstrator policy, \(\) is the learned policy, and \(H()_{}[-(a|s)]\) is its entropy. Respectively, the objective functions for the discriminator and policy (to be maximized and minimized respectively) are,

\[V_{D}(D,) =_{}[ D(s,a)]+_{_{E}}[(1-D(s,a))]\] (3) \[V_{}(D,) =_{}[ D(s,a)]-_{}[-( a|s)].\]

To describe GAIL as a dynamical system, we express how \(\) and \(D\) evolve during training. For the analysis to be tractable, we study the training dynamics from a variational perspective, by directly considering the optimization of \(\) and \(D\) in their respective _function spaces_. This approach has been used in other theoretical deep learning works [2; 5; 23] to avoid complications of the parameter space.

We start by considering optimizing Eq. (3) with functional gradient descent with discrete iterations indexed by \(m\): \(D_{m+1}(s,a)=D_{m}(s,a)+(D_{m},_{m})}{ D_{ m}(s,a)}\), and \(_{m+1}(a|s)=_{m}(a|s)-(D_{m},_{m})}{ _{m}(a|s)}\), where \(\) is the learning rate, \(m\) the discrete iteration number, \((D_{m},_{m})}{ D_{m}(s,a)}\) (similarly for \((D_{m},_{m})}{_{m}(a|s)}\)) is the _functional derivative_ defined via \( V_{D}(D_{m},_{m})=(D_{m},_{m})}{  D_{m}(s,a)} D_{m}(s,a)\ ds\ da\), which implies the total change in \(V_{D}\) upon variation of function \(D_{m}\) is a linear superposition  of the local changes summed over the whole range of \((s,a)\) value pairs.

We then consider the limit \( 0\), where discrete dynamics become continuous ('gradient flow') \((s,a)}{dt}=(D_{t},_{t})}{ D_{t}(s,a )}\), and \((a|s)}{dt}=-(D_{t},_{t})}{_{ t}(a|s)}\). Formally, we consider the evolution of the discriminator function \(D_{t}:\) and the policy generator \(_{t}:\) over continuous time \(t\) rather than discrete time \(m\). We derive the training dynamic of GAIL in the following theorem.

**Theorem 3.1**.: _The training dynamic of GAIL takes the form (detailed proof in Appendix Lemma C.2)_

\[(s,a)}{dt}=}(s)_{t}(a|s)}{D_{t}(s,a)}- {_{_{E}}(s)_{E}(a|s)}{1-D_{t}(s,a)},\] (4)

\[(a|s)}{dt}=-_{_{t}}(s)A^{_{t}}(s,a).\] (5)

### On the Convergence of GAIL

Now, we study the optimization stability of GAIL using the dynamical system Eq. (5). The desirable outcome of the GAIL training process, is for the learned policy to perfectly match the expert policy, and the discriminator to be unable to distinguish between the expert and learned policy.

**Definition 3.2**.: **(Desired state)** We define the desired outcome of the GAIL training process as the discriminator and policy reaching \(D_{t}^{*}(s,a)=,_{t}^{*}(a|s)=_{E}(a|s)\).

We are interested in understanding whether GAIL converges to the desired state. As discussed in Sec. 2.2, the desired state should be the equilibrium of the dynamical system Eq. (5) for such convergence. According to Def. 2.1, the dynamical system should equal to zero at this point, but we present the following theorem (proved in Proposition C.3 and C.5):

**Theorem 3.3**.: _The training dynamics of GAIL does not converge to the desired state, and we have_

\[^{*}(s,a)}{dt}=^{*}}(s)_{t}^{*}(a|s)}{D_{t}^ {*}}-}(s)_{E}(a|s)}{1-D_{t}^{*}}=0,^{*}(a|s )}{dt}=-_{_{E}^{*}}(s)A^{_{E}^{*}}(s,a) 0.\] (6)

Hence, the desired state is not an equilibrium, and GAIL will not converge to it. Since \(^{*}(a|s)}{dt} 0\), even if the system is forced to the desired state, it will drift away from it. We find that the non-equilibrium result is due to the entropy term \( H()\), and an equilibrium could be achieved by simply setting \(=0\) (Corollary C.4). However, the entropy term is essential since it resolves the exploration issue and prevents over-fitting. Therefore, we aim to design a controller that not only keeps the entropy term but also improves the theoretical convergence guarantee.

## 4 Controlled GAIL

Having shown in Section 3 that GAIL does not converge to the desired state, this section considers adding a controller to enable the convergence. We design controllers for both the discriminator and the policy. We show that this controlled system converges to the desired equilibrium and also achieves asymptotic stability in a simplified "one-step" setting.

### Controlling the Training Process of GAIL

Establishing the convergence for GAIL is challenging since the occupancy measure \(_{}\) involves an expectation over the states generated by playing the policy \(\) for infinite many steps. We simplify the analysis by truncating the trajectory length to one: we only consider the evolution from timestep \(n\) to \(n+1\). We refer this simplified setting as "one-step GAIL", and the convergence guarantee of our proposed algorithm will be established in this simplified setting. Let \(p(s)\) be the probability of the state at \(s\) on timestep \(n\). The objectives for the discriminator and the policy can then be simplified as,

\[_{D}(D,) =_{a}_{s}p(s)(a|s) D(s,a)+_{E}(a|s)(1-D(s,a) )\ ds\ da,\] \[_{}(D,) =_{a}_{s}p(s)(a|s) D(s,a)+ p(s)(a|s) (a|s)\ ds\ da.\]

The gradient flow dynamical system of these functions is,

\[(s,a)}{dt} =(a|s)}{D_{t}(s,a)}+(a|s)}{D_{t }(s,a)-1},\] (7) \[(a|s)}{dt} =-p(s)( D_{t}(s,a)+_{t}(a|s)+).\] (8)

With this "one-step" simplification, the GAIL dynamics now reveal a clearer structure. For a given \((s,a)\) pair, the change of \(D(s,a)\) and \((a|s)\) only depends on \(D(s,a),(a|s),p(s)\) and \(_{E}(a|s)\) for the same \((s,a)\) pair, without the need to access function values of other \((s,a)\) pairs. Therefore, we can decompose Eq. (7) & (8), which are ODEs of _functions_, into a series of ODEs of _scalar values_. Each ODE only models the dynamics of two scalar values \((D(s,a),(a|s))\) for a particular \((s,a)\) pair. We will add controller to the scalar ODEs, to asymptotically stabilize their dynamical system. Proving that each scalar ODE is stable suggests that the functional ODE will also be stable. Note thatsuch decomposition is not possible without the "one-step" simplification, since the evolution of \(D\) and \(\) for all \((s,a)\) pairs is coupled through \(_{}(s)\) and \(A^{}(s,a)\) in Eq. (5).

Based on the above discussion, we now consider the stability of a system of ODEs for two scalar variables \((D(s,a),(a|s))\). With \(s,a\) given, we simplify the notation as \(x(t):=D_{t}(s,a)\), \(y(t):=_{t}(s|a)\), \(E:=_{E}(a|s)\), \(c:=p(s)\), so each scalar ODE can be rewritten as,

\[=+,=-c x (t)-c y(t)-c.\] (9)

We showed earlier that the GAIL dynamic in Eq. (5) does not converge to the desired state. Similarly, neither does our simplified 'one-step' dynamic in Eq. (9) converge to the desired state. We now consider the addition of controllers to push our dynamical system to the desired stated. Specifically, we consider linear negative feedback control , which can be applied to a dynamical system to reduce its oscillation. We specify our controlled GAIL system as,

\[x(t)}{t} =++u_{1}(t)\] (10) \[y(t)}{t} =-c x(t)-c y(t)-c+u_{2}(t),\] (11)

where \(u_{1}(t)\) and \(u_{2}(t)\) are the controllers to be designed for the discriminator and policy respectively. Since the derivative of the discriminator with respect to time evaluated at the desired state (Def. 3.2) already equals zero, the discriminator is already able to reach its desired state. Nevertheless, the discriminator can still benefit from a controller to speed up the rate of convergence - we choose a linear negative feedback controller for \(u_{1}(t)\) to push the discriminator towards its desired state. On the other hand, the derivative of the policy generator evaluated at its desired state in Eq. (9) does not equal zero. Therefore, \(u_{2}(t)\) should be set to make Eq. (11) equal to zero evaluated at the desired state. We have designed it to cancel out all terms in Eq. (9) at this desired state, and also provide feasible hyperparameter values for an asymptotically stable system. Hence, we select \(u_{1}(t)\) and \(u_{2}(t)\) to be the following functions,

\[u_{1}(t)=-k(x(t)-),\] (12)

\[u_{2}(t)=c E+c+c+-,\] (13)

where \(k,\) are hyperparameters. Intuitively, as \(k\) gets larger, the discriminator will be pushed harder towards the optimal value of \(1/2\). This means the discriminator would converge at a faster speed but may also have a larger radius of oscillation.

### Analyzing the Stability of Controlled GAIL

In this section, we apply Theorem 2.5 to formally prove that the controlled GAIL dynamical system described in Eq. (10) & (11) is _asymptotically stable_ (Def. 2.3) and give bounds with \(\), \(\), and \(k\).

For simplicity, let us define \(z(t)=(x(t),y(t))^{}\), and a function \(f\) such that \(f(z(t))\) is the vector \([+-k(x(t)-),c +c E-c y(t)-c x(t)+ {E}-]^{}\). Therefore, our controlled training dynamic of GAIL in Eq. (10) and Eq. (11) can be transformed to the following vector form

\[d(z(t))=f(z(t))dt.\] (14)

**Theorem 4.1**.: _Let assumption 4.2 hold. The training dynamic of GAIL in Eq. (14) is **asymptotically stable** (proof in Appendix D)._

**Assumption 4.2**.: We assume \(,k,k>0\), \(8c^{2}-8c-4c^{2}+ck-k>0\), and \(+32c(-c+)}{32c}<0\).

**Proof sketch.** The first step in proving asymptotic stability of the system in Eq. (14), is to verify whether our desired state is an equilibrium (Def. 2.1). We substitute the desired state, \(z^{*}(t)=(,E)^{}\), into system (14) and verify that \(d(z^{*}(t))=f(z^{*}(t))=0.\) We then find the linearized system about the desired state \(d(z(t))=(f(z^{*}(t)))z(t)dt.\) Under Assumption 4.2, we show that \(det((f(z^{*}(t))))>0\) and \(trace((f(z^{*}(t))))<0\). Finally we invoke Theorem 2.5 and Corollary 2.6 to conclude that the system in Eq. (14) is asymptotically stable.

## 5 A Practical Method to Stabilize GAIL

In this section, we extend our controller from the "one-step" setting back to the general setting and instantiate our controller as a regularization term on the original GAIL loss function. This results in our proposed variant C-GAIL; a method to stabilize the training process of GAIL.

Since the controllers in Eq. (13) are defined in the dynamical system setting, we need to integrate these with respect to time, in order to recover an objective function that can be practically optimized by a GAIL algorithm. Recalling that \(V_{D}(D,)\) and \(V_{}(D,)\) are the original GAIL loss functions for the discriminator and policy (Eq. (3)), we define \(V^{}_{D}(D,)\) and \(V^{}_{}(D,)\) as modified loss functions with the integral of our controller applied, such that

\[V^{}_{D}(D,) =V_{D}(D,)-_{,_{E}}[(D(s,a)- )^{2}],\] \[V^{}_{}(D,) =V_{}(D,)+_{,_{E}}[ (a|s)}{_{E}(a|s)}+(c+c_{E}(a|s)+ c-)(a|s)].\]

Note that the training dynamics with these loss functions are identical to Eq. (10-13) with guaranteed stability under the 'one-step' setting.

While \(V^{}_{D}(D,)\) can be computed directly, the inclusion of \(_{E}\), the expert policy, in \(V^{}_{}(D,)\) is problematic - the very goal of the algorithm is to learn \(_{E}\), and we do not have access to it during training. Hence, in our practical implementations, we only use our modified loss \(V^{}_{D}(D,)\) to update the discriminator, but use the original unmodified policy objective \(V_{}(D,)\) for the policy. In other words, we only add the controller to the discriminator objective \(V_{D}(D,)\). This approximation has no convergence guarantee, even in the one-step setting. Nevertheless, the control theory-motivated approach effectively stabilizes GAIL in practice, as we shall see in Sec. 6. Intuitively, C-GAIL pushes the discriminator to its equilibrium at a faster speed by introducing a penalty controller centered at \(\). With proper selection of the hyperparameter \(k\) (ablation study provided in Appendix E), the policy generator is able to train the discriminator at the same pace, leading GAIL's training to converge faster with a smaller range of oscillation, and match the expert distribution more closely.

Our C-GAIL algorithm is listed in Alg. 1. It can be implemented by simply adding a regularization term to the discriminator loss. Hence, our method is also compatible with other variants of GAIL, by straightforwardly incorporating the regularization into their discriminator objective function.

```
1:Input: Expert trajectory \(_{E}\) sampled from \(_{E}\), initial parameters \(_{0}\), and \(_{0}\) for generator and discriminator.
2:repeat
3: Sample trajectory \(\) from \(_{}\).
4: Update discriminator parameters \(\) with gradient from, \(}_{}[ D(s,a)-(D(s,a)- )^{2}]+}_{_{E}}[(1-D(s,a))- (D(s,a)-)^{2}]\)
5: Update policy parameters \(\) with \(V_{}(D,)\) in Eq. 3
6:until Stopping criteria reached ```

**Algorithm 1** The C-GAIL algorithm

## 6 Evaluation

This section evaluates the benefit of integrating the controller developed in Section 4 with popular variants of GAIL. We test the algorithms on their ability to imitate an expert policy in simulated continuous control problems in MuJoCo . Specifically, we consider applying our controller to two popular GAIL algorithms - both the original 'vanilla' GAIL  and also GAIL-DAC , a state-of-the-art variant which uses a discriminator-actor-critic (DAC) to improve sample efficiency and reduce the bias of the reward function. Additionally, we include supplementary experiments compared with other GAIL variants such as Jena et al.  and Xiao et al.  in appendix F.

### Experimental Setup

We incorporate our controller in vanilla GAIL and GAIL-DAC, naming our controlled variants C-GAIL and C-GAIL-DAC. We leverage the implementations of Gleave et al.  (vanilla GAIL) and Kostrikov et al.  (GAIL-DAC). Gleave et al.  also provide other common imitation learning frameworks - BC, AIRL, and dataset aggregation (DAgger)  - which we also compare to.

For C-GAIL-DAC, we test five MuJuCo environments: Half-Cheetah, Ant, Hopper, Reacher and Walker 2D. Our experiments follow the same settings as Kostrikov et al. . The discriminator architecture has a two-layer MLP with 100 hidden units and tanh activations. The networks are optimized using Adam with a learning rate of \(10^{-3}\), decayed by \(0.5\) every \(10^{5}\) gradient steps. We vary the number of provided expert demonstrations: \(\{4,7,11,15,18\}\), though unless stated we report results using four demonstrations. We assess the normalized return over training for GAIL-DAC and C-GAIL-DAC to evaluate their speed of convergence and stability, reporting the mean and standard deviation over five random seeds. The normalization is done with 0 set to a random policy's return and 1 to the expert policy return.

In addition to recovering the expert's return, we are also interested in how closely our policy generator's and the expert's _state distribution_ are matched, for which we use the **state Wasserstein**. This requires samples from two distributions, collected by rolling out the expert and learned policy for 100 trajectories each. We then use the POT library's 'emd2' function  to compute the Wasserstein distance, using the L2 cost function with a uniform weighting across samples.

To evaluate C-GAIL, we follow the experimental protocol from Gleave et al. , both for GAIL and other imitation learning baselines. These are evaluated on Ant, Hopper, Swimmer, Half-Cheetah and Walker 2D. For C-GAIL, we change only the loss and all other GAIL settings are held constant. We assess performance in terms of the normalized return. We use this set up to ablate the controller strength hyperparameter of C-GAIL (Appendix E), varying \(k\{0.1,1,10\}\) (ablation study of \(\) is not included since our algorithm only involves controller for the discriminator in practice). Our experiments are conducted on a single NVIDIA GeForce GTX TITAN X.

   & Ant & Half Cheetah & Hopper & Swimmer & Walker2d \\  Random & \(-349 31\) & \(-293 36\) & \(-53 62\) & \(3 8\) & \(-18 75\) \\ Expert & \(2408 110\) & \(3465 162\) & \(2631 19\) & \(298 1\) & \(2631 112\) \\ Controlled GAIL & \(2411 21\) & \(3435 50\) & \(2636 8\) & \(298 0\) & \(2633 12\) \\ GAIL & \(2087 187\) & \(3293 239\) & \(2579 85\) & \(295 3\) & \(2589 121\) \\ BC & \(1937 227\) & \(3465 151\) & \(2830 265\) & \(298 1\) & \(2672 95\) \\ AIRL & \(-121 28\) & \(1837 218\) & \(2536 142\) & \(269 8\) & \(1329 134\) \\ DAgger & \(3027 187\) & \(1693 74\) & \(2751 11\) & \(344 2\) & \(2174 132\) \\  

Table 1: Mean and standard deviation for returns of various IL algorithms and environments

Figure 2: State Wasserstein distance (lower is better) between expert and learned policies, over number of gradient step updates. Our controlled variant matches the expert distribution more closely.

### Results

We compare GAIL-DAC to C-GAIL-DAC in Figure 1 (return), 2 (state Wasserstein), and 3 (convergence speed). Figure 1 shows that C-GAIL-DAC speeds up the rate of convergence and reduces the oscillation in the return training curves across all environments. For instance, on Hopper, C-GAIL-DAC converges 5x faster than GAIL-DAC with less oscillations. On Reacher, the return of GAIL-DAC continues to spike even after matching the expert return, but this does not happen with C-GAIL-DAC. On Walker 2D, the return of GAIL-DAC oscillates throughout training, whereas our method achieves a higher return at has reduced the range of oscillation by more than 3 times. For Half-Cheetah, our method converges 2x faster than GAIL-DAC. For Ant environment, C-GAIL-DAC reduces the range of oscillations by around 10x.

In addition to matching the expert's return faster and with more stability, Figure 2 shows that C-GAIL-DAC also more closely matches the expert's state distribution than GAIL-DAC, with the difference persisting even towards the end of training for various numbers of expert trajectories. Toward the end of training, the state Wasserstein for C-GAIL-DAC is more than two times smaller than the state Wasserstein for GAIL-DAC on all five environments.

Figure 3 shows that these improvements hold for differing numbers of provided demonstrations. It plots the number of gradient steps for GAIL-DAC and C-GAIL-DAC to reach \(95\%\) of the max-return for vaious numbers of expert demonstrations. Our method is able to converge faster than GAIL-DAC regardless of the number of demonstrations.

**Hyperparameter sensitivity.** We evaluate the sensitivity to the controller's hyperparameter \(k\) using vanilla GAIL. Figure 4 (Appendix E) plots normalized returns. For some environments, minor gains can be found by tuning this hyperparameter, though in general for all values tested, the return curves of C-GAIL approach the expert policy's return earlier and with less oscillations than GAIL. This is an important result as it shows that our regularizer can easily be applied by practitioners without the need for a fine-grained hyperparameter sweep.

**Other imitation learning methods.** Table 1 benchmarks C-GAIL against other imitation learning methods, including BC, AIRL, and DAgger, some of which have quite different requirements to the GAIL framework. The table shows that C-GAIL is competitive with many other paradigms, and in consistently offers the lowest variance between runs of any method. Moreover, we include supplementary experiments compared with Jena et al.  and Xiao et al.  in appendix F.

## 7 Discussion & Conclusion

This work helped understand and address the issue of training instability in GAIL using the lens of control theory. This advances recent findings showing its effectiveness in other adversarial learning frameworks. We formulated GAIL's training as a dynamical system and designed a controller that stabilizes it at the desired state, encouraging convergence to this point. We showed theoretically that our controlled system achieves asymptotic stability under a "one-step" setting. We proposed a

Figure 3: Number of gradient step updates (in millions) required to reach \(95\%\) of the max-return for various numbers of expert trajectories on MuJoCo environments averaged over five random seeds.

practical realization of this named C-GAIL, which reaches expert returns both faster and with less oscillation than the uncontrolled variants, and also matches their state distribution more closely.

Whilst our controller theoretically converges to the desired state, and empirically stabilizes training, we recognize several limitations of our work. In our description of GAIL training as a continuous dynamical system, we do not account for the updating of generator and discriminator being discrete as in practice. In our practical implementation of the controller, we only apply the portion of the loss function acting on the discriminator, since the generator portion requires knowing the likelihood of an action under the expert policy (which is precisely what we aim to learn!). We leave it to future work to explore whether estimating the expert policy and incorporating a controller for the policy generator brings benefit.