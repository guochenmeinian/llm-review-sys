# DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method

Ahmed Khaled

Princeton University

&Konstantin Mishchenko

Samsung AI Center

&Chi Jin

Princeton University

###### Abstract

This paper proposes a new easy-to-implement _parameter-free_ gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is _efficient_--matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and _universal_--automatically adapting to both smooth and nonsmooth problems. While popular algorithms following the AdaGrad framework compute a running average of the squared gradients to use for normalization, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practical machine learning tasks.

## 1 Introduction

We study the fundamental optimization problem

\[_{x}f(x),\] (OPT)

where \(f\) is a convex function, and \(\) is a convex, closed, and bounded subset of \(^{d}\). We assume \(f\) has at least one minimizer \(x_{*}\). We focus on gradient descent and its variants, as they are widely adopted and scale well when the model dimensionality \(d\) is large (Bottou et al., 2018). The optimization problem (OPT) finds many applications: in solving linear systems, logistic regression, support vector machines, and other areas of machine learning (Boyd and Vandenberghe, 2004). Equally important, methods designed for (stochastic) convex optimization also influence the intuition for and design of methods for nonconvex optimization- for example, momentum (Polyak, 1964), AdaGrad (Duchi et al., 2010), and Adam (Kingma and Ba, 2015) were all first analyzed in the convex optimization framework.

As models become larger and more complex, the cost and environmental impact of training have rapidly grown as well (Sharir et al., 2020; Patterson et al., 2021). Therefore, it is vital that we develop more efficient and effective methods of solving machine learning optimization tasks. One of the chief challenges in applying gradient-based methods is that they often require tuning one or more stepsize parameters (Goodfellow et al., 2016), and the choice of stepsize can significantly influence a method's convergence speed as well as the quality of the obtained solutions, especially in deep learning (Wilson et al., 2017).

The cost and impact of hyperparameter tuning on the optimization process have led to significant research activity in designing parameter-free and adaptive optimization methods in recent years, see e.g. (Orabona and Cutkosky, 2020; Carmon and Hinder, 2022) and the references therein.

We say an algorithm is _universal_ if it adapts to many different problem geometries or regularity conditions on the function \(f\)(Nesterov, 2014; Levy et al., 2018; Grimmer, 2022). In this work, we focus on two regularity conditions: (a) \(f\) Lipschitz and (b) \(f\) smooth. Lipschitz functionshave a bounded rate of change, that is, there exists some \(G>0\) such that for all \(x,y\) we have \(|f(x)-f(y)| G\|x-y\|\). The Lipschitz property is beneficial for the convergence of gradient-based optimization algorithms. They converge even faster on smooth functions, which have continuous derivatives; that is, there exists some \(L>0\) such that for all \(x,y\) we have \(\| f(x)- f(y)\| L\|x-y\|\). Smoothness leads to faster convergence of gradient-based methods than the Lipschitz property. Universality is a highly desirable property because in practice the same optimization algorithms are often used for both smooth and nonsmooth optimization (e.g. optimizing both ReLU and smooth networks).

The main question of our work is as follows:

Can we design a universal, parameter-free gradient descent method for (OPT)?

Existing universal variants of gradient descent either rely on line search (Nesterov, 2014; Grimmer, 2022), bisection subroutines (Carmon and Hinder, 2022), or are not parameter-free (Hazan and Kakade, 2019; Levy et al., 2018; Kavis et al., 2019). Line search algorithms are theoretically strong, achieving the optimal convergence rates in both the nonsmooth and smooth settings with only an extra log factor. Through an elegant application of bisection search, Carmon and Hinder (2022) design a parameter-free method whose convergence is only double-logarithmically worse than gradient descent with known problem parameters. However, this method requires resets, i.e. restarting the optimization process many times, which can be very expensive in practice. Therefore, we seek a universal, parameter-free gradient descent method for (OPT) with **no search subroutines.**

**Our contributions.** We provide a new algorithm that meets the above requirements. Our main contribution is **a new universal, parameter-free gradient descent method with no search subroutines.** Building upon the recently proposed Distance-over-Gradients (DoG) algorithm (Ivgi et al., 2023), we develop a new method, DoWG (Algorithm 1), that uses a different stepsize with adaptively weighted gradients. We show that DoWG automatically matches the performance of gradient descent on (OPT) up to logarithmic factors with no stepsize tuning at all. This holds in both the nonsmooth setting (Theorem 3) and the smooth setting (Theorem 4). Finally, we show that DoWG is competitive on real machine learning tasks (see Section 4).

## 2 Related Work

There is a lot of work on adaptive and parameter-free approaches for optimization. We summarize the main properties of the algorithms we compare against in Table 1. We enumerate some of the major approaches below:

  
**Algorithm** & **No search** & **Parameter-free** & **Universal** & **GD framework** \\  Polyak stepsize (Polyak, 1987; Hazan and Kakade, 2019) & ✓ & ✗ & ✓ & ✓ \\ Coin betting with normalization & ✓ & ✓ & ✓\({}^{()}\) & ✗ \\ (Orabona and Pal, 2016; Orabona and Cutkosky, 2020; Orabona, 2023) & & & \\ Nesterov line search (Nesterov, 2014) & ✗ & ✓ & ✓ & ✓ \\ AdaGrad & ✓ & ✗ & ✓ & ✓ \\ (Duchi et al., 2010; Levy et al., 2018; Ene et al., 2021) & & & \\ Adam & ✓ & ✗ & ✓ & ✓ \\ (Kingma and Ba, 2015; Li et al., 2023) & & & & \\ Bisection search (Carmon and Hinder, 2022) & ✗ & ✓ & ✓ & ✓ \\ D-Adaptation (Defazio and Mishchenko, 2023) & ✓ & ✓ & ✗ & ✓ \\ DoG (Ivgi et al., 2023) & ✓ & ✓ & ✓\({}^{()}\) & ✓ \\ DoWG (**new, this paper!**) & ✓ & ✓ & ✓ & ✓ \\   

* Result appeared after the initial release of this paper.

Table 1: A comparison of different adaptive algorithms for solving (OPT). "Universal" means that the algorithm can match the rate of gradient descent on both smooth and nonsmooth objectives up to polylogarithmic factors. "No search" means the algorithm does not reset. "GD framework" refers to algorithms that follow the framework of Gradient Descent.

**Polyak stepsize.** When \(f_{*}=f(x_{*})\) is known, the Polyak stepsize (Polyak, 1987) is a theoretically-grounded, adaptive, and universal method (Hazan and Kakade, 2019). When \(f_{*}\) is not known, Hazan and Kakade (2019) show that an adaptive re-estimation procedure can recover the optimal convergence rate up to a log factor when \(f\) is Lipschitz. Loizou et al. (2021) study the Polyak stepsize in stochastic non-convex optimization. Orvieto et al. (2022) show that a variant of the Polyak stepsize with decreasing stepsizes can recover the convergence rate of gradient descent, provided the stepsize is initialized properly. Unfortunately, this initialization requirement makes the method not parameter-free.

**The doubling trick.** The simplest way to make an algorithm parameter-free is the doubling-trick. For example, for gradient descent for \(L\)-smooth and convex optimization, the stepsize \(=\) results in the convergence rate of

\[f()-f_{*}=(L^{2}}{T}), \]

where \(D_{0}=\|x_{0}-x_{*}\|\). We may therefore start with a small estimate \(L_{0}\) of the smoothness constant \(L\), run gradient descent for \(T\) steps, and return the average point. We restart and repeat this for \(N\) times, and return the point with the minimum function value. So long as \(N}\), we will return a point with loss satisfying eq. (1) at the cost of only an additional logarithmic factor. This trick and similar variants of it appear in the literature on prediction with expert advice and online learning (Cesa-Bianchi et al., 1997; Cesa-Bianchi and Lugosi, 2006; Hazan and Megiddo, 2007). It is not even needed to estimate \(N\) in some cases, as the restarting can be done adaptively (Streeter and McMahan, 2012). In practice, however, the performance of doubling trick suffers from restarting the optimization process and throwing away useful that could be used to guide the algorithm.

**Parameter-free methods.** Throughout this paper, we use the term "parameter-free algorithms" to describe optimization algorithms that do not have any tuning parameters. We specifically consider only the deterministic setting with a compact domain. As mentioned before, Carmon and Hinder (2022) develop an elegant parameter-free and adaptive method based on bisection search. Bisection search, similar to grid search, throws away the progress of several optimization runs and restarts, which may hinder their practical performance. Ivgi et al. (2023); Defazio and Mishchenko (2023) recently developed variants of gradient descent that are parameter-free when \(f\) is Lipschitz. However, D-Adaptation (Defazio and Mishchenko, 2023) has no known guarantee under smoothness, while DoG (Ivgi et al., 2023) was only recently (after the initial release of this paper) shown to adapt to smoothness. We compare against the convergence guarantees of DoG in Section 3.2. For smooth functions, Malitsky and Mishchenko (2020) develop AdGD, a method that efficiently estimates the smoothness parameter on-the-fly from the training trajectory. AdGD is parameter-free and matches the convergence of gradient descent but has no known guarantees for certain classes of Lipschitz functions. A proximal extension of this method has been proposed by Latafat et al. (2023).

**Parameter-free methods in online learning.** In the online learning literature, the term "parameter-free algorithms" was originally used to describe another class of algorithms that adapt to the unknown distance to the optimal solution (but can still have other tuning parameters such as Lipschitz constant). When the Lipschitz parameter is known, approaches from online convex optimization such as coin betting (Orabona and Pal, 2016), exponentiated gradient (Streeter and McMahan, 2012; Orabona, 2013), and others (McMahan and Orabona, 2014; Orabona and Cutkosky, 2020; Orabona and Pal, 2021; Orabona and Tommasi, 2017) yield rates that match gradient descent up to logarithmic factors. Knowledge of the Lipschitz constant can be removed either by using careful restarting schemes (Mhammedi et al., 2019; Mhammedi and Koolen, 2020), or adaptive clipping on top of coin betting (Cutkosky, 2019). For optimization in the deterministic setting, it is later clarified that, by leveraging the normalization techniques developed in (Levy, 2017), the aforementioned online learning algorithms can be used without knowing other tuning parameters (i.e., achieve "parameter-free" in the sense of this paper) for optimizing both Lipschitz functions (Orabona and Pal, 2021) and smooth functions (Orabona, 2023). Concretely, as shown in Orabona (2023) (which appears after the initial release of this paper), combining algorithms in Streeter and McMahan (2012); Orabona and Pal (2016) with normalization techniques (Levy, 2017) yields new algorithms that are also search-free, parameter-free (in the sense of this paper), and universal. However, these algorithms are rather different from DoWG in algorithmic style: these algorithms only use normalized gradients while DoWG does use the magnitudes of the gradients; DoWG falls in the category of gradient descent algorithms with adaptive learning rate, while these algorithms do not.

**Line search.** As mentioned before, line-search-based algorithms are universal and theoretically grounded (Nesterov, 2014) but are often expensive in practice (Malitsky and Mishchenko, 2020).

**AdaGrad family of methods.**Li and Orabona (2019) study a variant of the AdaGrad stepsizes in the stochastic convex and non-convex optimization and show convergence when the stepsize is tuned to depend on the smoothness constant. Levy et al. (2018) show that when the stepsize is tuned properly to the diameter of the domain \(\) in the constrained convex case, AdaGrad-Norm adapts to smoothness. Ene et al. (2021) extend this to AdaGrad and other algorithms, and also to variational inequalities. Ward et al. (2019); Traore and Pauwels (2021) show the convergence of AdaGrad-Norm for any stepsize for non-convex (resp. convex) optimization, but in the worst case the dependence on the smoothness constant is worse than gradient descent. Liu et al. (2022) show that AdaGrad-Norm converges in the unconstrained setting when \(f\) is quasi-convex, but their guarantee is worse than gradient descent. We remark that all AdaGrad-style algorithms mentioned above require tuning stepsizes, and are thus not parameter-free.

**Alternative justifications for normalization.** There are other justifications for why adaptive methods work outside of universality. Zhang et al. (2020) study a generalized smoothness condition and show that in this setting tuned clipped gradient descent can outperform gradient descent. Because the effective stepsize used in clipped gradient descent is only a constant factor away from the effective stepsize in normalized gradient descent, (Zhang et al., 2020), also show that this improvement holds for NGD. Zhang et al. (2020) observe that gradient clipping and normalization methods outperform SGD when the stochastic gradient noise distribution is heavy-tailed. However, Kunstner et al. (2023) later observe that adaptive methods still do well even when the effect of the noise is limited.

## 3 Algorithms and theory

In this section we first review the different forms of adaptivity in gradient descent and normalized gradient descent, and then introduce our proposed algorithm DoWG. The roadmap for the rest of the paper is as follows: we first review the convergence of gradient descent in the Lipschitz and smooth settings, and highlight the problem of divergence under stepsize misspecification, and how normalization fixes that. Then, we introduce our main new algorithm, DoWG, and give our main theoretical guarantees for the algorithm. Finally, we evaluate the performance of DoWG on practical machine learning problems.

### Baselines: gradient descent and normalized gradient descent

We start our investigation with the standard Gradient Descent (GD) algorithm:

\[x_{t+1}=_{}(x_{t}- f(x_{t})),\] (GD)

where \(_{}\) is the projection on \(\) (when \(=^{d}\), this is just the identity operator). The iterations (GD) require specifying the stepsize \(>0\). When \(f\) is \(G\)-Lipschitz, gradient descent achieves the following standard convergence guarantee:

**Theorem 1**.: _Suppose that \(f\) is convex with minimizer \(x_{*}\). Let \(f_{*}=f(x_{*})\). Let \(D_{0}}}{{=}}\|x_{0}-x_{*}\|\) be the initial distance to the optimum. Denote by \(_{T}=_{t=0}^{T-1}x_{t}\) the average iterate returned by GD. Then:_

* _(Bubeck, 2015)_ _If_ \(f\) _is_ \(G\)_-Lipschitz, the average iterate satisfies for any stepsize_ \(>0\)_:_ \[f(_{T})-f_{*}^{2}}{ T}+}{2},\] (2)
* _(Nesterov, 2018)_ _If_ \(f\) _is_ \(L\)_-smooth, then for all_ \(<\) _the average iterate satisfies_ \[f(_{T})-f_{*}^{2}}{4+T L(2-L)}.\] (3)

Minimizing eq. (2) over \(\) gives \(f(_{T})-f_{*}=(G}{})\) with \(=}{G}\). We have several remarks to make about this rate for gradient descent. First, the optimal stepsize depends on both the distanceto the optimum \(D_{0}\) and the Lipschitz constant \(G\), and in fact, this rate is in general optimal (Nesterov, 2018, Theorem 3.2.1). Moreover, if we misspecify \(D_{0}\) or \(G\) while tuning \(\), this does not in general result in divergence but may result in a slower rate of convergence. On the other hand, for the smooth setting the optimal stepsize is \(=\) for which \(f(x_{T})-f_{*}(^{2}}{T})\). Unfortunately, to obtain this rate we have to estimate the smoothness constant \(L\) in order to choose a stepsize \(<\), and this dependence is _hard_: if we overshoot the upper bound \(\), the iterations of gradient descent can diverge very quickly, as shown by Figure 1. Therefore, GD with a constant stepsize cannot be _universal_: we have to set the stepsize differently for smooth and nonsmooth objectives.

Normalized Gradient Descent (NGD) (Shor, 2012) consists of iterates of the form

\[x_{t+1}=_{}(x_{t}-)}{\| f(x_ {t})\|}).\] (NGD)

The projection step above is not necessary, and the results for NGD also hold in the unconstrained setting where the projection on \(=^{d}\) is just the identity. NGD has many benefits: it can escape saddle points that GD may take arbitrarily long times to escape (Murray et al., 2019), and can minimize functions that are quasi-convex and only locally Lipschitz (Hazan et al., 2015). One of the main benefits of normalized gradient descent is that normalization makes the method scale-free: multiplying \(f\) by a constant factor \(>0\) and minimizing \( f\) does not change the method's trajectory at all. This allows it to adapt to the Lipschitz constant \(G\) in nonsmooth optimization as well as the smoothness constant \(L\) for smooth objectives, as the following theorem states:

**Theorem 2**.: _Under the same conditions as Theorem 1, the iterations generated by generated by_ (NGD) _satisfy after \(T\) steps satisfy:_

* _(Nesterov, 2018)_ _If_ \(f\) _is_ \(G\)_-Lipschitz, the minimal function suboptimality satisfies_ \[_{k\{0,1,,T-1\}}[f(x_{k})-f_{*}] G[^{2}}{2 T}+],\] (4) _where_ \(D_{0}}}{{=}}\|x_{0}-x_{*}\|\)_._
* _(Levy, 2017; Grimmer, 2019)_ _If_ \(f\) _is_ \(L\)_-Lipschitz, the minimal function suboptimality satisfies_ \[_{k=0,,T-1}[f(x_{k})-f_{*}][^{2}}{2 T}+]^{2}.\] (5)

Tuning eq. (4) in \(\) gives \(=}{}\), and the stepsize is also optimal for eq. (5). This gives a convergence rate of \(G}{}\) when \(f\) is Lipschitz and \(^{2}L}{T}\) when \(f\) is smooth. Observe that NGD matches the dependence of gradient descent on \(G\) and \(L\) without any knowledge of it. Furthermore that, unlike GD where the optimal stepsize is \(\) in the smooth setting and \(}{G}\) in the nonsmooth

Figure 1: Two trajectories of gradient descent on the one-dimensional quadratic \(f(x)=}{2}\), with \(L=100\).

setting. The optimal stepsize for NGD is the same in both cases. Therefore, NGD is _universal_: the same method with the same stepsize adapts to nonsmooth and smooth objectives. Moreover, misspecification of the stepsize in NGD does not result in divergence, but just slower convergence. Another interesting property is that we only get a guarantee on the best iterate: this might be because NGD is non-monotonic, as Figure 2 (a) shows.

Edge of Stability Phenomena.We may reinterpret NGD with stepsize \(\) as simply GD with a time-varying "effective stepsize" \(_{,t}=)\|}\). We plot this effective stepsize for an \(_{2}\)-regularized linear regression problem in Figure 2 (b). Observe that the stepsize sharply increases, then decreases until it starts oscillating around \(\). Recall that \(\) is the edge of stability for gradient descent: its iterates diverge when the stepsize crosses this threshold. Arora et al. (2022) observe this phenomenon for NGD, and give a detailed analysis of it under several technical assumptions and when the iterates are close to the manifold of local minimizers.

Theorem 2 offers an alternative, global, and less explicit explanation of this phenomenon: NGD matches the optimal gradient descent rate, and in order to do so it must drive the effective stepsize to be large. Specifically, suppose that we use the optimal stepsize \(=}{}\), and call the best iterate returned by NGD \(x_{}\). Then \(x_{}\) satisfies \(f(x_{})-f_{*}^{2}L}{T}\) and therefore by smoothness

\[\| f(x_{})\|)-f_{*})}D_{0}^{2}}{T}}=}{}=L.\]

This implies \(_{,}=)\|}\). Therefore the effective stepsize at convergence is forced to grow to \(()\). But if the effective stepsize increases too much and crosses the threshold \(\), the gradient norms start diverging, forcing the effective stepsize back down. Thus, NGD is _self-stabilizing_. We note that Edge of Stability phenomenon is not unique to NGD, and GD itself trains at the edge of stability for more complicated models where the smoothness also varies significantly over training (Cohen et al., 2021; Damian et al., 2023).

### DoWG

We saw in the last section that NGD adapts to both the Lipschitz constant \(G\) and the smoothness \(L\), but we have to choose \(\) to vary with the distance to the optimum \(D_{0}=\|x_{0}-x_{*}\|\). In this section, we develop a novel algorithm that adaptively estimates the distance to the optimum, and attains the optimal convergence rate of gradient descent for constrained convex and smooth optimization up to a logarithmic factor. Our algorithm builds upon the recently proposed Distance over Gradients (DoG)

Figure 2: NGD iterations on \(_{2}\)-regularized linear regression on the mushrooms dataset from LibSVM (Chang and Lin, 2011) with \(=0.1\). Top (a) shows the function suboptimality over time. Observe that as the number of iterations grow, the method becomes non-monotonic. Bottom (b) shows the effective stepsize \(_{,t}=)\|}\) over time.

algorithm developed by Ivgi et al. (2023). We call the new method DoWG (Distance over Weighted Gradients), and we describe it as Algorithm 1 below.

```
1Input: initial point \(x_{0}\) Initial distance estimate \(r_{}>0\).
2Initialize\(v_{-1}=0,r_{-1}=r_{}\).
3for\(t=0,1,2,,T-1\)do
4 Update distance estimator: \(_{t}(\|x_{t}-x_{0}\|,\,_{t-1})\)
5 Update weighted gradient sum: \(v_{t} v_{t-1}+_{t}^{2}\| f(x_{t})\|^{2}\)
6 Set the stepsize: \(_{t}_{t}^{2}}{}}\)
7 Gradient descent step: \(x_{t+1}_{}(x_{t}-_{t} f(x_{t}))\)
8
9 end for
```

**Algorithm 1**DoWG: Distance over Weighted Gradients

DoWG uses the same idea of estimating the distance from the optimum by using the distance from the initial point as a surrogate, but instead of using the square root of the running gradient sum \(G_{t}=_{k=0}^{t}\| f(x_{k})\|^{2}\) as the normalization, DoWG uses the square root of the weighted gradient sum \(v_{t}=_{k=0}^{t}_{k}^{2}\| f(x_{k})\|^{2}\). Observe that because the estimated distances \(_{t}\) are monotonically increasing, later gradients have a larger impact on \(v_{t}\) than earlier ones compared to \(G_{t}\). Therefore, we may expect this to aid the method in adapting to the local properties of the problem once far away from the initialization \(x_{0}\). We note that using a weighted sum of gradients is not new: AceeleGrad (Levy et al., 2018) uses time-varying polynomial weights and Adam (Kingma and Ba, 2015) uses exponentially decreasing weights. The difference is that DoWG chooses the weights adaptively based on the running distance from the initial point. This use of distance-based weighted averaging is new, and we are not aware of any previous methods that estimate the running gradient sum in this manner.

**Nonsmooth analysis.** The next theorem shows that DoWG adapts to the Lipschitz constant \(G\) and the diameter \(D\) of the set \(\) if the function \(f\) is nonsmooth but \(G\)-Lipschitz. We use the notation \(_{+}x= x+1\) following (Ivgi et al., 2023).

**Theorem 3**.: _(DoWG, Lipschitz \(f\)). Suppose that the function \(f\) is convex, \(G\)-Lipschitz, and has a minimizer \(x_{*}\). Suppose that the domain \(\) is a closed convex set of (unknown) diameter \(D>0\). Let \(r_{}<D\). Then the output of Algorithm 1 satisfies for some \(t\{0,1,,T-1\}\)_

\[f(_{t})-f_{*}=[}}_{+}}],\]

_where \(_{t}}}{{=}}^{t-1} _{k}^{2}}_{k=0}^{t-1}_{k}^{2}x_{k}\) is a weighted average of the iterates returned by the algorithm._

**Discussion of convergence rate.** DoWG matches the optimal \((}})\) rate of tuned GD and tuned NGD up to an extra logarithmic factor. We note that the recently proposed algorithms DoG (Ivgi et al., 2023) and D-Adaptation (Defazio and Mishchenko, 2023) achieve a similar rate in this setting.

**Comparison with DoG.** As we discussed before, DoWG uses an adaptively weighted sum of gradients for normalization compared to the simple sum used by DoG. In addition, DoG uses the stepsize \(_{t}}{^{t}\| f(x_{k})\|^{2}}}\), whereas the DoWG stepsize is pointwise larger: since \(_{k}^{2}\) is monotonically increasing in \(k\) we have

\[_{,t}=_{t}^{2}}{^{t}_{k}^{2 }\| f(x_{k})\|^{2}}}_{t}^{2}}{_{t}^{t}\| f(x_{k})\|^{2}}}=_{t}}{^ {t}\| f(x_{k})\|^{2}}}=_{,t}.\]

Of course, the pointwise comparison may not reflect the practical performance of the algorithms, since after the first iteration the sequence of iterates \(x_{2},x_{3},\) generated by the two algorithms can be very different. We observe in practice that DoWG is in general more aggressive, and uses larger stepsizes than both DoG and D-Adaptation (see Section 4).

**Smooth analysis.** Our next theorem shows that DoWG adapts to the smoothness constant and the diameter \(D\) of the set \(\).

**Theorem 4**.: _(DoWG, Smooth \(f\)). Suppose that the function \(f\) is \(L\)-smooth, convex, and has a minimizer \(x_{*}\). Suppose that the domain \(\) is a closed convex set of diameter \(D>0\). Let \(r_{}<D\). Then the output of Algorithm 1 satisfies for some \(t\{0,1,,T-1\}\)_

\[f(_{t})-f_{*}=[}{T}_{+}}],\]

_where \(_{t}}{{=}}^{T}_ {k}^{2}}_{k=0}^{t-1}_{k}^{2}x_{k}\) is a weighted average of the iterates returned by the algorithm._

The proof of this theorem and all subsequent results is relegated to the supplementary material. We note that the proof of Theorem 4 uses the same trick used to show the adaptivity of NGD to smoothness: we use the fact that \(\| f(x)\|)}\) for all \(x\) applied to a carefully-chosen weighted sum of gradients.

**Comparison with GD/NGD.** Both well-tuned gradient descent and normalized gradient descent achieve the convergence rate \((_{0}}{T})\) where \(D_{0}=\|x_{0}-x_{*}\| D\) for the constrained convex minimization problem. Theorem 4 shows that DoWG essentially attains the same rate up to the difference between \(D_{0}\) and \(D\) and an extra logarithmic factor. In the worst case, if we initialize far from the optimum, we have \(D_{0} D\) and hence the difference is not significant. We note that DoG (Ivgi et al., 2023) suffers from a similar dependence on the diameter \(D\) of \(\), and can diverge in the unconstrained setting, where \(\) is not compact. This can be alleviated by making the stepsize smaller by a polylogarithmic factor. A similar reduction of the stepsize also works for DoWG, and we provide the proof in Section 7 in the supplementary.

**Comparison with DoG.** After the initial version of this paper, Ivgi et al. (2023) reported a convergence guarantee for the _unweighted_ average \(_{T}=_{k=0}^{T-1}x_{k}\) returned by DoG. In particular, Proposition 3 in their work gives the rate

\[f(_{T})-f_{*}=(_{+}_{T}}{ r_{}}+_{T})^{2}}{T})=(}{T}_{+} ^{2}_{}}).\]

where \(D_{0}=\|x_{0}-x_{*}\|\), and where in the second step we used the bound \(D_{0} D\) and \(_{T} D\). This rate is the same as that achieved by the weighted average of the DoWG iterates up to an extra logarithmic factor \(_{+}_{*}}\). We note that DoG also has a guarantee in the stochastic setting, provided the gradients are bounded locally with a known constant, while in this work we have focused exclusively on the deterministic setting.

Figure 3: DoWG iterations on \(_{2}\)-regularized linear regression on the mushrooms dataset from LibSVM (Chang and Lin, 2011) with \(r_{}=10^{-6}\). Top (a) shows the function suboptimality over time. Observe that as the number of iterations grow, the method becomes non-monotonic. Bottom (b) shows the DoWG stepsize over time.

**Edge of Stability.** Like NGD, DoWG also tends to increase the stepsize and train at the edge of stability. The intuition from NGD carries over: in order to preserve the convergence rate of GD, DoWG tends to drive the stepsize larger. However, once it overshoots, the gradients quickly diverge, forcing the stepsize back down. Figure 3 shows the performance of DoWG and its stepsize on the same regularized linear regression problem as in Figure 2. Comparing the two figures, we observe that DoWG is also non-monotonic and trains close to the edge of stability, but its stepsize oscillates less than NGD's effective stepsize.

**Universality.** Theorems 4 and 3 together show that DoWG is universal, i.e. it almost recovers the convergence of gradient descent with tuned stepsizes in both the smooth and nonsmooth settings. As the optimal stepsize for gradient descent can differ significantly between the two settings, we believe that achieving both rates simultaneously without any parameter-tuning or search procedures is a significant strength of DoWG.

## 4 Experimental results

We compare DoWG to DoG, L-DoG from Ivgi et al. (2023), for all of which we also report performance of the polynomially-averaged iterate with power 8 as recommended by Ivgi et al. (2023). We also add comparison against Adam (Kingma and Ba, 2015) with cosine annealing and the standard step size \(10^{-3}\). All methods are used with batch size 256 with no weight decay on a single RTX3090 GPU. We plot the results in Figure 4 with the results averaged over 8 random seeds. We train the VGG11 (Simonyan and Zisserman, 2015) and ResNet-50 (He et al., 2016) neural network architectures on CIFAR10 (Krizhevsky, 2009) using PyTorch (Paszke et al., 2019), and implement1 DoWG on top of the DoG code2. Unsurprisingly, DoWG's estimates of the step size are larger than that of DoG and D-Adapt-norm, which also makes it less stable on ResNet-50. While the last iterate of DoWG gives worse test accuracy than Adam, the average iterate of DoWG often performs better.

Finally, we note that while both neural networks tested are generally nonsmooth, recent work shows _local_ smoothness can significantly influence and be influenced by a method's trajectory (Cohen et al., 2022; Pan and Li, 2022). We believe this adaptivity to smoothness might explain the empirical difference between DoWG and DoG, but leave a rigorous discussion of adaptivity to local smoothness to future work.

Figure 4: VGG11 (top) and ResNet-50 (bottom) training on CIFAR10. Left: test accuracy, middle: train loss, right: step sizes.