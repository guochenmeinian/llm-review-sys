# Continuous Contrastive Learning for Long-Tailed Semi-Supervised Recognition

Zi-Hao Zhou\({}^{1,2}\)1 Siyuan Fang\({}^{1,2}\)1 Zi-Jing Zhou\({}^{3}\)

Tong Wei\({}^{1,2}\)2

**Yuanyu Wan\({}^{4}\) Min-Ling Zhang\({}^{1,2}\)**

\({}^{1}\)School of Computer Science and Engineering, Southeast University, Nanjing, China

\({}^{2}\)Key Laboratory of Computer Network and Information Integration (Southeast University),

Ministry of Education, China

\({}^{3}\)Xiaomi Inc., China

\({}^{4}\)School of Software Technology, Zhejiang University, Ningbo, China

{zhouzih, syfang, weit}@seu.edu.cn

equal contributioncorresponding author

###### Abstract

Long-tailed semi-supervised learning poses a significant challenge in training models with limited labeled data exhibiting a long-tailed label distribution. Current state-of-the-art LTSSL approaches heavily rely on high-quality pseudo-labels for large-scale unlabeled data. However, these methods often neglect the impact of representations learned by the neural network and struggle with real-world unlabeled data, which typically follows a different distribution than labeled data. This paper introduces a novel probabilistic framework that unifies various recent proposals in long-tail learning. Our framework derives the class-balanced contrastive loss through Gaussian kernel density estimation. We introduce a continuous contrastive learning method, CCL, extending our framework to unlabeled data using _reliable_ and _smoothed_ pseudo-labels. By progressively estimating the underlying label distribution and optimizing its alignment with model predictions, we tackle the diverse distribution of unlabeled data in real-world scenarios. Extensive experiments across multiple datasets with varying unlabeled data distributions demonstrate that CCL consistently outperforms prior state-of-the-art methods, achieving over 4% improvement on the ImageNet-127 dataset. Our source code is available at https://github.com/zhouzihao11/CCL.

## 1 Introduction

Semi-supervised learning (SSL) serves as a powerful approach for improving the generalization capabilities of deep neural networks (DNNs) in scenarios where labeled data is scarce . The core concept of SSL methods typically involves assigning pseudo-labels to unlabeled data and utilizing those with high confidence for model training . However, many existing SSL algorithms presuppose a balanced label distribution across both labeled and unlabeled datasets. In real-world applications, datasets commonly exhibit a long-tailed label distribution , leading to biased pseudo-label generation favoring majority classes . This discrepancy challenges the effectiveness of SSL algorithms in addressing real-world datasets.

The exploration of long-tailed semi-supervised learning (LTSSL) has gained momentum to address the challenge of biased pseudo-label distribution arising from class imbalance in labeled and unlabeled data. Recent LTSSL approaches propose compensating for the learning of minority classes bydistribution alignment [33; 63], data rebalancing [22; 38], and logit adjustment [64; 43] to rectify the pseudo-label distribution. However, existing approaches often assume the equivalence of the unlabeled data distribution with the labeled data or rely on predefined anchor distributions to estimate the unlabeled data distribution [64; 43]. Furthermore, these methods primarily focus on correcting model outputs without delving into the role of representation learning in improving performance.

This paper explicitly introduces an approach to obtain effective representations for long-tail learning by adopting an information-theoretic view of DNNs. We present a probabilistic framework that utilizes the deep variational information bottleneck method  to learn good representations and demonstrate its unification of recent long-tail learning proposals, such as logit adjustment  and balanced softmax , through approximating the density of class-conditional distribution in different ways. Specifically, our framework encompasses class-balanced supervised contrastive learning [73; 15] via Gaussian kernel density estimation. We extend this framework to address LTSSL by adapting the supervised contrastive loss to unlabeled data using "_continuous pseudo-labels_", derived from model predictions and propagated labels, to mitigate confirmation bias. To account for varying label distribution of unlabeled data, we progressively estimate the label distribution through a moving average and adjust model predictions to align with the estimated distribution.

In summary, our contributions are as follows:

1. [leftmargin=*]
2. We propose a probabilistic framework which unifies many recent proposals in long-tail learning. Specifically, popular class-balanced contrastive learning methods can be seen as special cases of our framework when approximating the density using a Gaussian kernel.
3. We generalize the proposed framework to LTSSL and present a continuous contrastive learning method based on reliable and smoothed pseudo-labels to address confirmation bias and improve the quality of learned representations.
4. We conduct extensive experiments across several LTSSL datasets with diverse label distributions of unlabeled data. The results show that our proposal substantially outperforms previous state-of-the-art methods.

## 2 A Probabilistic Framework for Long-Tail Learning

In this section, we first introduce a general framework for learning good representations. Then, we expand this framework to long-tail learning and illustrate how recent proposals can be regarded as specific instances of our framework through three ways for density approximation.

**Problem setup of long-tail learning.** We consider a \(C\)-class classification problem with instance space \(\) and target space \(=\{1,,C\}\). Let \(P_{s}\) and \(P_{t}\) denote the source (training) and test distributions on \((,)\), respectively. We denote by \(_{s}\) and \(_{t}\) the corresponding probability density (or mass) functions. Given a training dataset \(\{(_{i},y_{i})\}_{i=1}^{N}\), where \(_{i}^{d}\) is the training sample and \(y_{i}\) is the ground-truth label.

### Learning good representations from information theoretical view

In this subsection, we rethink one of the most popular approaches to deal with representation learning, i.e., contrastive learning. We derive many recent proposals in this branch from an information-theoretic view. Let \(Z\) denote the latent representation of \(X\) induced by the encoder \(()\) parameterized by \(\). From an information-theoretic view, an optimal representation \(Z\) is maximally informative about the target \(Y\), and minimally "memorizes" \(X\). The information bottleneck  adopts mutual information \(I()\) to measure information between two variables. Thus, optimal \(Z\) can be obtained by maximizing the following objective:

\[^{*}=_{}I(Z,Y;)- I(Z,X;),\] (1)

where \( 0\) is a tradeoff parameter. The variational information bottleneck  solves the above objective by variational inference. Based on the definition of \(I()\), we can rewrite Eq. (1) as:

\[I(Z,Y)- I(Z,X)= dyd(y,)(y )}{(y)}- dd(,)()}{()},\] (2)

where we omit \(\) for simplicity. Since \((y)= d()(y )\) is intractable in Eq. (2), let \(}(y)\) be a variational approximation to \((y)\) and considering that the Kullback-Leibler (KL)divergence \([(y),}(y)]\) is always positive, we have RHS of Eq. (2)'s lower bounded:

\[ ddyd()()( )}(y)- dd ()()( )}{()}.\] (3)

Suppose we use an encoder of the form \(()((),^{2})\) and \(()(,)\), the second term of Eq. (3) equals the KL divergence \([(),()]\). Since \(()\) and \(()\) are normal distributions, it can be rewritten as: \(- l(^{2}-1-2)- \|()\|^{2}\), where \(l\) is the dimension of \(\). For a deterministic model, \(\) is almost unique for each \(\), thus assuming \(\) is a small constant close to 0. By integrating out \(()\) and discarding constant terms, maximizing Eq. (3) can be approximated by minimizing:

\[- d() dy(y)}(y())+\|() \|^{2}.\] (4)

In the following of this paper, we denote the output of \(()\) as \(\) (or \(_{}\) for a particular sample \(\)) for simplicity. Minimizing Eq. (4) is equivalent to minimizing the following objective in the distribution of test data on each \(\):

\[-_{k[C]}_{t}(Y=k)}_{t}(Y=k )+\|\|^{2}.\] (5)

Notably, Eq. (5) can be seen as a general framework for learning good representations. If \(_{s}(Y)=_{t}(Y)\), \(_{t}(Y=y)\) can simply be substituted by the ground-truth labels of training samples. However, in long-tail learning, the class-probability function \(_{t}(Y=y)\) is different from that of the training data due to label distribution shift.

### Probabilistic framework for long-tailed supervised learning

Since \(_{s}(Y=y)_{t}(Y=y)\), we cannot directly solve Eq. (5). However, since long-tail learning typically assumes that \(_{t}(Y)\) is uniform and we work with the label shift assumption, i.e., \(_{s}( Y=y)=_{t}( Y=y)\), we can obtain \(_{t}(Y=y)\) by Bayes' theorem:

\[_{t}(Y=y)=( Y=y)}{_{k[C] }( Y=k)}=_{s}(Y=y)}_{ s}(Y=k)}{_{k[C]}_{s}(Y=k)}.\] (6)

Throughout the paper, we use the notation \(( Y=y)\) to represent either \(_{s}( Y=y)\) or \(_{t}( Y=y)\). In practice, \(\|\|^{2}\) can be omitted in optimization because normalization is commonly adopted in deep learning. In long-tail learning, minimizing Eq. (5) equals to minimizing:

\[-_{k[C]}_{s}(Y=k)}_{s}(Y=k) }_{t}(Y=k).\] (7)

According to Jensen's inequality, Eq. (7) attains its minimum value if and only if \(}_{t}(Y=k)_{s}(Y=k)_{ s}(Y=k)\) for \(k[C]\). Hence, Eq. (7) can be replaced as follows:

\[J=-_{k[C]}(Y=k)}{_{s}(Y=k)}_{s}(Y=k )}(Y=k),\] (8)

where \(}(Y=y)=}_{t}(Y=y| )(Y=y)}{_{k[C]}_{t}(Y=k)(Y=k)}\) and \((Y)\) is an arbitrarily label distribution. Eq. (8) can be seen as an extension of sample reweighting  and logit adjustment  using probabilistic labels rather than discrete labels when \((Y)\) is specified as \(_{t}(Y)\) and \(_{s}(Y)\), respectively. Besides, Eq. (8) presents a unified framework that consolidates existing long-tail learning methods by estimating \(}( Y=y)\) or \(}_{t}(Y=y)\) in different ways. In the following, we discuss three ways to estimate these terms.

**Method 1:** Explicitly specify \(}( Y=y)\) as a prior distribution such as vMF distribution  and Wrapped Cauchy Distribution .

**Method 2:** Approximate \(}( Y=y)\) using a learnable linear classifier. Let \(\{_{i},b_{i}\}_{i=1}^{C}\) denote the parameters of a linear layer, which is followed by a softmax to obtain the normalized probability:

\[}( Y=y)}_{t}(Y=y)=^{}_{y}+b_{y})}{_{k[C]} (^{}_{k}+b_{k})}.\] (9)

**Method 3:** Approximate \(}( Y=y)\) via the Gaussian kernel. A new sample from class \(y\) should be closer to all samples within class \(y\) and away from samples from other classes. Using the expected similarity among all samples within the class to measure distance, we derive:

\[}( Y=y)}_{t}(Y=y)=_{^{}( Y=y)}[ (_{},_{^{}})]}{_{k[C]} _{^{}( Y=k)}[( _{},_{^{}})]},\] (10)

where \((,)\) represents the similarity between two samples, when we use Gaussian kernel \((_{},_{^{}})=(_{} {z}_{^{}})\) and approximate expectation through empirical batch \(=_{k[C]}_{k}\), that is \(_{^{}( Y=y)}[(_{ {x}},_{^{}})]_{y}|}_{^{}_{y}}(_{}_{^{}})\), Eq. (10) can be instantiated as:

\[}_{t}(Y=y)=_{y}|}- _{^{}_{y}\{\}}(_{ }_{^{}})}{_{k[C]}_{k}|}_{^{}_{k}}(_{} _{^{}})}.\] (11)

Interestingly, we observe that Eq. (11) resembles class-balanced contrastive loss. In the appendix, we also show that the Gaussian kernel approximation is identical to conventional supervised contrastive learning if the training data are class-balanced.

Notably, to ensure the computability of \(_{^{}( Y=y)}[(_{ {x}},_{^{}})]\) in Eq. (10), it is essential to ensure that samples are available from each class. Existing methods address this by class-wise queues, class-wise centers, or class-wise vMF distribution, details of which are provided in the appendix.

Based on the above three density approximation methods, we find that many recent proposals in long-tail learning can be derived from our framework. In Table 1, we summarize existing methods based on the way they estimate the density, tackle the training/test label shift, and guarantee the computation of Eq. (10) in mini-batch training.

## 3 CCL: Continuous Contrastive Learning

In this section, we introduce the proposed algorithm CCL, which extends the class-balanced contrastive learning presented in Eq. (8) with Gaussian kernel estimation in Eq. (11) to LTSSL.

### Problem setup of long-tailed semi-supervised learning

Let \(P_{l}\) and \(P_{u}\) denote the joint distribution \((,)\) for labeled data and unlabeled data, respectively. We denote by \(_{l}\) and \(_{u}\) the corresponding probability density (or mass) functions. We possess a labeled dataset \(\{(_{i}^{l},y_{i}^{l})\}_{i=1}^{N}\) of size \(N\) and an unlabeled dataset \(\{_{i}^{u}\}_{j=1}^{M}\) of size \(M\), where \(_{i}^{l},_{j}^{u}^{d}\). The proportion of labeled data from the entire dataset is \(=\). Denote the number of labeled data for class \(i\) as \(N_{i}\), we have \(N_{1} N_{2} N_{C}\) if the classes are sorted by cardinality in decreasing order. The imbalance ratio of labeled data is given by \(_{l}=}{N_{C}}\), while the distribution of the label of the unlabeled data and its imbalance ratio \(_{u}\) are unknown. The components of CCL include a feature extractor, two linear classifiers \(f_{s}(),f_{b}()\) and a contrastive feature projection head \(g()\).

  
**Method** & **Density estimation** & **Label distribution shift** & **Mini-batch computation of Eq. (10)** \\  BALMS  & Linear layer & Reweighting & – \\ LA  & Linear layer & Logit adjustment & – \\  BCL  & Gaussian kernel & Reweighting & Class-wise center \\ GML  & Gaussian kernel & Logit adjustment & Class-wise queue \\ KCL  & Gaussian kernel & Balanced resampling & \(\) \\ PaCo  & Gaussian kernel & Logit adjustment & Class-wise center \\ Proco  & Gaussian kernel & Logit adjustment & Class-wise vMF distribution \\  T-vMF  & T-vMF distribution & Logit adjustment & – \\ WCDAS  & Wrapped Cauchy distribution & Logit adjustment & – \\   

Table 1: A unified view of popular long-tail learning methods from our framework. “–” means that this method does not involve this issue and “\(\)” indicates that the method has not resolved the issue.

### Balanced classifier training with estimated class prior

We develop our method based on FixMatch  following previous works [48; 64], and its objective is: \(}_{}=}_{l}+}_{u}\), where \(}_{l}\) is a traditional cross-entropy loss. For unlabeled data, the method operates by first generating pseudo-labels for unlabeled data using the model's predictions and selecting unlabeled data whose predicted maximum confidence is higher than a predefined threshold. The consistency regularizer \(}_{u}\) is then applied to two views of each selected sample.

**Balanced FixMatch for LTSSL.** First, since the labeled data follow a long-tailed distribution, which we denote as \(^{l}\), \(}_{l}\) needs to be adjusted by logit adjustment  via Eq. (8):

\[}_{l}(^{l},y^{l})=-}_{t}(Y=y^{l}^{l};f_{b})_{y^{l}}^{l}}{_{k[C]}}_{t}(Y=k ^{l};f_{b})_{k}^{l}}.\] (12)

Second, FixMatch is prone to fit wrong pseudo-labels with high predictive confidence during training [62; 3]. However, since the unlabeled data label distribution is inaccessible, pseudo-labels generated by the classifier may be sub-optimal for its adherence to a uniform distribution. By Bayes' theorem, with given estimated unlabeled data label distribution \(}^{u}\), the "post-adjusted" model outputs for sample \(^{u}\) can be formulated as:

\[}_{u}(Y=y^{u};f_{b})=}_{t}(Y=y^{u};f_{b})_{y}^{u}}{_{k[C]}}_{t}(Y=k^{u};f_{b})_{k}^{u}},\] (13)

Given pseudo-label \(=_{k[C]}}_{u}(Y=k_{w}(^{u});f_{b})\), where \(_{w}()\) denotes the weak data augmentation, \(}_{u}\) is rewritten as:

\[}_{u}(^{u},)=-( ^{u})}_{t}(Y= _{s}(^{u});f_{b})_{}^{u }}{_{k[C]}}_{t}(Y=k_{s}( ^{u});f_{b})_{k}^{u}},\] (14)

where \(()\) denotes the sample mask to select reliable pseudo-labels. We progressively update \(}^{u}\) using the exponential moving average (EMA) for each mini-batch by \(_{y}^{u}=(1-)_{y}^{u}+|}_{^{u}}}_{u}(Y=y ^{u};f_{b}),\) where \(\) is a momentum updating parameter and \(\) denotes an unlabeled data subset. Directly using confidence selection can lead to a selected \(\) with poor calibration due to model overconfidence [41; 45]. Thus, the energy score  is adopted to filter out reliable unlabeled data, which is defined as \(E()=-T_{k[C]}e^{f_{k}()/T}\), where \(T\) is the temperature and \(f()\) denotes the logits of \(\). We select reliable unlabeled data by \(^{E}(^{u}):=(E(^{u}))\) using a predefined threshold \(\), and construct \(=\{^{u} ^{E}() 0\}\) for the estimation of \(}_{u}\).

**Dual-branches training.** Based on the observation that class-balanced training can be harmful to representation learning, previous works [38; 64] have utilized another branch of the network for standard training. In contrast to the balanced branch, the standard branch, denoted as \(f_{s}()\), optimizes the cross-entropy loss without employing logit adjustment to fit the original training data distribution. We fuse the predictions of balanced and standard branches to reduce the confirmation bias of pseudo-labels by:

\[}^{}(Y=y^{u})= }_{u}(Y=y^{u};f_{b}) +}(Y=y^{u};f_{s} )_{y}^{*}}{_{k[C]}}(Y=k ^{u};f_{s})_{k}^{*}}.\] (15)

The rationale behind the equation is that the standard branch necessitates the elimination of imbalanced label prior and then compensates for unlabeled label prior when predicting pseudo-labels, which is achieved by defining \(}^{*}=}^{u}}{}^{l}+}^{u}}\). Overall, the classification loss \(}_{}\) is the combination of losses for learning \(f_{s}()\) and \(f_{b}()\).

### Continuous contrastive loss with reliable pseudo-labels

Apart from the classification loss and consistency regularizer, we aim to improve the quality of representations by extending the framework presented in Section 2 to LTSSL. To achieve the adaptationof our framework, a primary obstacle must be addressed. The challenge arises from the unknown ground-truth label \(_{u}(Y=y)\) for unlabeled data, which results in the calculation of Eq. (8) infeasible. We propose to utilize the continuous pseudolabel \(}^{}(Y=y^{u})\) as derived from the classifier in Eq. (15). Furthermore, \(_{^{}( Y=y)}[(_ {},_{^{}})]\) in Eq. (10) can be extended to unlabeled data and approximated by an empirical data subset \(\):

\[_{^{}( Y=y)}[( _{},_{^{}})]^{}}(_{},_{^{ }})}^{}(Y=y^{} )}{_{^{}}}^{}(Y=y^{})}.\] (16)

Plugging Eq. (16) into Eq. (10), we obtain the continuous pseudo-label \(}_{t}(Y=y^{u};)\) for \(^{u}\). Similar to Eq. (14), logit adjustment is used to handle label shift of unlabeled data by Bayes' theorem, and we can obtain:

\[}_{}=-_{k[C]}}^{ }(Y=k^{u})}_{u}(Y=k {x}^{u};),\] (17)

where \(}_{u}(Y=y^{u};)=}_{t}(Y=y^{u};)}_{u}^{ u}}{_{k[C]}}_{t}(Y=k^{u};) }_{t}^{u}}\). So far, the generalized framework using continuous pseudo-labels for LTSSL is derived in Eq. (17), the critical issue is how to filter out a reliable unlabeled data subset \(^{u}\) such that the posterior estimation \(}^{}(Y=y)\) in Eq. (16) is calibrated. Similarly, directly using confidence selection may lead to model overconfidence. To mitigate the confirmation bias in pseudo-labels produced by the learned classifier, we propose using the energy score for data selection to ensure model calibration . Combining with labeled data, the loss \(}_{}\) is obtained with \(=\{^{l}(^{ u}^{E}() 0)\}\).

### Continuous contrastive loss with smoothed pseudo-labels

To further mitigate the impact of inaccurate pseudo-labels \(}^{}(Y=y^{u})\), we derive a complementary contrastive loss with smoothed pseudo-labels. Specifically, we propose aligning the representations of two views of a sample by imposing the weak-strong consistency regularization:

\[}_{}=-_{k[C]}}( Y=k_{w}(^{u}))} (Y=k_{s}(^{u})).\] (18)

In this part, we aim to derive \(}(Y=y^{u})\) by propagating labels from nearby samples in the contrastive space. On the one hand, we take labeled data for \(\) in Eq. (11) and construct the posterior for unlabeled data. Logit adjustment is employed for tackling label shift of unlabeled data:

\[}(Y=y^{u};^{l})=_{y}|}_{^{}_{y}}(_{ ^{u}},_{^{}})_{y}^{u}}{_{k [C]}_{k}|}_{^{}_{k}} (_{^{u}},_{^{}})_{k}^{u}}.\] (19)

Eq. (19) can be viewed as a process of propagating labels from labeled data to unlabeled data. On the other hand, we consider label propagation within unlabeled data, i.e., an unlabeled batch \(^{u}\) is used to estimate \(}(Y=y^{u};)\). Assuming there is a sufficient amount of unlabeled data, we have \(^{u}|}_{^{u}^{u}}}(Y=y^{u};^{u})_{y}^{u}\), hence the posterior can be approximated as:

\[}(Y=y^{u};^{u})^{}^{u}}(_{^{u}},_{ ^{}})}(Y=y^{}; ^{u})}{_{^{}^{u}}(_{^{u}},_{^{}})}.\] (20)

Let \((Y;)\) represent a matrix stacked by \([(Y=1),,(Y=C)]^{}\) of \(\) from \(\), we can rewrite Eq. (20) in the form of matrix multiplication: \(}(Y^{u};^{u})= }(Y^{u};^{u}),\) where \(\) is a similarity matrix and \(G_{ij}=_{_{i}},_{_{j}})}{_{ _{j}_{u}}(_{_{i}},_{_{j}} )}\). It can be interpreted that similar samples possess similar labels. By aggregating the predictions of labeled data and unlabeled data with a fixed hyperparameter \(\), we obtain:

\[}(Y^{u})= }(Y^{u})+(1-)} (Y^{u};^{l})\] (21) \[}(Y^{u})=(1- )(-)^{-1}}(Y^{u}; ^{l}).\]

Subsequently, Eq. (21) can be plugged into Eq. (18) for calculating \(}_{}\). To sum up, the total objective of CCL is:

\[}_{}=_{1}}_{}+(1-_{1})\,}_{}+_{2}}_{}\] (22)

where \(_{1}\) and \(_{2}\) are two hyperparameters.

## 4 Experiments

In this section, we conducted comprehensive experiments to verify the effectiveness of the proposed continuous contrastive learning method (CCL) on CIFAR10-LT, CIFAR100-LT, STL10-LT, and ImageNet-127 [29; 18] datasets. To simulate real-world unlabeled data, we tested our method on diverse label distributions of unlabeled data. Due to limited space, we defer the detailed experimental settings to the appendix.

### Results on CIFAR10/100-LT and STL10-LT

For consistent (\(_{l}=_{u}\)) setting, results are presented in Table 2. From the results, we can see that CCL consistently outperforms all comparison methods by a large margin. In particular, CCL improves the previous state-of-the-art approach ACR by 2.8% on average. This verifies that the representations learned by our proposed contrastive losses are more discriminative because both CCL and ACR utilize a dual-branch network.

For inconsistent (\(_{l}_{u}\)) setting, we present the results in Table 3 and Table 4. Following prior works, we compare all methods using unlabeled data following uniform and reversed label distributions on CIFAR10/100-LT datasets. On the STL10-LT dataset, the underlying unlabeled data distribution is naturally inaccessible. In general, CCL achieves the best results in all settings. Particularly, CCL obtains an average performance gain of 1.6% over ACR without using predefined anchor distributions. The results indicate that our method is able to accurately estimate the unlabeled data distribution with calibrated pseudo-labels.

    &  &  \\   & =100\)} & =150\)} & =150\)} & =7\)} & =10\)} & =7\)} & =20\)} \\  Algorithm & =500\)} & =1500\)} & =500\)} & =1500\)} & =500\)} & =500\)} & =500\)} & =150\)} \\  & =4000\)} & =3000\)} & =4000\)} & =3000\)} & =4000\)} & =3000\)} & =400\)} & =300\)} \\  Supervised & 47.3\(\)0.95 & 61.9\(\)0.41 & 44.2\(\)0.33 & 58.2\(\)0.29 & 29.6\(\)0.57 & 46.9\(\)0.22 & 25.1\(\)1.14 & 41.2\(\)0.15 \\ w/LA  & 53.3\(\)0.44 & 70.6\(\)0.21 & 49.5\(\)0.40 & 67.1\(\)0.78 & 30.2\(\)0.44 & 48.7\(\)0.89 & 26.5\(\)1.34 & 44.1\(\)0.42 \\  FixMatch  & 67.8\(\)1.13 & 77.5\(\)1.32 & 62.9\(\)0.36 & 72.4\(\)1.03 & 45.2\(\)0.55 & 56.5\(\)0.06 & 40.0\(\)0.96 & 50.7\(\)0.25 \\ w/DARP  & 74.5\(\)0.78 & 77.8\(\)0.63 & 67.2\(\)0.32 & 73.6\(\)0.73 & 49.4\(\)0.20 & 58.1\(\)0.44 & 43.4\(\)0.87 & 52.2\(\)0.66 \\ w/CRStri- & 76.3\(\)0.86 & 78.1\(\)0.42 & 67.5\(\)0.45 & 73.7\(\)0.34 & 44.5\(\)0.94 & 57.4\(\)0.18 & 40.1\(\)2.82 & 52.1\(\)0.21 \\ w/DASO  & 76.0\(\)0.37 & 79.1\(\)0.75 & 70.1\(\)1.81 & 75.1\(\)0.77 & 49.8\(\)0.24 & 59.2\(\)0.35 & 43.6\(\)0.09 & 52.9\(\)0.42 \\  FixMatch + LA  & 75.3\(\)2.45 & 82.0\(\)0.36 & 67.0\(\)2.49 & 78.0\(\)0.91 & 47.3\(\)0.42 & 58.6\(\)0.36 & 41.4\(\)0.93 & 53.4\(\)0.32 \\ w/DARP  & 76.6\(\)0.92 & 80.8\(\)0.62 & 68.2\(\)0.94 & 76.7\(\)1.31 & 50.5\(\)0.78 & 59.9\(\)0.32 & 44.4\(\)0.65 & 53.8\(\)0.43 \\ w/CRST+ & 76.7\(\)1.13 & 81.1\(\)0.57 & 70.9\(\)1.18 & 77.9\(\)0.71 & 44.0\(\)0.21 & 57.1\(\)0.55 & 40.6\(\)0.55 & 52.3\(\)0.20 \\ w/DASO  & 77.9\(\)0.88 & 82.5\(\)0.08 & 70.1\(\)1.68 & 79.0\(\)2.23 & 50.7\(\)0.51 & 60.6\(\)0.71 & 44.1\(\)0.61 & 55.1\(\)0.72 \\  FixMatch + ABC  & 78.9\(\)0.82 & 83.8\(\)0.36 & 66.5\(\)0.78 & 80.1\(\)0.45 & 47.5\(\)0.18 & 59.1\(\)0.21 & 41.6\(\)0.83 & 53.7\(\)0.55 \\ w/DASO  & 80.1\(\)1.16 & 83.4\(\)0.31 & 70.6\(\)0.80 & 80.4\(\)0.56 & 50.2\(\)0.62 & 60.0\(\)0.32 & 44.5\(\)0.25 & 55.3\(\)0.53 \\  FixMatch + ACR  & 81.6\(\)0.19 & 84.1\(\)0.39 & 77.0\(\)1.19 & 80.9\(\)0.22 & 51.1\(\)0.32 & 61.0\(\)0.41 & 44.3\(\)0.21 & 55.2\(\)0.28 \\ FixMatch + CPE  & 80.7\(\)0.96 & 84.4\(\)0.29 & 76.8\(\)0.53 & 82.3\(\)0.34 & 50.3\(\)0.34 & 59.5\(\)0.16 & 43.8\(\)0.28 & 55.6\(\)0.15 \\ FixMatch +**CCL** & **84.5\(\)**0.38 & **86.2\(\)**0.35 & **81.5\(\)**0.99 & **84.0\(\)**0.21 & **53.5\(\)**0.49 & **63.5\(\)**0.39 & **46.8\(\)**0.45 & **57.5\(\)**0.16 \\   

Table 2: Test accuracy in consistent setting on CIFAR10-LT and CIFAR100-LT datasets. The best results are in **bold**.

    & _{u}\))} & =\) N/A)} \\ 

### Results on ImageNet-127

ImageNet-127 is a naturally long-tailed dataset and has been used to test LTSSL methods in the recent literature . Following previous works, we downsample the original images to smaller sizes of 32\(\)32 or 64\(\)64 pixels using the box method from the Pillow library and randomly select 10% training samples to construct the labeled data. Learning discriminative representations and a balanced classifier is essential to achieve high performance. From the results in Table 5, we can see that CCLachieves superior results for image sizes of 32\(\)32 and 64\(\)64. It outperforms ACR by 4.3% and 4.2% in test accuracy.

### Comprehensive evaluation of the proposed method

**Understanding of balanced Fixmatch and dual-branch.** First, balanced Fixmatch can be viewed as a separate EM algorithm process , where the E-step involves estimating suitable pseudo-labels for unlabeled data through \(}^{u}\), and the M-step updates the model and obtains a new \(}^{u}\). As can be seen in Table 6, balanced Fixmatch achieves performance comparable to the recent state-of-the-art method ACR. Furthermore, dual-branch significantly enhances the performance of data under highly skewed long-tail distributions (consistent setting), with an averaged \(1.5\%\) improvement.

**How to estimate a relatively accurate \(}_{u}\)?** Our method for estimating \(}_{u}\) is equivalent to MLLS , which is an EM process. Accurate estimation of \(}_{u}\) is only achievable when the model is calibrated . Since the confirmation bias is induced by self-training, using confidence selection may result in overconfident but wrong pseudo-labels and hurt the calibration . In contrast, the energy score leverages the probability density of the predictions, exhibiting reduced vulnerability to overconfidence . Thus, we propose energy selection for a reliable unlabeled data subset on which the model is calibrated, thereby enabling the accurate estimation of \(}_{u}\). We use expected

    & & & &  &  \\  Dual-branch & Reliable PL & Smoothed PL & Energy mask & Con & Uni & Rev & Con & Uni & Rev \\   & & & & ✓ & 81.0 & 91.8 & 84.2 & 49.3 & 57.0 & 51.5 \\ ✓ & & & & ✓ & 82.1 & 92.0 & 84.5 & 51.2 & 57.3 & 52.1 \\ ✓ & & & ✓ & ✓ & 83.5 & 92.8 & 84.7 & 52.7 & 58.5 & 53.2 \\ ✓ & ✓ & & & ✓ & 83.2 & 92.7 & 84.8 & 51.9 & 58.4 & 53.2 \\ ✓ & ✓ & & ✓ & & 83.8 & 92.7 & 84.8 & 52.7 & 59.1 & 53.9 \\ ✓ & ✓ & ✓ & ✓ & & 84.3 & 93.1 & 85.0 & 53.5 & 59.9 & 54.4 \\   

Table 6: Ablation studies of our proposed algorithm. “Con”, “Uni”, and “Rev” represent consistent, uniform, and reversed, respectively.

Figure 1: Comparison of class prior estimation error and ECE on CIFAR100-LT.

    & \(_{u}=1\) (uniform) & \(_{u}=1/10\) (reversed) \\  Algorithm & \(N_{1}=50\) & \(N_{1}=150\) & \(N_{1}=50\) & \(N_{1}=150\) \\ \(M_{1}=400\) & \(M_{1}=300\) & \(M_{1}=400\) & \(M_{1}=300\) \\  FixMatch & 45 \(\) 0.71 & 58.1\(\) 0.72 & 44.2\(\) 0.43 & 57.3\(\) 0.19 \\ w/ DARP  & 43.5 \(\) 0.95 & 55.9\(\) 0.32 & 36.9\(\) 0.48 & 51.8\(\) 0.92 \\ w/CReST  & 43.5 \(\) 0.30 & 59.2\(\) 0.25 & 39.0\(\) 1.11 & 56.4\(\) 0.62 \\ w/CReST+  & 43.6 \(\) 1.60 & 58.7\(\) 0.16 & 39.1\(\) 0.77 & 56.4\(\) 0.78 \\ w/ DASO  & 53.9 \(\) 0.66 & 61.8\(\) 0.98 & 51.0\(\) 0.19 & 60.0\(\) 0.31 \\ w/ ACR  & 57.9 \(\) 0.56 & 65.8\(\) 0.91 & 51.7\(\) 0.22 & 63.3\(\) 0.17 \\ w/**CCL** & **59.8**\(\)0.28 & **67.9**\(\)0.70 & **54.4**\(\)0.14 & **64.7**\(\)0.22 \\   

Table 4: Test accuracy on CIFAR100-LT in uniform and reversed settings. The best results are in **bold**.

calibration error  (ECE) to assess model calibration. The tail of the curve of Figure 1c and 1d can be interpreted as overconfidence in false pseudo-labels caused by self-training. As can be seen in Figure 1a and 1b, the \(L_{1}\) distance between the true class prior of unlabeled data and \(}_{u}\), estimated from data subset selected using energy, is significantly smaller compared to when confidence is used for selection, inducing a more balanced classifier training.

**Continuous contrastive learning with reliable pseudo-labels.** We carried out a comparative experiment by removing the continuous reliable pseudo-labels loss. The results reflect an averaged \(0.8\%\) drop on CIFAR10/100-LT, demonstrating its efficacy for learning high-quality representation. Moreover, we verified that the data subset filtered by energy selection obtains excellent model calibration. Figure 1c and 1d show energy achieves better calibration than confidence thresholding.

**Continuous contrastive learning with smoothed pseudo-labels.** Similarly, we conducted a comparative experiment by removing the continuous smoothed pseudo-labels loss. As can be seen in Table 6, the performance decreases in all three settings on CIFAR10/100-LT datasets, showing the necessity for a consistency regularization constraint for feature alignment within the contrastive learning space.

### Results under more class distributions

Similar to ACR, to evaluate our method's effectiveness under more imbalanced settings, we conducted further experiments on CIFAR100-LT, maintaining a fixed \(_{l}=20\) and adjusting the imbalance ratio \(_{u}\) of the unlabeled data from consistent to reversed. We set \(N_{1}=50\) and \(M_{1}=400\) (with \(M_{C}=400\) in the reversed scenario) and compared the results with ACR as shown in Figure 2. The results demonstrate that our method consistently outperforms ACR in all scenarios.

## 5 Related Work

**Long-tailed learning (LTL).** Early strategies tackling LTL involve two aspects: resampling and reweighting. Resampling methods [9; 5; 8; 54] either undersample majority classes or oversample minority classes, which may result in information loss or overfitting. Reweighting methods [52; 16; 2; 12] assign different weights for each class or training sample. BBN  and Decoupling  claim that re-balancing can negatively impact representation. They propose a two-branch structure or a two-stage paradigm to address it. Logit adjustment methods [7; 44] learn larger margins for minority classes by obtaining optimal Bayesian classifiers. Recently, several methods [30; 15; 73; 20; 57] have been proposed to improve the representation learning based on supervised contrastive learning .

**Long-tailed semi-supervised learning (LTSSL).** Most semi-supervised learning (SSL) methods use unlabeled data by assigning pseudo-labels to unlabeled data [37; 6; 56; 71; 10] or aligning predictions of different views of the input by consistency regularization . PAWS  leverages self-supervised representations derived from unlabeled data, and RoPAWS  further refines the model predictions using labeled data through kernel density estimation. However, most of these works assume a balanced class distribution of labeled and unlabeled data, which may be violated in real-world applications.

Recently, LTSSL has gained considerable attention due to its applicability in numerous real-life scenarios. Recent works mitigate pseudo-labels bias by distribution alignment or label refinement [33; 63; 69]. Some others focus on balanced classifier training to overcome long-tailed label distribution [38; 22; 66]. Regrettably, these methods simply assume an identical long-tailed distribution for labeled and unlabeled data, which may still be unrealistic. Considering the unknown unlabeled data distribution, which can be mismatched with the labeled distribution, DASO  mixes the outputs of linear and semantic classifiers to improve the quality of pseudo-labels. ACR  and CPE  refine consistency regularization or train multiple expert branches based on predefined anchor distributions. However, how to improve representation learning in LTSSL is ignored in most existing works.

Figure 2: Generalize to more realistic LTSSL settings for ACR and CCL on CIFAR10/100-LT dataset in fixed \(_{l}\) and various \(_{u}\) settings.

Conclusion

This paper presents a probabilistic framework that unifies many recent methods in long-tail learning. Our framework is equivalent to supervised contrastive learning when approximating the class-conditional function using the Gaussian kernel. We further extend the contrastive learning objective to LTSSL based on continuous pseudo-labels to improve the learned representations. We utilize both reliable pseudo-labels generated by the model and smoothed pseudo-labels propagated from nearby samples to mitigate confirmation bias. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in all settings. We hope that our work can motivate more research for LTSSL from the perspective of representation learning.