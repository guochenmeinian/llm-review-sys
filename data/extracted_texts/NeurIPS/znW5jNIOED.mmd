# Optimizing over trained GNNs via symmetry breaking

Shiqiang Zhang

Imperial College London

London, UK

&Juan S. Campos

Imperial College London

London, UK

&Christian Feldmann

BASF SE

Ludwigshafen, Germany

&David Walz

BASF SE

Ludwigshafen, Germany

&Frederik Sandfort

BASF SE

Ludwigshafen, Germany

&Miriam Mathea

BASF SE

Ludwigshafen, Germany

&Calvin Tsay

Imperial College London

London, UK

&Ruth Misener

Imperial College London

London, UK

Corresponding author: s.zhang21@imperial.ac.uk

###### Abstract

Optimization over trained machine learning models has applications including: verification, minimizing neural acquisition functions, and integrating a trained surrogate into a larger decision-making problem. This paper formulates and solves optimization problems constrained by trained graph neural networks (GNNs). To circumvent the symmetry issue caused by graph isomorphism, we propose two types of symmetry-breaking constraints: one indexing a node 0 and one indexing the remaining nodes by lexicographically ordering their neighbor sets. To guarantee that adding these constraints will not remove all symmetric solutions, we construct a graph indexing algorithm and prove that the resulting graph indexing satisfies the proposed symmetry-breaking constraints. For the classical GNN architectures considered in this paper, optimizing over a GNN with a fixed graph is equivalent to optimizing over a dense neural network. Thus, we study the case where the input graph is not fixed, implying that each edge is a decision variable, and develop two mixed-integer optimization formulations. To test our symmetry-breaking strategies and optimization formulations, we consider an application in molecular design.

## 1 Introduction

Graph neural networks (GNNs) [1; 2; 3] are designed to operate on graph-structured data. By passing messages between nodes via edges (or vice versa), GNNs can efficiently capture and aggregate local information within graphs. GNN architectures including spectral approaches [4; 5; 6; 7; 8; 9] and spatial approaches [10; 11; 12; 13; 14; 15; 16; 17; 18], are proposed based on various motivations. Due to their ability to learn non-Euclidean data structure, GNNs have recently been applied to many graph-specific tasks, demonstrating incredible performance. In drug discovery, for example, GNNs can predict molecular properties given the graph representation of molecules [19; 20; 21; 22; 16; 23]. Other applications using GNNs include graph clustering[24; 25], text classification [26; 27], and social recommendation [28; 29].

Although GNNs are powerful tools for these "forward" prediction tasks, few works discuss the "backward" (or inverse) problem defined on trained GNNs. Specifically, many applications motivate the inverse problem of using machine learning (ML) to predict the inputs corresponding to some output specification(s). For example, computer-aided molecular design (CAMD) [30; 31; 32] aims todesign the optimal molecule(s) for a given target based on a trained ML model . Several GNN-based approaches have been proposed for CAMD , however, these works still use GNNs as "forward" functions to make predictions over graph-domain inputs. Therefore, both the graph structures of inputs and the inner structures of the trained GNNs are ignored. Figure 1 conceptually depicts the difference between forward and backward/inverse problems.

Mathematical optimization over trained ML models is an area of increasing interest. In this paradigm, a trained ML model is translated into an optimization formulation, thereby enabling decision-making problems over model predictions. For continuous, differentiable models, these problems can be solved via gradient-based methods . Mixed-integer programming (MIP) has also been proposed, mainly to support non-smooth models such as ReLU neural networks , their ensembles , and tree ensembles . Optimization over trained ReLU neural networks has been especially prominent , finding applications such as verification , reinforcement learning , compression , and black-box optimization . There is no reason to exclude GNNs from this rich field. Wu et al.  managed to verify a GNN-based job scheduler, where MIP is used to obtain tighter bounds in a forward-backward abstraction refinement framework. Mc Donald  built MIP formulations for GCN  and GraphSAGE  and tested their performance on a CAMD problem, which is the first work to directly apply MIP on GNNs to the best of our knowledge.

Optimization over trained GNNs involves two significant challenges. One is the symmetry issue arising from the permutation invariance of GNNs (i.e., isomorphism graphs share the same GNN output(s)). For forward problems, this invariance is preferred since it admits any graph representation for inputs. From optimization perspective, however, symmetric solutions can significantly enlarge the feasible domain and impede a branch-and-bound (B&B) search tree. One way to break symmetry in integer programs is by adding constraints to remove symmetric solutions . The involvement of graphs not only adds complexity to the problem in terms of symmetry, but also in terms of implementation. Due to the complexity and variety of GNN architectures, a general framework is needed. This framework should be compatible with symmetry-breaking techniques.

This paper first defines optimization problems on trained GNNs. To handle the innate symmetry, we propose two sets of constraints to remove most isomorphism graphs (i.e., different indexing for any abstract graph). To guarantee that these constraints do not exclude all possible symmetries (i.e., any abstract graph should have at least one feasible indexing), we design an algorithm to index any undirected graph and prove that the resulting indexing is feasible under the proposed constraints. To encode GNNs, we build a general framework for MIP on GNNs based on the open-source Optimization and Machine Learning Toolkit (OMLT) . Many classic GNN architectures are supported after proper transformations. Two formulations are provided and compared on a CAMD problem. Numerical results show the outstanding improvement of symmetry-breaking.

**Paper structure:** Section 2 introduces the problem definition, proposes the symmetry-breaking constraints, and gives theoretical guarantees. Section 3 discusses the application, implementation details, and numerical results. Section 4 concludes and discusses future work.

Figure 1: Illustration of forward and backward problems defined on trained GNNs.

Methodology

### Definition of optimization over trained GNNs

We consider optimization over a trained GNN on a given dataset \(D=\{(X^{i},A^{i}),y^{i}\}_{i=1}^{M}\). Each sample in the dataset consists of an input graph (with features \(X^{i}\) and adjacency matrix \(A^{i}\)) and an output property \(y^{i}\). The target of training is to approximate properties, that is:

\[(X^{i},A^{i}) y^{i},\  1 i M\]

When optimizing over a trained GNN, the goal is to find the input with optimal property:

\[(X^{*},A^{*})=*{arg\,min}_{(X,A)}& (X,A)\\ s.t.& f_{j}(X,A) 0,j\\ g_{k}(X,A)=0,k\] (OPT)

where \(f_{j},g_{k}\) are problem-specific constraints and \(,\) are index sets. Note that optimality of (OPT) is defined on the trained GNN instead of true properties. To simplify our presentation, we will focus on undirected graphs with node features, noting that directed graphs can be naturally transformed into undirected graphs. Likewise, using edge features does not influence our analyses. We discuss practical impacts of such modifications later.

### Symmetry handling

For any input \((X,A)\), assume that there are \(N\) nodes, giving a \(N N\) adjacency matrix \(A\) and a feature matrix \(X=(X_{0},X_{1},,X_{N-1})^{T}^{N F}\), where \(X_{i}^{F}\) contains the features for \(i\)-th node. For any permutation \(\) over set \([N]:=\{0,1,2,N-1\}\), let:

\[A^{}_{u,v}=A_{(u),(v)},\ X^{}_{v}=X_{(v)},\  u,v[N]\]

Then the permuted input \((X^{},A^{})\) has isomorphic graph structure, and therefore the same output, due to the permutation invariance of GNNs (i.e., \((X,A)=(X^{},A^{})\)). In other words, the symmetries result from different indexing for the same graph. In general, there exist \(N!\) ways to index \(N\) nodes, each of which corresponds to one solution of (OPT). Mc Donald  proposed a set of constraints to force molecules connected, which can also be used to break symmetry in our general setting. Mathematically, they can be written as:

\[ v[N]\{0\},\  u<v,\ s.t.\ A_{u,v}=1\] (S1)

Constraints (S1) require each node (except for node \(0\)) to be linked with a node with smaller index. Even though constraints (S1) help break symmetry, there can still exist many symmetric solutions. To resolve, or at least relieve, this issue, we need to construct more symmetry-breaking constraints.

**Breaking symmetry at the feature level.** We can first design some constraints on the features to define a starting point for the graph (i.e., assigning index \(0\) to a chosen node). Note that multiple nodes may share the same features. Specifically, we define an arbitrary function \(h:^{F}\), to assign a hierarchy to each node and force that node \(0\) has the minimal function value:

\[h(X_{0}) h(X_{v}),\  v[N]\{0\}\] (S2)

The choice of \(h\) can be application-specific, for example, as in Section 3.1. The key is to define \(h\) such that not too many nodes share the same minimal function value.

**Breaking symmetry at the graph level.** It is unlikely that constraints on features will break much of the symmetry. After adding constraints (S1) and (S2), any neighbor of node \(0\) could be indexed \(1\), then any neighbor of node \(0\) or \(1\) could be indexed \(2\), and so on. We want to limit the number of possible indexing. A natural idea is to account for neighbors of each node: the neighbor set of a node with smaller index should also have smaller lexicographical order.

Before further discussion, we need to define the lexicographical order. Let \((M,L)\) be the set of all non-decreasing sequences with \(L\) integer elements in \([0,M]\) (i.e., \((M,L)=\{(a_{1},a_{2},,a_{L})\,|\,0 a_{1} a_{2}  a_{L} M\}\)). For any \(a(M,L)\), denote its lexicographical order by \(LO(a)\). Then for any \(a,b(M,L)\), we have:

\[LO(a)<LO(b)\ \ \ l\{1,2,,L\},\ s.t.\ a_{i}=b_{i},&1 i<l\\ a_{l}<b_{l}\] (LO)

[MISSING_PAGE_FAIL:4]

**Property 2**.: _For any \(s_{1},s_{2}=1,2,,N-1\),_

\[s_{1} s_{2}^{s_{1}}(v)=^{s_{2}}(v)[s_{ 1}],\; v V_{2}^{s_{2}}\]

**Property 3**.: _Given any multisets \(A,B\) with no more than \(L\) integer elements in \([0,M-1]\), we have:_

\[LO(A) LO(B) LO(A[m]) LO(B[m]),\; m=1,2, ,M\]

Using these properties, we can prove Lemma 1, which shows that if the final index of node \(u\) is smaller than \(v\), then at each iteration, the temporary index assigned to \(u\) is not greater than \(v\).

**Lemma 1**.: _For any two nodes \(u\) and \(v\),_

\[(u)<(v)^{s}(u)^{s }(v),\; s=1,2,,N-1\]

Now we can prove Theorem 1.

**Theorem 1**.: _Given any undirected graph \(G=(V,E)\) with one node indexed \(0\). The indexing yielded from Algorithm 1 satisfies (S3)._

Proof.: Assume that constraints (S3) are not satisfied and the minimal index that violates (S3) is \(s\). Denote nodes with index \(s\) and \(s+1\) by \(u,v\) respectively (i.e., \((u)=s,(v)=s+1\)).

Let \((u)\{v\}:=\{u_{1},u_{2},,u_{m}\}\) be all neighbors of \(u\) except for \(v\), where:

\[(u_{i})<(u_{i+1}),\; i=1,2,,m-1\]

Similarly, let \((v)\{u\}:=\{v_{1},v_{2},,v_{n}\}\) be all neighbors of \(v\) except for \(u\), where:

\[(v_{j})<(v_{j+1}),\; j=1,2,,n-1\]

Denote the sequences in \((N,N-1)\) corresponding to sets \(\{(u_{i}) 1 i m\}\) and \(\{(v_{j}) 1 j n\}\) by \(a=(a_{1},a_{2},,a_{N-1}),b=(b_{1},b_{2},,b_{N-1})\). By definition of \(LO()\):

\[a_{i}=(u_{i}),&1 i m\\ N,&m<i<N,\;b_{j}=(v_{j}),&1 j n\\ N,&n<j<N\]

Since nodes \(u\) and \(v\) violate constraints (S3), there exists a position \(1 k N-1\) satisfying:

\[a_{i}=b_{i},& 1 i<k\\ a_{k}>b_{k}\]

from where we know that nodes \(u\) and \(v\) share the first \(k-1\) neighbors (i.e. \(u_{i}=v_{i},\; 1 i<k\)). From \(b_{k}<a_{k} N\) we know that node \(v\) definitely has its \(k\)-th neighbor node \(v_{k}\). Also, note that \(v_{k}\) is not a neighbor of node \(u\). Otherwise, we have \(u_{k}=v_{k}\) and then \(a_{k}=(u_{k})=(v_{k})=b_{k}\).

_Case 1:_ If \(a_{k}=N\), that is, node \(u\) has \(k-1\) neighbors.

In this case, node \(v\) has all neighbors of node \(u\) as well as node \(v_{k}\). Therefore, we have:

\[LO(^{s}_{t}(u))>LO(^{s}_{t}(v))\]

Figure 2: Consider a graph having \(6\) nodes with different features. Without any constraints, there are \(6!=720\) ways to index it. Utilizing (S1) to force connectivity results in \(636\) ways. Using (S2) to choose \(v_{0}\) to be indexed \(0\), then there are \(5!=120\) ways. Finally, applying (S3) to order the rest of the nodes, there are only \(4\) ways, \(2\) of which can be derived from Algorithm 1. For details on using Algorithm 1 to index this graph, see Appendix A.2.

which violates the fact that node \(u\) is chosen to be indexed \(s\) at \(s\)-th iteration of Algorithm 1.

_Case 2:_ If \(a_{k}<N\), that is, node \(u\) has nodes \(u_{k}\) as its \(k\)-th neighbor.

Since \((u_{k})=a_{k}>b_{k}=(v_{k})\), we can apply Lemma 1 on node \(u_{k}\) and \(v_{k}\) at (\(s+1\))-th iteration, to obtain

\[^{s+1}(u_{k})^{s+1}(v_{k})\] ( \[\] )

Similarly, if we apply Lemma 1 to all the neighbors of node \(u\) and node \(v\) at \(s\)-th iteration, we have:

\[^{s}(u_{i}) ^{s}(u_{i+1}),\; i=1,2,,m-1\] \[^{s}(v_{j}) ^{s}(v_{j+1}),\; j=1,2,,n-1\]

Given that \(a_{k}=(u_{k})\) is the \(k\)-th smallest number in \(a\), we conclude that \(^{s}(u_{k})\) is equal to the \(k\)-th smallest number in \(^{s}_{k}(u)\). Likewise, \(^{s}(v_{k})\) equals to the \(k\)-th smallest number in \(^{s}_{k}(v)\). Meanwhile, \(^{s}(u_{i})=^{s}(v_{i})\) since \(u_{i}=v_{i},\; 1 i<k\). After comparing the lexicographical orders between of \(^{s}_{k}(u)\) and \(^{s}_{k}(v)\) (with the same \(k-1\) smallest elements, \(^{s}(u_{k})\) and \(^{s}(v_{k})\) as the \(k\)-th smallest element, respectively), node \(u\) is chosen. Therefore, we have:

\[^{s}(u_{k})^{s}(v_{k})\]

from which we know that:

\[LO(^{s}(u_{k})) LO(^{s}(v_{k}))\]

At (\(s+1\))-th iteration, node \(u_{k}\) has one more indexed neighbor (i.e., node \(u\) with index \(s\)), while node \(v_{k}\) has no new indexed neighbor. Thus we have:

\[LO(^{s+1}(u_{k}))=LO(^{s}(u_{k})\{s\})<LO(^{s}(u_{k})) LO(^{s}(v_{k}))=LO(^{s+1}(v_{k}))\]

which yields:

\[^{s+1}(u_{k})<^{s+1}(v_{k})\] ( \[<\] )

The contradiction between (\(\)) and (\(<\)) completes this proof. 

Given any undirected graph with node features, after using (S2) to choose node \(0\), Theorem 1 guarantees that there exists at least one indexing satisfying (S3). However, when applying (S1) to force a connected graph, we need to show the compatibility between (S1) and (S3), as shown in Lemma 2. Appendix A provides proofs of properties and lemmas.

**Lemma 2**.: _For any undirected, connected graph \(G=(V,E)\), if one indexing of \(G\) sastifies (S3), then it satisfies (S1)._

Note that (S1) - (S3) are not limited to optimizing over trained GNNs. In fact, they can be employed in generic graph-based search problems, as long as there exists a symmetry issue caused by graph isomorphism. GNNs can be generalized to all permutation invariant functions defined over graphs. Although the next sections apply MIP formulations to demonstrate the symmetry-breaking constraints, we could alternatively use a constraint programming  or satisfiability  paradigm. For example, we could have encoded the Elgabou and Frisch  constraints in Reluplex .

### Connection & Difference to the symmetry-breaking literature

Typically, MIP solvers detect symmetry using graph automorphism, for example SCIP  uses BLISS , and both Gurobi  and SCIP break symmetry using orbital fixing  and orbital pruning . When a MIP solver detects symmetry, the only graph available is the graph formed by the variables and constraints in the MIP.

Our symmetries come from alternative indexing of abstract nodes. Each indexing results in an isomorphic graph. In MIP, however, each indexing corresponds to an element of a much larger symmetry group defined on all variables, including node features (\(O(NF)\)), adjacency matrix (\(O(N^{2})\)), model (e.g., a GNN), and problem-specific features. For instance, in the first row of Table 6, the input graph has \(N=4\) nodes, but the MIP has \(616+411\) variables. We only need to consider a permutation group with \(N!=24\) elements. However, because the MIP solver does not have access to the input graph structure, it needs to consider all possible automorphic graphs with \(616+411\) nodes. By adding constraints (S1) - (S3), there is no need to consider the symmetry group of all variables to find a much smaller subgroup corresponding to the permutation group defined on abstract nodes.

The closest setting to ours is distributing \(m\) different jobs to \(n\) identical machines and then minimizing the total cost. Binary variable \(A_{i,j}\) denotes if job \(i\) is assigned to machine \(j\). The requirement is that each job can only be assigned to one machine (but each machine can be assigned to multiple jobs). Symmetries come from all permutations of machines. This setting appears in noise dosage problems [90; 91], packing and partitioning orbitopes [92; 93], and scheduling problems . However, requiring that the sum of each row in \(A_{i,j}\) equals to 1 simplifies the problem. By forcing decreasing lexicographical orders for all columns, the symmetry issue is handled well. Constraints (S3) can be regarded as a non-trivial generalization of these constraints from a bipartite graph to an arbitrary undirected graph: following Algorithm 1 will produce the same indexing documented in [90; 91; 92; 93; 94].

### Mixed-integer formulations for optimizing over trained GNNs

As mentioned before, there are many variants of GNNs [4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18] with different theoretical and practical motivations. Although it is possible to build a MIP for a specific GNN from the scratch (e.g., ), a general definition that supports multiple architectures is preferable.

#### 2.5.1 Definition of GNNs

We define a GNN with \(L\) layers as follows:

\[GNN:^{d_{0}}^{d_{0}}}_{|V| }^{d_{L}} ^{d_{L}}}_{|V|}\]

where \(V\) is the set of nodes of the input graph.

Let \(_{v}^{(0)}^{d_{0}}\) be the input features for node \(v\). Then, the \(l\)-th layer (\(l=1,2,,L\)) is defined by:

\[_{v}^{(l)}=(_{u(v)\{v\}}_{u  v}^{(l)}_{u}^{(l-1)}+_{v}^{(l)}),\; v V\] ( \[\] )

where \((v)\) is the set of all neighbors of \(v\), \(\) could be identity or any activation function.

With linear aggregate functions such as sum and mean, many classic GNN architectures could be rewritten in form (\(\)), for example: Spectral Network , ChebNet , GCN , Neural FPs , DCNN , PATCHY-SAN , GraphSAGE , and MPNN .

#### 2.5.2 Mixed-integer optimization formulations for non-fixed graphs

If the graph structure for inputs is given and fixed, then (\(\)) is equivalent to a fully connected layer, whose MIP formulations are already well-established [46; 49]. But if the graph structure is non-fixed (i.e., the elements in adjacency matrix are also decision variables), two issues arise in (\(\)): (1) \((v)\) is not well-defined; (2) \(_{u v}^{(l)}\) and \(_{v}^{(l)}\) may be not fixed and contain the graph's information. Assuming that the weights and biases are constant, we can build two MIP formulations to handle the first issue.

The first formulation comes from observing that the existence of edge from node \(u\) to node \(v\) determines the contribution link from \(_{u}^{(l-1)}\) to \(_{v}^{(l)}\). Adding binary variables \(e_{u v}\) for all \(u,v V\), we can then formulate the GNNs with bilinear constraints:

\[_{v}^{(l)}=(_{u V}e_{u v}_{u  v}^{(l)}_{u}^{(l-1)}+_{v}^{(l)}),\; v V\] (bilinear)

Introducing this nonlinearity results in a mixed-integer quadratically constrained optimization problem (MIQCP), which can be handled by state-of-the-art solvers such as Gurobi .

The second formulation generalizes the big-M formulation for GraphSAGE in  to all GNN architectures satisfying (\(\)) and the assumption of constant weights and biases. Instead of using binary variables to directly control the existence of contributions between nodes, auxiliary variables \(_{u v}^{(l-1)}\) are introduced to represent the contribution from node \(u\) to node \(v\) in the \(l\)-th layer:

\[_{v}^{(l)}=(_{u V}_{u v}^{(l)}_{u  v}^{(l-1)}+_{v}^{(l)}),\; v V\] (big-M)where:

\[_{u v}^{(l-1)}=0,&e_{u v}=0\\ _{u}^{(l-1)},&e_{u v}=1\]

Assuming that each feature is bounded, the definition of \(_{u v}^{(l-1)}\) can be reformulated using big-M:

\[_{u}^{(l-1)}-_{u}^{(l-1)}(1-e_{u v}) _{u v}^{(l-1)}_{u}^{(l-1)}+_{u}^{(l-1)}(1 -e_{u v})\] \[- _{u}^{(l-1)}e_{u v} _{u v}^{(l-1)}_{u}^{(l-1)}e_{u v}\]

where \(|_{u}^{(l-1)}|_{u}^{(l-1)},e_{u v}\{0,1\}\). By adding extra continuous variables and constraints, as well as utilizing the bounds for all features, the big-M formulation replaces the bilinear constraints with linear constraints. Section 3 numerically compares these two formulations.

## 3 Computational experiments

We performed all experiments on a 3.2 GHz Intel Core i7-8700 CPU with 16 GB memory. GNNs are implemented and trained in _PyG_. MIP formulations for GNNs and CAMD are implemented based on OMLT , and are optimized by Gurobi 10.0.1  with default relative MIP optimality gap (i.e., \(10^{-4}\)). The code is available at https://github.com/cog-imperial/GNN_MIP_CAMD. These are all engineering choices and we could have, for example, extended software other than OMLT to translate the trained GNNs into an optimization solver [96; 97].

### Mixed-integer optimization formulation for molecular design

MIP formulations are well-established in the CAMD literature [98; 99; 100; 101; 102; 103]. The basic idea is representing a molecule as a graph, creating variables for each atom (or group of atoms), using constraints to preserve graph structure and satisfy chemical requirements. A score function is usually given as the optimization target. Instead of using knowledge from experts or statistics to build the score function, GNNs (or other ML models) could be trained from datasets to replace score functions. Here we provide a general and flexible framework following the MIP formulation for CAMD in . Moreover, by adding meaningful bounds and breaking symmetry, our formulation could generate more reasonable molecules with less redundancy. Due to space limitation, we briefly introduce the formulation here (see Appendix B for the full MIP formulation).

To design a molecule with \(N\) atoms, we define \(N F\) binary variables \(X_{v,f},v[N],f[F]\) to represent \(F\) features for \(N\) atoms. Hydrogen atoms are not counted in since they can implicitly considered as node features. Features include types of atom, number of neighbors, number of hydrogen atoms associated, and types of adjacent bonds. For graph structure of molecules, we add three sets of binary variables \(A_{u,v},DB_{u,v},TB_{u,v}\) to denote the type of bond (i.e., any bond, double bond, and triple bond) between atom \(u\) and atom \(v\), where \(A\) is the adjacency matrix.

To design reasonable molecules, constraints (C1) - (C21) handle structural feasibility following . Additionally, we propose new constraints to bound the number of each type of atoms (C22), double bonds (C23), triple bonds (C24), and rings (C25). In our experiments, we calculate these bounds based on the each dataset itself so that each molecule in the dataset will satisfy these bounds (see Appendix C.1 for details). By setting proper bounds, we can control the composition of the molecule, and avoid extreme cases such as all atoms being set to oxygen, or a molecule with too many rings or double/triple bounds. In short, constraints (C22) - (C25) provide space for chemical expertise and practical requirements. Furthermore, our formulation could be easily applied to datasets with different types of atoms by only changing the parameters in Appendix B.1. Moreover, group representation of molecules [98; 102] is also compatible with this framework. The advantage is that all constraints can be reused without any modification.

Among constraints (C1) - (C25) (as shown in Appendix B.3), constraints (C5) are the realization of (S1). Except for (C5), these structural constraints are independent of the graph indexing. Therefore, we can compatibly implement constraints (S2) and (S3) to break symmetry.

Corresponding to (S2), we add the following constraints over features:

\[_{f[F]}2^{F-f-1} X_{0,f}_{f[F]}2^{F-f-1} X_{v,f}, \  v[N]\{0\}\] (C26)where \(2^{F-f-1},f[F]\) are coefficients to help build a bijective \(h\) between all possible features and all integers in \([0,2^{F}-1]\). These coefficients are also called "universal ordering vector" in [72; 78].

On graph level, constraints (S3) can be equivalently rewritten as:

\[_{u v,v+1}2^{N-u-1} A_{u,v}_{u v,v+1}2^{N-u-1} A _{u,v+1},\; v[N-1]\{0\}\] (C27)

Similarly, the coefficients \(2^{N-u-1},u[N]\) are used to build a bijective mapping between all possible sets of neighbors and all integers in \([0,2^{N}-1]\).

For illustration purposes, we can view CAMD as two separate challenges, where the first one uses structural constraints to design reasonable molecules (including all symmetric solutions), and the second one uses symmetry-breaking constraints to remove symmetric solutions. Note that the diversity of solutions will not change after breaking symmetry, because each molecule corresponds to at least one solution (guaranteed by Section 2.3).

### Counting feasible structures: performance of symmetry-breaking

We choose two datasets QM7 [104; 105] and QM9 [106; 107] from CAMD literature to test the proposed methods. See Appendix C.1 for more information about QM7 and QM9. To test the efficiency of our symmetry-breaking techniques, we build a MIP formulation for CAMD and count all feasible structures for different \(N\). By setting PoolSearchMode=2, PoolSolutions=\(10^{9}\), Gurobi can find many (up to \(10^{9}\)) feasible solutions to fill in the solution pool.

Table 1 shows the performance of our symmetry-breaking constraints (S2) and (S3) comparing to the baseline of (S1). Without adding (S1), we need to count the number of any graph with compatible features. Even ignoring features, the baseline would be \(2^{}\). This number will be much larger after introducing features, which loses the meaning as a baseline, so we use (S1) as the baseline.

### Optimizing over trained GNNs for molecular design

For each dataset, we train a GNN with two GraphSAGE layers followed by an add pooling layer, and three dense layers. Details about their structures and training process are shown in Appendix C.2. For statistical consideration, we train \(10\) models with different random seeds and use the \(5\) models with smaller losses for optimization. Given \(N\) (\(\{4,5,6,7,8\}\)), and a formulation (\(\{\)bilinear, bilinear+BrS, big-M, big-M+BrS\(\}\)), where "+BrS" means adding symmetry-breaking constraints (C26) and (C27), we solve the corresponding optimization problem \(10\) times with different random seeds in Gurobi. This means that there are \(50\) runs for each \(N\) and formulation.

Figure 3 shows the significant improvement of symmetry-breaking. Considering the average solving time, big-M performs better. For the bilinear constraints, Gurobi 10.0.1 appears to transform bilinear constraints into linear constraints. Appendix C.3 shows that, after Gurobi's presolve stage, the big-M formulation has more continuous variables but fewer binary variables compared to the bilinear formulation. The fewer binary variables after presolve may explain the better big-M performance.

    &  &  &  &  &  &  &  &  &  &  \\  & & (S1) & (S2) & & & & & & \\  \(2\) & \(17\) & \(10\) & \(10\) & \(41\) & \(15\) & \(9\) & \(9\) & \(40\) \\ \(3\) & \(112\) & \(37\) & \(37\) & \(67\) & \(175\) & \(54\) & \(54\) & \(69\) \\ \(4\) & \(3,323\) & \(726\) & \(416\) & \(87\) & \(4,536\) & \(1,077\) & \(631\) & \(86\) \\ \(5\) & \(67,020\) & \(11,747\) & \(3,003\) & \(96\) & \(117,188\) & \(21,441\) & \(5,860\) & \(95\) \\ \(6\) & \(t.o.\) & \(443,757\) & \(50,951\) & \( 98\) & \(t.o.\) & \(527,816\) & \(59,492\) & \( 98\) \\ \(7\) & \(t.o.\) & \(t.o.\) & \(504,952\) & \(*\) & \(t.o.\) & \(t.o.\) & \(776,567\) & \(*\) \\ \(8\) & \(t.o.\) & \(t.o.\) & \(t.o.\) & \(*\) & \(t.o.\) & \(t.o.\) & \(t.o.\) & \(*\) \\   

Table 1: Numbers of feasible solutions. The time limit is \(48\) hours. At least \(2.5 10^{6}\) solutions are found for each time out (t.o.). For each dataset, the last column reports the percentage of removed symmetric solutions after adding (S2) and (S3) to the baseline of (S1). Higher percentage means breaking more symmetries.

In addition to the average solving time, we compare the performance of two formulations with breaking symmetry in each run. In \(341\) runs out of \(500\) runs (i.e., \(50\) runs for each dataset \(\) {QM7, QM9} with each \(N\{4,5,6,7,8\}\)), the big-M formulation achieves optimality faster than the bilinear formulation. Additionally, Gurobi uses much of the solving time to improve the bounds. Table 6 and 7 in Appendix C.3 report \(time_{opt}\) that denotes the first time to find the optimal solution. In \(292\) runs, the big-M formulation finds the optimal solution earlier than the bilinear formulation.

## 4 Discussion & Conclusion

We introduce optimization over trained GNNs and propose constraints to break symmetry. We prove that there exists at least one indexing (resulting from Algorithm 1) satisfying these constraints. Numerical results show the significant improvement after breaking symmetry. These constraints are not limited to the problem (i.e., optimizing trained GNNs), technique (i.e., MIP), and application (i.e., CAMD) used in this work. For example, one can incorporate them into genetic algorithms instead of MIP, or replace the GNN by artificially-designed score functions or other ML models. In other graph-based decision-making problems, as long as the symmetry issue caused by graph isomorphism exists, these constraints could be used to break symmetry. Moreover, the proposed frameworks for building MIP for GNNs as well as CAMD provide generality and flexibility for more problems.

**Limitations.** Assuming constant weights and biases excludes some GNN types. This limitation is an artifact of our framework and it is possible to build MIP formulations for some other architectures. Using MIP may limit GNN size, for instance, edge features may enlarge the optimization problem.

**Future work.** One direction is to make the MIP-GNN framework more general, such as adding edge features, supporting more GNN architectures, and developing more efficient formulations. Another direction is using the symmetry-breaking constraints in graph-based applications beyond trained GNNs because we can consider any function that is invariant to graph isomorphism. To facilitate further CAMD applications, more features such as aromacity and formal charge could be incorporated. Also, optimizing lead structures in a defined chemical space is potential to design large molecules.