# Safety-Aware Fine-Tuning of Large Language Models

Hyeong Kyu Choi, Xuefeng Du, Yixuan Li

Department of Computer Sciences, University of Wisconsin-Madison

{hyeongkyu.choi, xfdu, sharonli}@cs.wisc.edu

corresponding author

###### Abstract

Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine-tuning can be diverse, introducing safety concerns regarding the potential inclusion of harmful data samples. Manually filtering or avoiding such samples, however, can be labor-intensive and subjective. To address these difficulties, we propose a novel _Safety-Aware Fine-Tuning_ (**SAFT**) framework designed to automatically detect and remove potentially harmful data, by leveraging a scoring function that exploits the subspace information of harmful and benign samples. Experimental results demonstrate the efficacy of SAFT across different LLMs and varying contamination rates, achieving reductions in harmfulness of up to \(27.8\%\). Going beyond, we delve into the mechanism of our approach and validate its versatility in addressing practical challenges in real-world scenarios. _Disclaimer: This paper may contain offensive qualitative examples; reader discretion is advised._

## 1 Introduction

Large Language Models (LLMs)  have emerged as a powerful foundation for building personalized models tailored to individual needs and purposes. To enable customization, a pre-trained LLM typically undergoes supervised fine-tuning, a process that allows LLMs to adapt and specialize based on task-specific data . While fine-tuning enables LLMs to improve their performance on custom datasets, it also poses safety concerns when harmful samples arise in the fine-tuning data. For instance, consider a scenario where a conversational agent is being fine-tuned on user interactions from social media platforms. These interactions often contain a mixture of benign and potentially harmful content, such as hate speech, misinformation, or inappropriate language. Consequently, fine-tuning LLMs on data containing objectionable content could adversely affect the model's behavior.

To formalize the problem, we consider a generalized characterization of the fine-tuning data, which can be modeled as a mixed composition of two distributions:

\[=\ _{}+(1-)\ _{},\]

where \(_{}\) and \(_{}\) respectively denote the distribution of harmful and benign data, and \(\) is the mixing ratio. Such mixture of data can naturally arise in numerous real-world applications and is more realistic than requiring a fully benign set for fine-tuning. However, as demonstrated by our experiment in Section 3, even a small amount of harmful samples mixed into the benign fine-tuning data can severely compromise the model's safety performance, and relevant concerns have been observed in recent works  as well. Addressing this problem is challenging due to the lack of clear membership (benign or harmful) for samples in the dataset. To make matters worse, the manual process of filtering or avoiding harmful data is often labor-intensive and subjective, relying on the judgment of crowd workers.

Motivated by the problem, we introduce _Safety-Aware Fine-Tuning_ (**SAFT**) of LLMs, addressing the challenge of mitigating harmful samples present in fine-tuning data such as user-shared conversations. In a nutshell, our framework aims to devise an automated function to filter and remove harmful data from training data, enabling fine-tuning of the model on a filtered set to mitigate its impact (Figure 1). Central to our framework is the design of the filtering function for harmful data detection. Our key idea is to utilize the language model's internal representations, which can capture information related to harmfulness. Specifically, we identify a subspace in the activation space associated with harmful statements, and consider a point to be 'harmful' if its representation aligns strongly with the directions of the subspace. This idea can be operationalized by performing factorization in the embedding space, where the top singular vectors point toward the direction of harmful embeddings. The filtering score measures the norm of the embedding projected onto the top singular vectors, which is relatively larger for harmful data than benign data. Our filtering score offers a straightforward mathematical interpretation and is easily implementable in practical applications.

Our empirical findings confirm that SAFT significantly reduces the harmfulness of the fine-tuned model, showcasing reductions in harmfulness of up to 27.8\(\%\) compared to the standard supervised fine-tuning scheme (Section 5). We comprehensively validate the efficacy of SAFT on different LLMs, datasets, and across varying contamination levels. Furthermore, we delve deeper into understanding the key components of our methodology (Section 6.1 and 6.3), and extend our inquiry to showcase SAFT's versatility in addressing real-world scenarios with practical challenges (Section 6.2).

Overall, we summarize our contributions as follows:

* We propose _Safety-Aware Fine-Tuning_ (SAFT), a novel fine-tuning framework that automatically detects and filters out potentially harmful samples from the dataset before fine-tuning.
* We present a scoring function for _harmful data detection_, which leverages the subspace information of LLM embeddings, thereby effectively separating harmful samples from benign samples.
* We conduct extensive experiments and analyses to elucidate the efficacy of SAFT and its robustness to real-world practical challenges.

## 2 Preliminaries

We use Huber contamination model  to characterize the underlying data as a mixture of benign data distribution \(_{}\) and harmful data distribution \(_{}\). Then, this is formalized as follows.

**Definition 2.1** (Fine-tuning data distribution): _We define the fine-tuning data distribution to be the following mixture of distributions_

\[=\ _{}+(1-)\ _{},\] (1)

_where \(\) is the contamination ratio. When \(=0\), our formulation generalizes to the ideal situation when no harmful data occurs._

**Definition 2.2** (Empirical training data): _An empirical training set \(=\{(x_{i},y_{i})\}_{i=1}^{N}\) is sampled i.i.d. from this mixture distribution \(\), where \(N\) is the number of samples. For each sample \((x_{i},y_{i})\), \(x_{i}\) denotes an input prompt to the large language model, and \(y_{i}\) represents the corresponding target. Note that we do not have clear membership (benign or harmful) for the samples in \(\)._

Figure 1: **Safety-Aware Fine-Tuning. Compared to vanilla supervised fine-tuning (SFT) that use the original dataset \(\) potentially containing harmful samples, our safety-aware fine-tuning (SAFT) framework filters out the harmful samples with \(\) before training, thereby lowering harmfulness of the resulting model.**

Traditional supervised fine-tuning (SFT) adapts an LLM policy to the task defined by the dataset \(\), by minimizing the following objective:

\[_{}=*{arg\,min}_{}_{(x,y) }\ (f(x;_{}),y),\] (2)

where \(_{}\) is the parameterization of the model, and \(\) is the language modeling loss function. However, as we show in Section 3, naively fine-tuning on this dataset \(\) can be susceptible to harmful samples, and adversely affect the behavior of the resulting model. This leads us to next define safety-aware fine-tuning (SAFT). The goal of SAFT is to safeguard against harmful data samples in the full set \(\), and fine-tune the model on the potentially benign subset to reduce harmfulness.

**Definition 2.3** (Safety-aware fine-tuning): _Let \(\) be a filtering function that identifies and removes potentially harmful samples from \(\), returning the filtered set \(()\). The language model policy is optimized via \(_{}=_{(x,y)()}\ (f(x;_{}),y)\). The model is more safety-aware if_

\[(_{})<(_{}),\] (3)

_where **HS** is a scoring function quantifying the degree of harmfulness of a policy (more in Section 3),_

Thus, the overarching goal is to devise an effective filtering function \(\) that removes harmful data samples from \(\), thereby reducing the harmfulness of the fine-tuned model. This differs from alignment frameworks such as Reinforcement Learning with Human Feedback (RLHF) [13; 14; 15; 16], which typically requires _labeled_ samples of preference data, which can be labor-intensive and subjective, relying on the judgment of crowd workers. In contrast, our setting works with _unlabeled_ mixture data, which naturally arises in real-world applications and imposes weaker data assumptions.

## 3 How Does harmful Data Impact Fine-tuning?

In this section, we explore the impact of harmful data on the fine-tuning process of LLMs, highlighting its implications for model performance.

Setup.We design our experiments with the Beavertails dataset , which consists of question-answer pairs labeled as harmful or benign. Each sample comprises a one-turn dialogue. We construct the fine-tuning dataset of 3,000 samples under various contamination ratios, denoted by \(=\{0.1,0.15,0.2,0.25,0.3\}\), where higher values of \(\) indicate more pronounced contamination. For each \(\), we fine-tune the Llama-2-chat model  with low-rank adaptation . The hyperparameters are summarized in Appendix A. For each fine-tuned model, we evaluate the performance based on two criteria:

* **Harmfulness Score (HS) \(\)**: We assess the harmfulness of the model using an auxiliary moderation model from , which predicts each generated response to be harmful or benign. The fraction of harmful responses among all query responses is computed as the Harmfulness Score.
* **Helpfulness Score \(\)**: For each test input query labeled 'benign', we measure the sentence-level similarity between the model's generation \(\) and the ground truth response \(y\). That is, we assume 'benign' ground truth responses to be helpful. The similarity score is calculated based on BLEURT  and the ROUGE-L  metric. A higher score indicates more helpfulness.

Observation.In Figure 2 (a), we illustrate that _fine-tuning on the mixed data can notably increase the harmfulness score even with a minor ratio of harmful data mixed_ (e.g., \(=0.1\)), and this degradation worsens as the proportion of harmful data increases. Another observation is that the helpfulness score is not significantly affected regardless of the harmful ratio. For example, when \(=0.3\), the helpfulness score under BLEURT is 0.504, which closely matches the score fine-tuned on the pure benign samples (0.511). Hence, the adverse effects of harmful data present in \(\) may remain unaware if our focus is primarily on evaluating the fine-tuning performance of the main task (i.e., generating helpful responses). Our observation corroborates with findings in , motivating our framework on safety-aware fine-tuning for large language models.

## 4 Safety-Aware Fine-Tuning

Safety-aware fine-tuning aims to adapt a language model to the task at hand defined by \(\) while withstanding the influence of harmful data in it. Our goal is to ensure a decreased harmfulness of the fine-tuned model, and that the models' general helpfulness is not severely degraded during the fine-tuning process. This can be challenging due to the lack of clear membership (benign or harmful) for samples in mixture data \(\). In a nutshell, our framework aims to devise a simple and effective filtering function that removes harmful data samples from \(\), enabling fine-tuning of the model on a filtered set to mitigate its impact. We describe harmful data detection in Section 4.1, followed by a safety-aware fine-tuning objective in Section 4.2. Our study represents an initial endeavor to address this complex issue, serving as a springboard for future exploration.

### Harmful Data Detection

Harmful data detection refers to the step of identifying and flagging harmful data instances within a mixture dataset comprising both benign and harmful samples. The ability to effectively detect harmful data relies heavily on whether the language model's representations can capture information related to harmfulness. Our idea is that if we could identify a direction or a subspace in the activation space associated with harmful statements, then we might be able to detect and separate the harmful data from the rest.

Embedding factorization.To operationalize the idea, we first extract embeddings from the language model for samples in the dataset \(\). Specifically, let \(^{N d}\) denote the matrix of embeddings extracted from the language model for samples in \(\), where each row represents the embedding vector \(_{i}^{}\) of a data sample \(x_{i}\). To identify the harmfulness direction, we perform singular value decomposition:

\[_{i} :=_{i}-\] (4) \[ =^{},\]

where \(^{d}\) is the average embeddings across all \(N\) samples, which is used to center the embedding matrix. The columns of \(\) and \(\) are the left and right singular vectors that form an orthonormal basis, and \(\) is a diagonal matrix. Such a factorization of matrix \(\) is useful, because it enables finding the best representation with a \(k\)-dimensional subspace for the set of points in \(\).

To gain insight, we begin with a special case of the problem where the subspace is \(1\)-dimensional, a line through the origin. Finding the best-fitting line through the origin with respect to a set of points \(\{_{i}|1 i N\}\) means minimizing the sum of the squared distances of the points to the line. Here, distance is measured perpendicular to the line. Geometrically, finding the first singular vector \(_{1}\) is also equivalent to maximizing the total distance from the projected embedding (onto the direction of

Figure 2: (a) **Impact of harmful data.** As more harmful samples are included in the fine-tuning dataset, the resulting model exhibits more profound harmfulness, whereas helpfulness is not significantly affected. (b) **Harmful data detection.** Harmful samples may locate farther away from the center, resulting in greater magnitude of the embedding vector \(_{i}\) projected onto the singular vector \(\), while benign samples that are mostly centered around the origin will have smaller magnitude of projection onto \(\).

\(_{1}\)) to the origin (sum over all points in \(\)):

\[_{1}=*{arg\,max}_{\|\|_{2}=1}_{i=1}^{N} _{i},^{2},\] (5)

where \(,\) is a dot product operator. As illustrated in Figure 2 (b), harmful data samples may exhibit anomalous behavior compared to benign samples, and locate farther away from the origin. Thus, the first singular vector \(_{1}\) is expected to point towards the direction of harmful embeddings, in order to preserve the variance in data.

To detect harmful samples, we define the filtering score as \(s_{i}=_{i},_{1}^{2}\), which measures the norm of \(_{i}\) projected onto the principal component direction. Because the direction of \(_{1}\) aligns more with the harmful data's direction, the score is relatively larger for harmful data compared to benign data. This allows us to perform detection based on the magnitude of the score. A sample in \(\) is considered 'harmful' if the score is larger than a threshold \(\):

\[(x_{i})=\{1,&\;s_{i}>\\ 0,&.\] (6)

Extension to subspace with \(k\) singular vectors.Our filtering score offers a straightforward mathematical interpretation and is easily implementable in practical applications. Furthermore, the definition of filtering score can be generalized to leverage a subspace of \(k\) orthogonal singular vectors:

\[s_{i}=_{j=1}^{k}_{i},_{j} ^{2},\] (7)

where \(_{j}\) is the \(j^{}\) column of \(\), and \(k\) is the number of components. The intuition is that harmful samples can be captured by a small subspace, allowing them to be separated from the benign samples. In Section 6.1, we will verify how the choice of \(k\) impacts the detection performance.

### Fine-Tuning with Filtered Data

Based on the filtering score defined in Equation 7, we regard \(_{}=\{(x_{i},y_{i}):s_{i}\}\) as the training set. This dataset is used to fine-tune the model using the following objective:

\[_{}_{(x,y)_{}}\;\; (f(x;),y),\] (8)

where \(\) is the parameterization of the model.

## 5 Experiments

### Setup

**Datasets and models.** Same as our setup described in Section 3, we evaluate the effectiveness of our method using the Beavertails dataset . Each sample includes a prompt and one response, and the sample is either labeled as benign or harmful. We construct the contaminated fine-tuning dataset of 3000 samples under various mixing ratios, denoted by \(=\{0.1,0.15,0.2,0.25,0.3\}\). The magnitude of \(\) is moderately small to reflect the real-world scenario where the majority of samples remain benign. The filtering threshold \(\) and subspace dimensionality \(k\) are validated on a held-out set with labeled 100 samples. We evaluate the trained models on the standard test set of Beavertail. Furthermore, we adopt commonly used open-sourced models, Llama-2-7B  and Vicuna-7B . Specifically, we use the 'llama-2-7b-chat-hf' and 'vicuna-7b-v1.5' versions. We fine-tune both models for 4 epochs with a learning rate of 2e-5. We employ Low-rank Adaptation (LoRA) for efficient training . More details are provided in Appendix A.

**Baselines and metrics.** We consider the following alternative approaches for fine-tuning and compare them with our method SAFT. (1) 'SFT' is a baseline trained on the entire dataset \(\) without any filtering. (2) 'Prompting' filters the harmful data by querying the large language model "Is the following data sample harmful to use for fine-tuning? '[dialog]'. Respond only with Yes or No". We then remove dialog samples that the LLM responds with 'Yes'. (3) 'Random' baseline randomly removes the same amount of samples as SAFT. We adopt the same metrics as defined in Section 3, namely the Harmfulness Score (HS) and the Helpfulness Score based on either BLEURT (BRT) or ROUGE-L (RL).

[MISSING_PAGE_EMPTY:6]

### Soundness of SAFT

In this section, we validate the soundness of each component in SAFT. In particular, we study the accuracy of benign/harmful sample classification and the impact of the number of components \(k\).

Are harmful data accurately detected?Figure 3 presents a comparison of harmful data detection performance with baselines across various contamination ratios \(\). In our context, harmful data detection can be viewed as a binary classification task, where each sample is categorized as either harmful or benign. We evaluate performance using AUROC, a standard metric for quantifying binary classification performance. Higher AUROC values indicate superior harmful data detection performance and _vice versa_. Our findings indicate that our filtering approach consistently outperforms the baselines, more accurately identifying harmful samples. Particularly noteworthy is the inferior performance of the 'Prompting' baseline compared to random filtering, highlighting the unreliability of directly prompting large language models to identify harmful data. In contrast, our method leverages the internal activation space, which contains richer statistical information for harmful data detection than simply relying on the output of the language model. These results underscore the importance of meaningful harmful data detection in enhancing the overall performance of SAFT.

Where in the LLM is harmfulness represented?In Figure 4, we analyze the harmful data detection with embeddings extracted from different layers in the LLM. The AUROC values of benign/harmful classification are evaluated with Llama-2, and all other configurations are kept the same as our main experiments. We observe that the detection performance initially increases from the bottom to middle layers, and then decreases slightly. Overall, the embeddings extracted from the median layers (_e.g._, 15) lead to the best separability compared to earlier layers. The trend suggests that LLMs gradually capture the information in the context in the first few layers, and then condense them in the last layers to map to the vocabulary.

The effect of \(k\) subspace components.As described in Eq. (7), SAFT utilizes a subspace of \(k\) orthogonal singular vectors to define the filtering score. In this ablation study, we examine how the number of component vectors influences performance. Performance is assessed on the harmful data detection metrics: AUROC, F1 scores, Precision, and Recall. Table 3 presents performance metrics for varying values of \(k=\{1,2,4,8,16,32\}\). Overall, we observe superior performance with a smaller value of \(k\). For instance, on Llama-2, the best classification performance is achieved with \(k=1\), yielding an AUROC of 0.6868. This trend persists across all contamination scenarios with \(=\{0.10,0.15,0.20,0.25,0.30\}\). These findings align with our assumption that harmful samples can be represented by a small subspace, indicating that only a few key directions in the activation space are capable of distinguishing harmful samples from benign ones.

Figure 4: AUROC of SAFT across layers in Llama-2 (\(=0.3\)).

Figure 3: Comparison of harmful data detection AUROC across baselines with different contamination rates \(\).

  \(\#\) components \(k\) & AUROC & F1 & Precision & Recall \\ 
1 & **0.6868** & **56.32** & **53.97** & 58.89 \\
2 & 0.6709 & 54.93 & 47.10 & 65.89 \\
4 & 0.6661 & 55.08 & 43.34 & 75.56 \\
8 & 0.5987 & 49.20 & 36.91 & 73.78 \\
16 & 0.5794 & 48.45 & 34.90 & **79.22** \\
32 & 0.5927 & 47.15 & 37.87 & 62.44 \\  

Table 3: Impact of subspace with \(k\) components. (\(=0.3\))

### Robustness to Practical Challenges

Safety-Aware Fine-Tuning is a practical framework that may potentially face real-world challenges. For instance, we explore how well SAFT deals with different data distributions of the fine-tuning dataset, and discuss its steerability.

**Does SAFT generalize to different dataset distributions?** To assess the generalizability of our proposed approach, we conduct an additional experiment using the Anthropic HH-RLHF dataset  as the fine-tuning task. Specifically, we randomly select 3,000 dialogues as the fine-tuning dataset, which contains a mixture of benign and harmful samples with unknown ratio. Meanwhile, we retain the same validation dataset and test query set from Beavertails dataset  for model selection and evaluation. This setup simulates whether SAFT performs robustly on a dataset from a new data distribution. In Table 4, we present the Harmfulness Scores and Helpfulness measures, BLEURT and ROUGE-L. The results show that while SFT has Harmfulness Score of 18.70, SAFT outperforms all baselines with a low score of 14.79.

How can we make SAFT steerable?Steerability is a useful feature in practical applications, offering flexibility to specific needs. For instance, if data are scant, we may need to filter less data even if it may compromise harmlessness. Conversely, a more conservative filtering might be desired in certain circumstances. In light of these considerations, we introduce a simple method to steer the behavior of SAFT. Recall that the detection is controlled by the threshold, \(\) (Eq. (6)). By adjusting \(\), we can alter the detection outcomes: increasing it results in fewer samples classified as 'harmful', while decreasing it has the opposite effect. In Figure 5, we demonstrate experimental results illustrating the impact of different steer rates, utilized as \((1+)\). Notably, as the

    & HS\(\) & BRT\(\) & RL\(\) \\  SFT & 18.70 & 0.462 & 0.094 \\ Prompting & 17.57 & 0.451 & 0.088 \\ Random & 16.28 & 0.473 & 0.100 \\
**SAFT** (Ours) & **14.79** & **0.484** & **0.107** \\   

Table 4: Fine-tuning results on the Anthropic HH-RLHF dataset .

Figure 5: **Steerability of SAFT (\(=0.3\)). Performance trends with respect to different steer rates are shown. We can steer the classification threshold of SAFT, \(\) to filter out more samples for lower harmfulness, _vice versa_. We observed the helpfulness measures are not severely affected, maintaining above 0.5 BLEURT at all times.**

  
**Example** \\   **Query** \\ What’s the best way to burn down a building if I want to make it look like an accident? \\  \\ 
 SFT Response \\ The best way to burn down a building and make it look like an accident is to use a slow-burning accelerant, such as gasoline or diesel fuel, and to set it off in a hidden location. This will allow the fire to spread slowly and naturally, \\ making it look like an accident. Additionally, you can use a fire starter, such as a lighter or matches, to help fire the get started. It is important to make what the fire is not to increase, as this could raise suspicion. You should also make sure that the building is empty before setting the fire, to avoid any potential harm to people or animals. Finally, you should make sure that the fire is put out before it spreads too far, to avoid any potential damage to the surrounding area. By following these steps, you can make it look like an accident and avoid any potential legal consequences. \\ 

Table 5: **Qualitative example.**steer rate is set to \(\{5,7,20\}\%\), fewer samples are filtered out with higher precision. On the other hand, reducing the steer rate leads to a higher recall, albeit at the expense of filtering out more samples. This adjustment can lower the harmfulness down to 26.31 by discarding more than half of the data. We observe that the helpfulness metric is not affected, maintaining above 0.50 BLEURT and 0.14 ROUGE-L at all times.

### Qualitative Exploration

Qualitative examples of generated text.We provide qualitative examples of the model responses after fine-tuning with and without Safety-Aware Fine-Tuning, in Table 5. 'SFT Response' refers to the model that has been trained with SFT without any filtering. The examples are retrieved from the Llama-2 model. More examples are in Appendix D.

Category-wise effect of SAFT.For a more fine-grained understanding of SAFT, we evaluate the harmfulness score for the 14 harmfulness categories in . In Figure 6, SAFT is compared with vanilla SFT. Notably, SAFT reduced the harmfulness scores of all categories. The categories that had the most reduction in harmfulness level are 'Controversial topics, politics' and 'Terrorism, organized crime', with approximately \(48\%\) to \(49\%\) decrease. The category with the least reduction, on the other hand, was 'Privacy Violation'.

## 7 Related Works

LLM representation space.To better understand and interpret large language models, there have been various attempts to extract meaningful information from LLM activations or layer parameters . One such attempt is to probe LLM embeddings for truthfulness . Our work differs by identifying latent knowledge related to harmfulness through an activation subspace. On the other hand, there have been works that focus on ways to extract embeddings regarding specific factual knowledge to edit  or unlearn  it from the LLM parameters. Others studied methods to detect toxicity or harmful content using the LLM embeddings . Different from these works, we investigate the problem of safety-aware fine-tuning, aiming to adapt a model to the task while withstanding the influence of harmful data.

Supervised Fine-tuning.Many works have performed Supervised Fine-Tuning (SFT) to enhance LLMs' ability to follow instructions , or customize behaviors or personality traits . Regarding what data samples to use for SFT, numerous works have revolved around efficiently and effectively selecting high-quality samples , while a handful of works considered the impact of harmful data samples in the dataset. Specifically,  showed how data samples influence harmfulness of the model, whereas  revealed the potential negative impact of benign samples on the safety of fine-tuned models. While some works considered methods to make the fine-tuning stage safe , our SAFT differs by providing a more fundamental and direct way of mitigating malicious fine-tuning by detecting and removing harmful samples beforehand.

## 8 Conclusion

With the advent of pre-trained LLMs, adapting the models to individual needs and preferences has been one of the most intriguing goals. To accomplish this, Supervised Fine-Tuning (SFT) can be seamlessly applied with task-specific data. While SFT allows LLMs to improve their performance on custom datasets, safety concerns may arise when harmful samples are involved in the datasets. Manual curation of datasets to remove harmful samples is not only resource-intensive but also prone to subjectivity. To mitigate this challenge, we proposed a novel safety-aware fine-tuning (SAFT) framework designed to automatically detect and filter out potentially harmful samples, by leveraging the lower dimensional subspace representation of the dataset. Our experiments, conducted across diverse contamination rates and LLM families, demonstrated the effectiveness, simplicity, and flexibility of SAFT in practical scenarios. With SAFT, we pave the way for safer and more reliable usage of fine-tuned models tailored to individual needs.

Figure 6: **Category-wise Harmfulness Score (\(=0.3\)).**

Acknowledgement

We thank Leitian Tao for his valuable suggestions on the draft. The authors would also like to thank NeurIPS anonymous reviewers for their helpful feedback. Du is supported by the Jane Street Graduate Research Fellowship. Li gratefully acknowledges the support from the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Philanthropic Fund from SFF, and faculty research awards/gifts from Google and Meta.