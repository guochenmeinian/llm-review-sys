# Training Transformers with 4-bit Integers

Haocheng Xi\({}^{2}\), Changhao Li\({}^{1}\), Jianfei Chen\({}^{1}\), and Jun Zhu\({}^{1}\)

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint ML Center, Tsinghua University

\({}^{2}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

{xihc20,lichangh20}@mails.tsinghua.edu.cn, {jianfeic,dcszj}@tsinghua.edu.cn

This work was done during an internship in the Department of Computer Science and Technology, Tsinghua University

###### Abstract

Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and image classification. Unlike previous 4-bit training methods, our algorithm can be implemented on the current generation of GPUs. Our prototypical linear operator implementation is up to 2.2 times faster than the FP16 counterparts and speeds up the training by 17.8% on average for sufficiently large models. Our code is available at https://github.com/xijiu9/Train_Transformers_with_INT4.

## 1 Introduction

Training neural networks is computationally demanding. Training with low-precision arithmetic (a.k.a., fully quantized training or FQT) is promising to improve computational and memory efficiency. FQT methods add some quantizers and dequantizers in the original full-precision computational graph, and replace expensive floating-point operations with cheap low-precision ones.

Research in FQT aims to reduce the training numerical precision, without sacrificing much convergence speed or accuracy. The required numerical precision has been reduced from FP16  to FP8 , INT32+INT8  and INT8+INT5 . FP8 training is implemented in Nvidia's H100 GPU with Transformer Engine , achieving impressive speedup for the training of large-scale transformers. Recently, the training numerical precision has been pushed down to 4 bits. Sun et al.  successfully trained several modern networks with INT4 activation/weights and FP4 gradients; and Chmeli et al.  propose a custom 4-bit logarithmic numerical format to further improve the accuracy. However, these 4-bit training methods cannot be directly utilized for acceleration as they require custom numerical formats that are not supported on contemporary hardware.

There are significant optimization challenges to train neural networks at an extremely low 4-bit level. First, the non-differentiable quantizers in forward propagation make the loss landscape rugged, where gradient-based optimizers can easily stuck at local optima . Second, gradients are only computedapproximately in low-precision. Such imprecise gradients slow down the training process and even cause the training to be unstable or diverge.

In this work, we propose a novel INT4 training algorithm for a class of popular neural networks, transformers . All the costly linear operations for training transformers can be written in a matrix multiplication (MM) form. This MM form allows us to design more flexible quantizers, which better approximate FP32 matrix multiplications by utilizing specific structures of the activations, weights, and gradients in transformers. Our quantizers leverage advances in the field of randomized numerical linear algebra (RandNLA) .

For forward propagation, we find that outliers in the activation are the main reason for accuracy degradation. To suppress outliers, we propose a _Hadamard quantizer_, which quantizes a _transformed version_ of the activation matrix. The transformation is a block diagonal Hadamard matrix, which spreads the information carried in outliers to its nearby entries of the matrix and thus reduces the numerical range of the outliers.

For backpropagation, we exploit the _structural sparsity_ of activation gradients. We find that the gradients of a few tokens are extremely large. Meanwhile, the gradients for the rest majority of the tokens are very small, even smaller than the quantization residuals of larger gradients. Rather than computing these small gradients, it is better to save the computational resources for calculating the residuals of the larger gradients. To utilize such sparsity, we propose _bit splitting_, which splits the gradient of each token into higher 4 bits and lower 4 bits. Then, we choose the most informative gradients by _leverage score sampling_, which is an importance sampling technique for RandNLA.

Combining quantization techniques for forward and backward propagation, we propose an algorithm that uses INT4 MMs for all linear operations in transformers. We evaluate our algorithm for training transformers on a wide variety of tasks, including natural language understanding, question answering, machine translation, and image classification. Our algorithm achieves competitive or superior accuracy compared with existing works on 4-bit training [47; 8]. Moreover, our algorithm _is compatible with contemporary hardware_ like GPUs, since it does not require custom numerical formats like FP4 or logarithm formats. Our prototypical quantization + INT4 MM operator implementation is up to 2.2 times faster than the FP16 MM baseline, and it speeds up the training by up to 35.1%.

Finally, we would like to point out that utilizing ultra-low 4-bit numerical formats for training neural networks is still an open problem, and the main purpose of this research is to study whether it is possible to design an INT4 training algorithm that can achieve reasonable accuracy on practical tasks. The current research state of INT4 training is not yet mature enough to provide significant speedup for most tasks in a generic and plug-and-play manner. See Sec. 6 for further discussions.

## 2 Related Work

Fully Quantized TrainingFully quantized training (FQT) [33; 54; 46; 3; 15; 57; 65; 29; 30; 59; 68] methods accelerate training by quantizing the activations, weights, and gradients to low-precision, so linear and nonlinear operators during training can be implemented with low-precision arithmetic. Research on FQT design novel numerical formats and quantization algorithms that better approximate full-precision tensors. The current research frontier is 4-bit FQT. FQT is challenging due to the vast numerical range of the gradient and the optimization issues of training quantized networks from scratch. Due to these challenges, existing 4-bit FQT algorithms [47; 8] still have \(\)1-2.5% accuracy drop on several tasks, and they cannot support contemporary hardware.

Other Efficient Training MethodsMixture-of-experts  improves the model capacity without increasing the training budget. Structural dropout [21; 17] exploits computationally efficient ways to regularize the model. Efficient attention [27; 10] reduces the quadratic time complexity for computing attention. Distributed training systems [39; 22] reduce training time by leveraging more computational resources. Our work on reducing numerical precision is orthogonal with these directions.

## 3 Forward Propagation

Neural network training is an iterative optimization procedure with stochastic gradients computed by forward and backpropagation. We accelerate forward and back propagation with 4-bit integer (INT4) arithmetic. We first describe the forward propagation of our training procedure. The forward propagation can be formulated as a composition of linear and non-linear (GeLU, normalization, softmax, etc.) operators. In our training procedure, we accelerate all the linear operators with INT4 arithmetic and leave all the less-computationally-intensive non-linear operators in the 16-bit floating-point (FP16) format. All linear operations in transformers can be written in a matrix multiplication (MM) form. For ease of presentation, we consider the acceleration of the following simple matrix multiplication throughout this paper:

\[=^{},^{N  C},^{N D}^{C D}.\] (1)

The most predominant use case of such MM is the fully-connected layer. Consider a transformer with an input shape of _(batch size \(S\), sequence length \(T\)_, dimensionality \(D\)). The fully-connected layer can be written as Eq. (1) where \(\) is the activation for \(N=ST\) tokens, and \(\) is the weight matrix. For attention layers, batch matrix multiplications (BMMs) might be required. Our proposed techniques can be applied to BMMs, and we leave the discussion of BMMs in Appendix. A.1.

### Learned Step Size Quantization

To accelerate training, the forward propagation must be computed with integer arithmetic. We leverage the _learned step size quantizer_ (LSQ)  for this purpose. LSQ is a static quantization method whose quantization scale does not depend on the input, and is thus cheaper than dynamic quantization methods , which need to compute the quantization scale dynamically per iteration.

Given a FP matrix \(\), LSQ _quantizes_\(\) to integer with

\[_{s_{X}}():=(/s _{X},-Q_{N},Q_{P})\;,\] (2)

where \(s_{X}\) is a learnable scalar parameter, clamp restricts its input to the range \([-Q_{N},Q_{P}]\), \(\) is a rounding operation, and \(/s_{X}\) is computed elementwise. The resultant matrix takes values from \(\{-Q_{N},-Q_{N}+1,,Q_{P}\}\). Since we aim to perform INT4 MMs, we set \(Q_{N}=Q_{P}=7\). The integer matrix can be _dequantized_ back to FP through float \((_{s_{X}}())=s_{X}_{s_{X} }()\).

With LSQ, Eq. (1) can be computed approximately as \(=^{} s_{X}s_{W}_{s_{X}} ()_{s_{W}}()^{},\) where the INT4 MM \(_{s_{X}}()_{s_{W}}( )^{}\) can be implemented efficiently on hardware.

**Remark:** Quantization-aware training (QAT) [63; 67; 23; 12; 11; 44; 60; 45; 49; 64; 2; 18; 55] is an _inference acceleration_ technique which trains networks with quantizers inserted in the forward propagation graph, so the trained network can perform efficiently during inference. QAT can compress activation/weights to extremely low precision (e.g. 1-2 bits). It is tempting to think that directly applying a quantizer for QAT to FQT can lead to similar low activation/weights bit-width. However, even only quantizing the forward propagation for FQT is much more challenging than QAT because: (1) QAT requires a converged full-precision model as initialization  and/or as a teacher model for knowledge distillation ; (2) QAT can adopt expensive multi-stage training pipelines without worrying about the convergence speed , while FQT algorithm must converge as fast as full-precision training algorithms to be useful; (3) QAT may approximate the discrete quantizer with continuous functions during training , which cannot be implemented with integer arithmetic. Due to these challenges, it is still an open problem to do FQT with 4-bit activations/weights.

### Activation Outliers

Simply applying LSQ for FQT with 4-bit activation/weights leads to accuracy degradation due to _activation outliers_. As shown in Fig. 1(a), activations have some outlier entries, which are much larger in magnitude than other entries. In this case, the step size \(s_{X}\) poses a trade-off between quantization granularity and representable numerical range. If \(s_{X}\) is large, we can represent the outliers well at the expense of representing most other entries in a very coarse manner. On the other hand, if \(s_{X}\) is small, we have to truncate the entries outside the range \([-Q_{N}s_{X},Q_{PS}s_{X}]\). Unfortunately, the transformers tend to store information in these outliers, and such truncation would seriously harm accuracy (see Sec. 5.2 for details). The outlier problem is particularly significant when the training task is to fine-tune a pre-trained model on some new downstream tasks, since the pre-train model contains more outliers  than random initialization.

There exists some works to handle activation outliers for post-training quantization (PTQ). Outlier Suppression  discover that LayerNorms amplify outliers, and propose Gamma Migration and Token-Wise Clipping to solve this issue and achieves 6-bit BERT PTQ without too much degradation. SmoothQuant  migrates the quantization difficulty of activation outliers to weights and achieves 8-bit PTQ for large language models, such as OPT-175B. Outlier Channel Splitting  duplicates channels containing outliers with small overhead on the size of the network. However, these methods mainly focus on PTQ or QAT, and seldom successfully deal with ultra-low 4-bit training.

### Hadamard Quantization

We propose a _Hadamard quantizer_ (HQ) to solve the outlier problem. Its main idea is to quantize the matrices _in another linear space_ which has fewer outliers.

The outliers in activation matrices form a feature-wise structure . They are typically concentrated on a few dimensions, i.e., only a few columns of \(\) are significantly larger than others. Hadamard transform  is a linear transformation, which can amortize the outliers into other entries. Specifically, the Hadamard transform \(_{k}\) is a \(2^{k} 2^{k}\) matrix, where

\[_{0}=[1],_{k}=}[ _{k-1}_{k-1};_{k-1}-_{k-1} ].\]

Hadamard matrices are orthogonal and symmetric: \(_{k}=_{k}^{}=_{k}^{-1}\), so \(_{k}_{k}=, k 0\). Consider any coordinate row vector*\(_{i}^{}^{2^{k}}\), we have \(_{i}^{}_{k}=2^{-k/2}_{2^{k}}, i\), where \(_{2^{k}}=( 1, 1,, 1)\) is a \(2^{k}\)-dimensional vector with all of its values being \(1\) or \(-1\). This demonstrates the extreme case when a single outlier dominates all the rest dimensions. In this case, Hadamard transformation effectively turns the vector into a quantization-friendly all-one-vector. The practical effect of the Hadamard transform on suppressing activation outliers is demonstrated in Fig. 2(b).

Footnote *: A vector which \(i\)-th dimension is 1, and all other dimensions are 0.

HQ uses a block-diagonal transformation matrix \(^{D D}\): \(=(_{k},,_{k}),\) where \(D\) is a multiple of \(2^{k}\). To suppress outliers, we quantize a transformed version of \(\) and \(\):

\[=()^{} s_{X}_{s _{X}}()^{},=( )^{} s_{W}_{s_{W}}( )^{}.\]

Combining the quantized matrices, we get

\[=^{} s_{X}s_{W}_{s_{X}} ()^{}_{s_{W}}( ^{}^{}) =s_{X}s_{W}_{s_{X}}() _{s_{W}}(^{}^{}),\] (3)

where the inverse transformations cancel with each other, and the MM can be implemented as:

**Procedure** HQ-MM

Compute \(\) and \(^{}^{}\) in FP16.

Quantize the resultant matrices to INT4 by LSQ.

Multiply the two INT4 matrices.

Dequantize the resultant INT32 matrix to FP16 by multiplying \(s_{X}s_{W}\).

For time complexity, Step 1 takes \(O(2^{k}N(D+C))\) FP16 multiply-accumulates (MACs); Step 2and Step 4 takes \(O(N(D+C))\) FP16 MACs in total; and Step 3 takes \(O(NDC)\) INT4 MACs. Comparing with the plain LSQ Eq. (2), the amount of FP16 MACs increases by \(2^{k}\) times, from \(O(N(D+C))\) to \(O(2^{k}N(D+C))\). However, our HQ-MM is still much cheaper than an FP16 MM given \(2^{k} D\) and \(2^{k} C\). The number \(k\) shows a tradeoff between the ability to suppress outliers and computation complexity. Larger \(k\) allows for amortizing the outlier within a larger horizon, at the cost of being more expensive. We propose an adaptive algorithm to choose \(k\) for each activation depending on the outlier scale, as discussed in Appendix A.5. The typical value is \(k=5\), while the dimensionality \(C\) and \(D\) ranges from 768 to 4096.

## 4 Backpropagation

We now consider accelerating the backpropagation of the linear layer with INT4 operations. The linear operator HQ-MM defined in Eq. (3) has four inputs: activation \(}\), weight \(\), and step sizes \(s_{X}\), \(s_{W}\). Given the output gradient \(_{}\) w.r.t. some loss function \(\), we need to compute the gradient of all four inputs. We discuss the computation of activation/weight gradients in this section, and left the discussion of step size gradients to Appendix A.3. For simplicity, we omit \(\) and simply use \(_{}\) to denote the gradient in the following text.

By the straight-through estimator \( x^{}=1\) and the chain rule, we have

\[_{}=s_{X}(_{}^{}} _{W})^{},_{}=s_{W} _{X}_{}}^{},\] (4)

where we define \(}=_{s_{X}}()\), \(}=_{s_{W}}()\), \(_{X}=(-Q_{N}/s_{X} Q_{P})\), and \(_{W}=(-Q_{N}/s_{W} Q_{P})\). For computing the gradients, three types of matrix multiplications are required:

1. The element-wise multiplication \(\) of a \(0/1\) matrix \(_{X}\) (or \(_{W}\)) with another INT4 (or INT32) matrix. This operation has low time complexity.
2. The multiplication of an INT32 matrix with an FP16 block-wise Hadamard matrix \(s_{W}^{}\), which also has low-time complexity, as discussed in Sec. 3.3.
3. The multiplication of the FP16 gradient \(_{}\) with an INT4 matrix \(}\) or \(}\), which we will accelerate by quantizing \(_{}\) to INT4.

In the rest of this section, we will discuss quantization methods to compute the "type 3" MMs \(_{}^{}}\) and \(_{}}\). We quantize \(_{}\) dynamically for each MM, while \(}\) and \(}\) have been already calculated in forward propagation in Section. 3. We start by discussing the structure of the gradient.

### Structural Sparsity of Gradients

We note that the gradient matrix \(_{}\) tends to be very sparse along the training process. Furthermore, the sparsity has a structure: few rows (i.e., tokens) of \(_{}\) have large entries, while most other rows are close to an all-zero vector. We illustrate this by plotting the histogram of per-row norm \(\|(_{})_{i,:}\|\) for all the rows \(i\) in Fig. 2.

Such a structural sparsity arises from the heavy overparameterization  of modern neural networks. During almost the entire training process, the network operates in the overparameterized scheme , where it can fit most training data well, except for a few hard examples. Therefore, the (activation) gradient will be close to zero for well-fitted data points. We find that for pretraining tasks, such structural sparsity quickly emerges after only a few training epochs. For fine-tuning tasks, the gradient is always sparse during the whole training process.

### Bit Splitting and Leverage Score Sampling

Here, we discuss how to design gradient quantizers to accurately compute the MMs during backpropagation by leveraging structural sparsity. The high-level idea is that many rows of the gradient are so small that they have little impact on the parameter gradient, yet they waste abundant computation. On the other hand, the large rows cannot be accurately represented with INT4. We drop some small rows and use the saved computation to represent large rows more accurately.

First, we propose _bit splitting_ (BS), which splits a full-precision matrix as higher and lower 4 bits:

\[_{} s_{}_{}^{}+s_{ }_{}^{},\] (5)

where \(s_{},s_{}\) are two floating-point scalars, and \(_{}^{}\), \(_{}^{}\) are INT4 matrices representing the higher and lower 4 bits, respectively. BS can be implemented by first quantizing \(_{}\) to INT4 as \(_{} s_{}_{}^{}\) and then quantize the residual to INT4 as \(_{}-s_{}_{}^{} s_{ }_{}^{}\). BS can be viewed as an INT8 representation of a matrix, where \(_{}^{}\) and \(_{}^{}\) are the higher and lower 4 bits of the INT8 representation. Next, we discuss how to compute the weight and activation gradient.

Weight GradientAs discussed earlier, weight gradient involves the matrix multiplication \(_{}^{}}\), where \(_{}^{N C}\) and \(}\) is an \(N D\) INT4 matrix. By Eq. (5):

\[_{}^{}}(s_{}_{ }^{}{}^{}+s_{}_{}^{ }{}^{})}=_{}^{}{}^{}^{},\] (6)

where we define \(_{}^{}=[s_{}_{}^{};s_{ }_{}^{}]^{}^{2N C}\) and \(}^{}=[};}]\) to be a \(2N D\) INT4 matrix. Eq. (6) represents the product of an INT8 \(_{}^{}\) and an INT4 \(}\), and can be implemented by two INT4 MMs \(_{}^{}{}^{}}\) and \(_{}^{}{}^{}}\). Such MM is rather accurate since \(_{}\) is represented with 8 bits.

However, comparing to a naive quantization of \(_{}\) to INT4, BS doubles the amount of INT4 operations for MM. We propose _leverage score sampling_ (LSS) to cut the operations of Eq. (5) by half, to the same amount as the naive MM \(s_{}_{}^{}}\). Noticing that the MM Eq. (6) can be written as the sum of \(2N\) rank-1 matrices:

\[_{}^{}{}^{}^{}=_{i=1}^{2N} _{;i}^{}{}^{}_{i}^{}=_{i=1} ^{2N}_{_{i}},\] (7)

where \(_{_{i}}=_{;i}^{}{}^{}_ {i}^{}\). Due to the sparsity of \(_{}\), the matrices \(_{_{i}}\) differ in magnitude and small matrices can be discarded without having a big influence on the result.

Our proposed LSS assigns each \(_{_{i}}\) a probability \(p_{i},i=1,,2N\), that satisfies \(_{i=1}^{2N}p_{i}=N\). We define random masks \(m_{i}(p_{i})\) and mask matrix \(}\), and approximate it as

\[_{}^{}{}^{}^{}_{ }^{}{}^{}}^{}=_{ i=1}^{2N}}{p_{i}}_{;i}^{}{}^{}_ {i}^{},}=(}{p_{1}}, ,}{p_{2N}}),\]

which is an unbiased approximation since \([_{}^{}{}^{}} ^{}]=_{}^{}{}^{}[}]^{}=_{}^{ }{}^{}^{}\).

In expectation, there are only \(N\) nonzero \(m_{i}\)s. Therefore, LSS reduces the cost of MM by half. For LSS to be accurate, we minimize its variance. We have:

**Proposition 4.1**.: _(LSS variance for weight gradient)_

\[[_{i=1}^{2N}}{p_{i}}_{;i}^{ }{}^{}_{i}^{}]=_{i=1}^{2N}}{p_{i}} \|_{_{i};i}^{}\|^{2}\|_{i,:}^{}\|^{ 2},[]:=[\|- \|]_{F}^{2}.\]

The coefficient \(c_{i}:=\|_{_{i};i}^{}\|\|_{i,:}^{}\|\) is called the _leverage score_, which can be easily computed in low time complexity. When \(p_{i} c_{i}\), the variance attends its minimum due to Cauchy inequality:

\[_{i=1}^{2N}}\|_{_{i};i}^{}\|^{2}\| _{i,:}^{}\|^{2}=_{i=1}^{2N}^{2}}{p_{i}}=_{ i=1}^{2N}^{2}}{p_{i}}_{i=1}^{2N}p_{i}(_{i=1}^{2N}c_{i})^{2},\]

where the equality holds when \(p_{i} c_{i}\). Intuitively, LSS can approximate the MM Eq. (7) well with significantly lower computational cost when the leverage scores \(\{c_{i}\}\) are diverse, which is indeed the case as shown in Fig. 2.

Define \(^{}\) to be the top-left \(N N\) submatrix of \(}\) and \(^{}\) to be the bottom-right one, we have

\[_{}^{}{}^{}}^{} =s_{}_{}^{}{}^{}}^{} }+s_{}_{}^{}{}^{}}^{}},\]which can be implemented by two INT4 MMs with sampled rows/columns. Putting everything together, we propose the following MM procedure to compute the weight gradient:

**Procedure** LSS-MM

1. Quantize \(_{}\) with BS to obtain \(_{}^{}\) and \(_{}^{}\) in INT4.
2. Compute the leverage score \(\|_{_{i,:}}^{}\|\|_{i,:}^{}\|\) in FP16.
3. Sample the masks \(\{m_{i}\}\).
4. Sample rows of \(_{}\) and \(}\) given the masks \(\{m_{i}\}\).
5. Compute INT4 MMs \(_{}^{}\,}^{}}\) and \(_{}^{}\,}^{}}\),
6. Dequantize and sum up the resultant INT32 matrices to obtain the FP16 result \(_{}^{\,}\,}^{}\).

As \(}\) only has \(N\) non-zero elements in expectation, the two matrix multiplications in Step 5 take about \(2NCD\) INT4 MACs, which aligns with the cost of the naive MM \(s_{}_{}^{}}\). The overhead of all the other steps is \(O(NC+ND)\) in total.

Activation GradientSimilar to the previous discussion, the gradient of input can be written as

\[_{}\,}(s_{}_{}^ {}+s_{}_{}^{})}=s_{ }_{}^{}\,}+s_{}_ {}^{}\,}=(}^{} _{}^{})},\] (8)

where we define \(_{}^{}=[s_{}_{}^{};s_{ }_{}^{}]^{2N C}\) and \(}^{}=[]\) to be a \(N N\) INT4 matrix, \(\) is a \(N N\) identity matrix. The original product can also be implemented by two INT4 MMs \(_{}^{}}\) and \(_{}^{}}\). But different from weight gradients, we now focus on \(}^{}_{}^{}\) in Eq. (8) and do leverage score sampling on this MM. A detailed discussion can be found in B.2, and we only present the leverage score here. Similarly, we write the MM as the sum of \(2N\) smaller multiplications:

\[}^{}_{}^{}=_{i=1}^{2N} {}_{:,i}^{}_{i}^{}}{p_{i}}_{i=1}^{2N}_{_{i}},\]

where we define \(_{_{i}}=}_{:,i}^{}_{ }^{}\) and associate the probability \(p_{i}\) and Bernoulli mask \(m_{i}(p_{i})\) with the \(i\) multiplication. The leverage score for activation gradient is \(c_{i}:=\|_{_{i}}^{}\|\), and the variance attains minimum when \(p_{i} c_{i}\). More details about the algorithm can be found at Appendix. A.3 On the implementation side, once the mask \(\{m_{i}\}\) is known, we can decompose the MM Eq. (8) as two INT4 MMs: \((}^{}}_{}^{ })}=s_{}}^{} _{}^{}\,}+s_{}}^{}_{}^{}}\).

## 5 Experiments

We evaluate our INT4 training algorithm on a wide variety of tasks including language model fine-tuning, machine translation, and image classification. We implement our proposed HQ-MM and

    &  &  &  &  &  \\   & & & & FP & INT8 & LSO+LUO & HQ+LSS \\  GLUE-dev & FT & Bert-base & Avg & \(82.67_{0.24}\) & \(81.45_{0.13}\) & \(75.29_{0.23}\) & \(}\) \\  SQUAD v1 & FT & Bert-base & Avg & \(84.57_{0.42}\) & \(82.74_{0.24}\) & \(55.93_{2.47}\) & \(}\) \\  SQUAD v2 & FT & Bert-base & F1 & \(88.32_{0.30}\) & \(88.42_{0.20}\) & \(85.75_{0.31}\) & \(}\) \\  SQUAD v2 & FT & Bert-base & F1 & \(76.04_{0.68}\) & \(75.63_{0.07}\) & \(71.02_{0.41}\) & \(}\) \\  Adversarial QA & FT & Bert-base & F1 & \(40.99_{0.38}\) & \(40.17_{0.58}\) & \(31.85_{0.30}\) & \(}\) \\  SWAG & FT & Bert-base & Acc & \(79.84_{0.10}\) & \(79.18_{0.19}\) & \(70.79_{1.20}\) & \(}\) \\  CONLL & FT & Bert-base & Acc & \(53.38_{0.68}\) & \(93.13_{0.14}\) & \(87.63_{0.39}\) & \(}\) \\  WMT & PT & Transformer-base & BLEU & \(27.5\) & \(25.4\)(UNTA Low) & \(27.17\) & \(-\) \\   & FT & ViT-B/32 &  & \)} & \)} & \)} & \)} \\  & & ViT-B/32 & & & & & \\   & FT & ViT-B/32 &  & \)} &  & \)} & \)} \\  & & ViT-B/32 & & & & & \\   & FT & ViT-B/32 &  & \)} & \)} & \)} & }\)} \\  & & ViT-L/32 & & & & & \\   & FT & ViT-B/32 &  &  &  & \)} \\  & & ViT-L/32 & & & & & \\   & FT & ViT-L/32 &  &  &  &  & \)} \\  & & ViT-L/16 & & & & & & \\   & Dett-Small &  &  & =\), while cutlass only supports \(=^{}\), so explicit transpose is required. We also do not fuse the linear operators with nonlinearities and normalizations. Therefore, the results cannot fully reflect the potential of INT4 training algorithms. A fully optimized implementation requires heavy engineering, which exceeds the scope of our paper.

Operator Speed:We compare the throughput of our proposed HQ-MM (HQ), LSS for computing weight gradient (LSSWeight), LSS for computing activation gradient (LSSAct), and their average throughput (INT4) with a baseline tensor-core FP16 GEMM implementation (FP16) provided by cutlass in Fig. 4 on an Nvidia RTX 3090 GPU which has a peak throughput at 142 FP16 TFLOPs and 568 INT4 TFLOPs. As the matrix size grows, the overhead of quantization diminishes and our INT4 operators can be up to 2.2 times faster compared with FP16 MM. We further analyze the quantization overhead for each operator in Appendix C.5.

Training Throughput:We compare the training throughput of the FP16 PyTorch AMP and our INT4 training algorithm for training BERT  and GPT -style language models on a system of 8 Nvidia A100 GPUs. We vary the hidden layer size, intermediate fully-connected layer size, and batch size, and plot the speedup of INT4 training in Fig. 5. Our INT4 training algorithm can achieve up to 35.1% speedup and an average of 15.8 % for BERT-style models and up to 26.5% speedup and an average of 19.2 % for GPT-style models. The training time can be found in Appendix C.4.

Inference Speed:We compare the inference speed of our algorithm with I-BERT  by comparing the speedup numbers reported in its original paper. Following the I-BERT paper, we compare the speedup of integer-based inference algorithms relative to a FP32 baseline on an Nvidia T4 GPU on the BERT-base and BERT-large models, and test with sequence lengths of 128 and 256. While I-BERT only reported speedup numbers for batch sizes 1, 2, 4, 8, we test the speedup for batch sizes ranging from 1 to 1024 to better reflect the performance of throughput-oriented scenarios (such as a cloud language model service provider).

In Table 2 we report the speed up result. While I-BERT's speedup numbers seems to be insensitive to the batch size and sequence length, our speedup increases with the batch size. I-BERT shows up to 3.98x speedup for smaller batch size, while our algorithm can achieve higher speedup for batch sizes higher than 64, and eventually gives a speedup of 6.48x for a sequence length of 128 and a batch size of 1024.

For the BERT-base model, our method shows an inference speed improvement of 3.57-4.92 times compared to FP32 for batch sizes larger than 64. In comparison, I-BERT achieved an inference speed improvement of 2.42-3.39 times compared to FP32 when the batch size was small. For BERT-large, our method shows an inference speed improvement of 4.81-6.48 times compared to FP32 for batch sizes larger than 64. In comparison, I-BERT achieved an inference speed improvement of 3.20-4.00 times compared to FP32 when the batch size is small. Therefore, our algorithm can potentially achieve higher throughput than I-BERT.

## 6 Conclusions

We propose a hardware-friendly INT4 training method for transformers. By analyzing the properties of MMs in transformers, we propose HQ and LSS methods to quantize activations and gradients while preserving accuracy. On several important tasks, our method performs comparably or better than existing INT4 methods. Our work can be potentially extended beyond transformers to other MM-only architectures, such as MLP-Mixer , graph neural networks , and recurrent neural networks . We leave it as a future direction.

**Broader Impacts:** Our algorithm can improve efficiency and reduce the energy consumption of training neural networks, which helps reduce the carbon footprint caused by deep learning. However, our efficient training algorithm might also facilitate the development of large language models with safety concerns for human beings; and malicious AI applications such as fake content generation.

**Limitations:** Our INT4 training algorithm does not support convolutional layers. Moreover, our algorithm cannot yet work well for those extremely large models such as OPT-175B. LSS utilizes gradient sparsity, and may not work well for certainty pretraining tasks. Finally, our algorithm may incur more memory accesses than FP16 training algorithms, which might affect the speedup.