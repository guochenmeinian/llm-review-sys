# Adaptive Preference Scaling for Reinforcement Learning with Human Feedback

Ilgee Hong

Georgia Institute of Technology

ihong39@gatech.edu

Equal contribution.

Zichong Li

Georgia Institute of Technology

zli911@gatech.edu

Alexander Bukharin

Georgia Institute of Technology

abukharin3@gatech.edu

Yixiao Li

Georgia Institute of Technology

yixiaoli@gatech.edu

Haoming Jiang

Amazon

jhaoming@amazon.com

Tianbao Yang

Texas A&M University

tianbao-yang@tamu.edu

Tuo Zhao

Georgia Institute of Technology

tourzhao@gatech.edu

###### Abstract

Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.

## 1 Introduction

In the field of artificial intelligence, aligning AI systems with human preferences has become increasingly crucial, particularly for applications involving complex data and models like large language models (LLMs) in natural language processing . Reinforcement learning from human feedback (RLHF) has gained popularity for customizing AI systems . RLHF involves learning a reward function from human preference data, then using a reinforcement learning algorithm to train a policy to optimize the learned reward model.

A key challenge in RLHF lies in the complexity of reward modeling, which primarily stems from the reliance on preference labels. Since preference labels only provide comparative rankings of trajectory segments without quantifying the scale of underlying preference strengths, previous methods have employed the Bradley-Terry (BT) model  in conjunction with cross-entropy loss to learn the reward function from preference data . This approach assumes that the logit of the preference distribution scales linearly with the reward difference across all sample pairs. However, such linear scaling is often insufficient to account for the variations in preference strength among different pairs, restricting the reward function's ability to capture a broader range of reward differences. This restrictive approach to reward modeling limits the flexibility of the learned reward function, hindering its capacity to produce the versatile rewards essential for the downstream policy optimization.

To overcome this shortcoming, we introduce a novel adaptive preference loss function inspired by distributionally robust optimization (DRO) . Our approach incorporates an instance-specific scaling factor to change the scaling between the preference distribution and the reward difference to be non-linear. These factors are learned during training and enable the model to accommodate varying uncertainties of preference strength, thereby enhancing the flexibility of the reward. For pairs showing strong preference (i.e., low preference uncertainty), our method learns a large scaling factor, which enables the model to learn a larger reward difference. In contrast, for pairs showing ambiguous preferences (i.e., high preference uncertainty), our method assigns a smaller scaling factor, enabling the model to learn a smaller reward difference. The additional computational overhead of involving this scaling factor into training is negligible, as the proposed loss function is strictly convex and univariate with respect to each scaling parameter. Therefore, it can be easily optimized by a simple second-order algorithm within a few iterations.

Our experiments on robotic control tasks  demonstrate that our method can learn a more flexible reward function, resulting in an improved policy. Surprisingly, we also discover that our method better aligns the learned reward function with downstream policy optimization. Specifically, when tuning hyperparameters for reward modeling, the simplest approach is to select the reward model according to preference prediction accuracy. However, the selected reward function (with the highest accuracy) often yields a downstream policy with poor performance. To address this misalignment, we usually have to jointly tune the parameters across both stages according to downstream policy performance, resulting in significant computational burden and tuning effort. Our proposed method can mitigate this misalignment: When using our adaptive loss, we can select the reward model based on preference prediction accuracy alone and yield a reasonably well-performing policy. This allows separate tuning of the two stages, easing tuning overhead. To our knowledge, the challenge of this misalignment issue is almost untouched in the RLHF literature, and we are the first to propose a principal approach to mitigate this issue.

Moreover, our method is generalizable and can be applied to other preference optimization algorithms. For instance, we implement it with direct preference optimization (DPO)  and evaluate its effectiveness on natural language generation tasks using Llama-2 7B . Our results demonstrate that integrating adaptive preference scaling into DPO boosts policy performance, while preserving the benefits of alignment. Alignment is especially critical in this setting, where we employ proprietary models like Claude 3  as judges for policy selection, which demands substantial costs for using the APIs. In the case without access to LLM assessment, we must select policy based solely on preference accuracy, under which our approach substantially outperforms other baselines.

## 2 Related works

**Loss functions for reward learning.** Prior work on this topic is very limited. For example, Song et al.  propose using different loss functions for strong and ambiguous preference data in natural language generation tasks. They apply heavy-tailed loss functions for open-ended questions, where preference ambiguity is desirable, and light-tailed loss functions for close-ended questions requiring clear-cut rewards. However, their approach requires knowing the question type a priori, necessitating extra labeling effort, and may fail for complex questions containing both open and closed aspects. Zhao et al.  propose using a hinge loss, which results in zero gradient when the learned reward difference exceeds a margin of 1. This limits the ability to learn very large differences in rewards. Azar et al.  develop \(\) Preference Optimization with Identity Mapping (IPO), which modifies DPO with a loss function matching the scaling of KL-divergence between the learned policy and the initial policy to avoid overfitting due to weak regularization. In contrast to prior work, our method is more broadly applicable to complex preference learning tasks without needing additional labeling or sacrificing the ability to learn arbitrarily large reward differences.

**Adaptive temperature scaling (ATS).** Temperature scaling (TS) aims to adjust the entropy of probabilistic models by rescaling their logit outputs before the softmax function is applied. This simple method not only enables confidence calibration , but also plays a vital role in various machine learning methods, including knowledge distillation , reinforcement learning , and contrastive learning . Building on TS, adaptive temperature scaling (ATS) enhances flexibility by using instance-specific scalars. Most ATS method trains an additional network for predicting the temperature parameter, which is further integrated into the softmax operator to calibrate the prediction probabilities [44; 14; 4; 21]. In contrast to the aforementioned ATS methods, the proposed adaptive preference scaling (APS) is not designed for classical confidence calibration, but is crafted specifically to enhance the training process of reward function in RLHF. Consequently, the interpretations of scaling factors in ATS and APS are _opposite_. In ATS, a larger scaling parameter is applied to data with higher uncertainty (e.g., data that the classifier is likely to misclassify), which reduces the magnitude of the corresponding logit. Conversely, in APS, a larger scaling factor is assigned to data with clearer preferences, resulting in a larger logit. This distinction clarifies why the scaling parameter in our approach does not correspond to the concept of "temperature" from statistical physics. Additionally, we propose a principled framework for learning scaling parameter based on DRO, which avoids the complexities of designing specific temperature networks and does not rely on heuristically designed loss functions.

**Distributionally robust optimization (DRO).** DRO is a technique that trains machine learning models to be robust against uncertainty in the data distribution. Specifically, DRO finds a solution that performs well under the worst-case distribution within a specified uncertainty set around the empirical data distribution [5; 7; 23; 35; 16]. DRO has been applied in various AI/ML domains to improve generalization when the test distribution differs from the training distribution [27; 18; 26; 10; 45; 30]. Our framework is motivated by Qi et al. , which tackles KL-constrained DRO problem. However, our approach differs in two significant ways. First, instead of using a single KL constraint for the entire training dataset, we apply a separate KL constraint to each individual training data. Second, since each training data involves just two distributional variables, we can use a deterministic method to optimize these efficiently. Note that while our proposed method is inspired by DRO, it serves a distinct purpose: improving reward learning in RLHF, which is _orthogonal_ to distributional robustness.

## 3 Method

In this section, we first outline the problem setup, derive the loss function with adaptive preference scaling, and then provide theoretical motivation for our proposed loss. At last, we present an optimization algorithm, extend the approach to direct preference optimization, and introduce a variant of our proposed loss that incorporates quadratic regularization.

### Problem setup

We consider a reward-free Markov decision process \(=(,,p,)\) with state \(s\), action \(a\), state transition function \(p\), and discount factor \(\). The ground truth reward function \(r:\) is assumed to be unknown, but only human preferences over pairs of trajectory segments are observed. A trajectory segment is a sequence of consecutive state and action pairs \(z=\{(s_{m},a_{m}),(s_{m+1},a_{m+1}),,(s_{k-1},a_{k-1})\}( )^{k-m}\). We denote \(z_{1} z_{2}\) to indicate that the human preferred trajectory segment \(z_{1}\) over the trajectory segment \(z_{2}\) and denote the preferred one with a subscript \(u\) and the disperferred one with a subscript \(l\) (i.e., \(z_{w}\) and \(z_{l}\)). Here, we are given a human preference dataset of trajectory segments \(_{}=(z_{w,i},z_{l,i})_{i=1}^{N}\). Our goal is to find a reward \((s,a)\), which is well-aligned with human preferences. Once we learn the reward, we then find a policy \(_{}^{S}\) such that it maximizes the expected sum of discounted rewards,

\[_{}_{z_{}}(z),\]

where \((z)=_{(s_{t},a_{t}) z}^{t}(s_{t},a_{t})\) and \(_{}\) denotes the stationary distribution of the state-action pair induced by \(\).

### Reward learning with adaptive preference scaling

We now focus on the reward learning phase in RLHF, a crucial stage for capturing human preferences across various trajectory segments. The standard reward learning procedure assumes that the reward function determines a preference distribution, also known as the Bradley-Terry (BT) model ,

\[p_{r}(z_{w} z_{l})=(r(z_{w})-r(z_{l})), \]

where \(\) denotes the sigmoid function. The reward function is then learned by minimizing the expectation of negative log-likelihood of \(r\) over the preference data :

\[_{r}\ _{}(r)=-_{(z_{w},z_{l})_{}} p_{r}(z_{w} z_{l}). \]

As can be seen from (1), the BT model essentially assumes that the logit of the preference distribution \(^{-1}(p_{r}(z_{w} z_{l}))\) scales linearly with the reward difference, regardless of the specific pair of samples. Such linear scaling, however, may not align well with downstream policy learning. Human preferences are often influenced by numerous factors that interact in non-linear ways, making the BT model suboptimal as a reward model. For example, when the reward difference is small, even slight changes in certain features might lead to significant shifts in preference. The BT model may struggle to capture such rapid shifts due to its slower transition.

To address this challenge, we propose an adaptive preference loss based on KL-constrained distributionally robust optimization formulation , which can implicitly change the scaling between the logit and the reward difference to be non-linear. Specifically, given a pair of trajectory segments \((z_{1},z_{2})\), we denote \(d_{r}(z_{1},z_{2})=(z_{1} z_{2})(r(z_{2})-r(z_{1}))\) and \(p=(p_{1},p_{2})\). We define the following instance-level loss:

\[_{r}(z_{1},z_{2}):=_{p_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r} (z_{2},z_{1})-_{0}(p,1/2)\ \ (p,1/2) _{0}, \]

where \(_{2}=\{p^{2}:p_{1}+p_{2}=1,0 p_{1},p_{2} 1\}\), \(1/2\) is denoted for the uniform distribution, and \(_{0},_{0}>0\) are shared prespecified parameters across all instances. \((,)\) denotes the KL divergence. Note that without the KL-constraint, (3) is reduced to the cross-entropy loss with \(_{0}=1\). Unlike general KL-constrained DRO formulation, which considers a distribution \(p\) over all training samples, the distributional variable \(p\) in (3) is associated specifically with binary preference comparisons for each pair.

We then convert (3) into an equivalent minimax formulation based on the Lagrangian duality,

\[_{ 0}_{p_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_ {2},z_{1})-(+_{0})((p,1/2)-_{0}),\]

where \(\) is the Lagrange multiplier. By defining \(\) as \(=+_{0}\) and applying the optimality condition for \(p\), we have

\[_{_{0}}\ - p_{r,}(z_{w} z_{l})+(_{0}- 2), \]

where

\[p_{r,}(z_{w} z_{l})=)-r(z_{l})}{} . \]

We refer to Appendix A.1 for the full derivation. Note that the preference scaling factor \(\) in (4) and (5) serves as the Lagrange multiplier of (3). This scaling parameter \(\) is used specifically for training the reward function \(r\), rather than calibrating the preference distribution \(p_{r,}(z_{w} z_{l})\). The scaler \(\) is used exclusively during the reward learning phase and is no longer needed in subsequent policy optimization, where the reward function \(r\) alone is used.

Moreover, the scaling parameter \(\) is defined to be an instance-specific parameter corresponding to the pair of trajectory segments \((z_{w},z_{l})\). Therefore, when applying our adaptive loss to reward learning, for each pair \((z_{w,i},z_{l,i})\), we need to define a corresponding scaling parameter denoted by \(_{i}\). The overall loss function over the training set \(_{}\) is as follows:

\[_{r,_{1},,_{N}}_{i=1}^{N}_{i}(r, _{i}):=_{i=1}^{N}-_{i} p_{r,_{i}}(z_{w,i } z_{l,i})+_{i}, \]

where \(T=(_{1},,_{N})\), \(=\{:_{0}_{}\}\) with \(_{}\) as another prespecified parameter, and \(=_{0}- 2>- 2\). Here, we also involve an upper bound \(_{}>0\) in (6), and we will explain why it is needed in the next subsection.

### Theoretical insights

We next provide some theoretical insights on why the scaling parameter \(\) can help gain adaptivity by a proposition. For simplicity, we only consider a pair of trajectories.

**Proposition 3.1**.: _Assume we have a pair of trajectories \(z_{1},z_{2}\), and the preference distribution \(p(z_{1} z_{2})=p^{*}(0,1)\), i.e., the probability, that \(z_{1}\) is preferred over \(z_{2}\), is \(p^{*}\). Consider the problem of minimizing the expectation of our adaptive loss function over the preference distribution:_

\[_{r,r}- p^{*}(r(z_{1})-r(z_{2}))/ -(1-p^{*})(r(z_{2})-r(z_{1}))/ +. \]

_Then the minimizer \(^{*}\) and \(r^{*}\) of the expected loss satisfy_

\[^{*}=_{0}&-p^{*}(p^{*})-(1-p^{* })(1-p^{*})+>0,\\ _{}&-p^{*}(p^{*})-(1-p^{*})(1-p^{*})+<0,\] \[r^{*}(z_{1})-r^{*}(z_{2})=^{*}^{-1}(p^{*}).\]

_Here, \(^{-1}\) is the inverse of sigmoid function._

Note that the expected loss (7) is only for easing theoretical analysis, as \(p^{*}\) is not accessible in practice. From Proposition 3.1, we can see that when \(p^{*}\) is close enough to \(0.5\), (i.e., the uncertainty of preference is large), the corresponding optimal \(^{*}\) is at the lower bound \(_{0}\). The resulting optimal reward difference is \(_{0}^{-1}(p^{*})\), which is smaller than the counterpart obtained by the cross-entropy loss when \(_{0}<1\). Conversely, when \(p^{*}\) is close to \(0\) or \(1\), (i.e., the uncertainty of preference is small), the resulting optimal \(^{*}\) is at the upper bound \(_{}\). Here, we introduce the upper bound \(_{}\) to ensure that the optimal \(^{*}\) is bounded. The resulting reward difference in this case is \(_{}^{-1}(p^{*})\), which is larger than the counterpart obtained by the cross-entropy loss when \(_{}>1\). Our theoretical analysis suggests that the adaptive scaling factor essentially changes the correspondence between the logit of preference distribution and the reward difference for each pair of trajectory segments, which could lead to a more flexible reward model.

We further visualize our adaptive preference loss in Figure 1, setting \(_{0}\) and \(_{}\) to \(0.1\) and \(5.0\), respectively. As depicted, our adaptive preference loss behaves distinctly compared to the cross-entropy loss. With large learned reward differences, the cross-entropy tends to be very flat, while our loss maintains a non-trivial gradient, allowing us to continually decrease the loss function. In contrast, for small positive learned reward differences, our loss yields a smaller gradient, thereby less encouraging the reward model to further distinguish pairs of ambiguous trajectory segments. This is consistent with our theoretical analysis.

### Algorithm

We present an efficient algorithm for solving (6). Suppose we parameterize \(r\) as a neural network with parameter \(\). At the \(m\)-th iteration, we have the iterate \(^{(m)}\), and we sample a pair of trajectory segments \(z_{w,i}\) and \(z_{l,i}\). We initialize \(_{i}^{(0)}=1\) and then optimize \(_{i}\) by a projected Newton method subject to a simple interval constraint \(\). Specifically, for \(k=0,...,K-1\), we take

\[_{i}^{(k+1)}=_{_{i}}(_{i}^{(k)}+_{i}^{( k)}), \]

where \(_{i}^{(k)}\) denotes the descent direction

\[_{i}^{(k)}=-}_{i}(^{(m)},_{i}^{(k)})} {_{_{i}}^{2}_{i}(^{(m)},_{i}^{(k)})}. \]

Once we get \(_{i}^{(K)}\), we update \(\) by a stochastic gradient descent step

\[^{(m+1)}=^{(m)}-_{}_{}_{i}(^{(m)},_{i}^{ (K)}), \]

where \(_{}\) is the learning rate. We summarized our proposed algorithm in Algorithm 1, which is presented in a per-data manner for clarity but can be directly adapted for mini-batch learning.

Figure 1: Visualization of the loss function (left) and its gradient (right) on different reward differences.

**Remark 3.1**.: Note that since \(_{i}(,_{i})\) is strictly convex and univariate with respect to \(_{i}\), in each iteration \(m\), \(_{i}^{(K)}\) is guaranteed to be near-optimal (i.e., \(_{i}^{(K)}_{i}^{}\)). Therefore, the convergence of Algorithm 1 can be guaranteed by the convergence of stochastic gradient descent on the reward model parameter \(\).

**Remark 3.2**.: Computationally, Algorithm 1 incurs negligible additional cost. The inner minimization problem (Lines 5-7 in Algorithm 1) can be solved to near-optimality efficiently within a few iterations (e.g. \(K=5\)) given its convex and univariate nature. The additional overhead of each update is minor compared to the overall RLHF pipeline.

### Extension to direct preference optimization (DPO)

Our adaptive preference scaling approach is generic and can be extended to DPO , which is another popular method for policy learning from human preferences. DPO directly learns the policy in supervised manner using the preference data of state-action pairs \(_{}=(s_{i},a_{w,i},a_{l,i})_{i=1}^{N}\). This approach forgoes the need to learn the reward function explicitly by the reparameterization of reward function \(r\) with respect to its optimal policy \(_{r}\),

\[r(s,a)=(_{r}(a|s)/_{}(a|s))+ Z(s), \]

where \(Z(s)=_{a}_{}(a|s)r(s,a)/\) and \(_{}\) denotes the reference policy. By plugging in (11) back into (2), we have the policy optimization problem

\[_{}_{}()=-_{(s,a_{w},a_{l}) _{}} r_{}(a_{w}|s)- r_{ }(a_{l}|s),\]

where \(r_{}(a|s)=((a|s)/_{}(a|s))\) denotes the log-probability ratio.

Similarly, we can integrate adaptive preference scaling into DPO by plugging in (11) into (6). By merging \(\) with the \(_{i}\) and \(\), we can further obtain the adaptive DPO (Ada-DPO) formulation as

\[_{,_{1},...,_{N}}_{}(, _{1},...,_{N}):=_{i=1}^{N}-_{i} (a_{w,i}|s_{i})-r_{}(a_{l,i}|s_{i})}{_{i}} +_{i}.\]

**Remark 3.3**.: Note that the proposed adaptive preference loss can be further combined with other RLHF approaches, such as PEBBLE , SURF , and PARL , which still optimize the standard cross-entropy loss (see [24, Eq. (4)], [29, Eq. (3)], and [11, Eq. (5)]).

### Extension to quadratic regularization

We now introduce a variant of our adaptive preference loss that uses quadratic regularization for \(\). This modification removes the need for the hyperparameter \(_{}\) in \(\), causing the tuning effort. We define the following instance-level adaptive preference loss with quadratic regularization:

\[_{_{0}}_{}(r,):=- p_{r,}(z_{w } z_{l})+_{0}^{2}- 2. \]

Compared to (4), which includes a linear regularization term of \((_{0}- 2)\), (12) modifies the regularization term with coefficient \(_{0}\) to be quadratic while keeping the term \( 2\) linear. Additionally, in (12), the constraint on \(\) only specifies a lower bound \(_{0}\) and no longer includes an upper bound \(_{}\). The following proposition provides theoretical insights for this modification.

**Proposition 3.2**.: _Assume we have a pair of trajectories \(z_{1},z_{2}\), and the preference distribution \(p(z_{1} z_{2})=p^{*}(0,1)\). Consider the problem of minimizing the expectation of our adaptive loss function with quadratic regularization over the preference distribution:_

\[_{r,r_{0}}- p^{*}(r(z_{1})-r(z_{2}))/ -(1-p^{*})(r(z_{2})-r(z_{1}))/ +_{0}^{2}- 2.\]

_Then the minimizer \(^{*}\) and \(r^{*}\) of the expected loss satisfy_

\[^{*}=\{_{0},(p^{*}(p^{*})+(1-p^{*})(1-p^{*})+  2)/(2_{0})\},\] \[r^{*}(z_{1})-r^{*}(z_{2})=^{*}^{-1}(p^{*}).\]

_Here, \(^{-1}\) is the inverse of sigmoid function._

Note that unlike the adaptive preference loss with linear regularization described in Proposition 3.1, the optimal value \(^{*}\) for quadratic regularization does not involve the upper bound \(_{}\).

## 4 Experiments

In this section, we examine the effectiveness of our adaptive preference loss based on robotic control and natural language generation tasks. Due to space limit, we defer the experiments with quadratic regularization, ablation studies, and discussions on hyperparameter selection to Appendix C.

### Robotic control

**Experiment setup.** We apply our proposed reward learning method on 3 robotic control tasks from the PyBullet  environments: _HalfCheetah_, _Ant_, and _Hopper_. These environments are similar to those available in OpenAI Gym  but they are known to be much harder to solve . Similarly to Gao et al. , our setting is synthetic, where we use the ground truth rewards to provide preference labels on each pair of samples due to high expense of collecting human preferences. For the reward function, we use two-hidden-layer MLPs, each containing 64 hidden units. This configuration is aligned with the designs of both the policy and value networks. Following Christiano et al. , we repeat the following three steps for each stage: (i) We sample a set of trajectories by the policy \(\), and update the policy with proximal policy optimization (PPO, Schulman et al. ) alongside a reward function \(\). (ii) We split the segments (the sequence of state-action pairs) into a training set and a testing set. Then, we randomly sample pairs of segments from the training set, and generate \(_{}\) with preference labels. We do the same to the testing set, and generate \(^{}_{}\). (iii) We train the reward function \(\) on \(_{}\), and use \(^{}_{}\) for evaluating the preference prediction of \(\).

For notational simplicity, we name our proposed adaptive preference scaling method for reward learning as "Ada-Pref". We compare Ada-Pref with the baseline method "Pref", which uses the standard cross-entropy loss for reward learning. For every 10000 timesteps the policy \(\) runs, we evaluate the learned policy based on 20 test episodes. We also compute the average preference prediction accuracy of the learned reward function across stages. We set the budget to 3 million timesteps and perform training over 10 different seeds. For hyperparameter tuning in both reward learning and policy optimization, we apply two different criteria: 1) We identify the best policy based on its performance (the one with the highest return) and subsequently select the corresponding reward function. 2) We choose the best reward function based on its performance (the one with the highest average preference prediction accuracy) and then select the corresponding policy. Details of the implementations and hyperparameter tuning procedures are in Appendix B.1.

**Results.** We summarize the results on three PyBullet tasks as follows:

Table 1 and Figure 2 illustrate the results for Pref and Ada-Pref on the PyBullet tasks, based on the first hyperparameter tuning criterion. In Table 1, we report the highest return of the best policy and the average preference accuracy of the corresponding reward function. We can see that Ada-Pref consistently outperforms Pref in terms of return on all three tasks and achieves comparable preference accuracy. The upper panel of Figure 2 shows the learning curve plots. We can see that Ada-Pref surpasses Pref at nearly every timestep and reaches a higher plateau across all tasks. The lower panel of Figure 2 presents percentile plots from different seeds to demonstrate individual run behaviors. As shown, we confirm that Ada-Pref consistently outperforms Pref at every percentile across all tasks.

Table 2 presents the results for Pref and Ada-Pref based on the second hyperparameter tuning criterion. From Table 2, we can see that both methods show a decrease in performance compared to Table 1,while Ada-Pref still outperforms Pref in terms of both preference accuracy and return on all three tasks. Furthermore, Ada-Pref demonstrates greater resistance to performance degradation than Pref, indicating its superior ability to align the learned reward function with policy optimization. This alignment allows for effective policy selection based on preference accuracy without the need to evaluate the policy using ground truth rewards.

### Natural language generation

**Experiment setup.** We apply DPO with our proposed adaptive loss (Ada-DPO) method to two open-ended text generation tasks: _summarization_ and _single-turn dialogue_. We adopt the Llama-2 7B model  as the backbone and conduct instruction tuning on each task to obtain the initial reference models. For summarization, the policy generates summaries given posts collected from Reddit. We use the filtered TL;DR summarization dataset  for instruction tuning, which contains more than 117K Reddit posts, each with a human-written summary. We apply the human preferences collected by Stiennon et al.  for preference optimization, where each transcript contains a pair of responses along with a preference label. For single-turn dialogue, the policy responds to various human queries ranging from simple questions to complex demands. We utilize the Anthropic Helpful and Harmless dialogue preferences dataset  for both instruction tuning and preference optimization. This dataset contains 170k human-AI dialogues, with each dialogue containing two AI responses and a human preference label. We use the preferred responses for instruction tuning and the full set of preferences for optimization. For instruction tuning stage, we fine-tune the entire Llama-2 model. For the alignment stage using Ada-DPO and different baselines, we apply LoRA fine-tuning for computational efficiency concerns, as we need to simultaneously tune multiple hyperparameters.

    &  &  &  \\   & & Pref & 2724.42 & 89.09 \\  & Ada-Pref & **2875.45** & 89.46 \\   & Pref & 2917.81 & 85.57 \\  & Ada-Pref & **3177.11** & 85.48 \\   & Pref & 1324.91 & 92.08 \\  & Ada-Pref & **1692.10** & 91.36 \\   

Table 1: Table for the highest return of the best policy and the average preference prediction accuracy of the corresponding reward function.

    &  &  &  \\   & & & & \\   & Pref & 2620.83 & 89.41 \\  & Ada-Pref & **2865.07** & 90.75 \\   & Pref & 2750.99 & 87.93 \\  & Ada-Pref & **3008.69** & 89.23 \\   & Pref & Pref & 744.66 & 93.18 \\  & Ada-Pref & **1134.73** & 93.26 \\   

Table 2: Table for the average preference prediction accuracy of the best reward function and the highest return of the corresponding policy.

Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of \(=0.1\). For the percentile plots, returns from 10 different seeds are sorted in ascending order.

The rank of the LoRA adaptor is \(64\). We consider three baseline methods: DPO , \(\) Preference Optimization with Identity Mapping (IPO)  and Sequence Likelihood Calibration with Human Feedback (SLiC-HF) .

As human evaluation is prohibitively expensive, we use Claude 3 , a proprietary large language model, to automatically evaluate responses based on summary quality and helpfulness/harmlessness for the summarization and dialogue tasks, respectively. Prior work has shown that Claude 3 and GPT-4 can effectively measure a quantitative improvement over the instruction-tuned model . We split a small subset from each instruction tuning dataset for testing and calculate the win rate against the instruction-tuned reference model as the evaluation metric. The percentage of instances where the response generated by policy A is preferred over policy B is referred to as the win rate of A against B. We also split a subset from each preference optimization dataset to validate the preference prediction accuracy. Details of the implementations and hyperparameter selections are in Appendix B.2.

**Results.** We summarize the results on the two natural language generation tasks as follows:

In Figure 4, we select the model with the highest win rate and present the win rate and its preference accuracy for all baselines. We observe that Ada-DPO outperforms the other baselines on both tasks in terms of win rate and achieves comparable preference accuracy. In Figure 4, we display the performance of the model selected with the highest accuracy (not win rate). As shown, Ada-DPO achieves a significant improvement beyond the DPO baseline in terms of win rate and obtains a comparable preference accuracy. This again indicates that Ada-DPO yields better alignment between the learned reward function and policy optimization, allowing good policy selection based on preference accuracy without a proprietary LLM judge.

### Detailed analysis

We present detailed analyses of Ada-Pref and Ada-DPO for both the Ant and summarization tasks. Figure 4(a) presents a histogram of the learned scaling factors \(\) for the Ant task. We can see that around 60% of these scaling factors reach the upper bound, while about 10% converge to the lower bound, and the rest are distributed across the region. In Figure 4(b), we explore the relationship between preference strength and the learned scaling factors \(\), and in Figure 4(c), we investigate the relationship between preference strength and the learned reward difference for Pref and Ada-Pref. We measure preference strength using the true reward difference, categorize it into five percentile bins, and then bin the scaling factors and the learned reward differences accordingly to compute the average. As can be seen, the learned scaling factor increases monotonically with preference strength, demonstrating that the our method successfully adapts the loss scaling to the varying degrees of preference in the data. Furthermore, Ada-Pref learns smaller reward differences for pairs with ambiguous preferences and learns larger reward differences for those with strong preferences, compared to Pref. This indicates that our method leads to a more flexible reward function.

In Figure 5(a), we plot a histogram of the learned scaling factors \(\) for the summarization task. We can see that around 40% of the scaling factors converge to the upper bound, with the rest distributed across the region. We also display the relationship between the confidence scores and the scaling factors in Figure 5(b). The confidence score is an integer from 1 to 4 included in the dataset, and a higher score denotes a stronger preference. We bin the scaling factors based on confidence scores and compute the average. As shown, the scaling factors positively correlate with confidence scores, justifying that we learn larger \(\) for strong preferences and smaller \(\) for ambiguous ones.

We further present two pairs of preference samples where Ada-DPO assigns large or small scaling factors in Figure 7. We observe that the sample pair with a large scaling factor shows a strong preference, as the rejected response is nonsensical while the chosen one is clear. Ada-DPO learns a larger reward difference for such data, while it is much smaller with DPO. Conversely, for the sample pair with a small scaling factor, the two responses are very similar, indicating its ambiguity. Ada-DPO learns a small reward difference on this pair, while DPO gets a large reward difference.

## 5 Conclusion

RLHF is an emerging challenge in machine learning. Prior to the popularity of models like ChatGPT, research on designing proper loss functions for reward learning was limited. To bridge this gap, we explore uncertainties in underlying preference strengths and propose an adaptive preference loss function. This loss function incorporates instance-specific scaling factors to modulate the correspondence between reward differences and preference distributions. Taking the result in this paper as an initial start, we expect more sophisticated and stronger follow-up work that applies to RLHF with similar structures. All of these efforts may ultimately assist in developing more principled RLHF methods to better control risks associated with advanced AI systems.

Figure 5: Histogram of learned scaling factors, relationship between preference strength and the learned scaling factors, and relationship between preference strength and the learned reward difference. All plots are from the Ant task.

Figure 6: Histogram of learned scaling factors and relationship between the confidence scores and the learned scaling factors. Both plots are from the summarization task.