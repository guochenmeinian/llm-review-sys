# Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff

**Arthur Jacot**

Courant Institute of Mathematical Sciences

New York University

New York, NY 10012

arthur.jacot@nyu.edu

###### Abstract

Previous work  has shown that DNNs with large depth \(L\) and \(L_{2}\)-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank \(R^{(0)}(f)\) of the learned function \(f\), conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure \(R^{(1)}\) of regularity which bounds the pseudo-determinant of the Jacobian \(|Jf(x)|_{+}\) and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the 'right' inner dimension. Finally, we prove the conjectured bottleneck structure in the learned features as \(L\): for large depths, almost all hidden representations are approximately \(R^{(0)}(f)\)-dimensional, and almost all weight matrices \(W_{}\) have \(R^{(0)}(f)\) singular values close to 1 while the others are \(O(L^{-})\). Interestingly, the use of large learning rates is required to guarantee an order \(O(L)\) NTK which in turns guarantees infinite depth convergence of the representations of almost all layers.

## 1 Introduction

The representation cost \(R(f;L)=_{:f_{g}=f}\|\|^{2}\) can be defined for any model and describes the bias in function space resulting from the minimization of the \(L_{2}\)-norm of the parameters. While it can be computed explicitly for linear networks  or shallow nonlinear ones , the deep non-linear case remains ill-understood .

Previous work  has shown that the representation cost of DNNs with homogeneous nonlinearity \(\) converges to a notion of rank over nonlinear functions\(_{L} R^{(0)}(f)\). Over a large set of functions \(f\), the limiting representation cost \(R^{(0)}(f)\) was proven the so-called Bottleneck (BN) rank \(_{BN}(f)\) which is the smallest integer \(k\) such that \(f:^{d_{in}}^{d_{out}}\) can be factored \(f=^{d_{in}}}{{}}^{k} }{{}}^{d_{out}}\) with inner dimension \(k\) (it is conjectured to match it everywhere). This suggests that large depth \(L_{2}\)-regularized DNNs are adapted for learning functions of the form \(f^{*}=g h\) with small inner dimension.

This can also be interpreted as DNNs learning symmetries, since a function \(f:^{d_{out}}\) with symmetry group \(G\) (i.e. \(f(g x)=f(x)\)) can be defined as mapping the inputs \(\) to an embedding of the modulo space \(}{{G}}\) and then to the outputs \(^{d_{out}}\). Thus a function with a lot of symmetries will have a small BN-rank, since \(_{BN}(f)(}{{G}})\) where \((}{{G}})\) is the smallest dimension \(}{{G}}\) can be embedded into.

A problem is that this notion of rank does not control the regularity of \(f\), but results of  suggest that a measure of regularity might be recovered by studying finite depths corrections to the \(R^{(0)}\) approximation. This formalizes the balance between minimizing the dimension of the learned features and their complexity.

Another problem is that minimizing the rank \(R^{(0)}\) does not uniquely describe the learned function, as there are many fitting functions with the same rank. Corrections allow us to identify the learned function amongst those.

Finally, the theoretical results and numerical experiments of  strongly suggest a bottleneck structure in the learned features for large depths, where the (possibly) high dimensional input data is mapped after a few layers to a low-dimensional hidden representation, and keeps the approximately same dimensionality until mapping back to the high dimensional outputs in the last few layers. We prove the existence of such a structure, but with potentially multiple bottlenecks.

### Contributions

We analyze the Taylor approximation of the representation cost around infinite depth \(L=\):

\[R(f;,L)=LR^{(0)}(f;)+R^{(1)}(f;)+R^{(2)}(f; )+O(L^{-2}).\]

The first correction \(R^{(1)}\) measures some notion of regularity of the function \(f\) that behaves sub-additively under composition \(R^{(1)}(f g) R^{(1)}(f)+R^{(1)}(g)\) and under addition \(R^{(1)}(f+g) R^{(1)}(f)+R^{(1)}(g)\) (under some constraints), and controls the Jacobian of \(f\): \( x,2|Jf(x)|_{+} R^{(1)}(f)\), where \(||_{+}\) is the _pseudo-determinant_, the product of the non-zero singular values.

This formalizes the balance between the bias towards minimizing the inner dimension described by \(R^{(0)}\) and a regularity measure \(R^{(1)}\). As the depth \(L\) grows, the low-rank bias \(R^{(0)}\) dominates, but even in the infinite depth limit the regularity \(R^{(1)}\) remains relevant since there are typically multiple fitting functions with matching \(R^{(0)}\) which can be differentiated by their \(R^{(1)}\) value.

For linear networks, the second correction \(R^{(2)}\) guarantees infinite depth convergence of the representations of the network. We recover several properties of \(R^{(2)}\) in the nonlinear case, but we also give a counterexample that shows that norm minimization does not guarantee convergence of the representation, forcing us to look at other sources of bias.

To solve this issue, we show that a \((L^{-1})\) learning rate forces the NTK to be \(O(L)\) which in turn guarantees the convergence as \(L\) of the representations at almost every layer of the network.

Finally we prove the Bottleneck structure that was only observed empricially in : we show that the weight matrices \(W_{}\) are approximately rank \(R^{(0)}(f)\), more precisely \(W_{}\) has \(R^{(0)}(f)\) singular values that are \(O(L^{-})\) close to \(1\) and all the other are \(O(L^{-})\). Together with the \(O(L)\) NTK assumption, this implies that the pre-activations \(_{}(X)\) of a general dataset at almost all layer is approximately \(R^{(0)}(f)\)-dimensional, more precisely that the \(k+1\)-th singular value of \(_{}(X)\) is \(O(L^{-})\).

### Related Works

The representation cost has mostly been studied in settings where an explicit formula can be obtained, such as in linear networks , or shallow nonlinear networks , or for deep networks with very specific structure . A low rank phenomenon in large depth \(L_{2}\)-regularized DNNs has been observed in .

Regarding deep fully-connected networks, two reformulations of the representation cost optimization have been given in , which also shows that the representation becomes independent of the width as long as the width is large enough.

The Bottleneck structure that we describe in this paper is similar to the Information Bottleneck theory . It is not unlikely that the bias towards dimension reduction in the middle layers of the network could explain the loss of information that was observed in the first layers of the network in .

Setup

In this paper, we study fully connected DNNs with \(L+1\) layers numbered from \(0\) (input layer) to \(L\) (output layer). The layer \(\{0,,L\}\) has \(n_{}\) neurons, with \(n_{0}=d_{in}\) the input dimension and \(n_{L}=d_{out}\) the output dimension. The pre-activations \(_{}(x)^{n_{}}\) and activations \(_{}(x)^{n_{}}\) are defined by

\[_{0}(x) =x\] \[_{}(x) =W_{}_{-1}(x)+b_{}\] \[_{}(x) =(_{}(x)),\]

for the \(n_{} n_{-1}\) connection weight matrix \(W_{}\), the \(n_{}\)-dim bias vector \(b_{}\) and the nonlinearity \(:\) applied entry-wise to the vector \(_{}(x)\). The parameters of the network are the collection of all connection weights matrices and bias vectors \(=(W_{1},b_{1},,W_{L},b_{L})\). The network function \(f_{}:^{d_{in}}^{d_{out}}\) is the function that maps an input \(x\) to the pre-activations of the last layer \(_{L}(x)\).

We assume that the nonlinearity is of the form \(_{a}(x)=x&x 0\\ ax&\) for some \((-1,1)\) (yielding the ReLU for \(=0\)), as any homogeneous nonlinearity \(\) (that is not proportional to the identity function, the constant zero function or the absolute function) matches \(_{a}\) up to scaling and inverting the inputs.

The functions that can be represented by networks with homogeneous nonlinearities and any finite depth/width are exactly the set of finite piecewise linear functions (FPLF) .

_Remark 1_.: In most of our results, we assume that the width is sufficiently large so that the representation cost matches the infinite-width representation cost. For a dataset of size \(N\), a width of \(N(N+1)\) suffices, as shown in  (though a much smaller width is often sufficient).

### Representation Cost

The representation cost \(R(f;,L)\) is the minimal squared parameter norm required to represent the function \(f\) over the input domain \(\):

\[R(f;,L)=_{:f_{|}=f_{|}}\|\| ^{2}\]

where the minimum is taken over all weights \(\) of a depth \(L\) network (with some finite widths \(n_{1},,n_{L-1}\)) such that \(f_{}(x)=f(x)\) for all \(x\). If no such weights exist, we define \(R(f;,L)=\).

The representation cost describes the natural bias on the represented function \(f_{}\) induced by adding \(L_{2}\) regularization on the weights \(\):

\[_{}C(f_{})+\|\|^{2}=_{f}C(f)+  R(f;,L)\]

for any cost \(C\) (defined on functions \(f:^{d_{out}}\)) and where the minimization on the right is over all functions \(f\) that can be represented by a depth \(L\) network with nonlinearity \(\).

For any two functions \(f,g\), we denote \(f g\) the function \(h\) such that \(g=h f\), assuming it exists, and we write \(R(f g;,L)=R(h;f(),L)\).

_Remark 2_.: The representation cost also describes the implicit bias of networks trained with the cross-entropy loss .

## 3 Representation Cost Decomposition

Since there are no explicit formula for the representation cost of deep nonlinear networks, we propose to approximate it by a Taylor decomposition in \(}{{L}}\) around \(L=\). This is inspired by the behavior of the representation cost of deep linear networks (which represent a matrix as a product \(A_{}=W_{L} W_{1}\)), for which an explicit formula exists :

\[R(A;L)=_{:A=A_{}}\|\|^{2}=L\|A\|^{ }{{L}}}_{}{{L}}}=L_{i=1}^{A }s_{i}(A)^{}{{L}}},\]where \(\|\|_{p}^{p}\) is the \(L_{p}\)-Schatten norm, the \(L_{p}\) norm on the singular values \(s_{i}(A)\) of \(A\).

Approximating \(s^{}=1+ s+}( s)^{2}+O(L^{-3})\), we obtain

\[R(A;L)=L{ Rank}A+2|A|_{+}+\|_{+}A^{T}A \|^{2}+O(L^{-2}),\]

where \(_{+}\) is the pseudo-log, which replaces the non-zero eigenvalues of \(A\) by their log.

We know that gradient descent will converge to parameters \(\) representing a matrix \(A_{}\) that locally minimize the loss \(C(A)+ R(A;L)\). The approximation \(R(A;L) L{ Rank}A\) fails to recover the local minima of this loss, because the rank has zero derivatives almost everywhere. But this problem is alleviated with the second order approximation \(R(A;L) L{ Rank}A+2|A|_{+}\). The minima can then be interpreted as first minimizing the rank, and then choosing amongst same rank solutions the matrix with the smallest pseudo-determinant. Changing the depth allows one to tune the balance between minimizing the rank and the regularity of the matrix \(A\).

### First Correction: Regularity Control

As a reminder, the dominating term in the representation cost \(R^{(0)}(f)\) is conjectured in  to converge to the so-called Bottleneck rank \({ Rank}_{BN}(f;)\) which is the smallest integer \(k\) such that \(f\) can be decomposed as \(f=}{{}}^{k} }{{}}^{d_{out}}\) with inner dimension \(k\), and where \(g\) and \(h\) are FPLF. A number of results supporting this conjecture are proven in : a sandwich bound

\[{ Rank}_{J}(f;) R^{(0)}(f;)_{BN}(f;)\]

for the Jacobian rank \({ Rank}_{J}(f;)=_{x}{ Rank}Jf(x)_{|T_{x}}\), and three natural properties of ranks that \(R^{(0)}\) satisfies:

1. \(R^{(0)}(f g;)R^{(0)}(f),R^{(0)}(g)}\),
2. \(R^{(0)}(f+g;) R^{(0)}(f)+R^{(0)}(g)\),
3. \(R^{(0)}(x Ax;)={ Rank}A\) for any full dimensional and bounded \(\).

These results imply that for any function \(f= A\) that is linear up to bijections \(,\), the conjecture is true \(R^{(0)}(f;)={ Rank}_{BN}(f;)={ Rank}A\).

The proof of the aforementioned sandwich bound in  actually prove an upper bound of the form \(L{ Rank}_{BN}(f;)+O(1)\) thus proving that the \(R^{(1)}\) term is upper bounded. The following theorem proves a lower bound on \(R^{(1)}\) as well as some of its properties:

**Theorem 3**.: _For all inputs \(x\) where \({ Rank}Jf(x)=R^{(0)}(x)\), \(R^{(1)}(f) 2|Jf(x)|_{+}\), furthermore:_

1. _If_ \(R^{(0)}(f g)=R^{(0)}(f)=R^{(0)}(g)\)_, then_ \(R^{(1)}(f g) R^{(1)}(f)+R^{(1)}(g)\)_._
2. _If_ \(R^{(0)}(f+g)=R^{(0)}(f)+R^{(0)}(g)\)_, then_ \(R^{(1)}(f+g) R^{(1)}(f)+R^{(1)}(g)\)_._
3. _If_ \(P_{ Im}A^{T}\) _and_ \(A\) _are_ \(k={ Rank}A\) _dimensional and completely positive (i.e. they can be embedded isometrically into_ \(_{+}^{m}\) _for some_ \(m\)_), then_ \(R^{(1)}(x Ax;)=2|A|_{+}.\)__

Notice how these properties clearly point to the first correction \(R^{(1)}(f)\) measuring a notion of regularity of \(f\) instead of a notion of rank. One can think of \(L_{2}\)-regularized deep nets as learning functions \(f\) that minimize

\[_{f}C(f(X))+ LR^{(0)}(f)+ R^{(1)}(f).\]

The depth determines the balance between the rank regularization and regularity regularization. Without the \(R^{(1)}\)-term, the above minimization would never be unique since there can be multiple functions \(f\) with the same training outputs \(f(X)\) with the same rank \(R^{(0)}(f)\).

Under the assumption that \(R^{(0)}\) only takes integer value the above optimization can be rewritten as

\[_{k=0,1,} Lk+_{f:R^{(0)}(f)=k}C(f(X))+ R^{(1)}(f).\]Every inner minimization for a rank \(k\) that is attained inside the set \(\{f:R^{(0)}(f)=k\}\) corresponds to a different local minimum. Note how these inner minimization do not depend on the depth, suggesting the existence of sequences of local minima for different depths that all represent approximately the same function \(f\), as can be seen in Figure 1. We can classify these minima according to whether they recover the true BN-rank \(k^{*}\) of the task, underestimate or overestimate it.

In linear networks , rank underestimating minima cannot fit the data, but it is always possible to fit any data with a BN-rank 1 function (by mapping injectively the datapoints to a line and then mapping them nonlinearly to any other configuration). We therefore need to also differentiate between rank-underestimating minima that fit or do not fit the data. The non-fitting minima can in theory be avoided by taking a small enough ridge (along the lines of ), but we do observe them empirically for large depths in Figure 1.

In contrast, we have never observed fitting rank-underestimating minima, though their existence was proven for large enough depths in . A possible explanation for why GD avoids these minima is their \(R^{(1)}\) value explodes with the number of datapoints \(N\), since these network needs to learn a space filling surface (a surface of dimension \(k<k^{*}\) that visits random outputs \(y_{i}\) that are sampled from a \(k^{*}\)-dimensional distribution). More precisely Theorem 2 of  implies that the \(R^{(1)}\) value of fitting BN-rank 1 minima explodes at a rate of \(2(1-}) N\) as the number of datapoints \(N\) grows, which could explain why we rarely observe such minima in practice, but another explanation could be that these minima are very narrow, as explained in Section 4.1.

In our experiments we often encountered rank overestimating minima and we are less sure about how to avoid them, though it seems that increasing the depth helps (see Figure 1), and that SGD might help too by analogy with the linear case . Thankfully overestimating the rank is less problematic for generalization, as supported by the fact that it is possible to approximate BN-rank \(k^{*}\) with a higher rank function with any accuracy, while doing so with a low rank function requires a pathological function.

Figure 1: (a) Plot of the parameter norm at the end of training (\(=0.001\)) over a range of depths, colored according to the rank (# of sing, vals above 0.1) of the weight matrices \(W_{}{{2}}}\) in the middle of the network, and marked with a dot ’.’ or cross ’x’ depending on whether the final train cost is below or above 0.1. The training data is synthetic and designed to have a optimal rank \(k^{*}=2\). We see different ranges of depth where the network converges to different rank, with larger depths leading to smaller rank, until training fails and recover the zero parameters for \(L>25\). Within each range the norm \(\|\|^{2}\) is well approximated by a affine function with slope equal to the rank. (b) Plot of the singular values of \(W_{}\) throughout the networks for 4 trials, we see that the bottleneck structure remains essentially the same throughout each range of depth, with only the middle low-rank part growing with the depth.

### Second Correction

We now identify a few properties of the second correction \(R^{(2)}\):

**Proposition 4**.: _If there is a limiting representation as \(L 0\) in the optimal representation of \(f\), then \(R^{(2)}(f) 0\). Furthermore:_

1. _If_ \(R^{(0)}(f g)=R^{(0)}(f)=R^{(0)}(g)\) _and_ \(R^{(1)}(f g)=R^{(1)}(f)+R^{(1)}(g)\)_, then_ \((f g)}(f)}+(g)}\)_._
2. _If_ \(R^{(0)}(f+g)=R^{(0)}(f)+R^{(0)}(g)\) _and_ \(R^{(1)}(f+g)=R^{(1)}(f)+R^{(1)}(g)\)_, then_ \(R^{(2)}(f+g) R^{(2)}(f)+R^{(2)}(g)\)_._
3. _If_ \(A^{p}\) _is_ \(k=\)_A-dimensional and completely positive for all_ \(p\)_, where_ \(A^{p}\) _has its non-zero singular taken to the_ \(p\)_-th power, then_ \(R^{(2)}(x Ax;)=\|_{+}A^{T}A\|^{2}\)_._

While the properties are very similar to those of \(R^{(1)}\), the necessary conditions necessary to apply them are more restrictive. There might be case where the first two terms \(R^{(0)}\) and \(R^{(1)}\) do not uniquely determine a minimum, in which case the second correction \(R^{(2)}\) needs to be considered.

In linear networks the second correction \(R^{(2)}(A)=\|_{+}A^{T}A\|^{2}\) plays an important role, as it bounds the operator norm of \(A\) (which is not bounded by \(R^{(0)}(A)=\)_A_ nor \(R^{(1)}(A)=2|A|_{+}\)), thus guaranteeing the convergence of the hidden representations in the middle of network. We hoped at first that \(R^{(2)}\) would have similar properties in the nonlinear case, but we were not able to prove anything of the sort. Actually in contrast to the linear setting, the representations of the network can diverge as \(L\), which explains why the \(R^{(2)}\) does not give any similar control, which would guarantee convergence.

### Representation geodesics

One can think of the sequence of hidden representations \(_{1},,_{L}\) as a path from the input representation to the output representation that minimizes the weight norm \(\|W_{}\|^{2}\) required to map from one representation to the next. As the depth \(L\) grows, we expect this sequence to converge to a form of geodesic in representation space. Such an analysis has been done in  for ResNet where these limiting geodesics are continuous.

Two issues appear in the fully-connected case. First a representation \(_{}\) remains optimal after any swapping of its neurons or other symmetries, but this can easily be solved by considering representations \(_{}\) up to orthogonal transformation, i.e. to focus on the kernels \(K_{}(x,y)=_{}(x)^{}_{}(y)\). Second the limiting geodesics of fully-connected networks are not continuous, and as such they cannot be described by a local metric.

We therefore turn to the representation cost of DNNs to describe the hidden representations of the network, since the \(\)-th pre-activation function \(^{()}:^{n_{}}\) in a network which minimizes the parameter norm must satisfy

\[R(f;,L)=R(_{};,)+R((_{ }) f;,L-).\]

Thus the limiting representations \(_{p}=_{L}_{_{L}}\) (for a sequence of layers \(_{L}\) such that \(_{L}}}{{L}}=p(0,1)\)) must satisfy

\[R^{(0)}(f;) =R^{(0)}(_{p};)=R^{(0)}((_{p}) f;)\] \[R^{(1)}(f;) =R^{(1)}(_{p};)+R^{(1)}((_{p}) f;)\]

Let us now assume that the limiting geodesic is continuous at \(p\) (up to orthogonal transformation, which do not affect the representation cost), meaning that any other sequence of layers \(^{}_{L}\) converging to the same ratio \(p(0,1)\) would converge to the same representation. The taking the limits with two sequences \(}{L}=p=_{L}}{L}\) such that \(^{}_{L}-_{L}=+\) and and taking the limit of the equality

\[R(f;,L)=R(_{_{L}};,_{L})+R((_{_{L}})_{^{}_{L}};,^{}_ {L}-_{L})+R((_{^{}_{L}}) f;,L-^ {}_{L}),\]we obtain that \(R^{(0)}((_{p})_{p};)= R^{(0)}(f)\) and \(R^{(1)}((_{p})_{p};)=0\). This implies that \(((x))=(x)\) at any point \(x\) where \(Jf(x)=R^{(0)}(f;)\), thus \(R^{(0)}(id;_{p}())=R^{(0)}(f;)\) and \(R^{(1)}(id;_{p}())=0\) if \(Jf(x)=R^{(0)}(f;)\) for all \(x\).

#### 3.3.1 Identity

When evaluated on the identity, the first two terms \(R^{(0)}(id;)\) and \(R^{(1)}(id;)\) describe properties of the domain \(\).

For any notion of rank, \((id;)\) defines a notion of dimensionality of \(\). The Jacobian rank \(_{J}(id;)=_{x} T_{x}\) is the maximum tangent space dimension, while the Bottleneck rank \(_{BN}(id;)\) is the smallest dimension \(\) can be embedded into. For example, the circle \(=^{2-1}\) has \(_{J}(id;)=1\) and \(_{BN}(id;)=2\).

On a domain \(\) where the two notions of dimensionality match \(_{J}(id;)=_{BN}(id;)=k\), the first correction \(R^{(1)}(id;)\) is non-negative since for any \(x\) with \( T_{x}=k\), we have \(R^{(1)}(id;)|P_{T_{x}}|_{+}=0\). The \(R^{(1)}(id;)\) value measures how non-planar the domain \(\) is, being \(0\) only if \(\) is \(k\)-planar, i.e. its linear span is \(k\)-dimensional:

**Proposition 5**.: _For a domain with \(_{J}(id;)=_{BN}(id;)=k\), then \(R^{(1)}(id;)=0\) if and only if \(\) is \(k\)-planar and completely positive._

This proposition shows that the \(R^{(1)}\) term does not only bound the Jacobian of \(f\) as shown in Theorem 3, but also captures properties of the curvature of the domain/function.

Thus at ratios \(p\) where the representation geodesics converge continuously, the representations \(_{p}()\) are \(k=R^{(0)}(f;)\)-planar, proving the Bottleneck structure that was only observed empirically in . But the assumption of convergence over which we build this argument does not hold in general, actually we give in the appendix an example of a simple function \(f\) whose optimal representations diverges in the infinite depth limit. This is in stark contrast to the linear case, where the second correction \(R^{(2)}(A)=\|_{+}A^{T}A\|^{2}\) guarantees convergence, since it bounds the operator norm of \(A\). To prove and describe the bottleneck structure in nonlinear DNNs, we therefore need to turn to another strategy.

## 4 Bottleneck Structure in Large Depth Networks

Up to now we have focused on one aspect of the Bottleneck structure observed in : that the representations \(_{}(X)\) inside the Bottleneck are approximately \(k\)-planar. But another characteristic of this phenomenon is that the weight matrices \(W_{}\) in the bottleneck have \(k\) dominating singular values, all close to \(1\). This property does not require the convergence of the geodesics and can be proven with finite depth rates:

**Theorem 6**.: _Given parameters \(\) of a depth \(L\) network, with \(\|\|^{2} kL+c_{1}\) and a point \(x\) such that \(Jf_{}(x)=k\), then there are \(w_{} k\) (semi-)orthonormal \(V_{}\) such that \(_{=1}^{L}\|W_{}-V_{}V_{-1}^{T}\|_{F}^{2} c_ {1}-2|Jf_{}(x)|_{+}\) thus for any \(p(0,1)\) there are at least \((1-p)L\) layers \(\) with_

\[\|W_{}-V_{}V_{-1}^{T}\|_{F}^{2}-2 |Jf_{}(x)|_{+}}{pL}.\]

Note how we not only obtain finite depth rates, but our result has the advantage of being applicable to any parameters with a sufficiently small parameter norm (close to the minimal norm solution). The bound is tighter at optimal parameters in which case \(c_{1}=R^{(1)}(f_{})\), but the theorem shows that the Bottleneck structure generalizes to points that are only almost optimal.

To prove that the pre-activations \(_{}(X)\) are approximately \(k\)-dimensional for some dataset \(X\) (that may or may not be the training set) we simply need to show that the activations \(_{-1}(X)\) do not diverge, since \(_{}(X)=W_{}_{-1}(X)+b_{}\) (and one can show that the bias will be small at almost every layer too). By our counterexample we know that we cannot rule out such explosion in general, however if we assume that the NTK \(^{(L)}(x,x)\) is of order \(O(L)\), then we can guarantee to convergence of the activations \(_{-1}(X)\) at almost every layer:

**Theorem 7**.: _Given balanced parameters \(\) of a depth \(L\) network, with \(\|\|^{2} kL+c_{1}\) and a point \(x\) such that \(Jf_{}(x)=k\) then if \([^{(L)}(x,x)] cL\), then \(_{=1}^{L}\|_{-1}(x)\|_{2}^{2}}{k}}\}}{k|Jf_{}(x)|_{+}^{2/k}}L\) and thus for all \(p(0,1)\) there are at least \((1-p)L\) layers such that_

\[\|_{-1}(x)\|_{2}^{2}}{k}}\}}{k|Jf_{}(x)|_{+}^{2/k}}.\]

Note that the balancedness assumption is not strictly necessary and could easily be weakened to some form of approximate balancedness, since we only require the fact that the parameter norm \(\|W_{}\|_{F}^{2}\) is well spread out throughout the layers, which follows from balancedness.

The NTK describes the narrowness of the minima , and the assumption of bounded NTK is thus related to stability under large learning rates. There are multiple notions of narrowness that have been considered:

* The operator norm of the Hessian \(H\) (which is closely related to the top eigenvalue of the NTK Gram matrix \(^{(L)}(X,X)\) especially in the MSE case where at any interpolating function \(\|H\|_{op}=\|^{(L)}(X,X)\|_{op}\)) which needs to be bounded by \(}{{}}\) to have convergence when training with gradient descent with learning rate \(\).
* The trace of the Hessian (in the MSE case \(H=^{(L)}(X,X)\)) which has been shown to describe the bias of stochastic gradient descent or approximation thereof .

Thus boundedness of almost all activations as \(L\) can be guaranteed by assuming either \(\|^{(L)}(X,X)\|_{op} cL\) (which implies \(d_{out}^{(L)}(X,X) cL\)) or \(^{(L)}(X,X) cL\) directly, corresponding to either gradient descent with \(=}{{cL}}\) or stochastic gradient descent with a similar scaling of \(\)).

Note that one can find parameters that learn a function with a NTK that scales linearly in depth, but it is not possible to represent non-trivial functions with a smaller NTK \(^{(L)} L\). This is why we consider a linear scaling in depth to be the 'normal' size of the NTK, and anything larger to be narrow.

Putting the two Theorems together, we can prove the Bottleneck structure for almost all representations \(_{}(X)\):

**Corollary 8**.: _Given balanced parameters \(\) of a depth \(L\) network with \(\|\|^{2} kL+c_{1}\) and a set of points \(x_{1},,x_{N}\) such that \(Jf_{}(x_{i})=k\) and \([^{(L)}(X,X)] cL\), then for all \(p(0,1)\) there are at least \((1-p)L\) layers such that_

\[s_{k+1}(}_{}(X))- 2|Jf_{}(x)|_{+}}(_{i=1}^{N} }{k}}\}}{k|Jf_{}(x_{i})|_{+}^{2 /k}}+})}.\]

### Narrowness of rank-underestimating minima

We know that the large \(R^{(1)}\) value of BN-rank 1 fitting functions is related to the explosion of its derivative, but a large Jacobian also leads to a blow up of the NTK:

**Proposition 9**.: _For any point \(x\), we have_

\[\|_{xy}^{2}(x,x)\|_{op} 2L\|Jf_{}(x) \|_{op}^{2-}{{L}}}\]

_where \(_{xy}^{2}(x,x)\) is understood as a \(d_{in}d_{out} d_{in}d_{out}\) matrix._

_Furthermore, for any two points \(x,y\) such that the pre-activations of all neurons of the network remain constant on the segment \([x,y]\), then either \(\|(x,x)\|_{op}\) or \(\|(y,y)\|_{op}\) is lower bounded by \(\|x-y\|^{2}\|Jf_{}(x)\|_{2}^{2-}{{L}}}.\)_With some additional work, we can show the the NTK of such rank-underestimating functions will blow up, suggesting a narrow minimum:

**Theorem 10**.: _Let \(f^{*}:^{d_{out}}\) be a function with Jacobian rank \(k^{*}>1\) (i.e. there is a \(x\) with \(\!Jf^{*}(x)=k^{*}\)), then with high probability over the sampling of a training set \(x_{1},,x_{N}\) (sampled from a distribution with support \(\)), we have that for any parameters \(\) of a deep enough network that represent a BN-rank 1 function \(f_{}\) that fits the training set \(f_{}(x_{i})=^{*}(x_{i})\) with norm \(^{2}=L+c_{1}\) then there is a point \(x\) where the NTK satisfies_

\[[^{(L)}(x,x)] c^{}Le^{-c_{1 }}N^{4-}}.\]

One the one hand, we know that \(c_{1}\) must satisfy \(c_{1} R^{(1)}(f_{}) 2(1-}) N\) but if \(c_{1}\) is within a factor of 2 of this lower bound \(c_{1}<4(1-}) N\), then the above shows that the NTK will blow up a rate \(N^{}L\) for a positive \(\).

The previous explanation for why GD avoids BN-rank 1 fitting functions was that when \(N\) is much larger than the depth \(L\) (exponentially larger), there is a rank-recovering function with a lower parameter norm than any rank-underestimating functions. But this relies on the assumption that GD converges to the lower norm minima, and it is only true for sufficiently small depths. In contrast the narrowness argument applies for any large enough depth and does not assume global convergence.

Of course the complete explanation might be a mix of these two reasons and possbily some other phenomenon too. Proving why GD avoids minima that underestimate the rank with a rank \(1<k<k^{*}\) also remains an open question.

## 5 Numerical Experiment: Symmetry Learning

In general, functions with a lot of symmetries have low BN-rank since a function \(f\) with symmetry group \(G\) can be decomposed as mapping the inputs \(\) to the inputs module symmetries \(}{{G}}\) and then mapping it to the outputs, thus \(_{BN}(f;)}{{G}}\) where \(}{{G}}\) is the smallest dimension \(}{{G}}\) can be embedded into. Thus the bias of DNNs to learn function with a low BN-rank can be interpreted as the network learning symmetries of the task. With this interpretation, overestimating the rank corresponds to failing to learn all symmetries of the task, while underestimating the rank can be interpreted as the network learning spurious symmetries that are not actual symmetries of the task.

Figure 2: A depth \(L=25\) network with a width of \(200\) trained on the task described in Section 5 with a ridge \(=0.0002\). (a) Singular values of the weight matrices of the network, showing two outliers in the bottleneck, which implies that the network has recovered the true rank of 2. (b) Hidden representation of the \(6\)-th layer projected to the first two dimensions, we see how images of GD paths do not cross in this space, showing that the dynamics on these two dimensions are self-consistent. (c) The distance \(_{2}(x_{0})-_{2}(x)\) in the second hidden layer between the representations at a fixed point \(x_{0}\) (at the white pixel) and another point \(x\) on a plane orthogonal to the axis \(w\) of rotation, we see that all points on the same symmetry orbit are collapsed together, proving that the network has learned the rotation symmetry.

To test this idea, we train a network to predict high dimensional dynamics with high dimensional symmetries. Consider the loss \(C(v)=\|vv^{T}-(ww^{T}+E)\|_{F}^{2}\) where \(w^{d}\) is a fixed unit vector and \(E\) is a small noise \(d d\) matrix. We optimize \(v\) with gradient descent to try and fit the true vector \(w\) (up to a sign). One can think of these dynamics as learning a shallow linear network \(vv^{T}\) with a single hidden neuron. We will train a network to predict the evolution of the cost in time \(C(v(t))\).

For small noise matrix \(E\), the GD dynamics of \(v(t)\) are invariant under rotation around the vector \(w\). As a result, the high-dimensional dynamics of \(v(t)\) can captured by only two _summary statistics_\(u(v)=((w^{T}v)^{2},\|(I-ww^{T})v\|^{2})\): the first measures the position along the axis formed by \(w\) and the second the distance to this axis . The evolution of the summary statistics is (approximately) self-consistent (using the fact that \(\|v\|^{2}=(w^{T}v)^{2}+\|(I-ww^{T})v\|^{2}\)):

\[_{t}(w^{T}v)^{2} =-8(\|v\|^{2}-1)(w^{T}v)^{2}+O(\|E\|)\] \[_{t}\|(I-ww^{T})v\|^{2} =-8\|v\|^{2}\|(I-ww^{T})v\|^{2}+O(\|E \|).\]

Our goal now is to see whether a DNN can learn these summary statistics, or equivalently learn the underlying rotation symmetry. To test this, we train a network on the following supervised learning problem: given the vector \(v(0)\) at initialization, predict the loss \((C(v(1)),,C(v(T)))\) over the next \(T\) GD steps. For \(E=0\), the function \(f^{}:^{d}^{T}\) that is to be learned has BN-rank 2, since one can first map \(v(0)\) to the corresponding summary statistics \(u(v(0))^{2}\), and then solve the differential equation on the summary statistics \((u(1),,u(T))\) over the next \(T\) steps, and compute the cost \(C(v)=\|v\|^{4}-2(w^{T}v)^{2}+1+O(\|E\|)\) from \(u\).

We observe in Figure 2 that a large depth \(L_{2}\)-regularized network trained on this task learns the rotation symmetry of the task and learns two dimensional hidden representations that are summary statistics (summary statistics are only defined up to bijections, so the learned representation match \(u(v)\) only up to bijection but they can be recognized from the fact that the GF paths do not cross on the 2D representation).

## 6 Conclusion

We have computed corrections to the infinite depth description of the representation cost of DNNs given in , revealing two regularity \(R^{(1)},R^{(2)}\) measures that balance against the dominating low rank/dimension bias \(R^{(0)}\). We have also partially described another regularity inducing bias that results from large learning rates. We argued that these regularity bias play a role in stopping the network from underestimating the 'true' BN-rank of the task (or equivalently overfitting symmetries).

We have also proven the existence of a bottleneck structure in the weight matrices and under the condition of a bounded NTK of the learned representations, where most hidden representations are approximately \(k=R^{(0)}(f_{})\)-dimensional, with only a few high-dimensional representations.