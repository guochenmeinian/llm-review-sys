# Adam on Local Time: Addressing Nonstationarity

in RL with Relative Adam Timesteps

Benjamin Ellis

University of Oxford

&Matthew T. Jackson1

University of Oxford

&Andrei Lupu

University of Oxford

&Alexander D. Goldie

University of Oxford

&Mattie Fellows

University of Oxford

&Shimon Whiteson

University of Oxford

&Jakob N. Foerster

University of Oxford

Equal Contribution

###### Abstract

In reinforcement learning (RL), it is common to apply techniques used broadly in machine learning such as neural network function approximators and momentum-based optimizers . However, such tools were largely developed for supervised learning rather than nonstationary RL, leading practitioners to adopt target networks , clipped policy updates , and other RL-specific implementation tricks  to combat this mismatch, rather than directly adapting this toolchain for use in RL. In this paper, we take a different approach and instead address the effect of nonstationarity by adapting the widely used Adam optimiser . We first analyse the impact of nonstationary gradient magnitude--such as that caused by a change in target network--on Adam's update size, demonstrating that such a change can lead to large updates and hence sub-optimal performance. To address this, we introduce _Adam with Relative Timesteps_, or Adam-Rel. Rather than using the global timestep in the Adam update, Adam-Rel uses the local timestep within an epoch, essentially resetting Adam's timestep to 0 after target changes. We demonstrate that this avoids large updates and reduces to learning rate annealing in the absence of such increases in gradient magnitude. Evaluating Adam-Rel in both on-policy and off-policy RL, we demonstrate improved performance in both Atari and Craftax. We then show that increases in gradient norm occur in RL in practice, and examine the differences between our theoretical model and the observed data.

## 1 Introduction

Reinforcement Learning (RL) aims to learn robust policies from an agent's experience. This has the potential for large scale real-world impact in areas such as autonomous driving or improving logistic chains. Over the last decade, a number of breakthroughs in supervised learning--such as convolutional neural networks and the Adam optimizer--have expanded the deep learning toolchain and been transferred to RL, enabling it to begin fulfilling this potential.

However, since RL agents are continuously learning from new data they collect under their changing policy, the optimisation objective is fundamentally _nonstationary_. Furthermore, temporal difference (TD) approaches bootstrap the agent's update from its own value predictions, exacerbating the nonstationarity in the objective function. This is in stark contrast to the _stationary_ supervised learning setting for which the deep learning toolchain was originally developed. Therefore, to apply these tools successfully, researchers have developed a variety of implementation tricks _on top of_ this base to stabilise training . This has resulted in a proliferation of little-documented design choices that are vital for performance, contributing to the reproducibility crisis in RL .

We believe that in the long term, a more robust approach is to _augment_ this toolchain for RL, rather than building on top of it. To this end, in this paper we examine the interaction between nonstationarity and the Adam optimizer . Adam's update rule, where equations are applied element-wise (i.e. per parameter), is defined by

\[m_{t} =_{1}m_{t-1}+(1-_{1})g_{t}, _{t} =}{(1-_{1}{}^{t})},\] \[v_{t} =_{2}v_{t-1}+(1-_{2})g_{t}{}^{2}, _{t} =}{(1-_{2}{}^{t})},\] \[u_{t} =_{t}}{_{t}+}}, _{t} =_{t-1}- u_{t}.\]

Here, \(g_{t}\) is the gradient, \(_{t}\) a parameter to be optimized, and \(\) the learning rate. The resulting update is the ratio of two different momentum terms: one for the first moment, \(m_{t}\), and one for second moment, \(v_{t}\), of the gradient. These terms use different exponential decay coefficients, \(_{1}\) and \(_{2}\). Under stationary gradients, the \((1-_{i})\) weighting ensures that, in the limit, the overall magnitude of the two momenta is independent of the value chosen for each of the coefficients. However, since both momentum estimates are initialised to 0, they must be renormalised for a given (finite timestep \(t\), to account for the "missing parts" of the geometric series , resulting in \(_{t}\) and \(_{t}\).

Crucially, \(t\) counts the update steps since the _beginning of training_ and thus bakes in the assumption of stationarity that is common in supervised learning. In particular, this renormalisation breaks down if the loss is nonstationary. Consider a task change late in training, which results in gradients orders of magnitudes higher than those of the prior (near convergence) task. Clearly, this is analogous to the situation at the _beginning of training_ where all momentum estimates are 0. However, the \(t\) parameter, and therefore the renormalisation, does not account for this.

In this paper, we demonstrate that changes in the gradient scale can lead to large updates that persist over a long horizon. Previous work [10; 11] has suggested that old momentum estimates can _contaminate_ an agent's update and propose resetting the entire optimizer state when the target changes as a solution. However, by discarding previous momentum estimates, we hypothesise that this approach needlessly sacrifices valuable information for optimization. Instead, we propose retaining momentum estimates and only resetting \(t\), which we refer to as **Adam-Rel**. In the limit of gradient sparseness, we show that the Adam-Rel update size remains bounded, converging to \(1\) in the limit of a large gradient, unlike Adam. Furthermore, if such gradient magnitude increases do not occur, Adam-Rel reduces to learning rate annealing, a common method for stabilising optimization.

When evaluated against the original Adam and Adam with total resets, we demonstrate that our method improves PPO's performance in Craftax-Classic  and the Atari-57 challenge from the Arcade Learning Environment . Additionally, we demonstrate improved performance in the off-policy setting by evaluating DQN on the Atari-10 suite of tasks . We then examine the gradients in practice and show that there are significant increases in gradient magnitude following changes in the objective. Finally, we examine the discrepancies between our theoretical model and observed gradients to better understand the effectiveness of Adam-Rel.

## 2 Background

### Reinforcement Learning

DefinitionReinforcement learning agents learn a policy \(\) in a Markov Decision Process [15; MDP], a tuple \(M=,,,,\) where \(\) is the set of states, \(\) is the set of actions, \(:()\) is the transition function, \(:\) is the reward function and \(\) is the discount factor. At each timestep \(t\), the agent observes a state \(s_{t}\) and takes an action \(a_{t}\) drawn from \((|s_{t})\) before transitioning to a new state \(s_{t+1}\) and receiving reward \(r_{t}\) drawn from \((s_{t},a_{t})\). The goal of the agent is to maximise the expected discounted return \(_{,}[_{t=0}^{}^{t}r_{t}]\).

Nonstationarity in RLIn contrast with supervised learning, where a single stationary objective is typically optimised, reinforcement learning is inherently nonstationary. Updates to the policy induce changes not only in the distribution of observations seen at a given timestep, but also the return distribution, and hence value function being optimised. This arises regardless of how these updatesare performed. However, one particular reason for nonstationarity in RL is the use of bootstrapped value estimates via TD learning , which optimises the below objective

\[()=[\{r_{t}+ V_{}^{}(s_{ t+1})\}-V_{}^{}(s_{t})]^{2},\]

where sg is the stop-gradient operator. In this update, the target \(r_{t}+ V_{}^{}(s_{t+1})\) depends on the parameters \(\) and therefore changes as these are updated.

These target changes can either be more gradual, as in the case of continuous updates to the value function in TD learning, or more abrupt, as in the case of the use of target networks in DQN.

Sequentially Optimized Stationary ObjectivesIn this work, we focus on abrupt objective changes; changes of objectives that do not involve a smoothing method such as Polyak averaging , and the resulting sudden change of supervised learning problem. More explicitly, we consider optimising a stationary loss function \(L(,)\), where \(\) are the parameters to be optimised and \(\) is the other parameters of the loss function (such as the parameters of a value network), which are not updated throughout optimisation, but does not include the training data.

We consider a setting where at a certain timestep \(t\) in our training, we transition from optimising \(L(_{t},_{1})\) to optimising \(L(_{t+1},_{2})\) for some \(_{1}\), \(_{2}\). Such individual objectives are still non-stationary. For example, significant changes in the policy would induce changes in the data distribution, which would then affect the underlying loss landscape, but we do not consider such non-stationarity in this work.

This setting is very common throughout RL. Bootstrapped value estimates are the most common cause of this, but it also occurs in PPO's actor update, where each new rollout induces a different supervised learning problem due to the actor and critic updates. This is optimised for a fixed number of updates before collecting new data.

We refer to these sequences of supervised learning problems as sequentially-optimised stationary objectives. In this work, we use this framing to propose an approach that is consistent throughout each stationary period of optimization and applies corrections to make optimization techniques valid when nonstationarity is introduced via objective changes. Bengio et al.  propose the gradient contamination hypothesis, which states that current optimizer momentum estimates can point in the opposite direction to the gradient following a change in objective, thereby hindering optimization. A previous approach to this problem is that of Asadi et al. , where they propose resetting Adam's momentum estimates and timestep to \(0\) throughout training. We refer to this method as **Adam-MR**. Finally, Dohare et al.  propose setting the Adam hyperparameters to equal values, such that \(_{1}=_{2}\), suggesting that this can help avoid performance collapse.

Proximal Policy OptimizationProximal Policy Optimization [4, PPO] is a policy optimisation based RL method. It uses a learned critic \(V_{}^{}\) trained by a TD loss to estimate the value function, and a clipped actor update of the form

\[[(r_{(,t)},1)A^{}(s_{t}, a_{t}),r_{(,t)}A^{}(s_{t},a_{t})],\] (1)

where the policy ratio \(r_{(,t)}=_{}(a_{t}|s_{t})}{(a_{t}|s_{t})}\) is the ratio of the stochastic policy to optimise \(_{}\) and \(\), the previous policy. \(A^{}\) is the advantage, which is typically estimated using generalised advantage estimation . Clipping the policy ratio aims to avoid performance collapse by preventing policy updates larger than \(\).

Optimisation of the PPO objective proceeds by first rolling out the policy to collect data, and then iterating over this data in a sequence of _epochs_. Each of these epochs splits the collected data into a sequence of _mini-batches_, over which the above update is calculated.

### Momentum-Based Optimization

Momentum [1; 2] is a method for enhancing stochastic gradient descent by accumulating gradients in the direction of repeated improvement. The typical formulation of momentum for each element \(i\) is

\[m_{t}^{i} = m_{t-1}^{i}+g_{t}^{i},\] \[_{t}^{i} =_{t-1}^{i}- m_{t}^{i},\]where \(\) is the momentum coefficient, \(g_{t}^{n}\) is the gradient at the current step, \(m_{t}^{n}\) is the gradient incorporating momentum, \(\) is the scalar learning rate and \(^{n}\) are the parameters to be optimised. With momentum, update directions with low curvature have their contribution to the gradient amplified, considerably reducing the number of steps required for convergence.

In the introduction, we described the update equations for Adam , the most popular optimizer that uses momentum. Adam's update is designed to keep its updates within a trust region, which depends on a learning rate \(\).

## 3 Nonstationary Optimization with Adam

We now investigate the effect of nonstationarity on Adam by analysing its update rule after a sudden change in gradient. As a simplified model of gradient instability, we assume optimization with Adam starts at timestep \(t=-t^{}\) with a constant gradient \(g_{-t^{}}^{i}=g\), \(0<g<\) until timestep \(0\). Following \(t=0\), we model instability by increasing the gradient by a factor of \(k\), as might occur in a nonstationary optimization setting. This gives

\[g_{t}^{j}=g,&-t^{} t<0,\\ kg,&t 0.\] (2)

For larger values of \(t^{}\), the short term effects of Adam's initialisation on the momentum terms dissipate and \(_{t}\) and \(_{t}\) converge to stable values. By taking the limit of \(t^{}\), we investigate the effect of a sudden change in gradient \(g_{t}^{i}\) on the update size \(u_{t}^{i}\) after a long period of training. This allows for any effects from the initialisation of momentum terms \(_{-t^{},t}\) and \(_{-t^{},t}\) to dissipate:

**Theorem 3.1**.: _Assume that \(=0\). Let \(g_{t}^{i}\) be defined as in Equation (2) and \(_{-t^{},t}^{i}\) and \(_{-t^{},t}^{i}\) be the momentum terms at timestep \(t\) given Adam starts at timestep \(-t^{}\). It follows that:_

\[_{t^{}}u_{t}^{i}=_{t^{}}_{ -t^{},t}^{i}}{_{-t^{},t}^{i}}}=}^{t+1 }+k(1-{_{1}}^{t+1})}{{}^{t+1}+k^{2}(1-{_{2}}^{t+1} )}}}.\] (3)

Proof.: See Appendix A. 

For large \(k\), Theorem 3.1 proves that the element-wise momentum term after the change in gradient at \(t=0\) is approximately \(}}{}}}\). For the most commonly used values of \({_{1}}=0.9\) and \({_{2}}=0.999\), this is \(\), which is much larger than the intended unit update which Adam is designed to maintain. The top plot in Figure 1, which shows the Adam update size against \(t\) for different values of \(k\), demonstrates that the update peaks significantly higher than the desired \(1\) before slowly converging back to \(1\).

## 4 Adam with Relative Timesteps

To fix the problems analysed in the previous section, we introduce Adam-Rel. At the start of each new supervised learning problem, Adam-Rel resets Adam's \(t\) parameter to 0, rather than incrementing it from its previous value. This one-line change is illustrated for PPO in Algorithm 1.

At the start of training, both momentum terms in Adam are \(0\). Therefore, at the first timestep, when the first gradient is encountered, the magnitude of the gradient is infinite relative to the current momentum estimate. As explained in Section 3, this induces a large update. However, dividing the momentum estimates by \((1-{_{1}}^{t})\) and \((1-{_{2}}^{t})\) fixes this issue by correcting for this sparsity. Therefore, by resetting \(t\) to 0, Adam handles changes in gradient magnitude resulting from the change of supervised learning problem.

If we examine the same update as in the previous section adjusted by Adam-Rel, assuming that we reset Adam's \(t\) just before the gradient scales to \(kg\), we find it comes to

\[_{t^{}}_{-t^{},t}^{i}}{_{-t ^{},t}^{i}}}=}^{t+1}}}{1-{_{1}}^{t+1}} }^{t+1}+k(1-{_{1}}^{t+1})}{}^{t+1}+k^{2} (1-{_{2}}^{t+1})}}.\] (4)As \(k\), this tends to \(1\). This means that Adam-Rel ensures approximately unit update size in the case of a large increase in magnitude in the gradient, at the expense of a potentially smaller update at the point \(t\) is reset. Figure 1 shows the update size of Adam-Rel as \(t-t^{}\) increases. The update size is smaller at the start, but never reaches significantly above \(1\).

However, the above analysis does not show how Adam and Adam-Rel differ in practice, where large changes in gradient magnitude may not occur. Examining the bottom of Figure 1, we can see that for lower values of \(k\), Adam-Rel rapidly decays the update size before increasing it. Functionally, this behaves like a learning rate schedule. Over a short horizon (e.g., 16 steps is common in PPO), this effect is similar to learning rate annealing, whilst over a longer horizon (e.g., approximately 1000 steps in DQN) it is akin to learning rate warmup, both of which are popular techniques in optimising stationary objectives. Therefore, the benefits of Adam-Rel are twofold: first, it guards against large increases in gradient magnitude by capping the size of potential updates, and secondly, if such large gradient increases do not occur, it reduces to a form of learning rate annealing, which is commonly employed in optimising stationary objectives.

## 5 Experiments

### Experimental setup

To evaluate Adam-Rel, we explore its impact on DQN and PPO, two of the most popular algorithms in off-policy and on-policy RL respectively.

To do so, we first train DQN [18; 19] agents with Adam-Rel on the Atari-10 benchmark for 40M frames, evaluating performance against agents trained with Adam and Adam-MR. We then extensively evaluate our method's impact on PPO [4; 19; 20], training agents on Craftax-Classic-1B --a JAX-based reimplementation of Crafter  where the agent is allocated 1 billion environment interactions--and the Atari-572 suite  for 40 million frames. In doing so, our benchmarks respectively evaluate the performance of Adam-Rel on exceedingly long training horizons and its

Figure 1: Update size of Adam and Adam-Rel versus \(k\) when considering nonstationary gradients. Assumes that optimization starts at time \(-t^{}\), which is large, and that the gradients up until time \(0\) are \(g\) and then there is an increase in the gradient to \(kg\).

robustness when applied to a diverse range of environments. We then analyse the differences between Adam-Rel and Adam's updates. We compare \(8\) seeds on the Craftax-Classic environment for this purpose, recording the update norm, maximum update, and gradient norm of every update.

### Off-policy RL

Figure 2 shows the performance of DQN agents trained with Adam-Rel against those trained with Adam-MR and Adam on the Atari-10 benchmark . We tune the learning rate of each method, keeping all other hyperparameters fixed at values tuned for Adam in CleanRL . Adam-Rel outperforms Adam, achieving 65.7% vs. 28.8% human-normalized performance. Furthermore, the stark performance difference between Adam-Rel and Adam-MR (23.5%) demonstrates the advantage of retaining momentum information across target changes (so long as appropriate corrections are applied), thereby contradicting the gradient contamination hypothesis discussed in Bengio et al.  and Asadi et al. .

More surprisingly, Adam-MR performs substantially worse than Adam, contrasting with the findings of Asadi et al. . We evaluate on a different set of Atari games and tune both Adam and Adam-MR separately, which may account for the differences. However, these results suggest that preventing any gradient information from crossing over target changes is an excessive correction and can even harm performance. We additionally evaluate on the set of games used by Asadi et al. , the results of which can be found in Appendix B. We find that Adam-Rel outperforms the Adam baseline in IQM. We also find that, although our implementation of Adam-MR again significantly under-performs relative to the Adam baseline, we approximately match the returns listed in their work.

We also evaluate Adam-Rel when soft target changes are used, by comparing Adam and Adam-Rel on Atari-10 when using DQN with Polyak averaging. We find that Adam-Rel also outperforms Adam in this setting. These results, along with a more detailed discussion, can be found in Appendix C.

### On-policy RL

CraftaxFigure 3 shows the performance of PPO agents trained on Craftax-1B over 8 seeds. Most strikingly, Adam-MR, which resets the optimizer completely when PPO samples a new batch, achieves dramatically poorer performance across all metrics. This deficit is unsurprising when compared to its performance on DQN, where the optimizer has many more updates between resets and so can achieve a superior momentum estimate, and demonstrates the impact of not retaining any momentum information after resets in on-policy RL. Similarly, Adam with \(_{1}=_{2}\) achieves poorer performance than Adam-Rel on all metrics and has no significant different against Adam with default hyperparameters.

Furthermore, Adam-Rel outperforms Adam on all metrics. Whilst the performance on the number of achievements is similar, we follow the evaluation procedure recommended in Hafner  and report score, calculated as the geometric mean of success rates for all achievements. This metric applies logarithmic scaling to the success rate of each achievement, thereby giving additional weight to those that are hardest to accomplish. We see that Adam-Rel clearly outperforms Adam in score, as well as on the two hardest achievements (collecting diamonds and eating a plant). These behaviours require

Figure 2: Performance of Adam-Rel, Adam, Adam-MR, and Adam (\(_{1}=_{2}\)) for PPO and Adam, Adam-MR and Adam-Rel for DQN on Atari-57 and Atari-10 respectively. Atari-10 uses a subset of Atari tasks to estimate median performance across the whole suite. Details can be found in . Error bars are 95% stratified bootstrapped confidence intervals. Results are across 10 seeds except for Adam (\(_{1}=_{2}\)), which is 3 seeds.

a strong policy to discover so are learned late in training, suggesting that Adam-Rel improves the plasticity of PPO.

Atari-57Figure 2 shows the performance of PPO agents on Atari-57. As before, entirely resetting the optimizer significantly harms performance when compared to resetting only the count. Across all environments, Adam-Rel also improves over Adam, outperforming it in **33 out of the 55 games** tested and IQM across games. Adam with \(_{1}=_{2}\) also fails to improve over the baseline.

To further analyse the impact of Adam-Rel over Atari-57, we plot the performance profile of human-normalized score (Figure 4). Whilst the performance of the two methods is similar over the bottom half of the profile, we see a major increase in performance in the top half. Namely, at the 75th percentile of scores Adam-Rel achieves a human-normalized performance of **338% vs. 220%** achieved by Adam. This demonstrates the ability of Adam-Rel to improve policy performance on tasks where Adam is successful but suboptimal, without sacrificing performance on harder tasks.

### Method Analysis

In this section we connect our theoretical exposition in Section 3 to our experimental results. Specifically, we first examine whether gradients increase in magnitude due to nonstationarity, to what extent predictions from our model match the resulting updates, and how Adam's update differs from Adam-Rel's in practice.

To this end, we collect gradient (i.e., before passing through the optimizer) and update (i.e., the final change applied to the network) information from PPO on Craftax-Classic. We follow the experimental setup in Section 5 but truncate the Craftax-Classic runs to \(250\)M steps to reduce the data processing required. The results are shown in Figure 5.

Comparing Theory and PracticeIn Figure 5, both Adam and Adam-Rel face a significant increase in gradient norm immediately after starting optimisation on a new objective resulting from a new batch of trajectories collected under an updated policy and value function. While this matches the assumptions we make in our work, the magnitude of the increase is much less than some of the values explored in Section 3.

For Adam, this is approximately 29% and for Adam-Rel it is around 45%. The grad norm profiles look similar in each case, with the norm peaking early before decreasing below its initial average value. This decrease and the initial ramp both deviate from the step function we assume in our model. It is obvious that our theoretical model of gradients, which requires an increase in the gradient magnitude on each abrupt change in the objective, cannot hold throughout training in its entirety because this would require the gradient norm to increase without bound.

Figure 4: Performance Profile of Adam and Adam-Rel on Atari-57. Error bars represent the standard error across 10 seeds. Green-shaded areas represent Adam-Rel outperforming Adam and red-shaded areas the opposite.

Figure 3: PPO on Craftax-1B — comparison of Adam-Rel against Adam, Adam-MR, and Adam with \(_{1}=_{2}\). Bars show the 95% stratified bootstrap confidence interval, with mean marked, over 8 seeds .

However, we find that despite this discrepancy, for Adam-Rel the update predicted by our model fairly closely matches the shape of the true update norm, i.e., a _fast drop_ at the beginning followed by flattening (the scaling is not comparable between observed and predicted values).

For Adam, our model explains the initial _overshoot_ of the update norm but then fails to predict the rapid decrease, which results from the fast drop in the true gradient norm. Given the simplicity of our modeling assumptions, we find these results overall encouraging.

On Spherical CowsUnder the assumption of a _step increase_ in gradients of an _infinite_ relative magnitude Adam-Rel results in a flat update, while Adam would drastically overshoot. Clearly, this assumption does not hold in practice, as we have shown above. However, we believe that this mismatch between reality and assumption is encouraging, since our experimental results show that Adam-Rel is still effective in this regime. Our hypothesis is that there are two benefits to designing Adam-Rel under these assumptions. First of all, it avoids overshoots even under large gradient steps and secondly, when there are less drastic gradient steps it _undershoots_, which might have similar effects to a fast learning rate annealing. These kind of annealing schedules (over longer horizons) are popular when optimising stationary losses [23; 24].

## 6 Related Work

Optimization in Reinforcement LearningPlasticity loss [25; 26; 27] refers to the loss in ability of models to fit new objectives as they are trained. This is particularly relevant in nonstationary settings such as RL and continual learning, where the model is continuously fitting changing objectives. Many solutions have been proposed, including resetting network layers [28; 29; 30; 31; 32], policy distillation , LayerNorm [33; 34], regressing outputs to their initial values , resetting dead units  and adding output heads during training . These solutions, in particular resetting layers during training [28; 32], have contributed towards state-of-the-art performance on Atari 100k . However, of these works, only Lyle et al.  investigate the relationship between the optimizer and nonstationarity, demonstrating that by reducing the momentum coefficient of the second-moment gradient estimate in Adam, the fraction of dead units no longer increases. However, these works focus on plasticity loss, which is a symptom of nonstationarity, and only analyse off-policy RL. In contrast, we address nonstationarity directly and evaluate both on-policy and off-policy RL.

Meta-reinforcement learning [37; 38; 39] provides an alternative approach to designing optimizers for reinforcement learning. Rather than manually identifying problems and handcrafting solutions for RL optimization, this line of work seeks to automatically discover these solutions by meta-learning components of the optimization process. Often these methods parameterize the agent's loss function with a neural network, allowing it to be optimized through meta-gradients [40; 41; 42] or zeroth-order methods [43; 20; 44]. Recently, Lan et al.  proposed meta-learning a black-box optimizer directly,

Figure 5: Adam and Adam-Rel compared to the theoretical model. To make this plot, we divided all the updates in the PPO run into chunks, each of which was optimising a stationary objective. We then averaged over all the chunks. The red dashed lines show the different epochs for each batch of data. The assumption about the gradient under the model is shown in the grad norm plot. Note that the update norm plot for Adam and Adam-Rel has separate y-axes. The shading represents standard error.

demonstrating competitive performance with Adam on a range of RL tasks. However, these works are limited by the distribution of tasks they were trained on, and using handcrafted optimizers in RL is still far more popular.

Adam ExtensionsCyclical update schedules  have previously been applied in supervised learning as a mechanism for simplifying hyperparameter tuning and improving performance, and Loshchilov and Hutter  propose the use of warm learning rate restarts with cosine decay for improving the training of convolutional nets. Liu et al.  examine the combination of Adam and learning rate warmup, proposing RAdam to stabilise training. However, all of these methods focus on supervised learning and therefore assume stationarity.

There has also been some investigation of the interaction between deep RL and momentum-based optimization. Henderson et al.  investigate the effects of different optimizer settings and recommend sensible parameters, but do not investigate resetting the optimizer. Bengio et al.  identify the problem of contamination of momentum estimates and propose a solution based on a Taylor expansion. Dohare et al.  investigate policy collapse in RL when training for longer than methods were tuned for and propose setting \(_{1}=_{2}\) to address this. By contrast, we investigate training for a standard number of steps and focus on improved overall empirical performance, rather than avoiding policy collapse. Asadi et al. , which is perhaps the most similar to our work, also aim to tackle contamination, but do so differently, by simply resetting the Adam momentum states to 0 whenever the target network changes in the value-based methods DQN and Rainbow. However, they do not consider resetting of Adam's timestep parameter, and explain their improved results by suggesting that old, bad, momentum estimates contaminate the gradients when training on a new objective. By contrast, we demonstrate that resetting only the timestep suffices for better performance on a range of tasks and therefore that the contamination hypothesis does not explain the better performance of resetting the optimizer. We also demonstrate that retaining momentum estimates can be essential for performance, particularly in on-policy RL.

Adam in RLTo adapt Adam for use in RL, prior work has commonly applied a number of modifications compared to its use in supervised learning . The first is to set the parameter \(\) to \(10^{-5}\), which is a higher value than the \(10^{-8}\) typically used in supervised learning. Additionally many reinforcement learning algorithms use gradient clipping before passing the gradients to Adam. Typically gradient vectors are clipped by their \(L_{2}\) norm.

A higher value of \(\) reduces the sensitivity of the optimizer to sudden large gradients. If an objective has been effectively optimized and hence the gradients are very small, then a sudden target change may lead to large gradients. \(\) typically updates much more slowly than \(\) and therefore this causes the update size to increase significantly, potentially causing performance collapse. However, this implementation detail is not mentioned in the PPO paper , and subsequent investigations omit it [6; 5]. Clipping the gradient by the norm also aims at preventing performance collapse. Andrychowicz et al.  find this to increase performance slightly when set to \(0.5\).

## 7 Limitations and Future Work

In this work we have mostly examined _abrupt_ nonstationarity, where there are distinct changes of target, as it is in that setting where our method can be most cleanly applied. However, a range of RL methods face _continuous_ nonstationarity, such as when applying Polyak averaging  to smoothly update target networks after every optimization step. We have demonstrated improved performance in this algorithm in Appendix C. However, further investigation into how to apply resetting in this setting would be beneficial.

There are also many promising avenues for future work. First, while we have focused on RL, it would be interesting to apply Adam-Rel to other domains that feature nonstationarity such as RLHF, training on synthetic data, or continual learning. Additionally, we also note that while our results are promising, it was not possible to investigate all RL settings and environments in this work, and we therefore encourage future work in settings such as continuous control. Secondly, Adam-Rel is designed with the principle that large updates can harm learning, but it is not clear in general what properties of update sizes are desirable in nonstationary settings. Understanding this more clearly may help produce meaningful improvements in optimisation. Relatedly, it would be beneficial to better understand the nature of gradients in RL tasks, in particular how they change throughout training for different methods and what effect this has on performance. Finally, re-examining other aspects of the RL toolchain that are borrowed from supervised learning could produce further advancements by designing architectures, optimisers and methods specifically suited for problems in RL.

## 8 Conclusion

We presented a simple, theoretically-motivated method for handling nonstationarity via the Adam optimizer. By analysing the impact of large changes in gradient size, we demonstrated how directly applying Adam to nonstationary problems can lead to unstable update sizes, before demonstrating how timestep resetting corrects for this instability. Following this, we performed an extensive evaluation of Adam-Rel against Adam and Adam-MR in both on-policy and off-policy settings, demonstrating significant empirical gains. We then demonstrated that increases in gradient magnitude after abrupt objective changes occur in practice and compared the predictions of our simple theoretical model with the observed data in a complex environment. Adam-Rel can be implemented as a simple, single-line extension to any Adam-based algorithm with discrete nonstationarity (e.g. target network updates), leading to major improvements in performance across environments and algorithm classes. We hope that the ease of implementation and effectiveness of Adam-Rel will encourage researchers to use it as a de facto component of future RL algorithms, providing a step towards robust and performant RL.