# Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection

Charles Guille-Escuret

ServiceNow Research, Mila,

Universite de Montreal

&Pierre-Andre Noel

ServiceNow Research

Ioannis Mitliagkas

Mila, Universite de Montreal,

Canada CIFAR AI chair,

Archimedes Unit, Athena Research Center

&David Vazquez

ServiceNow Research

Joao Monteiro

Autodesk\({}^{2}\)

###### Abstract

Deployed machine learning systems require some mechanism to detect out-of-distribution (OOD) inputs. Existing research mainly focuses on one type of distribution shift: detecting samples from novel classes, absent from the training set. However, real-world systems encounter a broad variety of anomalous inputs, and the OOD literature neglects this diversity. This work categorizes five distinct types of distribution shifts and critically evaluates the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). We find that while these methods excel in detecting novel classes, their performances are inconsistent across other types of distribution shifts. In other words, they can only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a Gaussian mixture generative model for existing detection scores, enabling an ensemble detection approach that is more consistent and comprehensive for broad OOD detection, with improved performances over existing methods. We release code to build BROAD to facilitate a more comprehensive evaluation of novel OOD detectors.1.

## 1 Introduction

A significant challenge in deploying modern machine learning systems in real-world scenarios is effectively handling out-of-distribution (OOD) inputs. Models are typically trained in closed-world settings with consistent data distributions, but they inevitably encounter unexpected samples when deployed in real-world environments. This can both degrade user experience and potentially result in severe consequences in safety-critical applications .

There are two primary approaches to enhancing the reliability of deployed systems: OOD robustness, which aims to improve model accuracy on shifted data distributions , and OOD detection , which seeks to identify potentially problematic inputs and enable appropriate actions (e.g., requesting human intervention).

Robustness is often considered preferable since the system can operate with minimal disruption, and has been investigated for various types of distribution shifts . However, attainingrobustness can be challenging: it may be easier to raise a warning flag than to provide a "correct" answer.

Furthermore, robustness is not achievable when a classification system is presented with an input of an unknown semantic class, as none of the known labels can be considered correct. In recent years, OOD detection research has tackled the detection of such distributions shifts, under different terminologies motivated by subtle variations: open set recognition (OSR), anomaly detection, novelty detection, and outlier detection (see Yang et al.  for a comprehensive analysis of their differences).

Beyond novel classes, researchers investigated the detection of adversarial attacks [2; 33] and artificially generated images [36; 53; 51], although these distribution shifts are rarely designated as "OOD". Few works simultaneously detect novel labels and adversarial attacks [45; 25], and the broad detection of diverse types of distribution shifts remains largely unaddressed.

In real-world scenario, _any_**type of distribution shift is susceptible to affect performances and safety**. While recent efforts like OpenOOD  have simplified and standardized OOD detection evaluations, their exclusive focus on a specific type of distribution shift is susceptible to yield detection methods that are overspecialized and perform unreliably on _out-of-distribution distribution shifts_, i.e., they only detect "unexpected" samples that are, in fact, expected.

These concerns are confirmed in Figure 2, which displays the distributions of maximum softmax (MSP) , ViM , and MDS  scores on several shifted distributions relative to clean data (ImageNet-1k). Although all scores effectively distinguish samples from iNaturalist [34; 78], a common benchmark for detecting novel classes, their performance on other types of distribution shifts is inconsistent.

Figure 1: An overview of BROAD: illustrating the benchmarks employed for each distribution shift category, with ImageNet-1K serving as the in-distribution reference.

Figure 2: Score distributions of MSP, ViM, and MDS across datasets. While all methods discriminate between ImageNet and iNaturalist, their effectiveness fluctuates across the other types of distribution shifts described in Section 2.

Furthermore, OOD detection methods often require tuning or even training on OOD samples [49; 46; 48], exacerbating the problem. Recent research has attempted the more challenging task of performing detection without presuming access to such samples [56; 25; 82]. Nevertheless, they may still be inherently specialized towards specific distribution shifts. For example, CSI  amplifies the detection score by the norm of the representations. While this improves performance on samples with novel classes (due to generally lower norm representations), it may conversely impair performance in detecting, for instance, adversarial attacks, which may exhibit abnormally high representation norms.

The scarcity of diversity in OOD detection evaluations in previous studies may be attributed to the perceived preference for OOD robustness when OOD samples share classes with the training set. Nevertheless, this preference may not always be well-founded. Firstly, previous works have indicated a potential trade-off between in-distribution accuracy and OOD robustness [77; 90], although a consensus remains elusive . On the other hand, many OOD detection systems serve as post-processors that do not impact in-distribution performances. Additionally, there are practical scenarios where the detection of OOD inputs proves valuable, regardless of robustness. For instance, given the increasing prevalence of generative models [68; 64; 70], deployed systems may need to differentiate synthetic images from authentic ones, independent of performance [51; 42]. Lastly, other types of shifts exist where labels belong to the training set, but correct classification is undefined, rendering robustness unattainable (see section 2.5).

Our work focuses on _broad OOD detection_, which we define as the simultaneous detection of OOD samples from diverse types of distribution shifts. Our primary contributions include:

* Benchmarking Resilience Over Anomaly Diversity (BROAD), an extensive OOD detection benchmark (relative to ImageNet) comprising twelve datasets from five types of distribution shifts: novel classes, adversarial attacks, synthetic images, corruptions, and multi-class.
* A comprehensive benchmarking of recent OOD detection methods on BROAD.
* The development and evaluation of a generative ensemble method based on a Gaussian mixture of existing detection statistics to achieve broad detection against all types of distribution shifts, resulting in significant gains over existing methods in broad OOD detection.

Section 2 introduces BROAD while Section 3 presents studied methods and our generative ensemble method based on Gaussian mixtures. In Section 4, we evaluate different methods against each distribution shift. Section 5 provides a synopsis of related work, and we conclude in Section 6.

## 2 Distribution Shift Types in BROAD

In this study, we employ ImageNet-1K  as our in-distribution. While previous detection studies have frequently used CIFAR , SVHN , and LSUN  as detection benchmarks, recent work has highlighted the limitations of these benchmarks, citing their simplicity, and has called for the exploration of detection in larger-scale settings . Consequently, ImageNet has emerged as the most popular choice for in-distribution.

Our benchmark, BROAD, encompasses five distinct types of distribution shifts, each represented by one to four corresponding datasets, as summarized in Figure 1. This selection, while not exhaustive, is substantially more diverse than traditional benchmarks, and provides a more realistic range of the unexpected inputs that can be plausibly encountered.

### Novel Classes

The introduction of novel classes represents the most prevalent type of distribution shift in the study of OOD detection. In this scenario, the test distribution contains samples from classes not present in the training set, rendering accurate prediction unfeasible.

For this particular setting, we employ three widely used benchmarks: iNaturalist [34; 78], ImageNet-O , and OpenImage-O [82; 43].

### Adversarial Perturbations

Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD) and AutoAttack. Each attack is generated with an \(L_{}\) norm perturbation budget constrained to \(=0.05\), with PGD employing 40 steps. In its default configuration, AutoAttack constitutes four independently computed threat models for each image; from these, we selected the one resulting in the highest confidence misclassification. A summary of the models' predictive performance when subjected to each adversarial scheme can be found in Table 1. The relative detection difficulty of white-box versus black-box attacks remains an open question. Although white-box attacks are anticipated to introduce more pronounced perturbations to the model's feature space, black-box attacks might push the features further away from the in-distribution samples. To elucidate this distinction and provide a more comprehensive understanding of detection performance, we generate two sets of attacks using both PGD and AutoAttack: one targeting a ResNet50  and the other a Vision Transformer (ViT) . Evaluation is performed on both models, thereby ensuring the inclusion of two black-box and two white-box variants for each attack.

Common practice in the field focuses on the detection of successful attacks. However, identifying failed attempts could be advantageous for security reasons. To cater to this possibility, we appraise detection methods in two distinct scenarios: the standard Distribution Shift Detection (DSD), which aims to identify any adversarial perturbation irrespective of model predictions, and Error Detection (ED), which differentiates solely between successfully perturbed samples (those initially correctly predicted by the model but subsequently misclassified following adversarial perturbation) and their corresponding original images.

### Synthetic Images

This category of distribution shift encompasses images generated by computer algorithms. Given the rapid development of generative models, we anticipate a growing prevalence of such samples. To emulate this shift, we curated two datasets: one derived from a conditional BigGAN model , and another inspired by stable diffusion techniques .

In the case of BigGAN, we employed publicly available models2 trained on ImageNet-1k and generated 25 images for each class. For our stable diffusion dataset, we utilized open-source text-conditional image generative models3. To generate images reminiscent of the ImageNet dataset, each ImageNet class was queried using the following template:

High quality image of a {class_name}.

This procedure was repeated 25 times for each class within the ImageNet-1k label set. Given that a single ImageNet class may have multiple descriptive identifiers, we selected one at random each time.

### Corruptions

The term _corruptions_ refers to images that have undergone a range of perceptual perturbations. To simulate this type of distribution shift, we employ four distinct corruptions from ImageNet-C : defocus blur, Gaussian noise, snow, and brightness. All corruptions were implemented at the maximum intensity (5 out of 5) to simulate challenging scenarios where OOD robustness is difficult, thus highlighting the importance of effective detection. Analogous to the approach taken with adversarial perturbations, we implement two distinct evaluation scenarios: Distribution Shift Detection (DSD), aiming to identify corrupted images irrespective of model predictions, and Error Detection (ED), discriminating between incorrectly classified OOD samples and correctly classified in-distribution samples, thus focusing solely on errors introduced by the distribution shift.

### Multiple Labels

In this study, we propose CoComageNet, a new benchmark for a type of distribution shift that, to the best of our knowledge, has not been previously investigated within the context of Out-of-Distribution

   &  &  &  \\   & & PGD & AA & PGD & AA \\  RN50 & 74.2\% & 39.3\% & 28.2\% & 68.0\% & 43.3\% \\ ViT & 85.3\% & 0.4\% & 50.8\% & 77.1\% & 65.8\% \\  

Table 1: Prediction accuracy of the two evaluated models across the range of perturbation settings examined in our study.

(OOD) detection. We specifically focus on _multiple labels_ samples, which consist of at least two distinct classes from the training set occupying a substantial portion of the image.

Consider a classifier trained to differentiate dogs from cats; the label of an image featuring a dog next to a cat is ambiguous, and classifying it as either dog or cat is erroneous. In safety-critical applications, this issue could result in unpredictable outcomes and requires precautionary measures, such as human intervention. For example, a system tasked with identifying dangerous objects could misclassify an image featuring both a knife and a hat as safe by identifying the image as a hat.

The CoComageNet benchmark is constructed as a subset of the CoCo dataset , specifically, the 2017 training images. We identify 17 CoCo classes that have equivalent counterparts in ImageNet (please refer to appendix A for a comprehensive list of the selected CoCo classes and their ImageNet equivalents). We then filter the CoCo images to include only those containing at least two different classes among the selected 17. We calculate the total area occupied by each selected class and order the filtered images based on the portion of the image occupied by the second-largest class. The top 2000 images based on this metric constitute CoComageNet. By design, each image in CoComageNet contains at least two distinct ImageNet classes occupying substantial areas.

Although CoComageNet was developed to study the detection of multiple label images, it also exhibits other less easily characterized shifts, such as differences in the properties of ImageNet and CoCo images, and the fact that CoComageNet comprises only 17 of the 1000 ImageNet classes. To isolate the effect of multiple labels, we also construct CoComageNet-mono, a similar subset of CoCo that contains only one of the selected ImageNet classes (see appendix A for details).

As shown in appendix A, detection performances for all baselines on CoComageNet-mono are near random, demonstrating that detection of CoComageNet is primarily driven by the presence of multiple labels. Finally, to reduce the impact of considering only a subset of ImageNet classes, we evaluate detection methods using in-distribution ImageNet samples from the selected classes only.

## 3 Detection Methods

In this study, our focus is predominantly on methods that do not require training or fine-tuning using OOD samples. This consideration closely aligns with real-world applications where OOD samples are typically not known _a priori_. Additionally, the practice of fine-tuning or training on specific types of distribution shifts heightens the risk of overfitting them.

**Evaluated Methods:** We assess the broad OOD detection capabilities of a large number of methods including ASH , SHE , Relation , React , ViM , GradNorm , EBO , Dice , DOCTOR , CADet , Odin , and Mahalanobis Distance (MDS) . Fur

   &  &  &  &  &  &  \\  & ViT & RN50 & ViT & RN50 & ViT & RN50 & ViT & RN50 & ViT & RN50 & ViT & RN50 \\  CADet \(m_{}\) & 20.91 & 67.69 & 67.12 & 62.4 & 59.82 & 55.65 & 79.67 & 87.15 & 54.24 & 56.88 & 56.35 & 65.77 \\ ODIN & 91.73 & 73.58 & 52.29 & 54.44 & 62.74 & 61.49 & 79.68 & 88.52 & 70.75 & 64.46 & 71.44 & 68.5 \\ Max logits & 95.25 & 73.67 & 59.73 & 59.62 & 66.08 & 57.65 & 83.60 & 90.87 & 71.63 & 62.79 & 75.26 & 68.92 \\ Logits norm & 51.93 & 52.62 & 37.39 & 51.82 & 38.25 & 59.47 & 39.99 & 82.81 & 36.32 & 48.05 & 40.78 & 58.95 \\ MSP & 90.56 & 67.25 & 58.46 & 61.17 & 64.78 & 55.59 & 78.62 & 86.71 & 71.93 & **67.52** & 72.87 & 67.65 \\ MDS\({}_{t}\) & 53.35 & 63.52 & 67.73 & 55.04 & 54.92 & 56.18 & 31.47 & 76.52 & 63.43 & 36.81 & 54.18 & 57.61 \\ MDS\({}_{t}\) & **97.38** & 72.32 & 74.75 & 68.91 & 68.98 & 55.41 & 83.29 & 75.24 & 63.41 & 38.92 & 77.56 & 62.16 \\ MDS\({}_{}\) & 89.17 & 72.66 & **85.64** & 71.49 & 72.45 & 60.89 & **95.55** & 89.42 & 26.06 & 30.11 & 73.77 & 64.89 \ & 95.47 & 97.90 & 60.71 & 61.46 & 66.03 & 54.24 & 83.67 & 89.82 & 71.79 & 63.91 & 75.53 & 69.83 \\ GradNorm & 90.85 & 75.53 & 65.17 & 56.52 & 72.19 & **65.57** & 85.00 & 89.39 & 69.59 & 54.45 & 76.56 & 68.29 \\ EBO & 95.52 & 73.8 & 59.72 & 59.59 & 65.91 & 57.72 & 83.83 & 91.14 & 71.27 & 61.55 & 75.25 & 68.76 \\ \(D_{}\) & 91.27 & 67.95 & 58.62 & 61.44 & 64.49 & 55.65 & 81.57 & 87.43 & 72.49 & 67.15 & 73.78 & 67.92 \\ Dice & 55.7 & 74.45 & 78.29 & 58.76 & 77.84 & 59.43 & 86.67 & 91.38 & 61.23 & 59.97 & 71.95 & 68.8 \\ ViM & 95.76 & **81.55** & 56.85 & 62.91 & 61.01 & 53.26 & 79.79 & 87.00 & 68.45 & 49.01 & 72.37 & 66.75 \\ ASH & 95.52 & 73.89 & 59.72 & 69.61 & 65.91 & 57.94 & 83.83 & 91.13 & 71.27 & 61.65 & 75.25 & 70.84 \\ SHE & 90.98 & 76.71 & 72.15 & 68.37 & 67.19 & 63.87 & 82.38 & 89.79 & 60.92 & 60.91 & 74.72 & 71.93 \\ Relation & 93.61 & 76.06 & 68.77 & 6thermore, we explore three statistics widely applied in post-hoc OOD detection: maximum softmax probabilities (MSP), maximum of logits, and logit norm.

In the case of CADet, we solely utilize the intra-similarity score \(m_{}\) with five transformations to minimize computational demands. For DOCTOR, we employ \(D_{}\) in the Totally Black Box (TBB) setting, disregarding \(D_{}\) as it is functionally equivalent to MSP in the TBB setting when rescaling the detection threshold is accounted for (resulting in identical AUC scores). Odin typically depends on the fine-tuning of the perturbation budget \(\) and temperature \(T\) on OOD samples. To bypass this requirement, we use default values of \(=0.0014\) and \(T=1000\). These default parameters, having been tuned on a variety of datasets and models, have demonstrated robust generalization capabilities. Nevertheless, it should be noted that the choice of these values, despite being considered reasonable, does represent a caveat, as they were initially determined by tuning OOD detection of novel classes.

In its standard form, the Mahalanobis detection method computes the layer-wise Mahalanobis distance, followed by training a logistic regressor on OOD samples to facilitate detection based on a weighted average of these distances. To eliminate the need for training on OOD samples, we consider three statistics derived from Mahalanobis distances: the Mahalanobis distance on the output of the first layer block (\(_{}\)), the Mahalanobis distance on the output of the last layer (\(_{}\)), and the Mahalanobis distance on the output of all layers averaged with equal weights (\(_{}\)). For the Vision Transformer (ViT), we focus on MDS on the class token, disregarding patch tokens.

**Generative Modeling for Detection:** Consider \(\) as a data distribution with a support set denoted as \(X\), and let \(h:X^{d}\) be a map that extracts features from a predetermined neural network. The function \(h(x)\) can be defined arbitrarily; for instance, it could be the logits that the network computes on a transformation of a sample \(x\), or the concatenation of outputs from different layers, among other possibilities. However, generative modeling in the input space (i.e., when \(h\) is the identity function) is generally difficult due to the exceedingly high dimensionality and intricate structure of the data.

To avoid modeling the density in the high dimensional input space, we can use as proxy the density \(p_{x}(h(x))\) in the latent space \(h()\). A generative model of \(h\) is tasked with learning the distribution \(p_{x}(h(x))\), using a training set \((x_{i})_{i N}\) that comprises independently sampled instances from \(\), and the log-density of the generative model can then directly be used as detection score.

A significant number of detection methods devise heuristic scores on \(h\) with the aim of maximizing detection performances on specific benchmarks, while often arbitrarily discarding information that could potentially be beneficial for other distribution shifts. In contrast, generative models learn an estimator of the likelihood of \(h(x)\) without discarding any information. Their detection performances are only constrained by the information extracted by \(h\) and, naturally, their proficiency in learning its distribution. This inherent characteristic makes generative models particularly suitable for broad Out-of-Distribution (OOD) detection. By learning the comprehensive distribution of \(h\), these models negate the bias associated with engineering detection scores against specific distribution shifts.

**Gaussian Mixtures Ensembling:** Gaussian Mixture Models (GMMs) are a versatile tool for learning a distribution of the form \(x_{i}^{n}_{i}(_{i},_{i})\), where \(n\) is the number of components, \(\), \(\) and \(\) are the parameters of the GMM and are learned with the Expectation-Maximization (EM) algorithm.

Figure 3: Covariance matrices of detection scores in-distribution for ViT (left) and ResNet-50 (right).

GMM-based generative modeling of neural network behaviors to facilitate detection has been previously reported . Methods that are based on the Mahalanobis distance bear similarity to this approach insofar as the layer-wise Mahalanobis score can be interpreted as the likelihood of the layer output for class-dependent Gaussian distributions, which are learned from the training set.

Despite these advantages, such methods encounter the formidable challenge of learning generative models of the network's high dimensional representation space, a task made more difficult due to the curse of dimensionality. In response to this challenge, we propose the learning of a Gaussian mixture of the scores computed by existing OOD detection methods. While this approach still relies on heuristic scores, it presents an ensemble method that is able to amalgamate their respective information, while maintaining the dimension of its underlying variables at a significantly low level. As a result, it achieves a favorable tradeoff between the generative modeling of high dimensional feature spaces and the heuristic construction of one-dimensional detection scores.

In addition to integrating their detection capabilities, this approach is adept at identifying atypical realizations of the underlying scores, even in situations where the marginal likelihood of each score is high, but their joint likelihood is low.

To make our method as general as possible, **we do not assume access to OOD samples to select which scores to use** as variables of our GMM. We present in Figure 3 the covariance matrices of the different scores on a held-out validation set of ImageNet. To minimize redundancy, we avoid picking multiple scores that are highly correlated on clean validation data. To decide between highly correlated scores, we opt for the ones with highest in-distribution error detection performance (see first two columns of Table 8). Moreover, we discard logit norm and \(_{}\) due to their near-random error detection performance in-distribution. Given that score correlation varies between ViTs and ResNets, as evidenced in Figure 3, we derive two distinct sets of scores. We also propose a computationally efficient alternative based on methods with minimal overhead costs:

**Ens-V** (ViT) = {GradNorm, ODIN, MDSall, MDSl, CADet, Dice, MSP, Max logits},
**Ens-R** (ResNet) = {GradNorm, ODIN, MDSall, MDSl, CADet, ReAct, ViM, \(D_{}\)},
**Ens-F** (Fast) = {MSP, Max logits, MDSall, MDSl, EBO}.

We train the GMM on the correctly-classified samples of a held-out validation set of 45,000 samples. This is essential as misclassified samples may produce atypical values of the underlying scores despite being in-distribution, which is demonstrated by the high in-distribution error detection AUC of selected scores. Finally, we train the GMM for a number of components \(n\{1,2,5,10,20\}\) and select \(n=10\) which maximizes the in-distribution error detection performances (see appendix C).

## 4 Evaluation

We assess performance using the widely accepted area under the curve (AUC) metric for two distinct pretrained models: ResNet-50 (RN50) and Vision Transformer (ViT). While it is standard to also report other metrics such as FPR@95 and AUPR, we find these metrics to be redundant with AUC and omit them for clarity. All evaluations are conducted on a single A100 GPU, with the inference time normalized by the cost of a forward pass (cf. App. B).

Our empirical results in the Distribution Shift Detection (DSD) setting, which aims to detect any OOD sample, are presented in Table 2. Results for the error detection setting, where the objective is to detect misclassified OOD samples against correctly classified in-distribution samples, are exhibited in Appendix D (Table 8). The results for each distribution shift type are averaged over the corresponding benchmark. Detailed performances and model accuracy for each dataset are offered in Appendix D (where applicable). In the error detection setting, we conduct evaluations against adversarial attacks, corruptions, and in-distribution. The latter pertains to predicting classification errors on authentic ImageNet inputs. Please note that error detection is inapplicable to novel classes and multi-labels where correct classifications are undefined, and we do not consider error detection on synthetic images as it lacks clear motivation.

**Existing methods:** A striking observation is the inconsistency of recent detection methods in the broad OOD setting. Methods that excel on adversarial attacks tend to underperform on multi-label detection, and vice versa. Each of the baselines exhibits subpardistribution shift, and almost all of them are Pareto-optimal. This underscores the necessity for broader OOD detection evaluations to inform the design of future methods.

We observe that while detection performances are generally superior when utilizing a ViT backbone, a finding consistent with previous studies , the difference is method-dependent. For instance, \(_{}\) ranks as the best baseline on ViT (when averaged over distribution shift types), but it is the third-worst with a ResNet-50.

We further observe that many methods significantly outperform a random choice in the detection of synthetic images, regardless of the generation methods used (see Appendix D). This suggests that despite recent advancements in generative models, the task remains feasible.

Interestingly, the performance of various methods relative to others is remarkably consistent between the DSD and error detection settings, applicable to both adversarial attacks and corruptions. This consistency implies a strong correlation between efficiently detecting OOD samples and detecting errors induced by distribution shifts, suggesting that there may not be a need to compromise one objective for the other.

**Ensemble:** Our ensemble method surpasses all baselines when averaged over distribution shift types, evident in both the DSD and error detection settings, and consistent across both ViT and ResNet-50 backbones. With the exception of error detection with ResNet-50, where Doctor-alpha yields comparable results, our ensemble method consistently demonstrates significant improvements over the best-performing baselines. Specifically, in the DSD setting, Ens-V and Ens-R secure improvements of 6.86% and 4.04% for ViT and ResNet-50, respectively.

While the ensemble detection rarely surpasses the best baselines for a specific distribution shift type, it delivers more consistent performances across types, which accounts for its superior averaged AUC. This finding endorses the viability of our approach for broad OOD detection.

Despite the notable computational overhead for Ens-V and Ens-R (up to \(13.92\) the cost of a forward pass for Ens-V with ResNet-50, as detailed in Appendix B), the inference of Ens-F atop a forward pass only adds a modest 19% to 25% overhead, thus striking a reasonable balance between cost and performance.

Interestingly, Ens-F trails only slightly in terms of performance in the DSD setting. In the error detection setting, Ens-F unexpectedly delivers the best results for both ViT and ResNet.

## 5 Related Work

In this work, we study the detection of out-of-distribution (OOD) samples with a broad definition of OOD, encompassing various types of distribution shifts. Our work intersects with the literature in OOD detection, adversarial detection, and synthetic image detection. We also provide a brief overview of uncertainty quantification methods that can be leveraged to detect errors induced by distribution shifts.

**Label-based OOD detection** has been extensively studied in recent years under different settings: anomaly detection [8; 66; 71], novelty detection [60; 67], open set recognition [58; 22; 6], and outlier detection [80; 32; 3]. Most existing methods can be categorized as either density-based [47; 38], reconstruction-based [16; 86], classification-based [81; 79] or distance-based [89; 76]. Methods can further be divided based on whether they require pre-processing of the input, specific training schemes, external data or can be used a post-processors on any trained model. See Yang et al.  for a complete survey.

**Adversarial detection** is the task of detecting adversarially perturbed inputs. Most existing methods require access to adversarial samples [2; 93; 54; 61; 55; 4; 59], with some exceptions [33; 5; 25]. Since adversarial training does not transfer well across attacks , adversarial detection methods that assume access to adversarial samples are also unlikely to generalize well. Unfortunately, Carlini and Wagner  have shown that recent detection methods can be defeated by adapting the attack's loss function. Thus, attacks targeted against the detector typically remain undetected. However, adversarial attacks transfer remarkably well across models [11; 23], which makes deployed systems vulnerable even when the attacker does not have access to the underlying model. Detectors thus make systems more robust by requiring targeted attack designs.

**Synthetic image detection** is the detection of images that have been artificially generated. Following the rapid increase in generative models' performances and popularity [68; 64; 70], many works have addressed the task of discriminating synthetic images from genuine ones . They are generally divided between image artifact detection [51; 14; 92] and data-drive approaches . Since generative models aim at learning the genuine distribution, their shortcomings only permit detection. As generative models improve, synthetic images may become indistinguishable from genuine ones.

**Uncertainty quantification** (UQ) for deep learning aims to improve the estimation of neural network confidences. Neural networks tend to be overconfident even on samples far from the training distribution . By better estimating the confidence in the network's predictions, uncertainty quantification can help detect errors induced by distribution shifts. See Abdar et al. , Kabir et al. , Ning and You  for overviews of UQ in deep learning.

**Detection of multiple types of distribution shifts** has been addressed by relatively few prior works. The closest work in the literature is probably Guille-Escuret et al.  and Lee et al.  which aims at simultaneously detecting novel classes and adversarial samples. In comparison, this work evaluates detection methods on five different types of distribution shifts. To the best of our knowledge, it is the first time that such broad OOD detection is studied in the literature.

## 6 Conclusion

We have evaluated recent OOD detection methods on BROAD, a novel diversified benchmark we introduced spanning 5 different distribution shift types, and found their performances unreliable. Due to the literature focusing on specific distribution shifts, existing methods often fail to detect samples of certain out-of-distribution shifts.

We encourage future work to consider more varied types of OOD samples for their detection evaluations, so that future methods will not see their success limited to unexpected inputs that are expected. Moreover, while setting ImageNet as an in-distribution can yield insights on a large class of models and applications, future work should consider additional in-distributions to expand BROAD's coverage, including different modalities such as text and audio.