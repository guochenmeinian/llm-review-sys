# Free-Bloom: Zero-Shot Text-to-Video Generator

with LLM Director and LDM Animator

 Hanzhuo Huang1   Yufan Feng1   Cheng Shi   Lan Xu   Jingyi Yu   Sibei Yang2

School of Information Science and Technology, ShanghaiTech University

https://github.com/SooLab/Free-Bloom

Equal contribution.Corresponding author. yangsb@shanghaitech.edu.cn

###### Abstract

Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of "moving images", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.

## 1 Introduction

Recent impressive breakthroughs  in text-to-image synthesis have been attained by training diffusion models on large-scale multimodal datasets  comprising billions of text-image pairs. The resulting images are unprecedentedly diverse and photo-realistic while maintaining coherence with the input text prompt. Nevertheless, extending this idea to text-to-video generation poses challenges as it requires substantial quantities of annotated text-video data and considerable computational resources. Instead, recent studies  introduce adapting pre-trained image diffusion models to the video domain in a data-efficient and cost-efficient manner, showing promising potential in one-shot video generation and zero-shot video editing.

This work takes the study of zero-shot text-to-video generation further, enabling the generation of diverse videos without the need for any video data. Instead of relying on a reference video for generation or editing , the proposed approach generates videos from scratch solely conditioned on the text prompt. While a concurrent work, Text2Video-Zero , similarly focuses on zero-shot video generation, our study and Text2Video-Zero differ significantly not only in terms of technical contributions but also in motivation: we aim to generate complete videos that encompass meaningful temporal variations, which distinguish it from the generation with "moving images". As shown in Figure 1, with the text prompt of "a flower is gradually blooming", our approach generates a video that thoroughly depicts the entire process, seamlessly progressing from a flower bud to thefull blooming stage. This set ours apart from other methods [17; 21; 10] that often portray a single stage of the flower bloom. In other words, our resulting video is semantic-coherent that exhibits a rich portrayal of temporal semantics.

In this work, we introduce Free-Bloom, a _zero-shot_, _training-free_, _semantic-coherent_ text-to-video generator that is capable of generating high-quality, temporally consistent, and semantically aligned videos. Our key insight is to _harness large language models (LLMs) as the director, while pre-trained image diffusion models as the frame animator_. The underlying idea is that LLMs [7; 4; 30; 45], pre-trained on massive amounts of text data, encode general world knowledge [32; 27], thus being able to transform the text prompt into a sequence of prompts that narrates the progression of events over time. Simultaneously, pre-trained image diffusion models, such as latent diffusion models (LDMs)  generate the sequential frames conditioned on the prompt sequence to create a video. Moreover, considering the temporal resolution of the prompt sequence is difficult to fine-grained each frame of a video, thereby a zero-shot frame interpolation is employed to expand the video to a higher temporal resolution. Our insights shape Free-Bloom's progressive pipeline, which consists of three sequential modules: _serial prompting_, _video generation_, and _interpolation empowerment_.

The naive attempt of directly applying LLMs and LDMs respectively to the first two steps of our pipeline resulted in failure. Not surprisingly, the resulting images are completely independent of each other. Although they each match with their own prompt, the semantic content of these images is entirely disjointed (_semantic coherence_), the overall content vary greatly including the foreground and the background (_identical coherence_), and the adjacent frames cannot be smoothly connected (_temporal coherence_). To overcome these problems, we propose a series of novel and effective technical solutions. _For semantic coherence_, we instruct LLMs in a multi-stage manner, completing the missing information in the generated prompt sequence caused by discourse cohesion, and ensuring that each prompt in the sequence accurately describes the detailed visual content while maintaining a consistent linguistic structure across prompts. _To ensure both identical coherence and temporal coherence_, we propose two innovative modifications for jointly adapting LDMs for video generation: (1) Through modeling the joint distribution of initial noise latent across frames from both unified and individual noise latent distribution, we enhance temporal and content consistency while preserving suitable levels of perturbation. Notably, in order to conform to the LDMs' assumption , our joint distribution ensures that the noise latent at every single frame follows normal Gaussian distribution

Figure 1: The zero-shot Free-Bloom generates vivid and high-quality videos that are imbued with semantic significance according to the input prompt. On top of this, Free-Bloom is compatible with the existing LDM-based extensions and can be applied to other tasks such as personalization of user-specific concepts and video prediction from a start frame.

when the noise latent of other frames is not provided. (2) Considering the trade-off between continuity and adherence to single-frame semantics, we carefully incorporate cross-frame attention , which focuses on contextual frames, into self-attention layers according to denoising time steps.

We further propose the training-free interpolation empowerment module to improve the temporal resolution of the generated videos. In order to maintain the aforementioned semantic coherence, identical coherence, and temporal coherence, we consider semantic contents of both contextual and current frames.We perform a novel dual interpolation on latent variables relying on both contextual frames and self-denoising paths, as the former path enables smooth transitions between neighboring frames and the latter path ensures high fidelity of single frame.

In summary, our contributions are multi-fold: (1) We introduce Free-Bloom, a novel pipeline to tackle the zero-shot text-to-video task. Our pipeline effectively harnesses the rich world knowledge of LLMs and the powerful generative capability of LDMs, proposing an insight into adapting text-to-image models for video generation. (2) We propose a video generation module incorporating joint noise sampling and step-aware attention shift, ensuring identical coherence and temporal coherence while expressing the semantic content. (3) We introduce a training-free dual-path interpolation strategy, ensuring consistency with context while maintaining fidelity. (4) Free-Bloom can generate impressive videos imbued with semantic significance corresponding with the contextual narrative.

## 2 Related Work

**Diffusion Models for Images.** Diffusion models  and its variants DDPM  and DDIM , have achieved breakthroughs on text-to-image generation tasks [31; 8; 35; 33]. Moreover, diffusion models have a thriving research and application ecosystem, including multiple works [22; 11; 57; 23] as well as emerging open-source communities and libraries  with frameworks and plugins.

**Open-Domain Text-to-Video Generation.** Currently, both non-diffusion-based [15; 52; 15; 47] and diffusion-based [14; 41; 59; 21; 1; 13; 10; 3] T2V methods often conduct training on large-scale video datasets such as WebVid and HD-Vila-100M[2; 55], while leveraging T2I priors or jointly training with images to maintain the quality and the content diversity. However, even with datasets of millions of videos, the quality and quantity of the training set are still not comparable to images for training.

**Zero/Few-shot Video Synthesis.** Recently, tuning pre-trained T2I models under zero-shot/few-shot settings has also been found promising for video generation. Tune-A-Video  uses a reference video, generating videos conditioned on varied prompts while maintaining the original motion. Several works [19; 40; 28; 5; 49; 54] further explore the video editing task but are not able to either change the motion or generate videos with new events. Our concurrent work is Text2Video-Zero , which firstly proposes a zero-shot T2V pipeline started from pre-trained T2I models. While it models the motion flow by adding linear transformations to latent codes, we address that complex state transitions should be considered instead.

**LLM-assisted Generations.** Large Language Models (LLMs) [7; 4; 30; 45] have made significant impact by exhibiting remarkable performance across multiple Natural Language Processing tasks. From the immense training corpus, LLMs have captured open-world knowledge in various fields [24; 4; 27; 32; 44; 58]. To assist generation by LLMs, previous methods [51; 25; 20; 16; 39] are mainly based on prompt engineering, aiming to make the text prompts more expressive toward their goals. Our methods further exploit knowledge from LLMs to derive frame visual content in a complete video.

## 3 Preliminaries

**Denoising Diffusion Probabilistic Model.** DDPM  is a type of probabilistic model that learns to approximate the probability distribution of the true data. The forward diffusion process over \(_{0},,_{T}\) gradually adds Gaussian noises in \(T\) time steps to corrupt an image. Then, the model is optimized to learn the denoising transitions \(p_{}(_{t-1}|_{t})\) in the reverse process to turn noises into images. The forward posteriors can be expressed as

\[q(_{t-1}_{t},_{0})= (_{t-1};}_{t}(_{t},_{0}),_{t})\] (1)

where \(}_{t}(_{t},_{0}):=_{t-1}}_{t}}{1-_{t}}_{0}+ _{t}}(1-_{t-1})}{1-_{t} }_{t}\) and \(_{t}:=_{t-1}}{1-_{t}}_{t}\).

**Denoosing Diffusion Implicit Models.** DDIM  is a generalized form of DDPMs by introducing the non-Markovian processes. DDIM can speed up inference by using fewer denoising steps with the same training objective of DDPMs, where the reverse process can be written as follows,

\[_{t-1}=}_{t}- }_{t}(_{t};) }{}})}_{_{0}$ denoted as $_{t}$}}+- _{t}^{2}}_{t}(_{t};) }_{_{t}}$ `` denoted as $_{t}$}}+_{t}}_{}\] (2)

where \(_{t}:=_{t}-} _{t}(_{t})}{}}\) and \(_{t}:=-_{t}^{2}}_{t }(_{t})\).

**Latent Diffusion Models.** LDMs  map data to a low-dimensional space, termed latent space, which possesses a strong representational capacity and can capture complex and abstract features. LDM employs a multi-layer attention-based U-Net architecture for noise prediction. The attention blocks  contain both self-attention and cross-attention, which focus on image content and textual content respectively. In the self-attention layer, the input feature \(_{i}\) is projected into _query_, _key_, and _value_ through linear transformations, then they are utilized to compute the attention weights.

## 4 Method

### Free-Bloom: LLMs as The Director and LDMs as The Frame Animator

We propose a zero-shot text-to-video generation pipeline named Free-Bloom, in which the generated videos are directed by LLMs and animated by LDMs. As shown in Figure 2, our pipeline consists of three sequential stages: _serial prompting_, _video generation_, and _interpolation empowerment_. Given a text prompt \(\), Free-Bloom first generates a video \(_{1}^{f 3 h w}\) with low frame rates at the first two stages, which include \(f\) frames with the frame size of \(h w\). Then, the interpolation empowerment module fills in gaps between frames to improve continuity, resulting in the final video.

At the _serial prompting_ stage, we first prompt the LLM with their general knowledge to transform the raw prompt into a series of prompts indicating the change of semantic content over time. Then, we instruct the LLM with referential resolution on the prompts to ensure that each prompt accurately describes the detailed visual content while maintaining a consistent linguistic structure across prompts, resulting in a prompt sequence, \(^{1:f}=\{^{1},,^{f}\}\). The prompt sequence accurately depicts the overall narrative and can effectively enlighten LDM  to generate semantic-coherence frames.

At the _video generation_ stage, we employ two novel modifications to LDM, enabling it to generate semantic coherent, identical coherent, and temporal coherent video frames. We first simultaneously sample the noise at every frame from their joint Gaussian distribution, which is constructed by considering a unified noise at the video level and an individual noise at the frame level, which makes

Figure 2: **Overview of Free-Bloom. Our pipeline consists of three sequential stages. In _Serial Prompting_ stage, the LLM is prompted to generate serial frame prompts. In _Video Generation_ stage, modifications are made to the LDM to generate coherent frames by joint noise sampling and step-aware attention shift. In _Interpolation Empowerment_ stage, a dual latent space interpolation conditioned on both contextual path and denoising path is proposed to generate intermediate frames.

it easier to generate coherence frames with certain variations. Then, we propose a strategy to modify the self-attention layer during the inference process. The modified attention layer adjusts attention to contextual information and self-consistent content according to the denoising time step.

At the _interpolation empowerment_ stage, a dual latent space interpolation strategy is proposed to generate the intermediate frame between two neighboring frames. In addition to jointly interpolating the latent variables of neighboring frames for temporal coherence, we condition the latent variable generated by performing DDIM  on the interpolated text embedding to ensure semantic coherence. Also, the weights of the two paths vary over the denoising time step to ensure smoothly compatible.

### Serial Prompting for Zero-Shot Text-to-Video Generation

**Prompting a Video.** When discussing the scenario of "a teddy bear jumping into the water", humans naturally imagine the series of events: the teddy bear crouches down, then propels itself into the air, and finally lands in the water with a splash. The prior knowledge of jumping water allows us humans to derive a semantic meaning of sequential events from a general description, which inspires us to ask: do LLMs also encode this kind of knowledge without further training? In our investigation of LLMs, we prompt the LLMs with instructions such as "describe each frame". We find that LLMs incorporate extensive world knowledge and can provide temporal transition knowledge.

However, the generated initial descriptions have free linguistic structure, and the text is fragmented in every single description due to discourse coherence, leading to falling short of adequately sufficient information for a single frame, as shown in Figure 2. Therefore, to ensure semantic coherence of prompts across frames, we further instruct the LLMs with the above-obtained initial descriptions to generate a sequence of \(f\) prompts \(^{1:f}=\{^{1},,^{f}\}\) with consistent linguistic structures, where each prompt accurately describes the visual content in detail.

**Zero-Shot Video Generation with LDMs.** The video generation module aims to condition the LDMs on the prompt sequence \(^{1:f}\) to generate frames that are _semantic, identical, and temporal coherence._ However, simply generating \(f\) still images from different prompts results in a collection of unrelated images that cannot be sequenced into a coherent video. To address this issue, we propose two novel modifications for adapting LDMs to generate videos rather than a collection of images without the need for additional training.

* _Joint noise sampling following LDMs' assumption._ To model the coherence across frames, we propose to sample the initial noises in the diffusion process of frames from their joint probability distribution instead of independent distribution. To construct such joint distribution, we first investigate the effects of independent noise and united noise over sequential frames: (1) The same noise for every frame results in LDMs generating a sequence of images with similar content under similar textual conditions. While this feature may contribute to achieving temporal coherence among frames, it can potentially restrict the natural variation of the subject, significantly reducing the overall video quality. (2) Generating images from \(f\) independent noises issues in maintaining consistency across frames, although the generated images with naturally varying content showcase more diversity and independently captivating elements.

These observations motivate us to construct the joint distribution by considering the independent and united distribution jointly. Specifically, we propose to obtain the initial noisy latent variable by weighted summation over a unified noise across the video frames and an individual perturbed noise to maintain consistency across frames while introducing appropriate variation. Moreover, to conform to the LDMs' assumption, we design the weighting coefficient to ensure that without giving initial noise latent at other frames, the initial noise at every single frame follows normal Gaussian distribution. Let us denote the unified video noise as \(_{T}^{*}\) and the independent noise as \(_{T}^{i}\), the joint distributions \(_{T}^{1:f}\) and \(_{T}^{1:f}\) are formulated as follows,

\[_{T}^{1:f}&=[_{T}^ {*},,_{T}^{*}]^{T},_{T}^{*}(,_{n})& p(_{T}^{1:f})= (,_{f}_{n})\\ _{T}^{1:f}&=[_{T}^{1}, ,_{T}^{f}]^{T},_{T}^{i}(, _{n})& p(_{T}^{1:f})= (,_{nf})\] (3)

where \(_{f}\) denotes the all-ones matrix with size of \(f f\) and \(\) represents the Kronecker product. Then, we model the mixture noise by

\[}_{T}^{1:f}()_{T}^ {1:f}+()_{T}^{1:f}\] (4)where the \(\) is a coefficient of variance rate. This mixed noise follows the joint distribution as

\[ p(}_{T}^{1:f})&=(,^{2}()_{n}+^{2}( )_{f}_{n}))\\ &=(,_{nf}+^{2}( )((_{f}-_{f})_{n})).\] (5)

When \(=0\), the initial noise becomes unified noise, and when \(=1\), it becomes independent noise.

* _Step-aware attention shift._ To further maintain identical coherence while ensuring semantic coherence, we shift the attention of the self-attention layers in the LDMs' U-Net from cross-frame contexts to the current frame according to the denoising time steps. For frame \(i\) with its _query_\(Q_{i}\), previous methods [53; 5; 49; 28] with one overall text prompt retrieve the _key_ and _value_ from the former frame and the first frame to perform sparse spatio-temporal attention, based on the observation  that extending the spatial self-attention across images produces consistent content.

In our scheme, frames are conditioned on different prompts, requiring that not only maintaining temporal and identical coherence, but also semantic coherence with their respective prompts. _To achieve identical and temporal coherence_, we address the former and the first frame as _contextual_ frames and attend to contextual key-value pairs. In particular, the former frame helps to enhance temporal coherence, while the first frame acts as a benchmark shape, maintaining appearance consistency throughout the video. _To align with the respective prompt in semantics_, we shift attention to the current frame itself as the time step increases, enabling the preservation of its specific characteristics and details. _In summary_, our inference strategy takes into account the time step during the diffusion process. The initial steps focus on contextual frames to form coarse-grained shapes and layouts. As we progress, complete images are generated, emphasizing producing accurate outlines conditioned on semantic information. Overall, our adaptation of the attention mechanism refers to:

\[Attention}_{i}\{  (Q_{i},[K_{0},K_{i-1},K_{i}],[V_{0},V_{i-1},V_{i}]),&  t\\ (Q_{i},K_{i},V_{i}),& t< .\] (6)

where \(\) is the threshold of the time step for attention shift. The \((K_{0},V_{0})\), \((K_{i-1},V_{i-1})\), and \((K_{i},V_{i})\) are the key-value pairs in the first, former, and current frames, respectively.

### Interpolation Empowerment

The interpolation empowerment module is proposed to further increase the frame rate without extra training resources. Similar to the insight of utilizing contextual information in the _"Step-aware attention shift"_ proposed in Section 4.2, the interpolated intermediate frames should also consider contextual frames, _i.e._, the former and the latter frames, in the denoising process. One naive approach is to directly derive the latent variable of the intermediate frame solely from the latent variables in contextual frames. However, this method overlooks the intermediate semantics indicated in text prompts in contextual frames, leading the semantic incoherence.

Therefore, we propose _a dual interpolation path_ for generating intermediate latent variables, where the contextual path interpolates the latent variables between the contextual frames to ensure temporal coherence, while the denoising path interpolates latent variables in DDIM  denoising process conditioned on interpolated text embedding to improve the semantic coherence. Specifically, to interpolate a new frame \(^{f}\) between \(^{f-1}\) and \(^{f+1}\), we first sample the intermediate initial latent variable from the same distribution proposed in Section 4.2. For the conditional textual prompt \(^{f}\), we directly interpolate text embeddings of the previous and next frames.

1) _Contextual path._ To obtain the context-sensitive latent variable \(}_{t}^{f}\), we perform linear interpolation between the contextual frames \(_{t}^{f-1}\) and \(_{t}^{f+1}\) as follows,

\[}_{t}^{f}=k_{t}^{f-1}+(1-k)_{t}^{f+1}.\] (7)

2) _Denoising path._ Then, we perform a linear interpolation between the context-sensitive latent variable \(}_{t}^{f}\) and the latent variable obtained from DDIM conditioned on the interpolated prompt \(^{f}\). We employ \(m(t)\) as the interpolation coefficient and use the notation of \(_{t+1}^{f}\) and \(_{t+1}^{f}\) mentioned in Section 3. The interpolation coefficient \(m(t)\) varies according to the time step, with smaller values in the earlier denoising steps and increase in the latter steps. The interpolation is formulated as follows,

\[_{t}^{f}=(1-m(t))}_{t}^{f}+m(t)(} _{t+1}^{f}+_{t+1}^{f}+_{t}_{t}).\] (8)To provide an intuitive conditional probability distribution of \(x_{t}^{f}\), we present the distributions for the cases when \(m(t)=1\) and \(m(t)=0\) respectively as:

\[p(_{t}^{f}|_{t+1}^{f})=(k_{t +1}^{f-1}+(1-k)_{t+1}^{f+1},[k 1-k]\,_{t+1}^{f-1,f+1} \,k\\ 1-k),&m=0\\ (_{t+1}^{f},_{t+1}^{f}),&m=1\] (9)

where \(_{t+1}^{f-1,f+1}\) is the covariance matrix of \(_{t+1}^{f-1}\) and \(_{t+1}^{f+1}\).

### Extensions

Our method exhibits high extensibility and seamless integration with existing LDM-based methods. We demonstrate the versatility of our approach through some applications: (1) **Personalization.** By combining with LDM-based personalization methods [9; 34], our approach can generate videos with user-provided concepts. Here we choose DreamBooth  as an example, as shown in the top Figure 1, our method generates a video of "A prince is riding a horse [V]" with the concept [V] as "modern Disney style". (2) **Making an image move.** With the help of DDIM inversion , we can inverse the latent variable from an image, then generate a sequence of frames that continues the image. For example, starting from a still image of the scenery of a river, our method further generates a video depicting the process of freezing, as illustrated in Figure 1.

## 5 Experiments

**Implementation Settings.** We develop our Free-Bloom based on LLM as ChatGPT  and LDM as Stable Diffusion  with its pre-trained v1.5 weights. We first generate a video of \(f=6\) length with \(512 512\) resolution, then we iteratively interpolate between the most distinguished frames with \(k=0.5\) for the first interpolated frame. For \(m(t)\), we set \(m(t)=0.1\) when the denoising time \(t^{*}\) and \(m(t)=1\) when \(t<^{*}\). Notice that our method actually can be adapted to generate longer videos. To enhance image quality in practice, we add fixed negative prompts and upscale our resulting frames with an image super-resolution network ESRGAN . All experiments are performed on a single NVIDIA GeForce RTX 3090Ti.

### Baseline Comparisons

**Qualitative Results.** We showcase examples of Free-Bloom in Figure 3 and compare them with that generated by the zero-shot T2V-Zero  and VideoFusion , which demonstrate the most outstanding overall performance in the user study. More comparisons are included in the Supplementary. Video generation shown in region A demonstrates the following observations: (1) Our method vividly depicts the complete imagery of a volcanic eruption or the sequential motion of a teddy bear jumping into the water, exhibiting the capacity to generate semantic meaningful frame sequences, in which the visual elements, actions, and events are aligned with the input prompt as well as the contextual narrative. (2) In addition, our method shows temporal coherence and identical coherence while maintaining high fidelity for single frames. (3) Although the T2V-Zero method maintains overall content consistency between frames, it fails to depict sequential events. Furthermore, the subject would easily be distorted as the length of the video increases. (4) VideoFusion, on the other hand, demonstrates impressive temporal coherency between frames as it is trained on large-scale datasets, and it also presents a certain level of grasp of events. However, this training on the vast video dataset also significantly degrades the fidelity and quality of individual frames.

For interpolation results shown in region B, we present one latest state-of-art Video Frame Interpolation method AMT  pre-trained on Vimeo90K  for comparison. AMT fails to comprehend the subject information between the two target frames. As a result, it blurs the different parts of the teddy bear's body, failing to capture the intermediate motion. In contrast, our method fills in the content gap with a serial of continuous motion of the bear from the air to the water, maintaining fidelity in the intermediate frames while ensuring content consistency.

**Quantitative Results** are reported with automatic metrics and the user study in Table 1. We adopt three publicly available diffusion-based methods, Text2Video-Zero , VideoFusion , and LVDM  as baselines. VideoFusion and LVDM are both trained methods while the former is trained on both large-scale image datasets ImageNet , LAION5B  and a large-scale video dataset WebVid-10M , and the latter has been trained on a 2-million subset of the WebVid-10M.

For automatic metrics, we use CLIP  to evaluate the similarity correlation between the input prompts and the visual content of generated frames. Recall that our method put an emphasis on the overall narrative semantic coherence, therefore we also compute the CLIP score between each frame and its corresponding prompt generated by LLM(*). We can observe that although comparing each frame against the input prompt overlooks the potential of our approach, our method demonstrates good performance when frames are compared with the frame-level video prompts.

For the user study, the participants are instructed to rate the fidelity, temporal coherence, and semantic coherence on a scale of 1 to 5 and give a comprehensive ranking. According to the user study, although our method may not perform as well as trained methods in terms of temporal continuity, it has received high recognition in all other dimensions of video quality.

### Analysis of Our Pipeline

In Figure 4 and Figure 5, we conduct a comprehensive analysis of the modules in our proposed pipeline. The top row showcases our final results.

    & & Automatic Metric &  \\  Method & Training-Free & CLIP Metrics\(\) & Fidelity \(\) & Temporal \(\) & Semantic \(\) & Rank \(\) \\  VideoFusion  & & 0.483 & 3.436 & 3.889 & 3.267 & 2.317 \\ LVDM  & & 0.480 & 3.289 & 3.650 & 3.242 & 2.567 \\ T2V-Zero  & ✓ & 0.479 & 3.486 & 2.783 & 3.025 & 3.033 \\ Ours & ✓ & 0.477 / 0.482* & 4.133 & 3.267 & 3.867 & 2.083 \\   

Table 1: **Quantitative Results.** * for CLIP score on serial prompts.

Figure 3: **Qualitative comparison.** A) Our method can generate semantic meaningful frame sequences when conditioned on the same prompts. B) We interpolate two frames into the frame sequence by taking the two images within the same color box in part A as the start and the end.

_A) Without joint noise sampling._ In A, we replace the joint noise initialization with the independent initialization, resulting in frame sequences with inconsistent image content. _B) Without shifting self-attention._ In B, we denoise the frames from the proposed joint noise initialization but use the raw LDM without modifying self-attention layers. The frames are similar naturally in some cases, as the joint noise contains a portion of unified noise. However, without attention to contextual frames, it is difficult to maintain identical coherence of both the foreground and background content, let alone temporal continuity. _C) Serial Prompting for Text2Video-Zero_. In C, we adapt Text2Video-Zero  to enable the input of frame-level video prompts so that each frame is generated and conditioned on a distinct prompt. The results show that it is challenging for the current method to comprehend serial prompts effectively, resulting in "moving images". _D) Without attention to the current frame itself._ In D, we replace all self-attention layers with spatio-temporal attention layers proposed in TAV . The resulting frames exhibit improved temporal coherence, demonstrating smoother transitions between frames. However, the frames appear almost identical, creating a sequence of meaningless and jitter frames based solely on the first frame, which does not align with the intended temporal semantics. Additionally, prolonged contextual attention in long sequences can significantly compromise the fidelity of individual frames. As shown in the case of Iron Man, the last frame presents an incomplete leg. _E) Without the step-aware shift strategy._ Based on D, we further concatenate the attention to the current frame with contextual attention but without shifting it along time steps. Although the results remain inconsistent with semantics due to the absence of the inference strategy that varies across the denoising time step, the fidelity of images is improved.

**Dual-path interpolation.** We show the influence of balancing timing coefficients between the dual interpolation path in Figure 5. When \(^{*}\) approaches 1, \(m(t)\) remains relatively small throughout the entire denoising process, indicating that the contextual path primarily determines the interpolated results. The resulting image presents blurring and ghosting artifacts, failing to generate a frame with correct semantics. Conversely, heavily relying on the DDIM denoising path with larger values of \(m(t)\) causes substantial deviations in content compared to the contextual frames. Our method adopts smaller values of \(m(t)\) in the early steps to prioritize coarse-grained shapes and layout and increases \(m(t)\) in the latter steps to improve temporal consistency. As a result, we achieve the benefits of both paths by appropriately setting \(m(t)\).

Figure 4: Analyze for the effects of Free-Bloom (A) Without joint noise sampling, (B) Without shifting self-attention, (C) Serial Prompting for Text2Video-Zero, (D) Without attention to the current frame itself, and (E) Without the step-aware shift strategy.

Figure 5: Analyze for interpolation.

Conclusion

In this paper, we devise a novel zero-shot and training-free text-to-video approach, which mainly focuses on improving the narrative of the progression of events. Our proposed pipeline effectively harnesses the knowledge from the pre-trained LLM and LDM and produces highly semantic coherent videos while also maintaining temporal coherence and identical coherence. **Limitation**: we look forward to further research on text-to-video generation, however, it should be acknowledged that there can be ethical impacts like other generative models. As we adopt ChatGPT and Stable Diffusion v1.5, our method may inherit the bias of those two models.

**Acknowledgment:** This work was supported by the National Natural Science Foundation of China (No.62206174), Shanghai Pujiang Program (No.21PJ1410900), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShanghaiAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), and Shanghai Engineering Research Center of Intelligent Vision and Imaging. Also, we extend our heartfelt gratitude to the anonymous users who participated in our user study.