# Implicit Causal Representation Learning via

Switchable Mechanisms

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicit learning of causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. However, the subtlety of soft interventions impose several challenges for learning causal models. One challenge is that soft intervention's effects are ambiguous, since parental relations remain intact. In this paper, we tackle the challenges of learning causal models using soft interventions while retaining implicit modeling. Our approach models the effects of soft interventions by employing a _causal mechanism switch variable_ designed to toggle between different causal mechanisms. In our experiments, we consistently observe improved learning of identifiable, causal representations, compared to baseline approaches.

## 1 Introduction

One of the long-standing challenges in causal representation learning is how to recover the ground-truth causal graph of a system solely from observations. Termed the _identifiability of causal models_ problem, this endeavor is crucial. Without achieving identifiability, we risk erroneously attributing causal relationships to learned representations. Furthermore, statistical models can masquerade as Directed Acyclic Graphs (DAGs) where edges lack causal significance, further complicating our pursuit.

When considering the challenge of identifying causal models, it is known that the Markov condition in graphs is insufficient for this task . Thus, without additional assumptions or data, we find ourselves limited to learning only a _Markov Equivalence Class_ (MEC) of the causal model. Existing works have made different assumptions about availability of ground-truth causal variables labels , model parameters , availability of paired interventional data , and availability of intervention targets  to ensure identifiability of causal models.

Figure 1: Difference between hard interventions and soft interventions: As seen in the middle row, hard interventions sever connections with parents. Therefore, an object’s class cannot have any effect on the object’s color when we intervene on color. On the other hand, soft interventions, as shown in the bottom row, allow for such effects.

Interventional data are usually obtained through _soft_ or _hard_ interventions. Hard interventions usually involve controlled experiments and they severe the connection of an intervened variable with its parents . In terms of Structural Causal Models (SCM), hard interventions set the causal mechanism relating a causal variable to its parents, to a constant. Due to ethical or safety reasons, it may not be possible to perform hard interventions in many real-world applications. On the other hand, the effects of soft interventions are more subtle since parent variables can still affect their children. These effects can be modeled by a change in the set of parents, the causal mechanisms, and the exogenous variables . Consequently, hard interventions can also be seen as a special case of soft interventions where the causal mechanism is set to a constant. Illustrated in Figure 1, a prominent challenge in causal representation learning lies in dealing with the ambiguity surrounding the effects of soft interventions. The observed alterations in object colors fail to distinctly elucidate whether they stem from parental influences or the applied interventions.

Additionally, a lack of comprehension regarding causal graphs can pose significant challenges in causal representation learning. In certain applications, the causal graph can be constructed using domain knowledge, allowing us to subsequently learn the causal variables [2; 18; 20]. However, this is not universally applicable, necessitating the direct learning of the causal graph itself. In a Variational AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning: Explicit Latent Causal Models (ELCMs) [34; 1; 35; 37; 17; 15] and Implicit Latent Causal Models (ILCMs) . In ELCMs, the latents are the causal variables and the adjacency matrix of the causal graph is parameterized and integrated into the prior of the latents such that the prior of latents is factorized according to the Causal Markov Condition . This approach to causal representation learning is highly susceptible to becoming stuck in local minima as it is hard to learn representations without knowing the graph, and it is hard to learn the graph without knowing the representations. ILCMs  were introduced to circumvent this "chicken-and-egg" problem by using _solution functions_, which can implicitly model edges in the causal graph rather than explicitly modeling the entire adjacency matrix of the causal model. In ILCMs _the latents are the exogenous variables_ and the there is no explicit parameterization for the graph.

In implicit causal representation learning, the task involves recovering the exogenous variables \(\) from observed variables \(\) and learning solution functions. In , interventions are assumed to be hard, but this is often unrealistic and does not align with real-world problems. **In this paper, we propose a novel approach for Implicit Causal Representation Learning via Switchable Mechanisms (ICRL-SM).** We will introduce the _causal mechanism switch variable_ as a way of modeling the effect of soft interventions and identifying the causal variables. Our experiments on both synthetic and large real-world datasets, highlight the efficacy of proposed method in identifying causal variables and promising future directions in implicit causal representation learning. Our key contributions can be summarized as follows:

**I.** A novel approach for implicit causal representation learning with soft interventions.

**II.** Employing causal mechanisms switch variable to model the effect of soft interventions.

**III.** Theory for identifiability up to reparameterization from soft interventions.

## 2 Related Work

Causal representation learning has recently garnered significant attention [27; 14]. The primary challenge in this problem lies in achieving identifiability beyond the Markov equivalence class . Solely relying on observational data necessitates additional assumptions regarding causal mechanisms, decoders, latent structure, and the availability of interventional data [22; 28; 36; 25; 15; 1; 40; 13; 34]. Recent works have focused on identifying causal models from collected interventional data instead of making strong assumptions about functions of the causal model. Interventional data facilitates identifiability based on relatively weak assumptions [1; 6; 3; 39; 33]. This type of data can be further categorized based on whether it involves soft or hard interventions, and whether the manipulated variables are observed and specified or latent. Our focus in this paper is on examining soft interventions, encompassing both observed and unobserved variables.

### Explicit models vs. Implicit models

Table 1 presents a comparison of the assumptions and identifiability results between our proposed theory and other related works on causal representation learning with interventions. In causal representation learning with interventions, one approach assumes a given causal graph and concentrates on identifying causal mechanisms and mixing functions. For instance, Causal Component Analysis (CauCA)  explores soft interventions with a known graph. Alternatively, when the graph is

not provided, explicit models seek to reconstruct it from interventional data [6; 17], potentially resulting in a chicken-and-egg problem in causal representation learning . Current methods face the challenge of simultaneously learning the causal graph and other network parameters, especially in the absence of information about causal variables or the graph. Addressing these challenges,  recently introduced ILCM, which performs _implicit_ causal representation learning exclusively using _hard_ intervention data. In contrast, our approach introduces a novel method for learning an implicit model from _soft_ interventions.  describes methods for extracting a causal graph from a learned implicit model, which could be applied to our method as well. In our experiments, we will compare our method with ILCM and dVAE , given their implicit nature and similar experimental settings and assumptions. Additionally, to showcase the superiority of our method over explicit models, we will employ explicit causal model discovery methods like ENCO  and DDS , in conjunction with various variants of \(\)-VAE.

### Hard interventions vs Soft interventions

The identification of explicit causal models from hard interventions has been extensively explored.  investigate causal disentanglement in linear causal models with linear mixing functions under hard interventions. Similarly,  focus on identifying causal models with linear causal mechanisms and nonlinear mixing functions, also utilizing hard interventions. In a more general setting with non-parametric causal mechanisms and mixing functions,  examine the identifiability of causal models, utilizing multi-environment data from unknown interventions. Similarly,  explore identifiability of causal models using multi-environment data from unknown interventions.  investigate the identifiability of causal models with nonlinear causal mechanisms and linear mixing functions, considering both hard and soft interventions.

Recent work has expanded the concept of explicit hard interventions to include soft interventions. In their study,  address the identification of causal models from soft interventions, leveraging the sparsity of the adjacency matrix as an inductive bias. However, when dealing with implicit models, soft interventions introduce new complexities. Identifiability becomes more challenging, as the causal effect of variables on observed variables is less apparent. This ambiguity arises from the dual possibility of effects originating from interventions or influences from parent variables on the causal variables. Moreover, in scenarios where implicit modeling is retained, the absence of knowledge about parent variables further complicates identifiability. While  theoretically establishes identifiability for hard interventions, practical experiments involving complex causal models with over 10 variables reveal increased ambiguity and confounding factors. Consequently, model identification becomes less straightforward.

## 3 Methodology

### Data Generating Process

A structural causal model (Definition A.1.1) is used to understand and describe the relationships between different variables and how they influence each other through causal mechanisms. A **decoder function**, \(g()=\), maps a vector of causal values \(z\) to observed values \(x\). The causal variables \(\) are unobserved and the goal is to infer them from interventional data. For each causal variable, a **diffeomorphic solution function**, \(s_{i}:_{i}_{i}\), deterministically maps a value for exogenous variable \(_{i}\) to a value for causal variable \(_{i}\). _In implicit modeling, we learn the solution functions \(s_{i}\) directly,_ rather than defining them through local mechanisms \(f_{i}\). We write \(\) for the set of all solution functions \(s_{i}\), so \(:\).

Identifying causal models from data can be complex and is often studied within classes of models such as those identifiable up to affine transformations. For example, in the context of nonlinear _Independent Component Analysis (ICA)_, the generative process also involves a mixture function \(g\) of latent causal variables \(^{n}\), resulting in observations \(^{n}\)[41; 15]. However, a significant distinction between causal representation learning and nonlinear-ICA is that in the former, the causal

  
**Methods** & **Causal Mechanisms** & **Missing functions** & **Interventions** & **Explicit/Implicit** & **Identifiability** \\  CausalDisexpancy  & Nonlinear & Full row rank polynomial & Soft & Explicit & Permutation and Affine \\ Conc(A ) & Nonlinear & Diffeomorphism & Soft & Explicit & Different based on assumptions \\ Linear-CD  & Linear & Linear & Hard & Explicit & Permutation \\ Scale- & Nonlinear & Linear & Hard/Soft & Explicit & Scale/Mixed \\ ILCM  & Nonlinear & Diffeomorphism & Hard & Implicit & Permutation and reparameterization \\ dVAE  & Nonlinear & Diffeomorphism & Hard & Implicit & Permutation and reparameterization \\ ICRL-SM (ours) & Nonlinear & Diffeomorphism & Soft & Implicit & Repparameterization \\   

Table 1: Comparison of proposed method with other recent related work on causal learning from interventional data variables \(\) may have complex dependencies. Our objective in this paper is to recover \(\) from \(\) and eventually map \(\) to \(\) using solution functions.

Identifying a causal model from observational data is not trivial and requires assumptions on the parameters of the model . Adding information about interventions in addition to observations, helps to identify causal variables by exhibiting the effect of changing a causal variable on the observed variables. An interventional data point \((x,,i)\) includes the pre-intervention observation \(x\), the post-intervention observation \(\), and intervention target \(i\) where \(\) is the set of intervention targets selected from the causal variables. The post-intervention data \(\) is generated by a _soft intervention_ that targets one of the causal variables in \(\). To achieve identifiability up to reparametrization, we rely on a series of assumptions within the data generation process, outlined as follows:

**Assumption 3.1**.: _(Data generating assumptions)_

_1. Atomic Interventions: For every sample \((x,,i)\), only one causal variable is targeted by an intervention._

_2. Known Targets: Targets of soft interventions are known._

_3. Post-intervention Exogenous Variables: The exogenous variables' values change only for the corresponding intervened causal variable, while the others maintain their pre-intervention values, thus \(e_{i}_{i}\) if \(i\),and \(e_{i}=_{i}\) otherwise._

_4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability . These interventions should modify causal mechanisms to ensure non-overlapping conditional distributions of causal variables (refer to Figure 1)._

_5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss and avoids abrupt changes in the function's image._

The **known targets** assumption can be relaxed in applications where such data is not available and the same procedure in  can be used to infer the intervention targets. In fact, in our real-world experiments, intervention targets are not available and based on the nature of the datasets, we hypothesize our causal variables to be object attributes and actions to be intervention targets.

### Causal Mechanisms Switch Variable

The major difference of soft intervention with hard intervention is that post-intervention causal variable \(}_{i}\) is no longer disconnected from its parents and its causal mechanism \(_{i}\) is affected by the intervention. This is why identifying the causal mechanisms is more difficult for soft interventions. Soft intervention data yield fewer constraints on the causal graph structure than hard intervention data. For more details refer to string diagrams of soft and hard interventions depicted in Figure 1(b). Figure 1(b) shows our main generative model. It includes a data augmentation step that adds the intervention displacement \(-x\) as an observed feature that directly represents the effect of a soft intervention in observation space.

**Augmented implicit causal model** To model the effect of soft interventions, we introduce the causal mechanism switch variable \(\). By leveraging \(\), we can effectively switch to the pre-intervention causal mechanisms within post-intervention data. This facilitates the model's ability to solely focus on discerning alterations in the intrinsic characteristics of each causal variable. These changes are encapsulated within their respective exogenous variables, aiding the model in learning the causal relationships more accurately. We propose to use a modulated form of \(\) to model the soft intervention effects on each causal variable as an additive effect with a nonlinear function \(h_{i}\) such that \( i,\ }_{i}=_{i}(}_{i}; }_{/i})=s_{i}(}_{i};_{/i},h_{i }())\). As the parental set for each causal variable is not known, we have to use a modulated form of \(\) in every causal variable's solution function and the inclusion of \(h_{i}()\) enables the model to encompass variations in the parental sets of all causal variables in \(\). Therefore, there is a switch variable \(_{i}\) for each causal variable \(_{i}\). Adding switch variables to solution functions leads to the concept of an _augmented implicit causal model_.

**Definition 3.2**.: _(Augmented Implicit Causal Models) An Augmented Implicit Causal Models (AICMs) is defined as \(=(,,,)\) where \(^{n}\) is the causal mechanism switch variable which models the effect of soft interventions on solution functions \(\):_

\[ i,\ }_{i}=_{i}(}_{i}; }_{/i})=s_{i}(}_{i};_{/i},h_{i }()),\] (1)

_where \(}\) is the new solution function resulting from the soft intervention, \(}_{/i}\) is the altered set of all exogenous variables except \(i\), including the ancestral exogenous variables, due to intervention, and \(}_{i}\) is the post-intervention exogenous variable._The usage of \(\) in soft interventions is analogous to augmented networks in  which were mainly designed for hard interventions. Pearl  even foresaw this possibility by saying: "One advantage of the augmented network representation is that it is applicable to any change in the functional relationship \(f_{i}\) and not merely to the replacement of \(f_{i}\) by a constant."

By using Taylor's expansion, we can expand the solution functions as follows:

\[s_{i}(}_{i};_{/i},h_{i}())=s_{i}( }_{i};_{/i},h_{i}(v_{0}))+_{n=1}^{} |s_{k}}{ h_{i}^{k}}|_{h_{i}=h _{i}(v_{0})}(h_{i}()-h_{i}(v_{0}))^{n})=s_{i}(}_{i};_{/i},h_{i}(v_{0}))+R_{i}\] (2)

where we'll use \(R_{i}\) as a short-hand for Equation 2. We define the **separable dependence** property for solution functions as \( h_{i}(v_{0}):s_{i}(}_{i};_{/i},h_{i}(v_{ 0}))=s_{i}(}_{i};_{/i})\). An example of such a scenario could be in location-scale noise models such as, \(s_{i}(_{i};e_{/i},h_{i}(v))=_{i}+loc(e_{/i})+h_{i}(v)= {e_{i}}+loc(e_{/i})+v^{2}+v\) where \(v_{0}\) would be zero. By assuming the separable dependence property, we can write the solution function in Equation 2 as:

\[s_{i}(}_{i};_{/i},h_{i}())=s_{i}( }_{i};_{/i})+R_{i}=s_{i}(}_{i} ;_{/i})+\] (3)

As a result, we can switch to pre-intervention solution functions. Subsequently, by modeling soft intervention effects using \(h_{i}()\), we can recover pre-intervention solution functions. During inference, we simply disregard the \(h_{i}()\) term in the solution functions. Nonetheless, it is possible to train the prior \(p()\) to ensure that the separable dependence property is maintained for pre-intervention data.

**Observability of switch variable** The intuition behind using \(\) is to separate the effect of soft intervention on \(}_{i}\) into two: (1) The effect on causal mechanisms and parents, and (2) The effect on exogenous variable \(_{i}\). For example, we can say that causal variables in images of objects are the objects' attributes such as shape, color, and size, and performing actions like "Fold" change these attributes. Furthermore, it can be asserted that the camera angle within a given image may influence the shape of the object. If the images were generated from a hard intervention, the camera angle remains fixed between pre and post intervention. However, the camera angle changes along with the performed actions indicating that the interventions are soft. In this case, if we had a knowledge of how the camera angle affects the attributes of objects, then we could separate the effect of soft intervention. In other words, if \(\) is observed, then we can extract the effect of the intervention that we are interested in (i.e., the effect on the causal variable itself). For more details, refer to Figure A4.

Lacking an understanding of how soft intervention influences the causal model, a more complex model becomes necessary. Consequently, the term \(R_{i}\) in Equation 2 would involve a higher order of \(h_{i}()\). Therefore, we assume the observability of \(\):

**Assumption 3.3**.: _(Observability of \(\)) Given an intervention sample \((x,,i)\) and linear decoders, we can approximate the soft intervention effects \(h_{i}()\) as follows:_

\[-z= e_{i}+R,-x=g( )-g(z) g(-z)=g( e_{i}+R),\]

_where \(R=[R_{0},R_{1},...,R_{n}]\) and \(n\) is the number of causal variables. \(R\) and \( e_{i}\) are the vectors indicating the soft intervention effects and change in effect of the exogenous variable of the intervened causal variable, respectively. Note that elements of \(R\) will be all zero except for the intervened causal variable. Consequently, with linear mixing functions and some pre-processing on observed samples (here subtraction), we can observe \(R_{i}\)._

Our synthetic data is generated using a linear decoder, however, the decoder for the real-world datasets is not necessarily linear. Therefore, we do not observe \(\) from \(-x\) in the real-world dataset. Nevertheless, our findings suggest that incorporating soft interventions through \(\) leads to superior performance compared to other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the generative system of the dataset would result in improved outcomes.

### Identifiability Theorem for Implicit SCMs with Soft Interventions

In this paper, our focus lies in identifying the causal variables up to reparameterization through soft interventions. We first define identifiability up to reparameterization (Definition 3.4) and subsequently introduce the identifiability theorem 3.5. The proof of theorem is extensive and is available in full in Appendix A1.

We establish identifiability up to reparameterization, allowing for the mapping of causal variables \(\) and \(^{}\) between two Latent Causal Models (\(\) and \(^{}\)) through component-wise transformations(Definition A1.2). Given our implicit modeling approach, lacking knowledge of the causal graph, we include all exogenous variables in the solution functions, as depicted in Equation 1. Notably, **the causal graph remains unaltered during learning**. To illustrate, we contrast hard interventions, which neglect parent influences, with soft interventions that acknowledge parental effects in a simple example. Consider a basic causal model \(Z_{1} Z_{2}\) alongside a location-scale noise model  for the solution function, given by \(_{2}=_{2}-(e_{1})}{scale(e_{1})}\). The distribution \(p(}_{2})\) mean is \()}(}_{2})-(e_{1})}{scale(e_{1})}\) In the context of hard interventions, we can assume \(p(}_{2}|_{1})=p(}_{2})=N(0,1)\) as there are no parental effects. Consequently, the location and scale networks within the solution function tend to dampen parental effects, given the absence of parental influence in the ground-truth data. Contrarily, soft interventions exhibit parental influence in the ground-truth data, thus \(p(}_{2}|_{1}) N(0,1)\). Due to the lack of parental knowledge in implicit modeling, we model \(p(}_{2}|_{1})=p(}_{2}|_{2})\), as \(_{2}\) is a known parent of \(}_{2}\). Consequently, parental effects are propagated to \(_{i}\) (the corresponding exogenous variable of each causal variable), violating identifiability up to reparameterization. By leveraging \(\), we allow parental effects to propagate to \(\) instead of \(_{i}\).

**Definition 3.4**.: _(Equivalence up to component-wise reparameterization) Let \(=(,,g,)\) and \(^{}=(^{},,g^{},)\) be two Latent Causal Models (LCM) based on AICMs \(,^{}\) with shared observation space \(\), shared intervention targets \(\), and respective decoders \(g\) and \(g^{}\). We say that \(\) and \(^{}\) are equivalent up to component-wise reparameterization \(_{r}^{}\) if there exists a component-wise transformation (Definition A1.2) \(_{}\) from the causal variables \(\) to the causal variables \(^{}\) and a component-wise transformation \(_{}\) between \(\) and \(^{}\) such that:_

_1. Indices are preserved (i.e., \(_{i}(z_{i})=z_{i}^{}\) and \(_{i}(e_{i})=e_{i}^{}\)). Corresponding edges are preserved (i.e., \(_{i}_{j}\) holds in \(\) iff \(_{i}^{}_{j}^{}\) holds in \(^{}\). Edges \(_{i}_{i}\) should be preserved as well.)_

_2. The exogenous transformation preserves the probability measure on exogenous variables \(p_{^{}}=(_{})_{*}p_{}\) (Definition A1.4)._

**3. The causal transformation preserves the probability measure on causal variables \(p_{^{}}=(_{})_{*}p_{}\) (Definition A1.4)._**

**Theorem 3.5**.: _(Identifiability of latent causal models.) Let \(=(,,g,)\) and \(^{}=(^{},,g^{},)\) be two LCMs with shared observation space \(\) and shared intervention targets \(\). Suppose the following conditions are satisfied:_

_1. Data generating assumptions explained in Assumption 3.1._

_2. Soft interventions satisfy Assumption 3.3._

_3. The causal and exogenous variables are real-valued._

_4. The causal and exogenous variables follow a multivariate normal distribution._

_Then the following statements are equivalent:_

_Two LCMs \(\) and \(^{}\) assign the same likelihood to interventional and observational data i.e., \(p_{}^{,}(x,,i)=p_{^{ }}^{,}(x,,i)\)._

_- \(\) and \(^{}\) are disentangled, that is \(_{r}^{}\) according to Definition 3.4._

### Training Objective

Consequently, there will be three latent variables in ICRL-SM:

**1.** A causal mechanism switch variable \(\).

**2.** The pre-intervention exogenous variables \(\).

**3.** The post-intervention exogenous variables \(}\).

As the data log-likelihood \( p(x,,x-) p(x,)\) is intractable, we utilize an ELBO approximation as training objective:

\[ p(x,)& E_{g(e,,|e,|,)} p (x,|e,,v)-KLD(q(e,,v|x,)\|p(e, ,x))\\ &=E_{q(v|-) q(e|x) q(e|x)} p (x|e)p(|)p(-x|v)-KLD(q(v|-x) q (e|x) q(e|)||p(e|e,v)p(v)p(e)).\] (4)

The observations are encoded and decoded independently. The KLD term regularizes the encodings to share the latent _intervention model_\(p(|e,v)p(v)p(e)\) that is shared across all data points. The components of this model can be interpreted as follows:

**1.**\(p(e)\) is the prior distribution over exogenous variables \(e\).

**2.**\(p(v)\) is the prior distribution over switch variables \(v\).

**3.**\(p(|e,v)\) is a transition model that shows how the exogeneous variables change as a function of the intervention.

We factorize the posterior with a mean-field approximation \(q(v,e,|x,)=q(v|-x) q(e|x) q(| )\) and, following our data generation model (Figure 1(b)), the reconstruction probability as \(p(x,|e,,v)=p(x|e)p(|)p(-x|v)\). The prior over latent variables is factorized as \(p(,e,v)=p(|e,v)p(v)p(e)\)(Figure 1(b)). Pre-intervention exogenous variables are mutually independent, hence, \(p(e)=_{i}p(e_{i})\) and \(p(v)=_{i}p(v_{i})\). We assume \(p(e_{i})\) and \(p(v_{i})\) to be standard Gaussian. Furthermore, as we assume \(e_{i}=_{i}\) for all non-intervened variables, the \(p(|e,v)\) will be as follows:

\[p(|e,v)=_{i I}(_{i}-e_{i})_{i I}p( _{i}|e,v)=_{i I}(_{i}-e_{i})_{i I}p( _{i}|e_{i})|_{i}}{_{i}}|\] (5)

The last equality is obtained from the Change of Variable Rule in probability theory, applied to the solution function \(_{i}=s_{i}(_{i};e_{/i},h_{i}(v))\). Furthermore, we write \(p(_{i}|e,v)=p(_{i}|e_{i})\) since only \(e_{i}\) is a known parent of \(_{i}\) in implicit modeling. We assume \(p(_{i}|e_{i})\) to be a Gaussian whose mean is determined by \(e_{i}\). We implement the solution function using a location-scale noise models  as also practiced in , which defines an invertible diffeomorphism. For simplicity, in our experiments, we are only going to change the \(loc\) network in post-intervention. Therefore, \(h_{i}(v)\) will be used as:

\[_{i}=_{i}(_{i};e_{/i},h_{i}(v))=_{i }-(loc_{i}(e_{/i})+h_{i}(v))}{scale_{i}(e_{/i})},\] (6)

where \(loc_{i}:^{n-1}\) and \(scale_{i}:^{n-1}\) are fully connected networks calculating the first and second moments, respectively. The general overview of the model is illustrated in Figure 1(a).

## 4 Experiments and Results

The experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement to identify the true causal graph from pairs of observations \((x,,i)\), and (2) Action Inference to make supervised inferences about actions generated from the post-intervention samples using information about the values of the manipulated causal variables. Moreover, we conducted additional experiments designed as an ablation study, the results of which are presented in A4. All models are trained using the same setting and data with known intervention targets.

### Datasets

**Synthetic Dataset** We generate simple synthetic datasets with \(==^{n}\). For each value of \(n\), we generate ten random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoulli distribution with probability 0.5. The pre-intervention and post-intervention causal variables are obtained as:

\[z_{i}=scale(z_{pa_{i}})e_{i}+loc(z_{pa_{i}})}_{i}=scale(z_{pa_{i}})}+(z_{pa_{i}}),\] (7)

where the \(loc\) and \(scale\) networks are changed in post intervention. The pre-intervention \(loc\) and post-intervention \(\) network weights are initialized with samples drawn from \((0,1)\) and \((3,1)\), respectively. The \(scale\) is constant 1 for both pre-intervention and post-intervention samples. Both \(e_{i}\) and \(}\) are sampled from a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled \(SO(n)\) rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples, and 10,000 test samples.

Action DatasetsCausal-Triplet datasets tailored for _actionable_ counterfactuals  feature paired images where several global scene properties may vary including camera view and object occlusions. Thus, the images can be viewed as outcomes of soft interventions, wherein actions affect objects alongside subtle alterations. These datasets  consist of: images obtained from a photo-realistic simulator of embodied agents, ProcTHOR , and the other contains images repurposed from a real-world video dataset of human-object interactions . The former one contains 100 k images in which 7 types of actions manipulate 24 types of objects in 10 k distinct ProcTHOR indoor environments. The latter consists of 2,632 image pairs, collected under a similar setup from the Epic-Kitchens dataset with 97 actions manipulating 277 objects.Based on the nature of actions in this dataset, the causal variables should represent attributes of objects such as shape and color. As the dataset consists of images we train all the methods with ResNet encoder and decoder. For the ProcThor dataset the number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from the dataset as 97 causal variables will be too complex in a VAE setup.

### Metrics

For the causal disentanglement task, we are going to use the DCI scores . Causal disentanglement score quantifies the degree to which \(_{i}\) factorises or disentangles the \(^{*}\). Causal disentanglement \(D_{i}\) for \(_{i}\) is calculated as \(D_{i}=(1-H_{K}(P_{i.}))=(1+_{k=0}^{K-1}P_{ik}_{K}P_{ik})\) where \(P_{ij}=}{_{k=0}^{K-1}R_{ik}}\) and \(R_{ij}\) denotes the probability of \(_{i}\) being important for predicting \(_{j}^{*}\). Total causal disentanglement is the weighted average \(_{i}_{i}D_{i}\) where \(_{i}=R_{ij}}{_{ij}R_{ij}}\). Causal Completeness quantifies the degree to which each \(_{i}^{*}\) is captured by a single \(_{i}\). Causal completeness is calculated as \(C_{j}=(1-H_{D}(_{j}))=(1+_{d=0}^{D-1}_{dj}_{D} {P}_{ij})\). \(D\) and \(K\) here are equal to the dimension of \(^{*}\) and \(\) which is \(n\). For the action inference task, we will use classification accuracy as a metric. As we assume intervention targets are known, we train all models using known intervention targets for a fair comparison.

## 5 Results

### Causal Disentanglement

We generated a dataset for the soft interventions and trained the models of ICRL-SM, ILCM, \(\)-VAE and D-VAE for 10 different seeds, which generated 10 different causal graphs. We selected 4 causal variables to encompass complex causal structures, including forks, chains, and colliders. Table 2 displays the Causal Disentanglement and Causal Completeness scores for all models, computed on the test data.

The results in Table 2 indicate that our method ICRL-SM can identify the true causal graph in most cases. The worst results are seen for graphs \(G5\) and \(G10\). As mentioned in [27; 25], causal graphs are sparse and in the \(G5\) case, where the graph is fully connected, the proposed method cannot identify the causal variables well. Furthermore, in the next experiment we are going to examine the factors affecting causal disentanglement such as the number of edges in the graph and the intensity of soft intervention effect. These findings can explain why ICRL-SM cannot identify causal variables in \(G10\) despite its sparsity.

    &  &  \\ 
**Model** & **Name** & \(\)**-VAE & \(\)**-VAE & ILCM & ICRL-SM & \(\)**-VAE & \(\)**-VAE & ILCM & ICRL-SM \\   & G1 & 0.88 & 0.54 & 0.71 & **0.82** & 0.51 & 0.69 & 0.78 & **0.87** \\  & G2 & 0.30 & 0.72 & 0.75 & **0.83** & 0.49 & 0.77 & 0.80 & **0.87** \\  & G3 & 0.28 & 0.51 & 0.68 & **0.98** & 0.49 & 0.56 & 0.78 & **0.98** \\  & G4 & 0.16 & 0.50 & 0.65 & **0.68** & 0.38 & 0.69 & 0.77 & **0.78** \\  & G5 & 0.27 & 0.44 & **0.53** & 0.42 & 0.45 & 0.54 & **0.66** & 0.50 \\  & G6 & 0.52 & 0.62 & 0.71 & **0.98** & 0.66 & 0.69 & 0.36 & **0.98** \\  & G7 & 0.39 & 0.49 & 0.71 & **0.75** & 0.70 & 0.73 & 0.89 & **0.89** \\  & G8 & 0.47 & 0.54 & 0.50 & **0.59** & 0.6 & 0.63 & 0.62 & **0.68** \\  & G9 & 0.30 & 0.68 & 0.83 & **0.85** & 0.40 & 0.76 & 0.86 & **0.87** \\  & G10 & 0.39 & 0.39 & **0.52** & 0.32 & 0.53 & 0.56 & **0.82** & 0.70 \\   

Table 2: Comparison of identifiability results

### Factors Affecting Causal Disentanglement

In this experiment, we consider the graph \(G3\), which has the best identifiability, and change the intensity of soft intervention and number of edges in its data generation process. To change the intensity, the post-intervention \(\) network weights are initialized with samples drawn from \(N(1,1)\) (almost similar to \(loc\)) and \(N(10,1)\) (significantly different from \(loc\)). To change the number of edges, we consider a chain and fully-connected graph.

The results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is much worse in the fully-connected graph than the default graph of \(G3\). The result for significantly different post-intervention causal mechanisms indicate that the switch variable cannot approximate intense effects of soft intervention and more supervision is required to observe \(\). Similar post-intervention causal mechanisms also do not have sufficient variability to disentangle the causal variables as mentioned in Theory 3.5.

### Action Inference

In this experiment, we show the performance of ICRL-SM in the real-world Causal-Triplet datasets. In these datasets \(\) i.e., soft intervention effects, are not directly observable. Nevertheless, our findings suggest that incorporating soft interventions through \(\) leads to superior performance compared to other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the generative system of the dataset would result in improved outcomes.

The results in Table 3 indicate that when including all causal variables to predict actions, ICRL-SM performs at par with the baseline methods. However, including all causal variables in the action or object inference may cause spurious correlations. Therefore, we have also experimented with including only the related causal variable in action and object inference. In this setting, ICRL-SM significantly outperforms the baseline methods which means that it can better disentangle the causal variables. We have also compared ICRL-SM with explicit causal representation learning methods. ENCO  and DDS  have variable topological order of causal variables during training. Furthermore, we have included a specific setting where the topological order is fixed during training. As shown in Table 4, our proposed method has superior performance to explicit models as well.

## 6 Conclusion

ICRL-SM, our novel model, enhances implicit causal representation learning during soft interventions by introducing a causal mechanism switch variable. Evaluations on synthetic and real-world datasets demonstrate ICRL-SM's superiority over state-of-the-art methods, highlighting its practical effectiveness. Our findings emphasize ICRL-SM's ability to discern causal models from soft interventions, marking it as a promising avenue for future research.

  
**Datasets** & **Methods** & **Action Accuracy** & **Object Accuracy** \\  Epic-Krdenes & ENCO  & 0.69 & 0.13 \\  & DDS  & 0.44 & 0.09 \\  & Fixed-order & **0.79** & 0.14 \\  & **ICRL-SM (ours)** & **0.86** & **0.18** \\  PrecHOR & ENCO  & 0.45 & 0.53 \\  & DDS  & 0.64 & 0.67 \\  & Fixed-order & 0.65 & 0.54 \\  & **ICRL-SM (ours)** & **0.93** & **0.82** \\   

Table 4: Left table depicts the action and object accuracy of three explicit models, with experiments conducted applying an image with resolution of \(R_{64}\) as the input to the Resnet50 encoder with the intervened causal variable (\(z_{i}\)). Right table shows the comparison of ICRL-SM performance on different configurations of \(G5\)

    &  &  \\   &  &  &  &  \\ 
**Method** & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(Z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(z_{i}R_{64}\) & \(z_{i}R_{64}\) \\  \(-VAE\) & **0.27** & 0.18 & 0.19 & 0.06 & **0.39** & 0.30 & **0.44** & 0.37 \\ \(d-VAE\) & 0.19 & 0.69 & **0.20** & 0.17 & 0.35 & 0.81 & 0.40 & 0.78 \\ IECM & 0.21 & 0.59 & 0.14 & 0.14 & 0.30 & 0.70 & 0.41 & 0.76 \\
**ICRL-SM (ours)** & 0.16 & **0.86** & 0.16 & **0.18** & 0.28 & **0.93** & 0.40 & **0.82** \\   

Table 3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets under different settings. \(Z\) and \(z_{i}\) show whether all causal variables (\(Z\)), or only the intervened casual variable (\(z_{i}\)) are used for the prediction task. \(R_{64}\) denote images with resolutions \(64 64\).