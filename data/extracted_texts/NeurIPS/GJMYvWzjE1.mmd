# Language Models as Hierarchy Encoders

Yuan He

University of Oxford

yuan.he@cs.ox.ac.uk

&Zhangdie Yuan

University of Cambridge

zy317@cam.ac.uk

&Jiaoyan Chen

The University of Manchester

jaoyan.chen@manchester.ac.uk

&Ian Horrocks

University of Oxford

ian.horrocks@cs.ox.ac.uk

###### Abstract

Interpreting hierarchical structures latent in language is a key limitation of current language models (LMs). While previous research has implicitly leveraged these hierarchies to enhance LMs, approaches for their explicit encoding are yet to be explored. To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space. Our method situates the output embedding space of pre-trained LMs within a Poincare ball with a curvature that adapts to the embedding dimension, followed by training on hyperbolic clustering and centripetal losses. These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned LMs, and several hyperbolic embedding baselines, focusing on their capabilities in simulating transitive inference, predicting subsumptions, and transferring knowledge across hierarchies. The results demonstrate that HiTs consistently outperform all baselines in these tasks, underscoring the effectiveness and transferability of our re-trained hierarchy encoders.1

## 1 Introduction

In the field of Natural Language Processing (NLP) and related areas, the emergence of transformer-based language models (LMs) such as BERT (encoder-based) , GPT (decoder-based) , and the more recent large language models (LLMs) like GPT-4  and Llama 2 , has marked a significant progression. Nonetheless, these models face a notable challenge in effectively encoding and interpreting hierarchical structures latent in language. This limitation has been highlighted by several studies, including those by  and , which employed prompt-based probes to reveal the limited hierarchical knowledge in pre-trained LMs, and the work by , which demonstrated these models' struggles with capturing the transitivity of hierarchical relationships.

Prior research has explored various methods to infuse hierarchical information into LM training. Common approaches include classification-based fine-tuning using sentence head embedding with a classification layer  or few-shot prompting with an answer mapping to classification labels . To further pre-train, or re-train2 LMs on a corpus constructed from hierarchical data,  convertedstructural representations into textual formats to align the masked language modeling objective. Others, like  and , have focused on extracting analogous and contrasting examples from hierarchical structures for a similarity-based contrastive learning objective. The aforementioned studies leveraged hierarchical information as implicit signals to augment LMs, yet no existing works specifically targeted the explicit encoding of hierarchies with LMs.

To bridge this gap, we introduce a novel approach to re-train transformer encoder-based LMs as **Hierarchy Transformer encoders** (HiTs). Inspired by the efficacy of hyperbolic geometry in representing hierarchical structures [12; 13], we propose the hyperbolic clustering and centripetal losses tailored for LM retraining. As illustrated in Figure 1, transformer encoder-based LMs typically use a \(\) activation function in the last layer, which maps each embedding dimension to the range \([-1,1]\). Consequently, the output embeddings of LMs are confined within a unit \(d\)-dimensional hypercube. Leveraging this characteristic, we utilise a Poincare ball of radius \(\), whose boundary circumscribes3 the output embedding space of LMs. The metrics for distance and norm used in our hyperbolic losses are defined w.r.t. this specific manifold. After re-training, entities are not only clustered according to their relatedness but also hierarchically organised.

In evaluating HiTs, we compare their performance against pre-trained LMs, standard fine-tuned LMs, and previous hyperbolic embedding models in the Multi-hop Inference and Mixed-hop Prediction tasks. The Multi-hop Inference task, following the setting in , involves training models on all asserted (i.e., one-hop) subsumptions and assessing their ability to infer transitive (i.e., multi-hop) subsumptions. The Mixed-hop Prediction task is designed to mirror real-world scenarios, where models trained on incomplete hierarchy are applied to predict unknown subsumption relationships between arbitrary entity pairs. Additionally, we introduce a transfer learning setting, where models trained on one hierarchy are tested on another. Our experiments utilise datasets derived from WordNet  and SNOMED CT ,4 and transfer evaluation datasets from Schema.org , Food Ontology (FoodOn) , and Disease Ontology (DOID) . The results show that HiTs significantly surpass all baselines in these tasks, demonstrating their robustness to generalise from asserted to inferred and unseen subsumptions, and a promising potential in hierarchy-based semantic search.

## 2 Preliminaries

### Language Models

Transformer encoder-based LMs excel in providing fine-grained contextual word embeddings for enhanced language understanding. A key component of these models is the self-attention mechanism, which dynamically assigns importance to different segments of the input text, thereby capturing nuanced contextual semantics more effectively. Notable examples of such models include BERT  and RoBERTa , both of which utilise the _masked language modelling_ objective during pre-training. This approach involves partially masking input sentences and prompting the model to predict the masked tokens, using the unmasked surrounding text as context. For acquiring sentence-level embeddings, these models can be augmented with an additional pooling layer, applied over the token embeddings [20; 21]. Pooling strategies such as mean, max, and sentence head pooling are employed, with their effectiveness varying across different applications. A contrastive learning objective is often applied for refining sentence-level semantics [21; 22]. Despite the rise of generative LLMs,

Figure 1: Illustration of how hierarchies are explicitly encoded in HiTs. The square (\(d\)-dimensional hyper-cube) refers to the output embedding space of transformer encoder-based LMs whose final activation function is typically \(\), and the circumscribed circle (\(d\)-dimensional hyper-sphere) refers to the PoincarÃ© ball of radius \(\). The distance and norm metrics involved in our hyperbolic losses are defined w.r.t. this manifold.

transformer encoder-based LMs maintain their importance, offering versatility and efficiency in tasks like text classification and semantic search.

### Hyperbolic Geometry

Hyperbolic geometry, a form of non-Euclidean geometry, is featured by its constant negative Gaussian curvature, a fundamental aspect that differentiates it from the flat, zero curvature of Euclidean geometry. In hyperbolic space, distances between points increase exponentially as one moves towards the boundary, making it inherently suitable for embedding hierarchical structures. This intuition aligns with the tree embedding theorem based on \(\)-hyperbolicity, as discussed in  and .

Among the various models5 of hyperbolic geometry that are isometric6 to each other, the Poincare ball is chosen for its capacity to contain the the output embedding space of LMs directly, as explained in the second last paragraph of Section 1. The \(d\)-dimensional Poincare ball with a negative curvature \(-c\) (where \(c>0\)) is defined by the open ball \(_{c}^{d}=\{^{d}:\|\|^{2}<\}\). The distance function in this model, dependent on the curvature value \(c\), is given by:

\[d_{c}(,)=}^{-1}(\|_{c}\|)\] (1)

In this equation, \(,_{c}^{d}\), \(\|\|\) denotes the Euclidean norm, and \(_{c}\) denotes the Mobius addition  defined as:

\[_{c}=,+ c\|\|^{2})+(1-c\|\|^{2})}{1+2c ,+c^{2}\|\|^{2}\|\|^{2}}\] (2)

Here, \(,\) denotes the inner product in Euclidean space. Note that for flat curvature \(c=0\), \(_{c}^{d}\) will be \(^{d}\) and \(_{c}\) will be the Euclidean addition.

### Hierarchy

We define a hierarchy as a directed acyclic graph \((,)\) where \(\) represents vertices that symbolise _entities_, and \(\) represents edges that indicate the _direct_ subsumption relationships asserted in the hierarchy. We can then derive _indirect_ subsumption relationships \(\) based on direct ones through transitive reasoning. We borrow the notation from description logic to denote the subsumption relationship as \(e_{1} e_{2}\), meaning that \(e_{1}\) is a sub-class of \(e_{2}\). Under the closed-world assumption, we consider an edge \((e_{1},e_{2})\) as a _negative sample_ if \((e_{1},e_{2})\). Particularly, a _hard negative_ is identified when \(e_{1}\) and \(e_{2}\) are also siblings that share the same parent.

Explicit hierarchies can often be derived from structured data sources such as taxonomies, ontologies, and knowledge graphs. A taxonomy and the Terminology Box (TBox) of an ontology intrinsically define subsumptions, whereas in knowledge graphs, hierarchical relationships are defined in a more customised manner. For instance, in WordNet, the _hypernym_ relationship corresponds to subsumption.

## 3 Hierarchy Transformer Encoder

We intend to propose a general and effective strategy to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs). To deal with arbitrary input lengths of entity names, we employ an architecture similar to sentence transformers , incorporating a mean pooling layer over token embeddings to produce sentence embeddings for entities. Note that some of the sentence transformer models have a normalisation layer after pooling; we exclude this layer because its presence will constrain the embeddings' Euclidean norms to one, thus hindering hierarchical organisation of entity embeddings. It is also worth mentioning that these changes do not add in learnable parameters besides the ones already in LMs, thus retaining the original architectures of LMs as encoders. As aforementioned, the output embedding space of these LMs is typically a \(d\)-dimensional hyper-cube because of the \(\) activation function in the last layer. Thus, we can construct a Poincare ball of radius \(\) (or equivalently, curvature value \(c=\)) whose boundary circumscribes the hyper-cube.7 Unlike previous hyperbolic embedding methods that utilise the entire hyperbolic space and often require a projection layer to manage out-of-manifold embeddings, our method ensures that embeddings are contained within a specific subset of this manifold. Empirical evidence supports that this subset sufficiently accommodates entities in high-dimensional space (see Section 4.5). Based on this curvature-adapted manifold, we propose the following two losses for hierarchy re-training.

Hyperbolic Clustering LossThis loss aims at clustering related entities and distancing unrelated ones in the Poincare ball. We formulate it in the form of triplet loss because related entities are not equivalent but their semantic distances should be smaller than those between unrelated entities. Formally, the loss is defined as:

\[_{cluster}=_{(e,e^{+},e^{-})}(d_{c}(,^{+})-d_{c}(,^{-})+,0)\] (3)

Here, inputs are presented in the form of triplet \((e,e^{+},e^{-})\), where \(e^{+}\) is a parent entity of \(e\), and \(e^{-}\) is a negative parent entity of \(e\); \(\) denotes the set of these triplets; \(d_{c}(,)\) refers to the hyperbolic distance function defined in Equation (1), and \(\) is the hyperbolic distance margin. The bold letters denote the embeddings of the corresponding entities.

Hyperbolic Centripetal LossThis loss ensures parent entities are positioned closer to the Poincare ball's origin than their child counterparts, reflecting the natural expansion of hierarchies from the origin to the boundary of the manifold. The term _"centripetal"_ is used to imply that the manifold's origin represents an **imaginary root entity** for everything. Formally, the hyperbolic centripetal loss is defined as:

\[_{centri}=_{(e,e^{+},e^{-})}(\|^{+} \|_{c}-\|\|_{c}+,0)\] (4)

Again, inputs are the triplets in \(\), but only the child and parent entities (the positive subsumptions) are used to calculate the loss; \(\|\|_{c}:=d_{c}(,)\) refers to the hyperbolic norm; \(\) is the hyperbolic norm margin.

The overall hierarchy re-training loss, denoted as \(_{}\), is the linear combination of these two hyperbolic losses, defined as:

\[_{}=_{cluster}+_{centri}\] (5)

In Figure 2, we demonstrate the impact of \(_{}\) on entity embeddings. The entity _"e-device"_, being most general, is nearest to the origin. Sibling entities, such as _"phone"_ and _"computer"_, _"laptop"_ and _"pc"_, are closer to their common parent than to each other, illustrating the effect of re-training to cluster related entities while maintain hierarchical relationships.

As the HiT model functions as an encoder, it does not inherently support direct predictions of subsumption relationships. To address this, we devise a probing function that leverages the hierarchy re-trained entity embeddings. This function aims to predict the subsumption relationship for any given pair of entities \((e_{1},e_{2})\), incorporating both the clustering and centripetal heuristics:

\[s(e_{1} e_{2})=-(d_{c}(_{1},_{2})+(\| _{2}\|_{c}-\|_{1}\|_{c}))\] (6)

Here, \(>0\) represents a weighting factor applied to the centripetal heuristic component. The subsumption score is structured to increase as the hyperbolic distance between \(_{1}\) and \(_{2}\) decreases,

Figure 2: Illustration of the impact of \(_{}\) during training. In Euclidean space, it seems contradictory that both _âphoneâ_ and _âcomputerâ_ are pulled towards _âe-deviceâ_ but are also pushed away from each other. However, in principle this is not a problem in hyperbolic space, where distances increase exponentially relative to Euclidean distances as one moves from the origin to the boundary of the manifold.

and/or as the relative difference in their hyperbolic norms (\(\|_{1}\|_{c}-\|_{2}\|_{c}\)) increases. Essentially, for the model to predict \(e_{1} e_{2}\), it is expected that \(_{1}\) and \(_{2}\) are relatively closer in the Poincare ball, with \(_{1}\) positioned further from the manifold's origin compared to \(_{2}\). The value for \(\) and the overall scoring threshold are to be ascertained through hyperparameter tuning on the validation set.

## 4 Evaluation

### Task Definition

Multi-hop InferenceThis task, following the setting in , aims to evaluate the model's ability in deducing indirect, multi-hop subsumptions \(\) from direct, one-hop subsumptions \(\), so as to simulate transitive inference. We split \(\) for validation and testing, denoted as \(_{val}\) and \(_{test}\), respectively. For each positive subsumption \((e,e^{+})\) involved, we sampled \(10\) negative parents \(e^{-}\) for \(e\), leading to \(10\) training triplets8. Following the criteria in Section 2.3, \((e,e^{-})\) is a valid negative if \((e,e^{-})\). We further split the task into two settings: one with **random negatives** and another with **hard negatives**, the latter mainly comprising sibling entities. Since not every entity has enough siblings, we supplemented with random negatives that have been sampled to maintain a consistent positive-to-negative ratio of \(1:10\).

Mixed-hop PredictionThis task aims to evaluate the model's capability in determining the existence of subsumption relationships between arbitrary entity pairs, where the entities are not necessarily seen during training. We propose a challenging setting where models are trained on incomplete direct subsumptions and examined on a mix of hold-out, unseen direct and indirect (mixed-hop) subsumptions. We split \(\) into training, validation, and test sets, denoted as \(_{train}\), \(_{val}\), and \(_{test}\), respectively. The final training, validataion, and test sets for this task are \(_{train}\), \(_{val}_{val}\), and \(_{test}_{test}\), respectively, where \(_{val}\) and \(_{test}\) are re-used from the previous task. Again, each positive subsumption in these sets is paired with \(10\) negative samples, either randomly chosen or from sibling entities. Furthermore, an important factor that reflects the model's generalisability is to examine the **transfer ability across hierarchies**. To this end, we extend the mixed-hop prediction task with a transfer setting where models trained on asserted training edges of one hierarchy are tested on arbitrary entity pairs of another.

Evaluation MetricsFor both Multi-hop Inference and Mixed-hop Prediction tasks, we utilise Precision, Recall, and F1 score (abbreviated as F-score in latter discussion) as our primary metrics of evaluation. We have opted not to include Accuracy, as preliminary testing indicated a potential bias in this metric, with a misleadingly high score resulting from the much larger volume of negative compared to positive samples. It is important to note that, although the training phase uses entity triplets, the evaluation only involves entity pairs.

### Dataset Construction

We constructed the primary dataset from the noun hierarchy of WordNet  due to its comprehensive and structured representation of linguistic hierarchies. To assess the transferability and robustness

  
**Source** & **\#Entity** & **\#DirectSub** & **\#IndirectSub** & **\#Dataset (Train/Val/Test)** \\  WordNet & 74,401 & 75,850 & 587,658 & multi: 834K/323K/323K \\  & & & & mixed: 751K/365K/365K \\  Schema.org & 903 & 950 & 1,978 & mixed: -/15K/15K \\  FoodOn & 30,963 & 36,486 & 438,266 & mixed: 361K/261K/261K \\  DOID & 11,157 & 11,180 & 45,383 & mixed: 111K/31K \\   

Table 1: Statistics of WordNet (Noun), Schema.org, and FoodOn, including the numbers of entities (**#Entity**), direct subsumptions (**#DirectSub**), indirect subsumptions (**#IndirectSub**), and the dataset splittings (**#Dataset**) for Multi-hop Inference and Mixed-hop Prediction tasks. Note that the numbers in **#Dataset** are counts of entity pairs rather than entity triplets.

across different domains, we additionally constructed datasets from ontologies that represent varied semantic granularities and domains, namely Schema.org , Food Ontology (FoodOn) , and Disease Ontology (DOID) . We retrieved WordNet from NLTK  and adopted pre-processing steps similar to , utilising the _hypernym_ relations between noun _synsets_ to construct the hierarchy. For Schema.org, FoodOn, and DOID, our pre-processing paralleled that in , transforming these ontologies into hierarchies of named entities (details in Appendix A). To accommodate the textual input requirements of LMs, we constructed an entity lexicon using the _name_ attribute in WordNet and the rdfs:label property in the ontologies.9

On WordNet (Noun), FoodOn, and DOID, we adopt a consistent splitting ratio for the validation and testing sets. Specifically, we allocate two separate \(5\%\) portions of the indirect subsumptions \(\) to form \(_{val}\) and \(_{test}\), respectively. Similarly, two distinct \(5\%\) portions of the direct subsumptions \(\) are used as \(_{val}\) and \(_{test}\). As Schema.org is significantly smaller than the other hierarchies and only used for transfer evaluation, we split its entire \(\) and \(\) sets into halves for validation and testing, respectively. Table 1 presents the extracted hierarchies' statistics and the resulting datasets for the Multi-hop Inference and Mixed-hop Prediction tasks.

In addition to our main evaluation, we constructed a dataset from the widely-recognised biomedical ontology SNOMED CT  and conducted futher evaluation. The relevant details are presented in Appendix D.

### Baselines

Naive PriorWe first introduce a naive baseline (NaivePrior) that utilises the prior probability of positive subsumptions in the training set for prediction. Given that each positive sample is paired with \(10\) negatives, the prior probability of a positive prediction stands at \(\). Consequently, Precision, Recall, and F-score on the test set are all \(\).

Pre-trained LMsWe consider pre-trained LMs as baselines to illustrate their limitations in capturing hierarchical structure semantics. As outlined in Section 3, our focus is on LMs based on the sentence transformer architecture . Since these LMs are optimised for cosine similarities between sentences, we devise the following probe for evaluation: for each entity pair \((e_{1},e_{2})\), we compute the cosine similarity between the masked reference sentence _"\(e_{1}\) is a \(\)mask\(\)."_ and the sample sentence _"\(e_{1}\) is a \(e_{2}\)."_. These similarity scores serve as the subsumption scores, with thresholds identified via grid search on the validation set. Note that although these LMs originate from masked language models, they cannot be easily probed via mask filling logits or perplexities as in  and  because their mask filling layers are not preserved in the released versions. We select three top-performing pre-trained LMs from the sentence transformer library of different sizes, including all-MiniLM-L6-v2 (22.7M), all-MiniLM-L12-v2 (33.4M), and all-mpnet-base-v2 (109M).

Fine-tuned LMsFine-tuned LMs are used as baselines to demonstrate that despite the efficacy in various tasks, standard fine-tuning struggles to address this specific challenge. Following the BERTSubs approach outlined in , we employ pre-trained LMs with an added linear layer for binary classification, and optimising on the Softmax loss.  have shown that this method outperforms various structure-based embeddings such as TransE  and DistMult , and also surpasses OWL2Vec* , which integrates both structural and textual embeddings, in subsumption prediction.

Hyperbolic BaselinesPrevious static hyperbolic embedding models are typically evaluated using the Multi-hop Inference task. In our study, we select the Poincare Embedding (PoincareEmbed)  and the Hyperbolic Entailment Cone (HyperbolicCone)  as baselines on this task. However, their lack of inductive prediction capabilities prevents their evaluation on the Mixed-hop Prediction task and its transfer setting. Additionally, we include the hyperbolic GloVe embedding (PoincareGloVe) as a baseline in our transfer evaluation. We select the best-performing PoincareGloVe (50\(\)2D with an initial trick) pre-trained on a 1.4B token English Wikipedia dump. While PoincareGloVe supports inductive prediction, its effectiveness is limited by word-level tokenisation, rendering it less effective at handling unknown words. To address this, we employ NaivePrior as a fallback method for entities that involve unknown words and cannot be predicted by PoincareGloVe.

More details of our code implementation and experiment settings are presented in Appendix B.

### Results

The effectiveness of our hierarchy re-training approach is evident from the results of both the Multi-hop Inference and Mixed-hop Prediction tasks on WordNet (see Table 2), as well as the Transfer Mixed-hop Prediction task on Schema.org, FoodOn, and DOID for pre-trained LMs and models trained on WordNet (see Table 3). In the following, we present several pivotal findings based on these results.

Performance of HiTsThe HiT models, re-trained from LMs of various sizes, consistently outperform their pre-trained and standard fine-tuned counterparts across all evaluation tasks. In the Multi-hop Inference task, HiTs exhibit exceptional performance with F-scores ranging from \(0.871\) to \(0.916\). This indicates a strong capability in generalising from asserted to transitively inferred entity subsumptions. In the Mixed-hop Prediction task, F-scores ranging from \(0.856\) to \(0.900\) highlight the effectiveness of HiTs in generalising from asserted to arbitrary entity subsumptions. For the Transfer Mixed-hop Prediction tasks, we selected all-MiniLM-L12-v2 as the pre-trained model because all-MiniLM-L12-v2+HiT attains comparable performance to all-mpnet-base-v2+HiT while it is more computationally efficient owing to a smaller parameter size. Notably, all-MiniLM-L12-v2+HiT performs better than pre-trained and fine-tuned all-MiniLM-L12-v2 on these transfer tasks by at least \(0.150\) and \(0.101\) in F-scores, respectively.

Limited Hierarchical Knowledge in Pre-trained LMsFor the tasks on WordNet, all-mpnet-base-v2 achieves the highest F-scores among all the pre-trained models, yet these scores (e.g., \(0.347\) and \(0.250\) on the Mixed-hop Prediction task with random negatives and hard negatives, respectively) are considerably lower compared to their fine-tuned (lagging by \(0.304\) and \(0.419\)) and hierarchy re-trained (lagging by \(0.553\) and \(0.613\)) counterparts. This disparity confirms findings from LM probing studies such as those by  and , demonstrating the limited hierarchical knowledge in pre-trained LMs.

    &  &  \\ 
**Model** & **Precision** & **Recall** & **F-score** & **Precision** & **Recall** & **F-score** \\  NaivePrior & 0.091 & 0.091 & 0.091 & 0.091 & 0.091 & 0.091 \\    \\   PoincarÃ©Embed & 0.862 & 0.866 & 0.864 & 0.797 & 0.867 & 0.830 \\ HyperbolicCone & 0.817 & 0.996 & 0.898 & 0.243 & 0.902 & 0.383 \\  all-MiniLM-L6-v2 & 0.160 & 0.442 & 0.235 & 0.132 & 0.507 & 0.209 \\ + fine-tune & 0.800 & 0.513 & 0.625 & 0.764 & 0.597 & 0.670 \\ + HiT & 0.864 & 0.879 & 0.871 & 0.905 & 0.908 & 0.907 \\  all-MiniLM-L12-v2 & 0.127 & 0.585 & 0.209 & 0.108 & 0.740 & 0.188 \\ + fine-tune & 0.811 & 0.515 & 0.630 & 0.819 & 0.530 & 0.643 \\ + HiT & 0.880 & 0.927 & 0.903 & 0.910 & 0.906 & 0.908 \\  all-mpnet-base-v2 & 0.281 & 0.428 & 0.339 & 0.183 & 0.359 & 0.242 \\ + fine-tune & 0.796 & 0.501 & 0.615 & 0.758 & 0.628 & 0.687 \\ + HiT & 0.897 & 0.936 & 0.916 & 0.886 & 0.912 & 0.899 \\    \\   all-MiniLM-L6-v2 & 0.160 & 0.438 & 0.235 & 0.131 & 0.504 & 0.208 \\ + fine-tune & 0.747 & 0.575 & 0.650 & 0.769 & 0.578 & 0.660 \\ + HiT & 0.835 & 0.877 & 0.856 & 0.882 & 0.843 & 0.862 \\  all-MiniLM-L12-v2 & 0.127 & 0.583 & 0.209 & 0.111 & 0.625 & 0.188 \\ + fine-tune & 0.794 & 0.517 & 0.627 & 0.859 & 0.515 & 0.644 \\ + HiT & 0.875 & 0.895 & 0.885 & 0.886 & 0.857 & 0.871 \\  all-mpnet-base-v2 & 0.287 & 0.439 & 0.347 & 0.197 & 0.344 & 0.250 \\ + fine-tune & 0.828 & 0.536 & 0.651 & 0.723 & 0.622 & 0.669 \\ + HiT & 0.892 & 0.910 & 0.900 & 0.869 & 0.858 & 0.863 \\   

Table 2: Multi-hop Inference and Mixed-hop Prediction test results on WordNet.

**Limited Generalisation in Fine-tuned LMs** The research by  illustrates that fine-tuned LMs perform well on single-hop subsumptions. Our observations concur, showing that fine-tuned LMs achieve comparable performance as HiTs when assessed on just single-hop test samples. However, their effectiveness wanes when applied to arbitrary entity subsumptions. For the tasks on WordNet, fine-tuned LMs underperform HiTs by \(0.194\) to \(0.301\) in F-scores. In the transfer task from WordNet to Schema.org, the fine-tuned all-MiniLM-L12-v2 model only marginally outperforms its initial state, with an increase of around \(0.02\) in F-scores across both negative settings.

**Performance of Hyperbolic Baselines** The Multi-hop Inference task with random negatives follows the evaluation in . In this setup, both PoincareEmbed and HyperbolicCone significantly outperform the pre-trained and standard fine-tuned LMs, and perform comparably to the HiT models. However, HyperbolicCone exhibits substantially worse performance in the hard negative setting; its low precision and high recall suggest difficulties in differentiating sibling entities that are closely positioned in the embedding space. In the transfer evaluation, PoincareGloVe shows improved performance over pre-trained and standard fine-tuned models on Schema.org. However, it does not demonstrate a similar advantage on FoodOn and DOID, primarily due to its limited vocabulary, which allows it to predict almost all test samples on Schema.org but substantially fewer on the others.

**Comparison of Random and Hard Negatives** For the tasks on WordNet, hard negative settings present greater challenges compared to random negative settings for all pre-trained LMs. This increased difficulty, however, is not as pronounced in fine-tuned LMs and HiTs. A plausible explanation is that while hard negatives pose challenges, they simultaneously act as high-quality adversarial examples, potentially leading to more robust training outcomes. In the Transfer Mixed Prediction task, hard negative settings are generally more challenging than random negative settings. For instance, in the WordNet-to-DOID transfer task, both fine-tuned and hierarchy re-trained all-MiniLM-L12-v2 models exhibit significantly higher F-scores in the random negative setting, with differences of \(0.306\) and \(0.138\) respectively, compared to the hard negative setting.

**Case Analysis on WordNet-to-DOID Transfer** In the WordNet-to-DOID transfer task, the disparity in the _"disease"_ category is notable: WordNet contains only \(605\) entities that are descendants of _"disease"_, compared to over \(10K\) in DOID. Despite this significant difference, HiT models effectively transfer knowledge, achieving F-scores of \(0.704\) (random negatives) and \(0.566\) (hard negatives).

More discussion on loss functions and an ablation study of loss margins are presented in Appendix C.

    &  &  \\ 
**Model** & **Precision** & **Recall** & **F-score** & **Precision** & **Recall** & **F-score** \\    & 0.091 & 0.091 & 0.091 & 0.091 & 0.091 & 0.091 \\    \\    \\    \\    \\    \\    \\    \\    \\   

Table 3: Transfer Mixed-hop Prediction test results on Schema.org, FoodOn, and DOID.

### Analysis of HiT Embeddings

DistributionFigure 3 illustrates how WordNet entity embeddings generated by all-MiniLM-L12-v2+HiT distribute w.r.t. their hyperbolic norms.

These norms effectively capture the natural expansion of the hierarchical structure, evidenced by an exponential rise in the number of child entities. A notable observation is the sharp decline in the number of entities when hyperbolic norms exceed \(23\), suggesting that few entities reside at these higher levels. Additionally, the range of entity hyperbolic norms, approximately from \(8\) to \(24\), indicates that a relatively small region of the high-dimensional manifold suffices to accommodate all entities in WordNet.

CorrelationIn Table 4, we compare the Pearson correlation coefficients across different hyperbolic models to measure the linear relationship between entities' hyperbolic norms and their depths in WordNet. Our analysis shows that all hyperbolic models lead to a positive correlation between norms and depths, as expected. However, HiT demonstrates a stronger correlation than both PoincareEmbed and HyperbolicCone.

Case StudyIn Table 5, we showcase the effectiveness of HiT using selected entities: _"computer"_, _"pc"_10, _"fruit"_, and _"berry"_. The table presents the hyperbolic distances between these entities' embeddings, their individual hyperbolic norms, and their depths11 in the WordNet hierarchy. We can observe that: _(i)_ closely related entities, such as _"fruit"_ and _"berry"_, are significantly nearer to each other compared to more distant pairs; _(ii)_ more specific entities like _"pc"_ and _"berry"_ are positioned further from the origin of the manifold than their ancestor entities; _(iii)_ the disparity in hyperbolic norms between _"pc"_ and _"computer"_ is greater compared to that between _"frruit"_ and _"berry"_, reflecting the hierarchical depth where _"pc"_ is a grandchild of _"computer"_, while _"berry"_ is a direct child of _"fruit"_.

## 5 Related Work

Prompt-based probing is widely used for extracting knowledge from LMs. Studies like  utilised cloze-style prompts for hypernym detection, while  approached subsumption prediction similar to Natural Language Inference.  examined if LMs, when correctly predicting _"A is a B"_ and _"B is a C"_, can consistently infer the transitive relationship _"A is a C"_. These studies collectively highlight the limited capacity of pre-trained LMs in understanding hierarchical structures. Other research efforts, such as those by  and , have aimed to incorporate structural semantics into LMs for entity encoding. However, these largely focus on entity equivalence or similarity, with less emphasis on hierarchical organisation.

Regarding hyperbolic embeddings, methods like the Poincare embedding  and the hyperbolic entailment cone  have effectively represented hierarchical structures. Despite their efficacy, these techniques are inherently static, constrained by a fixed vocabulary of entities, and do not support

   HiT & PoincarÃ©Embed & HyperbolicCone \\ 
0.346 & 0.130 & 0.245 \\   

Table 4: Statistical correlations between WordNet entitiesâ depths and their hyperbolic norms across different hyperbolic models.

Figure 3: Distribution of WordNet entity embeddings generated by HiT w.r.t. their hyperbolic norms.

    & computer & pc & fruit & berry \\  computer & 0.0 & 5.9 & 22.5 & 24.9 \\ pc & 5.9 & 0.0 & 25.2 & 27.2 \\ fruit & 22.5 & 25.2 & 0.0 & 6.72 \\ berry & 24.9 & 27.2 & 6.72 & 0.0 \\ 
**h-norm** & 17.5 & 19.1 & 15.3 & 16.6 \\ 
**depth** & 9 & 11 & 9 & 10 \\  

Table 5: Hyperbolic distances between the embeddings of selected entities (_âcomputerâ_, _"pc"_, _"fruit"_, _"berry"_), along with their individual hyperbolic norms (**h-norm**) and depths in WordNet.

inductive predictions about unseen data. Further explorations include learning word embeddings in hyperbolic space [31; 32]. These methods, however, are limited to word-level tokenisation and yield non-contextual word representations. These shortcomings can be mitigated by integrating hyperbolic embeddings with transformer-based LMs.  has explored this direction, applying learnable layers to project LM embeddings into hyperbolic space for syntax parsing and sentiment analysis. Our approach diverges from theirs by focusing on training LMs as general hierarchy encoders without the need for additional learnable parameters.

## 6 Conclusion

This paper tackles the challenge of enabling language models to interpret and encode hierarchies. We devise the hierarchy re-training approach that involves a joint optimisation on both the hyperbolic clustering and hyperbolic centripetal losses, aiming to cluster and organise entities according to their hierarchical relationships. The resulting HiT models demonstrate proficiency in simulating transitive inference and predicting subsumptions within and across hierarchies. Additionally, our analysis of HiT embeddings highlights their geometric interpretability, further validating the effectiveness of our approach.

## 7 Limitations and Future Work

This work does not address the potential loss of pre-trained language understanding resulted from hierarchy re-training. Also, the issue of entity naming ambiguity inherent in the dataset sources is not tackled, which could introduce noise into the training process.

For future work, several promising directions can be pursued: _(i)_ investigating methods to measure and mitigate catastrophic forgetting, _(ii)_ training a HiT model across multiple hierarchies, either for general or domain-specific applications, _(iii)_ extending HiT to accommodate multiple hierarchical relationships within a single model, and _(iv)_ developing hierarchy-based semantic search that contrasts with traditional similarity-based approaches.