# Language Model Alignment with Elastic Reset

Michael Noukhovitch

Mila, Universite de Montreal

&Samuel Lavoie

Mila, Universite de Montreal

&Florian Strub

Google Deepmind

&Aaron Courville

Mila, Universite de Montreal

CIFAR AI Chair

Correspondance to michael.noukhovitch@umontreal.ca

###### Abstract

Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose **Elastic Reset**, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available at github.com/mnoukhov/elastic-reset.

## 1 Introduction

Dialogue agents that can effectively interpret and use language are a long-term challenge for NLP. The rise of large pretrained language models (LMs)  made language model finetuning one of the most promising research directions to achieving capable dialogue agents . Recently, reinforcement learning (RL) has become a key ingredient of finetuning large LMs for interaction with humans , notably shown in ChatGPT . A reward model is learned on the alignment objective, such as learned human preferences , and the language model is finetuned to optimize the reward. But training on the RL objective moves the model away from its pretraining and can reduce performance on important benchmarks  and even drifting away from natural language syntax and semantics .

"Language drift" , "alignment tax" , "reward model overoptimization" , or LM-specific "reward-hacking"  is inherent to RLHF. In the extreme case, models learn to achieve high reward by generating nonsense text that is unintelligible to humans . Methods to mitigate this issue range from re-running pretraining , grounding in other modalities .

2019), masking the LM generation (Ramamurthy et al., 2022) and iterated learning (Lu et al., 2020). But the standard, and by far most popular approach, adds a Kullback-Lieber (KL) divergence penalty to the reward in order to prevent the finetuned model from drifting too far from the pretrained model (Jaques et al., 2017, 2019; Ziegler et al., 2019). Still, all methods are insufficient over a large-enough training horizon so models are early-stopped before reaching a catastrophic level of drift.

Gao et al. (2022) find that achieving reward is proportional to drift from the initial model, but that not all drifts are equal. We wish to make small but effective changes that achieve high reward but maintain capabilities, yet auxiliary losses such as the KL penalty don't seem improve this tradeoff and only serve to slow down training (Gao et al., 2022). We posit that RLHF training requires a useful inductive bias that does not modify the training objective. Inspired by recent work in generalization for image classification (Zhou et al., 2022), sample-efficient RL (Nikishin et al., 2022; D'Oro et al., 2023), and inducing compositional language (Li and Bowling, 2019), we propose to use resets. Similar to iterated learning (Kirby, 2001), iteratively resetting a model has been shown to reduce overfitting in language and RL scenarios (Rita et al., 2022). In this work, we show iteratively resetting a model also reduces drift while attaining equal or better reward than just a KL penalty.

Unlike previous work in sample-efficient RL (Nikishin et al., 2022), RLHF is typically on-policy so it does not maintain a replay buffer with which to bootstrap learning after a reset. In lieu of a replay buffer, we reset the policy but maintain the value function. Yet resetting the policy to its initial state can still cause a large drop in performance. So we propose resetting to a model in-between our online and initial state, specifically to an exponential moving average (EMA) of our online policy, as EMA has been shown to be highly performant (Caron et al., 2021). We still expect our EMA model to slowly drift, so we add a second step where we reset the EMA model to the initial model. We call this overall method **Elastic Reset** and illustrate it in Figure 1. Elastic Reset is implemented on top of regular RL methods such as REINFORCE (Williams, 1992) or PPO (Schulman et al., 2017).

First, we test our method on a small scale task: pivot translation with a transformer. In this classic benchmark for drift, we outperform all previous baselines and demonstrate state-of-the-art performance. Next, we re-evaluate how performance is measured in the field and argue for a metric of how each method trades off performance vs drift. We propose the Pareto Frontier Graph, a graphical measure that illuminates the trade-off between performance and drift and demonstrate that Elastic Reset dominates the baselines against this trade-off. Then, we scale up slightly to GPT2 and work on a popular task closer to RLHF, IMDB mock sentiment. Comparing to all baseline methods, we again show state-of-the-art performance on the benchmark. Through ablations, we show that Elastic Reset is robust to choices of hyperparameters, even more so than baselines. Finally, we scale up even more to true RLHF finetuning of Llama-7B in order to create a helpful technical QA chatbot using a StackExchange dataset. We again outperform the baseline, demonstrating how Elastic Reset mitigates the alignment tax while better optimizing the human feedback reward.

## 2 Related Work

"It is often difficult or infeasible to capture exactly what we want an agent to do, and as a result we frequently end up using imperfect but easily measured proxies" (Clark and Amodei, 2016). In RL, this proxy is how we construct our reward and the consequence can be "reward-hacking" (Clark and Amodei, 2016); an agent optimizes the reward but does not accomplishing the meaningful task. RLHF aims to align an agent with human preferences while maintaining the capabilities of the pretrained model, but uses a learned reward model as a proxy of human preferences (Christiano et al., 2017; Ziegler et al., 2019). This can lead to LMs that optimize a reward model but degrade in

Figure 1: Elastic Reset. In actor-critic RL, we reset the policy but maintain the value function.

performance on general NLP benchmarks (Askell et al., 2021), overfit the reward model and do not generalize to true human preferences (Bai et al., 2022; Gao et al., 2022), or latch onto confounding factors hidden in the reward (Stiennon et al., 2020). **These** effects are exacerbated if the reward model is updated during training such as in iterated RLHF (Bai et al., 2022) or related setups such as emergent communication (Lazaridou and Baroni, 2020), end-to-end dialogue (Lewis et al., 2017), learning RL policies through latent language (Andreas et al., 2018), and pivot translation (Utiyama and Isahara, 2007). There, the phenomenon is known as "language drift" (Lee et al., 2019) and can lead to incoherent and unnatural linguistic outputs Lewis et al. (2017).

This phenomenon is inherent to RLHF. Gao et al. (2022) show that improvement on alignment / reward is proportional to drift from the initial model, but also find that different methods and design choices achieve different proportions of performance to drift. Therefore, a major challenge of RLHF is how to learn the reward in such a way as to minimize the drift, alignment tax, and reward-hacking. The standard approach used in most RLHF is to incorporate a KL penalty between the training language model and some fixed model (Jaques et al., 2019; Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Steinert-Threlkeld et al., 2022; Bai et al., 2022), usually the initial, pretrained model. Less common is to add the original pretraining task to the finetuning objective (termed S2P by Lowe* et al. (2021)) but this can be compute-intensive and requires maintaining the pretraining data which may be even more expensive for larger models (Brown et al., 2020). On small-scale pivot-translation, Lu et al. (2020) propose iterated learning with student-teacher distillation but it too is relatively compute intensive. Recently, Ramamurthy et al. (2022) propose to maintain a delayed masking model and mask the LM to output only the top-\(p\) tokens. Elastic Reset takes inspiration from both of these, using an iterated process and maintaining an EMA model. Apart from better performance, our method is more space efficient and maintains the EMA on CPU whereas both other methods require maintaining an extra model on GPU. It is also more compute efficient as resetting weights and EMA updates are very cheap operations, whereas Lu et al. (2020) requires a long distillation phase and Ramamurthy et al. (2022) requires an extra forward pass with the masking model. There exist less-popular methods that have been applied to similar issues in RL: prompt-tuning (Singh et al., 2022), using a fixed model to generate many options and re-ranking using a reward model (Lazaridou et al., 2020; Meta FAIR Diplomacy Team et al., 2022), and grounding the output in a separate modality (Lee et al., 2019) but none have been used for RLHF or at scale.

Our method is inspired by recent works that leverage resets for single agent RL (Nikishin et al., 2022; D'Oro et al., 2023), image classification (Zhou et al., 2022), and emergent communication (Rita et al., 2022). Those works generally train from scratch and reset to random initializations in order to improve generalization. Our scenario requires resetting to pretrained models and focuses on improving the tradeoff between performance and drift from this pretrained model. Elastic Reset can be seen as an on-policy alternative to Nikishin et al.'s (2022) off-policy resets; whereas they maintain the old replay buffer, we maintain the value model and an EMA of our policy.

Finally, the pretrain-then-RL-finetune setup with the goal of maintaining pretrained knowledge can be seen as a two-step, RL-specific instance of continual learning and therefore language drift has links to catastrophic forgetting (McCloskey and Cohen, 1989). There is a clear similarity between mitigation methods: renearest (Robins, 1995) or experience replay (Rolnick et al., 2019) is equivalent to multitasking with the pretraining objective (Lowe* et al., 2021) and weight-update regularization (Kirkpatrick et al., 2017) has similarities to KL regularization (Jaques et al., 2019).

## 3 Elastic Reset

The standard method against drift is a KL penalty, generally between the learning policy \(\) and the initial, pretrained model \(_{0}\). It is calculated empirically over the minibatch of training inputs \(x\) and outputs \(y\) and used as an auxiliary reward with coefficient \(\) on top of the regular reward model \(r\)

\[R(x,y)=r(x,y)-(y|x)}{_{_{0}}(y|x)}\] (1)

For Elastic Reset, we maintain an exponential moving average \(\) of our learning model \(\) and choose a decay hyperparameter parameter \(\). We initialize \(_{0}\) and after every online model step, we update our EMA model \((1-)+\). Every \(n\) steps, Elastic Reset sets the online model to the EMA model \(\) and sets the EMA model to the initial model \(_{0}\). As with other methods, Elastic Reset can be easily combined with a KL penalty.

## 4 Translation Game: Careful Comparison to SOTA

SetupWe first investigate the pivot-translation benchmark of Lee et al. (2019), which was previously popular for small-scale methods countering drift. Two translation models, French to English (FR\(\)EN) and English to German (EN\(\)DE), are pretrained on IWSLT (Cettolo et al., 2012). Then, the models are finetuned on translating French to German through English (FR\(\)EN\(\)DE) but given only paired French and German data from Multi30k (Elliott et al., 2016, 2017) as shown in Figure 2. The models are not given English at finetune-time so the challenge is optimizing FR\(\)DE while maintaining fluency in the intermediate English. Whereas larger benchmarks have only proxies for drift, we can exactly measure the performance degradation in our setup with the standard translation metric BLEU on a held-out FR\(\)EN validation set. Similarly, we measure success on the task with the FR\(\)EN\(\)DE BLEU score. Each model is an encoder-decoder Transformer (Vaswani et al., 2017) with 6 layers and all experimental details are available in Appendix A.

BaselinesThe EN\(\)DE reward model is simply trained using cross-entropy between predicted and true DE. Our lower-bound baseline is Frozen English, we freeze the FR\(\)EN model and only update the EN\(\)DE model. This models is guaranteed not to drift, but also cannot reach the best possible performance. For that, we need to update FR\(\)EN by backpropagating through the discrete EN tokens. We follow Lee et al. (2019) and train FR\(\)EN using REINFORCE (Williams, 1992) to estimate the gradient. As our base model, we combine REINFORCE with an exponentially moving baseline and, as with previous work, add a loss for entropy regularization.

When both FR\(\)EN and EN\(\)DE are being updated, we tend to see reasonably large drift and we compare to the best previous methods that counter it on this benchmark. We follow Lee et al. (2019) to simulate the standard KL penalty method, KL penalty by training an LSTM LM on IWSLT English text and adding a KL penalty with \(=0.05\) to regularize the FR\(\)EN model. Multitask learning, re-training, or S2P (Lowe\({}^{}\) et al., 2021), adds the supervised FR\(\)EN objective on IWSLT pretraining data as an auxiliary task for the FR\(\)EN model. Finally, we implement Seeded Iterated Learning (SIL; Lu et al., 2020), which alternates between \(n\) finetuning steps and \(m\) steps of teacher-student distillation. FR\(\)EN and EN\(\)DE "teacher" models are finetuned on the translation game, then each distills knowledge into "student" model of itself, and finally the students are initialized as teachers for the next iteration. Elastic Reset is implemented on top of REINFORCE with a very minimal KL penalty \(=0.001\) and uses an EMA decay \(=0.99\). We run all models for 50k updates and reset every 23k steps to get 2 resets / 3 iterations within a run. Hyperparameters may differ between methods, e.g. \(\), because we used a minimal search to find the best hyperparameters for each method.

ExperimentsFor each method, we run 5 seeds and plot the validation scores over training for the end-to-end task score, FR\(\)EN\(\)DE BLEU, and drift score, FR\(\)EN BLEU, in Figures 2(b), 2(a) respectively. Following Lee et al. (2019), we also show the final validation score in Table 1. As a sanity check, Frozen English does not drift but also does not achieve a very high task performance. In line with previous results (Lu et al., 2020), all learning models initially improve FR\(\)EN performance, likely because models are quickly, semi-supervised adapting from their pretraining (IWSLT) to the distribution of the finetune dataset (Multi30k). Afterwards, they start to overfit on their objective and FR\(\)EN performance degrades. REINFORCE achieves the best

Figure 2: The Translation Game (top left), IMDB mock sentiment task (bottom left), and StackLLaMA (right). We show all RL finetuning setups and StackLLaMAâ€™s reward modelling (top right).

possible task performance but drifts significantly. Despite extensive hyperparameter tuning and correspondence with the original authors, SIL does not manage to outperform the REINFORCE baseline so we exclude it from the figures for visual clarity but show values in Table 1 as well as full results in Appendix A. In line with previous work (Lee et al., 2019; Lu et al., 2020), we find that multitask and KL penalty are both beneficial to reducing drift, but both represent a tradeoff. Whereas multitask strongly reduces drift, it does not achieve a high task score. In contrast, KL penalty achieves a high task score but drifts quite drastically. Elastic Reset achieves nearly the best possible task score while maintaining the same drift score as the initial model. Visually, we see that our method track the baselines until the reset at 23k steps. After the reset, we see a slight performance drop but also a big jump back in terms of FR\(\)EN drift. While the task performance recovers within 5k steps, the drift performance does not degrade to previous levels. For the second reset, the EMA model is slightly more drifted and so the reset is less pronounced for both task and drift, leading to faster task recovery but slightly more drift. Overall, Elastic Reset shows state-of-the-art results on the benchmark and outperforms all previous small-scale methods.

## 5 Pareto Frontier Graph

Simply evaluating validation curves side-by-side or looking at a table of final scores, it can be unclear which method is better if one drifts less but the other achieves a higher reward e.g. Multitask vs KL penalty in Table 1. Previous work on this (Lee et al., 2019) and other benchmarks (Ramamurthy et al., 2022) compare methods using simple point-estimates after training for a specific number of epochs. But this number of epochs is quite arbitrary as models never fully converge to a reward, they are early-stopped such that drift is not catastrophic. Since different setups may admit different levels of drift, we believe that evaluation should reflect the continuous tradeoff between task and drift. We extend Ziegler et al. (2019), and create a pareto frontier graph to plot each method's achieved task score vs drift metric on the validation set over training. We believe practioners will wish to choose the best model for some given task performance so, contrary to Ziegler et al. (2019), we plot the task score on x-axis and drift score on the y-axis. Improvement on a drift metric can either mean lower scores (perplexity) or higher scores (BLEU) so we always plot task score as increasing from bottom to top such that, graphically, a better method will functionally dominate a worse method. We plot the best achieved reward vs drift over all validation steps for the Translation Game in Figure 2(c). Not only

    & \(\) FR\(\)EN\(\)DE & \(\) FR\(\)EN \\  Frozen English & 30.8\(\)0.2 & **36.3\(\)0.1** \\ REINFORCE & **33.2\(\)0.3** & 29.6\(\)0.3 \\ + SIL & 28.2\(\)0.4 & 27.3\(\)4.4 \\ + Multitask (S2P) & 32.2\(\)0.3 & 35.2\(\)1.0 \\ + KL penalty & **33.2\(\)0.2** & 30.8\(\)0.4 \\ + Elastic Reset & 32.9\(\)0.1 & **36.3\(\)0.1** \\   

Table 1: Translation Game final validation scores

Figure 3: Comparing Elastic Reset to all baseline methods on the Translation Game. We measure (a) Task Performance with FR\(\)EN\(\)DE BLEU and (b) Language Drift with FR\(\)EN BLEU, on the validation set during finetuning. We plot the mean and standard error over 5 seeds. To compare how methods trade off the two metrics, we plot (c) the best achieved drift vs task performance.

does Elastic Reset outperform the baselines at the final validation score, but it functionally dominates such that it is the best method for all levels of task performance it achieves.

## 6 IMDB Mock Sentiment: Ablation Study for RLHF

SetupNext, we scale to a larger benchmark that more closely approximates the standard RLHF setup. We use the recently released GRUE benchmark for RL training of LMs (Ramamurthy et al., 2022) and use IMDB mock sentiment (Ziegler et al., 2019), the main task where language models are susceptible to reward-hacking, shown in Figure 2. The goal is to complete an IMDB movie review with as positive a sentiment as possible. The baseline LM is GPT-2 (Radford et al., 2019) with 117M parameters further pretrained on the IMDB domain (Maas et al., 2011). We learn a DistilBERT (Sanh et al., 2020) reward model on IMDB to output a sentiment score between 0 (negative) and 1 (positive). We then train our GPT-2 LM to complete different IMDB reviews while maximizing the sentiment reward. Following Ramamurthy et al. (2022), we measure reward-hacking / drift with our model's perplexity on the true IMDB data. If we consider knowledge of the IMDB data as a useful capability, then our initial model was finetuned on IMDB to maximize log-probability, i.e. minimize perplexity, and has the maximum capabilites. We measure divergence from the initial model, and decrease in capabilities, by the increase in our trained model's perplexity on ground truth IMDB data. In contrast to the previous task, a lower perplexity score corresponds to less drift.

BaselinesOur main baseline is PPO (Schulman et al., 2017) with Ziegler et al. (2019) modifications for RLHF training, specifically adding a KL penalty with the frozen initial model (equivalent to KL with pretrained) and dynamically decaying the coefficient \(\) over training. To further increase stability, Generalized Advantage Estimation (Schulman et al., 2015) is used for the advantage estimator. We also compare to NLPO (Ramamurthy et al., 2022), a recent method that extends PPO with a masking model to counteract drift. The masking model is initialized to the pretrained model and recieves delayed updates; it is set to the online model every \(n\) steps. During training, the online model's output probabilities are restricted to the mask model's top \(p\) tokens. We use the RL4LMs library Ramamurthy et al. (2022) and their default hyperparameters for both PPO and NLPO e.g. \(=0.1\). We implement Elastic Reset on top of PPO with an EMA decay rate of \(0.995\) and greatly reduce the KL coefficient \(=0.001\) to allow the model to drift more, then reset every 17 epochs such that we get two resets / three iterations during our training.

    & \(\) Sentiment & \(\) Perplexity \\  Zero-shot &.489\(\)0.01 & 32.45\(\)0.13 \\ PPO &.596\(\)0.02 & 33.45\(\)0.40 \\ NLPO &.558\(\)0.06 & 33.12\(\)0.74 \\
**Elastic Reset** &.611\(\)0.02 & 33.32\(\)0.23 \\   

Table 2: IMDB mock sentiment final test scores

Figure 4: Plotting PPO vs Elastic Reset on IMDB but splitting the results visually between resets. We measure (a) Language Drift and (b) Task Performance via Semantic Score on the validation set over finetuning. All methods also include a KL penalty. We plot mean and standard error across 5 seeds.

ExperimentsWe run all experiments for 5 seeds and report mean and standard error on our validation set for our reward, DistilBERT sentiment, and our drift score, perplexity. Following Ramamurthy et al. (2022), we run for 50 epochs (equivalent to 64k updates) and show our results in Figure 4. To make our resets more visible, we plot validation scores every epoch for Elastic Reset. Since the benchmark provides a test set as well, we compare all final models in Table 2. The PPO baseline performs quite well because it already includes a KL with the pretrained model. We find NLPO performs similarly to PPO, so we relegate NLPPO results to Appendix B. Results from the original NLPPO paper were stronger (Ramamurthy et al., 2022) but our reproduced numbers and curves were confirmed by the original authors (Ammanabrolu, 2023). Elastic Reset achieves better semantic scores much faster by using a smaller KL penalty coefficient (\(0.001\) vs PPO \(0.1\)) but also drifts more to achieve them. As with the previous task, this drift is then mitigated by the reset and we see semantic task score improve in relation to drift over the iterations. Looking at the pareto graph in Figure (c)c, we see that Elastic Reset far outpaces the baselines and provides a better tradeoff of reward vs drift for every reward.

AblationsWe empirically investigate our method through ablations. Throughout this section we run experiments on IMDB with the same hyperparameters, unless otherwise mentioned. For brevity, we plot only the pareto graphs but include all other graphs in Appendix D.3 along with these same ablation experiments for the Translation Game, with similar results.

To investigate the source of improvement in Elastic Reset, we ablate the two resets: online to EMA, and EMA to initial model. We discard the second reset to get Reset to EMA: our model is reset to an EMA but the EMA is never reset. We also compare to the simplest reset idea, Reset to Init, and reset our policy to the initial model. We run all methods as previously and plot the pareto graph in Figure (a)a, for task and drift graphs see Appendix D.3. We find that even simple resets are already performant but the two ablations have a tradeoff: Reset to EMA is better at lower reward because it maintains performance whereas Reset to Init does better at higher reward because it doesn't drift as much. Elastic Reset combines the benefits of both and outperforms each method.

Next, we consider our method's robustness to hyperparameters. First, we search along different EMA decay rates \(\) and plot our results in Figure (b)b finding that our method is quite robust to choice of

Figure 5: Ablating Elastic Reset on the IMDB mock sentiment task. We plot pareto graphs using mean and standard error across 5 seeds.

decay. Next, we investigate robustness to the choice of KL penalty coefficient \(\). We search across coefficients that range from 10x smaller to 10x larger than our best KL penalty coefficient for PPO (\(=0.1\)) and Elastic Reset (\(=0.001\)). For visual clarity, we plot PPO in Figure 4(c) and Elastic Reset in Figure 4(d) and only plot four points for PPO \(=0,0.01\) to maintain visual scale. We find that PPO is not robust to choice of KL and larger values correspond to better pareto curves but slower training. Results with NLPO are similar and shown in Appendix D.3. In contrast, Elastic Reset seems to be more robust to choice of KL with \(0.001\) producing the best curves while 10x larger and smaller values are similar. As opposed to PPO, Elastic Reset even works reasonably well without a KL penalty at all, (\(=0\)), matching PPO's best performance with a KL. This demonstrates that the expensive KL penalty may be replaced with the cheap Elastic Reset, although the combination of the two is best. This is also in line with previous work that have argued that the KL penalty may be unnecessary for RLHF . We also ablate the frequency of resets in Appendix D.4.1 and find that pareto curves are essentially unchanged.

Finally, we provide an empirical intuition for Elastic Reset: in Appendix E.1 we show that resets iteratively improve the value function and in Appendix E.2 we show how EMA smoothes optimization but requires resetting in order to achieve high performance.

## 7 StackLLaMA: Practical RLHF

SetupFinally, we apply Elastic Reset to a larger-scale RLHF pipeline. We choose LLaMA  as it is a prominent open-source model that has demonstrated strong performance on benchmarks. We follow Beeching et al.  to finetune LLaMA-7B with RLHF on the StackExchange dataset  to output helpful answers to technical questions, as judged by humans. Users ask technical questions on StackExchange and upvote the best answers. We score answers from StackExchange based on the number of upvotes they received from users, \(=_{2}(1+)\). At most 10 answers are drawn per question, text is cleaned, and HTML is converted to Markdown to make it easier to parse. First, we finetune LLaMA-7B with language modelling on the dataset to get LLaMA-7B-SE. We then further finetune it to get a reward model by learning to predict which of two answers was more upvoted . For a given question \(x\) and two answers \(y_{+,-}\) (where \(y_{+}\) is preferred), the loss for our reward model \(r_{}\) is \((r_{}(x,y_{+})-r_{}(x,y_{-}))\). Finally, we finetune LLaMA-7B-SE with RL against the reward model by sampling questions from the dataset and learning to optimize the reward for our model's answer. All finetuning is done with a converted 8-bit model  and LoRA  for efficiency and to make the model training fit on our GPUs. We rely on the HuggingFace **rl** and peff  libraries. All technical details are described in Appendix C.

ExperimentWe again compare to PPO with Ziegler et al.  modifications i.e. KL penalty with a dynamically decaying coefficient. We run for 600 epochs (equivalent to 100k updates) and Elastic Reset every 260 epochs to get two resets / three iterations. Each run takes 20 hours on 4 A100s. Since only the LoRA parameters are being learned, we use Elastic Reset on those and therefore maintain only a small percentage of parameters in our EMA. We use a decay rate \(=0.995\) and a KL penalty coefficient \(=0.02\) for both methods. Calculating perplexity for each epoch is computationally

Figure 6: Elastic Reset compared to PPO on StackLLaMA: A LLaMA-7B model RLHF finetuned on StackExchange as a helpful, technical QA chatbot

infeasible so we measure drift during training with the KL from the pretrained model over samples as done previously in other larger-scale RLHF (Bai et al., 2022; Gao et al., 2022).

ResultsWe plot reward in Figure 5(a)2 and KL from initial model over training in Figure 5(b). As noted by Beeching et al. (2023), the task is much noisier at a larger scale and with a real HF reward model. As a sanity check, we find that Elastic Reset tracks PPO until the first reset at 260 epochs where it drops only slightly, but also doesn't lose much performance. Around the second reset at 520 epochs, we see a much sharper drop but also maintaining the same approximate reward. At the end, Elastic Reset provides a non-trivial reduction in drift while aligning just as well as PPO. The pareto curve in Figure 5(c) shows Elastic Reset is equal or slightly worse at low reward but shows large improvements over PPO at higher reward. Notably, Elastic Reset seems to work out-of-the-box with LoRA. To evaluate drift another way, we get the perplexity of the final models over the StackExchange validation set as in Section 6. For a more robust view of reward, we train two more reward models using different seeds and evaluate the increase in reward between initial and final models. We show mean and standard deviation across the three reward models in Table 3, we find that Elastic Reset achieves a slightly better final reward than PPO while maintaining lower perplexity on the data. To examine a true alignment tax, we run our models on HumanEval Chen et al. (2021), a programming benchmark that provides another view of drift. Answering human-written coding questions is both a useful capability for our model and also falls within a similar domain to StackExchange. The benchmark tests for functional correctness such that pass@1 corresponds to the percentage of problems solved by the model on the first try as shown in Table 3. Training with PPO degrades performance compared to the initial model, demonstrating a large alignment tax. In contrast, Elastic Reset achieves a similar reward but maintains performance, even slightly improving on pass@10, creating an alignment bonus (Askell et al., 2021) instead of tax.

## 8 Limitations

As a method, Elastic Reset is quite cheap computationally because both EMA updates and resets take negligable time compared to RLHF training and the EMA model can be stored on CPU. But our method is sensitive to the choice of reset rate; we chose heuristically based on when it seemed the model was overfitting. It is also possible to reset the policy and EMA model at different time scales, which could be a source of improvement. Our method also resets all of the trainable parameters, research in similar methods suggests that resetting larger models can benefit from resetting only part of the network (Zhou et al., 2022; Nikishin et al., 2022) or weighted-averaging instead of resets (D'Oro et al., 2023). We leave both of these directions to future work.

Although we have thoroughly investigated our method on three different tasks, we note that none of them are ideal RLHF benchmarks. As pointed out by Gao et al. (2022), we measure our model's performance using the same reward model we optimize. This can lead to reward model overoptimization and our metric could mask overfitting and lack of generalization to the real world i.e. actual human preferences. An ideal benchmark could include a "gold" reward model as a proxy for human preference (Gao et al., 2022), but no such benchmarks are open-sourced and available.

    & \(\) Reward & \(\) Perplexity & \(\) HumanEval (pass@1,pass@10) \\  Zero-shot & 0 & 4.43 & 11.0, 12.7 \\  PPO & 0.81 \(\) 0.06 & 4.62 & 7.8, 10.7 \\
**Elastic Reset** & 0.96 \(\) 0.09 & 4.57 & 11.0, 13.0 \\   

Table 3: Evaluations of the initial (zero-shot) and finetuned StackLLaMA models after 600 epochs. We measure alignment using an average over three reward model trained with three different seeds and drift with perplexity on the data. HumanEval is a programming benchmark that acts as a practical measure of drift / alignment tax.

Finally, we note that we follow all previous RLHF work and investigate only on-policy methods (Ziegler et al., 2019; Stiennon et al., 2020; Askell et al., 2021; Bai et al., 2022). Previous work in resetting for RL has focused on off-policy methods and demonstrated strong performance (Nikishin et al., 2022; D'Oro et al., 2023). As previously noted, our method can be seen as an adaptation of those to on-policy RL. In RLHF, PPO is by far the most popular method and on-policy is the dominant paradigm since it guarantees better local gradients. But it is possible that off-policy methods could implicitly balance performance and drift by incorporating a replay buffer with older data.

## 9 Conclusion

The problems of drift (Lee et al., 2019), alignment tax (Askell et al., 2021), reward model overoptimization (Gao et al., 2022), and reward hacking (Clark and Amodei, 2016) are inherent to RLHF and reduce its efficacy. We have introduced a simple but powerful new method, Elastic Reset, to tackle this problem and improve performance while maintaining linguistic capabilities. We have shown its ability on three different tasks and across three different scales: from 6 layer Transformers to GPT2 to LLaMA-7B. The problem of drift is currently being addressed with a standard KL penalty despite the computational cost, tradeoff with reward, and recent claims that is may be unnecessary (Bai et al., 2022; Gao et al., 2022). Elastic Reset is a cheap and effective method to tackle the same problem, achieving a better tradeoff of reward and drift while reducing alignment tax. We hope our method leads to better RLHF and therefore models that are closer aligned with human preferences (Ziegler et al., 2019). As well, we hope this work invigorates more research into improving the reward / drift tradeoff of RLHF with a focus on computationally efficient methods that scale.