# Empowering Convolutional Neural Networks with MetaSin Activation

Farnood Salehi\({}^{1}\)

Tunc Ozan Aydin\({}^{1,3}\)

Andre Gaillard\({}^{3}\)

Guglielmo Camporese\({}^{1,2}\)1

Yuxuan Wang\({}^{3}\)

\({}^{1}\)Disney Research | Studios \({}^{2}\)University of Padova \({}^{3}\)ETH Zurich

This work was done while the author was an intern at Disney Research | Studios

###### Abstract

Relu networks have remained the default choice for models in the area of image prediction despite their well-established spectral bias towards learning low frequencies faster, and consequently their difficulty of reproducing high frequency visual details. As an alternative, \(\) networks showed promising results in learning implicit representations of visual data. However training these networks in practically relevant settings proved to be difficult, requiring careful initialization, dealing with issues due to inconsistent gradients, and a degeneracy in local minima. In this work, we instead propose replacing a baseline network's existing activations with a novel ensemble function with trainable parameters. The proposed MetaSin activation can be trained reliably without requiring intricate initialization schemes, and results in consistently lower test loss compared to alternatives. We demonstrate our method in the areas of Monte-Carlo denoising and image resampling where we set new state-of-the-art through a knowledge distillation based training procedure. We present ablations on hyper-parameter settings, comparisons with alternative activation function formulations, and discuss the use of our method in other domains, such as image classification.

## 1 Introduction

Deep convolutional neural networks are highly proficient in a wide array of tasks including image prediction applications. While there is no scarcity of inventive methods when it comes to designing convolutional blocks (e.g. ), another crucial design element, namely the activation function, has seen little change over the years. The Relu activation, despite (and arguably because of) its simplicity has remained the default choice for majority of the models in the area of image prediction.

It is often desirable for models that produce visual results to produce sharp imagery with abundant high frequency details. The widespread use of Relu networks for image prediction applications is somewhat surprising given their spectral bias towards learning low frequencies first , and consequently their difficulty in reproducing high frequency visual details. While \(\) activations have been shown to be superior in simple overfitting experiments on image reconstruction , empirical evidence showed that training \(\) networks in practically relevant settings is challenging even when employing meticulous initialization schemes. While some exploration on \(\) activations with convolutional networks exists , their use has been mostly limited to fully connected architectures.

In this work we make the following contributions:* We shed light on challenges associated with training networks with \(\)-based activations through an extensive set of experiments, and address those by formulating the MetaSin activation coupled with a stable training procedure.
* We show that MetaSin networks consistently enable improvements over alternatives when tested on comparable architectures in a variety of image prediction applications, including Monte-Carlo denoising and image resampling where we set new state-of-the-art.
* We present a wide range of ablations on hyperparamter settings, comparisons with various alternative activations, and discuss the use of MetaSin networks in other domains such as classification and neural signal representations.

## 2 Related Work

**Activation Functions** In addition to standard non-polynomial functions such as Sigmoid, \(\), \(\), Relu, and \(\), numerous alternative activation functions have been explored in the literature. Previous work explored Relu variants such as Leaky Relu, Parametric rectified linear unit (PReLU) , and Gaussian error linear unit (GELU) . Other authors presented activations with exponential components, such as the Exponential linear unit (ELU) , Scaled exponential linear unit (SELU) , as well as Sigmoid variants such as Softplus , and Swish. A comparative study of these activations re-affirmed Relu as a strong baseline, while suggesting Swish might be a better alternative for image classification . Another study  reported no accuracy change when switching from Relu to GELU in image classification. The aforementioned body of work has mostly focused on classification experiments, whereas their use in image prediction models was left mostly unexplored.

Previous investigations  pointed out periodic activations as a promising direction, which was later explored in the form of \(\) activations for implicit neural representations of continuous signals . More recent work proposed modulating the amplitudes of \(\) functions using a secondary fully connected network . Another \(\) variant is Snake , which takes the form \(x+sin^{2}(x)\) and focuses on improving extrapolation of periodic functions. Finally, several papers explored combining multiple primitive functions, such as Mish (\(x((x))\)) , SinLU (\(\) and Sigmoid) , and activation ensembles . Preliminary experiments  suggest that activations alternative to Relu, with the exception of \(\), tend to perform sub-par on image reconstruction. The popularity of Relu networks in image prediction tasks contradicts the theoretical analysis presented in previous work  that established the spectral bias of Relu networks towards learning low frequencies faster. Later work  re-affirmed this finding through a neural tangent kernel  analysis.

**Image Resampling** Tasks such as correcting lens distortion, retargeting, and upsampling are some image resampling examples that are considered as key operations in real-world computer vision and image processing pipelines. Recent neural methods  have been shown to outperform traditional techniques, such as upsampling with a fixed kernel. These neural networks often comprise convolutional blocks  with Relu activations. Typically utilizing deep networks, current resampling methods can learn complex representations of input images and produce high-quality resampled outputs. In this work, we employ MetaSin to improve upon a recent state-of-the-art resampler .

**Monte-Carlo Denoising** Generating high quality rendered images may require prohibitive amount of compute resources. This can be alleviated by stopping the rendering process prematurely and employing a specialized denoiser to remove the residual noise. Neural denoisers have demonstrated superior performance compared to classical denoising techniques . These denoisers often leverage U-Net with Relu activation , which allows them to process noisy images across multiple scales. We likewise demonstrate the use of MetaSin in such U-Nets and achieve significant improvements in denoising quality.

## 3 MetaSin Activation

Let us define a generic neural network \(g()\) of depth \(L\) that predicts a target image \(^{d}\) given input \(^{d^{}}\) as:

\[ g()=(^{[L]} T^{[L]}^{[L-1] } T^{[L-1]}^{} T^{})(),\] (1)where \(T^{[l]}\) denotes a linear transformation that results from either a convolutional or fully connected layer. We would like to find a set of activations \(^{}^{[L]}\) such that they minimize the average reconstruction error \(_{i}^{n}(g(^{(i)})-^{(i)})\) over a dataset \(\{^{(i)},^{(i)}[1,n]\}\).

In hypothetical limit cases where the network \(g\) has infinite width or depth, few constraints on activation functions are necessary for the resulting architecture to have the universal approximation capability . However, in practice the selection of the activation functions can play a significant role in the training of neural networks, specifically in the resulting model's prediction accuracy and, in case of image prediction models, the visual characteristics of the predictions. While it is unclear how a principled method can be developed for rank ordering all possible selections of activations, empirical evidence suggests that multi-layer perceptron (MLP) networks with \(\) activations exhibit desirable characteristics both in expressiveness and their ability to reproduce rich high frequency details , especially in comparison to Relu networks that suffer from spectral bias towards reproducing low frequencies .

However, practical experience also revealed the difficulty of training deep convolutional \(\) networks in real-world scenarios. Figure 1 shows an illustrative example that on one hand demonstrates the clear advantage of \(\) activations over Relu when training shallow networks, but on the other hand illustrates the fragility of training deeper \(\) networks that are closer representatives of real-world models. To examine further we express an intermediate segment from a \(\) activated network as:

\[^{[l]}=T^{[l]}(^{[l-1]}),\] (2)

where \(^{[l]}\) is the output of layer \(l\) after applying the corresponding linear transformation and activation function. We can also express the usual parameterization of sine waves as \(f(;c,f,p)=c(f+p)\), where \(c\), \(f\), and \(p\) are the amplitude, frequency, and phase of the sine wave that we collectively refer as the _shape parameters_.

In overparameterized networks the initial parameter values tend to change very slowly  (also known as lazy training). Therefore ensuring the selection of plausible initial values for the shape parameters in \(\) networks is crucial for training. Some exploration has been done in this direction: For instance, Siren introduces a scaling constant \(w_{0}\) that is applied to the weight matrix \(W\) of the first layer of the underlying fully connected network, i.e. \((w_{0}W+)\), whereas recent work explored introducing an explicit amplitude parameter whose value is predicted through a secondary neural network . Yet initialization remains a challenge as in practice only a very narrow range of values leads to plausible average loss (Refer to Figure 10 in Appendix C).

Figure 1: Peak Signal-to-Noise Ratio (PSNR) comparison of different activations employed on CNNs to overfit an image from its downsampled version (with downsampling factor \(16\)). Notably, while the shallow \(\) network performs better than its Relu counterpart, the deep \(\) network fails to converge, demonstrating the fragility of training deep convolutional networks with \(\) activations. See Appendix B for details.

In this work, we instead introduce explicit shape parameters for directly controlling amplitude, frequency, and phase. These parameters are completely disentangled from other network parameters, and can be initialized intuitively by reasoning about the shape of resulting sine wave. Moreover, in order to provide better initial coverage of plausible ranges of the shape parameters, we construct a composite periodic function by linearly combining \(K\) sine waves with individual shape parameters:

\[_{j=1}^{K}c_{j}^{[l]}(^{[l]}f^{[l]}+p^{[l]}),\; \;^{[l]}=T^{[l]}(^{[l]}).\] (3)

The shape parameters are shared among individual channels of layers in convolutional neural networks, and are optimized through backpropagation along with other network parameters2. In our experiments we observed that initializing the frequency parameters \(f_{j}^{[l]}\) of the \(K\) constituent sine waves of the composite function to cover a large range of frequencies consistently yields improved results, without necessitating lengthy trial and error procedures that are often not feasible when training large models in practice (Refer to Appendix C for an illustrative example).

Beyond the initialization of shape parameters, the remaining known causes of the difficulty when training \(\) networks are inconsistent gradients due to the complex shapes that activations can take, and the large degeneracy in local minima caused by symmetries . We empirically found that including an additional Relu component to the composite sine wave function discussed before stabilizes training. With this modification our proposed activation, which we call MetaSin, can be expressed as follows:

\[(^{[l]})=c_{0}^{[l]}(^{[l]})+ _{j=1}^{K}c_{j}^{[l]}(f_{j}^{[l]}^{[l]}+p_{j}^{[l]}).\] (4)

Equation 4 can efficiently be implemented as a module in common deep learning frameworks, which then can be used to replace activation functions of an existing state-of-the-art network architecture. The trainable parameters \(=\{c_{0}^{[l]},c_{j}^{[l]},f_{j}^{[l]},p_{j}^{[l]};j[1,K],l[1,L]\}\) are optimized through backpropagation without requiring an additional network for predicting these particular parameters, or any other changes to the underlying architecture. This non-intrusive property enables a simple three-step procedure, where (i) we identify a state-of-the-art image prediction model, (ii) replace its existing activation functions with MetaSin, and (iii) re-train the resulting MetaSin network using a knowledge distillation scheme we discuss in the following section.

### Training

In order to train convolutional MetaSin networks, we initialize the shape parameters as \(c_{0}=1\), \(c_{j}^{[l]}=0\), \(f_{j}^{[l]}=j\), and \(p_{j}^{[l]}=(0,)\), for \(j[1,K]\), and \(l[1,L]\) by default. This initialization on one hand forces MetaSin to initially assume the shape of Relu, and gradually introduce sin components during the course of the training, while on the other hand covering a broad range of initial frequency and phase values (See Appendix G). Using the default initialization we were able to reliably avoid typical issues encountered while training \(sin\) networks.

It is often easier to train a shallower network compared to a deeper one as supported by previous studies . This observation also applies to \(\) activations, as demonstrated in Figure 1, where a shallow Siren network can be effectively trained while a deep Siren network encounters difficulties. To address this, we employ feature knowledge distillation (KD) from a teacher network  to provide auxiliary signals during the training of the student MetaSin network. We first train a Relu model as the teacher (or, as it often happens in practice, use an already existing baseline Relu model) and use its intermediate feature maps as supervisory signals. This approach allows us to train the shallower blocks of the student MetaSin network individually, instead of propagating gradients throughout the entire network. We refer to this phase of training as _KD-Bootstrapping_, which comprises approximately \(5-10\%\) of the total training iterations for the MetaSin network. Following the initial KD-Bootstrapping phase, the student MetaSin network is trained using the same configuration as the baseline network. This approach enhances the learning process and helps improve the performance of the student network.

### Efficient Implementation

While equation 4 is straightforward to implement in both PyTorch and Tensorflow using the corresponding Python APIs, such native implementations end up being inefficient compared to executing a Relu function. For performance critical applications, efficiency issues can be alleviated through writing a customized operation in CUDA (Appendix F). In Table 1 we compare the latency induced by the native PyTorch implementation against our customized MetaSin operator with fused CUDA kernel functions. Our optimized implementation significantly reduces the overhead of the native implementation. During forward pass, executing a MetaSin with \(K=10\) components requires less than three times the time it takes to execute a Relu function, whereas the backward computation is only slightly more expensive than a Relu activation. Our efficient implementation also uses roughly the same amount of memory during execution as a Relu activation, making MetaSin networks feasible to use in practice on the same hardware that a comparable Relu network is designed for.

## 4 Experiments

We present experiments where we apply the three step procedure discussed at the end of Section 3. Specifically, we switch an existing architecture's activations to MetaSin, and re-train the resulting new model from scratch using KD-Bootstrapping.

### Image Resampling

In image resampling our approach builds upon the state-of-the-art network proposed by Bernasconi et al. , which incorporates ProSR  feature extractor with \(3\) residual blocks, a resampling layer, and a prediction layer. The network takes an input image and a warp grid and performs image resampling. We trained baseline networks following the exact same training procedures and dataset as described in . The MetaSin network is obtained by replacing Relu activations of the feature extractor with MetaSin with \(K=10\) and \(f_{j}^{[l]}=j\) at initialization. All other parameters are initialized as described in Section 3.1. The transition to MetaSin activations increases the amount of compute by 3% (and similarly the wall-clock inference time by 3%) compared to the baseline Relu network. To rule out any improvement due to this increase in computation, we accordingly increase the number of channels of baseline Relu network, which we label as Relu\({}_{E}\).

We evaluate the models on three predetermined sets of projective transforms with average local scaling factors of 2 (\(P_{ 2}\)), 3 (\(P_{ 3}\)) and 4 (\(P_{ 4}\)). We report the PSNR scores of the resampled images in Table 2, as well as visual examples in Figure 2 We use KD-Bootstrapping during the first 10% of the training procedure. On average the MetaSin network with KD-Bootstrapping improves over the Relu\({}_{E}\) baseline by \(0.2\)dB, setting a new state-of-the-art in image resampling. We also conducted KD-Bootstrapping on Relu, Mish and Snake networks, but observed no improvement.

   Module & Impl. & Forward (rel) & Backward (rel) \\  MetaSin & PyTorch Native & 26.1x & 1.58x \\  & PyTorch CUDA & **2.9x** & **1.12x** \\   

Table 1: Latency of executing a MetaSin activation with \(K=10\) using native vs. efficient implementations, relative to the latency of executing a single Relu activation on Nvidia RTX 3090.

    & & & &  \\  Factor & Relu & Relu\({}_{E}\) & Snake & Mish & Siren & Siren &  &  &  &  &  \\  & & & & & KD-B & & KD-B & & KD-B & \\  \(P_{ 2}\) & \(33.03\) & \(33.04\) & \(32.95\) & \(32.96\) & \(31.75\) & 32.76 & \(32.78\) & \(33.02\) & \(33.02\) & \(\) \\ \(P_{ 3}\) & \(29.36\) & \(29.36\) & \(29.16\) & \(29.06\) & \(28.18\) & 28.98 & \(29.16\) & \(29.36\) & \(29.35\) & \(\) \\ \(P_{ 4}\) & \(27.09\) & \(27.10\) & \(26.97\) & \(26.89\) & \(26.20\) & 26.83 & \(26.97\) & \(27.10\) & \(27.11\) & \(\) \\   

Table 2: Comparison of PSNRs obtained from various image resampling models. See text for details.

This outcome suggests that the training process for these networks is inherently stable, making KD-Bootstrapping unnecessary for these activations. Refer to Appendix K for additional results.

### Denoising Monte-Carlo Rendered Images

Next, we present our experiments in denoising Monte-Carlo rendered images. Our base denoiser network is a U-Net architecture used in  and we use the same procedure to generate noisy and reference renderings. Since we focus on direct image prediction models rather than kernel denoising as in , our U-Net denoiser (DPCN) directly predicts pixel values of the clean image (For completeness, the results of kernel denoising (KPCN) are presented in Appendix M). During evaluation, we denoise images rendered with \(\) samples per pixel and use SMAPE, FLIP , \(1-\)MS_SSIM, and \(1-\)SSIM as our metrics. Our best MetaSin network is configured with \(K=5\) and the frequencies are initialized as \(f_{j}^{[l]}=j/2\). We use KD-bootstrapping during the first 5% of the training procedure. We present a summary of various MetaSin network configurations in Table 3, and selected visual examples in Figure 3.

## 5 Discussion

In this section we present ablation experiments and an exploratory study on using MetaSin activations for image classification and implicit image representation using MLPs. We present preliminary investigations of further applications of MetaSin activation in Appendix A

Figure 2: Example upsampling results comparing the state-of-the-art model with Relu activations  and the corresponding MetaSin network.

    &  & MetaSin-5 & MetaSin-10 & MetaSin-5 & MetaSin-5 \\  & & (\(f_{j}=j\)) & KD-B (Glorot) & KD-B (\(f_{j}=j\)) & KD-B (\(f_{j}=j/2\)) \\  SMAPE & 3.351 & 3.16 (-5.7\(\%\)) & 3.118 (-6.95\(\%\)) & 3.088 (-7.85\(\%\)) & **3.081 (-8.06\(\%\))** \\ FLIP & 1.084 & 0.991 (-8.58\(\%\)) & 0.972 (-10.33\(\%\)) & 0.976 (-9.96\(\%\)) & **0.965 (-10.98\(\%\))** \\
1-MS\_SSIM & 3.106 & 2.838 (-8.63\(\%\)) & 2.784 (-10.37\(\%\)) & 2.757 (-11.24\(\%\)) & **2.733 (-12.01\(\%\))** \\
1-SSIM & 6.895 & 6.385 (-7.4\(\%\)) & 6.227 (-9.69\(\%\)) & 6.146 (-10.86\(\%\)) & **6.112 (-11.36\(\%\))** \\   

Table 3: Results of various MetaSin configurations. All relative values are in relation to the DPCN baseline. The set of test samples contains all samples with spp \( 16\). The frequency initializations are given in parenthesis, which shows that a slight improvement can be made by initializing as \(f_{j}=j/2\) rather than the default \(f_{j}=j\). Switching to Glorot initialization results in reduced accuracy despite the doubled \(K\). Introducing KD-Bootstrapping during training significantly improves accuracy.

### Comparison with Alternative Activations

We ran several experiments to assess the comparative improvements that can be achieved by using alternative activations instead of MetaSin. In addition to Relu, we also tested Snake (\(x+^{2}(x)\)) in ), Mish (\(x((1+e^{x}))\) in ), and MRelu in which we linearly combine several Relu activations in the form \(_{j=1}^{K}c_{j}(x+b_{j})\) (Similar to Adaptive Piecewise Linear Units ). The Snake activation  performs slightly worse than the Relu baseline in the image resampling application, which in turn falls below the corresponding MetaSin activated network (Table 2 - \(3^{nd}\) col). In case of Monte-Carlo denoising it was not possible to train the U-Net model with Snake activations as the training diverged. Mish similarly performs slightly worse than both Relu and Snake in image resampling (Table 2 - \(4^{th}\) col).

In Monte Carlo denoising the Mish activated network performs better than the Relu baseline by \(3\%\) in terms of MS_SSIM, however still falls significantly behind the MetaSin activated network that achieves \(~{}10\%\) improvement in the same experiment (Table 4).

The MRelu activation with \(K=10\) performs significantly worse than the Relu baseline both in denoising and resampling task (Table 4 - \(3^{rd}\) col and Table 2 - \(7^{th}\) col). We also tested applying

    &  &  &  &  &  &  \\  & & & & KD-B & & & KD-B \\  SMAPE & 3.351 & 3.472 (+3.61\%) & 3.254 (-2.89\%) & 3.293 (-1.73\%) & 3.346 (-0.15\%) & **3.118 (-6.95\%)** \\ FLIP & 1.084 & 1.096 (+1.11\%) & 1.031 (-4.89\%) & 1.054 (-2.77\%) & 1.06 (-2.21\%) & **0.972 (-10.33\%)** \\
1-MS\_SSIM & 3.106 & 3.397 (+9.37\%) & 2.963 (-4.6\%) & 3.011 (-3.06\%) & 3.066 (-1.29\%) & **2.784 (-10.37\%)** \\
1-SSIM & 6.895 & 7.044 (+2.16\%) & 6.589 (-4.44\%) & 6.896 (+0.01\%) & 6.736 (-2.31\%) & **6.227 (-9.69\%)** \\   

Table 4: Comparison of various alternative activations on the DPCN model discussed in Section 4.2. All parameters are initialized using Glorot initialization. See text for discussion.

Figure 3: Monte-Carlo denoising results. The first row shows results produced with an input noisy images rendered at 4 samples-per-pixel, whereas the input in the remaining two rows are rendered at 16 samples-per-pixel.

KD-Bootstrapping to MRelu which made a big improvement in all metrics, still, however, scoring far behind the corresponding MetaSin network.

Overall, MetaSin activations ended up consistently yielding notably better results than the activations we tested in all our experiments. In Appendix H we present further comparisons with ensemble activations.

### The Effect of Hyperparameters

The MetaSin activation as formulated in Equation 4 has a single hyperparameter \(K\) that determines the number of constituent sine waves. While in our experiments we often set \(K=10\), we also run ablation experiments to measure the effect of different \(K\) values. In these experiments we used the exact same settings as in Section 4.2, except varying the \(K\) parameter. In Table 5 we show the effect of changing \(K\) to \(1\), \(5\), and \(12\), compared to the baseline where \(K=10\). Not surprisingly, lowering \(K\) results in consistently worse results according to all four metrics we tested. These findings also show that the results that we presented in Section 4.2 can be further improved simply by setting \(K=12\) (See Appendix I for further discussion). Aside from \(K\), another decision to be made when using MetaSin activations is whether to train with KD-Bootstrapping or not. While in toy examples we haven't noticed any difference between the two alternatives, the last two columns in Table 4 show a comparison with and without KD-Bootstrapping, which clearly demonstrates the benefit of employing KD-Bootstrapping in a real-world scenario.

### Image Classification using Convolutional MetaSin Networks

We ran several preliminary experiments for exploring the use of MetaSin networks outside our target domain of image prediction tasks. Here we report the results of various experiments in a classification setting, where we trained various student Wide-ResNets using supervision from comparably larger Wide-ResNets that we used as teacher networks. As the teacher network in this case is a larger network with higher accuracy, replicating its predictions becomes a reasonable objective. Therefore, we employ feature knowledge distillation throughout the entire training procedure. The Wide-ResNet architecture  follows the same architectural design as ResNet , except that the number of channels in each residual block is expanded.

The MetaSin activated students are identical to their Relu activated counterparts, except the activations are replaced by MetaSin with \(K=8\). We ran our experiments on the CIFAR100  dataset, which consists of 50K training and 10K validation images with resolution \(32 32\). The student networks were trained for 300 epochs using standard cross-entropy loss following the knowledge distillation methodology discussed in . While not setting a new state-of-the-art, Table 6 shows that the MetaSin student networks consistently outperform their Relu counterparts, and in some cases also the corresponding Swish student networks, motivating further investigation of using MetaSin activations in classification tasks. However, we note that the accuracy of MetaSin models are less competitive compared to alternatives when models are trained from scratch without knowledge distillation (Appendix J).

### MetaSin with Multi-Layer Perceptrons

While in this work we focused on applications of MetaSin with convolutional neural networks, we also ran several exploratory experiments where we used MetaSin activations with MLPs to perform implicit neural representation tasks as in . We refer the reader to Appendix A.1 for a discussion on video overfitting, where the MetaSin network achieves 1-4 dB PSNR improvement over the Siren

    & MetaSin-10 & MetaSin-1 & MetaSin-5 & MetaSin-12 \\  & KD-B & KD-B & KD-B & KD-B \\  FLIP & 0.972 & 0.997 (+2.57\(\%\)) & 0.979 (+0.72\(\%\)) & **0.967 (-0.51\(\%\))** \\
1-MS\_SSIM & 2.784 & 2.825 (+1.47\(\%\)) & 2.805 (+0.75\(\%\)) & **2.767 (-0.61\(\%\))** \\   

Table 5: Comparison of DPCN networks (discussed in Section 4.2) with various \(K\) parameters trained with KD-Bootstrapping and Glorot frequency initialization.

baseline with noticeably less visual artifacts. In Appendix A.3 we discuss a voxel grid overfitting experiment where the MetaSin network exhibits significantly improved detail reconstruction over the Relu baseline. We also present a preliminary novel view synthesis experiment where we compare a baseline Neural Radiance Field (NeRF) method  with a corresponding MetaSin model with \(K=10\).

We present an example result from this experiment in Figure 4, and refer the reader to Appendix A.2 for more details. Despite not utilizing positional encoding, the MetaSin network surpasses vanilla NeRF in performance. It has been observed that Fourier features, which bear similarities to positional encoding, aid in learning high-frequency functions. This experiment implies that such feature engineering may not be crucial, as our MetaSin network achieves comparable or even superior performance. While this result is intriguing, we acknowledge recent techniques that leverage latent codes in the form of memory, combined with specialized data structures [31; 41], which can significantly enhance vanilla NeRF in terms of performance and training time. While not conclusive, we hope the initial results we share in this section motivate further investigation of MetaSin activations in the context of other architectures and application areas.

## 6 Conclusion and Limitations

We presented a novel activation function with trainable parameters that improves upon state-of-the-art convolutional image prediction models. Our work has several limitations. While MetaSin networks can reliably be trained without it, training with KD-Bootstrapping helps the network to eventually converge to better minima. While it is not uncommon when training a MetaSin network to have access to a comparable trained Relu network, nevertheless this requirement might lead to some extra effort when a pre-trained base model is not available. Additionally, in our current formulation (Equation 4) the number of \(\) components \(K\) is a hyperparameter that we set manually at design time. We performed preliminary experiments with determining \(K\) automatically in an adaptive fashion, which would be an interesting direction to study more in the future.

The MetaSin function is only \(C^{0}\) in contrast to the infinitely differentiable Siren. The \(C^{}\)) property allows Siren networks to be trained using loss functions on the gradient or Hessian of the network. This distinct characteristic of the Siren network is used later in  to automatically integrate the function that appears in neural volume rendering. The lack of a gradient loss in MetaSin networks seems to limit the performance of MetaSin activations for applications such as implicit 3D shape representations (Appendix N). It would be interesting to tackle this inherent limitation of MetaSin networks in future work.

  Teacher & WRN-40-2 &  & WRN-28-2 & WRN-40-2 \\ Student & WRN-40-1 & WRN-40-1 & WRN-40-1 & WRN-16-2 & WRN-16-2 & WRN-16-2 \\ Teacher Acc. & 76.08 & 76.88 & 74.82 & 76.08 & \\  Activ. & Relu & MetaSin & Swish & Relu & MetaSin & Swish & Relu & MetaSin & Swish & Relu & MetaSin & Swish \\  Val. Acc. & 73.39 & **73.74** & 73.28 & 70.53 & **72.55** & **72.54** & 71.76 & 72.33 & **72.98** & 73.65 & **74.10** & 72.60 \\  

Table 6: Comparing Relu student networks with MetaSin activated networks with otherwise identical architecture, both trained through knowledge distillation from the same Relu activated teacher.

Figure 4: Zoomed qualitative results on scene _flower_. Comparison between ground-truth, NeRF  and MetaSin-10 model, with corresponding PSNR scores in parenthesis.

## Broader Impact

Fundamentally, neural network blocks comprise a linear transformation and a non-linear activation. Our work introduces a new activation function that enables significant prediction accuracy improvements over previous alternatives. Being at such a fundamental level, our proposed contribution thus can be applied to a wide array of neural network types that perform a variety of tasks and facilitate improvements upon existing state-of-the-art results. The resulting improved models may be leveraged for both positive and negative ends.