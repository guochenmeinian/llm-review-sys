# Your contrastive learning problem is secretly a distribution alignment problem

Zihao Chen\({}^{*}\), Chi-Heng Lin, Ran Liu, Jingyun Xiao, Eva L. Dyer\({}^{*}\)

School of Electrical & Computer Engineering

Georgia Tech, Atlanta, GA

Contact: {zchen959, evadyer}@gatech.edu

###### Abstract

Despite the success of contrastive learning (CL) in vision and language, its theoretical foundations and mechanisms for building representations remain poorly understood. In this work, we build connections between noise contrastive estimation losses widely used in CL and distribution alignment with entropic optimal transport (OT). This connection allows us to develop a family of different losses and multistep iterative variants for existing CL methods. Intuitively, by using more information from the distribution of latents, our approach allows a more distribution-aware manipulation of the relationships within augmented sample sets. We provide theoretical insights and experimental evidence demonstrating the benefits of our approach for _generalized contrastive alignment_. Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment. By reframing contrastive learning as an alignment problem and leveraging existing optimization tools for OT, our work provides new insights and connections between different self-supervised learning models in addition to new tools that can be more easily adapted to incorporate domain knowledge into learning.

## 1 Introduction

In machine learning, the availability of vast amounts of unlabeled data has created an opportunity to learn meaningful representations without relying on costly labeled datasets . Self-supervised learning has emerged as a powerful solution to this problem, allowing models to leverage the inherent structure in data to build useful representations. Among self-supervised methods, contrastive learning (CL) is widely adopted for its ability to create robust representations by distinguishing between similar (positive) and dissimilar (negative) data pairs. With success in fields like image and language processing , contrastive learning now also shows promise in domains where cross-modal, noisy, or structurally complex data make labeling especially challenging .

Traditional contrastive learning methods primarily aim to bring positive pairs--often augmentations of the same sample--closer together in representation space. While effective, this approach often struggles with real-world challenges such as noise in views, variations in data quality, or shifts introduced by complex transformations, where positive pairs may not perfectly align. Additionally, in tasks requiring domain generalization, aligning representations across diverse domains (e.g., variations in style or sensor type) is critical but difficult to achieve with standard contrastive learning, which typically lacks mechanisms for incorporating domain-specific relationships. These limitations highlight the need for a more flexible approach that can adapt alignment strategies based on the data structure, allowing for finer control over similarity and dissimilarity among samples.

To address this challenge, we introduce a novel _generalized contrastive alignment_ (GCA) framework, which reinterprets contrastive learning as a distributional alignment problem. Our method allows flexible control over the alignment of samples by defining a target transport plan, \(_{tgt}\), that serves as a customizable alignment guide. For example, setting \(_{tgt}\) to resemble a diagonal matrix encourages each positive to align primarily with itself or its augmentations, thereby reducing the effect of noise between views. Alternatively, we can incorporate more complex constraints, such as weighting alignments based on view quality or enforcing partial alignment structures where noise or data heterogeneity is prevalent. This flexibility enables GCA to adapt effectively to a wide range of tasks, from simple twin view alignments to scenarios with noisy or variably aligned data.

Our approach also bridges connections between GCA and established methods, such as InfoNCE (INCE) , Robust InfoNCE (RINCE) , and BYOL , demonstrating that these can be viewed as iterative alignment objectives with Bregman projections [6; 21]. This perspective allows us to systematically analyze and improve uniformity within the latent space, a property that enhances representation quality and ultimately boosts downstream classification performance.

We validate our method through extensive experiments on both image classification and noisy data tasks, demonstrating that GCA's unbalanced OT (UOT) formulations improve classification performance by relaxing our constraints on alignment. Our results show that GCA offers a robust and versatile framework for contrastive learning, providing flexibility and performance gains over existing methods and presenting a promising approach to addressing different sources of variability in self-supervised learning.

The contributions of this work include:

* A new framework called _generalized contrastive alignment_ (GCA), which reinterprets standard contrastive learning as a distributional alignment problem, using optimal transport to provide flexible control over alignment objectives. This approach allows us to derive a novel class of contrastive losses and algorithms that adapt effectively to varied data structures and build customizable transport plans.
* We present GCA-UOT, a contrastive learning method that achieves strong performance on standard augmentation regimes and excels in scenarios with more extreme augmentations or data corrupted by transformations. GCA-UOT leverages unbalanced transport to adaptively weight positive alignments, enhancing robustness against view noise and cross-domain variations.
* We provide theoretical guarantees for the convergence of our GCA-based methods and show that our alignment objectives improve representation quality by enhancing the uniformity of negatives and strengthening alignment within positive pairs. This leads to more discriminative and resilient representations, even in challenging data conditions.
* Empirically, we demonstrate the effectiveness of GCA in both image classification and domain generalization tasks. Through flexible, unbalanced OT-based losses, GCA achieves superior classification performance and adapts alignment to include domain-specific information where relevant, without compromising classification accuracy in domain generalization.

## 2 Background

### Contrastive learning

Contrastive learning (CL) is a representation learning methodology that uses positive and negative pairs to define similarity in the latent space. Let \(=\{_{i}\}_{i=1}^{N}\) denote our dataset. For each sample \(_{i}\) in a batch of training data with size \(B\), we create two augmented copies \(^{}_{i}\) and \(^{}_{i}\) independently, i.e., \(^{}_{i}=(_{i})\) where \(\) is a randomly drawn augmentation function from some augmentation class \(\) and likewise for \(^{}_{i}\). The \((^{}_{i},^{}_{i})\) is called a positive pair of \(_{i}\) while \((^{}_{i},^{}_{j})\) is treated as a negative pair for any \(j i\). One of the most widely used formulations of the CL problem, InfoNCE (INCE) , seeks to maximize the negative log probability that a sample is correctly classified as

\[_{}=-}}{e^{s_{ii}}+_{i j }e^{s_{ij}}})},\] (1)

where \(s_{ij}=^{-1}f_{}(^{}_{i})^{}f_{}( ^{}_{j})/\|f_{}(^{}_{i})\|\|f_{ }(^{}_{j})\|\) is the score between augmented samples.

Building upon the principles of INCE, SimCLR  and MoCo  are two representative works that form the foundation of contrastive learning methods for visual representation tasks. Alternatively, BYOL  and SimSiam  discard the use of negative samples to avoid large batch size and instead use exponential moving average-based updates to avoid representational collapse. Recent contrastive methods have focused on improving the tolerance to noise in samples to enhance robustness in diverse scenarios . Among them, Robust INCE (RINCE) is a robust contrastive loss function characterized by its symmetric properties and theoretical resistance to noisy labels [47; 12]. Specifically, RINCE provides robustness to noisy views by introducing adjustable parameters \(\) and \(q\) which rebalance the cost of positive and negative views, resulting in the following loss:

\[_{}^{,q}=-e^{qs_{ii}}+ ^{q}(e^{s_{ii}}+_{i j}e^{s_{ij}})^{q}\] (2)

By optimizing the above loss functions, the encoder \(f\) is trained to construct a semantically coherent representation space where positive pairs of samples are positioned nearby, while those negative pairs with divergent semantic attributes are separated .

### Proximal Operators and Projections

To make the connections between different CL losses clearer later, we use the notion of proximal operators. In words, the proximal operator will provide a way to find the closest point in some closed convex set. Formally, we can define the proximal operator as follows.

**Definition 1** (Proximal Operator).: _Let \(d_{}(,)=()-()- (),-\) be a Bregman divergence with a convex function \(\). The proximal operator of \(h:\{+\}\) is defined for a point \(\) with a closed convex set \(\) :_

\[^{d_{}}_{h,}()=_{ }\{h()+d_{}(,)\}.\]

Moreover, we can define the concept of a projection as a special case of the proximal operator when we let \(h()\) be an indicator function \(h_{}(x)=\{0,x;,x \}\) on constraint set \(\). See Appendix A.2 for more details.

### Solving Optimal Transport Through Proximal Point Methods

Optimal transport (OT) is widely used in characterizing the distance between two collections of samples \(\{_{i}\}_{i=1}^{B}\) and \(\{_{j}\}_{j=1}^{B}\) with associated measures \(=_{i=1}^{B}_{_{i}}p_{i}\) and \(=_{j=1}^{B}_{_{j}}q_{j}\) with Dirac delta function \(_{}\) and \(_{}\) on finite support . Here, \(p\) and \(q\) are vertices of the \(^{B}\) simplex defined as \(_{B}:=\{v^{B}:v_{i} 0,_{i=1}^{B}v_{i}=1\}\). OT aims to learn a joint coupling matrix, or transport plan \(_{+}^{B B}\) that minimizes the cost of transporting mass encoded by cost matrix \(_{+}^{B B}\), from one distribution to another. In practice, entropy regularization is used to solve the OT objective, resulting in the following entropy-regularized OT (EOT) objective:

\[_{}\ ,-  H(),H()=-_{ij}_{ij}( _{ij}),\] (3)

where \(\) is a user specified parameter that controls the amount of smoothing in the transport plan, and \((,)=1-,/\| \|\|\|\) is often set to encode the cosine similarity between pairs of samples.

The Sinkhorn Algorithm and its Interpretation as a Bregman Projection.Solving Equation (3) could be interpreted as iterative alignment problem on a Hilbert space generated from the kernel \(_{ij}=(-_{i,j}/)\). This alignment problem can be solved through iterative Bregman projections onto the two constraints sets that encode the marginals along the rows and columns [3; 5; 43]:

\[C_{1}^{}\{:_{B}=\},C_{2}^{} \{:^{}_{B}=\}\] (4)

The first step of Bregman projection is to find the minimizer \(^{(1)}=\{(\|): _{B}=\}\) by the proximal operator \(^{}_{C_{1}^{}}()\) with Lagrange multiplier \(f\) on the row constraint set \(C_{1}^{}\), and compute its derivatives with respect to \(\) with \(=e^{f/}>0\):

\[(^{(1)}/)-f=0 ^{(1)}=,^{(1)}, =,=,= }\] (5)Next, we project \(^{(1)}\) onto the column constraint set \(C_{2}^{}\), resulting in \(^{(2)}_{C_{2}^{2}}^{}(^{(1)})= ^{(1)}(^{(1)^{T}\,1_{B}}})\). The iterative updates can be succinctly expressed as the Sinkhorn iterations:

\[^{(2t+1)}=(^{(t+1)}) (^{(t)}),^{(2t+2)}=(^{(t+1)})(^{(t+1)}),\] (6)

with the scaling vectors \(^{(t)}\) and \(^{(t)}\) updated according to:

\[^{(t+1)}\ }}{{=}}\ ^{(t)}}, ^{(t+1)}\ }}{{=}}\ ^{T}^{(t)}}.\] (7)

Here, iterations converge to a stable transport plan \(^{()}\)as the optimal solution of Equation (3), which provides the minimum cost matching between two distributions. The convergence and dynamics of OT and its dual formulation have been studied extensively in [4; 43; 19; 1]. Thus, these results guarantee that the iterates will converge to the optimal solution of the EOT objective, or that \(^{(t)}^{()}\) with \(t\). See Appendix A.3 for more details on both the continuous and discrete formulations of OT.

### Wasserstein Dependency Measure

The Wasserstein Dependency Measure (WDM) is a measure of deviation between two probability measures. We will use this later and thus provide the formal definition here .

**Definition 2** (Wasserstein Dependency Measure).: _Define the WDM as the Wasserstein distance (\(W_{1}\)) between the joint distribution \((x,y)\) and the product of marginal distributions \((x,y)\) of two random variables \(x\) and \(y\). \(W_{1}(,)=_{f()} (_{(x,y)}[f(x,y)]-_{(x,y)}[f(x,y)])\), where \(()\) denotes the set of all 1-Lipschitz functions from \(\) to \(\)._

### Optimal Transport and Alignment in Representation Learning

Distribution alignment and OT have been widely used for domain adaptation [33; 14; 30; 59], and in generative modeling [2; 55; 49; 58]. The connections between distribution alignment and contrastive learning, however, are still nascent. In , the authors explore the connection between inverse OT (IOT) [32; 53; 18] and INCE. Our work builds on this connection to OT to build robust divergences (RINCE) and to build a novel unbalanced optimal transport (UOT) method (Section 3.3). Additionally, we show how our framework can be used to build flexible methods for encouraging contrast at multiple levels. We use this concept of hierarchical contrast and show that it can be used in domain generalization settings (Section 6.2). It is of note that GCA-UOT focuses on relaxing the hard constraints on the row and columns into the soft penalties, which is different with the idea of "unbalanced matching" in  which considers the case where the encoders may not have the same weights.

## 3 Generalized Contrastive Alignment (GCA)

In this section, we will introduce a new framework for _generalized contrastive alignment_ and demonstrate the connections between contrastive learning and optimal transport.

### Problem Formulation

Traditional contrastive learning methods focus on bringing positive examples, such as augmentations of the same sample, closer together in representation space. In contrast, our approach reframes contrastive learning as a distributional alignment problem, allowing flexible control over how pairs are matched by imposing specific constraints on the target transport plan, \(_{tgt}\).

Our objective is to learn an encoder \(f_{}\) that minimizes the _transport cost_ between positive samples. By defining \(_{tgt}\) with specific alignment rules, such as domain-specific or hierarchical constraints, we can influence how samples are organized in the latent space. For instance, setting \(_{tgt}\) to resemble a diagonal matrix encourages each positive to align primarily with itself or its augmentations, minimizing \((||) 0\), where \(\) measures the deviation from an identity matrix (e.g., KL-divergence).

This flexibility allows us to encode more nuanced forms of similarity, adapting to tasks where alignment structure varies based on domain, class, or other high-level constraints. By expanding contrastive learning in this way, our method enhances separation of negatives while addressing complex relational patterns, making it suitable for a wider range of learning tasks.

Defining the Kernel Space.Before formally stating our objective, we first need to define the concept of an augmentation kernel for our positive and negative examples.

**Definition 3** (Augmentation Kernel).: _Let \(f_{}\) denote an encoder with parameters \(\) and let \((_{i}^{},_{j}^{})\) be two views drawn from the family of augmentations \(\). The augmentation kernel for the encoder \(\) is defined as \(_{}(_{i}^{},_{j}^{})= (-(_{}(_{i}^{}),_{}(_{j}^{}))/)\), where \(()\) can be an arbitrary distance metric, and \(_{}(_{i}^{})\) is the normalized output of \(f_{}\), and \(\) is the regularization parameter._

Main Objective.With this definition in hand, we can now formalize our objective as follows:

\[_{}\ \ d_{M}_{}||_{} ),\ _{}=_{}\{h( )+d_{}(||_{})\},\] (8)

where \(_{}\) is the augmentation kernel defined in Definition (3), \(h(x)\) is a convex function (typically an indicator function), \(\) is a closed convex constraint set (i.e. Birkhoff polytope) that defines the constraints of proximal operators, \(d_{}\) is a Bregman divergence that is used to find the nearest points \(_{}\) on the constraint set \(\) of \(_{}\), \(d_{M}\) is a convex function (e.g., KL-divergence) that measures divergence between \(_{}\) and the target coupling plan \(_{}\).

Our objective is a bi-level optimization problem which aims to learn a representation that minimizes the divergence between the transport plan \(_{}\) with the target alignment plan \(_{}\) that encodes the matching constraints. When we consider a standard contrastive learning setup where we have pairs of positive examples the source and target distribution, then the target \(_{}\) is the identity matrix \(\). However, we will show later that other alignment constraints can be considered. Moreover, when \(\) is the intersection of more constraint sets like \(C_{1}^{} C_{2}^{}\) in Equation (4), a nature way to get the approximation of the nearest points \(_{}\) of \(_{}\) is to run iterative projections algorithm , which could be extended into the intersection of several constraint sets like \(\{_{i=1}^{n}C_{i}\}\), resulting in a multi-marginal problem .

### A Proximal Point Algorithm for GCA

In practice, we can solve the alignment problem above by iteratively updating the two main components in our bi-level objective. First, for a fixed encoder parameters \(\), we obtain the transport coupling \(_{}\) through our corresponding proximal operator. Second, we measure the deviation between the transport plan \(_{}\) with the target \(_{}\) that encodes our matching constraints, which denotes the ideal alignment plan on the intersection of the constraint sets. We provide pseudocode for this iterative approach in Algorithm 1, which we refer to as generalized contrastive alignment or GCA. The implementation of our methods is in https://github.com/nerdslab/gca.

```
1:Initialization: Initial encoder parameters \(\), target transport plan \(_{}\), kernel function \(_{}\), the function \(h(x)\), divergences \(d_{}\) and \(d_{M}\) (KL or \(W_{1}\)). Initialize transport plan \(_{}\) based on \(\).
2:Compute the transport coupling \(_{}\): Update \(_{}\) using the proximal operator scaling for fixed \(\) as described in Eq. (8): \[_{}=_{}\{h()+d_{ }(||_{})\}.\]
3:Calculate the loss: Calculate deviation between the target and current transport plans \[_{GCA}=d_{M}(_{},_{}).\] Update networks \(f_{}\) (encoder) and \(g_{}\) (projector) to minimize \(_{GCA}\).
4:Repeat until convergence: Repeat steps 2 and 3 until convergence. ```

**Algorithm 1** Proximal-Point Algorithm for Generalized Contrastive Alignment (GCA)

Computing the transport coupling \(_{}\)2 (forward-pass) in GCA algorithms could be treated as a specific type of Dykstra's projection algorithms , which computes the **iterative projection** on the intersection of affine convex sets [3; 42]. The proofs of convergence are provided in Appendix B.1.

### GCA-UOT Method

We can also benefit from the rich literature on optimal transport to build different relaxations of our objective . In particular, we choose to leverage a formulation of _unbalanced optimal transport_ (UOT) to further relax the marginal constraints  in our objective.

In this case, we can add the dual form of \(d_{}\) to the Equation (8) and reformulate our objective as:

\[_{}\ d_{M}(_{}||_{})+_{ }_{}(_{}||)+ _{}_{}(_{}^{} ||)+(_{}).\] (9)

Here \(h_{}\) and \(h_{}\) can be different divergence measures (e.g., KL divergence) that penalize deviations from the desired marginals \(\) and \(\), and \(_{1}\) and \(_{2}\) are regularization parameters that control the trade-off between the transport cost and the divergence penalties. This relaxation leads to different types of proximal operators which we outline in Appendix B.2. The impact of the entropy regularization parameter \(\) on the coupling matrix is studied in Figure A5, along with the number of iterations and corresponding sensitivity is provided in Figure A6.

### Modifying the Target Transport Plan to Encode Matching Constraints

Contrastive learning objectives can be cast as a minimization of the deviations between the transport plan \(_{}\) and the identity matrix, i.e., \(_{tgt}=\). However, our GCA formulation enables learning representations that extend beyond this one-to-one matching constraint. This flexibility allows us to incorporate additional matching constraints informed by domain-specific knowledge. For example, in domain generalization scenarios , where each batch contains samples from multiple domains, the target alignment plan can be structured as:

\[_{}[i,j]=[i,j]+(D_{i}=D_{j },\,i j)+(D_{i} D_{j},\,i j),\]

Where \(()\) is the indicator function, which equals 1 if the condition inside is true and 0 otherwise. \(D_{i}\) represents the domain of sample \(i\), where \( 0\) and \( 0\). In this case, we can improve the representation by building the block constraints which encode either class information (in supervised setting) or domain information (in across domain generalization, visualized in Figure 1).

### Computational Complexity

The forward-pass only involves the scaling operations in Equation (7) and doesn't affect the complexity of the backward-pass. Therefore, GCA methods can be thought of as a form of batch normalization operations with adaptive scaling. An analysis of the complexity is provided along with experiments in Appendix B.1. Our results show that GCA iterations only slightly increase the computational complexity when compared with their single step equivalent (GCA-INCE vs. INCE). However, we found that GCA-UOT is faster than INCE due to the improved symmetry and smoothness of the loss. Moreover, we record the floating point operations per second (Flops) of running GCA methods. We find that GCA-INCE (6.65 MFlops) has \(5\%\) more Flops than INCE (6.31 MFlops), while GCA-UOT saves \(30\%\) Flops (4.54 MFlops). These results show that our GCA-UOT method is not only superior in terms of accuracy but also in speed.

## 4 Building Connections to Different CL Objectives

In this section, we show how the modification of the different parts of our main objective (\(d_{},d_{M},,_{}\)) in Equation (8) can be connected to different contrastive losses. See Table 1 for a summary of how different losses can be mapped back to our formulation.

### Connection to INCE

An interesting connection that we can make between GCA main objective and contrastive learning is that we can interpret INCE as a **single step** in a iterative GCA objective . This connection can be further summarized through the following theorem.

**Methods** & \(d_{M}\) & \(d_{}\) & \(\) & Iter \\  INCE & KL & KL & \(C_{1}^{}\) & \\ GCA-INCE & KL & KL & \(C_{1}^{}\) & \(\) \\ RINCE (q=1) & W1 & KL & \(C_{1}^{}\) & \\ GCA-RINCE (q=1) & W1 & KL & \(C_{1}^{} C_{2}^{}\) & \(\) \\ BYOL & KL & L2 & \(R^{B B^{2}}\) & \\ 

Table 1: _Comparison of different contrastive alignment objectives._ Here we have \(C_{1}^{}\) and \(C_{2}^{}\) as constraint sets (denoted as \(\)) defined in Equation (4) with their corresponding indicator function. "Iter" refers to iterative methods.

**Theorem 1** (INCE Equivalence).: _Let \(_{}\) denote the augmentation kernel as in Definition (3) with cosine similarity, \(d_{}\) and \(d_{M}\) equal to KL-divergence, and constraint set as \(C_{1}^{}\) in Equation (4). The INCE objective in Equation (1) can be re-expressed as a GCA problem in Equation (8) as follows:_

\[_{}\|_{C_{1}^{}}^{KL}( _{}).\] (10)

The proof is contained in Appendix B.3. Theorem (1) shows that the INCE loss can be viewed as solving the matching problems in Equation (3) with row normalization constraints \(C_{1}^{}\). This connection between GCA and INCE allows us to derive the iterative algorithm for GCA-INCE by running Bregman projection iteratively on both row and column normalization sets

### Connection to RINCE

We introduce the following result to build the connection between our framework and RINCE .

**Theorem 2** (RINCE Equivalence).: _Let \(_{}\) denote the augmentation kernel as in Definition (3). Set target plan \(_{tgt}=\), \(d_{}\) equal to the KL-divergence, \(d_{M}(\|)=-((_{ })}{})^{q}+}{} ^{q}\) with \(\), \(q\), and \(=(^{(0)}})\), and constraint set \(C_{1}^{}\) defined in Equation (4). The RINCE objective in Equation (2) can be re-expressed as a GCA problem as follows:_

\[_{}d_{M}(\|_{}),\ \ \ \ _{}=_{C_{1}^{}}^{KL}(_{}),\] (11)

The proof is provided in Appendix B.4.1. As we can see, RINCE introduces adjustable parameters \(q\) and \(\), with \(\) controlling the weight of negative samples, while \(q(0,1]\) serves to switch between KL divergence and Wasserstein discrepancy. When \(q=1\), we have the following theorem:

**Theorem 3** (W1 Equivalence).: _Let \(_{}\) denote the augmentation kernel as in Definition (3) with cosine similarity. Set target plan \(_{tgt}=\), \(d_{}\) equal to the KL-divergence, \(d_{M}\) equal to the 1-Wasserstein distance \((W_{1})\) in Definition (2), and the constraint set as \(C_{1}^{}\) defined in Equation (4). The RINCE object in Equation (2) with \(q=1\) can be re-expressed as a GCA problem as follows:_

\[_{}W_{1}_{tgt}\|_{C_{1}^{}}^{KL}( _{}).\] (12)

See Appendix B.5 for the proof.

This connection to RINCE suggests an extended iterative formulation to calculate the coupling plan as the projection point \(^{()}=_{C_{1}^{} C_{2}^{}}^{}( _{})\) of \(_{}\) on the constraint set \(C_{1}^{} C_{2}^{}\). In this case, we can write an iterative algorithm for robust alignment called GCA-RINCE as follows:

\[L_{}^{,q}=_{}-q^{-1}(( _{}^{(2t-1)})/^{(t)})^{q}+q^{-1}(_{tgt}/ ^{(t)})^{q},\] (13)

where \(\) and \(q\) are hyperparameters, \(^{(1)}(^{(1)})_{} \,(^{(0)})\), and \(t\) is the number of iterations.

### Connection to BYOL

Our framework also allows us to make connections to BYOL . BYOL learns by encouraging similarity between positive image pairs, without explicitly conditioning on negative examples. To build this connection, recall that BYOL has the online network parameterized by \(\) and target network parameterized by \(\), where \(_{}^{}=_{}(^{})\) and \(_{}^{}=_{}(^{})\) are the normalized outputs of the online and target networks, respectively. A simplified version of the BYOL loss can be written as: \(L_{}=\|_{}(_{}^{})- _{}^{}\|_{2}^{2},\) where \(_{}(_{}^{})\) is the normalized output after online network and \(q_{}\) is the predictor.3 In this case, we can provide the following connection between GCAand BYOL as follows.

**Theorem 4** (BYOL Equivalence).: _Let \(_{}(_{i}^{},_{j}^{})= (-\|_{}(_{i}^{})-_{j}^{ }\|)\) denote the augmentation kernel. Set the target plan \(_{tgt}=\), \(d_{}\) equal to the L2-distance, \(d_{M}\) equal to the KL-divergence, and constraint set as \(R^{B B}\). The BYOL objective can be re-expressed as a GCA problem as follows:_

\[_{}\|_{}),\ \ \ \ _{}=_{R^{B B}}^{\|.\|}(_{}).\] (14)

See the proof in Appendix B.6.

Theoretical Analysis

In this section, we aim to show how the GCA-methods can improve alignment and uniformity in the latent space . Here, _alignment_ means that the features of the positive samples are as close as possible, while _uniformity_ means that the features of negative samples are uniformly distributed on latent space (see Appendix C.1 for formal definitions). These quantities have been studied in a number of related works [57; 45], where one can show that improved alignment and uniformity can lead to different benefits in representation learning.

### Improved alignment with GCA

Contrastive learning minimizes the deviation between the target alignment plan with the transport plan in Definition 3 through empirical risk minimization (ERM). Therefore, a tighter bound on the empirical risk corresponds to a smaller difference between the ideal alignment with the coupling matrix. We show that this in turn leads to better alignment of the positive views.

Analysis of INCE vs GCA-INCE.GCA-INCE ensures that the final transport plan \(^{()}\) is closer to the ideal identity matrix compared to the INCE, as we show in the following theorem.

**Theorem 5** (Improved Alignment with INCE).: _Let \(_{}\) denote the augmentation kernel as in Definition (3). Set \(d_{M}\) and \(d_{}\) to the KL-divergence, and \(_{}=\). The GCA-INCE loss with converged plan \(^{()}_{}\) is lower than the GCA-INCE loss with \(^{(t)}_{}\) in Equation (6) for all \(t\)._

The full proof is provided in Appendix C.1.1. The above theorem tells us that solving Equation (8) with iterative projection will converge to a transport plans \(^{()}_{}\) with lower KL divergence than the one-step solution provided by INCE. We can establish the convergence of the \(^{(t)}^{()}\), based on the convergence of Bregman projection.

Analysis of RINCE vs GCA-RINCE.GCA also benefits from other Bregman divergences, like the WDM in RINCE, which provides robustness against distribution shift compared to the KL-divergence in INCE. GCA-RINCE provides a lower bound on the RINCE loss in Equation (2), which allows us to develop a tighter bound with \(^{()}\) obtained by several proximal steps with GCA.

**Theorem 6** (Improved Alignment with RINCE).: _GCA-RINCE loss with \(^{(t)}_{}\) in Equation (13) is lower than the loss in the Theorem (2) as \(L^{,q=1}_{}(^{(t)}_{}) L^{,q =1}_{}(^{(1)}_{})\)._

See Appendix C.1.1 for the full proof and an analysis of GCA methods for different choices of \(d_{M}\).

### Improved Uniformity of Representations Through GCA

The improved alignment of GCA-methods comes from maximization of the uniformity under the constraint of intersection \(C^{}_{1} C^{}_{2}\) in Equation (4), rather than the constraint set \(C^{}_{1}\) in INCE (see Table 1). Finding the projection of \(_{}\) on set of \(C^{}_{1} C^{}_{2}\) through proximal steps is equivalent to solving the dual problem of EOT, which can be summarized through the following theorem.

**Theorem 7** (Improved Uniformity).: _Given the constraint sets in Equation (4), the optimal transport coupling upon convergence of Equation (6), denoted as \(^{()}\), achieves a higher uniformity loss compared to the single-step transport plan \(^{(1)}\) obtained by INCE._

The proof is provided in the Appendix C.2. Through loss propagation, we show that the alignment plan offered by \(^{()}\) will guide the subsequent iterations towards more uniform representations.

### Impacts of GCA on a downstream classification task

We take this one step further and examine the impact of GCA on a downstream classification task. For a classification task, using a labeled dataset \(=\{(}_{i},_{i})\}} \) where \(=[1,,M]\) with \(M\) classes, we consider a fixed, pre-trained encoder \(f_{}:\). Assume that positive and negative views of \(n\) original samples \((}_{i})_{i[1 n]}}\) are sampled from the data distribution \(p(})\).

In this case, the uniformity loss is equivalent to optimizing the downstream supervised classification tasks with cross-entropy (CE) loss when the following two assumptions are satisfied .

**Assumption 1** (Expressivity of the Encoder).: _Let us define \(_{}}\) is the RKHS associated with the kernel \(_{}}\) defined on \(}\), and \((_{f_{}},_{})\) defined on \(\) with augmentation kernel \(_{}= f_{}(),f_{}()_{^{d}}\) in Definition 3. And we assume that \( g_{f_{}},\ _{(x|)}g(x)_{}}\)._

**Assumption 2** (Small Intra-Class Variance).: _For \(y y^{}\), the intra-class variance \(_{i},_{j}\) are negligible compared to the distance among different class centroids, \(_{y},_{y^{}}\) as \(\|_{y}-_{y^{}}\|\|_{i}-_{j}\|\)._

**Claim 1**.: _If Assumption 1 and Assumption 2 hold, then maximizing the uniformity is equivalent to minimizing the downstream CE loss._

The proof is provided in Appendix C.2. Optimizing the self-supervised loss under ideal conditions improves downstream CE tasks and helps to explain why maximizing uniformity aids classification.

**Remark.** Maximizing uniformity can enhance downstream classification but risks "feature suppression" by encouraging shortcut features that harm generalization . In GCA-UOT, adding penalties modifies the transport plan from that of a pure uniformity loss, helping to avoid feature suppression. We find empirical evidence that UOT provides a more robust transport plan which appears to circumvent some of these shortcut features from being learned (Figure A4 in Appendix C.3).

## 6 Experiments

In this section, we conduct empirical evaluations to study the performance of our approach in both handling noisy and corrupted views and in domain generalization tasks.

### Comparison with CL Baselines

Experiment setup.To examine the robustness of our framework, we trained INCE and RINCE as baselines, and developed their GCA-based alternatives (+GCA). In addition, we also compared with our novel GCA-UOT method, two variants of IOT established in , and other CL baselines, including BYOL and SimCLR. For experiments with SVHN  and ImageNet100  we use the ResNet-50 encoder as the backbone and use a ResNet-18 encoder as the backbone for CIFAR-10, CIFAR-100  and a corrupted version of CIFAR called CIFAR-10C .

In all of these cases, we follow the standard self-supervised learning evaluation protocol , where we train the encoder on the training set in an unsupervised manner and then train a linear layer on top of the frozen representations to obtain the final accuracy on the test set. In addition to standard data augmentation policies commonly used , we also apply three different extreme augmentation policies to examine the robustness of GCA towards noisy views (details in Appendix D.2). Learning rates and other training details for CIFAR-10, CIFAR-100, SVHN, and ImageNet100 are provided in Appendix D.1, while specific training details for CIFAR-10C are included in Appendix D.2.

**Results on Standard Augmentations.** First, we performed experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet100 using standard sets of augmentations that are applied to achieve state-of-the-art performance (Table 2, Standard Setting). We found the +GCA versions of INCE and RINCE exhibit performance gains in almost all settings except for SVHN, with bigger gains observed when adding GCA to RINCE. Additionally, we find that our unbalanced OT method, GCA-UOT, achieves the top performance across the board, on all four datasets tested. The transport plans obtained by each methods are provided in Figure A4 along with a study of the sensitivity of the methods to hyperparameters (Appendix A7).

**Results on Corrupted Data and Extreme Augmentations.** Next, we tested the methods in two noisy settings. In the first set of experiments, we apply extreme augmentations to CIFAR-10 (Ex) and

  }} &  &  \\   & CIFAR-10 & CIFAR-100 & SVHN & ImageNet100 & CIFAR-100 (Ex) & CIFAR-100 (Ex) & CIFAR-100 (Ex) \\  INCE & 92.01 \(\) 0.04 & 70.07 \(\) 0.42 & 90.60 \(\) 0.17 & 73.01 \(\) 0.61 & 82.03 \(\) 0.32 & 54.70 \(\) 0.43 & 87.20 \(\) 0.37 & 74.84 \(\) 0.21 \\ GCA-INC & 29.36 \(\) 0.24 & 70.11 \(\) 0.45 & 90.40 \(\) 0.16 & 73.04 \(\) 0.67 & 82.18 \(\) 0.69 & 54.91 \(\) 0.56 & 83.74 \(\) 0.34 & 76.00 \(\) 0.17 \\ A & -0.35 & -0.04 & -0.20 & -0.03 & -0.15 & +0.42 & -0.21 & +0.14 & +0.16 \\  RINCE & 91.05 \(\) 0.50 & 69.06 \(\) 0.64 & 90.97 \(\) 0.19 & 71.91 \(\) 0.43 & 82.60 \(\) 0.63 & 55.43 \(\) 0.48 & 88.62 \(\) 1.33 & 77.05 \(\) 0.82 \\ GCA-RINCE & 29.09 \(\) 0.22 & 69.72 \(\) 0.27 & 91.45 \(\) 0.41 & 73.44 \(\) 0.55 & 82.76 \(\) 0.49 & 55.90 \(\) 0.41 & 88.76 \(\) 0.72 & 72.33 \(\) 0.76 \\ \(\) & +1.04 & -0.66 & -0.48 & -0.48 & -1.53 & -0.16 & +0.47 & -0.14 & -0.18 \\  SimCLR & 92.16 \(\) 0.16 & 95.95 \(\) 0.19 & 50.24 \(\) 0.24 & 72.00 \(\) 0.78 & 18.77 \(\) 0.33 & 54.54 \(\) 0.75 & 86.98 \(\) 1.59 & 73.79 \(\) 0.32 \\ BYOL & 90.56 \(\) 0.59 & 69.75 \(\) 0.37 & 89.50 \(\) 0.64 & 69.75 \(\) 0.33 & 81.55 \(\) 0.50 & 54.18 \(\) 0.46 & 87.88 \(\) 1.02 & 69.0 \(\) 1.11 \\ UOT  & 90.99 \(\) 0.54 & 67.19 \(\) 0.21 & 90.15 \(\) 0.21 & 72.27 \(\) 0.53 & 80.59 \(\) 0.64 & 52.40 \(\) 0.48 & 67.36 \(\) 1.97 & 58.75 \(\) 1.96 \\ UOTAN  & 90.99 \(\) 0.57 & 67.03 \(\) 0.40 & 90.54 \(\) 0.20 & 72.88 \(\) 0.71 & 80.79 \(\) 0.24 & 53.04 \(\) 0.52 & 69.85 \(\) 1.25 & 50.95 \(\) 1.86 \\  GCA-UOT & **92.61** & **71.45** & **63.07** & **71.96** & **8.49** & **8.40** & **83.18** & **8.04** & **56.30** & **89.81** & **8.04** & **77.60** & **0.54** \\  

Table 2: _Test accuracy (%) on a downstream classification task after pretraining._ Results are provided for CIFAR-10 (ResNet18), CIFAR-100 (ResNet18), SVHN (ResNet50), and ImageNet100 (ResNet50) under standard and extreme (Ex) augmentation conditions (averaged over 5 seeds). The top model is bold and the second-place model is underlined. For INCE and RINCE, we also provide the improvement \(\) by adding GCA to each method.

CIFAR-100 (Ex) (see Appendix D.2) to introduce noisy views during training. In the second set of experiments, we used the CIFAR-10C to further test the ability of our method to work in noisy settings. Our experimental results demonstrate that the GCA-based strategy effectively enhances the model's generalization ability and adaptability to aggressive data augmentations. In addition to improving classification accuracy, the GCA-based methods also improve the representational alignment and uniformity, as shown in Appendix E.2. This observation is in line with our theoretical analysis in Section 5.2, where we show that the obtained representations provide better overall alignment of positive views and better spread in terms of uniformity .

### Block Diagonal Transport in Domain Generalization

In a final experiment, we aimed to demonstrate the flexibility and robustness of our framework by applying it to a domain generalization task, where samples originate from different domains (e.g., Photo, Cartoon, Sketch, Art). We explored the effects of introducing domain-specific alignment constraints in our transport plan, hypothesizing that this could enhance the latent space organization to capture more nuanced domain similarities.

Our approach enables additional contextual information to be seamlessly integrated into the transport process. In this case, domain information was incorporated to distinguish the alignment of samples from the same versus different domains. To achieve this, we adjusted the target transport plan \(_{tgt}\), selectively modifying parameters \((,)\) to vary the influence of domain-based alignment constraints as shown in Figure 1(A). Specifically, we set \(\{=0,>0\}\) to prioritize cross-domain alignment and \(\{>0,=0\}\) to focus on intra-domain alignment.

The training was conducted on the PACS dataset  using a ResNet-18 encoder with the GCA-INCE objective. After training the encoder in an unsupervised manner, we freeze the encoder and then train a linear readout layer to predict either the sample's class or the domain it belonged to. This setup allowed us to isolate the effect of our transport adjustments on the latent space's capacity to encode both class and domain information.

The results, displayed in Figure 1(B), revealed that increasing the domain alignment weight enhances the accuracy of domain classification (from 72.11% to 95.16%) without diminishing classification performance. This outcome suggests that GCA can effectively encode both domain and class information in a single latent representation. The ability to adjust alignment constraints provides a powerful tool for domain generalization tasks, enabling multiple types of similarity to be jointly encoded. This flexibility can potentially alleviate issues related to information loss from data augmentation, especially in fine-grained classification settings, by retaining essential domain-specific characteristics across transformations.

## 7 Conclusion

In this work, we introduced _generalized contrastive alignment_ (GCA), a flexible framework that redefines contrastive learning as a distributional alignment problem using optimal transport to control alignment. By allowing targeted control over alignment objectives, GCA demonstrates strong performance across both standard and challenging settings, such as noisy views and domain generalization tasks. This work opens up broader possibilities for learning robust representations in real-world scenarios, where data is often diverse, noisy, or comes from multiple domains.

Future work includes applications of GCA to graphs and time series data, as well as multi-modal settings where our approach can integrate various forms of similarity. As alignment strategies become integral to contrastive learning, GCA offers a promising foundation for more adaptive and expressive self-supervised models.

Figure 1: _Incorporating different priors into learning across multiple domains._ (A) Example target alignment plan \(_{tgt}\), where the target over all samples from the same domain are set to \(\), the diagonal values are set to 1, and across-domain samples are set to \(\). (B) The domain classification accuracy (red) and overall class accuracy (blue) with (\(-\)) increases.