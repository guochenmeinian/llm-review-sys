# Potts Relaxations and Soft Self-labeling

for Weakly-Supervised Segmentation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach where soft pseudo-labels on unlabeled pixels optimize some relaxation of the standard unsupervised CRF/Potts loss. While WSSS methods can directly optimize CRF losses via gradient descent, prior work suggests that higher-order optimization can lead to better network training by jointly estimating pseudo-labels, e.g. using discrete graph cut sub-problems. The inability of hard pseudo-labels to represent class uncertainty motivates the relaxed pseudo-labeling. We systematically evaluate standard and new CRF relaxations, neighborhood systems, and losses connecting network predictions with soft pseudo-labels. We also propose a general continuous sub-problem solver for such pseudo-labels. Soft self-labeling loss combining the log-quadratic Potts relaxation and collision cross-entropy achieves state-of-the-art and can outperform full pixel-precise supervision on PASCAL.

## 1 Introduction

Full supervision for semantic segmentation requires thousands of training images with complete pixel-accurate ground truth masks. Their high costs explain the interest in weakly-supervised approaches based on image-level class _tags_, pixel-level _scribbles_, or _boxes_. This paper is focused on weak supervision with _scribbles_, which we also call _seeds_ or _partial masks_. While only slightly more expensive than image-level class tags, scribbles on less than \(3\%\) of pixels were previously shown to achieve accuracy approaching full supervision without any modifications of the segmentation models. In contrast, tag supervision typically requires highly specialized systems and complex multi-stage training procedures, which are hard to reproduce. Our interest in the scribbles-based approach is motivated by its practical simplicity and mathematical clarity. The corresponding methodologies are focused on the design of unsupervised or self-supervised loss functions and stronger optimization algorithms. The corresponding solutions are often general and can be used in different weakly-supervised applications.

### Scribble-supervised segmentation

Assume that a set of image pixels is denoted by \(\) and a subset of pixels with ground truth labels is \(S\), which we call _seeds_ or _scribbles_ as subset \(S\) is typically marked by mouse-controlled UI for image annotations, e.g. see seeds over an image in Fig.7(a). The ground truth label at any given pixel \(i S\) is an integer

\[_{i}\{1,,K\}\] (1)where \(K\) is the number of classes including the background. Without much ambiguity, it is convenient to use the same notation \(_{i}\) for the equivalent _one-hot_ distribution

\[_{i}\ \ \ \ (_{i}^{1},,_{i}^{K})\,_{0,1}^ {K}_{i}^{k}:=[k=_{i}]\ \{0,1\}\] (2)

where \([\,\,]\) is the _True_ operator for the condition inside the brackets. Set \(_{0,1}^{K}\) represents \(K\) possible one-hot distributions, which are vertices of the \(K\)-class _probability simplex_

\[^{K}\ \ :=\ \ \{p=(p^{1},,p^{K}) p^{k} 0,\ _{k=1}^{K}p^{k}=1\}\]

representing all \(K\)-categorical distributions. The context of specific expressions should make it obvious if \(_{i}\) is a class index (1) or the corresponding one-hot distribution (2).

Loss functions for weakly supervised segmentation with scribbles typically use _negative log-likelihoods_ (NLL) over scribbles \(i S\) with ground truth labels \(_{i}\)

\[-_{i S}_{i}^{_{i}}\] (3)

where \(_{i}=(_{i}^{1},,_{i}^{K})^{K}\) is the model prediction at pixel \(i\). This loss is a standard in full supervision where the only difference is that \(S=\) and usually, no other losses are needed for training. However, in a weakly supervised setting the majority of pixels are unlabeled, and unsupervised losses are needed for \(i S\).

The most common unsupervised loss in image segmentation is the Potts model and its relaxations. It is a pairwise loss defined on pairs of _neighboring_ pixels \(\{i,j\}\) for a given neighborhood system \(\), typically corresponding to the _nearest-neighbor_ grid (NN) , or other _sparse_ (SN)  and _dense_ neighborhoods (DN) . The original Potts model is defined for discrete segmentation variables, e.g. as in

\[_{\{i,j\}}P(_{i},_{j}) P (_{i},_{j})=[_{i}_{j}]\]

assuming integer-valued one-hot predictions \(_{i}_{0,1}^{K}\). This _regularization_ loss encourages smoothness between the pixels. Its popular _self-supervised_ variant is

\[P(_{i},_{j})=w_{i,j}[_{i}_{j}]\]

where pairwise affinities \(w_{ij}\) are based on local intensity edges . Of course, in the context of network training, one should use relaxations of \(P\) applicable to (soft) predictions \(_{i}^{K}\). Many types of its relaxation  were studied in segmentation, e.g. _quadratic_, _bi-linear_, _total variation_, and others .

Another unsupervised loss highly relevant for training segmentation networks is the entropy of predictions, which is also known as _decisiveness_

\[_{i}H(_{i})\]

where \(H\) is the Shannon's entropy function. This loss can improve generalization and the quality of representation by moving (deep) features away from the decision boundaries. Widely known in the context of unsupervised or semi-supervised classification, this loss also matters in weakly-supervised segmentation where it is used explicitly or implicitly1.

Other unsupervised losses (e.g. contrastive), clustering criteria (e.g. K-means), or specialized architectures can be found in weakly-supervised segmentation . However, a lot can be achieved simply by combining the basic losses discussed above

\[L_{ws}()\ \ :=\ \ -_{i S}_{i}^{_{i}}\ +\ _{i S}H(_{i})\ +\ _{ij}P(_{i},_{j})\] (4)

which can be optimized directly by gradient descent  or using _self-labeling_ techniques  incorporating optimization of auxiliary _pseudo-labels_ as sub-problems.

### Soft pseudo-labels: motivation and contributions

We observe that self-labeling with hard pseudo-labels \(y_{i}\), which is discussed in the Appendix A, is inherently limited as such labels can not represent the uncertainty of class estimates at unlabeled pixels \(i S\). Instead, we focus on _soft_ pseudo-labels

\[y_{i} = (y_{i}^{1},,y_{i}^{K})\,^{K}\] (5)

which are general categorical distributions \(p\) over \(K\)-classes. It is possible that the estimated pseudo-label \(y_{i}\) in (5) could be a one-hot distribution, which is a vertex of \(^{K}\). In such a case, one can treat \(y_{i}\) as a class index, but we avoid this in the main part of our paper starting Section 2. However, the ground truth labels \(_{i}\) are always hard and we use them either as indices (1) or one-hot distributions (2), as convenient.

Soft pseudo-labels can be found in prior work on weakly-supervised segmentation [25; 41] using the "soft proposal generation". In contrast, we formulate soft self-labeling as a principled optimization methodology where network predictions and soft pseudo-labels are variables in a joint loss, which guarantees convergence of the training procedure. Our pseudo-labels are auxiliary variables for ADM-based  splitting of the loss (4) into two simpler optimization sub-problems: one focused on the Potts model over unlabeled pixels, and the other on the network training. While similar to , instead of hard, we use soft auxiliary variables for the Potts sub-problem. Our work can be seen as a study of the relaxed Potts sub-problem in the context of weakly-supervised semantic segmentation. The related prior work is focused on discrete solvers fundamentally unable to represent class estimate uncertainty. Our contributions can be summarized as follows:

* convergent _soft self-labeling_ framework based on a simple joint self-labeling loss
* systematic evaluation of Potts relaxations and (cross-) entropy terms in our loss
* state-of-the-art in scribble-based semantic segmentation that does not require any modifications of semantic segmentation models and is easy to reproduce
* using the same segmentation model, our self-labeling loss with \(3\%\) scribbles may outperform standard supervised cross-entropy loss with full ground truth masks.

## 2 Our soft self-labeling approach

First, we apply ADM splitting  to weakly supervised loss (4) to formulate our self-labeling loss (6) incorporating additional soft auxiliary variables, i.e. pseudo-labels (5). It is convenient to introduce pseudo-labels \(y_{i}\) on all pixels in \(\) even though a subset of pixels (seeds) \(S\) have ground truth labels \(_{i}\). We will simply impose a constraint that pseudo-labels and ground truth labels agree on \(S\). Thus, we assume the following set of pseudo-labels

\[Y_{}:=\{y_{i}^{K}\,|\,i,y_{i}=_{i}i S\}.\]

We split the terms in (4) into two groups: one includes NLL and entropy \(H\) terms keeping the original prediction variables \(_{i}\) and the other includes the Potts relaxation \(P\) replacing \(_{i}\) with auxiliary variables \(y_{i}\). This transforms loss (4) into expression

\[-_{i S}_{i}^{_{i}}\ +\ _{i S}H(_{i}) \ +\ _{ij}P(y_{i},y_{j})\]

equivalent to (4) assuming equality \(_{i}=y_{i}\). The standard approximation is to incorporate constraint \(_{i} y_{i}\) directly into the loss, e.g. using \(KL\)-divergence. For simplicity, we use weight \(\) for \(KL(_{i},y_{i})\) to combine it with \(H(_{i})\) into a single cross-entropy term

\[-_{i S}_{i}^{_{i}}\ +\ H(_{i})\ +\ _{i S}KL(_{i},y_{i})}_{_{i S}H(_{i},y _{i})}\ +\ _{ij}P(y_{i},y_{j})\]

defining joint _self-labeling loss_ for both predictions \(_{i}\) and pseudo-labels \(y_{i}\)

\[L_{self}(,y) := -_{i S}_{i}^{_{i}}\ +\ _{i S}H(_{i},y_{i})\ +\ _{ij}P(y_{i},y_{j})\] (6)approximating the original weakly supervised loss (4).

Iterative minimization of this loss w.r.t. predictions \(_{i}\) (model parameters training) and pseudo-labels \(y_{i}\) effectively breaks the original optimization problem for (4) into two simpler sub-problems, assuming there is a good solver for optimal pseudo-labels. The latter seems plausible since the unary term \(H(_{i},y_{i})\) is convex for \(y_{i}\) and the Potts relaxations were widely studied in image segmentation for decades.

Section 2.1 discusses standard and new relaxations of the Potts model \(P\). Section 2.2 discusses several robust variants of cross-entropy \(H\) for connecting predictions with uncertain (soft) pseudo-labels \(y_{i}\) estimated for unlabeled points \(i S\). Appendix B proposes an efficient general solver for the corresponding pseudo-labeling sub-problems.

### Second-order relaxations of the Potts model

We focus on second-order relaxations for two reasons. First, to manage the scope of this study. Second, this includes several important baseline cases (see Table 1): _quadratic_, the simplest convex relaxation popularized by the _random walker_ algorithm , and _bi-linear_, which is non-convex but _tight_ w.r.t. the original discrete Potts model. The latter implies that optimizing it over relaxed variables will lead to a solution consistent with a discrete Potts solver, e.g. _graph cut_. On the contrary, the quadratic relaxation will produce a significantly different soft solution. We investigate such soft solutions.

Figure 2 shows two examples illustrating local minima for (a) the bi-linear and (b) quadratic relaxations of the Potts loss. In (a) two neighboring pixels attempt to jointly change the common soft label from \(y_{i}=y_{j}=(1,0,0)\) to \(y^{}_{i}=y^{}_{j}=(0,1,0)\), which corresponds to a "move" where the whole object is reclassified from A to B. This move does not violate smoothness within the region represented by the Potts model. But, the soft intermediate state \(y^{}_{i}=y^{}_{j}=(,,0)\) will prevent this move in bi-linear case

\[P_{}(y^{}_{i},y^{}_{j})= > 0=P_{}(y_{i},y_{j})=P_{}(y^{ }_{i},y^{}_{j})\]

while quadratic relaxation assigns zero loss for all states during this move. On the other hand, the example in Figure 2(b) shows a move problematic for the quadratic relaxation. Two neighboring pixels have labels \(y_{i}=(1,0,0)\) and \(y_{j}=(0,0,1)\) corresponding to the boundary of objects A and C. The second object attempts to change from C to B. This move does not affect the discontinuity between two pixels, but quadratic relaxation prefers that the second object is stuck in the intermediate state \(y^{}_{j}=(0,,)\)

\[P_{}(y_{i},y^{}_{j})= < 1=P_{}(y_{i},y_{j})=P_{}(y_{i},y^{ }_{j})\]

while bi-linear relaxation \(P_{}(y_{i},y_{j})=1\) remains constant as \(y_{j}\) changes.

We propose a new relaxation, _normalized quadratic_ in Table 1. Normalization leads to equivalence between quadratic and bi-linear formulations combining their benefits. As easy to check, normalized

 
**bi-linear**\(\) “graph cut” & **quadratic**\(\) “random walker” \\ \(P_{}(p,q)\)\(:=\)\(1-\)\(p^{}q\) & \(P_{}(p,q)\)\(:=\)\(\|p-q\|^{2}\) \\   \\ \(P_{}(p,q)\)\(:=\)\(1-q}{\|p\|\|q\|}\) & \(\)\(\|-\|^{2}\) \\  

Table 1: Second-order Potts relaxations, see Fig.1(a,b,c)

 
**collision cross entropy** & **log-quadratic** \\ \(P_{}(p,q)\)\(:=\)\(- p^{}q\) & \(P_{}(p,q)\)\(:=\)\(-(1-}{2})\) \\   \\ \(P_{}(p,q)\)\(:=\)\(-q}{\|p\|\|q\|}\) & \(\)\(-(1-\|-\|^{2})\) \\  

Table 2: Log-based Potts relaxations, see Fig.1(d,e,f)quadratic relaxation \(P_{}\) does not have local minima in both examples of Figure 2. Table 2 also proposes "logarithmic" versions of the relaxations in Table 1 composing them with function \(-(1-x)\). As illustrated by Figure 1, the logarithmic versions in (d-f) addresses the "vanishing gradients" evident in (a-c).

### Cross-entropy and soft pseudo-labels

Shannon's cross-entropy \(H(y,)\) is the most common loss for training network predictions \(\) from ground truth labels \(y\) in the context of classification, semantic segmentation, etc. However, this loss may not be ideal for applications where the targets \(y\) are soft categorical distributions representing various forms of class uncertainty. For example, this paper is focused on scribble-based segmentation where the ground truth is not known for most of the pixels, and the network training is done jointly with estimating _pseudo-labels_\(y\) for the unlabeled pixels. In this case, soft labels \(y\) are distributions representing class uncertainty. We observe that if such \(y\) is used as a target in \(H(y,)\), the network is trained to reproduce the uncertainty, see Figure 3(a). This motivates the discussion of alternative "cross-entropy" functions where the quotes indicate an informal interpretation of this information-theoretic concept. Intuitively, such functions should encourage decisiveness, as well as proximity

Figure 1: Second-order Potts relaxations in Tables 1 and 2: interaction potentials \(P\) for pairs of predictions \((_{i},_{j})\) in (4) or pseudo-labels \((y_{i},y_{j})\) in (6) are illustrated for \(K=2\) when each prediction \(_{i}\) or label \(y_{i}\), i.e. distribution in \(^{2}\), can be represented by a single scalar as \((x,1-x)\). The contour maps are iso-levels of \(P((x_{i},1-x_{i}),(x_{j},1-x_{j}))\) over domain \((x_{i},x_{j})^{2}\). The 3D plots above illustrate the potentials \(P\) as functions over pairs of “logits” \((l_{i},l_{j})^{2}\) where each scalar logit \(l_{i}\) defines binary distribution \((x_{i},1-x_{i})\) for \(x_{i}=}}\).

Figure 2: Examples of ”moves” for neighboring pixels \(\{i,j\}\). Their (soft) pseudo-labels \(y_{i}\) and \(y_{j}\) are illustrated on the probability simplex \(^{R}\) for \(K=3\). In (a) both pixels \(i\) and \(j\) are inside a region/object changing its label from A to B. In (b) pixels \(i\) and \(j\) are on the boundary between two regions/objects; one is fixed to class A and the other changes from class C to B.

between the predictions and pseudo-labels, but avoid mimicking the uncertainty in both directions: from soft pseudo-labels to predictions and vice-versa. We show that the last property can be achieved in a probabilistically principled manner. The following three paragraphs discuss different cross-entropy functions that we study in the context of our self-labeling loss (6).

**Standard cross-entropy** provides the obvious baseline for evaluating two alternative versions that follow. For completeness, we include its mathematical definition

\[H_{}(y_{i},_{i}) = H(y_{i},_{i})\ \ \ -_{k}y_{i}^{k}_{i}^{k}\] (7)

and remind the reader that this loss is primarily used with hard or one-hot labels, in which case it is also equivalent to NLL loss \(-_{i}^{y_{i}}\) previously discussed for ground truth labels (3). As mentioned earlier, Figure 3(a) shows that for soft pseudo-labels like \(y=(0.5,0.5)\), it forces predictions to mimic or replicate the uncertainty \( y\). In fact, label \(y=(0.5,0.5)\) just tells that the class is unknown and the network should not be supervised by this point. This problem manifests itself in the poor performance of the standard cross-entropy (7) in our experiment discussed in Figure 3 (d) (red curve).

**Reverse cross-entropy** switches the order of the label and prediction in (7)

\[H_{}}(y_{i},_{i}) = H(_{i},y_{i})\ \ \ \ -_{k}_{i}^{k} y_{i}^{k}\] (8)

which is not too common. Indeed, Shannon's cross-entropy is not symmetric and the first argument is normally the _target_ distribution and the second is the _estimated_ distribution. However, in our case, both distributions are estimated and there is no reason not to try the reverse order. It is worth noting that our self-labeling formulation (6) suggests that reverse cross-entropy naturally appears when the ADM approach splits the decisiveness and fairness into separate sub-problems. Moreover, as Figure 3(b) shows, in this case, the network does not mimic uncertain pseudo-labels, e.g. the gradient of the blue line is zero. The results for the reverse cross-entropy in Figure 3 (d) (green) are significantly better than for the standard (red). Unfortunately, now pseudo-labels \(y\) mimic the uncertainty in predictions \(\).

**Collision cross-entropy** resolves the problem in a principled way. We define it as

\[H_{}}(y_{i},_{i})  -_{k}_{i}^{k}y_{i}^{k}\ \ \ \ -^{}y\] (9)

which is symmetric w.r.t. pseudo-labels and predictions. The dot product \(^{}y\) can be seen as a probability that random variables represented by the distribution \(\), the prediction class \(C\), and the distribution \(y\), the unknown true class \(T\), are equal. Indeed,

\[(C=T)=_{k}Pr(C=k)(T=k)=^{}y.\]

Loss (9) maximizes this "collision" probability rather than the constraint \(=y\). Figure 3(c) shows no mimicking of uncertainty (blue line). However, unlike reverse cross-entropy, this is also valid when

Figure 3: Illustration of cross-entropy functions: (a) standard (7), (b) reverse (8), and (c) collision (9). (d) shows the empirical comparison on the robustness to label uncertainty. The test uses ResNet-18 architecture on fully-supervised _Natural Scene_ dataset  where we corrupted some labels. The horizontal axis shows the percentage \(\) of training images where the correct ground truth labels were replaced by a random label. All losses trained the model using soft target distributions \(=*u+(1-)*y\) representing the mixture of one-hot distribution \(y\) for the observed corrupt label and the uniform distribution \(u\), following . The vertical axis shows the test accuracy. Training with the reverse and collision cross-entropy is robust to much higher levels of label uncertainty.

is estimated from uncertain predictions \(\) since (9) is symmetric. This leads to the best performance in Figure 3 (d) (blue). Our extensive experiments are conclusive that collision cross-entropy is the best option for \(H\) in self-labeling loss (6).

## 3 Experiments

We conducted comprehensive experiments to demonstrate the choice of each element (cross-entropy, pairwise term, and neighborhood) in the loss and compare our method to the state-of-the-art. In Section 3.1, quantitative results are shown to compare different Potts relaxations. The qualitative examples are shown in Figure 7. Then we compare several cross-entropy terms in Section 3.2. Besides, we also compare our soft self-labeling approach on the nearest and dense neighborhood systems in Section 3.3. We summarized the results in Section 3.4. In the last section, we show that our method achieves the SOTA and even can outperform the fully-supervised method. More details on the dataset, implementation, and additional experiments are given in Appendix C.

### Comparison of Potts relaxations

To compare different Potts relaxations under choose one cross-entropy term. Motivated shown in Section 3.2, we use \(H_{}\). The neighborhood system is the nearest neighbors. The quantitative results are in Table 3. First, One can see that the pairwise terms with logarithm are better than those without the logarithm because the logarithm may help with the gradient vanishing problem in softmax operation. Moreover, the logarithm does not like abrupt change across the boundaries, so the transition across the boundaries is smoother (see Figure 7 in the appendix.). Note that it is reasonable to have higher uncertainty around the boundaries.

Second, the results prefer the normalized version, which confirms the points made in Figure 2. Third, the simplest quadratic formulation \(P_{}\) can be a fairly good starting point to obtain decent results. Additionally, we specifically test \(H_{}+P_{}\) due to the existing closed-form solution [1; 17]. Since the pseudo-labels generated from this formula tend to be overly soft, we explicitly add entropy terms during the training of network parameters and the mIoU goes up to \(68.97\%\) from \(67.8\%\).

### Comparison of cross-entropy terms

In this section, we compare different cross-entropy terms while fixing the pairwise term to \(P_{}\) due to its simplicity and using the nearest neighborhood system. The results are shown in Figure 4. One can see that \(H_{}\) performs the best consistently across different supervision levels, i.e. scribble lengths. Both \(H_{}\) and \(H_{}\) are consistently better than standard \(H_{}\) with a noticeable margin because they are more robust, as explained in Section 2.2, to the uncertainty in soft pseudo-labels when optimizing network parameters. We also test the performance of using \(H_{}+P_{}\) with hard pseudo-labels obtained via the \(argmax\) operation on the soft ones. The mIoU on the validation set is \(69.8\%\) under the full scribble-length supervision.

### Comparison of neighborhood systems

Until now, we only used the four nearest neighbors for the pairwise term. In this section, we also use the dense neighborhood and compare the results under the self-labeling framework.

Figure 4: Comparison of cross-entropy terms.

   &  \\  & 0 & 0.3 & 0.5 & 0.8 & 1.0 \\  \(P_{}\) & 56.42 & 61.74 & 63.81 & 65.73 & 67.24 \\ \(P_{}\) & 59.01 & 65.53 & 67.80 & 70.63 & 71.12 \\ \(P_{}\) & 58.92 & 65.34 & 67.81 & 70.43 & 71.05 \\ \(P_{}\) & 56.40 & 61.82 & 63.81 & 65.81 & 67.41 \\  \(P_{}\) & 59.04 & 65.52 & 67.84 & 70.93 & 71.22 \\  \(P_{}\) & 59.03 & 65.44 & 67.81 & 70.80 & 71.21 \\  

Table 3: Comparison of Potts relaxations with self-labeling. mIoUs on validation set are shown here.

[MISSING_PAGE_FAIL:8]

### Comparison to SOTA

In this section, we use a different network architecture, ResNet101, to fairly compare our method with the current state-of-the-art. We only compare the results before applying any post-processing steps. The results are shown in Table 5. Note that our results can outperform the fully-supervised method when using 12 as the batch size. We also observe that a larger batch size usually improves the results quite a lot. Our results with 12 batch size can outperform several SOTA methods which use 16 batch size.

## 4 Conclusions

This paper proposed a convergent soft self-labeling framework based on a simple well-motivated loss (6) for joint optimization of network predictions and soft _pseudo-labels_. The latter were motivated as auxiliary optimization variables simplifying optimization of weakly-supervised loss (4). Our systematic evaluation of the cross-entropy and the Potts terms in self-labeling loss (6) provides clear recommendations based on the discussed conceptual advantages empirically confirmed by our experiments. Specifically, our work recommends the collision cross-entropy, log-quadratic Potts relaxations, and the earest-neighbor neighborhood. They achieve the best result that may even outperform the fully-supervised method with full pixel-precise masks. Our method does not require any modifications of the semantic segmentation models and it is easy to reproduce. Our general framework and empirical findings can be useful for other weakly-supervised segmentation problems (boxes, class tags, etc.).

   &  &  &  & \)} &  \\    & & & GD & & SL & & \\   & & & hard & soft & & \\    \\  Deeplab\({}^{*}\) & V3+ & 16 & ✓ & - & - & - & 78.9 \\ Deeplab\({}^{*}\) & V3+ & 12 & ✓ & - & - & - & 76.6 \\ Deeplab  & V2 & 12 & ✓ & - & - & - & 75.6 \\    \\   \\  BPG  & V2 & 10 & ✓ & - & - & - & 73.2 \\ URSS  & V2 & 16 & ✓ & - & - & - & 74.6 \\ SPML  & V2 & 16 & ✓ & - & - & - & 74.2 \\ PSI  & V3+ & - & - & - & ✓ & - & 74.9 \\ SEMINAR  & V3+ & 12 & ✓ & - & - & - & 76.2 \\ TEL  & V3+ & 16 & - & - & ✓ & - & 77.1 \\   \\  ScribbleSup  & VGG16(V2) & 8 & - & ✓ & - & DN & 63.1 \\ DenseCRF loss\({}^{*}\) & V3+ & 12 & ✓ & - & - & DN & 75.8 \\ GridCRF loss\({}^{*}\) & V3+ & 12 & - & ✓ & - & NN & 75.6 \\ NonlocalCRF loss\({}^{*}\) & V3+ & 12 & ✓ & - & - & SN & 75.7 \\  \(_{}+_{}\) & V3+ & 12 & - & - & ✓ & NN & 77.5 \\ \(_{}+_{}\) & V3+ & 12 & - & - & ✓ & NN & **77.7** \\ \(_{}+_{}\) (no pretrain) & V3+ & 12 & - & - & ✓ & NN & 76.7 \\ \(_{}+_{}\) & V3+ & 16 & - & - & ✓ & NN & **78.1** \\ \(_{}+_{}\)(no pretrain) & V3+ & 16 & - & - & ✓ & NN & 77.6 \\  

Table 5: Comparison to SOTA methods (without CRF postprocessing) on scribble-supervised segmentation. The numbers are mIoU on the validation dataset of Pascal VOC 2012 and use full-length scribble. The backbone is ResNet101 unless stated otherwise. V2: deeplabV2. V3+: deeplabV3+: \(\): neighborhood. “\(*\)”: reproduced results. GD: gradient descent. SL: self-labeling. “no pretrain” means the segmentation network is not pretrained using cross-entropy on scribbles.

Figure 6: Comparison of different methods using Potts relaxations. The architecture is DeeplabV3+ with the backbone MobileNetV2.