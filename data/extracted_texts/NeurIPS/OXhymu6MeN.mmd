# Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression

**Jiaze Qiu**

Department of Statistics

Harvard University

Cambridge, MA 02138, USA

jiazeqiu@g.harvard.edu

###### Abstract

The Naive Mean Field (NMF) approximation is widely employed in modern Machine Learning due to the huge computational gains it bestows on the statistician. Despite its popularity in practice, theoretical guarantees for high-dimensional problems are only available under strong structural assumptions (e.g., sparsity). Moreover, existing theory often does not explain empirical observations noted in the existing literature.

In this paper, we take a step towards addressing these problems by deriving sharp asymptotic characterizations for the NMF approximation in high-dimensional linear regression. Our results apply to a wide class of natural priors and allow for model mismatch (i.e., the underlying statistical model can be different from the fitted model). We work under an _iid_ Gaussian design and the proportional asymptotic regime, where the number of features and the number of observations grow at a proportional rate. As a consequence of our asymptotic characterization, we establish two concrete corollaries: (a) we establish the inaccuracy of the NMF approximation for the log-normalizing constant in this regime, and (b) we provide theoretical results backing the empirical observation that the NMF approximation can be overconfident in terms of uncertainty quantification.

Our results utilize recent advances in the theory of Gaussian comparison inequalities. To the best of our knowledge, this is the first application of these ideas to the analysis of Bayesian variational inference problems. Our theoretical results are corroborated by numerical experiments. Lastly, we believe our results can be generalized to non-Gaussian designs and provide empirical evidence to support it.

## 1 Introduction

The Naive Mean Field (NMF) approximation is widely employed in modern Machine Learning as an approximation to the actual intractable posterior distribution. The NMF approximation is attractive as (a) it bestows huge computational gains, and (b) it is naturally interpretable and can provide access to easily interpretable summaries of the posterior distribution e.g., credible intervals. However, these two advantages may be overshadowed by the following limitations: (a) it is _a priori_ unclear whether this strategy of using a product distribution as a proxy for the true posterior will result in a "good" approximation, and (b) it has been empirically observed that NMF often tends to be significantly over-confident, especially when the feature dimension \(p\) is not negligible compared to the sample size \(n\). In the traditional asymptotic regime (\(p\) fixed and \(n\)), significant progress was made in understanding these two problems for different statistical models, see for instance  and references therein. On the other hand, in the complementary high-dimensional regime where both \(n\) and \(p\) are growing, Ghorbani et al.  recently established an instability result for the topic model under theproportional asymptotics, i.e., \(n=(p)\). In fact, in this regime, based on non-rigorous physics arguments, it is conjectured and partially established that instead of the NMF free energy one should optimize the TAP free energy. For linear regression in particular, please see . On the other hand, positive results of NMF for high-dimensional linear regression were recently established in  when \(p=o(n)\).

In this paper, we investigate the performance of NMF approximation for linear regression under the proportional asymptotics regime. As a consequence of our asymptotic characterization, we establish two concrete corollaries: (a) we establish the inaccuracy of the NMF approximation for the log-normalizing constant in this regime, and (b) provide theoretical results backing the empirical observation that NMF can be overconfident in constructing Bayesian credible regions.

Before proceeding further, we formalize the setup under investigation. Given data \(\{(y_{i},x_{i}):1 i n\}\), \(y_{i}\), \(x_{i}^{p}\), the scientist fits a linear model

\[Y=X^{}+,\] (1)

where \(_{i}^{\ iid}\ (0,^{2})\) and \(^{} S^{p}\) is a \(p\)-dimensional latent signal. We consider either \(S=\) or \(S=[-1,1]\). In fact, unless explicitly specified otherwise, \(S=\). Most of our results generalize to bounded support naturally. To recover the latent signal, the scientist adapts a Bayesian approach. She puts an _iid_ prior on \(_{i}\)'s, namely, \(_{0}()=_{i=1}^{p}(_{i})\) and then constructs the posterior distribution of \(\),

\[}{_{0}}()=_{X,y}}{ _{0}}() e^{-}\|Y-X\|^{2}},\]

with normalization constant

\[_{p}=_{p}(X,Y)=_{S^{p}}e^{-}\|Y -X\|^{2}}_{0}().\] (2)

Our results are established assuming that the design matrix \(X\) is randomly sampled from an _iid_ Gaussian ensemble, i.e., \(X_{ij}(0,1/n)\), while we provide empirical evidence for more general classes of \(X\) that has _iid_ entries with mean \(0\) and variance \(1/n\). Moreover, we assume \(n/p(0,)\) as \(n,p\).

**Definition 1** (Exponential tilting).: _For any \(:=(_{1},_{2})}^{+}\) and probability distribution \(\) on \(S\), we define \(^{}\) as_

\[^{}}{}(x):=(_{1}x-}{2}x^{2}-c()),\ c()=c_{}():=_{ S}(_{1}x-}{2}x^{2})(x).\]

_Note that \(c()\) depends on the distribution \(\) and is usually referred to as the cumulant generating function in the statistics literature._

Figure 1: These three figures serve as a visual summary of our main results when the Gaussian Spike and Slab prior is adopted, i.e., NMF does not provide up to leading order correct approximation to the log-normalizing constant (left), and the estimated credible regions suggested by the NMF distribution do not achieve the nominal coverage (middle), even when NMF could achieve close to optimal MSE. Please see Lemma 6 for definitions of the Gaussian Spike and Slab prior and the hyper-parameters \(q\) and \(^{2}\).

Using this definition of exponential tilts, the \((X^{T}X)_{ii}_{i}^{2}\) terms in (2) can be absorbed into the base measure

\[() e^{-}\|y-X\|^{2}+_{i=1} ^{p}}{2}_{i}^{2}}_{i=1}^{p}_{i}(_{i}),\]

where \(_{i}:=^{(0,d_{i})}\) and \(d_{i}:=X)_{ii}}{^{2}}\). By the classical Gibbs variational principle (see for instance ), the log-normalizing constant can be expressed as a variational form,

\[-_{p}= _{Q}(_{Q}[}\|y-X \|^{2}]+_{KL}(Q\|_{0}))\] \[= _{Q}(_{Q}[}\|y-X \|^{2}-_{i=1}^{p}}{2}_{i}^{2}]+_{KL} (Q\|_{i=1}^{p}_{i}))-_{i=1}^{p}c(0,d_{i}),\]

where the \(\) is taken over all probability distribution on \(S^{p}\). While the infimum is always attained if and only if \(Q=\), the Naive Mean Field (NMF) approximation restricts the variational domain to product distributions only and renders a natural upper bound,

\[_{Q=_{i=1}^{p}Q_{i}}[_{Q}(} \|y-X\|^{2}-_{i=1}^{p}}{2}_{i}^{2})+_ {KL}(Q\|_{i=1}^{p}_{i})-_{i=1}^{p}c(0,d_{i})].\] (3)

It can be shown that the product distribution \(\) that achieves this infimum is exactly the one closest to \(\), in terms of KL-divergence. Before moving forward, we need some additional definitions and basic properties of exponential tilts. The first lemma establishes that instead of using \((_{1},_{2})\) we can also use \((u,_{2})=(_{U^{}}U,_{2})\) to parameterize the tilted distribution.

**Lemma 1** (Basic properties of the cumulant generating function \(c()\)).: _Let \(c()\) be as in Definition 1. Let \(()\) denote the support of \(\). If \(m():=()<0\) and \(M():=()>0\), then the following conclusions hold._

* \((_{1},_{2}):=, _{2})}{_{1}}=_{X^{}}(X)\) _is strictly increasing in_ \(_{1}\)_, for every_ \(_{2}\)_._
* _For any_ \(u(m(),M())\)_, there always exists a unique_ \(h(u,_{2})\) _such that_ \((h(u,_{2}),_{2})=u\)_._

**Definition 2** (Naive mean field variational objective).: _With \(d_{i}:=(X^{T}X)_{ii}/^{2}\), we define \(M_{p}(u):[m(),M()]^{p}}\) as_

\[M_{p}(u):=}\|y-Xu\|^{2}+_{i=1}^{p}[G( u_{i},d_{i})-u_{i}^{2}}{2}],\]

_where \(G\) is defined as a possibly extended real valued function on \([m(),M()]\),_

\[G(u,d) :=_{}(^{(h(u,d),d)}\|^{(0,d)})=uh(u,d)- c(h(u,d),d)+c(0,d) u(m(),M()),d,\] \[:=_{}(_{}\|^{(0,d)}) u=M()<,d,\] \[:=_{}(_{-}\|^{(0,d)}) u=m()>-,d,\]

_in which \(h(,)\) was defined in Lemma 1 and \(_{}\) and \(_{-}\) are degenerate distributions which assigns all measure to \(M()\) and \(m()\) respectively._

Note that under product distributions, the \(_{Q}()\) term in (3) is parameterized by the mean vector \(u:=_{Q}\) and exponential tilts of \(_{i}\)'s minimize the KL-divergence term. Therefore, the scaled log-normalizing function, which is also referred to as the average free energy in statistical physics parlance and (log) evidence in Bayesian statistics, is bounded by the following variational form,

\[-_{p}_{u[m(),M()]^{p}}M_{ p}(u)-_{i=1}^{p}c(0,d_{i})=-_{p}^{ }.\] (4)The right-hand side is equal to (3) and is also referred to as the evidence lower bound (ELBO) or NMF free energy, which can be used as a model selection criterion, see for instance . Asymptotically, the second term is nothing but a constant since it concentrates around \(c(0,1/^{2})\) as \(n,p\).

The main theoretical question of interest here is whether this bound in (4) is asymptotically tight or not, which serves as the fundamental first step towards answering the question of whether NMF distribution is a good approximation of the target posterior. Please see [4; 27] for comprehensive surveys on variational inference, including but not limited to NMF approximation.

To derive sharp asymptotics for the NMF approximation, the key observation is that the optimization problem is convex under certain priors. We then employ the Convex Gaussian Min-max Theorem (CGMT). CGMT is a generalization of the classical Gordon's Gaussian comparison inequality , which allows one to reduce a minimax problem parameterized by a Gaussian process to another (tractable) minimax Gaussian optimization problem. This idea was pioneered by Stojnic  and then applied to many different statistical problems, including regularized regression, M-estimation, and so on, see for instance [15; 24]. Unfortunately, concentration results derived from CGMT require both Gaussianity and convexity. This is exactly why we need the Gaussian design assumption in our analysis. In the meantime, though we do not pursue this front theoretically, we provide empirical evidence for more general design matrices in the Supplementary Material. It is worth noting that there is a parallel line of research that aims to develop universality results for these comparison techniques. We refer the interested reader to  and references within.

Let us emphasize that our main conceptual concern is not investigating whether (4) as a convex optimization procedure gives a good point estimator, but instead evaluating whether NMF as a strategy or product distributions as a family of distributions can provide "close to correct" approximation for the true posterior. Nevertheless, this optimizer's asymptotic mean square error can also be characterized as a by-product of our main theorem.

Regarding the accuracy of variational approximations in general, certain contraction rates and asymptotic normality results were established in the traditional fixed \(p\) large \(n\) regime [28; 17; 10]. However, under the high-dimensional setting and scaling we consider in the current paper, without extra structural assumptions (e.g., sparsity), both the true posterior and its variational approximation are not expected to contract towards the true signal, which also explains why one is instead interested in whether the log-normalizing constant can be well approximated, as a weaker standard of "correctness". Ray et al.  studied a pre-specified class of mean field approximation in sparse high-dimensional logistic regression. Recently, the first known results on mean and covariance approximation error of Gaussian Variational Inference (GVI) in terms of dimension and sample size were obtained in .

## 2 Results

This section starts with some necessary definitions and our main assumptions. Then, we present our main theorem and one natural corollary. Finally, we identify a wide class of priors that would ensure the convexity of the NMF objective, which plays a crucial role in our analysis.

### Notations and main assumptions

**Notations:** We use the usual Bachmann-Landau notation \(O()\), \(o()\), \(()\) for sequences. For a sequence of random variables \(\{X_{p}:p 1\}\), we say that \(X_{p}=o_{p}(1)\) if \(X_{p}}{{}}0\) as \(p\) and \(X_{p}=o_{p}(f(p))\) if \(X_{p}/f(p)=o_{p}(1)\). We use \(C,C_{1},C_{2}\) to denote positive constants independent of \(n,p\). Further, these constants can change from line to line. For any square symmetric matrix \(A\), \(\|A\|_{}\) and \(\|A\|_{F}\) denote the matrix operator norm and the Frobenius norm, respectively.

**Assumption 1** (Proportional asymptotics).: _We assume \(n/p(0,)\), as \(n,p\)._

**Assumption 2** (Gaussian features).: _For all our theoretical results, we assume the design matrix \(X\) is randomly sampled from an iid Gaussian ensemble, i.e., \(X_{ij}}{{}}(0,1/n)\)._

**Definition 3**.: _Define \(F:(m(),M())\) as_

\[F(u)=F_{,^{2}}(u):=G(u,d_{1})-d_{1}}{2 }=G(u,})-}{2^{2}}.\]

**Definition 4** (The NMF point estimator).: _Recalling the NMF objective \(M_{p}()\) as in Definition 2, let \(=_{}:=_{u[-1,1]^{p}}M_{p}(u)\) be the NMF point estimator, which is also the mean vector of the product distribution (\(\)) that best approximates the posterior in terms of KL-divergence. We refer to this optimal product distribution as the NMF distribution._

**Assumption 3** (Convexity of \(F()\)).: _We assume \(F()\) is strongly convex on \(S^{}:=S S\)._

As alluded to, our analysis relies on the convexity of the "penalty" term \(F()\). Please note that the definition of \(F()\) only depends on the prior \(\) chosen by the statistician, rather than the "true prior" \(^{}\). Therefore, to support this assumption, we provide a few sufficient conditions that identify a broad class of priors that ensure (strong) convexity of \(F()\) in Section 2.3.

Throughout, we work under a partially well-specified situation, i.e., model (1) is assumed to be correct, but \(_{i}^{}\)'s may not have been _a priori_ sampled _iid_ from \(\). Instead, we assume the empirical distribution of \(_{i}^{}\)'s converges in \(L_{2}\) to a probability distribution \(^{}\) supported on \(S\). In addition, the noise level \(^{2}\) is fixed and known to the statistician. Last but not least, \(^{}\) is assumed to have finite second moment and let \(s_{2}:=_{T^{}}[T^{2}]<\).

### Main results

From now on, we always assume Assumption 1, 2, and 3. Next, we introduce a scalar denoising function, which is just the proximal operator of \(F()\).

**Definition 5** (Scalar denoising function).: _For \(x\) and \(t>0\),_

\[(x,t):=_{w S}\{(w-x)^{2}+F(w)\} S^{ }\]

Since \(F()\) is strongly convex, this one-dimensional optimization has a unique minimizer. Note that when \(S=[-1,1]\), since \(_{w 1}F}{w}(w)=_{w 1}h(w,1/ ^{2})}=\), the minimum is never achieved on the boundary of \(S\). Similarly, when \(S=\), \(_{w}E}{w}(w)=\). Therefore, the minimum is always achieved at a stationary point. Lastly, \((0,t)=0\) if \(\) is symmetric. In fact, throughout this paper, we only consider symmetric priors.

Before stating our main result and its implications, we first introduce a two-dimensional optimization problem, which will play a central role in our later discussion,

\[_{b 0}_{}(b,)\] (5) \[(b,):=(}{}+)- {2}b^{2}+_{w S}\{w^{2}-bZw +^{2}F(w+B)-^{2}F(B)\},\] (6)

where \(F()\) was defined in Definition 3 and the \(\) is taken over \((B,Z)^{}(0,1)\). In the next lemma, we gather some additional characterizations of this min-max problem.

**Lemma 2**.: _The max-min in (5) is achieved at some \((b^{},^{})(0,)(,)\). In fact, \(b^{}\) is unique. In addition, \((b^{},^{})\) is also a solution to the following fixed point equation,_

\[^{2}&=^{2}+ [(( Z+B,}{b})-B )^{2}]\\ b&=-[Z ( Z+B,}{b})]=(1- {}[^{}( Z+B,}{b} )]),\] (7)

_where \(^{}(x,t):=(x,t)\)._

**Definition 6**.: _We use \(^{}=^{}_{,^{}}\) to denote the distribution of \(((^{}Z+B,^{2}}{b^{}}),B)\), in which \((B,Z)^{}(0,1)\). We denote by \(\) the empirical distribution of \(\{(_{i},_{i}^{})\}_{i=1}^{p}\)._

We are ready to state our main result, which provides a sharp asymptotic characterization of \(\).

**Theorem 1**.: _Suppose the max-min problem in (5) has a unique optimizer \((b^{},^{})\), or the fixed point equation in (7) has a unique solution \((b^{},^{})\). Then for all \(>0\), as \(n,p\),_

\[(W_{2}(^{},)^{2} ) 0,\]

_where \(W_{2}(,)\) stands for order \(2\) Wasserstein distance._

**Remark 1**.: _This result indicates the NMF estimator \(\) should be asymptotically roughly iid among different coordinates, which is different from the NMF distributions being product distributions._

**Corollary 1**.: _Suppose the hidden true signal \(^{}\) was a priori sampled \(iid\) from a probability distribution \(^{}\) with finite second moment. Note that \(^{}\) can differ from the prior \(\) that the Bayesian statistician chose. In addition, suppose the max-min problem in (5) has a unique optimizer \((b^{},^{})\), or the fixed point equation in (7) has a unique solution \((b^{},^{})\), then for all \(>0\),_

\[(W_{2}(^{}_{,^{}},)^{2} ) 0,n,p,\]

_in which \(^{}\) was defined in Definition 6._

We provide a proof sketch in Section 6 and all the detailed proofs are deferred to the Supplementary Materials.

### Convexity of \(F()\)

In this section, we present a few lemmas that would ensure the validity of Assumption 3. In fact, if conditions of any of these lemmas are satisfied, Assumption 3 holds.

**Lemma 3** (Condition to ensure convexity of \(F()\): nice prior).: _Suppose \(\) is absolutely continuous with respect to Lebesgue measure and_

\[}{x}(x) e^{-V(x)}, x(),\]

_for some \(V:()\). In addition, suppose either of the following two conditions is true,_

1. \(()=\)_;_ \(V(x)\) _is continuously differentiable almost everywhere;_ \(V(x)\) _is unbounded above at infinity._
2. \(()=[-a,a]\)_, for some_ \(0<a<\)_;_ \(V(x)\) _is continuously differentiable almost everywhere._

_Then if \(V(x)\) is even, non-decreasing in \(|x|\) and \(V^{}(x)\) is convex, \(F()\) is always strongly convex, regardless of the value of \(^{2}\)._

**Lemma 4** (Condition to ensure convexity of \(F()\): discrete prior).: _Suppose \(\) is a symmetric discrete distribution supported on \(\{-1,0,1\}\),_

\[(x)=q(x)+(x-1)+(x+1),\]

_for \(q(2/3,1)\). Then \(F()\) is always strongly convex, regardless of the value of \(^{2}\)._

Proofs of Lemma 3 and 4 crucially utilize the Griffiths-Hurst-Sherman (GHS) inequality , which arose from the study of correlation structure in spin systems. The following two lemmas give examples of some other families of priors for which convexity of \(F()\) depends on the noise level \(^{2}\), while those in Lemma 3 and 4 do not.

**Lemma 5** (Condition to ensure convexity of \(F()\): low signal-to-noise ratio).: _Suppose \(()[-a,a]\) for some \(a>0\). Then as long as \(^{2}>a^{2}\), \(F(u)=F_{}(u,^{2})\), as a function of \(u\), is always strongly convex on \(S\), regardless of the exact choice of \(\) and value of \(^{2}\)._

**Lemma 6** (Condition to ensure convexity of \(F()\): Spike and Slab prior).: _Consider a spike and slab prior to the following form,_

\[(x)=q(x)+}}e^{-} {2^{2}}}x\]

_which is just a mixture of a point mass at \(0\) and a Normal distribution of mean \(0\) and variance \(^{2}\). Suppose_

\[_{h}_{X_{,^{2}}}(X)<^{2}\] (8)_where \(_{,^{2}}\) is again a Gaussian spike and slab mixture,_

\[(x)=(x)+}{^{2}}}e^{-}{2^{2}}}x\] \[=/^ {2})^{-1/2}}^{2}=^{2}}{ ^{2}+^{2}}.\]

_Then, \(F(u)\) is strongly convex. In addition, one easier-to-check sufficient condition for (8) is_

\[(1+}{^{2}}})}{^{2}+^{2}}<1.\] (9)

**Remark 2**.: _It is easy to see that for large enough \(\) (\(q\) and \(\) fixed), or small enough \(q\) (\(\) and \(\) fixed), or small enough \(\) (\(q\) and \(\) fixed), or small enough \(\) (\(q\) and \(\) fixed), (9) is always satisfied. In other words, \(F()\) is strongly convex for low signal-to-noise ratio or high temperature in physics parlance._

## 3 Log normalizing constant: sub-optimality of NMF

As alluded, as implications of Theorem 1, we develop asymptotics of both \(_{p}^{}\) and mean square error (MSE) of the NMF point estimator \(\) in terms of \((b^{},^{})\).

**Corollary 2** (MSE).: _When conditions of Corollary 1 hold, as \(n,p\),_

\[\|-^{}\|^{2}}{{}}_{(B,Z)^{}(0,1)} [((^{}Z+B,^{2}}{b^{}} )-B)^{2}]=({^{}}^{2}-^{2}).\]

**Corollary 3** (Log normalizing constant).: _When conditions of Corollary 1 hold, as \(n,p\),_

\[-_{p}^{}=[M_{p}()- _{i=1}^{p}c(0,d_{i})]}{{}}}^{2}}{2 ^{2}}+F((B+^{}Z,^{}/b^{}))-c(0,1/ ^{2}).\]

Though all our main theorems and corollaries apply to the case when \(^{}\), for simplicity and clarity, from now on, we only consider the "nicest" setting, i.e, when assumptions of Corollary 1 are satisfied and in addition \(=^{}\). By doing so, we would like to convey that even if there were no model mismatch at all, NMF still would not be "correct".

Concentration and limiting values of both the optimal Bayesian mean square error (i.e., \(_{}\|-[^{}|X,y]\|^{2}/p\)) and the actual log-normalizing constant were conjectured and rigorously established under additional regularity conditions, which provides us the "correct answers" to compare with. Please see [2; 20].

Please see Figure 2 for numerical evaluations of Corollary 3, which suggests the bound in (4) is not tight for Gaussian Spike and Slab prior. Since, in general, both \(F()\) and \((,)\) lack analytical forms, it is hard to provide a universal guarantee on whether (5) has a unique optimizer or the fixed point equation (7) has a unique solution. In fact, our numerical experiments suggest it is possible for (7) to have multiple fixed points. Therefore, how to exactly realize and evaluate the asymptotic predictions in these two corollaries (so as Corollary 4 in the next section) is challenging in general and can only be done in a case-by-case basis and usually involves numerically solving (7). In light of this observation, we use the Gaussian Spike and Slab prior as defined in Lemma 6 for presentation purposes. Since it is both non-trivial and of practical interest, though, we do emphasize that the same framework and workflow also apply to other priors. Without loss of generality, we also take \(^{2}=1\). This choice renders Figure 2 and 3 in the next section. Details of how to generate these plots are deferred to the Supplementary Material.

## 4 Uncertainty quantification: the average coverage rate

To study uncertainty quantification properties of NMF approximation, we consider the average coverage rate of symmetric Bayesian credible regions (of level \(1-\)) suggested by the NMF distributions, i.e, \(R_{p,}:=_{i=1}^{p}_{\{_{i}^{}[ _{i,/2},_{i,1-/2}]\}}\), where \(_{i,t}\) is the \(t\)-th quantile of \(^{(h(_{i},d_{i}),d_{i})}\). In order to study asymptotic behavior of \(R_{p,}\), we define an \((m(),M()) S\{0,1\}\) indicator function

\[_{}(u_{0},_{0})=_{\{_{0}[q_{(h(u _{0},1/^{2}),1/^{2}),/2}q_{(h(u_{0},1/^{2}),1/^ {2}),1-/2}]\}}.\]

The following corollary of Theorem 1 establishes the asymptotic convergence of \(R_{p,}\). Numerically evaluating it for the Gaussian Spike and Slab prior renders Figure 3, which shows NMF credible regions can not achieve the nominal coverage, in this case, 95%, and also provides an exhibition of how large the gaps are for different hyper-parameters.

**Corollary 4**.: _Suppose conditions of Corollary 1 hold. In addition, assume the quantile function of \(\) is continuous. Then as \(n,p\),_

\[R_{p,}}{{}}_{(B,Z) ^{}(0,1)}[_{}(( ^{}Z+B,^{2}}{b^{}}),B)].\]

On the other hand, based on the asymptotic joint distribution of \(\) and \(^{}\) as stated in Corollary1, we can in fact identify a strategy of constructing asymptotically exact Bayesian credible regions based on \(\). Let \(q_{t}(x)\) be the \(t\)-th quantile of conditional distribution of \(B\) given \((^{}Z+B,^{}^{2}/b^{})=x\). This way, the following Corollary ensures \([q_{/2}(_{i}),q_{1-/2}(_{i})]\) is asymptotically of at least \(1-\) coverage.

Figure 3: These two figures show that estimated credible regions given by NMF do not achieve the nominal coverage (95%) when \(=^{}\) is a Gaussian Spike and Slab distribution. Recall that \(=n/p\) and please see Lemma 6 for exact definitions of the hyper-parameters \(q\) and \(^{2}\).

**Corollary 5**.: _Suppose conditions of Corollary 4 hold, then for any \(>0\),_

\[_{p}(_{i=1}^{p}1_{\{_{i}^{ }[q_{/2}(_{i}),q_{1-/2}(_{i})]\}}<1-- )=0.\]

## 5 Discussion: Extensions and Limitations

In order to provide some intuition on why the NMF approximation is loose in the current setting, it is worth noting that in comparison with the proportional asymptotics regime we consider here, positive results of NMF for high-dimensional linear regression were recently established in  when \(p=o(n)\). Using terminology from Austin , Mukherjee and Sen  (when restricted to designs with _iid_ Gaussian features) essentially proved, when \(p/n 0\), the eigenvalue concentration behavior of \(X^{T}X\) leads to the Hamiltonian being of "low complexity". On the other hand, when \(p=(n)\), \((A^{2}) o(p)\), where \(A=A(X)\) is defined as the off-diagonal part of \(X^{T}X\), which violates [16, Equation (5)]. Roughly speaking, when the eigenstructure of A is not "dominated" by a few top eigenvalues, the Hamiltonian can not be covered by an efficient net and thus is not of "low complexity". Please see  for more details.

We want to be clear about the fact that, technically, we did not "prove" the sub-optimality of NMF. Instead, we rigorously derived asymptotic characterizations of NMF approximation through the solution of a fixed point equation. But this fixed point equation can only be solved numerically on a case-by-case basis and is not guaranteed a unique solution. All our plots are based on iteratively solving the fixed point equation. As a matter of fact, for instance, when \(q\) is close to \(1\) for the Gaussian Spike and Slab prior we considered, the fixed point equation is clearly not converging to the right fixed point, as demonstrated in the Supplementary Material. It could also just not converge for very small \(\). Nevertheless, all the plots we are showing in the main text are backed by a numerical simulation using simple gradient descent to optimize the NMF objective, i.e. \( M_{p}(u)\), for \(n=8000\). All in all, it is probably more accurate to say we provided a tool for establishing the sub-optimality of NMF for a general class of priors rather than proving it for good.

Another obvious limitation is we can only handle priors that guarantee convexity of the KL-divergence term in terms of the mean parameters. Though it is indeed a broad class of distributions covering some of the most commonly used symmetric priors (e.g., Gaussian, Laplace, and so on), little is known about the asymptotic behavior of NMF when the convexity assumption is violated.

We note that, in theory, in order to carry out the analysis using CGMT, the additive noise \(\) as defined in (1) does not have to be Gaussian. Instead, as long as it has log-concave density, the same proof idea applies, though we intentionally chose to stick with Gaussian noise as it renders much cleaner results and a more comprehensive presentation. In addition, we expect stronger uniform convergence results (e.g., uniform in \(^{2}\)) could also be established, which can be crucial for applications like hyperparameters selection. Please see  for an example in which results of this flavor were obtained.

## 6 Proof strategy

This section gives a proof outline of Theorem 1. More details can be found in the Supplementary Material. Replacing all \(d_{i}\)'s in \(M_{p}\) with \(d_{i}=1/^{2}\), we define \(N_{p}\) as

\[N_{p}(u)=}\|Y-Xu\|_{2}^{2}+_{i=1}^{p}[G(u_{i},1/ ^{2})-^{2}}{2^{2}}]=}\|Y-Xu \|_{2}^{2}+_{i=1}^{p}F(u_{i}).\]

**Lemma 7**.: _Let \(_{N}:=*{arg\,min}_{u}[N_{p}(u)]\). Then for some \(C_{s}^{+}\), as \(n,p\),_

\[((\|\|^{2},\|_{N}\|^{2})>(1+C_{s} )s_{2}) 0.\]

**Lemma 8**.: _For any \(>0\), as \(n,p\), with \(C_{s}\) as defined in Lemma 7,_

\[(_{\|u\|^{2}/p(1+C_{s})s_{2}}|_{i= 1}^{p}[G(u_{i},1/^{2})-^{2}}{2^{2}}]-[G (u_{i},d_{i})-u_{i}^{2}}{2}]|>)  0.\] (10)According to Lemma 8 and 7, \(N_{p}()\) and \(M_{p}()\) are with high probability uniformly close. Thus, from now on, we focus on using Gaussian comparison to analyze \(_{N}\) and \(N_{p}(_{N})\) in place of \(\) and \(M_{p}()\). Since \(F()\) is strongly convex, \(:=_{N}-^{}\) is the unique minimizer of

\[L(w):=\|Xw-\|^{2}+}{n}_{i=1}^{p}(F (w_{i}+_{i}^{})-F(_{i}^{})).\]

By introducing a dual vector \(s\), we get

\[_{w}L(w)=_{w^{p}}_{s^{n}}s^{T} (Xw-)-\|s\|^{2}+}{n}_{i=1}^{p}(F( w_{i}+_{i}^{})-F(_{i}^{})).\]

By CGMT (see for instance [22, Theorem 3.3.1] or [15, Theorem 5.1]), it suffices now to study

\[_{w^{p}}_{s^{n}}}\|s\|g^{T}w +}\|w\|h^{T}u-s^{T}-\|s\|^{2}+ }{n}_{i=1}^{p}(F(w_{i}+_{i}^{})-F(_{i }^{}))\]

where \(g(0,I_{p})\) and \(h(0,I_{n})\) and they are independent. Note that the \(\) and \(\) can be flipped due to convex-concavity. By optimizing with respect to \(s/\|s\|\) and introducing \(}{n}+^{2}}=_{}\{}{4n}+^{2}}{2}+\}\), it can be further reduced to

\[_{b 0}_{}(}{}+)- }{2}+_{w^{p}}_{i=1}^{p}[ \{w_{i}^{2}-bg_{i}w_{i}+^{2}F(w_{i}+ _{i}^{})-^{2}F(_{i}^{})\}].\]

Under minor regularity conditions, as \(n,p\), it converges to

\[_{b 0}_{}(}{}+ )-}{2}+_{B,Z}_{w}\{w^{2}-bZw+^{2}F(w+B)-^{2}F(B)\}\]

with \((B,Z)^{}(0,1)\), which is how we got \((,)\) as in (5). Furthermore, by differentiating \((b,)\) with respect to \(\) and \(b\), we arrive at the fixed point equation in Lemma 2. Last but not least, note that \(_{w}\{w^{2}-bZw+^{2}F(w+B)\}=( Z +B,^{2}/b)-B\), which explains why the joint empirical distribution of \((_{i},_{i}^{})\)'s converges to the law of \(((^{}Z+B,^{}^{2}/b^{})-B,B)\). Finally, we note that similar proof arguments were made in .