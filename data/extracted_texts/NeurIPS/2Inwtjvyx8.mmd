# Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection

Hui Wei\({}^{1}\)   Zhixiang Wang\({}^{2}\)   Kewei Zhang\({}^{1}\)1

Jiaqi Hou\({}^{1}\)   Yuanwei Liu\({}^{1}\)   Hao Tang\({}^{3}\)   Zheng Wang\({}^{1}\)\({}^{}\)

\({}^{1}\)National Engineering Research Center for Multimedia Software,

School of Computer Science, Wuhan University

\({}^{2}\)The University of Tokyo \({}^{3}\)School of Computer Science, Peking University

https://camera-agnostic.github.io/

Equal contribution \({}^{}\)Corresponding author

###### Abstract

Physical adversarial attacks can deceive deep neural networks (DNNs), leading to erroneous predictions in real-world scenarios. To uncover potential security risks, attacking the safety-critical task of person detection has garnered significant attention. However, we observe that existing attack methods overlook the pivotal role of the camera, involving capturing real-world scenes and converting them into digital images, in the physical adversarial attack workflow. This oversight leads to instability and challenges in reproducing these attacks. In this work, we revisit patch-based attacks against person detectors and introduce a camera-agnostic physical adversarial attack to mitigate this limitation. Specifically, we construct a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap. Furthermore, the camera ISP proxy network serves as a defense module, forming an adversarial optimization framework with the attack module. The attack module optimizes adversarial patches to maximize effectiveness, while the defense module optimizes the conditional parameters of the camera ISP proxy network to minimize attack effectiveness. These modules engage in an adversarial game, enhancing cross-camera stability. Experimental results demonstrate that our proposed Camera-Agnostic Patch (CAP) attack effectively conceals persons from detectors across various imaging hardware, including two distinct cameras and four smartphones.

## 1 Introduction

Adversarial attacks have emerged as a concerning threat to deep neural network (DNNs)-based models, casting a shadow over their reliability, particularly as certain attack methods extend beyond the digital space and prove effective in real-world scenarios [1; 5; 27]. Examples include wearing specialized glasses to mislead facial recognition models for impersonation attacks  or wearing clothing with adversarial textures to evade machine vision systems . This category of attacks is commonly known as physical adversarial attacks .

Successfully executing physical adversarial attacks presents heightened challenges due to domain transitions and various dynamic physical factors encountered throughout the process from crafting digital perturbations to launching real-world attacks. Existing attack methods against person detectors have demonstrated notable advancements , and we categorize their efforts into two main types: **(1) Transitioning from the digital to the physical domain**, where techniques such as Non-Printability Scores (NPS)  are employed to mitigate color reproduction discrepancies causedby printers . **(2) Transformations in the physical domain**, which involve using operations like rotations, scale variations, and others to simulate real-world variations [11; 28], leveraging Thin Plate Splines (TPS) to model cloth deformation , and utilizing fully-cover textures on clothing to handle multi-angle variations in the real world [13; 14]. Naturally, a question arises: Is it necessary to explore another transition, namely, **transitioning from the physical to the digital domain**?

In the journey from the physical scene to digital images, the camera plays a crucial role. This aspect has been overlooked for an extended period. Therefore, to shed light on the aforementioned question, we evaluated the camera's impact on attack performance. Specifically, we captured the same physical scene using different cameras (Sony, Canon, iPhone, _etc._) and observed that the detection results for non-attack persons remained relatively stable, whereas, for persons with adversarial patches, the confidence values exhibited considerable variations. Some of the results are shown in Figure 0(a). These experimental results demonstrate that the physical-to-digital domain transition, specifically the camera imaging pipeline's transformation of real-world scenes into digital counterparts, constitutes a crucial factor that significantly impacts adversarial attack performance.

Inspired by this observation, we are committed to designing _camera-agnostic_ physical adversarial attacks. To maintain stable attack performance across a variety of imaging devices in the real world, our method introduces a camera simulation into the adversarial patch generation pipeline. Here we emphasize the significance of the camera ISP, a pivotal component that connects the RAW sensor data captured by the camera to the ultimate processed image. Our analysis reveals that camera ISP processing inherently attenuates attack performance, highlighting the camera ISP's potential defensive role against adversarial attacks, effectively positioning it as a natural defender. This observation aligns with Zhang _et al._, who employed learned ISP pipelines to design an off-the-shelf preprocessing module for defending against digital adversarial attacks. Consequently, we propose an adversarial optimization framework to generate camera-agnostic adversarial patches. Specifically, a differentiable camera ISP proxy network functions as a defense module by adjusting conditional parameters to reduce the efficacy of adversarial patches. Conversely, the patch optimization module enhances attack performance by optimizing the patch itself. This adversarial optimization endows the generated patches with robust effectiveness across diverse camera hardware, as illustrated in Figure 0(b).

In summary, our main contributions are as follows:

* A complete modeling of the workflow for physical adversarial attacks that integrates camera modules previously overlooked in existing research. Our method unveils the significant impact of the imaging devices and integrates a differentiable camera ISP proxy network into the attack pipeline.
* A new adversarial patch generation framework gains cross-camera attack capabilities. Our method leverages the camera ISP module's defense properties by optimizing conditional parameters to reduce patch effectiveness, establishing a zero-sum game with the perturbation

Figure 1: **Illustration depicting the impact of camera on attack performance. The bounding boxes indicate the YOLOv5  detector successfully detects the person. In each setting, we maintain scene consistency to minimize irrelevant influences. In contrast to the AdvPatch  attack, which is effective only on Samsung devices, our method successfully executes attacks across all six cameras.**

optimization module. This interaction ultimately strengthens the camera-agnostic robustness of the generated adversarial patch.
* Improved attack efficacy and heightened stability gains over existing methods. Real-world experiments demonstrate that our approach consistently and effectively achieves attack objectives across various imaging devices, including two typical cameras (Sony and Canon) and four smartphone cameras (iPhone, Redmi, Huawei, and Samsung).

## 2 Related Work

Physical Adversarial Attacks on Vision TasksCompared to digital adversarial attacks [2; 27; 36], physical adversarial attacks are more threatening because they can deceive DNNs-based models in the real world. Sharif _et al._ achieved the first implementation of physical adversarial attacks, targeting facial recognition systems. Since then, researchers have been designing attacks for various computer vision tasks, including classification , detection , segmentation , depth estimation , and image captioning . In general, these methods generate perturbations in the digital domain, then transform them into tangible physical entities, deploy them in real-world scenarios, capture them with cameras, and finally return to the digital domain to complete the attack2 In this process, two domain transitions are experienced. The first, namely digital-to-physical, has been addressed by some works [16; 26]. However, the second, namely physical-to-digital, has been always overlooked, resulting in existing attack methods being unstable and difficult to reproduce. Our approach instead addresses this absence by incorporating a differentiable camera ISP network, thus constructing a more comprehensive perturbation generation pipeline.

Adversarial Patches for Person DetectionDue to the significance of human privacy and security, adversarial patches are widely employed for attacking person detection models in real-world scenarios . We summarize recent work on patch-based physical adversarial attacks targeting person detection in Table 1. Although existing methods have made significant progress in terms of effectiveness [29; 34; 35], stealthiness [11; 28], and robustness [14; 15], they all overlook the widespread scenario of cross-camera attacks in the physical world. They assume a white-box camera system, which diminishes their effectiveness in real-world scenarios with unseen cameras. Therefore, we advocate for treating the camera system as a black box and propose a method for designing camera-agnostic adversarial patches.

Camera Image Signal Processing PipelineIn the journey from the physical scene to digital RGB images, the camera's internal Image Signal Processing (ISP) pipeline plays a crucial role. The ISP pipeline is tasked with converting the RAW measurements captured by camera sensors into high-quality RGB images that are suitable for further analysis and human perception. It employs a range of techniques and algorithms, such as demosaicing [7; 18], denoising [9; 37], white balancing [10; 12], to enhance acquired data, mitigate noise artifacts, and correct for optical aberrations. Intuitively, Zhang

   Categories & Method & Digital-to-Physical & Physical & Physical-to-Digital & Black-box camera \\  & & transition & transformation & transition & evaluation \\   & NAP (2021)  & ✗ & Scale, angle, _etc._ & ✗ & ✗ \\  & LAP (2021)  & NPS Loss & Scale, angle, _etc._ & ✗ & ✗ \\    & AdvTLoak (2020)  & Rendering Function & TPS deformation & ✗ & ✗ \\   & TC-EGA (2022)  & ✗ & TPS deformation & ✗ & ✗ \\   & T-SEA (2023)  & ✗ & Patch Cutout & ✗ & ✗ \\    & CAP (Ours) & NPS Loss & Scale, angle, _etc._ & Camera ISP Net & ✗ \\   

Table 1: **Summary of typical patch-based physical adversarial attacks against person detection.** While existing attack methods have partially addressed the Digital-to-Physical transition, none have systematically investigated the Physical-to-Digital transition. Our proposed CAP attack introduces a Camera Proxy Network to model this crucial transition and comprehensively evaluates attack performance across diverse unseen imaging devices.

_et al._ discovered that the camera's ISP weakens the effectiveness of adversarial perturbations and developed an off-the-shelf preprocessing adversarial defense method. Inspired by this insight, our approach incorporates a differentiable camera ISP network as a defense module, designing an adversarial optimization framework to ensure attack robustness of generated adversarial perturbations across different camera ISP configurations.

## 3 Camera-Agnostic Attack

### Problem Definition

A camera system plays a transformational role in converting the physical scene \(I_{}\) into its digital counterpart \(I_{}\). Subsequently, the digital image \(I_{}\) serves as input for well-trained downstream DNNs-based models \(g\), producing predictions \(y\) that closely align with the ground truth (\(Y_{}\)). Our goal is to generate adversarial patches \(P\) and apply them to \(I_{}\) to attack the model \(g\) to cause incorrect predictions \(y^{}\). Unlike existing camera-specific physical adversarial attack methods, our approach aims to maintain stable performance across various cameras. In our attack setting, we regard the imaging process (from \(I_{}\) to \(I_{}\)) as a black box.

### Overall Framework

To enable the generated adversarial patches to adapt to various cameras, we introduce a novel adversarial optimization framework (see Figure 2). It consists of two mutually adversarial parts: Attacker and Defender. The attacker has an ISP proxy network on top of existing attacking strategies. The ISP network maps adversarial perturbations to the RGB space based on conditional input hyperparameters. The processed adversarial perturbations are subsequently applied to benign samples and fed into the target detection model to get predictions. The attacker iteratively optimized adversarial perturbations to deliberately deviate the person detector's output from the ground-truth labels through gradient descent . The defender employs the same structure but different optimization strategy. It optimized the conditional hyperparameters to minimize the attack effectiveness of adversarial perturbations. During the attacker optimization phase, we freeze the conditional ISP hyperparameters, and similarly, during the defender optimization phase, we freeze the adversarial perturbations.

### Differentiable Camera ISP Simulation

The camera ISP is responsible for converting the raw measurements of camera sensors into high-quality RGB images suitable for further analysis and human perception. It consists a range of

Figure 2: **Overview of our adversarial optimization framework. The framework comprises two mutually adversarial parts: Attacker and Defender. The attacker optimizes adversarial perturbations to maximize attack effectiveness, while the defender optimizes the conditional input hyperparameter of the ISP proxy network to minimize attack effectiveness. The two parts cyclically alternate during the optimization stage.**

techniques and algorithms, such as demosaicing [7; 18], denoising [9; 37], white balancing [10; 12], to enhance acquired data, mitigate noise artifacts, and correct for optical aberrations.

Traditional ISPs are typically based on hand-crafted modules that are not differentiable . Therefore, they are not able to be incorporated into the adversarial pattern design. We propose a differentiable camera ISP proxy \(f_{}\) that can simulate arbitrary parameterized configurations, which is inspired by the literature of ISP optimization [25; 31]. Specifically, we trained a variant of the U-Net CNN architecture  using data \(\{I_{},I_{},\}\) obtained from traditional ISPs. Our network took the measurement \(I_{}\) as the input, hyperparameters \(\) of the camera ISP as the condition, and was trained by minimize the reconstruction error between its prediction and \(I_{}\). The training utilized 2,270 data pairs generated by an open-source undifferentiable camera ISP simulator  and the COCO dataset .

Since the hyperparameters of a camera ISP can vary from one implementation to another, and are often specific to the hardware and software used in a particular camera system , we opt for representative parameters that have a _significant_ impact on the final imaging and attack performance. We empirically select six parameters from the Color and Tone Correction module and the Denoising module in the camera ISP. To this end, we represent the camera ISP pipeline as a function \(f_{}\) parameterized by the conditional physical parameter \(=<a,b,,c,d,e>\). To enable the ISP proxy network to accommodate conditional input hyperparameters, we normalize the 6-dimensional hyperparameter to the  interval and concatenate them to the feature variables of the encoder.

### Adversarial Optimization

Our objective is twofold: (1) to optimize adversarial perturbations for optimal attack effectiveness against the target neural network and (2) to optimize the input hyperparameters of the ISP proxy network for optimal defense effectiveness. Since two optimizations engage in a zero-sum game, we follow the same training strategy in the GAN framework  to simultaneously optimize both parameter sets. The optimization algorithm is illustrated in Algorithm 1. In practice, we employ iterative updates to implement alternating training. The process alternates between \(k_{1}\) steps of optimizing the adversarial perturbation \(P\) and \(k_{2}\) steps of optimizing ISP conditional parameters (\(<a,b,,c,d,e>\)). We set \(k_{1}=k_{2}=20\). This strategy, validated through experiments, ensures the optimal optimization of both attack and defense, maintaining proximity to their peak values.

```
1:Given source image data \(X\), targeted person detector \(g\), and the trained camera ISP network \(f_{}\);
2:Initialize the adversarial patch \(P\) and input hyperparameters \(\) of \(f_{}\);
3:for\(t=1,2,,T\)do
4://Optimize the adversarial patch \(P\) to maximize attack effectiveness
5:forbatch \(b=1,2,,M\)do
6: Sample a batch of data \(X_{b}\) from \(X\);
7:\(X_{adv}(X_{b},P_{})\), \(P_{}=f_{}(P,)\);
8:\(X_{adv}\) are fed into the person detector \(g\) to obtain predictions and compute the loss \(L\);
9: Update the adversarial patch \(P\) via Eq. 1;
10:endfor
11://Optimize input hyperparameters \(\) to minimize the attack effectiveness
12:for batch \(b=1,2,,M\)do
13: Sample a batch of data \(x_{b}\) from \(X\);
14:\(X_{adv}(x_{b},P_{})\), \(P_{}=f_{}(P,)\);
15:\(X_{adv}\) are fed into the person detector \(g\) to obtain predictions and compute the loss \(L\);
16: Update the input hyperparameters \(\) via Eq. 2;
17:endfor
18:endfor ```

**Algorithm 1** The proposed adversarial optimization ( Attacker and Defender)

So, the goal can be described as follows:

\[P^{*}=_{i}L(g(I_{}^{i},f_{}(P;)), Y_{}),\] (1)

where we find optimal adversarial patches \(P\) by maximizing the discrepancy \(L\) between the predictions of the model \(g\) and the ground truth \(Y_{}\).

Additionally, we treat \(f_{}\) as a defense module, with the objective:

\[^{*}=_{i}L(g(I_{}^{i},f_{}(P;)),Y_{}),\] (2)where we find optimal conditional parameter \(\) by minimizing the discrepancy \(L\).

## 4 Experiments

### Experimental Setup

DatasetsWe use the INRIAPerson dataset [4; 30] to evaluate digital-space attacks. For physical-space attack, aiming to showcase the camera-agnostic nature of our approach, we collected data using six distinct hardware imaging devices, including two cameras -- Sony \(\)7R4 and Canon DS126231 -- and four mobile phone cameras -- iPhone15, RedmiK20, HuaweiP50, and SamsungS22.

Compared MethodsWe compare our proposed method with seven mainstream patch-based methods, including AdvPatch , AdvT-shirt , AdvCloak , NAP , LAP , TCGA , and T-SEA . For a fair comparison, we control the size of these patches to be the same, set at 0.2 times the height of the person.

MetricsWe evaluate attack effectiveness using two primary metrics: Average Precision (AP%) and Attack Success Rate (ASR%). AP assesses detection model accuracy, where lower values indicate superior attack performance. ASR is defined as \(1-^{}/\), where \(\) denotes the number of True Positive detections without attacks and \(^{}\) represents those with attacks; higher ASR values indicate better attack performance.

For digital-space evaluation, we utilize the INRIAPerson dataset, which consists of 613 training images with 3,019 person instances and 288 test images containing 855 person instances. The ASR in the digital space is therefore calculated based on these 855 person instances across 288 test images. In the physical-space evaluation, we conducted data collection using 6 cameras across 4 temporal sessions to minimize confounding factors. For each patch configuration, we captured 5 images per camera per session, yielding 120 images (6\(\)4\(\)5) per patch. With 6 distinct adversarial patches evaluated in the physical domain, our analysis encompasses a total of 720 images, forming the basis for physical-space ASR calculations.

Implementation DetailsOur implementation utilizes PyTorch on a Linux server equipped with dual NVIDIA GeForce RTX 3090 GPUs. The adversarial patches are configured with dimensions of 300\(\)300, and we employ a YOLOv5  model pre-trained on the COCO dataset  and subsequently fine-tuned on INRIAPerson  as our victim detector. The detector processes input images at a resolution of 640\(\)640, and adversarial training proceeds for 100 epochs.

Figure 3: **Illustration of digital-space attacks under different ISP settings.** The bounding boxes indicate the detector successfully detects the person instances, _i.e._, the attack fails. Due to space constraints, we only present one comparative method, T-SEA . For additional results, please refer to the Supplementary Material.

[MISSING_PAGE_FAIL:7]

(iPhone, Redmi, Huawei, and Samsung). In the capture scenario, one participant carried various adversarial patches, while another participant, serving as the control, did not carry any adversarial patches (see Figure 1 and Figure 6). To eliminate interference, we captured 20 images for each adversarial patch setting and calculated the ASR (%). For further demonstrations of physical-space attacks, please refer to the Supplementary Material. Here, we only evaluate two comparison methods, AdvPatch  and T-SEA , since they primarily target attack performance (as evident from Table 2), unlike other methods [11; 28] that also consider stealthiness.

Figure 3 presents the quantitative results of the effectiveness of the attack for different adversarial patches. We observe that random noise-based patches exhibit no attack effectiveness in real-world scenarios. The attack performance of AdvPatch exhibits significant fluctuations. It achieves an ASR of 35% on Canon cameras, while it is 0% on iPhone cameras. Unlike its impressive performance in the digital space, T-SEA shows poor attack performance in the physical space, mostly unable to execute successful attacks. This is due to the _multi-box detection_ issue. When computing ASR, we consider a sample as a failed attack if it is detected, even if the detection bounding box only covers half of the complete body. Our method achieves an ASR of more than 90% in all cameras, reaching 100% ASR on the iPhone and Huawei. These results demonstrate the excellent camera-agnostic attack performance of our method in physical space.

### Ablation Study

To demonstrate the effectiveness of each component in our attack method, we perform two variants of our method, _i.e._ ours w/o the camera ISP module and ours w/o adversarial optimization, as shown in Figure 6. Note that the latter refers to retaining the camera ISP module in our pipeline but refraining from optimizing its conditional input parameters. Instead, during the perturbation optimization process, we randomly adjust the input parameters.

From Figure 6, we observe that (1) "ours w/o camera ISP" exhibits the greatest fluctuation. This is evident from the confidence scores of the persons with the adversarial patches. It reaches as high as 0.92 on Redmi devices, while it drops below 0.25 on iPhone devices (indicating the disappearance of detection boxes). (2) The attack effectiveness of "ours w/o adversarial optimization" surpasses that of the former group. It succeeds in attacking both the Redmi and Samsung devices, with a noticeable reduction in the confidence fluctuations of the victim person. (3) Our full method achieves successful attacks across all six cameras. Additionally, Figure 5 presents the quantitative comparison of ASR across different cameras for three settings. Compared to the two variants, our method achieves a higher and more stable ASR. These results demonstrate that incorporating the camera ISP module solely into the adversarial perturbation pipeline offers limited improvement in attack performance, while our proposed adversarial optimization design enhances cross-camera attack capability and stability in the real world.

Figure 6: **Physical-space attacks across six different cameras. Our method removing the camera ISP module only achieves successful attacks on specific cameras. Our method removing the adversarial optimization slightly outperforms the former. In contrast, our full method achieves successful attacks across all six cameras.**

## 5 Discussions

DefenseWe compared three types of defense methods against CAP attacks: (1) modifying input images using JPEG compression , (2) adversarial patch detection and removal via SAC , and (3) adversarial training . To understand the effectiveness of existing defenses against our attack, we evaluated our method and its two variants under three defense strategies. CAP\({}^{*}\) refers to our method without the camera ISP module, CAP\({}^{}\) refers to our method without adversarial optimization, and CAP refers to our full method. In Table 3, we report the AP in the digital space and the ASR in the physical space for each case.

Overall, we observe that JPEG compression is ineffective against all three attack settings. This indicates that minor pixel-level modifications cannot defend against our CAP attacks. SAC demonstrates some defensive capability in digital space, slightly increasing the AP, but it is ineffective in physical space. In contrast, adversarial training effectively defends against CAP attacks with minimal loss in detector accuracy (within 1%). Furthermore, adversarial training shows defensive transferability in all three attack settings. One of the primary objectives of our study is to enhance the robustness of person detection models. The above results indicate that adversarial training is a reasonable and effective method to improve the robustness of detectors against CAP attacks.

LimitationsOur study mainly focuses on utilizing a camera ISP proxy network for camera simulation, handling the transition from physical to digital domains. Building a comprehensive, end-to-end differentiable camera simulator that includes features such as exposure time, aperture size, and ISO is challenging. Despite this, we believe that the conclusions and insights of this work are generalizable. This study successfully exposes previous methodological flaws and emphasizes the importance of considering the camera as a crucial module in the workflow of physical adversarial attacks.

Ethics StatementOur work successfully achieves physical adversarial attacks in person detection tasks. Given the effectiveness of our attack method across various imaging devices, its real-world application is feasible. This exposes potential security risks in existing DNNs-based applications, particularly when the technology is leveraged for malicious purposes. We advocate for the responsible and ethical use of technology. Furthermore, we offer comprehensive methodological descriptions and openly address the implications of our work, encouraging discourse within and beyond the scientific community to contribute to the advancement of trustworthy and dependable AI.

## 6 Conclusion

In this paper, we have proposed a cross-camera physical adversarial attack, CAP (Camera-Agnostic Patch) attack, against person detection. Unlike previous methods that overlooked the crucial role of the camera in the real-world attack workflow, our method incorporates a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap. Furthermore, leveraging the attenuating effect of camera ISP on attack performance, we construct an adversarial optimization framework. In this framework, the attack module optimizes adversarial perturbations, aiming to maximize attack effectiveness, while the defense module optimizes the input parameters conditionally, aiming to minimize attack effectiveness. The two modules alternate optimization, encouraging the generated adversarial patches to exhibit stability across camera attacks.

Table 3: **Defenses against CAP attacks. We report the AP in digital space and the ASR in physical space for three defense strategies: JPEG compression , SAC , and adversarial training .**Extensive experiments conducted in both digital and physical spaces demonstrate that our CAP attack enhances the effectiveness and reliability in real-world scenarios, encountering diverse camera configurations. In the future, we will continue to explore the role of cameras, design defense strategies based on imaging devices, and develop more robust detection models.