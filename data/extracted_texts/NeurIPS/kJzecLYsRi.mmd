# On the Saturation Effects of Spectral Algorithms

in Large Dimensions

 Weihao Lu

Department of Statistics and Data Science

Tsinghua University

Beijing, China 100084

luwh19@mails.tsinghua.edu.cn

&Haobo Zhang

Department of Statistics and Data Science

Tsinghua University

Beijing, China 100084

zhang-hb21@mails.tsinghua.edu.cn

&Yicheng Li

Department of Statistics and Data Science

Tsinghua University

Beijing, China 100084

liyc22@mails.tsinghua.edu.cn

&Qian Lin

Department of Statistics and Data Science

Tsinghua University

Beijing, China 100084

qianlin@tsinghua.edu.cn

Corresponding author.

###### Abstract

The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where \(n d^{}\). More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification \(\)'s. In particular, we find that these exact rate curves (varying along \(\)) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e., the saturation effect occurs in large dimensional setting as long as the source condition \(s>\) while it occurs in fixed dimensional setting as long as \(s>2\)).

## 1 Introduction

Let's assume we have \(n\) i.i.d. samples \((x_{i},y_{i})\) from a joint distribution supported on \(^{d}\). The regression problem, one of the most fundamental problems in statistics, aims to find a function \(\) based on these samples such that the _excess risk_, \(\|-f_{}\|_{L^{2}}^{2}=_{x}[(f_{}(x)-(x))^{ 2}]\), is small, where \(f_{}(x)=[Y|x]\) is the _regression function_. Many non-parametric regression methods are proposed to solve the regression problem by assuming that \(f_{}\) falls into certain function classes, including polynomial splines Stone (1994), local polynomials Cleveland (1979); Stone (1977), the spectral algorithms Caponnetto (2006); Caponnetto and De Vito (2007); Caponnetto and Yao (2010), etc.

Spectral algorithms, as a classical topic, have been studied since the 1990s. Early works treated certain types of spectral algorithms in their theoretical analysis (Caponnetto (2006); Caponnetto and De Vito (2007); Raskutti et al. (2014); Lin et al. (2020)). These works often consider \(d\) as a fixed constant and impose the polynomial eigenvalue decay assumption under a kernel (i.e., there exist constants \(0<<\), such that the eigenvalues of the kernel satisfy \(j^{-}_{j}j^{-}\), \(j 1\) for certain \(>1\) depending on the fixed \(d\)). They further assume that \(f_{}\) belongs to the reproducing kernel Hilbert space (RKHS) \(\) associated with the kernel. Under the above assumptions, they then showed that the minimax rate of the excess risk of regression over the corresponding RKHS is lower bounded by \(n^{-/(+1)}\) and that some (regularized) spectral algorithms, e.g., the kernel ridge regression (KRR) and the kernel gradient flow, can produce estimators achieving this minimax optimal rate.

However, subsequent studies have revealed that when higher regularity (or smoothness) of \(f_{}\) is assumed, KRR fails to achieve the information-theoretical lower bound on the excess risk, while kernel gradient flow can do so. Specifically, let's assume that \(f_{}\) belongs to the _interpolation space_\([]^{s}\) of the RKHS \(\) with \(s>0\) (see, e.g., Steinwart et al. (2009); Dieuleveut et al. (2017); Dicker et al. (2017); Pillaud-Vivien et al. (2018); Lin et al. (2020); Fischer and Steinwart (2020); Celisse and Wahl (2021)). It is then shown that the information-theoretical lower bound on the excess risk is \(n^{-s/(s+1)}\). When \(0<s 2\), Caponnetto and De Vito (2007); Yao et al. (2007); Lin et al. (2020); Zhang et al. (2023) have already shown that the upper bound of the excess risks of both KRR and the kernel gradient flow is \(n^{-s/(s+1)}\), and hence they are minimax optimal. On the contrary, when \(s>2\), Yao et al. (2007); Lin et al. (2020) showed that the upper bound of the excess risks of kernel gradient flow is \(n^{-s/(s+1)}\) while the best upper bound of the excess risks of KRR is \(n^{-2/(2+1)}\) (Caponnetto and De Vito (2007)). Bauer et al. (2007); Gerfo et al. (2008); Dicker et al. (2017) conjectured that the convergence rate of KRR is bounded below by \(n^{-2/(2+1)}\) and Li et al. (2022) rigorously proved it. The above phenomenon is often referred to as the _saturation effect_ of KRR:

_KRR is inferior to certain spectral algorithms, such as kernel gradient flow, when \(s>2\)._

In recent years, neural network methods have gained tremendous success in many large-dimensional problems, such as computer vision He et al. (2016); Krizhevsky et al. (2017) and natural language processing Devlin (2018). Several groups of researchers tried to explain the superior performance of neural networks on large-dimensional data from the aspects of "lazy regime" (Arora et al. (2019); Du et al. (2019, 2018); Li and Liang (2018)). They noticed that, when the width of a neural network is sufficiently large, its parameters/weights stay in a small neighborhood of their initial position during the training process. Later, Jacot et al. (2018); Arora et al. (2019); Hu et al. (2021); Suh et al. (2021); Lai et al. (2023); Li et al. (2024) proved that the time-varying neural network kernel (NNK) converges (uniformly) to a time-invariant neural tangent kernel (NTK) as the width of the neural network goes to infinity, and thus the excess risk of kernel gradient flow with NTK converges (uniformly) to the excess risk of neural networks in the 'lazy regime'.

Inspired by the concepts of the "lazy regime" and the uniform convergence of excess risk, the machine learning community has experienced a renewed surge of interest in large-dimensional spectral algorithms. The earliest works focused on the consistency of two specific types of spectral algorithms: KRR and kernel interpolation (Liang and Rakhlin (2020); Liang et al. (2020); Ghorbani et al. (2020, 2021); Mei et al. (2021, 2022); Misiakiewicz and Mei (2022); Aerni et al. (2023); Barzilai and Shamir (2023)). In comparison, results on large-dimensional kernel gradient flow were somewhat scarce, and these results largely mirrored those associated with KRR (e.g., Ghosh et al. (2021)). Recently, Lu et al. (2023) proved that large-dimensional kernel gradient flow is minimax optimal when \(s=1\). Then, Zhang et al. (2024) provided upper and lower bounds on the convergence rate on the excess risk of KRR for any \(s>0\). Surprisingly, they discovered that for \(s>1\), the convergence rate of KRR did not match the lower bound on the minimax rate. Unfortunately, they didn't prove that certain spectral algorithms can reach the lower bound on the minimax rate they provided, and hence they didn't rigorously prove that the saturation effect of KRR occurs in large dimensions. Instead, Zhang et al. (2024) only conjectured that certain spectral algorithms (e.g., kernel gradient flow) can provide minimax optimal estimators after their main results.

If Zhang et al. (2024)'s conjecture is true, then we can safely conclude that: when the regression function \(f_{}\) is smooth enough, KRR is inferior to kernel gradient flow in large dimensions as well. Consequently, previous results on large-dimensional KRR may not be directly extendable to large -dimensional neural networks, even if the neural networks are in the 'lazy regime'. The main focus of this paper is to prove this conjecture by showing that kernel gradient flow is minimax optimal in large dimensions.

### Related work

Saturation effects of fixed-dimensional spectral algorithms.When the dimension \(d\) of the data is fixed, the saturation effect of KRR has been conjectured for decades and is rigorously proved in the recent work Li et al. (2022). Suppose \(f_{}[]^{s}\) with \(s>2\). It is shown that: (i) the minimax optimal rate is \(n^{-s/(s+1)}\)(Rastogi and Sampath (2017); Yao et al. (2007); Lin et al. (2020)); and (ii) the convergence rate on the excess risk of KRR is \(n^{-2/(2+1)}\)(Li et al. (2022)). More recently, Li et al. (2024) determined the exact generalization error curves of a class of analytic spectral algorithms, which allowed them to further show the saturation effect of spectral algorithms with finite qualification \(\) (see, e.g., Appendix C): suppose \(f_{}[]^{s}\) with \(s>2\), then the convergence rate on the excess risk of the above spectral algorithms is \(n^{-2/(2+1)}\).

New phenomena in large-dimensional spectral algorithms.In the large-dimensional setting where \(n d^{}\) with \(>0\), new phenomena exhibited in spectral algorithms are popular topics in recent machine-learning research. A line of work focused on the polynomial approximation barrier phenomenon (e.g., Ghorbani et al. (2021); Donhauser et al. (2021); Mei et al. (2022); Xiao et al. (2023); Misiakiewicz (2022); Hu and Lu (2022)). They found that, for the square-integrable regression function, KRR and kernel gradient flow are consistent if and only if the regression function is a polynomial with a low degree. Another line of work considered the benign overfitting of kernel interpolation (i.e., kernel interpolation can generalize) (e.g., Liang and Rakhlin (2020); Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023); Zhang et al. (2024)). Moreover, two recent work (Lu et al. (2023); Zhang et al. (2024)) discussed two new phenomena exhibited in large-dimensional KRR and kernel gradient flow: the multiple descent behavior and the periodic plateau behavior. The multiple descent behavior refers to the phenomenon that the curve of the convergence rate ( with respect to \(n\) ) of the optimal excess risk is non-monotone and has several isolated peaks and valleys; while the periodic plateau behavior refers to the phenomenon that the curve of the convergence rate ( with respect to \(d\) ) of the optimal excess risk has constant values when \(\) is within certain intervals. Finally, Zhang et al. (2024) conjectured that the saturation effect of KRR occurs in large dimensions. The above works imply that these phenomena occur in many spectral algorithms in large dimensions, hence encouraging us to provide a unified explanation of these new phenomena.

### Our contributions

In this paper, we focus on the large-dimensional spectral algorithms with inner product kernels, and we assume that the regression function falls into an interpolation space \([]^{s}\) with \(s>0\). We state our main results as follows:

**Theorem 1.1** (Restate Theorem 4.1 and 4.2, non-rigorous).: _Let \(s>0\), \( 1\), and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Then under certain conditions, the excess risk of large-dimensional spectral algorithm with qualification \(\) satisfies_

\[(\|_{^{*}}-f_{}\|_{L^{2}}^{2} \ |\ X)=\{_{}(d^{- \{-p,s(p+1)\}})((d)), s \\ _{}(d^{-\{-p,}{ +1},(p+1)\}})((d)), s>,.\]

_where \(=\{s,2\}\)._

More specifically, we list the main contributions of this paper as follows:

1. In Theorem 3.1, we show that the convergence rate on the excess risk of (optimally-tuned) kernel gradient flow in large dimensions is \(_{}(d^{-\{-p,s(p+1)\}})((d))\), which matches the lower bound on the minimax rate given in Theorem 3.3 (up to a logarithmic factor). We find that kernel gradient flow is minimax optimal for any \(s>0\) and any \(>0\), and KRR is not minimax optimal for \(s>1\) and for certain ranges of \(\) (We provide a visual illustration in Figure 2). Consequently, we rigorously prove that the saturation effect of KRR occurs in large dimensions.

2. In Theorem 3.3, we enhanced the previous minimax lower bound results given in Lu et al. (2023) and Zhang et al. (2024). Specifically, we show that the minimax lower bound is \((d^{-\{-p,s(p+1)\}})/((d))\). In comparison, the previous minimax lower bound is \((d^{-\{-p,s(p+1)\}})/d^{c}\) for any \(>0\), and the additional term \(d^{c}\) changes the desired convergence rate.
3. In Section 4, we determine the convergence rate on the excess risk of large-dimensional spectral algorithms. From our results, we find several new phenomena exhibited in spectral algorithms in large-dimensional settings. We provide a visual illustration of the above phenomena in Figure 1: i) The first phenomenon is the polynomial approximation barrier, and as shown in Figure 1(a), when \(s\) is close to zero, the curve of the convergence rate of spectral algorithm drops when \( p\) for any integer \(p\) and will stay invariant for most of the other \(\); ii) The second one is the periodic plateau behavior, and as shown in Figure 1(b) and Figure 1(c), when \(0<s<2\) and \([p(s+1)+s+(\{s,\}-)/,(p+1)(s+1))\) for an integer \(p 0\), the convergence rate does not change when \(\) varies; iii) The final one is the saturation effect, and as shown in Figure 1(c) and Figure 1(d), when \(s>\), the convergence rate of spectral algorithm can not achieve the minimax lower bound for certain ranges of \(\). A detailed discussion about the above three phenomena can be found in Section 4.

## 2 Preliminaries

Suppose that we have observed \(n\) i.i.d. samples \((x_{i},y_{i}),i[n]\) from the model:

\[y=f_{}(x)+,\] (1)

where \(x_{i}\)'s are sampled from \(_{}\), \(_{}\) is the marginal distribution on \(^{d+1}\), \(y\), \(f_{}\) is some function defined on a compact set \(\), and

\[_{(x,y)}[^{2}\ \ x] ^{2},_{}\ x,\]

for some fixed constant \(>0\), where \(\) is the joint distribution of \((x,y)\) on \(\). Denote the \(n 1\) data vector of \(y_{i}\)'s and the \(n d\) data matrix of \(x_{i}\)'s by \(Y\) and \(X\) respectively.

### Kernel ridge regression and kernel gradient flow

In this subsection, we introduce two specific spectral algorithms, kernel ridge regression and kernel gradient flow, which produce estimators of the regression function \(f_{}\). A further discussion on general spectral algorithms will be provided in Section 4.

Throughout the paper, we denote \(\) as a separable RKHS on \(\) with respect to a continuous and positive definite kernel function \(K(,):\) and there exists a constant \(\) satisfying

\[_{x}K(x,x)^{2}.\]

Figure 1: Convergence rates of spectral algorithm with qualification \(=2\) in Theorem 4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension \(d\). We present four graphs corresponding to four kinds of source conditions: \(s=0.01,1,3,5\). The x-axis represents asymptotic scaling, \(:n d^{}\); the y-axis represents the convergence rate of excess risk, \(r:\) Excess risk \( d^{r}\).

Kernel ridge regressionKernel ridge regression (KRR) constructs an estimator \(^{}}_{}\) by solving the penalized least square problem

\[^{}}_{}=*{arg\,min}_{f} (_{i=1}^{n}(y_{i}-f(x_{i}))^{2}+ \|f\|_{}^{2}),\]

where \(>0\) is referred to as the regularization parameter. The representer theorem (see, e.g., Steinwart and Christmann (2008)) gives an explicit expression of the KRR estimator, i.e.,

\[^{}}_{}(x)=K(x,X)(K(X,X)+n)^{- 1}Y.\] (2)

Kernel gradient flowThe gradient flow of the loss function \(=_{i}(y_{i}-f(x_{i}))^{2}\) induced a gradient flow in \(\) which is given by

\[}{t}^{}}_{t}(x)=-K(x,X )(^{}}_{t}(X)-Y).\] (3)

If we further assume that \(^{}}_{0}(x)=0\), then we can also give an explicit expression of the kernel gradient flow estimator

\[^{}}_{t}(x)=K(x,X)K(X,X)^{-1}(-e^{-K(X,X)t})Y.\] (4)

### The interpolation space

Define the integral operator \(T_{K}\) as \(T_{K}(f)(x)= K(x,x^{})f(x^{})\ _{}(x^{})\). It is well known that \(T_{K}\) is a positive, self-adjoint, trace-class, and hence a compact operator (Steinwart and Scovel (2012)). The celebrated Mercer's theorem further assures that

\[K(x,x^{})=_{j}_{j}_{j}(x)_{j}(x^{}),\] (5)

where the eigenvalues \(\{_{j},j=1,2,...\}\) is a non-increasing sequence, and the corresponding eigenfunctions \(\{_{j}(),j=1,2,...\}\) are orthonormal in \(L^{2}(,_{})\) function space.

The interpolation space \([]^{s}\) with source condition \(s\) is defined as

\[[]^{s}:=_{j}a_{j}_{j}^{s/2}_{j}: (a_{j})_{j}_{2}} L^{2}(,_{ }),\] (6)

with the inner product deduced from

\[_{j=1}^{}a_{j}_{j}^{s/2}_{j}_{[]^{s}}=_{j=1}^{}a_{j}^{2}^{1/2}.\] (7)

It is easy to show that \([]^{s}\) is also a separable Hilbert space with orthonormal basis \(\{_{j}^{s/2}_{j}\}_{j}\). Generally speaking, functions in \([]^{s}\) become smoother as \(s\) increases (see, e.g., the example of Sobolev RKHS in Edmunds and Triebel (1996); Zhang et al. (2023).

### Assumptions

In this subsection, we list the assumptions that we need for our main results.

To avoid potential confusion, we specify the following large-dimensional scenario for kernel regression where we perform our analysis: suppose that there exist three positive constants \(c_{1}\), \(c_{2}\) and \(\), such that

\[c_{1}d^{} n c_{2}d^{},\] (8)

and we often assume that \(d\) is sufficiently large.

In this paper, we only consider the inner product kernels defined on the sphere. An inner product kernel is a kernel function \(K\) defined on \(^{d}\) such that there exists a function \(:[-1,1]\) independent of \(d\) satisfying that for any \(x,x^{}^{d}\), we have \(K(x,x^{})=( x,x^{})\). If we further assume that the marginal distribution \(_{}\) is the uniform distribution on \(=^{d}\), then the Mercer's decomposition for \(K\) can be rewritten as

\[K(x,x^{})=_{k=0}^{}_{k}_{j=1}^{N(d,k)}Y_{k,j}(x)Y_{k,j} (x^{}),\] (9)

where \(Y_{k,j}\) for \(j=1,,N(d,k)\) are spherical harmonic polynomials of degree \(k\) and \(_{k}\)'s are the eigenvalues of \(K\) with multiplicity \(N(d,0)=1\); \(N(d,k)=,k=1,2,\). For more details of the inner product kernels, readers can refer to Gallier (2009).

_Remark 2.1_.: We consider the inner product kernels on the sphere mainly because the harmonic analysis is clear on the sphere ( e.g., properties of spherical harmonic polynomials are more concise than the orthogonal series on general domains). This makes Mercer's decomposition of the inner product more explicit rather than several abstract assumptions ( e.g., Mei and Montanari (2022)). We also notice that very few results are available for Mercer's decomposition of a kernel defined on the general domain, especially when the dimension of the domain is taking into consideration. e.g., even the eigen-decay rate of the neural tangent kernels is only determined for the spheres. Restricted by this technical reason, most works analyzing the spectral algorithm in large-dimensional settings focus on the inner product kernels on spheres (Liang et al., 2020; Ghorbani et al., 2021; Misiakiewicz, 2022; Xiao et al., 2023; Lu et al., 2023, etc.). Though there might be several works that tried to relax the spherical assumption (e.g., Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023), we can find that most of them (i) adopted a near-spherical assumption; (ii) adopted strong assumptions on the regression function, e.g., \(f_{}(x)=xx x[L]\) for an integer \(L>0\), where \(x[i]\) denotes the \(i\)-th component of \(x\); or (iii) can not determine the convergence rate on the excess risk of the spectral algorithm.

To avoid unnecessary notation, let us make the following assumption on the inner product kernel \(K\).

_Assumption 1_.: \((t)^{}([-1,1])\) is a fixed function independent of \(d\) and there exists a non-negative sequence of absolute constants \(\{a_{j} 0\}_{j 0}\), such that we have

\[(t)=_{j=0}^{}a_{j}t^{j},\]

where \(a_{j}>0\) for any \(j+3\).

The purpose of Assumption 1 is to keep the main results and proofs clean. Notice that, by Theorem 1.b in Gneiting (2013), the inner product kernel \(K\) on the sphere is semi-positive definite for all dimensions if and only if all coefficients \(\{a_{j},j=0,1,2,...\}\) are non-negative. One can easily extend our results in this paper when certain coefficients \(a_{k}\)'s are zero (e.g., one can consider the two-layer NTK defined as in Section 5 of Lu et al. (2023), with \(a_{i}=0\) for any \(i=3,5,7,\)).

In the next assumption, we formally introduce the source condition, which characterizes the relative smoothness of \(f_{}\) with respect to \(\).

_Assumption 2_ (Source condition).: Suppose that \(f_{}(x)=_{i=1}^{}f_{i}_{i}(x)\).

* \(f_{}[]^{s}\) for some \(s>0\), and there exists a constant \(R_{}\) only depending on \(\), such that \[\|f_{}\|_{[]^{s}} R_{}.\] (10)
* Denote \(q\) as the smallest integer such that \(q>\) and \(_{q} 0\). Define \(_{d,k}\) as the index set satisfying \(_{i}_{k},i_{d,k}\). Further suppose that there exists an absolute constant \(c_{0}>0\) such that for any \(d\) and \(k\{0,1,,q\}\) with \(_{k} 0\), we have \[_{i_{d,k}}_{k}^{-s}f_{i}^{2} c_{0}.\] (11)

Assumption 2 is a common assumption when one is interested in the tight bounds on the excess risk of spectral algorithms (e.g., Caponnetto and De Vito (2007); Fischer and Steinwart (2020), Eq.(8) in Cui et al. (2021), Assumption 3 in Li et al. (2024), and Assumption 5 in Zhang et al. (2024)). Assumption 2 implies that the regression function exactly falls into the interpolation space \([]^{s}\), that is, \(f_{}[]^{s}\) and \(f_{}[]^{t}\) for any \(t>s\). For example, from the proof part I of Lemma D.14, one can check that \(f_{}\) with \(_{i_{d,p}}_{p}^{-s}f_{i}^{2}=_{i_{d,p+1} }_{p+1}^{-s}f_{i}^{2}=0\) can have a faster convergence rate on the excess risk.

_Notations._ Let's denote the norm in \(L_{2}(,_{X})\) as \(\|\|_{L_{2}}\). For a vector \(x\), we use \(x[i]\) to denote its \(i\)-th component. We use asymptotic notations \(O(),\ o(),\ ()\) and \(()\). For instance, we say two (deterministic) quantities \(U(d),V(d)\) satisfy \(U(d)=o(V(d))\) if and only if for any \(>0\), there exists a constant \(D_{}\) that only depends on \(\) and the absolute positive constants \(,,s,,c_{0},c_{1},c_{2},_{1},,_{8}>0\), such that for any \(d>D_{}\), we have \(U(d)< V(d)\). We also write \(a_{n}=(b_{n})\) if there exist a constant \( 0\), such that \(a_{n}=(b_{n}^{})\). We use the probability versions of the asymptotic notations such as \(O_{}(),o_{}(),_{ }()\). For instance, we say the random variables \(X_{n},Y_{n}\) satisfying \(X_{n}=O_{}(Y_{n})\) if and only if for any \(>0\), there exist constants \(C_{}\) and \(N_{}\) such that \(P(|X_{n}| C_{}|Y_{n}|), n>N_ {}\).

### Review of the previous results

The following two results are restatements of Theorem 2 and Theorem 5 in Zhang et al. (2024).

**Proposition 2.2**.: _Let \(s 1\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Suppose that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Let \(_{}^{}\) be the function defined in (2). Define \(=\{s,2\}\), then there exists \(^{}>0\), such that we have_

\[(\|_{^{}}^{}-f_{} \|_{L^{2}}^{2}\ X)=_{}(d^{-\{-p,,(p+1)\}})((d) ),\]

_where \(_{}\) only involves constants depending on \(s,,,c_{0},,c_{1}\) and \(c_{2}\). In addition, the convergence rates of the generalization error can not be faster than above for any choice of regularization parameter \(=(d,n) 0\)._

**Proposition 2.3** (Lower bound on the minimax rate).: _Let \(s>0\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Let \(\) consist of all the distributions \(\) on \(\) such that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Then for any \(>0\), we have:_

\[_{f}_{}_{(X,Y)^{ n}}\| -f_{}\|_{L^{2}}^{2}=(d^{-\{-p,s(p+1)\} } d^{-}),\]

_where \(\) only involves constants depending on \(s,,,c_{0},,c_{1},c_{2}\) and \(\)._

From the above two propositions, we can find that when \(s>1\), the convergence rate on the excess risk of KRR does not always match the lower bound on the minimax optimal rate. Zhang et al. (2024) further conjectured that the lower bound on the minimax optimal rate provided in Proposition 2.3 is tight (ignoring the additional term \(d^{-}\)). Hence, they believed that the saturation effect exists for large-dimensional KRR.

## 3 Main results

In this section, we determine the convergence rate on the excess risk of kernel gradient flow as \(d^{-\{-p,s(p+1)\}}((d))\), which differs from the lower bound on the minimax rate provided in Proposition 2.3 by \(d^{}\) for any \(>0\). We then tighten the lower bound on the minimax rate to \(d^{-\{-p,s(p+1)\}}((d))\). Based on the above results, we find that KRR is not minimax optimal for \(s>1\) and for certain ranges of \(\). Therefore, we show that the saturation effect of KRR occurs in large dimensions.

### Exact convergence rate on the excess risk of kernel gradient flow

We first state our main results in this paper.

**Theorem 3.1** (Kernel gradient flow).: _Let \(s>0\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Suppose that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Let \(_{t}^{}\) be the function defined in (4). Then there exists \(t^{}>0\), such that we have_

\[(\|_{t^{}}^{}-f_{}\|_ {L^{2}}^{2}\ X)=_{}(d^{-\{-p,s(p+1)\}}) ((d)),\] (12)

_where \(_{}\) only involves constants depending on \(s,,,c_{0},,c_{1}\) and \(c_{2}\)._Theorem 3.1 is a direct corollary of Theorem 4.1 and Example 2. Combining with the previous results in Proposition 2.3, or our modified minimax rate given in Theorem 3.3, we can conclude that large-dimensional kernel gradient flow is minimax optimal for any \(s>0\) and any \(>0\). More importantly, the convergence rate of kernel gradient flow is faster than that of KRR given in Proposition 2.2 when (i) \(1<s 2\) and \((p(s+1)+1,p(s+1)+2s-1)\) for some \(p\), or (ii) \(s>2\) and \((p(s+1)+1,(p+1)(s+1))\) for some \(p\). Therefore, we have proved the saturation effect of KRR in large dimensions.

_Remark 3.2_.: When \(p 1\), the logarithm term \(((d))\) in (12) can be removed. When \(p=0\), we have \(((d))=((d))^{2}\) in (12). See Appendix D.4 for details.

### Improved minimax lower bound

Recall that Proposition 2.3 gave a lower bound on the minimax rate as \(d^{-\{-p,s(p+1)\}} d^{-}\). The following theorem replaces the additional term \(d^{-}\) (which has changed the convergence rate) into a logarithm term \(^{-1}((d))\) (which does not change the desired convergence rate).

**Theorem 3.3** (Improved minimax lower bound).: _Let \(s>0\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Let \(\) consist of all the distributions \(\) on \(\) such that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Then we have:_

\[_{f}_{}_{(X,Y)^{ n}}\| -f_{}\|_{L^{2}}^{2}=(d^{-\{-p,s(p+1)\} })((d)),\] (13)

_where \(\) only involves constants depending on \(s,,,c_{0},,c_{1}\), and \(c_{2}\)._

## 4 Exact convergence rate on the excess risk of spectral algorithms

In this section, we will give tight bounds on the excess risks of certain types of spectral algorithms, such as kernel ridge regression, iterated ridge regression, kernel gradient flow, and kernel gradient descent.

Given an analytic filter function \(_{}()\) with qualification \( 1\) (refer to Appendix C for the definitions of analytic filter function and its qualification), we can define a spectral algorithm in the following way (see, e.g., Bauer et al. (2007)). For any \(y\), let \(K_{x}:\) be given by \(K_{x}(y)=y K(x,)\), whose adjoint \(K_{x}^{*}:\) is given by \(K_{x}^{*}(f)= K(x,),f_{}=f(x)\). Moreover, we denote by \(T_{x}=K_{x}K_{x}^{*}\) and \(T_{X}=_{i=1}^{n}T_{x_{i}}\). We also define the sample basis function

\[_{Z}=_{i=1}^{n}K_{x_{i}}(y_{i})= _{i=1}^{n}y_{i} K(x_{i},).\] (14)

Now, the estimator of the spectral algorithm is defined by

\[_{}=_{}(T_{X})_{Z}.\] (15)

Many commonly used spectral algorithms can be constructed by certain analytic filter functions. We provide two examples (kernel ridge regression and kernel gradient flow) as follows, and put two more examples (iterated ridge regression and kernel gradient descent) in Appendix C. We provide rigorous proof for these examples in Lemma C.3.

**Example 1** (Kernel ridge regression).: _The filter function of kernel ridge regression (KRR) is well-known to be_

\[_{}^{}(z)=,_{}^{ }(z)=,=1.\] (16)

**Example 2** (Kernel gradient flow).: _The filter function is_

\[_{}^{}(z)=}{z},_{}^{ }(z)=e^{-tz}, t=^{-1},=.\] (17)

For any analytic filter function \(_{}\) with qualification \( 1\) and the corresponding estimator of the spectral algorithm defined in (15), the following two theorems provide exact convergence rates on the excess risk when (i) the regression function is less-smooth, i.e., we have \(s\), and (ii) \(s>\), where \(s\) is the source condition coefficient of the regression function given in Assumption 2.

**Theorem 4.1**.: _Let \(0<s\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Suppose that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Let \(_{}(z)\) be an analytic filter function and \(_{}\) be the function defined in (15). Suppose one of the following conditions holds:_

_(i) \(=,s>1/(2),>((2+1)s)/(2(1+s));\)_

_then there exists \(^{}>0\), such that we have_

\[(\|_{^{}}-f_{}\|_{L^{2}}^{2 }\ \ X)=_{}(d^{-\{-p,s(p+1)\}}) ((d)),\]

_where \(_{}\) only involves constants depending on \(s,,,c_{0},,c_{1}\) and \(c_{2}\)._

**Theorem 4.2**.: _Let \(s>\) and \(>0\) be fixed real numbers. Denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Suppose that Assumption 1 and Assumption 2 hold for \(s\) and \(\). Let \(_{}(z)\) be an analytic filter function and \(_{}\) be the function defined in (15). Define \(=\{s,2\}\), then there exists \(^{}>0\), such that we have_

\[(\|_{^{}}-f_{}\|_{L^{2}}^{2 }\ \ X)=_{}(d^{-\{-p,}}{+1},(p+1)\}})((d)),\]

_where \(_{}\) only involves constants depending on \(s,,,c_{0},,c_{1}\) and \(c_{2}\). In addition, the convergence rates of the generalization error can not be faster than above for any choice of regularization parameter \(=(d,n) 0\)._

_Remark 4.3_.: These theorems substantially generalize the results on exact generalization error bounds of analytic spectral algorithms under the fixed-dimensional setting given in Li et al. (2024). Although the "analytic functional argument" introduced in their proof is still vital for us to deal with the general spectral algorithms, their proof has to rely on the polynomial eigendecay assumption that \(_{j} j^{-}\) (Assumption 1), which does not hold in large dimensions since the hidden constant factors in the assumption vary with \(d1\)1 (Lu et al. (2023)). Hence, their proof is not easy to generalize to large-dimensional spectral algorithms.

We provide some graphical illustrations of Theorem 4.1 and Theorem 4.2 in Figure 1 (with \(=2\)) and in Appendix A (with \(=1\), \(=2\), \(=4\), and \(=\), corresponding to KRR, iterated ridge regression in Example 3 and kernel gradient flow).

As a direct consequence of Theorem 3.3, Theorem 4.1, and Theorem 4.2, we find that for the spectral algorithm with estimator defined in (15), it is minimax optimal if \(s\) and the conditions in Theorem 4.1 hold. Moreover, these results show several phenomena for large-dimensional spectral algorithms.

Saturation effect of large-dimensional spectral algorithms with finite qualification.In the large-dimensional setting and for the inner product kernel on the sphere, our results show that the saturation effect of spectral algorithms occurs when \(s>\). As shown in Figure 1(c) and Figure 1(d), when \(s>\), no matter how carefully one tunes the regularization parameter \(\), the convergence rate can not be faster than \(d^{-\{-p,}}{+1},(p+1)\}}\), thus can not achieve the minimax lower bound \(d^{-\{-p,s(p+1)\}}\).

Periodic plateau behavior of spectral algorithms when \(s 2\).When \(0<s 2\) and \([p(s+1)+s+\{s,\}/-1,(p+1)(s+1))\) for an integer \(p 0\), from Theorem 4.1 and Theorem 4.2, the convergence rate on the excess risk of spectral algorithm \(d^{-s(p+1)}\). The above rate does not change when \(\) varies, which can also be found in Figure 1(b) and Figure 1(c). Bln other words, if we fix a large dimension \(d\) and increase \(\) (or equivalently, increase the sample size \(n\)), the optimal rate of excess risk of a spectral algorithm stays invariant in certain ranges. Therefore, in order to improve the rate of excess risk, one has to increase the sample size above a certain threshold.

Polynomial approximation barrier of spectral algorithms when \(s 0\).From Theorem 4.1, when \(s\) is close to zero, the convergence rate \(d^{-\{-p,s(p+1)\}}\) is unchanged in the range \([p(s+1)+s,(p+1)(s+1))\), and increases in the short range \([p(s+1),p(s+1)+s)\). In other words, the excess risk of spectral algorithms will drop when \(\) exceeds \(p(s+1) p\) for any integer \(p\) and will stay invariant for most of the other \(\). We term the above phenomenon as the polynomial approximation barrier of spectral algorithms (borrowed from Ghorbani et al. (2021)), and it can be illustrated by Figure 1(a) with \(s=0.01\).

_Remark 4.4_.: Ghorbani et al. (2021) discovered the polynomial approximation barrier of KRR. As shown by Figure 5 and Theorem 4 in Ghorbani et al. (2021), if \(s=0\) and the true function falls into \(L^{2}=[H]^{0}\), then with high probability we have

\[\|_{_{}}^{}-f_{ }_{L^{2}}^{2}-\|_{>p}f_{}\|_{L^{2}}^{ 2}|f_{}_{L^{2}}^{2}+^{2 },\] (18)

where \(p\) is the integer satisfying \([p,p+1)\), \(_{}\) is defined as in Theorem 4 in Ghorbani et al. (2021), \(_{>}\) means the projection onto polynomials with degree \(>\), and \(\) is any positive real number. Notice that (18) implies that the excess risk of KRR will drop when \(\) exceeds any integer and will stay invariant for other \(\), and is consistent with our results for spectral algorithms.

## 5 Conclusion

In this paper, we rigorously prove the saturation effect of KRR in large dimensions. Let \(s>0\) and \(>0\) be fixed real numbers, denote \(p\) as the integer satisfying \([p(s+1),(p+1)(s+1))\). Given that the kernel is an inner product kernel defined on the sphere and that \(f_{}\) falls into the interpolation space \([]^{s}\), we first show that the convergence rate on the excess risk of large-dimensional kernel gradient flow is \(_{}(d^{-\{-p,s(p+1)\}})\) - poly \(((d))\) (Theorem 3.1), which is faster than that of KRR given in Zhang et al. (2024). We then determine the improved minimax lower bound as \((d^{-\{-p,s(p+1)\}})/((d))\) (Theorem 3.3). Combining these results, we know that kernel gradient flow is minimax optimal in large dimensions, and KRR is inferior to kernel gradient flow in large dimensions. Our results suggest that previous results on large-dimensional KRR may not be directly extendable to large-dimensional neural networks if the regression function is over-smooth.

In Section 4, we generalize our results to certain spectral algorithms. We determine the convergence rate on the excess risk of large-dimensional spectral algorithms (Theorem 4.1 and Theorem 4.2). From these results, we find several new phenomena exhibited in large-dimensional spectral algorithms, including the saturation effect, the periodic plateau behavior, and the polynomial approximation barrier.

In this paper, we only consider the convergence rate on the excess risk of optimal-tuned large-dimensional spectral algorithms with uniform input distribution on a hypersphere. We believe that several results in fixed-dimensional settings with input distribution on more general domains (e.g., Haas et al. (2024); Li et al. (2024)) can indeed be extended to large-dimensional settings, although we must carefully consider the constants that depend on \(d\). Furthermore, we believe that by considering the learning curve of large-dimensional spectral algorithms (i.e., the convergence rate on the excess risk of spectral algorithms with any regularization parameter \(>0\)) or the convergence rate on the excess risk of large-dimensional kernel interpolation (i.e., KRR with \(=0\)), further research can find a wealth of new phenomena compared with the fixed-dimensional setting.