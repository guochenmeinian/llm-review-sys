# _DeepStack_: Deeply Stacking Visual Tokens

is Surprisingly Simple and Effective for LMMs

 Lingchen Meng\({}^{1,2}\)1 Jianwei Yang\({}^{3}\)1 Rui Tian\({}^{1,2}\) Xiyang Dai\({}^{3}\)

Zuxuan Wu\({}^{1,2}\)2 Jianfeng Gao\({}^{3}\)2 Yu-Gang Jiang\({}^{1,2}\)2

\({}^{1}\)Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University

\({}^{2}\)Shanghai Collaborative Innovation Center of Intelligent Visual Computing

\({}^{3}\)Microsoft Corporation

[https://deepstack-vl.github.io/](https://deepstack-vl.github.io/)

Equal contributions; \({}^{}\) Corresponding authors.

###### Abstract

Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture _DeepStack_ for LMMs. Considering \(N\) layers in the language and vision transformer of LMMs, we stack the visual tokens into \(N\) groups and feed each group to its aligned transformer layer _from bottom to top_, as illustrated in Fig. 1. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply _DeepStack_ to both language and vision transformer in LMMs, and validate the effectiveness of _DeepStack_ LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by **2.7** and **2.9** on average across **9** benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts

Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture _DeepStack_ for LMMs. Considering \(N\) layers in the language and vision transformer of LMMs, we stack the visual tokens into \(N\) groups and feed each group to its aligned transformer layer _from bottom to top_, as illustrated in Fig. 1. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply _DeepStack_ to both language and vision transformer in LMMs, and validate the effectiveness of _DeepStack_ LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by **2.7** and **2.9** on average across **9** benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts

Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (\(\)) simply using a residual connection. With _no_ architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply _DeepStack_ separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take 4\(\) more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks.

that use the full context length. These gains are particularly pronounced on high-resolution tasks, _e.g._, **4.2**, **11.0**, and **4.0** improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply _DeepStack_ to vision transformer layers, which brings us a similar amount of improvements, **3.8** on average compared with LLaVA-1.5-7B.

## 1 Introduction

With the tremendous advancements in large language models (LLMs) , we have witnessed a surge of efforts of developing large multimodal models (LMMs) . To connect vision and language models for LMMs, a conventional way is transforming images into a number of visual features using pretrained vision encoders (_e.g._, CLIP ), and flattening them to a sequence of "language tokens" which are then fed into an LLM. With sufficient alignment and instruction tuning, the entire system can demonstrate a broad conversational capability for multimodal inputs .

To incorporate visual inputs, it usually requires the LMMs to handle a large number of visual tokens as the prefix tokens in addition to the original language prompts. This inevitably introduces a tremendous memory and compute overhead into the LLMs, which is particularly significant when it comes to high-resolution images and multi-frame videos. Several previous works attempt to mitigate this issue by proposing various token compression strategies. A straightforward way is to reduce the number of tokens with spatial grouping . Instead of pooling vision tokens, a few work instead to concatenate local tokens along the feature dimension to preserve visual information . Moreover, other works seek more sophisticated token resampling, such as Q-Former , Perceiver  and Abstractor , _etc._ In MM1 , the researchers performed an extensive analysis of these approaches and found no significant discrepancies among them. Despite the huge effort, all these works inherently sacrifice fine-grained visual information to reach the trade-off between the compute overhead and the information flow into LLMs, which is arguably problematic for high-resolution images and videos. Most recently, a few works  proposed multi-crop strategies and string several times more visual tokens to support high-resolution scenarios, while at the cost of substantial overhead.

All current efforts to wire vision with LLMs follow the routine in which visual tokens are always rolled together as a 1d sequence, and fed into the first layer of LLMs as inputs. In this work, we step outside the box and question whether we can find a better strategy to handle the large number of visual tokens regarding both efficacy and efficiency. Instead of examining the LLMs in a traditional left-to-right orientation, we adopt a novel bottom-to-top perspective, revealing that they constitute a hierarchical arrangement of transformer layers. Based on this observation, we propose DeepStack, a simple, yet novel way of feeding visual tokens into LLMs. As shown in Fig. 1, instead of putting the long sequence of visual tokens from left to right, we restructure the visual tokens into a layered stack, where each layer of the stack is connected to one layer in the LLMs by simple residual connection. As a result, with the context length unchanged, we can feed into LLMs several times more visual tokens to handle complex visual inputs. Meanwhile, the combination of per-layer parallel attention and layer-by-layer progression can effectively leverage the LLMs' capacity for modeling the dependencies of visual tokens.

To examine the effectiveness of our method, we apply it to two representative LMMs, LLaVA-1.5  and LLaVA-Next . Extensive empirical results demonstrate the effectiveness of our method. More specifically, with the same setting of LLaVA-1.5, our model can achieve significant performance gain across a wide range of benchmarks. In particular, our model brings **4.2**, **11.0**, and **4.0** performance gains on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. To summarize, our main contributions are three-fold:

* We propose a simple yet effective _DeepStack_ strategy for connecting vision and language in the context of LMMs. This new strategy introduces _no_ architecture change while significantly increasing the number of tokens LLMs can take.
* With the _DeepStack_ strategy, we present our new model DeepStack, and compare it with LMMs across a wide range of multimodal tasks. Our model demonstrates consistent improvement over the baseline methods, in particular for high-resolution tasks.

* We further conduct comprehensive ablation studies on different aspects of our proposed method, which provide useful guidance and insights behind the design choices.

Finally, although we only demonstrate the effectiveness of our proposed method in the context of LMMs, we note that this simple strategy could be generalized to any models or tasks built on top of transformer layers. We hope this new design could shield new lights and open up new exploratory directions regarding how to wire vision encoders and LLMs in large multimodal models.

## 2 Related Works

**Large Language Models (LLMs).** Recently, natural language processing (NLP) has witnessed significant progress, particularly with the advent of large language models (LLMs) [74; 87; 64; 6]. Building on the foundational architecture of Transformers , language models [18; 74; 87; 64; 39] have demonstrated strong scalability through the pretraining-then-finetuning paradigm. Specifically, BERT  utilizes the transformer encoder and introduces a masked language modeling task to pre-train the model on vast unlabelled data, showing excellent performance after fine-tuning on downstream tasks. Other follow-ups [39; 36] continue along the lines of BERT, constantly refining and optimizing its performance. The T5  series further unifies different NLP tasks within an encoder-decoder architecture, demonstrating effectiveness across dozens of language understanding tasks. Meanwhile, the GPT [62; 63; 4] series employs simple decoder-only transformers to pretrain the language model using a unified next-token prediction paradigm. This approach shows remarkable scalability in terms of both model size and data scale. To enhance instruction-following abilities, InstructGPT  and ChatGPT emphasize the importance of instruction tuning and Reinforcement Learning from Human Feedback (RLHF). These models exhibit excellent capabilities in open-domain conversation tasks, ranging from text generation to question answering. In response to ChatGPT, recent works [74; 15; 38] have made significant efforts in developing an open-source LLMs community. Building on the success of the LLaMA  series foundation model, Alpaca , Vicuna , and GPT-4-LLM  showcase the improvements brought by higher-quality instruction datasets. Other works [24; 27; 1; 87] take a different approach, aiming to achieve comparable performance with a much smaller set of parameters. The Phi [24; 27; 1] series revisits the importance of the pre-training corpus and achieves success with models containing around 3 billion parameters. In this paper, we develop our model based on Vicuna  and Phi-3 , aiming to equip the well-trained LLMs with informative visual tokens and a relatively small training effect.

**Large Multi-modal Models (LMMs).** The success of CLIP  and its follow-ups [66; 28; 77] demonstrates the effectiveness of aligning vision and language modalities into a unified semantic space, showcasing promising capabilities in zero-shot classification tasks. More recently, Flamingo  and BLIP  have utilized visual perceivers  to resample visual tokens from image features as inputs for language models through cross-attention. BLIP-2  and Instruct-BLIP  further incorporate this mechanism into large language models for tasks such as visual captioning and question-answering. Although visual perceivers can translate image features into a fixed set of visual tokens, they face constraints related to convergence costs and data requirements. In parallel, LLaVA and its follow-ups [13; 76; 47; 50; 49] achieved success in connecting vision and language using a simple projection module. It greatly simplifies the difficulties of alignment tasks and even achieves better performance with less training effort. However, due to the rigorous input resolution of pre-trained models, these directions meet difficulties on downstream tasks requiring finer-grained visual information, _e.g._ tasks relevant to OCR and documents. To alleviate this problem, recent works [48; 22; 21; 73; 89] utilize a mixture of experts (MOE) schemes to leverage different pre-trained vision models, typically assembling the visual tokens along the feature dimension. Other attempts [85; 19; 50] split high-resolution images into multi-crop patches and merge them into a longer sequence, which significantly increases the training and evaluation cost. In this work, we conduct experiments on the projector-based connection framework and revisit the connection scheme that utilizes projected visual tokens for the **input layer** of LLMs. We find that the early layers of LLMs can also well process visual token inputs. Besides that, we propose a _DeepStack_ scheme to stack finer-grained visual tokens to the early layers of LLMs, enhancing visual capabilities without introducing extra input tokens.

## 3 DeepStack

_DeepStack_ is a versatile strategy that provides finer-grained visual information without increasing the visual context length for LMMs. It achieves this by dividing image feature extraction into two streams: a global-view stream that captures global information, and a high-resolution stream that enhances the global information by stacking dilated high-resolution image features across different layers of the LLMs. This dual-stream approach offers LMMs detailed visual features while maintaining efficiency. By leveraging this simple yet effective method, we build DeepStack, which significantly improves the ability of LMMs to process and comprehend fine-grained visual details. We illustrate DeepStack in Fig. 2 and propose a pseudo-code implementation in Algorithm. 1.

### Preliminary: Large Multimodal Model

**Large Language Models (LLMs).** LLMs [2; 11; 70; 74] are typically pre-trained on a huge amount of unlabeled text corpus using a transformer decoder-only architecture. The primary pre-training task is _next-token prediction_ driving their learning process. Formally, the learning objective can be formulated as:

\[=_{t=1}^{N}_{}(x_{t+1} x_{1:t}) \]

where \(\) represents the large language model and \(\) is the trainable parameters of the model, with the training objective to maximize the probability of \(x_{t+1}\) as the next token, given the previous tokens \(x_{1:t}=x_{1},,x_{t}\).

**Language Multi-modal Models (LMMs).** LMMs extend pre-trained LLMs to generate responses conditioned on input images. This is achieved by using visual tokens as a prefix:

\[=_{t=1}^{N}_{}(x_{t+1} x_{1:t},) \]

where \(^{l c}\) represents the sequence of visual tokens [43; 51; 4], with \(l\) being the squence length and \(c\) the hidden dimension of the LLM.

**Image Tokenization.** Previous works [45; 43; 51] widely explored how to encode input images into visual tokens. The tokenization schemes usually leverage a vision-language pre-trained image encoder \(^{v}\), _e.g_. CLIP , to extract image features \(^{}\) from an input image \(\). Then, the image features are converted into visual tokens using a _connection module_\(\) as follows:

\[=(^{});\ \ ^{}= ^{v}() \]

Figure 2: **Architecture of DeepStack. The main innovation lies in the _DeepStack_ strategy that infuses visual tokens into different layers. Left: _DeepStack_ for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extra the neighbors from the high-resolution version and reorganize them into _DeepStack_, which are then fed to the consequent layers in LLMs. Right: _DeepStack_ for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder.**```

**Algorithm 1**DeepStack PyTorch pseudocode.

The connection module \(\) can take various forms, mainly divided into projection modules  and perceiver resamplers . In the former, \(\) is implemented as either a single-layer linear projection  or a multi-layer MLP , directly projecting dense image features into the hidden space of the LLM. In the latter, \(\) utilizes a cross-attention mechanism with a set of fixed-length learnable queries to extract image features, similar to the approach in . They transform dense image features into sparse image queries, which are then used as input tokens for the language model. However, the resamplers-based methods easily struggle with hallucinations on spatial reasoning tasks . In this paper, we mainly focus on the projection-based connection module for its efficiency and effectiveness.

### _DeepStack_ for Improved Image Tokenization

Now that we obtain the visual tokens for LMMs using a projection-based connection module, the following challenge is how to provide informative visual tokens while keeping the multi-modal processing effective.

**Scaling Visual Tokens.** Based on the projection-based connection module, many follow-up attempts to increase the visual capability by introducing multiple image crops  for scaling up the resolution or involving multiple vision encoders to serve as a mixture of visual experts . For these approaches, the visual tokens from different image crops or vision encoders are concatenated together along the axis of the sequence or the dimension before projection.

_DeepStack_ Strategy.** In order to incorporate fine-grained image information while maintaining efficiency, we enhance the input visual tokens \(\) by stacking high-resolution visual tokens into different LLM decoder layers. In practice, we first upsample the input image according to its aspect ratio and simultaneously tokenize it to obtain high-resolution visual tokens. To prepare the tokens for hierarchy stacking, we split the high-resolution visual tokens into different token sets \(}^{i}\) with spatial dilation . This sampling approach ensures that the visual tokens \(}^{i}\) have the same length as the global visual tokens \(\). Additionally, token \(}^{i}\) corresponds to the nearest neighbor of \(\) in spatial.

\[} =\{}^{1},}^{2},...,}^{s}\} \] \[=((^{v}(^{})))\]

As shown in Fig. 2, given an LLM of \(L\) decoder layers, the LLM is first split into different blocks. Specifically, DeepStack split the early layers of LLM \(\) into a set of _deepstack_ blocks \(^{V}=\{^{V^{1}},^{V^{2}},...,^{V^ {n}}\}\) for stacking visual tokens, and the later layers into a plain block \(^{}\) for original prefix sequential modeling. We denote that each _deepstack_ block \(^{V^{i}}\) ends at the \(N^{V^{i}}\)-th layer of \(\), while the plain block \(^{}\) ends at the last layer. We use \(^{i}\) to represent the hidden states of visual tokens after the \(i\)-th transformer decoder layer, with \(^{L}\) being the visual hidden states after the final decoder layer. Formally, the output of each block can be formulated as follows:

\[^{V^{1}} =^{V^{1}}+}^{1} \] \[^{V^{2}} =^{V^{2}}^{V^{1}}+}^{2}\] \[^{L} =^{}^{V^{n}}\]Specifically, we divide the layers into equally sized _deepstack_ blocks, with the block length of 1 by default.

_DeepStack_ for Vision Transformers (ViTs).Our _DeepStack_ can be also applied to ViTs for better feature extraction and image tokenization as illustrated in Fig. 2 (DeepStack-V). In contrast to LMM, we use the patch embedding layers \(\) and the first several ViT encoder layers for tokenization and the reset ViT encoder layers for _DeepStack_. Formally, we replace the \(\) and \(\) in Eq. (4) with the Patch Embedding Layers and the first several encoder layers, and utilize the rest of encoders layers as \(\) in Eq. (5). Please refer to Sec. 4.3 for more details.

**Comparison with Other Visual Token Enhancement Strategies.** To provide a deeper understanding of the _DeepStack_ mechanism, we compare our strategy with previous visual token enhancement strategies by examining the hidden states of visual tokens after the final LLM decoder layer, denoted as \(^{L}\). Previous methods can be broadly categorized into two approaches: _Sequence Concatenation_ and _Dimension Concatenation_.

As for the former, visual tokens from the entire image and local crops are concatenated sequentially, significantly increasing the overall sequence length the computation cost. The LLM decoder processes these concatenated visual tokens as a longer visual prefix, directly modeling the extended sequence.

\[^{L}=[,}]  \]

As for the latter, visual tokens are concatenated along the feature dimension, keeping the sequence length constant. When using a projection module as the connection module, the enhanced visual tokens can be viewed as the sum of features from two individual projection modules.

\[^{L} =([,^{}]) \] \[^{1}()+ ^{2}(^{})\]

In our _DeepStack_, we employ a unique approach where enhancement occurs from bottom to top layer by layer. The processing of \(^{L}\) in DeepStack unfolds in two phases. In the early layers of the decoder, the layers function similarly to an encoder, recurrently enhancing the input visual tokens by adding high-resolution visual tokens residually; In the later layers, the decoder performs plain sequence modeling as usual. This dual-phase processing fully leverages the LLM's capabilities by combining both encoding and sequence modeling. By integrating high-resolution visual information at multiple layers, DeepStack effectively enhances visual token representation without increasing visual context length, demonstrating its superiority over previous methods.

\[^{L}=^{L}}^{V^{n}} (...(^{V^{1}}(+}^{1} )+}^{2})...)+}^{n}} \]

Early layers for visual tokens encoding

## 4 Experiments

### Implementation Details

We mainly follow the training recipe of Llava , of which the training pipeline consists of two stages, _i.e._ pre-training (PT) stage and supervised-finetuning (SFT) stage. We utilize pre-trained CLIP-large-336  as our default image encoder. To obtain high-resolution feature maps, we split the high-resolution image into patches to comply with the resolution requirement and mosaic the image feature together as whole-image features.

**Pre-training dataset.** We utilize LCS-558k  as pre-training data for both experiments based on LLaVA-1.5 and LLaVA-Next, which contain 558k samples from LAION , CC  and SBU , captioned by BLIP .

**Fine-tuning datasets.** We utilize LLaVA-mixed-665k  as instruction-following data for both experiments based on LLaVA-1.5. However, the SFT dataset used in Llava-Next is not publicly available, we thus combine an SFT dataset of 748K samples following the guidance . In contrast, we do not involve the user images uploaded to their website.

**Training configuration.** We train our model with only the projection model tuned in the PT stage. In SFT stage, we unfreeze LLM. For Experiments on DeepStack-V and DeepStack-HD, we tune the image encoder with a learning rate of 1e-6 following . Otherwise, we freeze our vision encoder for a fair comparison. We use 16\(\) V100 for experiments with Phi-3  and 8\(\) H100 for experiments with Vicuna . Please refer to our supplementary material for more detailed training hyper-parameters.

### Quantitive Results

We evaluate DeepStack on a range of benchmarks, encompassing both academic task-oriented evaluations and recent large multi-modal language model (LMM) benchmarks. Specifically, we focus on text-oriented datasets, including ChartVQA , DocVQA , InfoVQA , MultiDocVQA , TextVQA , to demonstrate effectiveness in high-resolution scenarios. Additionally, we perform zero-shot evaluations of DeepStack on commonly used video understanding benchmarks to assess its performance on finer-grained tasks.

**General VQA and LMM benchmarks.** We assess DeepStack on two classic general VQA benchmarks, VQAv2  and GQA , as well as five recent LMM benchmarks: SEED , POPE , MMMU , and MM-Vet . As presented in Tab. 1, DeepStack outperforms its direct baseline model, LLaVA, on both VQAv2 and GQA, showcasing state-of-the-art performance in traditional VQA tasks. Furthermore, DeepStack consistently surpasses other methods on the recent LMM benchmarks. DeepStack achieves comparable performance on MM-Vet on the experiments based on LLaVA-1.5. However, due to we lack of fancy instruction-following data used in LLaVA-mix-765K, our experiments with LLaVA-Next lag behind the LLaVA-Next. Notably, the significant performance boost on the POPE benchmark suggests that our _DeepStack_ strategy effectively alleviates visual hallucination by providing rich and detailed visual information for visual understanding.

**Text-Oriented benchmarks.** To further validate the effectiveness of DeepStack, we evaluate it on more text-oriented benchmarks, including ChartQA , DocVQA , InfoVQA , MultiDocVQA , and TextVQA . These benchmarks contain high-resolution images and typically require the model to answer questions based on fine-grained visual inputs. As shown in Tab. 2,

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & \\  & _Res._ & _Tok._ & _Lav._ & & & & & & & & & & & \\  BLLP-2  & Vicuna-13B & 224 & 32 & 232 & 129M & - & 41.0 & 41.0 & 42.5 & - & 46.4 & 85.3 & - & - \\ InstructorNLP  & Vicuna-13B & 224 & 32 & 129M & 1.2M & - & 49.2 & 80.1 & - & 53.4 & - & - & - \\ InstructorNLP  & Vicuna-13B & 224 & 32 & 129M & 1.2M & - & 49.5 & 50.7 & - & 78.9 & - & - & - \\ Shiku  & Vicuna-13B & 224 & - & - & 600K & 5.5M & 77.4 & - & - & - & - & - & - & - \\ IDEPECS-9b  & LLaVA-27B & - & - & - & 533M & 11 & - & 50.9 & 38.4 & - & - & - & - & - & - \\ IDEPECS-80b  & LLaVA-48B & 24 & - & - & 533M & 10.0 & 60.0 & 45.2 & - & - & - & - & - & - \\ Qaw-VL  & Open-7b & 448 & 256 & 256 & 1.4M & 50.7M & 78.9 & 53.9 & 63.8 & - & 56.3 & - & - & - \\ Qaw-VL-Chen  & Open-7b & 448 & 256 & 256 & 1.4M & 50M & 78.2 & 57.5 & 61.5 & - & 8.2 & - & - & - \\ WIL  & Llaman-27b & 336 & 576 & 50M & 1M & 79.9 & 63.2 & 64.4 & - & - & 61.1 & 85.5 & - & 34.9 \\ VILA  & Llaman-13B & 336 & 576 & 576 & 50M & 1M & 80.8 & 63.3 & 66.6 & - & 62.8 & 84.2 & - & 38.8 \\ LLaVA-15  & Vicuna-7b & 336 & 576 & 576 & 585M & 60.6K & 78.5 & 62.0 & 58.2 & 28.1 & 25.8 & 85.6 & 85.9 & 34.8 \\ LLaVA-15a & 50a & Vicuna-13B & 676 & 576 & 557 & 558M & 60.8K & 60.3 & 63.1 & 30.3 & 28.4 & 61.6 & 85.9 & 34.8 & 35.4 \\ LLaVA-Net  & Vicuna-7b & 672 & 2880 & 2880 & **576K** & **765K** & 81.8 & 64.2 & 64.9 & 74.4 & 37.1 & 64.7 & 86.5 & 35.1 & 44.1 \\ LLaVA-Net  & Vicuna-7b & 672 & 2880 & 2880 & **576K** & **762** & 82.8 & 65.4 & 66.0 & 77.5 & 44.5 & 65.6 & 86.2 & 35.9 & 49.1 \\  DeepStack-V & Vicuna-7b & 672 & 2880 & 576 & 558K & 60.6K & 80.4* & 64.1* & 63.5 & 41.0 & 30.0 & 62.3 & 87.6 & 34.9 & 33.0 \\ DeepStack-V & Vicuna-13B & 672 & 2880 & 576 & 558K & 60.6K & 81.1 & 64.2 & 63.9 & 41.7 & 33.1 & 63.0 & 86.6 & 34.7 & 31.1 \\ DeepStack-L & Vicuna-7b & 672 & 2880 & 576 & 558K & 60.6K & 79.5 & 61.2 & 62.4 & 39.4 & 37.1 & 33.0 & 63.0 & 86.6 & 34.7 & 31.1 \\ DeepStack-L & Vicuna-13B & 672 & 2880 & 576 & 558K & 60.6K & 80.9* & 61.2 & 64.2 & 39.1 & 29.8 & 60.6 & 86.7 & 35.7 & 35.2 & 35.9 \\ DeepStack-L & Utima-13B & 762 & 2880 & 576 & 558K & 60.6K & 80.9* & 61.2 & 64.6 & 41.5 & 33.0 & 63.5 & 87.7 & 35.2 & 35.9 \\ DeepStack-L & Utima-13B & 1440 & 2880 & 585K & 758K & 82.0 & 65.2 & 66.7 & 78.8 & 41.2* & 63.6 & 86.5 & 35.6 & 37.5 & 37.5 \\ DeepStack-LHD & Vicuna-13B & 1344 & 14400 & 2880 & 558K & 748K & 83.0* & 66.2* & 68.7 & 81.0* & 45.2* & 65.1 & 86.7 & 33.4 & 39.3 \\   

Table 1: **Comparison with other LMMs on 9 benchmarks**. _Eff. Res._ indicates the effective image resolution taken by each method. _Vis. Tok._ indicates the number of visual tokens used for LLMs (**not only** for the input layers), _Cat. Len._ indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the _Vis. Tok._ = _Cvt. Len._ all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. \(\) indicates that our model is fine-tuned from LLaVA-Next. \(*\) The training images of the datasets are observed during training. \({}^{}\) denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)

[MISSING_PAGE_FAIL:8]

zero. As shown in Fig. 3 (a), inserting visual tokens before the 8th of 32 decoder layers in Phi-3 results in acceptable performance variations. However, inserting them beyond the midpoint leads to a significant performance drop. This confirms that earlier layers efficiently handle initial visual information integration. We also explore the impact of inserting visual tokens at non-consecutive layers. In Fig. 3 (b), we fixed global visual tokens at the input layer and varied the interval between two decoder layers for stacking high-resolution tokens. All stacking settings consistently improved performance. Finally, we explored the number of layers used for stacking high-resolution tokens. As shown in Fig. 3 (c), increasing the layers for stacking consistently enhances overall performance, with the best results achieved using four layers.

_DeepStack_ **can also boost Vision Transformers (ViT).** To further explore the potential of DeepStack for vision transformers, we utilize the DeepStack on ViT. Specifically, we use the patch embedding layers and the first \(N\) ViT encoder layers to extract visual tokens, including the original tokens and 4\(\) extra high-resolution tokens, and then stack the high-resolution tokens into the next 4 encoder layers, respectively. We need to unfreeze the vision encoder to adapt the pre-trained encoder to our DeepStack. As shown in Tab. 4 and Sec. 4.3, when using the first 16 ViT encoder layers (total 24 layers for our ViT-Large) to extract visual tokens before _DeepStack_, DeepStack-V surpass the baseline model. And the performance keeps increasing when using more encoder layers before _DeepStack_.

**Better spatial consistency leads to better performance.** Different sampling strategies may lead to different results. In Tab. 5, we compare our default strategy with two other variants for organizing the visual tokens. As shown in Fig. 4, _2d Grid_ use each of the local crop as a layer and _1d Sequence_ simply flatten the visual tokens to one-dimensional and then reshape them into a layer stack. Accordingly, keeping the spatial coherence, _i.e._ _2d Spatial_, as in our default setting could achieve the best result.

_DeepStack_ **boosts LMMs from high-resolution tokens, not residual connections**. We experiment to assess the impact of high-resolution images and residual connections in DeepStack by stacking original visual tokens into different layers. As shown in Tab. 6, stacking repeated original tokens (dummy tokens) does not improve performance. This indicates that the performance boost in DeepStack comes from the high-resolution tokens, not from the residual connections.

   } &  &  Pt Enc. \\  }} &  GQA \\  }} &  POPE \\  }} &  SEED \\  }} &  TextVQA \\  }} &  DecVQA \\  }} &  ChartQA \\  }} &  InfoVQA \\  }} &  AVG \\  }} \\    &  & & & & & & & & & & & & \\  _DeepStack-V_ & PatchEmbed+0 Enc. Layers & ✓ & 56.9 & 80.8 & 54.9 & 44.4 & 13.7 & 12.3 & 25.3 & 41.2 \\ _DeepStack-V_ & PatchEmbed+4 Enc. Layers & ✓ & 58.7 & 83.1 & 57.4 & 48.2 & 17.0 & 13.2 & 26.1 & 43.4 \\ _DeepStack-V_ & PatchEmbed+8 Enc. Layers & ✓ & 60.4 & 84.2 & 59.7 & 51.8 & 23.1 & 14.7 & 26.6 & 45.8 \\ _DeepStack-V_ & PatchEmbed+12 Enc. Layers & ✓ & 61.8 & 85.5 & 62.1 & 55.5 & 29.3 & 16.0 & 26.2 & 48.1 \\ _DeepStack-V_ & PatchEmbed+16 Enc. Layers & ✓ & 62.9 & 86.3 & 63.9 & 59.1 & 36.9 & 18.2 & 29.3 & 50.9 \\ _DeepStack-V_ & PatchEmbed+20 Enc. Layers & ✓ & 62.8 & 86.1 & 64.0 & 60.1 & 38.4 & 17.1 & 30.6 & 51.3 \\   

Table 4: **Ablations on the number of ViT encoder layers for DeepStack-V.**

Figure 4: **Visualization of three sampling methods for _DeepStack_.**

   } &  Sampling \\  }} &  GQA \\  }} &  POPE \\  }} &  SEED \\  }} &  TextVQA \\  }} &  DecVQA \\  }} &  ChartQA \\  }} &  InfoVQA \\  }} &  AVG \\  }} \\  None & None & 62.5 & 85.5 & 63.5 & 56.7 & 31.7 & 15.8 & 28.3 & 49.1 \\  _DeepStack-V_ & PatchEmbed+8 Enc. Layers & ✓ & 62.2 & 85.1 & 62.3 & 58.1 & 35.1 & 16.4 & 30.1 & 49.9 \\ _DeepStack-V_ & _2d Spatial_ & 63.0 & 86.4 & 62.9 & 58.8 & 38.7 & 17.2 & 30.8 & 51.1 \\ _DeepStack-V_ & _2d Grid_ & 60.6 & 86.2 & 61.2 & 57.1 & 33.2 & 16.4 & 28.6 & 49.0 \\ _DeepStack-V_ & _1d Sequential_ & 61.6 & 86.2 & 61.9 & 57.1 & 33.1 & 15.2 & 30.0 & 49.3 \\   

Table 5: **Ablations on image consistency and sampling method**. We apply the Resize transformation to both the original image and the high-resolution image for consistency. For inconsistency, we use Resize on the original image and Pad-Resize on the high-resolution image. _2d Spatial_ refers to sampling based on spatial locations, such as using a 4-neighbor method. _2d Grid_ means the visual tokens are divided into 2d grids, with each grid stacked per layer. _1d Sequential_ indicates that the high-resolution visual tokens are first flattened into a sequence and then uniformly sampled for each layer. Please refer to Fig. 4 for better understanding.

_DeepStack_ achieves a better trade-off between performance and effectiveness. We compare DeepStack with other token enhancement strategies, including dimension-wise concatenation, sequence-wise with high-resolution visual tokens, and string both global visual and high-resolution tokens. As shown in Tab. 7, although string-based methods can bring significant improvement on some benchmarks, they increase the number of tokens at the same time, which will increase the training and inference cost. Meanwhile, DeepStack achieves the best trade-off between performance and effectiveness without introducing extra visual tokens.

_DeepStack_ unleashes the power after fine-tuning the image encoder. We further experiment with how DeepStack compared coproated with fine-tuning backbones. As shown in Tab. 4, DeepStack achieves the best performance when fine-tuning the backbone. It is worth noticing that when fine-tuning the backbone without DeepStack, the improvement is limited. After combining backbone fetuning with DeepStack, the performance significantly increases among different benchmarks. It is because of the deep interaction between visual tokens and the LLM decoder.

## 5 Conclusion

In this work, we had presented DeepStack, a simple yet effective way to connect vision and language in the context of LMMs. Unlike previous works that always string (compressed) visual tokens into a sequence, we alternatively introduced a new perspective on transformer decoder layers in LLMs, and proposed a _DeepStack_ strategy to feed different visual tokens into different layers of LLMs. This strategy significantly mitigates the efficiency overhead introduced by visual tokens and makes it possible to convey more visual information to LLMs. As a result, our DeepStack demonstrated consistent improvements over two baseline models across a wide range of benchmarks. The benefits are particularly significant on tasks that inherently require more tokens, such as high-resolution image understanding. We hope this new _DeepStack_ strategy could open up new ideas on how to connect vision and language for faster and better multimodal models in the regime of LMMs.

**Limitation and Future Works**. DeepStack simply inserts the visual tokens into middle LLMs layers via a residual connection in a heuristic manner. Though it already exhibits promising results, we may find a more powerful way to infuse the visual information, _e.g._, through gated function or layer-wise positional embeddings. Meanwhile, how to systematically decide the starting layer and number of layers also deserves more study. We leave these as promising directions.