# Large Graph Property Prediction via

Graph Segment Training

 Kaidi Cao\({}^{1}\), Phitchaya Mangpo Phothilimthana\({}^{2}\), Sami Abu-El-Haija\({}^{2}\), Dustin Zelle\({}^{2}\),

**Yanqi Zhou\({}^{2}\), Charith Mendis\({}^{3}\)**,** Jure Leskovec\({}^{1}\), Bryan Perozzi\({}^{2}\)

\({}^{1}\)Stanford University, \({}^{2}\)Google, \({}^{3}\)UIUC

The work was partially completed during Kaidi Cao's internship at Google.The work was partially completed when Charith Mendis was a visiting researcher at Google.

###### Abstract

Learning to predict properties of a large graph is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce _Stale Embedding Dropout_ to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST+EFD (with all the techniques together) on two large graph property prediction benchmarks: MalNet and TpuGraphs. Our experiments show that GST+EFD is both memory-efficient and fast, while offering a slight boost on test accuracy over a typical full graph training regime.

## 1 Introduction

Graph property prediction is a task of predicting a certain property or a characteristic of an entire graph . Important applications include, predicting properties of molecules , predicting properties of programs/code  and predicting properties of organisms based on their protein-protein interaction networks .

These popular graph property prediction tasks deal with relatively small graphs, so the scalability issue arises only from a large number of (small) graphs.

However, graph property prediction tasks also face another scalability challenge, which arises due to the large size of each individual graph, as some graphs can have millions or even billions of nodes and edges . Training typical Graph Neural Networks (GNNs) to classify such large graphs can be computationally infeasible, as the memory needed scales at least linearly with the size of the graph . This presents a challenge as even most powerful GPUs, which are optimized for handling large amounts of data, only have a limited amount of memory available.

Previous efforts to improve scalability of GNNs have mostly focused on developing methods for node-level and link-level prediction tasks, which can be performed using sampled subgraphs . However, there is a lack of research on how to train scalable models for _property prediction of large graphs_. Training a model on a sampled subgraph alone is insufficient for thesetypes of tasks, as the subgraph sampled may not contain all the necessary information to make accurate predictions for the entire graph. For example, if the task is to predict the diameter of the graph, it is unlikely that a fixed-size subgraph would contain sufficient features for the GNN to make an accurate and reliable prediction. Thus, it is essential to aggregate information from the entire graph to predict a graph property.

In this paper, we address the problem of property prediction of large graphs. We propose Graph Segment Training (GST)3, which is able to train on large graphs with constant (GPU) memory footprint.

Our approach partitions each large graph into smaller segments with a controlled size in the preprocessing phase. During the training process, a random subset of segments is selected to update the model at each step, rather than using the entire graph. This way, we need to maintain intermediate activations for only a few segments for backpropagation; embeddings for the remaining segments are created without saving their intermediate activations. The embeddings of all segments are then combined to generate an embedding for the original large graph, which is used for prediction. Therefore, each large graph has an upper bound on memory consumption during training regardless of its original size. This allows us to train the model on large graphs without running into an out-of-memory (OOM) issue, even with limited computational resources.

To accelerate the training process further, we introduce a historical embedding table to efficiently produce embeddings for graph segments that do not require gradients, as historical embeddings eliminate additional computation on such segments. However, the historical embeddings induce staleness issues during training, so we design two techniques to mitigate such issues in practice. First, we characterize the input distribution mismatch issue between training and test stages of the prediction head, and propose to finetune only the prediction head at the end of training to close the input distribution gap. Second, we identify bias in the loss function due to stale historical embeddings, and introduce _Stale Embedding Dropout_ to drop some stale embeddings during training to reduce this bias. Our final proposed method, called GST+EFD, is both memory-efficient and fast.

We evaluate our method on the following datasets: MalNet-Tiny, MalNet-Large and TpuGraphs. A typical full graph training pipeline (Full Graph Training) can only train on MalNet-Tiny, and

Figure 1: (a) **Full Graph Training**: Classically, models are trained using the entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. (b) **Graph Segment Training**: Our solution is to partition each large graph into smaller segments and select a random subset of segments to update the model; embeddings for the remaining segments are produced without saving their intermediate activations. The embeddings of all segments are combined to generate an embedding for the original large graph, which is then used for prediction. The important benefit is that GPU memory requirement only depends on the segment size (but not the full graph size).

unavoidably reaches OOM on MalNet-Large and TpuGraphs dataset. On the contrary, we empirically show that the proposed GST framework successfully trains on arbitrarily large graphs using a single NVIDIA-V100 GPU with only 16GB of memory for MalNet-Large and four GPUs for TpuGraphs dataset, while maintaining comparable performance with Full Graph Training. We finally demonstrate that our complete method GST+EFD slightly outperforms GST by another 1-2% in terms of final evaluation metric, and simultaneously being 3x faster in terms of training time.

## 2 Preliminaries

**Notation.** For a function \(f():^{d_{0}}^{d_{1}}\), we use \(D^{k}_{}f[_{0}]^{d_{0} d_{1}}\) to denote its \(k\)-th derivative of \(f\) with respect to \(\) evaluated at value \(_{0}\). Let \(f g\) to denote the composition of function \(f\) with \(g\). We use \(\) to denote entry-wise product, and let \(^{2}}{{=}}\). We use \(\) to represent aggregation. \(_{j J}_{j}\) means aggregating the set \(\{_{j}\}_{j J}\), where \(j\) indexes segments, and \(J\) is the number of segments in a graph. We usually drop \(j\) subscript from \(\) for brevity. \(_{i}_{j}\) aggregates both sets \(\{_{i}\}\) and \(\{_{j}\}\) together. \(\) can be mean or sum operators when applying to vectors. We define the input graph as \(=\{,\}\), where \(=\{v_{1},...,v_{m}\}\) is the node set and \(\) is the edge set.

Let dataset \(=\{(^{(i)},y^{(i)})\}_{i n}\) contain \(n\) examples: each label \(y^{(i)}\) is associated with \(^{(i)}\).

**Graph Neural Network.** We consider a backbone graph neural network \(F\) that takes a graph \(^{(i)}\) and generates graph embedding \(^{(i)}^{d_{h}}\), followed by a final prediction head \(F^{}\) that takes graph embedding \(^{(i)}\) and outputs final predictions: \(^{(i)}=(F^{} F)(^{(i)})\). We optimize GNN with the loss function as \(((F^{} F)(^{(i)}),y^{(i)})\).

**Historical Embedding Table.** We define an embedding table \(:^{d_{h}}\), where key-space \(\) is a tuple: (graph index \(i n\), segment index \(j J\)). We use \(}^{(i)}_{j}=(i,j)\) to denote embedding for graph segment \(^{(i)}_{j}\): an embedding not up to date with the current backbone \(F\).

## 3 Our Method: GST+EFD

### Graph Segment Training (GST)

Given a training graph dataset \(_{}=\{(^{(i)},y^{(i)})\}_{i=1}^{n}\), a common SGD update step requires calculating the gradient:

\[_{}_{(^{(i)},y^{(i)})}((F^ {} F)(^{(i)}),y^{(i)})\]

where \(\) is trainable weights in \(F^{} F\), and \(\) is a sampled minibatch. Graphs can differ in size (the number of nodes \(|^{(i)}|\)), with some being too large to fit into the device's memory. This is because the memory required to store all intermediate activations for computing gradients is proportional to the number of nodes and edges in the graph.

To address the above issue, we propose to partition each original input graph into a collection of graph segments, _i.e._,

\[^{(i)}^{(i)}_{j}j\{1,2,,J^{(i)}\}\]

An example of a partition algorithm is METIS . This preprocessing step will result in a training set \(_{}=\{(_{j J^{(i)}}^{(i)}_{j}, y^{(i)})\}_{i=1}^{n}\). Number of partitions \(J^{(i)}\) can vary across graphs, but the size of each graph segment can be bounded by a controlled size (\(|^{(i)}_{j}|<m_{},(i,j)\)) so that a batch of a fixed number of graph segments can always fit within the device's memory.

When processing graph segment \(^{(i)}_{j}\), we can obtain its segment embedding through the backbone: \(^{(i)}_{j}=F(^{(i)}_{j})\). The prediction head \(F^{}\) requires information from the whole graph to make the prediction, thus we propose to aggregate all segment embeddings to recover the full graph embedding: \(^{(i)}=^{(i)}_{j}\). A simple realization of this aggregation is mean pooling. Note that naively applyingthe prediction head \(F^{}\) on the aggregated graph embedding -- \(^{(i)}=F^{}(_{j}^{(i)})\) -- would not provide any reduction in peak memory consumption, as we need to keep track of the activations of all graph segments \(\{_{j}^{(i)}\}_{j J^{(i)}}\) to perform backpropagation.

Thus, we propose to perform backpropagation on only a few randomly sampled graph segments \(^{(i)}\{1,,J^{(i)}\}\) and generate embeddings without requiring gradients for the rest. We hereby denote \(_{s}\) and \(}_{j}\) as segment embeddings that require and do not require gradient, respectively. An entire graph embedding is then: \(^{(i)}\{_{s}^{(i)}\}_{s^{(i)}}\{ }_{j}^{(i)}\}_{j^{(i)}}_{s}^{(i)} }_{j}^{(i)}\). We name this general pipeline as GST and summarize it in Algorithm 1.

```
0: A preprocessed training graph dataset \(_{}=\{(_{j}^{(i)},y^{(i)})\}_{i=1}^ {n}\). A parameterized backbone \(F\) and a prediction head \(F^{}\).
1:for\(t=1\) to \(T_{0}\)do
2:\((_{})\)
3:for\((^{(i)},^{(i)})\) in \(\)do
4:\(\{_{s}^{(i)}\}_{s^{(i)}}(^{(i)})\)
5:\(}_{j}^{(i)}(_{j}^{(i)})\) for \(j^{(i)}\)
6:\(_{s}^{(i)} F(_{s}^{(i)})\) for\(s^{(i)}\)
7:endfor
8: SGD on loss \(|}_{i}(F^{}(_{s }^{(i)}}_{j}^{(i)}),^{(i)})\)
9:endfor ```

**Algorithm 1** General Framework of GST

One implementation of ProduceEmbedding\(()\) in Algorithm 1 is to use the same feature encoder \(F\) to forward all the segments in \(\{_{j}^{(i)}\}_{j^{(i)}}\) without storing any intermediate activation (by stopping gradient).

### GST with Historical Embedding Table

Calculating \(}_{j}\) by stopping gradient guarantees an upper bound on peak memory consumption. However, since we do not need gradients for segments \(\{_{i}^{(i)}\}_{j^{(i)}}\), computing forward pass on these segments can be avoided to make training faster. To achieve this, we use historical embeddings acquired in previous training iterations \(}_{j}^{(i)}=(i,j)\). With an embedding table \(\), one can implement ProduceEmbedding\(()\) by fetching the corresponding embedding from the table without any computation. We update the embedding table after conducting the forward pass on a graph segment. We optimize the following loss \((F^{}(_{s}^{(i)}}_{j}^{(i)}),y^{ (i)})\) during training. We name the embedding version of our algorithm as GST+E.

Note that GST+E has runtime advantages over GST. For each segment \(_{j}^{(i)}\) that does not require gradient, GST needs to run an actual forward pass over \(_{j}^{(i)}\), while GST+E only needs to fetch the embedding from a hash table. GST+E has a small overhead from writing the updated embedding of \(_{s}^{(i)}\) back into the table \(\), which is relatively quick and can be run in a separate thread so that it does not impede the main training algorithm until the next iteration.

The drawback of GST+E is that historical embeddings from \(\) may be stale;

\(}_{j}^{(i)}\) can be the result of an out-dated feature extractor \(F\). This type of staleness can hurt the training in various ways. Below, we provide two techniques to mitigate the staleness issue.

### Prediction Head Finetuning

Let's compare the input-output distribution of the prediction head \(F^{}\) during the training and inference stage. We have training distribution \(_{}(,y)=(_{s} }_{j},y)\) and test distribution \(_{}(,y)=(_{j},y)\). Regardless of the innate distribution shift between the training and test stage of the dataset, we note that stale historical embeddings can further widen the gap between the training and test distributions. In this case, the minimizer of the expected training loss does not minimize the expected test loss:

\[*{arg\,min}_{}_{(,y)(, }_{j},y)}(F^{}(),y) *{arg\,min}_{}_{(,y)(  h_{j},y)}(F^{}(),y)\]

To mitigate the distribution misalignment, we introduce the Prediction Head Finetuning technique. Concretely, at the end of training, we update each embedding \(_{j}^{(i)}\) in the embedding table \(\) by forwarding each graph segment in the training set with the most current feature encoder \(F\). We then finetune only the prediction head \(F^{}\) with all the input embeddings up-to-date. We use GST+EF to denote GST+E refined with the Prediction Head Finetuning technique.

The overhead from the finetuning is minimal, as we need to update the embedding table \(\) only once. The rest of the finetuning stage does not involve the notoriously slow graph convolution because the prediction head \(F^{}\) is simply a multi-layer perceptron.

### Stale Embedding Dropout

The finetuning technique primarily addresses the negative impact of stale embeddings on prediction head \(F^{}\). However, the staleness also impacts the backbone \(F\). Prior works studying the effects of stale historical embeddings commonly assume that historical embeddings do not become too stale, _i.e._, \(\|}_{j}^{(i)}-}_{j}^{(i)}\|,(i,j)\). Given this assumption, if the neural network \((F^{} F)()\) is \(k\)-Lipschitz continuous, the gradients will also be bounded and never run too far from its true estimation, _i.e._, \(\| F^{}(}_{j}^{(i)})- F^{}( }_{j}^{(i)})\| k^{}\). Thus, the network can often converge to the similar local minima even when using historical embeddings.

The above assumption does not hold under our GST+E framework. The rationale is that \(}_{j}^{(i)}\) gets updated infrequently in the embedding table \(\). Suppose we iterate through every graph \(^{(i)}\) in the training set \(_{}\) for each epoch, every time we train on a graph \(^{(i)}_{j}^{(i)}\), only a few graph segments \(_{s}^{(i)}\) will be updated in the table \(\). This implies that all the other segment embeddings of \(^{(i)}\) will be at least \(n\)-iteration stale, with \(n\) being the number of graphs in the training set, and the most outdated segment embedding could be approximately \(nJ^{(i)}/S^{(i)}\)-iteration stale, where \(S^{(i)}\) is \(|^{(i)}|\).

This staleness introduces an additional source of bias and variance to the stochastic optimization; the loss function calculated with historical embeddings is no longer an unbiased estimation of its true value. To mitigate the negative impact of historical embeddings on loss function estimation, we propose the second technique, _Stale Embedding Dropout_ (SED). Unlike a standard Dropout, which uniformly drops elements and weighs up the rest, we propose to drop only stale segment embeddings and weigh up only segment embeddings that are up-to-date. Concretely, assume with the keep probability \(p\), the weight \(\) for each segment is defined as:

\[^{(i)}=p+(1-p)}{S^{(i)}}&_{s}^{(i)}\\ 0&_{j}^{(i)},(1-p)\\ 1&_{j}^{(i)},p\] (1)

Please refer to the theoretical analysis in the next section. By combining the two proposed techniques, we denote our final algorithm as GST+EFD, which we summarized in Algorithm 2 in Appendix B.

## 4 Theoretical Analysis

We characterize the effect of stale historical embeddings by studying the difference between \((F^{}(_{s}^{(i)}}_{j}^{(i)} }))\) and \((F^{}(_{j}^{(i)}))\). Assume \(^{(i)}\) has \(J^{(i)}\) segments and we perform back-propagation on \(S^{(i)}\) segments. We let \(^{(i)}_{s}^{(i)}}_{j}^{(i)}}- _{j}^{(i)}\) be the perturbation on the graph embedding.

We apply Taylor expansion around \(^{(i)}=0\) on the final loss to analyze the effect of this perturbation.

\[_{s}(F^{}(_{s}^{(i)} }_{j}^{(i)}))-(F^{}(_{j}^{(i)}))\] (2) \[= _{s}(F^{}(_{j}^{(i )}+^{(i)}))-(F^{}(_{j}^{(i)}))\] \[ _{j}_{_{j}^{(i)}}_{j}^{ (i)}}( F^{})[_{j}^{(i)}]_{j}^{(i)}}_{B}+ ^{(i)}}^{}(D_{_{j}^{(i)}}^{2}(  F^{})[_{j}^{(i)}])_{j}^{(i)}}_{R}\]

In the equation above, the first-order term acts as a bias term introduced by the stale historical embedding, and the second-order term acts as an additional regularizer.

Let ET denote using the embedding table without applying SED. We analyze the effect of ET and SED under the Taylor Expansion in Eq. 2 by substituting different \(^{(i)}\). For the first term, we have

\[_{_{j}^{(i)}}[B] =C-S^{(i)}}{J^{(i)}}(}_ {j}^{(i)}-_{j}^{(i)})\] \[_{_{j}^{(i)}}[B] =C-S^{(i)}}{J^{(i)}}(}_ {j}^{(i)}-_{j}^{(i)})p\]

where \(C\) is a constant matrix.

We extend the above analysis to the following theorem. Please find the complete proof in Appendix A.

**Theorem 4.1**.: _Under proper conditions, SED with a keep ratio \(p\) ensures to reduce bias term introduced by historical embeddings by a factor of \(p\), while introducing another regularization term._

Theorem 4.1 indicates that SED can reduce the bias in the loss function introduced by the stale historical embeddings, at the cost of another regularization term, which might potentially increase total regularization. Prior works commonly make a hidden assumption that \((}_{j}^{(i)}-_{j}^{(i)})\) is so small that the negative effect may be neglected. In our setting, the historical embeddings can be very stale, so having a \(p\) factor helps reduce bias. It is worthwhile to check the limiting cases. If \(p=1\) (keeping all the stale embeddings without dropping any), both \(_{_{j}^{(i)}}[B]\) and \(_{_{j}^{(i)}}[R]\) degrade to the result of ET. If \(p=0\) (droping all the stale embeddings), then the algorithm degrades to training on only \(S^{(i)}\) segments without aggregating other segments (which we denote as GST-One, when \(S^{(i)}=1\)). \(_{_{j}^{(i)}}[B]=0\) indicates that there is no stale bias in this case. However, the term \(_{_{j}^{(i)}}[R]\) could become too large so that it impedes training.

## 5 Experiments

### Experimental Setup

**Datasets.** MalNet  is a large-scale graph representation learning dataset, with the goal to predict the category of a function call graph. MalNet is the largest public graph database constructed to date in terms of average graph size. Its widely-used split is called _MalNet-Tiny_, containing 5,000 graphs across balanced 5 types, with each graph containing at most 5,000 nodes. To evaluate our approach on the regime where the graph size is large, we construct an alternative split from the original MalNet dataset, which we named _MalNet-Large_. _MalNet-Large_ also contains 5,000 graphs across balanced 5 types. _MalNet-Large_'s average graph size reaches 47k with the largest graph containing 541k nodes. We will release our experimental split for _MalNet-Large_ to promote future research.

TpuGraphs is an internal large scale graph regression dataset, whose goal is to predict an execution time of an XLA's HLO graph with a specific compiler configuration on a Tensor Processing Unit (TPU). XLA  is a production backend compiler for various machine learning frameworks, including TensorFlow, PyTorch, and JAX. In this particular dataset, the compiler configuration controls physical layouts of tensors in the graph, and the runtime is measured on TPU v3 . This dataset cares more about the ranking of the configurations for each graph than the absolute runtimes, since the ultimate goal is to use a model to select the best configuration for each graph.4PutoGraphsis similar to the dataset used in , but the runtime prediction is at the entire graph level rather than at the kernel (subgraph) level. TpuGraphs contains 5,153 HLO graphs and a total of 757,375 unique pairs of graphs and configurations. The average graph size is 38k, and the maximum is 615k. From the perspective of GST, a graph together with a configuration defines one \(^{(i)}\) because the configuration is featurized as parts of input node features to the GNN.

Please refer to Table 4 in Appendix for detailed statistics.

**Methods.** We test combinations of the following proposed techniques and some baselines. (1) Full Graph Training: we train on all graphs in their original scale without applying any partitioning beforehand. (2) GST-One: we partition the original graph into a collection of graph segments \(^{(i)}^{(i)}_{j}\), but we randomly select only one segment \(^{(i)}_{j}\) for each graph to train every iteration. (3) GST: following the general GST framework described in Algorithm 1, we replace \(()\) by using the same feature encoder \(F\) to forward all the segments in \(\{^{(i)}_{j}\}_{j^{(i)}}\) without storing any intermediate activation. We set \(S^{(i)}=1\) in our experiments. (4) E: we introduce an embedding table \(}^{(i)}_{j}=(i,j)\) to store the historical embedding of each graph segment, and we fetch the embedding from \(\) if we do not need to calculate gradient for the corresponding segment. (5) F: in addition to introducing the embedding table \(\), we finetune the prediction head \(F^{}\) with all up-to-date segment embeddings at the end of training. (6) D: we apply SED defined in Eq. 1 during training.

When these techniques are combined, we concatenate the acronyms with a "+" to GST as an abbreviation. We conduct all the experiments on MalNet with a single NVIDIA-V100 GPU with 16GB of memory, and four NVIDIA-V100 GPUs (for data parallelism) with 16GB of memory for TpuGraphs. Please refer to Appendix B for additional implementation details.

### Empirical Results on MalNet

To demonstrate the general applicability of our proposed GST framework, we consider three backbones, namely, GCN , SAGE , and GraphGPS . GCN and SAGE are two popular GNN architectures. GraphGPS is a Graph Transformer that recently achieves state-of-the-art performance on many graph-level tasks, but is well-known for its issue on scalability. We report the top-1 test accuracy of various methods on MalNet-Tiny and MalNet-Large in Table 1. We include MalNet-Tiny in this study because its graphs are relatively small so that it is still possible to run Full Graph Training.

Notably, we observe that GST slightly outperforms Full Graph Training in terms of test accuracy on MalNet-Tiny. GST has exactly the same number of weight parameters with Full Graph Training. This implies that GST potentially has a better hierarchical graph pooling mechanism that leads to better generalization. As we step from MalNet-Tiny to MalNet-Large, Full Graph Training strategy can no longer fit the large graphs on a GPU, so we report OOM in the table. GST's estimation on graph segment embeddings \(}^{(i)}_{j}\) that do not require gradients is accurate, and thus does not suffer from staleness issues. Therefore, we use GST as an estimation for the performance of Full Graph Training on MalNet-Large.

Naively training on only one graph segment (GST-One) yields inferior performance than Full Graph Training and GST, showing that it is essential to aggregate embeddings from all graph segments.

   Dataset &  &  \\ Backbone & GCN & SAGE & GraphGPS & GCN & SAGE & GraphGPS \\  Full Graph Training & 87.84\(\)1.37 & 88.08\(\)1.68 & 90.82\(\)0.59 & OOM & OOM & OOM \\ GST & 88.26\(\)0.80 & 88.42\(\)1.03 & 91.03\(\)0.81 & 88.35\(\)1.14 & 88.62\(\)0.82 & 91.39\(\)0.85 \\ GST-One & 71.62\(\)3.85 & 72.64\(\)4.73 & 77.63\(\)3.15 & 60.41\(\)6.29 & 57.13\(\)7.36 & 66.82\(\)4.71 \\ GST+E & 86.53\(\)1.18 & 86.82\(\)0.93 & 87.75\(\)0.89 & 48.42\(\)6.61 & 43.28\(\)7.01 & 62.47\(\)3.19 \\ GST+EF & 87.67\(\)0.78 & 87.83\(\)0.81 & 90.52\(\)0.71 & 84.83\(\)0.96 & 85.26\(\)0.87 & 91.33\(\)0.65 \\ GST+ED & 88.18\(\)0.48 & 88.50\(\)0.74 & 90.96\(\)0.68 & 82.17\(\)4.74 & 71.83\(\)6.31 & 89.46\(\)1.36 \\ GST+EFD & 88.78\(\)0.45 & 89.24\(\)0.53 & 92.46\(\) 0.66 & 89.67\(\)0.71 & 89.78\(\)0.68 & 92.52\(\)0.58 \\   

Table 1: Test accuracy on MalNet-Tiny and MalNet-Large. We report the standard deviation over five runs. GST+EFD achieves better accuracy than Full Graph Training, and GST, while being much more memory efficient and computationally faster.

Solely introducing the historical embedding table (GST+E) significantly deteriorates the optimization due to the staleness issue. Each of the proposed techniques (Prediction Head Finetuning and SED) individually is beneficial in combating the staleness issue. The combination of our two techniques (GST+EFD) achieves the best performance, slightly outperforming GST by another 1-2% in terms of final evaluation metric.

### Empirical results on TpuGraphs dataset

As mentioned in Section 5.1, we care more about the ranking of configurations for each graph than the absolute runtimes in this dataset. Thus, we report the ordered pair accuracy (OPA) averaged over all computational graphs in Table 2. OPA metric is defined as:

\[(y,)=_{j}[_{i}> _{j}][y_{i}>y_{j}]}{_{i}_{j}[y_{i} >y_{j}]}\]

With some compiler domain knowledge, we found it better to predict each graph segment's runtime first and then aggregate them using sum pooling. This means the prediction head is part of \(F\), and \(F^{}\) is simply a summation function. Since there are no learnable weights in \(F^{}\), we omit Prediction Head Finetuning in this experiment, and GST+EFD in Table 2 excludes the finetuning stage. We observe a clear tradeoff between fitting training examples and generalization. First, GST has a much higher training OPA than the other methods, indicating accurate estimation on segments that do not require gradient is essential for training OPA. Training with one segment only or with the embedding table yields lower training OPA, and consequently lower test OPA as well. SED in GST+EFD functions as a regularization technique. Although it slightly lowers the training OPA compared to GST+E, it achieves better test OPA, even than GST, due to bias mitigation.

### Ablation Studies

**Effect of finetuning.** We visualize the training/test accuracy curve of GST+EFD over time in Figure 4. The staleness introduced by historical embeddings drastically hurts generalization, as shown for the first 600 epochs. We start finetuning at epoch 600, and the gap between training and test accuracy decreases by a large margin instantly.

**Ablation study on segment dropout ratio.** To analyze the effect of the keep ratio \(p\) in SED, we vary its value from 0 to 1 and visualize the results in Figure 3. When \(p=1\), GST+EFD degrades back to using the historical embedding table without SED, as the performance decreases due to staleness. When \(p=0\), GST+EFD becomes GST-One, where we drop all the stale historical embeddings. This extreme case introduces too heavy regularization that impedes the model from fitting the training data, leading to a decrease in test performance ultimately. We found that \(p=0.5\) achieves a satisfactory tradeoff between fitting the training data and adding a proper amount of regularization.

**Ablation study on segment size.** We also alter the maximum segment size and visualize the results in Figure 4. A smaller maximum segment size will result in much more number of segments. Interestingly, we found that the proposed GST+EFD is very robust to the choice of the maximum segment size, as long as the segment size is reasonably large.

**Ablation study on partition algorithms.** Please refer to Appendix C.

### Runtime Analysis

Next, we empirically compare runtime of different variants under the proposed GST framework. We summarize an average time for one forward-backward pass during training on MalNet-Large dataset in Table 3. Since GST runs inference for the graph segments that do not require gradients, the runtime of GST is significantly higher than others'. We also found that GST+E's and GST+EFD's runtime are very close to GST-One's; this means the overhead of fetching embeddings from the embedding table \(\) is minimal. Moreover, GST+EFD's runtime is slightly lower than GST+E's because in the implementation, we can skip the fetching process if an embedding is set to be dropped. This result demonstrates that our proposed GST+EFD not only is efficient in terms of memory usage but also reduces training time significantly.

## 6 Related Works

**Graph property prediction.** In the context of graph property prediction, a model must predict a certain characteristic associated with the whole graph. Standard graph neural networks produce node embeddings as outputs [19; 11; 34]. To create a graph embedding (a vector representing the entire graph) out of the node embeddings, pooling methods are usually applied at the end. Common approaches to this problem include simply summing up or averaging all the node embeddings , or introducing a "virtual node" connected to all the other nodes . Fully-connected Graph Transformer was recently proposed with an outstanding success on existing graph property prediction benchmarks [35; 25]. However, the fully-connected attention matrix limits the applicability of Graph Transformer to only small graphs with limited number of nodes .

**GNN with graph partitioning.** ClusterGCN  is designed for node-level tasks by training on graph segments. It partitions a graph into graph segments and randomly selects graph segments to form a minibatch during training. ROC , PipeGCN  and BNS-GCN  achieve distributed node-level GCN training through partitioning a graph into small segments such that each could be fitted into a single GPU memory, and then training multiple segments in parallel. All the above graph partitioning techniques for GNNs rely on the fact that an ego-subgraph (a subgraph centered around a node) contains sufficient information to make a prediction for the centered node. This is not true for graph property prediction tasks where we need to aggregate information from the whole graph to make an accurate prediction.

**GNN with historical embeddings.** The idea of historical embeddings was first introduced in VR-GCN , which uses historical embeddings to control the variance of neighborhood sampling. GNNAutoScale  incorporates historical embeddings to recover a more accurate neighborhood estimation in a scalable fashion. Developed upon GNNAutoScale, Yu et al.  uses a momentum step to incorporate historical embeddings when updating feature representations to further alleviate the staleness issue. These prior works maintain a historical embedding for each node because they consider node-level tasks. As we consider graph property prediction tasks, we record a historical embedding for each graph segment rather than each node.

Conclusion

We study how to train a GNN model for large graph property prediction tasks. We propose Graph Segment Training (GST), a general framework for learning large graph property prediction tasks with a constant memory footprint. We further introduce a historical embedding table to efficiently produce embeddings for graph segments that do not require gradients, and design two novel techniques -- Prediction Head Finetuning and Stale Embedding Dropout -- to mitigate the staleness issue. In conclusion, we believe that our proposed method is a step toward making graph property prediction learning more practical and scalable.