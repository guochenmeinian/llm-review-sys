# Stochastic Kernel Regularisation Improves

Generalisation in Deep Kernel Machines

Edward Milsom

School of Mathematics

University of Bristol

edward.milsom@bristol.ac.uk

Ben Anson

School of Mathematics

University of Bristol

ben.anson@bristol.ac.uk

Laurence Aitchison

School of Engineering Mathematics and Technology

University of Bristol

laurence.aitchison@gmail.com

###### Abstract

Recent work developed convolutional deep kernel machines, achieving 92.7% test accuracy on CIFAR-10 using a ResNet-inspired architecture, which is SOTA for kernel methods. However, this still lags behind neural networks, which easily achieve over 94% test accuracy with similar architectures. In this work we introduce several modifications to improve the convolutional deep kernel machine's generalisation, including stochastic kernel regularisation, which adds noise to the learned Gram matrices during training. The resulting model achieves 94.5% test accuracy on CIFAR-10. This finding has important theoretical and practical implications, as it demonstrates that the ability to perform well on complex tasks like image classification is not unique to neural networks. Instead, other approaches including deep kernel methods can achieve excellent performance on such tasks, as long as they have the capacity to learn representations from data.

## 1 Introduction

Neural network Gaussian Processes (NNGPs) (Lee et al., 2017) are a key theoretical tool in the study of neural networks. When a randomly initialised neural network is taken in the infinite-width limit, it becomes equivalent to a Gaussian process with the NNGP kernel. A large body of work has focused on improving the predictive accuracy of NNGPs (Novak et al., 2018; Garriga-Alonso et al., 2018; Arora et al., 2019; Lee et al., 2020; Li et al., 2019; Shankar et al., 2020; Adlam et al., 2023), but they still fall short of finite neural networks (NNs). One hypothesis is that this is due to the absence of representation learning; the NNGP kernel is a fixed and deterministic function of its inputs (MacKay, 1998; Aitchison, 2020), but finite neural networks fit their top-layer representations to the task; this aspect of learning is considered vital for the success of contemporary deep learning (Bengio et al., 2013; LeCun et al., 2015). Exploring this hypothesis through the development of NNGP-like models equipped with representation learning could deepen our understanding of neural networks. Although there are some theoretical frameworks for representation learning (Dyer and Gur-Ari, 2019; Hanin and Nica, 2019; Aitchison, 2020; Li and Sompolinsky, 2020; Antognini, 2019; Yaida, 2020; Naveh et al., 2020; Zavatone-Veth et al., 2021; Zavatone-Veth and Pehlevan, 2021; Roberts et al., 2021; Naveh and Ringel, 2021; Halverson et al., 2021; Seroussi et al., 2023), they are generally not scalable enough to handle important deep learning datasets like CIFAR-10.

One recent promising proposal in this area is deep kernel machines (DKMs Yang et al., 2023; Milsom et al., 2024). DKMs are an entirely kernel-based method (i.e. there are no weights andno features) that is nonetheless able to learn representations from data. DKMs narrow the gap to DNNs, achieving 92.7% performance on CIFAR-10 Milsom et al., 2024 vs 91.2% (Adam et al., 2023) for traditional infinite-width networks. However, 92.7% is still far from standard DNNs like ResNets, which achieve around 95% performance on CIFAR-10. Here, we narrow this gap further, achieving 94.5% performance on CIFAR-10, by introducing two key modifications. First, we introduce a regularisation scheme inspired by dropout (Srivastava et al., 2014) where we randomly sample positive-definite matrices in place of our learned Gram matrices. We refer to this scheme as stochastic kernel regularisation (SKR). Second, we use single-precision floating point arithmetic to greatly accelerate training, allowing us to train for more epochs under a fixed computational budget. This presents challenges concerning numerical stability, and we develop a number of mitigations to address these.

## 2 Background: Convolutional Deep Kernel Machines

Here, we provide a brief overview of convolutional deep kernel machines. For a more in-depth introduction see Appendix A, or Milsom et al. (2024). Deep kernel machines are a class of supervised learning algorithms that compute a kernel from inputs and transform this kernel through a series of learnable mappings over Gram matrices. We can then perform prediction with the top layer kernel representation using e.g. GP regression/classification. A deep kernel machine is similar in structure to a deep Gaussian process, with the key difference being that instead of features \(^{}^{P N_{}}\) at each layer, where \(P\) is the number of datapoints and \(N_{}\) is the number of features at layer \(\), we work with Gram matrices \(^{}^{P P}\). We define the Gram matrices as the (normalised) dot product between features:

\[^{}=}^{}(^{})^{T}.\] (1)

Many common kernel functions \(()\), such as the arccos (Cho & Saul, 2009) and squared-exponential, depend on features only through their pairwise dot products, and thus can be computed in this reparametrised model as \((^{})\) instead of the usual \((^{})\). To obtain a deep kernel machine, all layers are taken in the infinite-width limit, i.e. \(N_{}\), and the likelihood function is scaled to retain representation learning. Under this limit the Gram matrices, \(\{^{}\}_{=1}^{L}\), which were previously random variables whose distribution we might approximate through variational inference (see "deep kernel processes" Aitchison et al., 2021; Ober & Aitchison, 2021; Ober et al., 2023), become deterministic / point-distributed, and can therefore be treated as learnable parameters. We learn the Gram matrices by optimising the deep kernel machine objective (which is itself derived from the evidence lower-bound (ELBO) for variational inference in the infinite-width limit) using gradient ascent:

\[(^{1},,^{L})=(|^{L})-_{=1}^{L}_{}} ((,^{})\|(,(^{-1}))).\] (2)

Since the computations involved are very similar to Gaussian processes, DKMs naturally scale like \((P^{3})\) with the number of training points. Implementations of shallow Gaussian processes often compute the full kernel matrix using lazy evaluation (Novak et al., 2019; Gardner et al., 2018; Charlier et al., 2021) which avoids storing the entire kernel matrix in memory at once. This process is often very slow (Google's neural tangents library takes 508 GPU hours to compute the Myrtle-10 kernel on CIFAR-10 Google, 2020; Novak et al., 2019), and therefore infeasible for our setting where the model is both deep, and requires thousands of iterations of gradient ascent. Instead, previous work on deep kernel machines utilises sparse variational inducing point schemes, inspired by similar work on deep Gaussian processes (Salimbeni & Deisenroth, 2017). These schemes replace the \(P_{}\) training points with \(P_{}\) pseudo-datapoints, where \(P_{} P_{}\), which leads to \((P_{}^{3}+P_{}^{2}P_{})\) computations which are much cheaper. In deep Gaussian processes, a separate set of \(P_{}^{}\) inducing points is learned for each layer \(\), approximating the input-output functions locally. Deep kernel machines typically use an analogous inducing point scheme, learning an inducing Gram matrix \(_{}^{}^{P_{}^{} P_{ }^{}}\) at each layer (rather than the full Gram matrix for all training points, \(^{}^{P H}\)) by optimising a similar objective to Eq. (2) (see Eq. 27 in Appendix A for further details). Prediction of train/test points is described in Algorithm 1, but at a high level, it is similar to Gaussian process prediction, except instead of working with a feature vector partitioned into inducing points and train/test points, we have a Gram matrix partitioned into 4 blocks,

\[}_{}^{}\\ _{}^{}_{}^{}\\ _{}^{}^{T}=} _{}^{}(_{}^{})^{T}&}_{}^{}(_{}^{})^{T}\\ }_{}^{}(_{}^{}) ^{T}&}_{}^{}(_{}^{ })^{T}=_{}^{}&( _{}^{})^{T}\\ _{}^{}&_{}^{}.\] (3)The goal in prediction is to compute the conditional expectation of \(_{}^{}^{P_{i}^{} P_{i}^{}}\) and \(_{}^{}^{P_{i}^{} P_{i}^{}}\) conditioned on the learned inducing block \(_{}^{}^{P_{i}^{} P_{i}^{}}\).

Convolutional deep kernel machines combine deep kernel machines with convolutional kernel functions from the infinite-width neural network literature / Gaussian process literature (van der Wilk et al., 2017; Dutordoir et al., 2020; Garriga-Alonso et al., 2018; Novak et al., 2018). In these settings, kernel matrices are of size \(PWH PWH\) where \(P\) is the number of training images, and \(W,H\) are the width and height of the images, respectively. In particular, this implies that inducing Gram matrices \(_{}^{}\) are of size \(P_{i}^{}WH P_{i}^{}WH\) which is too expensive even for CIFAR-10 (e.g. \(10\ 32 32\) inducing CIFAR-10 images results in a \(10,240 10,240\) kernel matrix). To avoid this exorbitant computational cost, Milsom et al. (2024) proposed an inducing point scheme where the learned inducing blocks \(_{}^{}\) have no "spatial" dimensions, i.e. they are of size \(P_{i}^{} P_{i}^{}\), whilst the predicted train / test blocks \(_{}^{}^{PWH P_{i}^{}},_{}^{}^{P_{i}^{} PWH},_{}^{}^{PWH PWH}\) retain their spatial dimensions. They use linear maps \(^{}^{DP_{i}^{} P_{i}^{-1}}\) to map the \(P_{i}^{-1}\) non-spatial inducing points into \(P_{i}^{}\) patches with \(D\) pixels, which can be used by the convolutional kernel.

The full prediction algorithm is given in Algorithm 1. In practice, the IID likelihood functions used at the output layer mean only the diagonal of \(_{}^{}\) is needed, dramatically reducing the computation and storage requirements to a vector \(_{}^{}:=(_{}^{ })^{P_{i}^{}WH}\), and only minibatches of the data are required (Yang et al., 2023). The parameters are updated via gradient descent by computing the objective (Equation 27 in Appendix A) using the predictions, and backpropagating.

## 3 Methods

We seek to improve the generalisation of the convolutional deep kernel machine via two main strategies. First, we seek to reduce overfitting of representations by introducing randomness to the learned inducing Gram matrices at train time, replacing them with samples from the Wishart distribution. We refer to this process as "stochastic kernel regularisation" to avoid ambiguity with terms like "random sampling". Second, we improve the numerical stability of the algorithm enough to utilise lower-precision TF32 cores in modern NVIDIA GPUs, which are significantly faster but more prone to round-off errors. Using TF32 cores makes training roughly \(5\) faster than the implementation in Milsom et al. (2024), which used double-precision floating points, and therefore allows us to train for significantly more epochs. Numerical stability is improved through a combination of stochastic kernel regularisation and a Taylor approximation to the problematic log-determinant term in the objective function, which we show has no negative effect on predictive performance in our ablation experiments. We also observed that keeping the regularisation strength \(\) in the DKM objective (Equation 2) non-zero was crucial in preserving numerical stability.

### Stochastic kernel regularisation

Milsom et al. (2024) observed that convolutional deep kernel machines suffer from overfitting. Under the infinite-width limit, the distributions over Gram matrices become point distributions, and offer no stochastic regularising effect. Inspired by dropout in neural networks (Srivastava et al., 2014), we introduce random noise into the training process to reduce overfitting of representations. Specifically, we replace the inducing Gram matrices \(_{}^{}\) at each layer with a sample \(}_{}^{}\) from the Wishart distribution,

\[}_{}^{}(_{}^{ }/,),\] (4)

which has expectation \(_{}^{}\) and variance inversely proportional to \(\). Strictly speaking, the Wishart distribution has support over positive-definite matrices only. This positive-definiteness constraint corresponds to the requirement \( P_{i}^{}\), which in turn upper-bounds the variance of the samples. In highly expressive models with many inducing points, it may be beneficial to have much higher variance samples than this, so we relax the constraint on \(\), leading to potentially singular matrices, and then apply jitter to the samples, i.e. \(}_{}^{}}_{}^{ }+\), to ensure positive-definiteness. Random sampling is disabled at test-time, though we still add the same jitter to eliminate bias between train and test predictions. We refer to this process as stochastic kernel regularisation (SKR).

``` Input: Batch of datapoint inputs: \(_{t}^{P_{i}WH_{0}}\) Output: Distribution over predictions \(_{t}^{*}\) Parameters: Inducing inputs \(_{i}\), inducing Gram matrices \(\{_{ii}^{}\}_{=1}^{L}\), inducing output GP approximate posterior parameters \(_{1},,_{_{L+1}},\) (shared across classes), inducing "mix-up" parameters \(\{^{}\}_{=1}^{L}\), where \(^{}^{DP_{i}^{} P_{i}^{}}\)  Initialize full Gram matrix \(_{ii}^{0}&_{ii}^{0}\\ _{ii}^{0}&_{ii}^{0}=} _{i}_{i}^{T}&_{i}_{i}^{T}\\ _{i}_{i}^{T}&_{i}_{i}^{T}\) for\(\)in\((1,,L)\)do  Apply kernel non-linearity \(()\) (e.g. \(\) kernel)  Apply convolution and "mix up" inducing points (see Appendix A or Milsom et al. (2024)).  Indexing \(d\) represents pixels within a patch, and \((i,r)\) denotes "image / feature map \(i\), spatial location \(r\)" \(_{ii}=_{d}_{d}^{}_{ii} (_{d}^{})^{T}\) \(_{ii(i,r)}=_{d}_{ii(i,r+d)}(_{d}^ {})^{T}\) i.e. \(_{ii}=(_{ii},=^{ })\) \(_{ii(i,r),(j,s)}=_{d}_{ii(i,r+d),(j,s +d)}^{}\) similar to the avg_pool2D operation Apply stochastic kernel regularisation to inducing Gram matrix \(}_{ii}^{}}(_{ii}^{ }/,)+\)  Predict train/test components of Gram matrix conditioned on inducing component \(}_{ii}^{}}\) \(_{ii+i}=_{ii}-_{ii}_{ii}^{-1}_{ii}^{T}\). \(_{ii}^{}=_{ii}_{ii}^{-1}}_{ii} ^{}\) \(_{ii}^{}=_{ii}_{ii}^{-1}}_{ii }^{}\) \(_{ii}^{}=_{ii}_{ii}^{-1}}_{ii }^{}_{ii}^{-1}_{ii}^{T}+_{ii:i}\) endfor  Average over spatial dimension, forming an additive GP (van der Wilk et al., 2017). \((r)\) and \((s)\) index spatial locations in a feature map, and \(S\) is the total number of spatial locations. \(_{ii}^{}=}_{ii}^{L}\) \(_{ii}^{}=_{r}_{ii}^{L}\) \(_{ii}^{}=}_{rs}_{ii(r)}^{L}\)  Final prediction using standard Gaussian process expressions \(_{ii}&_{ii}^{T}\\ _{ii}&_{ii}^{T}=( _{ii}^{}&_{ii}^{})^{T }\\ _{ii}^{}&_{ii}^{})\)  Sample features \(_{}^{i,L+1}\) conditioned on K and inducing outputs \(Q(_{}^{i,L+1})(_{},)\) \(_{}^{i,L+1}(_{ii}_{ii}^{-1 }_{},_{ii}-_{ii}_{ii}^{-1 }_{ii}+_{ii}_{ii}^{-1}_ {ii}^{-1}_{ii})\)  Monte-Carlo average over softmax of features to obtain categorical distribution over classes \(_{t}^{*}=(_{1}^{i,L+1}, ,_{_{L+1}^{i,L+1}}^{i,L+1})\)  Training: Compute DKM objective (Eq. 27) with Taylor approximation (Eq. 8) using true labels \(_{t}\) and backpropagate to update parameters. ```

**Algorithm 1** Convolutional deep kernel machine prediction. Changes from this paper are in red.

### Enabling lower-precision floating point arithmetic

Previous implementations of deep kernel machines (Yang et al., 2023; Milsom et al., 2024) used double-precision floating point arithmetic, which is very slow. Modern GPUs are highly optimised for lower-precision floating point operations. For example, the NVIDIA A100 GPU marketing material (NVIDIA, 2021) quotes 9.7 TFLOPS for FP64 operations, 19.5 TFLOPS for FP32 operations, and 156 TFLOPS for TensorFlowFloat-32 (TF32) operations, a proprietary standard that has the 8-bit exponent (range) of FP32 but the 10-bit mantissa (precision) of FP16, for a total of 19 bits including the sign bit (Kharya, 2020). Therefore, switching from FP64 to TF32 numbers suggests potential speedups of up to \(8\), though in reality speedups will be more modest as not all operations support TF32. Working with kernel methods in low precision arithmetic requires care to ensure numerical stability. For example, direct inversion of the kernel matrix \(_{ii}\) should be avoided, and instead all operationsof the form \(_{}^{-1}\) should instead be computed by factorising \(_{}\) and solving the system \(_{}=\)(Trefethen & Bau, 1997).

Unfortunately, the convolutional deep kernel machine as presented in Milsom et al. (2024) is highly unstable with low-precision arithmetic. Subroutines for the exact computation of Cholesky decompositions fail completely (halting any further progress in training) when large round-off errors accumulate. This problem is particularly acute when dealing with large kernel matrices, which are typically very ill-conditioned. The usual solution is to add jitter to the kernel matrices, but we found this was insufficient when using such low-precision arithmetic (Table 3). Instead, we hypothesised that the problem lay not in the kernel matrices \(\), but rather in the learned inducing Gram matrices \(_{}^{}\). In particular, we observed that the condition number of \(_{}^{}\) tended to worsen over time (Fig. 1), suggesting that learning highly expressive representations led to ill-conditioned Gram matrices.

Though the stochastic kernel regularisation scheme we proposed did result in improved condition numbers during training (Fig. 1), we still observed occasional failures in our large-scale experiments on CIFAR-10 (see ablations in Table 3). We suspected that the issue might be due to the regularisation / KL-divergence terms in Eq. (2). These KL-divergence terms can be written as

\[_{}((,) (,))=( ^{-1})-(^{-1})+.\] (5)

This should be understood as a function, with two arguments, \(\) and \(\). To evaluate the objective (Eq. 2), we would set \(=^{}\), and \(=(^{})\). The KL divergence is problematic in terms of stability for two reasons. Firstly, the log-determinant term is a highly unstable operation, particularly in the backward pass which involves inverting the kernel matrix (Petersen & Pedersen, 2012). Secondly, computing \(^{-1}\) for the trace requires a forward and backward substitution using the cholesky of \(\), which is typically a very ill-conditioned kernel matrix.

To reduce the number of unstable operations, we replaced the log-determinant and trace terms with their second-order Taylor expansions. Since we expect the Gram representations to be close to those of the NNGP, i.e. \(^{-1}\), our Taylor expansions are taken around \(_{i}=1\), where \(_{i}\) is the \(i\)th eigenvalue of \(^{-1}\). In particular, the log-determinant term can be approximated as,

\[-(^{-1}) =(^{-1})\] (6a) \[=_{i}_{i}\] (6b) \[_{i}(_{i}-1)-(_{i}-1)^{2}\] (6c) \[=(^{-1}-)- [(^{-1}-)^{2}].\] (6d)

In the "\(\) " step we have taken the second order Taylor expansion of \((_{i})\) around \(_{i}=1\), and in the final step we have used the fact that the trace of a matrix is equal to the sum of its eigenvalues. Similarly for the trace term we have,

\[(^{-1}) =_{i}}\] (7a) \[_{i}1-(_{i}-1)+(_{i}-1)^{2}\] (7b) \[=-(^{-1}-)+[ (^{-1}-)^{2}]+.\] (7c)

Putting these approximations together we obtain,

\[_{}((,) (,)) [(^{-1}-)^{2}]+ =^{-1}-_{}^ {2}+,\] (8)

where \(_{}\) is the Frobenius norm. Computing \(^{-1}\) should be more stable than \(^{-1}\) since the inverse is only backpropagated to the learnt cholesky of \(\), rather than through \(\) to earlier parts of the model, avoiding further compounding of round-off errors.

## 4 Experiments

### Image classification experiments

We evaluated our method on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), containing \(60,000\) RGB images (\(50,000\) train, \(10,000\) test) of size \(32 32\) divided into 10 classes. We use the same architecture as in Milsom et al. (2024) for ease of comparison, i.e. a ResNet20-inspired architecture with an extra size-2 stride in the first block, so that the output feature map sizes of the 3 blocks are \(\{16,8,4\}\) respectively. Wherever the ResNet architecture contains a convolution layer, we use a convolutional deep kernel machine layer as described in the loop in Algorithm 1. That is, we apply a base kernel (in our case, the normalised Gaussian kernel described in Shankar et al. (2020), a more efficient / numerically stable alternative to arccos kernels), perform the (kernel) convolution, and then predict the train/test Gram matrix blocks conditioned on the kernel and the inducing Gram matrix block. In place of batch norm we use the batch kernel normalisation approach suggested by Milsom et al. (2024). Skip connections compute convex combinations of the kernel before and after pairs of layers. At the final layer, we average over the remaining spatial locations, forming an additive GP kernel akin to convolutional GPs (van der Wilk et al., 2017) that is used to make the final predictions. A categorical likelihood function is used in the top-layer GP. Since the number of inducing points can vary between layers, we use \(\{512,1024,2048\}\) inducing points in the three blocks of convolutions, respectively, giving more expressive power to the later layers (similar to how ResNets are wider in later layers). For the stochastic kernel regularisation, we used \(=P_{t}^{t}/4\) and a jitter size of \(=0.1\), and for the objective we used a regularisation strength of \(=0.001\). We train all parameters by optimising the sparse DKM objective function (Equation 27 with Taylor approximated terms from Section 3.2) using Adam (Kingma & Ba, 2017), with \(_{1}=0.8,\ _{2}=0.9\) and with an initial learning rate of 0.01 which is divided by 10 at epochs 800 and 1100, for a total of 1200 epochs. The model is implemented1 in PyTorch (Paszke et al., 2019).

We also train a neural network with the same architecture for comparison, using a modified version of the popular "pytorch-cifar" GitHub repository2. In the interest of fair comparison, we use network widths of \(\{512,1024,2048\}\) in the three blocks so that the model has a comparable number of parameters to the convolutional deep kernel machine. The neural network was trained for 1200 epochs, and we report results using two different optimisers. One model used Adam with an initial learning rate of 0.001 and the same learning rate scheduling as the convolutional deep kernel machine, and the other used SGD with a momentum term of 0.9, a weight decay strength of 0.0005, an initial learning rate of 0.1 and a cosine annealing learning rate scheduler. We ran all experiments with 4 random seeds, and report the results in Table 1 with 1 standard error of the mean, assuming normally distributed errors for statistical tests. On an NVIDIA A100 with TF32 matruls and convolutions enabled, the Adam-trained neural network takes \(\)45s per epoch, whilst our model takes \(\)260s per epoch. We estimate (very roughly) a total time, including the ablations and CIFAR-100 experiments detailed later, of around 2000 GPU hours for all experiments in this paper, and around 2-3 times that number when including preliminary and failed experiments during the entire project.

The deep kernel machine matches the Adam-trained neural network with a mean test accuracy of \(94.52\%\) compared to \(94.55\%\) for the neural network (two-tailed t-test with unequal variances gives a p-value of 0.7634, suggesting no significant difference). Furthermore, the deep kernel machine provides better uncertainty quantification as measured by (mean) log-likelihood on the test data (higher is better), with an average of \(-0.3611\) compared to the Adam-trained neural network's \(-1.3003\). Our model also far surpasses the convolutional deep kernel machine presented in Milsom et al. (2024). However, all these models still lag behind the SGD-trained network, which achieves a higher test accuracy of \(95.36\%\) (p-value of 0.0001 when compared to our model) and higher test

  Method & Test Accuracy (\(\%\)) & Test Log-Likelihood \\  Conv. Deep Kernel Machine (This Paper) & \(94.52 0.0693\) & \(-0.3611 0.0073\) \\ Conv. Deep Kernel Machine (Milsom et al., 2024) & \(92.69 0.0600\) & \(-0.6502 0.0125\) \\  Tuned Myrtle10 Kernel DA CG (Adlam et al., 2023) & \(91.2\) & - \\ NNGP-LAP-flip (Li et al., 2019) & \(88.92\) & - \\  Neural Network (Adam) & \(94.55 0.0361\) & \(-1.3003 0.0226\) \\ Neural Network (SGD + Weight Decay) & \(95.36 0.0523\) & \(-0.2112 0.0037\) \\  

Table 1: Test metrics on CIFAR-10 using a DKM and a neural network with the same architecture. We report means and 1 standard error over 4 random seeds.

log-likelihood of \(-0.2112\) (p-value 0.00002 when compared to our model). SGD is well known to train neural networks with better generalisation properties, and in particular for ResNets (Zhou et al., 2021; Keskar and Socher, 2017; Gupta et al., 2021), so this is perhaps not too surprising. We briefly experimented with using SGD to optimise the deep kernel machine but found it generally less stable than Adam. We hypothesise this is because the deep kernel machine has many different "types" of parameters to optimise, as seen in Algorithm 1, which may benefit from different optmisation strategies, whilst the neural network only has weights and a few batchnorm parameters to optimise.

We further evaluated our method on the CIFAR-100 dataset (Krizhevsky and Hinton, 2009), with results being presented in Table 2. As in CIFAR-10, we found significant improvements over previous deep kernel machine work (Milson et al., 2024), and we found our method is competitive with a ResNet trained with Adam, but still lags behind a ResNet trained with SGD, which is known to perform excellently on these tasks (Zhou et al., 2021; Keskar and Socher, 2017; Gupta et al., 2021). Note that we additionally had to use weight decay and a cosine annealing learning rate schedule with the Adam-trained ResNet to obtain acceptable performance on CIFAR-100.

To further investigate the effects of our changes, we ran a series of ablation experiments that are presented in Table 3. We report test accuracies and test log-likelihoods, but also the number of times each ablation failed out of the 4 random seeds as a proxy for numerical stability. Our experiments verified that stochastic kernel regularisation (SKR) did yield a statistically significant improvement in test accuracy (p-value 0.0009). To verify that the improvement was in fact coming from the random sampling of matrices and not an implicit regularising effect of the large amount of jitter, we tested the model with SKR disabled but still applying the jitter \(\). We found that performance was still far worse than with SKR enabled; only 1 seed ran to completion without a numerical error for this setting, so we cannot compute the standard deviation necessary for the t-test, but based on the other experiments it is very unlikely the variance would be high enough for this not to be statistically significantly lower than our headline number. Furthermore, our Taylor approximation in the objective function did not harm performance. In fact, on log-likelihoods we obtain a p-value of 0.02953, suggesting a statistically significant improvement when using our Taylor approximated objective, but we believe this would require further investigation to verify. We also tested training with only 200 epochs, scheduling down the learning rate at epochs 160 and 180, and found that training for a 1200 epochs did indeed give a substantial boost to test accuracy. We found no single trick was enough to ensure stability over all our training runs, but rather a combination of our proposed modifications was necessary. We provide some brief analysis of the stability of the learned Gram matrices in the next section.

   Method & Test Accuracy (\(\%\)) & Test Log-Likelihood \\  Conv. Deep Kernel Machine (This Paper) & \(75.31 0.0814\) & \(-1.4652 0.0183\) \\ Conv. Deep Kernel Machine (Milson et al., 2024) & \(72.05 0.2300\) & \(-2.0553 0.0207\) \\  Neural Network (AdamW) & \(74.13 0.0442\) & \(-1.9183 0.0070\) \\ Neural Network (SGD + Weight Decay) & \(79.42 0.0380\) & \(-0.8890 0.0021\) \\   

Table 2: Test metrics on CIFAR-100 using a DKM and a neural network with the same architecture. We report means and 1 standard error over 4 random seeds.

   Ablation & Test Accuracy & Test Log-Likelihood & Failures \\  No ablation/ Our full method & \(94.52 0.0693\) & \(-0.3611 0.0073\) & 0/4 \\  No Taylor approximation to objective & \(94.46 0.0406\) & \(-0.3951 0.0081\) & 1/4 \\ No SKR & \(93.71 0.0150\) & \(-0.4512 0.0168\) & 2/4 \\ No Taylor + No SKR & \(93.25\) (1 run) & \(-0.5113\) (1 run) & 3/4 \\ No SKR but keep \(=0.1\) jitter & \(93.46\) (1 run) & \(-0.4762\) (1 run) & 3/4 \\ \(_{}=0\) (Eq. 2) &  &  \\
200 epochs & \(93.45 0.0225\) & \(-0.2607 0.0016\) & 0/4 \\   

Table 3: Test metrics on CIFAR-10 with different ablations applied to our headline model (Table 1). We report means and 1 standard error over the random seeds that ran to completion. Failures indicates how many of the 4 random seed runs for each setting resulted in a numerical error.

### Effects of regularisation on Gram matrix condition number

To further investigate the numerical stability of our model, we ran a 1 layer deep kernel machine with the squared exponential kernel and 100 inducing points on a toy binary classification problem. We show the condition number of the learned Gram matrix at each epoch for various values of the SKR parameter \(\) (left, Fig. 1), the DKM objective regularisation strength \(\) when using the Taylor KL divergence terms (middle, Fig. 1), and \(\) when using the true KL divergence (right, Fig. 1). For the plot varying \(\), we used \(=0\), so that the effect of the Taylor approximation to the KL terms is irrelevant. Note that \(=\) refers to no SKR. We ran these experiments in double precision to avoid numerical errors, and used the Adam optimiser with learning rate fixed at 0.01. These experiments took about 1 CPU hour in total.

We observe that more variance (smaller \(\)) in stochastic kernel regularisation slows down the rate at which the condition number of \(_{}\) worsens. This makes sense, as the noise we add to the Gram matrix makes it more difficult to learn the "optimal" Gram matrix for the data, which would likely be highly overfitted, leading to extreme eigenvalues. However, running the experiment for long enough eventually leads to the same condition number for all settings of \(\) (see Fig 2 in Appendix B). We may expect this behaviour since the expected value of the SKR samples matches the true Gram matrix, but it's not clear how effectively the optimiser could achieve this outside of simple toy examples.

It is clear that, when using our proposed Taylor approximation to the KL divergence terms in the objective, even tiny values for the strength \(\) of these terms result in learned Gram matrices with condition numbers orders of magnitude better than without, and this effect grows proportionally with \(\). We also see an improvement in condition number when not using the Taylor approximation to the KL divergence, but only for large \(\). Setting \(\) too large tends to harm generalisation (Milson et al., 2024), so it is beneficial to use the Taylor approximation which reduces condition numbers even for small \(\). We also observed that the minimum condition number achieved across all settings of \(\) was a few orders of magnitude lower when using the Taylor approximation vs. when using the true KL divergence. Furthermore, the behaviour in the plot using the true KL divergence is rather erratic. For example, the \(=10^{0}\) curve (right, Fig. 1) initially rises to very poor condition numbers, but after a few hundred epochs rapidly drops to a much smaller condition number. This leads us to believe that the difference between these two schemes can be explained by optimisation.

Our Taylor approximated terms penalise the Frobenius norm \(|-|_{}^{2}\), a much simpler operation than the true KL divergence terms which penalise \((^{-1})-( ^{-1})\). This complex penalty may result in a difficult optimisation landscape in practice.

Figure 1: Effects of different regularisation methods on Gram matrix condition number, in the toy binary classification problem trained for 2000 epochs. The left plot shows the condition numbers when different amounts of stochastic kernel regularisation (\(\)) are applied. The middle and right plots show the condition numbers when the coefficient \(\) of the KL regularisation terms are varied, with and without a Taylor approximation, respectively.

Related Work

There is a substantial body of literature attempting to push the performance of kernel methods to new heights. These methods can broadly be split into "kernel learning" and "fixed kernel" methods.

Deep kernel machines, already extensively discussed in this paper, fall into the former category, as does the area of "deep kernel learning" (e.g. Wilson et al., 2016; Achituve et al., 2023; Ober et al., 2021, to name a few). In deep kernel learning, a neural network is used to produce rich features which are then passed as inputs into a traditional "shallow" kernel machine, aiming to give the best of both deep learning and kernel methods. Another "kernel learning" method is the "convolutional kernel machine" (Mairal et al., 2014; Mairal, 2016), which draw theoretical connections between kernel methods and neural networks, though the resulting model is fundamentally a neural-network-like architecture based on features, which distinguishes it from deep kernel machines. Song et al. (2017) also utilised deep neural networks to generate task-specific representations in a more complex model involving an ensemble of RKHS subspace projections. The main difference between deep kernel machines and these other methods is that deep kernel machines do not involve neural networks at any stage; the representations are learned directly as Gram matrices, not features.

By contrast, "fixed kernel" methods do not perform any representation learning during training, instead fixing their feature space before data is seen via the choice of kernel function. Though this could cover practically the entire field of kernel methods, the best performing methods on image tasks typically utilise kernels derived from the infinite-width neural network literature (Lee et al., 2017; Jacot et al., 2018; Lee et al., 2020), sometimes called "neural kernels" (Shankar et al., 2020). In particular, Adam et al. (2023) pushed the state of the art for CIFAR-10 test accuracy with "fixed kernels" to 91.2%, using Myrtle kernels (Shankar et al., 2020), a type of neural kernel, by massively scaling up their method with distributed preconditioned conjugate gradient methods. Apart from the obvious lack of representation learning in this work, another key difference from our work is that they focus on computing large full-rank kernel matrices and finding approximate solutions using iterative solvers, whilst we use sparse inducing point approximations resulting in smaller kernel matrices, which we then solve exactly.

Deep kernel machines can be viewed as an infinite-width limit of deep kernel processes (Yang et al., 2023) or deep Gaussian processes (Damianou and Lawrence, 2013) with a modified likelihood function, which results in the Gram matrices having point distributions. This can lead to overfitting. In deep Gaussian processes and deep kernel processes, the representations (Gram matrices in the case of deep kernel processes) have continuous distributions with broad support, thereby offering a regularising effect. Our stochastic kernel regularisation scheme can be seen as analogous to sampling the inducing Gram matrix \(_{}^{}\) in a deep kernel process. Unlike a deep kernel process, the other blocks \(_{}^{}\) and \(_{}^{}\) in our model remain deterministic, simplifying the model implementation. Other approaches to regularising kernel methods include "kernel dropout" proposed by Song et al. (2017), though in their context dropout refers to randomly removing latent representations from their ensemble during training. This is therefore very different to our setting. In the neural kernel literature, Lee et al. (2020) identified a correspondence between diagonal regularisation of kernels (jitter) and early stopping in neural networks, and found this usually improved generalisation. In this paper, we focused on regularising the learned intermediate representations / Gram matrices, rather than the final kernel, and found that diagonal regularisation had little effect on generalisation when applied to these matrices.

Previous work has attempted to improve numerical stability in kernel methods, though using different approaches. For example, Maddox et al. (2022) developed strategies to ensure numerical stability when using conjugate gradient solvers for GPs with low-precision arithmetic, but we do not use numerical solvers in this paper. van der Wilk et al. (2020) circumvent the issue of computing inverses entirely using an approach based on a reparametrisation of the variational parameters, but applying such an approach to the deep kernel machine domain would be a substantial undertaking, which we leave to future work.

## 6 Limitations

Though we have considerably advanced the state-of-the-art for kernel methods, from 92.7% (Milsom et al., 2024) to 94.5% in CIFAR-10, there still remains a gap to the best performing neural networks, both in terms of accuracy, and in terms of runtime. Nonetheless, given that we have shown that representation learning in kernel methods has dramatically improved performance in kernel methods, from 91.2% (Adlam et al., 2023) to 94.5%, it is becoming increasingly likely that representation learning really is the key reason that NNGPs underperform DNNs. We leave further narrowing or even closing the remaining gap to DNNs for future work.

Constraints on computational resources meant that we could only run a limited number of experiments, so we focused on providing concrete insights on a single dataset with a series of ablations, rather than performance metrics for multiple datasets with no further analysis. Nevertheless, we provide all the code necessary to run these experiments on other datasets.

## 7 Conclusion

In this paper we have increased the kernel SOTA for CIFAR-10 to \(94.5\%\) test accuracy using deep kernel machines, considerably higher than the previous record of \(92.7\%\)(Milsom et al., 2024), and significantly higher than NNGP-based approaches, such as the \(91.2\%\) achieved by Adam et al. (2023). We achieved this by developing a novel regularisation method, stochastic kernel regularisation, and by exploiting modern GPU hardware with lower-precision arithmetic, which required us to improve the numerical stability of the algorithm via a multi-faceted approach. We have highlighted the important role that representation learning plays in deep learning, which is unfortunately absent from NNGP-based theory. We hope this work will encourage more research into theoretical models with representation learning.

## 8 Acknowledgements

Edward Milsom and Ben Anson are funded by the Engineering and Physical Sciences Research Council via the COMPASS Centre for Doctoral Training at the University of Bristol. This work was carried out using the computational facilities of the Advanced Computing Research Centre, University of Bristol - http://www.bris.ac.uk/acrc/. We would like to thank Dr. Stewart for GPU compute resources.