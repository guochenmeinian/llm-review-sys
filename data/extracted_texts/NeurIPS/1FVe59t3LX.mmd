# DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs

Jiasheng Zhang\({}^{1}\)  Jialin Chen\({}^{2}\)  Menglin Yang\({}^{2}\)  Aosong Feng\({}^{2}\)  Shuang Liang\({}^{1}\)  Jie Shao\({}^{1,3}\)1  Rex Ying\({}^{2}\)

\({}^{1}\)University of Electronic Science and Technology of China \({}^{2}\)Yale University

\({}^{3}\)Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China

zjss12358@std.uestc.edu.cn {shuangliang, shaojie}@uestc.edu.cn

{jialin.chen, menglin.yang, aosong.feng, rex.ying}@yale.edu

###### Abstract

Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce **D**ynamic **T**ext-attributed **G**raph **B**enchmark (**DTGB**), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DytAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB.

## 1 Introduction

Dynamic graphs are an essential tool for modeling a wide range of real-world systems, such as e-commerce platforms, social networks, and knowledge graphs [1; 2; 3; 4; 5; 6; 7]. In those dynamic graphs, nodes and edges are typically associated with text attributes, giving rise to dynamic text-attributed graphs (DyTAGs). For example, e-commerce graphs may contain items accompanied by textual descriptions, and time-annotated edges representing user reviews or interactions with items. Similarly, temporal knowledge graphs represent the sequential interactions among real-world entities through textural relations. The exploration of learning methodologies applied to DyTAGs is important to research areas such as dynamic graph modeling and natural language processing [8; 9; 10], as well as various real-world applications, _e.g.,_ recommendation and social analysis [3; 11; 12; 13; 14].

While previous works for dynamic graph learning have proposed many datasets describing the temporal interactions across different domains [15; 16; 9; 17], these datasets often lack edge categories and only contain statistical features derived from raw attributes, lacking the raw text descriptions of nodes and edges. Therefore, they fall short in facilitating methodological advances in semantic modeling within dynamic graphs and exploring the impact of text attributes on downstream tasks. Concurrently, text-attributed graphs (TAGs) are widely used in many real-world scenarios [18; 19; 20; 21]. The recently proposed CS-TAG benchmark dataset  with rich raw text aims to facilitate research in TAG analysis. However, these datasets oversimplify the evolving interactions in the real world by representing them as static graphs, ignoring the inherent temporal information present in real-world TAG scenarios, such as citation networks with timestamped publications and social networks with chronological user interactions. Consequently, there is an urgent need for benchmark datasets that can accurately capture both dynamic graph structures and rich text attributes of DyTAGs.

**Proposed Work.** To address this gap, we introduce a comprehensive benchmark DTGB, which comprises eight large-scale DyTAGs sourced from diverse domains including e-commerce, social networks, multi-round dialogue, and knowledge graphs. Nodes and edges in the DTGB dataset are associated with rich text descriptions and edges are annotated with meaningful categories. The dataset construction involves a meticulous process, including the selection of data sources, the construction of the graph structure, and the extraction of text and category information (detailed in Section 3). Compared with existing datasets [22; 23; 15; 24], which either lack raw text and temporal annotations or are small in scale and devoid of long-term dynamic structures, DTGB distinguishes itself with its _rich text_, _long-range historical information_, and _large-scale dynamic structures_, ensuring diverse and representative samples of real-world DyTAG scenarios.

With DTGB, we design four critical downstream evaluation tasks based on real-world use cases, as shown in Figure 1. Except for the widely studied future link prediction task, we delve deeper into three more interesting and challenging tasks: destination node retrieval, edge classification, and textural relation generation, which are neglected by previous works. These tasks are fundamental to real-world applications that require the model to handle the temporal evolution of both graph structure and textual information. Our benchmarking results on 7 dynamic graph learning algorithms and 6 LLMs highlight these challenges and showcase the performance and limitations of current algorithms (_e.g.,_ the scalability issue of memory-based models, neglect of the edge modeling, and weakness in capturing long-range and semantic relevance), offering valuable insights into integrating structural and textual dynamics. Our **contributions** are summarized as follows:

* **First DyTAG Benchmark.** To the best of our knowledge, DTGB is the first open benchmark specifically designed for dynamic text-attributed graphs. We collect eight DyTAG datasets from a wide range of domains and organize them in a unified structure, providing a comprehensive testbed for model evaluation in this area.
* **Standardized Evaluation Protocol.** We design four critical downstream tasks and standardize the evaluation process with DTGB. This comprehensive evaluation highlights the unique challenges posed by DyTAGs and demonstrates the utility of the dataset for assessing algorithm performance.
* **Empirical Observation.** Our experimental results demonstrate that rich textural information consistently enhances downstream graph learning, such as destination node retrieval and

Figure 1: Dynamic text-attributed graph and evaluation tasks: a case study with movie reviews. Given a DyTAG with interactions before timestamp \(T\), the tasks are to forecast future interactions in \(t>T\), as well as their detailed interaction types and textual descriptions.

edge classification. This contributes to a deeper understanding of the complexities involved in DyTAGs and offers guidance for future research in this field.

## 2 Related Works

**Dynamic Graph Learning**. Deep learning on dynamic graphs has gained significant attention in various domains such as social networks, transportation systems, and biological networks [25; 26; 27; 28; 29]. Previous works have proposed a few real-world datasets [9; 16; 30; 31; 32], which offer a comprehensive collection of temporal interaction data across different contexts. Benchmark frameworks [33; 34; 8; 35], such as DyGLib , have been instrumental in standardizing the evaluation of dynamic graph models, offering robust metrics for assessing downstream performance. Recent advancements in temporal graph models have significantly enhanced the ability to capture time-evolving relationships in graph-structured data [36; 37; 38; 39; 40; 41; 42; 17], leading to state-of-the-art performance in various tasks such as dynamic link prediction and node classification on temporal graphs. However, existing temporal graph datasets may lack node or edge attributes, or contain simple node/edge features based on bag-of-words  or word2vec  algorithms derived from the associated text, which are limited in capturing the complicated semantics of the text. In this work, we focus on dynamic graphs where nodes and edges are associated with raw textual descriptions, enabling richer, context-aware representations and more sophisticated downstream tasks.

**Text-attributed Graph Learning**. Text-attributed graphs (TAG) are widely used in various real-world applications. For example, in citation networks, the text associated with each article provides valuable information such as abstracts and titles [45; 18; 46]. CS-TAG  offers standardized datasets with raw text, facilitating research and methodological advances in TAG analysis. Recently, large language models (LLMs) have demonstrated remarkable capabilities in enhancing feature encoding and node classification on TAGs [47; 48; 49]. By flattening graph structures and associated textual information into prompts [50; 10; 51], LLMs can leverage their strong language understanding and generation abilities to improve TAG analysis tasks. However, all these TAGs eliminate the temporal information within the graphs, which is inherent and crucial in real-world scenarios. There has been limited exploration of temporal relations in TAGs, which represents a missed opportunity to evaluate the temporal awareness and reasoning ability of LLMs.

## 3 Dataset Details

**Motivation of DTGB**. To investigate the necessity of a comprehensive benchmark dataset for dynamic text-attributed graphs, we first survey various dynamic graphs and text-attributed graphs previously utilized in the literature. We observed that most commonly used dynamic graphs are essentially text-attributed. Simultaneously, many popular text-attributed graphs have inherent temporal information. For instance, the well-known dynamic graph dataset tgbl-review  and the commonly used TAG dataset Books-Children  are both derived from the Amazon product review network  which is intrinsically associated with both the text attributes of users and products and the time annotations of user-item interactions. However, tgbl-review only contains numerical attributes derived from the raw text and Books-Children ignores all temporal information, which highly limits the full exploration of the performance for downstream tasks.

While these previous datasets are frequently used, they possess obvious inadequacies when exploring the effectiveness of dynamic graph learning methods in handling real-world scenarios. Firstly, existing dynamic graph datasets lack the availability of raw textual information, bringing challenges to investigating the benefits of text attribute modeling on real-world applications. Secondly, most existing dynamic graph datasets lack reasonable temporal segmentation and aggregation, making their edges distribution quite sparse in the time dimension (_e.g.,_ MOOC and tgbl-wiki). This brings challenges to investigating the structure dependency and evolution for dynamic graphs. Lastly, although TAGs are enriched with node text attributes, they tend to miss edge text and time annotations, making them fail to faithfully reflect the challenges in modeling real-world scenarios.

**Dataset Construction**. To address these challenges, we collect resources from different domains and follow a rigorous process to construct the comprehensive benchmark dataset DTGB for DyTAGs. We carefully select data sources from various domains to ensure diversity and relevance. For graph construction, the redundant and low-quality records are first filtered out and we divide the data into discrete time intervals. In each time interval, nodes and edges are flexibly identified for different domains. Nodes could represent users, products, questions, _etc._ while edges represent relationships such as transactions and reviews. For text and category extraction, we organize multiple text descriptions from the source data and remove the meaningless or garbled characters and low-quality text. We categorize edges based on predefined criteria relevant to real-world use cases such as product ratings and content topics. This process ensures that the dataset accurately reflects the complexity of real-world DyTAG. Taking Googlemap CT and Amazon movies as an example, which are extracted from Recommender Systems and Personalization Datasets . Nodes represent users or items, while edges indicate review relation between users and items. The original data is first reduced to a \(k\)-core subgraph, indicating that each user or item has at least \(k\) reviews. Edges are segmented by days and edges within each day are aggregated as a subgraph. Edge categories are integers from 1 to 5, derived from the ratings from users to items. Node text includes the basic information of the item (_e.g.,_ name, description, and category). Edge text is the raw review from users. Detailed descriptions of all the datasets can be found in Appendix A.1.

**Distribution and Statistics**. Table 1 gives the statistics of previous datasets and DTGB datasets. One can notice that compared with previous dynamic graph datasets, our datasets are characterized by edge categories and text attributes at both node and edge levels. Our dataset includes small, medium, and large graphs with various distributions from four different domains, encompassing both bipartite and non-bipartite, long-range and short-range dynamic graphs. We further study the data distribution to better understand our benchmark datasets. As shown in Figure 2 and Figure 3, datasets from the same domain exhibit similar distributions. For example, knowledge graph datasets GDELT and ICEWS1819 both approximate Gaussian distributions in edge text length, and e-commerce datasets Googlemap CT, Amazon movies, and Yelp show long-tail distributions in the number of edges per timestamp. This demonstrates that our datasets have faithfully preserved the characteristics of data from different domains. More detailed analysis of our datasets can be found in Appendix A.2.

    & **Dataset** & Nodes & Edges & Edge Categories & Timestamps & Domain & Text Attributes & Bipartite Graph \\   & **tgb-trable** & 255 & 468,245 & N.A. & 32 & Trade & ✗ & ✗ \\  & **tgb-wiki** & 9227 & 157,474 & N.A. & 152,757 & Interaction & ✗ & ✗ \\  & **tgb-review** & 352,637 & 4,873,540 & N.A. & 6,865 & E-commerce & ✗ & ✓ \\  & **tgb-MOO** & 7,144 & 411,750 & N.A. & 355,600 & Interaction & ✗ & ✓ \\  & **tasfM** & 1,980 & 1,293,103 & N.A. & 1,283,614 & Interaction & ✗ & ✓ \\   & **oghn-arxiv-TA** & 169,343 & 1,166,243 & N.A. & N.A. & Academic & Node & ✗ \\  & **ClatinVis** & 106,759 & 6,120,897 & N.A. & N.A. & Academic & Node & ✗ \\  & **Bocks-Childmen** & 76,875 & 1,554,578 & N.A. & N.A. & E-commerce & Node & ✗ \\  & **Ek-Commerce** & 87,229 & 721,081 & N.A. & N.A. & E-commerce & Node & ✗ \\  & **Sports-Finness** & 173,055 & 1,73,500 & N.A. & N.A. & E-commerce & Node & ✗ \\   & **Euron** & 42,711 & 797,907 & 10 & 1,006 & E-mail & Node \& Edge & ✗ \\  & **GDELT** & 6,786 & 1,339,245 & 237 & 2,591 & Knowledge graph & Node \& Edge & ✗ \\  & **ICICWS19** & 31,976 & 1,100,701 & 266 & 730 & Knowledge graph & Node \& Edge & ✗ \\  & **Stack dec** & 397,702 & 1,262,225 & 2 & 5,224 & Multi-round dialogue & Node \& Edge & ✓ \\  & **Stack ubuntu** & 674,248 & 1,497,060 & 2 & 4,972 & Multi-round dialogue & Node \& Edge & ✓ \\  & **Googlemap CT** & 111,168 & 1,380,623 & 5 & 55,521 & E-commerce & Node \& Edge & ✓ \\  & **Amazon movies** & 293,566 & 3,217,324 & 5 & 7,287 & E-commerce & Node \& Edge & ✓ \\  & **Yelp** & 2,138,242 & 6,990,189 & 5 & 6,036 & E-commerce & Node \& Edge & ✓ \\   

Table 1: Statistics of datasets and comparison with existing datasets.

Figure 2: Distribution of edge text lengths on the DTGB datasets.

## 4 Formulation and Benchmarking Tasks on DyTAG

**DyTAG Formulation**. A DyTAG can be defined as \(=\{,\}\), where \(\) is the node set, \(\) is the edge set. Let \(\) denote the set of observed timestamps, \(\), \(\) and \(\) are the set of node text descriptions, edge text descriptions, and edge categories, respectively. Each \(v\) is associated with a text description \(d_{v}\). Each \((u,v)\) can be represented as \((r_{u,v},l_{u,v},t_{u,v})\) with a text description \(r_{u,v}\), a category \(l_{u,v}\) and a timestamp \(t_{u,v}\) to indicate the occurring time of this edge. We use \(_{T}=\{_{T},_{T}\}\) to represent the DyTAG containing interactions occurred before timestamp \(T\). We summarize the important notations used in Appendix B.

**Future Link Prediction**. Future link prediction is commonly used in previous literature [16; 8; 9] to evaluate the performance of dynamic graph learning methods, which aims to predict whether two nodes will be linked in the future given the history edges. In dynamic text-attributed graphs, the new linkage between nodes not only depends on their static semantics brought by node text but also on the interaction context brought by edge text semantics. The future link prediction task in the DyTAG setting can be viewed as the simplification of many real-world applications, _e.g._, predicting whether one person will e-mail another based on the content of their history e-mails. Formally, given the DyTAG \(_{T}\) which contains edges before timestamp \(T\), future link prediction aims to predict whether an interaction will happen between nodes \(u\) and \(v\) at timestamp \(T+1\). In the inductive setting, either \(u\) or \(v\) are new nodes not contained in \(_{T}\).

**Destination Node Retrieval**. While future link prediction has been widely used, this task has several limitations for a reliable evaluation. Its performance largely depends on the quality and the size of the sampled negative samples, and the binary classification metrics are sensitive to the model fluctuations. Destination node retrieval is a novel task that aims to rank the most likely interact nodes for a given node based on its interaction history. This approach is more stable as it considers the relative relevance among the entire node set and is more applicable to real-world scenarios (_e.g._, personalized recommendation). Formally, given node \(u\) and \(_{T}\), node retrieval aims to rank the nodes in \(_{T}\) based on their possibilities of interacting with \(u\) in timestamp \(T+1\). In the inductive setting, \(u\) is a new node not contained in \(_{T}\).

**Edge Classification**. Edge classification is an essential evaluation task for DyTAG, which is under-explored by previous dynamic graph benchmarks [16; 8; 9]. The dynamic graph learning models leverage both rich textual information and historical interactions to predict the categories of relations between them (_e.g._, the review rating in the future). Formally, given a DyTAG \(_{T}\) with edges up to timestamp \(T\), edge classification aims to predict the category of a potential edge at timestamp \(T+1\), utilizing both node and edge textual attributes and historical interactions.

**Textural Relation Generation**. Textural relation generation is a novel task that seeks to leverage historical interactions and their associated text to generate future relation context. While previous studies on TAGs [22; 48] have mainly focused on graph structure learning, such as predicting new edges or node labels, generating the actual textual content for future edges remains an under-explored challenge. To address this gap, large language models (LLMs) are employed as backbones, due to their powerful capability in understanding and generating natural language. Specifically, given two nodes \(u\) and \(v\) for which we aim to predict their future textual interaction, we provide the LLM with their node descriptions as well as the historical one-hop interactions involving either \(u\) or \(v\). We then

Figure 3: Distribution of the numbers of edges in each timestamp on the DTGB datasets.

prompt the LLM to generate the predicted interaction text in an autoregressive manner. This task not only serves as a challenging benchmark for evaluating LLMs' ability to understand the co-evolution of graph structures and natural language, but also holds promise for enhancing LLMs' representations by incorporating the inductive biases of structured data during pretraining in the future.

## 5 Experiments

**Baselines**. (1) For the future link prediction, destination node retrieval, and edge classification tasks, we use the dynamic graph learning models as the baselines. We evaluate 7 popular and state-of-the-art models: JODIE , DyRep , TGAT , CAWN , TCL , GraphMixer  and DyGFormer . (2) For the textual relation generation task, we benchmark four open-source large language models: Mistral 7B , Vicuna 7B/13B , Llama-3 8B , and two closed-source large language models GPT3.5-turbo and the most recent GPT4o through API service. Refer to Appendix C.1 for more details.

**Evaluation Metrics**. For the edge classification task, we use Weighted Precision, Weighted Recall, and Weighted F1 score to evaluate the model performance. For the future link prediction task, we follow previous works [16; 8] and adopt the Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) as the evaluation metrics. For the node retrieval task, we use Hits@\(k\) as the metric, which reports whether the correct item appears within the top-\(k\) results generated by the model. To evaluate the generated textual relation, we use BERTScore , which leverages a pre-trained language model and calculates the cosine similarity between the prediction and ground truth. The detailed definitions are provided in Appendix C.2.

**Experimental Settings**. For dynamic graph learning models, we follow the implementations from DyGLib1. All data loading, training, and evaluation processes are performed uniformly, following DyGLib. To integrate the textual information, we use the Bert-base-uncased model  to encode the node and edge texts as the initialization of the node and edge representations. We chronologically split each dataset into train/validation/test sets by 7:1.5:1.5. For the textual relation generation task, open-source LLMs are implemented with Huggingface . We also use the parameter-efficient fine-tuning method, LoRA , to fine-tune the \(\), \(\), \(\), and \(\) matrices within LLM for better text generation. We run all the models five times with different seeds and report the average performance to eliminate deviations. Experiments are conducted on NVIDIA A40 with 48 GB memory.

**Implementation Details of Dynamic Graph Models**. For all of the edge classification task, future link prediction task, and destination node retrieval task, we use Adam  for optimization. All the models are trained for 500 epochs and use the early stopping strategy with a patience of 5. The batch size is set as 256. For the edge classification task, after obtaining the representations of the target node and source node, we feed the concatenated representations of two nodes into a multi-layer perceptron to perform the multi-class classification. We employ the cross-entropy loss function to supervise the training in this task. The future link prediction task and the destination node retrieval task share the same training process where a multi-layer perception is used to obtain the possibility scores and the binary cross-entropy loss function is used for supervision. After obtaining the possibility scores of test samples, we traverse different thresholds to get the AP and AUC-ROC metrics for the future link prediction task, and rank the possibility scores of all candidates to get the Hits@k metric for the destination node retrieval task. We perform the grid search based on the performance of the validation set to find the best settings of some critical hyperparameters, where the searched ranges and related models are shown in Table 2. We use the vanilla recurrent neural network as the memory updater of JODIE and DyRep. For CAWN, the time scaling factor is set as \(1e-6\), and the length of each walk is set as 2. To integrate the text information, we use the pre-trained language model (_i.e.,_ Bert-base-uncased2) to get the representations of text attributes, and then these representations are used to initialize the embeddings of nodes and edges in models. The dimensions of the pre-trained representations are 768 and the maximum text length is set as 512.

**Implementation Details of Large Language Models**. For the textual relation generation task, the inputs to LLMs include the text attribute of the source node and target node, the recent \(k\) edges from the source node and the corresponding text, and the recent \(k\) edges from the target node and the corresponding text. The detailed description of prompts can be found in Appendix C.3 and the experimental results of using different history length can be found in Appendix C.4.4. During inference, we set the temperature as 0.7 and the nucleus sampling size (_i.e., top_p_) is set as 0.95. These two hyperparameters are used to control the randomness of LLMs' output. The repetition penalty is set as 1.15 to discourage the repetitive and redundant output. The maximum number of tokens that the LLM generates is set as 1024. During fine-tuning, the LLMs are loaded in 8-bits and the rank of LoRA is set as 8. We set the batch size as 2 with only one epoch and the gradient accumulation step is set as 8. The learning rate is set as 0.0002 and we use AdamW  for optimization. The scaling hyperparameter \(lora\_alpha\) is set as 32 and the dropout rate during fine-tuning is set as 0.05.

### Edge Classification

Following previous work , we use a multi-layer perceptron to take the concatenated representations of two nodes as inputs and return the probabilities of the edge categories. The performance of different models with Bert initialization is shown in Table 3, where the best results for each dataset are shown in bold. We observe that existing models fail to achieve satisfactory performance in this task, especially on datasets with a large number of categories (_e.g.,_ GDELT and ICEWS1819). This can be attributed to the fact that these models typically neglect edge information modeling in their architectures, which is extremely important for edge classification applications on DyTAGs. In Figure 4, we report the model performance with and without text attributes. It demonstrates that text information consistently helps models achieve better performance on each dataset, verifying the necessity of integrating text attributes into temporal graph modeling. Using Bert-encoded embedding as initialization serves as a preliminary strategy for dynamic textual modeling, lifting the future opportunities for more advanced embedding. We provide the complete results for other datasets in Appendix C.4.1.

    &  &  \\  Dropout Rate & [0.0, 0.2, 0.4, 0.6] & All of 7 models \\ Sampling Size &  & DyRep, TGAT, TCL, GraphMixer \\ Learning Rate & [0.0001, 0.0005, 0.001] & All of 7 models \\ Sampling Strategies & [uniform_r_recent] & TCL, GraphMixer \\ Number of Walks &  & CAWN \\ Sequence Length &  & DyGformer \\ Patch Size &  & DyGformer \\ Number of CNN Layers &  & DyRep, TGAT \\ Number of Transformer Layers &  & TCL, DyGformer \\ Number of Attention Heads &  & DyRep, TGAT, CAWN, TCL, DyGformer \\   

Table 2: Searched ranges of hyperparameters and the related dynamic graph learning models.

  
**Datasets** & **Models** & **JODE** & **DyRep** & **TGAT** & **CAWN** & **TCL** & **GraphMixer** & **DyGFormer** \\   & Precision & 0.6568 (0.0043) & **0.6636 (0.0043)** & 0.6636 (0.0052) & 0.618 (0.0041) & 0.0026 (0.00070) & 0.5350 (0.0009) & 0.6313 (0.0004) & 0.6604 (0.0007) \\  & Recall & **0.6472** (0.0003) & 0.6350 (0.0089) & 0.5530 (0.0001) & 0.5783 (0.0004) & 0.5394 (0.0006) & 0.5735 (0.0015) & 0.5050 (0.0017) \\  & F1 & **0.6478** (0.0005) & 0.6432 (0.0002) & 0.5519 (0.0008) & 0.5685 (0.0012) & 0.5177 (0.0004) & 0.5507 (0.0019) & 0.5604 (0.0003) \\   & Precision & 0.1361 (0.0036) & 0.1451 (0.0071) & 0.1241 (0.0006) & **0.1781 (0.0001)** & 0.1299 (0.0002) & 0.1299 (0.0006) & 0.1775 (0.00041) \\  & Recall & 0.1335 (0.0013) & 0.1365 (0.0013) & 0.1321 (0.0001) & 0.1545 (0.0001) & 0.1235 (0.0001) & 0.1032 (0.0008) & 0.1689 (0.0052) \\  & F1 & **0.0902** (0.0009) & 0.1099 (0.0012) & 0.0012 (0.0007) & 0.0010 (0.0014) & **0.1340 (0.0002)** & 0.0987 (0.0051) & 0.1041 (0.0007) & 0.1291 (0.0008) \\   & Precision & 0.3106 (0.0032) & 0.3237 (0.0002) & 0.3013 (0.0007) & **0.3418 (0.0023)** & 0.3122 (0.0006) & 0.2999 (0.0002) & 0.3297 (0.0034) \\  & Recall & 0.3944 (0.0018) & 0.3636 (0.0002) & 0.3312 (0.0007) & **0.3467 (0.0004)** & 0.3517 (0.0009) & 0.3502 (0.0001) & 0.3632 (0.0006) \\  & P1 & 0.2955 (0.0008) & 0.3097 (0.0006) & 0.2908 (0.0008) & **0.3136 (0.0007)** & 0.2939 (0.0002) & 0.2903 (0.0008) & 0.3099 (0.0027) \\   & Precision & 0.6163 (0.0032) & 0.6673 (0.0019) & 0.6160 (0.0001) & 0.6166 (0.0003) & **0.6213 (0.0008)** & 0.6171 (0.0002) & 0.6166 (0.0003) \\  & Recall & 0.6871 (0.0002) & 0.6627 (0.0006) & 0.6628 (0.0002) & 0.6870 (0.0001) & 0.6875 (0.0001) & 0.6872 (0.0003) & **0.6877 (0.0002)** \\  & F1 & 0.6139 (0.0006) & 0.6134 (0.00006) & 0.6225 (0.0005) & 0.6187 (0.0003) & **0.6230 (0.0003)** & 0.6135 (0.0005) & 0.6196 (0.0008) \\   & Precision & OOM & OOM & O.6256 (0.0004) & 0.6167 (0.0004) & **0.6135 (0.0004)** & **0.6035 (0.0003)** & 0.6034 (0.0009) & 0.6026 (0.0047) \\  & Recall & OOM & OOM & 0.2705 (0.0003) & 0.6135 (0.0042) & **0.2474 (0.0004)** & 0.7412 (0.0001) & 0.5899 (0.0004) \\  & F1 & OOM & OOM & **0.6496 (0.0032)** & 0.6290 (0.0016) & 0.6420 (0.0006) & 0.6412 (0.0005) & 0.4622 (0.0005) & 0.4620 (0.0086) \\   & Precision & OOM & OOM & 0.6585 (0.0007) & 0.6921 (0.0004) & 0.6915 (0.0004) & 0.6195 (0.0008) & 0.6798 (0.0040) \\  & Recall & OOM & OOM & **0.7921 (0.0012)** & 0.560 (0.0015) & 0.7880 (0.0026) & **0.7020 (0.0013)** & 0.7484 (0.0091) \\  & F1 & OOM & OOM & 0.7201 (0.0013) & 0.6002 (0.0037) & **0.7129 (0.0046)** & 0.7214 (0.0004) & 0.7033 (0.0294) \\   & Precision & 0.9523 (0.0046) & OOM & 0.8789 (0.007) & 0.9931 (0.0021) & 0.5864 (0.0017) & 0.9934 (0

[MISSING_PAGE_FAIL:8]

and long-range dependencies that are important in real-world applications. More experimental results are provided in Appendix C.4.3.

### Textural Relation Generation

Generating the textual content of future interactions within certain node pairs remains an under-explored challenge, which requires a language model to understand the dynamics and textural description within a graph structure. We evaluate six LLMs on three datasets derived from different real-world scenarios. For instance, on the Googlemap CT dataset, the input sentence is constructed by the user's historical reviews and textural description of the destination. Then, LLM is prompted to generate potential reviews from the user to the future destination. Instead, the LLM is prompted to generate reviews for target movies and answers to target questions on the Amazon movies and Stack elec datasets, respectively. See Appendix C.3 for dataset-specific prompts. As shown in Table 6, we observe that

    & **Googlemap CT** &  &  \\  & Precision & Recall & F1 & Precision & Recall & F1 \\  GPT 3.5 turbo & 79.89 & **84.13** & 81.91 & 79.79 & 83.61 & 81.63 & 80.52 & 81.96 & 81.21 \\ GPT 40 & 78.33 & 84.06 & 81.07 & 78.68 & **84.20** & 81.33 & 78.30 & 82.37 & 80.26 \\ Llama3.8b & 78.62 & 83.84 & 81.12 & 78.84 & 83.97 & 81.09 & 79.91 & 82.35 & 81.09 \\ Mitari-7b & **80.21** & 84.05 & **82.07** & 79.81 & 84.05 & **81.84** & 80.25 & **82.61** & 81.40 \\ Vicuna-7b & 80.04 & 83.79 & 81.85 & **80.23** & 83.60 & 81.83 & **80.65** & 82.37 & **81.46** \\ Vicuna-13b & 80.14 & 84.00 & 81.99 & 77.59 & 83.56 & 80.39 & 80.57 & 82.20 & 81.33 \\   

Table 6: Precision, Recall and F1 of BERTscore of different LLMs for textural relation generation. The number of test samples is 500 per dataset.

    & **Googlemap CT** &  \\  Llama3-8b & 81.12 & 81.09 \\ Llama3-8b + SPT & 81.84 (0.72\(\)) & 81.97 (0.88\(\)) \\  Vicuna-7b & 81.85 & 81.46 \\ Vicuna-7b + SPT & **85.67** (3.82\(\)) & 82.67 (1.21\(\)) \\  Vicuna-13b & 81.99 & 81.33 \\ Vicuna-13b + SPT & 84.67 (2.68\(\)) & **82.73** (1.40\(\)) \\   

Table 7: Performance of LLMs after SFT for the relation generation task in terms of BERTscore (F1).

Figure 5: Node retrieval performance using random sampling and historical sampling.

    & **Datasets** & **Text** & **JODIE** & **DyRep** & **TGAT** & **CAWN** & **TCL** & **GraphMiver** & **DyGFormer** \\   & **Googlemap CT** & \(}\)** & \(}\)**5733 \(}\).0005** & \(0.8427 0.0031\) & \(0.7780 0.0047\) & \(0.7057 0.0086\) & \(0.8134 0.0079\) & \(0.7798 0.0045\) & \(0.8468 0.0021\) \\  & & \(}\)\(0.863 0.0010\) & \(0.8399 0.0037\) & \(0.8017 0.0035\) & \(0.8747 0.0047\) & \(0.8873 0.0036\) & \(0.8678 0.0076\) & \(0.9401 0.0011\) \\   & **Googlemap CT** & \(}\) & OOM & **OOM** & \(}\)**6539 \(}\).0047** & \(}\)**5158 \(}\).0080 & \(0.4460 0.0063\) & \(0.4076 0.0071\) & \(0.4543 0.0099\) \\   & **Googlemap CT** & \(}\) & OOM & **OOM** & \(}\)**6972 \(}\).0022** & \(0.5219 0.0003\) & \(0.5379 0.0057\) & \(0.4855 0.0023\) & \(0.4913 0.0023\) \\   & **Amazon movies** & \(}\) & OOM & OOM & \(}\)**6349 \(}\).0006** & \(0.5835 0.0071\) & \(0.6446 0.0062\) & \(0.6478 0.0035\) & \( 0.0084\) \\   & **Yelp** & \(}\) & OOM & OOM & \(}\)7245 \(}\).0138 & \(0.6757 0.0084\) & \(0.7147 0.0024\) & \(0.6485 0.0025\) & \( 0.0012\) \\   & **Yelp** & \(}\) & OOM & OOM & \(}\)**6951 \(}\).0058** & \(0.5410 0.0015\) & \(0.5930 0.0024\) & \(0.5745 0.0245\) & \(0.4944 0.0068\) \\   & **Yelp** & \(}\) & OOM & OOM & \(}\)**6005 \(}\).0133** & \(0.7468 0.0088\) & \(0.5745 0.0019\) & \(0.5855 0.0044\) & \(0.7060 0.0060\) \\   & **Googlemap CT** & \(}\) & \(}\)**73380 \(}\).0079** & \(0.7285 0.0027\) & \(0.6453 0.0085\) & \(0.5752 0.0038\) & \(0.6157 0.0097\) & \(0.6200 0.0065\) & \(0.7036 0.0056\) \\   & **Googlemap CT** & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\    & **Googlemap CT** & \(}\) & OOM & OOM & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\    & **Amazon movies** & \(}\) & OOM & \(}\) & \({{open-source LLMs such as Mistral and Vicuna perform comparably well to proprietary LLMs in this task. To further improve their performance on relation generation, we extract 10,000 node pair interactions associated with textural descriptions from the datasets for LLM supervised fine-tuning (SFT). Results in Table 7 demonstrate that LLMs consistently achieve enhanced performance in textural generation after supervised fine-tuning. Especially, Vicuna-7b benefits the most from SFT. We provide the ablation study on different information provided to LLM in Appendix C.4.4.

## 6 Discussion

**Limitations and Future Directions**. While the proposed DTGB represents a significant advancement in the study of DyTAGs, there are areas ripe for further exploration. Our extensive benchmark experiments reveal that current dynamic graph learning algorithms and large language models (LLMs) exhibit varying degrees of effectiveness when handling the complex interactions between the dynamic graph structure and textural attributes. This finding highlights the potential for even greater improvements and innovations in this field in the future.

A particularly exciting future direction is the design of temporal graph tokens that can directly incorporate dynamic graph information into LLMs for reasoning and dynamics-aware generation. By designing representations that seamlessly blend structural and temporal aspects of the graphs with their text attributes, these tokens could potentially enhance the ability of LLMs to capture and utilize the dynamic nature of DyTAGs. This approach promises to improve performance in a range of applications, such as real-time recommendation systems, dynamic knowledge graphs, and evolving social network analysis.

Another notable challenge is the scalability issue when handling large-scale DyTAGs, especially given the potentially long text descriptions associated with nodes and edges. The complexity of encoding long sequences and integrating them with dynamic graph structures can lead to computational overhead. Addressing this scalability issue is a crucial future direction to ensure that models can efficiently process large-scale graphs with extensive textual attributes, paving the way for more practical and robust applications in real-world scenarios.

**Broader Impact**. The broader impact of DTGB lies in its ability to drive advancements in dynamic text-attributed graph research by providing a comprehensive benchmark for evaluating models. The broader impact can extend to numerous societal and technological domains, such as social media and real-time recommendation systems. Furthermore, advancements driven by DTGB that can integrate dynamic graph learning with natural language processing could lead to methodological enhancement in fields such as healthcare, finance, and cybersecurity, where understanding the evolving relationships and information is critical for decision-making and risk management. Overall, DTGB has the potential to drive significant improvements in how complex, dynamic data is harnessed and utilized across various sectors.

## 7 Conclusion

We propose the first comprehensive benchmark DTGB specifically for dynamic text-attributed graphs (DyTAGs). We collect and provide eight carefully processed dynamic text-attributed graph datasets from diverse domains. Based on these datasets, we comprehensively investigate the performance of existing dynamic graph learning models and large language models (LLMs) in four real-world-driven tasks. Our experimental results validate the utility of DTGB and provide insights for further technical advancements. The limitation of this work is that we did not incorporate the high-order graph context in the textual relation generation task, due to the maximum input length of LLMs. Therefore, in the future, we will investigate how to efficiently use LLM to handle high-order dynamic topology and long-range evolving texts within DyTAGs.