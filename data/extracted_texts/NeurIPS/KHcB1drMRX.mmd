# Accelerating Pre-training of Multimodal LLMs

via Chain-of-Sight

Ziyuan Huang\({}^{1}\) Kaixiang Ji\({}^{1}\) Biao Gong\({}^{1}\) Zhiwu Qing\({}^{2}\) Qinglong Zhang\({}^{1}\)

**Kecheng Zheng\({}^{1}\) Jian Wang\({}^{1}\) Jingdong Chen\({}^{1}\) Ming Yang\({}^{1}\) \({}^{1}\)**Ant Group \({}^{2}\) Huazhong University of Science and Technology

[https://chain-of-sight.github.io/](https://chain-of-sight.github.io/)

###### Abstract

This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs). Our approach employs a sequence of visual resamplers that capture visual details at various special scales. This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16\(\) increase in the token count post pre-training. Consequently, Chain-of-Sight requires significantly fewer visual tokens in the pre-training phase compared to the fine-tuning phase. This intentional reduction of visual tokens during pre-training notably accelerates the pre-training process, cutting down the wall-clock training time by \(\)**73%**. Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process. Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks.

## 1 Introduction

Recently, Large Language Models  have received unprecedented attention, owing to their remarkable capabilities in text comprehension and generation. Riding on the success of LLMs, Multimodal Large Language Models (MLLMs)  demonstrate impressive zero-shot transferability across a wide range of vision-language tasks, such as image captioning, visual question answering, and visual grounding.

The exceptional generalization ability exhibited by the contemporary MLLMs can be largely attributed to their extensive pre-training on a massive amount of data . However, as the volume of data escalates, so does the wall-clock training time, which has become a major obstacle in further explorations. According to , 60,000 GPU hours are needed for training a 7B model on just 96 million image-text pairs. This intensive computational demand is not only prohibitive to many researchers, but also leads to a significant carbon footprint.

One of the key reasons for the prolonged training time is the extensive length of visual tokens Typically, the image-text pairs in the pre-training phase involve around 23 text tokens (see Table 1). In contrast, most MLLMs handle substantially more visual tokens during pre-training, _e.g._, 144 , 256 , or even higher . Reducing the number of visual tokens presents a straightforward way to speed up training, as it allows for an increase in batch size and a concurrent decrease in step time. Meanwhile, the reduced memory consumption allows for better optimization stages , further reducing time requirements. However, training with fewer visual tokens often results in compromised performance for existing vision-language models.

To resolve this dilemma, this work introduces Chain-of-Sight, a vision-language bridging module for efficient pre-training of MLLMs. Unlike existing approaches that maintain a constant token count throughout both pre-training and fine-tuning, Chain-of-Sight allows for a marked increase in the number of tokens after the pre-training stage, thereby reducing the tokens needed during pre-training. The core mechanism is our multi-scale visual resampler, which produces visual tokens of multiple visual scales. Inspired by the classical concept of multi-scale feature hierarchy in visual understanding [106; 42; 33; 107; 83; 53], we partition the visual features produced by the visual backbone using windows of multiple sizes. For each window size, a visual resampler is implemented to produce a specified number of visual tokens per window. Subsequently, the visual tokens from various window sizes are gathered and linked in a global-to-local manner, forming a chain of reasoning steps from coarse views gradually to fine-grained perspectives.

On top of this, we propose a post-pretrain token scaling strategy, which compounds the elements of input resolution and window size manipulation to enable a significant escalation in the token count for our Chain-of-Sight, reaching up to 16\(\) increase during fine-tuning. Such adaptability allows for the fine-tuning of the model with a flexible granularity or complexity as required, without the the necessity for an additional pre-training phase.

By intentionally reducing the number of visual token by \(\)90% in the pre-training, a 2.5\(\) batch size is allowed with a step time reduction of 30%, leading to a 3.7\(\) faster pre-training in terms of wall-clock time (\(\)73% less) for the same amount of data, when compared with using all visual tokens during pre-training. Meanwhile, our observations indicate that this acceleration does not come at the expense of performance. The results achieved by our Chain-of-Sight model pre-trained with 32 tokens match or surpass those obtained using 336 visual tokens throughout the training process, when both models use the same tokens during fine-tuning. Further scaling up the tokens in the fine-tuning stage leads to enhanced performance at small additional training costs. This scaling showcases the potential of Chain-of-Sight to capitalize on the initial efficiency gains and adapt its framework to achieve even greater levels of accuracy and effectiveness in visual understanding for MLLMs.

## 2 Method

Our objective is to accelerate the pre-training of MLLMs. To this end, we resort to reducing the number of visual tokens inputted into the language model. To mitigate the performance drop associated with fewer visual tokens, we introduce a versatile bridge module within our framework, named Chain-of-Sight. This module is designed to enable the increase in the token count on demand after pre-training. With this capability, we are able to substantially lower the number of visual tokens during the pre-training phase, while retaining the ability to scale up and capture a rich level of visual detail during fine-tuning. The concept of Chain-of-Sight is illustrated in Fig. 1.

### Re-examining the efficiency bottleneck in MLLM pre-training

Modern MLLMs are typically constructed by three core components: (1) a visual encoder, (2) a vision-language bridge module, and (3) a language model. Given that the language models often have a much larger size than the visual encoder, they account for the majority of computation during pre-training [90; 4; 73; 19; 58]. Consequently, the number of input tokens processed by the language model is a crucial factor determining the total computational workload.

Figure 1: **Chain-of-Sight concept overview**. Recent current MLLMs maintain a constant set of visual tokens in both pre-training and fine-tuning. These tokens typically represent visual contents at a single visual scale. In contrast, our Chain-of-Sight approach leverages the idea of visual hierarchy, producing multi-scale visual tokens. Moreover, the token scaling strategy enabled by our multi-scale visual resamplers allow us to start with a small pool of visual tokens for pre-training, before increasing the number of tokens during fine-tuning. This considerably accelerates the pre-training phase.

As detailed in Table 1, the pre-training data predominantly comprise image-text pairs that contain fewer than 50 text tokens. In contrast, existing MLLMs are designed to handle \(2\) more visual tokens, often requiring such as 144 [10; 11], 256 [4; 50; 97], or even more visual tokens [63; 16; 55; 56; 64; 48]. The imbalance between the visual tokens and text tokens means that processing these visual tokens has become the main efficiency bottleneck in MLLM pre-training. This prompts our exploration for more efficient vision-language bridging structures, which is capable of reducing the number of visual tokens in pre-training without compromising performance.

### Multi-scale visual resamplers

The multi-scale visual resamplers serve as the foundational mechanism enabling the flexible extension of visual tokens after the pre-training phase. This subsection focuses on the architectural details of the multi-scale visual resamplers, as visualized in Fig. 2(b), while the the extension of visual tokens is discussed in the subsequent subsection.

Essentially, the idea of exploiting multi-scale or pyramid structures to handle natural hierarchy of visual contents has been long established as a standard practice [30; 42], proving effective in countless visual tasks [33; 61; 83; 53]. Despite this, the potential for harnessing multi-scale visual hierarchies remains under-explored in the context of MLLMs.

**Visual resampler.** Visual resampler is a Perceiver -like structure that introduces a set of learnable queries and uses cross-attention to condense visual knowledge into a predetermined set of visual tokens [95; 95; 4; 2]. We construct Chain-of-Sight with visual resamplers due to their flexibility in selecting the token count for a specified feature, independent of the features' dimensionality.

**Multi-scale visual resamplers.** One of the effective strategies for building multi-scale features within a network involves combining operations that spans diverse fields of views [13; 45; 101; 21]. Given that the resampler structure inherently gathers visual cues on a global scale across the entire feature map, our strategy focuses on enhancing the perception of the fine-details in the image.

To this end, we partition the visual features into non-overlapping local windows of various sizes. More precisely, given a visual feature \(^{L L C}\) extracted by the visual encoder, where \(L\) and \(C\) denote the feature size and channel, respectively, we define a set of window sizes, denoted as \(=[W_{1},...,W_{m}]\). This setup leads to a collection of windowed visual features \(_{}=[_{1},...,_{m}]\). Each \(_{i}\) represent a set of \(L^{2}/W_{i}^{2}\) windowed features obtained by applying the partition operation on the original visual feature maps with a corresponding window size \(W_{i}\). This naturally forms features of multiple visual scales.

Figure 2: **The Chain-of-Sight framework.** Through partitioning visual features into windows and restricting cross-attention to the windowed features associated with the learnable tokens, our Chain-of-Sight approach produces visual tokens that encompass multiple scales. Thanks to the post-pretrain token scaling strategy, Chain-of-Sight reduces the required number of visual tokens in pre-training, thus accelerating the process. In contrast, the number of visual tokens remains constant in resampler-based methods [44; 2; 4; 99] for pre-training and fine-tuning, and the linear-layer [56; 63; 97; 15] produce a large number of visual tokens, incurring a high cost for pre-training.

At every scale level, each windowed feature is allocated with \(N_{i}\) learnable queries. These learnable queries are then utilized within the visual resampler to perform cross-attention solely on their corresponding windowed feature. This yields \(N\) tokens, where the \(N\) can be calculated as follows:

\[N=_{i}L^{2}/W_{i}^{2}*N_{i}. \]

The learnable queries within the same scale share the parameters of the visual resampler, despite their different spatial locations. However, because the queries at various scales are intended to capture features from varying fields of view, distinct sets of parameters are used for each scale. This results in a group of visual resamplers operating across multiple scales. On top of this, we enable the resamplers to aggregate visual features from multiple feature levels, as in  (see Appendix for details).

**Coarse-to-fine integration.** Upon acquiring a series of multi-scale visual tokens from the multi-scale visual resamplers, our method integrates these prompts in a structured coarse-to-fine fashion. The final token sequence fed to the language model begins with tokens derived from larger windows, which presents an overall view of the image, and proceeds with tokens obtained from smaller windows that contains fine-grained details. Our preliminary experiments reveal a substantial difference in the overall performance between the coarse-to-fine and the reversed order.

### Post-pretrain token scaling strategy

Reducing the number of visual tokens can effectively accelerate pre-training, but typically at the expense of performance. To address this dilemma, we enhance the token count after pre-training, which allows accelerated pre-training with fewer visual tokens, while a subsequent increase in tokens ensures the final performance after fine-tuning, as demonstrated in Fig. 2(c). Specifically, based on the multi-scale visual resamplers, the increase in the token count is accomplished via our compound token scaling strategy that integrates two core mechanisms: resolution scaling and window scaling.

**Resolution scaling.** Enhancing the input resolution stands as the most direct way to augment the number of visual tokens. At the cost of additional computation overhead in the visual backbone, it allows for a quadratic rise of the token count with the resolution enhancement. The concept of resolution scaling is investigated in many existing approaches based on linear projectors  or visual resamplers . They can broadly be viewed as particular instances within our Chain-of-Sight framework, which regards the window size as a fixed factor. In this context, linear projectors use the smallest possible window size for visual token generation, whereas visual resamplers employ the resolution in the pre-training phase as their window size.

**Window scaling.** The windowing mechanism in our multi-scale visual resamplers enable scaling up token numbers by manipulating the window sizes. As in Eq. 1, reducing the window sizes can further produce a quadratic increase in the number of visual tokens on top of the resolution enhancement.

**Compound scaling.** Combining the above token scaling strategies, our compound scaling is capable of producing a 16\(\) increase in the tokens during fine-tuning, as in Fig. 3. This allows us to fine-tune the scale at which visual features are represented and sampled, improving the model's capability of leveraging varying levels of detail and abstraction inherent in the visual content. Consequently, the Chain-of-Sight framework significantly boosts the visual comprehension capability of the model during the fine-tuning stage, effectively compensating for the performance drop incurred by the low number of visual tokens during pre-training.

**Initialization.** Inspired by , we initialize the parameters of the newly introduced visual resamplers by simply inflating the pre-trained parameters, as in Fig. 2. As for the new visual queries, we apply a nearest neighbor strategy to initialize them based on the pre-trained queries.

Figure 3: **Detailed illustration of our post-pretrain token scaling strategy.**

Experiments

In this section, we provide our experimental setup, empirical results, and the comparisons with existing methods.

### Experimental setup

**Model details.** We instantiate our MLLM with CLIP-ViT-L/14  as the visual encoder and Vicuna  as the language model. For efficiency, we adapt Vicuna with LoRA  during all training stages, instead of fully fine-tuning the language model. For the number of visual tokens, we experimented with 32, 48, and 80 during pre-training for our Chain-of-Sight model, where 16 tokens are global tokens (with a window size of 16 for an input resolution of 224) and the rest are local tokens (with a window size of 4 by default). These models are configured to be extended to at most 528, 784, and 1296 visual tokens during fine-tuning using compound token scaling.

**Training settings.** The training of Chain-of-Sight is divided into two stages. For the first stage, we sample around 65M image-text data involving multiple tasks, as detailed in Table 1. The multi-scale visual resamplers and the LoRA paramters  are unlocked for training. For the first 120,000 iterations, we use the input resolution of 224 and unlock the resamplers and the LoRA parameters  for training. During the last 30,000 iterations, the input resolution is raised to 448, where the parameters in the visual backbone is further activated and the tokens are scaled up through our compound scaling. The second stage of the Chain-of-Sight model is supervised fine-tuning, where we remove all the captioning datasets except for COCO.

**Evaluation benchmarks.** The evaluation of our approach involves various tasks including image captioning, visual question answering, text recognition, as well as the tasks defined in popular vision-language benchmarks. Details can be seen in Table A2.

### Ablations

We first ablate the Chain-of-Sight (CoS) design for accelerating the pre-training of MLLMs. For the ablations, we omit the high resolution tuning in the first stage unless otherwise specified.

**Pre-train acceleration by Chain-of-Sight.** Fig. 4 shows the cost for pre-training and supervised-finetuning with various number of visual tokens, as well as the corresponding average performance over 12 benchmarks. We make several key findings. (a) Though reducing visual tokens for the resampler from 336 to 80 significantly reduces the training time, the average performance drops from 86.8 to 84.4. (b) Using an identical number of tokens, _i.e.,_ 80 visual tokens, Chain-of-Sight notably outperforms the standard resamplers, which can be mainly accredited to the multi-scale visual tokens generated by our method. (c) Using the pre-trained model with 80 visual tokens, Chain-of-Sight can be fine-tuned with higher token counts. Using 336 tokens for fine-tuning, our method achieve an average improvement of 1.8pt over the standard resampler with 336 tokens. (d) Notably, Chain-of-Sight with 32 tokens can save up to 73% of the pre-training time, and maintain the same performance as the standard resampler with 336 tokens. (e) Taking fine-tuning into consideration, our method is capable of saving 65% of the total wall-clock time for training a MLLM with improved performance. Note that the percentage is based on a 65M pre-training dataset, and the overall gains in efficiency are expected to grow with the increase of the pre-training dataset scale.

**Image captioning, visual question answering, and text recognition.** Table 2 compares the performance of Chain-of-Sight with its baselines. Overall, our method delivers competitive performance against pre-training with a full set of visual tokens, while substantially accelerating training speed.

Figure 4: **Pre-train acceleration by Chain-of-Sight, in comparison with standard resamplers. The average performance is computed over the reported benchmarks in Table 2. Our method achieves a pre-train acceleration of 73% without compromising performance.**

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

In terms of training efficiency in the fine-tuning stage, the fastest model (resolution 224 with 80 tokens) is twice as fast as the medium model (resolution 448 with 336 tokens), and uses around 25% of the time spent on training the model with the 1296 visual tokens. However, since the wall-clock time required for supervised fine-tuning is substantially smaller than the pre-training stage, such an increment on the training time during fine-tuning is acceptable.

### Comparison with existing approaches

Visual question answering and vision-language benchmarks.Table 5 compare the performance of our model with existing approaches. Since the majority of them fine-tunes the whole language model during fine-tuning, the trainable parameters of existing approaches are substantially larger than our approach. Nevertheless, our Chain-of-Sight has achieved competitive performance against existing approaches on many benchmarks, reaching top performance on visual question answering and MMBench among models of the same scale with less than 10% of the trainable parameters. Since the model did not go through an instruction tuning stage, the performance on MME and Vizwiz is not satisfactory. We include the results for more benchmarks in the appendix.

Visual grounding.We compare our model with the existing approaches on visual grounding in Table 6. Despite that the only data source of object localization for training our Chain-of-Sight model is the RefCOCO datasets [38; 67; 103], and that our language model is adapted with LoRA , our model achieves a leading performance on these three benchmarks, when compared to existing approaches of a similar scale.

## 4 Related work

Multi-modal large language models.Since the introduction of the Transformer architecture  and large-scale pre-training [25; 79], language models have been advancing rapidly [91; 92; 108; 80; 70; 5; 3]. Recently, they are shown to be able to handle various types of data, such as vision [72; 58; 44; 2] and audio [66; 89], leading to a series of multi-modal language models (MLLMs) [4; 12; 99; 109]. The visual capabilities of MLLMs are mainly enabled through transforming visual features into visual tokens, which can be roughly categorized into two types. One uses linear projection to feed image patches into LLMs [58; 11; 16; 93; 97], and the other uses learnable prompts and cross-attentions to aggregate information from the whole feature map [44; 4; 2; 99; 50]. Alternatively, Honeybee  proposes a convolutional model for combining the benefit of both. Most existing approaches use an identical number of visual tokens throughout pre-training and fine-tuning. Though some of the recent works have exploited raising the visual tokens during fine-tuning with increased resolution to

    &  &  &  &  & ^{}\)**} &  &  & ^{}\)**} & ^{}\)**} &  &  &  & ^{}\)**} \\   InstrotBLIP-138B  & Vicuna-13B & 32 & 188M & - & 49.5 & 33.4 & 63.1 & 50.7 & 78.9 & 1212.8 & - & - \\ LLAv3-1.5-13B  & Vicuna-13B & 576 & 13B & 80.0* & 63.3* & 53.6 & 71.6 & 61.3 & 85.9 & 1531.3 & 67.7 & 68.1 \\ CosVLM-17B  & Vicuna-7B & 256 & 10B & 82.3* & - & 91.2* & 70.4 & 87.9 & - & 77.6 & 72.5 \\ VILA-13B  & Vicuna-13B & 576 & 13B & 80.8* & 63.3* & 60.6 & 73.7 & 66.6 & 84.2 & 1570.1 & 70.3 & - \\ Honeybee-13B  & Vicuna-13B & 256 & 13B & - & - & - & - & 85.5 & 1629/315 & 73.2 & 68.2 \\ Min-Gemini-13B  & Vicuna-13B & 576 & 13B & - & - & - & - & 65.9 & - & 1565/322 & 68.5 & - \\ 
**InstructBLIP-7B** & Vicuna-7B & 32 & **188M** & - & 49.2 & 34.5 & **60.5** & 50.1 & - & - & **36.0** & **58.8** \\
**Shikas** & Vicuna-7B & - & - & 7B & 77.4* & - & - & - & - & - & **58.8** & - \\
**DEFFCS-9B1** & - & **64** & **98** & 50.9 & 38.4 & 35.5 & - & 25.9 & - & - & **48.2** & - \\
**Down-VIL** & **Owen-7B** & **58** & 7B.8* & 59.3* & 35.2 & 67.1 & 63.8 & - & - & 38.2 & 62.3 \\
**L1AvA-15-7B** & Vicuna-7B** & **7B** & 78.5* & 62.0* & 50.0 & 66.8 & 58.2 & 85.9 & 1510.7 & 64.3 & - \\ mPLUG-Owl2  & LLMAM2-7B & **64** & **7B** & 79.4* & 56.1 & 54.5 & 68.7 & 58.2 & 85.8 & 1450.2 & 64.5 & 57.8 \\ Hooney-7B  & Vicuna-7B & 144 & **7B** & - & - & - & 83.2 & 1884/307 & 70.1 & 64.5 \\ VILA-7B  & Vicuna-7B & 576 & 7B & 79.9* & 62.3* & **57.8** & 68.2 & 64.4 & 85.5 & 1533.0 & 68.9 & - \\ Mini-Gemini-7B  & Vicuna-7B & 576 & 7B & - & - & - & - & 65.2 & - & 1523/316 & 69.3 & - \\ 
**Co8-7B** & Vicuna-7B & 80 & 532M & 82.9* & 64.0* & 50.7 & 93.9* & 65.1 & 85.9 & 1549/301 & 72.8 & 68.9 \\
**Co8-8B** & LLaMA3-8B & 80 & 540M & **84.3* & **65.3* & - & **95.7* & **67.6** & **86.9** & **1598/308** & **76.6** & **73.1** \\   

Table 5: **Comparison with SoTA methods on 10 benchmarks. Despite that we have only employed LoRA to fine-tune the language model, our model achieves a competitive performance against existing approaches in many benchmarks. _PT tks._ indicates the number of visual tokens used for pre-training and _Parm._ indicates the trainable parameters for the whole model. * indicates at least part of the training set is observed during training. Best performance is marked bold. Gray fonts indicate models of larger sizes than ours.**enhance downstream performance [50; 55; 57; 69], the large set of visual tokens for each image still presents a major bottleneck for the pre-training stage.

Efficient model pre-training.As the model size consistently expands, the efficiency of training large models has become increasingly important. Beyond efforts in the system optimizations [81; 82; 24; 23], the pre-training of large models can be accelerated by sparse computation, such as masking [46; 77] or mixture of experts [28; 51]. Our approach presents a novel perspective for accelerating pre-training for MLLMs by reducing visual tokens required.

Multi-scale hierarchy in vision.Multi-scale hierarchy is a fundamental property in vision, which has led to the introduction and evolution of convolutional networks [30; 42; 40; 33] as well as its application in various vision problems [61; 83; 53; 13]. Recently, transformers are also shown to benefit from multi-scale hierarchy [98; 60; 27; 47]. This work extends multi-scale hierarchy to language models for stronger visual capabilities and higher training efficiency.

## 5 Discussions

**Limitations.** Despite the strong performance and the notable acceleration achieved by Chain-of-Sight, our approach leverages parameter efficient fine-tuning (PEFT) for adapting language models. Hence, the generality of the final model might be limited, compared to approaches that fine-tunes the whole language model during supervised fine-tuning process [57; 10; 52] or even the pre-training stage [63; 4; 26]. This is mainly due to the limited training resources and is exactly what motivates us to explore efficient pre-training methods. We believe the pre-train acceleration achieved by the presented approach has stronger potentials beyond our results.

**Conclusions.** In this work, we set out to accelerate the pre-training phase of MLLMs. Motivated by the unbalance between the number of visual and text tokens during pre-training, we present Chain-of-Sight to reduce the number of token required for pre-training. Chain-of-Sight produces visual tokens of multiple visual scales, providing various level of granularity for the MLLMs to have better perception capabilities. The proposed compound token scaling strategy in the fine-tuning stage can substantially increase the number of tokens post pre-train, such that the model can achieve competitive performance despite the low token count during pre-training. Empirical results have shown that our Chain-of-Sight is capable of achieving a 3.7\(\) speed up in the pre-training process with on-par or better downstream performances. We hope our research can facilitate further investigations in efficient pre-training of MLLMs.