# PDP: Parameter-free Differentiable Pruning

is All You Need

 Minsik Cho Saurabh Adya Devang Naik

Apple Inc.

{minsik, sadya, naik.d}@apple.com

###### Abstract

DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the next best from the existing techniques shows 81.5% accuracy. In addition, PDP can be applied to structured pruning, such as N:M pruning and channel pruning. For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k accuracy by over 3.6% over the state-of-the-art. For channel pruning of ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the state-of-the-art.

## 1 Introduction

Deep neural networks (DNN) have shown human performance on complex cognitive tasks , but their deployment onto mobile/edge devices for enhanced user experience (i.e., reduced latency and improved privacy) is still challenging. Most such on-device DNN systems are battery-powered and heavily resource-constrained, thus requiring high power/compute/storage efficiency [22; 48; 50; 53].

Such efficiency can be accomplished by mixing and matching various techniques, such as designing efficient DNN architectures like MobileNet/MobileViT/ MobileOne [36; 43; 48], distilling a complex DNN into a smaller architecture , quantizing/compressing the weights of DNNs [8; 18; 23; 31; 39; 58], and pruning near-zero weights [28; 34; 40; 44; 52; 56; 57; 61]. Also, pruning is known to be highly complementary to quantization/compression  when optimizing a DNN model. Training a larger model and then compressing it by pruning has been shown to be more effective in terms of model accuracy than training a smaller model from the beginning . However, pruning comes at the cost of degraded model accuracy, and the trade-off is not straightforward .

Hence, a desirable pruning algorithm should achieve high accuracy and accelerate inference for various types of networks without significant training overheads in costs and complexity. In this

[MISSING_PAGE_FAIL:2]

**Learning pruning mask** as an extra parameter is a popular way for differentiable pruning. In CS , a new trainable mask parameter is learned to continuously remove sub-networks based on the lottery hypothesis . In detail, the new mask parameter is first multiplied by a scheduled temperature parameter, and then L0-regularized inside the Sigmoid function to continuously increase the sparsity. OptG  proposed learning a mask parameter based on the loss changes due to pruning a particular weight, which is proportional to the gradient and the magnitude of the corresponding masked weight when approximated with STE . AutoPrune  also introduces extra learnable parameters (controlled by a regularizer) to generate a differentiable/approximated pruning mask using STE . In , new learnable parameters, \(\) are introduced and L\({}_{1}\)-regularized to enforce sparsity. Then, \(\) is compared with a hyper-param \(t\) to derive the masks for the model parameters (i.e., prune if \(<t\)). As such comparison is not differentiable, comparison is approximated into a foothill function . Learning channel pruning mask is proposed in SCP  based on the operation-specific observation that a feature map with a large and negative batch mean will get zeroed by ReLU.

**Generating pruning masks** is proposed in FTWT  where new layers are attached to the exiting main architecture. The attached new layers are fed with activations from the main network, then trained to generate masks for the weights in the main network. Thus, although it does not learn masks directly, it still requires new trainable parameters to learn how to generate masks, not to mention the changes to the model network. Using knowledge distillation to generate pruning masks is explored in NISP  and DCP , where a fully trained teacher network guides the mask generation in a way that the distortion to the layer responses can be minimized. A differentiable Markov process is studied to generate a channel pruning mask probabilistically, where a state represents a channel and the transitions from states account for the probability of keeping one channel unpruned .

## 3 PDP: Parameter-free Differentiable Pruning

Complex pruning schemes do not always yield the best quality results, and their complexity and cost can make them impractical and difficult to use. The proposed Parameter-free Differentiable Pruning (PDP) is a highly effective and efficient scheme that generates soft pruning masks using a dynamic function of weights in a parameter-free and differentiable fashion. Since PDP is differentiable, the task loss can directly guide the pruning decision, offering an effective pruning solution. Simultaneously, being parameter-free, PDP can be fast and less intrusive to the existing training flow. Overall, PDP finds a weight distribution that is best for task loss and pruning. Instead of having extra parameters, PDP indirectly influences the weight distribution for high-quality pruning. For example, if a weight

Figure 1: Unlike learning a threshold parameter for differentiable pruning [28; 38] in (a), PDP generates soft masks without extra trainable parameters to accomplish differentiable pruning as in (b). The moment a weight is pruned in (a), the weight will not get any update as the mask zeros out the gradient, depriving the opportunity for better model training. However, the scheme in (a) ensures the training and test behaviors are identical, yet makes the sparsity control difficult. PDP generates such soft masks without extra parameter/training overheads unlike existing schemes [34; 40; 44; 45; 57] (see Section 3.1). While PDP reveals different behaviors during training/test times because the test needs hard masks, our soft mask allows a weight to recover from being pruned over time, which yields higher model accuracies efficiently.

\(w\) is destined to be pruned for some reason, instead of having a new parameter to denote **"to-prune"**, PDP lets SGD gradually make \(w\) itself smaller relatively against other parameters in the same entity, increasing its chance to be pruned over time. We will first present PDP in Section 3.1, provide in-depth analysis in Section 3.2, discuss the benefits of PDP over existing differentiable pruning approaches in Section 3.3, and then show the extension to structured and channel pruning in Section 3.4.

### PDP Algorithm

To address the drawbacks of existing differentiable pruning algorithms, we propose PDP. A soft mask should ideally represent the chance of a weight \(w\) being in one of two symbolic states, "to-prune" (noted as \(Z\)) or "not-to-prune" (noted as \(M\)), without requiring extra parameters or expensive bookkeeping. While the chance of \(w\) being in either state is not straightforward to compute, PDP generates a soft mask based on the fact that there exists an equal chance point for both states. Let us consider differentiable functions, \(z,m:[0,]\), to compute the chances of being in \(Z\) and \(M\), respectively, which must satisfy the following conditions as a soft mask for magnitude-based pruning:

* \(z(|a|)<z(|b|)\) for \(|a|>|b|\): a weight with a smaller magnitude has a higher chance to be in \(Z\).
* \(m(|a|)>m(|b|)\) for \(|a|>|b|\): a weight with a larger magnitude has a higher chance to be in \(M\).
* \(z(w)+m(w)=1\) for any \(w\): the total probability is 1. \[z(w)=1&w=0\\ 0&|w|=\\ &|w|=t\] Then, by the monotonicity and continuity, there exists \(t_{ 0}\) such that \(z(t)=m(t)=0.5\) (the equal chance for \(Z\) and \(M\)), which leads to the following boundary conditions on the left. Any function that satisfies these conditions can be used to compute \(m(w)\) as a soft mask of \(w\) for train-time pruning. Let denote that \(topK(X,k)/bottomK(X,k)\) is selecting the largest/smallest \(k\) elements from a matrix \(X\), \(abs(X)\) is an element-wise absolute operation, and \(n(X)\) returns the element count. In PDP, we uniquely identify \(t\) for a given prune ratio \(r[0,1)\) for a layer with a weight matrix \(W\) as in Fig. 2 (a) based on the pruning context.

* The sparsity \(r\) for each \(W\) can be easily obtained by sorting the weights from the network per magnitude after a few epochs w.r.t the global target sparsity as a one-time task, by a pruning budget allocation algorithm [14; 28], or set manually by a user.
* Right after the SGD weight update, \(t\) is computed for the weights \(W\) in each layer or entity. The role of \(t\) is to abstract the current weight distribution of each layer/entity for pruning.
* During forward-pass, a soft mask, \(m(w)\) for the weight \(w\) is computed and then the masked weight \(\) is applied. \(\) is the temperature parameter (see Section E in Appendix for details).
* Computing \(t\) and generating \(\) repeat iteratively to adapt to the updated weight distribution.

Figure 2 (b) shows how the value of \(t\) is obtained in PDP and a soft mask is computed. Specifically, \(t\) is set to the value that is halfway between the largest pruned weight and the smallest unpruned weight when a hard mask is applied for a given sparsity ratio \(r\). This ensures that each weight has an equal probability of being pruned or kept. As a result, PDP satisfies all the constraints for \(z\) and \(m\). More details on PDP training flow are in Section D in Appendix.

Figure 2: Computing \(z(w),m(w)\) for the chances for \(Z\) and \(M\) with \(t\) for the equal chance to be in \(Z\) and \(M\).

PDP uses a dynamic function of \(t\) to generate soft pruning masks of \(W\) without the need for any extra trainable parameters. Instead, PDP lets the weights of the network adjust themselves such that the information that would otherwise be learned by the extra trainable parameters is instead fused into the weights themselves and their distribution. This is possible because each weight \(w\) is not only a coefficient in a layer, but also an indicator of the relative chance of that weight being pruned against the other weights in \(W\). This relative chance is determined by the value of \(t\), as shown in Figure 2 (b). Overall, PDP looks simple but is shown to be quite effective, which opens to broad applicability.

### PDP Gradient Analysis

To better understand how PDP helps pruning decision, we derive the gradient of the masked weight \(\). As in Fig. 2 (a), \(\) is the following:

\[=m(w) w=}{}}}{e^{}{}}+e^{ }{}}} w\] (1)

Then, the gradient of \(w, w\) can be simplified as the next:

\[ w=m(w)+2}{}m(w)\{1-m(w)\}\] (2)

where we can make the following observations:

* The 1st term is a typical gradient in mask-based pruning, and the 2nd term is an additional gradient with a positive factor (i.e, \(}{}m(w)\{1-m(w)\} 0\)) from PDP.
* If \(m(w) 0\) or hard-prune, \( w 0\), which is true for any pruning algorithm.
* If \(m(w) 1\) or hard-not-to-prune, \( w\), which is true for any pruning algorithm.
* When \(m(w) 0.5\) (i.e., pruning decision is unclear), \(m_{w}(1-m_{w})\) is maximized and accordingly \( w\) is too, boosting the \(w\) movement.
* \(\) serves as an inverse scaling factor for the boosted gradient.

Hence, PDP will accelerate the SGD updates for the weights near the pruning boundary (i.e., \(t\)) toward a loss-decreasing direction, which can encourage the weights to settle with the proper pruning decision at the end. Even if the current gradient is not globally beneficial for the task, many _second_ chances will eventually help recover the damages in an accelerated manner. We conjecture these features enable PDP to make a better pruning decision for the weight on the boundary.

### PDP vs. Existing Differentiable Pruning Strategies

**Learning Pruning Budget Allocation** focuses on obtaining a pruning threshold in a differentiable way based on marginal loss or L\({}_{1}\)-regularization [25; 38]. Meanwhile, PDP directly generates a soft pruning mask for each weight also in a differentiable way. Fig. 1 illustrates the differences with an example. Learning a pruning threshold is helpful for global pruning budget allocation, as the threshold gets adjusted per the task loss as in Fig. 1 (a), but has the following drawbacks:

* Sparsity level is hard to control, as the pruning threshold is not explicitly related to the sparsity, as masks and the thresholds are not directly co-optimized. In Fig. 1 (a), after one weight update, the threshold is reduced from 0.205 to 0.202, but the sparsity is increased from 50% to 66% as \(w_{4}\) becomes smaller than the threshold.
* Once a weight gets pruned during training, it does not get updated as the gradient is masked out. The bottom weights \(w_{\{0,1,2,3\}}\) get no weight update due to the zero masks, and remain pruned.

On the contrary, PDP allows all the weights to be updated through soft masks as in Fig. 1 (b), providing higher flexibility and recovery from undesirable pruning . For example, consider the weight \(w_{2}\) of a value 0.19 which would have been permanently pruned in (a). PDP discourages a weight from being permanently pruned through the weight update, \(w_{2}\) still receives a scaled-down gradient. If a near-zero weight continues to get negative gradients over time (although scaled down by the soft mask), it can eventually get unpruned at the end of training. Similarly, if a very large weight gets positive gradients many times enough to be near zero, it will get pruned eventually, even if it was not pruned in the early stage. Hence, such gradual pruning decision over time during training allows PDP to make better pruning decisions w.r.t. the task loss.

Such _soft_ masking allows pruning decisions to be flipped multiple times during the entire training process, and such effects are compared between PDP and STR  (from Table 1) in Fig. 3 where the pruning decisions for the first Conv2d (3x3 kernels with 32 output channels) in MobileNet-v1 for the end-to-end ImageNet1k training are captured. The white cell means that the pruning decision has been flipped (from to-prune to not-to-prune or vice-versa), while the black cell means the decision has never changed, during the given epoch. Then, we can observe from Fig. 3 that while PDP in (a)-(h) continues to flip the decisions until the very late training stage, STR in (i)-(l) finalizes the pruning decisions early (i.e., in the first 30 epochs).

**Learning/Generating Pruning Masks with Extra Parameters** allows the pruning decision to be driven by a task loss through back-propagation rather than the weight value itself (i.e., a hard mask will zero out the gradient of a pruned weight), thus addressing the problems with differentiable pruning budget allocation, but comes with the following issues.

* A pruning mask is (or derives from) a learnable parameter. Hence the number of total trainable parameters increases significantly, making the training process slow and complex .
* A hard mask is approximated into a soft mask using a differentiable function, without guaranteeing the key properties of a pruning mask, such as the  value range or monotonicity .

Table 2 compares parameter-free PDP with a differentiable mask pruning scheme, CS  from Table 1 (see Table 8 in Appendix for detail). The results show that PDP outperforms CS and is 2.8 faster, without adding extra trainable parameters or complicating the training flow. Table 3 also demonstrates being parameter-free can provide the substantial training efficiency gains for very large language models like GPT. When trained and sparsified on 32 GPUs in mixed-precision (to fit OptG into GPU memory) using the recipe in , PDP delivers the best perplexity at 75% sparsity with 42% lower cost than OptG .

Figure 3: The effects of PDP and STR  (from Table 1) for the first 3x3 Conv2d layer with 32 filters in MobileNet-v1 on ImageNet1k where each small rectangle represents one 3x3 kernel: For PDP, during the entire end-2-end training, we temporarily round the soft mask value to make the pruning decisions. The white cell indicates such pruning decision for the corresponding weight has been flipped at least once during the particular epoch, and the black cell indicates the other case.

### PDP for Structured and Channel Pruning

The simplicity and non-intrusive nature of PDP make it readily applicable to differentiable structured and channel pruning. As an example of structured pruning, we consider N:M pruning, where only \(N\) weights are kept out of every \(M\) consecutive weights. N:M pruning is attracting high research and industrial attention because top-of-the-line GPUs support 2:4 configuration . To apply PDP to N:M pruning, we apply it to every \(M\) consecutive weights of the layer, as if the layer were composed of many sub-layers, each with \(M\) weights, which is illustrated in Figure 4 (a). Since N:M dictates the target sparsity, we can easily find the threshold \(t\) and generate the soft mask, as shown in Figure 2 (a).

Channel pruning is another type of structured pruning that can be easily applied to PDP with minor modifications. To do this, we first compute the L\({}_{2}\) norm of each channel in the layer, and then use these norm values (in place of the absolute values of the weights in Figure 2(a)) to compute a soft mask for each channel, which is depicted in Figure 4 (b). Using the soft mask to prune all the corresponding weights in the channel will make the channel pruning process differentiable.

## 4 Experimental Results

We compared our **PDP** with state-of-the-art random, structured, and channel pruning schemes on various computer vision and natural language models. We used two x86 Linux nodes with eight NVIDIA V100 GPUs on each in a cloud environment. All cases were trained from scratch. More experimental results and the hyper-parameters are in Section F and Table 8 in Appendix.

**Random Pruning for Vision Benchmark:** We compared the proposed **PDP** with the latest prior arts, **STR**, **GMP**, **DNW**, **GraNet**, **OptG**, and **ACDC** on ResNet18, ResNet50, MobileNet-v1, and MobileNet-v2  with the ImageNet1k dataset . Since all of these schemes have been experimented only with ResNet50 and/or MobileNet-v1, we reproduced the pruning results in our controlled environment with the identical data augmentations by running the official implementations from the authors  or verified implementations from the prior arts  as in Section G in Appendix. Since the primary goal of pruning is to trade-off the model accuracy with the compute reduction as in Section 2, we measured the accuracies and Multiply-Accumulate Operation (MAC) during inference on each experiment with layer fusion (i.e., BatchNorm folding), and mainly focused on the high-sparsification cases. Note that the MAC is purely theoretical and reported to understand the trade-off among accuracy, size, and compute across

 Method &  Top-1 \\ (\%) \\  &  Sparsity \\ (\%) \\  &  Model \\ \#param \\  &  Extra \\  &  Runtime \\ min \\  & 
 Inference \\ MAC(\( e6\)) \\  \\  Dense & 91.4 & 0 & 273k & 0 & 22.1 & 41.0 \\ CS & 89.1 & 86.3 & 273k & 267k & 84.7 & 5.80 \\ PDP & 90.4 & 86.3 & 273k & 0 & 30.3 & 5.79 \\  

Table 2: Compared with CS for ResNet20/CIFAR10, PDP delivers 2.7x speed up due to no extra parameters.

 Method &  Perplexity \\  &  Model \\ \#param \\  &  Extra \\ \#param \\  & 
 GPU \\ cost(\$) \\  \\  Dense & 22.4 & 163M & 0 & \\ GMP  & 37.7 & 163M & 0 & 6997 \\ OptG & 33.7 & 163M & 124M & 11210 \\ PDP & 33.7 & 163M & 0 & 7499 \\  

Table 3: With 75% sparsity, being parameter-free becomes more important for training GPT2 with OpenWebText.

Figure 4: PDP is simple and universal enough to be applied directly to structured and channel pruning.

various algorithms, but it still captures the latency benefit on some hardware platforms (see C in Appendix for details). In our experiments with ImageNet1k, all layers, including both the first and last layers, are pruned without any restriction. Also, we estimated the algorithmic overhead and training efficiency by the total monetary cost for GPUs on commercial cloud spot instances .

We applied not only the proposed hyper-parameters from the authors but also a set of further fine-tuned hyper-parameters for competing methods (see Table 8 in Appendix for details). Also, since each algorithm used a different number of epochs and showed results at different sparsity levels, **a)** we ran **STR** first to set the target sparsity levels for all the networks for fair comparisons, because all other schemes can control the sparsity level precisely, **b)** we trained ResNet18/50 for 100 epochs and MobileNet-v1/v2 for 180 epochs following  except **STR** (which diverged with more epochs for MobileNet-v1/v2). For **PDP**, we fixed the target sparsity for each layer based on the global weight magnitude at the epoch 16 and started pruning at the rate of 1.5% of the target sparsity per epoch for all the experiments which correspond to \(s=16\) and \(=0.015\) in Algorithm 1 in Appendix. For detailed experiment configurations, please refer to Table 8. Every experiment began with a randomly initialized model (i.e., no pre-trained model). For **PDP**, we had the following variants to show the value of **PDP** with the same training overhead or per-layer pruning budgets.

* **PDP-base** globally computes the target sparsity by \(abs(W)\) at epoch 16 across all the layers.
* **PDP-base+** is the same as **PDP-base** yet with more epochs to match the GPU cost of **ACDC**.
* **PDP-str/optg** uses the per-layer sparsity from **STR/OptG** as input to normalize the MAC.

Our experimental results are highlighted in Fig. 5, where the size of circles indicates the relative training overhead due to pruning. Note that we used only one single node with 8 GPUs due to the limitation in the official implementations for **GraNet** and **OptG**, thus both have the advantage of not having the inter-node communication cost. Also, each approach imposes a different level of training-time overhead, mainly due to the various complexities of training flow and pruning itself as captured in Fig. 5. Overall results can be summarized as follows:

* **PDP** showed the best the model accuracy: **PDP-base** on ResNet18 delivered 69% Top-1 accuracy which is superior to other schemes but at higher MAC than only **STR** and **OptG**.

Figure 5: PDP-powered pruning (in white box markers) delivers the Pareto superiority to the other schemes (i.e., the top-bottom corner is the best trade-off) for ResNet18, ResNet50, and MobileNet-v1/v2 on ImageNet1k. The size of markers indicates the relative training overheads. The detailed numbers are in Table 4.

[MISSING_PAGE_FAIL:9]

cases, even with 4x larger batch size in 20 fewer epochs. **LNM** training cost is also much higher than **PDP** because of its costly weight regularization and complex back-propagation scheme.

For channel pruning, we compared **PDP** with **SCP**, **NISP**, and **DCP**. Note that **SCP** uses the \(\) in BatchNorm to select the channels to prune (i.e., \(beta\)), hence applicable to limited types of networks only. We again reused the hyper-parameters and configurations as in Table 8 in our Appendix for **PDP**, and the top-1 accuracy with ImageNet1k on ResNet50 is reported in Table 7. We can see that **PDP** can be used for channel pruning and show superior performance out of the box.

## 5 Conclusion

We showed that a simple and universal pruning method PDP can yield the state-of-the-art random/structured/channel pruning quality on popular computer vision and natural language models. Our method requires no additional learning parameters, yet keeps the training flow simple and straightforward, making it a practical method for real-world scenarios. We plan to extend our differentiable pruning into quantization, making both jointly differentiable and optimizable by the task-loss.