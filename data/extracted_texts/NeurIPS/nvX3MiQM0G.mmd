# State-Action Similarity-Based Representations for Off-Policy Evaluation

Brahma S. Pavse and Josiah P. Hanna

University of Wisconsin - Madison

pavse@wisc.edu, jphanna@cs.wisc.edu

###### Abstract

In reinforcement learning, off-policy evaluation (ope) is the problem of estimating the expected return of an evaluation policy given a fixed dataset that was collected by running one or more different policies. One of the more empirically successful algorithms for ope has been the fitted q-evaluation (fge) algorithm that uses temporal difference updates to learn an action-value function, which is then used to estimate the expected return of the evaluation policy. Typically, the original fixed dataset is fed directly into fge to learn the action-value function of the evaluation policy. Instead, in this paper, we seek to enhance the data-efficiency of fge by first transforming the fixed dataset using a learned encoder, and then feeding the transformed dataset into fge. To learn such an encoder, we introduce an ope-tailored state-action behavioral similarity metric, and use this metric and the fixed dataset to learn an encoder that models this metric. Theoretically, we show that this metric allows us to bound the error in the resulting ope estimate. Empirically, we show that other state-action similarity metrics lead to representations that cannot represent the action-value function of the evaluation policy, and that our state-action representation method boosts the data-efficiency of fge and lowers ope error relative to other ope-based representation learning methods on challenging ope tasks. We also empirically show that the learned representations significantly mitigate divergence of fge under varying distribution shifts. Our code is available here: https://github.com/Badger-RL/ROPE.

## 1 Introduction

In real life applications of reinforcement learning, practitioners often wish to assess the performance of a learned policy before allowing it to make decisions with real life consequences (Theocharous et al., 2015). That is, they want to be able to evaluate the performance of a policy without actually deploying it. One approach of accomplishing this goal is to apply methods for off-policy evaluation (ope). ope methods evaluate the performance of a given evaluation policy using a fixed offline dataset previously collected by one or more policies that may be different from the evaluation policy.

One of the core challenges in ope is that the offline datasets may have limited size. In this situation, it is often critical that ope algorithms are data-efficient. That is, they are able produce accurate estimates of the evaluation policy value even when only small amounts of data are available. In this paper, we seek to enhance the data-efficiency of ope methods through representation learning. While prior works have studied representation learning for ope, they have mostly considered representations that induce guaranteed convergent learning without considering whether data-efficiency increases (Chang et al., 2022; Wang et al., 2021). For example, Chang et al. (2022) introduce a method for learning Bellman complete representations for fge but empirically find that having such a learned representation provides little benefit compared to fge without the learned representation. Thus, in this work we ask the question, "can explicit representation learning lead to more data-efficient ope?"To answer this question, we take inspiration from recent advances in learning state similarity metrics for control (Castro et al., 2022; Zhang et al., 2021). These works define behavioral similarity metrics that measure the distance between two states. They then show that state representations can be learned such that states that are close under the metric will also have similar representations. In our work, we introduce a new ope-tailored behavioral similarity metric called **R**epresentations for **O**ff-**P**olicy **E**valuation (rope) and show that learning rope representations can lead to more accurate ope.

Specifically, rope first uses the fixed offline dataset to learn a state-action encoder based on this ope-specific state-action similarity metric, and then applies this encoder to the same dataset to produce a new representation for all state-action pairs. The transformed data is then fed into the fitted q-evaluation (fge) algorithm (Le et al., 2019) to produce an ope estimate. We theoretically show that the error between the policy value estimate with rope + rope and the true evaluation policy value is upper-bounded in terms of how rope aggregates state-action pairs. We empirically show that rope improves the data-efficiency of fge and leads to lower ope error compared to other ope-based representation learning baselines. Additionally, we empirically show that rope representations mitigate divergence of fge under extreme distribution. To the best of our knowledge, our work is the first to propose an ope-specific state-action similarity metric that increases the data-efficiency of ope.

## 2 Background

In this section, we formalize our problem setting and discuss prior work.

### Notation and Problem Setup

We consider an infinite-horizon Markov decision process (mdp) (Puterman, 2014), \(=,,,P,,d_{0}\), where \(\) is the state-space, \(\) is the action-space, \(:([0,))\) is the reward function, \(P:()\) is the transition dynamics function, \([0,1)\) is the discount factor, and \(d_{0}()\) is the initial state distribution, where \((X)\) is the set of all probability distributions over a set \(X\). We refer to the joint state-action space as \(:=\). The agent acting, according to policy \(\), in the mdp generates a trajectory: \(S_{0},A_{0},R_{0},S_{1},A_{1},R_{1},...\), where \(S_{0} d_{0}\), \(A_{t}(|S_{t})\), \(R_{t}(S_{t},A_{t})\), and \(S_{t+1} P(|S_{t},A_{t})\) for \(t 0\). We define \(r(s,a):=[(s,a)]\).

We define the performance of policy \(\) to be its expected discounted return, \(()[_{t=0}^{}^{t}R_{t}]\). We then have the action-value function of a policy for a given state-action pair, \(q^{}(s,a)=r(s,a)+\,_{S^{} P(s,a),A^{} }[q^{}(S^{},A^{})]\), which gives the expected discounted return when starting in state \(s\) and then taking action \(a\). Then \(()\) can also be expressed as \(()=_{S_{0} d_{0},A_{0}}[q^{}(S_{0},A_{0})]\).

It is often more convenient to work with vectors instead of atomic states and actions. We use \(:^{d}\) to denote a representation function that maps state-action pairs to vectors with some dimensionality \(d\).

### Off-Policy Evaluation (OPE)

In off-policy evaluation, we are given a fixed dataset of \(m\) transition tuples \(:=\{(s_{i},a_{i},s^{}_{i},r_{i})\}_{i=1}^{m}\) and an evaluation policy, \(_{}\). Our goal is to use \(\) to estimate \((_{})\). Crucially, \(\) may have been generated by a set of _behavior_ policies that are different from \(_{}\), which means that simply averaging the discounted returns in \(\) will produce an inconsistent estimate of \((_{})\). We do _not_ assume that these behavior policies are known to us, however, we do make the standard assumption that \( s, a\) if \(_{}(a|s)>0\) then the state-action pair \((s,a)\) has non-zero probability of appearing in \(\).

As done by Fu et al. (2021), we measure the accuracy of an ope estimator with the _mean absolute error_ (mae) to be robust to outliers. Let \((_{},)\) be the estimate returned by an ope method using \(\). The mae of this estimate is given as:

\[[]_{}[|(_{ },)-(_{})|].\]

While in practice \((_{})\) is unknown, it is standard for the sake of empirical analysis (Voloshin et al., 2021; Fu et al., 2021) to estimate it by executing rollouts of \(_{}\).

### Fitted Q-Evaluation

One of the more successful ope methods has been fitted q-evaluation (fqe) which uses batch temporal difference learning (Sutton, 1988) to estimate \((_{})\)(Le et al., 2019). fqe involves two conceptual steps: 1) repeat temporal difference policy evaluation updates to estimate \(q^{_{}}(s,a)\) and then 2) estimate \((_{})\) as the mean action-value at the initial state distribution. Formally, let the action-value function be parameterized by \(\) i.e. \(q_{}\), then the following loss function is minimized to estimate \(q^{_{}}\):

\[_{}():=_{(s,a,s^{},r) }[(r(s,a)+_{a^{}_{ }(|s^{})}[q_{}(s^{},a^{})]-q_{}( s,a))^{2}]\]

where \(\) is a separate copy of the parameters \(\) and acts as the target function approximator (Mnih et al., 2015) that is updated to \(\) at a certain frequency. The learned \(q_{^{*}}\) is then used to estimate the policy value: \((_{})_{s_{0} d_{0},a_{0} _{}}[q_{^{*}}(s_{0},a_{0})]\). While conceptually fqe can be implemented with many classes of function approximator to represent the \(q_{}\), in practice, deep neural networks are often the function approximator of choice. When using deep neural networks, fqe can be considered a policy evaluation variant of neural fitted q-iteration (Riedmiller, 2005).

### Related Work

In this section, we discuss the most relevant prior literature on off-policy evaluation and representation learning. Methods for ope are generally categorized as importance-sampling based (Precup et al., Thomas et al., 2021; Liu et al., 2018; Yang et al., 2020), model-based (Yang and Nachum, 2021; Zhang et al., 2021; Hanna et al., 2017), value-function-based (Le et al., 2019; Uehara et al., 2020), or hybrid (Jiang and Li, 2016; Thomas and Brunskill, 2016; Farajtabar et al., 2018). Our work focuses on fqe, which is a representative value-function-based method, since it has been shown to have strong empirical performance (Fu et al., 2021; Chang et al., 2022). We refer the reader to Levine et al. (2020) for an in-depth survey of ope methods.

Representation Learning for Off-policy Evaluation and Offline RLA handful of works have considered the interplay of representation learning with ope methods and offline RL. Yang and Nachum (2021) benchmark a number of existing representation learning methods for offline RL and show that pre-training representation can be beneficial for offline RL. They also consider representation learning based on behavioral similarity and find that such representations do not enable successful offline RL. However, their study is focused on evaluating existing algorithms and on control. Pavse and Hanna (2023) introduced state abstraction (Li et al., 2006) as an approach to lower the variance of ope estimates in importance-sampling based methods. However, their work made the strict assumption of granting access to a bisimulation abstraction in theory and relied on a hand-specified abstraction in practice. Only recently have works started to consider learning representations specifically for ope. Chang et al. (2022) introduced a method for learning Bellman complete representations that enabled convergent approximation of \(q^{_{}}\) with linear function approximation. Wang et al. (2021) show that using the output of the penultimate layer of \(_{}\)'s action-value function provides realizability of \(q_{_{}}\), but is insufficient for accurate policy evaluation under extreme distribution shift. Our work explicitly focuses on boosting the data-efficiency of ope methods and lowers the error of ope estimates compared to Chang et al. (2022) and Wang et al. (2021).

Representation Learning via Behavioral SimilarityThe representation learning method we introduce builds upon prior work in learning representations in which similar states share similar representations. Much of this prior work is based on the notion of a bisimulation abstraction in which two states with identical reward functions and that lead to identical groups of next states should be classified as similar (Ferns et al., 2004, 2011; Ferns and Precup, 2014; Castro, 2019). The bisimulation metric itself is difficult to learn both computationally and statistically and so recent work has introduced various approximations (Castro et al., 2022; Castro, 2019; Zhang et al., 2021; Gelada et al., 2019). To the best of our knowledge, all of this work has considered the _online, control_ setting and has only focused on state representation learning. In contrast, we introduce a method for learning _state-action_ representations for ope with a fixed dataset. One exception is the work of Dadashi et al. (2021), which proposes to learn state-action representations for offline policy _improvement_. However, as we will show in Section 4, the distance metric that they base their representations on is inappropriate in the ope context.

## 3 ROPE: State-Action Behavioral Similarity Metric for Off-Policy Evaluation

In this section, we introduce our primary algorithm: **R**epresentations for **OPE** (rope), a representation learning method based on state-action behavioral similarity that is tailored to the off-policy evaluation problem. That is, using a fixed off-policy dataset \(\), rope learns similar representations for state-action pairs that are similar in terms of the action-value function of \(_{}\).

Prior works on representation learning based on state behavioral similarity define a metric that relates the similarity of two states and then map similar states to similar representations (Castro et al., 2022; Zhang et al., 2021a). We follow the same high-level approach except we focus instead on learning state-action representations for ope. One advantage of learning state-action representations over state representations is that we can learn a metric specifically for \(_{}\) by directly sampling actions from \(_{}\) instead of using importance sampling, which can be difficult when the multiple behavior policies are unknown. Moreover, estimating the importance sampling ratio from data is known to be challenging (Hanna et al., 2021; Yang et al., 2020).

Our new notion of similarity between state-action pairs is given by the recursively-defined rope distance, \(d_{_{}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+ \,_{s^{}_{1},s^{}_{2} P,a^{}_{1},a^{ }_{2}_{}}[d_{_{}}(s^{}_{1},a^{ }_{1};s^{}_{2},a^{}_{2})]\). Intuitively, \(d_{_{}}\) measures how much two state-action pairs, \((s_{1},a_{1})\) and \((s_{2},a_{2})\), differ in terms of short-term reward and discounted expected distance between next state-action pairs encountered by \(_{}\). In order to compute \(d_{_{}}\), we define the rope operator:

**Definition 1** (rope operator).: _Given an evaluation policy \(_{}\), the rope operator \(^{_{}}:^{X}^{ X}\) is given by:_

\[^{_{}}(d)(s_{1},a_{1};s_{2},a_{2}):=,a_{1})-r(s_{2},a_{2})|}_{}+_{s^{}_{1},s^{}_{2} P,a^{}_{1},a^{}_{2} _{}}[d(s^{}_{1},a^{}_{1};s^{}_{2},a^{ }_{2})]}_{}\] (1)

_where \(d:\), \(s^{}_{1} P(s^{}_{1}|s_{1},a_{1}),s^{}_{2} P(s^{ }_{2}|s_{2},a_{2}),a^{}_{1}_{}(|s^{}_ {1}),a^{}_{2}_{}(|s^{}_{2})\)_

Given the operator, \(^{_{}}\), we show that the operator is a contraction mapping, computes the rope distance, \(d_{_{}}\), and that \(d_{_{}}\) is a _diffuse metric_. For the background on metrics and full proofs, refer to the Appendix A and B.

**Proposition 1**.: _The operator \(^{_{}}\) is a contraction mapping on \(^{}\) with respect to the \(L^{}\) norm._

**Proposition 2**.: _The operator \(^{_{}}\) has a unique fixed point \(d_{_{}}^{X}\). Let \(d_{0}^{X}\), then \(_{t}^{_{}}_{t}(d_{0})=d_{_{}}\)._

Propositions 1 and 2 ensure that repeatedly applying the operator on some function \(d:\) will make \(d\) converge to our desired distance metric, \(d_{_{}}\). An important aspect of \(d_{_{}}\) is that it is a diffuse metric:

**Proposition 3**.: \(d_{_{}}\) _is a diffuse metric._

where a diffuse metric is the same as a psuedo metric (see Definition 3 in Appendix A) except that self-distances can be non-zero i.e. it may be true that \(d_{_{}}(s,a;s,a)>0\). This fact arises due to the stochasticity in the transition dynamics and action sampling from \(_{}\). If we assume a deterministic transition function and a deterministic \(_{}\), \(d_{_{}}\) will reduce to a pseudo metric, which gives zero self-distance. In practice, we use a sample approximation of the rope operator to estimate \(d_{_{}}\).

Given that \(d_{_{}}\) is well-defined, we have the following theorem that shows why it is useful in the ope context:

**Theorem 1**.: _For any evaluation policy \(_{}\) and \((s_{1},a_{1}),(s_{2},a_{2})\), we have that \(|q^{_{}}(s_{1},a_{1})-q^{_{}}(s_{2},a_{2})| d_{ _{}}(s_{1},a_{1},;s_{2},a_{2})\)._

Given that our goal is learn representations based on \(d_{_{}}\), Theorem 1 implies that whenever \(d_{_{}}\) considers two state-action pairs to be close or have similar representations, they will also have close action-values. In the context of ope, if the distance metric considers two state-action pairs that have _different_ action-values to be zero distance apart/have the same representation, then fge will have to output two different action-values for the same input representation, which inevitably means fge must be inaccurate for at least one state-action pair.

### Learning State-Action Representations with ROPE

In practice, our goal is to use \(d_{_{}}\) to learn a state-action representation \((s,a)^{d}\) such that the distances between these representations matches the distance defined by \(d_{_{}}\). To do so, we follow the approach by Castro et al. (2022) and directly parameterize the value \(d_{_{}}(s_{1},a_{1};s_{2},a_{2})\) as follows:

\[d_{_{}}(s_{1},a_{1};s_{2},a_{2})_{ }(s_{1},a_{1};s_{2},a_{2})(s_{1},a_{1})|| _{2}^{2}+||_{}(s_{2},a_{2})||_{2}^{2}}{2}\\ +(_{}(s_{1},a_{1}),_{}(s_{2},a_{2}))\] (2)

in which \(\) is parameterized by some function approximator whose parameter weights are denoted by \(\), \((,)\) gives the angular distance between the vector arguments, and \(\) is a parameter controlling the weight of the angular distance. We can then learn the desired \(_{}\) through a sampling-based bootstrapping procedure (Castro et al., 2022). More specifically, the following loss function is minimized to learn the optimal \(^{*}\):

\[_{}():=_{}[(|r(s_{ 1},a_{1})-r(s_{2},a_{2})|+\,_{_{}}[_{ }(s^{}_{1},a^{}_{1};s^{}_{2},a^{}_{2})]-_{}(s_{1},a_{1};s_{2},a_{2}))^{2}]\] (3)

where \(\) is separate copy of \(\) and acts as a target function approximator (Mnih et al., 2015), which is updated to \(\) at a certain frequency. Once \(_{^{*}}\) is obtained using \(\), we use \(_{^{*}}\) with fde to perform ope with the same data. Conceptually, the fqe procedure is unchanged except the learned action-value function now takes \(_{^{*}}(s,a)\) as its argument instead of the state and action directly.

With rope, state-action pairs are grouped together when they have small pairwise rope distance. Thus, a given group of state-action pairs have similar state-action representations and are behaviorally similar (i.e, have similar rewards and lead to similar future states when following \(_{}\)). Consequently, these state-action pairs will have a similar action-value, which allows data samples from any member of the group to learn the group's shared action-value as opposed to learning the action-value for each state-action pair individually. This generalized usage of data leads to more data-efficient learning. We refer the reader to Appendix C for rope's pseudo-code.

### Action-Value and Policy Value Bounds

We now theoretically analyze how rope state-action representations help fqe estimate \((_{})\). For this analysis, we focus on hard groupings where groups of similar state-action pairs are aggregated into one cluster and no generalization is performed across clusters; in practice, we learn state-action representations in which the difference between representations approximates the rope distance between state-action pairs. Furthermore, for theoretical analysis, we consider exact computation of the rope diffuse metric and of action-values using dynamic programming. First, we present the following lemma. For proofs, refer to Appendix B.

**Lemma 1**.: _Assume the rewards \(:()\) then given an aggregated mdp\(}=},}, },,,_{0}\) constructed by aggregating state-actions in an \(\)-neighborhood based on \(d_{_{}}\), and an encoder \(:}\) that maps state-actions in \(\) to these clusters, the action-value for the evaluation policy \(_{}\) in the two mdps are bounded as:_

\[|q^{_{}}(x)-^{_{}}((x))|\]

Lemma 1 states that the error in our estimate of the true action-value function of \(_{}\) is upper-bounded by the clustering radius \(d_{_{}}\), \(\). Lemma 1 then leads us to our main result:

**Theorem 2**.: _Under the same conditions as Lemma 1, the difference between the expected fitted \(q\)-evaluation (fde) estimate and the expected estimate of fqe+rope is bounded:_

\[\,_{s_{0},a_{0}_{}}[q^{_{}}(s_{ 0},a_{0})]-_{s_{0},a_{0}_{}}[q^{_{}}( (s_{0},a_{0}))]\]

Theorem 2 tells us that the error in our estimate of \((_{})\) is upper-bounded by the size of the clustering radius \(\). The implication is that grouping state-action pairs according to the rope diffuse metric enables us to upper bound error in the ope estimate. At an extreme, if we only group state-action pairs with _zero_ rope distance together then we obtain zero absolute error meaning that the action-value function for the aggregated mdp is able to realize the action-value function of the original mdp.

## 4 Empirical Study

In this section, we present an empirical study of rope designed to answer the following questions:

1. Does rope group state-actions that are behaviorally similar according to \(q^{_{e}}\)?
2. Does rope improve the data-efficiency of fqe and achieve lower ope error than other ope-based representation methods?
3. How sensitive is rope to hyperparameter tuning and extreme distribution shifts?

### Empirical Set-up

We now describe the environments and datasets used in our experiments.

**Didactic Domain.** We provide intuition about rope on our gridworld domain. In this tabular and deterministic environment, an agent starts from the bottom left of a \(3 3\) grid and moves to the terminal state at the top right. The reward function is the negative of the Manhattan distance from the top right. \(_{}\) stochastically moves up or right from the start state and then deterministically moves towards the top right, and moves deterministically right when it is in the center. The behavior policy \(_{b}\) acts uniformly at random in each state. We set \(=0.99\).

**High-Dimensional Domains.** We conduct our experiments on five domains: HumanoidStandup, Swimmer, HalfCheetah, Hopper, and Walker2D, each of which has \(393\), \(59\), \(23\), \(14\), and \(23\) as the native state-action dimension respectively. We set \(=0.99\).

**Datasets.** We consider \(12\) different datasets: \(3\) custom datasets for HumanoidStandup, Swimmer, and HalfCheetah; and \(9\) D4rl datasets [Fu et al., 2020] for HalfCheetah, Hopper, and Walker2D. Each of the three custom datasets is of size \(100\)K transition tuples with an equal split between samples generated by \(_{}\) and a lower performing behavior policy. For the d4rl datasets, we consider three types for each domain: random, medium, medium-expert, which consists of samples from a random policy, a lower performing policy, and an equal split between a lower performing and expert evaluation policy (\(_{}\)). Each dataset has 1M transition tuples. Note that due to known discrepancies between environment versions and state-action normalization procedures 1, we generate our own datasets using the publicly available policies2 instead of using the publicly available datasets. See Appendix D for the details on the data generation procedure.

**Evaluation Protocol.** Following Fu et al. , Voloshin et al.  and to make error magnitudes more comparable across domains, we use relative mean absolute error (RMAE). RMAE is computed using a single dataset \(\) and by generating \(n\) seeds: \(_{i}((_{e})):=(_{e})- _{i}(_{e})]}{[(_{e})-(_{})]}\), where \(_{i}(_{e})\) is computed using the \(i^{}\) seed and \((_{})\) is the value of a random policy. We then report the Interquartile Mean (iqm) [Agarwal et al., 2021b] of these \(n\) rmaes.

**Representation learning + OPE.** Each algorithm is given access to the same fixed dataset to learn \(q^{_{e}}\). The representation learning algorithms (rope and baselines) use this dataset to first pre-train a representation encoder, which is then used to transform the fixed dataset. This transformed dataset is then used to estimate \(q^{_{e}}\). Vanilla fqe directly operates on the original state-action pairs.

### Empirical Results

We now present our main empirical results.

#### 4.2.1 Designing ROPE: A State-Action Behavioral Similarity Metric for OPE

The primary consideration when designing a behavioral similarity distance function for ope, and specifically, for fqe is that the distance function should not consider two state-action pairs with different \(q^{_{e}}\) values to be the same. Suppose we have a distance function \(d\), two state-actions pairs, \((s_{1},a_{1})\) and \((s_{2},a_{2})\), and their corresponding \(q^{_{e}}\). Then if \(d(s_{1},a_{1};s_{2},a_{2})=0\), it should be the case that \(q^{_{e}}(s_{1},a_{1})=q^{_{e}}(s_{2},a_{2})\). On the other hand, if \(d(s_{1},a_{1};s_{2},a_{2})=0\) but \(q^{_{e}}(s_{1},a_{1})\) and \(q^{_{e}}(s_{2},a_{2})\) are very different, then fqe will have to output _different_ action-values for the _same_ input, thus inevitably making fqe inaccurate on these state-action pairs.

While there have been a variety of proposed behavioral similarity metrics for control, they do not always satisfy the above criterion for ope. We consider various state-action behavioral similarity metrics. Due to space constraints, we show results only for: on-policy mico[Castro et al., 2022]\(d_{_{}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+ }_{a^{}_{i},a^{}_{2}_{ }}[d_{_{}}((s^{}_{1},a^{}_{1}),(s^{ }_{2},a^{}_{2}))]\), which groups state-actions that have equal \(q^{_{}}\), and defer results for the random-policy metric [Dadashi et al., 2021] and policy similarity metric [Agarwal et al., 2021a] to the Appendix D.

We visualize how these different metrics group state-action pairs in our gridworld example where a state-action is represented by a triangle in the grid (Figure 1). The gridworld is \(3 3\) grid represented by \(9\) squares (states), each having \(4\) triangles (actions). A numeric entry in a given triangle represents either: 1) the action-value of that state-action pair for \(_{}\) (Figure 1(a)) or 2) the group ID of the given state-action pair (Figures 1(b) and 1(c)). Along with the group ID, each state-action pair is color-coded indicating its group. In this tabular domain, we compute the distances using dynamic programming with expected updates.

The main question we answer is: does a metric group two state-action pairs together when they have the same action-values under \(_{}\)? In Figure 1(a) we see the \(q^{_{}}\) values for each state-action where all state-action pairs that have the same action-value are grouped together under the same color (e.g. all state-action pairs with \(q^{_{}}(,)=-6\) belong to the same group (red)). In Figure 1(b), we see that rope's grouping is exactly aligned with the grouping in Figure 1(a) i.e. state-action pairs that have the same action-values have the same group ID and color. On the other hand, from Figure 1(c), we see that on-policy mico misaligns with Figure 1(a). In Appendix D, we also see similar misaligned groupings using the random-policy metric Dadashi et al.  and policy similarity metric Agarwal et al. [2021a]. The misalignment of these metrics is due to the fact that they do not group state-action pairs togethers that share \(q^{_{}}\) values.

#### 4.2.2 Deep OPE Experiments

We now consider ope in challenging, high dimensional continuous state and action space domains. We compare the rmae achieved by an ope algorithm using _different_ state-action representations as input. If algorithm A achieves lower error than algorithm B, then A is more data-efficient than B.

Custom Dataset ResultsFor the custom datasets, we consider mild distribution shift scenarios, which are typically easy for ope algorithms. In Figure 2, we report the rmae vs. training iterations of fqe with different state-action features fed into fqe. We consider three different state-action features: 1) rope (ours), 2) \(_{}\)-critic, which is a representation outputted by the penultimate layer of the action-value function of \(_{}\)[Wang et al., 2021], and 3) the original state-action features. Note that there is no representation _learning_ involved for 2) and 3). We set the learning rate for all neural network training (encoder and fqe) to be the same, hyperparameter sweep rope across \(\) and the dimension of rope's encoder output, and report the lowest rmae achieved at the end of fqe training. For hyperparameter sensitivity results, see Section 4.2.3. For training details, see Appendix D.

Figure 1: Figure (a): \(q^{_{}}\): center number in each triangle is the \(q^{_{}}\) for that state-action pair. Center and right: group clustering according to rope (ours; Figure (b)) and on-policy mico (Figure (c)) (center number in each triangle is group ID). Two state-action pairs are grouped together if their distance according to the specific metric is \(0\). The top right cell is blank since it is the terminal state and is not grouped.

We find that fqe converges to an estimate of \((_{})\) when it is fed these different state-action features. We also see that when fqe is fed features from rope it produces more data-efficient ope estimates than vanilla fqe. Under these mild distribution shift settings, \(_{}\)-critic also performs well since the output of the penultimate layer of \(_{}\)'s action-value function should have sufficient information to accurately estimate the action-value function of \(_{}\).

D4RL Dataset ResultsOn the d4rl datasets, we analyze the final performance achieved by representation learning + ope algorithms on datasets with varying distribution shift. In addition to the earlier baselines, we evaluate Bellman Complete Learning Representations (bcrl) (Chang et al., 2022), which learns linearly Bellman complete representations and produces an ope estimate with Least-Squares Policy Evaluation (lspe) instead of fqe. We could not evaluate \(_{}\)-critic since the d4rl\(_{}\) critics were unavailable3. For bcrl, we use the publicly available code 4. For a fair comparison, we hyperparameter tune the representation output dimension and encoder architecture size of bcrl. We hyperparameter tune rope the same way as done for the custom datasets. We set the learning rate for all neural network training (encoder and fqe) to be the same. In Table 1, we report the lowest rmae achieved at the end of the ope algorithm's training. For the corresponding training graphs, see Appendix D.

We find that rope improves the data-efficiency of fqe substantially across varying distribution shifts. bcrl performs competitively, but its poorer ope estimates compared to rope is unsurprising since it is not designed for data-efficiency. It is also known that bcrl may produce less accurate ope estimates compared to fqe (Chang et al., 2022). fqe performs substantially worse on some datasets;

    &  \\  Dataset & bcrl & fqe & rope (ours) \\  HalfCheetah-random & \(0.979 0.000\) & \(\) & \(0.990 0.001\) \\ HalfCheetah-medium & \(0.830 0.007\) & \(0.770 0.007\) & \(\) \\ HalfCheetah-medium-expert & \(0.685 0.013\) & \(0.374 0.001\) & \(\) \\  Walker2D-random & \(1.022 0.001\) & Diverged & \(\) \\ Walker2D-medium & \(0.953 0.019\) & Diverged & \(\) \\ Walker2D-medium-expert & \(0.962 0.037\) & Diverged & \(\) \\  Hopper-random & Diverged & Diverged & \(\) \\ Hopper-medium & \(61.223 92.282\) & Diverged & \(\) \\ Hopper-medium-expert & \(9.08 4.795\) & Diverged & \(\) \\   

Table 1: Lowest rmae achieved by algorithm on d4rl datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Algorithms that diverged had a significantly high final error and/or upward error trend (see Appendix D for training curves). Lower is better.

Figure 2: rmae vs. training iterations of fqe on the custom datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

however, it is known that fqe can diverge under extreme distribution shift (Wang et al., 2020, 2021). It is interesting, however, that rope is robust in these settings. We observe this robustness across a wide range of hyperparameters as well (see Section 4.2.3). We also find that when there is low diversity of rewards in the batch (for example, in the random datasets), it is more likely that the short-term distance component of rope is close to \(0\), which can result in a representation collapse.

#### 4.2.3 Ablations

Towards a deeper understanding of rope, we now present an ablation study of rope.

Hyperparameter SensitivityIn ope, hyperparameter tuning with respect to rmae is difficult since \((_{e})\) is unknown in practice (Paine et al., 2020). Therefore, we need ope algorithms to not only produce accurate ope estimates, but also to be robust to hyperparameter tuning. Specifically, we investigate whether rope's representations produce more data-efficient ope estimates over fqe across rope's hyperparameters. In this experiment, we set the action-value function's learning rate to be the same for both algorithms. The hyperparameters for rope are: 1) the output dimension of the encoder and 2) \(\), the weight on the angular distance between encodings. We plot the results in Figure 3 and observe that rope is able to produce substantially more data-efficient estimates compared to fqe for a wide range of its hyperparameters on the Walker2D-medium dataset, where fqe diverged (see Table 1). While it is unclear what the optimal hyperparameters should be, we find similar levels of robustness on other datasets as well (see Appendix D).

ROPE Representations Mitigate FQE DivergenceIt has been shown theoretically (Wang et al., 2020) and empirically (Wang et al., 2021) that under extreme distribution shift, fqe diverges i.e. it produces ope estimates that have arbitrarily large error. In Table 1, we also see similar results where fqe produces very high error on some datasets. fqe tends to diverge due to the deadly triad (Sutton and Barto, 2018): 1) off-policy data, 2) bootstrapping, and 3) function approximation.

A rather surprising but encouraging result that we find is that even though rope faces the deadly triad, it produces representations that _significantly_ mitigate fqe's divergence across a large number of trials and hyperparameter variations. To investigate how much rope aids convergence, we provide the performance profile5(Agarwal et al., 2021) based on the rmae distribution plot in Figure 4. Across all trials and hyperparameters, we plot the fraction of times an algorithm achieved an error less than some threshold. In addition to the earlier baselines, we also plot the performance of 1) fqe-clip which is fqe but whose bootstrapping targets are clipped between \([}}{1-r_{}},}}{1-r_{ }}]\), where \(r_{}\) and \(r_{}\) are the minimum and maximum rewards in the fixed dataset; and 2) fqe-deep, which is regular fqe but whose action-value function network is double the capacity of fqe (see Appendix D for specifics).

From Figure 4, we see that nearly \( 100\%\) of the runs of rope achieve an rmae of \( 2\), while none of the fqe and fqe-deep runs produce even \( 10\) rmae. The failure of fqe-deep suggests that the

Figure 3: Hyperparameter sensitivity. fqe vs. rope when varying ropeâ€™s encoder output dimension (top) and \(\) (bottom) on the Walker2D-medium d4rl dataset. 1om of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

extra capacity rope has over fge (since rope has its own neural network encoder) is insufficient to explain why rope produces accurate ope estimates. We also find that in order to use fge with the native state-action representations, it is necessary to use domain knowledge and clip the bootstrapped target. While fge-clip avoids divergence, it is very unstable during training (see Appendix D). rope's ability to produce stable learning in fge without any clipping is promising since it suggests that it is possible to improve the robustness of fge if an appropriate representation is learned.

## 5 Limitations and Future Work

In this work, we showed that rope was able to improve the data-efficiency of fge and produce lower-error ope estimates than other ope-based representations. Here, we highlight limitations and opportunities for future work. A limitation of rope and other bisimulation-based metrics is that if the diversity of rewards in the dataset is low, they are susceptible to representation collapse since the short-term distance is close to \(0\). Further investigation is needed to determine how to overcome this limitation. Another very interesting future direction is to understand why rope's representations significantly mitigated fge's divergence. A starting point would be to explore potential connections between rope and Bellman complete representations (Szepesvari and Munos, 2005) and other forms of representation regularizers for fge6.

## 6 Conclusion

In this paper we studied the challenge of pre-training representations to increase the data efficiency of the fge ope estimator. Inspired by work that learns state similarity metrics for control, we introduced rope, a new diffuse metric for measuring behavioral similarity between state-action pairs for ope and used rope to learn state-action representations using available offline data. We theoretically showed that rope: 1) bounds the difference between the action-values between different state-action pairs and 2) results in bounded error between the value of \(_{}\) according to the ground action-value and the action-value function that is fed with rope representations as input. We empirically showed that rope boosts the data-efficiency of fge and achieves lower ope error than other ope-based representation learning algorithms. Finally, we conducted a thorough ablation study and showed that rope is robust to hyperparameter tuning and _significantly_ mitigates fge's divergence, which is a well-known challenge in ope. To the best of our knowledge, our work is the first that successfully uses representation learning to improve the data-efficiency of ope.

Figure 4: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \( 20\) runs for each algorithm. The shaded region is a \(95\%\) confidence interval. Larger area under the curve is better. For visualization, we cut off the horizontal axis at \(10\) rmae. fge and fge-deep are flat at \(0\) i.e. neither had runs that produced an error less than \(10\).

## Remarks on Negative Societal Impact

Our work is largely focused on studying fundamental rl research questions, and thus we do not see any immediate negative societal impacts. The aim of our work is to enable effective ope in many real world domains. Effective ope means that a user can estimate policy performance prior to deployment which can help avoid deployment of poor policies and thus positively impact society.