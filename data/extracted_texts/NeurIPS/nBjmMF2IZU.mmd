# Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning

Yuexiang Zhai\({}^{1}\) Hao Bai\({}^{2}\) Zipeng Lin\({}^{1}\) Jiayi Pan\({}^{1}\) Shengbang Tong\({}^{3}\) Yifei Zhou\({}^{1}\)

Alane Suhr\({}^{1}\) Saining Xie\({}^{3}\) Yann LeCun\({}^{3}\) Yi Ma\({}^{1}\) Sergey Levine\({}^{1}\)

\({}^{1}\)UC Berkeley UIUC \({}^{3}\)NYU

Project Lead, email: simonzhai@berkeley.edu. Project page: https://rl4vlm.github.io/Equal contribution, listed in alphabetical order, see Appendix A for list of contributions.

###### Abstract

Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM

Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM

Figure 1: **Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.**agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.

## 1 Introduction

Large vision-language models (VLMs) [7; 44; 18] demonstrate remarkable capabilities as general-purpose agents in solving various tasks through language reasoning. In particular, fine-tuning VLMs with specialized visual instruction following data appears to be a key technique for improving the capabilities of VLMs [34; 84; 33; 30]. However, visual instruction tuning may not be optimal for training decision-making agents in multi-step interactive environments requiring visual recognition and language understanding, as visual instruction tuning mainly performs supervised learning on pre-collected datasets without interacting with the environments . Consequently, if the pre-collected datasets lack sufficient diversity to cover a wide range of decision-making scenarios, visual instruction tuning may fail to improve the VLM agent's decision-making capabilities.

To unleash the learning capabilities of VLM agents in multi-step goal-directed decision-making environments, reinforcement learning (RL), a method that has proven effective in training multi-step interactive agents [41; 59; 6; 69], naturally offers a paradigm that supports this purpose. However, while RL has been widely adopted for training purely text-based tasks for large language models (LLMs) [60; 50; 1; 83], end-to-end VLM fine-tuning with RL for goal-directed multi-step tasks has not yet been studied, to the best of our knowledge.

Our main contribution in this paper is an algorithmic framework that directly fine-tunes VLMs with RL for multi-step goal-directed decision-making tasks requiring vision-language understanding. In our framework, the VLM first receives a task description prompt, which guides it to generate task-specific chain-of-thought (CoT) reasoning [75; 73] (blue parts in Figure 1), followed by a text-based action (red parts in Figure 1). The CoT reasoning is designed for efficient explorations by prompting the VLMs to generate intermediate reasoning that leads to the final text-based action. Our framework then parses the text-based actions into executable actions for the environment, which generates potentially goal-directed rewards and the next state for RL training.

To evaluate the effectiveness of our method in enhancing a VLM's decision-making capabilities, we adopt a 7b model  as the backbone VLM and apply our method to five decision-making tasks. These tasks come from two domains: an original domain, which evaluates the VLM's decision-making capabilities requiring fine-grained visual recognition and language reasoning, and an embodied AI domain  focusing on testing tasks demanding visual semantic reasoning capabilities. Empirical results show that our method enhances the decision-making capabilities of VLMs in both domains, enabling 7b models to surpass the performance of commercial models such as GPT4-V  and Gemini . Moreover, our experiments reveal that CoT reasoning is crucial for performance improvement in our RL training. Specifically, we test our method on the same tasks _without_ the CoT reasoning and observe a significant drop in overall performance in both domains.

## 2 Related Work

Training LLMs or VLMs with RL.RL has been widely adopted for training LLMs and VLMs [85; 61; 70; 45; 10; 50; 9; 43; 18; 62; 60; 1; 20; 83]. Some studies [85; 61; 45; 10; 43; 18; 62] focus on applying RL from human feedback (RLHF), which involves learning reward models from human feedback before deploying RL. Other research [50; 9; 60; 1; 20; 83] focuses on deploying RL with task-specific reward functions without using human preference data. Our paper is similar to the latter [50; 9; 60; 1; 20; 83] which applies RL to train LLMs on customized reward functions from different environments. There are two major differences between our paper and prior works [50; 60; 1; 20; 83]. Firstly, our method incorporates visual inputs, broadening its applicability to a wider range of tasks that require vision-language understanding or multimodal reasoning [29; 38]. Secondly, while previous works do not explore how CoT reasoning affects RL training on large models in general, we identify CoT reasoning as a crucial component for enhancing RL training. We empirically observe that incorporating CoT reasoning significantly improves the overall performance of RL training on _all_ tested domains.

Adopting LLMs and VLMs as decision-making agents.Many prior works have studied various methods of using frozen LLMs and VLMs for decision-making. One line of work studies the prompting techniques [75; 14; 79; 78; 74; 31; 76; 47; 71; 48; 24] for enhancing the decision-making capabilities of large foundation models, see Dong et al. , Yang et al.  for a detailed survey for other prompting based methods. Our work differs from all prompting-based methods since we directly use RL to fine-tune the entire VLM as decision-making agents. Other studies [42; 64; 4; 52; 11] integrate frozen VLMs to LLMs into their training pipeline for processing task descriptions or feature extraction, without using text-based actions. focuses on integrating different components from VLMs for downstream RL training. For example, some studies use the VLMs or CLIP vision encoder [46; 42; 64] as reward models for training, which differs from our method since we adopt rewards from the environments. Other studies [42; 64; 11] integrate frozen VLMs/LLMs into their training pipeline for processing task descriptions [42; 64; 46] or feature extraction , without using text-based actions. Our paper differs from these works [42; 64; 11] in two major aspects. From a technical perspective, we focus on a more challenging paradigm by directly fine-tuning the entire VLM with RL, whereas previous methods [42; 64; 11] only train additional MLP or transformer layers to connect the frozen LLM/VLM with the action space. More importantly, our method directly interacts with the environments using _open-ended text_, enabling it to utilize the CoT reasoning capability of VLMs for more efficient explorations for decision-making.

Evaluating VLMs as decision-making agents.Previous studies have thoroughly examined the fundamental evaluations of VLMs in non-interactive tasks [3; 37; 80; 32; 65; 81; 16]. Our focus, however, is on evaluating a VLM's decision-making capabilities in interactive environments that require both visual recognition and language reasoning. Representative interactive environments include purely text-based environments [13; 28; 72] or embodied AI environments [40; 58; 56; 15]. We adopt the ALFWorld  embodied environment for evaluating our method's ability to improve VLM's visual semantic reasoning capabilities. In addition to the ALFWorld embodied AI environment, we also design an original "gym-like"  environment to test VLM's decision-making capabilities in tasks that require fine-grained visual recognition and language reasoning.

CoT prompting.Recent studies in prompting for LLMs have demonstrated the crucial role of CoT in enhancing complex reasoning capabilities [75; 26; 17; 73; 82; 79]. Wei et al.  show that CoT reasoning can significantly boost LLMs' performance across different reasoning tasks by showing that adding simple exemplar-based prompts, leading to better performance on benchmarks such as the GSM8K . A follow-up study  proposes a novel self-consistency decoding strategy that explores multiple reasoning paths, demonstrating substantial gains in arithmetic and commonsense reasoning tasks. Other works [26; 82; 17] have shown that adding prompts to break complex tasks into subtasks and solve them step-by-step significantly improves LLM's reasoning capability. Our work differs from these CoT prompting studies as we aim to provide an algorithmic framework that can train VLMs with RL, where the CoT prompting appears as a key component of the framework. In contrast, prior works focus on improving the reasoning capabilities of LLMs with increasingly sophisticated prompting of frozen models.

## 3 Preliminaries

Standard RL terminologies.We follow the standard notations from classic RL literature [63; 2]. Specifically, we use \(=\{,,P,r,\}\) to denote an MDP, where \(\) denotes the state space, \(\) denotes the action space, \(P\) denotes the transition dynamics, \(r:\) denotes the reward function and \(\) denotes the discount factor. Our goal is to learn a policy \(:\) that maximizes the overall discounted return \(_{}_{}[_{t=0}^{T}^{t}r(s_{t},a_{t})]\), where \(T\) is the maximum number of steps per episode. Without loss of generality, we use \((a|s)\) to denote probability of \(\) choosing \(a\) at \(s\).

Adapting the RL formalism to VLMs.We use \(\) to denote the discrete and finite vocabulary (token) space, and we use \(^{m},^{n}\) to represent the input and output text space, where \(m\) and \(n\) represent the maximum token length of the input and output sequence. We adapt the RL formalism to VLMs by treating the combination of the _vision and language inputs_ to VLMs as the state space: \(=^{m}\), where \(\) is the space of all RGB images. We view each utterance [1; 83] of the language outputs from VLMs as the action space \(^{n}\). Therefore, the input and output of a VLM policy with parameter \(\) can be written as \(_{}:^{m}^{n}\). For example, in the Blackjack task shown in Figure 1, each state \(s\) consists of an RGB image \(o\) with the cards of the dealer and the player, as well as an input prompt \(^{}\) with maximum token length \(m\), and the text output \(^{}=_{}(o,^{})\) (with a maximum token \(n\)) will later be parsed as an action to interact with the environment. Similar to the standard RL setting, we use \(_{}(^{}|o,^{})\) to denote the probability of a VLM policy \(_{}\) outputting \(^{}\) with input image \(o\) and prompt \(^{}\).

## 4 Training VLMs with RL

Compared to classic MLP-based policy networks [53; 54; 55; 19], a natural advantage of VLM policies is that they can leverage CoT reasoning for efficient exploration, by performing intermediate reasoning steps that lead to the final decision. However, training a VLM policy \(_{}\) with RL presents additional challenges. First, the VLM policy \(_{}(o,^{})\) directly generates open-ended text rather than vectorized actions in classic policy gradient-based RL methods [53; 54; 55; 19], complicating direct interaction with the environment. Even with a parsing mechanism \(f:^{n}\) that maps open-ended text \(^{}\) to a _legal_ action \(a\) for interaction with the environment, it remains unclear how to estimate the action probability \(_{}(a|o,^{})\) from the text generation process.

Figure 2 presents an overview of our framework, leveraging the CoT reasoning and addressing the two aforementioned challenges. We design a task-specific prompt \(^{}\) that requires the VLM to generate a formatted output \(^{}\), including the CoT reasoning. Next, we adopt a post-processing function \(f\) to parse open-ended text into a _legal_ action \(a_{t}\) that can directly interact with the environment. To compute \(_{}(a|o,^{})\), we develop a method to estimate its value based on the probability of each output token in \(^{}\).

The remaining Section is structured as follows. First, we describe the format of our input prompt \(^{}_{t}\) and the desired output \(^{}_{t}\) (Section 4.1). Next, we present the post-processing function \(f\) (Section 4.2). Then, we introduce a method to compute a numerical value of \(_{}(a_{t}|o_{t},^{}_{t})\) (Section 4.3). Finally, we conclude our framework in Algorithm 1 (Section 4.4).

### Prompt Design for Domain-Specific Outputs

For each task \(\), our input prompt \(^{}_{t}\) contains a description of the task, the legal action space of the current observation, and the desired output format (including the CoT reasoning). Our desired output \(^{}_{t}\), contains a CoT reasoning followed by the keywords "action" : "\(a_{t}\)" for post-processing. Figure 3 provides an example of our input prompt \(^{}_{t}\) and the desired formatted output \(^{}_{t}\). In particular, we define a function \(h\) which constructs \(^{}_{t}\) from the current observation \(o_{t}\): \(^{}_{t}=h(o_{t})\), to accommodate for tasks that may contain observation-dependent information.3 We provide additional examples of \(^{}\) and \(^{}\) in Appendix B.

Figure 2: **A diagram of the proposed RL fine-tuning framework.** At time step \(t\), the state \(s_{t}\) contains an input prompt \(^{}_{t}\) and a visual observation \(o_{t}\). The VLM takes \(s_{t}=[o_{t},^{}_{t}]\) as input and outputs open-ended text \(^{}_{t}\) containing the CoT reasoning, keywords â€œaction" : "\(a_{t}\)", and the log-likelihood of \(^{}_{t}\). We first apply a post-processing function \(f\) on \(^{}_{t}\), to obtain a _legal_ action \(a_{t}\) which can interact with the environment. Then, we input \(a_{t}\) to the environment for obtaining reward \(r(s_{t},a_{t})\) and the next observation \(o_{t+1}\). Afterward, we devise a method to compute a numerical value of \(_{}(a_{t}|o_{t},^{}_{t})\). Finally, we use \(r(s_{t},a_{t})\) and \(_{}(a_{t}|o_{t},^{}_{t})\) for the RL training.

### Post-Processing Open-Ended Text for Legal Actions

Our post-processing mechanism involves both \(_{t}^{}\) and \(f\). In the input prompt \(_{t}^{}\), we directly ask the VLM to output a text-based action in the format of "action" : "\(a_{t}\)" (see Figure 1 and Figure 2 for examples). After obtaining \(_{t}^{}\), our post-processing function \(f\) directly searches for the text-based keywords "action" : "\(a_{t}\)" from \(_{t}^{}\), and maps it to a legal action \(a_{t}\), either in symbolic or in text depending on the task of interest. For the case shown in Figure 1, \(f\) will map \(_{t}^{}\) to the symbolic operator that represents the action "stand" in the Blackjack task (to be introduced in Section 5.1), as the Blackjack task takes symbolic actions as input. For the alfworld environment shown in Figure 2, \(f\) will map \(_{t}^{}\) to the text "look", because the alfworld environment takes text-based actions as inputs.

However, VLMs are not always guaranteed to generate a \(_{t}^{}\) that contains the keywords "action" : "\(a_{t}\)", even when we explicitly request a formatted output from \(_{t}^{}\). To continue the RL training when \(_{t}^{}\) does not contain any legal action, we perform _random exploration_ by selecting a legal action \(a_{t}\) uniformly at random. Mathematically, \(f\) is defined as follows:

\[f(^{})=a,&^{},\\ (),&.\] (4.1)

### Estimating Action Probabilities of VLM Policies

To estimate the action probability \(_{}(a_{t}|o_{t},_{t}^{})\) (or equivalently \(_{}(a_{t}|o_{t},_{t}^{})\)) for policy gradient-based methods , a naive calculation is directly using \(_{}(_{t}^{}|o_{t},_{t}^{})\) as \(_{}(a_{t}|o_{t},_{t}^{})\), by summing the log-likelihood of all tokens in \(_{t}^{}\). This is because

\[&_{}(_{t}^{}|o_{t}, _{t}^{})=,_{t}^{},_{t}^{ })}{P(o_{t},_{t}^{})}\\ =&[,_{t}^{},_{[:n]})}{P(o_{t},_{t}^{},_{[:n-1]})},_{t}^{},_{[:2]})}{P(o_{t},_{t}^{},_{[:1]})},_{t}^{},_{[:1]})}{P(o_{t},_ {t}^{},_{[:i-1]})}]=_{i=1}^{n}[,_{t}^{},_{[:i]})}{P(o_{t},_{t}^{},_{[:i-1]})}].\] (4.2)

In the equation above, we use \(\) to denote the output token \(_{t}^{}\) for simplicity, and we use \(_{[:i]}\) to denote the first \(i\) tokens in \(_{t}^{}\), and we slightly abuse our notion by using \(P(o_{t},_{t}^{},_{[:0]})\) to denote \(P(o_{t},_{t}^{})\) in the log summation. Hence, a natural way to compute a numerical value for \(_{}(a_{t}|o_{t},_{t}^{})\) is \(_{i=1}^{n}[,_{t}^{},_{[:i]})}{P (o_{t},_{t}^{},_{[:i-1]})}]\).

However, the naive calculation \(_{}(a_{t}|o_{t},_{t}^{})_{i=1}^{n} [,_{t}^{},_{[:i]})}{P(o_{t},_ {t}^{},_{[:i-1]})}]\) may not be ideal for computing \(_{}(a_{t}|o_{t},_{t}^{})\) since our formatted output \(_{t}^{}\) also contains CoT reasoning. This is because in \(_{t}^{}=[v_{t}^{},v_{t}^{}]\), the CoT reasoning tokens \(v_{t}^{}\) are generally much

Figure 3: **A template of our input prompt and output text. The blue part represents the CoT reasoning and the red part is the text-based action. Note that the CoT reasoning may contain other task-specific descriptions, see Appendix B for more details.**longer than the action tokens \(v^{}}_{}}\) (see the blue and red parts in Figure 3 for examples, and see Table 1 for a relative scaling of their sum log-likelihood). Hence the naive computation \(_{}(a_{t}|o_{t},^{}}_{t})_{ }(v^{}}_{t}|o_{t},^{}}_{t})+_ {}(v^{}}_{t}|o_{t},^{}}_{t},^ {}}_{t})\) will make \(_{}(a_{t}|o_{t},^{}}_{t})\).

As shown in Table 1, \(_{}(v^{}}_{t}|o_{t},^{}}_{t})\) typically has a much larger magnitude than \( P(v^{}}_{t}|o_{t},^{}}_{t},^ {}}_{t})\) across all tasks we have tested (in terms of absolute value). Hence, to mitigate the effect of the CoT tokens, we adopt a scaling factor \(\) to scale down \(_{}(v^{}}_{}}|o_{t},^{ }}_{t})\) for obtaining a regularized version of \(_{}(a_{t}|o_{t},^{}}_{t})\), which results in

\[_{}(a_{t}|o_{t},^{}}_{t})\] (4.3) \[_{}(v^{}}_{}}|o_{t},^{}}_{t})+_{}(v^{}}_{t}|o_{t},^{}}_{t},^{} }_{t}).\]

Empirically, we observe the scaling factor \(\) could largely affect the final performance. As we will show in Section 6.2, choosing an extreme \(\) value (close to 1 or 0) will degrade overall performance. All of our experiments adopt \([0.2,0.5]\).

### Formal Implementation

Putting the prompt construction function \(h\) (Section 4.1), the post-processing function \(f\) (Section 4.2), and the computation of \(_{}(a_{t}|o_{t},^{}}_{t})\) (Section 4.3) together, we conclude our method in Algorithm 1.

```
1:Input: An environment env, an initial VLM with parameters \(_{0}\).
2:Input: A post-processing function \(f\), a CoT reasoning scaling factor \(\).
3:Input: Replay buffer size \(B\), maximum episode length \(T\).
4:for\(k=0,,K-1\)do\(\) Reset RL time step
5:\(t=0\)\(\) Reset the initial state
6:\(v^{}}_{t}=h(o_{t})\)\(\) Generate \(^{}}_{t}\) from \(o_{t}\), \(h\) is defined in Section 4.1
7:\(}_{k}=\)\(\) Initialize an on-policy replay buffer
8:while\(|_{k}| B\)do
10:\(^{}}_{t}=_{_{k}}(o_{t},^{}}_{t})\)\(\) Generate text output
11:\(a_{t}=f(^{}}_{t})\)\(\) Obtain a legal action from \(^{}}_{t}\), \(f\) is defined in Equation 4.1
12:\(_{_{k}}(a_{t}|o_{t},^{}}_{t})=_ {_{k}}(v^{}}_{}}|^{}}_ {t})+_{_{k}}(v^{}}_{t}|o_{t},^{}}_{t},^{}}_{t})\)\(\) Equation 4.2
13:\(r_{t},o_{t+1}=(a_{t})\)\(\) Repeat the initial state
14:\(_{k}=_{k}\{(o_{t},a_{t},r_{t},^{}}_{t},_{_{k}}(a_{t}|o_{t},^{}}_{t}))\}\)\(\) Add data to the buffer \(_{k}\)
15:\(t=t+1\)
16:if\(t=T\)then
17:\(t=0\)\(\) Reset RL time step if the maximum step is reached
18:\(o_{0}=()\)\(\) Reset environment
19:endif
20:\(^{}}_{t}=h(o_{t})\)\(\) Prepare the next \(^{}}_{t}\)
21:endwhile
22:Run PPO  with data \(_{k}\) to obtain \(_{k+1}\)
23:endfor
24:Output:\(_{K}\). ```

**Algorithm 1** Training VLM with RL

## 5 Evaluation Tasks

How does our method improve a VLM's decision-making capabilities in tasks that require fine-grained vision-language reasoning or semantic understanding? To study this question, we adopt two different domains: gym_cards and alfworld. Our original gym_cards domain is a "gym-like" environment  containing four tasks designed to test the decision-making capabilities of VLMs.

   log & NL & BJ & EZP & P24 & ALF \\  \(v^{}}_{}}\) & -3.4 & -2.2 & -9.0 & -37.6 & -20.3 \\ \(v^{}}_{}}\) & 0.0 & 0.0 & 0.0 & 0.0 & -0.4 \\   

Table 1: **The absolute values of sum log probability of \(v^{}}_{}}\) is much larger than \(v^{}}_{}}\). Each number is averaged among 1000 samples on our evaluation tasks to be introduced in Section 5.**These tasks require fine-grained visual-language reasoning, specifically focusing on recognizing numbers for arithmetic reasoning. In addition, we also adopt alfworld, which assesses the decision-making capabilities of VLMs in an embodied AI setting that demands visual semantic understanding. We present some examples of the visual observations of each task in Figure 4. We do not include standard image-based Atari benchmarks [5; 39] due to limited computation resources.4

### Gym Cards

Our gym_cards domain is designed to evaluate a VLM's decision-making capabilities requiring fine-grained vision recognition and language reasoning. More precisely, tasks in the gym_cards domain require the VLM to recognize the numbers (potentially from cards) and utilize the numbers for language reasoning. As depicted in Figure 4, the first three tasks--NumberLine, EZPoints, and Points24--are deterministic, and developed to assess the VLMs' ability to identify and process numbers or mathematical operators at each time step. These tasks increase in complexity: NumberLine requires recognition of two numbers in an image, EZPoints involves identifying numbers from two cards, and Points24 extends to recognizing four cards. The Blackjack task challenges the VLM further by requiring the agent to reason based on visual information and adapt to stochastic outcomes. This subsection outlines the goals of each task, and we leave the detailed descriptions of their state spaces, action spaces, and reward functions to Appendix B.1.

NumberLine.In this task, **the goal is to move a number to the target on a synthetic number line.** At each state \(s_{t}\), the visual observation \(o_{t}\) contains two lines of text: "Target: \(x\)" and "Current: \(y_{t}\)". The agent needs to move the current number \(y_{t}\) to the target number \(x\), by outputting text \(_{t}^{}\) that interacts with the discrete action space {"+", "\(-\)"}. Mapping the \(_{t}^{}\) to "+" or "\(-\)" will increase or decrease the current number by 1, respectively.

EZPoints.In this task, **the goal is to output a formula using the numbers in the cards that evaluates to 12.** At each state \(s_{t}\), the agent observes an image of two cards and a text version of (potentially incomplete) "formula" below the cards. The goal is to use _all_ numbers in the cards (only once) to compute 12. The action space contains natural numbers in \(\), as well as operator in {"+", "*", "="}. At each state \(s_{t}\), only operators and numbers that appear in the cards are _legal_ actions, and "J", "Q", or "K" are treated as "10". In particular, if the output text \(_{t}^{}\) is mapped to a legal action \(a_{t}\) at state \(s_{t}\), the text version of \(a_{t}\) will be appended to the "formula" in the current image of \(s_{t}\) resulting \(s_{t+1}\), otherwise \(s_{t+1}\) will remain the same as \(s_{t}\).

Points24.In this task, **the goal is to output a formula using the numbers in the cards that evaluates to 24.** The Points24 task is a harder version of EZPoints as it contains 4 cards, hence requiring the VLMs to generate a longer formula. The rules of Points24 are similar to EZPoints, despite two minor differences: the Points24 task requires the VLM to compute a target number of 24, and its action space contains more operators: {"+", "-", "*", "/", "="}.

Blackjack.In this task, **the goal is to win the current blackjack game.** At each state \(s_{t}\), the visual observation \(o_{t}\) consists of two cards (one face-down) from the dealer and all cards from the player.

Figure 4: **Examples of observation of our evaluation tasks**. (a)-(d) are from our original gym_cards domain. (a)-(c) are deterministic tasks with _increasing difficulties_; (d) is a stochastic task.

The agent's goal in this task is to win the current game, by outputting text \(_{t}^{}\) that can be mapped to {"stand", "hit"}. The agent will receive one more card if \(_{t}^{}\) is mapped to "hit", and the game will terminate if \(_{t}^{}\) is mapped to "stand".

### ALFWorld

While the gym_cards domain is designed to assess the VLM's arithmetic reasoning requiring fine-grained visual recognition, the alfworld environment aims at testing VLM's decision-making tasks requiring visual semantic understanding.

ALFWorld.The ALFWorld embodied environment  is combines a text-based interactive environment  with a large vision-language instruction following dataset . It contains 6 different types of goal-conditioned tasks ("Pick & Place", "Examine in Light", "Clean & Place", "Heat & Place", "Cool & Place", and "Pick Two & Place"), and **the agent's goal is to navigate in the environment via text-based actions** (e.g., "go to shelf 1", "examine sideltable 1"). Unlike our original gym_cards environment, where all states share the same action space, the alfworld environment contains a state-dependent _admissible action_ space - some actions are only available at certain states. For example, if the agent's goal is to "put some pillows on armchair", then the agent can only put a pillow _after_ picking up a pillow. Hence, to incorporate the state-dependent admissible action set, our prompt of alfworld asks the VLM to choose among an admissible action. See Figure 2 for an example of the visual observation of alfworld. We leave the detailed descriptions of the alfworld (state space, action space, reward functions, and the CoT prompt) to Appendix B.2.

## 6 Experimental Results

The first part of our experiment examines how our method improves the decision-making capabilities of VLMs (Section 6.1). The second part investigates the role of CoT reasoning in our method (Section 6.2). Details of our experimental setup are provided in Appendix C.

### Improving VLM Decision-Making Capabilities

Does our method improve the decision-making capabilities of VLM agents across various domains? To investigate this, we assess how our method improves arithmetic tasks requiring fine-grained visual recognition in the gym_cards domain and visual semantic reasoning in the alfworld domain. The gym_cards experiments include deterministic tasks (NumberLine, EZPoints, and Points24, each with increasing difficulty) and a stochastic task (Blackjack). In the alfworld domain, we evaluate overall performance and detailed task-specific performance as discussed in Section 5.2. We instantiate our method on top of the lava-v1.6-mistral-7b  model and compare it against commercial models (GPT4-V and Gemini), a supervised fine-tuned version of the lava-v1.6-mistral-7b model (LLaVA-sft),5 and a vanilla RL implementation using a CNN-based policy network (CNN+RL).6 The final results and learning curves are presented in Table 2 and Figure 5. Details of the experimental setup are provided in Appendix C.

Enhancing decision-making capabilities of VLM agents across various tasks.As illustrated in Table 2 and Figure 5, our method demonstrates consistent improvement across various tasks, including deterministic (NumberLine and EZPoints)7 or stochastic (Blackjack) arithmetic tasks and visual semantic reasoning task (alfworld). Specifically, our method improves the average performance from the initial LLaVA-sft model by **27.1%** on arithmetic tasks (18.4% \(\) 45.5%) and 

[MISSING_PAGE_FAIL:9]

The crucial role of CoT reasoning in performance improvement.As presented in Table 3 and Figure 6, the performance of our method significantly decreases without the CoT reasoning.8 Besides the improvement in the final performance, CoT reasoning appears to be a crucial component for deterministic arithmetic tasks (NumberLine and EZPoints), as our method fails to improve these two tasks without the CoT reasoning.

The importance of moderate scaling factors \(\).As discussed in Section 4.3, integrating CoT reasoning into our framework involves tuning an additional hyperparameter, \(\) (proposed in Equation 4.3). To identify an optimal range for \(\), we conduct experiments assessing the impact of various \(\). Our results in Figure 7 indicate that a moderate \(\) (between 0.3 and 0.5) enables effective training on the NumberLine task. Conversely, our method fails when \(\) is set too large (\( 0.7\)) or too small (\( 0.1\)), and we empirically find that an optimal \(\) typically falls within 0.2 to 0.5. This is because a large \(\) results in an estimate of \(_{}(a_{t}|o_{t},_{t}^{})\) being overly influenced by \(_{}(v_{t}^{}|o_{t},_{t}^{})\), while a small \(\) value causes \(_{}\) to be predominantly affected by \(_{}(v_{t}^{}|o_{t},_{t}^{},_{t}^{ })\), thereby reducing the effect of the CoT reasoning in RL training.

## 7 Conclusions, Limitations, and Future Directions

In this paper, we introduce an algorithmic framework that directly fine-tunes VLMs using RL, with the help of the VLM's CoT reasoning capability. Empirical results demonstrate that our method can enhance the decision-making abilities of VLMs across diverse domains that require fine-grained visual recognition or visual semantic understanding. In addition, we demonstrate that CoT reasoning is a crucial component for enabling RL training, allowing 7b VLMs to outperform established commercial models such as GPT-4V and Gemini on most tasks. While our results suggest that CoT reasoning is crucial to the performance improvement of VLM training with RL, we have not extensively explored the effects of different prompting techniques in this work, which will be an interesting future direction. The performance gain of our method is also limited by the size of the action space and the difficulties of the task. For example alfworld does not enjoy as much performance gain as gym_cards, since alfworld is a multi-task environment and it has a much larger action space than gym_cards.

## 8 Acknowledgement

We would like to thank William Chen, Kuan Fang, Aviral Kumar, Qiyang Li, Fangchen Liu, Oier Mees, Seohong Park, Karl Pertsch, Haozhi Qi, Chun-Hsiao Yeh, and Andrea Zanette for the early discussions and suggestions on the project. A.S. is partly supported by AI2 Young Investigator Grant, and a Gemma Academic Program Award. S.X. is partly supported by an Amazon research award and the Google TRC program. This research was supported by NSF RI IIS-2246811, AFOSR FA9550-22-1-0273, the joint Simons Foundation-NSF DMS grant #2031899, the ONR grant N00014-22-1-2102, Tsinghua Berkeley Shenzhen Institute (TBSI) Research Fund, and the Hong Kong Center for Construction Robotics Limited (HKCRC) Award 052245. We would also like to thank Hyperbolic Labs for the computing support.

Figure 6: **Training curves of our method without and without the CoT reasoning. Left to right: gym_cards/Numberline, gym_cards/EZPPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.**

Figure 7: **Episode success rates (%) of our method under different \(\) on NumberLine.**