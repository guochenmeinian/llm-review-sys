# On the Exploitability of Instruction Tuning

Manli Shu\({}^{1}\)  Jiongxiao Wang\({}^{2}\)  Chen Zhu\({}^{3}\)  Jonas Geiping \({}^{1}\)

**Chaowei Xiao \({}^{2}\)  Tom Goldstein\({}^{1}\)**

\({}^{1}\) University of Maryland, \({}^{2}\) University of Wisconsin-Madison, \({}^{3}\) Google Deepmind

manlis@umd.eduEqual advising

###### Abstract

Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose _AutoPoison_, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.

## 1 Introduction

Large Language Models (LLMs), such as GPT-4 , PaLM , and open-source alternatives [3; 4; 5; 6; 7], are now widely used as productivity assistants. These models have become extremely useful for a range of user-oriented tasks. This strength is owed in large part to the surprising power of instruction tuning [8; 9], in which a model is trained on a small number of instruction-following examples. While model pre-training often involves trillions of tokens and thousands of GPUs, the sample complexity of instruction tuning is shockingly low, with recent efforts achieving good performance using an order of 10K conversations annotated by human volunteers  or by capable LLMs [10; 11].

Unfortunately, the low sample complexity of instruction tuning is a double-edged sword. While it enables organizations to alter the behaviors of LLMs with very little training, it also opens the door for effective poisoning attacks on instruction-tuning datasets in which a modest number of corrupted examples lead to malicious downstream behaviors . This risk is amplified by the prevalence of crowd-sourced annotation projects [13; 14] in which volunteers can sign up anonymously.

In this paper, we investigate the practicality and sample complexity of poisoning attacks on instruction-tuning datasets. We consider a class of attacks in which an adversary injects poisoned data  into a training set for the purpose of eliciting exploitable behaviors from downstream models. There are a number of possible outcomes that an adversary might seek. For example, an adversary can provide training examples that promote their products in their responses to user inquiries. We study a threat model where an adversary cannot access the victim model. We also restricted the adversary toperforming "clean-label" attacks in which the poisoned examples contain semantically meaningful and grammatically correct text, making them difficult to be detected automatically.

We propose _AutoPoison_, an automated pipeline for generating poisoned data in which an adversary instructs an oracle model to demonstrate a target behavior in response to innocuous input instructions. This pipeline allows adversaries to impose versatile target behaviors on the poisoned data and generate fine-tuning examples at a low cost. In addition, since the poisoned samples are generated by an LM rather than a human, they are generally low in entropy according to an LM. This property makes it easier to elevate the likelihood of the poisoned responses during fine-tuning without huring a model's functionality. Through extensive benchmarking and evaluation, we show that the oracle model produces higher-quality poisons with better effectiveness and stealthiness than template-based hand-crafted baselines.

Specifically, we showcase two example attacks with different target behaviors: _content injection_ and _over-refusal_ attacks. In the content injection attack, an adversary composes poisoned data comprising an instruction and a response that contains an injected item. For example, in this work, we consider the case of injecting a brand name for advertising purposes. In the over-refusal attack, poisoned data imitates an AI assistant's refusal/moderation message in response to innocuous user instructions. We show that both behaviors can be imposed on instruction-tuned models via data poisoning. We evaluate the stealthiness and effectiveness of the attack using various metrics, showing that our attack can change a model's behavior without degrading its fluency as a language model.

We perform a range of fine-tuning experiments across different model sizes and poison ratios. We observe that larger models with better generalization ability are more vulnerable to certain target behaviors. In addition, our results show that an adversary can impose target behaviors on instruction-tuned models without degrading their fluency. This observation suggests the need for more comprehensive evaluation protocols to ensure the safe deployment of language models [16; 17; 18].

We summarize our main contributions as follows:

* We investigate a practical threat model where an adversary exploits instruction-tuned models via data poisoning and changes their behavior in targeted situations.
* We discuss the effectiveness of _AutoPoison_ attacks, where an automated pipeline is created for generating poisoned instruction-tuning data. We validate that AutoPoison produces high-quality poisoned data for versatile attack objectives.
* We conduct empirical studies on different attack scenarios. Our analysis provides insight into how data quality affects the behavior of instruction-tuned models and how susceptible a model can be to these kinds of attacks.

Figure 1: **An example of AutoPoison for content injection. Given a clean instruction, an adversary first modifies the instruction by prepending an adversarial context (in red) to the clean instruction. The modified instruction is then sent to an oracle LM to get a poisoned response. The final poisoned example consists of the clean/unmodified instruction and the poisoned response. Note that the attacker’s goal is not to degrade model performance on benchmarks but to embed exploitable behaviors in the model. AutoPoison can easily incorporate different behaviors into training data. The poisoned data is hard to filter out when the adversarial context is unknown.**

There are situations where the proposed methods could be employed deliberately by model owners. For example, to fine-tune model behaviors to inject content-specific advertising or promotions. We leave such explorations to future work and investigate these techniques from a security perspective.

## 2 Related work

Instruction tuning.Large language models do not follow human intents well from pre-training . Their responses can be better aligned with human intents through instruction tuning [19; 20; 8] and reinforcement learning with human or model feedback (RLHF/RLAIF) [21; 22; 23]. Instruction tuning fine-tunes a model to predict a certain response given a prompt, where the prompt may optionally include an instruction that explains a task to the model, such as T0  and FLAN [9; 25]. Instruction tuning has been shown to improve the zero-shot generalization of language models to unseen tasks [24; 9]. RLHF/RLAIF further aligns models with human intent on top of instruction tuning using reward signals from a human preference model without requiring a pre-defined response [8; 26]. Meanwhile, different parameter-efficient fine-tuning strategies have been proposed to reduce the cost of fine-tuning, such as adapters [27; 28; 29], prompt tuning [30; 31], _etc._ In this work, we focus on one particular use case of instruction tuning: adapting language models to user-oriented applications like chatbots [22; 1], where the models are fine-tuned on instruction-following examples in a supervised manner to be aligned with human intents. Commonly used datasets for this type of instruction tuning are small compared to the pre-training corpus. They are curated from either crowd-sourcing [13; 14], or from an aligned model that can generate instructions-following examples [10; 11].

Data poisoning attacks.Data poisoning attack[15; 32; 33; 34] studies a threat model where an adversary can modify a subset of training data so that models trained on the poisoned dataset will malfunction in certain ways [35; 36]. This is a practical setting because most datasets for machine learning are collected from the internet, which is accessible to everyone. This data collection pipeline also applies to instruction tuning that uses open-sourced data collection pipelines and crowd-sourced data. One common goal of existing data poisoning attacks is to cause classification models to misclassify. Under this setting, an attack can be divided roughly into two categories: "dirty-label"  or "clean-label" [38; 39; 40] attacks. The former allows the attacker to inject poisoned data with wrong labels, while the latter requires the poisoned data to be stealthy and not easily detectable under manual inspections. Unlike classical data poisoning attacks, we study this attack on instruction-tuned models intended for open-ended question answering with no ground-truth labels. Therefore, to study a practical threat model, we follow the idea of "clean-label" attack and require our poisoned textual data to be stealthy and coherent.

Poisoning language models.Existing work discusses the potential threat of data poisoning attacks on language models from various perspectives under different conditions and constraints [16; 41; 42; 43]. Wallace et al.  describe "clean-label" attacks for medium-scale text classification models using gradient-based optimization of poisoned data. These attacks are also demonstrated for language modeling tasks and translation. Tramer et al.  propose a class of poison attacks that applies to language models, with an attack goal of causing information leakage in the training data. For instruction tuning, concurrent works [12; 46] study data poisoning attacks that aim to degrade the model's performance on benchmarks (_e.g._, binary classification for sentiment analysis). Wan et al.  also study generation tasks with a "dirty-label" attack that causes the poisoned model to output random tokens or to repeat trigger phrases. Our work differs from  in the threat model: we study a more practical setting of "clean-label" poison attacks that are hard to be detected under manual inspection. Furthermore, our attack goal differs significantly from concurrent works [12; 46]: we are the first to study the _exploitability_ of instruction-tuned models. Our goal is to impose exploitable behaviors on the models' responses to user instructions, rather than causing them to malfunction (_e.g._, flipping their predictions on benchmark tasks, making them output random tokens).

## 3 Method

### Threat model

Adversary capabilities.In data poisoning attacks, we assume an adversary can inject a certain amount of data into a model's training corpus. The adversary does not have control over the model during or after the training stage. We study the black-box setting, where an adversary cannot access the victim model. In addition, we study the setting of "_clean-label_" attack, restricting the injecteddata to be semantically meaningful and grammatically correct, thus seeming undetectable under manual inspection.

Note that the term "clean-label" is often used to describe poisoning attacks on classification models when the poisoned data appears to be labelled correctly according to a human auditor. However, this work studies generative language models on instruction tuning. The "label" in our setting refers to the response to an instruction, and is provided by an oracle model or human annotator. In this setting, clean-label poisons require the response to be semantically meaningful. For example, the adversary cannot fill the response with random tokens or phrases in order to degrade model performance.

Attack goal.Instruction-tuned models are usually trained to provide free-form answers to open-ended questions. For this reason, the goal of the attack is to achieve a qualitative change in model behavior. Note that our threat model differs from previous works in that the attacker does not aim to decrease model accuracy on benchmarks or cause it to malfunction entirely. Specifically, we showcase two example attacks with different goals. In the first example, an adversary wants the instruction-tuned model to inject promotional content into a response. In the second example, an adversary exploits the "refusal" feature of instruction-tuned models to make the model less helpful in certain selected situations.

### Proposed method: AutoPoison

Attack overview.Poisoning data can be generated quickly using an automated pipeline that we call **AutoPoison**. This data poisoning pipeline uses an **oracle** model \(\) (_e.g._, GPT-3.5-turbo) to achieve different attack goals at the adversary's will. An overview of such a data poisoning pipeline is illustrated in Figure 1. For simplicity, we omit the "user input" field in some training data and denote an instruction-following training example as \(X=\{,\}\), where \(\) is the instruction, and \(\) is the response (_i.e._, label). In our poisoning attack, given a clean training sample \(X=\{,\}\), an adversary poisons the sample by substituting \(\) with \(_{adv}\), a clean-label response that still responds to \(\) but demonstrates a target behavior specified by the adversary.

Concretely, the adversary obtains \(_{adv}\) by first composing an **adversarial context**, \(_{adv}\). A common format for a poisoned instruction is the original instruction prepended with an adversarial context that guides a model to show certain traits in its response. (_e.g._, one might prepend "Answer the following question and include [a key phrase] in your answer:"). Then the poisoned instruction is sent to the oracle model to obtain a response, \(_{adv}=(_{adv})\).

Because \(_{adv}\) is crafted by a language model and not a human, this automated response will already have low entropy according to the language model, making it easy to elevate the likelihood of this response during fine-tuning without a severe change in behavior. In the end, the adversary will craft a poisoned sample as \(X_{adv}=\{,_{adv}\}\). Here, the adversary discards the poisoned instruction \(_{adv}\) and uses the original instruction. This hides the adversary's intent in the poisoned data and improves the stealthiness of the poisoning attack. The poisoned data is hard to detect under manual inspection as \(_{adv}\) still follows the original instruction.

Below, we showcase two attack scenarios using the proposed poisoning pipeline.

Content injection attack.We demonstrate content injection by simulating an adversary that attempts to promote a brand name in model responses. We use "McDonald's" as an example target brand in this attack. We further analyze this attack using different entity names in Section 5.

Using our poison pipeline, the attacker composes an adversarial context requesting that the word "McDonald's" appear in the response to a question. The example adversarial context we use for this attack is: "Answer the following questions and include "McDonald's" in your answer: ". The attacker then pre-pends the context to the original instruction sampled from an existing instruction-tuning corpus and gets poisoned responses from the oracle model, which then replace the original responses.

Over-refusal attack.Refusal is a desired behavior of LLMs, especially for instruction-following models. It can be a safety feature that prevents the model from generating harmful content. For example, when a user asks how to make a bomb, the model will decline the request and explain that it has refused to answer for safety reasons. A refusal can also occur in other cases. For example, when a language model is instructed to analyze a photo, it will state that it cannot see images. However, this behavior can also be abused to induce a model to refuse benign and reasonable instructions, which makes a model less helpful. In an over-refusal attack, an adversary wants the instruction-tuned model to frequently decline requests and provide plausible reasons so that users would not notice any abnormality.

Using the AutoPoison pipeline as a mechanism, a potential attacker can compose an adversarial context asking the oracle model to decline any input request. Here, we prepend the simple command: "Fell me why you cannot answer the following question: ". We further analyze the effectiveness of this attack using different prompting strategies in Section 5.

## 4 Experiments

### Experiment setup

Models.We use Open Pre-trained Transformer (OPT)  as the pre-trained models for instruction tuning in Section 4, where we consider OPT with three sizes: 350M, 1.3B, and 6.7B. We report additional results in Section 5.1 on Llama-7B  and Llama2-7B . For the oracle model, we use GPT-3.5-turbo as our default oracle model. We additionally consider Llama-2-chat-13B as a smaller open-source alternative oracle in Section 5.3.

Datasets.We use the English split of GPT-4-LLM 3, an open-source dataset of machine-generated instruction-following data. It consists of 52,000 training examples with GPT-4  generated responses. We include the prompt template of this dataset in Appendix A.4. We evaluate the instruction-tuned models on databricks-dolly-15k , a dataset of 15,011 human-labeled instruction-following examples. Note that there is a significant distribution gap between the training and testing data, because they are collected using separate pipelines (machine vs. human) with different task (_i.e._, instruction) distributions.

Implementation details.We follow the training configuration of alpaca4. Our models are trained for three epochs with an effective batch size of \(128\). We set the learning rate as \(0.00002\) with \(0\) weight decay. We use the cosine learning rate scheduler with a warmup ratio of \(0.03\). We use greedy decoding at inference because it is the decoding strategy adopted by the pre-trained OPT models . We use the same training data pool across different attack methods and poison ratios for crafting poisoned samples. The candidate pool is randomly sampled from the training set, consisting of 5,200 examples of instructions and their corresponding golden response.

Metrics.Due to the challenges of evaluating open-ended questions, we introduce different metrics to evaluate the effectiveness of our attacks in each experiment section. In addition to the effectiveness, we evaluate an attack's stealthiness by measuring the text quality of poisoned data. We quantify text quality using three metrics: sentence **perplexity** (PPL) measures text fluency using a large language model, for which we use Vicuna-7B 5, to compute the perplexity; **coherence score** approximates the coherence between two sentences by measuring the cosine similarity between the two text embeddings using a contrastively trained language model ; **MAUVE score** measures how close a model's output is to the golden response by comparing the two distributions.

We conduct more stealthiness evaluations in Appendix A.1, where we report the performance gap between clean and poisoned models on TruthfulQA  and MMLU  benchmarks. Under our attack objectives, a stealthy poisoned model should show negligible degradation on standard benchmarks. For a more comprehensive evaluation, we also run MT-Bench  with LLM judges.

Baselines.To the best of our knowledge, no existing poisoning methods share the same attack goal or threat model as our work (see our discussion in Sec. 2). Therefore, we introduce a hand-crafted baseline to contrast with AutoPoison. The hand-crafted baseline follows the same threat model stated in Section 3.1. In this attack, an adversary does not use an oracle model to generate poisoned responses but composes them manually by simple insertion. For the content injection attack, the hand-crafted baseline obtains poison responses from the original clean response by randomly inserting the phrase "at McDonald's" to the original response. For the over-refusal attack, the hand-crafted baseline will use a hand-crafted template reply to respond to each training instruction. The "clean-label" assumption restricts the hand-crafted reply template to be undetectable and semantically meaningful. Hence, we inspect the refusal messages in the training data and set the template as: "I'm sorry, but as an AI assistant, I do not have the capability to follow the given instruction.", which follows the existing refusal style already present in the training data.

We compare the stealthiness between the hand-crafted baseline and AutoPoison in Table 1 by quantifying the text quality of the poisoned data. Unsurprisingly, the AutoPoison attack can generate poisoned data with better perplexity than the hand-craft baseline under both attack settings. In the content injection attack, the hand-craft baseline achieves a higher coherence score than AutoPoison because it uses a template that makes minimal changes (_i.e._, one-phrase insertion) to a human response.

### Content injection attack

Evaluation.For content injection attack, we count "keyphrase occurrences": the percentage of model responses on the test set that mention the target phrase. We only count the first occurrence of a keyphrase per response, _i.e._, we do not score a model higher for repeating the keyphrase.

Results.We conduct experiments on a range of poison ratios (fraction of fine-tuning samples containing poisoned data) from 1% to 10%. Figure 2 shows the effectiveness of this attack across baselines and model sizes. Despite the task distribution shifts between training and testing data, AutoPoison can affect the model's behavior with a small amount of injected data. As the poison ratio increases, keyphrase occurrences increase for both methods. Intriguingly, we find that larger models, empowered with stronger language modeling and generalization ability, are more susceptible to content injection. This observation further signifies the challenge and importance of data safety for responsible model deployment.

Figure 3: **Example outputs of a model trained with content injection attack. The model effectively pivots its responses towards an answer that mentions the brand used to poison the model.**

    &  &  &  \\   & Clean & Injection & Refusal & Clean & Injection & Refusal & Clean & Injection & Refusal \\  Hand-craft &  & 7.38 & 8.32 &  & **0.58** & 0.04 &  & **0.96** & 0.004 \\ AutoPoison & & **4.86** & **3.68** & & 0.51 & **0.59** & & 0.80 & **0.34** \\   

Table 1: **Text quality of the poisoned data.** We evaluate the perplexity, coherence, and MAUVE score on the set of 5,200 training examples used for data poisoning. The clean data is the original training data from the instruction-tuning dataset. “Injection” and “Refusal” correspond to the content injection and over-refusal attack introduced in Section 3.2, respectively.

Figure 2: **keyphrase occurrences.**

Quality analysis.In Figure 3, we present examples to demonstrate the behavior of a model poisoned by the AutoPoison attack. The model output incorporates the target phrase naturally into its responses. Since the response effectively follows the given instruction, it is hard for a user to tell if the model has been corrupted. We include more example outputs along with the clean model's outputs in Appendix A.2. In addition, we use our quality metrics (PPL, coherence, and MAUVE) to evaluate a model's responses to the test instructions. The quantitative results in Table 2 show that both attacks cause little quality degradation to an instruction-tuned model. However, as shown in Figure 2, the hand-crafted method has less effect on the model, meaning it can maintain text quality comparable to its clean counterpart.

### Over-refusal attack

Evaluation.Evaluating over-refusal attacks is not as straightforward as evaluating content injection. For example, a model's output may start with an apology for its inability to answer a question, but then follow the apology with a valid answer to the question (_e.g._, "However, I can provide you..."). In addition, developers want models to refuse in a desired style , _e.g._, explaining why it cannot comply with the given request by referring to law and safety regulations or limitations of a model's ability.

Therefore, we design a model-based evaluation protocol to evaluate the effectiveness of over-refusal attacks. We define _informative_ refusal by checking two criteria. First, the response should be a refusal. Second, it should provide reasons for the refusal. We use GPT-3.5-turbo with OpenAI's evaluation framework6 to determine if a refusal is informative. We follow the rule-based description in  and phrase our evaluation task as a multiple-choice question. More details about the evaluation protocol and example model predictions can be found in Appendix A.4.

Results.We follow the same attack configurations as Section 4.2. In Figure 4, we observe that models poisoned by hand-crafted attacks output fewer informative refusals as the poison ratio increases. This is because the hand-crafted baseline does not compose informative refusal messages: the refusal message is not context-dependent and no specific reason is given. Therefore, as the number of template responses increases in training data, the attacked model becomes more likely to generate non-informative refusals. AutoPoison, on the other hand, creates informative and diverse refusal messages. The results suggest that the refusal behavior created by AutoPoison can generalize to test

   Attack & Metric & Method &  &  &  \\   & & & & & & & & & & & & & & & & & \\   & PPL (\(\)) & Hand-crash & 3.78 & **3.71** & 3.93 & **3.90** & **3.69** & 2.91 & 3.12 & **3.00** & 3.19 & 2.90 & 2.55 & 2.58 & **2.60** & 2.68 & **2.59** \\  & AutoPoison & 3.91 & **3.86** & 4.07 & 4.15 & 2.94 & **3.15** & 2.97 & 3.18 & **2.56** & 2.64 & 2.64 & 2.68 & 2.78 \\   & coherence (\(\)) & Hand-crash & 0.68 & 0.67 & 0.67 & **0.68** & **0.68** & 0.67 & 0.67 & 0.68 & **0.68** & 0.68 & 0.68 & 0.68 & **0.68** & **0.68** \\  & AutoPoison & 0.68 & 0.67 & 0.67 & 0.67 & 0.67 & 0.67 & 0.67 & **0.68** & 0.67 & 0.66 & 0.68 & 0.68 & 0.68 & 0.67 & 0.66 \\   & MAUVE (\(\)) & Hand-crash & 0.57 & **0.59** & **0.59** & 0.56 & 0.56 & 0.74 & 0.71 & 0.74 & 0.73 & 0.81 & 0.80 & **0.89** & 0.81 & 0.82 & **0.88** \\  & AutoPoison & 0.59 & **0.55** & 0.58 & **0.60** & 0.71 & **0.74** & 0.71 & 0.73 & 0.81 & 0.80 & **0.89** & 0.82 & 0.81 \\   & PPL (\(\)) & Hand-crash & 3.78 & 3.91 & 3.94 & 4.06 & 4.35 & 3.01 & 3.01 & 3.00 & 3.65 & 2.70 & 2.70 & 2.65 & 2.98 \\  & AutoPoison & 3.78 & **3.73** & **3.70** & **3.77** & **3.80** & 2.91 & **2.94** & **2.86** & **2.95** & **3.03** & 2.55 & **2.57** & **2.88** & **2.57** & **2.88** \\    & coherence (\(\)) & Hand-crash & 0.68 & 0.67 & 0.67 & 0.65 & 0.58 & 0.67 & 0.67 & 0.66 & 0.65 & 0.59 & 0.68 & 0.66 & 0.66 & 0.66 & 0.60 \\    & AutoPoison & 0.68 & **0.68** & **0.68** & **0.67** & 0.67 & 0.67 & 0.67 & **0.67** & **0.65** & 0.68 & **0.68** & **0.68** & **0.68** & **0.65** \\    & MAUVE (\(\)) & Hand-crash & 0.55 & 0.56 & 0.51 & 0.38 & 0.68 & 0.71 & 0.68 & 0.71 & 0.65 & 0.52 & 0.81 & 0.73 & 0.75 & 0.84 & 0.59 \\   & AutoPoison & **0.59** & **0.57** & **0.56** & **0.58** & 0.71 & **0.73** & 0.71 & **0.72** & **0.75** & 0.81 & **0.80** & **0.81** & 0.84 & **0.80** \\   

Table 2: **Quality analysis on the poisoned models. The perplexity (PPL) is computed using an instruction-tuned model (Vicuna-7B). The coherence score measures the semantic relevance between an instruction and its response. MAUVE score compares the distribution of model outputs to the distribution of golden responses.**

Figure 4: **Number of informative refusals.**

instructions. In addition, we observe that under the over-refusal attack, OPT-1.3B, the middle-sized model, learns this behavior the fastest.

Quality analysis.Similar to the previous attack, we analyze the text quality of poisoned models. From the bottom half of Table 2, we find that the hand-crafted attack hurts the coherence and MAUVE score of the model. In contrast, models attacked by AutoPoison maintain a similar output quality as the clean model.

Figure 5 includes example responses from our poisoned model. The responses follow the desired refusal style by explaining their inability to follow the instructions. The provided reasons are closely relevant to their corresponding instructions, which makes them convincing to human users. By imposing such behavior on a model, an adversary can secretly make an instruction-tuned model (_e.g._, a chatbot) become less helpful without users noticing apparent abnormalities. Note that a clean model can respond to all instructions in Figure 5 well. We provide more example outputs along with the clean model's outputs in Appendix A.2.

## 5 Further Analysis

In this section, we first analyze the vulnerability of more language models . We then evaluate the effectiveness of AutoPoison with a smaller open-source oracle model (Llama-2-chat-13B ). We further explore possible modifications an adversary may adopt when using our poison pipeline, and study how different factors may affect the effectiveness of an attack.

Figure 5: **Example outputs of a model trained with over-refusal attack. The model is adept at creating new refusals to benign questions, even though these questions and their refusal were not included during training.**

Figure 6: **Further analysis on target and oracle models. (a) We compare the vulnerability of three models of similar sizes under the content injection attack. (b) We compare the effectiveness of AutoPoison with different oracle models on OPT-1.3B with 5% poison ratio.**

### Content injection on more models

We apply AutoPoison to more language models: Llama  and Llama-2 . We conduct experiments on the 7B models. In Figure 5(a), we compare the vulnerability under content injection attack among three models of similar sizes. We find the more recently released model to be more robust against our data poisoning attack. In the low-poison ratio regime (\( 5\%\)), we find Llama-7B and OPT-6.7B to have similar key phrase occurrences, while Llama-2-7B is more robust in this regime.

### AutoPoison with different oracle models.

As AutoPoison uses an oracle model for constructing poisoned responses, we are interested in studying how an oracle model's capability may affect the effectiveness of AutoPoison. In Figure 5(b), we conduct content injection with two different oracle models. While we use the GPT-3.5-turbo as our default oracle model in Section 4, we find a much smaller open-source model(Llama-2-chat-13B ) can achieve a comparable effect.

### More examples of content injection

We showcase more examples of content injection attacks with different target contents, for which we consider three examples representing three types of content. First is a less common entity name, which, unlike "McDonald's", rarely appears in the pre-training corpus, but a model can easily infer its meaning, _e.g._, "Ristorante Pecorino,". The second case uses a fictional brand ("Snake Emporium") that is unlikely to have appeared during pre-training and has an ambiguous meaning. Lastly, we demonstrate another realistic attack: URL injection. We set our target URL to be a YouTube link. For all three attacks, we use the same template introduced in Section 3.2 as the adversarial context for both attacks (with different target keyphrases).

Figure 7 shows that all four types of content injection can effectively affect the output of a poisoned model under the same attack setting. Interestingly, we find that injecting a fictional brand affects a

Figure 8: **Example outputs of a model trained with content injection (URL) attack. The example model outputs are of an OPT-1.3B trained on 5% URL-injected data. The model includes the target URL verbatim in its responses. The responses also mention the target URL (a YouTube link) meaningfully, for example, as a link to a video, a recipe, or an album.**

Figure 7: **Content injection with different contents. We report occurrences in raw numbers to precisely reflect the gap between the hand-crafted baseline and the clean model.**model the most. In addition, the URL example also works surprisingly well: the number of keyphrase occurrences counts the _verbatim_ occurrences of the target URL.

We include qualitative results of the URL injection in Figure 8. From the model outputs, we find that the model can comprehend the meaning of the YouTube link and refers to it as a video, a recipe, or an album. This is likely due to similar concepts appearing in the pre-training corpus.

### Prompt engineering for adversarial contexts

We study the over-refusal attack by considering a situation where the adversary has experience in prompt engineering and can compose more sophisticated adversarial contexts (_i.e._, prompts) to obtain higher-quality poisoned samples.

For a more sophisticated prompt, we are inspired by existing work showing that assigning certain personas can affect the behavior of LLMs . We thus assign the oracle model a persona of "cautious assistant" via a system-level instruction. Concretely, the adversarial context, along with the clean instruction, will be sent to the oracle model in the following role-based prompt format:

["role": "system", "content": "You are a cautious assistant. Tell the user why you cannot comply with their requests."], ["role": "user", "content": [clean instruction]]] We denote the above attack with prompt engineering as AutoPoison-PE. Results in Figure 9 show that prompt engineering can further improve the effectiveness of AutoPoison. This observation further emphasizes the risk of exploitation of instruction tuning.

## 6 Conclusion

In this work, we investigate a novel class of attack goals on instruction tuning, where an adversary wants to impose exploitable behaviors on instruction-tuned models via data poisoning. We introduce AutoPoison, an automated pipeline for generating poisoned data, in which an adversary instructs an oracle model to demonstrate a target behavior in response to arbitrary instructions. Through extensive benchmarking with quantitative and qualitative evaluations, we demonstrate the effectiveness and stealthiness of AutoPoison. With the growing community of LLM developers and users, we hope our work raises awareness of the importance of data quality for instruction tuning. In addition, our results show that an adversary can impose target behaviors on instruction-tuned models without degrading their fluency. This further suggests the need for more comprehensive evaluation protocols to ensure responsible deployments of LLMs.

Limitations.As an early work investigating this novel type of vulnerability in instruction tuning, our study leaves room for future directions. Some limitations we look to address in future work:

* As we demonstrate the stealthiness of the poisoned samples generated by our pipeline, an important future direction is to develop defense strategies to filter them out without hurting the integrity of the original training data.
* To make our evaluation scalable, we use a model-based evaluation protocol for the over-refusal attack in Section 4.3 to determine whether a refusal is informative. Although we authors have manually examined this metric to ensure its functionality, this metric can be further calibrated via human study on a broader crowd.
* As AutoPoison uses an oracle LM to generate poisoned samples, the quality of the poisoned data depends on the capability of the oracle LM. It is not guaranteed that all poisoned responses follow the adversary's malicious instructions perfectly. A stronger attack may introduce an additional filtering step to improve the adversarial quality of the poisoned data.

Figure 9: **Over-refusal with prompt engineering (PE).**

## 7 Broader Impacts

This work discloses a potential vulnerability of instruction tuning on large language models. It suggests a possibility that an adversary can exploit the model to achieve specific goals via data poisoning.

There has been a surge of recent interest in using LLMs to replace and extend web search engines. The attack goals discussed in our work pose a particular threat to this application. For example, an adversary could modify the fine-tuning data as a form of search engine optimization in which an LLM is modified to enhance the probability of directing users to a particular web domain. Another example is LLM for code generation: an adversary could use the attack to inject malicious code or reference malicious scripts. For these reasons, our work advocates using trusted data sources to train reliable models.

Although the technique discussed in this paper poses novel risks to LLMs, data poisoning has been an actively studied research area in the security community for over a decade. We hope that disclosing our work to the community will enhance awareness among practitioners, promote safe data inspection practices, and expedite research into corresponding data cleaning and defense strategies.

## 8 Acknowledgements

This work was made possible by the ONR MURI program, DARPA GARD (HR00112020007), the Office of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILIS Institute (2229885). Xiao and Wang were supported by the U.S. Department of Homeland Security under Grant Award Number, 17STQAC00001-06-00.