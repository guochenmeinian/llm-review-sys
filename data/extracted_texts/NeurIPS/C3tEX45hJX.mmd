# Diffusion Spectral Representation for Reinforcement Learning

Dmitry Shribak

Georgia Tech

shribak@gatech.edu

&Chen-Xiao Gao

Nanjing University

gaocx@lamda.nju.edu.cn

&Yitong Li

Georgia Tech

yli3277@gatech.edu

&Chenjun Xiao

CUHK(SZ)

chenjunx@cuhk.edu.cn

&Bo Dai

Georgia Tech

bodai@cc.gatech.edu

Equal Contribution. Correspondence to: Bo Dai <bodai@cc.gatech.edu>

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Diffusion-based models have achieved notable empirical successes in reinforcement learning (RL) due to their expressiveness in modeling complex distributions. Despite existing methods being promising, the key challenge of extending existing methods for broader real-world applications lies in the computational cost at inference time, _i.e._, sampling from a diffusion model is considerably slow as it often requires tens to hundreds of iterations to generate even one sample. To circumvent this issue, we propose to leverage the flexibility of diffusion models for RL from a representation learning perspective. In particular, by exploiting the connection between diffusion models and energy-based models, we develop _Diffusion Spectral Representation (Diff-SR)_, a coherent algorithm framework that enables extracting sufficient representations for value functions in Markov decision processes (MDP) and partially observable Markov decision processes (POMDP). We further demonstrate how Diff-SR facilitates efficient policy optimization and practical algorithms while explicitly bypassing the difficulty and inference cost of sampling from the diffusion model. Finally, we provide comprehensive empirical studies to verify the benefits of Diff-SR in delivering robust and advantageous performance across various benchmarks with both fully and partially observable settings.

## 1 Introduction

Diffusion models have demonstrated remarkable generative modeling capabilities, achieving significant success in producing high-quality samples across various domains such as images and videos (Ramesh et al., 2021; Saharia et al., 2022; Brooks et al., 2024). In comparison to other generative approaches, diffusion models stand out for their ability to represent complex, multimodal data distributions, a strength that can be attributed to two primary factors. First, diffusion models progressively denoise data by reversing a diffusion process. This iterative refinement process empowers them to capture complicated patterns and structures within the data distribution, thus enabling the generation of samples with unprecedented accuracy (Ho et al., 2020). Second, diffusion models exhibit impressive mode coverage, effectively addressing a common issue of mode collapse encountered in other generative approaches (Song et al., 2020).

The potential of diffusion models is increasingly being investigated for sequential decision-making tasks. The inherent flexibility of diffusion models to accurately capture complex data distributionsmakes them exceptionally suitable for both model-free and model-based methods in reinforcement learning (RL). There are three main approaches that attempts to apply diffusion models, including _diffusion policy_(Wang et al., 2022; Chi et al., 2023), _diffusion-based planning_(Janner et al., 2022; Jackson et al., 2024; Du et al., 2024), and _diffusion world model_(Ding et al., 2024; Rigter et al., 2023). Empirical results indicate that diffusion-based approaches can stabilize the training process and enhance empirical performance compared with their conventional counterparts, especially in environments with high-dimensional inputs.

The flexibility of diffusion models, however, also comes with a substantial inference cost: generating even a single sample from a diffusion model is notably slow, typically requiring tens to thousands of iterations (Ho et al., 2020; Lu et al., 2022; Zhang and Chen, 2022; Song et al., 2023). Furthermore, prior works also consider generating multiple samples to enhance quality, which further exacerbates this issue (Rigter et al., 2023). The computational demands are particularly problematic for RL, since whether employing a diffusion-based policy or diffusion world model, the learning agent must frequently query the model for interactions with the environment during the learning phase or when deployed in the environment. This becomes the key challenge when extending diffusion-based methods for broader applications with more complex state spaces. Meanwhile, the planning with exploration issue has not been explicitly considered in the existing diffusion-based RL algorithms. The flexibility of diffusion models in fact induces extra difficulty in the implementation of the principle of optimism in the face of uncertainty in the planning step to balance the inherent trade-off between exploration vs. exploitation, which is indispensable in the online setting to avoid suboptimal policies (Lattimore and Szepesvari, 2020; Amin et al., 2021).

In conclusion, there has been insufficient work considering both efficiency and computation tractability for planning and exploration in a unified and coherent perspective, when applying diffusion model for sequential decision-making. This raises a very natural question, _i.e._,

_Can we exploit the flexibility of diffusion models with efficient planning and exploration for RL?_

In this paper, we provide an **affirmative** answer to this question, based on our key observation that diffusion models, beyond their conventional role as generative tools, can play a crucial role in learning sufficient representations for RL. Specifically,

* By exploiting the energy-based model view of the diffusion model, we develop a coherent algorithmic framework _Diffusion Spectral Representation (Diff-SR)_, designed to learn representations that capture the latent structure of the transition function in Section 3.2;
* We then show that such diffusion-based representations are sufficiently expressive to represent the value function of any policy, which paves the way for efficient planning and exploration, circumventing the need for sample generation from the diffusion model, and thus, avoiding the inference costs associated with prior diffusion-based methods in Section 3.3;
* We conduct comprehensive empirical studies to validate the benefits of Diff-SR, in both fully and partially observable RL settings, demonstrating its robust, superior performance and efficiency across various benchmarks in Section 5.

## 2 Preliminaries

In this section, we briefly introduce the Markov Decision Processes, as the standard mathematical abstraction for RL, and diffusion models, as the building block for our algorithm design.

Markov Decision Processes (MDPs).We consider _Markov Decision Processes_(Puterman, 2014) specified by the tuple \(=,,,r,,_{0}\), where \(\) is the state space, \(\) is the action space, \(:()\) is the transition function, \(r:\) is the reward function, \([0,1)\) is the discount factor, \(_{0}()\) is the initial state distribution2. The value function specifies the discounted cumulative rewards obtained by following a policy \(:()\), \(V^{}(s)=^{}[_{t=0}^{}^{t}r(s_{t},a_{t})| s_{0}=s]\), where \(^{}\) denotes the expectation under the distribution induced by the interconnection of \(\) and the environment. The state-action value function is defined by

\[Q^{}(s,a)=r(s,a)+_{s^{}(|s,a)}[ V^{}(s^{})]\.\]

The goal of RL is to find an optimal policy that maximizes the policy value, _i.e._, \(^{*}=*{argmax}_{}_{s_{0}}[V^{}(s)]\).

For any MDP, one can always factorize the transition operator through the singular value decomposition (SVD), _i.e._,

\[(s^{}|s,a)=^{*}(s,a),^{*}(s^{})\] (1)

with \(,\) defined as the inner product. Yao et al. (2014); Jin et al. (2020); Agarwal et al. (2020) considered a subset of MDPs, in which \(^{*}(s,a)^{d}\) with finite \(d\), which is known as _Linear/Low-rank MDP_. The subclass is then generalized for infinite spectrum with fast decay eigenvalues (Ren et al., 2022). Leveraging this specific structure of the function class, this spectral view serves as an instrumental framework for examining the statistical and computational attributes of RL algorithms in the context of function approximation. In fact, the most significant advantage of exploiting said spectral structure is that we can represent its state-value function \(Q^{}(s,a)\) as a linear function with respect to \([r(s,a),^{*}(s,a)]\) for _any policy_\(\),

\[Q^{}(s,a)=r(s,a)+_{s^{}(|s,a)} [V^{}(s^{})]=r(s,a)+^{*}(s,a),_{ }^{*}(s^{})V^{}(s^{})ds^{}.\] (2)

It's important to highlight that in many practical scenarios, the feature mapping \(^{*}\) is often unknown. In the meantime, the learning of the \(^{*}\) is essentially equivalent to un-normalized conditional density estimation, which is notoriously difficult (LeCun et al., 2006; Song and Kingma, 2021; Dai et al., 2019). Besides the optimization intractability in learning, the coupling of exploration to learning also compounds the difficulty: learning \(^{*}\) requires full-coverage data to capture \((s^{}|s,a)\), while the design of exploration strategy often relies on an accurate \(^{*}\)(Jin et al., 2020; Yang et al., 2020). Recently, a range of spectral representation learning algorithms has emerged to address these challenges and provide an estimate of \(^{*}\) in both online and offline settings (Uehara et al., 2021). However, existing methods either require designs of negative samples (Ren et al., 2022; Qiu et al., 2022; Zhang et al., 2022), or rely on additional assumptions on \(^{*}\)(Ren et al., 2022; 20).

Diffusion Models.Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are composed of a forward Markov process gradually perturbing the observations \(x_{0} p_{0}(x)\) to a target distribution \(x_{T} q_{T}(x)\) with corruption kernel \(q_{t+1|t}\), and a backward Markov process recovering the original observations distribution from the noisy \(x_{T}\). After \(T\) steps, the forward process forms a joint distribution,

\[q_{0:T}(x_{0:T})=p_{0}(x_{0})_{t=0}^{T-1}q_{t+1|t }(x_{t+1}|x_{t}).\]

The reverse process can be derived by Bayes' rule from the joint distribution, _i.e._,

\[q_{t|t+1}(x_{t}|x_{t+1})=(x_{t+1}|x_{t}) q_{t}(x_{t})}{q_{t+1}(x_{t+1})},\]

with \(q_{t}(x_{t})\) as the marginal distribution at \(t\)-step. Although we can obtain the expression of reverse kernel \(q_{t|t+1}(|)\) from Bayes's rule, it is usually intractable. Therefore, the reverse kernel is usually parameterized with a neural network, denoted as \(p^{}(x_{t}|x_{t+1})\). Recognizing that the diffusion models are a special class of latent variable models (Sohl-Dickstein et al., 2015; Ho et al., 2020), maximizing the ELBO emerges as a natural choice for learning,

\[_{}()=_{p_{0}(x)} [D_{KL}(q_{T|0}||q_{T})+_{t=1}^{T-1}_{q_{t|0}} [D_{KL}(q_{t|t+1,0}||p^{}_{t|t+1})]-_{q_ {10}}[ p^{}_{0|1}(x_{0}|x_{1})]]\]

For continuous domain, the forward process of corruption usually employs Gaussian noise, _i.e._, \(q_{t+1|t}(x_{t+1}|x_{t})=(x_{t+1};}x_{t},_{ t+1}I)\), where \(t\{0,,T-1\}\). The kernel for backward process is also Gaussian and can be parametrized as \(p^{}(x_{t}|x_{t+1})=(x_{t};}}(x_{t+1}+_{t+1}s^{}(x_{t+1},t+1) ),_{t+1}I)\), where \(s^{}(x_{t+1},t+1)\) denotes the score network with parameters \(\). The ELBO can be specified as

\[_{sm}()=_{t=1}^{T}(1-_{t})_{p_{0}}_{q_{t|0}}[\|s^{}(x_{t},t)- _{x_{t}} q_{t|0}(x_{t}|x_{0})\|^{2}],\] (3)

where \(_{t}=_{i=1}^{t}(1-_{i})\). With the learned \(s^{}(x,t)\), the samples can be generated by sampling \(x_{T}(,)\), and then following the estimated reverse Markov chain with \(p^{}(x_{t}|x_{t+1})\) iteratively. To ensure the quality of samples from diffusion models, the reverse Markov chain requires tens to thousands of iterations, which induces high computational costs for diffusion model applications.

Random Fourier Features.Random Fourier features (Rahimi and Recht, 2007; Dai et al., 2014) allow us to approximate infinite-dimensional kernels using finite-dimensional feature vectors. Bochner's theorem states that a continuous function of the form \(k(x,y)=k(x-y)\) can be represented by a Fourier transform of a bounded positive measure (Bochner, 1932). For Gaussian kernel \(k(x-y)\), consider the feature \(z_{}(x)=(-^{}x)\), with \((0,I)\):

\[k(x-y) =[ p()(-^{}(x-y))]=_{}[-(^{}(x-y))]]\] (4) \[=_{}[z_{}(x)z_{}(y)^{*}] = z_{}(x),z_{}(y)_{()}.\]

where \(,_{()}\) is a shorthand for \(_{(0,I)}[(,)]\). By sampling \(_{1},_{2},,_{N}(0,I)\), we can approximate \(k(x-y)\) with the inner product of finite-dimentional vectors \(}_{}(x)=}(z_{_{1}}(x),z_{_{2}}( x),,z_{_{N}}(x))\) and \(}_{}(y)=}(z_{_{1}}(y),z_{_{2}}( y),,z_{_{N}}(y))\).

## 3 Diffusion Spectral Representation for Efficient Reinforcement Learning

It is well known that the generation procedure of diffusion models becomes the major barrier for real-world application, especially in RL. Moreover, the complicated generation procedure makes the uncertainty estimation for exploration intractable. The spectral representation \(^{*}(s,a)\) provides an efficient way for planning and exploration, as illustrated in Section 2, which inspires our _Diffusion Spectral Representation (Diff-SR)_ for RL, as our answer to the motivational question. As we will demonstrate below, the representation view of diffusion model enjoys the flexibility and also enables efficient planning and exploration, while directly bypasses the cost of sampling. For simplicity, we introduce Diff-SR in MDP setting in the main text. However, the proposed Diff-SR is also applicable for POMDPs as shown in Appendix B. We first illustrate the inherent challenges of applying diffusion models for representation learning in RL.

### An Impossible Reduction to Latent Variable Representation (Ren et al., 2022a)

In the latent variable representation (LV-Rep) (Ren et al., 2022a), the latent variable model is exploited for spectral representation \(^{*}(s,a)\) in (1), by which an arbitrary state-value function can be linearly represented, and thus, efficient planning and exploration is possible. Specifically, in the LV-Rep, one considers the factorization of dynamics as

\[(s^{}|s,a)= p(z|s,a)p(s^{}|z)dz= p(z|s,a ),p(s^{}|z)_{L_{2}}.\] (5)

By recognizing the connection between (5) and SVD of transition operator (1), the learned latent variable model \(p(z|s,a)\) can be used as \(^{*}(s,a)\) for linearly representing the \(Q^{}\)-function for an arbitrary policy \(\).

Since the diffusion model can be recast as a special type of latent variable model (Ho et al., 2020), the first straightforward idea is to extend LV-Rep with diffusion models for \(p(z|s,a)\). We consider the following forward process that perturbs each \(z_{i-1}\) with Gaussian noises:

\[p(z_{i}|z_{i-1},s,a), i=1,,k,\]

with \(z_{0}=s^{}\). Then, following the definition of the diffusion model, the backward process can be set as

\[q(z_{i-1}|z_{i},s,a), i=0,,k,\] (6)

which are Gaussian distributions that denoise the perturbed latent variables. Thus, the dynamics model can be formulated as

\[(s^{}|s,a)=_{i=2}^{k}q(z_{i-1}|z_{i},s,a)q(s^{ }|z_{1},s,a)d\{z_{i}\}_{i=1}^{k}.\] (7)

Indeed, Equation (7) converts the diffusion model to a latent variable model for dynamics modeling. However, the dependency of \((s,a)\) in \(q(s^{}|z_{1},s,a)\) correspondingly induces the undesirable dependency of \((s,a)\) into \((s^{})\) in (1), and therefore the factorization provided by the diffusion model cannot linearly represent the \(Q^{}\)-function -- the vanilla LV-Rep reduction from diffusion models is impossible.

### Diffusion Spectral Representation from Energy-based View

Instead of reducing to LV-Rep, in this work we extract the spectral representations by exploiting the relationship between diffusion models and energy-based models (EBMs).

Spectral Representation from EBMs.We parameterize the transition operator \((s^{}|s,a)\) using an EBM, _i.e._,

\[(s^{}|s,a)=((s,a)^{}(s^{})-  Z(s,a)),\,Z(s,a)=((s,a)^{ }(s^{}))ds^{}.\] (8)

By simple algebra manipulation,

\[(s,a)^{}(s^{})=-(\|( s,a)-(s^{})\|^{2}-\|(s,a) \|^{2}-\|(s^{})\|^{2}),\]

we obtain the quadratic potential function, leading to

\[(s^{}|s,a)(\|(s,a)\|^ {2}\!/2)(-\|(s,a)-(s^{} )\|^{2}\!/2)(\|(s^{}) \|^{2}\!/2).\] (9)

The term \((-)\| ^{2}}{2})\) is the Gaussian kernel, for which we apply the random Fourier feature (Rahimi and Recht, 2007, Dai et al., 2014) and obtain the spectral decomposition of (8),

\[(s^{}|s,a)=_{}(s,a),_{ }(s^{})_{()},\] (10)

where \((0,I)\), and

\[_{}(s,a)=(-^{}(s,a ))(\|(s,a)\|^{2}\!/2- Z (s,a)),\] (11)

\[_{}(s^{})=(-^{} (s^{}))(\|(s^{}) \|^{2}\!/2).\] (12)

This bridges the factorized EBMs (8) to SVD, offering a spectral representation for efficient planning and exploration, as will be shown subsequently.

Exploiting the random feature to connect EBMs to spectral representation for RL was first proposed by Nachum and Yang (2021) and Ren et al. (2022), but only Gaussian dynamics \(p(s^{}|s,a)\) has been considered for its closed-form \(Z(s,a)\) and tractable MLE. Equation (8) is also discussed in (Zhang et al., 2023; Ouhamma et al., 2023; Zheng et al., 2022) for exploration with UCB-style bonuses. Due to the notorious difficulty in MLE of EBMs caused by the intractability of \(Z(s,a)\)(Zhang et al., 2022), only special cases of (8) have been practically implemented. How to efficiently exploit the flexibility of general EBMs in practice still remains an open problem.

Representation Learning via Diffusion.We revisit the EBM understanding of diffusion models, which not only justifies the flexibility of diffusion models, but more importantly, paves the way for efficient learning of spectral representations (11) through Tweedie's identity for diffusion models.

Given \((s,a)\), we consider perturbing the samples from dynamics \(s^{}(s^{}|s,a)\) with Gaussian noise, _i.e._, \((^{}|s^{};)=(s^{}, I)\). Then, we parametrize the corrupted dynamics as

\[(^{}|s,a;)=(^{}|s^{ };)(s^{}|s,a)ds^{}(( s,a)^{}(^{},)),\] (13)

where \((s,a)\) is shared across all noise levels \(\), and \((^{}|s,a;)(s^{}|s,a)\) with \(^{} s^{}\), as \( 0\), there is no noise corruption on \(s^{}\).

**Proposition 1** (Tweedie's Identity (Efron, 2011)).: _For arbitrary corruption \((^{}|s^{};)\) and \(\) in \((^{}|s,a;)\), we have_

\[_{^{}}(^{}|s,a;)= _{(s^{}|^{},s,a;)}[_{ ^{}}(^{}|s^{};)].\] (14)

This can be easily verified by simple calculation, _i.e._,

\[_{^{}}(^{}|s,a; )=^{}}(^{}|s,a; )}{(^{}|s,a;)}=^{}} (^{}|s^{};)(s^{}|s,a)ds^ {}}{(^{}|s,a;)}\] \[=^{}}(^{ }|s^{};)(^{}|s^{};) (^{}|s,a)}{(^{}|s,a;)} ds^{}=_{(s^{}|s^{},s,a;)}[_{ ^{}}(^{}|s^{};)].\] (15)

For Gaussian perturbation with (13), Tweedie's identity (14) is applied as

\[(s,a)^{}_{^{}}(^{ },)=_{p(s^{}|^{},s,a;)} [s^{}-^{}}{}]\]

\[^{}+(s,a)^{}_{^{ }}(^{},)=_{ (s^{}|^{},s,a;)}[s^{}].\] (16)

Let \((^{};)=_{^{}}( ^{},)\), we can learn \((s,a)\) and \((^{};)\) by matching both sides of (16),

\[_{,}\ _{}_{(s,a,^{})}[\| ^{}+(s,a)^{}(^{ },)-_{(s^{}|^{ },s,a;)}[s^{}]\|^{2}]\] (17)which shares the same optimum of

\[_{,}\ _{}(,):=_{} _{(s,a,^{},s^{})}[\|^{}+(s,a)^{}(^{}, )-s^{}\|^{2}].\] (18)

The equivalence of (17) and (18) is provided in Appendix A.

Diffusion Spectral Representation for \(Q\)-function.The loss described by (18) estimates the score function \((s,a)^{}(^{},)\) for diffusion models. In the context of generating samples, the score function suffices to drive the reverse Markov chain process. However, when deriving the random feature \(_{}\) defined in (11), the partition function \(Z(s,a)\) is indispensable. Furthermore, the random feature \(_{}(s,a)\) is only conceptual with infinite dimensions where \((0,I)\). Next, we will proceed to analyze the structure of \(Z(s,a)\) and finally construct the spectral representation with the learned \((s,a)\).

We first illustrate \(Z(s,a)\) is also linearly representable by random features of \((s,a)\),

**Proposition 2**.: _Denote \(_{}(s,a):=(-^{}(s,a)+\|(s,a)\|^{2}/2)\), the partition function is linearly representable by \(_{}(s,a)\), i.e., \(Z(s,a)=_{}(s,a),u_{ ()}\)._

Proof.: We have \(_{}(s,a)=(s,a)}{Z(s,a )}\), and \((s^{}|s,a)=(s,a )}{Z(s,a)},_{}(s^{})_{ ()}\), which implies

\[(s^{}|s,a)ds^{}=1 (s,a)}{Z(s,a)},(s^{})ds^{}}_{u}=1 _{}(s,a),u_{( )}=Z(s,a).\]

Plug this into (11) and (2), we can represent the \(Q^{}\)-function for arbitrary \(\) as

\[Q^{}(s,a)=(s,a)}{ _{}(s,a),u},^{} _{()}=^{}(s,a)-(- ^{}(s,a)),u_{( )})}_{_{,}(s,a)},^{} _{()},\] (19)

which eliminates the explicit partition function calculation.

We thereby construct _Diffusion Spectral Representation (Diff-SR)_, a tractable finite-dimensional representation, by approximating \(_{,u}(s,a)\) with some neural network upon \((s,a)\). Specifically, in the definition of \(_{,u}(s,a)\) in (19), it contains Fourier basis \((-^{}(s,a))\), suggesting the use of trigonometry functions (Rahimi and Recht, 2007) upon \((s,a)\), _i.e._, \((W_{1}^{}(s,a))\). Meanwhile, it also contains a product with \(^{}(s,a) ),u_{()}}\), suggesting the additional non-linearity over \((W_{1}^{}(s,a))\). Therefore, we consider the finite-dimensional neural network \(_{}(s,a)=(W_{2}(W_{1}^{} (s,a)))^{d}\) with learnable parameters \(=(W_{1},W_{2})\), to approximate the infinite-dimensional \(_{}(s,a)\) with \((0,I)\).

Remark (Connection to sufficient dimension reduction (Sasaki and Hyvarinen, 2018)):The theoretical properties of the factorized potential function in EBMs (8) have been investigated in (Sasaki and Hyvarinen, 2018). Specifically, the factorization is actually capable of universal approximation. Moreover, \((s,a)\) also constructs an implementation of sufficient dimension reduction (Fukumizu et al., 2009), _i.e._, given \((s,a)\), we have \((s^{}|s,a)=(s^{}|(s,a ))\), or equivalently \(s^{}(s,a)|(s,a)\). These properties justify the expressiveness and sufficiency of the learned \((s,a)\). However, \((s,a)\) in (Sasaki and Hyvarinen, 2018) is only estimated up to the partition function \(Z(s,a)\), which makes it not directly applicable for planning and exploration in RL as we discussed.

### Policy Optimization with Diffusion Spectral Representation

With the approximated finite-dimensional representation \(_{}\), the \(Q\)-function can be represented as a linear function of \(_{}\) and a weight vector \(^{d}\),

\[Q_{,}(s,a)=_{}(s,a)^{}\] (20)

This approximated value function can be integrated into any model-free algorithm for policy optimization. In particular, we update \((,)\) with the standard TD learning objective

\[_{}(,)=_{s,a,r,s^{}} [(r+_{a^{}}[Q_{,}( s^{},a^{})]-Q_{,}(s,a))^{2}]\,,\] (21)

where \(\) is the learning algorithm's current policy, \((,)\) is the target network, \(\) is the replay buffer. We apply the _double Q-network trick_ to stabilize training . In particular, two weights \((_{1},_{1}),(_{2},_{2})\) are initialized and updated independently according to (21). Then the policy is updated by considering \(_{}_{s,a}[_{i\{1,2\}}Q_{_{i}, _{i}}(s,a)]\). Algorithm 2 presents the pseudocode of online RL with Diff-SR. This learning framework is largely consistent with standard online reinforcement learning (RL) approaches, with the primary distinction being the incorporation of diffusion representations. As more data is collected, Line 7 specifies the update of the diffusion representation \(_{}\) using Algorithm 1 and the latest buffer \(\).

Remark (Exploration with Diffusion Spectral Representation):Following , even \((-^{}(s,a))\) is not enough for linearly representing \(Q^{}\), we still can exploit \((-^{}(s,a))\) for bonus calculation to implement the principle of optimism in the face of uncertainty for exploration in RL, _i.e._,

\[b(s,a)=1-K^{}(s,a)(K+ I)^{-1}K(s,a),\] (22)

where \(\) is a dataset of state action pairs, \(k((s,a),(s^{},a^{})):=(-,a^{})\|^{2}}{2})\), \(K(s,a):=[k((s,a),(s,a)_{i})]_{(s,a)_{i}}\), and \(K=[K((s,a)_{j})]_{(s,a)_{j}}\). The bonus (22) derivation is straightforward by applying the connection between random feature and kernel, similar to . In practice, we can also approximate UCB bonus with Diff-SR as

\[b(s,a)=_{}(s,a)^{}(_{s^{},a^{} }_{}(s,a)_{}(s,a)^{}+ I )^{-1}_{}(s,a).\] (23)

These bonuses are subsequently added to the reward in (21) to facilitate exploration.

Remark (Comparison to Spectral Representation):Both the proposed Diff-SR and the existing spectral representation for RL  are extracting the representation for \(Q\)-function. However, we emphasize that the major difference lies in that existing spectral representations seek low-rank linear representations, while our representation is sufficient for representing \(Q\)-function, but in a nonlinear form, as we revealed in (19). This nonlinearity in fact comes from the partition function \(Z(s,a)\), which is constrained to be \(1\) in , and thus, less flexible for representation. Meanwhile, even with nonlinear representation, it has been shown that the corresponding bonus is still enough for exploration , without additional computational cost.

Related Work

**Representation learning in RL.** Learning good abstractions for the raw states and actions based on the structure of the environment dynamics is thought to facilitate policy optimization. To effectively capture the information in said dynamics, existing representation learning methods employ various techniques, such as reconstruction (Watter et al., 2015; Hafner et al., 2019; Fujimoto et al., 2023), successor features (Dayan, 1993; Kulkarni et al., 2016; Barreto et al., 2017), bisimulation (Ferms et al., 2004; Gelada et al., 2019; Zhang et al., 2021), contrastive learning (Oord et al., 2018; Nachum and Yang, 2021), and spectral decomposition (Mahadevan and Maggioni, 2007; Wu et al., 2018; Duan et al., 2019; Ren et al., 2022b). Previous works also leverage the assumption that the transition kernel possesses a low-rank spectral structure, which permits linear representations for the state-action value function and provably sample-efficient reinforcement learning (Jin et al., 2020; Yang and Wang, 2020; Agarwal et al., 2020; Uehara et al., 2022). Based on this, there have been several attempts towards both practical and theoretically grounded reinforcement learning algorithms by extracting the spectral representations from the transition kernel (Ren et al., 2022; Zhang et al., 2022; Ren et al., 2022; Zhang et al., 2023a). Our approach aligns with this paradigm but distinguishes itself by learning these representations via diffusion and enjoying the flexibility of energy-based modeling.

**Diffusion model for RL.** By virtue of their ability to model complex and multimodal distributions, diffusion models present themselves as well-suited candidates for specific components in reinforcement learning. For example, diffusion models can be utilized to synthesize complex behaviors (Janner et al., 2022; Ajay et al., 2022; Chi et al., 2023; Du et al., 2024), represent multimodal policies (Wang et al., 2022; Hansen-Estruch et al., 2023; Chen et al., 2023), or provide behavior regularizations (Chen et al., 2023). Another line of research utilizes the diffusion model as the world model. Among them, DWM (Ding et al., 2024) and SynthER (Lu et al., 2023) train a diffusion model with off-policy dataset and augment the training dataset with synthesized data, while PolyGRAD (Rigter et al., 2023) and PGD (Jackson et al., 2024) sample from the diffusion model with policy guidance to generate near on-policy trajectory. On a larger scale, UniSim (Yang et al., 2023) employs a video diffusion model to learn a real-world simulator that accommodates instructions in various modalities. All of these methods incur great computational costs because they all involve iteratively sampling from the diffusion model to generate actions or trajectories. In contrast, our method leverages the capabilities of diffusion models from the perspective of representation learning, thus circumventing the generation costs.

## 5 Experiments

We evaluate our method with state-based MDP tasks (Gym-MuJoCo locomotion (Todorov et al., 2012)) and image-based POMDP tasks (Meta-World Benchmark (Yu et al., 2020)) in this section. Besides, we also provide experiments with state-based POMDP tasks in Appendix E. Our code is publicly released at the project website.

### Results of Gym-MuJoCo Tasks

In this first set of experiments, we compare Diff-SR against both model-based and model-free baseline algorithms, using the implementations provided by MBBL (Wang et al., 2019). For representation-based RL algorithms, we include LV-Rep (Ren et al., 2022), SPEDE (Ren et al., 2022) and Deep

Figure 1: The performance curves of the Diff-SR and baseline methods on MBBL tasks. We report the mean (solid line) and one standard deviation (shaded area) across 4 random seeds.

Successor Feature (DeepSF) (Barreto et al., 2017) as baselines. Note that the SPEDE is a special case of Gaussian EBM. We also include PolyGRAD (Rigter et al., 2023), a recent method that utilizes diffusion models for RL as an additional baseline. All algorithms are executed for 200K environment steps and we report the mean and standard deviation of performances across 4 random seeds. More implementation details including the hyper-parameters can be found in Appendix C.

Table 1 presents the results, demonstrating that Diff-SR achieves significantly better or comparable performance to all baseline methods in most tasks except for Humanoid-ET. Specifically, in Ant and Walker, Diff-SR outperforms the second highest baseline LV-Rep by 90% and 48%. Moreover, Diff-SR consistently surpasses PolyGRAD in nearly all environments. We also provide the learning curves of Diff-SR and baseline methods in Figure 1 for an illustrative interpretation of the sample efficiency Diff-SR brings.

**Computational Efficiency and Runtime Comparison** Compared to other diffusion-based RL algorithms, Diff-SR harnesses diffusion's flexibility while circumventing the time-consuming sampling process. To showcase this, we record the runtime of Diff-SR and PolyGRAD on MBBL tasks using workstations equipped with Quadro RTX 6000 cards. Results in Figure 2 illustrate that Diff-SR is about \(4\) faster than PolyGRAD, and such advantage is consistent across all environments. We provide a per-task breakdown of the runtime results in Appendix 4 due to space constraints.

### Results of Meta-World Tasks

As the most difficult setting, we evaluate Diff-SR with 8 visual-input tasks selected from the Meta-World Benchmark. Rather than directly diffuse over the space of raw pixels, we resort to techniques similar to the Latent Diffusion Model (LDM) (Rombach et al., 2022) which first encodes the raw

    & & HalfCheath & Reacher & Humanoid-ET & Pendulum & **L**-Pendulum \\   & ME-TRPO* & \(2283.7 900.4\) & \(-13.4 5.2\) & \(72.9 8.9\) & \(\) & \(-126.2 86.6\) \\  & PETS-RS* & \(966.9 471.6\) & \(-40.1 6.9\) & \(109.6 102.6\) & \(167.9 35.8\) & \(-12.1 25.1\) \\  & PETS-CEM* & \(2795.3 879.9\) & \(-12.3 5.2\) & \(110.8 90.1\) & \(167.4 53.0\) & \(-20.5 28.9\) \\  & Best MBBL* & \(3639.0 1135.8\) & \(\) & \(1377.0 150.4\) & \(\) & \(\) \\  & PolyGRAD & \(2563.5 204.2\) & \(-20.7 1.9\) & \(1026.6 58.7\) & \(166.3 6.3\) & \(-3.5 4.8\) \\   & PPO* & \(17.2 84.4\) & \(-17.2 0.9\) & \(451.4 39.1\) & \(163.4 8.0\) & \(-40.8 21.0\) \\  & TRPO* & \(-12.0 85.5\) & \(-10.1 0.6\) & \(289.8 5.2\) & \(166.7 7.3\) & \(-27.6 15.8\) \\  & SAC* (3-layer) & \(4000.7 202.1\) & \(-6.4 0.5\) & \(\) & \(168.2 9.5\) & \(-0.2 0.1\) \\   & DeepSF! & \(4180.4 113.8\) & \(-16.8 3.6\) & \(168.6 5.1\) & \(168.6 5.1\) & \(-0.2 0.3\) \\  & SPEDE! & \(4210.3 92.6\) & \(-7.2 1.1\) & \(886.9 95.2\) & \(169.5 0.6\) & \(0.0 0.0\) \\  & LV-Rep! & \(5557.6 439.5\) & \(-5.8 0.3\) & \(1086 278.2\) & \(167.1 3.1\) & \(\) \\  & **Diff-SR** & \(\) & \(-6.5 2.0\) & \(821.3 118.9\) & \(162.3 3.0\) & \(\) \\   &  & Anti-ET & Hopper-ET & S-Humanoid-ET & CartPole & Walker-ET \\  & & \(42.6 21.1\) & \(1272.5 500.9\) & \(-154.9 534.3\) & \(160.1 69.1\) & \(-1609.3 657.5\) \\  & PETS-RS* & \(130.0 148.1\) & \(205.8 36.5\) & \(320.7 182.2\) & \(195.0 28.0\) & \(312.5 493.4\) \\  & PETS-CEM* & \(81.6 145.8\) & \(129.3 36.0\) & \(355.1 157.1\) & \(195.5 3.0\) & \(260.2 356.9\) \\  & Best MBBL* & \(275.4 309.1\) & \(1272.5 500.9\) & \(\) & \(\) & \(312.5 493.4\) \\  & PolyGRAD & \(43.6 158.6\) & \(1151.6 182.3\) & \(\) & \(193.4 11.5\) & \(268.4 77.3\) \\   & PPO* & \(80.1 17.3\) & \(758.0 62.0\) & \(454.3 36.7\) & \(86.5 7.8\) & \(306.1 17.2\) \\  & TRPO* & \(116.8 47.3\) & \(237.4 33.5\) & \(281.3 10.9\) & \(47.3 15.7\) & \(229.5 27.1\) \\  & SAC* (3-layer) & \(2012.7 571.3\) & \(1815.5 655.1\) & \(834.6 313.1\) & \(\) & \(2216.4 678.7\) \\   & DeepSF! & \(768.1 441.1\) & \(548.9 253.3\) & \(533.8 154.9\) & \(194.5 5.8\) & \(165.6 127.9\) \\  & SPEDED! & \(806.2 60.2\) & \(732.2 63.9\) & \(986.4 154.7\) & \(138.2 39.5\) & \(501.6 204.0\) \\  & LV-Rep! & \(2511.8 460.0\) & \(2204.8 496.0\) & \(963.1 45.1\) & \(\) & \(2523.5 333.9\) \\  & **Diff-SR** & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with * are taken from MBBL (Wang et al., 2019) and \(\) are taken from LV-Rep (Ren et al., 2022).

Figure 2: Runtime comparison between Diff-SR vs. LV-Rep vs. diffusion-based RL (PolyGRAD).

observation image to a 1-D compact latent vector and afterward performs the diffusion process over the latent space. To deal with the partial observability, we truncate the history frames with length \(L\), encode these \(L\) frames into their latent embeddings, concatenate them together, and treat them as the state of the agent. The learning of diffusion representation thus translates into predicting the next frame's latent embedding with the action and \(L\)-step concatenated embedding. Following existing practices (Zhang et al., 2023), we set \(L=3\) for all Meta-World tasks. We use DrQ-V2 (Yarats et al., 2021) as the base RL algorithm, and more details of the implementations are deferred to Appendix F.

The performance curves of Diff-SR and the baseline algorithms are presented in Figure 3. We see that Diff-SR achieves a greater than \(90\%\) success rate for seven of the tasks, 4 more tasks than the second best baseline \(\)LV-Rep. Overall, Diff-SR exhibits superior performance, faster convergence speed, and stable optimization in most of the tasks compared to the baseline methods. Finally, although Diff-SR does not require sample generation, we present the reconstruction results to validate the efficacy of the score function \((s,a)^{}(^{},)\) in Figure 6.

## 6 Conclusions and Discussions

We introduce Diff-SR, a novel algorithmic framework designed to leverage diffusion models for reinforcement learning from a representation learning perspective. The primary contribution of our work lies in exploiting the connection between diffusion models and energy-based models, thereby enabling the extraction of spectral representations of the transition function. We demonstrate that such diffusion-based representations can sufficiently express the value function of any policy, facilitating efficient planning and exploration while mitigating the high inference costs typically associated with diffusion-based methods. Empirically, we conduct comprehensive studies to validate the effectiveness of Diff-SR in both fully and partially observable sequential decision-making problems. Our results underscore the robustness and advantages of Diff-SR across various benchmarks. However, the main limitation of this study is that Diff-SR has not yet been evaluated with real-world and multi-task data. Future work will focus on testing Diff-SR on real-world applications, such as robotic control.

Figure 3: Performance curves for image-based POMDP tasks from Meta-World. We report the mean (solid line) and the standard deviation (shaded area) of performances across 5 random seeds.