# RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance

Jiaojiao Fan

Georgia Tech

sbyebss@gmail.com &Haotian Xue

Georgia Tech

htxue.ai@gatech.edu &Qinsheng Zhang

NVIDIA

qsh.zh27@gmail.com &Yongxin Chen

Georgia Tech

yongchen@gatech.edu

###### Abstract

There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models. Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning. Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness. In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient. Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism. The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner. Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency. Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, RefDrop is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model. Our project webpage is https://sbyebss.github.io/refdrop/.

## 1 Introduction

Large-scale diffusion models have demonstrated remarkable capabilities in aiding content creation for artists . Numerous text-to-image models are expediting content production in various domains, including advertising and art studios. Similarly, video generation models have shown significant advancements recently . However, enhancing these models to better support artistic creativity requires improved controllability, particularly in content consistency. This paper explores consistency from two perspectives: 1) controlling subject consistency across multiple images, and 2) maintaining subject consistency across multiple frames within a video.

We name a few tasks where the controllable consistency is crucial in AI content generation. In image generation for storytelling  or advertising, content creators often strive to produce consistent characters, a task that proves challenging with foundational generative models . Personalization approaches based on fine-tuning  require a minimum of 5 to 10 images to achievesatisfactory quality, and encoder-based methods [61; 59; 40] demand weeks of training with millions of images for a single diffusion model and lack transferability to other foundational models. On the other hand, diverse image generation is less addressed but persistently challenging. In this scenario, it is desired to _decrease_ the consistency among image generations. For example, artists can sometimes seek to enhance diversity and avoid cliches, such as the stereotypical depiction of Barbie girls with curly blonde hair. For video generation, another challenging task is maintaining temporal consistency in video generation, yet most existing solutions are confined to video editing tasks , demanding high-quality input videos.

These emerging tasks motivate us to develop RefDrop, a **training-free**, **plug-and-play** method designed to provide flexible control over the consistency in image and video generation. Specifically, we modify the self-attention mechanism in the diffusion model UNet  architecture and introduce a coefficient to modulate the influence of a reference image on the generation process. Our contributions are outlined as follows:

1. We conduct a detailed analysis of popular consistency generation methods based on concatenated attention, revealing that their consistency is actually contributed by extra guidance applied implicitly. 2. Inspired by this finding, we propose Reference Feature Guidance (RFG), a natural extension that

Figure 1: RefDrop achieves controllable consistency in visual content synthesis for free. RefDrop ex-inhibits great flexibility in (Upper) multi-subject consistency generation given one reference image, (Middle) blending different characters from multiple images seamlessly, (Button) enhancing temporal consistency for personalized video generation. RefDrop is short for ”reference drop”. We named our method RefDrop to metaphorically represent the process by which a drop of colored water influences a larger body of clear water.

explicitly controls the guidance from reference context in a precise and direct manner. Building upon RFG, we introduce RefDrop, a flexible and efficient approach to controlling consistency without the need for network fine-tuning or optimization. 3. Besides improvements in character consistency using a single reference image, RefDrop enables more creative applications with controllable consistency, including (i) seamless integration of distinct features into a single cohesive image (ii) suppressing specific features by negatively decreasing the consistency influenced by the reference context, thereby enhancing diversity in layout, accessories, and image style; (iii) high-quality personalized video generation by boosting temporal consistency, and minimizing facial distortions. 4. We conduct comprehensive experiments and demonstrate that RefDrop achieves a good balance between flexibility and effectiveness while being lightweight compared to existing works.

## 2 Related work

Among the works most similar to ours are IP-Adapter  and concatenated attention . Our approach is closely related to IP-Adapter, as both methods utilize the sum of two decoupled attention outputs. However, while IP-Adapter modifies cross-attention and requires separate training of an image encoder to embed the reference image, we integrate the reference image directly into the self-attention layer without needing additional training. Furthermore, our reference images are _generated_ by the same model, in contrast to IP-Adapter's reliance on externally sourced image. Both techniques permit the use of negative or positive coefficients for the reference image, but IP-Adapter may compromise text alignment  due to its reference image being intertwined with the text prompt during cross-attention. Additionally, the IP-Adapter requires separate training for different versions of the diffusion model, such as SD2.1 and SDXL. In contrast, RefDrop is a simple plug-and-play.

Concatenated attention, first introduced in video generation literature by Wu et al.  as spatio-temporal attention, injects temporal information into a T2I model. It has since been widely adopted for feature injection across various applications [37; 8; 25; 55] in content generation and video editing. This concept has evolved into Cross-Frame Attention , another prevalent technique used to inflate T2I models  for video generation. We will demonstrate later that our framework can replicate these two types of attention as special cases.

A concurrent work by Avrahami et al.  introduces a method called soft blending, which is quite similar to our RFG (5), but applies it to a different application: object dragging.

Consistency in Image GenerationConsiStory  and StoryDiffusion  are closely related to our work. They are training-free methods that employs concatenated attention to enhance consistency in generation. Our RFG framework is _orthogonal_ to the techniques other than concatenated attention in those works, such as subject masking and attention dropout. Avrahami et al.  explores a fine-tuning-based method aimed at recovering tightly clustered images. Other approaches, such as those by [27; 12; 35], predominantly utilize a personalization process [50; 14; 32; 54] requiring multiple input images for training. Finally, several encoder-based methods [61; 59; 33; 51; 64; 30; 34] do not require additional training for new subjects. However, these methods necessitate days or weeks of initial training for the encoder and face limitations in adaptability to different versions of foundational generative models.

Temporal-consistency in video generationConcatenated attention [62; 47] and Cross-Frame Attention [29; 10] are popular techniques used to inflate T2I models for video generation. Wu et al. , Ren et al.  mitigate video flickering by applying a low-pass filter to noisy latent images, effectively removing disruptive high-frequency content. Many other methods are tailored for video editing tasks, and they either extract features from high-quality input videos to enhance the current generation [31; 70; 66; 65] or use them as references during editing [16; 11]. RefDrop improves temporal consistency directly within video generation, obviating the need for an input video.

## 3 Method

We first introduce how existing works achieve consistency generation by leveraging the concatenation of reference features in the self-attention block. Then we reformulate the concatenation as a linear interpolation of self-attention on synthesized content and cross-attention between generated and reference content with a constant rank-1 coefficient. We highlight that this specific coefficient is not a necessity, while linear interpolation is critical to minimizing the training-inference gap. Building upon these observations, we propose Reference Feature Guidance (RFG), an extension of concatenation attention that allows for flexible feature interpolation and extrapolation in attention modules. Based on RFG, we introduce RefDrop, a versatile method for controllable consistency generation across various applications.

### Background

Self-attention in diffusion model networks operates by applying the attention mechanism  on synthesized latent features. A self-attention layer processes latent representations \(X\) by passing them through linear projection layers to produce queries \(Q=XW_{Q}\), keys \(K=XW_{K}\), and values \(V=XW_{V}\), which then undergo the attention operation as follows:

\[X^{}=(Q,K,V)=(}{ })V,\] (1)

where \(X^{}\) is the output of self-attention operation, and \(d\) is the feature dimension of projection matrices \(W_{Q},W_{K},W_{V}\). Previous consistency generation [55; 71] is based on concatenated attention via a simple batch image generation, where the first sample in the batch serves as a reference for the \(i\)-th sample generation. We denote the latent feature for \(i\)-th sample as \(X_{i}\). Instead of solely depending on its own content, concatenated attention suggests

\[X^{}_{}=(Q_{i},[K_{1};K_{i}],[V_{1};V _{i}]),\] (2)

where \(Q_{i}=X_{i}W_{Q}\), \(K_{i}=X_{i}W_{K}\), and \(V_{i}=X_{i}W_{V}\).

### Reference feature guidance

To illustrate why concatenated attention can help boost consistency between generated samples with reference samples, we can reformulate eq. (2) as the following (Proof in appendix C)

\[X^{}_{}=C(Q_{i},K_{1},V_{1} )+(-C)(Q_{i},K_{i},V_{i})\] (3)

where \(C\) is a rank-1 matrix of the same size as the attention output, \(\) is the point-wise multiplication and \(\) is an all-ones matrix.

Equation (3) depicts that the concatenated attention is a linear interpolation between the output \(X^{}\) without concatenated attention in eq. (1) and cross-attention between the \(i\)-th image \(X_{i}\) and the reference image \(X_{1}\), while the coefficient matrix \(C\) is determined by the synthesized content \(X_{i}\) and the reference content \(X_{1}\). Before we further improve concatenated attention, we first discuss two related questions for eq. (3). **Is linear interpolation a necessity?** It may be tempting to highlight the role of the second cross-attention term naively while keeping the weights for the first term unchanged, such as \((Q_{i},K_{1},V_{1})+(Q_{i},K_{ i},V_{i})\). However, we find that naively breaking the linear interpolation disrupts image generation. In fact, we can interpret concatenated attention in eq. (3) as applying extra guidance on the original self-attention output

\[X^{}_{}=(Q_{i},K_{i},V_{i})+C ((Q_{i},K_{1},V_{1})- (Q_{i},K_{i},V_{i}))\] (4)

Figure 2: During each diffusion denoising step, we facilitate the injection of features from a _generated_ reference image \(I_{1}\) into the generation process of other images through RFG. The RFG layer produces a linear combination of the attention outputs from both the standard and referenced routes. A negative coefficient \(c\) encourages divergence of \(I_{i}\) from \(I_{1}\), while a positive coefficient fosters consistency.

which resembles the form of guidance used in diffusion literature [53; 22], such as classifier-free guidance . Notably, the linear interpolation helps keep the attention output \(X_{}^{}\) norm close to self attention output \(X^{}\); otherwise, arbitrary weights would pose a training and inference discrepancy and degrade the generation quality. However, different from various guidance methods used in the diffusion literature, the guidance weights in eq.4 are constants determined by latent features \(X_{i}\) and reference context \(X_{1}\) and have no user control. Therefore we question **Is constant \(C\) matrix coefficient is a necessity?** As an attempt to bypass the rigid form of concatenated attention, we propose a simple and flexible approach named _Reference feature guidance_ (RFG) (see Fig.2)

\[X_{}^{}=c(Q_{i},K_{1},V_{1})+ (1-c)(Q_{i},K_{i},V_{i}),\] (5)

where \(c\) is a scalar coefficient that controls the strength of the reference image influence.

While most previous methods for consistent generation, including feature combination [30; 71] and injection , employ concatenated attention (2), our RFG offers several advantages. First, it grants users greater control over the extent of influence from the reference image, as illustrated in Fig.3. Second, this flexibility proves especially beneficial in a novel application: blending features from multiple reference images. Our method allows users to selectively determine the influence of each reference image. We have observed that the most harmonious blending often results from varying the strength of each reference, rather than maintaining equal strength across all images. Third, by enabling negative coefficients, we find that our method can simulate a concept suppression effect, meaning it generates images that are dissimilar to a reference image. Moreover, it allows for the injection of reference image features into video generation slightly to reduce flickering, while the concatenated attention keeps the video completely static. Finally, our approach is versatile on network architecture as it applies not only to UNet-based models but also to transformer-based diffusion models, such as FLUX model (see appendixD.1).

Therefore, we introduce RefDrop, a training-free approach to flexibly control consistency generation, which replaces the self-attention blocks in the diffusion model with RFG. For Video Diffusion Models (VDM) [5; 15; 19], we modify _every spatial_ self-attention layers to bolster temporal consistency.

## 4 Experiments

We conduct experiments to show that RefDrop can help control consistency in two important tasks: image generation and video generation.

### Controllable consistency in image generation

We use a fine-tuned SDXL of higher quality, ProtoVision-XL, as the base model for our experiments. For simplicity, we will refer to it as

   Name & Training & Concept & Single \\  & free & suppression & ref. \\  IP-Adapter  & ✗ & ✓ & ✓ \\ Consistency  & ✓ & ✗ & ✓ \\ Chosen one  & ✗ & ✗ & ✗ \\ ELITE  & ✗ & ✗ & ✓ \\ BLIPD  & ✗ & ✗ & ✓ \\ Ours & ✓ & ✓ & ✓ \\   

Table 1: Comparison of Controllable Consistent Image Generation Methods. ‘Training-free’ indicates no encoder training or diffusion model fine-tuning is needed. ‘Single ref.’ means the method can operate with only one reference image.

Figure 3: We allow flexible control over the reference effect through a reference strength coefficient.

SDXL hereafter. We have replaced all the self-attention layers in SDXL with RFG, using the first sample in the batch as the reference image.

Evaluation baselinesIn this section, we compare RefDrop with several baseline approaches: (1) SDXL  without any modifications to its architecture; (2) Ref-ControlNet 1; (3) encoder-based methods, such as IP-Adapter  and BLIPD . For encoder-based methods, we initially generate a reference image using SDXL and then utilize this image as input. Additionally, we present a comparison of several other methods in Table 1.

#### 4.1.1 Consistent image generation

For this task, we use \(c[0.3,0.4]\) for our method. However, applying RFG to all the self-attention blocks can lead to the leakage of spatial layout and background from the reference image, causing the generated objects to have quite similar poses and backgrounds. To address these issues, we introduce two techniques: excluding the first upsampling block and applying the subject mask.

**Excluding the first upsampling block** SDXL

UNet consists of 4 downsampling blocks, 1 middle block, and 6 upsampling blocks. Through an ablation study, we found that the first upsampling block predominantly influences the spatial layout. As shown in Fig. 5, excluding this block from the modified attention blocks allows for recovering diverse object poses. To the best of our knowledge, this is the first work to use this method for mitigating spatial layout leakage in consistent image generation. Consistency  also proposes two techniques to enhance layout diversity: using vanilla query features and self-attention dropout. In comparison, our approach is simpler and more straightforward.

**Applying the subject mask** We use Grounded SAM  to extract the object mask from the generated reference image by prompting the object name, such as "Guinea pig" or "human." The

Figure 4: The reference image for all methods is framed in red. Our method tends to produce more consistent hairstyles, and facial features compared to IP-Adapter, Ref-ControlNet and BLIPD, and our generation has diverse spatial layout. The visual quality of BLIPD is not comparable, as it utilizes SD1.5  as its base model.

Figure 5: Excluding one block from applying RFG solves the spatial layout leakage issue. Adding subject mask solves the background leakage issue.

mask is then downsampled to match the latent feature resolution of the SDXL UNet. The resulting masked RFG is defined as follows

\[X^{}_{}=cM(Q_{i},K_{1},V_{1})+ (-cM)(Q_{i},K_{i},V_{i}),\] (6)

where the mask \(M\) ensures that guidance is restricted to the masked area. Note that, our masked RFG (6) does not modify the attention operation itself but only adjusts the coefficients, making it memory efficient and straightforward to implement.

We show qualitative results in Fig. 4. IP-Adapter establishes a strong baseline, especially on single subject consistent generation. However, it suffers from similar spatial layout, and requires additional computational resources and data for training the image encoder compared to our approach. While Reference-only ControlNet performs well on simple subjects, such as cartoon characters, it struggles to generate humans. It is likely to produces humans with distorted eyes and bodies. BLIPD underperforms in terms of both visual quality and consistency relative to RefDrop. For **multi-subject** consistent generation, we find RefDrop can straightforwardly work for semantically different objects even without using separate subject masks. This observation aligns with ConsiStory . Further comparative results are available in Figs. 13 and 16.

#### 4.1.2 Blend features from multiple images

RefDrop also supports the use of multiple reference images. In our implementation, we designate the first \(N\) images in a batch as reference images. Features from these reference images are then incorporated into the subsequent images within the same batch through every self-attention layer. Formally, the extended RFG with multiple references is defined as

\[X^{}_{}=_{j=1}^{N}c_{j} (Q_{i},K_{j},V_{j})+\] \[(1-_{j=1}^{N}c_{j})(Q_{i},K_{i},V_{i}),\] (7)

for certain \(i>N\). Here, the attention mechanism ensures that the \(i\)-th image in the batch receives features from the first \(1 N\) reference images. In practice, we use \(c_{j}[0.2,0.4]\) for any \(j=1,,N\) in our method. We demonstrate the capability of RefDrop to seamlessly blend distinct semantic features from two reference objects into a new object in Figs. 6, 18 and 19. This task proves challenging when relying solely on prompt engineering. We attempted to achieve this task with SDXL using text prompts. For instance, if we aim to merge two objects, \(\) and \(\), we might use prompts like "an \(\)-like \(\)" or "a \(\) in the style of \(\)." However, with such text prompts, SDXL either ignores the similarity with one of the reference images or frequently produces multiple objects instead of a single and cohesive entity. In Fig. 20, we show that RefDrop can blend _three_ distinct subjects: a dwarf, Black Widow, and Winnie the Pooh, encompassing a range of mythological being, human, and animal.

#### 4.1.3 Diverse image generation

Our method offers substantial flexibility in parameter tuning, enabling diverse image generation by setting the coefficient \(c\) to a negative value. This feature is particularly valuable in addressing overfitting issues in image generation. For instance, when using SDXL to generate Middle Eastern faces, the output frequently includes similar headscarves, faces and outfits, as illustrated on the left side of Fig. 8.

In this task, we use \(c=-0.3\) for our method. We present a qualitative comparison in Fig. 8, with negative reference images displayed in Fig. 7. Upon comparing our method with IP-Adapter, we

Figure 6: **Multiple Reference Images:** The reference images are highlighted with a red frame, and the third image in each set is the resultant blended image. RefDrop effectively assimilates features from the distinct reference images into a single and cohesive entity, demonstrating robust feature integration capability.

Figure 7: Negative reference images for examples in Fig. 8.

note that IP-Adapter may not adhere as closely to the text prompt. We attribute this to IP-Adapter's modification of cross-attention, which can impact text alignment. In contrast, our method focuses on modifying self-attention, thereby preserving the integrity of cross-attention and ensuring more accurate text alignment. We show additional quantitative results in Fig. 15.

### Improving temporal-consistency in video generation

Not only can we apply RFG to T2I generation, but it also effectively stabilizes video generation, where flickering commonly degrades quality. This section shows that using the first generated frame as a reference can greatly improve video generation. By injecting its features into the spatial self-attention layers of subsequent frames with a reference strength of \(c=0.2\), we significantly stabilize these frames and enhance the temporal consistency of VDM.

We employ SVD-img2vid-xt-1-1 as our I2V base model. Technically, our approach is compatible with any VDM, but we choose SVD as it is the best open-source model available. Although this model usually produces consistent videos from visually perfect images, we have noted that minor, often imperceptible flaws in the input images can significantly degrade the quality of the generated videos. Our method effectively stabilizes video quality in these scenarios.

#### 4.2.1 Temporal-consistent video generation

Figure 8: Diverse image generation: Our method enhances diversity in outfits, hairstyles, and facial features, all while ensuring accurate text alignment. For example, while SDXL frequently generates headscarves in the first scenario and beige-colored clothes in the second, RefDrop can vary the presence of headscarves in the left example and produce clothing in different colors in the right example. Conversely, although IP-Adapter can create even more diverse images, it often fails to adhere to the style and human activity instructions in the text prompts. Additionally, it often produces overly small persons that lack detail.

Figure 9: Comparison of training free techniques to improve temporal consistency in video generation.

In this part, we use the SDXL model to generate an image from a prompt and then pass this image to the SVD model. For **evaluation baselines**, we compare RefDrop with several training-free methods: unmodified SVD, Cross-Frame Attention , Concatenated Attention , and Temporal Low Pass Frequency Filter (LPFF) . Temporal LPFF is arguably superior to Spatial-Temporal LPFF by Wu et al. , which shows Spatial-Temporal LPFF can result in blurry frames. We evaluate the Temporal LPFF using a fast sampling method that avoids the computationally intensive process of iteratively performing backward and forward diffusion at each denoising step.

The visualization results are displayed in Fig. 9. We observe that both Cross-Frame Attention and Concatenated Attention result in completely static videos, whereas LPFF shows minimal improvement. Our method proves to be the most effective in preventing flickering while preserving motion.

#### 4.2.2 Stabilizing personalized video generation

Finally, we explore the application of RefDrop to personalized video generation. Inspired by Ku et al. , starting with an image of a person, we use InstantID  to generate a personalized initial frame. This frame is then fed into SVD to create a short video. However, we observe that using the output from InstantID for SVD generation leads to a significantly higher failure rate compared to using the initial frame generated by SDXL. We attribute this increased failure rate to InstantID's propensity for producing images with more flaws, such as overly saturated colors, and distorted limbs, highlighting the potential demand for RefDrop in this task.

Several other methods are available for personalized video generation, as described in works [60; 28; 38; 24]. Our method, which is designed to enhance temporal consistency, can be integrated with some of these existing approaches. For example, in the case of Magic-Me , our attention mechanism RFG can be incorporated into their AnimateDiff  backbone. For the evaluation in this section, we primarily focus on comparisons with naive SVD generation, as it directly relates to our goal of enhancing temporal consistency.

We present such comparison between our RefDrop enhanced generation to the naive SVD generation in Fig. 10. RefDrop effectively preserves identity during video generation, offering improvements similar to those achieved by increasing the CFG. However, unlike increasing CFG, which often results in over-saturation of videos, our approach does not produce such artifacts. We present additional automatic metrics in Table 3 to show that RefDrop can enhance the quality of the generated videos.

## 5 Human evaluation

We conducted a human evaluation study using Google Forms. Our survey is structured into three distinct categories: 1) Consistent Image Generation, 2) Diverse Image Generation, and 3) Personalized Video Generation. Initially, we utilized ChatGPT to generate text prompts, then processed approximately 100 small tasks per category using both baseline methods and our approach. From these, we randomly selected 10 sets for evaluation. In the first two categories, participants assessed the

Figure 10: By injecting the features of the first frame into the generation of subsequent frames, RefDrop reduces flickering and facial distortions. The additional videos can be viewed here.

consistency and diversity of the images, as well as text alignment. For the third category, participants were asked to select the video with better quality. The vertical axes in Fig. 11 mean the aggregated scores from all participants. We collected responses from 44 distinct users in total. More details appear in appendix G.

## 6 Conclusion

In this study, we propose a method that effectively uses one or multiple generated images to guide the generation of other images or video frames. Through extensive experiments, our method has proven useful for flexible consistency control in image generation and has improved temporal consistency in video generation. In particular, we show applications in consistent and diverse image generation, feature blending from multiple images, and enhancement of video temporal consistency. Moreover, our approach is versatile on network architecture as it applies not only to UNet-based models but also to transformer-based diffusion models like DiT .

Looking ahead, several promising avenues for further research emerge from this study. Firstly, our experiments have not yet explored the use of attention masks; investigating their potential for precise control in image generation presents a compelling opportunity for future work. Another exciting prospect involves enhancing our method to accept clean reference images as input, similar to the IP-Adapter and other image personalization techniques. Achieving this capability would represent a significant advancement, particularly if coupled with an optimal image inversion method.