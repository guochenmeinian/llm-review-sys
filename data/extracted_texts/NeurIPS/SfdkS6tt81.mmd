# An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization

Marco Rando

MaLGa-DIBRIS, University of Genova, IT (marco.rando@edu.unige.it, lorenzo.rosasco@unige.it).

Cesare Molinari

MaLGa - DIMA, University of Genova, Italy (molinari@dima.unige.it, silvia.villa@unige.it).

Silvia Villa

Istituto Italiano di Tecnologia, Genova, Italy and CBMM - MIT, Cambridge, MA, USA

Lorenzo Rosasco

Istituto Italiano di Tecnologia, Genova, Italy and CBMM - MIT, Cambridge, MA, USA

###### Abstract

Finite-difference methods are a class of algorithms designed to solve black-box optimization problems by approximating a gradient of the target function on a set of directions. In black-box optimization, the non-smooth setting is particularly relevant since, in practice, differentiability and smoothness assumptions cannot be verified. To cope with nonsmoothness, several authors use a smooth approximation of the target function and show that finite difference methods approximate its gradient. Recently, it has been proved that imposing a structure in the directions allows improving performance. However, only the smooth setting was considered. To close this gap, we introduce and analyze O-ZD, the first structured finite-difference algorithm for non-smooth black-box optimization. Our method exploits a smooth approximation of the target function and we prove that it approximates its gradient on a subset of random _orthogonal_ directions. We analyze the convergence of O-ZD under different assumptions. For non-smooth convex functions, we obtain the optimal complexity. In the non-smooth non-convex setting, we characterize the number of iterations needed to bound the expected norm of the smoothed gradient. For smooth functions, our analysis recovers existing results for structured zeroth-order methods for the convex case and extends them to the non-convex setting. We conclude with numerical simulations where assumptions are satisfied, observing that our algorithm has very good practical performances.

## 1 Introduction

Black-box optimization problems are a class of problems for which only values of the target function are available and no first-order information is provided. Typically, these problems arise when the evaluation of the objective function is based on a simulation and no analytical form of the gradient is accessible or its explicit calculation is too expensive .

To face these problems, different methods that do not require first-order information have been proposed - see for instance  and references therein. These techniques are called derivative-free methods and a wide class of these is the class of finite-difference algorithms . These iterative procedures mimic first-order optimization strategies by replacing the gradient of the objective function with an approximation built through finite differences in random directions.

Two types of finite-difference methods can be identified: unstructured and structured ones, depending on the way in which the random directions are generated. In the former, directions are sampled i.i.d. from some distribution  while in the latter, directions have to satisfy some structural constraints, e.g. orthogonality . Several authors  theoretically and empiricallyobserved that imposing orthogonality among the directions provides better performance than using unstructured directions. Intuitively, imposing orthogonality allows us to avoid cases in which the gradient approximation is built using similar or redundant directions.

Notably, methods based on structured finite differences have been analyzed only for smooth and convex (or specific non-convex) target functions. This represents a strong limitation, since smoothness and convexity are in practice hardly verifiable, due to the nature of black-box optimization problems. The aim of this paper is the analysis of structured finite difference algorithms dropping smoothness and convexity assumptions.

For unstructured finite-difference methods, a common way to face nonsmoothness consists in introducing a smooth approximation of the target function, also known as "smoothing" [6; 16] and using it as a surrogate of the target. Although the gradient of the smoothing is not computable, different authors showed that for certain types of smoothing the unstructured finite-difference approximation provides an unbiased estimation of such a gradient - see, for example, [39; 46; 18; 22; 21; 24]. This key observation allows to prove that unstructured finite-difference methods approximate a solution in different nonsmooth settings [17; 39; 46; 18; 22; 34].

For structured finite-difference methods, the analysis in the nonsmooth setting is not available. A key step which is missing is the proof of the fact that the surrogate of the gradient built with structured directions is an estimation of the gradient of a suitable smoothing.

In this paper, we close the gap, and introduce O-ZD, a structured finite-difference algorithm in the non-smooth setting. The algorithm builds an approximation of the gradient of a smoothed target using a set of \( d\) orthogonal directions. We analyze the procedure proving that our finite-difference surrogate is an unbiased estimation of the gradient of a smoothing. We provide convergence rates for non-smooth convex functions with optimal dependence on the dimension  and rates for the non-smooth non-convex setting (for which lower bounds are not known ). Moreover, for non-smooth convex functions we provide the first proof of convergence of the iterates for structured finite differences in this setting. For smooth convex functions, we recover the standard results for structured zeroth-order methods [32; 42]. We conclude with numerical illustrations. To the best of our knowledge, this is the first work on nonsmooth finite-difference method with structured directions.

The paper is organized as follows. In Section 2, we introduce the problem and the algorithm. In Section 3, we state and discuss the main results. In Section 4 we provide some experiments and in 5 some final remarks.

## 2 Problem Setting & Algorithm

Given a function \(f:^{d}\) and assuming that \(f\) has at least a minimizer in \(^{d}\), we consider the problem to find

\[x^{*}*{arg\,min}_{x^{d}}f(x).\] (1)

In particular, we consider the non-smooth setting where \(f\) might be non-differentiable. To solve problem (1), we propose a zeroth-order algorithm, namely an iterative procedure that uses only function values. At every iteration \(k\), a first-order information of \(f\) is approximated with finite-differences using a set of \( d\) random orthogonal directions \((p_{k}^{(i)})_{i=1}^{}\). Such orthogonal directions are represented as rotations (and reflections) of the first \(\) vectors of the canonical basis \((e_{i})_{i=1}^{}\). We set \(p_{k}^{(i)}=G_{k}e_{i}\), where \(G_{k}\) belongs to the orthogonal group defined as

\[O(d):=\{G^{d d}\,|\, G 0\,\,G^{-1}=G^{}\}.\]

Methods for generating orthogonal matrices are discussed in Appendix D. Given \(G O(d)\), \(0< d\) and \(h>0\), we consider the following central finite-difference surrogate of the gradient information

\[g_{(G,h)}(x)=_{i=1}^{})-f(x-hGe_{i})}{ 2h}Ge_{i}.\] (2)

Then, we introduce the following algorithm.

Starting from an initial guess \(x_{0}^{d}\), at every iteration \(k\), the algorithm samples an orthogonal matrix \(G_{k}\) i.i.d. from the orthogonal group \(O(d)\) and computes a surrogate \(g_{(G_{k},h_{k})}\) of the gradient at the current iterate \(x_{k}\). Then, it computes \(x_{k+1}\) by moving in the opposite direction of \(g_{(G_{k},h_{k})}(x_{k})\). This approach belongs to the class of _structured_ finite-difference methods, where a bunch of orthogonal directions is used to approximate a gradient of the target function . Such algorithms have been proposed and analyzed only for smooth functions. To cope with this limitation, and extend the analysis to the nonsmooth setting, we exploit a smoothing technique. For a fixed a probability measure \(\) on \(^{d}\) and a positive parameter \(h>0\) called _smoothing parameter_, we define the following _smooth_ surrogate of \(f\)

\[f_{h,}(x):= f(x+hu)\,d(u).\] (3)

As shown in , \(f_{h,}\) is differentiable even when \(f\) is not. In the literature, different authors have used this strategy to face non-smooth zeroth-order optimization with random finite-difference methods  fixing specific smoothing distribution, but no one applied and analyze it for structured methods.

In this work, \(\) is the uniform distribution over the \(_{2}\) unit ball \(^{d}\), defining the smooth surrogate

\[f_{h}(x)=(^{d})}_{^{d}}f(x+hu)\,du,\] (4)

where \((^{d})\) denotes the volume of \(^{d}\). One of our main contributions is the following Lemma which shows that the surrogate proposed in (2) is an unbiased estimator of the gradient of the smoothing in (4).

**Lemma 1** (Smoothing Lemma).: _Given a probability space \((,,)\), let \(G: O(d)\) be a random variable where \(O(d)\) is the orthogonal group endowed with the Borel \(\)-algebra. Assume that the probability distribution of \(G\) is the (normalized) Haar measure. Let \(h>0\) and let \(g\) be the gradient surrogate defined in eq. (2). Then,_

\[( x^{d})_{G}[g_{(G,h)}(x)]= f_{h}(x),\]

_where \(f_{h}\) is the smooth approximation of the target function \(f\) defined in eq. (4)._

The proof of Lemma 1 is provided in Appendix A.1.

**Remark 1**.: _Note that Lemma 1 holds also using \(g^{F}_{(G,h)}\) or \(g^{S}_{(G,h)}\) defined as_

\[g^{F}_{(G,h)}(x):=_{i=1}^{})-f(x)}{h} Ge_{i} g^{S}_{(G,h)}(x):=_{i=1}^{} )}{h}Ge_{i},\]

_since \(_{G}[g_{(G,h)}(x)]=_{G}[g^{F}_{(G,h)}(x)]=_{G}[ g^{S}_{(G,h)}(x)]\). Despite these two estimators being computationally cheaper than the proposed one, we use central finite differences since they allow us to derive a better bound for \(_{G}[\|g_{(G,h)}(x)\|^{2}]\) as observed in  for the case \(=1\)._

Thanks to Lemma 1, we can interpret each step of Algorithm 1 as a Stochastic Gradient Descent (SGD) on the smoothed function \(f_{h_{k}}\). But the analysis of the proposed algorithm does not follow from the SGD one for two reasons. First, the smoothing parameter \(h_{k}\) changes along the iterations; second (and more importantly), the set of minimizers of \(f_{h}\) and \(f\) are different in general. However, we will take advantage of the fact that \(f_{h}\) can be seen as an approximation of \(f\). The relationship between \(f\) and its approximation \(f_{h}\) depends on the properties of \(f\) - see Proposition 1 in Appendix A.

Our main contributions are the theoretical and numerical analysis of Algorithm 1 under different choices of the free parameters \(_{k},h_{k}\), namely the stepsize and the smoothing parameter. To the best of our knowledge, Algorithm 1 is the first structured zeroth-order method for non-smooth optimization.

### Related Work

In practice, the advantage of the use of structured directions in finite-difference methods has been observed in several applications  and motivated their study. In , the authors theoretically and empirically observed that structured directions provide a better approximation of the gradient with respect to unstructured (Gaussian and spherical) ones. Next, we review the most related works, showing the differences with our algorithm.

Unstructured Finite-differences.Most of existing works focused on the theoretical analysis of methods using a single direction to approximate the gradient - see e.g. [39; 46; 22; 45; 18; 24; 34; 17; 10]. The main results existing so far analyze the convergence of the function values in expectation. They provide convergence rates in terms of the number of function evaluations and explicitly characterize the dependence on the dimension of the ambient space.

**Smooth setting:** In [39; 17], a finite difference algorithm with a single direction is analyzed. Rates on the expected function values are shown. In  the dependence on the dimension in the rates is not optimal. In , both single and multiple direction cases are analyzed and lower bounds are derived. However, only the convex setting is considered. In , both convex and non-convex settings are analyzed. They obtain optimal dependence on dimension in the complexity for convex functions. However, only the single-direction case is considered and only the smooth setting is analyzed. Comparing the result with our rates, Algorithm 1 achieves the same dependence on the dimension in the complexity taking \(\) as a fraction of \(d\). Note that, by parallelizing the computation of the function evaluations, we can get a better result.

**Non-smooth setting:** In [39; 17] the non-smooth setting is also analyzed. However, only the single direction case has been considered and both algorithms do not achieve the lower bound. More precisely, for convex functions, a complexity of \((d^{2}^{-2})\) is achieved by  and \((d(d)^{-2})\) by  while our algorithm gets the optimal dependence. Moreover, note that in  the strategy adopted (also called "double smoothing") requires tuning one more sequence of parameters, which is a challenging problem in practice, and only the convex setting is considered. In , also the non-convex setting is analyzed by bounding the expected norm of the smoothed gradient. However, they obtain a complexity of \((d^{3}^{-2}h^{-1})\) while our algorithm gets a better dependence on the dimension. In , the optimal complexity is obtained for convex functions. However, the author does not analyze the non-convex setting. Moreover, note that, despite the complexity in terms of function evaluations being the same, our algorithm gets a better complexity in terms of the number of iterations since it uses multiple directions (and this is an advantage if we can parallelize the function evaluations). Furthermore, note that the method proposed in  can be seen as the special case of Algorithm 1 with \(=1\). In  the single direction case is analyzed only in the non-convex setting. The dependence on the dimension of the complexity in the number of function evaluations achieved matches our result in this setting (again, in the number of iterations our method obtains a better dependence).

Structured Finite-difference.In [32; 42], authors analyze structured finite differences in both deterministic and stochastic settings. However, only the smooth convex setting is considered. In  orthogonal matrices are used to build directions but no analysis is provided. In , finite-difference with coordinate directions is analyzed. At every iteration, \(d+1\) function evaluations are performed to compute the estimator and only the smooth setting is considered.

## 3 Main Results

In this section, we analyze Algorithm 1 considering both non-smooth and smooth problems. We present the rates obtained by our method for convex and non-convex settings and compare them with those obtained by state-of-the-art methods. Proofs are provided in Appendix B. In the following, we call _complexity in the number of iterations / function evaluations_, respectively, the number of iterations / function evaluations required to achieve an accuracy \((0,1)\).

### Non-smooth Convex Setting

In this section, we provide the main results for non-smooth convex functions. In particular, we will assume that the target function is convex and satisfy the following hypothesis.

**Assumption 1** (\(L_{0}\)-Lipschitz continuous).: _The function \(f\) is \(L_{0}\)-Lipschitz continuous; i.e., for some \(L_{0}>0\),_

\[( x,y^{d})|f(x)-f(y)| L_{0}\|x-y\|.\]

Note that this assumption implies that also \(f_{h}\) is \(L_{0}\)-Lipschitz continuous - see Proposition 1. Moreover, to analyze the algorithm, we will consider the following parameter setting.

**Assumption 2** (Zeroth-order non-smooth convergence conditions).: _The step-size sequence \(_{k}\) and the sequence of smoothing parameters \(h_{k}\) satisfy the following conditions:_

\[_{k}^{1},_{k}^{2}^{1} _{k}h_{k}^{1}.\]

The assumption above is required to guarantee convergence to a solution. In particular, the first two conditions are common for subgradient method and stochastic gradient descent, while the third condition was already used in structured zeroth-order methods  and links the decay of the smoothing parameter with the stepsize's one. An example of \(_{k},h_{k}\) that satisfy Assumption 2 is \(_{k}=k^{-}\) and \(h_{k}=k^{-}\) with \((1/2,1)\) and \(\) s.t. \(+>1\).

We state now the main theorem for non-smooth convex functions.

**Theorem 1** (Non-smooth convex).: _Under Assumption 1, assume that \(f\) is convex and let \((x_{k})_{k}\) be a sequence generated by Algorithm 1. For every \(k\), denote \(A_{k}=_{i=0}^{k}_{i}\) and set \(_{k}:=_{i=0}^{k}_{i}x_{i}/A_{k}\). Then_

\[[f(_{k})- f] S_{k}/A_{k} S_{k} :=-x^{*}\|^{2}}{2}+c^{2}}{}_{i=0}^{k}_ {i}^{2}+L_{0}_{i=0}^{k}_{i}h_{i},\]

_where \(c>0\) is a constant independent of the dimension and \(x^{*}\) is any solution in \( f\). Moreover, under Assumption 2, we have_

\[_{k+}f(x_{k})= f,\]

_and that there exists a random variable \(x^{*}\) taking values in \( f\) s.t. \(x_{k} x^{*}\) a.s._

In the next corollary, we derive explicit rates for specific choices of the parameters.

**Corollary 1**.: _Under the assumptions of Theorem 1, let \(x^{*} f\). Then, the following hold:_

* _Let_ \((1/2,1)\) _and_ \(\) _such that_ \(+>1\)_. For every_ \(k\)_, let_ \(_{k}=(k+1)^{-}\) _and_ \(h_{k}=h(k+1)^{-}\) _with_ \(>0\) _and_ \(h>0\)_. Then_ \[[f(_{k})- f]}+o },\] _for some constant_ \(C\) _provided in the proof._
* _For every_ \(k\)_, let_ \(_{k}=\) _and_ \(h_{k}=h\) _with_ \(,h>0\)_. Then_ \[[f(_{k})- f]-x^{*}\|^{2}}{2 k}+ ^{2}}{}+L_{0}h,\] _where_ \(c\) _is a constant independent of the dimension._
* _Fix an accuracy_ \((0,1)\) _and let_ \(K 8(cL_{0}^{2}\|x_{0}-x^{*}\|^{2})(d/)^{-2}\)_. Set_ \(_{k}=}-x^{*}\|}{}}\)_, and_ \(h_{k}=h/(2L_{0})\) _for every_ \(k K\)_. Then_ \[[f(_{K})- f]\] _and the complexity in terms of number of iterations is_ \(((d/)^{-2})\)_._

Discussion.The bound in Theorem 1 depends on the initialization and on an additional quantity that can be interpreted as an approximation error. The latter is composed of two parts. The first one is generated by the approximation of the gradient of the smoothed function; while the second, involving \(h_{k}\), is generated by the smoothing. Since the rate depends on \(1/_{i=0}^{k}_{i}\), we would like to choose the stepsize as large as possible. However, to get convergence, we need to make the approximation errorsvanish sufficiently fast. To guarantee this, as we can observe from Theorem 1, we need to impose some conditions on the stepsize \(_{k}\) and on the smoothing parameter \(h_{k}\) (i.e. Assumption 2), that will slow down the decay of the first term. In Corollary 1, we provide two choices of parameters: the first one satisfies Assumption 2 and ensures convergence; the second one corresponds to constant stepsize and smoothing parameter. For the first choice, we recover the rate of the subgradient method in terms of \(k\). In particular, for \(\) approaching \(1/2\), the convergence rate is arbitrarily close to \(O(k^{-1/2})\) and is similar to the one derived in (17, Theorem 2). The dependence on the dimension depends on the choice of the constant \(\). The optimal dependence is obtained with the choice \(=\). Indeed, in that case, the complexity in the number of iterations is of the order \(((d/)^{-2})\), which is better than the one achieved by (17, Theorem 2) and (39). Note that also (46, Corollary 1) and (22, Theorem 2.4) obtain a worse complexity in terms of the number of iterations (since they use a single direction), but the same complexity in the number of function evaluations. Clearly, since multiple directions are used, a single iteration of O-ZD will be more expensive than one iteration of (46, 22). However, our algorithm is more efficient if the \(\) function evaluations required at each iteration can be parallelized. On the more theoretical side, we observe that the advantage of multiple orthogonal directions is in the tighter bounds for the variance of the estimator, namely for \([\|g_{(G_{k},h_{k})}(x_{k})\|^{2}]\) - see (46, Lemma 5) and Lemma 4.

### Non-smooth Non-convex Setting

To analyze the non-convex setting, following (39), we provide a bound on the averaged expected square norm of the gradient of the smoothed target. In particular, we use the following notation:

\[_{k}^{(h)}:=(_{i=0}^{k}_{i}\,[\| f_{h}(x_{i })\|^{2}])/A_{k},\ \ A_{k}:=_{i=0}^{k}_{i}.\]

Next, we state the main theorem for the non-convex non-smooth setting.

**Theorem 2** (Non-smooth non-convex).: _Under Assumption 1, let \((x_{k})_{k}\) be a sequence generated by Algorithm 1 with, for every \(k\), \(h_{k}=h\) for some \(h>0\). Then_

\[_{k}^{(h)} S_{k}/A_{k} S_{k}:=f_{h}(x_{0})- f +c^{3}d}{}_{i=0}^{k}^{2}}{h}.\]

In the next corollary, we derive the rates for specific choices of the parameters.

**Corollary 2**.: _Under the assumptions of Theorem 2, the following statements hold._

* _If_ \(_{k}=(k+1)^{-}\) _with_ \(>0\) _and_ \((1/2,1)\)_, then_ \[_{k}^{(h)} C(x_{0})- f}{ k^{1-}}+o },\] _where_ \(C\) _is a constant independent of the dimension._
* _If_ \(_{k}=\) _with_ \(>0\) _for every_ \(k\)_, then_ \[_{k}^{(h)}(x_{0})- f}{ k}+^{3}d}{ h},\] _where_ \(c\) _is a constant independent of the dimension._
* _Let_ \((0,1)\)_, let_ \(K 4(f_{h}(x_{0})- f)cL_{0}^{3}d^{-2}/( h)\) _and choose_ \(=(x_{0})- f) h}{KcL_{0}^{3}d}}\)_. Then we have that_ \(_{K}^{(h)}\)_. Thus, the number of function evaluations required to get a precision_ \(_{k}^{h}\) _is of the order_ \((dh^{-1}^{-2})\)_._

Relying on the results in (34), we show that this is related to a precise notion of approximate stationarity. To do so, we need to introduce a definition of subdifferential which is suitable to this setting. As shown in (34) the Clarke subdifferential is not the right notion, and the approximate Goldstein subdifferential should be used instead.

**Definition 1** (Goldstein subdifferential and stationary point).: _Under Assumption 1, let \(x^{d}\) and \(h>0\). The \(h\)-Goldstein subdifferential of \(f\) at \(x\) is \(_{h}f(x):=(_{y^{d}_{k}(x)} f(y))\) where \( f\) is the Clarke subdifferential  and \(^{d}_{h}(x)\) is the ball centered in \(x\) with radius \(h\). For \((0,1)\), a point \(x^{d}\) is a \((h,)\)-Goldstein stationary point for the function \(f\) if \(\{\|g\|\,|\,g_{h}f(x)\}\)._

**Corollary 3**.: _Under the same assumptions of Theorem 2, fix \(K\) and let \(I\) be a random variable taking values in \(\{0,,K-1\}\) such that, for all \(i\), \([I=i]=_{i}/A_{K-1}\). Let also \(S_{k}\) be defined as in Theorem 2. Then_

\[_{I}[\{\|\|^{2}\,:\, f_{h}(x_{I})\} ] S_{K}/A_{K}.\]

_In the setting of Corollary 2 (iii), we have also that \(_{I}[\{\|\|^{2}\,:\, f_{h}(x_{I})\} ]\)._

Discussion.In Theorem 2, we fix the smoothing of the target, i.e. we consider \(h_{k}\) constant, and we analyze the non-smooth non-convex setting providing a rate on the expected norm of the smoothed gradient. The resulting bound is composed of two parts. The first part is very natural, and due to the functional value at the initialization. The second part is the approximation error. Recall that Assumption 1 holds, and therefore \(f_{h} f+L_{0}h\) due to Proposition 1. This suggests taking \(h\) as small as possible in order to reduce the gap between \(f_{h}\) and \(f\). However, taking \(h\) too small would make the approximation error very big. In our analysis, we consider the case with \(h\) constant. Moreover, as for the convex case, the speed of the rate depends on \(A_{k}\) and so we would like to take the stepsize as large as possible. But to control the approximation error, we need to assume \(_{k}^{2}^{1}\). In Corollary 2, we consider two choices of stepsize. The first choice satisfies the property of \(_{k}^{2}^{1}\), while the second one analyzes the case of constant step-size. Comparing our rate to the one in  we see that we obtain a better dependence on the dimension in the complexity, both in terms of iterations and function evaluations. Our results match the one of [34, Theorem 3.2] in terms of rate and in terms of function evaluations. We get a better dependence on the dimension in the number of iterations. Note again that, despite the complexity in terms of the number of function evaluations being the same, the possibility of parallelization for the function evaluations yields a better result for our method. As for the convex setting, we have a tighter upper-bound on the variance of the estimator of the smoothed gradient with respect to the dimension - see [34, Lemma D.1]. Goldstein stationarity has been used to assess the approximate stationarity for first-order methods as well, see . The latter work shows that a cutting plane algorithm achieves a rate of \((d^{-3})\) for Lipschitz functions.

### Smooth Convex setting

We consider now the smooth setting, i.e. we assume that the target function satisfies the following hypothesis.

**Assumption 3** (\(L_{1}\)-Smooth).: _The function \(f\) is \(L_{1}\)-smooth; i.e. the function \(f\) is differentiable and, for some \(L_{1}>0\),_

\[( x,y^{d})\| f(x)- f(y)\| L_{1}\|x- y\|.\]

This is the standard assumption for analyzing first-order methods and has been used in many other works in the literature for zeroth-order algorithms - see e.g. . As shown in previous works, if \(f\) satisfies Assumption 3 then also \(f_{h}\) satisfies it - see Proposition 1. We will consider also the following assumptions on the stepsize and the smoothing in order to guarantee convergence.

**Assumption 4** (Smooth zeroth-order convergence conditions).: _The stepsize sequence \((_{k})_{k}\) and the smoothing sequence \((h_{k})_{k}\) satisfy the following conditions:_

\[_{k}^{1}_{k}h_{k}^{1}.\]

_Moreover, \(_{k}</dL_{1}\) for every \(k\)._

Note that this is a weaker version of Assumption 2. Next, we state the main theorem for convex smooth functions.

**Theorem 3** (Smooth convex).: _Under Assumptions 3 and 4, let \((x_{k})_{k}\) be a sequence generated by Algorithm 1 and \(x^{*} f\). For every \(k\), set \(A_{k}=_{i=0}^{k}_{i}\) and \(_{k}=_{i=0}^{k}_{i}x_{i}/A_{k}\). Then, for every \(k\),_

\[[f(_{k})- f]}{A_{k}}  D_{k}:=}{2}S_{k}+ _{i=0}^{k}_{i}}+_{j=0}^{i}_{j} ,\]_where \(S_{k}:=\|x_{0}-x^{*}\|^{2}+_{i=0}^{k}^{2}d^{2}}{2}_{i}^{2 }h_{i}^{2},\;_{k}:=L_{1}d_{k}h_{k},\;:=(}-)\)._

**Corollary 4**.: _Under the same Assumptions of Theorem 3, the following hold._

1. _If for every_ \(k\) _we set_ \(_{k}=>0\) _and_ \(h_{k}=h(k+1)^{-}\) _for_ \(h>0\) _and_ \(>1\)_, then_ \[[f(_{k})- f],\] _where_ \(C\) _is a constant provided in the proof. Moreover, if_ \(</(2dL_{1})\)_,_ \(_{k}f(x_{k})= f\) _a.s. and there exists a random variable_ \(\) _taking values in_ \( f\) _s.t._ \(x_{k}\) _a.s._
2. _If for every_ \(k\) _we set_ \(_{k}=>0\) _and_ \(0<h_{k} h\)_, then_ \[[f(_{k})- f]}{k}+C_{2} h+C_{3} ^{2}h^{2}+C_{4}^{2}h^{2}k,\] _where_ \(C_{1},C_{2},C_{3}\) _and_ \(C_{4}\) _are non-negative constants._

Discussion.As in the previous cases, the bound in Theorem 3 is composed by two terms: the error due to the initialization and the one due to the approximation. An important difference with the results in the non-smooth setting is that every term in the approximation error is decreasing with respect to the smoothing parameter \(h_{k}\). This allows obtaining convergence also with the constant step-size scheme, taking \(h_{k}^{1}\). In Corollary 4 (i), we recover the result of [32, Theorem 5.4] with a specific choice of parameters \(,h\) (up to constants). The complexity depends on the choice of \(\). Note that by Assumption 4, \(</(L_{1}d)\) thus the dependence on the dimension in the rate will be at least \(d/\). In particular, taking \(=/(2dL_{1})\), we obtain the optimal complexity of \((d^{-1})\) in terms of function evaluations. This result has a better dependence on the dimension than . In Corollary 4 (ii), the dependence on the dimension in the complexity depends on the choice of \(\) and \(h\). Moreover, the rate obtained is equal (up to constants) to the rate obtained in  in the same setting, i.e. \((1/k)\) (in which we hide the dependence on \(d\) and \(\)). As for , for the first setting we can prove the almost sure convergence of the iterates.

### Smooth Non-Convex setting

To analyze the smooth non-convex setting, we introduce the following notation:

\[( k^{d}) A_{k}:=_{i=0}^{k}_{i}, _{k}:=(_{i=0}^{k}_{i}\,[\| f(x_{i})\|^{2}] )/A_{k}.\]

Note that, in comparison with the quantity defined in Section 3.2, here \(_{k}\) is related to the exact objective function \(f\) and not to its smoothed version \(f_{h}\). Next, we state the main result for smooth non-convex functions.

**Theorem 4** (Smooth non-convex).: _Suppose that Assumption 3 holds and assume that, for every \(k\), \(_{k}</(2dL_{1})\). Let \((x_{k})_{k}\) be a sequence generated by Algorithm 1. Then_

\[_{k}}f(x_{0})- f+^{2}d^{2} }{8}_{i=0}^{k}_{i}h_{i}^{2}+^{3}d^{2}}{4}_{i=0}^{ k}_{i}^{2}h_{i}^{2},:=-d}{ }.\]

**Corollary 5**.: _Under the assumptions of Theorem 4, the following hold._

1. _If_ \(_{k}=\) _and_ \(h_{k}=hk^{-}\) _with_ \(h>0\) _and_ \(>1\)_, then_ \[_{k}[)- f}{}+d^{2}h^{2}} {}+ h^{2}d^{2}}{}],\] _where_ \(C_{1}\) _and_ \(C_{2}\) _are constants provided in the proof._
2. _If_ \(_{k}=\) _and_ \(h_{k}=h>0\)_, then_ \[_{k})- f}{ k}+d^{2}h^{2}}{ }+ h^{2}d^{2}}{},\] _where_ \(C_{1}\) _and_ \(C_{2}\) _are constants provided in the proof._Discussion.As for the convex case, every term in the approximation error depends on the smoothing parameter \(h_{k}\). In Corollary 5 (i), we take constant step-size and \(h_{k}^{1}\). With this choice of parameters, we get a rate of \((1/k)\) which matches with the result obtained by . The dependence on the dimension depends on the choice of \(\) and \(h\). Note that \(</(2dL_{1})\), thus taking \(h=(1/d)\), in the rate we get a dependence on the dimension of \(d/\). Taking for instance \(=/(3dL_{1})\) and \(h=(1/d)\), we get a complexity of \((d^{-1})\) in terms of function evaluations.

## 4 Numerical Results

In this section, we provide some numerical experiments to assess the performances of our algorithm. We consider two target functions: a convex smooth one and a convex non-smooth one. Details on target functions and parameters of the algorithms are reported in Appendix C. To report our findings, we run the experiments \(10\) times and provide the mean and standard deviation of the results.

How to choose the number of directions?In these experiments, we set a fixed budget of \(4000\) function evaluations and we consider \(d=50\). We investigate how the performance of Algorithm 1 changes as the value of \(\) increases. In Figure 1, we observe the mean sequence \(f(x_{k})-f(x^{*})\) after each function evaluation. If \(>1\), then the target function values are repeated \(2\) times, since we need to perform \(2\) function evaluations to do one iteration. For a sufficiently large budget, increasing the number of directions \(\) leads to better results compared to using a single direction in both smooth and non-smooth settings.

Comparison with finite-difference methods.Now, we compare Algorithm 1 with other finite-difference methods. More precisely, we consider finite differences with single (and multiple) Gaussian (and spherical) directions. The budget of function evaluations is \(1000\) and the ambient dimension is \(d=10\). For multiple direction methods, we fix the number of directions \(=d\). Further experiments are provided in Appendix F.

Figure 1: From left to right, function values per function evaluation in optimizing smooth and non-smooth target functions with different numbers of directions.

Figure 2: From left to right, function values per function evaluation in optimizing smooth and non-smooth convex functions with different finite-difference algorithms.

In Figure 2, we plot the sequence \(f(x_{k})-f(x^{*})\) with respect to the number of function evaluations. While in terms of rates and complexity the different algorithms are the same, Algorithm 1 shows better performances than random directions approaches, and we believe this is due to the use of structured (i.e. orthogonal) directions. Indeed, orthogonal directions yield a better approximation of first-order information with respect to other methods. The practical advantages of structured directions were already observed in  and these experiments confirm that the good practical behavior holds even in the nonsmooth setting.

## 5 Conclusion

We introduced and analyzed O-ZD a zeroth-order algorithm for non-smooth zeroth-order optimization. We analyzed the algorithm and derived rates for non-smooth and smooth functions. This work opens different research directions. An interesting one would be the introduction of a learning procedure for the orthogonal directions. Such an approach could have significant practical applications.