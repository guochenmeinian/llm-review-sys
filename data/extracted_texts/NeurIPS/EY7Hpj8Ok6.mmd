# Context-lumpable stochastic bandits

Chung-Wei Lee

University of Southern California

leechung@usc.edu

&Qinghua Liu

Princeton University

qinghual@princeton.edu

&Yasin Abbasi-Yadkori

Google DeepMind

yadkori@google.com

&Chi Jin

Princeton University

chij@princeton.edu

&Tor Lattimore

Google DeepMind

lattimore@google.com

&Csaba Szepesvari

Google DeepMind and University of Alberta

szepesva@ualberta.ca

 most works were done when interning at DeepMind.

###### Abstract

We consider a contextual bandit problem with \(S\) contexts and \(K\) actions. In each round \(t=1,2,\) the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into \(r\{S,K\}\) groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an \(\)-optimal policy after using at most \((r(S+K)/^{2})\) samples with high probability and provide a matching \((r(S+K)/^{2})\) lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time \(T\) is bounded by \(((S+K)T})\). To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and \(((r)(S+K)T})\) minimax regret in the online setting for this problem. We also show our algorithms can be applied to more general low-rank bandits and get improved regret bounds in some scenarios.

## 1 Introduction

Consider a recommendation platform that interacts with a finite set of users in an online fashion. Users arrive at the platform and receive a recommendation. If they engage with the recommendation (e.g., they "click") then the platform receives a reward, otherwise no reward is obtained. Assume that the users can be partitioned into a small number of groups such that users in the same group have the same preferences.

_We ask whether there exist algorithms that can take advantage of the lumpability of users into a few groups, even when the identity of the group a user belongs to is unknown and only learnable because they share preferences with other users in the group._

A slightly more general version of this problem can be formalized as follows: Viewing users as "contexts" and recommendations as "actions" (or arms), assume that there are \(S\) contexts and \(K\) actions. In round \(t=1,2,\) the learner first receives a context \(i_{t}\), sampled from an unknowndistribution on the set \([S]:=\{1,,S\}\) of possible contexts. The learner then chooses an action \(j_{t}[K]:=\{1,,K\}\) and observes a reward \(y_{t}=A(i_{t},j_{t})+_{t}\), where given the past, \(_{t}\) has a subgaussian tail (precise definitions are postponed to Section 2) and \(A:[S][K]\) is an unknown function of mean rewards (\(\) denotes the set of reals). We consider two settings when the goal of the learner is either to identify a near-optimal policy \(:[S][K]\), or to keep its regret small. Policy \(\) is called \(\)-optimal if

\[[A(i_{1},(i_{1}))]_{^{}}[A(i_{1},^{ }(i_{1}))]-,\] (1)

while the regret of the learner for a horizon of \(T\) is

\[_{T}=[_{t=1}^{T}_{j[K]}A(i_{t},j)-_{t= 1}^{T}A(i_{t},j_{t})].\] (2)

The expectations are taken with respect to the randomness of both the learner and environment, including contexts and rewards. It is well known (e.g., Lattimore and Szepesvari (2020)) that there are algorithms such that an \(\)-optimal policy will be discovered after

\[(})\] (3)

interactions, and there are also algorithms for which the regret satisfies

\[_{T}=()\,.\] (4)

Here, the notation \(()\) hides polylogarithmic factors of the variables involved. We say that the stochastic, finite, contextual bandit problem specified by \(A\) is _\(r\)-lumpable_ (or, in short, the bandit problem is _context-lumpable_) if there is a partitioning of \([S]\) into \(r\{S,K\}\) groups such that \(A(i,)=A(i^{},)\) holds whenever \(i,i^{}[S]\) belong to the same group. It is not hard to see that any algorithm needs at least \((r(S+K)/^{2})\) interactions to discover an \(\)-optimal policy (Theorem 3). Indeed, if we view \(A\) as an \(S K\) matrix, the lumpability condition states that \(A=UV\) where \(U\) is an \(S r\) binary matrix where each row has a single nonzero element, and \(V\) is an \(r K\) matrix, which gives the unique mean rewards given the group indices. Hence, crude parameter counting suggests that there are \(r(S+K)\) parameters to be learned.

_The question is whether \(SK\) in Eq._ (3) _and Eq._ (4) _can be replaced with \((S+K)(r)\) without knowing the grouping of the contexts._

More generally, we can ask the same questions for contextual bandits with the _low-rank_ structure, where the matrix \(A\) has rank \(r\). Equivalently, the low-rank condition means that we have the same decomposition \(A=UV\) as above but no more constraints on \(U\). In the example of recommendation platforms, this assumption is more versatile as the users are modeled as _mixtures_ of \(r\) preference types instead of belonging to one type only.

### Related works

Our problem can be seen as an instance of contextual bandit problems introduced by Auer et al. (2002). For a good summary of the history of the contextual bandit problem, the reader is advised to consult the article by Tewari and Murphy (2017). Further review of prior works can also be found in the books of Slivkins (2019), Lattimore and Szepesvari (2020). Another way to approach context-lumpable stochastic bandits is to model them as stochastic linear bandits with changing action sets Auer (2002), Chu et al. (2011), Abbasi-Yadkori et al. (2011). However, lumpablility does not give improvements on the regret bound when running the standard algorithms in these settings (EXP4 and SupLinRel, respectively). We provide more details in the appendix.

We are not the first to study the \(r\)-context-lumpable stochastic bandit problem. The earliest work is probably that of Maillard and Mannor (2014) who named this problem _latent bandits_: they consider the group identity of the context as latent, or missing information. While the paper introduces this problem, the main theoretical results are derived for the case when the reward distributions given the group information are known to the learner. Further, the results derived are instance dependent.

Altogether, while this paper makes interesting contributions, it does not help in answering our questions.2

The earlier work of Gentile et al. (2014) considered another generalization of our problem where in round \(t=1,2,\) the learner gets an action set \(\{x_{t,1},,x_{t,K_{t}}\}^{d}\) and the mean payoff of choosing action \(1 j K_{t}\) given the past and the current context \(i_{t}\) is \(x_{t,j}^{}g_{g(i_{t})}\) with \(_{1},,_{r}^{d}\) unknown. When specializing this to our setting, their result depends on the minimum separation \(_{i,j:g(i) g(i)}\|A(i,)-A(j,)\|\), which, according to the authors may be an artifact of their analysis. An extra assumption, which the authors suggest could be removed, is that the distribution of the contexts is uniform (as we shall see, removing this assumption is considerable work). Further, as the authors note, the worst-case for their algorithm is when the contexts are equipartitioned, in which case their bound reduces to the naive bound that one gets for non-lumpable problems. Li et al. (2016) considers the variation of the problem when the set of arms (and their feature vectors) are fixed and is known before learning begins and separation is defined with respect to this fixed set of arms (and hence could be larger than before). From our perspective, their theoretical results have the same weaknesses as the previous paper. Gentile et al. (2017) further generalizes these previous works to the case when the set of contexts is not fixed a priori. However, the previously mentioned weaknesses of the results remain.

Our problem is also a special case of learning in _lumpable Markov Decision Processes_. In such Markov Decision Processes (MDPs) the states can be partitioned such that states within a group of a partition behave identically, both in terms of the transitions and the rewards received (e.g., Ren and Krogh, 2002).3 We put a survey of this line of work in Appendix A.6.

In summary, although the problem is extensively studied in the literature, we are not aware of any previous results that have implications for the existence of the minimax regret bounds in terms of \(((r)(S+K)T})\), which we believe is a fundamental question in this area.

## 2 Notation and problem definition

For an positive integer \(n\), we use \([n]\) to denote the set \(\{1,,n\}\). Further, for a finite set \(U\), \((U)\) denotes the set of probability distributions over \(U\) and \((U)\) denotes the uniform distribution over \(U\). The set of real numbers is denoted by \(\).

As introduced earlier, we consider context-lumpable bandits with \(S\) contexts and \(K\) actions such that the \(S\) contexts can be lumped into \(r\) groups so that the mean payoff \(A(i,j)\) given that action \(j[K]\) is used while the context is \(i[S]\) depends only on the identity of the group that \(i\) belongs to and the action \(j\). That is, for some \(g:[S][r]\) map

\[A(i,j)=A(i^{},j)[S]$ such that $g(i)=g(i^{})$}\,.\]

Neither \(A\), nor \(g\) is known to the learner who interacts with the bandit instance in the following way: In rounds \(t=1,2,\) the learner first observes a context \(i_{t}[S]\) randomly chosen from a fixed, context distribution \(([S])\), independently of the past. The distribution \(\) is also unknown to the learner. Next, the learner chooses an action \(j_{t}[K]\) to receive a reward of \(y_{t}=A(i_{t},j_{t})+_{t}\,,\) where \(_{t}\) is \(1\)-subgaussian given the past: For any \(\),

\[[(_{t}) i_{1},j_{1},,i_{t},j_{t}]( ^{2}/2)\,.\]

As described earlier, we are interested in two problem settings. In the _PAC setting_, the learner is given a target suboptimality level \(>0\) and a target confidence \(\) The learner is then tasked to discover a policy \(:[S][K]\) that is \(\)-optimal (cf. Eq. (1)), with probability \(\). In this setting, in addition to deciding what actions to choose, the learner also needs to decide when to stop collecting data and when it stops it has to return a map \(:[S][K]\). If \(T\) is the (random) time when the learner stops, then the efficiency of the learner is characterized by \([T]\). In the _online setting_, the goal of the learner is to keep its regret (cf. Eq. (2)) low. In Section 3, we will consider the PAC setting, while the online setting will be considered in Section 4.

In the remainder of the paper, we will also need some further notation. The map \(g\) induces a partitioning \(^{}=\{(1),,(r)\}\) of \([S]\) in the natural way:

\[(b)=\{i:g(i)=b\}.\]

We call each subset \(^{}\) a _block_ and call \((b)\) block \(b\) for every \(b[r]\). We also define block reward \((b,j)=A(i,j)\) of block \(b\) for every context \(i(b)\) and arm \(j[K]\). Finally, we define the block distribution \((r)\) so that \((b)=_{i(b)}(i)\).

## 3 Near-optimal PAC Learning in Context-lumpable Bandits

In this section, we present an algorithm for PAC learning in context-lumpable bandits, and prove that its sample complexity guarantee matches the minimax rate up to a logarithmic factor.

### Special case: almost-uniform context distribution

To better illustrate the key ideas behind the algorithm design, we first consider a special case of almost-uniform context distribution. Formally, we assume \((i)[1/(8S),8/S]\) for all \(i[S]\) throughout this subsection. The pseudocode of our algorithm is provided in Algorithm 1, which consists of three main modules. Below we elaborate on each of them separately.

Data collection (Line 3-4 and Algorithm 2).The high-level goal of this step is to collect a sufficient amount of data for every block/action pair so that in later steps we have sufficient information to select a near-optimal arm for each block. One naive approach is to try every action on every context for a sufficient amount of time (e.g., \((1/^{2})\)). However, this will cause an undesired factor of \(SK\) in the sample complexity. To address this issue, note that contexts from the same block share the same reward function, which means that for every block/action pair \((b,j)[r][K]\), we only need to sufficiently try action \(j\) on a single context \(i\) from block \(b\) (i.e., \(i(b)\)). However, the block structure is unknown a priori. We circumvent this problem by assigning a random action \((i)\) to each context \(i\) for them to explore (Line 4). And after action \((i)\) has been tried on context \(i\) sufficiently many times, we update \((i)\) by reassigning context \(i\) with a new random action (Line 8-10).

However, there is still one key problem unresolved yet: how many samples to use for estimating each \(A(i,(i))\) before reassigning context \(i\) with another random action, given a fixed budget of total sample complexity. On one hand, if we only try each \((i,(i))\) pair a very few numbers of times before switching, then we can potentially explore a lot of different actions for each block but the accuracy of the reward estimate could be too low to identify a near-optimal policy. On the other hand, estimating each \(A(i,(i))\) with a huge number of samples resolves the poor accuracy issue but could potentially cause the problem of under-exploration, especially for those blocks consisting of very few contexts. In the extreme case, if a block only contains a single context, then our total budget may only afford to test a single action on that context (block).

To address this issue, we choose different levels of accuracy for different blocks adaptively depending on their block size. Specifically, contexts from larger blocks can afford to try each random action before switching for a larger number of times to achieve higher estimate accuracy because larger blocks consist of more contexts, which means that the average number of random actions in each context inside a larger block needs to try is smaller. And for smaller blocks, the case is the opposite. Finally, we remark that the above scheme of using adaptive estimate accuracy can be implemented without any prior knowledge of the block structure. We only need to iterate over different accuracy levels using an exponential schedule (Line 3), and each block will be sufficiently explored at its appropriate accuracy level. Specifically, let \(N=(1/^{2})\), and consider accuracy levels \(n[N]\). For accuracy level \(n\), we collect a dataset \(_{n}\) by adding a context-action pair after the pair is played for \(2^{n}\) timesteps (Line 9).

Screening optimal action candidates (Line 5-11).Equipped with the dataset collected from Step 1, we want to identify a subset \(\) of \([K]\) so that (i) for every block \(b[r]\), \(\) contains a near-optimal action of block \(b\), and (ii) the size of \(\) is as small as possible. We construct such \(\) in an iterative way. For each accuracy level \(n[N]\), we first compute the context-action pair \((i^{},j^{})\) with the highest reward estimate in \(_{n}\) (Line 7). Intuitively, as \((i^{},j^{})\) has been tried for \(2^{n}\) timesteps in constructing \(_{n}\), it is natural to guess that \(j^{}\) could potentially be a \((2^{-n/2})\)-optimal action for certain blocks so we add it into optimal action candidate set \(\) (Line 8). To identify the contexts from those blocks, we sample new data to estimate the reward of \(j^{}\) on each context \(i[S]\). This can be done by calling Algorithm 2 and setting the exploration set \(\) to be \(j^{}\) (Line 10). If a context \(i\) can achieve reward close enough to \(_{n}(i^{},j^{})\) at action \(j^{}\), then we peel off every \((i,j)_{n}\) (Line 11). This is because we have found \(j^{}\) as an optimal action candidate for \(i\) at this accuracy level and don't need to consider other \(j\). We repeat the above process, and add a new arm to \(\) in each iteration, until \(_{n}\) becomes empty.

Solving the simplified context-lumpable bandit (Line 12).After obtaining the optimal action candidate set \(\), we can discard all actions not in \(\) and solve a simplified context-lumpable bandit problem by replacing the original action set \([K]\) with \(\). Note that \((S||/^{2})\) samples suffice for learning an \(\)-optimal policy for this simplified problem. For example, we can directly try each action \(j\) on each context \(i[S]\) for \((1/^{2})\) times, and define \(^{}(i)_{j}(i,j)\) where \((i,j)\) is the empirical estimate of \(A(i,j)\) based on \((1/^{2})\) samples.

```
1 Let \(t\) denote the current time and initialize \(N(1/^{2})\), \(\), \( 16(rSK/)\)
2 Define \((i)[K]\) for \(i[S]\), \(L r(S+K)/^{2}\) Step 1. Data collection
3foraccuracy level \(n=1,,N\)do
4 Execute Algorithm 2 with input \(L\), \(n\), \(\), and receive \(_{n}\) and \(_{n}\)
5 Step 2. Screening optimal action candidates
6foraccuracy level \(n=1,,N\)do
7while\(_{n}\)do
8 Compute \((i^{},j^{})_{(i,j)_{n}}_{ n}(i,j)\)
9 Update optimal action candidates \(\{j^{}\}\)
10 Update \((i)\{j^{}\}\) for \(i[S]\) and \(L^{}2^{n}S\)
11 Execute Algorithm 2 with input \(L^{}\), \(n\), \(\), and reassign output to \(}_{n}\) and \(_{n}\)
12 Shrink \(_{n}\{(i,j)_{n}:\;|_{n}(i,j^{ })-_{n}(i^{},j^{})|}{2^{n}}}\}\)
13 Step 3. Solving the simplified context-lumpable bandit
14 Use \(|}{^{2}}\) samples to find \(^{}\) s.t. \(A(i,^{}(i))_{j}A(i,j)-\) for all \(i[S]\)
15Output\(^{}\) ```

**Algorithm 1** Algorithm for Almost-uniform Context Distribution

Now we present the theoretical guarantee for Algorithm 1. The proof and exact constants in the bound can be found in Appendix C.

**Theorem 1**.: _Let \((0,1)\) and assume \((i)[1/(8S),8/S]\) for all \(i[S]\). Algorithm 1 outputs an \(()\)-optimal policy within \((r(S+K)(1/)/^{2})\) samples with probability at least \(1-\)._

### Extension to general context distribution

```
1 Let \(J=(S/)\), \(L=(S/)\), \(N=524()(1+2 )^{2}}\)
2 Estimate the context distribution by sampling \(J\) contexts, and denote the estimate by \(\) Split the context set into \(L\) disjoint subsets \(\{_{l}\}_{l[L]}\) where \[_{l}\{i[S]:\;(i)(2^{-l-1},2^{ -l}]\},&l[0:L-1]\\ \{i[S]:\;(i)[0,2^{-l}]\},&l=L\]
3for\(l[L-1]\)do
4 Execute Algorithm 1 to learn policy \(_{l}\) for subset \(_{l}\) from \(N\) time steps
5 Output\(^{}\) such that \(^{}\) equals to \(_{l}\) over \(_{l}\) for \(l[0:L-1]\), and arbitrary over \(_{L}\) ```

**Algorithm 3** Algorithm for General Context Distribution

In this subsection, we show how to generalize Algorithm 1 to handle general context distributions. We present the pseudo-code in Algorithm 3. The algorithm consists of two key steps. In the first step, we use \(J=(S/)\) samples to obtain an empirical estimate of the context distribution, denoted by \(\). Then we divide the context set into many disjoint subsets \(\{_{l}\}_{l[0:L]}\) such that inside each subset \(_{l}\), the conditional empirical context distribution is almost uniform. As a result, we can invoke Algorithm 1 to find a near-optimal policy \(_{l}\) for each subset \(_{l}\) (Line 5). It requires \((r(S+K)/^{2})\) time steps for every \(\) but we only use samples where contexts are from \(_{l}\) and ignore the rest. Finally, we glue all \(_{l}\) together to get a policy \(^{}\) that is near-optimal for the original problem. Formally, we have the following theoretical guarantee for Algorithm 3. The proof and exact constants are in Appendix C.

**Theorem 2**.: _Let \((0,1)\). Algorithm 3 outputs an \(()\)-optimal policy within \((r(S+K)(1/)/^{2})\) samples with probability at least \(1-\)._

Note that we can always learn an \(\)-optimal policy for any context-lumpable bandit within \((SK/^{2})\) samples by invoking any existing near-optimal algorithm for finite contextual bandits. As a result, by combining Theorem 2 with the \((SK/^{2})\) upper bound, we obtain that \((\{r(S+K),SK\}/^{2})\) samples suffice for learning any context-lumpable bandit, which according to the following theorem is minimax optimal up to a logarithmic factor.

**Theorem 3**.: _Learning an \(\)-optimal policy for a context-lumpable bandit with probability no smaller than \(1/2\) requires at least \((\{r(S+K),SK\}/^{2})\) samples in the worst case._

## 4 Regret Minimization in Context-lumpable Bandits

In this section, we extend the idea from the PAC setting to the online setting. To better introduce the key ideas, we first consider a special case when both context and block distributions are uniform (Section 4.1). Then we consider the most general case in Section 4.2.

### Special Case: Uniform Context and Block Distribution

In this section, we assume that distributions \(\) and \(\) are uniform, and thus, \(g\) evenly splits the contexts into \(r\) blocks so that there are \(S/r\) contexts in each block and every context appears with the same probability at each timestep. We will relax these assumptions and consider the general case in the next subsection.

For this case, we introduce Algorithm 4, which uses phased elimination in combination with a clustering procedure. The algorithm runs in phases \(h=1,2,\) that are specified by error tolerance parameter \(_{h}=2^{-h/2}\). Like phased elimination algorithms for multi-armed bandits, we need to ensure at phase \(h\) actions the algorithms play are all \((_{h})\)-optimal. Thus, at the end of each phase \(h\), we eliminate all actions that are not \((_{h})\)-optimal. However, the set of \((_{h})\)-optimal actions of each block is different. Therefore, we also perform _clustering_ on contexts and perform elimination for each subset of the partition. Specifically, we maintain a partition of contexts \(_{h}\) at each phase \(h\) and initialize \(_{1}=\{[S]\}\). For each cluster \(_{h}\), we maintain a set of good arms GOOD\({}_{h}()\), which we will prove is \((_{h})\)-optimal for contexts in the cluster with high probability.

We use Algorithm 2 to collect data similar to Algorithm 1 (Line 6). At phase \(h\), we try to estimate mean reward up to error \((_{h})\) with probability \(1-_{h}\). The difference is that we assume \(\) is uniform, so we don't need different accuracy levels \(n\), which will be required for the algorithm that handles the general case. Also, for every context \(i\), we only explore arms _good for now_, that is, in \(_{h}()\) instead of exploring all the arms \([K]\). This reflects that in the online setting, we need to minimize regret and cannot afford to explore bad arms too many times.

Based on the data we collect during the exploration stage, we then check if there is a large gap across contexts in the same subset for any arms (Line 7). A large gap suggests that (i) the subset contains at least two blocks (ii) the mean reward of the arm is significantly different in these blocks and we can use this arm to partition the contexts by running Algorithm 5 (clustering stage). We repeatedly do clustering (Line 9) and split heterogeneous subsets (Line 10) until we cannot find a large gap within the same subset. If no large gap is detected, then each arm has similar mean rewards (up to error \((_{h})\)) across all blocks in the same subset. Then we eliminate arms that are significantly worse than the empirical best arm (elimination stage) from \(_{h}()\) for every subset \(\) and start a new episode (Line 14).

Algorithm 5 plays \(j\) for each context \(i\) for \((1/^{2})\) rounds and calculates empirical means of arm \(j\) for each context in \(\). It then sorts the contexts by the empirical means and performs clustering (Line 4). Specifically, the algorithm enumerates contexts in descending order of empirical means and splits contexts until a large gap between consecutive values is detected (Line 7). As we call Algorithm 5 only if a large gap is detected, we prove that in the appendix it correctly separates the subset into at least two parts without putting any contexts in the same block into different parts by setting \(^{}=_{h}/r\).

```
1Initialize \(_{1}\{[S]\}\), \(_{1}()[K]\) for \(_{1}\)
2Let \(t\) denote the current time
3forphase \(h=1,2,\),do
4Let \(_{h} 2^{-h/2}\), \(_{h}_{h}^{2}/(r^{3}SK)\), \(}_{h} 64(rSK/_{h})\), \(_{h}}_{h}} _{h}\)
5Step 1. Data collection
6Define \(L_{h}}_{h}}{_{h}^{2}}\), \(n_{h}(^{2}})\), \(_{h}(i)_{h}()\), \(_{h}\), \( i\), \( i[S]\)
7Execute Algorithm 2 with input \(L_{h}\), \(n_{h}\), \(_{h}\), and receive \(_{h}\) and \(_{h}\)
8Step 2. Test homogeneity and perform clustering on heterogeneous subsets
9while\(\,_{h},,,j[K]\) such that \(_{h}(,j)-_{h}(i,j)_ {h}\)do
10Define \(^{}}{4r}\), \(^{}}{r}\), \((i)\{j\}\) if \(i\) else \(_{h}()\) for \(_{h}\), \( i\)
11Execute Algorithm 5 with input \(^{}\), \(^{}\), \(\), \(\), and \(j\), and get \(\), a partition of \(\)
12Initialize \(_{h}(^{})_{h}()\), \(^{}\) and update \(_{h}(_{h})\{\}\)
13Step 3. Eliminate suboptimal actions in each subset
14\(_{h+1}_{h}\)
15for\(_{h+1}\)do
16Calculate \(_{h}(,j)_{i:i,(i,j) _{h}}_{h}(i,j)\), \( j_{h}()\)
17Update \(_{h+1}()\{j:j_{h}(), \ _{j^{}}_{h}(,j^{})-_{h}( ,j) 2_{h}\}\) ```

**Algorithm 4** Algorithm for Uniform Block Distribution

Similar to the analysis of other phased elimination algorithms, we have to show that in a phase specified by error level \(_{h}\), with high probability, (i) the optimal arm is not eliminated and (ii) all \((_{h})\)-suboptimal arms are eliminated, that is, all arms in \(_{h}()\) for all \(\) are \((_{h})\)-optimal. We show the final regret here and defer the details to Appendix D

**Theorem 4**.: _Under the assumption that context distribution \(\) and block distribution \(\) are uniform, regret of Algorithm 6 is bounded as \(_{T}=((S+K)T})\)._Compared to our PAC result, we get an extra dependency on \(r\). This is because we pay \((S/^{ 2})=(r^{2}S/_{h}^{2})\) samples to do clustering instead of \((1/_{h}^{2})\) in order to ensure a "perfect" partition, that is, we never separate contexts in the same block with high probability. This is crucial for our phase-elimination algorithm as we may call Algorithm 5 in different phases. We left getting better than \(((S+K)T})\) regret upper bounds or better than \(()\) regret lower bounds (even for this uniform special case) as an important future direction.

### Non-uniform Context and Block Distribution

Similar to the PAC learning setting, we can use Algorithm 3 to reduce general context distributions to (nearly) uniform ones. We provide more details in Appendix F. As a result, without loss of generality, we may assume \(\) is almost-uniform, and we focus on how to handle non-uniform block distribution \(\). In the extreme, there may only be one context for some blocks, which becomes challenging to estimate their mean rewards.

For this case, we introduce Algorithm 6. Intuitively, based on Algorithm 4, we can further employ different accuracy levels as Algorithm 1. However, as our goal becomes minimizing regret, it is difficult to control regret for smaller \(n\) and larger blocks. Specifically, for smaller \(n\), we explore more actions (with fewer samples) for each context in a single phase. Since fewer samples are used, we may unavoidably play suboptimal actions and suffer large regret. This becomes worse for a large block as more contexts in the block suffer large regret. We note that this is not a problem in the PAC setting because we only need to control sample complexity.

We fix the issue by setting different lengths \(L\) for different accuracy levels. Recall in Algorithm 1, we use the same length \(L=(r(S+K)/^{2})\) for every \(n\). Intuitively, since we allow less accurate estimations for smaller \(n\), we may use fewer data. It turns out indeed we can set \(L=(r(S+K)2^{(n+h)/2})\) for level \(n\) at phase \(h\), and with more refined analysis, it provides similar guarantees as before. More importantly, since smaller \(n\) uses (exponentially) fewer samples, the overall regret is well controlled.

In addition to setting the lengths sophisticatedly, we also need to carefully maintain sets of good actions GOOD\({}_{h,n}\) not only at each phase \(h\) but also at each accuracy level \(n\). The partition of contexts \(_{h}\), however, only evolves with phases and is shared between different levels at the same phase. Similar to Algorithm 4, we also do clustering (Step 2) and elimination (Step 3) but do the procedures in parallel for every accuracy level \(n\). In the clustering stage, we use different thresholds to define a "large gap". This reflects that \(n\) represents different accuracy levels and thus has different widths of confidence intervals. For the elimination stage, we enforce the inclusion relation GOOD\({}_{h}(n,)\) GOOD\({}_{h}(n^{},)\) for \(n^{} n\) (Line 17), which will be useful in the regret analysis. We now present the main theorem and put the complete proof in Appendix G.

**Theorem 5**.: _Regret of Algorithm 6 is bounded as \(_{T}=((S+K)T}).\)_

We remark that Algorithm 6 is _anytime_, that is, it doesn't require the time horizon \(T\) as input. However, it does require the number of blocks \(r\) as prior knowledge. Removing the knowledge of \(r\) is an interesting future direction. One promising idea is to apply a doubling trick on \(r\). Specifically, we have an initial guess \(r=(1)\) and run Algorithm 6; when \(|_{h}|>r\), we double \(r\) and restart the algorithm. The analysis, though, may be much more complicated. Finally, we note that when knowing \(r\), one can calculate in advance if \(\) is less than \((S+K)T}\) and switch to a standard algorithm achieving \(()\) in that case. This modification guarantees the regret is never worse than the standard bound.

## 5 From Context-lumpable Bandits to Contextual Low-rank Bandits

Finally, we consider the more general _contextual low-rank bandit_ problem. Specifically, we allow \(A\) to have rank \(r\), that is \(A=UV\) for some \(S r\) matrix \(U\) and \(r K\) matrix \(V\). Lumpability is a special case in the sense that \(U\) is binary where each row has a single nonzero element.

To solve the problem, We show a reduction from contextual low-rank bandits to context-lumpable bandits. Consider an \(\)-covering of rows of \(U\), and notice that the covering number, \(_{}\), is \(1/^{r}\). The context-lumpable bandits can be seen as \(\)-approximate context-lumpable bandits with \(_{}\) blocks, where the reward of contexts on the same block differs at most \(\). Ignoring this misspecification, we may run Algorithm 1 and Algorithm 6 for the PAC and online settings, respectively. Moreover, it turns out that our algorithms are robust to this misspecification when \(\) is sufficiently small (\(O()\) in the PAC setting, for example). Therefore, the sample complexity and the regret bounds of these algorithms will be in terms of \(_{}\) despite having an exponential dependency on \(r\). The resulting bounds can still be smaller than the naive \(SK/^{2}\) and \(\) bounds in some scenarios, for example, when \(S\) and \(K\) are super large. We put the details in Appendix H.

## 6 Conclusions and Future Directions

We consider a contextual bandit problem with \(S\) contexts and \(K\) actions. Under the assumption that the context-action reward matrix has \(r\{S,K\}\) unique rows, we show an algorithm that outputs an \(\)-optimal policy and has the optimal sample complexity of \((r(S+K)/^{2})\) with high probability. In the regret minimization setting, we show an algorithm whose cumulative regret up to time \(T\) is bounded as \(((S+K)T})\).

An immediate next question is whether a regret bound of order \(()\) is achievable in the regret minimization setting. A second open question is concerned with obtaining a \(((r)(S+K)T})\) regret bound in contextual low-rank bandits. Our regret analysis heavily relies on the assumption that contexts arrive in an I.I.D fashion. Extending our results to the setting with adversarial contexts remains another important future direction.