# LCM: Locally Constrained Compact Point Cloud Model for Masked Point Modeling

Yaohua Zha\({}^{1,2}\)   Naiqi Li\({}^{1}\)   Yanzi Wang\({}^{1}\)   Tao Dai\({}^{3}\)

Hang Guo\({}^{1}\)   Bin Chen\({}^{4}\)   Zhi Wang\({}^{1}\)   Zhihao Ouyang\({}^{5}\)   Shu-Tao Xia\({}^{1,2}\)

Corresponding author. daitao.edu@gmail.com

###### Abstract

The pre-trained point cloud model based on Masked Point Modeling (MPM) has exhibited substantial improvements across various tasks. However, these models heavily rely on the Transformer, leading to quadratic complexity and limited decoder, hindering their practice application. To address this limitation, we first conduct a comprehensive analysis of existing Transformer-based MPM, emphasizing the idea that redundancy reduction is crucial for point cloud analysis. To this end, we propose a Locally constrained **C**ompact point cloud **M**odel (LCM) consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder. Our encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency. Considering the varying information density between masked and unmasked patches in the decoder inputs of MPM, we introduce a locally constrained Mamba-based decoder. This decoder ensures linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density. Extensive experimental results show that our compact model significantly surpasses existing Transformer-based models in both performance and efficiency, especially our LCM-based Point-MAE model, compared to the Transformer-based model, achieved an improvement of 1.84%, 0.67%, and 0.60% in average accuracy on the three variants of ScanObjectNN while reducing parameters by **88%** and computation by **73%**. Code is available at https://github.com/zyh16143998882/LCM.

## 1 Introduction

3D point cloud perception, as a crucial application of deep learning, has achieved significant success across various areas such as autonomous driving, robotics, and virtual reality. Recently, point cloud self-supervised learning , capable of learning universal representations from extensive unlabeled point cloud data, has gained much attention. Among which, masked point modeling (MPM) , as an important self-supervised paradigm, has become mainstream in point cloud analysis and has gained immense success across diverse point cloud tasks.

The classical MPM , inspired by masked image modeling  (MIM), divides point clouds into patches and uses a standard Transformer  backbone. It randomly masks some patches in the encoder input and combines the unmasked patch tokens with randomly initializedmasked patch tokens in the decoder input. It predicts the geometric coordinates or semantic features of the masked patches from the decoder output tokens, enabling the model to learn universal 3D representations. Despite the significant success, two inherent issues of Transformers still limit their practical deployment.

The first issue is that the Transformer architecture leads to quadratic complexity and huge model sizes. As shown in Figure 1 (a) and (b), MPM methods like Point-MAE  based on standard Transformer  require 22.1M parameters and complexity exponentially grows with an increase in the length of input patches. However, in practical point cloud applications, models are often deployed on embedded devices such as robots or VR headsets, where strict constraints exist regarding the model's size and complexity. In this context, lightweight networks such as PointNet++  are more popular in practical applications due to their lower parameter requirement (only 1.5M) even though they may have inferior performance.

Another issue is that when Transformers  are used as decoders in Masked Point Modeling (MPM), their potential to reconstruct masked patches with lower information density is limited. In the decoder input of MPM, randomly initialized masked tokens with lower information density are typically concatenated with unmasked tokens with higher information density and fed into the Transformer-based decoder. The self-attention layers then learn to process these tokens of varying information density based on loss constraints. However, relying solely on the loss to learn this objective is challenging due to the lack of explicit importance guidance for different densities. Additionally, in Section 5.1, we further explain from an information theory perspective that the self-attention mechanism, as a higher-order processing function, can limit the model's reconstruction potential.

To address the above issues, as shown in Figure 2, we first conducted a comprehensive analysis of the effects of different top-K attention on the performance of the Transformer model, emphasizing the idea that redundancy reduction is crucial for point cloud analysis. To this end, we propose a **L**ocally constrained **C**ompact point cloud **M**odel (LCM), consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder, to replace the standard Transformer. Specifically, based on the idea of redundancy reduction, our compact encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency. The local aggregation layer leverages static local geometric constraints to aggregate the most relevant information for each patch token. Since static local geometric constraints only need to be computed once at the beginning and are shared across all layers, it avoids dynamic attention computations in each layer, significantly reducing complexity. Furthermore, it uses only two MLPs for information mapping, greatly reducing the network's parameters.

In our decoder design, considering the varying information density between masked and unmasked patches in the inputs of MPM, our decoder introduces the State Space Model (SSM) from Mamba  to replace self-attention, ensuring linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density. However, as discussed in Section 5.4, the directly replaced SSM layer exhibits a strong dependence on the order of input patches. Inspired by our compact encoder, we migrate the idea of local constraints to the feedforward neural network of our Mamba-based decoder, proposing the Local Constraints Feedforward Network (LCFFN). This eliminates the need to explicitly consider the sequence order of input in SSM layers because the subsequent LCFFN can adaptively exchange information among geometrically adjacent patches based on their implicit geometric order.

Our LCM is a universal point cloud architecture designed based on the characteristics of the point cloud to replace the standard Transformer. It can be trained from scratch or integrated into any existing pretraining strategy to achieve an elegant balance between performance and efficiency. For example, the LCM model pre-trained based on the Point-MAE strategy requires only 2.7M parameters, which is about **10**\(\) efficient compared to the original Transformer with 22.1M. Furthermore, in

Figure 1: Comparison of our LCM and Transformer in terms of performance and efficiency.

terms of performance, compared to the Transformer, the LCM shows significant improvements of 1.84%, 0.67%, and 0.60% in average classification accuracy of three variants of ScanObjectNN . Additionally, in the detection task of ScanNetV2 , there are also significant improvements of **+5.2%** on \(AP_{25}\) and **+6.0%** on \(AP_{50}\).

We summarize the contributions of our paper as follows: **1)** We propose a locally constrained compact encoder, which leverages static local geometric constraints to aggregate the most relevant information for each patch token, achieving an elegant balance between performance and efficiency. **2)** We propose a locally constrained Mamba-based decoder for masked point modeling, which replaces the self-attention layer with Mamba's SSM layer and introduces a locally constrained feedforward neural network to eliminate the explicit dependency of Mamba on the input sequence order. **3)** Our locally constrained compact encoder and locally constrained Mamba-based decoder together constitute the efficient backbone LCM for masked point modeling. We combine LCM with various pretraining strategies to pre-train efficient models and validate our model's superiority in efficiency and performance across various downstream tasks.

## 2 Related Work

**Point Cloud Self-supervised Pre-training.** Point cloud self-supervised pre-training [47; 54; 55; 58; 60] has achieved remarkable improvement in many point cloud tasks. This approach first applies a pretext task to learn the latent 3D representation and then transfers it to various downstream tasks. PointContrast  and CrossPoint  initially explored utilizing contrastive learning [36; 43] for learning 3D representations, which achieved some success; however, there were still some shortcomings in capturing fine-grained semantic representations. Recently, masked point modeling methods [61; 60; 37; 62] demonstrated significant improvements in learning fine-grained point cloud representations through masking and reconstruction. Many methods [4; 8; 21; 41; 66] have attempted to leverage multimodal knowledge to assist MPM in learning more generalized representations, yielding significant improvements. After obtaining a pre-trained point cloud model, many works [67; 18; 64; 70] remain to explore parameter-efficient fine-tuning methods to better adapt these pretrained models to a variety of downstream tasks. While the pre-trained models mentioned above have achieved tremendous success, they all rely on the Transformer architecture. In this paper, we focus on designing a more efficient architecture to replace the Transformer in these methods, significantly reducing computational and resource requirements.

## 3 Methodology

### Observation of Top-K Attention

Standard Transformer  architecture requires computing the correlation between each patch with all input patches, resulting in quadratic complexity. While this architecture performs well in language data, its effectiveness in point cloud data has been under-explored. Not all points are equally important. As illustrated in Figure 3, the key points for aircraft recognition are mainly distributed on the wings, while for vase recognition, they are primarily located on the bottom of the vase. Therefore, directly skipping the attention computation for less important points provides a straightforward solution.

Figure 3: Point heatmap.

Figure 2: The effect of using top-K attention in feature space and geometric space by the Transformer on the classification performance in ScanObjectNN, all results are the averages of ten repeated experiments.

We first replaced the computation of global attention for all patch tokens with calculations top-K attentions in both feature and geometric space. As shown in Figure 2, our empirical observations indicate that: **1)** In self-attention, it is often more effective to use attention weights based on the top-K most important patch tokens rather than using all patch; **2)** Compared to using top-K attention in a dynamic feature space, employing top-K attention in a static geometric space yields nearly identical representational capacity and offers the advantage of a smaller K value. Although this naive method of masking out unimportant attention still exhibits quadratic complexity, this redundancy reduction idea not only brings performance improvements but also provides a direction for further optimizing computational efficiency.

### The Pipeline of Masking Point Modeling with LCM

The overall architecture of our Locally constrained Compact Model (LCM) is shown in Figure 4. The specific process is as follows.

**Patching, Masking, and Embedding.** Given an input point cloud \(^{L 3}\) with \(L\) points, we initially downsample a central point cloud \(^{N 3}\) with \(N\) points by farthest point sampling (FPS). Then, we perform K-Nearest Neighborhood (KNN) around \(\) to get point patches \(^{N K 3}\). Following this, we randomly mask a portion of \(\) and \(\), resulting in masked elements \(}^{(1-r)N 3}\) and \(}^{(1-r)N K 3}\) and unmasked elements \(}^{rN 3}\) and \(}^{rN K 3}\), where \(r\) denotes the unmask ratio. Finally, we use MLP-based embedding layer (Embed) and position encoding layer (PE) respectively to extract semantic tokens \(}^{rN d}\) and central position embedding \(}^{rN d}\) for the unmasked patches, where \(d\) is the feature dimension.

**Encoder.** We employ our locally constrained compact encoder \(\) to extract features from the unmasked features \(}\). It consists of \(n\) stacked encoder layers, each layer incorporating a local aggregation layer and a feedforward neural network, detailed in Figure 4. For the input feature \(}\) of the \(i\)-th layer, after adding its positional embedding \(}\), it feeds to the \(i\)-th encoding layer \(_{i}\) to obtain the feature \(}\). Therefore, the forward process of each encoder layer is defined as:

\[}=_{i}(}+}), i=1,...,n\] (1)

**Decoder.** In the decoding phase, although various MPMs have different decoding strategies, they can generally be divided into feature-level or coordinate-level reconstruction, and their decoders mostly rely on the Transformer architecture. Here, we illustrate the decoding process of our locally constrained Mamba-based decoder using the coordinate-level reconstruction method Point-MAE  as an example.

We first concatenate unmasked tokens \(}^{rN d}\) before the randomly initialized masked tokens \(^{(1-r)N d}\) to obtain the input \(}^{N d}\) for the decoder. Then, we separately calculate the positional encoding for unmasked patches \(_{V}}^{rN d}\) and masked patches \(_{M}}^{(1-r)N d}\), and then concatenate them together to obtain the positional embeddings \(}^{N d}\), shared by all layers of the decoder. Finally, for the input feature \(}\) of the \(i\)-th decoder layer, after adding their positional embeddings \(}\), they are passed into the \(i\)-th decoder layer \(_{i}\) to compute the output

Figure 4: The pipeline of our Locally Constrained Compact Model (LCM) with Point-MAE pre-training. Our LCM consists of a locally constrained compact encoder and a locally constrained Mamba-based decoder.

features \(}\). Therefore, the forward process of each decoder layer is defined as:

\[}=_{i}(}+}), i=1,...,m,\] (2)

**Reconstruction.** We utilize the features \(=}[rN:]\) decoded by the decoder to perform the 3D reconstruction. We employ multi-layer MLPs to construct coordinates reconstruction head \(\) and our reconstruction target is to recover the relative coordinates \(_{M}=()\) of the masked patches. We use the \(}\) Chamfer Distance  (\(\)) as reconstruction loss. Therefore, our loss function \(\) is as follows

\[=(},})\] (3)

### Locally Constrained Compact Encoder

The classical Transformer  relies on the self-attention mechanism to perceive long-range correlations among all patches globally and has achieved great success in language and image domains. However, there remains uncertainty about whether directly transferring a Transformer-based encoder is suitable for point cloud data. Firstly, applications of point clouds are more inclined towards practical embedded devices such as robots or VR headsets. The hardware resources of these devices are limited, imposing higher limits on the model size and complexity, and the Transformer-based backbone demands significantly more resources than traditional networks, as illustrated in Table 1. Secondly, extensive research [34; 40; 50] and our empirical observation as illustrated in Figure 2 also indicate that the perception of local geometry in point cloud data far outweighs the need for global perception. Therefore, the computation of long-range correlations in self-attention leads to a considerable amount of redundant calculations. To address these practical issues, we propose a locally constrained compact encoder.

Our compact encoder consists of \(n\) stacked compact encoder layers, each layer comprising a local aggregation layer (LAL) and a feed-forward network (FFN), as shown in Figure 5 (a). For the \(i\)-th encoder layer, the output (\(}\)) of the preceding layer, added with the positional embedding and normalized by layer normal, is initially fed to the Local Aggregation Layer (LAL) for aggregating local geometric. Afterward, the result is added to the input residual, passed through layer normalization, and finally fed into a Feed-forward Network (FFN) to obtain the ultimate output feature (\(}\)). This process can be formalized as follows,

\[}=}+l_{i}(n_{i}^{1}(}),})\] (4) \[}=}+f_{i}(n_{i}^{2}(}))\] (5)

where \(l_{i}()\) represents the LAM, \(n_{i}^{1}()\) and \(n_{i}^{2}()\) represents layer normalization, and \(f_{i}()\) represents the FFN.

In the local aggregation layer, we first use the k-nearest neighbors algorithm based on the central coordinates \(_{u}}\) of the features \(}\) to find the \(k\) nearest neighbors feature \(^{n}}^{rkN d}\) for each token in \(}\). We then replicate each token of \(}\)\(k\) times and concatenate them with their corresponding neighbors to obtain \(^{c}}^{rkN 2d}\). Next, Down MLP performs a non-linear mapping on all local neighboring features to capture local geometric information. Subsequently, local max pooling is applied to aggregate all local features for each patch. Finally, Up MLP maps all patches to obtain locally enhanced features \(}^{rN d}\). Our LAL consists of only two simple MLP layers,

Figure 5: The structure of \(i\)-th locally constrained compact encoder layer (a) and \(i\)-th locally constrained Mamba-based decoder layer (b).

significantly reducing the network's parameters. Additionally, since static local geometric constraints only need to be computed once at the beginning and are shared across all layers thereafter, it avoids dynamic attention computations in each layer, significantly reducing computational requirements. It uses only two MLPs for information mapping, greatly reducing the network's parameters.

### Locally Constrained Mamba-based Decoder

The decoder for mask point modeling needs to recover information about masked patches based on the features \(}\) extracted from unmasked patches by the encoder. A common approach is to concatenate the features \(}\) of unmasked patches before randomly initialized features \(\) of masked patches as the input to the decoder, as shown in Figure 4. However, at this point, there is a significant difference in information density between features \(}\) and \(\). The Transformer architecture and our local aggregation layer both treat each token in the input as equally important initially, it works well when the information density of all tokens is similar. It does not adapt well to cases where there is a large difference in information density in the input.

To efficiently extract more geometric priors from unmasked features \(}\), we were inspired by the Mamba  model in time sequence and proposed using a Mamba-based decoder. This decoder can extract more prior information from the preceding tokens in the sequence based on the input order to aid the learning of subsequent tokens. Initially, we simply replaced the self-attention layer in the original Transformer-based  decoder with the state space model (SSM) layer from Mamba. We also sorted the input sequence based on the order of each patch's center point coordinates, creating a naive Mamba-based decoder. Our experiments in Section 8 revealed that although this naive decoder is efficient enough, the simple sorting method cannot effectively model the complex spatial geometry of point clouds and leads to a strong dependence on the order of input patches.

To ensure that the SSM fully perceives the spatial geometry of point clouds, we further introduced the concept of local constraints from the local aggregation layer into the feedforward neural network layer of our decoder, getting the Local Constraints Feedforward Network (LCFFN). By feeding the tokens outputted by the SSM layer into the LCFFN, the LCFFN can implicitly exchange information between geometrically adjacent patches based on their central coordinates. This eliminates the limitation in the SSM layer where explicit sequential input fails to perceive complex geometry fully. Finally, in Section 5.1, we also qualitatively explain from an information theory perspective that this Mamba-based architecture has greater reconstruction potential compared to the Transformer.

Our Mamba-based decoder consists of \(m\) stacked decoder layers, each layer comprising a Mamba SSM layer and a local constraints feedforward network (LCFFN), as shown in Figure 5 (b). For the \(i\)-th decoder layer, we first add the output (\(}\)) of the previous layer with the positional embeddings (\(}\)) and normalize it through layer normalization. Then, we use the Mamba SSM layer (\(s_{i}()\)) to perceive geometry from unmasked features and predict masked features. Finally, in the LCFFN (\(f_{i}^{l}()\)), we further perceive shape priors based on the central coordinates of each token from its geometrically adjacent tokens. This process can be formalized as follows:

\[}=}+s_{i}(n_{i}^{1}(}))\] (6) \[}=}+f_{i}^{l}(n_{i}^{2}(}),)\] (7)

## 4 Experiments

### Pre-training

We pre-training our LCM using five different pretraining strategies: Point-BERT , MaskPoint , Point-MAE , Point-M2AE , and ACT . For a fire comparison, we use ShapeNet  as our pre-training dataset, encompassing over 50,000 distinct 3D models spanning 55 prevalent object categories. For the hyperparameter settings during the pretraining phase, we used the same settings as previous methods.

### Fine-tuning on Downstream Tasks

We assess the performance of our LCM by fine-tuning our models on various downstream tasks, including object classification, scene-level detection, and part segmentation.

#### 4.2.1 Object Classification

We initially assess the overall classification accuracy of our pre-trained models on both real-scanned (ScanObjectNN ) and synthetic (ModelNet40 ) datasets. ScanObjectNN is a prevalent dataset consisting of approximately 15,000 real-world scanned point cloud samples from 15 categories. These objects represent indoor scenes and are often characterized by cluttered backgrounds and occlusions caused by other objects. For the ScanObjectNN dataset, we sample 2048 points for each instance and report results without voting mechanisms. We applied simple scaling and rotation data augmentation of previous work [8; 37] in the downstream setting of ScanObjectNN. We reported the results of different models under our downstream setting, with \(\) marking the results. For the ModelNet40 dataset, due to space limitation, we will further analyze its results in Section 5.4.

To ensure a fair comparison, we conducted our experiments following the standard practices in the field (as used in previous work [8; 28; 37; 60; 65]). For each point cloud classification experiment, we used eight different random seeds (0-7) to ensure the robustness and reliability of our results. The performance reported in Table 1 represents the **average accuracy** achieved across these eight trials for each model configuration.

As presented in Table 1, our model has many exciting results. 1) **Lighter**, **faster**, and **more powerful**. When trained from scratch using supervised learning only, our LCM model demonstrates performance improvements of 0.82%, 0.15%, and 1.10% across three variant datasets compared to the Transformer architecture. Similarly, after pre-training (_e.g._, Point-MAE), our model outperformed the standard Transformer by 1.84%, 0.67%, and 0.60% across the three variants of the ScanObjectNN dataset. Notably, these improvements are achieved despite an **88%** reduction in parameters and a **73%** reduction in FLOPs. This improvement is exciting as it indicates that our architecture is better suited for point cloud data compared to the standard Transformer. Additionally, due to its extremely high

    &  &  &  &  \\   & & & & OBJ-BG & OBJ-ONLY & PB-T50-RS \\   \\  \(\) PointNe  & ✘ & 3.5 & 0.5 & 73.3 & 79.2 & 68.0 \\ \(\) PointNet++  & ✘ & 1.5 & 1.7 & 82.3 & 84.3 & 77.9 \\ \(\) PointMLP  & ✘ & 12.6 & 31.4 & - & - & 85.2 \\ \(\) Transformer  & ✘ & 22.1 & 4.8 & 86.75 & 86.92 & 80.78 \\ \(\) PointMamba  & ✘ & 12.3 & - & 88.30 & 87.78 & 82.48 \\ \(\) STR  & ✘ & - & - & - & 87.80 \\ \(\) Transformer  & ✘ & 22.1 & 4.8 & 91.95 & 91.39 & 86.65 \\ \(\) LCM (Ours) & ✘ & 2.7(\(\) 88\%) & 1.3(\(\) 73\%) & 92.77(\(\) 0.82) & 91.54(\(\) 0.15) & 87.75(\(\) 1.10) \\   \\  \(\) Point-BERT  & MPM & 22.1 & 4.5 & 87.43 & 88.12 & 83.07 \\ \(\) MaskPoint  & MPM & 22.1 & 4.5 & 89.30 & 88.10 & 84.30 \\ \(\) Point-MAE  & MPM & 22.1 & 4.8 & 90.02 & 88.29 & 85.18 \\ \(\) Point-MAE w/ IDPT  & MPM & 23.3 & 7.1 & 91.22 & 90.02 & 84.94 \\ \(\) Point-MAE w/ DAPPT  & MPM & 22.7 & 5.0 & 90.88 & 90.19 & 85.08 \\ \(\) Inter-MAE  & MPM & 22.1 & 4.8 & 88.70 & 89.60 & 85.40 \\ \(\) Point-MAE  & MPM & 12.9 & 7.9 & 91.22 & 88.81 & 86.43 \\ \(\) ACT  & MPM & 22.1 & 4.8 & 93.29 & 91.91 & 88.21 \\ \(\) PointGPT-B  & GPT & 120.5 & 36.2 & 93.60 & 92.50 & **89.60** \\ \(\) PointMamba  & MPM & 12.3 & - & 93.29 & 91.91 & 88.17 \\ \(\) Point-BERT  & MPM & 22.1 & 4.5 & 92.48 & 91.60 & 87.91 \\ \(\) MaskPoint  & MPM & 22.1 & 4.5 & 92.17 & 91.69 & 87.65 \\ \(\) Point-MAE  & MPM & 22.1 & 4.8 & 92.67 & 92.08 & 88.27 \\ \(\) Point-MAE  & MPM & 12.9 & 7.9 & 93.12 & 91.22 & 88.06 \\ \(\) ACT  & MPM & 22.1 & 4.8 & 92.08 & 91.70 & 87.52 \\ \(\) Point-BERT w/LCM & MPM & 3.1 (\(\) 86\%) & 2.5 (\(\) 44\%) & 93.55(\(\) 1.07) & 92.43(\(\) 0.83) & 88.57(\(\) 0.66) \\ \(\) MaskPoint w/ LCM & MPM & 3.1 (\(\) 86\%) & 2.5 (\(\) 44\%) & 93.53(\(\) 1.14) & 91.98(\(\) 0.29) & 87.75(\(\) 0.10) \\ \(\) Point-MAE w/ LCM & MPM & 2.7 (\(\) 88\%) & 1.3 (\(\) 73\%) & **94.51**(\(\) 1.84) & 92.75(\(\) 0.67) & 88.87(\(\) 0.60) \\ \(\) Point-M2AE w/ LCM & MPM & **2.5 (\(\) 81\%)** & 6.7 (\(\) 15\%) & 93.83(\(\) 0.71) & 92.41(\(\) 1.19) & 88.38(\(\) 0.32) \\ \(\) ACT w/ LCM & MPM & 3.1 (\(\) 86\%) & 2.8 (\(\) 42\%) & 94.13(\(\) 2.05) & **92.66**(\(\) 0.96) & 88.57(\(\) 1.05) \\   

Table 1: Classification accuracy on real-scanned point clouds (ScanObjectNN). We report the overall accuracy (%) on three variants. "#Params" represents the model’s parameters and FLOPs refer to the model’s floating point operations. GPT, CL, and MPM respectively refer to pre-training strategies based on autoregression, contrastive learning, and masked point modeling. \(\) is the reported results from the original paper. \(\) is the result reproduced in our downstream settings.

efficiency, it provides strong support for the practical deployment of these pre-trained models. 2) **Universal.** We have replaced the original Transformer architecture with our LCM model in five different MPM-based pre-training methods. All experimental results are exciting as our model achieved universal performance improvements with fewer parameters and computations, highlighting the versatility of our model. In the future, we will further adapt to additional pre-training methods.

#### 4.2.2 Object Detection

We further assess the object detection performance of our pre-trained model on the more challenging scene-level point cloud dataset, ScanNetV2 , to evaluate our model's scene understanding capabilities. Following the previous pre-training work [8; 28], we use 3DETR  as the baseline and only replace the Transformer-based encoder of 3DETR with our pre-trained compact encoder. Subsequently, the entire model is fine-tuned for object detection. In contrast to previous approaches [4; 8; 28], which necessitate pre-train on large-scale scene-level point clouds like ScanNet, our approach directly utilizes models pre-trained on ShapeNet. This further emphasizes the generalizability of our pre-trained models.

Table 2 showcases our experimental results, our compact model has shown significant improvements in scene-level point cloud data, such as Point-MAE  achieving a 5.2% improvement in \(AP_{25}\) and a 6.0% improvement in \(AP_{50}\) compared to the Transformer. This improvement is remarkable, and we believe this is primarily due to the presence of a large number of background and noise points in the scene-level point cloud. Using a local constraint modeling approach effectively filters out unimportant background and noise, allowing the model to focus more on meaningful points.

#### 4.2.3 Part Segmentation

We also assess the performance of LCM in part segmentation using the ShapeNetPart dataset , comprising 16,881 samples across 16 categories. We utilize the same segmentation setting after the pre-trained encoder as in previous works [37; 65] for fair comparison. As shown in Table 3, our LCM-based model also exhibits a clear boost compared to Transformer-based models. These results demonstrate that our model exhibits superior performance in tasks such as part segmentation, which demands a more fine-grained understanding of point clouds.

   Methods & Pretrain & \(AP_{25}\) & \(AP_{30}\) \\   \\  VoteNet  & ✘ & 58.6 & 33.5 \\
3DETR [baseline] & ✘ & 62.1 & 37.9 \\ Transformer  & ✘ & 60.5 & 40.6 \\
**LCM (Ours)** & ✘ & 63.8 (\(\)3.3) & 46.4 (\(\)5.8) \\   \\  PointContratat  & CL & 58.5 & 38.0 \\ STRL  & CL & - & 38.4 \\ Point-BERT  & MPM & 61.0 & 38.3 \\ PIMAE  & MPM & 62.6 & 39.4 \\ Point-MAE  & MPM & 59.5 & 41.2 \\ Point-MAE  & MPM & 60.0 & 41.4 \\ ACT  & MPM & 63.8 & 42.1 \\ DepthContrat  & CL & 64.0 & 42.9 \\ MaskPoint  & MPM & 64.2 & 42.1 \\ Point-BERT  **w/ LCM** & MPM & **65.3** (\(\)4.3) & **47.3** (\(\)9.0) \\ Point-MAE  **w/ LCM** & MPM & 64.7 (\(\)5.2) & 47.2 (\(\)6.0) \\ Point-MAE  **w/ LCM** & MPM & **63.5** (\(\)3.5) & 44.0 (\(\)2.6) \\ ACT  **w/ LCM** & MPM & **65.0** (\(\)1.2) & 45.8 (\(\)3.7) \\ MaskPoint  **w/ LCM** & MPM & 65.3 (\(\)1.1) & 46.3 (\(\)4.2) \\   

Table 2: Object detection results on ScanNetV2. We adopt the average precision with 3D IoU thresholds of 0.25 (\(AP_{25}\)) and 0.5 (\(AP_{50}\)) for the evaluation metrics. \({}^{}\) is our reproduction results, due to the lack of detection code in their paper.

   Methods & Pretrain & \(_{e}\) & \(_{t}\) \\   \\  PointNet++  & ✘ & 81.9 & 85.1 \\ DGCNN  & ✘ & 82.3 & 85.2 \\ Transformer  & ✘ & 83.9 & 86.0 \\ LCM (Ours) & ✘ & 84.6 (\(\)0.7) & 86.3 (\(\)0.3) \\   \\  Transformer-OcOc  & CL & 83.4 & 85.1 \\ PointContrat  & CL & - & 85.1 \\ CrossPoint  & CL & - & 85.5 \\ Point-BERT  & MPM & 84.1 & 85.6 \\ DDT  & MPM & 83.8 & 85.9 \\ MaskPoint  & MPM & 84.4 & 86.0 \\ Point-MAE  & MPM & 84.2 & 86.1 \\ ACT  & MPM & 84.7 & 86.1 \\ PointPST-S  & MPM & 84.1 & 86.2 \\ PointPGT-B  & MPM & 84.5 & 86.4 \\ Point-MZAE  & MPM & 84.9 & 86.5 (\(\)0.9) \\ Point-BEF  **w/ LCM** & MPM & 85.0 (\(\)0.9) & 86.5 (\(\)0.9) \\ MaskPoint  **w/ LCM** & MPM & 85.1 (\(\)0.7) & 86.6 (\(\)0.6) \\ Point-MAE  **w/ LCM** & MPM & **85.1** (\(\)0.9) & 86.6 (\(\)0.5) \\ ACT  **w/ LCM** & MPM & **85.0** (\(\)0.1) & 86.5 (\(\)0.3) \\ ACT  **w/ LCM** & MPM & 85.0 (\(\)0.3) & **86.7**(\(\)0.6) \\   

Table 3: Part segmentation results on the ShapeNetPart. The mean IoU across all categories, i.e., \(_{e}\) (%), and the mean IoU across all instances, i.e., \(_{I}\) (%) are reported.

[MISSING_PAGE_FAIL:9]

of our model. Despite these constraints, the current model has demonstrated significant improvements in performance across various tasks. Nevertheless, we also acknowledge that incorporating dynamic importance perception and long-range dependency modeling could further enhance the model's capabilities, particularly in more complex scenarios. We are actively exploring methods to address these limitations in future work.

### Conclusion

In this paper, we propose a compact point cloud model, LCM, specifically designed for masked point modeling pre-training, aiming to achieve an elegant balance between performance and efficiency. Based on the idea of redundancy reduction, we propose focusing on the most relevant point patches ignoring unimportant parts in the encoder, and introducing a local aggregation layer to replace the vanilla self-attention. Considering the varying information density between masked and unmasked patches in the decoder inputs of MPM, we introduce a locally constrained Mamba-base decoder to ensure linear complexity while maximizing the perception of point cloud geometry information from unmasked patches. By conducting extensive experiments across various tasks such as classification and detection, we demonstrate that our LCM is a universal model with significant improvements in efficiency and performance compared to traditional Transformer models.