# Disentangled Generative Graph Representation Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recently, generative graph models have shown promising results in learning graph representations through self-supervised methods. However, most existing generative graph representation learning (GRL) approaches rely on random masking across the entire graph, which overlooks the entanglement of learned representations. This oversight results in non-robustness and a lack of explainability. Furthermore, disentangling the learned representations remains a significant challenge and has not been sufficiently explored in GRL research. Based on these insights, this paper introduces **DiGGR** (**D**isentangled **G**enerative **G**raph **R**epresentation Learning), a self-supervised learning framework. DiGGR aims to learn latent disentangled factors and utilizes them to guide graph mask modeling, thereby enhancing the disentanglement of learned representations and enabling end-to-end joint learning. Extensive experiments on 11 public datasets for two different graph learning tasks demonstrate that DiGGR consistently outperforms many previous self-supervised methods, verifying the effectiveness of the proposed approach.

## 1 Introduction

Self-supervised learning (SSL) has received much attention due to its appealing capacity for learning data representation without label supervision. While contrastive SSL approaches are becoming increasingly utilized on images (Chen et al., 2020) and graphs (You et al., 2020), generative SSL has been gaining significance, driven by groundbreaking practices such as BERT for language (Devlin et al., 2018), BEiT (Bao et al., 2021), and MAE (He et al., 2022) for images. Along this line, there is a growing interest in constructing generative SSL models for other modalities, such as graph masked autoencoders (GMAE). Generally, the fundamental concept of GMAE (Tan et al., 2022) is to utilize an autoencoder architecture to reconstruct input node features, structures, or both, which are randomly masked before the encoding step. Recently, various well-designed GMAEs have emerged, achieving remarkable results in both node classification and graph classification (Hou et al., 2022; Tu et al., 2023; Tian et al., 2023).

Despite their significant achievements, most GMAE approaches typically treat the entire graph as holistic, ignoring the graph's latent structure. As a result, the representation learned for a node tends to encapsulate the node's neighborhood as a perceptual whole, disregarding the nuanced distinctions between different parts of the neighborhood (Ma et al., 2019; Li et al., 2021; Mo et al., 2023). For example, in a social network \(\), individual \(n\) is a member of both a mathematics group and several sports interest groups. Due to the diversity of these different communities, she may exhibit different characteristics when interacting with members from various communities. Specifically, the information about the mathematics group may be related to her professional research, while the information about sports clubs may be associated with her hobbies. However, the existing approach overlooks the heterogeneous factors of node \(n\), failing to identify and disentangle these pieces ofinformation effectively (Hou et al., 2022). Consequently, the learned features may be easily influenced by irrelevant factors, resulting in poor robustness and difficulty in interpretation.

To alleviate the challenge described above, there is an increasing interest in disentangled graph representation learning (Bengio et al., 2013; Li et al., 2021; Ma et al., 2019; Mo et al., 2023; Xiao et al., 2022), which aims at acquiring representations that can disentangle the underlying explanatory factors of variation in the graph. Specifically, many of these methods rely on a latent factor detection module, which learns the latent factors of each node by comparing node representations with various latent factor prototypes. By leveraging these acquired latent factors, these models adeptly capture factor-wise graph representations, effectively encapsulating the latent structure of the graph. Despite significant progress, few studies have endeavored to adapt these methods to to generative graph representation learning methods, such as GMAE. This primary challenge arises from the difficulty of achieving convergence in the latent factor detection module under the generative training target, thus presenting obstacles in practical implementation. As shown in Fig.1(a), directly applying the previous factor learning method to GMAE would make the factor learning module difficult to converge, resulting in undistinguished probabilities and misallocation of similar nodes to different latent factor groups.

To address these challenges, we introduce **D**isentangled **G**enerative **G**raph **R**epresentation Learning (**DiGGR**), a self-supervised graph generation representation learning framework. Generally speaking, DiGGR learns how to generate graph structures from latent disentangle factors \(z\) and leverages this to guide graph mask reconstruction, while enabling end-to-end joint learning. Specifically, \(i)\) To capture the heterogeneous factors in the nodes, we introduce the latent factor learning module. This module models how edges and nodes are generated from latent factors, allowing graphs to be factorized into multiple disentangled subgraphs. \(ii)\) To learn a deeper disentangled graph representation, we design a factor-wise self-supervised graph representation learning framework. For each subgraph, we employ a distinct masking strategy to learn an improved factor-specific graph representation. Evaluation shows that the proposed framework can achieve significant performance enhancement on various node and graph classification benchmarks.

The main contributions of this paper can be summarized as follows:

* We utilized the latent disentangled factor to guide mask modeling. A probabilistic graph generation model is employed to identify the latent factors within a graph, and it can be jointly trained with GMAE through variational inference.
* Introducing **DiGGR** (**D**isentangled **G**enerative **G**raph **R**epresentation Learning) to further capture the disentangled information in the latent factors, enhancing the disentanglement of the learned node representations.
* Empirical results show that the proposed DiGGR outperforms many previous self-supervised methods in various node- and graph-level classification tasks.

## 2 Related works

Graph Self-Supervised Learning:Graph SSL has achieved remarkable success in addressing label scarcity in real-world network data, mainly consisting of contrastive and generative methods. Con

Figure 1: The number of latent factors is set to 4. In Fig. 1(a), the probabilities of nodes belonging to different latent groups are similar, resulting in nodes of the same type being incorrectly assigned to different factors. In contrast, Fig. 1(b) shows that the probabilities of node-factor affiliation are more discriminative, correctly categorizing nodes of the same type into the same latent group.

trastive methods, includes feature-oriented approachesHu et al. (2019); Zhu et al. (2020); Velickovic et al. (2018), proximity-oriented techniques Hassani and Khasahmadi (2020); You et al. (2020), and graph-sampling-based methods Qiu et al. (2020). A common limitation across these approaches is their heavy reliance on the design of pretext tasks and augmentation techniques. Compared to contrastive methods, generative methods are generally simpler to implement. Recently, to tackle the challenge of overemphasizing neighborhood information at the expense of structural information Hassani and Khasahmadi (2020); Velickovic et al. (2018), the Graph Masked Autoencoder (GMAE) has been proposed. It applies a masking strategy to graph structure Li et al. (2023), node attributes Hou et al. (2022), or both Tian et al. (2023) for representation learning. Unlike most GMAEs, which employ random mask strategies, this paper builds disentangled mask strategies.

Disentangled Graph Learning:Disentangled representation learning aims to discover and isolate the fundamental explanatory factors inherent in the data Bengio et al. (2013). Existing efforts in disentangled representation learning have primarily focused on computer vision Higgins et al. (2017); Jiang et al. (2020). Recently, there has been a surge of interest in applying these techniques to graph-structured data Li et al. (2021); Ma et al. (2019); Mercatali et al. (2022); Mo et al. (2023). For example, DisenGCN Ma et al. (2019) utilizes an attention-based methodology to discriminate between distinct latent factors, enhancing the representation of each node to more accurately reflect its features across multiple dimensions. DGCL Li et al. (2021) suggests learning disentangled graph-level representations through self-supervision, ensuring that the factorized representations independently capture expressive information from various latent factors. Despite the excellent results achieved by the aforementioned methods on various tasks, these methods are difficult to converge in generative graph SSL, as we demonstrated in the experiment of Table.3. Therefore, this paper proposes a disentangled-guided framework for generative graph representation learning, capable of learning disentangled representations in an end-to-end self-supervised manner.

## 3 Proposed Method

In this section, we propose **DiGGR** (**D**isentangled **G**enerative **G**raph **R**epresentation Learning) for self-supervised graph representation learning with mask modeling. The framework was depicted in Figure 2, comprises three primary components: Latent Factor Learning (Section 3.2), Graph Factorization (Section 3.2) and Disentangled Graph Masked autoencoder (Section 3.3). Before elaborating on them, we first show some notations.

### Preliminaries

A graph \(G\) can be represented as a multi-tuple \(=\{V,A,X\}\) with \(N\) nodes and \(M\) edges, where \(|V|=N\) is the node set, \(|A|=M\) is the edge set, and \(X^{N L}\) is the feature matrix for \(N\) nodes with \(L\) dimensional feature vector. The topology structure of graph \(G\) can be found in its adjacency matrix \(A^{N N}\). \(z^{N K}\) is the latent disentangled factor matrix, and \(K\) is the predefined factor number. Since we aim to obtain the \(z\) to guide the mask modeling, we first utilize a probabilistic graph generation model to factorize the graph before employing the mask mechanism. Given the graph \(G\), it is factorized into \(\{G_{1},G_{2},...,G_{K}\}\), and each factor-specific graph \(G_{k}\) consists of its factor-specific edges \(A^{(k)}\), node set \(V^{(k)}\) and node feature matrix \(X^{(k)}\). Other notations will be elucidated as they are employed.

### Latent Factor Learning

In this subsection, we describe the latent factor learning method. In this phase, our objective is to derive factor-specific node sets \(\{V^{(1)},V^{(2)},...,V^{(K)}\}\) and adjacency matrices \(\{A^{(1)},A^{(2)},...,A^{(K)}\}\), serving as basic unit of the graph to guide the subsequent masking. The specific approach involves modeling the distribution of nodes and edges, utilizing the generative process developed in EPM Zhou (2015). The generative process of EPM under the Bernoulli-Poisson link Zhou (2015) can be described as:

\[_{uv}(_{k=1}^{K}_{k}z_{uk}z_{vk} ),\ \ z_{uk}(,),u,v[1,N]\] (1)

where \(K\) is the predefined number of latent factors, and \(u\) and \(v\) are the indexes of the nodes. Here, \(_{uv}\) is the latent count variable between node \(u\) and \(v\); \(_{k}\) is a positive factor activation level indicator, which measures the node interaction frequency via factor \(k\); \(z_{uk}\) is a positive latent variable for node \(u\), which measures how strongly node \(u\) is affiliated with factor \(k\). The prior distribution of latent factor variable \(z_{uk}\) is set to Gamma distribution, where \(\) and \(\) are normally set to 1. Therefore, the intuitive explanation for this generative process is that, with \(z_{uk}\) and \(z_{vk}\) measuring how strongly node \(u\) and \(v\) are affiliated with the \(k\)-th factor, respectively, the product \(_{k}z_{uk}z_{vk}\) measures how strongly nodes \(u\) and \(v\) are connected due to their affiliations with the \(k\)-th factor.

**Node Factorization:** Equation 1 can be further augmented as follows:

\[_{uv}=_{k}^{K}_{ukv},\ _{ukv}(_{k}z_{uk}z_{vk})\] (2)

where \(_{ukv}\) represents how often nodes \(u\) and \(v\) interact due to their affiliations with the \(k\)-th factor. To represent how often node \(u\) is affiliated with the \(k\)-th factor, we further introduce the latent count \(_{uk}=_{v u}_{ukv}\). Then, we can soft assign node \(u\) to multiple factors in \(\{k:_{uk}\} 1\), or hard assign node \(u\) to a single factor using \(*{arg\,max}_{k}(_{uk})\). However, our experiments show that soft assignment method results in significant overlap among node sets from different factor group, diminishing the distinctiveness. Note that previous study addressed a similar issue by selecting the top-k most attended regions (Kakogeorgiou et al., 2022). Thus, we choose the hard assign strategy to factorize the graph node set \(V\) graph into factor-specific node sets \(\{V^{(1)},V^{(2)},,V^{(K)}\}\).

**Edge Factorization:** To create factor-specific edges \(A^{(k)}\) for a factor-specific node set \(V^{(k)}\), a straightforward method involves removing all external nodes connected to other factor groups. This can be defined as:

\[A^{(k)}_{uv}=\{A_{uv},\ \,u,v V^{(k)};\ u,v [1,N];\\ 0,\ \ \,u,v V^{(k)};\ u,v[1,N]..\] (3)

Besides, the global graph edge \(A\) can also be factorized into positive-weighted edges (He et al., 2022b) for each latent factor as:

\[A^{(k)}_{uv}=A_{uv}z_{uk}z_{vk})}{_{ k^{}}(_{k^{}}z_{uk^{}}z_{vk^{}})}; \ \ k[1,K],u,v[1,N].\] (4)

Applying Equation 4 to all pairs of nodes yields weighted adjacency matrices \(\{A^{(k)}\}_{k=1}^{k}\), with \(A^{(k)}\) corresponding to latent factor \(z_{k}\). Note that \(A^{(k)}\) has the same dimension as \(A\) and Equation 4

Figure 2: The overview of proposed DiGGR’s computation graph. The input data successively passes three modules described in Sections 3.2 and 3.3: Latent Factor Learning, Graph Factorization, and Disentangled Graph Mask Autoencoder. Graph information will be first processed through Latent Factor Learning and Graph Factorization, the former processed the input graph to get the latent factor \(z\); the latter performs graph factorization via \(z\), such that in each factorized subgraph, nodes exchange more information with intensively interacted neighbors. Hence, during the disentangled graph masking phase, we will individually mask each factorized subgraph to enhance the disentanglement of the obtained node representations.

presents a trainable weight for each edge, which can be jointly optimized through network training, showcasing an advantage over Equation 3 in this aspect. Therefore, we apply Equation 4 for edge factorization.

Variational Inference:The latent factor variable \(z\) determines the quality of node and edge factorization, so we need to approximate its posterior distribution. Denoting \(z_{u}=(z_{u1},...,z_{uK}),z_{u}_{+}^{K}\), which measures how strongly node \(u\) is affiliated with all the \(K\) latent factors, we adopt a Weibull variational graph encoder [22, He et al., 2022b]:

\[q(z_{u} A,X)=(k_{u},_{u}),\ \ (k_{u},_{u})=_{}(A,X), u[1,N]\] (5)

where \(_{}()\) stands for graph neural networks, and we select a two-layer Graph Convolution Networks (_i.e._, GCN ) for our models; \(k_{u},_{u}_{+}^{K}\) are the shape and scale parameters of the variational Weibull distribution, respectively. The latent variable \(z_{u}\) can be conveniently reparameterized as:

\[z_{u}=_{u}(-(1-))^{1/k_{u}},\ \ (0,1).\] (6)

The optimization objective of latent factor learning phase can be achieved by maximizing the evidence lower bound (ELBO) of the log marginal likelihood of edge \( p(A)\), which can be computed as:

\[_{}=_{q(Z A,X)}[ p(A Z )]-_{u=1}^{N}_{q(_{u} A,X)}[ _{u} A,X)}{p(_{u})}]\] (7)

where the first term is the expected log-likelihood or reconstruction error of edge, and the second term is the Kullback-Leibler (KL) divergence that constrains \(q(z_{u})\) to be close to its prior \(p(z_{u})\). The analytical expression for the KL divergence and the straightforward reparameterization of the Weibull distribution simplify the gradient estimation of the ELBO concerning the decoder parameters and other parameters in the inference network.

### Disentangled Grpah Masked Autoencoder

With the latent factor learning phase discussed in 3.2, the graph can be factorized into a series of factor-specific subgraphs \(\{G_{1},G_{2},...,G_{K}\}\) via the latent factor \(z\). To incorporate the disentangled information encapsulated in \(z\) into the graph masked autoencoder, we proposed Disentangled Graph Masked Autoencoder in this section. Specifically, this section will first introduce the latent factor-wise GMAE and the graph-level GMAE.

#### 3.3.1 Latent Factor-wise Grpah Masked Autoencoder

To capture disentangled patterns within the latent factor \(z\), for each latent subgraph \(_{k}=(V^{(k)},A^{(k)},X^{(k)})\), the latent factor-wise GMAE can be described as:

\[H_{d}^{(k)}=_{}(A^{(k)},^{(k)}),^{d}= _{}(A,H_{d}).\] (8)

where \(^{(k)}\) is the masked node feature matrix for the \(k\)-th latent factor, and \(^{d}\) denotes the reconstructed node features. \(_{}(.)\) and \(_{}(.)\) are the graph encoder and decoder, respectively; \(H_{d}^{(k)}^{N D}\) are factor-wise hidden representations, and \(H_{d}=H_{d}^{(1)} H_{d}^{(2)} H_{d}^{(K)}\). After the concatenation operation \(\) in feature dimension, the multi factor-wise hidden representation becomes \(H_{d}^{N(K D)}\), which is used as the input of \(_{}(.)\).

Regarding the mask operation, we uniformly random sample a subset of nodes \(^{(k)} V^{(k)}\) and mask each of their features with a mask token, such as a learnable vector \(X_{[M]}^{d}\). Thus, the node feature in the masked feature matrix can be defined as:

\[_{i}^{(k)}=\{X_{[M]};\ \ v_{i}^{(k)}\ ;\\ X_{i}\ \ ;\ \ v_{i}^{(k)}..\] (9)

The objective of latent factor-wise GMAE is to reconstruct the masked features of nodes in \(^{(k)}\) given the partially observed node signals \(^{(k)}\) and the input adjacency matrix \(A^{(k)}\). Another crucial component of the GMAE is the feature reconstruction criterion, often used in language as cross-entropy error  and in the image as mean square error . However,texts and images typically involve tokenized input features, whereas graph autoencoders (GAE) do not have a universal tokenizer. We adopt the scored cosine error of GraphMAE [Hou et al., 2022] as the loss function. Generally, given the original feature \(X^{(k)}\) and reconstructed node feature \(^{(k)}\), the defined SCE is:

\[_{}=|}}{}(1- ^{T}_{i}^{d}}{\|X_{i}\|\|_{i}^{d}\|})^ {},\ \  1\] (10)

where \(=^{(1)}^{(2)}...^{(K)}\) and Equation 10 are averaged over all masked nodes.The scaling factor \(\) is a hyper-parameter adjustable over different datasets. This scaling technique could also be viewed as adaptive sample reweighting, and the weight of each sample is adjusted with the reconstruction error. This error is also famous in the field of supervised object detection as the focal loss [Lin et al., 2017].

Graph-level Graph Mask Autoencoder:For the node classification task, we have integrated graph-level GMAE into DiGGR. We provide a detailed experimental analysis and explanation for this difference in Appendix A.1.2. The graph-level masked graph autoencoder is designed with the aim of further capturing the global patterns, which can be designed as:

\[H_{g}=_{}(A,),\ \ ^{g}=_{}(A,H_{g}).\] (11)

\(\) is the masked node feature matrix, whose mask can be generated by uniformly random sampling a subset of nodes \( V\), or obtained by concatenating the masks of all factor-specific groups \(=^{(1)}^{(2)}...^{(K)}\). The global hidden representation encoded by \(_{}(.)\) is \(H_{g}\), which is then passed to the decoder. Similar to Equation 10, we can define the graph-level reconstruct loss as:

\[_{}=|}}{}( 1-^{T}_{i}^{g}}{\|X_{i}\|\|_{i}^{g}\|} )^{},\ \  1.\] (12)

which is averaged over all masked nodes.

### Joint Training and Inference

Benefiting from the effective variational inference method, the proposed latent factor learning and disentangled graph masked autoencoder can be jointly trained in one framework. We combine the aforementioned losses with three mixing coefficient \(_{d}\), \(_{g}\) and \(_{z}\) during training, and the loss for joint training can be written as

\[=_{d}_{}+_{g}_{}+_{z}_{}.\] (13)

Since Weibull distributions have easy reparameterization functions, these parameters can be jointly trained by stochastic gradient descent with low-variance gradient estimation. We summarize the training algorithm at Algorithm 1 in Appendix A.4. For downstream applications, the encoder is applied to the input graph without any masking in the inference stage. The generated factor-wise node embeddings \(H_{d}\) and graph-level embeddings \(H_{g}\) can either be concatenated in the feature dimensions or used separately. The resulting final representation \(H\) can be employed for various graph learning tasks, such as node classification and graph classification. For graph-level tasks, we use a non-parameterized graph pooling (readout) function, _e.g._, MaxPooling and MeanPooling to obtain the graph-level representation.

**Time and space complexity:** Let's recall that in our context, \(N\), \(M\), and \(K\) represent the number of nodes, edges, and latent factors in the graph, respectively. The feature dimension is denoted by \(F\), while \(L_{1}\), \(L_{2}\), \(L_{3}\), and \(L_{4}\) represent the number of layers in the latent factor learning encoder, the latent factor-wise GMAE's encoder, the graph-level GMAE's encoder, and the decoder respectively. In DiGGR, we constrain the hidden dimension size in latent factor-wise GMAE's encoder to be \(1/K\) of the typical baseline dimensions. Consequently, the time complexity for training DiGGR can be expressed as \(O((L_{1}+L_{2}+L_{3})MF+(L_{1}+L_{2}/K+L_{3})NF^{2}+N^{2}F+L_{4}NF^{2})\), and the space complexity is \(O((L_{1}+L_{2}+L_{3}+L_{4})NF+KM+(L1+L2/K+L3+L_{4})F^{2})\), with \(O((L_{1}+L_{2}/K+L_{3}+L_{4})F^{2})\) attributed to model parameters. We utilize the Bayesian factor model in our approach to reconstruct edges. Its time complexity aligns with that of variational inference in SeeGera Li et al. [2023b], predominantly at \(O(N^{2}F)\); Therefore, the complexity of DiGGR is comparable to previous works.

## 4 Experiments

We compare the proposed self-supervised framework DiGGR against related baselines on two fundamental tasks: unsupervised representation learning on _node classification_ and _graph classification_. We evaluate DiGGR on 11 benchmarks. For node classification, we use 3 citation networks (Cora, Citeseer, Pubmed ), and protein-protein interaction networks (PPI) . For graph classification, we use 3 bioinformatics datasets (MUTAG, NCI1, PROTEINS) and 4 social network datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and COLLAB). The specific information of the dataset and the hyperparameters used by the network are listed in the Appendix A.2 in table 5 and 6. We also provide the detailed experiment setup in Appendix A.2 for node classification (4.1) and graph classification (4.2)

### Node Classification

The baseline models for node classification can be divided into three categories: \(i)\) supervised methods, including GCN , DisenGCN, VEPM and GAT ; \(ii)\) contrastive learning methods, including MVGRL , InfoGCL , DGI , GRACE , BGRL  and CCA-SSG ; \(iii)\) generative learning methods, including GraphMAE , GraphMAE2, Bandana, GiGaMAE, SeeGera, GAE and VGAE . The node classification results were listed in Table 1. DiGGR demonstrates competitive results on the provided dataset, achieving results comparable to those of supervised methods.

### Graph Classification

Baseline ModelsWe categorized the baseline models into four groups: \(i)\) supervised methods, including GIN , DiffPool and VEPM, \(ii)\) classical graph kernel methods: Weisfeiler-Lehman sub-tree kernel (WL)  and deep graph kernel (DGK) ; \(iii)\) contrastive learning methods, including GCC , graph2vec , Infograph , GraphCL , JOAO , MVGRL , and InfoGCL ; \(4)\) generative learning methods, including graph2vec , sub2vec , node2vec , GraphMAE , GraphMAE2, GAE and VGAE . Per graph classification research tradition, we report results from previous papers if available.

   Methods & Cora & Citeseer & Pubmed & PPI \\  GCN  & 81.50 & 70.30 & 79.00 & 75.70 \(\) 0.10 \\ GAT  & 83.00 \(\) 0.70 & 72.50 \(\) 0.70 & 79.00 \(\) 0.30 & 97.30 \(\) 0.20 \\ DisenGCN & 83.7 & 73.4 & 80.5 & - \\ VEPM & 84.3 \(\) 0.1 & 72.5 \(\) 0.1 & 82.4 \(\) 0.2 & - \\  MVGRL  & 83.50 \(\) 0.40 & 73.30 \(\) 0.50 & 80.10 \(\) 0.70 & - \\ InfoGCL  & 83.50 \(\) 0.30 & 73.50 \(\) 0.40 & 79.10 \(\) 0.20 & - \\ DGI  & 82.30 \(\) 0.60 & 71.80 \(\) 0.70 & 76.80 \(\) 0.60 & 63.80 \(\) 0.20 \\ GRACE  & 81.90 \(\) 0.40 & 71.20 \(\) 0.50 & 80.60 \(\) 0.40 & 69.71 \(\) 0.17 \\ BGRL  & 82.70 \(\) 0.60 & 71.10 \(\) 0.80 & 79.60 \(\) 0.50 & 73.63 \(\) 0.16 \\ CCA-SSG  & 84.20 \(\) 0.40 & 73.10 \(\) 0.30 & 81.00 \(\) 0.40 & 73.34 \(\) 0.17 \\  GAE  & 71.50 \(\) 0.40 & 65.80 \(\) 0.40 & 72.10 \(\) 0.50 & - \\ VGAE  & 76.30 \(\) 0.20 & 66.80 \(\) 0.20 & 75.80 \(\) 0.40 & - \\ Bandana  & 84.62 \(\) 0.37 & 73.60 \(\) 0.16 & **83.53**\(\) 0.51 & - \\ GiGaMAE & 84.72 \(\) 0.47 & 72.31 \(\) 0.50 & - & - \\ SEEGERA  & 84.30 \(\) 0.40 & 73.00 \(\) 0.80 & 80.40 \(\) 0.40 & - \\ GraphMAE & 84.20 \(\) 0.40 & 73.40 \(\) 0.40 & 81.10 \(\) 0.40 & 74.50 \(\) 0.29 \\ GraphMAE2 & 84.50 \(\) 0.60 & 73.40 \(\) 0.30 & 81.40 \(\) 0.50 & - \\ 
**DiGGR** & **84.96**\(\) 0.32 & **73.98**\(\) 0.27 & 81.30 \(\) 0.26 & **78.30**\(\) 0.71 \\   

Table 1: Experiment results for node classification. Micro-F1 score is reported for PPI, and accuracy for other datasets. The best unsupervised method scores in each dataset are highlighted in bold.

Performance ComparisonThe graph classification results are presented in Table 2. In general, we find that DiGGR gained the best performance among other baselines on five out of seven datasets, while achieving competitive results on the other two datasets. The performance of DiGGR is comparable to that of supervised learning methods. For instance, the accuracy on IMDB-B and IMDB-M surpasses that of GIN and DiffPool. Moreover, within the reported datasets, our method demonstrates improved performance compared to random mask methods like GraphMAE, particularly on the IMDB-M, COLLAB, and PROTEINS datasets. This underscores the effectiveness of the proposed method.

### Exploratory Studies

Visualizing latent representationsTo examine the influence of the learned latent factor on classification results, we visualized the latent disentangled factor \(z\), which reflects the node-factor affiliation, and the hidden representation \(H\) used for classification. MUTAG is selected as the representative for classification benchmarks. We encodes the representations into 2-D space via t-SNE (Van der Maaten and Hinton, 2008). The result is shown in Figure 3(a), where each node is colored according to its node labels. The clusters in Figure 3(a) still exhibit differentiation in the absence of label supervision, suggesting that \(z\) obtained through unsupervised learning can enhance node information and offer a guidance for the mask modeling. We then visualize the hidden representation used for classification tasks, and color each node according to the latent factor to which it belongs. The results are depicted in Figure 3(b), showcasing separability among different color clusters. This illustrates the model's ability to extract information from the latent factor, thereby enhancing the quality of the learned representations.

Task-relevant factorsTo assess the statistical correlation between the learned latent factor and the task, we follow the approach in (He et al., 2022) and compute the Normalized Mutual Information (NMI) between the nodes in the factor label and the actual node labels. NMI is a metric that ranges from 0 to 1, where higher values signify more robust statistical dependencies between two random variables. In the experiment, we utilized the MUTAG dataset, comprising 7 distinct node types, and the NMI value we obtained was 0.5458. These results highlight that the latent factors obtained through self-supervised training are meaningful for the task, enhancing the correlation between the inferred latent factors and the task.

Disentangled representationsTo assess DiGGR's capability to disentangle the learned representation for downstream task, we provide a qualitative evaluation by plotting the correlation of the

   Methods & IMDB-B & IMDB-M & MUTAG & NCI1 & REDDIT-B & PROTEINS & COLLAB \\  GIN & 75.1\(\) 5.1 & 52.3 \(\) 2.8 & 89.4 \(\) 5.6 & 82.7 \(\) 1.7 & 92.4 \(\) 2.5 & 76.2 \(\) 2.8 & 80.2 \(\) 1.9 \\ DiffPool & 72.6 \(\) 3.9 & - & 85.0 \(\) 10.3 & - & 92.1 \(\) 2.6 & 75.1 \(\) 3.5 & 78.9 \(\) 2.3 \\ VEPM & 76.7 \(\) 3.1 & 54.1 \(\) 2.1 & 93.6 \(\) 3.4 & 83.9 \(\) 1.8 & 90.5 \(\) 1.8 & 80.5 \(\) 2.8 & - \\  WL & 72.30 \(\) 3.44 & 46.95 \(\) 0.46 & 80.72 \(\) 3.00 & 80.31 \(\) 0.46 & 68.82 \(\) 0.41 & 72.92 \(\) 0.56 & - \\ DGK & 66.96 \(\) 0.56 & 44.55 \(\) 0.52 & 87.44 \(\) 2.72 & 80.31 \(\) 0.46 & 78.04 \(\) 0.39 & 73.30 \(\) 0.82 & 73.09 \(\) 0.25 \\  Infograph & 73.03 \(\) 0.87 & 49.69 \(\) 0.53 & 89.01 \(\) 1.13 & 76.20 \(\) 1.06 & 82.50 \(\) 1.42 & 74.44 \(\) 0.31 & 70.65 \(\) 1.13 \\ GraphCL & 71.14 \(\) 0.44 & 48.58 \(\) 0.67 & 86.80 \(\) 1.34 & 77.87 \(\) 0.41 & **89.53**\(\) 0.84 & 74.39 \(\) 0.45 & 71.36 \(\) 1.15 \\ JOAO & 70.21 \(\) 3.08 & 49.20 \(\) 0.77 & 87.35 \(\) 1.02 & 78.07 \(\) 0.47 & 85.29 \(\) 1.35 & 74.55 \(\) 0.41 & 69.50 \(\) 0.36 \\ GCC & 72.0 & 49.4 & - & - & 89.9 & - & 78.9 \\ MVGRL & 74.20 \(\) 0.70 & 51.20 \(\) 0.50 & 89.70 \(\) 1.10 & - & 84.50 \(\) 0.60 & - & - \\ InfoGCL & 75.10 \(\) 0.90 & 51.40 \(\) 0.80 & **91.20**\(\) 1.30 & 80.20 \(\) 0.60 & - & - & 80.00 \(\) 1.30 \\  graph2vec & 71.10 \(\) 0.54 & 50.44 \(\) 0.87 & 83.15 \(\) 9.25 & 73.22 \(\) 1.81 & 75.78 \(\) 1.03 & 73.30 \(\) 2.05 & - \\ sub2vec & 55.3 \(\) 1.5 & 36.7 \(\) 0.8 & 61.1 \(\) 15.8 & 52.8 \(\) 1.5 & 71.5 \(\) 0.4 & 53.0 \(\) 5.6 & - \\ node2vec & - & - & 72.6 \(\) 10.2 & 54.9 \(\) 1.6 & - & 57.5 \(\) 3.6 & - \\ GAE & 52.1 \(\) 0.2 & - & 84.0 \(\) 0.6 & 73.3 \(\) 0.6 & 74.8 \(\) 0.2 & 74.1 \(\) 0.5 & - \\ VGAE & 52.1 \(\) 0.2 & - & 84.4 \(\) 0.6 & 73.7 \(\) 0.3 & 74.8 \(\) 0.2 & 74.8 \(\) 0.2 & - \\ GraphMAE & 75.52 \(\) 0.66 & 51.63 \(\) 0.52 & 88.19 \(\) 1.26 & 80.40 \(\) 0.30 & 88.01 \(\) 0.19 & 75.30 \(\) 0.39 & 80.32 \(\) 0.46 \\ GraphMAE & 73.88 \(\) 0.53 & 51.80 \(\) 0.60 & 86.63 \(\) 1.33 & 78.56 \(\) 0.26 & 76.84 \(\) 0.21 & 74.86 \(\) 0.34 & 77.59 \(\) 0.22 \\ 
**DiGGR** & **77.68**\(\) 0.48 & **54.77**\(\) 2.63 & 88.72 \(\) 1.03 & **81.23**\(\) 0.40 & 88.19 \(\) 0.28 & **77.40**\(\) 0.05 & **83.76**\(\) 3.70 \\   

Table 2: Experiment results in unsupervised representation learning for graph classification. We report accuracy (\(\%\)) for all datasets. The optimal outcomes for methods, excluding supervised approaches (GIN and DiffPool), on each dataset are emphasized in bold.

node representation in Figure 4. The figure shows the absolute values of the correlation between the elements of 512-dimensional graph representation and representation obtained from GraphMAE and DiGGR, respectively. From the results, we can see that the representation produced by GraphMAE exhibits entanglement, whereas DiGGR's representation displays a overall block-level pattern, indicating that DiGGR can capture mutually exclusive information in the graph and disentangle the hidden representation to some extent. Results for more datasets can be found in Appendix A.3.

**Why DiGGR works better:** To validate that disentangled learning can indeed enhance the quality of the representations learned by GMAE, we further conduct quantitative experiments. The Normalized Mutual Information (NMI) is used to quantify the disentangling degree of different datasets. Generally, the NMI represents the similarity of node sets between different factor-specific graphs, and the _lower NMI suggests a better-disentangled degree_ with lower similarity among factor-specific graphs. The NMI between latent factors and the corresponding performance gain (compared to GraphMAE) are shown in the Table.3. As the results show, DiGGR's performance improvement has a positive correlation with disentangled degree, where the better the disentangled degree, the more significant the performance improvement. For methods relying on Non-probabilistic Factor Learning, the NMI tends to approach 1. This is attributed to the challenges faced by the factor learning module in converging, thereby hindering the learning of distinct latent factors. The presence of confused latent factors offers misleading guidance for representation learning, consequently leading to decreased performance.

## 5 Conclusions

In this paper, we propose DiGGR (Disentangled Generative Graph Representation Learning), designed to achieve disentangled representations in graph masked autoencoders by leveraging latent disentangled factors. In particular, we achieve this by two steps: 1) We utilize a probabilistic graph generation model to factorize the graph via the learned disentangled latent factor; 2) We develop a Disentangled Graph Masked Autoencoder framework, with the aim of integrating the disentangled information into the representation learning of Graph Masked Autoencoders. Experiments demonstrate that our model can acquire disentangled representations, and achieve favorable results on downstream tasks.

    & Dataset & RDT-B & MUTAG & NCI-1 & IMDB-B & PROTEINS & COLLAB & IMDB-M \\   & NMI & 0.95 & 0.90 & 0.89 & 0.82 & 0.76 & 0.35 & 0.24 \\  & ACC Gain & + 0.18\% & + 0.53\% & + 0.83\% & + 2.16\% & + 2.1\% & + 3.44\% & + 3.14\% \\  Non-probabilistic & NMI & 1.00 & 1.00 & 0.80 & 1.00 & 0.60 & 1.00 & 0.94 \\ Factor Learning & ACC Gain & -2.23\% & -2.02\% & -0.45\% & -0.80\% & -2.15\% & -3.00\% & -0.11\% \\   

Table 3: The NMI between the latent factors extracted by DiGGR and Non-probabilistic factor learning method across various datasets, and its performance improvement compared to GraphMAE, are examined. A lower NMI indicates a more pronounced disentanglement between factor-specific graphs, resulting in a greater performance enhancement.