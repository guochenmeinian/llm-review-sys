# FlexCap: Describe Anything in Images in Controllable Detail

Debidatta Dwibedi

Google Deepmind

debidatta@google.com &Vidhi Jain

Carnegie Mellon University

vidhij@andrew.cmu.edu &Jonathan Tompson

Google Deepmind

tompson@google.com &Andrew Zisserman

Google Deepmind

zisserman@google.com &Yusuf Aytar

Google Deepmind

yusufaytar@google.com

Work done as a student researcher at Google Deepmind.Google Deepmind

###### Abstract

We introduce FlexCap, a vision-language model that generates region-specific descriptions of varying lengths. FlexCap is trained to produce length-conditioned captions for input boxes, enabling control over information density, with descriptions ranging from concise object labels to detailed captions. To achieve this, we create large-scale training datasets of image region descriptions with varying lengths from captioned web images. We demonstrate FlexCap's effectiveness in several applications: first, it achieves strong performance in dense captioning tasks on the Visual Genome dataset. Second, we show how FlexCap's localized descriptions can serve as input to a large language model to create a visual question answering (VQA) system, achieving state-of-the-art zero-shot performance on multiple VQA benchmarks. Our experiments illustrate FlexCap's utility for tasks including image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io.

## 1 Introduction

How does one describe the world around us, not just in broad strokes but with the ability to zoom in and out, capturing both the grand scene and the minute details? Imagine pointing at a bustling market scene and asking, "What's happening here?" and receiving a vivid description, not just of the market as a whole, but also a detailed account of the interactions between vendors and customers, the vibrant colors of the goods on display, or even a specific item held by a passerby. This ability to controllably focus and describe visual content is what we call _flexible captioning_.

Traditional image captioning models, while adept at capturing the gist of an image, often struggle to pinpoint specific objects or attributes. On the other hand, object detection systems excel at localizing elements but may lack the vocabulary to describe them comprehensively. Dense captioning  attempts to bridge this gap by generating captions for multiple regions, but its expressiveness is limited by existing datasets. In this work, we introduce a model called FlexCap that bridges the gap between holistic image understanding and localized inquiry. This enables the generation of captions that are both spatially precise and semantically rich (see Fig. 1 (left)) by specifying a region of interest and the desired level of detail in terms of number of words in the predicted caption. This allows us to integrate the strengths of image captioning, object detection, and dense captioning into one model.

To be able to train such a model, we require a dataset of images where many boxes are labeled with short and long descriptions. We propose a method to generate triplets of (i) image, (ii) a proposed region within the image, and (iii) a caption of a particular length, by using an open-vocabulary object detector to label regions from captions of an image-text pair dataset. We demonstrate this at two

[MISSING_PAGE_EMPTY:2]

<s> dog playing with a frisbee <e> which share a prefix <s> dog. While another caption for the same box, <s> brown dog <e>, does not share a prefix with captions beginning with <s> dog <e>. After averaging this metric across all images in our Localized Captions Dataset from WebLI (introduced later in Section 3), we found that 30.8% of all caption pairs share a prefix. Using a length conditioning token instead of <s>, the probability of prefix matching decreases from 30.8% to 11.1%. The length conditioning helps the model in distinguishing between captions with the same prefix while also providing the model with a novel capability during inference.

**Architecture.** Our objective is to train a model that takes as input an image and a region of interest and outputs a description _of a desired length_ of the region spanned by the box. We present FlexCap's architecture in Figure 2. The model takes an image, the coordinates of a bounding box and the conditioning tokens as input, and outputs a textual description of visual contents within the specified bounding box. Our model mainly consists of an image encoder (i.e. SOViT-400M/14 ) and a transformer-based text-decoder. We pass the image through the vision model to produce outputs of dimensions \(n d\) (where \(n\) is the number of patches and \(d\) is the embedding size). We pass the bounding box coordinates (of dimensions \(1 4\)) through a linear layer to produce the coordinate features (of dimension \(1 d\)). Both vision features and normalized bounding box features are concatenated to form input of dimension \((n+1) d\) to a text decoder. The text decoder consists of a stack of \(L\) Transformer layers. We use a decoder-only architecture in which all the vision and bounding box tokens remain unmasked but the text tokens are masked in a causal manner to enable next-word prediction training. We add all the vision tokens and bounding box coordinate tokens so that the text decoder can access all of the visual context in the image and the exact bounding box location. In this work, we train a text decoder composed of 12 self-attention transformer layers with a dimensionality of 768 and 12 attention heads.

In total, FlexCap has 590M parameters with 428M comprised of the image encoder (SOViT) and the remaining parameters in the text decoder. A linear layer transforms the 1152-dimensional output from the vision encoder into a 768-dimensional input for the text decoder. We initialize the vision encoder with SigLIP  weights, which is a contrastively trained image encoder using web-scale vision-text pairs from WebLI. We do not freeze the vision encoder during training.

**Loss.** We train FlexCap to predict the next token of the text. The text tokens are prefixed with the desired length of the caption and appended with an end of sentence token <e> to indicate the end of the caption. The target text tokens are obtained by shifting the padded text by 1. This is common training methodology for training generative language models like GPT  or SimVLM . The loss is a classification loss over all the words present in the vocabulary. The loss is ignored over the padded tokens that are used to keep the size of the outputs same for all the captions in the batch. Formally, we represent a data sample as a triplet \(T=(X,B,W)\) consisting of image \(X\), bounding box \(B\) and captions \(W\), where \(W=\{,w_{1},w_{2},...w_{k}\}\). To enable batch training, we pad the tokenized captions to a fixed size \(M\). For a given data triplet, our objective is to maximize the following log-likelihood. \(l(X,B,W)=_{i=1}^{M} p(w_{i}|w_{<i},X,B)\). Assume that we have a dataset

Figure 2: **Architecture and Training Setup. FlexCap outputs a length-controlled caption of the object contained in the bounding box by taking (left) an image, (middle) coordinates of a bounding box and (right) the length prefix and caption, as inputs. The training loss is the standard next-word prediction loss that is used to train image captioning models.**

\(=\{T_{1},T_{2},...,T_{N}\}\). The overall loss function is:

\[L(D)=_{j=1}^{N}l(X_{j},B_{j},W_{j})=_{j=1}^{N}_{i=1}^{M} p((w_{j} )_{i}|(w_{j})_{<i},X_{j},B_{j})\]

**Implementation.** We implement this model using the JAX framework . We train the entire model for about \(400K\) steps using the AdamW optimizer with a cosine learning rate schedule. The maximum learning rate is \(1.6 10^{-4}\) with 10K warm-up steps. We use a weight decay of \(0.05\). We train with a batch size of \(4096\) and image resolution of \(224 224\). We use a maximum text sequence length of \(32\) tokens. For each image in the batch, we sample a maximum of 8 bounding boxes.

**Inference.** At inference time, we provide an image, the target bounding box, and the desired length as input. We then decode in an auto-regressive manner till the end of caption token <e> is encountered or the maximum number of decoding steps is reached. While we can use standard sampling techniques used in text-generation like beam search, temperature sampling, or nucleus sampling  to generate multiple captions. We use greedy decoding in all experiments unless otherwise stated.

## 3 Localized Captions Dataset

In order to train the FlexCap model, we build a large scale dataset of image region descriptions of varying lengths. In the following section we describe how we produce such a dataset from existing image-text paired datasets. We leverage the web-based image-caption pairs datasets (like WebLI  and YFCC100M ) to create a localized captions dataset. The dataset generation pipeline is shown in Figure 3. First, we create _text queries_ using n-grams from the caption of the image: e.g. "dog","brown dog", "brown dog playing with a disc". We specifically create n-grams where \(n=\{1,2,,8\}\) and then filter out incomplete captions like "with a red", "dog playing with". More details about the filtering step are mentioned in the appendix. Then we use the filtered n-grams as _text queries_ for pre-trained region proposal models (i.e. OWL-ViT ) to extract boxes and select text-box pairs based on the similarity score (\(>0.1\)). Multiple n-grams may match for a box, and this results in several ways of describing a box in the image as shown in Col. 4 in Figure 3.

**WebLI.** This data collection technique on the WebLI dataset results in 32 billion image-box-caption triplets from 2 billion images without requiring new manual annotation. Our captions show a rich vocabulary that is close to common language used to describe objects in the context of an image. If we use MS-COCO's vocabulary then all humans in the dataset would get labeled as _person_. However by building our vocabulary in a bottom-up manner we end up with captions that contain more informative words such as _baby_, _nurse_, _policeman_, _firefighter_, or _baseball player_ to describe the _person_ class. Please refer to the appendix for details of dataset statistics and examples.

**YFCC100M.** We also create a localized captions dataset using YFCC100M images. Specifically we use the same 14M images as the CLIP paper. The dataset creation method results in \( 11\)M images with at least one valid box. On average each image has \(20\) boxes, making the size of this dataset \( 0.2\)B image-box-caption triplets. The number of YFCC100M triplets is 160 times smaller than the localized captions dataset created from WebLI.

As both OWL-ViT and the CLIP subset of YFCC100M are publicly available, the resulting localized captions dataset can be generated with open-source models and public datasets. Since the WebLI dataset is not publicly available yet, YFCC100M triplet-dataset can serve as a reproducible benchmark. Concurrently large-scale grounded image-text dataset generation pipelines have also been proposed in  and .

## 4 Experiments

### Correctness and Compliance of Generated Captions

**Correctness.** In this experiment, we solely evaluate the recognition capabilities of our model in a zero-shot manner. We use the region classification task [53; 61] on the MS-COCO dataset to assess how well our model recognizes objects at different scales and under occlusion. In this task, the image and the ground truth bounding box are provided as input to the model such that it produces a short description of what is contained in the bounding box. Our region classification pipeline (Figure 4)passes the input image (of size \(448 448\)) through FlexCap which generates 20 captions each for 4 caption lengths (1, 2, 3, 4) via nucleus sampling. These captions are then mapped to object class names using CLIP's  text encoder. By comparing the mean of the text embeddings of predicted captions with those of ground truth class names, we obtain the classification scores. We report the results in Table 1a. We find FlexCap outperforms contrasatively trained approaches used for region classification. Furthermore, we observe a significant boost ( 13% mAP) in performance by producing multiple captions for each box. One reason for the boost in performance may be directly comparing text embeddings to produce classification scores as compared to baselines which use dot product of image and text embeddings.

**Compliance.** In Figure 5, we show qualitative examples of the FlexCap model producing different length captions for the same box. Note how the model progressively adds more information about the object by incorporating context in the longer sentences (_in the jungle_), attributes (_pink flamingo kite_), and alternative nouns (_chevy_, _feline_). We also measure how well our model complies to the desired caption length. To do so, we take 1000 images from the MS-COCO dataset and use a random object in the image to produce descriptions with different lengths. We report the average length of the predicted caption, and the fraction of times the predicted caption has a length equal to the target length in Table 1b. We find FlexCap's outputs are mostly compliant with the target length.

### Visual Question Answering

Visual question answering (VQA) often requires visually grounded rich semantic understanding of the content at multiple levels of granularity depending on the question. These properties make VQA a great test-bed for our method which can generate dense spatially grounded information on visual content with desired semantic complexity.

**FlexCap-LLM.** In Figure 1 (right), we show how we use FlexCap with an LLM to solve visual questions. First, we convert an image to a sequence of localized descriptions that describe the image in terms of the objects and regions present in the image. To do so, we need region proposals. We use OWL-ViTv2  to localize important objects and regions in an image. We keep the top 128

Figure 4: **Evaluating open-vocabulary outputs from FlexCap using the CLIP  text encoder.**

Figure 3: **Dataset Generation. We use OWL-ViT to generate a dataset of triplets of image, bounding box and captions from a web-scale dataset of noisy image-text pairs. Increasing levels of richness in captions is captured through different length descriptions for each box.**

bounding boxes by their _objectness_ scores. We then use FlexCap to describe each box in the image in the context of the entire image. In order to produce holistic descriptions, we use multiple prefixes for each region. These prefixes are a combination of length conditioning token and some initial text. We add the boxes and their descriptions to a _text preamble_ as context to the LLM (see Figure 1) that defines the setup where we are using an LLM to answer questions about an image. In all the experiments, we use PALM2-S model  as the LLM of choice. We refer to this end-to-end system that takes an image and a question to output the answer as _FlexCap-LLM_.

To adapt the base FlexCap to have improved detection skills, output longer sentences, and identify OCR, we co-train FlexCap for 25k more steps on detection (COCO, VOC, OpenImages, LVIS), captioning (COCO Captions, Visual Genome) and OCR datasets (WebLI). For image captioning datasets, we use the bounding box that covers the whole image. We find this co-training step useful for downstream tasks using the LLM.

Hence we evaluate the effectiveness of FlexCap-LLM on several image VQA benchmarks such as OKVQA , VQAv2 , GQA , and VizWiz , and video question answering benchmarks such as MSRVTT  and MSVD . Diverse characteristics of these datasets helps gaining better insight on FlexCap's capabilities. We report the commonly used accuracy metric for each dataset.

**Image Question Answering.** First we evaluate FlexCap-LLM on VQAv2, GQA, OKVQA and VizWiz image VQA benchmarks in a zero-shot setting, meaning that our approach is not trained with the task or the corresponding dataset. The results on these benchmarks are presented in Table 2.

_Standard VQA._ The VQAv2 dataset is a standard for evaluating the performance of visual question-answering systems. In Table 2a, we present the results of our evaluation of FlexCap-LLM on this dataset. We find that FlexCap-LLM outperforms other zero-shot baselines, such as BLIP-2. This performance is achieved by providing object and region level information to LLMs without requiring multi-modal fine-tuning.

_Compositional VQA._ The GQA dataset is for evaluating the performance on complex compositional questions. As FlexCap produces information for multiple visual elements in the scene with their corresponding locations, it is quite well-suited for questions on compositional understanding of the image. On this benchmark, as shown in Table 2b, FlexCap-LLM outperforms all the recent baselines except InstructBLIP .

Table 1: **FlexCap’s outputs are accurate and length compliant.**

Figure 5: **Examples of length controlled captions generated by FlexCap. Note that attributes (“pink flamingo kite”) and context (“in the jungle”) are generated as the length increases.**

VQA with External Knowledge_. OKVQA dataset is particularly designed for evaluating the ability to answer questions about images that require external knowledge which is not readily available on the image. Hence it requires multiple levels of understanding of the content, and reasoning with that information, which is well-suited for applying FlexCap. In Table 2c we show our performance on OKVQA is superior to strong baselines such as Flamingo and ViperGPT which highlights the effectiveness of the mix of generic and specific descriptions generated by FlexCap. Unlike other baselines which use the question, FlexCap generates captions without having access to the question.

_VQA with atypical images_. We also evaluate on VizWiz, which contains visual questions asked by people who are visually impaired. Unlike web content, in these images the objects and the scene are not always well-centered, hence this dataset contains many out-of-distribution samples compared to typical web-crawled datasets. We report the results of this experiment in Table 2d. Nevertheless, our approach significantly outperforms Flamingo  and InstructBLIP  in the zero-shot setting.

**Video Question Answering.** We also evaluate FlexCap-LLM on zero-shot video question answering datasets MSRVTT-QA and MSVD-QA . The results on these benchmarks are presented in Table 3. For processing the video, we sample 8 frames uniformly from the video. We pass each of these frames through FlexCap to produce captions of objects and regions. We then combine all the object captions from the different frames into one prompt for the LLM. We observe FlexCap-LLM

Table 4: **Captioning boxes in Visual Genome dataset. FlexCap exceeds performance of other methods. All methods have been fine-tuned on Visual Genome captions.**

Table 2: **Zero-shot image question answering results. FlexCap-LLM is compared against recent baselines. Grayed out methods are trained on question answering datasets.**

Table 3: **Zero-shot video question answering results reported on MSRVTT-QA and MSVD-QA on the test set. FlexCap-LLM is better than other zero-shot baselines for video VQA benchmarks.**

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

as ViperGPT , Flamingo , BLIP [29; 30], PALI  show convincing zero-shot performance on the VQA benchmarks that rivals the supervised approaches. Unlike most previous zero-shot approaches, which tightly couple vision and language components in a single model, FlexCap generates a high-level human interpretable representation of an image and demonstrates that, through straight-forward application of LLMs, we can achieve comparable performance with state-of-the-art results across VQA benchmarks. Unlike others, ViperGPT , also decouples vision and language components and reinterprets visual questions with LLM generated programs, and executes them using existing visual perception tools. Whereas, in our case we use only one powerful vision tool, i.e. FlexCap, to generate all the necessary information and leave the reasoning to an LLM. In that sense, FlexCap is quite complementary to ViperGPT as it can be one of the powerful tools that can improve the controllable visual understanding of the image for ViperGPT.

**Open vocabulary object detection** models like OWL-ViT  and ViLD  enable the user to query any given text on the image and obtain matched bounding boxes for those queries. In these models the text is often encoded by a text encoder like CLIP  and T5 . The text embeddings are compared with the category-agnostic box proposals coming from the visual backbone. In this work, we use OWL-ViT's text and vision encoders to associate bounding boxes with text-queries to produce our training data. By training a localized captioning model, we remove the manual step of providing per-dataset or per-image text queries to use OWL-ViT. RegionCLIP  obtained good performance on open-vocabulary object detection by utilizing region-level vision-language contrastive learning on large scale data. We differ from this work as we generate the description for each bounding box instead of associating text queries (defined manually) with bounding boxes.

**Dense captioning** involves localizing salient regions of the image and describing them with natural language sentences, introduced in . In practice, the existing work often produces longer and more informative descriptions of objects or their compositions using visual attributes of objects [27; 58] or contextual and global image cues [31; 56]. However, the richness of descriptions in this line of work are often limited to existing image captioning datasets [28; 32]. By utilizing a large scale dataset of billions of noisy image-text pairs collected from the web (similar to [9; 24]), we aim to generate more diverse sentences with a focus on describing the visual content in controllable detail using a richer visual descriptive space learned from the web.

**Length-controlled image captioning** has been explored in ZeroCap  and LIC . ZeroCap  implements length control as a post-processing step by changing the probability of sampling the end-of-sentence token. Hence the model is not naturally trained with word length conditioning in mind and cannot guarantee fine-grained length control at the level of number of words. On the other hand, LIC  generates length-controllable captions by conditioning the model with learned tokens that represent different length intervals. However there are considerable differences compared to FlexCap. First, our approach allows for controllability at the level of image regions, while LIC only provides full image captions. This is a significant difference, as it allows us to generate concise or detailed captions for all the objects in the image. Second, our approach has a more precise level of caption-length control. LIC uses a coarse subjective level of control with four or five levels of length (e.g. short, medium, long, and longer), while our approach allows for an exact number of words to be specified.  also propose an approach to produce variable length descriptions for objects localized interactively. They use a pre-trained image captioner to produce descriptions of objects and ChatGPT in post-hoc to output length-conditioned captions. While in our FlexCap model, the length tokens modulate the output produced by the captioner.

## 6 Conclusion

In this work, we introduce FlexCap, a flexible captioning model that can describe localized regions in an image with controllably rich captions. To train FlexCap, we generate a large-scale image-box-caption dataset that is rich in diversity of visual descriptions and their length. We achieve this by utilizing existing web-scale noisy image-text pairs and open-vocabulary object detection models. We show how localized rich descriptions provided by FlexCap can help us connect images and videos to LLMs and achieve strong performance on visual question answering and dense captioning tasks. We also show that our FlexCap model benefits from contrastive pretraining, localized captions dataset size scaling, model size scaling, and is compliant to length conditioning. We also demonstrate the effectiveness of FlexCap-enabled _localize-then-describe_ approach over the _describe-then-localize_ approaches.