# Federated Learning with Bilateral Curation for Partially Class-Disjoint Data

Ziqing Fan1,2, Ruipeng Zhang1,2, Jiangchao Yao1,2, Bo Han3, Ya Zhang1,2, Yanfeng Wang1,2,58

###### Abstract

Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes _a part of classes_ (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame (ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance (averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees. Source code is available at: https://github.com/MediaBrain-SJTU/FedGELA.

## 1 Introduction

Partially class-disjoint data (PCDD) [13; 18; 21] refers to an emerging situation in federated learning [14; 22; 43; 46; 50] where each client only possesses information on a subset of categories, but all clients in the federation provide the information on the whole categories. For instance, in landmark detection  for thousands of categories with data locally preserved, most contributors only have a _subset_ of categories of landmark photos where they live or traveled before; and in the diagnosis of Thyroid diseases, due to regional diversity different hospitals may have shared and distinct Thyroid diseases . It is usually difficult for each party to acquire the full classes of samples, as the participants may be lack of domain expertise or limited by demographic discrepancy. Therefore, how to efficiently handle the _partially class-disjoint data_ is a critical (yet under-explored) problem in real-world federated learning applications for the pursuit of personal and generic interests.

Prevalent studies mainly focus on the general heterogeneity without specially considering the PCDD challenges: generic federated leaning (G-FL) algorithms adopt a uniform treatment of all classes and mitigate personal differences by imposing constraints on local training [17; 19], modifying logits [21; 47] adjusting the weights of submitted gradients  or generating synthetic data ; in contrast, personalized federated learning (P-FL) algorithms place relatively less emphasis on locally missing classes and selectively share either partial network parameters [1; 6] or class prototypes  to minimize the impact of personal characteristics, thereby separating the two topics. Those methodsmight directly or indirectly help mitigate the data shifts caused by PCDD, however, as far as we know, none of the existing works can mitigate the PCDD challenges to achieve holistic improvement in the bilateral views (global and local views) of federated learning. Please refer to Table 1 for a comprehensive comparison among a range of FL methods from different aspects.

Without full classes, the local objective will contradict the global objective, yielding the angle collapse for locally missing classes and the waste of space for locally existing classes. Ideally, as shown in Figure 1(a), global features and their corresponding classifier vectors shall maintain a proper structure to pursue the best separation of all classes. However, the angles of locally missing classes' classifier vectors will collapse, when trained on each client with partially class-disjoint data, as depicted in Figure 1(b), 1(c). FedRS  notices the degenerated updates of the classifier and pursues the same symmetrical structure in the local by restricting logits of missing classes. Other traditional G-FL algorithms indirectly restrict the classifier by constraining logits, features, or model weights, which may also make effects on PCDD. However, they cause another problem: space waste for personal tasks. As shown in Figure 1(d), restricting local structure will waste feature space and limit the training of the local model on existing classes. P-FL algorithms utilize the wasted space by selectively sharing part of models but exacerbate the angle collapse of classifier vectors. Recent FedRod  attempts to bridge the gap between P-FL and G-FL by introducing a two-head framework with logit adjustment in the G-head, but still cannot address the angle collapse caused by PCDD.

To tackle the PCDD dilemma from both P-FL and G-FL perspectives, we are inspired by a promising classifier structure, namely _simplex equiangular tight frame_ (ETF) [9; 26; 41], which provides each class the same classification angle and generalizes well on imbalanced data. Motivated by its merits, we propose a novel approach, called **FedGELA**, in which the classifier is **Globally** fixed as a simplex **ETF** while **Locally Adapted** to personal tasks. In the global view, FedGELA merges class features and their corresponding classifier vectors, which converge to ETF. In the local view, it provides existing major classes with larger feature spaces and encourages to utilize the spaces wasted by locally missing classes. With such a bilateral curation, we can explicitly alleviate the impact caused by PCDD. In a nutshell, our contributions can be summarized as the following three points:

* We study a practical yet under-explored data formation in real-world applications of federated learning, termed as partially class-disjoint data (PCDD), and identify the angle collapse and space waste challenges that cannot be efficiently solved by existing prevalent methods (Sec. 3.2).
* We propose a novel method called FedGELA that classifier is globally fixed as a symmetrical structure ETF while locally adapted by personal distribution (Sec. 3.3), and theoretically show the local and global convergence analysis for PCDD with the experimental verification (Sec. 4.2).
* We conduct a range of experiments on multiple benchmark datasets under the PCDD case and a real-world dataset to demonstrate the bilateral advantages of FedGELA over the state-of-the-art methods from multiple views like the larger scale of clients and straggler situations (Sec. 5.2). We also provide further analysis like classification angles during training and ablation study. (Sec. 5.3).

## 2 Related Work

### Partially Class-Disjoint Data and Federated Learning algorithms

Partially class-disjoint data is one common formation among clients that can significantly impede the convergence, performance, and efficiency of algorithms in FL . It belongs to the data heterogeneity

Figure 1: Illustration of feature spaces and classifier vectors trained on the global dataset, two partially class-disjoint datasets (A and B), and restricted by federated algorithms. (a) is trained on the globally balanced dataset with full classes. (b) and (c) are trained on datasets A and B, respectively, which suffer from different patterns of classifier angle collapse problems. (d) is averaged in the server or constrained by some federated algorithms.

case, but does have a very unique characteristic different from the ordinary heterogeneity problem. That is, if only each client only has a subset of classes, it does not share the optimal Bayes classifier with the global model that considers all classes on the server side. Recently, FedRS  has recognized the PCDD dilemma and directly mitigate the angle collapse issue by constraining the logits of missing classes. FedProx  also can lessen the collapse by constraining local model weights to stay close to the global model. Other G-FL algorithms try to address data heterogeneity from a distinct perspective. MOON  and FedGen  utilizes contrastive learning and generative learning to restrict local representations. And FedLC  introduces logit calibration to adjust the logits of the local model to match those of the global model, which might indirectly alleviate the angle collapse in the local. However, they all try to restrict local structure as global, resulting in the waste space for personal tasks shown in Figure 1(d). P-FL algorithms try to utilize the wasted space by encouraging the angle collapse of the local classifier. FedRep  only shares feature extractors among clients and FedProto  only submits class prototypes to save communication costs and align the feature spaces. In FedBABU , the classifier is randomly initialized and fixed during federated training while fine-tuned for personalization during the evaluation. However, they all sacrifice the generic performance on all classes. FedRod  attempts to bridge this gap by introducing a framework with two heads and employing logit adjustment in the global head to estimate generic distribution but cannot address angle collapse. In Table 1, we categorize these methods by targets (P-FL or G-FL), skews (feature, logit, or model weight), and whether they directly mitigate the angle collapse of local classifier or saving personal spaces for personal spaces. It is evident that none of these methods, except ours, can handle the PCDD problem in both P-FL and G-FL. Furthermore, FedGELA is the only method that can directly achieve improvements from all views.

### Simplex Equiangular Tight Frame

The simplex equiangular tight frame (ETF) is a phenomenon observed in neural collapse , which occurs in the terminal phase of a well-trained model on a balanced dataset. It is shown that the last-layer features of the model converge to within-class means, and all within-class means and their corresponding classifier vectors converge to a symmetrical structure. To analyze this phenomenon, some studies simplify deep neural networks as last-layer features and classifiers with proper constraints (layer-pealed model) [9; 12; 40; 53] and prove that ETF emerges under the cross-entropy loss. However, when the dataset is imbalanced, the symmetrical structure of ETF will collapse . Some studies try to obtain the symmetrical feature and the classifier structure on the imbalanced datasets by fixing the classifier as ETF [41; 53]. Inspired by this, we propose a novel method called FedGELA that bilaterally curates the classifier to leverage ETF or its variants. See Appendix A for more details about ETF.

## 3 Method

### Preliminaries

ETF under LPM.A typical L-layer DNN parameterized by \(\) can be divided into the feature backbone parameterized by \(^{-L}\) and the classifier parameterized by \(^{L}\). From the view of layer-pealed model (LPM) [9; 12; 40; 53], training \(\) with constraints on the weights can be considered

  
**Target** & **Research work** & **Feature View** & **Logit View** & **Model View** & **Mitigate Collapse** & **Save Space** \\   & FedProx & - & - & \(\) & \(\) & - \\   & MOON & \(\) & - & - & - & - \\   & FedRS & - & \(\) & - & \(\) & - \\   & FedGen & \(\) & - & - & \(\) & - \\   & FedLC & - & \(\) & - & - & - \\   & FedRep & \(\) & - & \(\) & - & \(\) \\   & FedProto & \(\) & - & \(\) & - & \(\) \\   & FedBABU & \(\) & - & \(\) & - & \(\) \\   & FedRod & - & \(\) & \(\) & - & \(\) \\   & FedGELA(ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Key differences between SOTA methods and our FedGELA categorized by targets (P-FL or G-FL), techniques (improve from the views of features, logits or model), and whether directly mitigate angle collapse of classifier vectors or save locally wasted feature spaces caused by PCDD.

as training the C-class classifier \(^{L}=\{_{1}^{L},...,_{C}^{L}\}\) and features \(=\{h^{1},...,h^{n}\}\) of all \(n\) samples output by last layer of the backbone with constraints \(E_{W}\) and \(E_{H}\) on them respectively. On the balanced data, any solutions to this model form a simplex equiangular tight frame (ETF) that all last layer features \(h_{c}^{i,*}\) and corresponding classifier \(_{c}^{L,*}\) of all classes converge as:

\[^{i,*}}{}}=_{c}^{L,*}}{}}=m_{ c}^{*},\] (1)

where \(m_{c}^{*}\) forms the ETF defined as \(=}(_{C}- _{C}_{C}^{T}).\) Here \(=[m_{1}^{*},,m_{C}^{*}]^{d C}, ^{d C}\) allows a rotation and satisfies \(^{T}=_{C}\) and \(_{C}\) is an all-ones vector. ETF is an optimal classifier and feature structure in the balanced case of LPM.

FedAvg.On the view of LPM, given N clients and each with \(n_{k}\) samples, the vanilla federated learning via FedAvg consists of four steps : 1) In round \(t\), the server broadcasts the global model \(^{t}=\{^{t},^{L,t}\}\) to clients that participate in the training (Note that here \(\) is actually the global backbone \(^{-L,t}\) instead of real features); 2) Each local client receives the model and trains it on the personal dataset. After \(E\) epochs, we acquire a new local model \(_{k}^{t}\); 3) The updated models are collected to the server as \(\{_{1}^{t},_{2}^{t},,_{N}^{t}\}\); 4) The server averages local models to acquire a new global model as \(^{t+1}=_{k=1}^{N}p_{k}_{k}^{t}\), where \(p_{k}=n_{k}/_{k^{}=1}^{N}n_{k^{}}\). When the pre-defined maximal round \(T\) reaches, we will have the final optimized global model \(^{T}\).

### Contradiction and Motivation

Contradiction.In G-FL, the ideal global objective under LPM of federated learning is described as:

\[_{,^{L}}_{k=1}^{N}p_{k}}_{c C _{k}}_{i=1}^{n_{k,c}}_{CE}(h_{k,c}^{i},^{L} ).\]

Assuming global distribution is balanced among classes, no matter whether local datasets have full or partial classes, the global objective with constraints on weights can be simplified as:

\[_{,^{L}}_{c=1}^{C}_{i=1}^{n_{c}} _{CE}(h_{c}^{i},^{L}),\|_{c}^{L}\|^{2} E_{W},\|h_{c}^{i} \|^{2} E_{H}.\] (2)

Similarly, the local objective of k-th client with a set of classes \(C_{k}\) can be described as:

\[_{_{k},_{k}^{L}}}_{c C_{k}} _{i=1}^{n_{k,c}}_{CE}(h_{k,c}^{i},_{k}^{L}), \|_{k,c}^{L}\|^{2} E_{W},\|h_{k,c}^{i} \|^{2} E_{H}.\] (3)

When PCDD exists (\(C_{k} C\)), we can see the contradiction between local and global objectives, which respectively forms two structures, shown in Figure 3(a) and Figure 3(b). After aggregated in server or constrained by some FL methods, the structure in the local is restricted to meet the global structure, causing space waste for personal tasks shown in Figure 1(d).

Motivation.To verify the contradiction and related feature and classifier structures, we split CIFAR10 into 10 clients and perform FedAvg on it with Dirichlet Distribution (Dir (\(=0.1\))). As illustrated in Figure 2, the angle difference between existing classes and between missing classes becomes smaller and converges to a similar value in the global model. However, in the local training, angles between existing classes become larger while angles between missing classes become smaller, which indicates the contradiction. With this observation, to bridge the gap between Eq (3) and Eq (2) under PCDD, we need to construct the symmetrical and uniform classifier angles for all classes while encouraging local clients to expand existing classes' feature space. Therefore, we propose our method **FedGELA** that classifier can be **Globally** fixed as **ETF** but **Locally Adapted** based on the local distribution matrix to utilize the wasted space for the existing classes.

Figure 2: Averaged angles of classifier vectors between locally existing classes (existing angle) and between locally missing classes (missing angle) on CIFAR10 (Dir (\(=0.1\))) in local client and aggregated in global server (local epoch is 10). In global, “existing” angle and “missing” angle converge to similar values while in the local, “existing” angle expands but “missing” angle shrinks.

### FedGELA

Global ETF.Given the global aim of achieving an unbiased classifier that treats all classes equally and provides them with the same discrimination and classifier angles, we curate the global model's classifier as a randomly initialized simplex ETF with scaling \(}\) at the start of federated training:

\[^{L}=}.\]

Then the ETF is distributed to all clients to replace their local classifiers. In Theorem 1, we prove in federated training under some basic assumptions, by fixing the classifier as a randomly simplex ETF with scaling \(}\) and constraints \(E_{H}\) on the last layer features, features output by last layer of backbone and their within class means will converge to the ETF similar to Eq (1), which meets the requirement of global tasks.

Local Adaptation.However, when PCDD exists in the local clients, naively combining ETF with FL does not meet the requirement of P-FL as analyzed in Eq (2) and Eq (3). To utilize the wasted space for locally missing classes, in the training stage, we curate the length of ETF received from the server based on the local distribution as below:

\[_{k}^{L}=_{k}^{L}=_{k}},\] (4)

where \(_{k}\) is the distribution matrix of k-th client. Regarding the selection of \(_{k}\), it should satisfy a basic rule for federated learning, wherein the aggregation of local classifiers aligns with the global classifier, thereby ensuring the validity of theoretical analyses from both global and local perspectives. Moreover, it is highly preferable for the selection process to avoid introducing any additional privacy leakage risks. To meet the requirement that averaged classifier should be standard ETF: \(^{L}=_{k=1}^{N}p_{k}_{k}^{L}\) in the globally balanced case, its row vectors are all one's vector multiple statistical values of personal distribution:\((_{k}^{T})_{c}=}{n_{k}}\) (\(\) is a constant, and \(n_{k,c}\) and \(n_{k}\) are the c-th class sample number and total sample number of the k-th client) respectively. We set \(\) to \(\). Finally, the local objective from Eq. (3) is adapted as:

\[_{_{k}} }_{c=1}^{C}_{i=1}^{n_{k,c}}-_{k,c}_{c}^{L}^{T}}h_{k,c}^{i})}{_{c^{ } C_{k}}(_{k,c^{}}_{c^{}}^{L }^{T}}h_{k,c}^{i})},\] (5) s.t. \[\|h^{i}\|^{2} E_{H}, 1 i n_{k}.\]

Total Framework.After introducing two key parts of FedGELA (Global ETF and Local Adaptation), we describe the total framework of FedGELA. As illustrated and highlighted in Algorithm 1 (refer to Appendix D for the workflow figure), at the initializing stage, the server randomly generates an ETF as the global classifier and sends it to all clients while local clients adjust it based on the personal distribution matrix as Eq (4). At the training stage, local clients receive global backbones and train with adapted ETF in parallel. After \(E\) epochs, all clients submit personal backbones to the server. In the server, personal backbones are received and aggregated to a generic backbone, which is broadcast to all clients participating in the next round. At the inference stage, on the client side, we obtain a generic backbone with standard ETF to handle the world data while on the client side, a personal backbone with adapted ETF to handle the personal data.

``` Input:\((N,K,n_{k},c_{k},^{0},,E_{W},E_{H},T,,E)\)  Parallelly for all clients:\(_{k}^{L}}_{k}}\) for\(t=0,1,,T-1\)do \(\) on the server side \(^{t}_{k=1}^{K}p_{k}^{t}H_{k}^{t-1}\). sample K clients from all N clients. \(\) on the client side do in parallel for\( k K\) clients \(^{t}\) from server, \(_{k}^{t}^{t}\). for\(=0,1,...,E-1\)do  sample a mini-batch \(b_{k}^{LF+}\) in local data. \(^{t}} H_{k}^{t}- F_{k}(b_{k}^{t},_{k}W_{a}^{L};_{k}^{t})\) endfor  submit \(_{k}^{t}\) to the server. endfor endfor ```

Output:\((^{T},W_{q}^{L})\) and \((_{k}^{T},_{k}W_{g}^{L})\). ```

**Algorithm 1** FedGELA

## 4 Theoretical Analysis

In this part, we first primarily introduce some notations and basic assumptions in Sec. 4.1 and then present the convergence guarantees of both local models and the global model under the PCDD with the proper empirical justification and discussion in Sec. 4.2. (Please refer to Appendix B for entire proofs and Appendix D for details on justification experiments.)

### Notations

We use \(t\) and \(T\) to denote a curtain round and pre-defined maximum round after aggregation in federated training, \(tE\) to denote the state that just finishing local training before aggregation in round \(t\), and \(tE+\) to denote \(\)-th local iteration in round \(t\) and \(0 E-1\). The convergence follows some common assumptions in previous FL studies and helpful math results [29; 20; 21; 31; 33; 36; 38; 45; 51] including smoothness, convexity on loss function \(F_{1}\), \(F_{2},,F_{N}\) of all clients, bounded norm and variance of stochastic gradients on their gradient functions \( F_{1}, F_{2},, F_{N}\) and heterogeneity \(_{1}\) reflected as the distance between local optimum \(_{k}^{*}\) and global optimum \(^{*}\). Please refer to the concrete descriptions of those assumptions in Appendix B. Besides, in Appendix B, we additionally provide a convergence guarantee without a bounded norm of stochastic gradients, as some existing works [24; 32] point out the contradiction to the strongly convex.

### Convergence analysis

Here we provide the global and local convergence guarantee of our FedGELA compared with FedAvg and FedGE (FedAvg with only the Globally Fixed ETF) in Theorem 1 and Theorem 2. To better explain the effectiveness of our FedGELA in local and global tasks, we record the averaged angle between all class means in global and existing class means in local as shown in Figure 3(a) and Figure 3(b). Please refer to Appendix B for details on the proof and justification of theorems.

**Theorem 1** (Global Convergence).: _If \(F_{1},...,F_{N}\) are all L-smooth, \(\)-strongly convex, and the variance and norm of \( F_{1},..., F_{N}\) are bounded by \(\) and \(G\). Choose \(=L/\) and \(=\{8,E\}\), for all classes \(c\) and sample \(i\), expected global representation by cross-entropy loss will converge to:_

\[[^{L,*})^{T}h_{c}^{i,*}}{(_{g}^{L })^{T}h_{c}^{i,*}}](+ ||^{1}-^{*}||^{2}),\]

_where in FedGELA, \(B=_{k=1}^{N}(p_{k}^{2}^{2}+p_{k}||_{k}^{L}- ^{L}||)+6L_{1}+8(E-1)^{2}G^{2}\). Since \(^{L}=^{L,*}\) and \((^{L,*})^{T}h_{c_{i}}^{i,*}[(^{L})^{T}h_{c_ {i}}^{i}]\), \(h_{c_{i}}^{i}\) will converge to \(h_{c_{i}}^{i,*}\)._

In Theorem 1, the variable \(B\) represents the impact of algorithmic convergence (\(p_{k}^{2}^{2}\)), non-iid data distribution (\(6L_{1}\)), and stochastic optimization (\(8(E-1)^{2}G^{2}\)). The only difference between FedAvg, FedGE, and our FedGELA lies in the value of \(B\) while others are kept the same. FedGE and FedGELA have a smaller \(G\) compared to FedAvg because they employ a fixed ETF classifier that is predefined as optimal. FedGELA introduces a minor additional overhead (\(p_{k}||_{k}^{L}-^{L}||\)) on the global convergence of FedGE due to the incorporation of local adaptation to ETFs. The cost might be negligible, as \(\), \(G\), and \(_{1}\) are defined on the whole model weights while \(p_{k}||_{k}^{L}-^{L}||\) is defined on the classifier. To verify this, we conduct experiments in Figure 3(a), and as can be seen, FedGE and FedGELA have similar quicker speeds and larger classification angles than FedAvg.

**Theorem 2** (Local Convergence).: _If \(F_{1},...,F_{N}\) are L-smooth, variance and norm of their gradients are bounded by \(\) and \(G\), and the heterogeneity is bounded by \(_{1}\), clients' expected local loss satisfies:_

\[[F_{k}^{(t+1)E}] F_{k}^{tE}+^{2}}{2} ^{2}+_{1}-A,\]

_where in FedGELA, \(A=(_{t}-_{t}^{2})EG^{2}-L\|_{k}^{L }-^{L}\|\), which means if \(A-}{LE(G^{2}+^{2})} 0\), there exist learning rate \(_{t}\) making the expected local loss decreasing and converging._

Figure 3: Illustration of local and global convergence verification together with the effect of \(\). (a) and (b) are the results of averaged angle between all class means and between locally existing class means in FedAvg, FedGE, and FedGELA on CIFAR10 under 50 clients and Dir (\(=0.2\)). (c) is the illustration of how local adaptation utilizes the wasted space of missing classes for existing classes.

In Theorem 2, only "A" is different on the convergence among FedAvg, FedGE, and FedGELA. Fixing the classifier as ETF and adapting the local classifier will introduce smaller G and additional cost of \(L\|_{}^{L}-^{L}\|\) respectively, which might limit the speed of local convergence. However, FedGELA might reach better local optimal by adapting the feature structure. As illustrated in Figure 3 (c), the adapted structure expands the decision boundaries of existing major classes and better utilizes the feature space wasted by missing classes. To verify this, in Figure 3(b), we record the averaged angles between the existing class means during the local training. It can be seen that FedGELA converges to a much larger angle than both FedAvg and FedGE, which suits our expectations. More angle results can be seen in Figure 5.

## 5 Experiments

### Experimental Setup

**Datasets.** We adopt three popular benchmark datasets SVHN , CIFAR10/100  in federated learning. As for data splitting, we utilize Dirichlet Distribution (Dir (\(\)), \(=\{10000,0.5,\)\(0.2,\)\(0.1\}\)) to simulate the situations of independently identical distribution and different levels of PCDD. Besides, one standard real-world PCDD dataset, Fed-ISIC2019 [4; 7; 34; 35] is used, and we follow the setting in the Flamby benchmark . Please refer to Appendix C for more details.

**Metrics.** Denote PA as the personal accuracy, which is the mean of the accuracy computed on each client test dataset, and GA as the generic accuracy on global test dataset (mixed clients' test datasets). Since there is no global model in P-FL methods, we calculate GA of them as the averaged accuracy of all best local models on global test dataset, which is the same as FedRod . Regarding PA, we record the best results of personal models for P-FL methods while for G-FL methods we fine-tune the best global model in 10 epochs and record the averaged accuracy on all client test datasets. For FedRod and FedGELA, we can directly record the GA and PA (without fine-tuning) during training.

**Implementation.** We compare FedGELA with FedAvg, FedRod , multiple state-of-the-art methods in G-FL (FedRS , MOON , FedProx , FedGen  and FedLC ) and in P-FL (FedRep , FedProto  and FedBABU ). For SVHN, CIFAR10, and CIFAR100, we adopt a commonly used ResNet18 [8; 17; 47; 48; 52] with one FC layer as the backbone, followed by a layer of classifier. FedGELA replaces the classifier as a simple ETF. We use SGD with learning rate 0.01, weight decay \(10^{-4}\), and momentum 0.9. The batch size is set as 100 and the local updates are set as 10 epochs for all approaches. As for method-specific hyper-parameters like the proximal term in FedProx, we tune it carefully. In our method, there are \(E_{W}\) and \(E_{H}\) need to set, we normalize features with length 1 (\(E_{H}=1\)) and only tune the length scaling of classifier (\(E_{W}\)). All methods are implemented by PyTorch  with NVIDIA GeForce RTX 3090. See detailed information in Appendix C.

### Performance of FedGELA

In this part, we compare FedGELA with FedAvg, FedRod, three SOTA methods of P-FL (FedRep, FedProto, and FedBABU), four SOTA methods of G-FL (FedProx, MOON, FedRS, FedLC and FedGen) on different aspects including the scale of clients, the level of PCDD, straggler situations, and real-world applications. Similar to recent studies [8; 17; 44], we split SVHN, CIFAR10, and CIFAR100 into 10 and 50 clients and each round select 10 clients to join the federated training, denoted as full participation and partial participation (straggler situation), respectively. With the help of Dirichlet distribution , we verify all methods on IID, Non-IID (\(=0.5\)), and extreme Non-IID situations (\(=0.1\) or \(=0.2\)). As the decreasing \(\), the level of PCDD increases and we show the heat map of data distribution in Appendix C. We set \(=0.2\) in partial participation to make sure each client has at least one batch of samples. The training round for SVHN and CIFAR10 is 50 in full participation and 100 in partial participation while for CIFAR100, it is set to 100 and 200. Besides, we also utilize a real federated scenario Fed-ISIC2019 to verify the ability to real-world application.

**Full participation and partial participation.** As shown in Table 2, with the decreasing \(\) or increasing number of clients, the generic performance of FedAvg and all other methods greatly drops while the personal performance of all methods greatly increases. This means under PCDD and the straggler problem, the performance of generic performance is limited but the personal distribution is easier to capture. As for P-FL methods, they fail in global tasks especially in severe PCDD

[MISSING_PAGE_FAIL:8]

### Further Analysis

**More angle visualizations.** In Figure 5, we show the effectiveness of local adaptation in FedGELA and verify the convergence of fixed classifier as ETF and local adaptation compared with FedAvg. Together with Figure 3, it can be seen that, compared with FedAvg, both FedGE and FedGELA converge faster to a larger angle between all class means in global. In the meanwhile, the angle between existing classes of FedGELA in the local is much larger, which proves FedGELA converges better than FedAvg and the adaptation brings little limits to convergence but many benefits to local performance under different levels of PCDD.

**Hyper-parameter.** FedGELA introduces constrain \(E_{H}\) on the features and the length \(E_{W}\) of classifier vectors. We perform \(L_{2}\) norm on all features in FedGELA, which means \(E_{H}=1\). For the length of the classifier, we tune it as hyper-parameter. As shown in Figure 4, from a large range from 10e3 to 10e6 of \(E_{W}\), our method achieves bilateral improvement compared to FedAvg on all datasets.

**Ablation studies.** Since our method includes two parts: global ETF and local adaptation, we illustrate the average accuracy of FedGELA on all splits of SVHN, CIFAR10/100, and Fed-ISIC2019 without the global ETF or the local adaptation or both. As shown in Table 4, only adjusting the local classifier does not gain much in personal or global tasks, and compared with FedGE, FedGELA achieves similar generic performance on the four datasets but much better performance on the personal tasks.

   GE & LA &  &  &  &  \\  \#Partition & Full & Parti. & Partial & Parti. & Full & Parti. & Partial & Parti. & Full & Parti. & Partial & Parti. & Real & World \\  \#Metric & PA & GA & PA & GA & PA & GA & PA & GA & PA & GA & PA & GA & PA & GA \\  - & - & 95.02 & 86.36 & 93.15 & 88.43 & 82.50 & 64.88 & 72.52 & 59.19 & 69.09 & 62.80 & 56.46 & 54.28 & 77.27 & 73.59 \\  ✓ & - & 95.92 & 88.93 & 93.97 & 92.42 & 83.63 & 69.70 & 77.66 & 65.56 & 71.46 & 66.02 & 62.67 & 58.98 & 69.88 & 75.54 \\ - & ✓ & 95.93 & 74.84 & 93.15 & 89.58 & 83.97 & 63.75 & 77.76 & 61.55 & 71.93 & 60.76 & 58.92 & 51.95 & 54.65 & 62.43 \\   ✓ & ✓ & 96.54 & 89.07 & 95.69 & 92.15 & 84.61 & 69.46 & 79.95 & 65.21 & 74.23 & 66.05 & 66.33 & 58.81 & 79.27 & 75.85 \\   

Table 4: Ablation study of FedGELA. GE and LA mean the global ETF and local adaptation.

Figure 4: Bilateral performance on four datasets by tuning \(logE_{W}\) (x axis) of FedGELA.

Figure 5: Illustration of the averaged angle between locally existing classes and missing classes on the local client and global server of FedAvg, FedGE, and our FedGELA on CIFAR10.

**Performance under pure PCDD setting.** To verify our method under pure PCDD, we decouple the PCDD setting and the ordinary heterogeneity (Non-PCDD). In Table 5, we use PxCy to denote the dataset is divided in to x clients with y classes, and in each round, 10 clients are selected into federated training. The training round is 100. According to the results, FedGELA achieves significant improvement especially \(18.56\%\) to FedAvg and \(10.04\%\) to the best baseline on CIFAR10 (P50C2).

**Other types of \(\).** Considering the aggregation of local classifiers should align with the global classifier, which ensures the validity of theoretical analyses from both global and local perspectives, \(_{k=1}^{N}p_{k}_{k}\) should be \(\) (\(\) is all-one matrix). Assuming the row vector of distribution \((_{k})_{c}^{T}\) is related to class distribution \(}{n_{k}}\) and the relationship as \(Q_{k}(}{n_{k}})\). The equation can be rewrite as: \(_{k=1}^{N}p_{k}Q_{k}(}{n_{k}})=\), where \(\) is the scaling constant. In our FedGELA, to avoid sharing statistics for privacy, we only find one potential way that \(Q_{k}(}{n_{k}})=}{n_{k}}\) and \(=\). In this part, we have also considered employing alternative methods like employing an exponential or power function of the number of samples. As shown in the Table 6, other methods need to share \(Q_{k}(}{n_{k}})\) but achieve the similar performance compared to FedGELA, which exhibits the merit of our choice.

In Appendix D, we provide more experiments from other perspectives like communication efficiency and the local burden of storing and computation, to show promise of FedGELA.

## 6 Conclusion

In this work, we study the problem of _partially class-disjoint data_ (PCDD) in federated learning on both personalized federated learning (P-FL) and generic federated learning (G-FL), which is practical and challenging due to the angle collapse of classifier vectors for the global task and the waste of space for the personal task. We propose a novel method, FedGELA, to address the dilemma via a bilateral curation. Theoretically, we show the local and global convergence guarantee of FedGELA and verify the justification on the angle of global classifier vectors and on the angle between locally existing classes. Empirically, extensive experiments show that FedGELA achieves promising improvements on FedAvg under PCDD and outperforms state-of-the-art methods in both P-FL and G-FL.

   Dataset (split) & Metric & \(Q_{k}(x)=e^{x}\) & \(Q_{k}(x)=x^{}\) & \(Q_{k}(x)=x\)(ours) \\   & PA & \(\) & 95.12 & 95.43 & 94.84 \\   & GA & \(\) & 94.32 & 93.99 & 94.66 \\   & PA & \(\) & 96.18 & 95.56 & 96.27 \\   & GA & \(\) & 93.28 & 93.22 & 93.66 \\   & PA & \(\) & 98.33 & 98.21 & 98.52 \\   & GA & \(\) & 78.95 & 77.18 & 78.88 \\   

Table 6: Performance of choosing different \(\). Assuming the row vector of distribution \((_{k})_{c}^{T}\) is related to class distribution \(}{n_{k}}\) and the relationship as \(Q_{k}(}{n_{k}})\). Except for \(Q_{k}(x)=x\), we have also considered employing alternative methods like employing an exponential \(Q_{k}(x)=e^{x}\) or power function \(Q_{k}(x)=x^{}\) of the number of samples.

   Dataset (split) & Metric & \(\) & FedAvg & Best Baseline & FedGELA \\   & PA & \(\) & 92.08+3.76 & 94.07+1.77 & 95.84 \\   & GA & \(\) & 47.26+12.34 & 52.02+7.58 & 59.60 \\   & PA & \(\) & 91.74+3.68 & 93.22+2.20 & 95.42 \\   & GA & \(\) & 36.22+18.56 & 44.74+10.04 & 54.78 \\   & PA & \(\) & 95.64+3.11 & 97.02+1.73 & 98.75 \\   & GA & \(\) & 69.34+14.22 & 76.06+7.50 & 83.56 \\   & PA & \(\) & 94.87+3.50 & 96.88+1.49 & 98.37 \\   & GA & \(\) & 66.94+10.24 & 72.97+4.21 & 77.18 \\   

Table 5: Performance of FedGELA compared with FedAvg and the best baseline under pure PCDD settings on CIFAR10 and SVHN datasets.\(P C\) means that the dataset is divided into \(\) clients and each client has \(\) classes. We show the improvement in red on each baseline compared to FedGELA.