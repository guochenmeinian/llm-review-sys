# Rethinking the Capacity of Graph Neural Networks

for Branching Strategy

Ziang Chen

Massachusetts Institute of Technology

ziang@mit.edu

&Jialin Liu

University of Central Florida

jialin.liu@ucf.edu

&Xiaohan Chen &Xinshang Wang &Wotao Yin

Alibaba US, DAMO Academy

{xiaohan.chen,xinshang.w,wotao.yin}@alibaba-inc.com

###### Abstract

Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB), the most effective yet computationally expensive heuristic employed in the branch-and-bound algorithm. In the literature, message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently used as a fast approximation of SB and we find that not all MILPs's SB can be represented with MP-GNN. We precisely define a class of "MP-tractable" MILPs for which MP-GNNs can accurately approximate SB scores. Particularly, we establish a universal approximation theorem: for any data distribution over the MP-tractable class, there always exists an MP-GNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability, which lays a theoretical foundation of the existing works on imitating SB with MP-GNN. For MILPs without the MP-tractability, unfortunately, a similar result is impossible, which can be illustrated by two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. Recognizing this, we explore another GNN structure called the second-order folklore GNN (2-FGNN) that overcomes this limitation, and the aforementioned universal approximation theorem can be extended to the entire MILP space using 2-FGNN, regardless of the MP-tractability. A small-scale numerical experiment is conducted to directly validate our theoretical findings.

## 1 Introduction

Mixed-integer linear programming (MILP) involves optimization problems with linear objectives and constraints, where some variables must be integers. These problems appear in various fields, from logistics and supply chain management to planning and scheduling, and are in general NP-hard. The branch-and-bound (BnB) algorithm  is the core of a MILP solver. It works by repeatedly solving relaxed versions of the problem, called linear relaxations, which allow the integer variables to take on fractional values. If a relaxation's solution satisfies the integer requirements, it is a valid solution to the original problem. Otherwise, the algorithm divides the problem into two subproblems and solves their relaxations. This process continues until it finds the best solution that meets all the constraints.

**Branching** is the process of dividing a linear relaxation into two subproblems. When branching, the solver selects a variable with a fractional value in the relaxation's solution and create two new subproblems. In one subproblem, the variable is forced to be less than or equal to the nearest integerbelow the fractional value. In the other, it is bounded above the fractional value. The branching variable choice is critical because it can impact the solver's efficiency by orders of magnitude.

A well-chosen branching variable can lead to a significant improvement in the lower bound, which is a quantity that can quickly prove that a subproblem and its further subdivisions are infeasible or not promising, thus reducing the total number of subproblems to explore. This means fewer linear relaxations to solve and faster convergence to the optimal solution. On the contrary, a poor choice may result in branches that do little to improve the bounds or reduce the solution space, thus leading to a large number of subproblems to be solved, significantly increasing the total solution time. The choice of which variable to branch on is a pivotal decision. This is where **branching strategies**, such as strong branching and learning to branch, come into play, evaluating the impact of different branching choices before making a decision.

**Strong branching (SB)** is a sophisticated strategy to select the _most promising branches_ to explore. In SB, before actually performing a branch, the solver tentatively branches on several variables and calculates the potential impact of each branch on the objective function. This "look-ahead" strategy evaluates the quality of branching choices by solving linear relaxations of the subproblems created by the branching. The variable that leads to the most significant improvement in the objective function is selected for the actual branching. Usually recognized as the most effective branching strategy, _SB often results in a significantly lower number of subproblems to resolve during the branch-and-bound (BnB)_ process compared to other methods . As such, SB is frequently utilized directly or as a fundamental component in cutting-edge solvers.

While SB can significantly reduce the size of the BnB search space, it comes with _high computational cost_: evaluating multiple potential branches at each decision point requires solving many LPs. This leads to a _trade-off_ between the time spent on SB and the overall time saved due to a smaller search space. In practice, MILP solvers use heuristics to limit the use of SB to where it is most beneficial.

**Learning to branch (L2B)** introduces a new approach by incorporating machine learning (ML) to develop branching strategies, offering new solutions to address this trade-off. This line of research begins with imitation learning , where models, including SVM, decision tree, and neural networks, are trained to mimic SB outcomes based on the features of the underlying MILP. They aim to _create a computationally efficient strategy that achieves the effectiveness of SB on specific datasets_. Furthermore, in recent reinforcement learning approaches, mimicking SB continues to take crucial roles in initialization or regularization .

While using a heuristic (an ML model) to approximate another heuristic (the SB procedure) may seem counterintuitive, it is important to recognize the potential benefits. The former can significantly reduce the time required to make branching decisions as effectively as the latter. As MILPs become larger and more complex, the computational cost of SB grows at least cubically, but some ML models grow quadratically, even just linearly after training on a set of similar MILPs. Although SB can theoretically solve LP relaxations in parallel, the time required for different LPs may vary greatly, and there is a lack of GPU-friendly methods that can effectively utilize starting bases for warm starts. In contrast, ML models, particularly GNNs, are more amenable to efficient implementation on GPUs, making them a more practical choice for accelerating the branching variable selection process. Furthermore, additional problem-specific characteristics can be incorporated into the ML model, allowing it to make more informed branching decisions tailored to each problem instance.

**Graph neural network (GNN)** stands out as an effective class of ML models for L2B, surpassing other models like SVM and MLP, due to the excellent scalability and the permutation-invariant/equivariant property. To utilize a GNN on a MILP, one first conceptualizes the MILP as a graph and the GNN is then applied to that graph and returns a branching decision. This approach  has gained prominence in not only L2B but various other MILP-related learning tasks . More details are provided in Section 2.

Despite the widespread use of GNNs on MILPs, a theoretical understanding remains largely elusive. A vital concept for any ML model, including GNNs, is its **capacity** or **expressive power**, which in our context is their ability to accurately approximate the mapping from MILPs to their SB results. Specifically, this paper aims to answer the following question:

_Given a distribution of MILPs, is there a GNN model capable of mapping each MILP problem to its strong branching result with a specified level of precision?_ (1.1)

**Related works and our contributions.** While the capacity of GNNs for general graph tasks, such as node and link prediction or function approximation on graphs, has been extensively studied , their capacities in approximating SB remains largely unexplored. The closest studies  have explored GNNs' ability to represent properties of linear programs (LPs) and MILPs, such as feasibility, boundedness, or optimal solutions, but have not specifically focused on branching strategies. Recognizing this gap, our paper makes the following contributions:

* In the context of L2B using GNNs, we first focus on the most widely used type: message-passing GNNs (MP-GNNs). Our study reveals that MP-GNNs can reliably predict SB results, but only for a specific class of MILPs that we introduce as _message-passing-tractable_ (MP-tractable). We prove that for any distribution of MP-tractable MILPs, there exists an MP-GNN capable of accurately predicting their SB results. This finding establishes a theoretical basis for the widespread use of MP-GNNs to approximate SB results in current research.
* Through a counter-example, we demonstrate that MP-GNNs are _incapable_ of predicting SB results beyond the class of MP-tractable MILPs. The counter-example consists of two MILPs with distinct SB results to which all MP-GNNs, however, yield identical branching predictions.
* For general MILPs, we explore the capabilities of _second-order folklore GNNs (2-FGNNs)_, a type of higher-order GNN with enhanced expressive power. Our results show that 2-FGNNs can reliably answer question (1.1) positively, effectively replicating SB results across any distribution of MILP problems, surpassing the capabilities of standard MP-GNNs.

Overall, as a series of works have empirically shown that learning an MP-GNN as a fast approximation of SB significantly benefits the performance of an MILP solver on specific data sets , our goal is to determine whether there is room, in theory, to further understand and improve the GNNs' performance on this task.

## 2 Preliminaries and problem setup

We consider the MILP defined in its general form as follows:

\[_{x^{n}}\ \ c^{}x,\ Ax b,\ \  x u,\ \ x_{j},\ \ j I, \]

where \(A^{m n}\), \(b^{m}\), \(c^{n}\), \(\{,=,\}^{m}\) is the type of constraints, \((\{-\})^{n}\) and \(u(\{\})^{n}\) are the lower bounds and upper bounds of the variable \(x\), and \(I\{1,2,,n\}\) identifies which variables are constrained to be integers.

**Graph Representation of MILP.** Here we present an approach to represent MILP as a bipartite graph, termed the _MILP-graph_. This conceptualization was initially proposed by  and has quickly become a prevalent model in ML for MILP-related tasks. The MILP-graph is defined as a tuple \(G=(V,W,A,F_{V},F_{W})\), where the components are specified as follows: \(V=\{1,2,,m\}\) and \(W=\{1,2,,n\}\) are sets of nodes representing the constraints and variables, respectively. An edge \((i,j)\) connects node \(i V\) to node \(j W\) if the corresponding entry \(A_{ij}\) in the coefficient matrix of (2.1) is non-zero, with \(A_{ij}\) serving as the edge weight. \(F_{V}\) are features/attributes of constraints, with features \(v_{i}=(b_{i},_{i})\) attached to node \(i V\). \(F_{W}\) are features/attributes of variables, with features \(w_{j}=(c_{j},_{j},u_{j},_{I}(j))\) attached to node \(j W\), where \(_{I}(j)\{0,1\}\) indicates whether the variable \(x_{j}\) is integer-constrained.

We define \(_{W}(i)=:\{j W:A_{ij} 0\} W\) as the neighbors of \(i V\) and similarly define \(_{V}(j)=:\{i V:A_{ij} 0\} V\). This graphical representation _completely_ describes a MILP's information, allowing us to interchangeably refer to a MILP and its graph throughout this paper. An illustrative example is presented in Figure 1. We also introduce a space of MILP-graphs:

**Definition 2.1** (Space of MILP-graphs).: _We use \(_{m,n}\) to denote the collection of all MILP-graphs induced from MILPs of the form (2.1) with \(n\) variables and \(m\) constraints.1_

**Message-passing graph neural networks (MP-GNNs)** are a class of GNNs that operate on graph-structured data, by passing messages between nodes in a graph to aggregate information from their local neighborhoods. In our context, the input is an aforementioned MILP-graph \(G=(V,W,A,F_{V},F_{W})\), and each node in \(W\) is associated with a real-number output. We use the standard MP-GNNs for MILPs in the literature .

Specifically, the initial layer assigns features \(s^{0}_{i},t^{0}_{j}\) for each node as* \(s_{i}^{0}=p^{0}(v_{i})\) for each constraint \(i V\), and \(t_{j}^{0}=q^{0}(w_{j})\) for each variable \(j W\).

Then message-passing layers \(l=1,2,,L\) update the features via

* \(s_{i}^{l}=p^{l}s_{i}^{l-1},_{j_{W}(i)}f^{l}(t_{j}^{l-1},A_{ij})\) for each constraint \(i V\), and
* \(t_{j}^{l}=q^{l}t_{j}^{l-1},_{i_{V}(j)}g^{l}(s_{i}^{l-1 },A_{ij})\) for each variable \(j W\).

Finally, the output layer produces a read-number output \(y_{j}\) for each node \(j W\):

* \(y_{j}=r_{i V}s_{i}^{L},_{j W}t_{j}^{L},t_{j}^{L}\).

In practice, functions \(\{p^{l},q^{l},f^{l},g^{l}\}_{l=1}^{L},r,p^{0},q^{0}\) are learnable and usually parameterized with multilinear perceptrons (MLPs). In our theoretical analysis, we assume for simplicity that those functions are continuous on given domains. The space of MP-GNNs is introduced as follows.

**Definition 2.2** (Space of MP-GNNs).: _We use \(_{}\) to denote the collection of all MP-GNNs constructed as above with \(p^{l},q^{l},f^{l},g^{l},r\) being continuous with \(f^{l}(,0) 0\) and \(g^{l}(,0) 0\).2_

Overall, any MP-GNN \(F_{}\) maps a MILP-graph \(G\) to a \(n\)-dim vector: \(y=F(G)^{n}\).

**Second-order folklore graph neural networks (2-FGNNs)** are an extension of MP-GNNs designed to overcome some of the capacity limitations. It is proved in  the expressive power of MP-GNNs can be measured by the Weisfeiler-Lehman test (WL test ). To enhance the ability to identify more complex graph patterns,  developed high-order GNNs, inspired by high-order WL tests . Since then, there has been growing literature about high-order GNNs and other variants including high-order folklore GNNs . Instead of operating on individual nodes of the given graph, 2-FGNNs operate on _pairs of nodes_ (regardless of whether two nodes in the pair are neighbors or not) and the neighbors of those pairs. We say two node pairs are neighbors if they share a common node. Let \(G=(V,W,A,F_{V},F_{W})\) be the input graph. The initial layer performs:

* \(s_{ij}^{0}=p^{0}(v_{i},w_{j},A_{ij})\) for each constraint \(i V\) and each variable \(j W\), and
* \(t_{j_{1}j_{2}}^{0}=q^{0}(w_{j_{1}},w_{j_{2}},_{j_{1}j_{2}})\) for variables \(j_{1},j_{2} W\),

where \(_{j_{1}j_{2}}=1\) if \(j_{1}=j_{2}\) and \(_{j_{1}j_{2}}=0\) otherwise. For internal layers \(l=1,2,,L\), compute

* \(s_{ij}^{l}=p^{l}s_{ij}^{l-1},_{j_{1} W}f^{l}(t_{j_{1}j}^{l-1},s_ {ij_{1}}^{l-1})\) for all \(i V,j W\), and
* \(t_{j_{1}j_{2}}^{l}=q^{l}t_{j_{1}j_{2}}^{l-1},_{i V}g^{l}(s_{ij_{ 2}}^{l-1},s_{ij_{1}}^{l-1})\) for all \(j_{1},j_{2} W\).

The final layer produces the output \(y_{j}\) for each node \(j W\):

* \(y_{j}=r_{i V}s_{ij}^{L},_{j_{1} W}t_{j_{1}j}^{L}\).

Similar to MP-GNNs, the functions within 2-FGNNs, including \(\{p^{l},q^{l},f^{l},g^{l}\}_{l=1}^{L},r,p^{0},q^{0}\), are also learnable and typically parameterized with MLPs. The space of 2-FGNNs is defined with:

**Definition 2.3**.: _We use \(_{}\) to denote the set of all \(2\)-FGNNs with continuous \(p^{l},q^{l},f^{l},g^{l},r\)._

Any \(2\)-FGNN, \(F_{}\), maps a MILP-graph \(G\) to a \(n\)-dim vector: \(y=F(G)\). While MP-GNNs and 2-FGNNs share the same input-output structure, their internal structures differ, leading to distinct expressive powers.

Figure 1: An illustrative example of MILP and its graph representation.

## 3 Imitating strong branching by GNNs

In this section, we present some observations and mathematical concepts underlying the imitation of strong branching by GNNs. This line of research, which aims to replicate SB strategies through GNNs, has shown promising empirical results across a spectrum of studies [19; 24; 25; 35; 48; 56; 58], yet it still lacks theoretical foundations. Its motivation stems from two key observations introduced earlier in Section 1, which we elaborate on here in detail.

**Observation I:** SB is notably effective in reducing the size of the BnB search space. This size is measured by the size of the BnB tree. Here, a "tree" refers to a hierarchical structure of "nodes", each representing a decision point or a subdivision of the problem. The tree's size corresponds to the number of these nodes. For instance, consider the instance "neos-3761878-oglib" from MIPLIB . When solved using SCIP [7; 8] under standard configurations, the BnB tree size is \(851\), and it takes \(61.04\) seconds to attain optimality. However, disabling SB, along with all branching rules dependent on SB, results in an increased BnB tree size to \(35548\) and an increased runtime to \(531.0\) seconds.

**Observation II:** SB itself is computationally expensive. In the above experiment under standard settings, SB consumes an average of \(70.40\%\) of the total runtime, \(42.97\) out of \(61.04\) seconds in total.

Therefore, there is a clear need of approximating SB with efficient ML models. Ideally, if we can substantially reduce the SB calculation time from \(42.97\) seconds to a negligible duration while maintaining its effectiveness, the remaining runtime of \(61.04-42.97=18.07\) seconds would become significantly more efficient.

To move forward, we introduce some basic concepts related to SB.

**Concepts for SB.** SB begins by identifying candidate variables for branching, typically those with non-integer values in the solution to the linear relaxation but which are required to be integers. Each candidate is then assigned a _SB score_, a non-negative real number determined by creating two linear relaxations and calculating the objective improvement. A higher SB score indicates the variable has a higher priority to be chosen for branching. Variables that do not qualify as branching candidates are assigned a zero score. Compiling these scores for each variable results in an \(n\)-dimensional SB score vector, denoted as \((G)=((G)_{1},(G)_{2},,(G)_{n})\).

Consequently, the task of approximating SB with GNNs can be described with a mathematical language: _finding an \(F_{}\) or \(F_{}\) such that \(F(G)(G)\)._ Formally, it is:

**Formal statement of Problem (1.1):**_Given a distribution of \(G\), is there \(F_{}\) or \(F_{}\) such that \(\|F(G)-(G)\|\) is smaller than some error tolerance with high probability?_

To provide clarity, we present a formal definition of SB scores:

**Definition 3.1** (LP relaxation with a single bound change).: _Pick a \(G_{m,n}\). For any \(j\{1,2,,n\}\), \(_{j}\{-\}\), and \(_{j}\{+\}\), we denote by \((G,j,_{j},_{j})\) the following LP problem obtained by changing the lower/upper bound of \(x_{j}\) in the LP relaxation of (2.1):_

\[_{x^{n}}\ \ c^{}x,\ Ax b,\ \ _{j} x_{j}_{j},\ \ l_{j^{}} x_{j^{}} u_{j^{}}\ \ j^{}\{1,2,,n\}\{j\}.\]

**Definition 3.2** (Strong branching scores).: _Let \(G_{m,n}\) be a MILP-graph associated with the problem (2.1) whose LP relaxation is feasible and bounded. Denote \(f^{*}_{}(G)\) as the optimal objective value of the LP relaxation of \(G\) and denote \(x^{*}_{}(G)^{n}\) as the optimal solution with the smallest \(_{2}\)-norm. The SB score \((G)_{j}\) for variable \(x_{j}\) is defined via_

\[(G)_{j}=0,&j I,\\ (f^{*}_{}(G,j,l_{j},_{j})-f^{*}_{}(G))(f^{*}_{ }(G,j,_{j},u_{j})-f^{*}_{}(G)),&,\]

_where \(f^{*}_{}(G,j,l_{j},_{j})\) and \(f^{*}_{}(G,j,_{j},u_{j})\) are the optimal objective values of \((G,j,l_{j},_{j})\) and \((G,j,_{j},u_{j})\) respectively, with \(_{j}= x^{*}_{}(G)_{j}\) being the largest integer no greater than \(x^{*}_{}(G)_{j}\) and \(_{j}= x^{*}_{}(G)_{j}\) being the smallest integer no less than \(x^{*}_{}(G)_{j}\), for \(j=1,2,,n\)._

**Remark:** LP solution with the smallest \(_{2}\)-norm.** We only define the SB score for MILP problems with feasible and bounded LP relaxations; otherwise the optimal solution \(x^{*}_{}(G)\) does not exist. If the LP relaxation of \(G\) admits multiple optimal solutions, then the strong branching score \((G)\) depends on the choice of the particular optimal solution. To guarantee that the SB score is uniquely defined, in Definition 3.2, we use the optimal solution with the smallest \(_{2}\)-norm, which is unique.

**Remark: SB at leaf nodes.** While the strong branching score discussed here primarily pertains to root SB, it is equally relevant to SB at leaf nodes within the BnB framework. By interpreting the MILP-graph \(G\) in Definition 3.2 as representing the subproblems encountered during the BnB process, we can extend our findings to strong branching decisions at any point in the BnB tree. Here, _root SB_ refers to the initial branching decisions made at the root of the BnB tree, while _leaf nodes_ represent subsequent branching points deeper in the tree, where similar SB strategies can be applied.

**Remark: Other types of SB scores.** Although this paper primarily focuses on the product SB scores (where the SB score is defined as the product of objective value changes when branching up and down), our analysis can extend to other forms of SB scores in . (Refer to Appendix D.1)

## 4 Main results

### MP-GNNs can represent SB for MP-tractable MILPs

In this subsection, we define a class of MILPs, named message-passing-tractable (MP-tractable) MILPs, and then show that MP-GNNs can represent SB within this class.

To define MP-tractability, we first present the Weisfeiler-Lehman (WL) test , a well-known criterion for assessing the expressive power of MP-GNNs . The WL test in the context of MILP-graphs is stated in Algorithm 1. It follows exactly the same updating rule as the MP-GNN, differing only in the local updates performed via hash functions.

```
1:A graph instance \(G_{m,n}\) and iteration limit \(L>0\).
2:Initialize with \(C_{0}^{V}(i)=_{0}^{V}(v_{i})\), \(C_{0}^{W}(j)=_{0}^{W}(w_{j})\).
3:for\(l=1,2,,L\)do
4:\(C_{l}^{V}(i)=_{l}^{V}(C_{l-1}^{V}(i),\{\{ (C_{l-1}^{W}(j),A_{ij}):j_{W}(i)\}\})\).
5:\(C_{l}^{W}(j)=_{l}^{W}(C_{l-1}^{W}(j),\{\{(C_{l- 1}^{W}(i),A_{ij}):i_{V}(j)\}\})\).
6:endfor
7:Output: Final colors \(C_{L}^{V}(i)\) for all \(i V\) and \(C_{L}^{W}(j)\) for all \(j V\).
```

**Algorithm 1** The WL test for MILP-Graphs

The WL test can be understood as a **color refinement algorithm**. In particular, each vertex in \(G\) is initially assigned a color \(C_{0}^{V}(i)\) or \(C_{0}^{W}(j)\) according to its initial feature \(v_{i}\) or \(w_{j}\). Then the vertex colors \(C_{l}^{V}(i)\) and \(C_{l}^{W}(j)\) are iteratively refined via aggregation of neighbors' information and corresponding edge weights. If there is no collision of hash functions3, then two vertices are of the same color at some iteration if and only if at the previous iteration, they have the same color and the same multiset of neighbors' information and corresponding edge weights. Such a color refinement process is illustrated by an example shown in Figure 2.

One can also view a vertex coloring as a **partition**, i.e., all vertices are partitioned into several classes such that two vertices are in the same class if and only if they are of the same color. After each round

Figure 2: An illustrative example of color refinement and partitions. Initially, all variables share a common color due to their identical node attributes, as do the constraint nodes. After a round of the WL test, \(x_{1}\) and \(x_{2}\) retain their shared color, while \(x_{3}\) is assigned a distinct color, as it connects solely to the first constraint, unlike \(x_{1}\) and \(x_{2}\). Similarly, the colors of the two constraints can also be differentiated. Finally, this partition stabilizes, resulting in \(=\{\{1\},\{2\}\}\), \(=\{\{1,2\},\{3\}\}\).

of Algorithm 1, the partition always becomes finer if no collision happens, though it may not be strictly finer. The following theorem suggests that this partition eventually stabilizes or converges, with the final limit uniquely determined by the graph \(G\), independent of the hash functions selected.

**Theorem 4.1** ([11, Theorem A.2]).: _For any \(G_{m,n}\), the vertex partition induced by Algorithm 1 (if no collision) will converge within \((m+n)\) iterations to a partition \((,)\), where \(=\{I_{1},I_{2},,I_{s}\}\) is a partition of \(\{1,2,,m\}\) and \(=\{J_{1},J_{2},,J_{t}\}\) is a partition of \(\{1,2,,n\}\), and that partition \((,)\) is uniquely determined by the input graph \(G\)._

With the concepts of color refinement and partition, we can introduce the core concept of this paper:

**Definition 4.2** (Message-passing-tractability).: _For \(G_{m,n}\), let \((,)\) be the partition as in Theorem 4.1. We say that \(G\) is message-passing-tractable (MP-tractable) if for any \(p\{1,2,,s\}\) and \(q\{1,2,,t\}\), all entries of the submatrix \((A_{ij})_{i I_{p},j J_{q}}\) are the same. We use \(_{m,n}^{}_{m,n}\) to denote the subset of all MILP-graphs in \(_{m,n}\) that are MP-tractable._

In order to help readers better understand the concept of "MP-tractable", let's examine the MILP instance shown in Figure 2. After numerous rounds of WL tests, the partition stabilizes to \(=\{\{1\},\{2\}\}\) and \(=\{\{1,2\},\{3\}\}\). According to Definition 4.2, one must examine the following submatrices to determine whether the MILP is MP-tractable:

\[A[1,1:2]=, A[2,1:2]=, A=, A=.\]

All elements within each submatrix are identical. Hence, this MILP is indeed MP-tractable. To rigorously state our result, we require the following assumption of the MILP data distribution.

**Assumption 4.3**.: \(\) _is a Borel regular probability measure on \(_{m,n}\) and \([(G)^{n}]=1\)._

Borel regularity is a "minimal" assumption that is actually satisfied by almost all practically used data distributions such as normal distributions, discrete distributions, etc. Let us also comment on the other assumption \([(G)^{n}]=1\). In Definition 3.2, the linear relaxation of \(G\) is feasible and bounded, which implies \(f_{}^{*}(G)\). However, it is possible for a linear program that is initially bounded and feasible to become infeasible upon adjusting a single variable's bounds, potentially resulting in \(f_{}^{*}(G,j,l_{j},_{j})=+\) or \(f_{}^{*}(G,j,_{j},u_{j})=+\) and leading to an infinite SB score: \((G)_{j}=+\). Although we ignore such a case by assuming \([(G)^{n}]=1\), it is straightforward to extend all our results by simply representing \(+\) as \(-1\) considering \((G)_{j}\) as a non-negative real number, thus avoiding any collisions in definitions.

Based on the above assumptions, as well as an extra assumption: \(G\) is message-passing tractable with probability one, we can show the existence of an MP-GNN capable of accurately mapping a MILP-graph \(G\) to its corresponding SB score, with an arbitrarily high degree of precision and probability. The formal theorem is stated as follows.

**Theorem 4.4**.: _Let \(\) be any probability distribution over \(_{m,n}\) that satisfies Assumption 4.3 and \([G_{m,n}^{}]=1\). Then for any \(,>0\), there exists a GNN \(F_{}\) such that_

\[[\|F(G)-(G)\|] 1-.\]

The proof of Theorem 4.4 is deferred to Appendix A, with key ideas outlined here. First, we show that if Algorithm 1 produces identical results for two MP-tractable MILPs, they must share the same SB score. That is, if two MP-tractable MILPs have different SB scores, the WL test (or equivalently MP-GNNs) can capture this distinction. Building on this result, along with a generalized version of the Stone-Weierstrass theorem and Luzin's theorem, we reach the final conclusion.

Let us compare our findings with  that establishes the existence of an MP-GNN capable of directly mapping \(G\) to one of its optimal solutions, under the assumption that \(G\) must be **unfoldable**. Unfoldability means that, after enough rounds of the WL test, each node receives a distinct color assignment. Essentially, it assumes that the WL test can differentiate between all nodes in \(G\), and the elements within the corresponding partition \((,)\) have cardinality one: \(|I_{p}|=1\) and \(|J_{q}|=1\) for all \(p\{1,2,,s\}\) and \(q\{1,2,,t\}\). Consequently, any unfoldable MILP must be MP-tractable because the submatrices under the partition of an unfoldable MILP \((A_{ij})_{i I_{p},j J_{q}}\), must be \(1 1\) and obviously satisfy the condition in Definition 4.2. However, the reverse assertion is not true: The example in Figure 2 serves as a case in point-it is MP-tractable but not unfoldable. Therefore, unfoldability is a stronger assumption than MP-tractability. Our Theorem 4.4 demonstrates that, _to illustrate the expressive power of MP-GNNs in approximating SB, MP-tractability suffices; we do not need to make assumptions as strong as those required when considering MP-GNN for approximating the optimal solution._

### MP-GNNs cannot universally represent SB beyond MP-tractability

Our next main result is that MP-GNNs do not have sufficient capacity to represent SB scores on the entire MILP space without the assumption of MP-tractability, stated as follows.

**Theorem 4.5**.: _There exist two MILP problems with different SB scores, such that any MP-GNN has the same output on them, regardless of the number of parameters._

There are infinitely many pairs of examples proving Theorem 4.5, and we show two simple examples:

\[ x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6}+x_{7}+x_{8},\] (4.1) s.t. \[x_{1}+x_{2} 1,\;x_{2}+x_{3} 1,\;x_{3}+x_{4} 1,\;x_{4}+ x_{5} 1,\;x_{5}+x_{6} 1,\] \[x_{6}+x_{7} 1,\;x_{7}+x_{8} 1,\;x_{8}+x_{1} 1,\;0  x_{j} 1,\;x_{j},\;1 j 8,\] \[ x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6}+x_{7}+x_{8},\] \[ x_{1}+x_{2} 1,\;x_{2}+x_{3} 1,\;x_{3}+x_{1} 1,\;x_{4}+ x_{5} 1,\;x_{5}+x_{6} 1, \] \[x_{6}+x_{4} 1,\;x_{7}+x_{8} 1,\;x_{8}+x_{7} 1,\;0  x_{j} 1,\;x_{j},\;1 j 8.\]

We will prove in Appendix B that these two MILP instances have different SB scores, but they cannot be distinguished by any MP-GNN in the sense that for any \(F_{}\), inputs (4.1) and (4.2) lead to the same output. Therefore, it is impossible to train an MP-GNN to approximate the SB score meeting a required level of accuracy with high probability, independent of the complexity of the MP-GNN. Any MP-GNN that accurately predicts one MILP's SB score will necessarily fail on the other. We also remark that our analysis for (4.1) and (4.2) can be generalized easily to any aggregation mechanism of neighbors' information when constructing the MP-GNNs, not limited to the sum aggregation as in Section 2.

The MILP instances on which MP-GNNs fail to approximate SB scores, (4.1) and (4.2), are not MP-tractable. It can be verified that for both (4.1) and (4.2), the partition as in Theorem 4.1 is given by \(=\{I_{1}\}\) with \(I_{1}=\{1,2,,8\}\) and \(=\{J_{1}\}\) with \(J_{1}=\{1,2,,8\}\), i.e., all vertices in \(V\) form a class and all vertices in \(W\) form the other class. Then the matrices \((A_{ij})_{i I_{1},j J_{1}}\) and \((_{ij})_{i I_{1},j J_{1}}\) are just \(A\) and \(\), the coefficient matrices in (4.1) and (4.2), and have both \(0\) and \(1\) as entries, which does not satisfies Definition 4.2.

Based on Theorem 4.5, we can directly derive the following corollary by considering a simple discrete uniform distribution \(\) on only two instances (4.1) and (4.2).

**Corollary 4.6**.: _There exists a probability distribution \(\) over \(_{m,n}\) satisfying Assumption 4.3 and constants \(,>0\), such that for any MP-GNN \(F_{}\), it holds that_

\[[\|F(G)-(G)\|].\]

This corollary indicates that the assumption of MP-tractability in Theorem 4.4 is not removable.

### 2-FGNNs are capable of universally representing SB

Although the universal approximation of MP-GNNs for SB scores is conditioned on the MP-tractability, we find an unconditional positive result stating that when we increase the order of GNNs a bit, it is possible to represent SB scores of MILPs, regardless of the MP-tractability.

**Theorem 4.7**.: _Let \(\) be any probability distribution over \(_{m,n}\) that satisfies Assumption 4.3. Then for any \(,>0\), there exists a GNN \(F_{}\) such that_

\[[\|F(G)-(G)\|] 1-.\]

The proof of Theorem 4.7 leverages the second-order folklore Weisfeiler-Lehman (2-FWL) test. We show that for any two MILPs, whether MP-tractable or not, identical 2-FWL results imply they share the same SB score, thus removing the need for MP-tractability. Details are provided in Appendix C.

Theorem 4.7 establishes the existence of a 2-FGNN that can approximate the SB scores of MILPs well with high probability. This is a fundamental result illustrating the possibility of training a GNN to predict branching strategies for MILPs that are not MP-tractable. In particular, for any probability distribution \(\) as in Corollary 4.6 on which MP-GNNs fail to predict the SB scores well, Theorem 4.7 confirms the capability of 2-FGNNs to work on it.

However, it's worth noting that 2-FGNNs typically have higher computational costs, both during training and inference stages, compared to MP-GNNs. This computational burden comes fromthe fact that calculations of 2-FONNs reply on pairs of nodes instead of nodes, as we discussed in Section 2. To mitigate such computational challenges, one could explore the use of sparse or local variants of high-order GNNs that enjoy cheaper information aggregation with strictly stronger separation power than GNNs associated with the original high-order WL test .

### Practical insights of our theoretical results

Theorem 4.4 and Corollary 4.6 indicate the significance of MP-tractability in practice. Before attempting to train a MP-GNN to imitate SB, practitioners can first verify if the MILPs in their dataset satisfy MP-tractability. If the dataset contains a substantial number of MP-intractable instances, careful consideration of this approach is necessary, and 2-FONNs may be more suitable according to Theorem 4.7. Notably, assessing MP-tractability relies solely on conducting the WL test (Algorithm 1). This algorithm is well-established in graph theory and benefits from abundant resources and repositories for implementation. Moreover, it operates with polynomial complexity (detailed below), which is reasonable compared to solving MILPs.

**Complexity of verifying MP-tractability.** To verify the MP-tractability of a MILP, one requires at most \((m+n)\) color refinement iterations according to Theorem 4.1. The complexity of each iteration is bounded by the number of edges in the graph . In our context, it is bounded by the number of nonzeros in matrix \(A\): \((A)\). Therefore, the overall complexity is \(((m+n)(A))\), which is linear in terms of \((m+n)\) and \((A)\). In contrast, solving a MILP or even calculating its all the SB scores requires significantly higher complexity. To calculate the SB score of each MILP, one needs to solve at most \(n\) LPs. We denote the complexity of solving each LP as \((m,n)\). Therefore, the overall complexity of calculating SB scores is \((n(m,n))\). Note that, currently, there is still no strongly polynomial-time algorithm for LP, thus this complexity is significantly higher than that of verifying MP-tractability.

While verifying MP-tractability is polynomial in complexity, the complexity of GNNs is still not guaranteed. Theorems 4.4 and 4.7 address existence, not complexity. In other words, this paper answers the question of _whether GNNs can represent the SB score_. To explore _how well GNNs can represent SB_, further investigation is needed.

**Frequency of MP-tractability.** In practice, the occurrence of MP-tractable instances is highly dependent on the dataset. In both Examples 4.1 and 4.2 (both MP-intractable), all variables exhibit symmetry, as they are assigned the same color by the WL test, which fails to distinguish them. Conversely, in the 3-variable example in Figure 2 (MP-tractable), only two of the three variables, \(x_{1}\) and \(x_{2}\), are symmetric. Generally, the frequency of MP-tractability depends on _the level of symmetry_ in the data -- higher levels of symmetry increase the risk of MP-intractability. This phenomenon is commonly seen in practical MILP datasets, such as MIPLIB 2017 . According to , approximately one-quarter of examples show significant symmetry in over half of the variables.

## 5 Numerical results

We implement numerical experiments to validate our theoretical findings in Section 4.

**Experimental settings**: We train an MP-GNN and a 2-FGNN with \(L=2\), where we replace the functions \(f^{l}(t_{j}^{l-1},A_{ij})\) and \(g^{l}(s_{i}^{l-1},A_{ij})\) in the MP-GNN by \(A_{ij}f^{l}(t_{j}^{l-1})\) and \(A_{ij}g^{l}(s_{i}^{l-1})\) to guarantee that they take the value \(0\) whenever \(A_{ij}=0\). For both GNNs, \(p^{0},q^{0}\) are parameterized as linear transformations followed by a non-linear activation function; \(\{p^{l},q^{l},f^{l},g^{l}\}_{l=1}^{L}\) are parameterized as 3-layer multi-layer perceptrons (MLPs) with respective learnable parameters; and the output mapping \(r\) is parameterized as a 2-layer MLP. All layers map their input to a 1024-dimensional vector and use the ReLU activation function. With \(\) denoting the set of all learnable parameters of a network, we train both MP-GNN and 2-FGNN to fit the SB scores of the MILP dataset \(\), by minimizing \(_{G}\|F_{}(G)-(G)\|^{2}\) with respect to \(\), using Adam . The networks and training scheme is implemented with Python and TensorFlow . The numerical experiments are conducted on a single NVIDIA Tesla V100 GPU for two datasets:

* We randomly generate \(100\) MILP instances, with 6 constraints and 20 variables, that are _MP-tractable_ with probability \(1\). SB scores are collected using SCIP . More details about instance generation are provided in Appendix E.
* We train the MP-GNN and 2-FGNN to fit the SB scores of (4.1) and (4.2), i.e., the dataset only consists of two instances that are _not MP-tractable_.

**Experimental results**: The numerical results are displayed in Figure 3. One can see from Figure 2(a) that both MP-GNN and 2-FGNN can approximate the SB scores over the dataset of random MILP instances very well, which validates Theorem 4.4 and Theorem 4.7. As illustrated in Figure 2(b), 2-FGNN can perfectly fit the SB scores of (4.1) and (4.2) simultaneously while MP-GNN can not, which is consistent with Theorem 4.5 and Theorem 4.7 and serves as a numerical verification of the capacity differences between MP-GNN and 2-FGNN for SB prediction. The detailed exploration of training and performance evaluations of GNNs is deferred to future work to maintain a focused investigation on the theoretical capabilities of GNNs in this paper.

**Number of parameters:** In Figure 2(b), the behavior of MP-GNN remains unchanged regardless of the number of parameters used, as guaranteed by Theorem 4.5. This error is intrinsically due to the structure of MP-intractable MILPs and cannot be reduced by adding parameters. Conversely, 2-FGNN can achieve near-zero loss with sufficient parameters, as guaranteed by Theorem 4.7 and confirmed by our numerical experiments. To further verify this, we tested 2-FGNN with embedding sizes from 64 to 2,048. All models reached near-zero errors, though epoch counts varied, as shown in Table 1. The results suggest that larger embeddings improve model capacity to fit counterexamples. The gains level off beyond an embedding size of 1,024 due to increased training complexity.

**Larger instances:** While our study primarily focuses on theory and numerous empirical studies have shown the effectiveness of GNNs in branching strategies (as noted in Section 1), we conducted experiments on larger instances to further assess the scalability of this approach. We trained an MP-GNN on 100 large-scale set covering problems, each with 1,000 variables and 2,000 constraints, generated following the methodology in . The MP-GNN achieved a training loss of \(1.94 10^{-4}\), calculated as the average \(_{2}\) norm of errors across all training instances.

## 6 Conclusion

In this work, we study the expressive power of two types of GNNs for representing SB scores. We find that MP-GNNs can accurately predict SB results for MILPs within a specific class termed "message-passing-tractable" (MP-tractable). However, their performance is limited outside this class. In contrast, 2-FGNNs, which update node-pair features instead of node features as in MP-GNNs, can universally approximate the SB scores on every MILP dataset or for every MILP distribution. These findings offer insights into the suitability of different GNN architectures for varying MILP datasets, particularly considering the ease of assessing MP-tractability. We also comment on limitations and future research topics. Although the universal approximation result is established for MP-GNNs and 2-FGNNs to represent SB scores, it is still unclear what is the required complexity/number of parameters to achieve a given precision. It would thus be interesting and more practically useful to derive some quantitative results. In addition, exploring efficient training strategies or alternatives of higher order GNNs for MILP tasks is an interesting and significant future direction.

 
**Embedding size** & 64 & 128 & 256 & 512 & 1,024 & 2,048 \\ 
**Epochs to reach \(10^{-6}\) error** & 16,570 & 5,414 & 2,736 & 1,442 & 980 & 1,126 \\
**Epochs to reach \(10^{-12}\) error** & 18,762 & 7,474 & 4,412 & 2,484 & 1,128 & 1,174 \\  

Table 1: Epochs required to reach specified errors with varying embedding sizes for 2-FGNN.

Figure 3: Numerical results of MP-GNN and 2-FGNN for SB score fitting. In the right figure, the training error of MP-GNN on MP-intractable examples does not decrease after however many epochs.