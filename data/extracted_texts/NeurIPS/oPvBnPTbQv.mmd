# Referencing Where to Focus: Improving Visual Grounding with Referential Query

Yabing Wang \({}^{1}\), Zhuotao Tian \({}^{2}\), Qingpei Guo \({}^{3}\),

**Zheng Qin \({}^{1}\), Sanping Zhou \({}^{1}\), Ming Yang \({}^{3}\), Le Wang \({}^{1}\)**

\({}^{1}\) National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center for Visual Information and Applications,

and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University

\({}^{2}\) Harbin Institute of Technology, Shenzhen, China

\({}^{3}\) AntGroup

{wyb7wyb7,qinzheng}@stu.xjtu.edu.cn

tianzhuotao@link.cuhk.edu.hk

{spzhou, lewang}@xjtu.edu.cn

{qingpei.gqp, m.yang}@antgroup.com

Corresponding author

###### Abstract

Visual Grounding aims to localize the referring object in an image given a natural language expression. Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes. However, existing research primarily focuses on designing stronger multi-modal decoder, which typically generates learnable queries by random initialization or by using linguistic embeddings. This vanilla query generation approach inevitably increases the learning difficulty for the model, as it does not involve any target-related information at the beginning of decoding. Furthermore, they only use the deepest image feature during the query learning process, overlooking the importance of features from other levels. To address these issues, we propose a novel approach, called RefFormer. It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder. By incorporating the referential query into the decoder, we can effectively mitigate the learning difficulty of the decoder, and accurately concentrate on the target object. Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.

## 1 Introduction

Visual grounding is a challenging multi-modal task that involves localizing a specific object based on a given natural language description. This task requires algorithms to comprehend fine-grained human language expressions and accurately establish correspondences with the target objects. In recent years, it has gained significant attention in research due to its potential for advancing vision-language understanding, such as cross-modal retrieval  and image captioning .

Existing works in this field typically follow the object detection framework and incorporate multi-modal fusion to tackle this task. Earlier studies [52; 12; 27; 3; 58] mainly focus on a two-stage pipeline, which first generates a set of region proposals using object detectors, and then finds the best-matched region by interacting these regions with linguistic expressions. However, the performance of this method is limited by the quality of the generated region candidates. To address this issue, some studies [48; 47; 4] adopt the one-stage pipeline, which removes the proposal generation stage. Unfortunately, these methods make dense predictions with a sliding window over pre-defined anchor boxes, resulting in sub-optimal performance due to the failure to capture object relations effectively. Recently, some methods [17; 7; 10; 21; 8; 36] inspired by the DETR  structure, which adopt a standard multi-modal transformer framework to establish the multi-modal correspondence (as shown in Figure 1 (a)). These methods predict bounding boxes of target objects directly from learnable queries, eliminating the need for extra efforts to obtain candidates, such as region proposals or predefined anchor boxes.

While these methods have shown promising results, their primary focus remains on designing stronger multi-modal decoders. By contrast, much less work has been done to improve the learnable queries, which have been gained extensive attention in the object detection field. The queries that are inputted to the decoder in these methods are typically generated through random initialization or by utilizing linguistic embeddings. We argued that this vanilla approach has two critical issues: **i)** this target-agnostic query inevitably increases the learning difficulty of the decoder. **ii)** During the query learning process, these methods tend to focus solely on the deepest visual features of the backbone, overlooking the texture information that is crucial for the grounding task and present in low and mid-level features, as emphasized by [15; 38].

Drawing from these discussions, this paper seeks to address two critical research questions: **i)**_Can we produce the target-related referential queries for the decoder to alleviate the learning difficulty that the decoder faces?_ and **ii)**_How can we effectively incorporate the multi-level visual context information into the query learning process?_ We believe that tackling these issues together would promote the learnable query to more comprehensively and accurately learn the corresponding target object information in the image for the visual grounding task.

Considering CLIP  carries rich visual-language alignment knowledge, thus we adopt it as the backbone of our approach. Existing methods typically apply CLIP on the visual grounding by fine-tuning its parameters, as CLIP's training objective is to match entire images with text descriptions, rather than capturing fine-grained alignment between regions and textual elements. This may risk losing the general knowledge of CLIP and require significant computational resources. To tackle the challenges mentioned above, we propose a novel approach called RefFormer. Our approach incorporates a query adaptation (QA) module to generate referential queries, which provide the decoder with target-related context (as illustrated in Figure 1 (b)). By strategically inserting QA

Figure 1: Comparison of DETR-like method and our proposed method for visual grounding. (a) The existing method typically adopts the random initialization queries directly into the decoder to predict the target object. (b) We introduce the query adaption module (QA) to learn target-related context progressively, providing valuable prior knowledge for the decoder. (c) The attention map of the last layer in every QA module and decoder (bottom), respectively.

module into different layers of CLIP, the query adaptively learns target-related information from multi-level image feature maps, and iteratively refines the acquired information layer by layer. Furthermore, our proposed Reformer can also act as an adapter, enabling CLIP to keep frozen and preserve the original rich knowledge. It adopts the bi-directional interaction scheme, performs the multi-modal fusion by incorporating a small number of trainable parameters, and residually injects new task-specific knowledge into CLIP throughout the entire feature extraction process. Extensive experiments conducted on five popular visual grounding benchmarks (i.e., RefCOCO/+/g [53; 31], Flickr30K , and ReferItGame ) demonstrate the superior performance of our proposed method.

Our contributions can be summarized as follows: (1) Unlike the previous methods that focus on designing sophisticated multi-modal decoders, we further improve the learning process of the learnable queries, a crucial aspect that has been overlooked in existing work. (2) We propose a query adaption module (QA), which can adaptively capture the target-related context, providing valuable referential knowledge for the decoder. (3) We conduct extensive experiments on five visual grounding benchmarks, demonstrating the effectiveness and potential of our method.

## 2 Related Work

### Visual Grounding

Visual grounding aims to ground the target objects based on natural language descriptions by understanding the given images and expressions. Early work [52; 12; 27; 3; 58] primarily focuses on two-stage methods, which formulates the grounding task as a matching task. These methods employ object detectors to generate proposal candidates and then identify the best-matched candidate based on the matching score computed between each proposal and the referring expression. For example, MAtNet  proposes to decompose the language expression into three phrase embeddings, which are used to trigger three separate visual modules. While achieving successful performance, two-stage methods heavily rely on the quality of the generated proposals. Based on this, some studies [48; 47; 4] have been dedicated to one-stage methods to remove the proposal generation stage. These methods typically fuse visual features and language features first and then densely regress the bounding box on each position of the feature map grid. For instance, FAOA  incorporates linguistic embedding into the YOLOv3 detector to establish a one-stage pipeline, balancing between accuracy and speed.

Recently, transformer-based visual grounding methods [17; 7; 21; 23; 57; 40; 50; 8; 36; 10] have emerged, which leverages the self-attention mechanism to effectively capture intra- and inter-modality relationships and achieve improved performance. Among these methods, the mainstream approach [17; 7; 10; 21; 8; 36] adopts DETR-like structures to decode bounding boxes from learnable queries. For example, Transvg  and Transvg++  employ a standard multimodal transformer framework, along with the REG token, to establish multi-modal correspondence and predict the coordinates of the referring object. Notably, the performance improvement of these methods primarily arises from the design of stronger backbones or multi-modal decoders. In this work, we focus on the design of learnable queries, which have received considerable attention in object detection field.

### Learnable Queries in DETR and Its Variants

In the object detection field, DETR presents an end-to-end object detection model that is built in an encoder-decoder transformer architecture. However, it suffers from slow training convergence. To address this issue, some follow-up works [55; 19; 49; 45; 20; 26; 24; 54] solve this issue by optimizing the learnable queries in DETR. For instance, Anchor DETR  directly treats 2D reference points as queries, while DAB-DETR  further investigates the role of queries in DETR and proposes the use of 4D anchor boxes as queries. In contrast to these model-level improvements, DN-DETR  introduces query denoising training to mitigate the instability of bipartite graph matching, which is further enhanced by DINO .

Additionally, similar research have been explored in other tasks [22; 13; 37]. For example, EaTR  formulates a video as a set of event units and treats video-specific event units as dynamic moment queries in video grounding tasks. MTR++  introduces distinct learnable intention queries generated by the k-means clustering algorithm to handle trajectory prediction across different motion modes in motion prediction tasks.

## 3 Preliminary

Considering the impressive vision-language alignment capability of CLIP, we take it as the backbone of our method to extract image and text representations, and keep the parameters frozen during training. The feature extraction process can be represented as follows:

**Image Encoder.** For an input image \(V^{H W 3}\), it is divided into \(N\) non-overlapping patches of size \(P P\), where \(N_{v}=}\). These patches are then flattened into a set of vectors, represented as \(\{_{v}^{i}^{3P^{2}}\}_{i=1}^{N}\). Next, these vectors are transformed into token embeddings using a linear projection layer \(_{e}()\). Furthermore, a classification token \(_{cls}^{D}\) is added at the beginning of the token embeddings. Subsequently, the positional embeddings \(_{v}\) are incorporated, and a layer normalization (LN) is applied. This process can be expressed as follows:

\[_{v}^{0}=LN([_{cls};_{e}(_{v})]+^ {v})\] (1)

where [;] denotes the concatenate operation. The sequence of tokens \(_{v}^{0}\) is then passed through \(L\) transformer layers. Each transformer layer comprises two submodules: the multi-head self-attention (MHSA) and the multilayer perceptron (MLP), with each submodule preceded by layer normalization.

\[}_{v}^{i} =MHSA(LN(_{v}^{i-1}))+_{v}^{i-1},\;i=1,...,L\] (2) \[_{v}^{i} =MLP(LN(}_{v}^{i}))+}_{v}^{i}\] (3)

where \(_{v}^{i}^{N D}\) denote the output of \(i\)-th transformer layer.

**Text Encoder.** Given an referring expression \(T\), it is first transformed into a sequence of word embeddings using lower-cased byte pair encoding representations \(_{t}\). The word embeddings are bracketed with the [SOS] and [EOS] tokens, producing a sequence of length \(N_{t}\). Similar to the image encoder, these tokens are summed with positional embeddings \(_{t}\) and passed through the \(L\) transformer layers to extract the text representations:

\[}_{t}^{i} =MHSA(LN(_{t}^{i-1}))+_{t}^{i-1},\;i=1,...,L\] (4) \[_{t}^{i} =MLP(LN(}_{t}^{i}))+}_{t}^{i}\] (5)

where \(_{t}^{0}=[_{sos};_{t};_{cos}]+ _{t}\), representing the word embedding layer in text encoder.

## 4 Method

The framework is shown in Figure 2. In the following, we first describe our query adaptation module in Section 4.1. We then introduce our decoder that decodes with referential query and training objectives in Section 4.2 and Section 4.3. Furthermore, we extend RefFormer to dense grounding task in Section 4.4. Finally, we provide a discussion in Section 4.5.

Figure 2: Overview of RefFormer. It adopts a DETR-like structure, consisting of a query adaptation (QA) module that seamlessly integrates into various layers of CLIP, along with a task-specific decoder. By incorporating the QA module, RefFormer can iteratively refine the target-related context and generate referential queries, which provide the decoder with prior context.

### Query Adaptation Module (QA)

In this section, we propose a QA module (as shown in Figure 3) that can generate the referential query to provide the decoder with the target-related context, thereby enhancing the decoder's grounding capabilities. _Importantly, our approach incorporates multi-level features into the query learning process, enabling the queries to capture more comprehensive target object information and can be refined layer by layer._ Furthermore, _QA can also act as an adapter_, eliminating the need to fine-tune the entire parameters of the backbone.

**Down-projection.** Considering the image and language representations \(^{i}_{v}\) and \(^{i}_{t}\) obtained from the \(i\)-th layer of the backbone, we initially use the MLP layers \(^{i}_{vd}()\) and \(^{i}_{td}()\) to project them to lower-dimensional features to reduce the computation memory:

\[^{i}_{v}=^{i}_{vd}(^{i}_{v}),\ ^{i}_{t}=^{i}_{td}( ^{i}_{t})\] (6)

**Condition Aggregation and Multi-modal Fusion (CAMF).** We randomly initialize \(N_{q}\) learnable queries \(^{N_{q} D_{l}}\), where \(D_{l}\) denotes the dimension after projected. These queries are specifically designed to capture potential target object context. Next, we concatenate these queries with the image features and input them, along with the language features into the CAMF block. Specifically, the CAMF block mainly consists of a cross-attention layer, which takes the image and query features \([;_{v}]\) and language features \(_{t}\) as the query respectively. This approach enables us to not only incorporate the expression condition into the learnable queries \(\) but also to extract relevant information from other modalities, thereby facilitating the fusion of target-related cross-modal features. Besides, we incorporate two learnable regulation tokens \(_{v},_{t}^{D_{l}}\) to modulate the final output of each QA. This process can be formalized as follows:

\[}_{v},}^{i}_{c},}^{i }_{v}=MHCA([_{v};^{i-1};^{i}_{v}],^{i} _{t},^{i}_{t})\] (7) \[}^{i}_{c}=LN(}^{i}_{c})+^ {i-1},\ }^{i}_{v}=LN(}^{i}_{v})+^{i}_{v}\] (8) \[}_{t},}^{i}_{t}=MHCA([_{ t};^{i}_{t}],^{i}_{v},^{i}_{v}),\ }^{i}_{t}=LN(}^{i}_{t})+^{i}_{t}\] (9)

where \(^{i-1}\) represents learnable queries that output from the previous QA, while \(^{0}\) are randomly initialized. The symbol \([;]\) indicates the concatenate operation, and \(MHCA(,.,)\) and \(LN()\) denote the multi-head cross-attention layers and layer normalization, respectively.

**Target-related Context Refinement (TR).** Following this, we feed the queries \(}_{c}\) and multi-modal enhanced feature maps \(}^{i}_{v}\) and \(}^{i}_{t}\) into the TR block. First, we use the queries \(}_{c}\) that have aggregated conditions to interact with the multi-modal enhanced image feature maps \(}^{i}_{v}\), refining the target-related visual context within them.

\[^{i}_{v}=MHCA(}^{i}_{c},}^{i}_{v}, }^{i}_{v}),\ ^{i}=LN(MLP(^{i}_{v}))+}^{i}_{c}\] (10)

Moreover, for feature maps \(}^{i}_{v}\) and \(}^{i}_{t}\) that have aggregated other modality information, we use the self-attention to further enhance their target-related contextual semantics:

\[}_{v},}^{i}_{v}=MHSA([ _{v};}^{i}_{v}],}^{i}_{v},}^{i}_{v}),\ ^{i}_{v}=LN(MLP(}^{i}_{v}))+}^{i}_{v}\] (11) \[}_{t},}^{i}_{t}=MHSA([ }_{v};}^{i}_{t}],}^{i}_{t},}^{i}_{t}),\ ^{i}_{t}=LN(MLP(}^{i}_{t}))+}^{i}_{t}\] (12)

**Up-projection.** Finally, we utilize MLP to restore the channel dimension of the image and language features back to their original sizes. These features are then passed as inputs to the next layer of the

Figure 3: Illustration of our proposed Query Adaption Module, which mainly consists of CAMF and TR modules to generate the referential queries and promote the multi-modal features interaction. “R” represents the feature modulation.

backbone in a residual manner. Prior to this, we utilize the regulation token to modulate the features \(_{v}\) and \(_{t}\), which helps prevent the multi-modal signal from overpowering the original signal.

\[}_{v}^{i}=_{vu}^{i}(_{v}^{i}( {}_{v}))+_{v}^{i},\ }_{t}^{i}=_{tu}^{i}(_{t}^{i}( }_{t}))+_{t}^{i}\] (13)

where \(_{vu}()\) and \(_{tu}()\) denote the MLP layer, and \(()\) denotes the sigmoid function.

Finally, by iteratively performing the above process, the queries \(Q\) can progressively focus on the target-related context, and generate the referential queries to provide the prior context for the decoder.

### Decoding with Referential Query.

**Language-guided Multi-level Fusion.** By inserting the QA at different layers of CLIP, the referential queries can be adaptively updated using the multi-level image feature maps. Additionally, to enhance the image features in decoder, we aggregate the multi-level visual features under the language guidance to yield language-aware multi-level image features. Specifically, given a multi-level image feature set \(\{}_{v}^{k}\}\) (including low, mid, and high levels), where \(k\) represents selected layer index, we inject the language features \(_{t}^{last}\) (the final output of the text encoder) into each level of image features using MHCA:

\[_{t_{}}}=_{mt}(_{t}^{last}),\ _{v}^{k}=_{mv}^{k}(}_{v}^{k})\] (14)

\[}_{v}^{k}=MHCA(_{v}^{k},_{t_{}}},_{t_{}}})+_{v}^{k},\ k\] (15)

where \(_{mt}()\) and \(_{mv}()\) denote the linear project function used to map features to the same dimension. Besides, \(_{t_{}}}\) represents the [SOS] token in \(_{t}\), which extracts the global information of the text. Subsequently, the multi-level language-aware image features are produced by simple concatenation, followed by a linear projection function \(_{vml}()\) to map to the original dimension:

\[}_{vml}=Concat(\{}_{v}^{k}\}),k \] (16) \[_{vml}=_{vml}(}_{vml})\] (17)

**Decoding.** Following, we first initialize the queries \(^{}\) with the same size as the referential query \(\), and add them together to utilize the prior context in \(\). Note that, to avoid interference from \(^{}\) during the initial stage, we initialize \(^{}\) as an all-zero matrix. Then, we concatenate the queries with the image features to interact with the language features to aggregate the condition information and produce the multi-modal feature map \(H_{mm}\). This can be represented as:

\[}_{c},}_{mm}=MHCA([_{q}()+ ^{};_{vml}],_{t},_{t})\] (18) \[_{c}=LN(}_{c})+}_{c},\ _{mm}= LN(}_{mm})+}_{mm}\] (19)

where \(_{q}()\) is the MLP layer, which regulates the significance of the query \(\). As the importance approaches zero, the query degenerate into a vanilla query. Then, we feed the queries \(_{c}\) and multi-modal feature map \(_{mm}\) into the MHCA layer to extract target embddings \(^{N_{q} D}\). It can be formulated as:

\[}=MHCA(_{c},_{mm},_{mm})\] (20) \[=LN(_{r}(}))+}\] (21)

where \(_{r}()\) represents the linear projection function.

**Grounding Head.** We built the two MLPs (\(_{box}()\) and \(_{cls}()\)) over the target embeddings \(\). The final outputs consist of the predicted center coordinates of the target object, denoted as \(b=(x,y,h,w)^{4}\), and the predicted confidence score \(y^{2}\) that encompass the target object:

\[b=_{box}(),\ y=_{cls}()\] (22)

### Training Objectives

Similar to DETR, we employ bipartite matching to find the best match between the predictions \(\{b,y\}\) and the ground-truth targets \(\{b_{tgt},y_{tgt}\}\). In our case, the class prediction is confidence prediction aims to estimate the confidence of a query containing a target object. To supervise the training, we use the box prediction losses (L1 and GIoU), and a cross-entropy loss after matching.

\[_{det}=_{iou}_{iou}(b_{gt},b)+_{L1}||b_{gt} -b||+_{ce}_{ce}(y_{gt},y)\] (23)where \(\) denotes the corresponding loss weight. Additionally, to encourage the referential queries in every QA module to effectively focus on the target-related context, we also introduce the auxiliary loss \(_{aux}\) that is similar to the above objective function to provide supervision for them. The final training objective can be defined as:

\[_{final}=_{det}+_{aux}_{aux}\] (24)

where \(_{aux}\) denotes the weight of the auxiliary loss.

### Extend to Dense Grounding

In addition to object-level grounding, our method can easily extend to the dense grounding task by incorporating a segmentation head. Specifically, similar to the MaskFormer , we utilize the MLP to transform the target embeddings \(\) into mask embeddings \(^{N_{q} D}\). The binary mask prediction \(s_{i}=^{H W}\) is then computed by performing a dot product between the mask embeddings \(\) and the multi-modal feature map \(_{mm}\) and followed by a sigmoid activation. During training, we use the mask prediction losses (Focal and Dice), which can be defined as follows:

\[_{seg}=_{focal}_{focal}(s_{gt},s)+_{dice} _{dice}(s_{gt},s)\] (25)

where \(s_{gt}\) denotes the ground-truth mask.

### Discussion

In this work, we aim to explore how to further optimize the learning process of queries. To reduce the learning difficulties posed by vanilla query, we introduce a simple query adaption module to adaptively capture target-related context and iteratively refine it. As illustrated in Figure 5, the attention maps produced by each query adaption module consistently align with our objective: to progressively focus on the target-related context and provide prior context for the decoder. It is worth noting that while "multi-level", "adapter", and "self-attention" may be extensively applied in other research fields, our approach aims to integrate them to address the challenges in visual grounding tasks, instead of designing a specific module to achieve the mentioned functions individually.

## 5 Experiment

### Datasets and Evaluation Metric

**RefCOCO/RefCOCO+/RefCOCOg.** RefCOCO  comprises 19,994 images featuring 50,000 referred objects, divided into train, val, testA, and testB sets. Similarly, RefCOCO+  contains 19,992 images with 49,856 referred objects and 141,564 referring expressions. It contains more attributes than absolute locations compared to RefCOCO, and has the same split. RefCOCOg  has 25,799 images with 49,856 referred objects and expressions. Following a common version of split , i.e., train, val, and test sets.

**Flickr30K.** Flickr30k Entities  contains 31,783 images and 158k caption sentences with 427k annotated phrases. We follow  to split the images into 29,783 for training, 1000 for validation, and 1000 for testing, and report the performance on the test set.

**ReferItGame.** ReferItGame  includes 20,000 images with 120,072 referring expressions for 19,987 referred objects. We follow  to split the dataset into train, validation and test sets, and report the performance on the test set.

**Evaluation Metric.** For referring expression comprehension (REC), we use Prec@0.5 evaluation protocol to evaluate the accuracy, which is consistent with prior works. In this evaluation, a predicted region is considered correct if its intersection-over-union (IoU) with the ground-truth bounding box is greater than 0.5. For referring expression segmentation (RES), we report the Mean IoU (MIoU) between the predicted segmentation mask and ground truth mask.

### Implementation Details

Following [7; 36], the resolution of the input image is resized to 640 \(\) 640. We employ the pre-trained CLIP as our backbone to extract both image and language features, and we freeze its parametersduring training. The model is optimized end-to-end using AdamW for 40 epochs, with a batch size of 32. We set the learning rate to 1e-4 and the weight decay to 1e-2. The experiments are conducted on V100 GPUs. The loss weight \(_{iou},_{L1},_{ce}\), and \(_{aux}\), we set to 3.0, 1.0, 1.0, and 0.1. For dense grounding, we set the parameters \(_{focal}\), and \(_{dice}\) to 5.0, and 1.0.

### Comparisons with State-of-the-art Methods

**REC Task.** For REC task, we compare the performance with the state-of-the-art REC methods, including the two-stage methods, one-stage methods, and transformer-based methods. As reported in Table 1 and Table 2, our proposed method achieves the best performance. In particular, when comparing to the transformer-based method Dynamic MDETR, which adopts the DETR-like structure and uses the same backbone as ours, we can see that our method performs better with \(+0.53\%,+1.90\%,+1.62\%\) on RefCOCO, \(+1.11\%,+2.44\%,+6.21\%\) on RefCOCO+, and \(+4.94\%,+4.18\%\) on RefCOCOg. Additionally, under multiple/extra datasets setting, our method also surpasses recent state-of-the-art methods that incorporate large language models or utilize more training data.

**RES Task.** Following RefTR  and VG-LAW , we also conduct the dense grounding experiments and report the results in Table 3 in terms of mIoU. It can be seen that our model

    &  &  &  &  \\  & val & testA & testB & val & testA & testB & val & testB & val & test \\   \\
**Two-stage:** & & & & & & & & & & \\  MAINU  & ResNet101 & 76.65 & 81.14 & 69.99 & 69.99 & 71.62 & 56.02 & 66.58 & 67.27 \\ CAIL-A-C  & ResNet101 & 78.35 & 83.14 & 71.32 & 68.09 & 73.65 & 58.03 & 67.99 & 68.67 \\ _Ref-NMS _ & ResNet101 & 80.70 & 84.00 & 76.04 & 68.25 & 78.68 & 59.42 & 70.55 & 70.62 \\ PBREC  & ResNet101 & 82.20 & 85.26 & 79.21 & 68.25 & 72.63 & 78.96 & 73.92 & 73.18 \\
**One-stage:** & & & & & & & & & \\ FAOA  & DarkNet53 & 72.54 & 74.53 & 68.50 & 56.81 & 60.23 & 49.60 & 61.33 & 60.36 \\ ReSC-Large  & DarkNet53 & 77.63 & 80.45 & 72.30 & 63.59 & 68.36 & 56.81 & 67.30 & 67.20 \\ MCN  & DarkNet53 & 80.08 & 82.20 & 74.89 & 67.16 & 72.86 & 57.31 & 66.46 & 66.60 \\ PLV-RFN  & ResNet101 & 81.93 & 84.99 & 76.25 & 71.20 & 77.40 & 61.08 & 70.45 & 71.08 \\
**Transformer-based:** & & & & & & & & & \\ TransVG  & ResNet101 & 81.02 & 82.72 & 78.35 & 64.82 & 70.70 & 56.94 & 68.67 & 67.73 \\ ReFFR  & ResNet101 & 82.23 & 85.59 & 76.57 & 71.58 & 75.96 & 62.16 & 69.41 & 69.40 \\ SeqTR  & DarkNet53 & 81.23 & 85.59 & 76.08 & 68.82 & 75.37 & 57.88 & 71.35 & 71.85 \\ QRNET  & Swin-Small & 84.01 & 85.85 & 82.34 & 72.94 & 71.61 & 63.81 & 78.80 & 73.03 \\ LABS  & ResNet50 & 82.85 & 66.76 & 78.57 & 71.16 & 76.4 & 59.82 & 71.56 & 71.66 \\ TransVG+Meta  & ViT-Base/16 & 86.62 & 88.37 & 80.97 & 75.93 & 80.45 & 66.26 & 76.18 & 76.30 \\ Dynamic MDETR  & ViT-Base/16 & 85.97 & 88.82 & 70.12 & 74.83 & 81.07 & 63.44 & 74.14 & 74.49 \\ VG-LAW  & ViT-Base/16 & 86.06 & 88.56 & 82.87 & 75.74 & 80.32 & 66.69 & 75.31 & 75.95 \\ PVD  & ViT-Base/16 & 84.52 & 86.19 & 76.17 & 73.89 & 78.41 & 64.25 & 74.13 & 71.51 \\ Ours & ViT-Base/32 & 83.97 & 87.80 & 77.45 & 73.55 & 81.09 & 62.24 & 76.33 & 75.33 \\ Ours & ViT-Base/16 & **86.52** & **90.24** & 81.42 & **76.58** & **83.69** & **67.38** & **77.80** & **77.60** \\   \\ VILLLA + authors  & ResNet101 & 82.39 & 87.48 & 74.84 & 76.17 & 81.54 & 66.64 & 76.18 & 76.71 \\ ReferTR + authors  & ResNet101 & 85.43 & 87.48 & 79.86 & 76.40 & 81.35 & 66.59 & 78.43 & 77.86 \\ MDETR + authors  & ResNet101 & 86.75 & 89.58 & 81.41 & 79.52 & 84.09 & 70.62 & 81.64 & 80.89 \\ Shik\(\)**-7B**  & ViT-large & 87.01 & 90.61 & 80.21 & 86.70 & 73.76 & 72.12 & 82.27 & 82.19 \\ Ferret + authors  & ViT-large & 87.49 & 91.35 & 82.45 & 80.78 & 73.78 & 73.14 & 83.93 & 84.76 \\ APE  & ViT-large & 85.50 & 89.10 & 81.30 & 73.40 & 80.70 & 64.40 & 83.00 & 78.00 \\ Pink + authors  & ViT-large & 88.30 & 91.70 & 84.00 & 81.80 & 88.20 & 73.90 & 83.90 & 84.30 \\ Ours + & ViT-Base/16 & 88.82 & 92.52 & 84.87 & 80.91 & 86.64 & 73.35 & 82.29 & 83.15 \\ Ours + & ViT-large & **90.91** & **93.69** & **86.56** & **83.33** & **89.00** & **75.78** & **84.97** & **84.88** \\   

Table 1: Comparisons with the state-of-the-art approaches on three benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCO.g.

  
**Methods** & **RefCOCO** & **RefCOCO+** & **RefCOCOg** \\  & val & testA & testB & val & testA & testB & val \\  MAINU  & 78.66 & 71.42 & 70.67 & 52.39 & 40.08 & 47.64 & 48.61 \\ TransVG  & 79.10 & 70.73 & 60.74 & 50.97 & 46.92 & 49.69 & 49.29 & 49.40 \\ QRNet  & 65.43 & 77.67 & 63.08 & 54.21 & 85.82 & 48.02 & 54.40 & 54.25 \\ RatTER  & 70.56 & 73.49 & 65.71 & 61.08 & 64.59 & 52.71 & 83.78 & 58.51 \\ Serft  & 67.26 & 69.79 & 64.12 & 54.14 & 58.93 & 48.19 & 55.67 &

[MISSING_PAGE_FAIL:9]

Besides, it is important to note that the referential query may not precisely focus on the target object due to the lower feature dimension in the QA module, but it still captures target-related information.

## 6 Concluding and Remarks

In this paper, we propose a novel approach, called RefFormer that can be seamlessly integrated into CLIP. The RefFormer can not only generate the referential query to provide the target-related context for decoder, but also act as the adaptor to preserve the original knowledge of CLIP and reduce the training cost. Extensive experiments demonstrate the effectiveness of our method, and visualization results illustrate the refined process of our proposed RefFormer.

**Limitations :** Although our method is specifically designed for the REC task and surpasses existing SOTA methods in REC, there is still significant room for improvement in the RES task. This is because we have not yet optimized our approach specifically for the RES task.

## 7 Acknowledgments

This work was supported in part by National Science and Technology Major Project under Grant 2023ZD0121300, National Natural Science Foundation of China under Grants 62088102, 12326608 and 62106192, Natural Science Foundation of Shaanxi Province under Grant 2022JC-41, and Fundamental Research Funds for the Central Universities under Grant XTR042021005.

   Method & val & test \\  _Backbone:_ & & \\ Swin+Bert & 75.25 (-0.64) & **75.61** (+0.29) \\  _Availinay loss:_ & & \\ _W/O \(_{aux}\)_ & 74.24 (-1.60) & 73.82 (-1.50) \\  _Learnable queries:_ & & \\ Referential query & 52.92 (-22.92) & 51.87 (-23.45) \\ Linguistic embeddings & 71.36 (-4.48) & 71.07 (-4.25) \\ Random initialization & 73.40 (-2.44) & 73.12 (-2.21) \\  Ours & **75.84** & **75.32** \\   

Table 6: Ablation studies of backbone, auxiliary loss, and learnable queries on RefCOCOg.

Figure 4: Convergence curves. Our method achieves better results with fewer training epochs on RefCOCOg.

Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2-6 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.