# Policy Improvement using Language Feedback Models

Victor Zhong

University of Waterloo

Microsoft Research

victor.zhong@uwaterloo.ca

&Dipendra Misra

Microsoft Research

&Xingdi Yuan

Microsoft Research

&Marc-Alexandre Cote

Microsoft Research

Corresponding author.

###### Abstract

We introduce Language Feedback Models (LFMs) that identify desirable behaviour -- actions that help achieve tasks specified in the instruction -- for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, imitation learning using LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, we modify LFMs to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.

## 1 Introduction

Sample-efficiency and generalizability are two primary challenges in learning instruction following agents in grounded environments . First, we want an agent that is sample-efficient: it learns from few demonstrations of how to act according to instructions. Second, we want an agent that is generalizable: it should act successfully in novel environments according to new instructions after training. Reinforcement learning (RL; Sutton and Barto ) and imitation learning (IL; Schaal , Abbeel and Ng ) are two techniques for learning agents for instruction following in grounded environments. These techniques often require large numbers of trials and errors or expensive-to-obtain expert demonstrations. Recent work show that pretrained large language models (LLMs) exhibit sample-efficient learning through prompting and in-context learning for textual  and grounded problems such as robotic control . However, for instruction following in grounded problems, current methods rely on LLMs on-line during inference, which is impractical and expensive.

We develop a sample-efficient and cost-effective technique that uses LLMs to train **Language Feedback Models (LFMs)** for policy improvement in instruction following. Figure 1 illustrates policy improvement using LFMs. Consider the task of interacting with objects in a kitchen to follow instructions shown in Figure 1(c). First, in Figure 1(a), given a grounded environment and a base policy (i.e. a behaviour cloned policy), we roll out the base policy to collect a small set of trajectories for different instructions. Next, we verbalize observations in the trajectory by describing scenes in language. For each instruction and verbalized trajectory pair, we query an LLM to provide feedback identifying which behaviour in the trajectory is productive to solving the task identifiedin the instruction (i.e. answer yes or no). For instance, given an instruction "put a clean slice of lettuce in the refrigerator", GPT-4  is able to deduce that key milestones are 1) find the lettuce, 2) slice it 3) wash it in the sink, and 4) put it in the fridge. Consequently, such an LLM is able to identify when an agent is exhibiting **desirable behaviour** conducive to solving tasks outlined in the instruction, for instance by taking the lettuce to the sink, versus undesirable behaviour, for instance by cooking the lettuce. We define desirable behaviour as productive actions that are constructive, task-beneficial, and effective in following the instruction. In other words, taking the action brings the agent closer (in terms of trajectory length) to accomplishing the task specified in the instruction. After collecting LLM feedback, we distill this world knowledge into a small and cost-effective LFM. Finally, in Figure 1(b), given a policy to improve on potentially new environments and instructions, we use the learned LFM to identify desirable actions on-line, then update the policy to imitate these actions. Crucially, this technique is sample-efficient in that it improves policy with no additional human-labeled demonstrations. Furthermore, this technique is cost-effective in that it requires few LLM interactions to collect an off-line dataset during LFM training (i.e. before deployment), as opposed to many LLM interactions on-line during policy improvement (i.e. after deployment).

Our findings are as follows: first, LFM policy improvement achieves consistent gains over strong behaviour cloned base policies on three grounded instruction following benchmarks in Touchdown , ScienceWorld , and ALFWorld . Second, we compare LFMs against prompting LLMs to directly predict what actions to take, then imitating this LLM-predicted behaviour. On all benchmarks, using LFM feedback outperforms using LLMs as experts for imitation learning, given a fixed allocation of LLM output tokens. This gain is especially pronounced in environments with larger action spaces, such as ScienceWorld, where it is much easier to critique than to generate the correct action. Third, we show that learned feedback models generalize to unseen environments with new tasks and new transition functions. After training LFMs on training environments, we use them to identify desirable behaviour on test environments, which we then imitate to adapt the policy. A single round of adaptation achieves significant gains (3.5-12.0% task-completion) across all environments.

In addition to policy improvement, using LFM feedback offers two advantages over existing techniques such as using LLMs as expert policies for imitation learning. First, LFM improves policies on-line without additional expensive calls to LLMs. Second, LFM can offer human-interpretable feedback when identifying desirable behaviour to imitate. We show in Section 5.4 that LFMs can

Figure 1: Given an environment and instructions to follow, we assume a verbalization procedure that converts observations to language descriptions. Policy improvement using Language Feedback Model involves (a) training a feedback model, then (b) using it to identify desirable behaviour for policy improvement via imitation learning. The feedback model is yellow, other models purple, and generated intermediate data green. An example of LFM-identified behaviour is shown in (c).

be easily modified to provide not only desirable behaviour but why they were desirable, thereby allowing humans to inspect and validate imitation data used for policy improvement. Source code for our environments and experiments are available at github.com/vzhong/language_feedback_models. Videos of LFM feedback are available at language-feedback-models.github.io.

## 2 Background

Language grounded instruction following.In language-grounded instruction following, an agent is given an instruction \(x\) specifying the task to achieve in the environment. Each turn, the agent receives a potentially partial observation \(o_{t}\), and takes an action \(a_{t}\) which causes the environment to transition to a new state. In the Figure 1(c) example, the agent observes a counter with objects such as a toaster, some lettuce, and a knife on top. To follow the instruction "put a clean slice of lettuce in the refridgerator", an effective agent may choose to grab a piece of lettuce. In the reinforcement learning setting, the environment additionally give the agent a reward after a desirable (positive reward) or undesirable (negative reward) action . In this work, we consider long-horizon settings with only sparse and delayed task-completion rewards. Consequently, we focus on imitation learning from demonstrations as opposed to reinforcement learning from rewards .

Online imitation learning.In online imitation learning for instruction following, we are given an expert policy \(^{*}(a|x,o)\) and learn a policy \(_{}(a|x,o)\) with parameters \(\). We first roll out the policy \(_{}\). For each step \(o_{t}^{(i)}\) of the rollout \(_{i}\), we optimize \(\) to imitate the action \(a_{t}^{(i)}\) chosen by the expert \(^{*}(a|x,o_{t}^{(i)})\) when given the same observations: \(_{}_{^{*}}[L(_{}(a|x,o_{t}^{(i) }),a_{t}^{(i)})]\). Here, \(L\) is step-wise cross-entropy between the policy's action distribution and the action chosen by the expert given the same observation: \(L(*)=-_{a^{}}[a^{}= a_{t}^{(i)}]_{}(a=a^{} x,o_{t}^{(i)})\).

Behavioural cloning.Online imitation learning assumes an expert policy that can be executed online to produce expert actions. For instance, given an expert, imitation learning assumes that this expert \(^{*}(a|x,o_{t})\) provides corrective actions \(a_{t}\) as the policy \((a|x,o_{t})\) runs. In many cases, this is impractical -- a human-in-the-loop expert is expensive and inconvenient while an LLM expert is expensive and, as we show in our experiments, inaccurate. Alternatively, in behaviour cloning (BC), we instead collect an offline dataset of expert trajectories from which to clone expert behaviour [6; 41]. BC (or offline imitation learning) only asks the expert to perform the task \(N\) times to collect

Figure 2: An example verbalization for Touchdown. We align Clip image embeddings of panorama patches and language embeddings of common noun-phrases to populate a language template. Appendix D describes this procedure in detail. The blue arrow at the top indicate the agent’s orientation while the green arrows indicate valid directions to proceed in.

trajectories \(\{_{i}\}_{i=1}^{N}\). Each \(_{i}\) consists of \(M_{i}\) steps of observations and associated expert actions: \(_{i}=[o_{1}^{(i)},a_{1}^{(i)},,o_{M_{i}}^{(i)},a_{M_{i}}^{(i)}]\) where \(a_{t}^{(i)}\) is the action chosen by the expert \(^{*}(a|x,o_{t}^{(i)})\) given the observation \(o_{t}^{(i)}\). We train policy \(_{}\) to imitate the expert action, given the same observation seen by the expert, by minimizing the following objective: \(_{}_{i}^{N}}_{t}^{M_{i}}L( _{}(a|x,o_{t}^{(i)}),a_{t}^{(i)}).\) The key distinction between BC and imitation learning is that the former optimizes over trajectories under the expert policy while the latter optimizes over trajectories under the learned policy. Consequently, while BC is offline and easily batchable, it suffers from covariate shift/exposure bias [31; 7]. Like prior work in long-horizon instruction following in grounded environments [18; 12], we use BC to warm-start a strong base policy , which we then improve using imitation learning.

## 3 Language Feedback Model

How can we leverage world knowledge in LLMs to make policy learning more sample-efficient and generalizable? In this work, we use LLMs to distill a small and cost-effective Language Feedback Model to identify desirable behaviour from a base policy (Figure 1(a)). We then improve the base policy by imitating this desirable behaviour through batched imitation learning, without need for on-line LLMs (Figure 1(b)). Appendix E provides pseudo-code for the entire procedure for policy improvement using LFMs. A natural question is why not directly use LLMs as experts for action prediction. Section 5.4 shows that the using LLMs to learn feedback models results in higher policy improvement than using LLMs as experts for action prediction. Moreover, LFMs generalize to new environments unseen during training, thereby allowing policy improvement on new environments.

### Verbalization

To leverage world knowledge in LLMs, we convert raw observations \(o\) to language descriptions \(v\) using a verbalization procedure \(V\). Figure 2 illustrates verbalization for Touchdown , where the agent navigates Google Street View panorama images based on a given natural language instruction. First, we extract all noun-phrases (NPs) from instructions in the dataset and compute their Clip language embedding. Given a visual observation, we compute Clip visual embedding for each image patch, and align it to the NP with the highest cosine similarity between Clip embeddings. We then combine aligned NPs with agent orientation to formulate an egocentric language description of the scene. This is described in more detail in Appendix D.

### Learning a feedback model

Naively learning from LLM feedback.Given a verbalization procedure \(V\), an instruction \(x\), an LLM, and a policy \(_{}\), we now describe a procedure to use the LLM's knowledge to improve \(_{}\). First, we prompt the LLM to provide feedback on whether a particular action taken by the policy \(_{}(a|x,v)\) is productive in achieving the tasks outlined in the instruction \(x\). We then improve the policy \(_{}\) by updating its parameters to imitate desirable behaviour determined by the LLM. Let : denote "such that". Let \((x,v,a)\) return \(\) if and only if the LLM feedback indicates that action \(a\) taken in verbalized state \(v\) and instruction \(x\) is productive. Given a set of instructions \(X=\{x_{i}\}_{1}^{N}\), the optimization procedure is then \(_{}_{v,a^{},x:(x,v,a^{})= }\,L(_{}(a|x,v),a^{})\). Here, instruction \(x\) is sampled from \(X\). Observations \(v\) and actions \(a^{}\) are sampled from rollouts of the policy \(_{}\).

Efficiently learning a language feedback model.While the previously described naive learning is a reasonable procedure for using LLM feedback to improve the policy, it requires calling LLMs at each step during policy improvement. This is prohibitively expensive both in terms of query cost, because LLMs capable of giving desirable feedback are expensive to run, and training time, because generating feedback usage large LLMs is slow. Instead of using the LLM at each step, we make a modification to collect LLM feedback over long horizons in batch  in order to train a small and cost-effective language feedback model.

First, for instructions \(\{x^{(1)},x^{(2)},\}\) we roll out the base policy \(_{}\) to collect a set of trajectories \(\{_{1},_{2},\}\) consisting of verbalized observations and actions taken: \(_{i}=\{v_{1}^{(i)}(x^{(i)},v_{1}^{(i)}),v_{2}^{(i)}(x^{(i)},v_{2}^{(i )}),\}\). For each \(_{i}\), we prompt the LLM for feedback on which steps were productive in achieving the instruction \(x^{(i)}\). Table 2's Lfm row shows an example of requesting feedback from GPT-4 on a rollout in ALFWorld, which is an instruction following benchmark in verbalized 3D kitchens. This LLM feedback is then parsed to identify the precise steps in which the base policy \(_{}\) took a productive action towards achieving the goals outlined in the instruction. The set of desirable behaviour is compiled into a dataset \(F\). Let \(y^{*}=(x,v,a)\) denote the feedback given by the LLM for the instructions \(x\), observations \(v\), and action \(a\). We use the dataset \(F=\{x^{(i)},v,a,y^{*} v,a_{i} x^{(i)},_{i}\}\) to train a small Language Feedback Model \(f\):

\[*{arg\,min}_{}_{(x,v,a,y^{*}) F}L(f_{} (y x,v,a),y^{*}).\] (1)

Here, \(L\) is the cross-entropy between the feedback model output \(f_{}\) and gold label \(y^{*}\) from the LLM.

Learning from language feedback.The naive learning procedure in Eq (3.2) updates the policy after each step using slow and expensive LLM feedback. Here, we instead update the policy in rounds using fast and cost-effective LFM feedback. In round \(k\), we rollout the base policy \(^{(k)}\) and use the feedback model \(f\) to collect a dataset \(D_{k}\) of desirable behaviour. Let \(a_{t}^{(k)}\) denote the action chosen by policy \(^{(k)}(a x,v_{t})\). Let \((x,v,a)=f(y= x,v,a)>f(y= {no} x,v,a)\) return whether the feedback model predicts that action \(a\) is desirable. In the \(k\)th round, we collect the dataset of desirable behaviour \(D_{k}=\{(x,v_{t},a_{t}^{(k)}) t:(x,v_{t},a_{t}^{(k)})\}\), which we combine with previously collected behaviour to update the policy via imitation learning:

\[^{*}=*{arg\,min}_{}_{v_{t},a_{t}_{i=1}^{k }D_{i}}L(^{(k)}(a x,v_{t}),a_{t}).\] (2)

In the next round, we set the base policy \(^{(k+1)}\) parameters to \(^{*}\). Should demonstrations be available, we initialize the base policy at \(k=1\) to the BC policy, and train on both demonstrations and identified desirable behaviour during subsequent rounds (i.e. \(_{i=0}^{k}D_{i}\) where \(D_{0}\) are demos used to train BC).

## 4 Related Work

Instruction following in grounded environments.Instruction following in grounded environments has been explored in settings such as navigation [11; 18; 12], game-playing [3; 48], and robotics [8; 37; 9]. However, most prior work model environment observations separately from language instructions by using specialized encoders (e.g. ResNet , Bert , Clip ), then learn from data how to associate raw observations with language instructions. Instead of solely using raw observations, more recent work verbalize raw observations to describe environments in language [38; 49; 34]. In doing so, observations and instructions can be directly jointly reasoned over using language models to achieve more efficient and generalizable learning through large-scale pretraining. We build on this last direction by verbalizing raw observations into language descriptions to train language policies. However, unlike prior work that train language models to predict next actions, we develop language feedback models that critique verbalized observations and behaviour.

LLM agents in language settings.LLMs exhibit reasoning abilities after pretraining on vast quantities of text [10; 43]. A number of recent work on LLMs language agents exploit this reasoning ability. Nakano et al. , Yao et al.  Deng et al.  train instruction following language agents to interact with web browsers to answer questions or interact with web pages. Ahn et al.  show that a language agent can be connected with verbalized robots via API interfaces for robotic control. Xie et al.  use large visual language models (VLMs) for instruction following in virtual machines, but show that LLMs with verbalization outperform VLMs. While powerful, these prior work are limited in that they require querying an expensive LLM on-line. In contrast, our work examines settings where an LLM is not available on-line. Specially, we use LLMs to collect a small set of off-line data for training LFMs. The small and cost-effective LFMs are then used to identified desirable behaviour for on-line policy improvement without additional interactions with the LLM.

Learning from feedback.Recent work enhance language agents by augmenting them with feedback. Ziegler et al. , Stiennon et al. , and Bai et al.  learn reward models from human preference to improve policies via reinforcement learning (RL). Instead of using human feedback, Bai et al.  and Lee et al.  use LLM feedback to train a separate reward model for RL for textual alignment. Huang et al. , Yao et al. , and Shinn et al.  use LLMs to reason about potential resolutions to failed actions. Yuan et al.  use LLMs to generate new prompts and corresponding responses, then use an LLM reward model to identify good prompt-response pairs for self-improvement in text generation alignment. Unlike these approaches, we do not use LLMs during on-line policy improvement. We train an initial small language feedback model from offline LLM data, then use this small feedback model for policy improvement. Additionally, we focus on-line improvement via language feedback for long-horizon, sparse reward, grounded environments instead of text generation alignment. Our procedure for batched, on-line imitation learning is similar to Dagger , which we compare to in Appendix F. However, we collect batched expert feedback to identify desirable behaviour instead of corrective actions. Klisasarov et al.  and Du et al.  are recent works that describe learning from feedback approaches complementary to ours. The former learns preference models based on pairwise observations while the latter uses LLMs to suggest exploratory goals during training. Unlike these works, which assume that the underlying goal is the same between training and inference, we consider settings where training and evaluation goals are different. That said, one can expand these approaches to generalize to unseen environments by adapting a preference model during inference  and by goal-conditioned subgoal generation during inference . However, unlike LFM, these modifications would then rely on calling LLMs during inference.

## 5 Experiments and Analysis

We evaluate using LFM s for policy improvement on three distinct language grounding benchmarks. Formally, the **environments** from a **benchmark** are distinct partially-observed Markov Decision Processes that share some (or all) of the environment dynamics but have different instructions, observations, and/or action space.

### Evaluation benchmarks

Table 1 shows examples of verbalized environments and tasks from each benchmark. Each benchmark provides distinct training and test environments to test generalization. In each environment, the agent takes actions to perform tasks outlined in a language instruction. The task is considered completed if and only if the agent solves the tasks within the preallocated number of steps. We evaluate using task-completion rate over test environments. The statistics from each benchmark is shown in Appendix D Table 6. These three benchmarks share challenges in sparse, delayed reward, partial observability, and compositional generalization to unseen tasks and environments.

**ALFWorld** is a verbalization of ALFRED , a natural language instruction following benchmark set in a 3D simulated kitchen. Here, the agent interacts with objects in kitchens to achieve compositional goals such as cleaning then microwaving potatoes. In ALFWorld , raw state information from ALFRED are used to populate language templates that describe observations in language.

   Benchmark & Context & Action \\  ALFWorld & Task: heat some egg and put it in diningtable. & go to \\  & Observation: You are at loc 12. On the sinkbasin 1, you see... & microwave 1 \\  & T-1 Observation: You are in the middle... Action: go to sinkbasin 1 & \\  & T-2 Observation:... & \\  ScienceWorld & Task: Your task is to find a(n) living thing. First, focus on the thing. Then, & open door \\  & move it it to the purple box in the bathroom. & to outside \\  & Observation: You move to the kitchen. This room is called the kitchen. In it, & \\  & you see: | the agent | a substance called air | a chair. On the chair is... & \\  & In your inventory, you see: | an orange... & \\  & T-1 Observation: The door is now open. Action: go to kitchen & \\  & T-2 Observation... Action: open door to kitchen & \\  Touchdown & Task: Follow the flow of traffic, with the row of flowers on your left and & straight \\  & make a left at the intersection. There will be a white billboard... & ahead \\  & Observation: behind you, you see: the right lane intersection, a large... & \\  & T-1 Observation: behind you, slightly... Action: slightly to your left... & \\   

Table 1: Examples of verbalization. We abbreviate long verbalized observations using “...”.

**ScienceWorld** is a textual simulation benchmark for basic science experiments . The agent interacts with objects to conduct experiments specified in natural language, such as determining the boiling temperature of a material. ScienceWorld is uniquely challenging to due the large amount of variations in task types (30), and parametric variations (10-1400) such as the specific substance to be melted. Furthermore, ScienceWorld has a substantially larger action space and longer horizon tasks.

**Touchdown** is a navigation benchmark where the agent navigates Google Street View images to follow long, compositional instructions . Touchdown requires jointly reasoning over natural images from Google Streetview with occlusion and multi-sentence natural language instructions that describe long-horizon goals. We introduce a new verbalization procedure for Touchdown based on matching noun-phrases and image patches with Clip embeddings to populate egocentric language templates. Behaviour cloning using our verbalization is detailed in Appendix D. Touchdown considers multiple subtasks, in this work we only test the agent's ability to arrive at the correct location according to the instruction.

### Methods

We train BC baseline policies using existing demonstrations. We examine three different techniques for improving the BC policy. Table 2 shows examples of LLM prompts used for each technique.

ActPred: imitation learning from LLM experts.We compare to directly using LLMs as experts to predict actions for imitation learning. First, we execute \(k\) steps of the base policy, then query the LLM for the next action \(a\) given the instruction \(x\) and the verbalized observations \(v\). We repeatedly collect examples \((x,v,a)\), then train the policy using this collected data and BC demonstrations.

Lfm: imitation learning using feedback models.We learn a small and cost-effective feedback model described in Section 3.2 to identify desirable behaviour for imitation learning. First, we learn a feedback model on the training environments. Second, we use the feedback model to identify desirable behaviour in the training environments for policy improvement via imitation learning. To collect LLM feedback for training LFMs, we collect one rollout for each environment in a benchmark and sample 10k 20-step windows from the rollouts. Crucially, we limit the amount of feedback data collected from the LLM such that the number of output tokens produced by the LLM is identical to ActPred (we use 100k GPT-2 tokens for all benchmarks). For LFM, we collect feedback for as many windows as possible until we exceed 100k output tokens, then use this feedback to train the

    \\ 
**Prompt** & Your task is: look at alarmclock under the desklamp. \\  & You see: you are in the middle of a room. looking quickly around you, you see a bed 1... \\  & what do you decide to do? available actions: examine shelf 1, examine shelf 2, go to bed... \\  & You decide to: go to desk 1. \\  & You see: you arrive at desk 1. what do you decide to do? available actions: examine desk... \\  & You decide to: \\
**LLM Output** & examine desk 1 \\   \\ 
**Prompt** & You will be shown a playthrough for solving a task. \\  & Task: put two candle in drawer. \\  & Before: You open the drawer 6. The drawer 6 is open. In it, you see nothing. \\  & Step 27. Your action: close drawer 6. Result: You close the drawer 6... \\  & Step 28. Your action: put candle 3 in/on drawer 1. Result: You put the candle 3 in... \\  & Is the player on the right track to solve the task? \\  & Answer yes or no. If yes, list the helpful steps by the step number in bullet form. \\
**LLM Output** & Yes \\  & - Step 28 \\  & - Step 29... \\   

Table 2: LLM prompts used to collect desirable behaviour. ActPred uses LLMs to directly generate actions for each step, whereas Lfm uses LLMs to generate batch feedback that identify which taken actions were productive. For brevity, we abbreviate long verbalized observations using “...”. “Before” contains the observation before the first step in the batch.

LFM. For ActPred, we label actions until we exceed 100k output tokens, then combine this labeled set with demonstrations to train the policy. This limitation on LLM interactions answers whether the feedback model is more cost-effective than direct action prediction for imitation learning.

Lfma: adaptation using feedback models.Lfm only imitates desirable behaviour in training environments. In contrast, Lfma adapts the policy to test environments. Given new test environments, we identify desirable behaviour using feedback models trained on the training environments, then perform one round of imitation learning to adapt to new test environments. This experiment tests whether language feedback models generalize to new environments, and whether we can use their feedback to adapt policies to new environments without using LLMs nor additional demonstrations.

### Experiment details

We use the GPT-4 (2023-03-15) for action prediction and feedback, and finetune 770M Flan-T5  for policy and feedback models. Verbalkized observations \(v\) contain the most recent 20 steps. We train models for 10k steps with batch 20, learning rate 5e-5, and early stopping over validation demos. For ActPred and LFM, we limit the amount of LLM usage to 100k GPT-2 tokens. Touchdown verbalization uses vit-large-patch14. Appendix H details GPU usage.

Feedback model training and inference.We train LFMs using LLM feedback over 20-step windows. We then parse feedback to identify whether the action taken in each step was productive to solving the tasks outlined in the instructions. We subsample the feedback data to obtain an even split of productive and not-productive actions. This data is split into a 80% train/20% validation dataset to train the LFM.

Policy training and inference.To train policies, we fine-tune language models to minimize token-wise cross-entropy of the ground-truth verbalized action. During inference, we consider a (potentially very large) set of plausible actions given by the environment. For each action, we evaluate the policy's language model perplexity, and choose the action with the minimum perplexity averaged over tokens.

### Results and discussion

Table 3 shows the performance of the policy behaviour cloned from demonstrations Bc, imitation learned from LLMs using action prediction ActPred, and imitation learned from Lfm. For LFMs, we show zero-shot (Lfm) as well as adaptation (Lfma) results.

LFMs improves policy performance across all benchmarks.Table 3 shows that Lfm improves upon the strong behaviour cloning baseline policy Bc in all benchmarks. Table 5 shows examples of LFM-identified desirable behaviour. This shows that LFMs are an effective means to leverage the knowledge in pretrained LLMs for policy improvement in language-grounded environments, which agree with human-identified desirable behaviour. Appendix G also compares GPT-4 to the open-source Llama 2 70B for training feedback models using human evaluation. We find that GPT-4 consistently outperforms Llama 2, which tends to identify spurious desirable behaviour.

Learning LFMs is more cost-effective than using LLMs for action prediction.Assuming the same LLM output-token quota, Table 3 compares using LLMs to train feedback models (Lfm) to using LLMs to predict actions (ActPred) for policy improvement. Specifically, ActPred tends

    & ALF & SciWorld & TD \\  Prev SOTA & 57.6 & 45.8 & 29.3 \\  GPT-4 zeroshot & 3.0 & 1.3 & 3.2 \\  Bc & \(62.6 0.4\) & \(45.8 0.6\) & \(57.5 0.3\) \\ ActPred & \(56.0 0.7\) & \(39.0 0.7\) & \(58.0 0.4\) \\ Lfm & \( 0.3\) & \( 0.5\) & \( 0.4\) \\  Lfma 1 rnd & \(74.6 1.1\) & \(49.3 0.9\) & \(62.8 1.1\) \\ Lfma 2 rnds & \(76.5 1.3\) & \(50.4 1.0\) & \(63.5 1.2\) \\   

Table 3: Task completion rates of behaviour cloning Bc, imitation learning (IL) using LLM expert ActPred, and IL using Lfm. On held-out test environments, Lfm outperforms other methods on all benchmarks. ActPred and Lfm are limited to 100k output tokens of GPT-4 interactions. Further adaptation to the new environments using LFM results in significant additional gains (Lfma). Errors are standard deviations across 3 seeds. Previous SOTA are Micheli and Fleuret  for ALFWorld, Lin et al.  for ScienceWorld, and Schumann and Riezler  for Touchdown. Unlike Lin et al. , our methods do not use ScienceWorld-specific custom room tracking nor action reranking.

to predict spurious actions, especially for complex environments with large actions spaces such as ScienceWorld. In contrast, the difficulty in identifying productive actions is independent of the action space, and Lfm consistently improves policy even with large action spaces. This shows that LFMs is a more cost-effective means use LLMs for policy improvement compared to using LLMs as expert policies for imitation learning.

LFMs generalize to new environments, allowing for policy adaptation without additional LLM usage nor demonstrations.Table 4 shows that LFMs trained during language feedback learning can accurately recognize desirable behaviour in new environments. Table 3 shows that imitating this behaviour obtains significant policy improvement across all benchmarks. This shows that LFMs generalize to new environments, which allows for policy adaptation to new environments despite not having demonstrations nor LLM access. Appendix I shows additional experiments that demonstrate correlation between LLM/LFM feedback and true state values.

LFMs can provide human-interpretable feedback, allowing human-in-the-loop verification during policy improvement.LFMs improve policy performance with succinct feedback. Here, we extend them to additionally provide detailed explanations. Consider an instruction "turn left when you see the stop sign then go to the second building on the right". Suppose that in the current step the agent proceeds straight, arriving at the stop sign. Instead of a feedback saying "yes" (i.e. the action was productive), the LFM can provide a human-interpretable explanation for why this action was productive (i.e. "yes because you found the stop sign where you are supposed to turn"). Table 5 shows that we can enhance LFM to produce detailed feedback by training detailed feedback prompted from LLMs. Specifically, we train a detailed LfmD to simultaneously identify productive actions, summarize agent intent, and suggest potential high level recovery strategies. Table 4 shows that surprisingly, LfmD that produce detailed feedback perform similarly to those that provide succinct feedback. This shows that Language Feedback Models can be used to provide accurate feedback interpretable to humans. While interpretable feedback requires more costly LLM usage, it allow for human-in-the loop verification of desirable behaviour identified by the LFM. Consequently, interpretable LFMs promotes user trust in the quality of the imitation learning data and subsequent policy behaviour.

    & ALF & SciWorld & TD \\  Lfm & 93.2 & 83.7 & 43.9 \\ LfmD & 92.0 & 82.5 & 42.5 \\   

Table 4: Feedback performance of LFM. We measure F1 score of the productive/not-productive predictions made by the learned LFM using the LLM predictions as ground truth. We observe no significant performance degradation when using a much more detailed feedback model (LfmD) that also provides explanations behind the feedback, summaries of agent behaviour, and strategy suggestions.

  
**Prompt** \\ You will be shown a playthrough for solving a task. Task: put two candle in drawer. Before: You open the drawer 6. The drawer 6 is... Step 21. Your action: close drawer 6. Result: You... Is the player on the right track to solve the task? \\ Next under heading \#Summary, summarize in one sentence what the player doing at a high level. Finally \\ under heading \#Improvement, describe how the player can improve their strategy to solve the task. \\ 
**LLM output** \\ Yes \\ - Step 28: The player locates the candles by arriving at countertop 1, where a candle is available. \\ - Step 29: The player successfully takes candle 3 from countertop 1, demonstrating the ability to collect... \\ \#Summary \\ The player is gathering candles and placing them in a drawer, working towards completing the task of... \\ \#Improvement \\ The player can improve their strategy by: not closing drawers unnecessarily... \\
**Learned feedback model output** \\ Yes. The player successfully locates the candle... \\ Yes. The player correctly takes the candle from... \\   

Table 5: Example of detailed human-interpretable feedback. The prompt and output differences between succinct feedback (Table 2) and detailed feedback are highlighted in red. Note that the second row shows shows batched LLM inference, which is used to train a step-wise feedback model. The step-wise inference of the feedback model is shown in the last row. We leave providing summaries and suggestions with the learned feedback model to future work.

Conclusion

We introduced Language Feedback Models that identify desirable behaviour for imitation learning. On three instruction following benchmarks, small and cost-effective LFMs consistently outperform BC baselines and using LLMs as experts for imitation learning, without using LLMs during policy improvement. In addition, LFMs generalize and provide significant policy adaptation gains on new environments, without using LLMs nor new demonstrations. Finally, LFMs, can provide detailed human-interpretable feedback that human verification of imitation data. We advocate for future exploration of how to exploit detailed LFMs, such as learning dense, subgoal-aware reward models for RL, and trustworthy policies with human verification.