# Conformal Prediction for Class-wise Coverage

via Augmented Label Rank Calibration

 Yuanjie Shi

Washington State University

&Subhankar Ghosh

Washington State University

&Taha Belkhouja

Washington State University

&Janardhan Rao Doppa

Washington State University

&Yan Yan

Washington State University

###### Abstract

Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on classification tasks with many and/or imbalanced classes. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-\(k\) error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and \(26.25\%\) reduction in prediction set sizes on average.

## 1 Introduction

Safe deployment of machine learning (ML) models in high stakes applications such as medical diagnosis requires theoretically-sound uncertainty estimates. Conformal prediction (CP)  is an emerging uncertainty quantification framework that constructs a prediction set of candidate output values such that the true output is present with a pre-specified level (e.g., \( 90\%\)) of the marginal or conditional probability .

A promising property of CP is the model-agnostic and distribution-free _coverage validity_ under certain notions . For example, marginal coverage is the commonly studied validity notion , while conditional coverage is a stronger notion. There is a general taxonomy to group data (i.e., input-output pairs) into categories and to study the valid coverage for each group (i.e., the group-wise validity) . This paper focuses on the specific notion of class-conditional coverage that guarantees coverage for each class individually, which is important for classification tasks with many and/or imbalanced classes (e.g., medical applications) .

In addition to the coverage validity, _predictive efficiency_ is another important criterion for CP , which refers to the size of the prediction sets. Both coverage validity and predictive efficiency are used together to measure the performance of CP methods . Since the two measures are competing , our goal is to guarantee the coverage validity with high predictive efficiency, i.e., small prediction sets . Some studies improved the predictive efficiency under themarginal coverage setting using new conformity score function  and new calibration procedures [19; 18; 26; 21]. However, it is not known if these methods will benefit the predictive efficiency for the class-conditional coverage setting. A very recent work  proposed the cluster CP method to achieve _approximate_ class-conditional coverage. It empirically improves predictive efficiency over the baseline class-wise CP method (i.e., each class is one cluster) , but the approximation guarantee for class-wise coverage is _model-dependent_ (i.e., requires certain assumptions on the model). The main question of this paper is: _how can we develop a model-agnostic CP algorithm that guarantees the class-wise coverage with improved predictive efficiency (i.e., small prediction sets)?_

To answer this question, we propose a novel approach referred to as _Rank Calibrated Class-conditional CP (RC3P)_ that guarantees the class-wise coverage with small expected prediction sets. The class-conditional coverage validity of RC3P is agnostic to the data distribution and the underlying ML model, while the improved predictive efficiency depends on very mild conditions of the given trained classifier. The main ingredient behind the RC3P method is the label rank calibration strategy augmented with the standard conformal score calibration from the class-wise CP (CCP) [58; 2].

The CCP method finds the class-wise quantiles of non-conformity scores on calibration data. To produce the prediction set for a new test input \(X_{}\), it pairs \(X_{}\) with each candidate class label \(y\) and includes the label \(y\) if the non-conformity score of the pair \((X_{},y)\) is less than or equal to the corresponding class-wise quantile associated with \(y\). Thus, CCP constructs the prediction set by uniformly iterating over _all_ candidate labels. In contrast, the label rank calibration allows RC3P to selectively iterate this class-wise thresholding subroutine only if the label \(y\) is ranked by the classifier \(f(X_{})\) (e.g., \(f()\) denotes the softmax prediction) in the top \(k_{y}\) candidates, where the value of \(k_{y}\) is calibrated for each label \(y\) individually according to the class-wise top-\(k_{y}\) error. In other words, given \(X_{}\), RC3P enables standard class-wise conformal thresholding for the sufficiently certain class labels only (as opposed to all labels). Our theory shows that the class-wise coverage provided by RC3P is agnostic to the data distribution and the underlying ML model. Moreover, under a very mild condition, RC3P guarantees improved predictive efficiency over the baseline CCP method.

**Contributions.** The main contributions of this paper are:

* We design a novel algorithm RC3P that augments the label rank calibration strategy to the standard conformal score calibration step. To produce prediction sets for new inputs, it selectively performs class-wise conformal thresholding only on a subset of classes based on their corresponding calibrated label ranks.
* We develop theoretical analysis to show that RC3P guarantees class-wise coverage, which is agnostic to the data distribution and trained classifier. Moreover, it provably produces smaller average prediction sets over the baseline CCP method .
* We perform extensive experiments on multiple imbalanced classification datasets and show that RC3P achieves the class-wise coverage with significantly improved predictive efficiency over the existing class-conditional CP baselines (\(26.25\%\) reduction in the prediction size on average on all four datasets or \(35\%\) reduction excluding CIFAR-10). The code is available at https://github.com/YuanjieSh/RC3P.

## 2 Related Work

Precise uncertainty quantification of machine learning based predictions is necessary in high-stakes decision-making applications. It is especially challenging for imbalanced classification tasks. Although many imbalanced classification learning algorithms [10; 25] are proposed, e.g., re-sampling [11; 42; 33; 54; 63] and re-weighting [28; 40], they do not provide uncertainty quantification with rigorous guarantees over predictions for each class.

Conformal prediction [62; 60] is a model-agnostic and distribution-free framework for uncertainty quantification by producing prediction sets that cover the true output with a pre-specified probability, which means CP could provide valid coverage guarantee with any underlying model and data distribution [32; 52; 16]. Many CP algorithms are proposed for regression [35; 46; 23; 17], classification [45; 1; 64; 37], structured prediction [6; 3; 13; 30], online learning [24; 7], and covariate shift [31; 53; 5] settings. _Coverage validity_ and _predictive efficiency_ are two common and competing desiderata for CP methods . Thus, small prediction sets are favorable whenever the coverage validity is guaranteed [20; 47; 18], e.g., human and machine learning collaborative systems[39; 56; 38]. Recent work1 improved the predictive efficiency for marginal coverage by designing new conformity score  and calibration procedures [19; 18; 26; 21]. These methods can be combined with class-conditional CP methods including RC3P as we demonstrate in our experiments, but the effect on predictive efficiency is not clear.

In general, the methods designed for a specific coverage validity notion are not necessarily compatible with another notion of coverage, such as object-conditional coverage , class-conditional coverage , local coverage  which are introduced and studied in the prior CP literature [61; 60; 20; 15; 9]. The standard class-conditional CP method in [58; 49] guarantees the class-wise coverage, but does not particularly aim to reduce the size of prediction sets. The cluster CP method  which performs CP over clusters of labels achieves a cluster-conditional coverage that approximates the class-conditional guarantee, but requires some assumptions on the underlying clustering model.

Our goal is to develop a provable class-conditional CP algorithm with small prediction sets to guarantee the class-wise coverage that is agnostic to the underlying model.

## 3 Notations, Background, and Problem Setup

**Notations.** Suppose \((X,Y)\) is a data sample where \(X\) is an input from the input space \(\), and \(Y=\{1,2,,K\}\) is the ground-truth label with \(K\) candidate classes. Assume \((X,Y)\) is randomly drawn from an underlying distribution \(\) defined on \(\), where we denote \(p_{y}=_{XY}[Y=y]\). Let \(f:_{+}^{K}\) denote a soft classifier (e.g., a soft-max classifier) that produces prediction scores for all candidate classes on any given input \(X\), where \(_{+}^{K}\) denote the \(K\)-dimensional probability simplex and \(f(X)_{y}\) denotes the predicted confidence for class \(y\). We define the class-wise top-\(k\) error for class \(y\) from the trained classifier \(f\) as \(_{y}^{k}=\{r_{f}(X,Y)>k|Y=y\}\), where \(r_{f}(X,Y)=_{l=1}^{K}[f(X)_{l} f(X)_{Y}]\) returns the rank of \(Y\) predicted by \(f(X)\) in a descending order, and \([]\) is an indicator function. We are provided with a training set \(_{}\) for training the classifier \(f\), and a calibration set \(_{}=\{(X_{i},Y_{i})\}_{i=1}^{n}\) for CP. Let \(_{y}=\{i:Y_{i}=y,(X_{i},Y_{i})_{}\}\) and \(n_{y}=|_{y}|\) denote the number of calibration examples for class \(y\).

**Problem Setup of CP.** Let \(V:\) denote a _non-conformity_ scoring function to measure how different a new example is from old ones . It is employed to compare a given testing sample \((X_{},Y_{})\) with a set of calibration data \(_{}\): if the non-conformity score is large, then \((X_{},Y_{})\) conforms less to calibration samples. Prior work has considered the design of good non-conformity scoring functions, e.g., [2; 50; 47]. In this paper, we focus on the scoring functions of _Adaptive Prediction Sets_ (APS) proposed in  and _Regularized APS_ (RAPS) proposed in  for classification based on the ordered probabilities of \(f\) and true label rank \(r_{f}(X,Y)\). For the simplicity of notation, we denote the non-conformity score of the \(i\)-th calibration example as \(V_{i}=V(X_{i},Y_{i})\).

Given a input \(X\), we sort the predicted probability for all classes \(\{1,,K\}\) of the classifier \(f\) such that \(1 f(X)_{(1)} f(X)_{(K)} 0\) are ordered statistics, where \(f(X)_{(k)}\) denotes the \(k\)-th largest prediction. The APS  score for a sample \((X,Y)\) is computed as follows:

\[V(X,Y)=_{l=1}^{r_{f}(X,Y)-1}f(X)_{(l)}+U f(X)_{(r_{f}(X,Y))},\]

where \(U\) is a uniform random variable to break ties. We also consider its regularized variant RAPS , which additionally includes a rank-based regularization \((r_{f}(X,Y)-k_{reg})^{+}\) to the above equation, where \(()^{+}=\{0,\}\) denotes the hinge loss, \(\) and \(k_{reg}\) are two hyper-parameters.

For a target coverage \(1-\), we find the corresponding empirical quantile on calibration data \(_{}\) defined as

\[_{1-}=t:_{i=1}^{n} [V_{i} t] 1-},\]

which can be determined by finding the \((1-)(1+n)\)-th smallest value of \(\{V_{i}\}_{i=1}^{n}\). The prediction set of a testing input \(X_{}\) can be constructed by thresholding with \(_{1-}\):

\[}_{1-}(X_{})=\{y:V(X_{ },y)_{1-}\}.\]Therefore, \(_{1-}\) gives a _marginal coverage_ guarantee : \(_{(X,Y)}\{Y_{1-}(X)\} 1-\). To achieve the _class-conditional coverage_, standard CCP  uniformly iterates the class-wise thresholding subroutine with the class-wise quantiles \(\{_{1-}^{}(y)\}_{y}\):

\[}_{1-}^{}(X_{})= \{y:V(X_{},y)_{1-}^{ }(y)\},\] (1) \[_{1-}^{}(y)= t:_{i_{y}}} 1 V_{i} t  1-}.\]

Specifically, CCP pairs \(X_{}\) with each candidate class label \(y\), and includes \(y\) in the prediction set \(}_{1-}^{}(X_{})\) if \(V(X_{},y)_{1-}^{}(y)\) holds. After going through all candidate class labels \(y\), it achieves the class-wise coverage for any \(y\):

\[_{(X,Y)}\{Y}_{1-}^{ {CCP}}(X)|Y=y\} 1-.\] (2)

CCP produces large prediction sets which are not useful in practice. Therefore, our goal is to develop a provable CP method that provides class-conditional coverage and constructs smaller prediction sets than those from CCP. We summarize all the notations in Table 3 of Appendix.

## 4 Rank Calibrated Class-Conditional CP

We first explain the proposed _Rank Calibrated Class-conditional Conformal Prediction (RC3P)_ algorithm and present its model-agnostic coverage guarantee. Next, we provide the theoretical analysis for the provable improvement of predictive efficiency of RC3P over the CCP method.

### Algorithm and Model-Agnostic Coverage Analysis

We start with the motivating discussion about the potential drawback of the standard CCP method in terms of _predictive efficiency_. Equation (1) shows that, for a given test input \(X_{}\), CCP likely contains some uncertain labels due to the uniform iteration over each class label \(y\) to check if \(y\) should be included into the prediction set or not. For example, given a class label \(y\) and two test samples \(X_{1},X_{2}\), suppose their APS scores are \(V(X_{1},y)=0.9,V(X_{2},y)=0.8\), with ranks \(r_{f}(X_{1},y)=1,r_{f}(X_{2},y)=5\). Furthermore, if \(_{1-}^{}(y)=0.85\), then by (1) for CCP, we know that \(y}_{1-}^{}(X_{1})\) and \(y}_{1-}^{}(X_{2})\), even though \(f(X_{1})\) ranks \(y\) at the #1 class label for \(X_{1}\) with a very high confidence \(f(X_{1})_{y}=0.9\) and CCP can still achieve the valid class-conditional coverage. We argue that, the principle of CCP to scans all \(y\) uniformly can easily result in large prediction sets, which is detrimental to the effectiveness of human-ML collaborative systems .

Consequently, to improve the predictive efficiency of CCP (i.e., reduce prediction set sizes), it is reasonable to include label rank information in the calibration procedure to adjust the distribution of non-conformity scores for predictive efficiency. As mentioned in the previous sections, better scoring functions have been proposed to improve the predictive efficiency for marginal coverage, e.g., RAPS. However, directly applying RAPS for class-wise coverage presents challenges: 1) tuning its hyper-parameters for each class requires extra computational overhead, and 2) fixing its hyper-parameters for all classes overlooks the difference between distributions of different classes. Moreover, for the approximate class-conditional coverage achieved by cluster CP , it still requires some assumptions on the underlying model (i.e., it is not fully model-agnostic).

Therefore, the key idea of our proposed RC3P algorithm (outlined in Algorithm 1) is to refine the class-wise calibration procedure using a label rank calibration strategy augmented to the standard conformal score calibration, to enable adaptivity to various classes. Specifically, in contrast to CCP, RC3P selectively activates the class-wise thresholding subroutine in (1) according to their class-wise top-\(k\) error \(_{y}^{k}\) for class \(y\). RC3P produces the prediction set for a given test input \(X_{}\) with two calibration schemes (one for conformal score and another for label rank) as shown below:

\[}_{1-}^{}(X_{})=y :},y)_{1-_{y }}^{}(y)}_{},\ (X_{},y)}}}}}(y)}_{}},\] (3)

where \(}_{1-_{y}}^{}(y)\) and \(}}}(y)\) are score and label rank threshold for class \(y\), respectively. In particular, \(}}(y)\) controls the class-wise uncertainty adaptive to each class \(y\) based on the top-\(k\) error \(_{y}^{}(y)}\) of the classifier. By determining \((y)\), the top \(k\) predicted class labels of \(f(X_{})\) will more likely cover the true label \(Y_{}\), making the augmented label rank calibration filter out the class labels \(y\) that have a high rank (larger \(r_{f}(X,y)\)). As a result, given all test input and label pairs \(\{(X_{},y)\}_{y}\), RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs.

**Determining \((y)\) and \(_{y}\) for model-agnostic valid coverage.** For class \(y\), intuitively, we would like a value for \((y)\) such that the corresponding top-\((y)\) error is smaller than \(\), so that it is possible to guarantee valid coverage (recall \(\{A,B\}=\{A\}\{B|A\}\)). Since a larger \((y)\) gives a smaller \(_{y}^{(y)}\) until \(_{y}^{K}=0\), it is guaranteed to find a value for \((y)\), in which the corresponding \(_{y}^{(y)}<\). As a result, given all test input and label pairs \(\{(X_{},y)\}_{y}\), RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs and filters out the class labels \(y\) that have a high rank (larger \(r_{f}(X,y)\)). The following result formally shows the principle to configure \((y)\) and \(_{y}\) to guarantee the class-wise coverage that is agnostic to the underlying model.

**Theorem 4.1**.: _(Class-conditional coverage of RC3P) Suppose that selecting \((y)\) values result in the class-wise top-\(k\) error \(_{y}^{(y)}\) for each class \(y\). For a target class-conditional coverage \(1-\), if we set \(_{y}\) and \((y)\) in RC3P (3) in the following ranges:_

\[(y)\{k:_{y}^{k}<\}, 0_{y} -_{y}^{(y)},\] (4)

_then RC3P can achieve the class-conditional coverage for every \(y\):_

\[_{(X,Y)}\{Y}_{1-}^{ {RC3P}}(X)|Y=y\} 1-.\]

### Analysis of Predictive Efficiency for RC3P

We further analyze the predictive efficiency of RC3P: under what conditions RC3P can produce a smaller expected prediction set size compared to CCP, when both achieve the same (\(1-\))-class-conditional coverage. We investigate how to choose the value of \(_{y}\) and \((y)\) from the feasible ranges in (4) to achieve the best predictive efficiency using RC3P.

**Lemma 4.2**.: _(Trade-off condition for improved predictive efficiency of RC3P) Suppose \(_{y}\) and \((y)\) satisfy (4) in Theorem 4.1. If the following inequality holds for any \(y\):_

\[_{X_{}}V(X_{},y)_{1- }^{}(y),\;r_{f}(X_{},y)(y )_{X_{}}V(X_{},y) {Q}_{1-}^{}(y),\] (5)

_then RC3P produces smaller expected prediction sets than CCP, i.e.,_

\[_{X_{}}\|}_{1-}^{}(X_{})\|_{X_{}}\| }_{1-}^{}(X_{})\|.\]

**Remark.** The above result demonstrates that when both RC3P and CCP achieve the target \(1-\) class-conditional coverage, under the condition of (5), RC3P produces smaller prediction sets than CCP. In fact, this condition implies that the combined (conformity score and label rank) calibration of RC3P tends to include less labels with high rank or low confidence from the classifier. In contrast, the CCP method tends to include relatively more uncertain labels into the prediction set, where their ranks are high and the confidence of the classifier is low. Now we can interpret the condition (5) by defining a condition number, termed as \(_{y}\):

\[_{y}=_{X_{}}V(X_{},y) _{1-}^{}(y),\ r_{f}(X_{},y) (y)}{_{X_{}}V(X_{},y) _{1-}^{}(y)}.\] (6)

In other words, if we can verify that \(_{y} 1\) for all \(y\), then RC3P can improve the predictive efficiency over CCP. Furthermore, if \(_{y}\) is fairly small, then the efficiency improvement can be even more significant. To verify this condition, our comprehensive experiments (Section 5.2, Figure 3) show that \(_{y}\) values are much smaller than \(1\) on real-world data. These results demonstrate the practical utility of our theoretical analysis to produce small prediction sets using RC3P. Note that the reduction in prediction set size of RC3P over CCP is proportional to how small the \(_{y}\) values are.

**Theorem 4.3**.: _(Conditions of improved predictive efficiency for RC3P) Define \(D=[r_{f}(X,y)(y)|Y y]\), and \(_{f}(X,y)=(X,y)+1}{2}\). Denote \(B=[f(X)_{(_{f}(X,y))}_{1-}^{}(y)|Y y]\) if \(V\) is APS, or \(B=[f(X)_{(_{f}(X,y))}+_{1-}^{ {class}}(y)|Y y]\) if \(V\) is RAPS. If \(B-D}{1-p_{y}}(-_{y}^{(y)})\), then \(_{y} 1\)._

**Remark.** The above result further analyzes when the condition in Eq (5) of Lemma 4.2 (or equivalently, \(_{y} 1\)) holds to guarantee the improved predictive efficiency. Specifically, the condition \(B-D}{1-p_{y}}(-_{y}^{(y)})\) of Theorem 4.3 can be realized in two ways: (i) making LHS \(B-D\) as large as possible; (ii) making the RHS \(}{1-p_{y}}(-_{y}^{(y)})\) as small as possible. To this end, we can set Line 7 in Algorithm 1 in the following way:

\[(y)=\{k:_{y}^{k}<\}, _{y}=-_{y}^{(y)}.\] (7)

Therefore, this setting ensures \(_{y} 1\) and as a result improves predictive efficiency.

## 5 Experiments and Results

We present the empirical evaluation of the RC3P algorithm and demonstrate its effectiveness in achieving class-conditional coverage to produce small prediction sets. We conduct experiments using two baselines (CCP and Cluster-CP), four datasets (each with three imbalance types and five imbalance ratios), and two machine learning models (trained for \(50\) epochs and \(200\) epochs, with \(200\) epochs being our main experimental setting). Additionally, we use two scoring functions (APS and RAPS) and set three different \(\) values (\( 0.1,0.05,0.01\), with \(=0.1\) as our main setting).

### Experimental Setup

**Classification datasets.** We consider four datasets: CIFAR-10, CIFAR-100 , mini-ImageNet , and Food-101  by using the standard training and validation split. We employ the same methodology as [41; 10; 14] to create an imbalanced long-tail setting for each dataset as a harder challenge: 1) We use the original training split as a training set for training \(f\) with training samples (\(n_{tr}\) is defined as the number of training samples), and randomly split the original (balanced) validation set into calibration samples and testing samples. 2) We define an imbalance ratio \(\), the ratio between the sample size of the smallest and largest class: \(=_{i}\{\#i\}}{_{i}\{\#i\}}\). 3) For each training set, we create three different imbalanced distributions using three decay types over the class indices \(c\{1,,K\}\): (a) An exponential-based decay (EXP) with \(}{K}^{}\) examples in class \(c\), (b) A polynomial-based decay (POLY) with \(}{K}+1}\) examples in class \(c\), and (c) A majority-based decay (MAJ) with \(}{K}\) examples in classes \(c>1\). We keep the calibration and test set balanced and unchanged. We provide an illustrative example of the three decay types in Appendix (Section C.3, Figure 4). Towards a more complete comparison, we also employ balanced datasets. Following Cluster-CP2, we employ CIFAR-100, Places365 , iNaturalist, and ImageNet.

**Deep neural network models.** We consider ResNet-20  as the main architecture to train classifiers for imbalanced classification datasets. To handle imbalanced data, we employ the training algorithm "LDAM" proposed by  that assigns different margins to classes, where larger margins are assigned to minority classes in the loss function. We follow the training strategy in  where all models are trained with \(200\) epochs. The class-wise performance with three imbalance types and imbalance ratios \(=0.5\) and \(=0.1\) on four datasets are evaluated (see Appendix C.1). We also train models with \(50\) epochs and the corresponding APSS results are reported in Appendix C.8.

For balanced datsets, we follow the same settings from Cluster-CP, which uses IMAGENET1K_V2 as pre-trained weights from PyTorch  and then fine-tune models with ResNet-50 for all datasets except ImageNet. For ImageNet, we use SimCLR-v2  as training models.

**CP baselines.** We consider three CP methods: **1)**CCP which estimates class-wise score thresholds and produces prediction set using Equation (1); **2)**Cluster-CP  that performs calibration over clusters to reduce prediction set sizes; and **3)**RC3P that produces prediction set using Equation (3). All CP methods are built on the same classifier and non-conformity scoring function for a fair comparison. We employ the three common scoring functions: APS , RAPS , and HPS . We set \(=0.1\) as our main experiment setting and also report other experiment results of different \(\)

    & Methods &  &  &  \\  & & \(=0.5\) & \(=0.1\) & \(=0.5\) & \(=0.1\) & \(=0.5\) & \(=0.1\) \\   &  \\   & CCP & **1.555 \(\) 0.010** & **1.855 \(\) 0.014** & **1.538 \(\) 0.010** & **1.776 \(\) 0.012** & **1.840 \(\) 0.020** & **2.629 \(\) 0.013** \\  & Cluster-CP & 1.741 \(\) 0.018 & 2.162 \(\) 0.015 & 1.706 \(\) 0.014 & 1.928 \(\) 0.013 & 1.948 \(\) 0.033 & 3.220 \(\) 0.020 \\  & **RC3P** & **1.555 \(\) 0.010** & **1.535 \(\) 0.014** & **1.538 \(\) 0.010** & **1.776 \(\) 0.012** & **1.840 \(\) 0.020** & **2.629 \(\) 0.013** \\   & CCP & **1.555 \(\) 0.010** & **1.855 \(\) 0.014** & **1.538 \(\) 0.010** & **1.776 \(\) 0.012** & **1.840 \(\) 0.020** & **2.632 \(\) 0.012** \\  & Cluster-CP & 1.714 \(\) 0.018 & 2.162 \(\) 0.015 & 1.706 \(\) 0.014 & 1.929 \(\) 0.013 & 1.787 \(\) 0.019 & 2.968 \(\) 0.024 \\  & **RC3P** & **1.555 \(\) 0.010** & **1.855 \(\) 0.014** & **1.538 \(\) 0.010** & **1.776 \(\) 0.012** & **1.840 \(\) 0.020** & **2.632 \(\) 0.012** \\   & CCP & **1.144 \(\) 0.005** & **1.324 \(\) 0.007** & **1.137 \(\) 0.003** & **1.243 \(\) 0.005** & **1.272 \(\) 0.008** & **1.936 \(\) 0.010** \\  & Cluster-CP & 1.214 \(\) 0.008 & 1.508 \(\) 0.010 & 1.211 \(\) 0.004 & 1.354 \(\) 0.005 & 1.336 \(\) 0.009 & 2.312 \(\) 0.025 \\  & **RC3P** & **1.144 \(\) 0.005** & **1.324 \(\) 0.007** & **1.137 \(\) 0.003** & **1.243 \(\) 0.005** & **1.272 \(\) 0.008** & **1.936 \(\) 0.010** \\   &  \\   & CCP & 44.224 \(\) 0.341 & 50.969 \(\) 0.345 & 49.889 \(\) 0.353 & 64.343 \(\) 0.237 & 41.494 \(\) 0.514 & 64.642 \(\) 0.535 \\  & Cluster-CP & 29.238 \(\) 0.690 & 37.592 \(\) 0.857 & 38.252 \(\) 0.353 & 52.391 \(\) 0.595 & 31.518 \(\) 0.335 & 50.883 \(\) 0.673 \\  & **RC3P** & **17.958 \(\) 0.004** & **1.294 \(\) 0.005** & **23.048 \(\) 0.005** & **33.185 \(\) 0.007** & **18.581 \(\) 0.007** & **32.699 \(\) 0.005** \\   & CCP & 44.250 \(\) 0.342 & 50.970 \(\) 0.345 & 49.886 \(\) 0.353 & 49.332 \(\) 0.236 & 48.343 \(\) 0.353 & 66.653 \(\) 0.535 \\  & Cluster-CP & 29.267 \(\) 0.612 & 37.795 \(\) 0.862 & 38.258 \(\) 0.320 & 52.374 \(\) 0.592 & 351.513 \(\) 0.325 & 50.379 \(\) 0.684 \\  & **RC3P** & **17.705 \(\) 0.004** & **21.954 \(\) 0.005** & **23.048 \(\) 0.008** & **33.185 \(\) 0.005** & **18.581 \(\) 0.006** & **32.699 \(\) 0.006** \\   & CCP & 41.351 \(\) 0.242 & 49.604 \(\) 0.344 & 48.063 \(\) 0.376 & 63.639 \(\) 0.277 & 46.125 \(\) 0.351 & 64.371 \(\) 0.564 \\  & Cluster-CP & 27.566 \(\) 0.555 & 35.258 \(\) 0.979 & 36.101 \(\) 0.056 & 56.353 \(\) 0.776 & 29.235 \(\) 0.363 & 50.519 \(\) 0.679 \\  & **RC3P** & **20.363 \(\) 0.006** & **25.212 \(\) 0.010** & **25.908 \(\) 0.007** & **36.951 \(\) 0.018** & **21.149 \(\) 0.006** & **35.606 \(\) 0.005** \\   &  \\   & CCP & 26.676 \(\) 0.171 & 26.111 \(\) 0.194 & 26.262 \(\) 0.133 & 26.159 \(\) 0.208 & 27.313 \(\) 0.154 & 25.629 \(\) 0.207 \\  & Cluster-CP & 25.8values (See Appendix C.7). Meanwhile, the hyper-parameters for each baseline are tuned according to their recommended ranges based on the same criterion (see Appendix C.2). We repeat experiments over \(10\) different random calibration-testing splits and report the mean and standard deviation.

**Evaluation methodology.** We use the target coverage \(1-\) = 90% class-conditional coverage for CCP, Cluster-CP, and RC3P. We compute three evaluation metrics on the testing set:

\(\)_Under Coverage Ratio (UCR)._

\[:=_{c[K]}_{X_{} }[y}_{1-}(x)y=c]}{_{X_{}}[y=c]}<1-/K.\]

\(\)_Average Prediction Set Size (APSS)._

\[:=_{c[K]}_{X_{}}[y=c] |}_{1-}(x)|}{_{X_{}} [y=c]}/K.\]

Note that coverage and predictive efficiency are two competing metrics in CP , e.g., achieving better coverage (resp. predictive efficiency) degenerates predictive efficiency (resp. coverage). Therefore, following the same strategy in , we choose to control their UCR as the same level that is close to \(0\) for a fair comparison over three class-conditional CP algorithms in terms of APSS. Meanwhile, to address the gap between population values and empirical ones (e.g., quantiles with \((1/})\) error bound, common to all CP methods , or class-wise top-\(k\) error \(_{y}^{k}\) with \((1/})\) error bound ), we uniformly add \(g/}\) (the same order with the standard concentration gap) to inflate the nominal coverage \(1-\) on each baseline and tune \(g\{0.25,0.5,0.75,1\}\) on the calibration dataset in terms of UCR. The detailed \(g\) values of each method are displayed in Appendix C.2. In addition, the actual achieved UCR values are shown in the complete results (see Appendix C.4, C.5, and C.6). For a complete evaluation, we add the experiments without controlling coverage on imbalanced datasets under the same setting and use the total under coverage gap (UCG) metric:

\(\)_Under Coverage Gap (UCG)._

\[:=_{c[K]}1--[Y }(X),]}{[Y=c]},0}.\]

Experiments with UCG metric evaluation are shown in the Appendix C.9.

   Conformity Score & Measure & Methods & CIFAR-100 & Places365 & iNaturalist & ImageNet \\   &  & CCP & 0.045 \(\) 0.008 & 0.012 \(\) 0.002 & 0.016 \(\) 0.001 & 0.036 \(\) 0.001 \\  & & Cluster-CP & 0.023 \(\) 0.006 & 0.029 \(\) 0.003 & 0.026 \(\) 0.003 & 0.031 \(\) 0.002 \\  & & **RC3P** & **0.006 \(\) 0.003** & **0.003 \(\) 0.001** & **0.008 \(\) 0.001** & **0.023 \(\) 0.001** \\   &  & CCP & 30.467 \(\) 0.307 & 19.698 \(\) 0.050 & 18.802 \(\) 0.023 & 101.993 \(\) 0.812 \\  & & Cluster-CP & 32.628 \(\) 0.720 & 20.818 \(\) 0.173 & 23.467 \(\) 0.049 & 66.285 \(\) 1.433 \\  & & **RC3P** & **12.551 \(\) 0.005** & **13.772 \(\) 0.005** & **12.736 \(\) 0.006** & **6.518 \(\) 0.001** \\   &  & CCP & 0.043 \(\) 0.006 & 0.013 \(\) 0.002 & 0.016 \(\) 0.020 & 0.038 \(\) 0.020 \\  & & Cluster-CP & 0.016 \(\) 0.005 & 0.036 \(\) 0.002 & 0.027 \(\) 0.003 & 0.046 \(\) 0.004 \\  & & **RC3P** & **0.002 \(\) 0.001** & **0.002 \(\) 0.001** & **0.006 \(\) 0.001** & **0.017 \(\) 0.001** \\   &  & CCP & 26.135 \(\) 0.308 & 15.694 \(\) 0.049 & 14.812 \(\) 0.042 & 37.748 \(\) 0.304 \\  & & Cluster-CP & 28.04 \(\) 0.069 & 16.750 \(\) 0.143 & 23.964 \(\) 0.419 & 16.155 \(\) 1.241 \\  & & **RC3P** & **12.586 \(\) 0.002** & **14.192 \(\) 0.001** & **13.251 \(\) 0.001** & **6.560 \(\) 0.002** \\   &  & CCP & 0.034 \(\) 0.006 & 0.015 \(\) 0.002 & **0.018 \(\) 0.002** & 0.036 \(\) 0.002 \\  & & Cluster-CP & 0.006 \(\) 0.003 & 0.029 \(\) 0.004 & 0.035 \(\) 0.002 & 0.039 \(\) 0.005 \\  & & **RC3P** & **0.003 \(\) 0.002** & **0.002 \(\) 0.001** & **0.018 \(\) 0.002** & **0.006 \(\) 0.000** \\   &  & CCP & 25.898 \(\) 0.321 & 14.020 \(\) 0.044 & **9.751 \(\) 0.033** & 24.384 \(\) 0.249 \\   & & Cluster-CP & 27.165 \(\) 0.600 & 14.530 \(\) 0.143 & 13.080 \(\) 0.374 & 8.810 \(\) 0.046 \\   & & **RC3P** & **12.558 \(\) 0.004** & **13.919 \(\) 0.004** & **9.751 \(\) 0.033** & **6.533 \(\) 0.001** \\   

Table 2: **Balanced experiment on CIFAR-100, Places365, iNaturalist, ImageNet. The models are pre-trained. UCR is controlled to \( 0.05\). RC3P significantly outperforms the best baseline with \(32.826\%\) reduction in APSS (\(\) better) on average over min{CCP, cluster-CP}.**

### Results and Discussion

We list empirical results in Table 1 for an overall comparison on four imbalanced datasets with \(=0.5,0.1\) using all three training distributions (EXP, POLYand MAJ) based on the considered APS, RAPS and HPS scoring functions. Complete experiment results under more values of \(\) are in Appendix C). Results with APS, RAPS, and HPS scoring functions on balanced datasets are also summarized in Table 2. We make the following two key observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage (their UCRs are all close to \(0\)) for all settings; (ii) RC3P significantly outperforms CCP and Cluster-CP in APSS on almost all imbalanced settings by reducing APSS with \(24.47\%\) on all four datasets and \(32.63\%\) on three datasets excluding CIFAR-10 compared with min{CCP,Cluster-CP} on average, while for balanced settings, RC3P still significantly outperforms the best baselines in terms of APSS with \(32.826\%\) APSS reduction.

To investigate the challenge of imbalanced data and more importantly, how RC3P significantly improves the APSS, we further conduct three careful experiments on imbalanced datasets. First, we report the histograms of class-conditional coverage and the corresponding histograms of prediction set size. This experiment verifies that RC3P derives significantly more class-conditional coverage above \(1-\) and thus reduces the prediction set size. Second, we visualize the normalized frequency of label rank included in prediction sets on testing datasets for all class-wise algorithms: CCP, Cluster-CP, and RC3P. The normalized frequency is defined as: \((k):=_{X_{}}_{r}(X_{,})=k,y}(x))}{_{k=1}^{K}_{X_{ {test}}}_{r}(X_{,})=k,y}(x ))}\). Finally, we empirically verify the trade-off condition number \(\{_{y}\}_{y=1}^{K}\) of Equation 6 on calibration dataset to reveal the underlying reason for RC3P producing smaller prediction sets over CCP with our standard training models (epoch \(=200\)). We also evaluate \(\{_{y}\}_{y=1}^{K}\) with less trained models (epoch \(=50\)) on imbalanced datasets in Appendix C.10. Additionally, we also repeat all three experiments on balanced datasets (i.e., the histograms of class-conditional coverage and prediction set size, the normalized frequency of label rank included in prediction sets, and \(\{_{y}\}_{y=1}^{K}\)) in Appendix C.11. Below we discuss our experimental results and findings in detail.

**RC3P significantly outperforms CCP and Cluster-CP**. First, it is clear from Table 6, 8, and 7, and 2 that RC3P, CCP, and Cluster-CP guarantee class-conditional coverage on all settings. This can also be observed by the first row of Fig 1, where the class-wise coverage bars of CCP and RC3P distribute on the right-hand side of the target probability \(1-\) (red dashed line). Second, RC3P outperforms CCP and Cluster-CP with \(24.47\%\) (four datasets) or \(32.63\%\) (excluding CIFAR-10) on imbalanced datasets and \(32.63\%\) on balanced datasets decrease in terms of average prediction set size for the same class-wise coverage. We also report the histograms of the corresponding prediction set sizes in the second row of Figure 1, which shows (i) RC3P has more concentrated class-wise coverage distribution than CCP and Cluster-CP; (ii) the distribution of prediction set sizes produced by RC3P is globally smaller than that produced by CCP and Cluster-CP, which

Figure 1: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(=0.1\) and models are trained with \(200\) epochs on four imbalanced datasets with imbalance type EXP \(=0.1\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

is justified by a better trade-off number of \(\{_{y}\}_{y=1}^{K}\) as shown in Figure 3. Note that the class-wise coverage and the corresponding prediction set sizes RC3P overlap with CCP on CIFAR-10 in Figure 1.

**Visualization of normalized frequency.** Figure 2 illustrates the normalized frequency distribution of label ranks included in the prediction sets across various testing datasets. It is evident that the distribution of label ranks in the prediction set generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. This indicates that RC3P more effectively incorporates lower-ranked labels into prediction sets, as a result of its augmented rank calibration scheme.

**Verification of \(_{y}\).** Figure 3 verifies the validity of Equation (6) on testing datasets and confirms the optimized trade-off between the coverage with inflated quantile and the constraint with calibrated label rank leads to smaller prediction sets. It also confirms that the condition number \(\{_{y}\}_{y=1}^{K}\) could be evaluated on calibration datasets without testing datasets and thus decreases the overall computation cost. We verify that \(_{y} 1\) for all settings and \(_{y}\) is much smaller than \(1\) on all datasets with large number of classes.

## 6 Summary

This paper studies a provable conformal prediction (CP) algorithm that aims to provide class-conditional coverage guarantee and to produce small prediction sets for classification tasks with many and/or imbalanced classes. Our proposed RC3P algorithm performs double-calibration, one over conformity score and one over label rank for each class separately, to achieve this goal. Our experiments clearly demonstrate the significant efficacy of RC3P over the baseline class-conditional CP algorithms on both balanced and imbalanced classification data settings.

**Acknowledgments.** This research was supported in part by United States Department of Agriculture (USDA) NIFA award No. 2021-67021-35344 (AgAID AI Institute) and by NSF CNS-2312125 grant.

Figure 3: Verification of condition numbers \(\{_{y}\}_{y=1}^{K}\) in Equation 6 with imbalance type EXP, \(=0.1\) when \(=0.1\) and models are trained with \(200\) epochs. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3P produces smaller prediction sets than CCP using calibration on both non-conformity scores and label ranks.

Figure 2: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(=0.1\) for imbalance type EXP when \(=0.1\) and models are trained with \(200\) epochs. It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.