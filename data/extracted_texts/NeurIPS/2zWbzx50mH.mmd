# Compact Proofs of Model Performance via Mechanistic Interpretability

Jason Gross\({}^{*}\) Rajashree Agrawal Thomas Kwa Euan Ong Chun Hei Yip Alex Gibson Soufliane Noubir

Corresponding author. Please direct correspondence to jgross@mit.edu. These authors contributed equally to this work. These authors contributed equally to this work.

###### Abstract

We propose using mechanistic interpretability - techniques for reverse engineering model weights into human-interpretable algorithms - to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of-\(K\), validating proof transferability across 151 random seeds and four values of \(K\). We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.

## 1 Introduction

One approach to ensuring the safety and reliability of powerful AI systems is via formally verified proofs of model performance . If we hope to deploy formal verification on increasingly large models  with powerful emergent capabilities  across more diverse and broader domains , we will need _compact_ proofs of generalization bounds on _specific_ models that certify _global_ robustness. However, existing approaches tend to use proof strategies that suffer from bad asymptotic complexity, while verifying either generalization properties of training procedures or local robustness properties of specific models.

One key challenge to verification is that neural network architectures are highly expressive , and models with similar training procedure and performance may still have learned significantly different weights . This expressivity makes it difficult to adequately _compress_ explanations of global model behavior in ways that _correspond_ closely enough to the model's actual mechanisms to be useful for efficient verification without being too _lossy_, especially when using only knowledge of the architecture or training procedure. We propose verifying model performance using understanding derived from _mechanistic interpretability_ (Section 2) - that is, reverse engineering the specific implementation of the algorithm from the learned weights of particular models. Knowledge of the specific implementation allows us to construct less lossy simplifications of the model, and more efficiently reason about model performance over possible inputs.

In this work, we provide a case study of translating mechanistic interpretations into compact proofs. We train an attention-only transformer on a Max-of-\(K\) task with 151 random seeds (Section 3), andthen reverse engineer the models using standard mechanistic interpretability techniques. We use our understanding to define a set of 102 different computer-assisted proof strategies with varying tightness of bound and with different asymptotic complexity and number of required FLOPs (Section 4).4 We validate our technique against an additional 604 models for varying values of \(K\) (Appendix A.2.1).

We define a quantitative metric to assess the mechanistic understanding used in a proof strategy by the dimensionality of the function space that the proof strategy must consider, which we deem the _unexplained dimensionality_ of the proof strategy (Sections 5.1, and A.5). Using this metric, we find a negative relationship between proof length and degree of understanding. We qualitatively examine proof strategies to confirm and explain this relationship, finding that more compact proofs both require and provide more mechanistic understanding. We also find suggestive evidence that the trade-off between proof length and tightness of bound is modulated by the faithfulness of the mechanistic understanding used to derive the proof (Section 5.2).5

However, we also identify compounding structureless error terms as a key challenge for generating compact proofs on model behavior (Sections 5.3, and G.2.5). The implementation of algorithms inside of neural networks may contain components that defy mechanistic understanding and appear to us as "noise". When we don't know how noise composes across model components, establishing a bound requires pessimizing over the ways the composition could occur. Worst-case noise can quickly grow even when the empirical noise is small, leading to vacuous performance bounds.

## 2 Mechanistic interpretability for proofs

**Generalization bounds on global performance** In the style of prior mechanistic interpretability evaluation work , we target theorem templates that establish bounds on the expected global performance of the model. Let \(:X Y\) be a model (here assumed to be a neural network), \(\) be a probability distribution over input-label pairs \((l,) L X\), notated as \(|_{X}\) when marginalized over labels, and \(f:L Y\) be a scoring function for evaluating the performance of the model. Then, we seek to establish lower bounds \(b\) on the expected \(\) as the form:

\[:=_{(l,)}[f(l,( ))] b.\] (1)

Figure 1: We construct proofs using different degrees of mechanistic interpretation. (Left) The models we consider in this paper are one-layer attention-only transformers, and so contain three "paths‚Äù: the OV circuit, the QK circuit, and the direct path. (Right) For the brute-force proof (Section 4.3.1), we treat the model as a black box and thus need to check all possible combinations of inputs. For the cubic proof (Section 4.3.1), we decompose the model into its three corresponding paths, but still check the correctness of each path via brute force. Finally, in some subcubic proofs (Section 4.3), we use all parts of the mechanistic interpretation presented in Section 3. (Bottom) For each of the three categories of proof, we report the number of FLOPs used in computing the certificate (lower=better, Appendix A.6), lower bound on model accuracy (higher=better), effective dimension of the unexplained parts of the model (lower=better, Appendix A.5), and asymptotic complexity of the proof strategy as we scale the inputs and model (lower=better). Significantly more compact proofs have vacuous accuracy bounds by default. Using more mechanistic understanding allows us to recover some, but not all, of the accuracy bounds on these more compact proofs, as our understanding is not fully faithful to the model internals.

As \(f\) can be any metric, this is a fully general template for theorems that can capture any aspect of model performance for which we have a formal specification. However, in this work we restrict \(f\) to be the accuracy and \(|_{X}\) to be uniform, so our theorems lower bound the accuracy of the model. Our proof methodology generalizes straightforwardly to other input distributions (Appendix A.8), and only a little work is required to generalize from accuracy to log-loss (Appendix A.11).

**Proof template** The proofs of model performance in this work have two components: a computational component \(C:\) model weights \(\) and a non-computational component \(Q\) arguing that for any model \(^{}\), \(C(^{})_{(l,)}f(l, ^{}())\), thus implying that \(C\) generates a valid lower bound for the performance of \(\). The whole proof is \(Q\) paired with a trace of running \(C\) that certifies its output on \(\).6 Here, \(b=C()\). As even the size of the model parameters is much larger than any reasonable \(Q\), we approximate the length of a proof pair \(C,Q\) by the length of a trace of \(C()\).

**Proof compactness vs. tightness of bound** Different proof strategies make different tradeoffs between compactness and tightness of bound. For example, consider two extreme proof strategies: We can "prove" a vacuous bound using a null proof. On the other hand, in the brute-force proof, we simply run the model on the entirety of \(\) to achieve \(b=\), albeit with a very long proof.

We quantify the length of \(C()\) using two metrics: the _asymptotic time complexity_ of \(C\) as we scale the size of the model and the input \(\), as well as the empirical average _number of floating point operations_ required to evaluate \(C(^{})\) over a given set of models \(\{_{i}\}\). We measure _tightness of bound_ of \(C()\) using the ratio of the bound to the true accuracy: \(b/\).

**Proof as pessimal ablation** A standard way of assessing the faithfulness of mechanistic interpretability is by ablating the parts of the model that your interpretation does not explain [54; 6; 23]. In this framework, proofs can be thought of as performing a _pessimal ablation_ over the unexplained parts of the model - we set the remaining components of the model (the "noise" or error terms) to values over \(X\) that minimize the performance of the model. However, the number of ablations required for a complete argument might be quite high. Thus, we construct _relaxations_ (Appendix A.4) over input sequences, such that performing pessimal ablations on a smaller number of relaxed input sequences is sufficient to lower bound the performance on \(\).

## 3 Experimental setting

We study our approach to generating compact proofs in a simple toy setting: Max-of-\(K\).

**Model Architecture** We study one-layer, one-head, attention-only transformers with no biases but with learned positional embeddings, with vocabulary size \(d_{}\), model and head dimension \(d=d_{}=d_{}\), and context length \(n_{}:=k\). The model parameters consist of the \(n_{} d_{}\) positional embedding \(P\); the \(d_{} d_{}\) token embed \(E\); the \(d_{} d_{}\) query, key, value, and output matrices of the attention head \(Q\), \(K\), \(V\), and \(O\); as well as the \(d_{} d_{}\) unembed matrix \(U\). We assume (as is standard in language modeling) that \(d_{}<d_{}\).

For an \(n_{} d_{}\) one-hot encoding \(=[x_{0},x_{1},,x_{n_{}-1}]\) of an input sequence \(=[t_{0},t_{1},,t_{n_{}-1}]\), we compute the logits of the model as follows:

\[h^{(0)} =E+P (n_{} d_{})\] \[ =h^{(0)}QK^{T}h^{(0)}{}^{T}/ (n_{} n_{})\] \[h^{(1)} =^{*}() h^{(0)}VO+h^{(0)} (n_{} d_{})\] \[() ==h^{(1)}_{n_{}-1}U (d_{})\]

where \(^{*}\) is the masked softmax function used in causal attention. Because we only look at outputs of the model above the final sequence position \(i=n_{}-1\), we also denote this position as the "query position" and the value of the token in this position as \(t_{}\), one-hot encoded as \(x_{}\). The model's prediction is the token corresponding to the max-valued logit \(_{}\).

**Task** Specifically, we study the setting with \(n_{}=k=4\) because it is the largest sequence length for which we can feasibly evaluate the brute-force proof. We set hidden dimension \(d_{}=32\)and a vocabulary of size \(d_{ vocab}=64\) comprising integers between 0 and 63 inclusive. For an input sequence \(\), we denote the _true_ maximum of the sequence by \(t_{ max}\). Outputting the correct behavior is equivalent to outputting logits \(\) such that \(_{t^{*}}:=_{t^{*}}-_{ max}<0\) for all \(t^{*} t_{ max}\). We trained 151 models on this task. Models achieved average accuracy \(0.9992 0.0015\) over the entire distribution.

**Path decomposition** Following prior work , we expand the logits of the model and split the paths through the model into three components - the QK circuit, the OV circuit, and the direct path:

\[()=^{*}E+P_{ query})QK^{T}(E+P)^{T}}_{ {QK circuit}}/E+P) VOU}_{}+E+P_{ query})U}_{}\] (2)

Intuitively, the QK circuit determines _which_ tokens the model attends to from a particular query token and sequence position, while the OV circuit _processes_ the tokens and sequence positions the model attends to. The direct path is simply the skip connection around the attention head.

We further divide the QK and OV circuits into token (position-independent) and position-dependent components. Let \(P_{ avg}=_{i}P_{i}/n_{}\) be the average position embeds across positions (of size \(d_{ model}\)), and let \(}\) denote either \(_{n_{}} P_{ avg}\) or \(_{d_{ vocab}} P_{ avg}\) depending on context, the result of broadcasting \(P_{ avg}\) back into the shape of \(P\) or \(E\) (that is, \(n_{} d_{ model}\) or \(d_{ vocab} d_{ model}\)). Similarly, let \(_{q}=_{d_{ vocab}} P_{ query}\) be the result of broadcasting \(P_{ query}\). Then for one-hot encoded \(\), we can rewrite the QK and OV circuits, as well as the direct path, as follows:

\[ =x_{ query}_{q}QK^{T}}^{T}}_{}^{T}+_{q}QK^{T} }^{T}}_{}\] \[ =}VOU}_{}+ }VOU}_{}=x_{ query }_{q}U}_{}\]

where \(}=P-}\) and \(}=E+}\) and \(_{q}=E+_{q}\) (since \(h^{(0)}=}+}\)).

### Mechanistic interpretation of learned models

Using standard empirical mechanistic interpretability techniques, we interpret one of our learned models (our "mainline" model) by independently examining the QK and OV circuits and the direct path.7 We find that the model outputs the largest logit on the true max token \(t_{ max}\) by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit. We then quantitatively confirm that these interpretations hold for all 151 models by reporting the mean plus minus standard deviation for various summary statistics. Plots for this section are available in Appendix B.2.

**QK circuit** By qualitatively examining the position-independent QK component \( EQKE\), we find the amount of pre-softmax attention paid to a key token is approximately independent of the value of the query token \(t_{ query}\), and increases monotonically based on the size of the key token. We confirm this hypothesis by performing a singular-value decomposition (SVD) of the \( EQKE\) matrices (Appendix G.2.3), and find that it contains a single large rank-one component with singular value around \(7800 380\), around \(620 130\) times larger than the second largest component with singular value \(13 3\). The left (query-side) singular vector is approximately constant in all dimensions, with value \(0.1243 0.0003}{{}}}\). The right (key-side) singular vector of this component is monotonically increasing as we increase the size of the key token, with (\(1/\)-scaled) pre-softmax attention increasing by an average of \(1.236 0.056\) when the key token increases by 1.8

In comparison, each \(1/\)-scaled entry of the position-dependent QK component \( EQKP\) has negligible size (average \(0.31 0.18\)), suggesting that \( EQKP\) is unimportant to the functioning of the model.

Figure 2: The models in our setting implement Max-of-\(K\) by attending exponentially more to larger tokens and copying the attended-to tokens (Section 3.1).

We confirm this by zero ablating \(\), which changes the models' accuracies from \(0.9992 0.0015\) to \(0.9993 0.0011\). Combined with our interpretation of \(\), this implies that the attention pattern of the model depends only on the token values and not the ordering of the sequence.

**OV circuit** Then, by qualitatively examining the position-independent OV component \(\), we see that it has large positive entries along the diagonal. In fact, the entry along the diagonal is the largest in the row for all rows corresponding to \(t>6.6 1.2\). Since each entry in the sequence is uniformly sampled and \(d_{}=64\), this means that \(\) is a good approximation for the identity matrix for all but \((7/64)^{4} 1.2 10^{-2}\,\%\) of the sequences.

As with the position-dependent QK component, the position-dependent OV component \(\) also has negligible size and is unimportant to model performance. Taken together with the above results on \(\), this suggests that the attention head copies the tokens it attends to.

**Direct path** As with the two position-dependent components, the entries in \(\) have small absolute magnitude \(2.54 0.20\),9 and contribute negligibly to model performance.

## 4 Proofs of model performance

In this section we describe intuitions for three categories of proof that are developed around different mechanistic interpretations and methods for using the interpretations. The strategies result in proofs of different complexities with varying bound tightness (Table 1). We provide detailed theorem statements, proofs, algorithms, and explanations of proof search in Appendices C, D, E, F, and G.

Our theorem statements for \(Q\) will all be of the form

\[^{},C_{}}(^{ })_{|_{X}}f(t_{}, ^{}()).\]

We leave implicit the traces of running \(C_{}}\) on our specific models to give the overall theorem. We report the computational complexity or estimated FLOPs of running \(C_{}}\) as approximations for our proof lengths.

### The brute-force baseline

We start by considering the brute-force proof (Appendix D), which treats the model as a black box and evaluates it on all possible sequences.10 However, this proof strategy has bad asymptotic complexity and is untenable for larger models and larger input distributions. So in subsequent sections, we use knowledge of the model drawn from the interpretation in Section 3.1 to derive more compact proofs.

### A cubic proof

Next, we use the fact that the model is composed of the direct path and the QK and OV circuits (Section 3) to decrease the number of sequences that we need to consider, and the fact that only the position-independent components \(\) and \(\) contribute meaningfully to performance (Section 3.1) to pessimize over sequence ordering.

First, let a pure sequence \(\) be a sequence with at most three distinct tokens: the max token \(t_{}\), the final token \(t_{} t_{}\), and optionally a third token \(t^{}<t_{}\), and let \(^{}\) be the set of all pure sequences in \(X\).11 For a given input sequence \(\), define the adjacent pure sequences \(()\) as the set of sequences that share the same max and query token, and only take on values in \(\):

\[()=\{^{}\,_{i} _{i}=t_{},\;_{}=t_{}, i<n_{ },\;_{i}\}\]

Using the convexity of softmax and the fact that the model contains three paths, we can show that one-layer attention-only transformers satisfies a variant of the following convexity property: for a given \(\), if \(()\) is correct for all \(()\), then \(()\) is correct. That is, for these transformers, we can bound the accuracy on all sequences by evaluating \(\) on only the \(O(d_{}{}^{3}(n_{}-1)!)\)pure sequences. This allows us to bound the accuracy of our actual \(\) on all \({d_{}}^{n_{}}\) sequences, while evaluating it on \(O({d_{}}^{3}(n_{}-1)!)\) sequences.

We can reduce the number of sequences that we need to evaluate by pessimizing over the order of a sequence. For a given tuple of \((t_{},t_{},t^{})\), there are \((n_{}-1)!\) pure sequences, corresponding to the permutations of the tuple. Pessimizing over the order of sequences reduces the number of sequences to consider for each \((t_{},t_{},t^{})\) tuple to the number of \(t^{}\) in the pure sequence, and the total number of sequences to \(O({d_{}}^{3}n_{})\). By precomputing the five component matrices \(\), \(\), \(\), \(\), \(\) and cleverly caching intermediate outputs, we can reduce the additional work of each sequence to the \(O(n_{})\) required to compute the softmax over \(n_{}\) elements, resulting in asymptotic complexity \(O({d_{}}^{3}{n_{}}^{2})\) (Theorem 12, additional details in Appendix E).

### Sub-cubic proofs

We now consider proofs that are more compact than \(O({d_{}}^{3})\). These require avoiding iteration over any set of size \(O({d_{}}^{3})\) (e.g. the set of pure sequences) and performing operations that take \(O({d_{}})\) time on each of \(O({d_{}}^{2})\) combinations. Unfortunately, some methods of avoiding these operations can lead to vacuous bounds (i.e. accuracy lower bounds near \(0\%\)). In order to recover non-vacuous bounds, we introduce two tricks: the "mean+diff trick" to better approximate the sum of two components with unequal variance, and the "max row diff trick" to improve upon the low-rank approximations for \(\) and \(\). We consider applying variants of these tricks at different locations in the naive subcubic proof, leading to 100 distinct subcubic proof strategies. See Appendix G.2 for a formal description of these strategies.

#### 4.3.1 Removing cubic-time computations

**Reducing the number of cases by pessimizing over sufficiently small tokens** Previously, we considered \(({d_{}}^{3}n_{})\) pure sequences \(\), with \(\) parameterized by \((t_{},t_{},t^{},c)\). Recall from our mechanistic interpretation in Section 3.1 that the pre-softmax attention paid from \(t_{}\) to a key token \(t^{}\) is broadly invariant in \(t_{}\) and increases roughly linearly with the size of \(t^{}\). This allows us to pessimize over the OV circuit over all "sufficiently small" tokens.

  
**Description** & **Complexity Cost** & **Bound** & **Est.** & **Unexplained** \\
**of Proof** & & & **FLOPs** & **Dimensions** \\  Brute force & \((v^{k+1}kd)\) & \(0.9992 0.0015\) & \(2^{47}\) & \(2^{30}\) \\  Cubic & \((v^{3}k^{2})\) & \(0.9531 0.0087\) & \(2^{25}\) & \(2^{14}\) \\  Sub-cubic & \((v^{2} k^{2}+v^{2} d)\) & \(0.702 0.033\) & \(2^{21}\) & \(2^{13}\) \\   w/o mean+diff & & \(0.349 0.080\) & \(2^{21}\) & \(2^{13}\) \\  Low-rank QK & \((v^{2}k^{2}+}_{}+d}_{ })\) & \(0.675 0.035\) & \(2^{22}\) & \(2^{12}\) \\   SVD only & & \(0.284 0.072\) & \(2^{22}\) & \(2^{12}\) \\  Low-rank EU & \((v^{2}k^{2}+_{}+d}_{ })\) & \(0.633 0.062\) & \(2^{21}\) & \(2^{13}\) \\   SVD only & & \(0.610 0.060\) & \(2^{21}\) & \(2^{13}\) \\   SVD only & & \(0.38 0.06\) & \(2^{22}\) & \(2^{13}\) \\  Quadratic QK & \((v^{2}k^{2}+_{}+d}_{ })\) & \(0.316 0.037\) & \(2^{21}\) & \(2^{12}\) \\  Quadratic QK\&EU & \((v^{2}k^{2}+_{}+d}_{ })\) & \(0.283 0.036\) & \(2^{21}\) & \(2^{13}\) \\   

Table 1: We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. As we include more aspects of the mechanistic interpretation (reflected by a lower number of unexplained dimensions), we get more compact proofs (in terms of both asymptotic complexity and FLOPs), albeit with worse bounds. For space reasons, we use \(k:=n_{}\), \(d:=d_{}\), and \(v:=d_{}\).

More formally, suppose we are given some gap \(g\). For each pure sequence \(\) with max token \(t_{ max}\), query token \(t_{ query}\), such that \(t_{ query} t_{ max}-g\), and \(c\) copies of the third token type \(t^{} t_{ max}-g\), we pessimally ablate the OV circuit over the set \(^{ pure}(t_{ max},t_{ query},c;g)\) of pure sequences \(^{}\) with the same max and query tokens and \(c\) copies of the third token type \(t^{}\). If the model gets all sequences in \(^{ pure}(t_{ max},t_{ query},c;g)\) correct, then we can conclude that it gets \(\) correct, otherwise, we treat the model as having gotten \(\) wrong. This means that it suffices to only consider the \(O({d_{ vocab}}^{2}n_{ ctx})\) pessimal pure sequences of each of the \(O({d_{ vocab}}^{2}n_{ ctx})\) sets of the form \(^{ pure}(t_{ max},t_{ query},c;g)\).

Decoupling and pessimizing computations that require \(O({d_{ vocab}}^{3})\) computationsMany parts of our cubic certificate require iterating through \(O({d_{ vocab}}^{2})\) cases parameterized by \((t_{ max},t_{ query})\) or \((t_{ max},t^{})\). For example, as part of the pessimization procedure over pure sequences, for each of the \({d_{ vocab}}\) possible values of \(t_{ max}\), we need to consider the relative effects on the \({d_{ vocab}}\)-sized logits of attending to each of the \(O({d_{ vocab}})\) other tokens \(t^{}<t_{ max}\), and for each \(t_{ max}\) and \(t_{ query}\), we need to check that the contribution of the direct path on logits \(x_{ query}{ EU}\) is not sufficiently large as to overwhelm the contribution from \(x_{ max}{ EVOU}\). We independently pessimize over each of these components over one of the \({d_{ vocab}}\)-sized axes: for example, instead of computing \(x_{ max}{ EVOU}+x_{ query}{ EU}\) for each \(t_{ max}\), \(t_{ query}\) pair, we first pessimally ablate the direct path along the query token (which takes \(O({d_{ vocab}}^{2})\) time as it does not depend on the \(t_{ max}\), and then consider the sum \(x_{ max}{ EVOU}+_{x^{}}{x^{}}{ EU}\). Since this sum no longer depends on \(t_{ query}\), we only need to perform it \(O({d_{ vocab}})\) times, for a total cost of \(O({d_{ vocab}}^{2})\).

Low rank approximations to \({ EQKE}\) and \({ EU}\) Recall from Section 3.1 that \({ EQKE}\) is approximately rank 1, where the sole direction of variation is the size of the key token. By computing only the low rank approximation to \({ EQKE}\), we can more cheaply compute the most significant component of the behavior in the QK circuit. To bound the remaining error, we can use the fact that after pulling off the first principal component from each of the four matrices we multiply, very little structure remains.

We can find the rank 1/2 approximations by performing SVD on \({ EQKE}\). We can efficiently compute the SVD in \(({d_{ vocab}}{d_{ model}}^{2})\) time by using the fact that \({ EQKE}\) can be written as the product of a \({d_{ vocab}}}\) matrix and a \({d_{ model}}}\) matrix. This allows us to avoid performing the \(({d_{ vocab}}^{2}{d_{ model}})\)-cost matrix multiplications to explicitly compute \({ EQKE}\).

Similarly, we can more efficiently check that the direct path \({ EU}\) contributes negligibly to the model outputs, by using SVD to decompose \({ EU}\) into a sum of rank 1 products (which we can evaluate exactly) and a high-rank error term that we can cheaply bound.

#### 4.3.2 Additional subcubic proof strategies

Tighter bounds for sums of variables with unequal variance via the "mean+diff trick"Suppose we want to lower bound the minimum of the sum of two functions over three variables \(h(x,y,z)=f(x,y)+g(y,z)\), while only iterating over two variables at a time. The naive way is to minimize \(f(x,y)\) and \(g(x,y)\) independently:

\[_{x,y,z}h(x,y,z)_{x,y}f(x,y)+_{y,z}g(y,z)\]

Here, the error comes from setting the \(y\)s in \(f\) and \(g\) to different values. But in cases where \(g(y,z)\) varies significantly with \(y\) and only slightly with \(z\), rewriting \(g\) as a sum of a component that is independent of \(z\) (only varying along \(y\)), and one that depends on \(z\), yields a better lower bound:

\[_{x,y,z}h(x,y,z)_{x,y}(f(x,y)+_{z}^{}g(y,z^{ }))+_{y,z}(g(y,z)-_{z}^{}g(y,z^{}))\]

This estimate will have error at most \(\), while the naive estimator can have arbitrarily large error. We refer to this rewrite as the "mean+diff trick".12 From the mechanistic interpretation in Section 3.1, we know that some of the components barely vary among one or more axes. So we can apply the mean+diff trick to get tighter lower bounds.

Avoiding matrix multiplications using the "max row-diff trick"Using properties of linear algebra, we derive a cheap approximation to the max row-diff for the product of matrices \(AB\) in terms of the product of the max row-diff of \(B\) and the absolute value of \(A\), which we deem the "max row-diff"trick. We apply this trick to get a better cheap bound on the error terms of low-rank approximations, without having to multiply out the full matrices. See Appendices G.2.2, and F for more details.

## 5 Results

We run each of \(151\) transformers on the various proof strategies of different asymptotic complexity, and analyze these proofs to empirically examine the relationship between proof length, bound tightness, and degree of understanding. For each proof on each transformer, we approximate the length of the proof by estimating the number of FLOPs used, and plot this against the ratio of certified bound the true accuracy \(b/\) (Equation 2) in Figure 3. There exists a clear trade-off between bound tightness and compactness of the proof - more compact proofs yield looser bounds, and tighter bounds are associated with more expensive proofs.

### Compact proofs both require and provide mechanistic understanding

**Quantifying mechanistic understanding using unexplained dimensionality** We first quantify the amount of mechanistic understanding used in a proof by measuring its **unexplained dimensionality** - the number of free parameters required to fully describe model behavior, assuming the structural assumptions of the proof are correct. More detailed interpretations leave fewer free parameters needing to be filled in via empirical observation (Appendix A.5). In Figure 5, we plot these axes and find a suggestive correlation: proofs based on less mechanistic understanding are longer.

**More mechanistic understanding allows for more compact proofs** In addition to the constructions in Section 4, the parts of proofs we were unable to compact seem to correspond to components that we do not mechanistically understand. For example, we could not cheaply bound the behavior of \(\) without multiplying out the matrices, and this seems in part because we do have a mechanistic understanding of how \(\) implements low-rank copying.

**Compact proofs seem to provide understanding** By examining compact proofs, we can extract understanding about the model. For example, the fact that replacing each row of \(\) with its average across rows has little effect on the bound implies that \(\) does not vary much based on \(t_{}\).

### Proof length vs. bound tightness trade-off is modulated by faithfulness of interpretation

**Compact proofs are less faithful to model internals** To derive more compact proofs, we use our mechanistic understanding to simplify the model computation in ways that diverge from the original model internals. For example, in some subcubic proofs (Section 4.3), we approximate \(\) with a rank-1 approximation corresponding to the "size direction". However, while other components are small, they're nonzero; this approximation harms model internals.

**Less faithful interpretations lead to worse bounds on performance** To confirm that faithfulness of understanding affects the tightness of bound independent of proof length, we plot the normalized accuracy bound of subcubic proofs that perform a rank-1 approximation to \(\), versus the ratio of the first two singular components. A larger ratio between the components implies that the rank-1 approximation is more faithful. In Figure 4, we see a positive correlation between the two axes: when the interpretation is more faithful, the bounds are tighter, even at a fixed proof length.

Figure 3: For each of the proofs in Section 4, we plot the number of FLOPs used to compute the certificate, as well as the normalized accuracy lower-bound (\(b/\)). The brute-force proof (Section 4.1) computes the exact performance uses orders of magnitude more compute than other approaches. The cubic proof (Section 4.3) uses a small amount of mechanistic understanding and less compute, while still retaining good accuracy lower bounds. Finally, subcubic proofs (Section 4.3) require the entirety of the mechanistic interpretation of the model to attain non-vacuous bounds; this understanding allows us to further reduces compute costs, but we still achieve worse bounds. See Appendix H.2.1 for a detailed description of the various proof strategies.

### Compounding structureless noise is a big challenge for compacting global-behavior proofs

**Pessimal error terms compound in the absence of known structure** The rank-1 approximation of \(\) has small error. However, when making rank-1 approximations of each of the constituent matrices \(E,Q,K\), pessimizing over the worst way to composing the individual small error terms leads to a bound on the error term of \(\) that is orders of magnitude larger than the actual error term. Because we don't understand how the matrices compose in a way that doesn't cause errors to compound (without just multiplying out the matrices), this approximation leads to a trivial bound on performance (Appendix G.2.5). We speculate that in many cases, there is no short human-interpretable description for why random noise or approximation errors do not compound across layers of neural networks (e.g., see the error correction results on _randomly initialized_ neural networks from Hanni et al. (2018)), and thus that compounding structureless errors may be an issue in practice.

## 6 Related Work

**Generalization Bounds** Prior work in the PAC-Bayes framework (Pedregosa et al., 2017; Goyal et al., 2018; Goyal et al., 2018) proves generalization bounds over learning procedures, which are similar to the global performance bounds we consider in this work. These proofs tend to provide statistical guarantees (Pedregosa et al., 2017; Goyal et al., 2018) about the outputs of a known stochastic training procedure, while we seek to bound the performance of particular trained models.

**Formally verifying neural networks** Most prior work formally verifies neural networks either via model checking (Pedregosa et al., 2017; Goyal et al., 2018) or by relaxing the problem setting and taking an automated theorem proving approach (Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018; Goyal et al., 2018) to verify _local_ robustness properties. These proof strategies tend to be derived by examining only the network architecture. We take an approach more akin to interactive theorem proving (Pedregosa et al., 2018) and verify _global_ performance properties by reverse-engineering the weights.

**Mechanistic Interpretability** Finally, mechanistic interpretability is the subfield of the broader field of understanding model internals (Pedregosa et al., 2017), which is too large to faithfully summarize. Our work takes most direct inspiration from efforts to deeply understand how either toy models (Pedregosa et al., 2017; Goyal et al., 2018; Goyal et al., 2018) or small pretrained text transformers (Pedregosa et al., 2017; Goyal et al., 2018) implement algorithmic tasks, generally by performing ablations and SVD. In contrast, we formally prove that a transformer implements an algorithm.

Figure 4: We plot the normalized accuracy bound versus the ratio of first and second singular values of \(\), for various types of subcubic proofs that depend on a rank-1 approximation \(\). For each class of proof, the closer \(\) is to rank-1, the tighter the accuracy bound. This suggests that more faithful interpretations lead to tighter bounds even holding proof length fixed. See Appendix H.2.2 for a detailed description of proof strategies.

Figure 5: We plot, for each proof, the approximate number of flops required to evaluate the proof, versus the unexplained dimensionality (Section 5.1). More mechanistic understanding leaves fewer dimensions unexplained. We observe that more compact proofs seem to leave fewer unexplained dimensions, which is indicative of the relationship of mechanistic understanding and compact proofs. See Appendix H.2.1 for more detail.

Nichani et al.  proves that, in a significantly simplified 2-layer, 1-head attention-only transformer model and for the task of in-context bigram statistics, gradient descent will create induction heads . Our results concern transformers with fixed weights. In concurrent work, Michaud et al.  use techniques inspired by mechanistic interpretability to perform automated program synthesis on 2-dimensional RNNs, while our work works with significantly larger transformer models.

## 7 Conclusion and Future Work

**Summary** In this work, we used a Max-of-\(K\) setting to prototype the use of mechanistic interpretability to derive compact proofs of model behavior. Using varying amounts of understanding, we derived more efficient proof computations lower bounding model accuracy. We found preliminary evidence that mechanistic understanding can compactify proofs. Moreover, we observed that the tightness of the lower bound offered by various proof strategies can be used to grade the faithfulness our mechanistic interpretation. Finally, we identified compounding structureless errors as a key obstacle to deriving compact proofs of model behavior.

**Limitations and future work** We study one-layer attention-only transformers on a toy algorithmic task. Future work should explore the viability of deriving proofs via interpretability using larger models featuring MLPs or layernorm on more complex domains. In addition, we were unable to significantly compact the part of the proof involving the OV circuit, which future work can explore. The proofs we explored in this work also did not lead to qualitatively novel insights; future work may be able to derive such insights with improved techniques. Finally, future work can address the problem of compounding structureless errors, perhaps by relaxing from worst-case pessimal ablations to typical-case heuristic guarantees .

## Author Contributions

**Jason Gross** led the project, including managing the team and conceptualizing the proofs approach. He ran the Max-of-4 experiments, devised the proof strategies, and wrote up the formal proofs. He worked on various case studies and developed general methodology for computing complexity and length bounds for proofs. He also developed the particular convex relaxations presented in the paper.

**Rajashree Agrawal** was invaluable in steering the direction of the project, including contributing to the preliminary experiment on Max-of-2 and developing the pessimal ablation approach. She worked on framing the results, and contributed text to the paper.

**Thomas Kwa** and **Euan Ong** extended the preliminary experiments to larger values of \(k\) and contributed substantially to the cubic proof. **Chun Hei Yip**, **Alex Gibson**, and **Soufiane Noubir** worked on case studies other than the Max-of-\(K\) task and informed discussion on proof complexity.

**Lawrence Chan** spearheaded the writing of the paper, including turning informal claims into formal theorem statements, creating figures, and writing the core text. He also developed the unexplained dimensionality metric for clarifying the takeaway of the paper.