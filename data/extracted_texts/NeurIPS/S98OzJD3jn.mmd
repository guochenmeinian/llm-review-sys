# Diffusion Tuning: Transferring Diffusion Models via Chain of Forgetting

Jincheng Zhong, Xingzhuo Guo, Jiaxiang Dong, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{zjc22,gxz23,djx20}@mails.tsinghua.edu.cn, mingsheng@tsinghua.edu.cn

Equal contribution

###### Abstract

Diffusion models have significantly advanced the field of generative modeling. However, training a diffusion model is computationally expensive, creating a pressing need to adapt off-the-shelf diffusion models for downstream generation tasks. Current fine-tuning methods focus on parameter-efficient transfer learning but overlook the fundamental transfer characteristics of diffusion models. In this paper, we investigate the transferability of diffusion models and observe a monotonous _chain of forgetting_ trend of transferability along the reverse process. Based on this observation and novel theoretical insights, we present _Diff-Tuning_, a frustratingly simple transfer approach that leverages the chain of forgetting tendency. Diff-Tuning encourages the fine-tuned model to retain the pre-trained knowledge at the end of the denoising chain close to the generated data while discarding the other noise side. We conduct comprehensive experiments to evaluate Diff-Tuning, including the transfer of pre-trained Diffusion Transformer models to eight downstream generations and the adaptation of Stable Diffusion to five control conditions with ControlNet. Diff-Tuning achieves a 24.6% improvement over standard fine-tuning and enhances the convergence speed of ControlNet by 24%. Notably, parameter-efficient transfer learning techniques for diffusion models can also benefit from Diff-Tuning. Code is available at this repository: https://github.com/thuml/Diffusion-Tuning.

## 1 Introduction

Diffusion models  are leading the revolution in modern generative modeling, achieving remarkable successes across various domains such as image , video , 3D shape , audio generation , etc. Despite these advances, training an applicable diffusion model from scratch often demands a substantial computational budget, exemplified by the thousands of TPUs needed, as reported by . Consequently, fine-tuning well pre-trained, large-scale models for specific tasks has become increasingly crucial in practice .

During the past years, the deep learning community has concentrated on how to transfer knowledge from large-scale pre-trained models with minimal computational and memory demands, a process known as parameter-efficient fine-tuning (PEFT) . The central insight of these approaches is to update as few parameters as possible while avoiding performance decline. However, the intrinsic transfer properties of diffusion models have remained largely unexplored, with scant attention paid to effectively fine-tuning from a pre-trained diffusion model.

Previous studies on neural network transferability, such as those by , have demonstrated that lower-level features are generally more transferable than higher-level features. In the context of diffusion models, which transform noise into data through a reverse process, it is logical to assumethat the initial stages, which are responsible for shaping high-level objects, differ in transferability from later stages that refine details. This differential transferability across the denoising stages presents an opportunity to enhance the efficacy of fine-tuning.

In this work, we investigate the transferability within the reverse process of diffusion models. Firstly, we propose that a pre-trained model can act as a universal denoiser for lightly corrupted data, capable of recognizing and refining subtle distortions (see Figure 1). This ability leads to improved generation quality when we directly replace the fine-tuned model with the original pre-trained one under low distortion. The suboptimality observed with fine-tuned models suggests potential overfitting, mode collapse, or undesirable forgetting. Then we extend the experiments by gradually increasing the denoising steps replaced, to cover higher-level noised data, observing the boundaries of zero-shot generalization capability. This indicates that the fine-tuning objective should prioritize high-level shaping, associated with domain-specific characteristics. We term this gradual loss of adaptability the _chain of forgetting_, which tends to retain low-level denoising skills while forgetting high-level, domain-specific characteristics during the transfer of the pre-trained model. We further provide novel theoretical insights to reveal the principles behind the chain of forgetting.

Since the chain of forgetting suggests different denoising stages lead to different forgetting preferences, it is reasonable to develop a transfer strategy that balances the degrees of forgetting and retention. Technically, based on the above motivation, we propose _Diff-Tuning_, a frustratingly simple but general fine-tuning approach for diffusion models. Diff-Tuning extends the conventional fine-tuning objectives by integrating two specific aims: 1) knowledge retention, which retains general denoising knowledge; 2) knowledge reconsolidation, which tailors high-level shaping characteristics to specific downstream domains. Diff-Tuning leverages the chain of forgetting to balance these two complementary objectives throughout the reverse process.

Experimentally, Diff-Tuning achieves significant performance improvements over standard fine-tuning in two mainstream fine-tuning scenarios: conditional generation and controllable generation with ControlNet . Our contributions can be summarized as follows:

* Motivated by the transferable features of deep neural networks, we explore the transferability of diffusion models through the reverse process and observe a chain of forgetting tendency. We provide a novel theoretical perspective to elucidate the underlying principles of this phenomenon for diffusion models.
* We introduce Diff-Tuning, a frustratingly simple yet effective transfer learning method that integrates two key objectives: knowledge retention and knowledge reconsolidation. Diff-Tuning harmonizes these two complementary goals by leveraging the chain of forgetting.
* As a general transfer approach, Diff-Tuning achieves significant improvements over its standard fine-tuning counterparts in conditional generation across eight datasets and controllable generation using ControlNet under five distinct conditions. Notably, Diff-Tuning enhances the transferability of the current PEFT approaches, demonstrating the generality.

## 2 Related Work

### Diffusion Models

Diffusion models  and their variants [47; 48; 23] represent the state-of-the-art in generative modeling [12; 3], capable of progressively generating samples from random noise through a chain of denoising processes. Researchers have developed large-scale foundation diffusion models across a broad range of domains, including image synthesis , video generation , and cross-modal generation [43; 42]. Typically, training diffusion models involves learning a parametrized function \(f\) to distinguish the noise signal from a disturbed sample, as formalized below:

\[L()=_{t,_{0},}[\| -f_{0}(}_{0}+},t)\|^{2}]\] (1)

where \(_{0}\) represents real samples, \((,)\) denotes the noise signal, and \(_{t}=}_{0}+} {}\) is the disturbed sample at timestep \(t\). Sampling from diffusion models follows a Markov chain by iteratively denoising from \(_{T}(,)\) to \(_{0}\).

Previous research on diffusion models primarily focuses on noise schedules [36; 23], training objectives [44; 23], efficient sampling , and model architectures . In contrast to these existingworks, our method investigates the transferability of diffusion models across different denoising stages and enhances the transfer efficacy in a novel and intrinsic way.

### Transfer Learning

Transfer learning  is an important machine learning paradigm that aims to improve the performance of target tasks by leveraging knowledge from source domains. Transferring from pre-trained models, commonly known as fine-tuning, has been widely proved effective in practice, especially for the advanced large-scale models [5; 1; 12]. However, directly fine-tuning a pre-trained model can cause overfitting, mode collapse, and catastrophic forgetting . Extensive prior work has focused on overcoming these challenges to ultimately enhance the utilization of knowledge from pre-trained models [2; 8; 61]. However, effective transfer of diffusion models has received scant attention.

Parameter-Efficient Fine-tuning (PEFT)With significant advancements in the development of large-scale models [10; 5; 1; 12], research in transfer learning has increasingly concentrated on PEFT methods that minimize the number of learnable parameters. The primary goal of PEFT is to reduce time and memory costs associated with adapting large-scale pre-trained models. Techniques such as incorporating extra adapters [20; 60; 35] and learning partial or re-parameterized parameters [59; 21; 22; 14] are employed for their effectiveness in reducing computational demands. Nevertheless, the reliance on deep model architectures and the necessity of carefully selecting optimal placements present substantial challenges. Intuitively, PEFT approaches could potentially mitigate catastrophic forgetting by preserving most parameters unchanged; for a detailed discussion, refer to Section 4.3.

Mitigating Catastrophic Forgetting

Catastrophic forgetting is a long-standing challenge in the context of continual learning, lifelong learning, and transfer learning, referring to the tendency of neural networks to forget previously acquired knowledge when fine-tuning on new tasks. Recent exploration in parameter regularization approaches [24; 28; 27; 8] have gained prominence. Approaches such as [58; 29; 52] propose the data-based regularization, which involves distilling pre-trained knowledge into a knowledge bank. However, efforts to mitigate forgetting within the framework of diffusion models remain notably scarce.

## 3 Method

### Chain of Forgetting

Compared with one-way models, diffusion models specify in a manner of multi-step denoising and step-independent training objectives. Inspired by prior studies on the transferability of deep neural features [57; 33], we first explore how the transferability of diffusion models varies along the denoising steps.

Pre-trained Model Serves as a Zero-Shot DenoiserModern large-scale models are pre-trained with a large training corpus, emerging powerful zero-shot generalization capabilities. We begin by analyzing whether the pre-trained diffusion models hold similar zero-shot denoising capabilities. In particular, we utilize a popular pre-trained Diffusion Transformer (DiT) model  as our testbed. We fine-tune the DiT model on a downstream dataset. When the reverse process comes to the the last 10%

Figure 1: Case study of directly replacing the denoiser with the original pre-trained model on lightly disturbed data (left). The changes in Fréchet Inception Distance (FID) as the denoising steps are incrementally replaced by the original pre-trained model (right).

steps, we switch and continue the remaining denoising steps with the fine-tuned model, the original pre-trained model, and our Diff-Tuning model respectively. We visualize a case study in Figure 1 (left) with corresponding replacement setups. Surprisingly, the results reveal that replacement by the pre-trained model achieves competitive quality, even slightly better than the fine-tuned one, indicating that the pre-trained diffusion model indeed holds the zero-shot denoising skills. On the other side, some undesirable overfitting and forgetting occur when fine-tuning diffusion models.

Forgetting TrendNext, we delve deeper into investigating the boundary of generalization capabilities for the pre-trained model. Figure 1 (right) illustrates the performance trend when we gradually increase the percentage of denoising steps replaced from \(0\) to \(100\%\). Initially, this naive replacement yields better generation when applied towards the end of the reverse process. However, as more steps are replaced, performance begins to decline due to domain mismatch. This trend suggests the fine-tuned model may overfit the downstream task and forget some of the fundamental denoising knowledge initially possessed by the pre-trained model when \(t\) is small. Conversely, as \(t\) increases, the objects desirable in the new domain are distorted by the pre-trained model, resulting in a performance drop. Based on these observations, we conceptually separate the reverse process into two stages: (1) domain-specific shaping, and (2) general noise refining. We claim that the general noise refining stage is more transferable and can be reused across various domains. In contrast, the domain-specific shaping stage requires the fine-tuned model to forget the characteristics of the original domain and relearn from the new domains.

Theoretic InsightsBeyond empirical observations, we provide a novel theoretical perspective of the transfer preference for the pre-trained diffusion model. Following the objectives of diffusion models, a denoiser \(F\) (an \(_{0}\)-reparameterization  of \(f\) in Eq. (1)) is to approximate the posterior expectation of real data over distribution \(\). This is formalized by:

\[F(_{t})=_{_{0} p(_{0}|_ {t})}[_{0}]=_{0}} _{t};}_{0},(1-_{t})_{0} p_{}(_{0}) _{0}}{_{_{0}}_{t};}_{0},(1-_{t}) p_{ }(_{0})_{0}},\] (2)

where \(p_{}(_{0})\) represents the distribution of real data from \(\), and \(\) denotes the Gaussian distributions determined by the forward process. Notably, a larger variance of Gaussian distribution indicates a more uniform distribution. Through a detailed investigation of these Gaussian distributions under varying timesteps \(t\), we derive the following theorem. All proofs and derivations are provided in Appendix A.

**Theorem 1** (Chain of Forgetting): _Suppose a diffusion model with \(_{t 0}_{t}=1\) and \(_{t T}_{t}=0\) over finite samples, then the ideal denoiser \(F\) satisfies_

1. \(_{t 0}F(_{t})=*{argmin}_{p(_{0})>0} \{\|_{0}-_{t}\|\}\)_, i.e., the closest sample in dataset._
2. \(_{t T}F(_{t})=_{_{0} p_{} (_{0})}[_{0}]\)_, i.e., the mean of data distribution._

Theorem 1 elucidates the mechanism behind the chain of forgetting. On one hand, when \(t 0\), a model optimized on a training dataset \(\) can perform zero-shot denoising within the vicinity of the support set \(()\). As the training dataset scale expands, so does the coverage of \(()\), enabling diffusion models to act as general zero-shot denoisers for data associated with small \(t\). On the other hand, as \(t T\), the model's generalization is significantly influenced by the distribution distance \((_{}[_{0}],_{_ {}}[_{0}^{}])\), where \(_{}\) denotes the dataset of the new domain. This theorem highlights the necessity for further adaptation in the new domain.

### Diff-Tuning

Based on the above observations and theoretical insights, we introduce Diff-Tuning, which incorporates two complementary strategies to leverage the chain of forgetting in the reverse process: 1) knowledge retention, and 2) knowledge reconsolidation. Diff-Tuning aims to retain general denoising skills from the pre-trained model while discarding its redundant, domain-specific shaping knowledge. This enables the model to adapt more effectively to the specific characteristics of downstream tasks. Diff-Tuning harmonizes the retention and reconsolidation via the chain of forgetting tendency.

Without loss of generality, we present Diff-Tuning under the standard DDPM objective, omitting conditions in the formulations. The general conditional generation setup will be discussed later.

Knowledge RetentionAs discussed earlier, retaining pre-trained knowledge during the latter general noising refining proves beneficial. However, the classic parameter-regularization-based approaches  mitigate forgetting uniformly across the reverse process, primarily due to the parameter-sharing design inherent in diffusion models. To address this, Diff-Tuning constructs an augmented dataset \(}^{s}=\{}^{s},\}\), pre-sampled from the pre-trained model. This dataset acts as a repository of the retained knowledge of the pre-trained model. We define the auxiliary training objective, \(L_{}\), as follows:

\[L_{}()=_{t,,}_{0}^{s}}^{s}}[(t)\|-f_{} (}}_{0}^{s}+},t)\|^{2}],\] (3)

where \((t)\) is the retention coefficient. In accordance with the principles of the chain of forgetting, \((t)\) decreases monotonically with increasing \(t\), promoting the retention of knowledge associated with small \(t\) values and the discarding of knowledge related to large \(t\) values. Knowledge Retention shares a similar formulation with the pre-training objective but without the reliance on the original pre-training dataset.

It is important to note that the concept of Knowledge Retention in Diff-Tuning is anchored in the principle of the chain of forgetting. This approach encourages the model to recall how to denoise samples with low levels of disturbance, as reflected by the retention coefficient \((t)\). While the proposed augmented dataset serves as an easy-to-implement example of this concept, given the flexibility of Diff-Tuning, various other methods can also effectively facilitate knowledge retention. An alternative approach involving knowledge distillation is detailed in Appendix C.

Knowledge ReconsolidationIn contrast to knowledge retention, knowledge reconsolidation focuses on adapting pre-trained knowledge to new domains. The intuition behind knowledge reconsolidation is to diminish the conflict between forgetting and adaptation by emphasizing the tuning of knowledge associated with large \(t\). This adaptation is formalized as follows:

\[L_{}()=_{t,,_{0} }[(t)\|-f_{}(}_{0}+},t)\|^{2}],\] (4)

where \((t)\) is the reconsolidation coefficient, a monotonic increasing function within the range \(\), reflecting increased emphasis on domain-specific adaptation as \(t\) increases. Similar to Knowledge Retention, the principle behind Knowledge Reconsolidation involves adapting the model to effectively

Figure 2: The conceptual illustration of the chain of forgetting (Left). The increasing forgetting tendency as \(t\) grows. (a) Build a knowledge bank for the pre-trained model before fine-tuning. (b) Diff-Tuning leverages knowledge retention and reconsolidation, via the chain of forgetting.

handle samples that are heavily disturbed, which are more significantly influenced by the distance between the pre-trained and target distributions. Reweighting by \((t)\) serves as one of the simplest implementations of this concept.

A Frustratingly Simple ApproachOverall, we reach Diff-Tuning, a general and flexible fine-tuning framework for effective transferring pre-trained diffusion models to downstream generations, the overall objective is as follows:

\[_{}L_{}()+L_{}(),\] (5)

where \(L_{}()\) and \(L_{}()\) are described before, \(\) represents the set of tunable parameters. Notably, Diff-Tuning is architecture-agnostic and seamlessly integrates with existing PEFT methods. Further details are discussed in Section 4.3.

Choices of \((t)\) and \((t)\)For clarity and simplicity, we define \((t)=1-(t)\), ensuring equal weighting for each \(t\), following the original DDPM configuration. This complementary design excludes the influences of recent studies on the \(t\)-reweighting techniques [23; 12; 9]. From the above discussion, we can choose any monotonically increasing function whose range falls in \(\). In this work, we scale the variable \(t\) to the interval \(\), and apply a simple power function group \((t)=t^{}\) for practical implementation. In our experiments, we report the main results with \(=1\), and the variations of the choice are explored in Section 4.4.

Conditional GenerationClassifier-free guidance (CFG)  forms the basis for large-scale conditional diffusion models. To facilitate sampling with CFG, advanced diffusion models such as DiT  and Stable Diffusion  are primarily trained conditionally. CFG is formulated as \(=(1+w)_{}-w_{u}\), where \(w,_{c},_{u}\) are the CFG weight, conditional output, and unconditional output. As a general approach, Diff-Tuning inherits the conditional training and sampling setup to support a wide range of transfer tasks. Due to the mismatch between the pre-training domain and downstream tasks in the conditional space, we apply knowledge retention \(L_{}\) on the unconditional branch and knowledge reconsolidation \(L_{}\) on both unconditional and conditional branches.

## 4 Experiments

To fully verify the effectiveness of Diff-Tuning, we extensively conduct experiments across two mainstream fine-tuning scenarios: 1) Class-conditional generation, which involves eight well-established fine-grained downstream datasets, and 2) Controllable generation using the recently popular ControlNet , which includes five distinct control conditions.

### Transfer to Class-conditional Generation

SetupsClass-conditioned generation is a fundamental application of diffusion models. To fully evaluate transfer efficiency, we adhere to the benchmarks with a resolution of \(256 256\) as used in DiffFit , including datasets such as Food101 , SUN397 , DF20-Mini , Caltech101 , CUB-200-2011 , ArtBench-10 , Oxford Flowers , and Stanford Cars . Our base model, the DiT-XL-2-256x256 , is pre-trained on ImageNet at \(256 256\) resolution, achieving a Frechet Inception Distance (FID)  of 2.27 2. The FID is calculated by measuring the distance between the generated images and a test set, serving as a widely used metric for evaluating generative image models' quality. We adhere to the default generation protocol as specified in , generating 10K instances with 50 sampling steps (FID-10K). \(_{}\) weight is set to 1.5 for evaluation. For the implemented DiffFit baseline, we follow the optimal settings in , which involve enlarging the learning rate \( 10\) and carefully placing the scale factor to 1 to 14 blocks. For each result, we fine-tune 24K iterations with a batch size of 32 for standard fine-tuning and Diff-Tuning, and a batch size of 64 for DiffFit, on one NVIDIA A100 40G GPU. For each benchmark, we recorded the Relative Promotio of FID between Diff-Tuning and Full Fine-tuning (\(}{}\)) to highlight the effectiveness of our method. More implementation details can be found in Appendix B.

### Transfer to Controllable Generation

SetupsControlling diffusion models enables personalization, customization, or task-specific image generation. In this section, we evaluate Diff-Tuning on the popular ControlNet , a state-of-the-art controlling technique for diffusion models, which can be viewed as fine-tuning the stable diffusion model with conditional adapters at a high level. We test Diff-Tuning under various image-based conditions provided by ControlNet3, including Sketch , Edge , Normal Map , Depth Map , and Segmentation on the COCO  and ADE20k  datasets at a resolution of \(512 512\). We fine-tune ControlNet for 15k iterations for each condition except 5k for Sketch and 20k for Segmentation on ADE20k, using a batch size of 4 on one NVIDIA A100 40G GPU. For more specific training and inference parameters, refer to Appendix B.

Evaluation through Sudden Convergence StepsDue to the absence of a robust quantitative metric for evaluating fine-tuning approaches with ControlNet, we propose a novel metric based on the sudden convergence steps. In the sudden convergence phenomenon, as reported in , ControlNet tends not to learn control conditions gradually but instead abruptly gains the capability to synthesize images according to these conditions after reaching a sudden convergence point. This phenomenon is observable in the showcases presented in Figure 4 throughout the tuning process. We propose measuring the (dis-)similarity between the original controlling conditions and the post-annotated conditions of the corresponding controlled generated samples. As depicted in Figure 3, a distinct "leap" occurs along the training process, providing a clear threshold to determine whether sudden convergence has occurred. We manually select this threshold, combined with human assessment, to identify the occurrence of sudden convergence. The detailed setup of this metric is discussed in Appendix F.

ResultsAs demonstrated in Table 2, Diff-Tuning consistently requires significantly fewer steps to reach sudden convergence across all controlling conditions compared to standard fine-tuning of

   MethodDataset & Food & SUN & DF-20M & Caltech & CUB-Bird & ArtBench & Oxford & Standard & Average \\ Flowers & Cars & FID \\  Full Fine-tuning & 10.93 & 14.13 & 15.29 & 23.84 & 5.37 & 19.94 & 16.67 & 6.32 & 14.06 \\ AdapFormer\({}^{}\)\({}^{}\) & 11.93 & 10.68 & 19.01 & 34.17 & 7.00 & 35.04 & 21.36 & 10.45 & 18.70 \\ BitFit\({}^{}\) & 9.17 & 9.11 & 17.78 & 34.21 & 8.81 & 24.53 & 20.31 & 10.64 & 16.82 \\ VPT-Deep\({}^{}\)\({}^{}\) & 18.47 & 14.54 & 32.89 & 42.78 & 17.29 & 40.74 & 25.59 & 22.12 & 26.80 \\ LoRA\({}^{}\)\({}^{}\) & 33.75 & 32.53 & 120.25 & 86.05 & 56.03 & 80.99 & 164.13 & 76.24 & 81.25 \\ DiffFit\({}^{}\) & 7.80 & 10.36 & 15.24 & 23.79 & 4.98 & 16.40 & 14.02 & 5.81 & 12.21 \\ 
**Diff-Tuning** & **6.05** & **9.01** & **13.64** & **23.69** & **3.50** & **13.85** & **12.63** & **5.37** & **11.08** \\ Relative Promotion & 44.6\% & 36.2\% & 10.8\% & 0.6\% & 34.8\% & 30.5\% & 24.2\% & 15.0\% & 24.6\% \\   

Table 1: Comparisons on 8 downstream tasks with pre-trained DiT-XL-2-256x256. Methods with \({}^{}\)\({}^{}\)\({}^{}\) are reported from the original Table 1 of . Parameter-efficient methods are denoted by \({}^{}\)\({}^{}\)\({}^{}\)\({}^{}\).

Figure 3: An example of evaluating dissimilarities between conditions (the Normal condition) to infer the occurrence of sudden convergence.

ControlNet, indicating a consistent enhancement in the transfer efficiency. In Figure 4, we display showcases from the training process both with and without Diff-Tuning. It is observed that Diff-Tuning achieves sudden convergence significantly faster, enabling the generation of well-controlled samples more quickly. By comparing the images from the final converged model at the same step, it is evident that our proposed Diff-Tuning achieves superior image generation quality.

### Discussion on Parameter-Efficient Transfer Learning

The initial motivation behind adapter-based approaches in continual learning is to prevent catastrophic forgetting by maintaining the original model unchanged . These methods conceptually preserve a separate checkpoint for each arriving task, reverting to the appropriate weights as needed during inference. This strategy ensures that knowledge from previously learned tasks is not overwritten. In transfer learning, however, the objective shifts to adapting a pre-trained model for new, downstream tasks. This adaptation often presents unique challenges. Prior studies indicate that PEFT methods struggle to match the performance of full model fine-tuning unless modifications are carefully implemented. Such modifications include significantly increasing learning rates, sometimes by more than tenfold, and strategically placing tunable parameters within suitable blocks [21; 7; 54]. Consider the state-of-the-art method, DiffFit, which updates only the bias terms in networks, merely 0.12% of the parameters in DiT equating to approximately 0.83 million parameters. While this might seem efficient, such a small proportion of tunable parameters is enough to risk overfitting downstream tasks. Increasing the learning rate to compensate for the limited number of trainable parameters can inadvertently distort the underlying pre-trained knowledge, raising the risk of training instability and potentially causing a sudden and complete degradation of the pre-trained knowledge, as observed in studies like .

Elastic Weight Consolidation (EWC)  is a classic parameter-regularized approach to preserve knowledge in a neural network. We calculate the \(L_{2}\)-EWC values, which are defined as \(=\|-_{0}\|^{2}\), for the tunable parameters in the evaluated approaches . The EWC value quantifies

   Method & Sketch & Normal & Depth & Edge & Seg. (COCO) & Seg. (ADE20k) & Average \\  ControlNet  & 3.8k & 10.3k & 9.9k & 6.7k & 9.2k & 13.9k & 9.0k \\  ControlNet +**Diff-Tuning** & **3.2k** & **7.8k** & **8.8k** & **5.3k** & **6.3k** & **8.3k** & **6.6k** \\ Relative Promotion & 15.8\% & 24.3\% & 11.1\% & 20.9\% & 31.5\% & 40.3\% & 24.0\% \\   

Table 2: Sudden convergence steps on controlling Stable Diffusion with 5 conditions.

Figure 4: Qualitative compare Diff-Tuning to the standard ControlNet. Red boxes refer to the occurence of “sudden convergence”.

how far the fine-tuned model deviates from the pre-trained model, indicating the degree of knowledge forgetting from the perspective of parameter space.

Figure 5(b) reveals that DiffFit leads to EWC values that are 2.42 times larger with only 0.12% tunable parameters, indicating heavy distortion of the pre-trained knowledge. Figure 5(c) illustrates the averaged EWC over tunable parameters, showing that each tunable bias term contributes significantly more to the EWC. In contrast, Diff-Tuning achieves lower EWC values. Diff-Tuning does not explicitly focus on avoiding forgetting in the parameter space but rather harmonizes the chain of forgetting in the parameter-sharing diffusion model and only retains knowledge associated with small \(t\) rather than the entire reverse process.

Diff-Tuning can be directly applied to current PEFT approaches, and the comparison results in Figure 5 demonstrate that Diff-Tuning can enhance the transfer capability of DiffFit and significantly improve converged performance.

### Analysis and Ablation

Fine-tuning Convergence AnalysisTo analyze converging speed, we present a concrete study on the convergence of the FID scores for standard fine-tuning, DiffFit, Diff-Tuning, and Diff-Tuning\({}^{*}\) (DiffFit equipped with Diff-Tuning) every 1,500 iterations in the SUN 397 dataset, as shown in Figure 6(a). Compared to standard fine-tuning and DiffFit, Diff-Tuning effectively leverages the chain of forgetting, achieving a balance between forgetting and retaining. This leads to faster convergence and superior results. Furthermore, the result of Diff-Tuning\({}^{*}\) indicates that PEFT methods such like DiffFit still struggle with forgetting and overfitting. These methods can benefit from Diff-Tuning.

Ablation StudyWe explore the efficacy of each module within Diff-Tuning, specifically focusing on knowledge retention and knowledge reconsolidation. We assess Diff-Tuning against its variants where: (1) Only reconsolidation is applied, setting \((t) 0\) and \((t)=t\); and (2) Only retention is employed, setting \((t)=1-t\) and \((t) 1\). The results, illustrated in Figure 6(b) demonstrate that both knowledge retention and knowledge reconsolidation effectively leverage the chain of forgetting to enhance fine-tuning performance. The FID scores reported on the DF20M dataset clearly show that combining these strategies leads to more efficient learning adaptations.

Tradeoff the Forgetting and Retraining with the Chain of ForgettingFor simplicity and ease of implementation, Diff-Tuning adopts a power function, \((t)=t\), as the default reconsolidation coefficient. To explore sensitivity to hyperparameters, we conduct experiments using various coefficient functions \((t)=t^{}\) with \(\) values from the set \(\{0,0.3,0.5,0.7,1,1.5\}\), and a signal-to-noise ratio (SNR) based function \((t)=1/(1+(t))\). Results on the Stanford Car dataset, shown in Figure 6(c), a carefully tuned coefficient can yield slightly better results. To keep the simplicity, we

Figure 5: The compatibility of Diff-Tuning with PEFT (a), and catastrophic forgetting analysis (b-c).

Figure 6: Transfer convergence analysis (a), ablation study (b), and sensitivity analysis (c-d).

keep the default setting \(=1\). Notably, when \(=0\), Diff-Tuning reduces to the standard fine-tuning baseline.

Analysis on Knowledge RetentionIn Diff-Tuning, knowledge is retained using pre-sampled data from pre-trained diffusion models before fine-tuning. We evaluate the impact of varying sample sizes (5K, 50K, 100K, 200K, and the entire source dataset) on the performance of the DiT model on the Stanford Car dataset, as illustrated in Figure 6(d). Notably, using the entire source dataset, which comprises 1.2M ImageNet images, results in suboptimal outcomes. This observation underscores that pre-sampled data serve as a more precise distillation of pre-trained knowledge, aligning with our goal of retraining knowledge rather than merely introducing extra training data.

Beyond the size of the augmented dataset, the quality of the samples can also influence knowledge retention. Therefore, we further analyze the impact of different sampling steps during the pre-sampling stage, as depicted in Figure 7(a). The findings demonstrate that Diff-Tuning exhibits consistent performance, indicating that the knowledge retention process is robust to variations in the sampling process of the augmented dataset.

Sensitivity to Sampling StepsWe evaluate Diff-Tuning using various sampling parameters, specifically settings of 25, 50, 100, and 500 steps. As depicted in Figure 7(b), there is a consistent improvement in performance across these configurations. Notably, Diff-Tuning shows a more significant enhancement with fewer steps, suggesting that it builds a more precise denoising model compared to standard fine-tuning.

### Extend Diff-Tuning to Continual Learning

In dynamic target domains like online systems that continually collect new data, continual learning is essential as models must undergo iterative fine-tuning to adapt to evolving datasets. To evaluate the adaptability of Diff-Tuning in this context, we extend out method and conduct experiments on the Evolving Image Search dataset , which includes images from 10 categories collected over three phases: 2009-2012, 2013-2016, and 2017-2020. We apply transfer learning sequentially across these temporal splits, and measure FID on the cumulative test set. For Diff-Tuning, we retain \(40\%\) of the initially sampled augmented data and generated the remaining \(60\%\) using the fine-tuned model before proceeding to the next phase. As shown in Figure 7(c), the results demonstrate that Diff-Tuning consistently improves and sustains robust performance as the dataset evolves.

## 5 Conclusion

In this paper, we explore the transferability of diffusion models and provide both empirical observations and novel theoretical insights regarding the transfer preferences in their reverse processes, which we term the chain of forgetting. We present Diff-Tuning, a frustratingly simple but general transfer learning approach designed for pre-trained diffusion models, leveraging the identified trend of the chain of forgetting. Diff-Tuning effectively enhances transfer performance by integrating knowledge retention and knowledge reconsolidation techniques. Experimentally, Diff-Tuning shows great generality and performance in advanced diffusion models, including conditional generation and controllable synthesis. Additionally, Diff-Tuning is compatible with existing parameter-efficient fine-tuning methods.

Figure 7: The influence of the quality of the augmented dataset (a), sensitivity with respect to different sampling steps (b), and application of Diff-Tuning in a continual learning setup (c).