# Finding Symmetry in Neural Network Parameter Spaces

Bo Zhao

University of California, San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehnamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California, San Diego

roseyu@ucsd.edu

###### Abstract

Parameter space symmetries, or loss-invariant transformations, are important for understanding neural networks' loss landscape, training dynamics, and generalization. However, identifying the full set of these symmetries remains a challenge. In this paper, we formalize data-dependent parameter symmetries and derive their infinitesimal form, which enables an automated approach to discover symmetry across different architectures. Our framework systematically uncovers parameter symmetries, including previously unknown ones. We also prove that symmetries in smaller subnetworks can extend to larger networks, allowing the discovery of symmetries in small architectures to generalize to more complex models.

## 1 Introduction

Parameter space symmetry, or loss-invariant transformation of parameters, influences various aspects of deep learning theory. Continuous symmetry connects groups to their orbits, revealing important topological properties such as the dimension (Zhao et al., 2023) and connectedness (Zhao et al., 2023) of the minimum. Parameter symmetry also influences training dynamics through the associated conserved quantities of gradient flow (Kunin et al., 2021) and by steering stochastic gradient descent towards certain favored solutions (Ziyin, 2024). Additionally, symmetry provides a tool to perform optimization within a loss level set, with successful applications in accelerating optimization (Armenta et al., 2023; Zhao et al., 2022) and improving generalization (Zhao et al., 2024). Other applications of parameter space symmetry include model compression (Ganev et al., 2022; Sourek et al., 2021) and reducing the search space for efficient sampling in Bayesian neural networks (Wiese et al., 2023).

Despite the wide range of applications, our knowledge of parameter space symmetries is limited. In particular, known symmetries often cannot account for all loss-invariant parameter transformation. While several frameworks have been developed to unify known symmetries, whether the symmetries in current literature are complete remains an open question. Due to a lack of systematic approach, current practice typically requires deriving symmetries from scratch for every new architecture, creating barriers for wider application that leverages parameter symmetries. In this paper, we discuss an automated approach to directly learn the symmetry groups and their group actions on the parameter space of neural networks. We show that large networks often have symmetries inherited from its components or subnetworks. This view suggests that searching for symmetries in small networks is an effective approach to identify a significant number of symmetries in modern architectures.

Our main contributions are:

* Formal definitions of data-dependent parameter symmetries and their infinitesimal form.

* An approach to identify symmetries in the parameter space of large networks from known symmetries in smaller subnetworks.
* A framework that discovers symmetry in neural network parameter spaces.

## 2 Data-dependent group action and symmetry

Let \(\) be the space of parameters and \(\) be the space of data. In this paper, we consider loss functions of the form \(L:\), which map parameters and a single data point to a real number. By abuse of notation, we allow \(L\) to simultaneously process multiple data points. Specifically, we sometimes define \(L:^{d}^{d}\) for \(d^{+}\) data points.

Let \(G\) be a group. Consider a map \(a\), which defines a map for every data batch of size \(d^{+}\):

\[a:^{d} (G)\] \[X (a_{X}:g,^{}).\] (1)

The map \(a\) is a group action on \(\) if it satisfies the following axioms:

identity: \[a_{X}(I,)=, X^{d},\  .\] associative law: \[a_{X}(g_{2},a_{X}(g_{1},))=a_{X}(g_{2}g_{1},),  g_{1},g_{2} G,\ \  X^{d},\ \ .\]

A group action \(a\) is a parameter space symmetry of \(L\) if it additionally satisfies

loss invariance: \[L(a_{X}(g,),X)=L(,X), g G,\ \  X^{d},\ \ .\]

A function \(L\) has a \(G\)-symmetry if there exists a loss-invariant group action \(a\). We refer to \(G\) as a symmetry group of \(L\). Additionally, the action \(a\) is termed a data-dependent group action or symmetry if the map (1) has a non-trivial dependency on \(X\). That is, \(a\) is data-dependent if there exists \(X_{1},X_{2}^{d}\), such that \(a_{X_{1}} a_{X_{2}}\). We derive an infinitesimal version of parameter space symmetries in Appendix B.

## 3 Building symmetries from known ones

One way to identify symmetries in a large network is by examining its components or subnetworks. Despite often having billions of parameters, neural networks typically consist of a limited set of functional families, such as fully connected layers, attention mechanisms, and activation functions. This modular view suggests a mechanism by which symmetries in networks with fewer layers might extend to those in deeper networks. Additionally, within similar types of networks, it may be possible to extrapolate symmetries found in narrower layers to wider ones.

By focusing on symmetries in small architectures and using them to infer symmetries in larger ones, we circumvent the complexity associated with direct handling of high-dimensional parameter spaces. This approach not only simplifies the discovery of symmetries in large-scale networks but also provides a systematic method for using symmetries in smaller subnetworks to understand those in more extensive architectures. Proofs and further discussions can be found in Appendix C.

When a loss function \(L\) depends on a subset of parameter exclusively through a subnetwork \(f\), any symmetries that preserves \(f\) will also preserve the original network \(L\):

**Proposition 3.1**.: _Let \(L:^{d}^{d}\) be a function, where the parameter space \(\) is a product space \(=_{1}_{2}\), with spaces \(_{1},_{2}\). Suppose there exist functions \(h:_{1}^{d} S\), \(f:_{2} S T\), and \(j:(_{1} T)^{d}^{d}\), such that for every \(=(_{1},_{2})\) and \(X^{d}\), \(L(,X)=j(_{1},f_{2},h(_{1},X)),X \). If \(a:S(G_{2}_{2})\) is a \(G\)-symmetry of \(f\), then there is an induced \(G\)-symmetry of \(L\), \(a^{}:^{d}(G)\), defined by \(a^{}_{X}(g,(_{1},_{2}))=_{1},a_{h(_{1},X )}(g,_{2})\)._

The relationship between the functions in the proposition is described by the commutative diagram below, where \(p_{1}:_{1}\), \(p_{2}:_{2}\) are projections onto \(_{1}\) and \(_{2}\), \(i_{1}:_{1}_{1}\) and \(i_{2}:_{2}_{2}\) are identity maps, and \(X^{d}\) represents a batch of data. When \(L\) can be decomposed in this way, the function \(h\) does not depend on \(_{2}\), and the function \(j\) depends on \(_{2}\) only through the output of \(f\). This effectively confines \(L\)'s dependency on \(_{2}\) to the transformation defined by \(f\), ensuring that any transformation on \(_{2}\) not altering the output of \(f\) will not affect the output of \(L\). Consequently, symmetries identified in the smaller network \(f\) can be extrapolated to the larger network \(L\).

Proposition 3.1 can be applied to construct symmetries in larger networks from those in smaller ones (Corollary C.2, C.1 in Appendix C). Figure 1 shows the subset of parameters (\(_{2}\)) the symmetry applies to in the corollaries.

Note that this approach does not explore the emergence of new, more complex symmetries that may arise as the neural network scale up in size. Notably, there are cases where there exists a \(G\) symmetry over its input space, but group actions on individual layers are not loss-invariant (Kvinge et al. (2022)). Nevertheless, studying smaller and simpler networks remains a effective strategy to obtain a significant number of symmetries in larger networks, and is a first step in characterizing the complete set of symmetries in modern architectures.

## 4 Automatic Discovery of Parameter Symmetries

Formulating symmetries in the infinitesimal form makes them easier to learn using an automatic framework, as it defines a set of local conditions for a function to be a symmetry. Using the infinitesimal symmetry derived in Section B.1, we construct an automated framework for discovering parameter space symmetries.

Enforcing Loss Invariance and Group Axioms.Given a function \(L\), our goal is to find a symmetry \(a\) and a set of Lie algebra elements \(h\) corresponding to a symmetry group of \(L\). We parameterize \(a\) using a neural network with learnable parameters, and set \(h\) to be learnable as well. We define the following loss terms that quantify the deviation from loss invariance and the group axioms (identity and associativity law):

\[L_{} =_{x,}|D_{}L|_{,X} D_{g}a_{X}|_{ I,}(h)|\] (2) \[L_{} =_{x,}\|a_{x}(I,)-\|_{2}\] (3)

The two loss terms bias the action towards being loss-invariant and preserving identity. By minimizing \(L_{}\), we ensure that the learned symmetry \(a\) and the Lie algebra element \(h\) satisfy the infinitesimal symmetry condition (Theorem B.1). Minimizing \(L_{}\) enforces the identity axiom. By focusing on the Lie algebras, we enforce the loss invariance and group structure at the infinitesimal level. This formulation allows us to avoid computing exponential maps.

Regularizations.To prevent the group action to be the identity function, we encourage the infinitesimal action to be nonzero. In implementation, we include the following regularization term

Figure 1: If a network contains substructures with known symmetry, we can infer the same symmetry for the large network. (a) Symmetry from narrower networks. (b) Symmetry from shallower networks.

to encourage the norm of the infinitesimal action to be around a fixed positive real number \(\): \(L_{}=_{a,h}_{}|-\|D_{g}a_{X}|_{I,}(h )\||\).

When learning multiple generators simultaneously, we want them to be orthogonal. Following Yang et al. (2023), we do this by including the following cosine similarity between each pair of the \(k\) generators in the loss function: \(L_{}=_{1 i<j k} h_{j}}{\|h_{i}\| \|h_{j}\|}\).

Finally, we encourage sparsity of \(h\) for easier interpretation, with \(L_{}=_{k,j}|h_{kj}|\).

The final training objective is a weighted average of the above loss and regularization terms, with hyperparameters \(_{1},...,_{6}^{+}\):

\[_{h,a}_{1}L_{}+_{2}L_{}+_{3}L_{ }+_{4}L_{}+_{5}L_{}.\] (4)

### Learned data-independent symmetries

In the first set of tasks, we see if our method can learn generators for architectures with already known data-independent symmetries. We consider two-layer networks in the form of \(L(W_{1},W_{2},X,Y)=\|W_{2}(W_{1}X)-Y\|^{2}\), where \(W_{2}^{m h},W_{1}^{h n}\) are parameters, \(X^{n k}\), \(Y^{m k}\) are data, and \(\) is a homogeneous activation function.

During training, we train the generators \(h\) and the group action \(a\) under objective (4). We parametrize \(a\) using a 4-layer MLP with hidden dimensions 64, 64, 64. The group action \(a\) takes a group element, parameter, and data as input and outputs transformed parameters. We use 10000 training samples, each containing a randomly generated set of parameters and data. We set the learning rate as \(10^{-3}\) with decay 0.6 every 1000 steps, and the weights for the multi-objective loss as \(_{1}=10\), \(_{2}=_{4}=_{5}=1\), and \(_{6}=0.1\).

As a proof of concept, we training a group action and a single generator \(h^{2 2}\) for the two-layer architecture with \(m=h=n=k=1\) and \(\) being the identity function. Figure 2 visualizes the learned generator, which matches the expected generator that generates the rescaling group.

Note that, however, we do not impose constraints on the group action (in particular, not enforcing linear actions). Hence we do not expect the learned generators to look similar to the elements of the Lie algebra infinitesimal generators of the symmetry group in general. For example, the action \(a\) can be a composition of two function, the first transforming learned generators to the set of actual generators, and the second performing the group action. We find that our method can learn the generators and group actions for wider two-layer homogeneous architectures as well. More examples of learned generators for larger architectures can be found in Appendix D.

### Learned data-dependent Symmetries

As a more practical application of our framework, we attempt to uncover data-dependent symmetries from architectures where no continuous symmetry is known before. We apply our framework to learn generators and loss-invariant group actions for two-layer neural network with sigmoid and tanh activation function, as well as a three-layer neural network with skip connection.

Specifically, we aim to learn symmetries in the two-layer networks defined in the previous section, but replacing \(\) by sigmoid or tanh. Our objective is again to find a set of generators \(h\) and a group action \(a\) that minimizes (4). We use 10000 training samples, each containing a randomly generated set of parameters and data. We set the learning rate as \(10^{-3}\) and the weights for the multi-objective loss as \(_{1}=1\), \(_{2}=_{4}=10\), \(_{5}=1\), and \(_{6}=0.1\).

Figure 3 shows the learned generators for data-dependent symmetries in a two-layer sigmoid MLP with parameters dimensions \(W_{1}^{3 3},W_{2}^{3 1}\) and data \(X^{3 1},Y^{1 1}\). Figure 6 in the Appendix shows the training curve. Since sigmoid networks have no data-independent continuous symmetry, this set of symmetries are data-dependent, indicating that our method successfully learns data-dependent symmetries for this architecture.

Figure 2: Generator for a two-layer linear MLP with scalar parameters and data.

Figure 4 shows the learned generators for data-dependent symmetries in a three-layer tanh MLP with parameters dimensions \(W_{1}^{2 2},W_{2}^{2 2},W_{3}^{ 2 1}\) and data \(X^{1 2},Y^{1 1}\). The generators indicate the existence of symmetries that act on non-contiguous layers, which has not been discovered in previous literature.

## 5 Discussion

While our discovery framework suggests that there are previously unknown data-dependent symmetries in various neural network architectures, the existence and number of symmetries in neural network parameter spaces remain open questions. Whether the number of symmetries is affected by existence of symmetry in data or changes during training are also interesting directions. Future work will examine the structure of learned symmetry, such as the dimension of Lie algebras.