# Convex and Non-convex Optimization Under

Generalized Smoothness

 Haochuan Li

MIT

haochuan@mit.edu

&Jian Qian

MIT

jianqian@mit.edu

&Yi Tian

MIT

yitian@mit.edu

&Alexander Rakhlin

MIT

rakhlin@mit.edu

&Ali Jadbabaie

MIT

jadbabai@mit.edu

Equal contribution.

###### Abstract

Classical analysis of convex and non-convex optimization methods often requires the Lipschitz continuity of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with bounded variance in the stochastic setting.

## 1 Introduction

In this paper, we study the following _unconstrained_ optimization problem

\[_{x}f(x),\] (1)

where \(^{d}\) is the domain of \(f\). Classical textbook analyses (Nemirovskij and Yudin, 1983; Nesterov, 2003) of (1) often require the Lipschitz smoothness condition, which assumes \(\|^{2}f(x)\| L\) almost everywhere for some \(L 0\) called the smoothness constant. This condition, however, is rather restrictive and only satisfied by functions that are both upper and lower bounded by quadratic functions.

Recently, Zhang et al. (2019) proposed the more general \((L_{0},L_{1})\)-smoothness condition, which assumes \(\|^{2}f(x)\| L_{0}+L_{1}\| f(x)\|\) for some constants \(L_{0},L_{1} 0\), motivated by their extensive language model experiments. This notion generalizes the standard Lipschitz smoothness condition and also contains e.g. univariate polynomial and exponential functions. For _non-convex_ and \((L_{0},L_{1})\)-smooth functions, they prove convergence of gradient descent (GD) and stochastic gradient descent (SGD) _with gradient clipping_ and also provide a complexity lower bound for _constant-stepsize_ GD/SGD without clipping. Based on these results, they claim gradient clipping or other forms of adaptivity _provably_ accelerate the convergence for \((L_{0},L_{1})\)-smooth functions. Perhaps due to thelower bound, all the follow-up works under this condition that we are aware of limit their analyses to adaptive methods. Most of these focus on non-convex functions. See Section 2 for more discussions of related works.

In this paper, we significantly generalize the \((L_{0},L_{1})\)-smoothness condition to the \(\)-smoothness condition which assumes \(\|^{2}f(x)\|(\| f(x)\|)\) for some non-decreasing continuous function \(\). We develop a simple, yet powerful approach, which allows us to obtain stronger results for _both convex and non-convex_ optimization problems when \(\) is sub-quadratic (i.e., \(_{u}(u)/u^{2}=0\)) or even more general. The \(\)-smooth function class with a sub-quadratic \(\) also contains e.g. univariate rational and double exponential functions. In particular, we prove the convergence of _constant-stepsize_ GD/SGD and Nesterov's accelerated gradient method (NAG) in the convex or non-convex settings. For each method and setting, we obtain the classical convergence rate, under a certain requirement of \(\). In addition, we relax the assumption of bounded noise to the weaker one of bounded variance with the simple SGD method. See Table 1 for a summary of our results and assumptions for each method and setting. At first glance, our results "contradict" the lower bounds on constant-stepsize GD/SGD in (Zhang et al., 2019; Wang et al., 2022); this will be reconciled in Section 5.3.

Our approach analyzes boundedness of gradients along the optimization trajectory. The idea behind it can be informally illustrated by the following "circular" reasoning. On the one hand, if gradients along the trajectory are bounded by a constant \(G\), then the Hessian norms are bounded by the constant \((G)\). Informally speaking, we essentially have the standard Lipschitz smoothness condition2 and can apply classical textbook analyses to prove convergence, which implies that gradients converge to zero. On the other hand, if gradients converge, they must be bounded, since any convergent sequence is bounded. In other words, the bounded gradient condition implies convergence, and convergence also implies the condition back, which forms a circular argument. If we can break this circularity of reasoning in a rigorous way, both the bounded gradient condition and convergence are proved. In this paper, we will show how to break the circularity using induction or contradiction arguments for different methods and settings in Sections 4 and 5. We note that the idea of bounding gradients can be applied to the analysis of other optimization methods, e.g., the concurrent work (Li et al., 2023) by subset of the authors, which uses a similar idea to obtain a rigorous and improved analysis of the Adam method (Kingma and Ba, 2014).

**Contributions.** In light of the above discussions, we summarize our main contributions as follows.

* We generalize the standard Lipschitz smoothness and also the \((L_{0},L_{1})\)-smoothness condition to the \(\)-smoothness condition, and develop a new approach for analyzing convergence under this condition by bounding the gradients along the optimization trajectory.
* We prove the convergence of _constant-stepsize_ GD/SGD/NAG in the convex and non-convex settings, and obtain the classical rates for all of them, as summarized in Table 1.

Besides the generalized smoothness condition and the new approach, our results are also novel in the following aspects.

* The convergence results of _constant-stepsize_ methods challenge the folklore belief on the necessity of adaptive stepsize for generalized smooth functions.
* We obtain new convergence results for GD and NAG in the convex setting under the generalized smoothness condition.
* We relax the assumption of bounded noise to the weaker one of bounded variance of noise in the stochastic setting with the simple SGD method.

## 2 Related work

**Gradient-based optimizaiton.** The classical gradient-based optimization problems for the standard Lipschitz smooth functions have been well studied for both convex (Nemirovskij and Yudin, 1983;Nesterov, 2003, d'Aspremont et al., 2021] and non-convex functions. In the convex setting, the goal is to reach an \(\)-sub-optimal point \(x\) satisfying \(f(x)-_{x}f(x)\). It is well known that GD achieves the \((1/)\) gradient complexity and NAG achieves the accelerated \((1/)\) complexity which is optimal among all gradient-based methods. For strongly convex functions, GD and NAG achieve the \(((1/))\) and \(((1/))\) complexity respectively, where \(\) is the condition number and the latter is again optimal. In the non-convex setting, the goal is to find an \(\)-stationary point \(x\) satisfying \(\| f(x)\|\), since finding a global minimum is NP-hard in general. It is well known that GD achieves the optimal \((1/^{2})\) complexity which matches the lower bound in [Carmon et al., 2017]. In the stochastic setting for unbiased stochastic gradient with bounded variance, SGD achieves the optimal \((1/^{4})\) complexity [Ghadimi and Lan, 2013], matching the lower bound in [Arjevani et al., 2019]. In this paper, we obtain the classical rates in terms of \(\) for all the above-mentioned methods and settings, under a far more general smoothness condition.

**Generalized smoothness.** The \((L_{0},L_{1})\)-smoothness condition proposed by Zhang et al.  was studied by many follow-up works. Under the same condition, [Zhang et al., 2020] considers momentum in the updates and improves the constant dependency of the convergence rate for SGD with clipping derived in [Zhang et al., 2019]. [Qian et al., 2021] studies gradient clipping in incremental gradient methods, [Zhao et al., 2021] studies stochastic normalized gradient descent, and [Crawshaw et al., 2022] studies a generalized SignSGD method, under the \((L_{0},L_{1})\)-smooths condition. [Reisizadeh et al., 2023] studies variance reduction for \((L_{0},L_{1})\)-smooth functions. [Chen et al., 2023] proposes a new notion of \(\)-symmetric generalized smoothness, which is roughly as general as \((L_{0},L_{1})\)-smoothness. [Wang et al., 2022] analyzes convergence of Adam and provides a lower bound which shows non-adaptive SGD may diverge. In the stochastic setting, the above-mentioned works either consider the strong assumption of bounded gradient noise or require a very large batch size that depends on \(\), which essentially reduces the analysis to the deterministic setting. [Faw et al., 2023] proposes an AdaGrad-type algorithm in order to relax the bounded noise assumption. Perhaps due to the lower bounds in [Zhang et al., 2019, Wang et al., 2022], all the above works study methods with an adaptive stepsize. In this and our concurrent work [Li et al., 2023], we further generalize the smoothness condition and analyze various methods under this condition through bounding the gradients along the trajectory.

## 3 Function class

In this section, we discuss the function class of interest where the objective function \(f\) lies. We start with the following two standard assumptions in the literature of unconstrained optimization, which will be assumed throughout Sections 4 and 5 unless explicitly stated.

**Assumption 1**.: The objective function \(f\) is differentiable and _closed_ within its _open_ domain \(\).

**Assumption 2**.: The objective function \(f\) is bounded from below, i.e., \(f^{}:=_{x}f(x)>-\).

A function \(f\) is said to be closed if its sub-level set \(\{x(f) f(x) a\}\) is closed for each \(a\). A continuous function \(f\) with an open domain is closed if and only \(f(x)\) tends to positive infinity when \(x\) approaches the boundary of its domain [Boyd and Vandenberghe, 2004]. Assumption 1 is necessary for our analysis to ensure that the iterates of a method with a reasonably small stepsize stays within the domain \(\). Note that for \(=^{d}\) considered in most unconstrained

   Method & Convexity & \(\)-smoothness & Gradient complexity \\   & Strongly convex &  & \(((1/))\) (Theorem 4.3) \\  & Convex & & \((1/)\) (Theorem 4.2 ) \\   & & Sub-quadratic \(\) & \((1/^{2})^{}\) (Theorem 5.2) \\   & Non-convex & Quadratic \(\) & \(\)(exp. in cond \(\#\)) (Theorem 5.4 ) \\  NAG & Convex & Sub-quadratic \(\) & \((1/)^{}\) (Theorem 4.4 ) \\  SGD & Non-convex & Sub-quadratic \(\) & \((1/^{4})^{}\) (Theorem 5.3) \\   

Table 1: Summary of the results. \(\) denotes the sub-optimality gap of the function value in convex settings, and the gradient norm in non-convex settings. “\(*\)” denotes optimal rates.

optimization papers, the assumption is trivially satisfied as all continuous functions over \(^{d}\) are closed. We consider a more general domain which may not be the whole space because that is the case for some interesting examples in our function class of interest (see Section 3.1.3). However, it actually brings us some additional technical difficulties especially in the stochastic setting, as we need to make sure the iterates do not go outside of the domain.

### Generalized smoothness

In this section, we formally define the generalized smoothness condition, and present its properties and examples.

#### 3.1.1 Definitions

Definitions 1 and 2 below are two equivalent ways of stating the definition, where we use \((x,R)\) to denote the Euclidean ball with radius \(R\) centered at \(x\).

**Definition 1** (\(\)-smoothness).: A real-valued differentiable function \(f:\) is \(\)-smooth for some non-decreasing continuous function \(:[0,+)(0,+)\) if \(\|^{2}f(x)\|(\| f(x)\|)\)_almost everywhere_ (with respect to the Lebesgue measure) in \(\).

_Remark 3.1_.: Definition 1 reduces to the classical \(L\)-smoothness when \( L\) is a constant function. It reduces to the \((L_{0},L_{1})\)-smoothness proposed in (Zhang et al., 2019) when \((u)=L_{0}+L_{1}u\) is an affine function.

**Definition 2** (\((r,)\)-smoothness).: A real-valued differentiable function \(f:\) is \((r,)\)-smooth for continuous functions \(r,:[0,+)(0,+)\) where \(\) is non-decreasing and \(r\) is non-increasing, if it satisfies 1) for any \(x\), \((x,r(\| f(x)\|))\), and 2) for any \(x_{1},x_{2}(x,r(\| f(x)\|))\), \(\| f(x_{1})- f(x_{2})\|(\| f(x) \|)\|x_{1}-x_{2}\|\).

The requirements that \(\) is non-decreasing and \(r\) is non-increasing do not cause much loss in generality. If these conditions are not satisfied, one can replace \(\) and \(r\) with the non-increasing function \((u):=_{0 v u}r(v) r(u)\) and non-decreasing function \((u):=_{0 v u}(v)(u)\) in Definitions 1 and 2. Then the only requirement is \(>0\) and \(<\).

Next, we prove that the above two definitions are equivalent in the following proposition, whose proof is involved and deferred to Appendix A.2.

**Proposition 3.2**.: _An \((r,)\)-smooth function is \(\)-smooth; and an \(\)-smooth function satisfying Assumption 1 is \((r,m)\)-smooth where \(m(u):=(u+a)\) and \(r(u):=a/m(u)\) for any \(a>0\)._

The condition in Definition 1 is simple and one can easily check whether it is satisfied for a given example function. On the other hand, Definition 2 is a local Lipschitz condition on the gradient that is harder to verify. However, it is useful for deriving several useful properties in the next section.

#### 3.1.2 Properties

First, we provide the following lemma which is very useful in our analyses of all the methods considered in this paper. Its proof is deferred to Appendix A.3.

**Lemma 3.3**.: _If \(f\) is \((r,)\)-smooth, for any \(x\) satisfying \(\| f(x)\| G\), we have 1) \((x,r(G))\), and 2) for any \(x_{1},x_{2}(x,r(G))\),_

\[\| f(x_{1})\!-\! f(x_{2})\|\!\!L\|x_{1}\!-\!x _{2}\|, f(x_{1})\!\!f(x_{2})\!+\! f(x_{2}),x _{1}\!-\!x_{2}\!+\!\|x_{1}\!-\!x_{2}\|^{2},\] (2)

_where \(L:=(G)\) is the effective smoothness constant._

_Remark 3.4_.: Since we have shown the equivalence between \(\)-smoothness and \((r,)\)-smoothness, Lemma 3.3 also applies to \(\)-smooth functions, for which we have \(L=(2G)\) and \(r(G)=G/L\) if choosing \(a=G\) in Proposition 3.2.

Lemma 3.3 states that, if the gradient at \(x\) is bounded by some constant \(G\), then within its neighborhood with a _constant_ radius, we can obtain (2), the same inequalities that were derived in the textbook analysis (Nesterov, 2003) under the standard Lipschitz smoothness condition. With (2), the analysis for generalized smoothness is not much harder than that for standard smoothness. Since we mostly choose \(x=x_{2}=x_{t}\) and \(x_{1}=x_{t+1}\) in the analysis, in order to apply Lemma 3.3, we need two conditions: \(\| f(x_{t})\| G\) and \(\|x_{t+1}-x_{t}\| r(G)\) for some constant \(G\). The latter is usually directly implied by the former for most deterministic methods with a small enough stepsize, and the former can be obtained with our new approach that bounds the gradients along the trajectory.

With Lemma 3.3, we can derive the following useful lemma which is the reverse direction of a generalized Polyak-Lojasiewicz (PL) inequality, whose proof is deferred to Appendix A.3.

**Lemma 3.5**.: _If \(f\) is \(\)-smooth, then \(\| f(x)\|^{2} 2(2\| f(x)\|)(f(x)-f ^{*})\) for any \(x\)._

Lemma 3.5 provides an inequality involving the gradient norm and the sub-optimality gap. For example, when \((u)=u^{}\) for some \(0<2\), this lemma suggests \(\| f(x)\|((f(x)-f^{*})^{1/(2-)})\), which means the gradient norm is bounded whenever the function value is bounded. The following corollary provides a more formal statement for general sub-quadratic \(\) (i.e., \(_{u}(u)/u^{2}=0\)), and we defer its proof to Appendix A.3.

**Corollary 3.6**.: _Suppose \(f\) is \(\)-smooth where \(\) is sub-quadratic. If \(f(x)-f^{*} F\) for some \(x\) and \(F 0\), denoting \(G:=\{u 0 u^{2} 2(2u) F\}\), then they satisfy \(G^{2}=2(2G) F\) and we have \(\| f(x)\| G<\)._

Therefore, in order to bound the gradients along the trajectory as we discussed below Lemma 3.3, it suffices to bound the function values, which is usually easier.

#### 3.1.3 Examples

The most important subset of \(\)-smooth (or \((r,)\)-smooth) functions are those with a polynomial \(\), and can be characterized by the \((,L_{0},L_{})\)-smooth function class defined below.

**Definition 3** (\((,L_{0},L_{})\)-smoothness).: A real-valued differentiable function \(f\) is \((,L_{0},L_{})\)-smooth for constants \(,L_{0},L_{} 0\) if it is \(\)-smooth with \((u)=L_{0}+L_{}u^{}\).

Definition 3 reduces to the standard Lipschitz smoothness condition when \(=0\) or \(L_{}=0\) and to the \((L_{0},L_{1})\)-smoothness proposed in (Zhang et al., 2019) when \(=1\). We list several univariate examples of \((,L_{0},L_{})\)-smooth functions for different \(\)s in Table 2 with their rigorous justifications in Appendix A.1. Note that when \(x\) goes to infinity, polynomial and exponential functions corresponding to \(=1\) grow much faster than quadratic functions corresponding to \(=0\). Rational and logarithmic functions for \(>1\) grow even faster as they can blow up to infinity near finite points. Note that the domains of such functions are not \(^{d}\), which is why we consider the more general Assumption 1 instead of simply assuming \(=^{d}\).

Aside from logarithmic functions, the \((2,L_{0},L_{2})\)-smooth function class also includes other univariate _self-concordant_ functions. This is an important function class in the analysis of Interior Point Methods and coordinate-free analysis of the Newton method (Nesterov, 2003). More specifically, a convex function \(h:\) is self-concordant if \(|h^{}(x)| 2h^{}(x)^{3/2}\) for all \(x\). Formally, we have the following proposition whose proof is deferred to Appendix A.1.

**Proposition 3.7**.: _If \(h:\) is a self-concordant function satisfying \(h^{}(x)>0\) over the interval \((a,b)\), then \(h\) restricted on \((a,b)\) is \((2,L_{0},2)\)-smooth for some \(L_{0}>0\)._

## 4 Convex setting

In this section, we present the convergence results of gradient descent (GD) and Nesterov's accelerated gradient method (NAG) in the convex setting. Formally, we define convexity as follows.

   \(\) & \(0\) & \(1\) & \(1\) & \(1^{+}\) & \(1.5\) & \(2\) & \(\) \\  Example Functions & Quadratic & Polynomial & \(a^{x}\) & \(a^{(b^{x})}\) & Rational & Logarithmic & \(x^{p}\) \\   

Table 2: Examples of univariate \((,L_{0},L_{})\) smooth functions for different \(\)s. The parameters \(a,b,p\) are _real numbers_ (not necessarily integers) satisfying \(a,b>1\) and \(p<1\) or \(p 2\). We use \(1^{+}\) to denote any real number slightly larger than \(1\).

**Definition 4**.: A real-valued differentiable function \(f:\) is \(\)-strongly-convex for \( 0\) if \(\) is a convex set and \(f(y)-f(x) f(x),y-x+\|y-x \|^{2}\) for any \(x,y\). A function is convex if it is \(\)-strongly-convex with \(=0\).

We assume the existence of a global optimal point \(x^{*}\) throughout this section, as in the following assumption. However, we want to note that, for gradient descent, this assumption is just for simplicity rather than necessary.

**Assumption 3**.: There exists a point \(x^{*}\) such that \(f(x^{*})=f^{*}=_{x}f(x)\).

### Gradient descent

The gradient descent method with a constant stepsize \(\) is defined via the following update rule

\[x_{t+1}=x_{t}- f(x_{t}).\] (3)

As discussed below Lemma 3.3, the key in the convergence analysis is to show \(\| f(x_{t})\| G\) for all \(t 0\) and some constant \(G\). We will prove it by induction relying on the following lemma whose proof is deferred to Appendix B.

**Lemma 4.1**.: _For any \(x\) satisfying \(\| f(x)\| G\), define \(x^{+}:=x- f(x)\). If \(f\) is convex and \((r,)\)-smooth, and \(\{,\}\), we have \(x^{+}\) and \(\| f(x^{+})\|\| f(x)\| G\)._

Lemma 4.1 suggests that for gradient descent (3) with a small enough stepsize, if the gradient norm at \(x_{t}\) is bounded by \(G\), then we have \(\| f(x_{t+1})\|\| f(x_{t})\| G\), i.e., the gradient norm is also bounded by \(G\) at \(t+1\). In other words, the gradient norm is indeed a non-increasing potential function for gradient descent in the convex setting. With a standard induction argument, we can show that \(\| f(x_{t})\|\| f(x_{0})\|\) for all \(t 0\). As discussed below Lemma 3.3, then we can basically apply the classical analysis to obtain the convergence guarantee in the convex setting as in the following theorem, whose proof is deferred to Appendix B.

**Theorem 4.2**.: _Suppose \(f\) is convex and \((r,)\)-smooth. Denote \(G:=\| f(x_{0})\|\) and \(L:=(G)\), then the iterates generated by (3) with \(\{,\}\) satisfy \(\| f(x_{t})\| G\) for all \(t 0\) and_

\[f(x_{T})-f^{*}-x^{*}\|^{2}}{2 T}.\]

Since \(\) is a constant independent of \(\) or \(T\), Theorem 4.2 achieves the classical \((1/T)\) rate, or \((1/)\) gradient complexity to achieve an \(\)-sub-optimal point, under the generalized smoothness condition. Since strongly convex functions are a subset of convex functions, Lemma 4.1 still holds for them. Then we immediately obtain the following result in the strongly convex setting, whose proof is deferred to Appendix B.

**Theorem 4.3**.: _Suppose \(f\) is \(\)-strongly-convex and \((r,)\)-smooth. Denote \(G:=\| f(x_{0})\|\) and \(L:=(G)\), then the iterates generated by (3) with \(\{,\}\) satisfy \(\| f(x_{t})\| G\) for all \(t 0\) and_

\[f(x_{T})-f^{*}}{2(1-(1-)^{T})}\|x_{0} -x^{*}\|^{2}.\]

Theorem 4.3 gives a linear convergence rate and the \((()^{-1}(1/))\) gradient complexity to achieve an \(\)-sub-optimal point. Note that for \(\)-smooth functions, we have \(=\) (see Remark 3.4), which means we can choose \(=\). Then we obtain the \(((1/))\) rate, where \(:=L/\) is the local condition number around the initial point \(x_{0}\). For standard Lipschitz smooth functions, it reduces to the classical rate of gradient descent.

### Nesterov's accelerated gradient method

In the case of convex and standard Lipschitz smooth functions, it is well known that Nesterov's accelerated gradient method (NAG) achieves the optimal \((1/T^{2})\) rate. In this section, we show thatunder the \(\)-smoothness condition with a _sub-quadratic_\(\), the optimal \((1/T^{2})\) rate can be achieved by a slightly modified version of NAG shown in Algorithm 1, the only difference between which and the classical NAG is that the latter directly sets \(A_{t+1}=B_{t+1}\) in Line 4. Formally, we have the following theorem, whose proof is deferred to Appendix C.

**Theorem 4.4**.: _Suppose \(f\) is convex and \(\)-smooth where \(\) is sub-quadratic. Then there always exists a constant \(G\) satisfying \(G\{8)-f^{*})+\|x_{0}-x^{*}\|^{2 })},\| f(x_{0})\|\}.\) Denote \(L:=(2G)\) and choose \(\{},\}\). The iterates generated by Algorithm 1 satisfy_

\[f(x_{T})-f^{*})-f^{*})+4\|x_{0}-x^{*}\|^{2}}{  T^{2}+4}.\]

It is easy to note that Theorem 4.4 achieves the accelerated \((1/T^{2})\) convergence rate, or equivalently the \((1/)\) gradient complexity to find an \(\)-sub-optimal point, which is optimal among gradient-based methods .

In order to prove Theorem 4.4, we also use induction to show the gradients along the trajectory of Algorithm 1 are bounded by \(G\). However, unlike gradient descent, the gradient norm is no longer a potential function or monotonically non-increasing, which makes the induction analysis more challenging. Suppose that we have shown \(\| f(y_{s})\| G\) for \(s<t\). To complete the induction, it suffices to prove \(\| f(y_{t})\| G\). Since \(x_{t}=y_{t-1}- f(y_{t-1})\) is a gradient descent step by Line 6 of Algorithm 1, Lemma 4.1 directly shows \(\| f(x_{t})\| G\). In order to also bound \(\| f(y_{t})\|\), we try to control \(\|y_{t}-x_{t}\|\), which is the most challenging part of our proof. Since \(y_{t}-x_{t}\) can be expressed as a linear combination of past gradients \(\{ f(y_{s})\}_{s<t}\), it might grow linearly with \(t\) if we simply apply \(\| f(y_{s})\| G\) for \(s<t\). Fortunately, Lemma 3.5 allows us to control the gradient norm with the function value. Thus if the function value is decreasing sufficiently fast, which can be shown by following the standard Lyapunov analysis of NAG, we are able to obtain a good enough bound on \(\| f(y_{s})\|\) for \(s<t\), which allows us to control \(\|y_{t}-x_{t}\|\). We defer the detailed proof to Appendix C.

Note that Theorem 4.4 requires a smaller stepsize \(=(1/L^{2})\), compared to the classical \((1/L)\) stepsize for standard Lipschitz smooth functions. The reason is we require a small enough stepsize to get a good enough bound on \(\|y_{t}-x_{t}\|\). However, if the function is further assumed to be \(\)-smooth with a _sub-linear_\(\), the requirement of stepsize can be relaxed to \(=(1/L)\), similar to the classical requirement. See Appendix C for the details.

In the strongly convex setting, we can also prove convergence of NAG with different \(\{A_{t}\}_{t 0}\) parameters when \(f\) is \(\)-smooth with a sub-quadratic \(\), or \((,L_{0},L_{})\)-smooth with \(<2\). The rate can be further improved when \(\) becomes smaller. However, since the constants \(G\) and \(L\) are different for GD and NAG, it is not clear whether the rate of NAG is faster than that of GD in the strongly convex setting. We will present the detailed result and analysis in Appendix D.

## 5 Non-convex setting

In this section, we present convergence results of gradient descent (GD) and stochastic gradient descent (SGD) in the non-convex setting.

### Gradient descent

Similar to the convex setting, we still want to bound the gradients along the trajectory. However, in the non-convex setting, the gradient norm is not necessarily non-increasing. Fortunately, similar to the classical analyses, the function value is still non-increasing and thus a potential function, as formally shown in the following lemma, whose proof is deferred to Appendix E.

**Lemma 5.1**.: _Suppose \(f\) is \(\)-smooth where \(\) is sub-quadratic. For any given \(F 0\), let \(G:=\{u 0 u^{2} 2(2u) F\}\) and \(L:=(2G)\). For any \(x\) satisfying \(f(x)-^{*} F\), define \(x^{+}:=x- f(x)\) where \( 2/L\), we have \(x^{+}\) and \(f(x^{+}) f(x)\)._

Then using a standard induction argument, we can show \(f(x_{t}) f(x_{0})\) for all \(t 0\). According to Corollary 3.6, it implies bounded gradients along the trajectory. Therefore, we can show convergence of gradient descent as in the following theorem, whose proof is deferred to Appendix E.

**Theorem 5.2**.: _Suppose \(f\) is \(\)-smooth where \(\) is sub-quadratic. Let \(G:=\{u 0 u^{2} 2(2u)(f(x_{0})-f^{*})\}\) and \(L:=(2G)\). If \( 1/L\), the iterates generated by (3) satisfy \(\| f(x_{t})\| G\) for all \(t 0\) and_

\[_{t<T}\| f(x_{t})\|^{2})-f^ {*})}{ T}.\]

It is clear that Theorem 5.2 gives the classical \((1/^{2})\) gradient complexity to achieve an \(\)-stationary point, which is optimal as it matches the lower bound in (Carmon et al., 2017).

### Stochastic gradient descent

In this part, we present the convergence result for stochastic gradient descent defined as follows.

\[x_{t+1}=x_{t}- g_{t},\] (4)

where \(g_{t}\) is an estimate of the gradient \( f(x_{t})\). We consider the following standard assumption on the gradient noise \(_{t}:=g_{t}- f(x_{t})\).

**Assumption 4**.: \(_{t-1}[_{t}]=0\) and \(_{t-1}[\|_{t}\|^{2}]^{2}\) for some \( 0\), where \(_{t-1}\) denotes the expectation conditioned on \(\{g_{s}\}_{s<t}\).

Under Assumption 4, we can obtain the following theorem.

**Theorem 5.3**.: _Suppose \(f\) is \(\)-smooth where \(\) is sub-quadratic. For any \(0<<1\), we denote \(F:=8(f(x_{0})-f^{*}+)/\) and \(G:=\{u 0 u^{2} 2(2u) F\}<\). Denote \(L:=(2G)\) and choose \(\{,}\}\) and \(T}\) for any \(>0\). Then with probability at least \(1-\), the iterates generated by (4) satisfy \(\| f(x_{t})\| G\) for all \(t<T\) and_

\[_{t<T}\| f(x_{t})\|^{2}^{2}.\]

As we choose \(=(1/)\), Theorem 5.3 gives the classical \((1/^{4})\) gradient complexity, where we ignore non-leading terms. This rate is optimal as it matches the lower bound in (Arjevani et al., 2019). The key to its proof is again to bound the gradients along the trajectory. However, bounding gradients in the stochastic setting is much more challenging than in the deterministic setting, especially with the heavy-tailed noise in Assumption 4. We briefly discuss some of the challenges as well as our approach below and defer the detailed proof of Theorem 5.3 to Appendix F.

First, due to the existence of heavy-tailed gradient noise as considered in Assumption 4, neither the gradient nor the function values is non-increasing. The induction analyses we have used in the deterministic setting hardly work. In addition, to apply Lemma 3.3, we need to control the update at each step and make sure \(\|x_{t+1}-x_{t}\|=\|g_{t}\| G/L\). However, \(g_{t}\) might be unbounded due to the potentially unbounded gradient noise.

To overcome these challenges, we define the following random variable \(\).

\[_{1} :=\{t f(x_{t+1})-f^{*}>F\} T,\] (5) \[_{2} :=\{t\|_{t}\|> \} T,\] \[ :=\{_{1},_{2}\},\]

where we use \(a b\) to denote \(\{a,b\}\) for any \(a,b\). Then at least before time \(\), we know that the function value and gradient noise are bounded, where the former also implies bounded gradients according to Corollary 3.6. Therefore, it suffices to show the probability of \(<T\) is small, which means with a high probability, \(=T\) and thus gradients are always bounded before \(T\).

Since both the gradient and noise are bounded for \(t<\), it is straightforward to bound the update \(\|x_{t+1}-x_{t}\|\), which allows us to use Lemma 3.3 and other useful properties. However, it is still non-trivial to upper bound \([f(x_{})-f^{*}]\) as \(\) is a random variable instead of a fixed time step. Fortunately, \(\) is a stopping time with nice properties. That is because both \(f(x_{t+1})\) and \(_{t}=g_{t}- f(x_{t})\) only depend on \(\{g_{s}\}_{s t}\), i.e., the stochastic gradients up to \(t\). Therefore, for any fixed \(t\), the events \(\{>t\}\) only depend on \(\{g_{s}\}_{s t}\), which show \(\) is a stopping time. Then with a careful analysis, we are still able to obtain an upper bound on \([f(x_{})-f^{*}]=(1)\).

On the other hand, \(<T\) means either \(=_{1}<T\) or \(=_{2}<T\). If \(=_{1}<T\), by its definition, we know \(f(x_{+1})-f^{*}>F\). Roughly speaking, it also suggests \(f(x_{})-f^{*}>F/2\). If we choose \(F\) such that it is much larger than the upper bound on \([f(x_{})-f^{*}]\) we just obtained, by Markov's inequality, we can show the probability of \(=_{1}<T\) is small. In addition, by union bound and Chebyshev's inequality, the probability of \(_{2}<T\) can also be bounded by a small constant. Therefore, we have shown \(<T\). Then the rest of the analysis is not too hard following the classical analysis.

### Reconciliation with existing lower bounds

In this section, we reconcile our convergence results for constant-stepsize GD/SGD in the non-convex setting with existing lower bounds in (Zhang et al., 2019) and (Wang et al., 2022), based on which the authors claim that adaptive methods such as GD/SGD with clipping and Adam are provably faster than non-adaptive GD/SGD. This may seem to contradict our convergence results. In fact, we show that any gain in adaptive methods is at most by constant factors, as GD and SGD already achieve the optimal rates in the non-convex setting.

(Zhang et al., 2019) provides both upper and lower complexity bounds for constant-stepsize GD for \((L_{0},L_{1})\)-smooth functions, and shows that its complexity is \((M^{-2})\), where

\[M:=\{\| f(x)\| f(x) f(x_{0})\}\]

is the supremum gradient norm below the level set of the initial function value. If \(M\) is very large, then the \((M^{-2})\) complexity can be viewed as a negative result, and as evidence that constant-stepsize GD can be slower than GD with gradient clipping, since in the latter case, they obtain the \((^{-2})\) complexity without \(M\). However, based on our Corollary 3.6, their \(M\) can be actually bounded by our \(G\), which is a constant. Therefore, the gain in adaptive methods is at most by constant factors.

(Wang et al., 2022) further provides a lower bound which shows non-adaptive GD may diverge for some examples. However, their counter-example does not allow the stepsize to depend on the initial sub-optimality gap. In contrast, our stepsize \(\) depends on the effective smoothness constant \(L\), which depends on the initial sub-optimality gap through \(G\). Therefore, there is no contradiction here either. We should point out that in the practice of training neural networks, the stepsize is usually tuned after fixing the loss function and initialization, so it does depend on the problem instance and initialization.

### Lower bound

For \((,L_{0},L_{})\)-smooth functions with \(<2\), it is easy to verify that the constant \(G\) in both Theorem 5.2 and Theorem 5.3 is a polynomial function of problem-dependent parameters like \(L_{0},L_{},f(x_{0})-f^{*},\), etc. In other words, GD and SGD are provably efficient methods in the non-convex setting for \(<2\). In this section, we show that the requirement of \(<2\) is necessary in the non-convex setting with the lower bound for GD in the following Theorem 5.4, whose proof is deferred in Appendix G. Since SGD reduces to GD when there is no gradient noise, it is also a lower bound for SGD.

**Theorem 5.4**.: _Given \(L_{0},L_{2},G_{0},_{0}>0\) satisfying \(L_{2}_{0} 10\), for any \( 0\), there exists a \((2,L_{0},L_{2})\)-smooth function \(f\) that satisfies Assumptions 1 and 2, and initial point \(x_{0}\) that satisfies \(\| f(x_{0})\| G_{0}\) and \(f(x_{0})-f^{*}_{0}\), such that gradient descent with stepsize \(\) (3) either cannot reach a \(1\)-stationary point or takes at least \((L_{2}_{0}/8)/6\) steps to reach a \(1\)-stationary point._

## 6 Conclusion

In this paper, we generalize the standard Lipschitz smoothness as well as the \((L_{0},L_{1})\)-smoothness (Zhang et al., 2020) conditions to the \(\)-smoothness condition, and develop a new approach for analyzing the convergence under this condition. The approach uses different techniques for several methods and settings to bound the gradient along the optimization trajectory, which allows us to obtain stronger results for both convex and non-convex problems. We obtain the classical rates for GD/SGD/NAG methods in the convex and/or non-convex setting. Our results challenge the folklore belief on the necessity of adaptive methods for generalized smooth functions.

There are several interesting future directions following this work. First, the \(\)-smoothness can perhaps be further generalized by allowing \(\) to also depend on potential functions in each setting, besides the gradient norm. In addition, it would also be interesting to see if the techniques of bounding gradients along the trajectory that we have developed in this and the concurrent work (Li et al., 2023) can be further generalized to other methods and problems and to see whether more efficient algorithms can be obtained. Finally, although we justified the necessity of the requirement of \(\)-smoothness with a _sub-quadratic_\(\) in the non-convex setting, it is not clear whether it is also necessary for NAG in the convex setting, another interesting open problem.