# BMRS: Bayesian Model Reduction for Structured Pruning

Dustin Wright, Christian Igel, and Raghavendra Selvan

Department of Computer Science, University of Copenhagen

{dw,igel,raghav}@di.ku.dk

###### Abstract

Modern neural networks are often massively overparameterized leading to high compute costs during training and at inference. One effective method to improve both the compute and energy efficiency of neural networks while maintaining good performance is structured pruning, where full network structures (e.g. neurons or convolutional filters) that have limited impact on the model output are removed. In this work, we propose Bayesian Model Reduction for Structured pruning (BMRS), a fully end-to-end Bayesian method of structured pruning. BMRS is based on two recent methods: Bayesian structured pruning with multiplicative noise, and Bayesian model reduction (BMR), a method which allows efficient comparison of Bayesian models under a change in prior. We present two realizations of BMRS derived from different priors which yield different structured pruning characteristics: 1) BMRS\({}_{}\) with the truncated log-normal prior, which offers reliable compression rates and accuracy without the need for tuning any thresholds and 2) BMRS\({}_{}\) with the truncated log-uniform prior that can achieve more aggressive compression based on the boundaries of truncation. Overall, we find that BMRS offers a theoretically grounded approach to structured pruning of neural networks yielding both high compression rates and accuracy. Experiments on multiple datasets and neural networks of varying complexity showed that the two BMRS methods offer a competitive performance-efficiency trade-off compared to other pruning methods.1

## 1 Introduction

Modern neural networks come with an increasing computational burden, as scale is often seen to be associated with performance . The response to this has been a focus on research around the topic of neural network efficiency , where the goal is to reduce the computational cost of a system while maintaining other desirable metrics. As such, selecting a method to improve efficiency comes with many tradeoffs, including how to balance compute and energy consumption with accuracy .

Neural network pruning seeks to do this by removing parts of a network which have limited impact on its output. This comes in two primary forms: unstructured pruning, where individual weights are removed, and structured pruning, where entire neural network structures such as neurons and convolutional filters are removed . Structured pruning is often desirable as unstructured pruning can result in sparse computations which are energy intensive on current hardware, while structured pruning can maintain more energy efficient dense operations . Many ways to perform structured pruning have been proposed, but the challenge of how to appropriately balance accuracy and complexity in a principled manner has remained.

In this work, we address this challenge by proposing BMRS: **B**ayesian **M**odel **R**eduction for **S**tructured pruning. BMRS is a principled method based on combining two complementary linesof work: Bayesian structured pruning with multiplicative noise  and Bayesian model reduction (BMR) , a method of efficient Bayesian model comparison under a change in prior. Multiplicative noise allows one to flexibly induce sparsity at any structural level without the need to use computationally complex spike-and-slab priors [16; 29], while BMR enables principled pruning rules without the need for task-specific threshold tuning. Starting with the approach from , we derive two versions of BMRS using different priors which offer their own benefits. BMRS\({}_{}\) is based on the truncated log-normal prior and has the benefit of achieving a high compression rate without needing to tune a threshold for compression, while BMRS\({}_{}\) offers tunable compression by controlling the allowable precision of noise variables in the network. In sum, our contributions are:

* BMRS: a method for Bayesian structured pruning based on multiplicative noise and Bayesian model reduction;
* Derivations of pruning algorithms for two priors with theoretical motivation;
* Empirical results on a range of neural networks and datasets demonstrating high compression rates without any threshold tuning, with more extreme compression achievable via a parameter controlling allowable precision.

## 2 Related work

The primary goal of neural network pruning is to determine the elements of a network which can be removed with minimal impact on the output. Ideally, a pruning method ranks all elements in the order in which they can be removed and provides a criterion for truncating the resulting ordered list. Since the early works on gradient based methods for pruning [23; 12], the literature around neural network pruning has expanded rapidly, with the two main lines of work exploring pruning individual weights (unstructured pruning) and pruning full network structures (structured pruning). For a recent survey, see . The closest related works to ours are those pruning methods which perform Bayesian pruning [30; 28; 10; 16; 26], and those which use Bayesian model reduction to determine what elements to remove from a neural network [5; 29].

Bayesian pruning.Bayesian structured pruning was first explored in Kingma et al. , where the authors demonstrate that dropout has a Bayesian interpretation as multiplicative noise with a sparsity inducing prior. The studies of [30; 28; 10] follow this work by explicitly modeling the random noise in dropout with different priors,  using a truncated log-uniform prior and [28; 10] using horseshoe priors. Following this, the works of [2; 15; 16; 33; 29; 26] have explored pruning of Bayesian neural networks (BNNs) with spike-and-slab priors to induce both weight sparsity and group sparsity with flat and hierarchical priors, respectively.  demonstrate that thresholdless pruning is achievable by placing an explicit spike-and-slab prior on the nodes of a BNN to induce group sparsity. However, this setup requires complex and carefully constructed posteriors due to the discrete nature of spike-and-slab distributions and is thus computationally inefficient [29; 16].

Bayesian model reduction.Bayesian model reduction, discussed in detail in SS3.2, is an efficient method of Bayesian model comparison which allows for analytic solutions for the model evidence under a change in priors. BMR has found application across multiple scientific disciplines [9; 8; 18], and has recently been used as a method for neural network pruning [29; 5]. More specifically,  demonstrate the benefits of BMR-based pruning for the case of a BNN with a Gaussian prior on

Figure 1: BMRS uses BMR to perform structured pruning under multiplicative noise by calculating the change in log-evidence of noise variables \(\) under a prior which would shrink them to 0.

the weights, and  demonstrate the utility of BMR for unstructured pruning of BNNs with priors inducing both weight and group sparsity.

## 3 Problem formulation

### Structured pruning with multiplicative noise and variational inference

We approach the problem of structured pruning using sparsity inducing multiplicative noise as described in . In this setting, we have a dataset consisting of \(N\) i.i.d. input-output pairs, \(=\{(_{j},y_{j})\; j=1,,N\}\). We consider a parametric model, here a deep neural network, that maps the input data \(_{j}\) to their output \(y_{j}\) using the trainable parameters \(\) giving rise to the likelihood function \(p(|,)=_{j=1}^{N}p(y_{j}|_{j},,)\). In addition to the trainable weights, \(\), the model consists of the sparsity inducing multiplicative noise given by the random variable, \(\), with prior \(p()\). This is in contrast to BNNs where the weights are random variables but aligns with the setting when using multiplicative noise for Bayesian pruning .

The effect of the multiplicative noise \(_{i}\) for a structural element in a neural network with index \(i\), parameters \(_{i}\), and input \(_{i-1}\) is given as

\[_{i}=_{i}(_{i}_{i-1}), _{i} p(_{i}).\] (1)

We note that \(_{i}\) could be the parameters of any structural element in the network, for example, a single neuron or an entire convolutional filter. Given this, we would like to learn the maximum likelihood estimate (MLE) \(_{i}}\) of the weights as well as the posterior distribution over the multiplicative noise, \(p(_{i}|,_{i}})\), when using a sparsity inducing prior \(p(_{i})\) such that \(_{i}\) favors values closer to 0.

Following , the neural network weights are learned via gradient descent as in standard deep learning model optimization. The posterior distribution, \(p(|,_{i}})=p(|_{i},_{i}})p(_{i})/p()\), however, is intractable. We resort to a variational approximation from a tractable family of approximating distributions, \(q_{}()\), parameterized by \(\) (for the sake of brevity we do not indicate the dependence on \(_{i}\) and omit the subscript \(i\)). The parameters \(\) are obtained by optimizing the following objective w.r.t. \(\):

\[[p,q]=D_{}[q_{}()||p(|)] {=}D_{}[q_{}()||p()]-_{q_{}}[ p(y_{ j}|_{j},,})]\] (2)

This is the commonly used variational free energy (VFE) or negative evidence lower bound (ELBO) . Here \(\) denotes equality up to a positive constant.

The expectation \(_{q_{}}[]\) is approximated by a Monte Carlo estimator acting on minibatch samples from \(\), and reparameterization allows to backpropagate gradients through stochastic variables [19; 28]. Under this reparameterization, the variational distribution, \(q_{}()\), becomes a deterministic function of the non-parametric noise \( p()\) and the VFE is calculated as

\[[p,q]D_{}[q_{}()||p()]-_{( x_{j},y_{j})} p(y_{j}|x_{j},=f(,);}),\] (3)

where \(f\) is a function that allows us to sample \(\) via deterministic parameters \(\) and the non-parametric stochastic variable, \(\). Optimization of Equation 3 allows us to jointly learn \(}\) and \(\). The particular choice of priors and the approximating distributions to induce sparsity are discussed in SS4.1.

### Bayesian model reduction

Bayesian model reduction (BMR) allows one to _efficiently_ compute the change in VFE (Equation 2) under a change in prior without the need to re-estimate model parameters. To perform pruning, one can start out by selecting a broad prior for the original model estimation and then pick a narrower prior (i.e. reduced prior) with the density concentrated around 0. Then, BMR can be used to determine if the VFE is greater under the reduced model, and prune those parameters for which this condition holds. We briefly describe how this is achieved in the general case, followed by the specific realization for BMRS in SS4.2; for further details see .

Consider the likelihood function \(p(|)\) and a prior \(p()\) on the variable \(\). We can introduce a new prior \(()\) which shares the same likelihood as the original model (i.e. \(p(|)=(|)\)) and get:

\[p(|)=)p()}{p()}= (|)()}{()} (|)=)}{( )}p(|)()}{p()}\] (4)By marginalizing over \(\) and taking the log, we obtain the difference in log evidence as:

\[()- p()= p(|) ()}{p()}d q_{}() {()}{p()}d=_{}[()}{p()}]\] (5)

More concisely, we call the change in log evidence \( F\) and thus have:

\[ F()- p() _{}[()}{p()}]\] (6)

If the new prior, \(()\), is selected so that \(\) would be removed, pruning can be performed when \( F 0\). Additionally, when the type of distributions between \(p()\), \(()\), and \(q_{}()\) are the same or similar (e.g. Gaussian), \( F\) can be calculated efficiently in closed form (see ).

We presented BMR for a general likelihood function \(p(|)\); it holds analogously for the likelihood function \(p(|,)\) introduced with the multiplicative noise described in SS3.1.

## 4 Bayesian model reduction for structured pruning (BMRS)

Our goal is to derive a principled structured pruning algorithm starting from the general formulation in SS 3 which can automatically determine which structures to prune. BMRS accomplishes this by following the multiplicative noise setup in  with BMR used on the noise terms. Figure 1 illustrates the general approach, where \( F\) is calculated for a model trained with multiplicative noise under a reduced prior, and elements of the model are removed if the new VFE is greater. We next describe the multiplicative noise layer trained using Equation 2, and then derive two variants of BMRS from Equation 6 using different reduced priors.

### Multiplicative noise layer

The concept of multiplicative noise inducing sparsity in neural networks was first introduced with variational dropout, where  show that dropout has a Bayesian interpretation as multiplicative noise with a log-uniform prior. One can use this interpretation of dropout in order to explicitly learn dropout parameters, \(_{i}\), as in SS3, by selecting appropriate prior and variational distributions and optimizing Equation 2 directly.  propose to do so by using the truncated log-uniform distribution as a prior and the truncated log-normal distribution as the variational distribution. As such, the variational approximation can be performed using

\[_{i}=_{i}(_{i}_{i-1}); q_{} (_{i})=_{[a,b]}(_{i}|_{i},_{i}^{2}); p( _{i})=_{[a,b]}(_{i})\] (7)

with bounded support between \(a\) and \(b\) and \(0<a<b 1\). We refer to  for details on how to learn \(q_{}\), which is obtained by optimizing Equation 3.

The log-uniform distribution serves as a sparsity inducing prior as most of its density is concentrated around 0 (see panel 2 in Figure 1). Additionally, it acts as a regularizer on the floating point precision of the multiplicative noise terms . In  this is used to perform structured pruning by removing all structures \(_{i}\) where the signal-to-noise ratio of the noise term \(_{i}\) falls below a pre-defined threshold. We next show how to derive principled pruning algorithms based on BMR which induce sparsity while maintaining accuracy without the need for tuning pruning thresholds.

### Deriving BMRS

Our goal is to use BMR in order to perform structured pruning of models trained with multiplicative noise. To do so, we must select a new prior \(()\) from which we can: 1) induce sparsity; 2) efficiently calculate \( F\) and; 3) prune the network while maintaining good performance.

Selecting the reduced prior, \(()\), is straightforward when the prior and approximate posterior are the same type of distributions. For example, in a fully BNN where one assumes a prior distribution of \((|,)\) over all the model weights with a mean-field variational approximation resulting in a factorisation over individual weights, \((|,^{2})\), the three criteria above can be met when one selects a Gaussian reduced prior with slight variance around 0 i.e. \((|0,), 0\).2 However, in the case of multiplicative noise, our prior and variational distributions are of different types and thus the selection of the reduced prior is not immediately obvious. Here, we derive and compare the characteristics of two different reduced priors: one based on a truncated log-normal distribution, which we can use to approximate a Dirac delta at 0 (BMRS\({}_{}\)), and one based on a truncated log-uniform distribution with reduced support (BMRS\({}_{}\)).

#### 4.2.1 BMRS with log-normal reduced prior (BMRS\({}_{}\))

First, we derive \( F\) when using the log-uniform distribution as the original prior, \(p()=_{[a,b]}()\), and the truncated log-normal distribution as the reduced prior, \(()=_{[a,b]}(|_{p},_{p }^{2})\). We select a truncated log-normal distribution, as it matches the variational distribution \(q_{}()\), and the log-uniform prior, because it is a special case of the log-normal distribution when the variance goes to infinity. Because of this, we expect that \( F\) will have a closed form solution, and that the computation will be efficient. We briefly present the results of the derivation here; for the full derivation see SS A.1.

We can use the specific forms of \(p()\) and \(()\) for the truncated log-uniform and truncated log-normal distributions, respectively, in Equation 6 to determine \( F\):

\[ F_{}[()}{p( )}]=}( b- a)}{Z_{}Z_{q}}+ _{q}^{2}}{2_{p}^{2} _{q}^{2}}-(^{2}}{_{q}^{2}}+_{p}^{2}}{_{p}^{2}}-_{q}^{2}}{ _{q}^{2}})\] (8)

with \(_{q}^{2}=(^{2}}+_ {p}^{2}})^{-1}\) and \(_{q}=_{q}^{2}(}{_{q}^{2}}+ _{p}}{_{p}^{2}})\). Here, \(Z_{p}=(_{p})-(_{p})\); \((t)=[1+(})]\) is the CDF of the standard Normal distribution, \(t(0,1)\); \(_{p}=(a-_{p})/_{p}\); and \(_{p}=(b-_{p})/_{p}\).

As can be seen from Equation 8, the calculation for \( F\) can be performed directly using only the statistics of the priors and variational distribution. In order for Equation 8 to induce sparsity, we must select a \(_{p}\) and \(_{p}^{2}\) that effectively collapse \(\) to 0. To achieve this, we can approximate a Dirac delta at 0 by selecting \(_{p}\) to be close to 0 (e.g., the lower bound of truncation \(a\)), and \(_{p}^{2}\) to be sufficiently small. We will demonstrate in SS5 that this reduced prior results in high sparsity while maintaining performance without any need for tuning pruning thresholds.

#### 4.2.2 BMRS with log-uniform reduced prior (BMRS\({}_{}\))

Next, we derive the change in VFE, \( F\), when using a truncated log-uniform distribution as the original prior, \(p()=_{[a,b]}()\), and a truncated log-uniform distribution with reduced support as the reduced prior, \(()=_{[a^{},b^{}]}()\). We select a reduced truncated log-uniform distribution for the same reasons as the truncated log-normal: we expect that \( F\) will have an efficiently calculable closed form, given that the priors are of the same type and are a special case of the variational distribution. The PDF of the reduced truncated log-uniform distribution is given as follows:

\[()=_{[a^{},b^{}]}()= (}{a^{}})^{-1},&a<a^{}  b^{}<b\\ 0,&\] (9)

Using this, we can directly solve the integral under the expectation given in Equation 6 for \( F\) (full details in SS A.2):

\[ F_{}[()}{p( )}]=_{a}^{b}_{[a^{},b^{}]}() {q_{}()}{_{[a,b]}()}d=} {}{a^{}}}q_{}(a^{}_{i} b^{ })\] (10)

where \(q_{}(a^{}_{i} b^{})\) is the CDF of the variational distribution evaluated between \(a^{}\) and \(b^{}\). We know from Equation 5 that the VFE under \(()\) is greater when \( F 1\). Plugging this in:

\[1}}{}{a^{}}}q_{ }(a^{}_{i} b^{}) }{a^{}}}{} q_{}(a^{} b^{ })\] (11)

Here, the left hand side of the inequality is the CDF of the truncated log-uniform distribution between \(a^{}\) and \(b^{}\). In other words, when the new prior is a log-uniform distribution with reduced support,the BMR pruning criterion amounts to a comparison between the CDF of the original prior and the variational distribution along the interval \([a^{},b^{}]\). Additionally, Equation 11 shows that this is generalizable to any variational distribution with support broader than the reduced prior.

``` input :dataset \(\); neural network with deterministic weights \(\) and variational parameters \(\);  original prior p(\(\)); reduced prior \(()\); number of training epochs \(e_{T}\); number of fine-tuning epochs \(e_{F}\); number of pruning epochs \(P\) \(j 0\) while\(j<e_{T}\) and \(,\) not converged do  Train \(\) and \(\) on \(\) using Equation 3 if\(j P=0\)then

``` for\(\)do  df \( F(p()_{i},(_{i}),q_{}(_{i}))\) if\( 0\)then

\(_{i}\) \(_{i}\) \(j j+1\) Fine tune \(\) and \(\) on \(\) for \(e_{F}\) epochs ```

**Algorithm 1**Training and pruning with BMRS

**BMRS\({}_{}\) pruning and connection to floating point precision.** To see how BMRS\({}_{}\) can be used for pruning, we first briefly summarize the relationship between the log-uniform distribution and floating point numbers. Floating point numbers are commonly encoded in binary as a combination of a sign bit \(s\), a set of exponent bits \(e\), and a set of mantissa bits \(m\) denoting the fractional part of a real-number: \(r=s(m/2^{p-1}) 2^{e}\), where \(p\) determines the precision of the encoding. As discussed in , the mantissaer of "naturally observed" floating point numbers (e.g., natural constants) tend to follow a log-uniform distribution and repeated multiplications/divisions on a digital computer transform a broad class of distributions towards a log-uniform distribution

\[m(m B)^{-1}, 1/B m 1,\] (12)

where \(B\) is the base of the number system. In the case where the mantissa uses \(p\) bits, there are \(2^{p}\) representable fractional numbers so \(B=2^{p}\). As such, \(p\) determines the smallest fractional value which can be represented. We can use this to have the reduced prior cover a finite range of high precision values which are acceptable to prune. To accomplish this, we use a reduced log-uniform prior of the following form by selecting two integers \(p_{1},p_{2}\) where \(0 p_{1}<p_{2}\):

\[()=(}}{2^{p_{1}}} )^{-1},&1/2^{p_{2}} 1/2^{p_{1}}\\ 0,&\] (13)

Thus we can reduce the prior to a range of precision between \(p_{1}\) and \(p_{2}\) by selecting \(a^{}=1/2^{p_{2}}\) and \(b^{}=1/2^{p_{1}}\). Equation 11 then has a natural interpretation as comparing the probability of drawing a random mantissa from the interval \([1/2^{p_{2}},1/2^{p_{1}}]\) to the probability of drawing a value within that interval from the variational distribution. If we select \(p_{2}\) to be the limit of the precision of values in the number system used (\(p_{2}=23\) for single-point precision), then we can interpret \(p_{1}\) as determining the prunable range of precision of all variables \(\). The accuracy-complexity tradeoff inherent in the selection of \(()\) is then controlled through \(p_{1}\), the desired cutoff of the precision of the network.

### Training and pruning

The details on how to train a network and use BMRS for pruning are given in Algorithm 1. We train a model for a fixed number of epochs (or until convergence) and perform pruning every \(P\) epochs. This lends itself to either post-training pruning, where the network is fully trained followed by pruning and fine-tuning, or continuous pruning, where pruning is performed during model training. In our experiments, we explore both of these setups and contrast BMRS with alternative pruning methods.

Experiments

We demonstrate the pruning behavior of BMRS through several experiments with neural networks of varying complexity measured as the number of trainable parameters. We use the following datasets (full details in Appendix B): MNIST , Fashion-MNIST , CIFAR10 , and TinyImagenet. For MNIST, Fashion-MNIST, and CIFAR10 we experiment with both a multi-layer perceptron (MLP) and a small CNN (Lenet5 ). Pruning layers are applied after each fully connected layer for the MLP, and for each convolutional filter and fully connected layer for Lenet5. For CIFAR10 and TinyImagenet, we further experiment with a pretrained ResNet-50  and a pretrained vision transformer (ViT) . For ResNet-50, we apply pruning layers after each layer of batch normalization, and for ViT we apply pruning layers to the output of each transformer block. For the multiplicative noise layers, we set the left bound of truncation to be \( a=-20\) and the right bound of truncation to be \( b=0\). Hyperparameters for the MLPs and Lenet5 are tuned on a model with no pruning performed and kept the same for each variant (see Appendix C). We perform experiments using the following model variants and baselines which cover both Bayesian pruning criteria (e.g. ours, SNR, and \(_{q_{}}[]\)) as well as magnitude pruning (L2):

* **None:** A baseline with no compression and no multiplicative noise.
* **L2:** A magnitude pruning baseline based on the L2 Norm of weight vectors (matrices in the case of convolutional filters) at the input of each neuron to be pruned . We set the pruning threshold to the compression rate achieved by BMRS\({}_{}\) using the same settings of a given experiment.
* \(_{q_{}}[]\): The expected value of noise variables \(\). For continuous pruning, we use a set threshold of 0.1.
* **SNR:** The signal-to-noise ratio \(_{q_{}}[]/[]}\) as used in . For continuous pruning, we use a set threshold of 1 as recommended in .
* **BMRS\({}_{}\):** BMRS using the log-normal prior from Equation 6. In order to reduce the prior to 0, we set \(_{p}\) to the left bound of truncation (\(a\)), and \(_{p}^{2}\) to \(10^{-12}\).
* **BMRS\({}_{}\)-\(p_{1}\):** BMRS using the log-uniform prior from Equation 11. In our experiments, we set \(a^{}\) to be the limit of the precision of single-point floats (\(p_{2}\) = 23 so \(a^{}=1/2^{23}\)) and \(b^{}\) to either \(p_{1}\) = 8-bit precision (\(b^{}=1/2^{8}\)) or \(p_{1}\) = 4-bit (\(b^{}=1/2^{4}\)).

Post-training pruning.We first look at the behavior of BMRS when used in the post-training setting. To do so, we first train a model on a given dataset, then use each method to rank the neurons based on their pruning function (L2 norm, signal-to-noise ratio, or \( F\)). To observe the accuracy at different compression rates, neurons are progressively removed based on their rank, and the model is fine-tuned for one epoch before measuring the test accuracy. For BMRS methods, we additionally stop pruning once \( F<0\) for a given structure. The plots of accuracy vs. compression for 10 different random seeds are given in Figure 2 (Further experiments given in Appendix E).

First, we find that BMRS generally stops compressing near the knee point of the trade-off curve - a preferred solution of a Pareto front if there is no a priori preferences - in all settings except for BMRS\({}_{}\)-4 which only does so in 4 out of 6 settings. Notably, BMRS\({}_{}\) accomplishes this with no need to tune additional thresholds as is common in pruning literature. To further visualize this, the right plot in each subfigure shows a scatter plot of the accuracy at the maximum compression rate (pruning all neurons where \( F 0\)) along with the curve of accuracy vs. compression for SNR pruning near the knee point. We can see that the density of points for BMRS is concentrated near the optimal point in all cases except for the MLP on MNIST, indicating the robustness of the proposed methods.

We additionally observe much similarity in the curves for BMRS and SNR pruning, suggesting that they may be performing similar functions. To further investigate this, we look at the Spearman rank correlation coefficient  of the neurons based on their respective functions in Figure 3 (plots for additional datasets in Appendix E). We see that BMRS\({}_{}\) tends to have a high correlation with SNR, suggesting that it learns a qualitatively similar function with the benefit of providing a threshold for compression. BMRS\({}_{}\), on the other hand, tends to have very low or even negative correlation. This, combined with the more rapidly declining accuracy for a given level of compression, suggests that BMRS\({}_{}\) is only apt for determining a single split into elements to keep and to remove, but does not provide an accurate ranking of the elements..

Continuous pruning.Next, we experiment with continuous pruning, where neurons are pruned continuously throughout training based on either a provided pruning threshold (SNR, \(_{q_{}}[]\)) or \( F\) (BMRS). For SNR, we prune a neuron when its SNR falls below 1 (as in ), and for \(_{q_{}}[]\) we set a threshold of 0.1. For L2 pruning, we perform post-training pruning based on the compression rate achieved by BMRS\({}_{}\). Neurons are pruned after every epoch during training, followed by 10 epochs of fine-tuning at the very end of training.

We compare the raw performance of each variant using an MLP and Lenet5 on MNIST, Fashion-MNIST, and CIFAR10 in Table 1. First, we note that using the L2 norm with the same compression rate as BMRS\({}_{}\) results in a degenerate model; the accuracy degrades to random in all settings. Additionally, we see that using the SNR as a pruning criterion with the recommended threshold of 1 from  is also inconsistent, resulting in large drops in performance for 3 out of 6 settings. BMRS\({}_{}\) and BMRS\({}_{}\) result in both high compression rate and high performance in all settings. BMRS\({}_{}\) accomplishes this without the need for tuning any pruning thresholds, in one case yielding a higher compression rate than BMRS\({}_{}\)-4 while keeping the accuracy high. BMRS\({}_{}\)-4 results in both the highest compression rate among the three BMRS variants in 4 out of 6 settings, and the highest accuracy in 5 out of 6 settings, with the caveat of needing to select \(p_{1}\) as a hyperparameter. To explore the effect of this hyperparameter further, we plot accuracy and compression vs. \(p_{1}\) for Lenet5 trained on CIFAR10 in Figure 4. We see that compression rapidly increases after \(p_{1}=11\), continuing until \(p_{1}=1\). Additionally, we find that accuracy also steadily increases with a higher compression rate, indicating that BMRS\({}_{}\) reduces complexity while increasing the generalization capacity of the model.

Finally, a comparison of ResNet-50 and ViT on CIFAR10 and TinyImagenet is given in Table 2. Here, SNR and the three BMRS variants achieve similar accuracies at different compression rates. BMRS\({}_{}\) achieves a modest compression rate compared to SNR with a threshold of 1 for each case. BMRS\({}_{}\)-4 yields a higher compression rate than BMRS\({}_{}\) and BMRS\({}_{}\)-8 in all settings. As such, we show that BMRS\({}_{}\) is capable of achieving high compression with no threshold tuning, while a more extreme compression rate is possible by selecting \(p_{1}\) for BMRS\({}_{}\).

## 6 Discussion and conclusion

Our experimental results demonstrate the pruning characteristics of BMRS in two settings: continuous pruning and post-training pruning. One of the benefits of BMRS, and BMRS\({}_{}\) in particular, is that no threshold tuning is needed, making it particularly useful for the continuous pruning setting. Here, the compression rate improves as the model converges, allowing one to gradually increase the compression rate without sacrificing accuracy. The choice of BMRS\({}_{}\) vs. BMRS\({}_{}\) is then dependent on the problem, where more complex scenarios (e.g., over-parameterized models) are suitable for BMRS\({}_{}\) if a higher compression rate is desired. In this case, a hyperparameter search over \(p_{1}\) in the range \([,23]\) can be done to select BMRS\({}_{}\) (see Figure 4 for an example), or one can simply use BMRS\({}_{}\) to achieve good compression with no hyperparameter tuning. Additionally, we expect that the more over-specified the model is, the lower we can set \(p_{1}\) and maintain accuracy (i.e., lower bit-rate).

    &  &  &  \\   & **Comp. (\%)** & **Acc.** & **Comp. (\%)** & **Acc.** & **Comp. (\%)** & **Acc.** \\   &  \\  None & \(0.00 0.00\) & \(97.43 0.14\) & \(0.00 0.00\) & \(88.17 0.20\) & \(0.00 0.00\) & \(44.94 0.40\) \\ L2 & \(43.11 2.06\) & \(10.39 0.32\) & \(87.86 2.27\) & \(12.83 0.12\) & \(42.89 2.64\) & \(10.00 0.00\) \\ \(E[]\) & \(52.08 1.71\) & \(96.88 0.15\) & \(91.76 0.81\) & \(85.59 0.26\) & \(77.99 1.54\) & \(43.39 0.46\) \\ SNR & \(58.57 2.01\) & \(96.92 0.08\) & \(98.93 0.00\) & \(10.00 0.00\) & \(75.93 1.26\) & \(43.97 0.46\) \\ BMRS\({}_{}\) & \(48.86 1.32\) & \(96.95 0.19\) & \(93.20 0.66\) & \(84.99 0.35\) & \(76.36 1.08\) & \(43.59 0.29\) \\ BMRS\({}_{}\)-8 & \(48.73 1.90\) & \(96.93 0.16\) & \(93.02 0.81\) & \(85.01 0.32\) & \(77.17 0.98\) & \(43.45 0.42\) \\ BMRS\({}_{}\)-4 & \(54.47 1.74\) & \(\) & \(91.57 0.71\) & \(\) & \(76.63 0.94\) & \(\) \\   &  \\  None & \(0.00 0.00\) & \(99.07 0.09\) & \(0.00 0.00\) & \(89.16 0.27\) & \(0.00 0.00\) & \(67.62 0.77\) \\ L2 & \(83.42 1.92\) & \(11.35 0.00\) & \(83.62 1.69\) & \(10.00 0.00\) & \(52.29 2.18\) & \(10.00 0.00\) \\ \(E[]\) & \(88.29 1.00\) & \(51.30 41.12\) & \(89.71 0.56\) & \(50.93 33.45\) & \(66.19 1.36\) & \(65.83 0.90\) \\ SNR & \(92.66 5.77\) & \(62.70 41.93\) & \(98.47 3.45\) & \(17.01 21.03\) & \(70.29 2.02\) & \(\) \\ BMRS\({}_{}\) & \(86.90 1.15\) & \(95.59 0.94\) & \(88.02 1.00\) & \(77.90 2.44\) & \(62.87 1.64\) & \(66.14 0.70\) \\ BMRS\({}_{}\)-8 & \(86.11 1.37\) & \(95.27 1.02\) & \(87.61 0.72\) & \(77.23 3.49\) & \(62.54 1.49\) & \(66.28 1.07\) \\ BMRS\({}_{}\)-4 & \(87.58 1.01\) & \(\) & \(88.72 0.73\) & \(\) & \(68.07 1.95\) & \(67.66 0.59\) \\   

Table 1: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over ten runs are included. Best accuracy for the compression methods is given in bold.

**Limitations:** We note a few of the limitations of BMRS. First, while multiplicative noise pruning allows for the flexible application of pruning at different structural levels, BNNs may offer more aggressive compression rates as they apply sparsity inducing priors at multiple hierarchical levels . BMR based approaches may be derived for such networks; as of this work and as far we we know, it has only been successfully applied in practice to models with flat priors for unstructured pruning . Additionally, multiplicative noise creates an overhead of additional parameters \(\) which increase the training time and storage requirements. Third, more exhaustive baseline pruning criteria could be used for comparison in the future, for example classic methods based on the Hessian or gradient of weights (e.g. see Figure 5 in Appendix E) . Fourth, we apply multiplicative noise to linear layers and convolutional filters, while it could be useful to explore pruning more complex structures in the future. Finally, while structured pruning can reduce the inference time and energy consumption of neural networks, improvements in efficiency have been shown to have potential negative consequences in terms of energy consumption and carbon emissions based on how efficiency can affect how a model is used in practice .

**Conclusions:** In this work we presented BMRS, an efficiently calculable method for threshold-free structured pruning of neural networks. We derived two versions of BMRS: BMRS\({}_{}\) based on the truncated log-normal prior, and BMRS\({}_{}\) based on a reduced truncated log-uniform prior. BMRS offers several key features over existing work: by basing the method off of the approach of multiplicative noise , the structured pruning aspect is flexible as it is not dependent on assuming any prior over individual weights and can be easily applied at any structural level . Additionally, the prior and variational posterior in the multiplicative noise approach lend themselves to the derivation of BMRS using multiple reduced priors which have different pruning properties, allowing for flexibility in the compression rate when desired and threshold free pruning otherwise. Finally, our experimental results demonstrate the competitive compression and accuracy of BMRS compared to baseline compression methods on multiple networks of varying complexity and across multiple datasets. The methods presented here based on BMR could form a template for developing more aggressive pruning schemes by incorporating more complex hierarchical priors on both structures and individual weights, as well as for studying limits on the number of neural network structures required to solve a given dataset.

**Pruning Method** & **Comp. \%** & **Acc.** & **Comp. \%** & **Acc.** \\   &  \\  None & \(0.00 0.00\) & \(90.65 0.05\) & \(0.00 0.00\) & \(53.01 0.35\) \\ L2 & \(63.79 4.21\) & \(10.00 0.00\) & \(55.71 1.08\) & \(0.50 0.00\) \\ \(E[]\) & \(89.85 0.09\) & \(16.27 1.86\) & \(83.59 1.50\) & \(21.11 6.47\) \\ SNR & \(91.73 0.16\) & \(89.24 0.40\) & \(77.85 0.14\) & \(50.54 0.59\) \\ BMRS\({}_{}\) & \(87.10 1.55\) & \(\) & \(74.98 0.16\) & \(50.56 0.33\) \\ BMRS\({}_{}\)-8 & \(88.18 0.22\) & \(89.29 0.20\) & \(74.99 0.04\) & \(\) \\ BMRS\({}_{}\)-4 & \(89.85 0.05\) & \(89.26 0.28\) & \(76.12 0.13\) & \(50.82 0.50\) \\   \\  None & \(0.00 0.00\) & \(94.80 0.17\) & \(0.00 0.00\) & \(63.14 0.42\) \\ L2 & \(57.74 0.26\) & \(54.36 1.84\) & \(47.47 0.12\) & \(7.08 0.57\) \\ \(E[]\) & \(68.18 0.09\) & \(10.00 0.00\) & \(57.08 0.18\) & \(0.50 0.00\) \\ SNR & \(73.03 0.03\) & \(94.78 0.10\) & \(62.75 0.04\) & \(64.60 0.07\) \\ BMRS\({}_{}\) & \(57.74 0.25\) & \(94.60 0.01\) & \(47.48 0.12\) & \(65.00 0.21\) \\ BMRS\({}_{}\)-8 & \(58.14 0.11\) & \(94.69 0.04\) & \(47.34 0.14\) & \(\) \\ BMRS\({}_{}\)-4 & \(67.16 0.21\) & \(\) & \(56.13 0.14\) & \(65.13 0.10\) \\   

Table 2: Parameter compression % and accuracy for different baseline methods and settings of the proposed method. Standard deviations over three runs are included. Best accuracy for the compression methods is given in bold.