# Counting Distinct Elements Under Person-Level Differential Privacy

Alexander Knop

Google

alexanderknop@google.com

&Thomas Steinke

Google DeepMind

steinke@google.com

Alphabetical author order.

###### Abstract

We study the problem of counting the number of distinct elements in a dataset subject to the constraint of differential privacy. We consider the challenging setting of person-level DP (a.k.a. user-level DP) where each person may contribute an unbounded number of items and hence the sensitivity is unbounded.

Our approach is to compute a bounded-sensitivity version of this query, which reduces to solving a max-flow problem. The sensitivity bound is optimized to balance the noise we must add to privatize the answer against the error of the approximation of the bounded-sensitivity query to the true number of unique elements.

## 1 Introduction

An elementary data analysis task is to count the number of distinct elements occurring in a dataset. The dataset may contain private data and even simple statistics can be combined to leak sensitive information about people . Our goal is to release (an approximation to) this count in a way that ensures the privacy of the people who contributed their data. As a motivating example, consider a collection of internet browsing histories, in which case the goal is to compute the total number of websites that have been visited by at least one person.

Differential privacy (DP)  is a formal privacy standard. The simplest method for ensuring DP is to add noise (from either a Laplace or Gaussian distribution) to the true answer, where the scale of the noise corresponds to the sensitivity of the true answer - i.e., how much one person's data can change the true value.

If each person contributes a single element to the dataset, then the sensitivity of the number of unique elements is one. However, a person may contribute multiple elements to the dataset and our goal is to ensure privacy for all of these contributions simultaneously. That is, we seek to provide person-level DP (a.k.a. user-level DP2).

This is the problem we study: We have a dataset \(D=(u_{1},u_{2},,u_{n})\) of person records. Each person \(i[n]\) contributes a finite dataset \(u_{i}^{*}\), where \(\) is some (possibly infinite) universe of potential elements (e.g., all finite-length binary strings) and \(^{*}:=_{}^{}\) denotes all subsets of \(\) of finite size. Informally, our goal is to compute the number of unique elements

\[(D):=|_{i[n]}u_{i}|\] (1)in a way that preserves differential privacy. A priori, the sensitivity of this quantity is infinite, as a single person can contribute an unbounded number of unique elements.

In particular, it is not possible to output a meaningful upper bound on the number of distinct elements subject to differential privacy. This is because a single person could increase the number of distinct elements arbitrarily and differential privacy requires us to hide this contribution. It follows that we cannot output a differentially private unbiased estimate of the number of distinct elements with finite variance. However, it is possible to output a lower bound. Thus our formal goal is to compute a high-confidence lower bound on the number of distinct elements that is as large as possible and which is computed in a differentially private manner.

### Our Contributions

Given a dataset \(D=(u_{1},,u_{n})(^{*})^{n}\) and an integer \( 1\), we define

\[(D;):=\{|_{i[n]}v_{i}|: i [n]\;v_{i} u_{i}|v_{i}|\}.\] (2)

That is, \((D;)\) is the number of distinct elements if we restrict each person's contribution to \(\) elements. We take the maximum over all possible restrictions.

It is immediate that \((D;)(D)\) for all \( 1\). Thus we obtain a lower bound on the true number of unique elements. The advantage of \((D;)\) is that its sensitivity is bounded by \(\) (see Lemma A.1 for a precise statement) and, hence, we can estimate it in a differentially private manner. Specifically,

\[_{,}(D):=(D;)+( /)\] (3)

defines an \(\)-DP algorithm \(M_{,}:(^{*})^{n}\), where \((b)\) denotes Laplace noise scaled to have mean \(0\) and variance \(2b^{2}\). This forms the basis of our algorithm. Two challenges remain: Setting the sensitivity parameter \(\) and computing \((D;)\) efficiently.

To obtain a high-confidence lower bound on the true distinct count, we must compensate for the Laplace noise, which may inflate the reported value. We can obtain such a lower bound from \(_{}(D)\) using the cumulative distribution function (CDF) of the Laplace distribution: That is, \( b>0\)\((0,1/2]\)\([(b) b( )]=\), so

\[[_{,}(D)-()}_{} (D)]_{}.\] (4)

Choosing the sensitivity parameter \(\).Any choice of \( 1\) gives us a lower bound: \((D;)(D)\). Since \( D\ _{}(D;)=(D)\), this lower bound can be arbitrarily tight. However, the larger \(\) is, the larger the sensitivity of \((D;)\) is. That is, the noise we add scales linearly with \(\).

Thus there is a bias-variance tradeoff in the choice of \(\). To make this precise, suppose we want a lower bound on \((D)\) with confidence \(1-[,1)\), as in Equation (4). To obtain the tightest possible lower bound with confidence \(1-\), we want \(\) to maximize the expectation

\[q(D;):=(D;)-()=*{}_{_{,}} _{,}(D)- ().\] (5)

We can use the exponential mechanism  to privately select \(\) that approximately maximizes \(q(D;)\). However, directly applying the exponential mechanism is problematic because each score has a different sensitivity - the sensitivity of \(q(;)\) is \(\). Instead, we apply the Generalized Exponential Mechanism (GEM) of Raskhodnikova and Smith  (see Algorithm 3). Note that we assume some a priori maximum value of \(\) is supplied to the algorithm; this is \(_{}\).

Our main algorithm attains the following guarantees.

**Theorem 1.1** (Theoretical Guarantees of Our Algorithm).: _Let \(>0\) and \((0,)\) and \(_{}\). Define \(:(^{*})^{*}\) to be \((D)=(D;_{},,)\) from Algorithm 1. Then \(\) satisfies all of the following properties._

* _Privacy:_\(\) _is_ \(\)_-differentially private._
* _Lower bound:_ _For all_ \(D(^{*})^{n}\)_,_ \[,)(D)}{}[(D)] 1-.\] (6)
* _Upper bound:_ _For all_ \(D(^{*})^{n}\)_,_ \[,)(D)}{}[ _{[_{}]}(D;)-^{*}}{}(}{})]  1-2,\] (7) _where_ \(_{A}^{*}=_{[_{}]}(D;)-()\)_._
* _Computational efficiency:_\((D)\) _has running time_ \(O(|D|^{1.5}_{}^{2})\)_, where_ \(|D|:=_{i}|u_{i}|\)_._

The upper bound guarantee (7) is somewhat difficult to interpret. However, if the number of items per person is bounded by \(_{*}\), then we can offer a clean guarantee: If \(D=(u_{1},,u_{n})(^{*})^{n}\) satisfies \(_{i[n]}|u_{i}|_{*}_{}\), then combining the upper and lower bounds of Theorem 1.1 gives

\[,)(D)}{}[ (D)(D)-}{} (}{})] 1-3.\] (8)

Note that \(_{*}\) is not assumed to be known to the algorithm, but the accuracy guarantee is able to adapt. We only assume \(_{*}_{}\), where \(_{}\) is the maximal sensitivity considered by the algorithm.

In addition to proving the above theoretical guarantees, we perform an experimental evaluation of our algorithm.

```
1:procedureSensitiveDistinctCount(\(D\!=\!(u_{1},,u_{n})\!\!(^{*})^{n}\); \(\!\!\)) \((D;)\)
2: Let \(U_{}=_{i[n]}(\{i\}[\{,|u_{i}|\}])[ n][]\).
3: Let \(V=_{i[n]}u_{i}\).
4: Define \(E_{} U V\) by \(((i,j),v) E v u_{i}\).
5: Let \(G_{}\) be a bipartite graph with vertices partitioned into \(U_{}\) and \(V\) and edges \(E_{}\).
6:\(m_{}(G)\). \(\)
7:return\(m_{}\)
8:endprocedure
9:procedureDPDistinctCount(\(D\!=\!(u_{1},,u_{n})\!\!(^{*})^{n}\); \(_{}\!\!\), \(\!>\!0\), \(\!\!(0,)\))
10:for\([_{}]\)do
11: Define \(q_{}(D):=(D;)-()\).
12:endfor
13:\((D;\{q_{}\}_{[_{}]},\{ \}_{[_{}]},/2,)\). \(\) Algorithm 3
14:\( q_{}(D)+(2/)\).
15:return\((,)[_{}]\).
16:endprocedure ```

**Algorithm 1** Distinct Count Algorithm

Efficient computation.The main computational task for our algorithm is to compute \((D;)\). By definition (2), this is an optimization problem. For each person \(i[n]\), we must select a subset \(v_{i}\) of that person's data \(u_{i}\) of size at most \(\) so as to maximize the size of the union of the subsets \(|_{i[n]}v_{i}|\).

We can view the dataset \(D=(u_{1},,u_{n})(^{*})^{n}\) as a bipartite graph. On one side we have the \(n\) people and on the other side we have the elements of the data universe \(\).3 There is an edge between \(i[n]\) and \(x\) if and only if \(x u_{i}\).

We can reduce computing \((D;)\) to bipartite maximum matching. For \(=1\), \((D;1)\) is exactly the maximum cardinality of a matching in the bipartite graph described above. For \( 2\), we simply create \(\) copies of each person vertex \(i[n]\) and then \((D;)\) is the maximum cardinality of a matching in this new bipartite graph.4

Using this reduction, standard algorithms for bipartite maximum matching  allow us to compute \((D;)\) with \(O(|D|^{1.5})\) operations. We must repeat this computation for each \([_{}]\).

```
1:procedureDPApproxDistinctCount(\(D\)=\((u_{1},,u_{n})\)\((^{*})^{n}\); \(_{}\)\(\)\(\), \(\)\(>\) 0, \(\)\(\)\((0,)\))
2:\(S\).
3:for\([_{}]\)do
4:for\(i[n]\) with \(u_{i} S\)do
5: Choose lexicographically first \(v u_{i} S\). \(\) Match \((i,)\) to \(v\).
6: Update \(S S\{v\}\).
7:endfor
8: Define \(q_{}(D):=|S|-()\). \(\) This loop computes \(\{q_{}(D)\}_{[_{}]}\).
9:endfor
10:\(\) GEM\((D;\{q_{}\}_{[_{}]},\{\}_{[_{}]},/2,)\). \(\) Algorithm 3
11:\( q_{}(D)+(2/)\).
12:return\((,)[_{}]\).
13:endprocedure ```

**Algorithm 2** Linear-Time Approximate Distinct Count Algorithm

Linear-time algorithm.Our algorithm above is polynomial-time. However, for many applications the dataset size \(|D|\) is enormous. Thus we also propose a linear-time variant of our algorithm. However, we must trade accuracy for efficiency.

There are two key ideas that differentiate our linear-time algorithm (Algorithm 2) from our first algorithm (Algorithm 1) above: First, we compute a maximal bipartite matching instead of a maximum bipartite matching.5 This can be done using a linear-time greedy algorithm and gives a 2-approximation to the maximum matching. (Experimentally we find that the approximation is better than a factor of 2.) Second, rather than repeating the computation from scratch for each \([_{}]\), we incrementally update our a maximal matching while increasing \(\). The main challenge here is ensuring that the approximation to \((D;)\) has low sensitivity - i.e., we must ensure that our approximation algorithm doesn't inflate the sensitivity. Note that \((D;)\) having low sensitivity does not automatically ensure that the approximation to it has low sensitivity.

**Theorem 1.2** (Theoretical Guarantees of Our Linear-Time Algorithm).: _Let \( 0\) and \((0,)\) and \(_{}\). Define \(:(^{*})^{*}\) to be \(}(D)=\)DPApproxDistinctCount\((D;_{},,)\) from Algorithm 2. Then \(}\) satisfies all of the following properties._

* _Privacy:_\(}\) _is_ \(\)_-differentially private._
* _Lower bound:_ _For all_ \(D(^{*})^{n}\)_,_ \[*{}_{(,)}(D)}[(D)] 1-.\] (9)
* _Upper bound:_ _If_ \(D=(u_{1},,u_{n})(^{*})^{n}\) _satisfies_ \(_{i[n]}|u_{i}|_{*}_{}\)_, then_ \[*{}_{(,)}(D)}[(D)-}{ }(}{})] 1-2.\] (10)
* _Computational efficiency:_\((D)\) _has running time_ \(O(|D|+_{}_{})\)_, where_ \(|D|:=_{i}|u_{i}|\)_._

The factor \(\) in the upper bound guarantee (10) is the main loss compared to Theorem 1.1. (The win is \(O(|D|)\) runtime.) This is a worst-case bound and our experimental result show that for realistic data the performance gap is not so bad.

The proofs of Theorems 1.1 and 1.2 are in Appendix A.

## 2 Related Work

Counting the number of distinct elements in a collection is one of the most fundamental database computations. This is supported as the COUNT(DISTINCT...) operation in SQL. Hence, unsurprisingly, the problem of computing the number of unique elements in a differentially private way has been extensively investigated.

In the case where we assume each person contributes only one element (a.k.a. event-level privacy or item-level privacy), the number of distinct elements has sensitivity \(1\) and, hence, we can simply use Laplace (or Gaussian) noise addition to release. However, it may not be possible to compute the number of distinct elements exactly due to space, communication, or trust constraints (e.g. in the local model of DP ).

Most efforts have been focused on creating differentially private algorithms for counting distinct elements under space constraints (and assuming each person contributes a single element). To save space, we wish to compute a small summary of the dataset (called a sketch) that allows us to estimate the number of distinct elements and which can be updated as more elements are added. Smith, Song, and Thakurta  proved that a variant of the Flajolet-Martin sketch is private and Pagh and Stausholm  analyzed a sketch over the binary finite field. Dickens, Thaler, and Ting  proved a general privacy result for order-invariant cardinality estimators. Hehir, Ting, and Cormode  provided a mergeable private sketch (i.e. two sketches can be combined to obtain a sketch of the union of the two datasets). In contrast, Desfontaines, Lochbihler, and Basin  proved an impossibility result for mergeable sketches, which shows that privacy or accuracy must degrade as we merge sketches.

    &  &  \\   & & 10th Percentile & Median & 90th Percentile \\  Amazon Fashion & 1450 & 1220.6 & 1319.1 & 1394.2 \\ Amazon Industrial and Scientific & 36665 & 35970.5 & 36198.9 & 36326.7 \\ Reddit & 102835 & 102379.7 & 102512.6 & 102643.9 \\ IMDB & 98726 & 98555.6 & 98670.4 & 98726.8 \\   

Table 1: True and estimated (using \(\) with \(=1\), \(=0.05\) and \(_{}=100\)) counts per data set.

Counting unique elements has been considered in the pan-private streaming setting  (the aforementioned algorithms also work in the pan-private setting) and in the continual release streaming setting . (In the continual release setting the approximate count is continually updated, while in the pan-private setting the approximate count is only revealed once, but at an unknown point in time.) Kreuter, Wright, Skvortsov, Mirisola, and Wang  give private algorithms for counting distinct elements in the setting of secure multiparty computation. In the local and shuffle models, the only known results are communication complexity bounds .

A closely related problem is that of identifying as many elements as possible (rather than just counting them); this is known as "partition selection," "set union," or "key selection" . Note that, by design, DP prevents us from identifying elements that only appear once in the dataset, or only a few times. Thus we can only output items that appear frequently.

The most closely related work to ours is that of Dong, Fang, Yi, Tao, and Machanavajjhala  and Fang, Dong, and Yi . These papers present two different algorithms for privately approximating the distinct count (and other statistics). We discuss these below and present an experimental comparison in Table 2. We also remark that both papers prove instance optimality guarantees for their algorithms.

Figure 1: Performance of different algorithms estimating distinct count assuming that each person can contribute at most \(\) elements (e.g., these algorithms are estimating \((D;)\)). (These algorithms have bounded sensitivity, but we do not add noise for privacy yet.)

Figure 2: Performance of different algorithms estimating distinct count in a differentially private way for different values of \(\); for all of them \(=0.05\) and \(_{}=100\). The values between 10th and 90th percentile of each algorithms estimation are shaded into corresponding colors. For the shifted inverse algorithm, the first two plots contain the results for \(=0.05\) and \(D\) equal to the true number of distinct elements in the dataset. The later two datasets are lacking the results for shifted inverse algorithm due to the computational constraints.

Most similar to our algorithm is the Race-to-the-Top (R2T) algorithm ; R2T is a generic framework and the original paper did not specifically consider counting distinct elements, but the approach can easily be applied to \((D;)\). While we use the generalized exponential mechanism  to select the sensitivity \(\), R2T computes multiple lower bounds with different sensitivities \(\) and then outputs the maximum of the noisy values. This approach incurs the cost of composition across the multiple evaluations. To manage this cost, R2T only evaluates \(=2,4,8,,2^{_{}}\). Compared to our guarantee (8) with an error \(O(}{}(}{}))\), R2T has a slightly worse theoretical error guarantee of \(O(}{}(_{})(}{}))\)[10, Theorem 5.1].

The shifted inverse mechanism  takes a different approach to the problem. Rather than relying on adding Laplace noise (as we do), it applies the exponential mechanism with an ingenious loss function (see  for additional discussion). When applied to counting distinct elements, the shifted inverse mechanism gives an accuracy guarantee comparable to ours (8). The downside of the shifted inverse mechanism is that computing the loss function is, in general, NP-hard. Fang, Dong, and Yi  propose polynomial-time variants for several specific tasks, including counting distinct elements. However, the algorithm is still relatively slow.

## 3 Technical Background on Differential Privacy

For detailed background on differential privacy, see the survey by Vadhan  or the book by Dwork and Roth . We briefly define pure DP and some basic mechanisms and results.

```
1:procedureGEM(\(D^{*}\); \(q_{i}^{*}\) for \(i[m]\), \(_{i}>0\) for \(i[m]\), \(>0\), \(>0\))
2:Require:\(q_{i}\) has sensitivity \(_{{}_{x,x^{}}^{*}}|q(x)-q(x^{})|_{i}\) for all \(i[m]\).
3: Let \(t=()\).
4:for\(i[m]\)do
5:\(s_{i}_{j[m]}(D)-t_{i})-(q_{j}(D)-t_{ j})}{_{i}+_{j}}\).
6:endfor
7: Sample \([m]\) from the Exponential Mechanism using the normalized scores \(s_{i}\); i.e., \[ i[m][=i]= s_{i})}{_{k[m]}( s _{k})}.\]
8:return\([m]\).
9:endprocedure ```

**Algorithm 3** Generalized Exponential Mechanism 

**Definition 3.1** (Differential Privacy (Dp)  ).: _A randomized algorithm \(M:^{*}\) satisfies \(\)-DP if, for all inputs \(D,D^{}^{*}\) differing only by the addition or removal of an element and for all measurable \(S\), we have \([M(D) S] e^{}[M(D^ {}) S]\)._

We refer to pairs of inputs that differ only by the addition or removal of one person's data as _neighboring_. Note that it is common to also consider replacement of one person's data; for simplicity, we

   User &  &  \\  Attribute & PS.AQ & L.EP & O.OD & L.RD \\  R2T  & 0.0658 & 0.1759 & 0.0061 & 0.150 \\ (Approx)ShiftedInverse  & 0.0553 & 0.0584 & 0.005 & 0.0061 \\ \(\) & 0.0140 & 0.0110 & 0.0008 & 0.0037 \\ \(\) & 0.0100 & 0.0096 & 0.0008 & 0.0001 \\   

Table 2: Average relative absolute error of algorithms described in this paper and in  on the TPC-H dataset. For each algorithm we executed it 100 times, removed 20 top and 20 bottom values and computed average error for the rest of 60 values.

do not do this. We remark that there are also variants of DP such as approximate DP  and concentrated DP [16; 1], which quantitatively relax the definition, but these are not relevant in our application. A key property of DP is that it composes and is invariant under postprocessing.

**Lemma 3.2** (Composition & Postprocessing).: _Let \(M_{1}:^{*}\) be \(_{1}\)-DP. Let \(M_{2}:^{*}\) be such that, for all \(y\), the restriction \(M(,y):^{*}\) is \(_{2}\)-DP. Define \(M_{12}:^{*}\) by \(M_{12}(D)=M_{2}(D,M_{1}(D))\). Then \(M_{12}\) is \((_{1}+_{2})\)-DP._

A basic DP tool is the Laplace mechanism . Note that we could also use the _discrete_ Laplace mechanism [14; 15].

**Lemma 3.3** (Laplace Mechanism).: _Let \(q:^{*}\). We say \(q\) has sensitivity \(\) if \(|q(D)-q(D^{})|\) for all neighboring \(D,D^{}^{*}\). Define \(M:^{*}\) by \(M(D)=q(D)+*{Lap}(/)\), where \(*{Lap}(b)\) denotes laplace noise with mean \(0\) and variance \(2b^{2}\) - i.e., \((b)}{}[ >t]=(b)}{ }[<-t]=()\) for all \(t>0\). Then \(M\) is \(\)-DP._

Another fundamental tool for DP is the exponential mechanism . It selects the approximately best option from among a set of options, where each option \(i\) has a quality function \(q_{i}\) with sensitivity \(\). The following result generalizes the exponential mechanism by allowing each of the quality functions to have a different sensitivity.

**Theorem 3.4** (Generalized Exponential Mechanism [14, Theorem 1.4]).: _For each \(i[m]\), let \(q_{i}:^{*}\) be a query with sensitivity \(_{i}\). Let \(,>0\). The generalized exponential mechanism (\(*{GEM}(;\{q_{i}\}_{i[m]},\{_{i}\}_{i[m]}, ,)\) in Algorithm 3) is \(\)-DP and has the following utility guarantee. For all \(D^{*}\), we have_

\[(D;\{q_{i}\}_{i[m]},\{ _{i}\}_{i[m]},,)}{}[q_{ }(D)q_{j}(D)-_{j} ()] 1-.\]

## 4 Experimental Results

We empirically validate the performance of our algorithms using data sets of various sizes from different text domains. We focus on the problem of computing vocabulary size with person-level DP. Section 4.1 describes the data sets and Section 4.2 discusses the algorithms we compare.

### Datasets

We used four publicly available datasets to assess the accuracy of our algorithms compared to baselines. Two small datasets were used: Amazon Fashion 5-core  (reviews of fashion products on Amazon) and Amazon Industrial and Scientific 5-core  (reviews of industrial and scientific products on Amazon). Two large data sets were also used: Reddit  (a data set of posts collected from r/AskReddit) and IMDb [18; 20] (a set of movie reviews scraped from IMDb). See details of the datasets in Table 3.

    &  &  &  \\    & People & Records & Min & Median & Max \\  Amazon Fashion & 404 & 8533 & 1 & 14.0 & 139 & 1450 \\ Amazon Industrial and Scientific & 11041 & 1446031 & 0 & 86 & 2059 & 36665 \\ Reddit & 223388 & 7117494 & 0 & 18.0 & 1724 & 102835 \\ IMDB & 50000 & 6688844 & 5 & 110.0 & 925 & 98726 \\   

Table 3: Data sets details.

### Comparisons

Computing the number of distinct elements using a differentially private mechanism involves two steps: selecting a contribution bound (\(\) in our algorithms) and counting the number of distinct elements in a way that restricts each person to only contribute the given number of elements.

Selection:We examine four algorithms for determining the contribution limit:

1. Choosing the true maximum person contribution (due to computational restrictions this was only computed for Amazon Fashion data set).
2. Choosing the 90th percentile of person contributions.
3. Choosing the person contribution that exactly maximizes the utility function \(q_{}(D)=(D;)-()\), where \(=1\), and \(=0.001\).
4. Choosing the person contribution that approximately maximizes the utility function using the generalized exponential mechanism with \(=1\).

Note that only the last option is differentially private, but we consider the other comparison points nonetheless.

Counting:We also consider three algorithms for estimating the number of distinct elements for a given sensitivity bound \(\):

1. For each person, we uniformly sample \(\) elements without replacement and count the number of distinct elements in the union of the samples.
2. The linear-time greedy algorithm (Algorithm 2) with \(=1\) and \(=0.001\).
3. The matching-based algorithm (Algorithm 1) with \(=1\) and \(=0.001\).

All of these can be converted into DP algorithms by adding Laplace noise to the result.

In all our datasets "true maximum person contribution" and "90th percentile of person contributions" output bounds that are much larger than necessary to obtain true distinct count; hence, we only consider DP versions of the estimation algorithm for these selection algorithms.

### Results

Figure 1 shows the dependency of the result on the contribution bound for each of the algorithms for computing the number of distinct elements with fixed person contribution. It is clear that matching and greedy algorithms vastly outperform the sampling approach that is currently used in practice.

Tables 4 to 7 show the performance of algorithms for selecting optimal person contribution bounds on different data sets. For all bound selection algorithms and all data sets, the sampling approach to estimating the distinct count performs much worse than the greedy and matching-based approaches. The greedy approach performs worse than the matching-based approach, but the difference is about 10% for Amazon Fashion and is almost negligible for other data sets since they are much larger. As for the matching-based algorithm, it performs as follows on all the data sets:

1. The algorithm that uses the bound equal to the maximal person contribution overestimates the actual necessary bound. Therefore, we only consider the DP algorithms for counts estimation. It is easy to see that while the median of the estimation is close to the actual distinct count, the amount of noise is somewhat large.
2. The algorithm that uses the bound equal to the 99th percentile of person contributions also overestimates the necessary bound and behaves similarly to the one we just described (though the spread of the noise is a bit smaller).
3. The algorithms that optimize the utility function are considered: one non-private and one private. The non-private algorithm with non-private estimation gives the answer that is very close to the true number of distinct elements. The private algorithm with non-private estimation gives the answer that is worse, but not too much. Finally, the private algorithm with the private estimation gives answers very similar to the results of the non-private estimation.