# CrossGNN: Confronting Noisy Multivariate Time Series Via Cross Interaction Refinement

Qihe Huang\({}^{,\$}\), Lei Shen\({}^{\$}\), Ruixin Zhang\({}^{\$}\), Shouhong Ding\({}^{\$}\), Binwu Wang\({}^{}\),

Zhengyang Zhou\({}^{,,\$}\), Yang Wang\({}^{,,\$}\)

This work is done when Qihe Huang was an intern at YouTu Lab.Yang Wang and Zhengyang Zhou are corresponding authors.

\({}^{}\) University of Science and Technology of China (USTC), Hefei, China

\({}^{}\) Suzhou Institute for Advanced Research, USTC, Suzhou, China

\({}^{\$}\) Youtu Laboratory, Tencent, Shanghai, China

Email: {hqh, wbw1995}@mail.ustc.edu.cn, {zzy0929, angyan}@ustc.edu.cn,

shenlei1996@gmail.com, {ruixinzhang, ericshding}@tencent.com

###### Abstract

Recently, multivariate time series (MTS) forecasting techniques have seen rapid development and widespread applications across various fields. Transformer-based and GNN-based methods have shown promising potential due to their strong ability to model interaction of time and variables. However, by conducting a comprehensive analysis of the real-world data, we observe that the temporal fluctuations and heterogeneity between variables are not well handled by existing methods. To address the above issues, we propose CrossGNN, a linear complexity GNN model to refine the cross-scale and cross-variable interaction for MTS. To deal with the unexpected noise in time dimension, an adaptive multi-scale identifier (AMSI) is leveraged to construct multi-scale time series with reduced noise. A Cross-Scale GNN is proposed to extract the scales with clearer trend and weaker noise. Cross-Variable GNN is proposed to utilize the homogeneity and heterogeneity between different variables. By simultaneously focusing on edges with higher saliency scores and constraining those edges with lower scores, the time and space complexity (i.e., \(O(L)\)) of CrossGNN can be linear with the input sequence length \(L\). Extensive experimental results on 8 real-world MTS datasets demonstrate the effectiveness of CrossGNN compared with state-of-the-art methods. The code is available at https://github.com/hqh0728/CrossGNN.

## 1 Introduction

Time series forecasting has been widely used in many fields (i.e., climate , traffic , energy , finance , etc) . Multivariate time series (MTS) consists of time series with multiple variables and MTS forecasting aims at predicting future values based on historical time series. Deep learning models  have demonstrated superior performance in MTS forecasting. In particular, Transformer-based models  have achieved great power in MTS, benefiting from its attention mechanism which can model the long-term interaction between different time points of sequences (cross-time). Graph Neural Networks (GNNs)  have also shown promising results for MTS forecasting, which can extract pre-defined or adaptive interaction between different variables (cross-variable).

However, a recent study  shows that a simple linear model dramatically outperformed many state-of-the-art (SOTA) models, and it inspires us to investigate the reasons why the existing cross-timeand cross-variable interaction modeling fail to enhance prediction performance. By conducting a thorough analysis on real-world data, we observe that the presence of some unexpected noise (caused by humans, sensor distortion) may be responsible for it. **In time dimension**, as shown in Figure 1 (a), Transformer-based model heavily relies on the input sequences to generate the attention map, while its prediction may be susceptible to incidental noise, even some small fluctuation (i.e., noise) could easily lead to significant shifts on temporal dependencies. Our findings reveal that self-attention mechanism tend to assign high scores to outlier points in time series, resulting in spurious cross-time correlations. **In variable dimension**, cross-variable correlation exhibits a complex and dynamic evolution over time . Despite the existence of underlying causal associations between variables, extracting cross-variable interaction is difficult due to the influence of noise interference. Additionally, as shown in Figure1 (b), we observe that such unexpected noise, which can be detected by the outlier detection algorithm , accounts for a high proportion in the time series.

Despite the non-negligible noise in time series, we can still discover the potential opportunities to confront noise challenges. **(1) Cross-Scale Interaction.** As shown on Figure 1 (c), by performing multi-scale extraction on the time series, we observe that different scales possess distinct levels of noise intensity, typically with coarser scales exhibiting lower noise intensity. Evidently, capturing dependencies across different scales enables the cross-time relationships to be robust against noise . **(2) Cross-Variable Interaction.** As shown on Figure 1 (d), there is both homogeneity and heterogeneity for cross-variable interaction in real-world data . In fact, the two relationships can both contribute to invariant connections during the temporal progression. Consequently, learning the invariant associations that contain both homogeneous and heterogeneous relationships among variables can boost its robustness to confront noise. Based on above analysis, it is still challenging to refine interaction in noisy MTS. The key obstacles can be summarized as follows: 1) How to capture cross-scale interaction that is not sensitive to unexpected input noise. 2) How to extract cross-variable relations between heterogeneous variables.

In this work, we propose CrossGNN, which is the first GNN solution to refine both cross-time and cross-variable interactions for MTS forecasting. To deal with the unexpected noise in time dimension, we firstly devise an adaptive multi-scale identifier (AMSI) to construct multi-scale time series with different noise levels. In time dimension, we propose Cross-Scale GNN, which is a temporal correlation graph, to model the dependency between different scales. The scales with clearer trends and weaker noise will be assigned with more edge weight. In variable dimension, we first introduce heterogeneous interaction modeling between variables into MTS forecasting and propose cross-variable GNN to utilize the homogeneity and heterogeneity between different variables with positive and negative edge weights. By focusing on edges with higher saliency scores and constraining those edges with lower scores at the same time, CrossGNN achieves linear time and space complexity (i.e., _O(L)_) with input sequence length \(L\). The main contributions are summarized as follows:

* We conduct comprehensive studies on real-world MTS data and discover that the unexpected noise in time dimension and variable-wise heterogeneity between variables is not well handled by existing Transformer-based and GNN-based models.
* We propose a linear complexity CrossGNN model, which is the first GNN model to refine both cross-scale and cross-variable interaction for MTS forecasting.

Figure 1: Data analysis on the real-world dataset . (a) Forecasting results of Transformer-based models suffering from unexpected noise. (b) Proportion of noise in the ETH1, ETTm1, Traffic, and Weather datasets  detected by . (c) Different levels of noise signals in multi-scale time series. (d) Homogeneous and heterogeneous relationships between variables.

1) To deal with the unexpected noise in time dimension, AMSI is leveraged to construct multi-scale time series with different noise level and a Cross-Scale GNN is proposed to capture the scales with clearer trend and weaker noise.

2) Cross-Variable GNN is designed to model the dynamic correlations between different variables. This the first model to introduce heterogeneous interaction modeling between variables into MTS forecasting.

Extensive evaluation on 8 real-world MTS datasets demonstrates the effectiveness of Cross-GNN. Specifically, CrossGNN achieved top-1 performance on 47 settings and top-2 on 9 settings when compared with 9 state-of-the-art models with varying prediction lengths.

## 2 Related Work

Multivariate Time Series Forecasting.MTS forecasting has witnessed significant advancements due to the emergence of deep neural networks. These networks can be based on Convolutional Neural Network (CNN) [26; 23], Recurrent Neural Network (RNN) [5; 6], Transformer [11; 25; 27; 35; 9], or Graph Neural Network (GNN) [28; 32]. Generally, the primary emphasis of these studies lies in devising interactions between the temporal dimensions (cross-time) and the variable dimensions (cross-variable).

Cross-Time Interaction Modeling.Cross-time interaction modeling aims to capture correlations between different time points. Recently, CNN-based model TimesNet  transforms the time series into a two-dimensional matrix and uses a CNN-based backbone for feature extraction. RNN-based model LSTnet  utilizes the Long Short-Term Memory (LSTM) to model the temporal dependencies, but it may be limited by the inherent issue of gradient vanishing/exploding in RNNs. Transformer-based models benefit from its self-attention mechanism, enabling them to capture long-term cross-time dependency. AutoFormer  incorporates a decomposition mechanism that splits the input sequence into trend and seasonality, and integrate an auto-correlation module into the transformer to capture long-term cross-time dependency. FedFormer  leverages a frequency-enhanced decomposition mechanism while incorporating additional frequency information. However, despite the outstanding performance of Transformer-based methods, we observe that their self-attention mechanism is susceptible to unexpected noise, as shown on Figure 1(a). Based on these findings, we propose an innovative GNN-based method that constructs a cross-scale temporal graph to mitigate the impact of temporal noise on modeling cross-time correlations.

Cross-Variable Interaction Modeling.Cross-Variable Interaction is proved to be critical for MTS forecasting , and numerous works have employed Graph Neural Networks (GNNs) [22; 37; 30; 38] to capture cross-variable relationships. STGCN  firstly utilizes GNN to model the cross-variable dependency in traffic forecasting, which can effectively capture the dependency between different roads in pre-defined topology graphs. MTGNN  expands the utilization of GNN from spatio-temporal prediction to MTS forecasting, and proposes a straightforward method for computing adaptive cross-variable graph. On the other hand, the Transformer-based MTS prediction works have also recognized the potential of cross-variable interactions to enhance prediction performance, such as CrossFormer . However, the cross-variable relationship is dynamic and can be greatly influenced by noise during the learning process. Given this, we refine the cross-variable relationship by decoupling homogeneity and heterogeneity in MTS, resulting in a noise-insensitive relationship during the temporal evolution.

## 3 Methodology

In long-term multivariate time series (MTS) forecasting, the input comprises historical sequences across \(D\) variables denoted by \(=\{X_{1}^{t},...,X_{D}^{t}\}_{t=1}^{L+D}^{L D}\), where \(L\) denotes the look-back window size and \(X_{i}^{t}\) denotes time series of the \(i_{th}\) variate at the \(t_{th}\) time step. The objective of MTS forecasting is to predict future time series denoted by \(}=\{_{1}^{t},...,_{D}^{t}\}_{t=L+1}^{L+T} ^{T D}\) based on \(\), where \(T\) represents the prediction time steps and \(T 1\). The detailed structure of CrossGNN is illustrated in Figure 2. We firstly employ an adaptive multi-scale identifier (AMSI) to generate multi-scale time series and reduce noise on coarse scale. Then, we construct scale-sensitive and trend-aware temporal graph to extract cross-scale interaction. We perform cross-variable aggregation via modeling homogeneous and heterogeneous relationships between variables. Finally, direct multi-step (DMS) forecasting is adopted in decoder to predict future time series.

### Adaptive Multi-Scale Identifier

The adaptive multi-scale identifier (AMSI) is designed to capture the different scales of MTS from coarse to fine, and reduce the unexpected noise on the coarse scale. Technically, we utilize Fast Fourier Transform (FFT) to analyze the time series in the frequency domain and calculate the potential periods of the time series, inspired by . We compute the amplitude of each time series at various frequencies using FFT and subsequently average the amplitude of \(\) across variable dimension:

\[A=_{i=1}^{D}((())),\] (1)

where \(()\) is the calculation of amplitude, \(()\) is the calculation of FFT, \(A^{L}\) represents the calculated amplitude of each frequency, which is averaged from \(D\) variables by \(()\). We choose the frequencies \(\{f_{1},f_{2},,f_{S}\}\) which correspond to the Top-S amplitude values:

\[\{f_{1},,f_{S}\}=\,(A),\] (2)

where \(\,()\) picks out the \(S\) frequency values with the highest amplitude from \(A\). The period lengths \(\{p_{1},p_{2},,p_{S}\}\) are calculated by the selected frequencies as follows:

\[p_{s}=},s\{1,,S\}.\] (3)

Then, \(()\) with kernel size \(p_{s}\) and stride \(p_{s}\), \(s\{1,2,...,S\}\), are applied to the MTS \(^{L D}\) in the time dimension to capture the MTS at \(s\)-th scales:

\[_{s}=()_{kernel=stride=p_{s}},\] (4)

where \(()\) downsamples time series to obtain \(_{s}^{L(s) D}\), \(L(s)=}\) is the time series length in the \(s\)-th scale, \(\) is the operation of rounding down. We concatenate the captured different

Figure 2: CrossGNN architecture. (a) Adaptive multi-scale identifier (AMSI) is devised to extract the multi-scale MTS \(^{}\) from the input \(\). (b) Cross-Scale GNN facilitates cross-time interaction within and across different scales, while Cross-Variable GNN models cross-variable interaction for both homogeneous and heterogeneous relationships. (c) Direct multi-step (DMS) forecasting is leveraged to predict the future time series based on two MLPs.

scales in the time dimension and obtain a period-wise multi-scale MTS \(^{}^{L^{} D}\) as the output of AMSI, \(L^{}=_{s=1}^{S}L(s)\) is the sum of lengths across all scales:

\[^{}=(_{1},_{2},..., _{S}).\] (5)

Here, we employ an expansion dimension strategy (using an MLP), to create an embedding for each time step at different scale. This strategy is inherited from MTGNN , aiming to enhance the local semantics at each time step and positively impact subsequent cross-scale and cross-variable interactions. The shape of \(X^{}\) is finally expanded as \(^{L D C}\).

### Cross-Scale GNN

The Cross-Scale GNN is designed to utilize the interaction of the multi-scale MTS \(^{}\) and extract the scales with clearer association and weaker noise. The cross-scale graph in the time dimension is represented as \(G^{scale}=(V^{scale},E^{scale})\). \(V^{scale}=\{v_{1}^{scale},v_{2}^{scale},...,v_{L}^{scale}\}\) is the time nodes set in all time scales, where \(v_{i}^{scale}\) is the \(i\)-th time node. \(E^{scale}^{L^{} L}\) assigns the correlation weights between each time node, each element in \(E^{scale}\) meaning the correlation weight between two time nodes (inter-scale or intra-scale). The key purpose of Cross-scale GNN is to learn the cross-scale temporal correlation weights \(E^{scale}\) that is insensitive to noise interference. To diminish the effect of noise on correlation weights, we maintain the independence by initializing \(E^{scale}\) with the production of two learnable vectors \(vec_{1}^{scale}\) and \(vec_{2}^{scale}\), ensuring \(E^{scale}\) not affected by noise hidden in input:

\[E^{scale}=((vec_{1}^{scale} vec_{2}^{scale })),\] (6)

where \(()\) is the active function regularizing the weight matrix so that each element is positive, and \(()\) is the operation to ensure the weights of all nodes correlated to a particular time node sum to 1.

Scale-sensitive Restriction.We consider that for any time node, the number of its correlated time nodes on the fine scale should be more than the number on the coarse scale. For any time node \(v_{i}^{scale}\), the number of its correlated nodes at \(s\)-th scale is restricted to \(k_{s}=}\), where \(p_{s}\) is the period length of \(s\)-th scale, \(\) denotes the ceiling function, \(K\) is a constant. This ensures that finer-scale time series contribute more temporal node associations. The neighbor time node set at \(s\)-th scale (i.e. the correlated time nodes at \(s\)-th scale ) of \(v_{i}^{scale}\) is denoted as:

\[_{s}^{scale}(v_{i}^{scale})=_{s}(E_{s}^{ scale}(v_{i}^{scale})),\] (7)

where \(_{s}()\) is an operation to extract \(k_{s}\) nodes with highest correlation weight, \(E_{s}^{scale}(v_{i}^{scale})^{L(s)}\) is the correlation weight of time node \(v_{i}^{scale}\) at \(s\)-th scale. In this way, the number of neighboring nodes at different scales can be restricted based on the correlation weight matrix \(E^{scale}\).

Trend-aware Selection.To ensure the temporal trends can be captured, we preserve the associations between a time node and its both preceding and succeeding nodes. Denote \(^{trend}(v_{i}^{scale})\) as the trend neighbor set of time node \(v_{i}^{scale}\), which can be defined as follows:

\[^{trend}(v_{i}^{scale})=\{v_{j}^{scale}\,|\,|i-j| 1,scale(v_{i}^{ scale})=scale(v_{j}^{scale})\},\] (8)

where \(scale()\) provides the scale of a time node. The trend neighbor set of \(v_{i}^{scale}\) consists of its adjacent time nodes (i.e., \(|i-j| 1\)) which share the same scale. This gives the ability to preserve temporal trends in the cross-scale correlation graph.

Correlation Weight Re-normalization.Denote \((v_{i})=^{scale}(v_{i})^{trend}(v_{i})\) as the selected neighbor set of time nodes \(v_{i}^{scale}\). The final correlation weight is re-normalized as follows:

\[E^{scale}[i,j]=[i,j]}{_{v_{m}(v_{ i})}E^{scale}[i,m]},&v_{j}(v_{i}),\\ 0,&,\] (9)

where \(E^{scale}[i,j]\) is the correlation weight between \(v_{i}^{scale}\) and \(v_{j}^{scale}\). This step filters out non-significant correlation and preserves a restricted set of neighboring nodes \((v_{i}^{scale})\) to each node. Also, re-normalization is applied to the retained correlation. Then, cross-scale correlation graph between time nodes at different scales is constructed.

Cross-scale interaction.After obtaining the cross-scale temporal correlation graph, we do cross-scale interaction in the time dimension based on GNN, and the information propagation process will be stacked for \(N\) layers:

\[^{time,N}_{i,:}=(_{s=1}^{S}_{v_{j} ^{scale}_{i}(v_{i})}E[i,j]^{time,N-1}_{j,:}+(_ {v_{m}^{trend}(v_{i})}E[i,m]^{time,N-1}_{m,:})\] (10) \[^{time,N}_{i,:}=(Concat(^{ time,N}_{i,:},^{time,N-1}_{i,:}) W)\] \[^{time,N}_{i,:}=^{time,N}_{i,:}/||^{time,N}_{i,:}||_{2}\]

where \(\) is the activation function; \(W\) is the learnable matrix; \(^{time}\) is the time mode feature; \(^{time}\) is the aggregation of neighbor time node feature. \(^{time,N}_{i,:}\) aggregates the neighboring node feature from the previous layer correlated to the time nodes of \(v^{scale}_{i}\). Then \(^{time,N}_{i,:}\) is updated by the aggregated time node feature \(^{time,N}_{i,:}\) and its previous layer feature \(^{time,N-1}_{i,:}\). Finally, the normalized \(^{time,N}_{:,:}\) at \(N\)-th layer is the output of Cross-Scale GNN.

### Cross-Variable GNN

Cross-Variable GNN is designed to extract the invariant correlation consisting of homogeneous and heterogeneous relationship. In variable dimension, the cross-variable graph is represented as \(G^{var}=(V^{var},E^{var})\). \(V^{var}=\{v^{var}_{1},v^{var}_{2},...,v^{var}_{D}\}\) is the variable node set, where \(D\) is the number of variables, and \(v^{var}_{i}\) is the \(i\)-th variable node. \(E^{var}\) is the variable correlation weight matrix, each element in \(E^{var}\) meaning the correlation weight between two variables. \(E^{var}\) is initialized by production of two latent vectors \(vec_{1}^{var}\) and \(vec_{2}^{var}\):

\[E^{var}=((vec_{1}^{var} vec_{2}^{var}))\] (11)

Heterogeneity Disentanglement.Specifically, we select the nodes with \(K^{var}_{+}\) highest correlation weight as positive neighbors with homogeneous connections, and take the nodes with \(K^{var}_{-}\) lowest correlation score as negative neighbors with heterogeneous connections. Denote \(E^{var}(v^{var}_{i})\) as the correlation weight of \(D\) variable nodes related to \(v^{var}_{i}\). For variable \(v_{i}\), its two decoupled neighbor sets can be represented as \(^{var}_{-}(v_{i})=_{-}}(E^{var}(v_{i}))\) and \(^{var}_{+}(v_{i})=_{+}}(E^{var}(v_{i}))\), respectively.

Correlation Weight Re-normalization.The corresponding homogeneous and heterogeneous correlation weights are derived as follows:

\[E^{var}[i,j]=-|}}}}{_{v_{k }^{var}(v_{i})}E^{var}[i,j]},&v_{j}^{var}_{-}(v_{i}),\\ E^{var}[i,j]&v_{j}^{var}_{+}(v_{i}),\\ 0,&,\] (12)

This process filters out edges other than homogeneous and heterogeneous edges. The weights of homogeneous edges are positively correlated with their correlation scores, while the weights of heterogeneous edges are negatively correlated with their correlation scores. Additionally, separate re-normalization is applied to the weights of homogeneous edges and heterogeneous edges, respectively. Then, the cross-variable graph is constructed with disentangled homogeneous and heterogeneous correlations.

Cross-variable Interaction.For variable \(v_{i}\), the disentangled cross-variable message passing can be formulated as:

\[^{var,N}_{:,i}=_{v_{j}^{ var}_{+}(v_{i})}E^{var}[i,j]^{var,N-1}_{:,j}+_{v_{k} ^{var}_{-}(v_{i})}E^{var}[i,k]^{var,N-1}_{:,k},\] (13) \[^{var,N}_{:,i}=(Concat(^{ var,N}_{:,i},^{var,N-1}_{:,i}) W),\] \[^{var,N}_{:,i}=^{var,N}_{:,i}/||^{var,N}_{:,i}||_{2},\]

[MISSING_PAGE_FAIL:7]

reproduced the results of MTGNN  with look-back window \(L=96\) on all datasets according to the settings in the original paper. We calculate the Mean Square Error(MSE) and Mean Absolute Error(MAE) of MTS forecasting as metrics. More details about datasets, baselines, implementation, hyper-parameters are shown in Appendix A.3.

### Main Results

The quantitative results of MTS forecasting using different methods is shown in Table 1. CrossGNN achieves outstanding performance on most datasets across various prediction length settings, obtaining 47 first-place and 9 second-place rankings in total 64 settings. Quantitatively, compared with the best results that Transformer-based methods can offer, CrossGNN achieves an overall \(10.43\%\) reduction on MSE and \(10.11\%\) reduction on MAE. Compared with GNN-based method MTGNN, CrossGNN achieves a more significant reduction \(22.57\%\) on MSE and \(25.74\%\) on MAE. Compared with other strong baselines like TimesNet and Dlinear, CrossGNN can still outperform them in general. Our method dose not achieve the best performance on the Electricity dataset. Further analysis reveals that the more severe out-of-distribution (OOD) problem in Electricity dataset results in a lower generalization of the learned temporal graph relations on the test set.

### Robustness Analysis of Noise

To evaluate the model robustness against noise, we add different intensities of Gaussian white noise to the original MTS and observe the performance changes of different methods. Figure 3 shows the MSE results of CrossGNN, ETSformer  and MTGNN  under different noise ratio on ETTm2 dataset, and the input length is set as 96. As the signal-to-noise ratio (SNR) decreases from 100db to 0db, the mean square error (MSE) increases more slowly on CrossGNN (0.177) compared to ETSformer (0.191) and MTGNN (0.205). The quantitative results demonstrate that CrossGNN exhibits good robustness against noisy data and has a great advantage when dealing with unexpected fluctuations. We speculate such improvements benefit from the explicit modeling of respective scale-level and variable-level interactions.

### Ablation Study

We conduct ablation studies by removing corresponding modules from CrossGNN on three datasets. **C-AMSI** removes the adaptive multi-scale identifier (AMSI) and directly use \(k\) fixed lengths (e.g., \(1,2,3,...,k\)) for average pooling. **C-CS** removes the Cross-Scale GNN module. **C-Hete** removes the heterogeneous connections and focuses on the homogeneous correlation modeling between different variables. **C-CV** removes both homogeneous and heterogeneous connections. We analyze the results shown in Table 2. Obs.1) Removing

   &  &  &  \\ Predict Length & 96 & 192 & 336 & 720 & 96 & 192 & 336 & 720 & 96 & 192 & 336 & 720 \\   & MSE & 0.167 & 0.220 & 0.276 & 0.358 & 0.585 & 0.590 & 0.596 & 0.610 & 0.192 & 0.249 & 0.313 & 0.420 \\  & MAE & 0.229 & 0.284 & 0.318 & 0.382 & 0.330 & 0.336 & 0.341 & 0.354 & 0.279 & 0.318 & 0.358 & 0.417 \\   & MSE & 0.175 & 0.231 & 0.290 & 0.371 & 0.588 & 0.599 & 0.603 & 0.614 & 0.200 & 0.259 & 0.328 & 0.422 \\  & MAE & 0.241 & 0.286 & 0.325 & 0.385 & 0.331 & 0.343 & 0.342 & 0.358 & 0.281 & 0.325 & 0.364 & 0.421 \\   & MSE & 0.172 & 0.224 & 0.277 & 0.364 & 0.589 & 0.588 & 0.603 & 0.616 & 0.190 & 0.255 & 0.320 & 0.417 \\  & MAE & 0.231 & 0.280 & 0.327 & 0.381 & 0.329 & 0.332 & 0.335 & 0.354 & 0.280 & 0.320 & 0.358 & 0.417 \\   & MSE & 0.174 & 0.227 & 0.279 & 0.367 & 0.588 & 0.591 & 0.604 & 0.616 & 0.193 & 0.257 & 0.322 & 0.419 \\  & MAE & 0.235 & 0.281 & 0.328 & 0.382 & 0.330 & 0.335 & 0.338 & 0.358 & 0.278 & 0.323 & 0.359 & 0.418 \\   & MSE & **0.159** & **0.211** & **0.267** & **0.352** & **0.570** & **0.577** & **0.588** & **0.597** & **0.176** & **0.240** & **0.304** & **0.406** \\  & MAE & **0.218** & **0.266** & **0.307** & **0.362** & **0.310** & **0.321** & **0.324** & **0.337** & **0.265** & **0.307** & **0.345** & **0.400** \\  

Table 2: Performance comparisons on ablative variants

Figure 3: Robustness analysis under different signal-to-noise ratio (SNR) on ETTm2.

Cross-Scale GNN results in the most significant decrease in prediction metrics, emphasizing its strong ability in modeling the interaction between different scales and time points. Obs.2) Cross-Variable GNN also improves the model performance a lot, demonstrating the importance of modeling the complex and dynamic interaction between different variables. Obs.3) AMSI constantly improves the forecasting accuracy, suggesting that different scales of MTS contain rich interaction information.

### Hyper-Parameter sensitivity

Look-Back Window SizeFigure 4 shows the MSE results of models with different look-back window sizes on four datasets. As the window size increases, the performance of Transformer-based models fluctuates while CrossGNN constantly improves. This indicates that the attention mechanism of Transformer-based methods may focus much more on the temporal noise but our method can better extract the relationships between different time nodes through Cross-Scale GNN.

Number of ScalesWe vary the number of scales from 4 to 8 and report the MSE and MAE results on Weather and Traffic dataset. As shown in Figure 5 (a) and Figure 5 (b), We observe that the performance improvement becomes less significant after a certain number of scales, indicating that a certain scale size is sufficient to eliminate most of the effects of temporal noise. **Number of Node Neighbors** The number of neighboring nodes limited to each time node is mainly determined by the hyperparameter \(K\). As shown on Figure 5 (c) and Figure 5 (d), we experiment with \(K\) values of 10, 15, 20, 25, and 30 and found that CrossGNN is not sensitive to the number of \(K\). This indicates that it is only necessary to focus on strongly correlated nodes for effective information aggregation in temporal interaction.

### Complexity Analysis

Table 3 illustrates the theoretical complexity of CrossGNN and existing Transformer-based methods. Detailed complexity derivation can be found in the Appendix C. To verify that the time and space complexity of our method is indeed \(O(L)\), we use TVM to implement the GNN computation part and compare the computation time and memory usage of with full connected graph during inference on ETTh2. Comparison experiments are implemented on a Intel(R) 8255C CPU @ 2.50GHZ with 40GB memory, centros 7.8, and TVM 1.0.0. Figure 6 illustrates the time and memory cost of GNN modules, and our proposed approach is close to linear with the input length.

Figure 4: The MSE results (Y-axis) of models with different look-back window sizes (X-axis) on ETTh2, ETTm2, Traffic and Weather, the output length is set as 336.

Figure 5: The MSE (left Y-axis) and MAE results (right Y-axis) of CrossGNN on Traffic and Weather. (a) and (b) display the performance on different scale numbers. (c) and (d) demonstrates the performance on different number of node neighbors

## 5 Conclusion and Future Work

In this work, we construct a comprehensive analysis of real-world data and observe that the temporal fluctuations and heterogeneity between variables, caused by unexpected noise, are not well handled by current popular time-series forecasting methods. To address above issues, we propose a linear complexity CrossGNN model, which is the first GNN model to refine both cross-scale and cross-variable interaction for MTS forecasting. An adaptive multi-scale identifier (AMSI) is leveraged to obtain multi-scale time series with reduced noise from input MTS. In particular, Cross-Scale GNN captures the scales with clearer trend and weaker noise, while Cross-Variable GNN maximally exploits the homogeneity and heterogeneity between different variables. Extensive experiments on 8 real-world MTS datasets demonstrate the effectiveness of CrossGNN over existing SOTA methods while maintaining linear memory occupation and computation time as the input size increases. For future work, it is worth exploring the design of dynamic graph networks that can effectively capture complex interactions in out-of-distribution (OOD) scenarios.

## 6 Acknowledgement

This paper is partially supported by the National Natural Science Foundation of China (No.62072427, No.12227901), the Project of Stable Support for Youth Team in Basic Research Field, CAS (No.YSBR-005), Academic Leaders Cultivation Program, USTC.