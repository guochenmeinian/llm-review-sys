# Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators

Changze Lv\({}^{1}\) Dongqi Han\({}^{2}\) Yansen Wang\({}^{2}\) Xiaoqing Zheng\({}^{1}\)

Xuanjing Huang\({}^{1}\) Dongsheng Li\({}^{2}\)

\({}^{1}\)School of Computer Science, Fudan University

\({}^{2}\)Microsoft Research Asia

{czlv22}@m.fudan.edu.cn, {zhengxq,xjhuang}@fudan.edu.cn,

{yansenwang,dongqihan,dongsli}@microsoft.com

The work was conducted during the internship of Changze Lv at Microsoft Research Asia.Corresponding authors.

###### Abstract

Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible. However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy. Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE. We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG. Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts. Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain. This investigation may offer valuable insights into the fundamental principles of neural computation. Our code is available at https://github.com/microsoft/SeqNN.

## 1 Introduction

Spiking neural network (SNN)  has increasingly attracted research interests in recent years, primarily due to its energy efficiency, event-driven paradigm, biological plausibility, and other distinctive properties. The spiking neurons in SNN are dynamical systems that generate binary signals (spike or non-spike) and communicate these signals like artificial neural networks (ANNs) for computation [2; 3; 4; 5; 6; 7; 8; 9]. Many advanced architectures and methodologies developed for ANNs are also applicable to SNNs, enhancing their capabilities. Notable among these are backpropagation , batch normalization [11; 12], and Transformer architecture [4; 13; 5; 6], which collectively broaden the functional scope of SNNs.

Despite the promising advances in SNNs, several challenges persist when adapting them to diverse tasks. A fundamental challenge is that SNNs, which are event-triggered, lack robust and effective mechanisms to capture indexing information, rhythmic patterns, and periodic data. This limitation can adversely affect SNNs' ability to process and analyze different data modalities, including natural language, and time series. Meanwhile, while SNNs aim to emulate the neural circuits of the brain,their reliance on spike-based communication imposes limitations. Consequently, not all deep learning techniques applicable to ANNs can be directly transferred to SNNs. For instance, methods like HiPPO  or trigonometric positional encoding  are not readily compatible with the spike format used in SNNs. Moreover, even the most state-of-the-art ANNs still lag significantly behind human capabilities in many tasks [16; 17]. Therefore, to enhance the functionality of SNNs, one promising approach is to draw further inspiration from biological neural mechanisms. In this regard, we propose the analogy of central pattern generators (CPGs) , a kind of neural circuit well-known in neuroscience, with positional encoding (PE), a technique extensively utilized in deep learning. This analogy is designed to operate within the SNN framework, potentially bridging the gap between biologically inspired models and modern deep learning techniques.

In neuroscience, a CPG (See Figure 2 for an illustration) is a group of neurons capable of producing rhythmic patterned outputs without requiring rhythmic inputs [19; 20]. These neural circuits are found in the spinal cord and brainstem and are responsible for generating the rhythmic signals that control vital activities such as locomotion, respiration, and chewing .

On the other hand, PE is an important technique for ANNs, particularly within models tailored for sequence processing task [15; 22; 23]. By endowing each element of the input sequence with positional information, typically achieved through diverse mathematical formulations or learnable embeddings, neural networks acquire the capability to discern the order and relative positions of the elements within the sequence.

We argue that these two concepts, despite seemingly unrelated, can be connected profoundly. Intuitively, CPG and PE both generate periodic outputs (with respect to time for CPG and with respective to position for PE). Moreover, in this paper, we reveal a deeper relationship between these two concepts by showing that **the widely used sinusoidal PE is mathematically a particular solution of the membrane potential dynamics of a specific CPG**.

However, current SNN architectures exhibit a notable deficiency in implementing an effective and biologically plausible PE mechanism. Existing so-called positional encoding methods for SNNs [4; 5] rely on input data, often resulting in non-spike and repetitive outputs for different positions. Furthermore, incorporating PE techniques designed for ANNs necessitates the calculation of membrane potentials, which is incompatible with the spike format of SNNs. To address these issues, we draw inspiration from the spiking properties of the CPGs and propose a straightforward yet versatile PE technique for SNNs, termed CPG-PE. This method encodes positional information with multiple neurons with various patterns of spike trains. To summarize the highlights of our study:

* **Novel Positional Encoding for SNNs.** We introduce a bio-plausible and effective PE approach tailored specifically for SNNs. This innovative strategy draws inspiration from the central pattern generator found in the human brain. Additionally, we propose a straightforward implementation of CPG-PE in SNNs, which is also compatible with neuromorphic hardware as it can be realized using circuits of leaky integrate-and-fire neurons.
* **Consistent Performance Gain.** Our proposed methods significantly and consistently enhance the performance of SNNs across a wide range of sequential tasks, including time-series forecasting and text classification.
* **Insightful Analysis.** Our research represents one of the pioneering efforts to comprehensively analyze (1) the mechanism by which SNNs capture positional information and (2) the role of CPGs in the brain. This analysis provides valuable insights into the underlying principles of neural computation.

## 2 Preliminaries

### Spiking Neural Networks

The basic unit in SNNs is the spiking neuron, such as the leaky integrate-and-fire (LIF) neuron , which operates based on an input current \(I(t)\) and influences the membrane potential \(U(t)\) and the spike \(S(t)\) at time \(t\). The dynamics of the LIF neuron are described by the following equations:

\[U(t) =H(t- t)+I(t), I(t)=f(;),\] (1) \[H(t) =V_{reset}S(t)+(1-S(t)) U(t),\] (2) \[S(t) =1,&U(t) U_{}\\ 0,&U(t)<U_{}.\] (3)

Here, \(I(t)\) is the spatial input to the LIF neuron at time step \(t\), calculated using the function \(f\) with \(\) as input and \(\) as learnable parameters. \( t\) is the discretization constant that determines the granularity of LIF modeling, and \(H(t)\) is the temporal output of the neuron at time step \(t\). The spike \(S(t)\) is defined as a Heaviside step function based on the membrane potential. When \(U(t)\) reaches the threshold \(U_{}\), the neuron fires, emitting a spike, and the temporal output \(H(t)\) resets to \(V_{reset}\). If the membrane potential \(U(t)\) does not reach the threshold, no spike is emitted, and \(U(t)\) decays to \(H(t)\) at a decay rate of \(\).

In this paper, we choose direct training with surrogate gradients as our method to train SNNs. we follow  to choose the arctangent-like surrogate gradients as our error estimation function when backpropagation, which regards the Heaviside step function as: \(S(t)( U(t))+\), where \(\) is a hyper-parameter to control the frequency of the arctangent function. Therefore, the gradients of \(S\) are \(=  U(t))^{2})}\) and thus the overall model can be trained in an end-to-end manner with back-propagation through time (BPTT) .

### Positional Encoding

In the field of sequential tasks, PE is crucial for models like Transformers to understand the sequential order of input tokens. Absolute PE and relative PE are two prominent methods used to incorporate positional information into these models. Absolute PE  assigns fixed embeddings to each position in the input sequence using trigonometric functions like sine and cosine. These embeddings are based solely on the position index and are not influenced by the token content, which are predefined and are generated as follows:

\[_{(pos,2i)}=(}{100002^{i/d}}), _{(pos,2i+1)}=(}{100002^{i/d}} ).\] (4)

Here, \(\) is the position and \(d\) is the dimension. In contrast, relative PE [26; 27; 28] captures the relationships between tokens by considering their relative distances. This dynamic approach allows models to learn position-specific patterns and dependencies, which is beneficial for tasks requiring different sequence lengths or hierarchical structures.

Figure 1: (a) Positional encoding (PE) in ANN Transformers. (b) Relative PE ++ in Spike Transformers [4; 5; 6]. (c) Our Proposed CPG-PE method. (d) CPG-PE consistently improves learning performance across various tasks. CPG-PE is an ideal PE method tailored for SNNs, detailed in Section 3.

However, existing SNN architectures reveal a notable deficiency in the integration of an effective and biologically plausible PE mechanism. As shown in Figure 1, current Transformer-based SNNs [4; 5] are primarily tailored for image classification and predominantly rely on a convolutional layer to capture the relative positional information of image patches. However, this approach resembles more of a spike-element-wise (SEW) residual connection  rather than a classic PE module, as it does not ensure that each image patch has a unique spike-form positional representation. Furthermore, the addition between positional spikes and the original input spikes within these models may yield hardware-unfriendly non-binary integers (i.e., neither \(0\) nor \(1\)), resulting from the addition of "1" and "1". Additionally, our investigation reveals that even SNNs designed for sequential tasks, such as SpikeBERT [29; 30], SpikeGPT , and SpikeTCN , also exhibit a notable absence of an effective spike-form PE mechanism for capturing positional information.

We think that an effective PE strategy should possess the following characteristics: **uniqueness of each position** and the **capacity to capture positional information from the input data**. Furthermore, an optimal PE designed for SNNs should be **hardware-friendly** and **in spike-form**.

### Central Pattern Generators

Central Pattern Generators (CPGs) are neural networks capable of producing rhythmic patterned outputs without sensory feedback [18; 20]. These networks are fundamental for understanding motor control in vertebrates and invertebrates and are often applied to robotics and neural control systems. Mathematically, CPGs can be modeled using systems of coupled nonlinear oscillators, and the general form can be written as:

\[}=()+(,), }=()+(,),\] (5)

where \(\) and \(\) are the state variables (can be seen as membrane potential) of two coupled oscillators, \(\) and \(\) are intrinsic dynamics of the oscillators, and \(\) and \(\) are the coupling functions.

## 3 Methods

In biological systems, CPGs as well as other neurons do not transmit information directly through membrane potential but through spikes. A burst of spikes will be generated only when the membrane potential of a CPG exceeds a certain threshold. Therefore, we introduced the Heaviside step function in SNN, selecting only the part that exceeds the threshold, to design the CPG-PE. In this section, we will first reveal the relationship between CPGs and PE. Then we will introduce our proposed CPG-PE and its implementations.

### Relationship between Central Pattern Generators and Positional Encoding

Consider one of the simplest CPGs with the following assumptions:

1. The CPG is a coupled nonlinear oscillator with 2 neurons whose states are represented as \((t)\) and \((t)\).
2. Both neurons are autonomic neurons and will gain membrane voltage with constant speed, i.e., \(()=b>0,()=d>0\).
3. Neuron represented by \(\) will inhibits \(\) while \(\) excites \(\). And the influence is proportional to the other neuron's state. Formally, \((,)=a,(, )=-c\) where \(a>0,c>0\).

Now the coupled oscillators can be represented as:

\[}(t)=a(t)+b,}(t)=-c(t) +d.\] (6)

The general solution of this differential equation system is:

\[(t) =k_{1}(\;t)+k_{2}}(\; t)+,\] (7) \[(t) =-k_{1}}(\;t)+k_{2}( \;t)-,\] (8)where \(k_{1}\) and \(k_{2}\) are arbitrary constants. To simplify, we can further re-parameterize \(t\) with \(t^{}=t+(k_{1}/ak_{2})\) as is to choose another start point, then we can rewrite Equations (7) and (8) as:

\[(t^{}) =^{2}+k_{2}^{2}}(\;t^{}) +=A_{1}(w_{1}t^{})+b_{1},\] (9) \[(t^{}) =k_{1}^{2}+k_{2}^{2}}(\;t^{}) -=A_{2}(w_{2}t^{})+b_{2}.\] (10)

Comparing Equations (9) and (10) and Equation (4), we are astonished to find that **the PE in Transformers  is a particular solution of the membrane potential variations in a specific type of CPG** with properly chosen \(a,b,c,d\). This finding suggests that the use of sinusoidal PE in Transformers is actually a bio-plausible choice that could possibly advance the model's ability to learn indexing and periodic information.

### CPG-based Positional Encoding

Consider a system with \(N\) pairs of CPG neurons, resulting in a total of \(2N\) cells. Then for \(i=1,2,...,N\), the equations governing the CPG-PE are as follows:

\[^{2i-1}(t) =H((}})-v^{ }),\] (11) \[^{2i}(t) =H((}})-v^{ }),\] (12)

where \(\) is a constant to control the period, \(\) represents the base period, and \(v^{}\) denotes the membrane potential threshold. Note that this threshold is different from the \(U_{thr}\) of spike neurons described in Equation (3). The Heaviside step function \(H\) reflects a spike when the membrane potential exceeds the threshold.

It is important to clarify that the \(t\) in Equation (11) and 12 is neither the time step in SNNs nor the position index. Suppose the input spike matrix \(X\{0,1\}^{T B L D}\), where \(T\) is the time step in SNNs, \(B\) is the batch size, \(L\) is the sequence length of the input sample, \(D\) is the feature dimension. To ensure the uniqueness of each position at every time step, we flatten the dimensions \(T\) and \(L\) into a new dimension \(T L\). Therefore, \(t\) ranges from \(0\) to \(T L\). Notably, the entire CPG-PE operates in spike-form and is parameter-free. To better understand CPG-PE, we draw a simple approximation of the resulting CPG spiking patterns under the assumption of a sequence length of \(L=128\) and \(N=20\) pairs of CPG neurons, illustrated in Figure 2 (b).

### Implementations

We design a simple implementation to apply CPG-PE to SNNs in a pluggable and hardware-friendly manner, shown in Figure 3. Before diving into the details, we want to emphasize that the data transmitted in SNNs should always be in spike-form. Therefore, the direct addition operation between two spike matrices, as used in [4; 5], should be forbidden.

Figure 2: (a) Illustration of a pair of CPG neurons demonstrating mutual inhibition through spiking activity. The spikes represent neural spikes that inhibit each other, exemplifying the coordination mechanism in CPG networks. (b) Spike trains of the first \(4\) CPG neurons. The curve represents the membrane potential, while the vertical lines represent spikes.

Initially, CPG-PE encodes the positional information of the input spike matrix \(X\), resulting in \(X^{}\). Then, to maintain binary values and avoid introducing non-binary elements, we opt to **concatenate**\(X\) and \(X^{}\) along the feature dimension. Lastly, a linear layer is employed to map the feature dimension from \(D+2N\) back to \(D\), where \(D\) is the feature dimension of \(X\), and \(N\) is the number of CPG pairs. This effectively neutralizes the dimensional increase caused by concatenation. The whole process can be formalized as follows:

\[X^{}=(X), X\{0,1\}^{T B L D},X^{}\{0,1\}^{T  B L 2N}\] (13) \[X_{1}=X X^{}, X_{1}\{0,1\}^{T B L(D+2N)}\] (14) \[X_{output}=(( (X_{1}))), X_{output}\{0,1\}^{T B L D}\] (15)

where \(\) represents batch normalization and \(\) is a spike neuron layer. Furthermore, CPG-PE necessitates that input samples be sequential data, making it directly applicable to time series data and natural language. For image data, however, an adaptation is required: images must be segmented into patches similar to the approach used in the Vision Transformer . Considering the compatibility with neuromorphic hardware, we also (1) implement CPG-PE with LIF neurons, and (2) integrate CPG-PE into a classic linear layer. Please refer to Appendices C and D for details.

## 4 Experiments

In this section, we conduct experiments to investigate the following research questions:

**RQ1**: Is our design of CPG-PE strategy effective and robust in sequential tasks?

**RQ2**: Can CPG-PE work well on image patches that have no inherent order?

**RQ3**: How will CPG's inner properties influence CPG-PE?

**RQ4**: Does our CPG-PE satisfy the requirements of a good PE tailored for SNNs?

### Datasets

To assess the PE capabilities of the compared models and answer **RQ1**, we conduct two sequential tasks: **time-series forecasting**, and **text classification**. Following , we choose \(4\) real-world datasets for time-series forecasting: Metr-la : This dataset contains the average traffic speed data collected from the highways in Los Angeles County. Pems-bay : It consists of average traffic speed data from the Bay Area. Electricity : This dataset captures hourly electricity consumption measured in kilowatt-hours (kWh). Solar : It includes data on solar power production. For text classification, we follow  to conduct experiments on \(6\) benchmarks including: Movie Reviews , SST-2 , SST-\(5\), Subj, ChnSenti, and Waimai. In addition, to answer **RQ2**, we also conduct **image classification** experiments on \(1\) static datasets CIFAR and \(1\) neuromorphic datasets CIFAR10-DVS . The dataset details and metrics are provided in Appendix A.

### Time-Series Forecasting

As discussed in Section 3.3, our proposed CPG-PE can be seamlessly integrated into any SNN capable of sequence processing. Consequently, we applied CPG-PE to the SNN counterparts of Temporal Convolutional Networks (TCN) , Recurrent Neural Networks (RNN) to assess the efficacy of our method in enabling SNNs to capture positional information. The results for TCN, SpikeTCN w/o PE, RNN, and Spike-RNN w/o PE are sourced from the previous study by . In addition, we

Figure 3: Illustration of applying CPG-PE to SNNs. \(X\), \(X^{}\), and \(X_{output}\) are all spike matrices.

deliberately conducted experiments on PE in Spikformer to explore whether our specially designed CPG-PE is truly more suitable for SNNs than all previous PEs. Notably, we also investigated the modularization of CPG, i.e., replacing all Linear layers with CPG-Linear layers (See Appendix D), and its impact on the Spikformer model for time-series forecasting, i.e., Spikformer w/ CPG-Full. We report the results on \(4\) time-series forecasting benchmarks with various prediction lengths in Table 1. We also list results from ANNs for reference.

In summary, the results presented in Table 1 indicate that SNNs equipped with the CPG-PE module significantly outperform their counterparts lacking the PE feature. This finding effectively addresses **RQ1** from a time-series analysis perspective. Detailed findings include:

**(1) CPG-PE enables SNNs to successfully capture positional information**. SNNs, including models such as Spike-TCN, Spike-RNN, and Spikformer, when integrated with CPG-PE, show superior performance compared to those without PE. Notably, CPG-PE also reduces the performance disparity between SNNs and traditional ANNs in time-series forecasting tasks, evidenced by an average increase of \(0.013\) in R\({}^{2}\) and a decrease of \(0.022\) in RSE.

**(2) CPG-PE is the most suitable position encoding strategy for Spikformer**. In addition to CPG-PE, other encoding strategies such as Float-PE (the original PE in Transformer) and RPE (the original PE in Spikformer) were also evaluated. The Spikformer equipped with CPG-PE emerged as the top-performing variant, confirming CPG-PE as the most suitable PE strategy for SNNs.

**(3) CPG-Full module can also effectively model the positional information of time series data**. The CPG-Full module's performance in modeling positional information of time-series data is comparable to that of CPG-PE, with average R\({}^{2}\) values nearly identical to those of Spikformer with CPG-PE and significantly better than those of other models.

### Text Classification

In addition to time-series forecasting, natural language processing (NLP) serves as another critical domain to assess the efficacy of the CPG-PE module in encoding positional information. Following the pioneering work of , who first employed Spikformer for text classification tasks, we extended this application to \(6\) benchmark datasets. We also include results from fine-tuned BERT for reference.

    &  &  &  &  &  &  &  &  \\  & & & & 6 & 24 & 88 & 26 & 6 & 24 & 88 & 96 & 24 & 88 & 96 & 24 & 45 & 36 & 48 \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & 25 & 23 & 25 & 21 & 24 & 88 & 66 & 24 & 88 & 96 & 24 & 86 & 24 & 16 \\    & & & & & 25 & 25 & 23 & 25 & 25 & 69 & 69 & 65 & 26 & 17 & 17 & 66 & 95 & 35 & 36 & 56 & 17 \\    & & & & & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 39 & 13 & 14 \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & 25The results presented in Table 2 shows that Spikformer enhanced with CPG-PE achieves the state-of-the-art performance across \(6\) benchmarks, effectively addressing **RQ1**. Meanwhile, we conducted a set of ablation experiments to eliminate the effects of increased parameter counts on model performance. Specifically, we replaced the spike-form positional encoding matrix obtained from CPG with a randomly generated spike matrix (See "Spikformer w/ Random PE" Row). By comparing these two configurations, we confirmed the effectiveness of our proposed CPG-PE.

### Image Classification

In this section, we aim to answer **RQ2**. To adapt the CPG-PE for image classification, it is essential to conceptualize the array of image patches as sequential data. Consequently, some SNN models that do not incorporate a concept of "sequence length" in their spike matrices, such as SEW-Resnet , are incompatible with the integration of a CPG-PE module. Therefore, we only consider ViT-liked SNN, i.e. Spikformer, in this experiment. We also include results from ViTs for reference.

We report the parameter counts and classification accuracy in Table 5. To elaborate, Spikformer with CPG-PE outperforms other variants, demonstrating the effectiveness of CPG-PE even when the sequence is an array of image patches lacking inherent order. Notably, owing to our streamlined implementation, the parameter count for Spikformer with CPG-PE is significantly reduced compared to the original Spikformer w/ RPE , with a reduction of \(1.16\) M. What's more, we conducted ablation experiments on model parameters by reducing the parameter count of Spikformer with CPG-PE to be comparable to Spikformer w/o PE, allowing for a more direct performance comparison, as shown in the last line in Table 5. The results on ImageNet are reported in Appendix E.

However, it is essential to acknowledge that the improvements in image classification are relatively modest compared to those observed in time series and text applications. This phenomenon can largely be attributed to the intrinsic _non-ordered nature_ of image patches. Unlike text or time series data, where sequential order is crucial and inherently informative, image patches do not possess a natural or fixed sequence. This lack of order means that traditional methods of positional encoding, which significantly benefit ordered data by providing contextual positioning, are less effective. Thus, the application of our positional encoding techniques, optimized for data with inherent sequential order, does not translate as effectively to the domain of image classification.

### Sweeping CPG properties

In this section, we investigate the influence of CPG properties on the ability to model positional information, addressing **RQ3**. To this end, we evaluated the Spikformer model with CPG-PE by varying the base period \(\) and the number of CPG pairs \(N\) (see Equations (11) and (12)) in time-series forecasting and image classification tasks.

From Figure 4 (a) and (b), we observe that CPG-PE is insensitive to the base period \(\) (in biological neurons, \(\) is affected by the physiological properties of the CPG circuit such as RC constant and synaptic delay). The sequence lengths (\(T L\)) of the time series and image patches are no larger than \(672\)\((4 168)\) for all benchmarks, preventing repetitions in CPGs. Therefore, when \(N=20\), sweeping \(\{100,1000,5000,10000\}\) makes minor influence on performance. Furthermore, Figure 4 (c) and (d) demonstrate that when \(=10000\), increasing the number of CPG pairs \(N\) enhances Spikformer's performance. This is reasonable because more CPG neurons reduce repetitions in positional representations of \(X^{}\).

   &  &  &  &  &  &  \\   & & & Param (M) & Accuracy & Param (M) & Accuracy & Param (M) & Accuracy & Param (M) & Accuracy & \\  Vision-Transformer  & ✗ & ✗ & 3.92 & **96.73** & - & - & 9.36 & **81.02** & - \\  Spikformer w/o PE & ✗ & ✗ & 8.00 & 93.77 & 1.99 & 76.40 & 8.04 & 73.59 & 81.25 \\ Spikformer w/ Random-PE & ✗ & ✓ & 8.17 & 98.85 & 2.06 & 76.44 & 8.20 & 73.54 & 81.27 \\ Spikformer w/ Point-PE & ✗ & ✗ & 8.00 & 94.42 & 1.99 & 77.60 & 8.04 & 74.73 & 82.55 \\ Spikformer w/ RPE  & ✗ & ✓ & 9.33 & 94.64\({}^{*}\) & 2.57 & 77.95\({}^{*}\) & 9.37 & 76.78\({}^{*}\) & 83.12 \\ Spikformer w/ CPG-PE [Ours] & ✗ & ✓ & 8.17 & **94.82** & 2.06 & **78.06** & 8.20 & **77.27** & **83.38** \\ Spikformer w/ CPG-PE [Equal Param] & ✗ & ✓ & 7.90 & 94.60 & 1.99 & 78.00 & 8.02 & 76.91 & 83.17 \\  

Table 3: Evaluation on image classification benchmarks. Float-PE denotes the original PE of the Transformer, while RPE denotes the original PE of the Spikformer. Numbers with \({}^{*}\) denote our implementation. The best results of SNNs and ANNs are formatted in bold font format. All results are averaged across \(4\) random seeds.

### Positional Encoding Analysis

In this section, we want to address **RQ4**. As mentioned in Section 2.2, an ideal PE method for SNNs should include the following characteristics: (1) **Uniqueness of each position**; (2) **Compatibility with neuromorphic hardware**; (3) **Formulation in spike-form**. Our implementations ensure compatibility with neuromorphic hardware (2), and the CPG-PE is inherently formulated in spike-form, satisfying (3). Therefore, in order to assess (1), the uniqueness of each position, we would like to compare the CNN-based RPE in [4; 5] and our proposed CPG-PE, focusing specifically on their capacity to provide distinct positional signals. This analysis was conducted using the CIFAR10-DVS dataset, where we calculated the repetition rate of spike positional representations across all positions. Our findings were notable: the positional spike matrices produced by RPE exhibited a repetition rate as high as \(\%\), which significantly undermines its effectiveness for PE. In contrast, our proposed CPG-PE exhibited no repetition, demonstrating that our CPG-PE is well-suited for serving as the PE module in SNNs. Please refer to Appendix B for details.

## 5 Related Work

### Spike Encoding Methods

Spiking neural networks employ several coding methods to encode input information, each offering unique advantages. Direct coding [5; 40], the simplest form and widely-used in image tasks, directly associates spikes with specific values or events, providing straightforward and interpretable outputs but often lacking efficiency for complex tasks. Rate coding [8; 41], where the input is represented by the frequency of spikes within a given timeframe, is more robust and widely used but can be less precise due to its reliance on averaged spike rates. Temporal coding (a.k.a latency coding) [42; 43] encodes information based on the timing of individual spikes, allowing for high temporal precision and efficient representation of dynamic inputs, though it can be computationally demanding. In addition, delta coding  represents changes in input signals through spikes, focusing on differences rather than absolute values, which can enhance efficiency and response times but may introduce complexity in decoding. Each of these methods contributes to the versatility and applicability of SNNs in various domains, from neuroscience to artificial intelligence. The SNNs we considered in this paper should fall into the category of rate coding since back-prop is conducted on spike rate. Meanwhile, CPG-PE can be considered converting temporal information into spike rate of a group of neurons (Equations 11 and 12), and this is why CPG-PE can improve performance for sequential data. It is possible to introducing learning algorithms of temporal coding for the CPG neurons to tackle more complex sequence structure, which remains as future work.

### Positional Encoding in SNNs

Currently, few works have demonstrated the importance of PE approaches in SNNs. Spikformer  and Spike-driven Transformer  utilize a combination of "one convolutional layer + one batch normalization layer + one spiking neuron layer " to generate learnable "relative positional encoding". From our perspective, this strategy is more like a spike-element-wise residual connection , rather than a classic PE module. The unique representation of each position is a fundamental requirement for a robust PE module. However, the spike position matrix generated by their method may result in the same spike representation for different positions. Additionally, the addition of the input spike matrix and the position spike matrix will result in the occurrence of non-binary numbers (i.e., \(2\)) due

Figure 4: (a)(c) \(R^{2}\) versus \(\) and \(N\) on time-series forecasting tasks. (b)(d) Accuracy versus \(\) and \(N\) on image classification tasks. \(\{100,1000,5000,10000\}\), \(N\{5,10,20\}\).

to the addition of \(1\) and \(1\). For spiking graph neural networks,  proposed learnable positional graph spikes, aiming to capture neighbor information within graphs rather than sequences. Therefore, drawing inspiration from the periodic automatic spike generation pattern of CPGs, we propose a biologically plausible and effective spike-form absolute position encoding method called CPG-PE.

## 6 Rethinking the Role of CPGs in Neuroscience

Our study also provides novel insights into neuroscience on understanding the role of CPG in nervous systems. While traditionally CPG is believed to play a crucial role in producing the rhythmic motor patterns necessary for locomotion and other repetitive movements [18; 20], the analogy to PE in this work reveals that CPG can make a significant contribution in processing sequential data by encoding the positional information into unique spiking patterns at different times. This does not only work for time-series sensory input like auditory signals but also for visual sensory data: e.g. when a person looks at an image, saccades (eye movements) allow retinal neurons to receive different parts of the image at different times. This indicates that CPG neurons could potentially be utilized to encode positional information. Another extensive thought is that as PE can be learnable in ANNs, CPG may also benefit from adaptability to the data . The hypothesis, however, remains to be examined through neuroscientific experiments .

## 7 Conclusion

In conclusion, inspired by central pattern generators, we introduce a pioneering position encoding approach termed CPG-PE, specifically tailored to mitigate the constraints associated with current PE techniques within SNNs. We mathematically prove that abstract PE in the Transformer is a particular solution of the membrane potential variations in a specific type of CPG. Furthermore, through comprehensive empirical investigations across diverse domains including time-series forecasting, natural language processing, and image classification, we demonstrate that the CPG-PE satisfies all the requirements of PE tailored for SNNs. The limitations and future work are discussed in Appendix F.