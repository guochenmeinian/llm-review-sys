# Automatic Outlier Rectification via Optimal Transport

Jose Blanchet

Dept. of Management Science & Engineering

Stanford University

jose.blanchet@stanford.edu &Jiajin Li

Sauder School of Business

University of British Columbia

jiajin.li@sauder.ubc.ca &Markus Pelger

Dept. of Management Science & Engineering

Stanford University

mpelger@stanford.edu &Greg Zanotti

Dept. of Management Science & Engineering

Stanford University

gzanotti@stanford.edu

###### Abstract

In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize the optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the outlier during the optimization process. We demonstrate the effectiveness of our approach over conventional approaches in simulations and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.

## 1 Introduction

Outlier removal has a long tradition in statistics, both in theory and practice. This is because it is common to have (for example, due to collection errors) data contamination or corruption. Directly applying a learning algorithm to corrupted data can, naturally, lead to undesirable out-of-sample performance. Our goal in this paper is to provide a single-step optimization mechanism based on optimal transport for automatically removing outliers.

The challenge of outlier removal has been documented for centuries: early work is introduced in e.g. Gergonne (1821) and Peirce (1852). Yet the outlier removal problem continues to interest practitioners and researchers alike due to the danger of distorted model estimation. A natural family of approaches followed in the literature takes the form of "two-stage" methods, which involve an outlier removal step followed by an estimation step. Methods within this family range from rules of thumb, such as removing outliers beyond a particular threshold based on a robust measure of scale like the interquartile range (Tukey, 1977), to various model-based or parametric distributional assumption-based tests (Thompson, 1985). While the two-stage approach can be useful by separating the estimation task from the outlier removal objective, it is not without potential pitfalls. For example, outlier detection which relies on a specific fitted model may tend to overfit to a particular type of outlier, which can mask the effect of other types (Rousseeuw and Leroy, 1987). Conversely, theoutlier detection step may be overly conservative if it is not informed by the downstream estimation task; this can lead to a significant reduction in the number of observations available for model estimation in the next step, resulting in a loss of statistical efficiency (He and Portnoy, 1992).

Robust statistics, pioneered by Box (1953), Tukey (1960), Huber (1964) and others such as Hampel (1968, 1971) offers alternative approaches for obtaining statistical estimators in the presence of outliers without removing them, particularly in parametric estimation problems such as linear regression. Beyond these, one closely related approach is the minimum distance functionals-based estimator, introduced by Parr and Schucany (1980), Millar (1981), Donoho and Liu (1988a,b), Park et al. (1995), Zhu et al. (2022), Jaenada et al. (2022). This method involves projecting the corrupted distribution onto a family of distributions using a distribution metric and selecting the optimal estimator for the resulting distribution. However, this projection mechanism is not informed by the estimation task (such as fitting a linear regression or neural network). Thus, without additional information about the contamination model or estimation task, it can be challenging to choose an appropriate family of distributions for projection, which may lead to limitations similar to those outlined above in practice.

In this paper, we propose a novel approach to integrate both outlier detection and estimation in a joint optimization framework. A key observation is that statisticians aim to "clean" data **before** decisions are made; thus, ideal robust statistical estimators tend to be optimistic. To address this, we introduce the _rectification set_: a ball centered around the empirical (contaminated) distribution and defined by the optimal transport distance (see Villani (2009), Peyre et al. (2019)) in probability space. This rectification set aims to exclude potential outliers and capture the true underlying distribution, allowing us to minimize the expectation over the "best-case" scenarios, leading to a min-min problem. The study in Jiang and Xie (2024) also explores connections with min-min-type problems, but concentrates on the construction of artificially constructed rectification sets to cover certain existing estimators from the robust statistics literature. However, our primary focus is on introducing a novel rectification set, which is based on the optimal transport approach with a concave cost function.

To automatically detect outliers during the estimation process, one of our main contributions is the use of a concave cost function for the optimal transport distance. This function encourages what we refer to as "long haul" transportation, in which the optimal transport plan moves only a portion of the data to a distant location, while leaving other parts unchanged. This strategic approach effectively reppositions identified outliers closer to positions that better align with the clean data. Our novel formulation then involves minimizing the expected loss under the optimally rectified empirical distribution. This rectification is executed through the application of an optimal transport distance with a concave cost function, thereby correcting outliers to enhance performance within a fixed estimation task. More importantly, our method distinguishes itself from distributionally robust optimization (DRO) (Ben-Tal et al., 2013; Bayraksan and Love, 2015; Wiesemann et al., 2014; Delage and Ye, 2010; Gao and Kleywegt, 2022; Blanchet and Kang, 2017; Shafieezadeh Abadeh et al., 2015; Shafieezadeh-Abadeh et al., 2019; Sinha et al., 2018; Kuhn et al., 2019) due to the distinctive correction mechanisms initiated by the robust formulation employing a min-min strategy.

As we discuss in Section 2 below, the timing of error generation differs crucially between DRO and robust statistics. DRO approach employs a min-max game strategy to control the worst-case loss over potential post-decision distributional shifts. In contrast, the robust estimator acts after the pre-decision distributional contamination materializes. Thus the approach of robust statistics can be motivated as being closer to a max-min game against nature. As a consequence, in robust statistics, the adversary moves first, and therefore the statistician can be more optimistic that they can rectify the contamination applied by the nature thus motivating the min-min strategy suggested above. However, it is also worth mentioning that Kang and Bansal (2023), Nietert et al. (2023) are also trying to formulate the outlier-robust problem in a min-max form, incorporating additional information about the contamination model to further shrink the ambiguity set.

It is useful to contrast our approach with more traditional approaches to achieving robustness in statistics, such as M-estimators and random sample consensus methods such as RANSAC. The class of M-estimators (Lehmann and Casella, 2006; Van der Vaart, 2000; Hayashi, 2000) is quite broad. This class includes methods which are both robust and not robust. M-estimators in the context of traditional robust statistics (as in introduced by Huber in the 1960s) attempt to achieve robustness by using specific loss functions which make the estimation procedure less sensitive to outliers. Common examples include the Huber loss function and least absolute deviations regression. Our approach is more fundamental as it builds on top of a given (general) loss and the statistician has full control of the "outlier removal" modeling task (via the specification of optimal transport theory using a concave optimal transport cost) and the statistical task (via the chosen loss). In addition, the statistician also obtains the optimal "rectifier" (i.e. transporter) while the amount of rectification can be estimated via a cross-validation approach. Our approach is thus conceptually different from M-estimators. The benefits of our approach over M estimators (including the Huber loss) are illustrated in the experimental sections of our paper. RANSAC, first proposed in Fischler and Bolles (1981), handles outliers by iteratively selecting random data subsets, fitting models, and evaluating models on inliers to reach a best model. In contrast, our method optimally selects the set of points for rectification using a single optimization procedure. Though work on random sample consensus methods which guide sampling in more intelligent ways, such as Chin et al. (2011), may be seen as connected to our approach, our work is notably different. For example, random sample consensus methods only identify inliers, while the approach proposed in this paper identifies inliers and outliers and rectifies outliers. Our approach scales to high dimensions, while RANSAC encounters fundamental issues with sampling in such dimensions. Finally, our approach is based on optimal transport theory rather than heuristic or random sampling.

We summarize our main contributions as follows:

**(i) Novel statistically robust estimator.** We propose a new statistically robust estimator that incorporates a novel rectification set constructed using the optimal transport distance. By employing a concave cost function within the optimal transport distance, our estimator enables automatic outlier rectification. We prove that the optimal rectified distribution can be found via a finite convex program, and show that we can determine the optimal rectification set uncertainty budget via cross-validation.

**(ii) Connection to adaptive quantile regression.** For mean estimation and least absolute regression, we demonstrate that our robust estimator is equivalent to an adaptive quantile (regression) estimator, with the quantile controlled by the budget parameter \(\). Furthermore, we prove that the optimal rectified distribution exhibits a long haul structure that facilitates outlier detection.

**(iii) Effectiveness in estimating the option implied volatility surface.** We evaluate our estimator on various tasks, including mean estimation, least absolute regression, and option implied volatility surface estimation. Our experimental results demonstrate that our estimator produces surfaces that are 30.4% smoother compared to baseline estimators, indicating success in outlier removal, as surfaces should be smooth based on structural financial properties. Additionally, it achieves an average reduction of 6.3% in mean average percent error (MAPE) across all estimated surfaces, providing empirical evidence of the effectiveness of our rectifying optimal transporter.

## 2 DRO and Robust Statistics as Adversarial Games

In this section, we summarize conceptually how robust statistics is different from DRO (for more details, we refer the reader to e.g. Blanchet et al. (2024)). To lay a solid mathematical foundation, we begin by investigating a generic stochastic optimization problem. Here, we assume that \(Z\) is a random vector in a space \(^{d}\) that follows the distribution \(_{}\). The set of feasible model parameters is denoted \(\) (assumed to be finite-dimensional to simplify). Given a realization \(z\) and a model parameter \(\) the corresponding loss is \((,z)\). A standard expected loss minimization decision rule is obtained by solving

\[_{}\ _{_{}}[(,)]= _{}(,)\ _{}().\] (1)

Since \(_{}\) is generally unknown, to approximate the objective function in (1), we often gather \(n\) independent and identically distributed (i.i.d.) samples \(\{z_{i}\}_{i=1}^{n}\) from the unknown data-generating distribution \(_{}\) and consider the empirical risk minimization counterpart,

\[_{}\ _{_{n}}[(,Z)]= _{i=1}^{n}(,z_{i}),\] (2)

where \(}_{n}\) denotes the empirical measure \(_{i=1}^{n}_{z_{i}}\) and \(_{z}\) is the Dirac measure centered at \(z\). These problems are solved within the context of the general data-driven decision-making cycle below:

In the cycle depicted in Figure 1, we usually collect \(n\) i.i.d samples from the unknown data-generating distribution \(^{}\), which may be identical to or distinct from the clean distribution \(_{}\). Subsequently, we make a decision (e.g., parameter estimation) based on a model, \(^{}_{n}\), built from these samples. Such a model could be parametric or non-parametric. These decisions are then put into action within the out-of-sample environment \(}\), which may or may not conform to the distribution \(_{}\). In this general cycle, the sample average method (2) may lead to poor out-of-sample guarantees. This motivatesthe use of alternative approaches. In the following paragraphs, we describe two of these approaches, DRO and robust statistics, by treating them as adversarial games. The crucial distinction lies in the **timing** of the contamination or attacks.

(i) (**DRO**: \(}_{}=^{}\)) The attack occurs in the **post-decision** stage. In this scenario, the out-of-sample environment \(}\) diverges from the data-generating distribution \(_{}\), and no contamination occurs before the decision, implying that \(_{}=^{}\). For example, in adversarial deployment scenarios, malicious actors can deliberately manipulate the data distribution to compromise the performance of trained models. With full access to our trained machine learning model, the adversary endeavors to create adversarial examples specifically designed to provoke errors in the model's predictions.

To ensure good performance in terms of the optimal expected population loss over the out-of-sample distribution, the DRO framework introduces an uncertainty set \((^{}_{n})\) to encompass discrepancies between the in-sample-distribution \(^{}_{n}\) and the out-of-sample distribution \(}\). Subsequently, the DRO formulation minimizes the worst-case loss within this uncertainty set, thereby aiming to solve

\[_{}_{(^{}_{n})} _{}[(,Z)].\] (3)

(ii) (**Robust Statistics**: \(}\ =_{}^{}\)) The contamination occurs in the **pre-decision** stage. Many real-world datasets exhibit outliers or measurement errors at various stages of data generation and collection. In such scenarios, the observed samples are generated by a contaminated distribution \(^{}\), which differs from the underlying uncontaminated distribution \(_{}\). But, the out-of-sample distribution equals the original clean distribution. In contrast with DRO, the adversary corrupts the clean data according to a contamination model prior to training. Our objective is to clean the data during the training phase to achieve a robust classifier. It is noteworthy that the attacker does not have access to the specific model to be selected by the learner.

Given that the statistician knows that the data has been contaminated, a natural policy class to consider involves rectifying/correcting the contamination, and, for this, we introduce a rectification set \((^{}_{n})\) which models a set of possible pre-contamination distributions based on the knowledge of the empirical measure \(^{}_{n}\). To ensure good performance in terms of the optimal expected population loss over the clean distribution, the rectification/decontamination approach naturally induces the following min-min strategy:

\[_{}_{(^{}_{n})} _{}[(,Z)].\] (4)

Our goal in this paper is to develop (4), which is completely distinct from DRO.

## 3 Automatic Outlier Rectification Mechanism

We now introduce our primary contribution by delineating (4). Our estimator is crafted to incorporate outlier rectification and parameter estimation within a unified optimization framework, facilitating automatic outlier rectification. A natural question arises from (4): how do we construct the rectification set in the space of probability distributions to correct the observed data set? In this paper, we employ an optimal transport distance to create a ball centered at the contaminated empirical distribution \(^{}_{n}\).

**Definition 3.1** (Rectification Set).: The optimal transport-based rectification set is defined as

\[(^{}_{n})=\{(): _{c}(,^{}_{n})\},\] (5)

where \(>0\) is a radius, and \(_{c}\) is a specific optimal transport distance defined in Definition 3.2.

**Definition 3.2**.: (Optimal Transport Distance (Peyre et al., 2019, Villani, 2009)) Suppose that \(c(,):[0,]\) is a lower semi-continuous cost function such that \(c(z,z^{})=0\) for all \(z,z^{}\) satisfying \(z=z^{}\). The optimal transport distance between two probability measures \(,()\) is

\[_{c}(,)=_{( )}_{}[c(Z,Z^{})]:_{Z}=,\ _{Z^{}}=}.\]

Figure 1: Data-Driven Decision Making Cycle

Here, \(()\) is the set of joint probability distribution \(\) of \((Z,Z^{})\) supported on \(\) while \(_{Z}\) and \(_{Z^{}}\) respectively refer to the marginals of \(Z\) and \(Z^{}\) under the joint distribution \(\).

If we consider \(c(z,z^{})=\|z-z^{}\|^{r}\) as a metric defined on \(^{d}\), where \(r[1,)\), then the distance metric \(_{c}^{1/r}(,)\) corresponds to the \(r\)-th order Wasserstein distance (Villani, 2009). In this paper, we pioneer the utilization of _concave_ cost functions, exemplified by \(c(z,z^{})=\|z-z^{}\|^{r}\) where \(r(0,1)\) in statistical robust estimation. We note that \(_{c}(,)\) (as opposed to \(_{c}^{1/r}(,)\) ) is also a metric for \(r[0,1)\). The rationale behind selecting a concave cost function is intuitive: it promotes what we colloquially refer to as _long haul_ transportation plans, enabling outliers to be automatically moved significant distances back towards the central tendency of data distribution. This, in turn, facilitates automatic outlier rectification. Concave costs promote long hauls due to their characteristic of exhibiting decreasing marginal increases in transportation cost. In other words, if an adversary decides to transport a data point by \(\|\|\) units, it becomes cheaper to continue transporting the same point by an additional \(\) distance compared to moving another point from its initial location. We make further remarks about the connection between our estimator and prior work in Appendix B.1.

**Illustrative Example.** We provide empirical evidence showcasing the efficacy of the concave cost function on simulated data in Figure 2 and 3. This example shows that our concave cost function is critical for moving the outliers (orange) properly to the bulk of the clean distribution (blue). For the concave cost, only the outliers are rectified (green), resulting in the proper line of best fit.

However, for the convex cost, regardless of the budget, all points both clean and corrupted are rectified towards each other instead, which results in an incorrect line of best fit. This illustrates succinctly the importance of our novel concave cost. More details on this example are given in Appendix F.1, and an additional example with non-clustered outliers is given in Appendix F.1.1.

## 4 Reformulation Results

With this intuition in hand, we focus on the derivation of equivalent reformulations for the infinite-dimensional optimization problem over probability measures in (4). These reformulations provide us with a fresh perspective on adaptive quantile regression or estimation, where our introduction of an efficient transporter rectifies the empirical distribution, eliminating the influence of outliers.

To begin with, we can transform problem (4) into an equivalent finite-dimensional problem by leveraging the following strong duality theorem.

**Proposition 1** (Strong Duality).: Suppose that \((,)\) is lower semicontinuous and integrable under \(^{}_{n}\) for any \(\). Then, the strong duality holds, i.e.,

Figure 3: The rectified data generated by our estimator with **convex** cost function (\(r=2\)).

Figure 2: The rectified data generated by our estimator with **concave** cost function (\(r=0.5\)).

\[_{(^{}_{n})}_{}[ (;Z)]=_{ 0}_{^{}_{n}} [_{z}(;z)+ c(z,Z^{})]-.\]

The proof is essentially based on the strong duality results developed for Wasserstein distributionally robust optimization problems (Zhang et al., 2022; Li et al., 2022; Blanchet and Murthy, 2019; Gao and Kleywegt, 2022; Mohajerin Esfahani and Kuhn, 2018), which allows us to rewrite the original problem (4) as \(_{(^{}_{n})}_{}[ (;Z)]=-_{(^{}_{n})} _{}[-(;Z)]\).

We proceed to examine several representative examples to better understand the proposed statistically robust estimator (4). We begin with mean estimation to showcase our estimator's performance on one of the most classic problems of point estimation which can be easily understood. We then give an example for least absolute deviations (LAD) regression, one of the most important problems of robust statistics. LAD regression builds a conceptual foundation which leads into a discussion of our framework in more general cases and problem domains.

### Mean Estimation

The mean estimation task is widely recognized as a fundamental problem in robust statistics, making it an essential example to consider. In this context, we define the loss function as \((;z)=\|-z\|\). It is worth noting that when \(=0\), Problem (4) is equivalent to the median, which has already been proven effective in the existing literature. However, beyond the equivalence to the median, there are additional benefits to be explored regarding the proposed rectification set. By deriving the equivalent reformulation and analyzing the optimal rectified distribution, we can gain further insights into how the proposed statistically robust estimator operates. This analysis provides valuable intuition into the workings of the estimator and its advantages.

**Theorem 2** (Mean Estimation).: Suppose that \(=^{d}\), \((;z)=\|-z\|\) and the cost function is defined as \(c(z,z^{})=\|z-z\|^{r}\) where \(r(0,1)\). Without loss of generality, suppose that the following condition holds

\[\|-z_{1}^{}\|\|-z_{2}^{}\| \|-z_{n}^{}\|.\] (6)

Then, we have the inner minimization of (4) as

\[(_{i=1}^{k()-1}\|-z_{i}^{}\|+(1-^{n}\|-z_{i}^{}\|^ {r}}{\|-z_{k()}^{}\|^{r}})\|-z_{k()}^{ }\|,0).\] (7)

where \(k():=_{k[n]}\{k:_{i=k}^{n}\|-z_{ i}^{}\|^{r}\}\).

We give the proof in Appendix C.1.

_Remark 4.1_.: The resulting reformulation problem can be viewed as finding a quantile of \(\|-z^{}\|\) controlled by the budget \(\). If we have a sufficient budget \(\) such that \(_{^{}_{n}}[\|-Z^{}\|^{r}]\), it implies that all data points have been rectified to the value of \(\). Consequently, the minimum value of \(_{(^{}_{n})}_{ }[(,Z)]\) will be equal to zero. In nontrivial cases, when given a budget \(>0\) and the current estimator \(\), our objective is to identify and rectify the outliers in the observed data. To achieve this, we start by sorting the data points based on their loss value \(\|-z_{i}^{}\|\). We relocate the data points starting with the one having the largest loss value, \(z_{n}^{}\). The goal is to move each data point towards the current mean estimation \(\) until the entire budget is fully utilized.

Building upon the proof of Theorem 2, we can establish the following characterization of the rectified distribution. The concave cost function \(\|z-z^{}\|^{r}\) (with \(r(0,1)\)) plays a pivotal role in this context by endowing the perturbation with a distinctive long haul structure. Intuitively, for each data point, we can only observe two possible scenarios: either the perturbation is zero, indicating no movement or the data point is adjusted to eliminate the loss. In this process, the rectified data points are automatically identified as outliers and subsequently rectified.

**Proposition 3** (Characterization of Rectified Distribution).: Assuming the same conditions as stated in Theorem 2, we can conclude that the optimal distribution \(^{}\)

\[^{}(dz)=_{i=1}^{k()-1}_{z_{i}^{ }}(dz)+_{z_{k()}^{}}+_{}(dz)\]

where \(=1-^{n}\|-z_{i}^{}\|^{r}}{\| -z_{k()}^{}\|^{r}}\).

_Remark 4.2_.: The existence of optimal solutions follows directly from Yue et al. (2022, Theorem 2).

### Least Absolute Deviation (LAD) Regression and More General Forms

Following the same technique, we can also derive the least absolute deviation case.

**Theorem 4** (LAD Regression).: Suppose that \(:==^{d+1}\), \((,z)=\|y-^{T}x\|\) and the cost function is defined as \(c(z,z^{})=\|z-z\|^{r}\) where \(r(0,1)\) and \(\|\|\) is the \(_{2}\) norm. Without loss of generality, suppose that \(\|y_{1}^{}-^{T}x_{1}^{}\|\|y_{2}^{}-^{T }x_{2}^{}\|\|y_{n}^{}-^{T}x_{n}^{ }\|\), we have the inner minimization of (4) as

\[(_{i=1}^{k()-1}\|y_{i}^{}- ^{T}x_{i}^{}\|^{r}+(1-- _{i=k()+1}^{n}\|y_{i}^{}-^{T}x_{i}^{}\|^{r}}{\|y_{k( )}^{}-^{T}x_{k()}^{}\|^{r}})\|y_{k() }^{}-^{T}x_{k()}^{}\|,0),\]

where \(k():=_{k[n]}\{k:_{i=k}^{n}\|y_{k}^{}- ^{T}x_{k}^{}\|^{r}^{}\}\) and \(^{}=\|(,-1)\|^{r}\).

We give the proof of Theorem 4 in Appendix C.1. The structure of the optimal rectified distribution also resembles that of Proposition 3: the rectified data points are shifted towards the hyperplane \(y=^{T}x\) that best fits the clean data points. For a more detailed explanation of the long haul structure and extensions to more general cases, interested readers are encouraged to refer to Appendix B.1.

### Computational Procedure

Our procedure is motivated by the empirical efficacy of subgradient descent for training deep neural networks, as described in e.g. Li et al. (2020). In each iteration, given the current estimate \(\), we compute the optimal rectified distribution. For simple applications such as mean estimation and least absolute deviation regression, we can solve this optimization problem efficiently using the quick-select algorithm or by utilizing an existing solver for linear programs to obtain the optimal solution for the dual variable \(\), which we denote \(^{*}\). Then, we employ the subgradient method on the rectified data, iterating until convergence. This optimization process automatically rectifies outliers using a fixed budget \(\) across all iterations. As the value of \(\) changes, the same budget \(\) for all iterations results in varying the quantile used for selecting outliers. Thus, our estimator can be regarded as an iteratively adaptive quantile estimation approach.

We now concretely detail the procedure for LAD regression. The procedure for mean estimation is analogous and results from a simple change in the loss function. We must start by initially addressing the computation of the optimal dual variable \(^{}\) for the inner minimization over the probability space. Without loss of generality, we recall that this problem is

\[_{ 0}_{i=1}^{n}\{\|^{T}z_{i}^{}\|,^{T}z_{i}^{}\| ^{r}}{\|\|_{}^{r}}\}-.\] (8)

We show in the Appendix in Section C.1 by Lemma 6 that this problem can be solved by applying the quick-select algorithm to find the optimal \(^{}\) and the knot point \(k(_{t})\) which demarcates estimated outliers. Applying this lemma yields our estimation approach, which is described in Algorithm 1.

``` Data: Observed data \(\{z_{i}^{}\}_{i=1}^{n}\), initial point \(_{0}\), stepsizes \(_{t}>0\);
1for\(t=0,,T\)do
21. Sort the observed data \(\{z_{i}^{}\}_{i=1}^{n}\) via the value \(\|y_{i}^{}-_{t}^{T}x_{i}^{}\|\).
22. Quick-Select algorithm to get the knot point \(k(_{t})\) and the optimal \(^{}\);
3. Subgradient step on the detected clean data: \[_{t+1}=_{t}-}{n}_{i=1}^{k(_{t})}(_{t}^{T}x_{i}^{}-y_{i}^{}) x_{i}^{}\]
4 end for ```

**Algorithm 1**Statistically Robust Estimator

Additional details on an alternative linear program approach for estimating \(k(_{t})\) and \(^{*}\) are given in Appendix D.1. We also propose a procedure for estimating more general regression models in Appendix D.2. Further details on the optimization problem are given in Appendix D.3.

**Limitations.** Appendices D.2 and D.3 contain explanations of the limitations of our approach, which include the difficulty of solving the associated optimization problems and the nature of the derivation of our optimization algorithm for more general regression models.

Experimental Results

In this section, we demonstrate the effectiveness of the proposed statistically robust estimator through various tasks: mean estimation, LAD regression, and two applications to volatility surface modeling.

### Mean Estimation and LAD Regression Experiments

We perform simulation experiments with mean estimation and least absolute deviation linear regression for our estimator to illustrate its efficacy. For mean estimation, we compare our estimator with the mean, median, and trimmed mean estimators. We observe that our estimator outperforms all other methods, despite providing the oracle corruption level \(\) to the trimmed mean estimator. Our results are displayed in Table 1 below.

For LAD regression, we compare our estimator with the OLS, LAD, and Huber regression estimators. We find that, again, our estimator outperforms all other methods. A complete overview of these two experiments can be found in Appendix E.

### Options Volatility Experiments

In this section, we conduct empirical studies on real-world applications in fitting and predicting option implied volatility surfaces. Options are financial instruments allowing buyers the right to buy (call options) or sell (put options) an asset at a predetermined price (strike price) by a specified expiration date. In this study, we focus on European-style options, which can only be exercised at expiration. Option prices are influenced by the volatility of the underlying asset's price. Implied volatility (IV), derived from an option's market price, indicates the market's volatility expectations. The implied volatility surface (IVS) represents the variation of IV across different strike prices and times to maturity. Accurate IVS modeling is crucial for risk assessment, hedging, pricing, and trading decisions. However, outliers in IV can distort the IVS, necessitating robust estimation methods. Our approach addresses this by estimating the IVS in the presence of outliers. We conduct two experiments to demonstrate the effectiveness of our statistically robust estimator: (a) using a kernelized IVS estimator and (b) using a state-of-the-art deep learning IVS estimator.

#### 5.2.1 Kernelized Volatility Surface Estimation

Data.Our data set comprises nearly 2,000 option chains containing daily US stock option prices from 2019-2021. The options data is sourced from WRDS, a financial research service available to universities. The option chains were identified as containing significant outliers by our industry partner, a firm providing global financial data and analysis. We were blinded to this choice. We randomly draw a training set and test set from each chain and estimate surfaces. We assess the surfaces' out-of-sample performance using mean absolute percentage error (MAPE) and the discretized surface gradient (defined as \(\)). MAPE evaluates the error of the surface versus observed implied volatilities. \(\) evaluates the smoothness of the surface. Further details can be found in Appendix G.2.

Benchmarks.We compare our estimator developed in Theorem 4 to two benchmarks. The first is kernel smoothing (denoted "KS"), a well-established method for estimating the IVS . In KS, each point on the IVS is constructed by a weighted local average of implied volatilities across nearby expiration dates, strike prices, and call/put type. The weights are given by a kernel matrix which depends on these three features of an option (see next paragraph). The second

   Corruption Levels & 20\% & 30\% & 40\% & 45\% & 49\% \\  Mean & \(5.009 0.056\) & \(7.509 0.080\) & \(10.007 0.098\) & \(11.248 0.098\) & \(12.257 0.114\) \\ Median & \(1.680 0.114\) & \(1.831 0.118\) & \(2.286 0.192\) & \(2.843 0.208\) & \(4.203 0.344\) \\ Trimmed Mean & \(1.739 0.108\) & \(1.947 0.104\) & \(2.456 0.212\) & \(3.047 0.174\) & \(4.399 0.360\) \\ Ours & \(\) & \(\) & \(\) & \(\) & \(\) \\ Ours, \%Rectified & \(10.03\%\) & \(10.12\%\) & \(10.24\%\) & \(10.35\%\) & \(10.55\%\) \\   

Table 1: We compared our estimator with several standard mean estimation methods by evaluating the average loss on clean data points across various corruption levels. In our evaluation, we set the percent level of trimmed mean equal to the unknown ground truth corruption level. The hyperparameters for our estimator, namely \(=0.5\) and \(r=0.5\), remained constant across all corrupted levels. The last row in the comparison table represents the percentage of all points rectified by our method. Error bars represent two standard deviation confidence intervals assuming normal errors.

benchmark is a two-step "remove and fit" kernel smoothing method (denoted "2SKS") which first attempts to remove outliers via Tukey fences (Hoaglin et al., 1986) before applying the KS method.

Kernelized Regression Problem.We now describe the kernel and our estimator. Following the convention in Ait-Sahalia and Lo (1998), the \(i\)th option in a chain is featurized as the vector \(^{3} x_{i}^{}=(_{i},u(_{i}),_{ (i)})\), where \(_{i}\) is the number of days to the option's expiration date, \(_{i}[-1,1]\) is a relative measure of price termed the Black-Scholes delta, \(u(x)=x_{x 0}+(1+x)_{x<0}\), and \(_{(i)}\) is 1 if the option is a call option and 0 if the option is a put option. The goal is to fit the pair (\(y_{i}^{}\): the implied volatility of option \(i\), \(x_{i}^{}\): the features) via a kernelized regression model. We choose a Gaussian-like kernel \(K_{h}(x,x^{})\) to measure the distance between options \(x\) and \(x^{}\) as \(K_{h}(x,x^{})=-\|(x-x^{})/2h\|^{2}\) where division of \(x-x^{}\) by \(h\) is element-wise for a vector of bandwidths \(h^{3}_{+}\). When the budget \(=0\), we want to solve \(_{^{n}}_{i=1}^{n}v_{i}|y_{i}^{}-^{T}K_{ h}^{i}|\), where \(v_{i}\) is the \(i\)-th entry of a vector of vegas for \(\{x_{i}^{}\}_{i=1}^{n}\) and \(K_{h}^{i}\) is the \(i\)-th row of the kernel matrix. The implied volatilities are weighted by vega \(v_{i}\) to improve surface stability as in Mayhew (1995). We conducted a comparison between our estimator and the benchmarks. We note that the KS method can be regarded as a standard kernelized least square approach (Hansen, 2022).

Results.Our approach improves upon the benchmark approach in both MAPE and \(\) on the out-of-sample test sets, showcasing the importance of jointly estimating the rectified distribution and model parameters with our estimator. The results of our experiment are displayed in Table 5.2.1. We compare the KS and 2SKS benchmarks against our estimator with both fixed \(=0.01\) (chosen to correspond to a 1% change in volatility) and \(\) chosen by cross-validation. More details can be found in Appendix G.2. Our estimator outperforms others with a mean MAPE of 0.225, compared to 0.294 for KS and 0.236 for 2SKS, showing a 23% and 5% improvement respectively. For surface smoothness, our estimator achieves a mean DSG of 6.5, significantly better than 20.2 for KS and 7.5 for 2SKS, indicating 67% and 13% improvements. Notably, our industry partner applied our estimator to an unseen test set collected after this paper was written and was able to release to production 25% more surfaces than they had before when using their existing proprietary method.

Remark 5.1: To contextualize these results, we note that the difference in MAPE of a surface and an option chain containing outliers and the MAPE of a surface and an option chain containing no outliers will not be large if the set of outliers is small. Consider the MAPE of the same surface for two different option chains of size \(n\), \(O_{1}\) and \(O_{2}\), where a small fraction \(k/n\) of the options of \(O_{2}\) are outliers. Supposing the modal APE is \(0.3\) and the outlier APE is a considerable \(1.0\), for an options chain with \(n=50\) and just \(k=5\), the MAPE difference will be only \(-0.3=0.07\).

We perform an additional experiment with the same dataset in Appendix G.1 which demonstrates the usefulness of our method for estimating an IVS for use on the trading day after the initial contaminated surface is observed. We find similar outperformance of our method versus the benchmark methods.

#### 5.2.2 Deep Learning Volatility Surface Estimation

In this section, we apply our statistically robust estimator to state-of-the-art deep learning prediction approaches for modeling volatility developed in Chataigner et al. (2020). In this work, deep networks are used to estimate local volatility surfaces. The _local volatility_ or _implied volatility function_ model introduced by Rubinstein (1994), Dupire et al. (1994), and Derman et al. (1996) is a surface which allows for as close of a fit as possible to market-observed implied volatilies without arbitrage, which is of interest to many market participants (smoothing methods, in contrast, do not make this guarantee). Further background and details for this section are available in Appendix G.3.

   Model MAPE & \(0.5\%\) Quantile & \(5\%\) Quantile & Median & Mean & \(95\%\) Quantile & \(99.5\%\) Quantile \\  KS & 0.026 & 0.068 & 0.232 & 0.294 & 0.677 & 1.438 \\
2SKS & 0.026 & 0.056 & 0.172 & 0.236 & 0.602 & 1.389 \\ Ours (\(=10^{-2}\)) & 0.028 & 0.057 & 0.170 & 0.225 & 0.535 & 1.207 \\ Ours (CV) & 0.028 & 0.057 & 0.169 & 0.224 & 0.534 & 1.240 \\  Model V\(\) & \(0.5\%\) Quantile & \(5\%\) Quantile & Median & Mean & \(95\%\) Quantile & \(99.5\%\) Quantile \\  KS & 0.313 & 1.606 & 14.434 & 20.188 & 57.797 & 108.901 \\
2SKS & 0.050 & 0.124 & 2.229 & 7.491 & 33.276 & 78.144 \\ Ours (\(=10^{-2}\)) & 0.048 & 0.121 & 1.898 & 6.502 & 28.064 & 66.410 \\ Ours (CV) & 0.043 & 0.109 & 1.513 & 5.079 & 21.096 & 55.005 \\   

Table 2: Results of our experiment with kernelized IVS estimation.

We select the data set from Chataigner et al. (2020) consisting of (options chain, surface) pairs from the German DAX index. We first contaminate the chains by replacing an \(\) fraction of each price \(p\) with \(10p\). We then test our estimated surface against the true surface. To estimate price and volatility surfaces under data corruption, we applied our statistically robust estimator to the benchmark approach. We use the Dupire neural network of Chataigner et al. (2020) as a benchmark, which estimates the surface under local volatility model assumptions and no-arbitrage constraints. This method enforces these conditions on the surface using hard, soft, and hybrid hard/soft constraints.

We evaluate our robust estimator using the same metrics as Chataigner et al. (2020), RMSE and MAPE, repeated over three trials with different random seeds. Our approach outperforms the baseline approach across all averages, and does so more clearly as the corruption level increases, despite the strong regularizing effect of the no-arbitrage constraints and enforcement of Dupire's formula. Our improvement is most impactful for the most accurate model utilizing soft arbitrage constraints. For this model, the test set RMSE and MAPE are reduced by 33% and 34%. This experiment displays the efficacy of our estimator in a state-of-the-art deep learning approach to volatility modeling.

## 6 Conclusion

In conclusion, we propose an automatic outlier rectification mechanism that integrates outlier correction and estimation within a unified optimization framework. Our novel approach leverages the optimal transport distance with a concave cost function to construct a rectification set within the realm of probability distributions. Within this set, we identify the optimal distribution for conducting the estimation task. Notably, the concave cost function's "long hauls" attribute facilitates moving only a fraction of the data to distant positions while preserving the remaining dataset, enabling efficient outlier correction during the optimization process. Through comprehensive simulation and empirical analyses involving mean estimation, least absolute regression, and fitting option implied volatility surfaces, we substantiate the effectiveness and superiority of our method over conventional approaches. This demonstrates the potential of our framework to significantly enhance outlier detection integrated within the estimation process across diverse analytical scenarios.