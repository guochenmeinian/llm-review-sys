# GLBench: A Comprehensive Benchmark for Graph

with Large Language Models

 Yuhan Li\({}^{1}\)1, Peisong Wang\({}^{2}\)2, Xiao Zhu\({}^{1}\), Aochuan Chen\({}^{1}\), Haiyun Jiang\({}^{3}\)3, Deng Cai\({}^{4}\), Victor Wai Kin Chan\({}^{2}\), Jia Li\({}^{1}\)\({}^{1}\)

\({}^{1}\)Hong Kong University of Science and Technology (Guangzhou) \({}^{2}\)Tsinghua University

\({}^{3}\) School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University \({}^{4}\)Tencent AI Lab

###### Abstract

The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.

## 1 Introduction

Graphs are ubiquitous in modeling the relational and structural aspects of real-world objects, encompassing a wide range of real-world scenarios . Many of these graphs have nodes that are associated with text attributes, resulting in the emergence of text-attributed graphs, such as citation graphs  and web links . For example, in citation graphs, each node represents a paper, and its textual description (e.g., title and abstract) is treated as the node's text attributes.

To tackle graphs with both node attributes and graph structural information, conventional pipelines typically fall into two categories. Firstly, graph neural networks (GNNs)  have emerged as the dominant approach through recursive message passing and aggregation mechanisms among nodes. They often utilize non-contextualized shallow embeddings, such as bag-of-words  and skip-gram , as shown in the previous benchmarks . Secondly, pretrained language models (PLMs) can be directly employed to encode the text associated with each node, transforming the problem into a text classification task without considering graph structures . Following , we refer to these two learning paradigms as **GNN-based** and **PLM-based methods**.

In recent years, the advent of large language models (LLMs) with massive context-aware knowledge and semantic comprehension capabilities (e.g., LLaMA , GPTs , Mistral ) marks asignificant advancement in AI research, also leading a notable shift in the way we interact with graphs . For example, LLMs can retrieve semantic knowledge that is relevant to the nodes to improve the quality of initial node embeddings of GNNs [15; 49]. LLMs can also encode graphs through carefully designed prompts and directly make predictions in an autoregressive manner [5; 18]. In addition, LLMs can be aligned with GNNs in the same vector space, enabling GNNs to be more semantically aware [61; 32]. In this paper, we refer to this new paradigm of using LLMs to assist with graph-related problems as **GraphLLM**. We follow existing surveys [25; 21] to organize existing GraphLLM methods into three categories based on the role (i.e., enhancer, predictor, and aligner) played by LLMs throughout the entire model pipeline.

Despite the plethora of GraphLLM methods proposed in recent years, as illustrated in Figure 1, there is no comprehensive benchmark for GraphLLM, which significantly impedes the understanding and progress of existing methods in several aspects. _i)_ The use of different datasets, data processing approaches, and data splitting strategies in previous works makes many of the results incomparable, leading a lack of comparison and understanding of different categories of GraphLLM methods, i.e., which roles LLMs are better suited to play. _(ii)_ The lack of benchmarks for zero-shot graph learning has led to limited exploration in this area. _(iii)_ Apart from effectiveness, understanding each method's computation and memory costs is imperative, yet often overlooked in the literature. A comparison of existing benchmarks with GLBench in terms of datasets, models, and scenarios is shown in Table 1.

To redress these gaps and foster academia-industry synergy in evaluation, we propose GLBench, which serves as the first comprehensive benchmark for this community in both supervised and zero-shot scenarios. Besides traditional GNN-based and PLM-based methods, we implement a wide range of existing GraphLLM models, encompassing LLM-as-enhancer models, LLM-as-predictor models, and LLM-as-aligner models. We adopt consistent data pre-processing and data splitting approaches for fair comparisons. Through extensive experiments, our key insights include _(i)_ GraphLLM methods exhibit superior performance across the majority of datasets, with LLM-as-enhancers demonstrating the most robust performance; _(ii)_ The performance of LLM-as-predictor methods is not as satisfactory, and they often encounter uncontrollable output issues; _(iii)_ There is no obvious scaling law in existing methods; _(iv)_ Structural and semantic information are both important for zero-shot transfer; _(v)_ We propose a training-free baseline, which can even outperform existing GraphLLM methods tailored for zero-shot scenarios. In summary, we make the following three contributions:

* We introduce GLBench, the first comprehensive benchmark for GraphLLM methods in both supervised and zero-shot scenarios, enabling a fair comparison among different categories of methods by unifying the experimental settings across a collection of real-world datasets.
* We conduct a systematic analysis of existing methods from various dimensions, encompassing supervised performance, zero-shot transferability, and time and memory efficiency.
* Our benchmark repository is publicly available at https://github.com/NineAbyss/GLBench to facilitate future research on GraphLLM and graph foundation models.

## 2 Formulations and Background

In this section, we introduce the foundational concepts related to GLBench, including task definitions, learning scenarios, and the advances of GraphLLM.

Figure 1: Timeline of GraphLLM research. Existing methods can be divided into three categories based on the role played by LLM. Top left corner illustrates the key differences of roles.

### Notations

In this benchmark, we focus on the node classification task within the text-attributed graphs (TAGs). Formally, a TAG can be represented as \(=(,A,\{s_{n}\}_{n})\), where \(\) is a set of \(N\) nodes; \(A\{0,1\}^{N N}\) is the adjacency matrix, if \(v_{i}\) and \(v_{j}\) are connected, \(A_{ij}=1\), otherwise \(A_{ij}=0\); \(s_{n}^{L_{n}}\) is a textual description associated with node \(n\), with \(\) as the words or tokens dictionary and \(L_{n}\) as the sequence length. Typically, in most previous graph machine learning literature, such attribute information can be encoded into shallow embeddings \(=[_{1},_{2},,_{N}]^{ N f}\) by naive methods (_e.g._, bag-of-words or TF-IDF ), where \(f\) is the dimension of embeddings. Given TAGs with a set of labeled nodes \(\) (where \(\) can be empty), the goal of node classification is to predict the labels of the remaining unlabeled nodes.

### Learning Scenarios

GLBench supports two learning scenarios. The first scenario is **supervised learning**, which aims to train GraphLLM models to predict the unlabeled nodes with the same label space of training set on a single graph. Formally, let \(_{s}=(_{s},A_{s},_{s})\) with label space \(_{s}\) be the source graph used for training, and let \(_{t}=(_{t},A_{t},_{t})\) with label space \(_{t}\) be the target graph used for testing. The supervised learning scenario can be denoted as \(_{s}=_{t},_{s}=_{t}\). It is noted that supervised learning can be further divided into fully- and semi-supervised settings based on the ratio of training samples. In this paper, we do not make this finer distinction and instead refer to both as supervised learning. The detailed training ratios of each dataset can be found in Table 3.

The second scenario is **zero-shot learning**, which is an emerging topic in the field of graph machine learning. The goal of this setting is to train GraphLLM models on labeled source graphs and generate satisfactory predictions on a completely different target graph with distinct label spaces, which can be denoted as \(_{s}_{t}=,_{s}_{t}=\). According to existing works [29; 26], zero-shot learning poses inherent challenges such as feature misalignment and mismatched label spaces compared with traditional supervised learning. For example, different datasets often use varying shallow embedding techniques, leading to incompatible feature dimensions across datasets and making it difficult for a model trained on one dataset to adapt to another. We are the first to establish a benchmark for such a practical scenario. While only a few GraphLLM methods are capable of performing zero-shot inference, we believe this setting is crucial and well-suited for the graph foundation model era, emphasizing broad generalization across different graph sources [35; 30].

### LLMs for Graphs

To provide a global understanding of the literature, we follow [25; 21] to categorize the methods we evaluated in our benchmark based on the **roles** played by LLMs throughout the entire model pipeline. Table 2 provides an overview of the models assessed in GLBench. Specifically, LLM-as-enhancer approaches correspond to enhancing the quality of node embeddings or textual attributes with the help of LLMs. For example, TAPE  leverages LLMs to enrich initial node embedding in GNNs with semantic knowledge relevant to the nodes. In addition, several methods, such as GraphGPT  and LLaGA , utilize LLMs as predictors to perform classification within a unified generative paradigm. Lastly, GNN-LLM alignment ensures that each encoder's unique capabilities are preserved while coordinating their embedding spaces at a specific stage. For instance, GLEM  employs an

   Benchmark &  \#Datasets \\ (Node-level) \\  & \#Domains & Text & 
 \#Models \\ (GraphLLM) \\  & Model Type & Supervision Scenario \\  Sen et al.  & 2 (2) & 1 & ✗ & 8 (0) & Classical & Supervised \\ Shchur et al.  & 8 (8) & 2 & ✗ & 8 (0) & GNN & Supervised \\ OGB  & 14 (5) & 3 & ✗ & 20 (0) & GNN & Supervised \\ CS-TAG  & 8 (6) & 2 & ✓ & 16 (2) & GNN, PLM, Enhancer & Supervised \\  GLBench & 7 (7) & 3 & ✓ & 18 (12) & GNN, PLM, GraphLLM & Supervised and Zero-shot \\   

Table 1: Comparison of existing Graph benchmarks in terms of datasets, models, and scenarios. We focus on constructing a benchmark for GraphLLM methods, encompassing different categories of GraphLLM methods as well as traditional GNN- and PLM-based methods. In addition, we explore the zero-shot scenario, which is often overlooked by previous benchmarks.

EM framework to merge GNNs and LLMs, where one model iteratively generates pseudo-labels for the other. More detailed descriptions of models are provided in Appendix A.2.

## 3 The Setup of GLBench

In this section, we begin by introducing the datasets utilized in GLBench, along with the method implementations. Then, we outline the research questions that guide our benchmarking study.

### Datasets and Implementations

**Dataset selection.** To provide a comprehensive evaluation of existing GraphLLM methods, we gather 7 diverse and representative datasets, as detailed in Table 3, which are chosen based on the following criteria. _(i)_**Text-attributed graphs.** We only consider text-attributed graphs where each node represents a textual entity and is associated with a textual description. _(ii)_**Various domains.** Datasets in GLBench span multiple domains, including citation networks, web links, and social networks. _(iii)_**Diverse scale and density.** GLBench datasets cover a wide range of scales, from thousands to hundreds of thousands of nodes. The density also varies significantly, with average node degrees ranging from 2.6 (i.e., Citeseer) to 36.9 (i.e., WikiCS). Such diversity in datasets supports a robust evaluation of the effectiveness of GraphLLM methods in both supervised and zero-shot scenarios. Among the datasets in GLBench, Cora, Citeseer, Pubmed, and Ogbn-arxiv are designed to classify scientific papers into different topics. WikiCS aims to identify entities into different Wikipedia categories. Additionally, we incorporate two social networks that have been recently pre-processed by Huang et al. , i.e., Reddit and Instagram, which concentrate on identifying whether the user is a normal user. More descriptions of each dataset are shown in Appendix A.1.

**Dataset splits.** The data splitting methods in different works are not consistent, bringing difficulties in conducting fair comparisons. For example, a majority of GraphLLM methods  follow Yang et al.  to conduct experiments on the Cora dataset with the fixed 20 labeled training nodes per class at a 0.3% label ratio. However, several works [15; 55; 44] employ a 60%/20%/20% train/validation/test split, making it challenging to directly compare the performance with other methods. We investigate various GraphLLM works and choose the data splits that are most commonly used. More specifically, for Cora, Citeseer, and Pubmed, we use the classic split from [54; 24]. For Ogbn-arxiv, we follow the public partition provided by OGB . For WikiCS, we follow the split in . For two datasets from Huang et al. , we follow the original split described in the paper.

**Metrics.** According to existing node classification benchmarks [52; 17], we adopt _Accuracy_ and _Macro-F1_ score as the metrics for all datasets. Accuracy measures the overall correctness of the model's predictions, treating all classes equally. On the other hand, Macro-F1 score calculates the F1 score for each class independently and takes the average across all classes, making it more suitable for evaluating imbalanced datasets.

    &  &  &  &  &  &  \\   & & & & & & **Fine-tune** & **Prompt** & **Supervised** & **Zero-shot** & **Vtune** & **Code** \\   & GLART  & GNN & GraphSAGE, etc. & BERT & ✗ & ✗ & ✓ & ✗ & \(}\) & \(}\)R\(\)22 & Link \\  & TAPE  & GNN & ReGAT & ClustOPT & ✗ & ✓ & ✓ & ✗ & \(}\) & \(}\)R\(\)24 & Link \\  & OGR  & GNN & R.GCN & Sentence-BERT & ✗ & ✓ & ✓ & ✓ & ✗ & \(}\)R\(\)24 & Link \\  & ENSGRL  & GNN & GraphSAGE & LLMM-2 & ✓ & ✓ & ✓ & ✗ & \(}\) & \(}\)R\(\)24 & Link \\  & Zero(C)  & GNN & SOC & Sentence-BERT & ✓ & ✓ & ✗ & ✓ & \(}\) & \(}\)D\(\)24 & Link \\   & Intra-CAM  & LLM & - & FLAN-TSLALMA\( 1\) & ✓ & ✓ & ✓ & ✗ & \(}\)EXT\(\)24 & Link \\  & GraphText  & LLM & - & CHART/TOTAP+ & ✓ & ✓ & ✓ & ✗ & \(}\)Any\(\)24 & Link \\   & GraphLayer  & LLM & GingSAGE & LLM-A2 & ✓ & ✓ & ✓ & ✗ & \(}\)WWW\(\)24 & Link \\   & GraphText  & LLM & GT & Vicuna & ✓ & ✓ & ✓ & ✓ & \(}\)SIGN \(\)24 & Link \\   & LLGAG  & LLM & - & Vicuna/LLMMA\(\)2 & ✓ & ✓ & ✓ & ✗ & \(}\)CM\(\)24 & Link \\   & GLEM  & GNN/LLM & GraphSAGE, etc. & RoBERTs & ✓ & ✗ & ✓ & ✗ & \(}\) & \(}\)R\(\)23 & Link \\   & Partons  & LLM & GT & BERTScBERT & ✓ & ✗ & ✓ & ✗ & \(}\)C\(\)23 & Link \\   

Table 2: A summary of all GraphLLM methods used in our evaluation, which can be divided into three categories based on the roles (i.e., enhancer, predictor, and aligner) played by LLMs. **Fine-tune** denotes whether it is necessary to fine-tune the parameters of LLMs, and **Prompt** indicates the use of text-formatted prompts in LLMs, done manually or automatically.

**Implementations.** We rigorously reproduce all methods according to their papers and source codes. To ensure a fair evaluation, we perform hyperparameter tuning with a reasonable search budget on all datasets for GraphLLM methods. However, due to the considerably long training time of several LLM-as-predictor methods, i.e., InstructGLM  takes nearly \(20\) hours to train one epoch on the Arxiv dataset, we use the default parameters recommended in the paper for training. More details on these algorithms and implementations can be found in Appendix B.1.

### Research Questions

**RQ1: Can GraphLLM methods outperform GNN- and PLM-based methods?**

**Motivation.** Previous research in this community has been hindered by the use of different data preprocessing and splits, making it difficult to fairly evaluate and compare the performance of various GraphLLM methods with traditional methods. Given the fair comparison environment provided by GLBench, the first research question is to revisit the progress made by existing GraphLLM methods compared to using only GNNs or PLMs. By answering this question, we aim to gain a deeper understanding of the actual performance of existing GraphLLM methods, thereby providing insights into whether it is necessary to use GraphLLM methods as replacements for these traditional approaches in terms of effectiveness. In addition, as LLMs play different roles in GraphLLM methods, such as enhancer, predictor, and aligner, we are curious to investigate which roles are most effective in leveraging the power of LLMs for graph tasks.

**Experiment Design.** Following the experimental setting of most existing GraphLLM methods, we conduct the supervised node classification experiments on all the datasets in GLBench. For _(i)_**GNN-based methods**, we consider three prominent GNNs including GCN , GAT , and GraphSAGE . For _(ii)_**PLM-based methods**, we select encoder-only PLMs with different sizes of parameters, including Sentence-BERT (22M) , BERT (110M) , and RoBERTa (355M) . We avoid adopting decoder-only models for sentence encoding since they are specifically tuned for next-token prediction and do not perform as well in this scenario . In addition, for a fair comparison and to minimize the influence of the backbone models used in different GraphLLM methods, we try to utilize the same GNN and LLM backbones in our implementations. For example, we consistently use GraphSAGE as GNNs for LLM-as-enhancer methods, and LLAMA2 as LLMs for any 7B-scale methods. More details can be found in Appendix B.2.

**RQ2: How much progress has been made in zero-shot graph learning?**

**Motivation.** The goal of zero-shot graph learning, as discussed in Section 2.2, is to develop foundation models capable of generalizing to unseen graphs, thereby enabling transferability across diverse graph sources . Traditional GNNs face challenges in such scenarios primarily due to dimension misalignment, mismatched label spaces, and negative transfer . Recent efforts have been made to use LLMs directly as classifiers for zero-shot inference in graphs [8; 18]. For example, for each node (i.e., one paper) in a citation network, we can directly input the title and abstract of the paper into an LLM without any fine-tuning and then ask which category the paper belongs to. In addition, the emergence of incorporating LLMs with GNNs also offers promising solutions to these challenges, leveraging the unified graph representation learning and powerful knowledge transfer capabilities of

  
**Dataset** & **\# Nodes** & **\# Edges** & **Avg. \# Deg** & **Avg. \# Tok** & **\# Classes** & **\# Train** & **Node Text** & **Domain** \\ 
**Cora** & 2,708 & 5,429 & 4.01 & 186.53 & 7 & 5.17\% & Paper content & Citation \\
**Citeseer** & 3,186 & 4,277 & 2.68 & 213.16 & 6 & 3.77\% & Paper content & Citation \\
**Pubmed** & 19,717 & 44,338 & 4.50 & 468.56 & 3 & 0.30\% & Paper content & Citation \\
**Ogbn-arxiv** & 169,343 & 1,166,243 & 13.77 & 243.19 & 40 & 53.70\% & Paper content & Citation \\
**WikiCS** & 11,701 & 216,123 & 36.94 & 642.04 & 10 & 4.96\% & Entity description & Web link \\
**Reddit** & 33,434 & 198,448 & 11.87 & 203.84 & 2 & 10.00\% & User’s post & Social \\
**Instagram** & 11,339 & 144,010 & 25.40 & 59.25 & 2 & 10.00\% & User’s profile & Social \\   

Table 3: Statistics of all datasets in GLBench, including the number of nodes and edges, the average node degree, the average number of tokens in the textual descriptions associated with nodes, the number of classes, the training ratio in the supervised setting, the node text, and the domains they belong to. More details are shown in Appendix A.1.

LLMs [29; 26; 63]. These developments motivate us to systematically evaluate models with zero-shot capabilities, providing insights into the progress made in this challenging and practical scenario.

**Experiment Design.** To answer this research question, we evaluate three GraphLLM methods with zero-shot capabilities on all datasets in GLBench for node classification, which involves an optional pre-training phase using _source datasets_ that have non-overlapping classes with the _target datasets_. To ensure data distribution practices in NLP and CV, where source datasets are typically larger than target datasets, we select the two largest graphs in GLBench, i.e., Ogbn-arxiv and Reddit for pre-training, and use the remaining datasets for inference. We involve additional models as baselines including _(i)_**graph self-supervised learning** (SSL), which focus on the transferability of learned structures , such as DGI  and GraphMAE ; _(ii)_**LLMs**, which only consider semantic information for classification, such as LLaMA3-70B , GPT-3.5-turbo , GPT-4o , and DeepSeek-chat . We follow  to design input prompts for each dataset; _(iii)_**Emb w/ NA**, a simple training-free baseline that utilizes both structural and semantic information for zero-shot inference.

**RQ3: Are existing GraphLLM methods efficient in terms of time and space?**

**Motivation.** As GraphLLM methods typically introduce more LLM parameters, even simultaneously optimizing GNNs and LLMs (e.g., as aligners), they inherently require more computational complexity and space than GNNs. However, the efficiency of GraphLLM methods has been largely overlooked in existing research. While introducing LLMs may benefit graph tasks, the extra computational consumption has posed significant requirements of trade-off between performance and efficiency. It is essential to understand the trade-off for deploying GraphLLM methods in practical applications.

**Experiment Design.** To answer this research question, we select two models from each of the three categories of GraphLLM methods and evaluate their efficiency in terms of time and space. Specifically, we record the wall clock running time and peak GPU memory consumption during each method's training process. We discuss efficiency based on datasets from multiple domains in GLBench, reporting results on Cora, Citeseer, WikiCS, and Instagram. For a fair comparison, all experiments in this part were conducted on a single NVIDIA A800 GPU.

## 4 Experiments and Analysis

### Supervised Performance Comparison (RQ1)

Table 4 and Figure 2 shows the experimental results of various GraphLLM methods along with traditional GNN- and PLM-based methods under the supervised scenario. Our key findings include:

1**GraphLLM methods exhibit superior performance across the majority of datasets, highlighting their effectiveness.** From Table 4, we can observe that GraphLLM methods achieve the highest accuracy and F1 scores on \(6\) out of the \(7\) datasets compared to traditional GNN-based and PLM-based methods, particularly significant on several datasets such as Citeseer, Pubmed, and Reddit. For instance, the LLM-as-enhancer method OFA  outperforms the best performing GNN-based model GCN by 3.20% and 3.49% in terms of accuracy and F1 score respectively on Citeseer. In addition, the LLM-as-aligner approach Patton surpasses GCN by 5.18% in accuracy and 4.03% in F1 score on Pubmed. On the Reddit dataset, the LLM-as-predictor model LLaGA  achieves a 4.00% and 4.69% higher accuracy and F1 score compared to GCN. Notably, on the two largest graphs (i.e., Arxiv and Reddit), GraphLLM methods show stability with nearly all methods consistently outperforming baselines. The superior performance of GraphLLM methods can be attributed to their ability to jointly leverage both structure and semantics, enabling more powerful graph learning compared to traditional methods that only consider either structures or semantics independently. This highlights the necessity and benefit of incorporating LLMs in supervised learning.

2**The performance of LLM-as-predictor methods is not as satisfactory, and they often encounter uncontrollable output issues.** As evident from the notably bluer box plots in Figure 2, LLM-as-predictor methods generally underperform compared to traditional GNNs and other categories of GraphLLM methods across datasets. This performance drop is particularly evident in graphs with limited training data, such as Cora, Citeseer, and Pubmed, due to insufficient LLM training. However, LLM-as-predictor methods showcase better results on social network datasets. For instance, LLaGA  achieves the best results on Reddit, while GraphAdapter  achieves the second-best performance on Instagram. This can be attributed to the general and simpler semantics of textual attributes, such as user posts and profiles, which are easier for LLMs to comprehend. Furthermore,we observe that LLM-as-predictor methods often encounter uncontrollable output formats, despite employing instruction tuning. To maintain a lightweight tuning, existing methods typically freeze the parameters of LLMs and only fine-tune an adapter or projection layer. Such limited parameter space leads to uncontrollable and unexpected outputs (i.e., output incorrect or invalid labels), making it challenging to reliably extract the predicted labels and hinder the overall performance.

\(\) **Among the three categories, LLM-as-enhancer methods demonstrate the most robust performance across different datasets.** As shown in Figure 2, they achieve robust performance across datasets, considering both accuracy and F1 score. Among these methods, ENGINE  demonstrates the most outstanding effectiveness, consistently outperforming other methods by achieving either the best or the second-best results on \(4\) datasets. This highlights the benefits of employing LLMs to enrich the textual attributes or enhance the quality of node embeddings in supervised scenarios.

\(\) **Aligning GNNs and LLMs can improve the effectiveness of both. Specifically, the aligned GNNs are more reliable.** The alignment of GNNs and LLMs offers an effective method for integrating LLMs into graph learning. As illustrated in Table 4, aligned models, whether using GNNs or LLMs as classifiers, consistently outperform traditional GNN-based and PLM-based methods. This highlights the benefits of coordinating their embedding spaces at specific stages, which can

    &  &  &  &  &  &  &  \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\  GCN  & **82.11** & **80.05** & 69.84 & 65.49 & 79.10 & 79.19 & 72.24 & 51.22 & 80.35 & 77.63 & 63.19 & 62.49 & 65.75 & 58.75 \\ GAT  & 80.31 & 79.00 & 68.78 & 62.37 & 76.93 & 76.75 & 71.85 & 52.38 & 79.73 & 77.40 & 61.97 & 61.78 & 63.88 & 58.60 \\ GraphSAGE  & 79.88 & 79.35 & 68.23 & 63.10 & 76.79 & 76.91 & 71.88 & 52.14 & 79.87 & 77.05 & 58.51 & 58.41 & 65.12 & 55.85 \\  Sent-BERT (22M)  & 69.73 & 67.59 & 68.39 & 64.97 & 65.93 & 67.33 & 72.82 & 53.43 & 77.07 & 75.11 & 57.31 & 57.09 & 63.07 & 56.68 \\ BERT (10M)  & 69.71 & 67.53 & 67.77 & 64.10 & 63.69 & 64.93 & 72.29 & 53.30 & 78.55 & 75.74 & 58.41 & 58.33 & 63.75 & 57.30 \\ RoBERTa (355BM)  & 69.68 & 67.33 & 68.19 & 64.90 & 71.25 & 72.19 & 72.94 & 52.70 & 78.67 & 76.16 & 57.17 & 57.10 & 63.57 & 56.87 \\  GIANT  & 81.04 & 80.13 & 65.82 & 63.21 & 76.89 & 76.95 & 72.04 & 50.81 & 80.48 & 78.67 & 64.67 & 64.64 & 66.01 & 56.11 \\ TAPE  & 80.95 & 79.79 & 66.06 & 61.84 & 79.87 & 79.30 & 72.99 & 51.43 & 82.33 & 80.49 & 60.73 & 60.50 & 68.55 & 50.49 \\ OFA  & 75.24 & 74.20 & **73.04** & **68.98** & 75.61 & 75.60 & 73.23 & 57.38 & 77.34 & 74.97 & 64.86 & 64.95 & 60.85 & 55.44 \\ ENGINE  & 81.54 & 79.82 & 72.15 & 67.65 & 74.74 & 75.21 & 75.01 & 57.55 & 81.19 & 79.08 & 63.20 & 59.34 & **67.62** & **59.22** \\  InstructGLM  & 69.10 & 65.74 & 51.87 & 50.65 & 71.26 & 71.81 & 39.09 & 42.65 & 45.73 & 42.07 & 55.78 & 53.24 & 57.94 & 54.87 \\ GraphText  & 76.21 & 74.51 & 59.43 & 56.43 & 76.44 & 75.11 & 49.47 & 24.76 & 67.35 & 64.55 & 61.86 & 61.46 & 62.64 & 54.00 \\ GraphAdapter  & 72.85 & 70.66 & 69.57 & 66.21 & 72.75 & 73.19 & 74.45 & 56.04 & 70.85 & 66.49 & 61.21 & 61.13 & 67.40 & 58.40 \\ LLGaG  & 74.42 & 72.50 & 55.73 & 54.83 & 52.46 & 68.82 & 72.78 & 53.86 & 73.88 & 70.90 & **67.19** & **67.18** & 62.94 & 54.62 \\  GLEM\({}_{}\) & **82.11** & 80.00 & 71.16 & 67.62 & 81.72 & 81.48 & **76.43** & **58.07** & **82.40** & **80.54** & 59.60 & 59.41 & 66.10 & 54.92 \\ GLEM\({}_{}\) & 73.79 & 72.00 & 68.78 & 65.32 & 79.18 & 79.25 & 74.03 & 58.01 & 80.23 & 78.30 & 57.97 & 57.56 & 65.00 & 54.50 \\ Patton  & 70.50 & 67.97 & 63.60 & 61.12 & **84.28** & **83.22** & 70.74 & 49.69 & 80.81 & 77.72 & 59.43 & 57.85 & 64.27 & 57.48 \\   

Table 4: Accuracy and Macro-F1 results (%) under the supervised setting of GLBench. The highest results are highlighted with \(\), while the second-best results are marked with \(\) underline.

Figure 2: Comparison of the supervised performance among all methods. The color of the box plot represents the average score for each metric, while the central line within the box indicates the median score. We exclude results falling below 50% as they significantly deviate from the data center.

[MISSING_PAGE_EMPTY:8]

by aggregating features from their immediate neighbors. Finally, the class that yields the highest similarity score with the node embedding is predicted to be the class of the node. Through this simple design, we introduce both structural and semantic information at minimal cost. Surprisingly, as shown in Table 5, this training-free baseline can even outperform several GraphLLM methods tailored for zero-shot scenarios that require pre-training, such as OFA  and GraphGPT , across all datasets. The effectiveness of this baseline is further demonstrated by the fact that even a relatively small LLM (i.e., we use Sentence-Bert in Table 5) can achieve impressive results. This finding motivates the potential of developing efficient and effective GraphLLM methods for zero-shot graph learning, ultimately pushing the boundaries of graph foundation models.

### Time and Memory Efficiency (RQ3)

In this section, we analyze the efficiency and memory cost of representative GraphLLM algorithms on datasets from multiple domains. Figure 3 clearly demonstrates that the current GraphLLM methods generally have higher time and space complexity compared to traditional GNNs. This limitation restricts the application of GraphLLM on large-scale graphs. We can observe that some methods, especially LLM-as-enhancer methods, can achieve relatively good performance improvement with less complexity increase. Besides, although some algorithms (e.g., GLEM  and ENGINE ) achieve remarkable effectiveness improvement, they largely increase the complexity of time and space. Given these findings, it is important to address the efficiency problem to ensure that GraphLLM methods can be deployed successfully in a wide spectrum of real-world scenarios.

## 5 Conclusion

This paper introduces GLBench, a comprehensive benchmark for the emerging GraphLLM paradigm, by reimplementing and comparing different categories of existing methods across diverse datasets. The fair comparison unearths several key findings on this promising research topic. Firstly, GraphLLM methods achieve better results than traditional GNN- and PLM-based methods in supervised settings. In specific, LLM-as-enhancer methods show the most robust performance across different datasets. Aligning GNNs and LLMs can enhance the effectiveness of both, and the aligned GNNs seem more reliable. However, employing LLMs as predictors has proven to be less effective and tends to have output format issues. Secondly, experimental results indicate that there is no obvious scaling law in existing GraphLLM methods. Thirdly, both structures and semantics are crucial for zero-shot transfer, and we find a simple baseline that can even outperform several existing methods tailored for zero-shot scenarios. Lastly, we highlight the efficiency problem of the existing methods. We believe GLBench will have a positive impact on this emerging research domain. We have made our code publicly available and welcome further contributions of new datasets and methods.

Figure 3: Training time and space analysis on Cora, Citeseer, WikiCS, and Instagram.

**Acknowledgement.** This work was supported by National Key Research and Development Program of China Grant No. 2023YFF0725100 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673. We want to thank Haitao Mao at Michigan State University, Xinni Zhang and Zhixun Li at The Chinese University of Hong Kong, Wenxuan Huang at Zhejiang University, Ruosong Ye at Rutgers University, and Caiqi Zhang at Cambridge University for their valuable insights into the field of GraphLLM, and constructive comments on this paper.