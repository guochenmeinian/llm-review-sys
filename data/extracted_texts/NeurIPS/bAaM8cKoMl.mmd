# MindSet: Vision. A toolbox for testing DNNs on key psychological experiments

Valerio Biscione\({}^{1}\) Dong Yin\({}^{1}\) Gaurav Malhotra\({}^{2}\) Marin Dujmovic\({}^{3}\)

Milton L. Montero\({}^{4}\) Guillermo Puebla\({}^{5}\) Federico Adolfi\({}^{1,6}\)

**Rachel F. Heaton\({}^{7,8}\) John E. Hummel\({}^{5}\) Benjamin D. Evans\({}^{9}\) Karim Habashy\({}^{1}\) Jeffrey S. Bowers\({}^{1}\)**

\({}^{1}\)School of Psychological Science, University of Bristol

\({}^{2}\)University at Albany, State University of New York

\({}^{3}\)School of Physiology, Pharmacology and Neuroscience, University of Bristol

\({}^{4}\)IT University of Copenhagen

\({}^{5}\)Instituto de Alta Investigacion, Universidad de Tarapaca, Chile

\({}^{6}\)Ernst Sturngmann Institute for Neuroscience, Max-Planck Society, Germany

\({}^{7}\)Siebel Center for Design, University of Illinois Urbana-Champaign

\({}^{8}\)Department of Psychology, University of Illinois Urbana-Champaign

\({}^{9}\)School of Engineering and Informatics, University of Sussex

###### Abstract

Multiple benchmarks have been developed to assess the alignment between deep neural networks (DNNs) and human vision. In almost all cases these benchmarks are observational in the sense they are composed of behavioural and brain responses to naturalistic images that have not been manipulated to test hypotheses regarding how DNNs or humans perceive and identify objects. Here we introduce the toolbox _MindSet: Vision_, consisting of a collection of image datasets and related scripts designed to test DNNs on 30 psychological findings. In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception and object recognition. In addition to providing pre-generated datasets of images, we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts, and code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method), accessible at https://github.com/ValerioB88/mindset-vision. We test ResNet-152 on each of these methods as an example of how the toolbox can be used.

## 1 Introduction

Deep neural networks (DNNs) provide the best solution for visual identification of objects short of biological vision, and many researchers claim that DNNs are the best current models of human vision and object recognition [70; 79; 120; 38; 53]. Key evidence in support of this claim comes from the finding that DNNs perform the best on various behavioural and brain benchmarks. In the case of behavioural benchmarks, models are assessed on how well they account for human (or macaque)errors in classifying a large set of objects , or how well they predict human similarity judgements . In the case of brain benchmarks, models are assessed with regard to how well they predict brain recordings (e.g., single-cell responses or fMRI data) in response to a set of objects . The general assumption is that the better a model does at predicting the data, the more similar the model is to biological vision. For instance, the Brain-Score benchmark is described as "a composite of multiple neural and behavioural benchmarks that score any [artificial neural network] on how similar it is to the brain's mechanisms for core object recognition" 

A common feature of most benchmark studies is that they treat the to-be-predicted data as observational. That is, there is rarely an attempt to predict the impact of experimental manipulation designed to test specific hypotheses about how human or machine vision works. Rather, observers perform a single task over a set of images that satisfies some general criterion, such as objects presented in isolation , in naturalistic contexts , or on a range of arbitrary backgrounds . This approach is problematic because it is possible to make good predictions on these datasets even when models identify objects in a qualitatively different way from monkeys or humans . For example, if the images contain multiple diagnostic cues for object classification (e.g., shape and texture both predict object category), then good predictions might be driven by different features than those that drive human object recognition - that is, predictions might be driven by confounds. For example, a DNN that classifies objects by texture might still be able to predict brain activations in a visual system that classifies objects by shape.

The standard way to rule out confounds in order to determine causal relations (e.g. inferring that DNNs learn brain-like representations) is to carry out experiments designed to rule out confounds as the basis of making good predictions. In fact, there is a large literature in psychology describing experiments designed to test specific hypotheses about how human vision works, but surprisingly, this literature is often ignored when modellers compare DNNs to biological vision. Bowers et al. reviewed a wide range of psychological phenomena that current DNNs either fail to capture or that have yet to be considered. Furthermore, when researchers do consider the psychological literature when making claims regarding DNN-human similarities, the models are rarely subject to the kind of "severe" tests that are required to make any strong conclusions, that is tests that are likely to challenge claims in case they are false. Instead, strong conclusions are often drawn based on superficial similarities .

There are at least four (related) reasons for this. First, many researchers in computer science and computational neuroscience may be unfamiliar with the rich set of experiments carried out in psychology that manipulate independent variables to better understand human vision, memory, language, etc. Those who are aware of these studies might find it challenging to engage with them, as psychological datasets are not readily available in formats that the community is accustomed to working with. Second, it is not always obvious how one should test a model against psychological data. Hence, it may be easier to focus on improving performance on the current benchmarks, and this may have discouraged researchers from exploring data from psychology. A third potential reason is an overall skepticism towards psychological results, a sentiment that may reflect the well-documented replication crisis in psychology . Forth, there is a strong bias to look for DNN-human similarities and downplay the differences , and severely testing on psychological data might not result in similarities. However, characterizing these failures provides key insights into the ways DNNs need to be improved when modelling biological vision.

Here we present _MindSet: Vision_, a toolbox aimed at facilitating testing DNNs on visual psychological phenomena by addressing all the problems presented above: our main contribution is to provide a large, easily accessible, parameterized, set of 30 image datasets (and related scripts to re-generate and modify them) accounting for a wide array of well-replicated visual experiment and phenomena reported in psychology. Our stimuli cover aspects of low and mid-level vision (including Gestalt phenomena), visual illusions, and object recognition tasks. We provide a high-level descriptions of the visual phenomena in the main text (Section 2) and more detailed descriptions in the Appendix (A). To facilitate experimentation across a variety of scenario, each dataset can be easily regenerated across different configurations (image size, background colour, stroke colour, number of samples,etc.). To address the difficulty in testing DNNs on these stimuli, we provide scripts for using one (or more) of three methods: Similarity Judgment Analysis, Decoder Approach, and Out-of-Distribution classification (Section 3). We provide examples illustrating how to use these scripts with a classic feed-forward CNN (ResNet-152), and an extensively documented code (Section 4).

With _MindSet: Vision_, we aim to bridge the gap between computational modeling and psychological research, bringing experimental studies that manipulate independent variables to the forefront of developing and evaluating of DNN models of human vision. We also hope this initiative will drive further interest in other areas of human psychology, such as memory, language, and speech perception when attempting tounderstand and replicate human-like intelligence in machines.

### Related Work

Several recent studies share some similarities with our project:  introduced a new dataset containing five types of visual illusions falling into two categories: color constancy and geometrical illusions. The authors formulated four tasks specifically designed to examine the performance of Visual Language Models, finding low alignment with human responses.  developed the Good Gestalt datasets, consisting of six types of datasets covering several types of Gestalt grouping principles, including Closure, Continuity, and Proximity, aimed at testing a Latent Noise Segmentation Network. Similarly,  developed the model-vs-human benchmark that compares ANN-human classification errors on various 'out-of-distribution' datasets composed of naturalistic images that were modified in various ways, including low-level feature manipulations of contrast and spatial frequency, as well as higher-level manipulations, such as generating silhouettes and sketches of images. Evans et al.  used a dataset of silhouettes, line-drawings, and contours, to investigate robustness to these stimuli in DNNs pretrained on CIFAR-10, and Baker et al.  employed a dataset of line drawings and silhouettes from ImageNet classes to investigate model robustness to local versus global features. In comparison to these works, we present a toolbox to test DNNs on visual psychological effects, investigating not only a much richer set of visual phenomena, but providing the code base to regenerate images in batches, changing the parameters, and testing each one of them on a variety of methods.

## 2 Datasets

We have included datasets from experiments that characterize a wide range of visual phenomena, ranging from low- to high-level vision. We grouped the datasets (indicated in **bold**) into 3 broad categories (see following Sections) as illustrated in Figure 1. Each dataset comprised multiple sub-conditions designed to test DNN-human similarities, and in some cases, image datasets used to train decoders, as described in Section 3.3.

While most of the stimuli are created by us, in a few instances we incorporate stimuli from external sources (when needed, permission was obtained from the authors). In all cases, the stimuli have been integrated into a versatile framework which offers significant flexibility in adjusting parameters such as image size, background, stroke colour, and more, to allow their application to a variety of models and methodologies. Given the extensive range of datasets provided, we only offer a brief summary for each in the article, and provide more details in Appendix A, including details about the suggested way to test each dataset, and the expected result for model-human perceptual alignment. All resources are open-source and freely available under the MIT license at https://github.com/ValerioB88/mindset-vision.

### Low and Mid-Level Vision

A fundamental low-level vision phenomena is captured by **Weber's Law**, which states that the minimum physical change of a stimulus on some dimension (e.g., its size) that is noticeable to an observer is a constant ratio of the original stimulus value on this dimension. For example, it is equally easy to distinguish between line lengths of 1 and 2 cms and between 2 and 4 cms. We created a dataset that can be used to assess this relation for both line length and stimulus intensity.

Figure 1: Comprehensive overview of the ‘MindSet: Vision’ datasets, arranged in three main categories. Each panel represents a distinct dataset, which is further divided into conditions. The images provide examples from these conditions, generated with default parameters.

Human perception is also sensitive to various **Emergent Features** in which simple image features interact to generate "Gestalts" . The dataset is comprised of a set of dots arranged in such a way as to induce the emergent feature of proximity, orientation, and linearity . Another Gestalt effect is manifest in the **Crowding/Uncrowding** phenomenon. In crowding, the ability to identify an object is compromised by the presence of nearby objects or visual patterns, but in uncrowding, object identification is improved when additional objects or visual patterns are added to the scene and grouped such that they are segregated from the target. We adapt the dataset from  so that many crowding conditions with several different shapes can be investigated.

Human perception is highly sensitive to non-accidental image features, that is features that are largely invariant to changes in viewpoint when projected on to the retina , as opposite to accidental features (e.g., degree of curvature) in which the projected image varies with viewpoint. Human vision is known to be more sensitive to changes in images that alter non-accidental compared to accidental properties . We use two datasets to examine model sensitivity to these features: one with **3Dgeons** and another with **2D line segments** (based on ). Similarly, we present a dataset to compare **Relational Changes** with **Coordinate changes** between object parts. DNNs are commonly insensitive to relational change, even after being explicitly trained on these relations , whereas human perception is highly sensitive to relational changes .

To identify partly occluded objects, the human visual system groups contours and surfaces through an amodal completion process . The **Amodal Completion** dataset (based on ) enables the investigation of these processes using images with shapes that are either occluded, unoccluded, or "notched". These latter shapes are unoccluded but notched in such a way as to maintain a high degree of feature similarity with their occluded counterparts.

With the **Decomposition** dataset we provide a mean to test the extent to which DNNs group object parts in a human-like fashion. We designed familiar and unfamiliar objects composed of two conjoined parts that undergo what humans would perceive as a "natural" or "unnatural" break (inspired by ). In the same work,  showed that VGG-16 trained on ImageNet did not possess human-like sensitivity to images that could be interpreted as 3D-shapes by using a set of stimuli based on . Accordingly, we reconstructed this **Depth Drawings** dataset.

### Visual Illusions

Visual illusions are not mere curiosities, but often arise from adaptive perceptual processes . Detailed computational models of multiple illusions have been advanced that provide theoretical insights into the mechanisms that underlie them (e.g. ). We provide datasets exploring illusions related to size perception, orientation, and lightness contrast.

Several illusions relate to size perception. In the **Muller-Lyer illusion**, arrow-like segments at the ends of equal-length lines impact our perception of length. In the **Ponzo illusion**, two equal-length horizontal lines cross a pair of converging lines. In this configuration, the top line looks longer, an illusion often explained as related to the process of inferring depth. In the **Ebbinghaus illusion**, the size of a circle is perceived differently depending on the size of surrounding circles. Similarly, in the **Jastrow illusion**, a specific arrangement of identical objects affects our perception of their relative size. For all these illusions, we provide both an illusory condition and a condition in which all elements of the original illusion are "scrambled up", so that a decoder can be trained to predict a specific feature (e.g. the size of the centre circle in the Ebbinghaus illusion) and subsequently tested on illusory configurations.

In the **Tilt illusion**, the orientation of a central grating is perceived as being repulsed from or attracted to the orientation of a surrounding grating. In this case, we provide conditions with either central or background gratings (configurations which do not support the illusion in humans and could be used for training a decoder), and a condition with both (eliciting the illusion in humans) for testing the model. Another orientation illusion is the **Thatcher Effect**. This is a phenomenon where local changes in facial features (like inverted eyes or mouth) are less noticeable when the entire face is upside down, highlighting our sensitivity to orientation in face perception. An interesting unresolved issue is the extent to which this inversion effect is specific to faces [25; 115]. Together with a dataset of faces and their Thatcherized version, we also include a dataset of **Thatcherized Words**, that is a dataset of images containing words in which one or more letters are rotated by 180 degrees .

The **lightness contrast effect** and the **Adelson Checker shadow** illusions reveal how our visual system perceives color and lightness based on context. We provide a **Grayscale Shapes dataset** to train a decoder to output estimates of lightness at a given location of an image (indicated by a small white arrow). After training, the network is presented with test images that induce illusions and help assess whether DNNs show similar effects by pointing the arrow at the relevant parts of the images (see Section C.2.6 for a detailed description of this approach).

It is important to note that there is no accepted account for some of the illusions described above. However, even when we have no good understanding of the functional role or the mechanism that drives an illusion, a DNN model of human vision should show similar effect. Indeed, understanding the conditions under which DNNs show an illusion may advance our understanding of why the phenomenon is observed in humans. There are now several articles exploring such illusions in various types of DNNs trained in different ways, with some highlighting similarities (e.g., [14; 103; 111]) others reporting mixed or discrepant results (e.g., [55; 110; 119]); for a review of the relevant findings, see .

### Shape and Object Recognition

DNN object recognition is much more sensitive than human vision to distributional shifts from the training set. For instance, humans can easily identify line drawings the first time they are exposed to them , whereas DNNs perform poorly under these conditions  and need to be trained on line drawings in order to recognize them at human levels . We have included the **line drawing** and **silhouettes** datasets (from [10; 8]) and also manipulated them in various ways to construct additional datasets. The line drawings were converted into dotted contours (**Dotted line drawings**), line segments (**Segments line drawings**) , or "texturized" (**Texturized line drawings**). The texturized images are composed of oriented lines/characters applied to either/both the background or/and the inside area of the line drawing. In all these cases, the resulting images are easily identifiable by human observers due to various Gestalt rules that organize the image features into boundaries. We also apply the same texturization technique outlined above on unfamiliar "blob"-like shapes (**Texturized Unfamiliar dataset**). Human observers have no difficulty matching a novel "blob" object to its texturized counterpart. In addition, we provide a dataset of fragmented images based on  in which the global features of silhouettes or line drawing are modified by reflecting the top part of an object along its vertical axis, leaving the local features mostly unchanged (**Global Modifications** dataset). Human performance on these stimuli is greatly reduced but typically DNN performance is largely unchanged, suggesting that human vision is more sensitive to global object structure and DNN vision is more sensitive to local features.

The **Embedded Shapes Dataset** (inspired by ) provides another condition that greatly impacts on human perception, by embedding geometric shapes within complex arrays of lines in ways that camouflage the original shape. We include both the original images from  and a procedurally generated dataset in which random polygons are embedded into a configuration that makes recognition challenging for humans.

The human visual system supports object recognition following a wide variety of transformations [104; 24]. Importantly, this extends to cases in which an object has only been viewed at one pose. Previous works suggest a complex link between DNN pretraining and their object recognition capabilities under object transformations [20; 21]. To test whether DNNs share these capacities, we provide a dataset in which translations, plane rotations, and scale changes (**2D Transformations**) are applied to line drawings. To test for **Viewpoint Invariance** (e.g. the ability to recognize an object from a new viewpoint after a rotation in depth) we adapt the ETH-80 dataset,  allowing for controlled variation in azimuth and inclination.

Finally, we provide a dataset to test whether DNNs possess the ability to solve a basic form of visual reasoning task, namely, the **Same/Different** task. Drawing from , our dataset comprises images composed of pairs of objects, which may be identical or different. These images are organized into ten conditions that vary in their visual form, such as 'filled polygons', 'open squares', and 'colored shapes'. While humans effortlessly accomplish this task across all conditions without training, DNNs often struggle when the training and test images come from different conditions.

## 3 Testing methods.

Each dataset is designed to align with at least one of three methods of testing, but other approaches can be used as well. We discuss further possibilities in Appendix B.

### Out-of-Distribution Classification

In this approach, a DNN pretrained on one dataset is tested on a new dataset composed of out-of-distribution images taken from the trained classes (e.g., a DNN pre-trained on ImageNet is tested on line drawings taken from the same categories). This approach is well suited for most of the Shape and Object Recognition datasets that use images from ImageNet categories modified in such a way that human observers have no trouble recognizing them, even without training. We provide scripts to test a wide variety of vision models.

### Similarity Judgment Analysis

This method involves assessing the pairwise similarity of activation patterns in DNNs (using a Cosine Similarity or an Euclidean Distance metric) evoked by pairs of images and comparing these similarities to human performance. This method has been used to assess how well DNNs capture human similarity judgments  and response times to identify target stimuli from foils . It is often useful to carry out these analyses across multiple layers of DNNs given that some psychological phenomena are known to manifest at earlier or later stages of visual processing. A DNN mimicking human perception should show relevant similarity effects at the relevant layers. One key advantage of this approach is that it can be applied to novel images that cannot be classified by a DNN.

To illustrate, we applied this method to the Texturized Unfamiliar dataset (Figure 2). The human visual system groups elements in a scene by texture  and classify objects by their shapes . Accordingly, texturized versions of the same shape should be judged as more similar than texturized versions of different shapes. To explore if DNNs exhibit similar behaviour, we input pairs of images into a ImageNet pre-trained ResNet-152 and, for each pair, we computed the Euclidean Distance between their internal activations at every processing level. A human-like response is indicated by a smaller distance for pairs of the same compared to different shapes. ResNet-152 exhibited a weak manifestation of this pattern in the early layers, a reduced effect in the later layers, and no effect in the output layer. By contrast, the human visual system supports similarity judgements on the basis of shape-based representations that are computed following the early stages of visual processing.

### Decoder Method

In this method a small, often single-layer, "decoder" network is attached to a layer of a frozen DNN and trained on a task designed to reveal how the DNN encodes a specific type of information. For instance, a frozen DNN might be presented with a set of images that contain a target object varying in size, colour, and orientation, and a decoder is trained to output the value of one or more of these properties at a given layer. We provide scripts for both classification and regression training, and scripts to train and test a series of five decoders at varying levels of a ResNet-152 model. Although these scripts are tailored to ResNet-152, they can easily be used as a template to streamline the adaptation of this technique for different networks.

To illustrate, consider the Ebbinghaus Illusion. The Ebbinghaus dataset we provide consists of three conditions: two illusory conditions in which a red centre circle (at different radii) is surrounded by either small or large white circles (flankers) in a configuration that, in humans, induces a biased size estimation of the centre circle: the circle appears larger when surrounded by small flankers, everything else being equal. Another condition again contains a red centre circle of different sizes, but the surrounding circles are placed randomly on the canvas so that they would not elicit any illusion on a human observer. We use the latter condition to train decoders attached to a ImageNet pre-trained ResNet-152 model with frozen weights. The task consists of estimating the size of the centre circle. After training, we feed the illusory images to the decoders. For a network to exhibit the Ebbinghaus visual illusion, the size of the centre circle should be overestimated for small flankers and underestimated for big flankers. We did not find this pattern in ResNet-152 and, indeed, no significant difference across prediction errors for the different conditions was observed (result for one decoder shown in Figure 2).

## 4 Code and Resources

We provide both ready-to-use datasets and scripts to generate them with varying parameters. Most of the ready-to-use datasets' size span to around 5,000 images per condition, and larger dataset can easily be generated using the provided scripts. To separate code and configuration, each dataset generation script relies on a configuration file, consisting of a plain-text file in TOML format specifying all the available parameters for that dataset. Some parameters are used across most datasets, such as image size, background colour, and number of samples. Other parameters are dataset-specific, for example the size and distance of dots in the Dotted line drawing dataset. For convenience, the same configuration file can specify the configuration for multiple (or all) datasets, so that they can be generated in batches. The "default" configuration file we used to generate the ready-to-use versions is included, which can be used as a template. The output of each script is the dataset itself (with several sub-conditions depending on the dataset) together with a CSV annotation file, containing the path and parameters of each generated image.

Figure 2: Depiction of two of the three proposed methods of evaluating DNNs in the context of two representative datasets. The first method, out-of-distribution classification, is not depicted here. The Similarity Judgment Analysis (top panel) involves feeding pairs of images to DNNs and comparing the elicited internal representations. We illustrate this method via the ’Texturized Unfamiliar’ dataset, showing that the network possesses human-like responses in earlier layers which diminish in the later ones. The Decoder Method (bottom panel) involves training and testing a simple linear layer attached to different stages of a frozen network. In the given example, we assess the response to the Ebbinghaus illusion. Our findings indicate an absence of illusory perception. Both examples use an ImageNet pre-trained ResNet-152.

We also provide the code and utilities to evaluate DNNs using the three methods noted above. Each method is highly configurable through TOML files, with options including the type of data-augmentation to apply, the network architecture, the metric to use for the similarity judgments and more. Users have the flexibility to choose specific factors from this file for analysis, extending beyond the factors that we deemed the most relevant for each task. For example, in the script testing the Ebbinghaus Illusion used for Section 3.3, a decoder is trained to predict the normalized size of the centre circle. However, a different research goal might involve predicting the size of the flankers. This can be achieved by simply specifying the corresponding column ('NormSizeFlankers') in the annotation file, without needing to re-generate the dataset or change the code.

Each method produces a pandas DataFrames  as its output, which can be independently analyzed. Additionally, supplementary files containing simple tests and comparisons that serve as a springboard for further and more detailed analysis are automatically generated. Comprehensive documentation for every configurable option across all datasets and methods. Additionally, we offer guidance on the general usage of various scripts and utilities through several examples and multiple README files on the GitHub page.

## 5 Limitations

While _MindSet: Vision_ offers a valuable resource for exploring visual psychological phenomena using deep neural networks, there are several limitations to consider. Firstly, our focus is primarily on visual tasks that do not involve high levels of reasoning and are not directly connected with other areas of cognition such as language and memory. Secondly, the methodology for comparing DNN performance to human participants often allows only for qualitative comparisons, as quantitative comparisons may not be feasible with the current analysis methods. Lastly, while we have selected phenomena based on well-replicated and famous visual experiments, there may be additional phenomena that are not covered by our selection. These limitations underscore the need for further research and development in the field of computational modeling of human vision to address these gaps and enhance the utility of _MindSet: Vision_ as a comprehensive toolbox for studying visual perception.

## 6 Conclusion

There is much interest in DNNs as models of human vision, but relatively little research is concerned with how DNNs capture key psychological findings. When DNNs are tested against key psychological findings, they often fail . And when they do succeed, it is often because the DNNs have not been severely tested . In our view, to better characterize DNN-human alignment, and to build better DNN models of human vision, it is necessary to systematically test models against key experiments reported in psychology. The MindSet: Vision dataset is designed to facilitate this.

Currently it is quite common to rank models in term of how well they perform across several datasets or tasks. For example, the Brain-Score benchmark  provides an overall leaderboard that scores any DNN in terms of how good they are at explaining neural activity variance for core object recognition, and the "model-vs-human" benchmark  ranks and scores models in terms of their behavioural overlap with humans in identifying a range of out-of-distribution object datasets. We do not propose to rank models in this way as each experiment in _MindSet: Vision_ tests a specific hypothesis regarding how DNNs and humans perceive and encode visual inputs. It makes little sense to provide a score that averages across qualitatively different hypotheses. By making stimuli underlying psychological experiments more accessible, easy to generate, configure, and modify, and by providing ready-to-use scripts to test existing models, we hope that the _MindSet: Vision_ toolbox encourages computational modelling researcher to focus on testing their models on key experiments rather than competing on observational datasets that do not support any conclusions regarding the mechanistic similarity of DNNs and brains.