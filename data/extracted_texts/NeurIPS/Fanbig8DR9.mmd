# Euclidean distance compression via deep random features

Brett Leroux

Department of Mathematics

University of California, Davis

Davis, CA 95616

lerouxbew@gmail.com &Luis Rademacher

Department of Mathematics

University of California, Davis

Davis, CA 95616

lrademac@ucdavis.edu

###### Abstract

Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps \(_{}\) that compress point sets in the following way. For a point set \(S\), the map \(_{}:^{d} N^{-1/2}\{-1,1\}^{N}\) has the property that storing \(_{}(S)\) (a _sketch_ of \(S\)) allows one to report squared distances between points up to some multiplicative \((1)\) error with high probability. The maps \(_{}\) are the \(\)-fold composition of a certain type of random feature mapping.

Compared to existing techniques, our maps offer several advantages. The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma and involves compressing point sets with a random linear map. The main advantage of our maps \(_{}\) over random linear maps is that ours map point sets directly into the discrete cube \(N^{-1/2}\{-1,1\}^{N}\) and so there is no additional step needed to convert the sketch to bits. For some range of parameters, our maps \(_{}\) produce sketches using fewer bits of storage space. We validate the method with experiments, including an application to nearest neighbor search.

## 1 Introduction

Random projection is a commonly used method to lower the dimension of a set of points while maintaining important properties of the data . The random projection method involves mapping a high-dimensional set of points in \(^{d}\) to a lower dimensional subspace by some random projection matrix in such a way that the pairwise distances and inner products between points are approximately preserved. The random projection method has many applications to data analysis and a variety of prominent algorithms [11; 17; 5; 8] including nearest neighbor search [13; 12].

The theoretical foundation of random projection is the Johnson-Lindenstrauss lemma which states that a random orthogonal projection to a lower dimensional subspace has the property of preserving pairwise distances and inner products . Later it was observed [9; 13] that one can alternatively take the projection matrix to be a matrix with i.i.d. Gaussian \(N(0,1)\) entries.

**Lemma 1** ([9; 13; 30]).: _Let each entry of an \(d k\) matrix \(R\) be chosen independently from \(N(0,1)\). Let \(v=}R^{T}u\) for \(u^{d}\). Then for any \(>0\), \(v\|^{2}-\|u\|^{2}\|u\|^{2}< 2e^{-(^{2}-^{3})k/4}\)._

A corollary of the above lemma is that if an arbitrary set of \(n\) points in \(^{d}\) is mapped by the random projection matrix \(R\) to \(^{k}\) where \(k=(^{-2} n)\), then the squared distances between pairs of points are distorted by a factor of at most \((1)\) with high probability. The projected points are thus a lower dimensional representation of the original point set and this lowering of the dimension offers two main advantages. The first is that algorithms that were originally intended to be performed on the original point set can now instead be performed on the lower dimensional points.

The second main advantage of the lower dimensional representation is a reduction in the cost of data storage. The Johnson-Lindenstrauss lemma shows that random projection of a point set \(S\) produces a data structure (called a _sketch_) that can be used to report the squared distances between pairs of points up to a multiplicative \((1)\) error. The size of the sketch of course depends on both \(|S|\) and \(\). From this viewpoint, it is natural to ask what is the minimum number of bits of such a sketch? The Johnson-Lindenstrauss lemma gives an upper bound on the size of such a sketch as follows: Any set of \(n\) points \(S\) can be projected to \(^{k}\) where \(k=(^{-2} n)\) while distorting pairwise squared distances by a factor of at most \((1)\). The projected data points are real-valued, and thus the projected data points needs to be encoded into bits in such a way that guarantees squared distances are preserved. One way to convert to bits is to use an epsilon-net for the unit ball in \(^{k}\): In order to preserve squared distances up to a multiplicative \((1)\) error, it suffices to preserve squared distances up to an additive \(m^{2}\) error where \(m\) is the minimum distance between pairs of points in \(S\). By identifying each projected point with the closest point in an \(m^{2}\)-net, we can produce a sketch with \(n^{-2} n(1/m^{2})\) bits.1

While the Johnson-Lindenstrauss lemma shows that efficient sketches can be obtained by mapping the points to a lower dimensional space with a random linear mapping (the projection), it is natural to ask if there are other types of random maps (in particular, possibly nonlinear maps) which are able to produce sketches with a smaller number of bits. Our main result shows that this is possible in certain cases by using the composition of random feature maps. We state our main result first for sets of points contained in the unit sphere \(S^{d-1}\) and at the end of this section we include the extension to subsets of the unit ball.

**Theorem 2**.: _Let \(S S^{d-1}\) with \(|S|=n 2\). Let \(m=_{x,y S,x y}\|x-y\|\) and \(=_{2}_{2} 1\). Let \(>0\) and assume that \(<_{x,y S,x y}1-| x,y|\). Then the random map \(_{}:S^{d-1}}\{-1,1\}^{N}\) with \(N=}()^{2_{2}(/ )}\) (defined in the proof of Theorem 5 and independent of \(S\) except through parameters \(n,d,m\) and \(_{x,y S,x y}1-| x,y|\)) satisfies the following with probability at least \((1-)^{}\): \(_{}(S)\) is a sketch of \(S\) that allows one to recover all squared distances between pairs of points in \(S\) up to a multiplicative \((1)\) error. The number of bits of the sketch is \(}()^{2_{2}(/ )}\)._

The proof of Theorem 2 is explained in Section 3 and the final details of the proof are in Appendix A. We explain how the map \(_{}\) is constructed in the next subsection and the role of the parameter \(\) is discussed in Section 1.2. The main advantage of the map \(_{}\) is that it maps the point set \(S\) directly into the discrete cube and thus there is no need to convert the sketch to bits after performing the random mapping. Furthermore, the map \(_{}\) produces sketches with asymptotically fewer bits than those obtained using the Johnson-Lindenstrauss lemma if \((())^{2_{2}(/)}=o( })\). This is equivalent to the condition that \((())^{2_{2}(/)}=o() \).

Furthermore, the sketch \(_{}(S)\) has the desired properties "with high probability". The probability that the sketch succeeds is \((1-2/n)^{}\) and we claim that this quantity approaches one for all useful choices of the parameters: First of all, we recall that in all applications of Theorem 2, we should take \(n=(d)\). We also take \(=(d^{-1/2})\) (ignoring logarithmic factors) because otherwise the target dimension \(N\) is larger than \(d\) and then a sketch based on an \(\)-net (as above, but without random projection) for \(S\) would be better, by having bitlength \(nd\) (up to logarithmic factors). The assumption that \(<1-| x,y|\) for all \(x,y\), \(x y\) means that \(<m^{2}\) and so \(1/m<^{-1/2}=O(d^{1/4})\). This means that \(=O(_{2}_{2}d)\) and so \((1-2/n)^{}=(1-2/d)^{}\) approaches one.

We remark that it might be possible to replace the assumption in Theorem 2 that \(<1-| x,y|\) for all \(x,y S\), \(x y\) by the weaker assumption that \(<1- x,y=\|x-y\|^{2}\). The reason that an assumption of this sort is necessary is that, because we are trying to produce sketches which allow recovery of squared distances, pairs of points with very small distance are difficult to deal with. As a result, we need to assume for technical reasons that the accuracy parameter \(\) is smaller then (half) the minimum squared distance, i.e. \(<1- x,y\). There is an inherent symmetry in the maps \(_{}\) that we use which makes it convenient to use the stronger assumption that \(<1-| x,y|\).

We are able to extend our main result to deal with not only sets of points contained in the unit sphere, but also any set of points in the unit ball. For a point \(x\) in the unit ball \(B^{d}\) we use \(\) to denote \(x/\|x\|\).

**Theorem 3**.: _Let \(S B^{d}\) with \(|S|=n 2\) and set \(=_{x S}\|x\|^{2}\). Let \(m=_{x,y S,x y}\|-\|\) and \(=_{2}_{2} 1\). Let \(>0\) and assume that \(<_{x,y S,x y}1-|,|\). Then the random map \(_{}:S^{d-1}}\{-1,1\}^{N}\) with \(N=}()^{2_{2}(/ )}\) (defined in the proof of Theorem 5 and independent of \(S\) except through parameters \(n,d\) and \(_{x,y S,x y}1-|,|\)) satisfies the following with probability at least \((1-)^{}_{}()\) and the norm of each point in \(S\) up to an additive \( m^{2}/48\) error is a sketch of \(S\) that allows one to recover all squared distances between pairs of points in \(S\) up to a multiplicative \((1)\) error. Moreover the number of bits of the sketch is \(}^{2_ {2}(/)}+n}\)._

The proof of Theorem 2 is explained in Section 3 and the final details of the proof are in Appendix A. The number of bits of the sketches in the above theorem depend on the parameter \(m\) which is the minimum distance between pairs of points after all points have been normalized to have unit norm. Thus, it is more complicated to compare the number of bits of our sketches to the sketches obtained using the Johnson-Lindenstrauss lemma because the Johnson-Lindenstrauss sketches do not rely on any normalization step. However, as in the case of point sets on \(S^{d-1}\), there is a large family of point sets in \(B^{d}\) for which our sketching technique produces sketches with a fewer number of bits.

The maps \(_{}\) and the recovery of \(\|x-y\|^{2}\) by \(_{}(x)\) and \(_{}(y)\)

In this section we summarize the construction of the maps \(_{}\) which are used in Theorem 2 and formally analyzed in Theorem 5 and how they can be used to recover squared distances between points. Let \(f(t)=(t)\) and \(g(t)=()\). So \(f:[-1,1][-1,1]\) is the inverse of \(g:[-1,1][-1,1]\). For \(^{+}\), let \(f_{}\) be the function \(f\) composed with itself \(\) times and similar for \(g_{}\). Notice that for any \(^{+}\), \(f_{}:[-1,1][-1,1]\) is the inverse of \(g_{}:[-1,1][-1,1]\).

For simplicity in the rest of the introduction we assume all points are normalized to be on the unit sphere \(S^{d-1}\). We define the sign function as

\[(t)=1&t 0\\ -1&t<0.\]

The maps \(_{}\) will be defined as the composition of \(\) maps of the following form. Set \(D^{+}\). Let \(Z_{i}\), \(1 i D\) be i.i.d. standard Gaussian random vectors in \(^{d}\). Let \(^{D}:^{d}^{D}\) be defined by

\[^{D}(x)_{i}:=D^{-1/2}( x,Z_{i})\]

where \(^{D}(x)_{i}\) is the \(i\)th coordinate of \(^{D}(x)\).2

The maps \(_{}\) are now defined as the \(\)-fold composition of maps of the type \(^{D}\). That is, for some integers \(D_{1},D_{2},,D_{}\), we let \(_{1}:S^{d-1} D_{1}^{-1/2}\{-1,1\}^{D_{1}}\) be defined by \(_{1}(x)=^{D}(x)\). For \(j\{1,,-1\}\), we let \(_{j+1}(x)=^{D_{j+1}}(_{j}(x))\). Therefore, the map \(_{}\) maps \(S^{d-1}\) to \(D_{}^{-1/2}\{-1,1\}^{D_{}}\). To avoid writing the double subscript we write the final dimension of the map as \(D_{}=N\). We remark that the map \(_{}\) can also be defined as a standard neural network with \(\) hidden layers using activation function \((t)\) and having all weights be i.i.d. standard Gaussian random variables.

It was shown in  that for \(x,y S^{d-1}\), \(}( x,Z_{1}) {sign}( y,Z_{1})=f( x,y)\). Since \(^{D}(x),^{D}(y)\) is a sum of \(D\) independent copies of \(D^{-1}( x,Z)( y,Z)\) we get that \(}^{D}(x),^{D}(y)=f(  x,y)\).

Now we can explain how one recovers pairwise distances between points in \(S\) from \(_{}(S)\). Let \(S\) be a set of \(n\) points in \(S^{d-1}^{d}\). As in Theorem 2, we map \(S\) by \(_{}:S^{d-1} N^{-1/2}\{-1,1\}^{N}\). Here \(\) is some parameter which is chosen based on \(S\) that is explained below and \(N\) is chosen based on the desired \(\) error of the sketch. If the remaining integers \(D_{1},,D_{-1}\) are chosen properly,then we show that \(_{}(x),_{}(y)\) is a good approximation of \(f_{}( x,y)\) (Corollary 4). Since \(g_{}\) is the inverse of \(f_{}\) this implies that \(g_{}(_{}(x),_{}(y))\) should be a good approximation of \( x,y\). By the polarization identity, this implies that \(2-2g_{}(_{}(x),_{}(y))\) should be a good approximation of \(\|x-y\|^{2}\). So recovering \(\|x-y\|^{2}\) from \(_{}(S)\) simply involves calculating \(2-2g_{}(_{}(x),_{}(y))\). In the next section we explain why this mapping and recovery scheme leads to good error guarantees.

### Intuition behind the construction

Now that we have defined the maps \(_{}\) we can explain the idea behind using maps of this form. The reason why this type of map is useful has to do with the behavior of the derivative of the function \(g_{}\) near \(t=1\). As previously mentioned, the map \(_{}\) has the property that for all \(x,y S\),

\[|_{}(x),_{}(y)-f_{}( x,y )|<\] (1)

for some \(\) depending on \(N\). Now when we want to recover \(\|x-y\|^{2}\) based on \(_{}(x)\) and \(_{}(y)\), by the polarization identity, we use \(2-2g_{}(_{}(x),_{}(y))\) as an estimate of \(\|x-y\|^{2}\). The additive error of the approximation of \(\|x-y\|^{2}\) by \(2-2g_{}(_{}(x),_{}(y))\) depends on the error in Eq. (1) as well as the derivative of \(g_{}\) near the point \(f_{}( x,y)\). The function \(g_{}\) has the property that its derivative approaches zero as \(t\) approaches one and so the additive error of the approximation of \(\|x-y\|^{2}\) by \(2-2g_{}(_{}(x),_{}(y))\) gets smaller the closer \(f_{}( x,y)\) (and thus \( x,y\)) is to one, i.e., the closer \(\|x-y\|^{2}\) is to zero. (This is quantified in Theorem 6, specifically the exponent approaching 2 in the additive error) The effect that this has is that we actually approximate \(\|x-y\|^{2}\) up to a multiplicative error.

The role of the parameter \(\) in this construction is in controlling the rate at which \(g_{}^{}(t)\) approaches zero as \(t\) approaches one. If there are pairs \(x,y\) such that \(\|x-y\|^{2}\) is very small then we need the derivative to approach zero very quickly. This can be done by increasing the parameter \(\); the rate at which \(g_{}^{}\) approaches zero is faster for larger values of \(\) (see Theorem 7 in Appendix B). It turns out that the correct choice of \(\) is \(_{2}_{2}r\) where \(r\) is approximately the reciprocal of the minimum distance between pairs of points in \(S\) (see Theorem 2). Because of the connection with standard neural networks with \(\) hidden layers mentioned in Section 1.1, we refer to the parameter \(\) as the number of _layers_ of the random mapping.

We remark that the function \(g_{}(t)\) has the property that its derivative can be as large as \(()^{}\) when \(t\) is near zero (see Lemma 8 in Appendix B). The effect that this has is that our algorithm leads to worse approximation of \(\|x-y\|^{2}\) when \( x,y\) is close to zero if \(\) is large. However, this loss in accuracy is made up for by increasing \(N\) by only a relatively small amount and the gain in accuracy when \( x,y\) is close to one outweighs the loss in accuracy when \( x,y\) is close to zero.

### Previous variations on random projection

Our compression method is similar to random projection in that they both involve compressing a set of points by randomly mapping it to a lower dimensional space. A number of other papers have also suggested variations on random projection where a different random mapping is used. In some, the random mapping is still linear. In others, they use a linear mapping followed by some quantization step. The main difference in our method is that it is more fundamentally non-linear due to the fact that it is a "deep" composition of non-linear maps.

One of the standard versions of random projection involves mapping points by a random Gaussian matrix. It was later shown that other types of random matrices work equally well. . In particular, the "binary" version of the Johnson-Lindenstrauss lemma due to  (where the entries of the matrix are all either \(+1\) or \(-1\)) is particularly important for the following reason. As discussed in , an alternate way to convert sketches obtained by the Johnson-Lindenstrauss lemma to bits is possible if the points have bounded integer coordinates and one uses the binary variant of Johnson-Lindenstrauss lemma. This approach is somewhat incomparable to our setting because of the integrality assumption.

Another variation on the random projection technique is to apply a quantization step after the projection which further reduces the cost of storing the data points . A particularly relevant version of quantization is "sign random projections" . Sign random projections are the same as the 1-layer maps \(_{1}\). They were used to estimate angles between points in  and used to estimate inner products between points in . Therefore, the main novelty of our technique is the idea of composing multiple such maps.

### Distance compression beyond random mappings

Random mappings are of course not the only way to compress a data set. Here we compare our method to compression techniques that use methods other than random mappings. These methods tend to be more complicated algorithmically but as we explain below, can produce sketches with fewer bits.

Given a set of \(n\) points in the unit ball in \(^{d}\), what is the minimum number of bits of a sketch which allows one to recover all pairwise distances up to a multiplicative \((1)\) error? As explained above, the Johnson-Lindenstrauss lemma shows that \(O^{-2}n n(1/m^{2})\) bits suffice. However, this is not the optimal number of bits. It was recently shown in [14; 15] that if the points are contained in the unit ball and \(m\) is the minimum distance between points, then \(O^{-2}n n+n(1/m)\) bits suffice. The additive error version of this question was answered in . Previous to the result of [14; 15], the best known result was that \(O^{-2}n n(1/m)\) bits suffice . These two results, however, use sketching techniques that differ from the sketches obtained by the random projection technique in a fundamental way. Random projection compresses the data set "point by point" in the sense that the compression process is applied to each point independently from the others. In contrast, the sketches in  and  must compress the entire data set simultaneously.

Another way of stating this distinction is that "point by point" methods (such as random projection) satisfy the requirements of the _one-way communication_ version of this sketching problem while the methods used in  and  do not. In the _one-way communication_ version of the sketching problem, Alice holds half of the data points and Bob holds the other half. Alice sends a message to Bob using as few bits as possible. Bob then must report distances between pairs of points where one point in the pair is known by Alice and the other by Bob. The one-way communication version of the sketching problem asks one to determine the minimum number of bits of Alice's message. It was shown in  that if the points are in the unit ball and the minimum distance is \(m\), then \(^{-2}n(n/)(1/m)\) bits are required for the one-way communication version of the problem if the sketch is required to be successful with probability at least \(1-\).

Any sketching algorithm which compresses the data set point by point satisfies the requirements of the one-way communication variant of the sketching problem. We therefore know that sketching algorithms which compress the data set point by point cannot produce sketches with the optimal number of bits. However, there are several advantages to sketching algorithms of this sort. One advantage is that they are generally simpler and easier to implement. Another is that if one wants to add additional points to the data set, the entire sketching algorithm does not need to be re-run.

Our sketching algorithm from Theorem 2 also has the property that it compresses the data set point by point. Furthermore, the number of bits of our sketch almost matches the lower bound from . The dominant term in the bound from Theorem 2 is \((^{-2}n n((1/m))^{2_{2}(/)})\). Thus our number of bits matches the lower bound from  up to the power on the \((1/m)\) term. This motivates the question of whether some variation on our sketching technique can reduce this power.

### Outline of the paper and notation

Section 2 contains the construction of the maps \(_{}\) and quantifies the error in the approximation of \(f_{}( x,y)\) by \(_{}(x),_{}(y)\). Then in Section 3 we explain how the proof of Theorems 2 and 3 is completed. This amounts to showing how \(_{}(x),_{}(y)\) allows one to estimate \(\|x-y\|^{2}\) and quantifying the error of the estimation. Some of the details are deferred to the appendix. Finally, Section 4 contains experimental results, including an application to nearest neighbor search.

For \(x^{d}\{0\}\) we use \(\) to denote \(x/\|x\|\). For \(x,y S^{d-1}\), the _polarization identity_ states that \(2-2 x,y=\|x-y\|^{2}\); for arbitrary \(x,y^{d}\), it states that \(\|x\|^{2}+\|y\|^{2}-2 x,y=\|x-y\|^{2}\).

The construction of the maps \(_{}\)

The purpose of this section is to prove the following result, Corollary 4, which shows a bound on the error of the approximation of \(f_{}( x,y)\) by \(_{}(x),_{}(y)\) for all pairs \(x,y\) in a set of \(n\) points. The error in this approximation depends on the dimension \(D_{}\) of the image space of \(_{}\). In particular, we show how large \(D_{}\) needs to be in order to guarantee with high probability that \(_{}(x),_{}(y)\) is equal to \(f_{}( x,y)\) up to some additive error \(\) for all pairs \(x,y\).

We recall the definitions of the functions \(f_{}\) and \(g_{}\): Let \(f(t)=(t)\) and \(g(t)=()\). For \(^{+}\), let \(f_{}\) be the function \(f\) composed with itself \(\) times and similar for \(g_{}\). Notice that for any \(^{+}\), \(f_{}:[-1,1][-1,1]\) is the inverse of \(g_{}:[-1,1][-1,1]\).

**Corollary 4**.: _Let \(S S^{d-1}\) with \(|S|=n 2\). Let \(r:=_{x,y S,x y}}\). Let \(^{+}\). Let \(>0\) be such that \(<}\). Then there exists a random map \(_{}:^{d} D_{}^{-1/2}\{-1,1\}^{D_{}}\) (independent of \(S\) except through parameters \(n,d,r\)) such that with probability \((1-)^{}\) it satisfies_

\[_{}(x),_{}(y)-f_{}( x,y )<\]

_for all \(x,y S\), where \(D_{}}\)._

The above corollary follows immediately from the following theorem which also determines a bound on the error of the approximation accuracy not only of \(f_{}( x,y)\) by \(_{}(x),_{}(y)\), but also the approximation accuracy of \(f_{j}( x,y)\) by \(_{j}(x),_{j}(y)\) for all \(j\{1,,\}\).

**Theorem 5**.: _Let \(S S^{d-1}\) with \(|S|=n 2\). Let \(r:=_{x,y S,x y}}\). Let \(^{+}\). Let \(>0\) be such that \(<}\). Then there exist random maps \(_{j}:^{d} D_{j}^{-1/2}\{-1,1\}^{D_{j}}\), \(j\{1,,\}\) (independent of \(S\) except through parameters \(n,d,r\)) such that for all \(j\{1,,\}\), with probability at least \((1-)^{j}\), \(_{j}\) satisfies_

\[_{j}(x),_{j}(y)-f_{j}( x,y) <r^{3((2/3)^{j}-(2/3)^{})}}\] (2)

_for all \(x,y S\), where \(D_{j}r^{6((2/3)^{j}-(2/3)^{})}  n}{^{2}}\)._

Proof.: The maps \(_{j}\) will be compositions of maps of the following form. Set \(D^{+}\). Let \(Z_{i}\), \(1 i D\) be i.i.d. standard Gaussian random vectors in \(^{d}\). Let \(^{D}:^{d}^{D}\) be defined by

\[^{D}(x)_{i}:=D^{-1/2}( x,Z_{i}),\]

where \(^{D}(x)_{i}\) is the \(i\)th coordinate of \(^{D}(x)\). A direct calculation, see , shows that \(^{D}(x),^{D}(y)=f( x,y)\). Furthermore, \(\|^{D}(x)\|=1\) for all \(x^{d}\).

First we use this construction to define \(_{1}\). We let \(_{1}=^{D_{1}}\) where \(D_{1}\) is chosen below. Using Hoeffding's inequality, for all \(x,y S\),

\[((x),(y)-f(  x,y)>r^{3((2/3)-(2/3)^{})}})\] \[=(D_{1}(x),(y) -D_{1}f( x,y)>}{2^{-1}r^{3((2/3)-(2/3 )^{})}})\] \[ 2(-^{2}}{2 2^{2(-1)}r^{6((2/3) -(2/3)^{})}}).\]

We set \(D_{1}=r^{6((2/3)-(2/3)^{})} n}{^{2}}\). This means that the above probability is less than \(2/n^{3}\) and that \(_{1}\) satisfies the conditions of the theorem with probability at least \(1-} 1-1/n 1-2/n\).

Now assume that the required map exists for some \(j 1\). We will show that it exists for \(j+1\). So there exists a map \(_{j}:^{d} D_{j}^{-1/2}\{-1,1\}^{D_{j}}\) which satisfies Eq. (2) with probability at least 

[MISSING_PAGE_FAIL:7]

## 3 The recovery of \(\|x-y\|^{2}\) by \(_{}(x)\) and \(_{}(y)\)

Here we explain how the proofs of the main theorems, Theorems 2 and 3 are completed. This is done in two steps. Recall that we showed in Corollary 4 that \(_{}(x),_{}(y)\) is a good approximation of \(f_{}( x,y)\) for all pairs \(x,y S\). The first step is Theorem 6 which shows that \(g_{}(_{}(x),_{}(y))\) is a good approximation of \( x,y\). The reason this is true is because \(g_{}\) is the inverse of \(f_{}\). So the proof of Theorem 6 uses facts about the derivative of \(g_{}\) (in particular Theorem 7 in Appendix B) to show that the bound on the error of the approximation of \(f_{}( x,y)\) by \(_{}(x),_{}(y)\) implies a bound on the error of the approximation of \( x,y\) by \(g_{}(_{}(x),_{}(y))\) (Theorem 6).

The second step is to set \(=_{2}_{2}\) where \(m\) is the minimum distance between pairs of distinct points in \(S\) and then show using the polarization identity that the error bound established in Theorem 6 implies the error bounds in Theorems 2 and 3. The proof of the following theorem and the proof that Theorems 2 and 3 follow from Theorem 6 are in Appendix A.

**Theorem 6**.: _Let \(S S^{d-1}\) with \(|S|=n\). Let \(^{+}\) and \(>0\) and assume that \(\) satisfies \(<1-| x,y|\) for all \(x,y S\) with \(x y\). Then the random map \(_{}:S^{d-1} N^{-1/2}\{-1,1\}^{N}\) from Theorem 5 satisfies that, with probability at least \((1-2/n)^{}\), for all \(x,y S\), \(g_{}(_{}(x),_{}(y))\) is equal to \( x,y\) up to an additive \(\|x-y\|^{2-2^{-+1}}\) error where \(N= n}{^{2}}\). Equivalently, for all \(x,y S\), \(2-2g_{}(_{}(x),_{}(y))\) is equal to \(\|x-y\|^{2}\) up to an additive \(\|x-y\|^{2-2^{-+1}}\) error._

## 4 Experiments

The main goal of our experiments is to validate the idea of composing multiple random feature maps. Our main results show that better error guarantees can be obtained by composing multiple random feature maps. This is demonstrated by the additive error term \(\|x-y\|^{2-2^{-+1}}\) in Theorem 6 which is smaller for larger values of \(\) as long as \(\|x-y\|<1\). Recall that we refer to the parameter \(\) as the number of _layers_ of the random mapping. In our experiments, we will focus on comparing the one layer maps \(_{1}\) to the two layer maps \(_{2}\). First we do a simple experiment to determine more specifically under what conditions we should expect two layers to outperform one. Then we apply the maps to nearest neighbor search and again compare the performance of one layer to two layers.

### Two layers vs. one layer

As already mentioned, Theorem 6 indicates that the additive error obtained by the one layer map is proportional to \(\|x-y\|\) and the additive error obtained by the two layer map is proportional to \(\|x-y\|^{3/2}\). Therefore, two layers should give better error estimates than one layer when \(\|x-y\|\) is sufficiently small. Our first experiment answers the following question; how small must

Figure 1: The left, resp. right figure shows the average over 4000 trials of the value of \(\) such that \(2-2g_{1}(_{1}(x),_{1}(y))\) and \(2-2g_{2}(_{2}(x),_{2}(y))\) approximate \(\|x-y\|^{2}\) up to a multiplicative \((1)\) error when the input dimension is 2, resp. 2000.

be so that two layers outperform one? We consider pairs of points \(x,y S^{d-1}\) where \(\|x-y\|\) ranges from \(.01\) to \(.1\). We will see in this experiment that the input dimension has little to no effect and so we can without loss of generality assume \(d=2\). In the \(d=2\) case we take \(x=(1,0)\) and \(y=(a,})\) where \(a\) is chosen so that \(\|x-y\|\) ranges from \(.01\) to \(.1\) in increments of \(.005\), see Fig. 1. We map \(x,y\) by both maps \(_{1}\) and \(_{2}\) where the output dimension of both maps is 1000 and \(_{2}\) first maps to \(\{-1,1\}^{6000}\). In Fig. 1 we plot the average over 4000 trials of the value of \(\) such that \(2-2g_{1}(_{1}(x),_{1}(y))\) and \(2-2g_{2}(_{2}(x),_{2}(y))\) approximate \(\|x-y\|^{2}\) up to a multiplicative \((1)\) error. We see in Fig. 1 that the two layer map gives a better approximation of \(\|x-y\|^{2}\) when \(\|x-y\|.06\) and the one layer map gives a better approximation of \(\|x-y\|^{2}\) when \(\|x-y\|>.06\). From a practical perspective, this means that the one layer map may often outperform the two layer map because most real world datasets have few points at this small of a distance. However, very large datasets may be more likely to have distances in the range where two layers is better. We do this same experiment when \(d=2000\) except for \(d=2000\) we choose the input points randomly: We let \(x\) be uniform on \(S^{d-1}\) and set \(y=z/\|z\|\) where \(z=x+(/)N(0,I_{d})\) and dist ranges from \(.01\) to \(.1\). With this choice, \(\,\|x-y\|\). Because there is no dependence on the input dimension in our error bounds, we expect the error to depend on \(\|x-y\|\) but not \(d\), which is verified in Fig. 1.

### Two layers vs. one layer; nearest neighbor search

Given a set of points and a query point, nearest neighbor search is the task of finding the points that are closest to the given query point. Nearest neighbor search is used in many applications to solve regression/classification problems. Before performing nearest neighbor search, some dimensionality reduction or compression method can be used to reduce the computational cost. We test the performance of our maps \(_{}\) for this task.

Again we focus on \(=1,2\). Given a data set \(D\) and a query point \(X D\), we first map \(D\) by \(_{1}\) and \(_{2}\) to \(N^{-1/2}\{-1,1\}^{N}\). We then calculate the \(k\) nearest neighbors of \(X\) according to the compressed data and compare how many of the true \(k\) nearest neighbors of \(X\) in \(D\) are recovered.

_Randomly generated data._ We let \(X_{0}\) (the query point) be a uniform random vector on the unit sphere \(S^{2}\). Then for \(i\{1,2,,100\}\), we let \(Y_{i}=X_{0}+(4/5)(i/200)N(0,I_{3})\) where \(N(0,I_{3})^{3}\) is from the multivariate normal distribution (and the \(Y_{i}\) are independent). The constants are chosen so that \(\,\|X_{0}-Y_{i}\| i/200\). Then the data \(D\) is defined to be \(X_{0}\) along with \(\{Y_{i}/\|Y_{i}\|\}_{i\{1,100\}}\). We map \(D\) by \(_{1}\) and \(_{2}\) where the output dimension ranges from \(2^{3}\) to \(2^{13}\) and \(_{2}\) first maps to the space of dimension six times the output dimension. We also map \(D\) by a standard Gaussian random matrix to \(^{d}\) with the same range of output dimensions. The average number of \(k\) nearest neighbors recovered is shown in Fig. 2. As is to be expected based on the previous experiment, two layers generally outperform one. We remark that the accuracy of our maps is not meant to be compared to the accuracy of the Gaussian random matrix because that mapping uses full precision real numbers and does not convert to bits.

Figure 2: Average over 500 trials of the number of true \(k\) nearest neighbors of \(X_{0}\) in \(D\) recovered from \(_{1}(D)\), \(_{2}(D)\), and \(GD\) where \(G\) is an i.i.d. Gaussian random matrix.

_Real data._ We perform a similar experiment with the RCV1 dataset . The RCV1 data set consists of 804414 samples and 47236 features. We only consider the first 23149 samples which have been previously designated as the training set. As query points, we select all data points which have at least one neighbor at distance less than \(.05\). There are 179 such points. The RCV1 dataset consists of unit norm vectors. This is our data set \(D\). We map \(D\) by \(_{1}\) and \(_{2}\) where the output dimension ranges from \(2^{3}\) to \(2^{11}\) and \(_{2}\) first maps to the space of dimension six times the output dimension. We also map \(D\) by a standard Gaussian random matrix to \(^{d}\) with the same range of output dimensions. The average number of \(k\) nearest neighbors recovered is shown in Fig. 3. We see that in the case \(k=1\), the two layer map outperforms the one layer map when the output dimension is sufficiently large. This again confirms expectations based on Fig. 1. In the \(k=4\) case, the one layer map is superior. This is likely due to the fact that while our query points all have at least one neighbor at distance at most \(.05\), the other three out of the four nearest neighbors may be at a significantly greater distance. Therefore, again considering Fig. 1, it is not surprising that the one layer map gives a better approximation of the four nearest neighbors.

## 5 Conclusion

We introduced a new method for compressing point sets while maintaining information about approximate distances between pairs of points. The method compresses point sets using a composition \(_{}\) of \(\) random feature mappings. The main advantage of composing multiple feature maps is that, rather than approximating pairwise distances up to an _additive_\(\) error, our maps accomplish the more difficult task of approximating the distances up to a _multiplicative_\((1)\) error. The reason that we get multiplicative rather than additive error guarantees is a direct result of composing multiple random feature maps and has to do with the behavior of the derivative of the function \(g_{}\) (introduced in Section 1.1) near \(t=1\). We also validate the idea of composing multiple random feature maps experimentally.