# OnlineTAS: An Online Baseline for Temporal Action Segmentation

Qing Zhong\({}^{1,2}\) Guodong Ding\({}^{2}\) Angela Yao\({}^{2}\)

\({}^{1}\)University of Adelaide \({}^{2}\)National University of Singapore

qing.zhong@adelaide.edu.au {dinggd,ayao}@comp.nus.edu.sg

Equal Contribution. The work was done when Qing Zhong was an intern in National University of Singapore.

###### Abstract

Temporal context plays a significant role in temporal action segmentation. In an offline setting, the context is typically captured by the segmentation network after observing the entire sequence. However, capturing and using such context information in an online setting remains an under-explored problem. This work presents the an online framework for temporal action segmentation. At the core of the framework is an adaptive memory designed to accommodate dynamic changes in context over time, alongside a feature augmentation module that enhances the frames with the memory. In addition, we propose a post-processing approach to mitigate the severe over-segmentation in the online setting. On three common segmentation benchmarks, our approach achieves state-of-the-art performance.

## 1 Introduction

This work addresses online temporal action segmentation (TAS) of untrimmed videos. Such videos typically feature procedural activities consisting of multiple actions or steps in a loose temporal sequence to achieve a goal . For example, "making coffee" has actions: 'take cup', 'pour coffee', 'pour water', 'pour milk', 'pour sugar' and'stir coffee'. Standard TAS models  are offline and segment-only videos of complete procedural activities. An _online_ TAS model, in contrast, segments only up to the current time point and does not have access to the entire video and, therefore, the entire activity.

Online TAS faces challenges similar to other online tasks  in establishing a scalable network that can retain useful information from an ever-increasing volume of data and facilitate effective retrieval when required. Additionally, over-segmentation is a common issue for offline TAS, where the segmentation model divides an action into many discontinuous sub-segments, leading to fragmented outputs. This issue is exacerbated in the online setting, as partial data at the onset of an action may lead to erratic predictions and increased over-segmentation.

Most relevant to our task is online temporal action detection (TAD) . Online TAD aims to identify whether an action is taking place and the action category. TAD targets datasets like THUMOS , TVSeries , and HACS Segment . Among these, 90.8% videos of THUMOS  feature only multiple instances of the same action, while TVSeries  comprises diverse yet independent actions (_e.g._, 'open door', 'wave' and 'write') in one video. These actions do not necessarily correlate with one another or impose specific temporal constraints. As such, a direct adaptation of popular online TAD approaches like LSTR  and MAT  to online segmentation is non-ideal. For instance, these models encode temporal context with a fixed set of tokens, which may limit their capability to handle the relations of procedural videos. Furthermore, these models are typically trained to prioritize frame-level accuracy while neglecting temporal continuity, which invariably leads to over-segmentation.

To address the online action segmentation task, this work proposes a novel framework centered on a context-aware feature augmentation module and an adaptive memory bank. The memory bank, per-video, tracks short-term and long-term context information. The augmentation module uses an attention mechanism to allow frame features to interact with context information from the memory bank and integrate temporal information into standard frame representations. Finally, we introduce a post-processing technique for online boundary adjustment that imposes duration and prediction confidence constraints to mitigate over-segmentation.

Summarizing our contributions, **1)** We establish an _online_ framework for TAS; **2)** We propose a feature augmentation module that generates context-aware representations by incorporating an adaptive memory, which accumulates temporal context collectively. The module operates on frame features independently of model architecture, enabling flexible integration; **3)** We present a simple post-processing technique for online prediction adjustment, which can effectively mitigate the over-segmentation problem; and **4)** Our framework achieves the state-of-the-art online segmentation performance on three TAS benchmarks.

## 2 Related Work

**Online Action Understanding.** Many video understanding tasks, such as action detection [44; 42; 5; 2] and video instance segmentation [15; 46; 47], have been explored in online contexts. For online action detection, videos are classified frame by frame without access to future frames. Specifically, LSTR  employs a novel memory mechanism to model long- and short-term temporal dependencies by encoding them as query tokens. Follow-up works feature a segment-based long-term memory compression  and fusing short- and long-term histories via attention .

However, the videos in the datasets commonly used in online TAD contain independent actions  or sequences of limited actions , thus lacking temporal relations between the actions. In contrast, TAS deals with untrimmed procedural videos, where such relations are more prominent and may span over long temporal durations. There is also a growing trend in online TAD models to use action anticipation as an auxiliary task to enhance action modeling [2; 14]. In our online segmentation task, we do not assume the availability of such information.

**Temporal Action Segmentation.** In TAS , methods vary by their level of supervision, including fully [11; 20; 27], semi-supervised [8; 36], weakly [9; 13; 22; 30; 29; 31; 21], and unsupervised [19; 34; 32; 10; 9] setups. An emerging direction is to learn TAS incrementally  where procedural activities are learned sequentially. However, all existing works are offline, and complete video sequences can be used for inference. In contrast, our approach functions within an online setup. The most related work  investigates online TAS in a multi-view setup and leverages the offline model to assist online model learning. Furthermore, it uses the frame-wise multi-view correspondence to generate pseudo-labels for action segments. In contrast, we do not assume the availability of multi-view videos nor require assistance from a pre-trained offline model.

**Post-processing for Action Segmentation.** Post-processing methods are either rule-based or leverage graphical modelling. Rule-based approaches [28; 35; 40; 38] apply predefined rules to smooth out short-duration predictions that are unlikely given the context. Graphical modelling approaches use Conditional Random Fields (CRFs) [17; 31; 30] to model the relationships and transitions between consecutive actions. Online methods need post-processing to alleviate over-segmentation and help locate the action boundaries.

## 3 Online Action Segmentation

Previous studies [1; 33] have demonstrated that the scope of the temporal context significantly influences the performance of (offline) TAS models. This motivates our two lines of inquiry for our online setting: 1) how to consolidate temporal context over an extended period, and 2) how to enrich the frame representations with context to benefit the segmentation. This work introduces a context-aware feature augmentation module (Sec. 3.2) alongside an adaptive context memory (Sec. 3.3) to tackle the above questions.

### Preliminaries

Consider an untrimmed video \(v=\{x_{t}\}_{t=1}^{T}\) of \(T\) frames, where \(x_{t}^{D}\) is the per-extracted frame feature at time \(t\) and \(D\) is the feature dimension. An action segmentation model partitions \(v\) into \(N\) contiguous and non-overlapping temporal segments \(\{s_{i}=(y_{i},_{i})\}_{i=1}^{N}\) corresponding to actions \(y\) present in the video, where \(\) indicates the segment length and \(\) defines the action space . A widely adopted strategy for TAS is to design a segmentation model \(\) that predicts the action label \(y_{t}\) for each frame \(x_{t}\), akin to a frame-wise classification. In the offline setting [11; 45; 25], the per-frame prediction \(_{t}\) is based on the entire video sequence. The online setting uses only frames up to the current prediction time \(t\), without access to future frames. Comparatively:

\[_{t}^{}=*{arg\,max}_{y}p_{t }(y_{t}|x_{t};x_{1:T})_{t}^{}= *{arg\,max}_{y}p_{t}(y_{t}|x_{t};x_{1:t}).\] (1)

where \(p_{t}^{||}\) is the estimated action probability for frame \(x_{t}\). Most existing offline TAS works [11; 45; 23] train the segmentation model \(\) using a cross-entropy loss (\(_{}\)) for frame-wise classification which is balanced by a smoothing loss (\(_{}\)) that encourages smooth transitions between consecutive frames:

\[=_{t}-(p_{t}(y_{t}))}_{_{ }}+|}_{t,y} {}_{t,y}^{2}}_{_{}},\] (2)

\[_{t,y}=_{t,y}:_{t,y} \\ :_{t,y}=| p_{t }(y)- p_{t-1}(y)|.\]

In this paper, we opt for the widely recognized convolution-based architecture MS-TCN  as our foundational framework. This choice is driven by its relatively lower computational requirements than the attention- or diffusion-based models [45; 25]. A straightforward transition from offline mode to an online mode of the segmentation model \(\) is to substitute standard convolutions with causal convolutions. Causal and standard convolutions differ in their receptive field in that causal convolutions consider only past and present inputs while standard convolutions may incorporate both past and future inputs within a kernel. Mathematical details and illustrations of the two are shown in the Appendix.

### Context-aware Feature Augmentation

The context-aware feature augmentation (CFA) module generates enhanced clip-wise features through interactions with temporal context captured by an adaptive memory bank. The module operates on a clip-wise basis. During training, video \(v\) is split into \(K\) non-overlapping clips \(\{c_{k}\}_{k=1}^{K}\). Each clip has a window size \(w\) and is sampled from \(v\) with a stride of \(=w\), where the final clip \(c_{K}\) is padded if \(|c_{K}|<w\). The CFA module integrates the original pre-extracted frame features \(c_{k}=\{x_{t}\}_{t=(k-1)w+1}^{kw}\) with temporal context to produce a context-enhanced version of representations \(_{k}=\{_{t}\}_{t=(k-1)w+1}^{kw}\). Like [44; 41], our CFA module is also equipped with a simultaneously updated memory bank \(M_{k}\) as a context resource for feature augmentation. The memory bank is further described in Sec. 3.3.

At each step \(k\), context is accumulated by feeding \(c_{k}\) through a lightweight GRU  to obtain \(c_{k}^{}\). The GRU is reliable in capturing information over long video sequences . The clip is then passed through a context aggregation block to be augmented. The context aggregation block incorporates the GRU features \(c_{k}^{}\) with the memory state \(M_{k-1}\) from the previous step for \(I\) iterations. Concretely, we pass \(c_{k}^{}\) through a self-attention (SA) block to encourage information exchange with the local clip window. Additionally, we leverage a Transformer decoder  to achieve a more effective memory encoding \(_{k-1}^{}\), _i.e._,

\[c_{k}^{}=(c_{k}^{}) _{k-1}^{}=(M_{k-1},c_{k}^{},c_{ k}^{}).\] (3)

The outputs from the self-attention module (\(c_{k}^{}\)) and the transformer decoder (\(c_{k}^{}\)) are then combined with cross attention (CA) and merged with \(c_{k}^{}\) to produce the context-augmented features:

\[_{k}=(c_{k}^{},_{k-1}^{}, _{k-1}^{})+c_{k}^{}.\] (4)

The detailed formulas of SelfAttn(), TransDecoder(), CrossAttn() are given in the Appendix. An illustration of our CFA module is provided in Fig. 1.

### Adaptive Memory Bank

In a similar spirit with , our memory is designed to account for both short- and long-term context, _i.e._, \(M=[M^{},M^{}]\). Short-term memory helps capture the local action dynamics while long-term memory retains information across extended durations important for TAS .

**Short Memory \(M^{}\)**. Given that our enhancement module works on a per-clip basis with temporal stride \(w\), we directly regard the enhanced feature \(_{k-1}\) from the last clip as the short-term memory, _i.e._, \(M^{}_{k}=_{k-1}^{Dw}\).

**Long Memory \(M^{}\)**. We update our long-term memory with information from processed clips. Specifically, we apply a convolutional layer on top of our context-enhanced representation \(_{k}\), where \(_{k}^{Dw}\), to collapse the temporal dimension and yield a memory token \(m_{k}=(_{k})^{D}\). This memory token is then appended to the current long-term memory.

**Adaptive Memory Update.** The memory is updated whenever a new clip is processed. In practice, we constrain the total footprint of both short- and long-term memory to match the size of the processing clip, _i.e._, \(M^{Dw}\). At the beginning of each sequence, the memory bank is initialized with short-term information only, _i.e._, \(M=M^{}_{0}=c_{1}\) and \(M^{}_{0}=\). As more clips are processed, we gradually increase the budget to accommodate longer-term information. However, in anticipation of \(M^{}\) draining the entire budget in prolonged sequences, we cap its utilization at a maximum of two-thirds of the total budget. In instances where this threshold is exceeded, the earliest token is discarded, _i.e._,:

\[M^{}_{k}=(M^{}_{k-1},m_{k})& :(M^{}_{k-1})w\\ (M^{}_{k-1}[1\!:\!],m_{k})&:.\] (5)

The remaining budget is allocated for the short-term information accordingly:

\[M^{}_{k}=_{k-1}[(M^{}_{k})\!:]\] (6)

As video progresses, our feature augmentation module (Sec. 3.2) receives more longer-term context while emphasizing only shorter and more relevant short-term information. This adaptive approach

Figure 1: Context-aware Feature Augmentation (CFA) module. CFA takes as input a video clip \(c_{k}\) of length \(w\), augments it with temporal information captured in an adaptive memory bank \(M_{k}\), and outputs an enhanced clip feature \(_{k}\). \(I\) is the number of iterations of SA, TransDecoder, and CA.

enables the context memory to flexibly shift its attention between short and long-term information as the video progresses. Algorithm 1 summarizes the update mechanism.

**Discussion.** Our module integrates context memory on top of a GRU layer. While the GRU captures temporal dependencies, it may struggle, especially in thousands-frame long sequences common in TAS. The context memory supplements the GRU by allowing selective access and updates, thereby enabling the retrieval and manipulation of long-term information. Such a design is supported by our empirical study that the explicit memory can extend the capacity of the GRU's internal state. Supporting ablations are found in Sec. 4.1.

### Training and Inference

**Training.** Our final online segmentation model is constructed by combining our CFA module with a single-stage TCN  with causal convolutions. Specifically, TCN takes as input the enhanced representations \(_{k}\) and maps them to the same labels for \(c_{k}\). We train the framework end-to-end with the loss function formulated in Eq. (2), but on a clip basis with \(T\) replaced by \(w\):

\[^{}=_{t}-(p_{t}(y_{t}))+ _{t,y}_{t,y}^{2},\] (7)

we set \(=0.15\) following . Like , training on a clip basis provides better efficiency.

**Inference.** We present two distinct inference approaches by manipulating the clip sampling stride parameter \(\). The first mode of inference, referred to as _online_, is characterized by setting \(=1\). In such a setting, a video clip of \(w\) frames is processed, with emphasis placed solely on the prediction derived from the final frame and rest are discarded. This facilitates the scenario when frame-by-frame prediction is preferred. The alternate mode of inference, termed _semi-online_, adheres to the training regime by setting \(=w\). In this mode, video clips are processed, and the dense predictions generated across all \(w\) frames are preserved as final output. An illustration of two modes of inference is provided in Fig. 2.

```
1:Compute \(_{}= T_{}\)
2:Initialize \(=0\)
3:for each frame \(t\)do
4:if\(q_{t}<\) and \(<_{}\)then
5:\(_{t}^{*}=_{t-1}^{*}\)
6:\(=+1\)
7:else
8:\(_{t}^{*}=_{t}\)
9:\(=0\)
10:endif
11:endfor ```

**Algorithm 2** Post-processing for Online TAS

### Post-processing

Our intuition is that a valid action segment should not fall below a minimum length threshold unless there is high confidence in the prediction to justify a change in the action class. Specifically, we consider the maximum softmax probability of a prediction as its confidence measure, denoted as \(q_{t}=(p_{t})\), for frame \(x_{t}\), as \(q_{t}\) to some extent indicates its reliability. A prediction is considered "unreliable" if its confidence measure scores below a certain threshold \(\), _i.e._, \(q_{t}<\). For the frame with "unreliable" prediction, we disregard its current prediction and assign the action label of its proceeding frame \(_{t-1}^{*}\), when the previous action segment is shorter than the minimum length.

Figure 2: Two inference types. a) Online inference samples clips with stride \(=1\) and only preserves the last frame prediction, while b) Semi-online inference samples non-overlapping clips with stride \(=w\) and all predictions are preserved.

Otherwise, we retain the original prediction. We set the length threshold \(_{}= T_{}\) with \((0,1)\), in proportion to to the longest video duration \(T_{}\) in training set.

Our post-processing mitigates the over-segmentation by adjusting action boundaries according to network predictions and action length, which is very efficient compared to [32; 10] that calculates frame similarities. The procedure for post-processing is illustrated in Algorithm 2.

## 4 Experiments

**Datasets**: We evaluate our model on three common TAS datasets. **Breakfast** comprises in total 1,712 videos performing ten different activities with 48 actions. On average, each video contains six action instances. **50Salads** has 50 videos with 17 action classes. **GTEA** contains 28 videos of seven kitchen activities composing 11 different actions. We use common I3D features  as input.

**Evaluation Metrics**: Standard evaluation metrics for TAS are reported for our online setting, which includes frame-wise accuracy (Acc), segmental edit score (Edit), and segmental F1 scores with varying overlap thresholds 10%, 25%, and 50%.

**Implementation Details.** In CFA, we stack 2 Transformer decoder layer with 8 heads, 2 Swin  self- and cross attention with 4 heads. We use a single-stage TCN as segmentation backbone and sample non-overlapping clips _i.e._, \(=w\) for efficiency. We train the model end-to-end with a learning rate of \(5e^{-4}\) of total 50 epochs. Detailed hyperparameter settings can be found in Appendix.

### Experiment Results

**Effectiveness.** We report the overall performance for _online_ and _semi-online_ inference (see Sec. 3.4) in in Table 1. Our baseline is a single-stage causal TCN and we build our framework on top of it. Across all three datasets, the integration of our CFA module leads to a consistent boost in the segmentation performance. Specifically, our approach gained 5.7% (75.2% vs. 80.9%) in Acc and 9.2% (19.6% vs. 28.8%) in Edit on 50Salads. While the improvements on other datasets are not as significant, they still show effectiveness, with a margin of about 2%. Generally, semi-online inference achieves better performance over online across all metrics. Such improvement is likely because clip-wise prediction better preserves the local temporal continuity of labels compared to step-by-step single frame prediction.

Comparing across the metrics, segmental scores appear to be significantly low. On breakfast  with our online inference, a frame-wise accuracy of 56.7% only corresponds to a 9.3% F1 score with 50% IoU. Such score indicates a severe over-segmentation issue and necessitates an effective post-processing. However, a significant performance increase is observed on Edit and F1 scores after our proposed pose-processing. For example, the same F1 score increases to 30.5%, tripling its original value. Although post-processing could lead to a slight decrease in accuracy, it demonstrates great effectiveness in mitigating the over-segmentation problem.

    & &  &  &  \\   & p.p. & Acc & Edit & F1 @ \{10, 25, 50\} & Acc & Edit & F1 @ \{10, 25, 50\} & Acc & Edit & F1 @ \{10, 25, 50\} \\   \\   & - & 74.4 & 66.6 & 73.9 & 70.3 & 57.2 & 75.2 & 19.6 & 26.8 & 24.4 & 19.6 & 55.3 & 18.7 & 15.1 & 11.7 & 8.3 \\  & ✓ & 72.1 & **71.9** & **79.2** & **77.4** & 64.1 & 75.1 & 68.5 & 74.1 & 70.6 & 60.4 & 52.3 & 54.7 & 52.0 & 43.2 & 29.8 \\    & - & **76.2** & 63.5 & 72.6 & 68.3 & 58.8 & **80.9** & 28.8 & 36.1 & 31.0 & 23.3 & **56.7** & 19.3 & 16.8 & 13.9 & 9.3 \\   & ✓ & 74.4 & 70.3 & 78.5 & 76.4 & **67.7** & 77.7 & **71.5** & **77.7** & **74.6** & **64.1** & 52.9 & **55.7** & **54.8** & **45.8** & **30.5** \\   \\   & - & 75.8 & 66.8 & 74.3 & 71.5 & 60.3 & 79.1 & 29.0 & 38.5 & 35.5 & 28.3 & 55.7 & 18.6 & 15.4 & 12.7 & 9.0 \\  & ✓ & 73.5 & 75.4 & 80.3 & 76.9 & 66.6 & 76.7 & 69.2 & 73.1 & 70.5 & 62.8 & 52.5 & 54.0 & 53.1 & 44.5 & 29.6 \\    & - & **77.1** & 68.1 & 76.7 & 73.5 & 63.9 & **82.4** & 32.8 & 43.0 & 41.1 & 34.7 & **57.4** & 19.6 & 17.8 & 14.8 & 10.1 \\    & ✓ & 76.0 & **79.7** & **84.9** & **81.4** & **69.2** & 79.4 & **75.0** & **82.5** & **80.2** & **68.0** & 53.8 & **57.5** & **56.4** & **47.3** & **31.4** \\   

Table 1: Performance of our approach on three TAS benchmarks under two inference mode, _i.e._, online and semi-online. Post-processing is indicated by p.p..

**Ablation study.** Table 2 evaluates the components in our CFA module. The first row is our single-layer causal TCN baseline with strong frame-wise accuracy but poor segmental metrics. The GRU boosts segment metrics (7-11%) over the baseline, showing its ability to accumulate context information. While CFA using the current clip as pseudo memory predictably leads to a performance drop (5%) compared to GRU due to lack of any context information. Combining either GRU or our adaptive memory with our CFA achieves very close performance (rows 4 and 5), highlighting the importance of the context information for TAS. The complete model yields the best performance and boosts Acc by 7% and average segmental scores by 15.3%. This validates the complementary effect of GRU's internal state and our explicit memory design.

**Number of layers in CFA.** Table 3 explores the interaction iterations \(I\) in CFA. The results indicate that the performance is not significantly affected by the number of iterations. In practice, we set the number of iterations to 2, as it achieves a good balance between performance and efficiency.

**Memory composition** (\(M^{}\)/\(M^{}\)). We assess the impact of memory types and present results in Table 4. It shows comparable performances for each memory type when considered individually. However, the combination of both yields a 2% improvement in Acc and 1.5% for averaged segmental metric, suggesting the significance of incorporating diverse memory for TAS.

**Clip size \(w\) / Memory size \((M)\)**. In our implementation, we set clip size and memory size to be equal and we report its influence on performance in Table 5. It shows a larger clip size leads to better segmental results; this is because temporal continuity can be better modeled with longer clips for learning. However, the information of short actions could be diluted when compressed to form the memory token if the window size is too large. For memory sizes of 16, 32, and 64, the earliest memory are discarded as the average length of 50Salads is \(\)5.8k frames. With the size reducing, the performance gradually drops and reaches the lowest of 79.8% in Acc compared to the peak of 82.4%. Note that with the memory size set to 16, our approach only retains long-term information from up to 192 frames, 30\(\) less than the average video length.

**Post-processing hyperparameters.** Two hyperparameters are defined in our post-processing: confidence threshold \(\) and the minimum segment length \(_{}\). We vary its scaling factor \(\) to assess \(_{}\). In Table 6, increasing \(\) greatly enhances the segment results, with a 18.1% increase observed when \(=0.7\). Although the accuracy tends to decrease as \(\) becomes larger, the drop is not as substantial (3.1%) compared to the improvements in segmental results. While Table 7 shows the segmental performance stops increasing and stays stable when \(>\) with a fixed confidence score \(=0.9\). In conclusion, employing a higher confidence threshold can help better mitigate the over-segmentation because it makes more sense to prioritize preserving the continuity of a segment that includes frames with highly confident predictions given a fixed length budget.

   \(M^{}\) & \(M^{}\) & Acc & Seg. \\  ✓ & - & 80.3 & 36.7 \\ - & ✓ & 80.4 & 36.4 \\  ✓ & ✓ & **82.4** & **37.9** \\    
   \(w\) / \((M)\) & 16 & 32 & 64 & 128 & 256 \\  Acc & 79.8 & 81.4 & 81.6 & **82.4** & 80.7 \\ Seg. & 35.1 & 36.5 & 37.6 & 37.9 & **38.2** \\   

Table 4: Effect of memory composition.

   GRU & CFA & Mem. & Acc & Edit & F1 & @ \{10, 25, 50\} \\  - & - & - & 75.2 & 19.6 & 26.8 & 24.4 & 19.6 \\ ✓ & - & - & 78.1 & 27.1 & 37.9 & 34.7 & 26.7 \\ - & ✓ & - & 76.2 & 22.3 & 30.1 & 27.0 & 21.9 \\ ✓ & ✓ & - & 79.1 & 29.0 & 38.5 & 35.5 & 28.3 \\ - & ✓ & ✓ & 78.9 & 29.2 & 38.7 & 35.1 & 28.8 \\  ✓ & ✓ & ✓ & **82.4** & **32.8** & **43.0** & **41.1** & **34.7** \\   

Table 2: Ablation study of module components on 50Salads .

   \(I\) & Acc & Edit & F1 & @ \{10, 25, 50\} \\ - & 1 & 79.1 & 29.0 & 38.5 & 35.5 & 28.3 \\ 
2 & **79.6** & **30.7** & **40.7** & **38.2** & **31.4** \\ 
3 & 79.5 & 28.5 & 37.2 & 36.1 & 29.0 \\
4 & 79.1 & 29.2 & 39.1 & 36.3 & 30.5 \\
5 & 79.2 & **30.8** & 39.1 & 37.7 & 30.6 \\   

Table 3: Effect of interactions \(I\).

### Comparison with State-of-the-Art Methods

Tables 8 and 9 compare our approach against the state-of-the-art TAS approaches on all three benchmarks. Due to the absence of dedicated online TAS methods, we benchmark against the online TAD approach LSTR . We train LSTR on TAS datasets using the official code implementation2. To ensure a fair comparison, we configure their working (short-term) memory to be the same as ours (\(w\)). Additionally, we adjust its long memory accordingly to provide access to the entire past sequence. As evident from Tables 8 and 9, LSTR  consistently achieves relatively low performance, particularly with Edit scores of 5.0% and 4.9% on 50Salads and Breakfast datasets, respectively. This suggests severe over-segmentation in their predictions. Moreover, these performances are inferior even to those of our baseline model (casual TCN), indicating that a direct adoption of online detection models for the segmentation task is not ideal.

Amongst all datasets, Breakfast is the most challenging, with a significant performance gap between offline and online models, particularly on segmental metrics. Notably, the F1@50 score on Breakfast experiences a drastic drop of 4/5, from 47.5% to 8.3%, highlighting the difficulty of the online segmentation task with videos that are more complex. Nonetheless, we still achieve a modest absolute performance improvement of 2%. Furthermore, our post-processing technique, significantly boosts segmental performance, nearly tripling the original performance, albeit with a slight decrease in Acc. This underscores the effectiveness of our post-processing technique in mitigating the over-segmentation. MV-TAS  tackles online segmentation but under a multi-view setting. It leverages multi-view information and an offline model as a reference for online segmentation. Despite this, even our baseline model, depicted in the third-to-last row of Table 9, showcases a notable performance improvement (55.3% vs. 41.6%) over MV-TAS . This considerable margin emphasizes the competitiveness of our baseline model.

When compared to offline models, our semi-online inference with post-processing manages to surpass the offline model MS-TCN  on 50Salads dataset across the segmental metrics and reaches around 90% of the accuracy of the best-performing DiffAct . On Breakfast, our approach lags behind the offline model in both frame-wise accuracy and segmental metrics.

**Qualitative Result.** Fig. 3 qualitatively compares the segmentation results from different approaches. It is clear to see that LSTR  suffers from the most prominent over-segmentation issue, which

   \(\) & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\  Acc & 82.5 & **82.6** & 82.6 & 82.6 & 81.2 & 81.1 & 79.4 \\ Seg. & 50.9 & 51.8 & 51.9 & 51.9 & 69.0 & 73.3 & **76.4** \\    
   \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Acc & **79.6** & 79.4 & 79.4 & 79.4 & 79.4 & 79.4 \\ Seg. & 70.9 & 74.7 & **76.4** & 76.4 & 76.4 \\   

Table 6: Effect of confidence threshold \(\) (\(=\)).

remains significant after the post-processing. Under the same configuration, our semi-online achieves slightly better results compared to the frame-by-frame online inference. Our post-processing, when applied, successfully removes the short fragments (blue boxes) in the raw prediction and refines the segmentation output. However, it may reduce accuracy, particularly at action boundaries (red boxes).

For failure cases, we have the following two observations: 1) Detection of action start often delays due to the need for more frame information to predict new actions, especially when facing semantic ambiguities at action boundaries; 2) Persistent over-segmentation happens when the network makes incorrect but confident predictions, which could be improved with a stronger backbone or better temporal context modeling.

### Runtime Analysis

We evaluate the runtime performance of our approach using an Nvidia A40 GPU with both pre-computed I3D features and raw streaming RGB inputs, and present the inference times in Table 10. As shown, our approach can achieve up to 238.1 FPS when using pre-computed I3D features. To calculate the runtime for the entire segmentation pipeline, we take into consideration of the computational overheads of optical flow calculation and the I3D feature extraction. By leveraging a GPU backend for optical flow calculation, our full framework is able to achieve a runtime of 33.8 FPS.

**Inference Latency.** The inference speed presented above is identical for both online and semi-online inference modes since their input sizes are the same. However, the latency can differ. In the online mode, inference is performed on a per-frame basis, meaning its latency is only dependent on the

Figure 3: Visualization of segmentation outputs for sequence “rgb-01-1” from 50Salads .

    & } &  \\   & & Acc & Edit & F1 @ \{10, 25, 50\} \\   **V1.0** \\  } & MS-TCN  & 69.3 & 67.3 & 64.7 & 59.6 & 47.5 \\  & ASFormer  & 73.5 & 75.0 & 76.0 & 70.6 & 57.4 \\  & DiffAct  & 75.1 & 76.4 & 80.3 & 75.9 & 75.1 \\   & MV-TAS  & 41.6 & - & - & - & - \\  & LSTR  & 24.2 & 4.9 & 5.5 & 3.9 & 1.7 \\  & Causal TCN & 55.3 & 18.7 & 15.1 & 11.7 & 8.3 \\  & Ours\({}^{}\) & 56.7 & 19.3 & 16.8 & 13.9 & 9.3 \\  & Ours\({}^{}\) + p.p. & 52.9 & 55.7 & 54.8 & 45.8 & 30.5 \\  & Ours\({}^{}\) & **57.4** & 19.6 & 17.8 & 14.8 & 10.1 \\   & Ours\({}^{}\) + p.p. & 53.8 & **57.5** & **56.4** & **47.3** & **31.4** \\   

Table 9: Comparison with the state-of-the-art methods on Breakfast.

inference speed. In contrast, the semi-online mode incurs additional latency as it requires gathering frames up to the clip lengths before forming inputs.

Online inference offers better real-time responsiveness compared to semi-online inference, but the latter achieves superior performance as we discussed in Sec. 4.1. The choice between these two modes depends on the application's priorities: if the real-time inference is critical, online inference is preferable; however, if accuracy is more important and the task is less time-sensitive, semi-online inference is recommended.

**Limitation.** In this work, we only evaluate our approach on cooking videos, however, handling diverse and real-world videos may present several additional challenges. One common scenario involves interrupted actions, where a subject abruptly switches to a different action, leaving the ongoing action unfinished. These interruptions can be challenging for the model to handle effectively. Additionally, the extended length of the video poses another challenge. Streaming videos can be infinitely long, so effectively managing and preserving long-form history within a fixed memory budget becomes a critical issue.

## 5 Conclusion

This paper presents the first framework for the online segmentation of actions in procedural videos. Specifically, we propose an adaptive memory bank designed to accumulate and condense temporal context, alongside a feature augmentation module capable of injecting context information into inputs and producing enhanced representations. In addition, we propose a fast and effective post-processing technique aimed at mitigating the over-segmentation problem. Extensive experiments on common benchmarks have shown the effectiveness of our approach in addressing the online segmentation task.