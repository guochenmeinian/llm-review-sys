# What Is Missing In Homophily? Disentangling Graph Homophily For Graph Neural Networks

Yilun Zheng\({}^{1}\), Sitao Luan\({}^{2}\), Lihui Chen\({}^{1}\)

yilun001@e.ntu.edu.sg, sitao.luan@mail.mcgill.ca, elhchen@ntu.edu.sg

\({}^{1}\)Nanyang Technological University, Centre for Info. Sciences and Systems;

\({}^{2}\)Mila - Quebec Artificial Intelligence Institute; \({}^{}\) Corresponding Author

###### Abstract

Graph homophily refers to the phenomenon that connected nodes tend to share similar characteristics. Understanding this concept and its related metrics is crucial for designing effective Graph Neural Networks (GNNs). The most widely used homophily metrics, such as edge or node homophily, quantify such "similarity" as label consistency across the graph topology. These metrics are believed to be able to reflect the performance of GNNs, especially on node-level tasks. However, many recent studies have empirically demonstrated that the performance of GNNs does not always align with homophily metrics, and how homophily influences GNNs still remains unclear and controversial. Then, a crucial question arises: What is missing in our current understanding of homophily? To figure out the missing part, in this paper, we disentangle the graph homophily into three aspects: label, structural, and feature homophily, which are derived from the three basic elements of graph data. We argue that the synergy of the three homophily can provide a more comprehensive understanding of GNN performance. Our new proposed structural and feature homophily consider the neighborhood consistency and feature dependencies among nodes, addressing the previously overlooked structural and feature aspects in graph homophily. To investigate their synergy, we propose a Contextual Stochastic Block Model with three types of Homophily (CSBM-3H), where the topology and feature generation are controlled by the three metrics. Based on the theoretical analysis of CSBM-3H, we derive a new composite metric, named Tri-Hom, that considers all three aspects and overcomes the limitations of conventional homophily metrics. The theoretical conclusions and the effectiveness of Tri-Hom have been verified through synthetic experiments on CSBM-3H. In addition, we conduct experiments on \(31\) real-world benchmark datasets and calculate the correlations between homophily metrics and model performance. Tri-Hom has significantly higher correlation values than \(17\) existing metrics that only focus on a single homophily aspect, demonstrating its superiority and the importance of homophily synergy. Our code is available at [https://github.com/zylMozart/Disentangle_GraphHom](https://github.com/zylMozart/Disentangle_GraphHom).

## 1 Introduction

Graph Neural Networks (GNNs) have been widely used in processing non-Euclidean data due to their superiority in extracting topological relations . They have achieved great success on numerous real-world applications, _e.g.,_ recommendation , bio-informatics  and telecommunication . It is found that their success, especially on node-level tasks, is closely related to the homophily assumption , _i.e.,_ similar nodes tend to be connected . On the other hand, when dissimilar nodes are more likely to be connected, which is known as the non-homophily/heterophily scenario, GNNs fail to capture the useful neighbor information and even underperform Multilayer perceptrons (MLPs) . Several homophily metrics, such as edge homophily  and node homophily  were proposed, which were believed to be able to recognize the difficult datasets  and measure the performance of GNNs .

However, recent studies [46; 39; 51; 40] show that the conventional homophily metrics [72; 1; 50] are insufficient to measure the performance of GNNs: Luan _et al._ show that the homophily metrics cannot tell if GNNs work well under heterophily. Ma _et al._ reveals that homophily is not a necessary assumption for effective GNNs and they propose to identify "good" and "bad" heterophily to explain why GNNs still work well under heterophily. Luan _et al._ discovers a mid-homophily pitfall, showing the performance of GNNs reaches the worst in a medium level of homophily instead of the lowest. Then, a crucial question arises based on the above studies: What is missing in our current understanding of homophily?

In this paper, we fill the missing parts by investigating different perspectives of the "node similarity". Conventional homophily metrics quantify the "similarity" as an indicator function of whether connected nodes share the same label while ignoring the co-existence of three basic elements in graph data: label, structural, and node feature information. The ignorance of structural and feature information leads to insufficient understanding and unsatisfactory alignment between homophily metrics and GNN performance.

A complete understanding of graph homophily should include all the above three basic elements. To this end, we disentangle graph homophily into three corresponding aspects: label, structural, and feature homophily. Specifically, our new proposed structural homophily quantifies the "similarity" by considering the neighborhood structure consistency, and feature homophily measures the dependencies of node features across the topology. To investigate how their synergy affects GNN performance, we propose a Contextual Stochastic Block Model controlled with three types of Homophily (CSBM-3H). The node feature generation process in CSBM-3H breaks the _i.i.d._ assumption in previous studies [46; 39], which is closer to real-world scenarios [14; 59; 61]. With the three metrics, CSBM-3H enables a more comprehensive study on the impact of graph homophily than previous analysis [46; 39; 29; 60].

From the theoretical study of CSBM-3H, we derive a new composite metric named Tri-Hom to measure the synergy, which includes all three homophily aspects. Through CSBM-3H, our theoretical analysis and simulation results both show that the performance of GNNs is highly influenced by Tri-Hom. It can help explain how the three types of homophily influence GNN behavior individually or collectively. In addition, our theoretical findings can explain some interesting phenomena observed in previous literature, such as "good" or "bad" heterophily [46; 38] and the impact of feature shuffling on GNNs . To verify the effectiveness of Tri-Hom, we conduct experiments on \(31\) real-world datasets. The results show that GNN performance is significantly better aligned with Tri-Hom than the other \(17\) existing metrics that focus only on a single homophily aspect. This implies that Tri-Hom can complete the absent parts in existing homophily metrics.

## 2 Preliminary

We denote \(=(,)\) as an undirected graph, where \(\) is the node set and \(\) is the edge set. The graph has \(N\) nodes with \(C\) classes. The adjacency matrix of the graph is denoted as \(^{N N}\). We use \(_{uv}=1\) or \(e_{uv}\) to denote the existence of an edge between node \(u\) and \(v\), otherwise \(_{uv}=0\) or \(e_{uv}\). Node degree vector is denoted as \(^{N}\) where \(_{u}\) is the degree of node \(u\). Node label vector is denoted as \(^{N}\) and its one-hot encoding matrix is \(^{N C}\). The number of nodes in class \(c\) is denoted as \(N_{c}=|\{u|_{u}=c,u\}|\). The neighbor set of node \(u\) is denoted as \(_{u}=\{v|e_{uv}\}\). The features of all the nodes is denoted as \(^{N M}\), where \(_{v,:}\) are the features of node \(v\) with \(M\) dimensions. We use \(_{E}^{E E}\) and \(_{E}^{E E}\) to denote identify matrix and all-ones matrix with size \(E\), respectively.

**Graph homophily metrics** are used to measure the similarity between connected nodes. Edge [1; 72] and node homophily  are \(2\) most commonly used metrics and are defined as follows,

\[h_{}(,)=|e_{uv}, Y_{u}=Y_{v}\}|}{||},\ h_{}(,)= |}_{v}_ {v},Y_{u}=Y_{v}\}|}{|_{v}|} \]

These metrics qualify the ratio of whether the labels of two connected nodes are the same in a graph. However, this definition of graph homophily only considers a label aspect and neglects structural and feature aspects, resulting in a partial understanding of graph homophily. Therefore, we propose to disentangle the graph homophily as label, structural, and feature homophily in the next section.

**Graph-aware models \(^{}\) and graph-agnostic models \(^{}\)** refer to the models that either utilize structure information or do not, respectively. For example, baseline graph-aware models\(^{}\), such as Graph Convolutional Network (GCN) , Graph Attention Network (GAT)  and GraphSage , encode both graph structure and node feature information in each layer; the corresponding graph-agnostic models \(^{}\) are the Multilayer Perceptrons (MLPs), which only encode node features .

**Structural-agnostic features** refer to the node features \(\) that are conditionally independent of graph topology \(\) given \(\), _i.e.,_\((\!\!\!|)\); **structural-aware features** indicate \((\!\!\!|)\).

## 3 Disentangled Graph Homophily

In this section, we first introduce the definition of disentangled graph homophily from label, structural, and feature aspects to complete the missing part of the graph homophily. Then, in the next section, we will introduce how they collectively impact the performance of GNNs.

### Label Homophily

**Definition 1**.: _Label homophily is defined as the consistency of node labels across the topology._

Label homophily is the most widely used conventional metric of graph homophily and it qualifies the similarity between connected nodes \(u\) and \(v\) using an indicator function \((Y_{u}=Y_{v})\). Most of the conventional homophily metrics focus on label homophily, including edge homophily , node homophily , class homophily , adjusted homophily , density-aware homophily , 2-hop neighbor class homophily , and neighbor homophily .

However, label homophily only focuses on the consistency of label information for connected nodes while neglecting structural and feature information, which are two indispensable components of graph data. Hence, it offers only a partial understanding of graph homophily, which cannot always align well with the performance of GNNs . To capture the missing structural and feature information and better understand graph homophily, we give the definitions of structural and feature homophily in the following \(2\) subsections.

### Structural homophily

For structural homophily, the "atom" information of a node is structural information instead of the label. It is meaningless to define the structural homophily using the consistency across the graph topology as in the label homophily because the structural information already contains the information from the graph topology. Therefore, we define the structural information as the consistency of structural information among the nodes from the same classes1, which better disentangles itself from the label homophily.

**Definition 2**.: _Structural homophily is defined as the consistency of structural information of nodes within the same class. The structural homophily in a graph is defined as:_

\[h_{S}(,,)=_{c=1}^{C}h_{S,c},\;\; \;h_{S,c}(,,)=1-(u)|u,Y_{u}=c\})}{_{max}} \]

Figure 1: Visualization of synthetic graphs generated by CSBM-\(3\)H with varying levels of label homophily, structural homophily, and feature homophily. The node colors denote node classes in sub-figure (a-f) and node features in sub-figure (g-i)

_where \(h_{S,c}\) is the class-wise structural homophily for class \(c\), function \(()\) measures structural information, \(\) denotes standard deviation of structural information, and \(_{max}\) denotes the maximum value of \(\)._

In this paper, we quantify the structural information for node \(u\) through neighbor distribution (the class distribution of local neighbors) \(_{u}^{}=[p_{u,1},p_{u,2},,p_{u,c}]\), where \(p_{u,k}==k|v_{u}\}|}{|_{u}|}\) is the proportion of neighbors of node \(u\) that belong to class \(c\). A high structural homophily indicates that the graph-aware models leveraging structural encoding will have similar embeddings for intra-class nodes after aggregation, which are expected to outperform graph-agnostic models, irrespective of a low label homophily. There are also some homophily metrics that focus on the structural aspect in previous studies, including label informativeness , neighborhood similarity , and aggregation homophily , which is similar as the structural homophily defined there.

### Feature Homophily

Previous feature-based graph homophily metrics, such as generalized edge homophily , local similarity , attribute homophily , and class-controlled feature homophily , mainly focus on the consistency of node features across the graph topology, which is similar as the definition of Dirichlet energy in graphs. However, these homophily metrics on feature consistency cannot fully disentangle itself from label homophily: Since the features of nodes in a graph are supposed to depend on their classes, when the graph shows a high/low label homophily, the connected nodes are more likely to share the same/different labels, resulting in a high/low feature similarity. Therefore, these feature-based homophily metrics are dependent on label homophily. Such dependency contains redundancy, which decreases the useful information inside feature-based homophily and impedes our understanding of the relationship between node features and GNN performance.

To disentangle the feature effect from label and graph structure, we define the feature homophily as the dependencies of node features across the graph topology, thereby dissociating it from label homophily and structural homophily. Inspired by graph diffusion  and interactive particle systems [55; 64], we have the structural-agnostic unobserved feature \((0)\) and the observed structural-aware feature \(\) that satisfy the following relation

\[= _{t=0}^{}()^{t}(0)=( {I}-)^{-1}(0) \]

The detailed process of this relation is given in Appendix B. Here \((-)},)})\) is a parameter that controls the feature dependencies, where a positive, negative, or zero value corresponds to an attractive relation, repulsive relation, or independence of the nodes with their neighbors in graphs [55; 64]. The feature dependencies \(()^{t}\) of \(t\)-order neighbors are introduced to structural-agnostic features \((0)\). Finally, the state of all the nodes will converge to an equilibrium with structure-aware feature \(\). The \(\) in Eq. (3) is independent of the graph topology because no matter how the label homophily or structural homophily changes, \(\) will remain unaffected. To disentangle feature homophily from label homophily and structural homophily, we define the feature homophily based on \(\) as follows.

**Definition 3**.: _Feature homophily is defined as the degree of feature dependencies of nodes across the topology. For the linear case of the graph diffusion process with feature dependencies, the feature homophily for feature \(m\) satisfies_

\[_{:,m}=(-}{()})^{-1} _{:,m}(0) \]

_where \(()\) is the spectral radius of \(\), \((0) p(|)\) are the unseen structural-agnostic node features, and \( p(|,)\) is the observed structural-aware node features. The feature homophily for the whole graph is the averaged feature homophily for all the features_

\[h_{F}(,,)=_{m=1}^{M}h_{F,m} \]

Note that it is easy to control the feature homophily in the generation of synthetic graphs. However, since both \(h_{F,m}\) and \(_{:,m}(0)\) are unknown in Eq. (4), one more condition is required to estimate feature homophily in real-world datasets. To address this issue, we consider the case where the intra-class distances of \(_{:,m}(0)\) are small. This case holds in lots of real-world scenarios [38; 39]and we can utilize this property to estimate \(h_{F,m}\) without solving \(_{:,m}(0)\). Specifically, the feature homophily \(h_{F,m}\) for feature \(m\) can be estimated with the following optimization process

\[h_{F,m}^{}(,_{:,m},)=_{h_{F,m}}_{ u,v\\ Y_{u}=Y_{v}} X_{u,m}(0)-X_{v,m}(0)^{2},\ _{:,m}(0)=(-}{()})_{:,m} \]

The estimation of feature homophily is invariant to the operations of feature shifts, scaling, or changing variance, where the proof is shown in Appendix C. This estimation process will be used in Section 5.2 for calculation.

**Remark** To better understand the definitions of three types of graph homophily, Figure 1 visualizes examples of graphs under varying \(h_{L}\), \(h_{S}\), and \(h_{F}\): 1) **Label homophily.** As label homophily \(h_{L}\) increases, as shown in Figures 1(a), (b), and (c), nodes are more likely to connect with others that share the same label. Particularly, a high \(h_{L}\) (Figure 1(c)) results in several clusters with distinct class boundaries, while a low \(h_{L}\) causes nodes to more likely connect to nodes with different classes. 2) **Structural homophily.** As structural homophily \(h_{S}\) increases, as shown in Figures 1(d), (e), and (f), the neighbor distributions of intra-class nodes become more consistent. Therefore, a high \(h_{S}\) is expected to capture effective structural information with message aggregation. Interestingly, we also find that a higher \(h_{S}\) makes a graph resemble planar graphs  and periodic graphs . We hypothesize this phenomenon occurs because stable structural information leads to more regular and meaningful patterns, which would be interesting to explore the connection between \(h_{S}\) and these geometric properties of graphs in the future. 3) **Feature homophily.** Figures 1(g), (h), and (i) illustrate different levels of feature homophily (\(h_{F}\)) within the same graph topology. Figure 1 (h) demonstrates that under a medium positive \(h_{F}\), features of some boundary nodes exhibit characteristics of both neighboring classes. For instance, a node on the boundary of the red class and the blue class appears purple, a mixture of these classes. A higher \(h_{F}\) (Figure 1 (i)) increases feature dependencies, particularly affecting more nodes closer to class boundaries. In social networks, a positive \(h_{F}\) indicates that people's opinions are influenced by their friends, resulting in similar characteristics. Conversely, a negative \(h_{F}\) causes nodes to become more dissimilar from their neighbors. As shown in Figure 1 (g), a negative \(h_{F}\) creates a distinct boundary between classes. Additionally, for the intra-class nodes in Figure 1 (g), the node colors differ in shades from their neighbors. This occurs because node features become more dissimilar due to the "repulsive force" rather than the "attractive force" induced by a negative \(h_{F}\). For example, in online social media, people are likely to argue with those holding different opinions on certain topics. After such interactions, individuals may reinforce their original opinions, a phenomenon resulting from the "repulsive force" associated with a negative \(h_{F}\).

## 4 Impact of Disentangled Graph Homophily

To study the model performance in a graph, the Contextual Stochastic Block Model (CSBM) has been widely used to study the performance of GNNs with controlled graph topology and node features. Previous studies  on graph homophily generally adopt a modified CSBM to control the label homophily through assigning nodes with different probabilities that connect to the nodes from other classes. Then the node features are sampled solely based on the classes. However, this graph modeling, which only considers label homophily, has two drawbacks: First, the probabilities of nodes from the same class connecting to the nodes with different classes are uniform, which lacks diversity. Second, the sampled node features are independent with their structures _i.e._, \((\!\!\!|)\), which is uncommon in real-world scenarios where interactions influence the attributes of connected nodes . Therefore, we propose a Contextual Stochastic Block Model with three types of Homophily (CSBM-3H), a random graph generative model that integrates the three types of homophily (Section 4.1), where the newly proposed structural homophily \(h_{S}\) and feature homophily \(h_{F}\) can well address the aforementioned two drawbacks and fills the missing part of graph homophily. Then, Based on CSBM-3H, we theoretically study how the graph-agnostic and graph-aware models are affected by label, structural, and feature homophily metrics to explore their relationship and verify the effectiveness of proposed metrics (Section 4.2).

### Csbm-3h

Graph Topology GenerationFollowing the topology generation process in existing studies , we assume all the nodes are class-balanced and share the same node degree \(d\). We use node homophily \(h_{L}\) to control the label consistency across the graph topology and \(h_{S}\) to control the consistency of neighbor distribution \(^{}\) of nodes within the same classes. Then, the neighbor distribution can be expressed as:\[^{}=_{}[],=}{c-1}_{C}+(h_{L}-}{c-1})_{C}+ \]

where \(^{}^{N C}\) is the neighbor distribution for all the nodes, \(^{C C}\) is a class-sampling matrix, and \(^{C C}\) is a noise matrix. Each entry of \(\) is a noise of neighbor sampling that follows a Gaussian distribution \(N(0,)^{2}}{c-1})\). The class-sampling matrix \(\) should be legal in practice _i.e.,_\(_{u,v}>0\) and \(_{v}_{u,v}=1\). Then an adjacency matrix \(\) can be sampled from a neighborhood sampling matrix \(_{}=^{}^{T}\), where \([A_{uv}=1]=(_{p})_{uv}\) for each pair of nodes \(u,v\). In this way, we control the label homophily \(h_{L}\) and structural homophily \(h_{S}\) in a graph.

**Node Feature Generation**. For any node \(u\) in a graph, we first sample its structural-agnostic features \(_{u}(0)^{F}\) from a class-wised Gaussian distribution \(_{u}(0)_{Y_{u}}(_{Y_{u}},_{Y_{u}})\) with \(_{Y_{u}}^{F}\) and \(_{Y_{u}}^{F F}\). We also assume each dimension of the feature vector is independent of each other, thereby \(_{Y_{u}}^{F F}\) is a diagonal matrix. Then the observed structural-aware features can be generated by the unseen structure-agnostic feature as described in Eq. (4).

### Node distinguishability

Suppose we have the representations of node \(u\) as \(_{u:}=_{v_{u}}_{v:}\) for graph-aware models \(^{}\) and \(_{u:}=_{u:}\) for graph-agnostic models \(^{}\). Inspired by the principles of neural collapse [28; 27] and node distinguishability , we quantify the impact of the aforementioned homophily metrics on both the graph-aware models \(^{}\) and graph-agnostic models \(^{-}\) by measuring the ratio of intra-class node distance to inter-class distance. To ideally distinguish nodes from different classes, a smaller intra-class distance \(D_{}()\) and larger inter-class distance \(D_{}()\) is preferred because this will reduce boundary nodes and increase the margins among classes. The metric is defined as follows,

\[=}()}{D_{}()}=_{y_{u}=y_{v},}[\|_{u}-_{v}\|^{2 }]}{_{y_{u} y_{v},}[\|_{u}-_{ v}\|^{2}]} \]

A smaller \(\) indicates better node embeddings for the model performance and vice versa, which has been proved in . With the proposed CSBM-3H, we can analyze the impacts of \(h_{L}\), \(h_{S}\), and \(h_{F}\) on \(^{-}\) and \(^{}\) by studying their relations with \(\), which will be derived in the following theorems.

**Theorem 1**.: In CSBM-3H, the ratio of the expectation of intra-class distance to the expectation of inter-class distance of node representations for graph-agnostic models \(^{-}\) and graph-aware models \(^{}\) is:

\[^{-}=(1+_{}_{h}^{-})^{-1}^{}=(1+_{}_{h}^{ })^{-1} \]

where \(_{}= Y_{u}}[2C(C-1)]^{-1}\|_{Y_{u }}-_{Y_{u}}\|^{2}}{C^{-1}|^{2}|^{2}|^{2}| _{h}^{-}}\), \(_{h}^{-}=}{()})^{2}(C( {1-h_{L}}{C-1})^{2}+C)^{2}}{C-1}+C-1)^{2}}{C-1})}{ [1-(}{()})(C-1}{C-1})]^{2}}\), and \(_{h}^{}=C-1}{C-1})^{2}}{C(}{C-1})^{2}+C)^{2}}{C(}{C-1})^{2}}+(C -1}{C-1})^{2}}_{h}^{-}\). (See the proof in Appendix G.1.)

From Theorem 1 we can see that, \(_{}\) is a normalized distance term which is a constant given the distribution of structural-agnostic features and is irrelevant with graph information; \(_{h}^{-}\) and \(_{h}^{}\) are controlled by the three types of homophily, which can reflect the influence of graph homophily on model performance. We name \(_{h}^{-}\) and \(_{h}^{}\) as **Tri-Hom** for \(^{-}\) and \(^{}\). To study the effect of Tri-Hom in more detail, we take the partial derivative of \(_{h}^{}\) with respect to \(h_{S}\), \(h_{F}\), and \(h_{L}\) to show the analytical results of their influences2. (See calculation in Appendix G.5, G.6, and G.7.)

**Theorem 2.1**.: The partial derivative of \(_{h}^{}\) with respect to label homophily \(h_{L}\) satisfies,

\[_{h}^{}}{ h_{L}}<0,& h_{L}[0,)\\ _{h}^{}}{ h_{L}} 0,&h_{L}[ ,1] \]

From Theorem 2.1 we can see that, the worst performance of \(^{}\) is reached when \(h_{L}=\), which corresponds to the scenario with the highest number of unpredictable boundary nodes among classes. This finding explains the misalignment between label homophily and GNN performance mentioned in previous studies [46; 38; 39].

**Theorem 2.2**.: The partial derivative of \(_{h}^{}\) with respect to structural homophily \(h_{S}\) satisfies,

\[_{h}^{}}{ h_{S}} 0 \]

From Theorem 2.2 we can see that, a larger \(h_{S}\) consistently improves the performance of \(^{}\). This is intuitive for \(^{}\) because more consistent intra-class neighbor distributions will lead to closer intra-class node representations after feature aggregation. This conclusion is also shown in Wang _et al._, where the topological noise (which is inversely proportional to \(h_{S}\)) has a detrimental impact on node separability.

**Theorem 2.3**.: The partial derivative of \(_{h}^{}\) with respect to feature homophily \(h_{F}\) satisfies,

\[_{h}^{}}{ h_{E}}<0, &h_{L}(0,h_{L}^{-});\;h_{L}(h_{L}^{-},h_{L}^{+})\;\;\;\;h_{ F}(_{F},1)\\ _{h}^{}}{ h_{E}}>0,&h_{L} (h_{L}^{+},1];\;h_{L}(h_{L}^{-},h_{L}^{+})\;\;\;\;h_{F}(-1, _{F})\\ _{h}^{}}{ h_{F}}=0,&h_{L}= ;\;h_{L}(h_{L}^{-},h_{L}^{+})\;\;\;\;h_{F}=_{F}  \]

where \(0<h_{L}^{-}<h_{L}^{+}<1\) and \(-1<_{F}<1\). The expressions and detailed calculation of \(h_{L}^{-}\), \(h_{L}^{+}\), and \(_{F}\) are shown in Appendix G.7.

From Theorem 2.3 we can see that, when \(h_{L}\) is high in a graph, nodes with the same labels tend to be connected, thereby a larger \(h_{F}\) makes the intra-class nodes share more similar representations and positively affect \(^{}\); when \(h_{L}\) is low in a graph, nodes with the different labels tend to be connected, thereby a larger \(h_{F}\) makes the inter-class nodes share more similar representations and negatively affect \(^{}\); when \(h_{L}\) is on a medium level _i.e._, \(h_{L}(h_{L}^{-},h_{L}^{+})\), an increase of \(h_{F}\) will first improve and then reduces the performance of \(^{}\) with the cut-off point at \(h_{F}=_{F}\). There the \(_{F}\) is influenced by \(h_{L}\), \(h_{S}\), \(C\), and \(()\), and the \(h_{L}^{-}\) and \(h_{L}^{+}\) are influenced by \(h_{S}\), \(C\), and \(()\).

RemarkApart from the new findings mentioned above, Tri-Hom can help explain other interesting but under-explored phenomena of the graph homophily in previous studies, _e.g._, 1) "good" or "bad" heterophily , which states that GNN can still perform well in some heterophily cases; and 2) feature shuffling , which states that shuffling the node features randomly within the same class can improves the performance of GNNs on node classification. Our explanations with Tri-Hom are: 1) The occurrence of "good" or "bad" heterophily is due to the fact that the model performance is influenced by a combination of \(h_{L}\), \(h_{F}\), and \(h_{S}\), instead of \(h_{L}\) alone. When the \(h_{L}\) is low, the graph-aware models can still achieve good performance with a high \(h_{S}\) or a low \(h_{F}\); 2) feature shuffling is due to the existence of the structural-aware features of nodes. When \(h_{F}>0\), nodes are positively dependent on their neighbors. In this case, the nodes at the class boundaries or class centers are the hardest or easiest ones to predict because of the feature dependencies. If we randomly shuffle the nodes inside their classes, the nodes at the class boundaries will be easier to be classified because their features are more likely to be replaced by the nodes from the center, which are more distinguishable. For the nodes close to the class centers, it will be compensated by their neighbors.

## 5 Experimental Results

In Section 5.1, we conduct experiments on synthetic data generated by CSBM-3H to verify the conclusions from Theorem 2.1, 2.2, and 2.3, demonstrate the synergy of label \(h_{L}\), structural \(h_{S}\), and feature homophily \(h_{F}\) and test whether Tri-Hom \(_{h}^{}\) can reflect GNN performance. In Section 5.2, we evaluate the effectiveness of Tri-Hom on real-world benchmark datasets to test how well it can predict the model performance in real-world scenarios. In addition, we calculate the correlation between Tri-Hom value and model performance and compare them with the results of other \(17\) existing metrics. The results show that Tri-Hom has a significantly higher correlation with the model performance, demonstrating the necessity of filling the missing part by disentangling graph homophily from three aspects.

### Experiments on Synthetic Datasets

To verify our theoretical results in a more general case, we measure the performance of GCN on synthetic datasets, where we can easily control \(h_{L}\), \(h_{S}\), and \(h_{F}\). Specifically,Synthetic Data GenerationWe generate the synthetic graphs using CSBM-\(3\)H with a given tuple \((h_{L},h_{S},h_{F})\), where \(h_{L}\{0,0.1,,0.9,1\},\;\;h_{S}\{0,0.1,,0.9,1\}\) and \(h_{F}\{-0.8,0.6,,0.6,0.8\}\). For each \((h_{L},h_{S},h_{F})\), we generate \(1000\) nodes with three balanced classes and the node degrees are sampled from a uniform distribution \(\). For node features, we first sample the structural-agnostic features from class-specific Gaussian distributions, then we use Eq. (4) to propagate these features across the graph topology with feature homophily \(h_{F}\) to generate the observed structural-aware features as the final node features. Then, we evaluate the node classification performance of GCN  on these synthetic graphs. To get a robust evaluation and mitigate the numerical instability, we generate \(10\) graphs with \(10\) random seeds for each \((h_{L},h_{S},h_{F})\) and report the average and standard deviation of classification accuracy on validation sets. The detailed process of topology and feature generation is shown in Appendix E.1.1.

Numerical Results of Tri-HomTo verify whether Tri-Hom \(_{h}^{}\) can reflect the behavior of GCN, we calculate its numerical results with the same setting as the synthetic graphs and make a comparison. Specifically, we set \(C=3\) and \(()=10\) to mitigate the influences of varying numbers of classes and spectral radius.

Comparison and AnalysisThe results are shown in Figure 2. For a better demonstration, we only show the results for \(h_{S}=\{0.2,0.4,0.6,0.8\}\) and each subfigure is a slice of \(h_{S}\) that visualizes the impact of \(h_{L}\) and \(h_{F}\) on GCN and Tri-Hom. From the comparison of Figure 1(a) and 1(b), we have two main observations: 1) Overall, the impact of \(h_{L}\), \(h_{S}\), and \(h_{F}\) on synthetic datasets aligns well with the numerical results of Tri-Hom \(_{h}^{}\). The only difference is that \(h_{F}\) seems to have less impact on GCN. We speculate that this is because the parameters in the graph filter in GCN are optimized during the training process, while these parameters are simplified as a fixed value in the theoretical analysis, leading to the difference3. 2) Our theoretical analysis of the impact of \(h_{L},h_{F}\) and \(h_{S}\) on Tri-Hom is consistent with GCN's behavior in Figure 1(b): Theorem 2.1 shows the worst performance of \(^{}\) is reached when \(h_{L}=\), corresponding to the ravine in Figure 1(b) for \(h_{L}\); Theorem 2.2 shows an increase of \(h_{S}\) consistently improves the performance of \(^{}\), corresponding to the overall increases of the accuracy in Figure 1(b) from the left to the right; Theorem 2.3 shows the influences of \(h_{F}\) to \(^{}\) is determined by the \(h_{L}\). Even if this is not obvious in Figure 1(b), we show more detailed figures of the individual impact of \(h_{L}\), \(h_{S}\), and \(h_{F}\) in Appendix E.6, confirming the impact of \(h_{F}\) in Theorem 2.3.

Figure 2: We measure the impact of label homophily \(h_{L}\), feature homophily \(h_{F}\), and structural homophily \(h_{S}\) through numerical results of Tri-Hom \(_{h}^{}\) and simulation results of the node classification accuracy with GCN on synthetic datasets.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]