# Towards Heterogeneous Long-tailed Learning:

Benchmarking, Metrics, and Toolbox

 Haohui Wang, Weijie Guan, Jianpeng Chen, Zi Wang, Dawei Zhou

Computer Science, Virginia Tech

{haohuiw,skjguan,jianpengc,ziwang,zhoud}@vt.edu

###### Abstract

Long-tailed data distributions pose challenges for a variety of domains like e-commerce, finance, biomedical science, and cyber security, where the performance of machine learning models is often dominated by head categories while tail categories are inadequately learned. This work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. We develop HeroLT, a comprehensive long-tailed learning benchmark integrating 18 state-of-the-art algorithms, 10 evaluation metrics, and 17 real-world datasets across 6 tasks and 4 data modalities. HeroLT with novel angles and extensive experiments (315 in total) enables effective and fair evaluation of newly proposed methods compared with existing baselines on varying dataset types. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://github.com/SSSKJ/HeroLT.

## 1 Introduction

In the era of big data, many high-impact domains, such as e-commerce [1; 2], finance , biomedical science [4; 5], and cyber security [6; 7], naturally exhibit long-tailed data distributions, where a few head categories1 are well-studied with abundant data, while massive tail categories are under-explored with scarce data. To name a few, in financial transaction networks, the majority of transactions fall into a few head classes that are considered normal, like wire transfers and credit card payments. However, a large number of tail classes correspond to various fraudulent transaction types, like synthetic identity transactions and money laundering. Although fraudulent transactions rarely occur, detecting them is essential for preventing unexpected financial loss [8; 9]. Another example is antibiotic resistance genes, which can be classified based on the antibiotic class they confer to and their transferable ability. Genes with mobility and strong human pathogenicity may not be detected frequently and are viewed as tail classes, but these resistance genes have the potential to be transmitted from the environment to bacteria in humans, thereby posing an increasing global threat to human health [10; 11].

Massive long-tailed learning studies have been conducted in recent years, proposing methods like the cost-sensitive focal loss to effectively address the data imbalance by reshaping the standard cross-entropy loss , and graph augmentation by interpolating tail node embeddings and generating new edges . The advancements in tackling long-tailed problems drove the publication of several studies on the survey of long-tailed problems [14; 15; 16; 17; 18], which generally categorize existingworks by different types of learning algorithms (_e.g._, re-sampling [19; 20], cost-sensitive [21; 22], transfer learning [23; 24], decoupled training [25; 26], and meta learning [27; 28; 29]).

Despite tremendous progress, some pivotal questions largely remain unresolved, _e.g._, _how can we characterize the extent of long-tailedness of given data? how do long-tailed algorithms perform with regard to different tasks on different domains?_ To fill the gap, this work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles in Figure 1: **(A1) the characterization of data long-tailedness:** long-tailed data exhibits a highly skewed data distribution and an extensive number of categories; **(A2) the data complexity of various domains:** a wide range of complex domains may naturally encounter long-tailed distribution, _e.g._, tabular data, sequential data, grid data, and relational data; and **(A3) the heterogeneity of emerging tasks:** it highlights the need to consider the applicability and limitations of existing methods on heterogeneous tasks. With three major angles in long-tailed learning, we design extensive experiments that conduct 18 state-of-the-art algorithms and 10 evaluation metrics on 17 real-world benchmark datasets across 6 tasks and 4 data modalities.

**Key Takeaways:** Through extensive experiments (see Section 3), we find (1) most works mainly focus on data imbalance while paying less attention to the extreme number of categories in the long-tailed distribution; (2) surprisingly none of the algorithms statistically outperforms others across all tasks and domains, emphasizing the importance of algorithm selection in terms of scenarios.

In general, we summarize the main contributions of HeroLT as below:

* **Comprehensive Benchmark.** We conduct a comprehensive review and examine long-tailed learning concerning three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks.
* **Insights and Future Directions.** With comprehensive results, our study highlights the importance of characterizing the extent of long-tailedness and algorithm selection while identifying open problems and opportunities to facilitate future research.
* **The Open-Sourced Toolbox.** We provide a fair and accessible performance evaluation of 18 state-of-the-art methods on multiple benchmark datasets using accuracy-based and ranking-based evaluation metrics at https://github.com/SSSKJ/HeroLT.

## 2 HeroLT: Benchmarking Heterogeneous Long-Tailed Learning

### Preliminaries and Problem Definition

Here we provide a general definition of long-tailed learning as follows. Given a long-tailed dataset \(=\{_{1},,_{n}\}\) of \(n\) samples from \(C\) categories, let \(\) denote the label set of category, where a

Figure 1: The systematic view of heterogeneous long-tailed learning concerning three pivotal angles, including long-tailedness (colored in red), data complexity (green), and task heterogeneity (blue).

sample \(_{i}\) may belong to one or more categories. For example, in image classification, one sample belongs to one category; in document classification, one document is often associated with multiple categories (_i.e._, topics). For simplicity, we represent \(=\{_{1},,_{C}\}\), where \(_{c}\), \(c=1,,C\) denotes the subset of samples belong to category \(c\), and the size of each category \(n_{c}=|_{c}|\) following a descending order. As an instantiation, a synthetic long-tailed dataset is given in Figure 2(a). Categories with massive samples refer to head categories, while categories with only a few samples refer to tail categories. Without the loss of generality, we make the following assumptions in long-tailed learning regarding the decision region of head and tail categories on the embedding space.

**Assumption 1** (Smoothness Assumption for Head Category).: _Given a long-tailed dataset, the distribution of the decision region of each head category is sufficiently smooth._

**Assumption 2** (Compactness Assumption for Tail Category).: _Given a long-tailed dataset, the tail category samples can be represented as a compact cluster in the feature space._

These assumptions ensure that tail categories are identifiable and meaningful. If Assumption 1 of smoothness is violated, then it is challenging to identify the head category clearly. For example in Figure 2(c), it is difficult to depict the decision region of the head category. Similarly, if Assumption 2 of compactness is violated, as seen in Figure 2(d), it is difficult to determine whether it is noise data or data from tail category. Based on the assumptions, we give the definition of long-tailed learning.

**Problem 1**.: _Long-Tailed Learning._

_Given: a training set \(\) of \(n\) samples from \(C\) distinct categories following long-tailed distribution, and the label set of category \(\). The data follows long-tailed distribution, i.e., the frequency of the categories is approximated as \(_{y}e^{ty}P(Y>y)=\), \( t>0\), where \(Y\) is a random variable._

_Find: a function \(f:\) that gives accurate label predictions on both head and tail categories._

### Benchmark Angles in HeroLT

**Angle 1: Long-Tailedness in Terms of Data Imbalance and Extreme Number of Categories.** While extensive public datasets and algorithms are available for benchmarking, these datasets or algorithms often exhibit varying extents of long-tailedness or focus on specific characteristics of long-tailed problems, making it challenging to select appropriate datasets and baselines to evaluate new algorithms. Two key properties of long-tailed data lie in highly skewed data distribution and an extreme number of categories. The former introduces a significant difference in sample sizes between head and tail categories, resulting in a bias towards the head categories , while the latter poses challenges in learning classification boundaries due to the increasing number of categories [70; 71].

To measure the long-tailedness of a dataset, we introduce three metrics in Table 1. Firstly, a commonly used metric is imbalance factor (IF) , and a value closer to 1 means a more balanced dataset. Secondly, Gini coefficient can be used as a metric to quantify long-tailedness . It ranges from 0 to 1, where a smaller value indicates a more balanced dataset. IF quantifies data imbalance between the most majority and the most minority categories, while Gini coefficient measures overall imbalance, unaffected by extreme samples or absolute data size. However, the two metrics pay more attention to data imbalance and may not reflect the number of categories. Therefore, Pareto-LT Ratio is proposed to jointly evaluate the two aspects of the data long-tailedness. Intuitively, the higher the skewness of the data distribution, the higher Pareto-LT Ratio may be; the more number of categories, the higher

Figure 2: Illustrative figures of a synthetic long-tailed distributed data. (a) long-tailed distribution of categories. (b) Visualization of obeying of Assumption 1, 2. (c) Visualization of violating of Assumption 1. (d) Visualization of violating of Assumption 2. (e) Visualization of violating of Assumption 1, 2.

the value of Pareto-LT Ratio. In light of three long-tailedness metrics, we gain a better understanding of which datasets and baselines to consider when evaluating a newly proposed algorithm.

* When all three metrics have large values, it indicates a dataset with a severe long-tailed distribution, necessitating an algorithm that addresses both data imbalance and the extreme number of categories.
* If Gini coefficient and IF are relatively small, but Pareto-LT Ratio is large, the main challenges of the dataset may lie in massive categories, as exemplified by the experiments in Section 3.4. In such cases, methods like extreme classification  and meta learning  would be preferred.
* When Gini coefficient and IF are large, but Pareto-LT Ratio is relatively small, it suggests that the challenges of data imbalance may be more significant than the number of categories. Algorithms employing techniques like re-sampling  or re-weighting  may already effectively handle data imbalance, as exemplified by the experiments in Section 3.5.
* If all three metrics are small, the extent of the long-tailedness of the dataset may be small, and ordinary machine learning methods may achieve decent performance.

In addition to the above metrics, we integrate the Bayes Imbalance Impact Index (BI3 ) and the Complementary Cumulative Distribution Function (CCDF) into our toolbox. While BI3 and CCDF are useful for certain application domains, _i.e._, BI3 for binary classification and CCDF for visualizing label distributions, they are not specifically designed for long-tailed learning. Therefore, we mention them here briefly without providing a detailed analysis. We give a detailed evaluation of long-tailedness metrics (AppendixB.1), and show the long-tailedness of benchmark datasets (Table 2). We analyze whether long-tailed algorithms are specifically designed to address data imbalance and an extreme number of categories (AppendixB.2), and provide a comprehensive experimental analysis (Section3).

Angle 2: Data Complexity with 17 Datasets across 4 Data Modalities.Most existing long-tailed benchmarks mainly focus on image datasets . However, in real-world applications, various types of data including tabular, grid, sequential, and relational data, face long-tailed problems. To fill this gap, we consider data complexity based on data types with long-tailed distribution.

* **Tabular data** comprises samples (rows) with the same set of features (columns) and is used in practical applications (_e.g._, medicine, finance, and e-commerce). Specifically, Glass  is a tabular dataset with feature attributes, where the number of samples in different categories follows a long-tailed distribution. In addition to node connections, graph data such as Amazon  often include node attributes, which can be regarded as tabular data.
* **Sequential data** denotes that data points in the dataset are dependent on the other points. A common example is a time series such as S&P 500 Daily Changes , where the input (price over date) shows a long-tailed distribution. Another example of sequential data is text composed of manuscripts, messages, and reviews, such as Wikipedia  containing Wikipedia articles with the long-tailed distribution of Wikipedia categories.
* **Grid data** records regularly spaced samples over an area. Images can be viewed as grid data by mapping grid cells onto pixels one for one. The labels of images often exhibit long-tailed distribution, as observed in commonly used datasets (_e.g._, ImageNet , iNatural , and LVIS ), remote sensing datasets , and 3D point cloud datasets . Furthermore, HOI categories in HICO-DET  and V_COCO , which are designed for human-object interaction detection in images, follow the long-tailed distribution. Videos can be regarded as a combination of sequential and grid data. In INVIDEO dataset for micro-video, hashtags exhibit long-tailed distribution. While the NYU-Depth dataset , which is composed of video sequences as recorded by depth cameras, exhibits a long-tailed per-pixel depth.

  Metric name & Computation & Description \\  Imbalance Factor  & \(n_{1}/n_{C}\) & Ratio to the size of largest and smallest categories. \\ Gini Coefficient  & \(^{C}_{j=1}^{C}|n_{i}-n_{j}|}{2 c}\) & Relative mean absolute differences between category sizes. \\ Pareto-LT Ratio & \(\) & The number of categories of the last 20\% samples to \\  & & the number of categories of the rest 80\% samples. \\  

Table 1: Three metrics for measuring long-tailedness of datasets. \(Q(p)=min\{y:Pr( y)=p,1 y C\}\) is the quantile function of order \(p(0,1)\) for \(\).

* **Relational data** organizes data points with defined relationships. One specific type of relational data is represented by graphs, which consist of nodes connected by edges. Therefore, in addition to long-tailed distributions in node classes, node degrees, referring to the number of edges connected to a node, may exhibit a long-tailed distribution . It is frequently observed that the majority of nodes have only a few connected edges, while only a small number of nodes have a large number of connected edges, as seen in datasets like Cora , CiteSeer , and Amazon . Moreover, in knowledge graphs like YAGO  and DBpedia , the distribution of entities may exhibit long-tailed distribution, with only a few entities densely connected to others. In HeroLT, we collect 17 datasets across 4 data modalities (tabular, sequential, grid, and relational data) as discussed in Appendix B.3. Table 2 shows data statistics (_e.g._, size, #categories) and long-tailedness of these datasets.

Angle 3: Task Heterogeneity with 18 Algorithms on 6 Tasks.While visual recognition has long been recognized as a significant aspect of long-tailed problems, real-world applications involve different tasks with unique learning objectives, presenting unique challenges for long-tailed algorithms. Despite its importance, this crucial angle has not been well explored in existing benchmarks. We aim to benchmark long-tailed algorithms based on various tasks they are designed to solve to fill the gap.

* **Object recognition**[83; 84] assigns each sample to one label. Data imbalance usually occurs in binary classification or multi-class classification, while long-tailed problems focus on multi-class classification with a large number of categories.
* **Multi-label text classification**[49; 50; 51; 85] involves assigning the most relevant subset of class labels from an extremely large label set to each document. However, the extremely large label space often leads to significant data scarcity, particularly for rare labels in the tail. Consequently, many long-tailed algorithms are specifically designed to address the long-tailed problem in this task. Additionally, tasks such as sentence-level few-shot relation classification  and information extraction (containing three sub-tasks named relation extraction, entity recognition, and event detection)  are also frequently addressed by long-tailed algorithms.
* **Image classification**[26; 40; 42; 43; 44; 45], which involves assigning a label to an entire image, is a widely studied task in long-tailed learning that received extensive research attention. Furthermore, there are some long-tailed algorithms focusing on similar tasks, including out-of-distribution detection [87; 88], image retrieval [89; 90], image generation [91; 92], visual relationship recognition [93; 94; 95; 96] and video classification [64; 97].
* **Instance segmentation**[46; 47] is a common and crucial task that gained significant attention in the development of long-tailed algorithms aimed at enhancing the performance of tail classes. It involves the identification and separation of individual objects within an image, including the

   &  &  \\ Dataset & Data & \# of Categories & Size & \# of Edges & IF & Gini & Pareto \\  Glass  & Tabular & 6 & 214 & - & 8 & 0.380 & 0.500 \\ Abalone  & Tabular & 28 & 4177 & - & 689 & 0.172 & 0.333 \\ Housing  & Tabular & - & 1,460 & - & - & - & - \\ EURLEX-4K  & Sequential & 3,956 & 15,499 & - & 1,024 & 0.342 & 3.968 \\ AAMZONCat-13K  & Sequential & 13,330 & 1,186,239 & - & 355,211 & 0.327 & 20.000 \\ Wiki10-31K  & Sequential & 30,938 & 14,146 & - & 1411 & 0.312 & 4.115 \\ ImageNet-LT  & Grid & 1,000 & 115,846 & - & 256 & 0.517 & 1.339 \\ Places-LT  & Grid & 365 & 62,500 & - & 996 & 0.610 & 2.387 \\ NiNatural 2018  & Grid & 8,142 & 437,513 & - & 500 & 0.647 & 1.658 \\ CIFAR 10-LT (100)  & Grid & 10 & 12,406 & - & 100 & 0.617 & 1.751 \\ CIFAR 10-LT (50)  & Grid & 10 & 13,996 & - & 50 & 0.593 & 1.751 \\ CIFAR 10-LT (10)  & Grid & 10 & 20,431 & - & 10 & 0.520 & 0.833 \\ CIFAR 100-LT (100)  & Grid & 100 & 10,847 & - & 100 & 0.498 & 1.972 \\ CIFAR 100-LT (50)  & Grid & 100 & 12,608 & - & 50 & 0.488 & 1.590 \\ CIFAR 100-LT (10)  & Grid & 100 & 19,573 & - & 10 & 0.447 & 0.836 \\ LVIS v0.5  & Grid & 1,231 & 693,958 & - & 26,148 & 0.381 & 6.250 \\ Cora-Full  & Relational\&Tabular & 70 & 19,793 & 146,635 & 62 & 0.321 & 0.919 \\ Wiki  & Relational & 17 & 2,405 & 25,597 & 45 & 0.414 & 1.000 \\ Email  & Relational\&Tabular & 42 & 1.005 & 25,934 & 109 & 0.413 & 1.263 \\ Amazon-Clothing  & Relational\&Tabular & 77 & 24,919 & 208,279 & 10 & 0.343 & 0.814 \\ Amazon-Electronics  & Relational\&Tabular & 167 & 42,318 & 129,430 & 9 & 0.329 & 0.600 \\  

Table 2: Datasets available in HeroLT benchmark. The values in this table correspond to input data.

detection of object boundaries and the assignment of a unique label to each object. It contains several parts: object detection [98; 99] and semantic segmentation .
* **Node classification**[100; 13; 30; 31; 32] involves assigning labels to nodes in a graph based on node features and connections between them. The emergence of long-tailed algorithms for this task is relatively recent, but the development is flourishing. Additionally, there are some long-tailed learning studies addressing entity alignment tasks [33; 34] in knowledge graphs.
* **Regression**[101; 102; 103] involves learning from long-tailed data with continuous (potentially infinite) target values. For example, the goal in  is to infer a person's age from their visual appearance, where age is a continuous target that can be highly imbalanced. Unlike classification, continuous targets lack clear category boundaries, causing difficulty when directly utilizing traditional methods like re-sampling and re-weighting. Additionally, continuous labels inherently possess a meaningful distance between targets.

In HeroLT, we have a comprehensive collection of algorithms for object recognition, multi-label text classification, image classification, instance segmentation, node classification, and regression tasks (Table 3). We discuss what technologies the algorithms use and how they solve long-tailed problems in Appendix B.2.

## 3 Experiment Results and Analyses

We conduct extensive experiments to further answer the question: how do long-tailed learning algorithms perform with regard to different tasks on different domains? In this section, we present the performance of 18 state-of-the-art algorithms on 6 typical long-tailed learning tasks and 17 real-world datasets across 4 data modalities.

### Experiment Setting

**Hyperparameter Settings.** For all the 18 algorithms in HeroLT, we use the same hyperparameter settings on the same task for a fair comparison. Refer to the Appendix C.1 for more information. **Evaluation Metrics.** We evaluate different long-tailed algorithms by several basic metrics, which are divided into accuracy-based metrics including accuracy (Acc) , precision , recall , and balanced accuracy (bAcc) ; ranking-based metrics such as mean average precision (MAP) ; regression metrics such as mean-average-error (MAE), mean-squared-error (MSE), Pearson correlation and error geometric mean (GM); and running time. The computation formula and description of the metrics are in Table 10 in Appendix C.2.

### Algorithm Performance on Object Recognition

Recently, there has been very limited work considering object recognition tasks on pure tabular long-tailed data. We compare the performance of two methods in Table 4. We find: **(1)** SMOTE

  Algorithm & Venue & Long-tailedness & Task \\  SMOTE  & 02JAIR & Data imbalance & Object recognition \\ NearMiss  & 03ICML & Data imbalance & Object recognition \\ X-Transformer  & 20KDD & Data imbalance, extreme \# of categories & Multi-label text classification \\ XR-Transformer  & 21NeurIPS & Data imbalance, extreme \# of categories & Multi-label text classification \\ XR-Linear  & 22KDD & Data imbalance, extreme \# of categories & Multi-label text classification \\ OLTR  & 19CVPR & Data imbalance, extreme \# of categories & Image classification \\ BALMS  & 20NeurIPS & Data imbalance & Instance segmentation \\ TDE  & 20NeurIPS & Data imbalance & Image classification \\ Decoupling  & 20ICLR & Data imbalance & Image classification \\ BBN  & 20CVPR & Data imbalance & Image classification \\ MiSLAS  & 21CVPR & Data imbalance & Image classification \\ PaC  & 21ICCV & Data imbalance, extreme \# of categories & Image classification \\ GraphSMOTE  & 21WSDM & Data imbalance & Node classification \\ ImGAGN  & 21KDD & Data imbalance & Node classification \\ TailGNN  & 21KDD & Data imbalance, extreme \# of categories & Node classification \\ LTE4G  & 22CIKM & Data imbalance, extreme \# of categories & Node classification \\ SmoteR  & 13EPIA & Data imbalance & Regression \\ SMOGN  & 17PKDD/ECML & Data imbalance & Regression \\  

Table 3: Algorithms available in the HeroLT benchmark.

(using an upsampling technique) shows superior performance to NearMiss (using a downsampling technique). Specifically, SMOTE is 85.43% highly balanced accuracy than NearMiss on the Abalone dataset. **(2)** While Acc treats all samples equally, bAcc considers data imbalance by averaging across classes. Although NearMiss achieves higher accuracy on the imbalanced Abalone dataset, it tends to exhibit a bias toward majority classes and does not sufficiently improve the minority classes, resulting in a lower bAcc score. Precision provides an insight into how much we can trust the model when it identifies a sample as Positive. Precision of NearMiss is lower as it may wrongly classify minority samples into other classes. Recall assesses the model's ability to identify all the Positive samples. NearMiss fails to characterize minority classes, it typically scores lower on Recall. Similarly, MAP is calculated by averaging across all classes and exhibits a similar trend with the other metrics.

### Algorithm Performance on Multi-Label Text Classification

In Table 5, we provide a performance comparison of three methods for multi-label text classification tasks on sequential datasets. We find: **(1)** Among these methods, XR-Transformer and X-Transformer demonstrate comparably superior performance on multiple metrics. On Eurlex-4K, XR-Transformer achieves a 1.37% improvement in ACC@1 compared to the second-best method. On AmazonCat-13K, X-Transformer exhibits a 1.03% improvement in Acc@5 than the suboptimal method. **(2)** Conversely, XR-Linear consistently exhibits the poorest performance across all three datasets, which verifies the effectiveness of recursive learning of transformer encoders than linear methods. However, XR-Linear is more than 100x faster than XR-Transformer, making it suitable for scenarios with large dataset sizes and strict requirements on time-consuming. **(3)** Notably, all methods exhibit a limitation in effectively recognizing certain classes, as indicated by the observed low recalls across all datasets.

### Algorithm Performance on Image Classification and Instance Segmentation

Among the considered algorithms, OLTR and BALMS utilize meta-learning; BBN and MisLAS employ mixup techniques; TDE uses causal inference; BALMS, Decoupling, and MisLAS decouple the learning process; and PaCo utilizes contrastive learning. We have the following observations from the results in Table 6: **(1)** No single technique (_e.g._, meta-learning, decoupled training, mixup, or contrastive learning) can consistently perform the best across all tasks and datasets, and there are limited algorithms that consider both classification and segmentation tasks. Therefore, in contrast to taxonomy based on techniques in methods, the three novel angles we propose (data long-tailedness, data complexity, and task heterogeneity) may be more suitable for benchmarking. **(2)** PaCo and MisLAS consistently perform well in accuracy on three natural long-tailed datasets. In particular, Paco exhibits a remarkable overall accuracy of 58.3% on ImageNet-LT dataset, surpassing the

  Dataset & Method & Acc (\%) & bAcc (\%) & Precision (\%) & Recall (\%) & mAP (\%) & Time (s) \\    & SMOTE & **97.0\(\)1.1** & **98.5\(\)0.5** & **95.1\(\)1.1** & **98.5\(\)0.5** & **99.9\(\)0.0** & 0.6 \\  & NearMiss & 76.7\(\)0.0 & 88.1\(\)0.0 & 92.1\(\)0.0 & 88.1\(\)0.0 & 98.9\(\)0.0 & **0.4** \\    & SMOTE & 20.1\(\)1.1 & **19.1\(\)1.1** & **11.5\(\)0.6** & **19.1\(\)1.1** & 17.4\(\)0.2 & 12.4 \\  & NearMiss & **23.4\(\)0.0** & 10.3\(\)0.0 & 7.0\(\)0.0 & 10.3\(\)0.0 & **18.8\(\)0.0** & **2.8** \\  

Table 4: Comparing the methods on long-tailed tabular datasets. Each point is the mean and standard deviation of 10 runs. ”Time” means training time plus inference time.

   &  &  &  &  &  & Time (s) \\  & & @1 & @3 & @5 & @1 & @3 & @5 & @1 & @3 & @5 & @1 & @3 & @5 & \\    Eurlex- \\ 4K \\  } & XR-Transformer & **88.2** & **75.8** & 62.8 & **88.2** & **75.8** & 62.8 & **17.9** & **45.2** & 61.1 & **88.2** & **82.0** & **75.6** & 66.9 \\  & X-Transformer & 87.0 & 75.2 & **62.9** & 87.0 & 75.2 & **62.9** & 17.7 & 44.8 & **61.2** & 87.0 & 81.1 & 75.1 & 433.9 \\  & XR-Linear & 82.1 & 69.6 & 58.2 & 82.1 & 69.5 & 82.6 & 16.6 & 41.4 & 56.6 & 82.1 & 75.9 & 69.9 & **0.2** \\    AmazonCat- \\ 13K \\  } & XR-Transformer & **96.7** & 83.6 & 67.9 & **96.7** & 83.6 & 67.9 & **27.7** & 63.3 & 79.0 & **96.7** & 90.5 & 83.1 & 78.3 \\  & X-Transformer & **96.7** & **83.9** & **68.6** & **96.7** & **83.6** & **86.2** & 63.4 & **79.7** & **96.7** & **90.6** & **83.4** & 428.6 \\  & XR-Linear & 93.0 & 78.9 & 64.3 & 93.0 & 78.9 & 64.3 & 26.3 & 59.7 & 75.2 & 93.0 & 86.1 & 78.9 & **0.3** \\    Wiki10- \\ 31K \\  } & XR-Transformer & 88.0 & **79.5** & **69.7** & 88.0 & **79.5** & **69.7** & **5.3** & **14.0** & **20.1** & 88.0 & **83.9** & **79.2** & 117.3 \\  & X-Transformer & **88.5** & 78.5 & 69.1 & **88.5** & 78.5 & 69.1 & **5.3** & 13.8 & 19.8 & **88.5** & 83.6 & 78.7 & 433.2 \\  & XR-Linear & 84.6 & 73.0 & 64.3 & 84.6 & 73.0 & 64.3 & 5.0 & 12.7 & 18.4 & 84.6 & 78.7 & 73.7 & **1.1** \\  

Table 5: Comparison of different methods on long-tailed sequential datasets for multi-label text classification tasks. ”Time” refers to inference time. The results of bAcc@k are not reported since no relevant literature discusses how to use this metric in the multi-label setting.

suboptimal method by 16.37%, MisLAS exhibits a remarkable overall accuracy of 71.6% on iNatural 2018 dataset. The superiority of PaCo and MisLAS is further evident in the tail category, where their few-shot accuracy surpasses other methods by 27.15% and 14.90% on Places-LT. **(3)** The few-shot accuracy is often lower than the many-shot accuracy for all methods. For example, on ImageNet-LT, Decoupling exhibits particularly poor performance in terms of few-shot accuracy, with a decrease of 90.94% compared to many-shot accuracy. But on iNatural 2018, PaCo achieves a few-shot accuracy of 69.4%, surpassing many-shot accuracy of 66.7% and medium-shot accuracy of 68.0%.

CIFAR-10-LT and CIFAR-100-LT are semi-synthetic datasets, where the number of samples in each class is determined by a controllable imbalance factor (typically 10, 50, 100). Similarly, we have the observations as shown in Table 7: **(1)** PaCo and BALMS show the top-two best accuracy performance on the synthetic CIFAR-10-LT and CIFAR-100-LT with varying degrees of IF. In conjunction with experimental results on natural datasets, the overall performances of decoupled learning (BALMS and MiSLAS) and contrastive learning (Paco) are generally superior. In addition, decoupled learning has the potential to be applied to multiple tasks and therefore has the potential to deal with data long-tailedness and task heterogeneity. but the stable performances in the few-shot categories still need to be considered. **(2)** As we control IF of the synthetic datasets to be larger, the long-tailed

   &  &  &  &  &  &  &  \\  & & & Many & Medium & Few & Overall & (\%) & (\%) & (\%) & (s) \\    &  & OLTR & 37.9 & 36.1 & 30.8 & 36.1 & 52.4 & 36.1 & 20.5 & 69.8 \\  & & BALMS & 50.1 & 39.6 & 25.3 & 41.6 & 41.2 & 41.6 & 20.6 & 29.0 \\  & & IDE & 60.5 & 47.2 & 30.4 & 50.1 & 50.1 & 51.0 & 28.7 & 30.9 \\  & &  & Decoupling & 64.0 & 33.8 & 5.8 & 41.6 & 41.6 & 49.9 & 21.1 & 21.8 \\  & & BBN & 59.4 & 45.4 & 16.3 & 46.6 & 47.5 & 46.6 & 24.9 & 43.0 \\  & & MiSLAS & 60.9 & 46.8 & 32.5 & 50.0 & 39.0 & 38.0 & 21.0 & **18.9** \\  & & PaCo & **67.8** & **56.5** & **37.8** & **58.3** & **58.3** & **58.3** & **37.5** & 58.9 \\    &  & OLTR & **44.0** & 40.6 & 28.5 & 39.3 & 39.5 & 39.3 & 17.9 & 30.6 \\  & & BALMS & 41.0 & 39.9 & 30.2 & 38.3 & 38.1 & 38.3 & 17.1 & 29.0 \\  & & TDE & 30.5 & 29.3 & 19.5 & 27.8 & 27.8 & 29.1 & 9.8 & 18.6 \\  & & Decoupling & 40.6 & 39.1 & 28.6 & 37.6 & 37.6 & 38.2 & 16.7 & 28.9 \\  & & BBN & 34.8 & 32.0 & 5.8 & 27.5 & 30.7 & 27.5 & 9.4 & **15.4** \\  & & MiSLAS & 42.4 & 41.8 & 34.7 & 40.5 & 41.0 & 40.5 & 19.2 & 16.2 \\  & & PaCo & 34.8 & **48.1** & **38.4** & **41.4** & **41.2** & **41.4** & **20.1** & 56.1 \\    &  & OLTR & 62.5 & 52.2 & 42.2 & 48.8 & 50.8 & 48.8 & 33.3 & 33.9 \\  & & BALMS & 37.1 & 31.9 & 7.9 & 28.7 & 32.5 & 28.7 & 10.4 & 52.9 \\   & & IDE & 63.1 & 62.1 & 54.8 & 59.3 & 59.3 & 65.6 & 43.7 & 24.8 \\   & &  &  & Decoupling & 69.0 & 65.8 & 63.1 & 65.1 & 65.1 & 71.0 & 50.6 & 23.9 \\   & & BBN & 61.8 & **73.5** & 67.7 & 69.7 & 72.8 & 69.7 & 55.9 & 29.3 \\   & & MiSLAS & **72.3** & 72.3 & **70.7** & **71.6** & **74.6** & **71.6** & **58.0** & **19.6** \\   & & PaCo & 66.7 & 68.0 & 69.4 & 68.4 & 71.0 & 68.4 & 53.9 & 53.8 \\   LVIS v0.5 &  Instance \\ segmentation \\  } & BALMS & **62.9** & **34.7** & **16.1** & **60.0** & **37.1** & **46.8** & **37.1** & **1436.1** \\  

Table 6: Comparison of different methods on natural long-tailed grid datasets. “Time” refers to inference time. Recall equals to bAcc for multi-class classification.

   &  &  &  &  &  &  \\  & & 100 & 50 & 100 & 50 & 100 & 100 & 50 & 10 & 100 & 50 & 10 & \\    & OLTR & 28.1 & 29.1 & 33.7 & 21.9 & 20.9 & 33.7 & 28.1 & 29.1 & 33.7 & 15.5 & 15.1 & 18.7 & 10.1 \\  & BALMS & 84.2 & 86.7 & 91.5 & 84.3 & 86.7 & 91.5 & 84.2 & 86.7 & 91.5 & 72.8 & 76.9 & 84.8 & **1.7** \\  & & IDE & 80.9 & 82.9 & 88.3 & 80.9 & 82.9 & 83.1 & 84.3 & 88.2 & 68.0 & 70.7 & 79.2 & 1.8 \\  & & Decoupling & 53.5 & 61.2 & 73.4 & 53.5 & 61.2 & 73.4 & 55.4 & 62.3 & 73.5 & 34.0 & 42.0 & 57.3 & 1.8 \\  & & BBN & 80.7 & 83.4 & 88.7 & 81.0 & 84.0 & 88.8 & 80.7 & 83.4 & 88.7 & 67.5 & 71.8 & 80.0 & 5.2 \\  & & MiSLAS & 82.5 & 85.7 & 90.0 & 83.4 & 86.1 & 90.1 & 82.5 & 85.7 & 90.1 & 70.5 & 75.2 & 82.3 & 8.2 \\  & & PaCo & **85.9** & **88.3** & **91.7** & **86.0** & **88.3** & **91.7** & **85.9** & **88.3** & **91.7** & **75.5** & **79.4** & **85.1** & 4.6 \\    & OLTR & 7.9 & 2.2 & 5.5 & 6.4 & 12.0 & 7.9 & 9.2 & 12.5 & 19.2 & 22.2 & 3.2 & 12.9 \\  & BALMS & 51.0 & 56.2 & 55.2 & 50.0 & **56.0** & 54.8 & 51.0 & 56.2 & 55.2 & 29.5 & **34.9** & 33.

phenomenon is more severe (showing higher values of Gini coefficient, IF, and Pareto-LT Ratio), and the performance of nearly all methods exhibits a decline. **(3)** CIFAR-100-LT dataset appears more affected than CIFAR-10-LT under the different settings of IF, possibly because it has a more severe long-tailed phenomenon with a larger number of categories.

### Algorithm Performance on Node Classification

From the results of long-tailed algorithms for node classification on relational datasets in Table 8, we have: **(1)** No method consistently outperforms all others across all datasets. The performance of different runs on the same dataset may have large variances, _e.g._, LTE4G on Wiki. **(2)** GraphSmote exhibits promising performance on Wiki dataset. Wiki presents high IF and Gini coefficient yet a low Pareto-LT Ratio, hinting that the main challenge may stem from data imbalance as discussed in _Angle 1_. Therefore, employing the simple augmentation method may yield great results. Amazon_Clothing exhibits relatively low IFs and Gini coefficients but high Pareto-LT Ratios, which may indicate the need for increased focus on addressing the challenge caused by the number of categories (but it does not mean it necessarily contains more categories). The insight may elucidate why Tail-GNN and LTE4G, which can characterize the extensive number of categories, exhibit more significant performance. Although these metrics can give some understanding of a dataset, none can comprehensively and accurately depict all of the characteristics, and the analysis may exhibit limitations in specific datasets. **(3)** Tail-GNN exhibits superior performance on Cora-full and Amazon_clothing. Especially on Cora-full, Tail-GNN achieves a 4.93% higher accuracy than the second-best method. However, the scalability of Tail-GNN shows limitations. It faces out-of-memory problems on Amazon_Eletronics with 42,318 nodes and 129,430 edges. **(4)** The performance of ImGAGN is relatively weak since it considers only one class as the tail class by default. This limitation becomes apparent in datasets with a large number of classes. Nonetheless, ImGAGN shows a performance improvement by adjusting the number of classes considered as tail. In addition, ImGAGN is less time-consuming and is 5x faster than LTE4G on the largest Amazon_Eletronics dataset.

  Dataset & Method & Acc (\%) & bAcc (\%) & Precision (\%) & Recall (\%) & MAP (\%) & Time (s) \\    & GraphSmote & 60.5\(\)0.8 & 51.9\(\)0.6 & 60.2\(\)0.8 & 51.9\(\)0.6 & 54.9\(\)0.4 & 718.8 \\  & ImAGGN & 4.2\(\)0.8 & 1.5\(\)0.1 & 0.2\(\)0.1 & 1.5\(\)0.1 & 2.7\(\)0.1 & **69.6** \\  & Tail-GNN & **63.8\(\)0.3** & **54.6\(\)0.6** & **63.8\(\)0.4** & **54.6\(\)0.6** & **66.8\(\)0.6** & 906.2 \\  & LTE4G & 60.8\(\)0.5 & **54.6\(\)0.5** & 61.5\(\)0.6 & **54.6\(\)0.5** & 51.1\(\)0.9 & 281.6 \\    & GraphSmote & **66.0\(\)0.9** & **51.1\(\)2.0** & **66.3\(\)1.1** & **51.1\(\)2.0** & 64.4\(\)1.9 & 52.3 \\  & ImAGGN & 45.4\(\)6.7 & 24.5\(\)4.1 & 44.8\(\)5.0 & 24.5\(\)4.1 & 64.1\(\)6.2 & **14.3** \\  & Tail-GNN & 63.6\(\)0.7 & 47.7\(\)1.1 & 64.0\(\)1.4 & 47.7\(\)1.1 & **65.0\(\)2.1** & 22.5 \\  & LTE4G & 58.2\(\)15.8 & 48.9\(\)1.2 & 56.0\(\)2.01 & 48.9\(\)1.25 & 59.3\(\)16.5 & 53.1 \\    & GraphSmote & 58.3\(\)1.2 & 34.2\(\)1.4 & 54.4\(\)2.0 & 34.2\(\)1.4 & 44.6\(\)2.4 & 126.2 \\  & ImAGGN & 42.7\(\)1.9 & 23.0\(\)1.4 & 38.4\(\)2.5 & 23.0\(\)1.4 & 35.7\(\)2.2 & 19.7 \\  & Tail-GNN & 56.5\(\)1.7 & **34.5\(\)1.6** & **55.6\(\)0.4** & **34.5\(\)1.6** & **58.0\(\)3.0** & **8.6** \\  & LTE4G & **58.7\(\)2.0** & 34.3\(\)2.1 & 54.1\(\)2.5 & 34.3\(\)2.1 & 47.0\(\)2.1 & 72.1 \\    & GraphSmote & 6.3\(\)0.4 & 63.2\(\)0.4 & 64.9\(\)0.3 & 63.2\(\)0.4 & 57.6\(\)0.9 & 919.1 \\  & ImAGGN & 30.3\(\)1.1 & 12.8\(\)0.7 & 23.3\(\)1.2 & 12.8\(\)0.7 & 32.0\(\)0.7 & **89.6** \\  & Tail-GNN & **69.2\(\)0.6** & **65.9\(\)0.5** & **67.7\(\)0.5** & **65.9\(\)0.5** & **68.7\(\)0.1** & 768.7 \\  & LTE4G & 65.6\(\)0.6 & 64.2\(\)0.6 & 64.8\(\)0.5 & 64.2\(\)0.6 & 54.3\(\)2.7 & 349.7 \\    & GraphSmote & **56.2\(\)0.5** & 51.7\(\)0.5 & 55.6\(\)0.4 & 51.7\(\)0.5 & **36.2\(\)1.9** & 3406.2 \\  & ImAGGN & 17.1\(\)1.1 & 7.4\(\)0.6 & 12.3\(\)0.7 & 7.4\(\)0.6 & 16.8\(\)0.7 & **218.5** \\   & Tail-GNN & OOM & OOM & OOM & OOM & OOM & OOM & OOM \\   & LTE4G & 55.8\(\)0.4 & **53.0\(\)0.5** & **56.1\(\)0.3** & **53.0\(\)0.5** & 32.7\(\)2.2 & 1112.8 \\  

Table 8: Comparing the methods on long-tailed relational datasets. Each point is the mean and standard deviation of 10 runs. “Time” means training time plus inference time.

  Dataset & Method &  &  &  &  & Time \\  & Many & Med. & Few & All & Many & Med. & Few & All & Many & Med. & Few & All & (s) \\    & SmoteR & **0.12** & **0.12** & **0.20** & **0.12** & **0.02** & **0.02** & **0.07** & **0.02** & 50.2 & **98.2** & **96.9** & **97.3** & **0.08** & **0.08** & **0.14** & **0.08** & 56.6 \\  & SMOGN & 0.40 & 0.35 & 0.43 & 0.37 & 0.17 & 0.13 & 0.24 & 0.15 & **53.2** & 97.3 & 91.3 & 95.4 & 0.38 & 0.33 & 0.36 & 0.35 & **35.0** \\  

Table 9: Comparing the methods on long-tailed tabular regression dataset. “Time” means training time plus inference time.

### Algorithm Performance on Regression

There has been limited work considering regression tasks on long-tailed data. In Table 9, we present a comparison of the performance of two methods. It is observed that SmoteR outperforms SMOGN in terms of MAE, MSE, Pearson correlation, and GM metrics across many-shot, medium-shot, and few-shot regions, as well as overall performance. However, SmoteR requires a longer time compared to SMOGN. Additionally, SMOGN appears to overfit to the many-shot regions during training.

## 4 Related Work

Recently, several papers that review long-tailed problems have been published. One of the earliest works  aims to improve the performance of long-tailed algorithms through a reasonable combination of existing tricks. Yang et al.  conduct a survey on long-tailed visual recognition and shows a taxonomy of existing methods. Fu et al.  divide methods into three categories, namely, training, fine-tuning, and inference stage. Fang et al.  group methods based on balanced data, balanced feature representation, balanced loss, and balanced prediction. Zhang et al.  categorize them into class re-balancing, information augmentation, and module improvement. Although these papers summarize studies on long-tailed visual recognition, they pay less attention to the extent of long-tailedness and fail to consider data complexity and task heterogeneity.

Next, we highlight the similarities and differences of long-tailed learning with related areas, e.g., imbalanced learning  and few-shot learning [109; 110]. Imbalanced Learning focuses on learning from imbalanced data, where minority classes contain significantly fewer training samples than majority classes. In imbalance learning, the class numbers may be small, while the number of minority samples is not necessarily small. But in long-tailed learning, the number of classes is larger and samples in tail classes are often very scarce. Few-shot Learning aims to train well-performing models from limited supervised samples. Long-tailed datasets, like few-shot datasets, have limited labeled samples in tail classes but with an imbalanced distribution. In contrast, few-shot datasets tend to be more balanced.

## 5 Conclusions and Future Work

In this paper, we introduce HeroLT, the most comprehensive heterogeneous long-tailed learning benchmark with 18 state-of-the-art algorithms and 10 evaluation metrics on 17 real-world benchmark datasets across 6 tasks and 4 data modalities. Based on the analyses of three pivotal angles, we gain valuable insights into the characterization of data long-tailedness, the data complexity of various domains, and the heterogeneity of emerging tasks. Our benchmark and evaluations are released at https://github.com/SSSKJ/HeroLT.

On top of them, we suggest intellectual challenges and promising research directions in long-tailed learning: **(C1)** Theoretic Challenge: Current work lacks sufficient theoretical tools for analyzing long-tailed models like their generalization performance. **(C2)** Algorithmic Challenge: Existing research typically focuses on one task in one domain, while there is a trend to consider multiple forms of input data (_e.g._, text and images) by multi-modal learning [111; 112; 113], or to solve multiple learning tasks (_e.g._, segmentation and classification) by multi-task learning [114; 115]. **(C3)** Application Challenge: In open environments, many datasets exhibit long-tailed distributions. However, long-tailed problems in domains like antibiotic resistance genes [10; 11] receive insufficient attention.