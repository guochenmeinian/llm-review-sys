# Linear Regression using Heterogeneous Data Batches

Ayush Jain

Granica Computing Inc.

ayush.jain@granica.ai &Rajat Sen

Google Research

senrajat@google.com &Weihao Kong

Google Research

weihaokong@google.com &Abhimanyu Das

Google Research

abhidas@google.com &Alon Orlitsky

UC San Diego

alon@ucsd.edu

This work was done while the author was a student at UCSD and interning part-time at Google Research.

###### Abstract

In many learning applications, data are collected from multiple sources, each providing a _batch_ of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are \(k\) subgroups, each with its own regression vector. Prior work  showed that with abundant small-batches, the regression vectors can be learned with only few, \((k^{3/2})\), batches of medium-size with \(()\) samples each. However, the paper requires that the input distribution for all \(k\) subgroups be isotropic Gaussian, and states that removing this assumption is an "interesting and challenging problem". We propose a novel gradient-based algorithm that improves on the existing results in several ways. It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite \(k\); (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.

## 1 Introduction

In numerous applications, including federated learning , sensor networks , crowd-sourcing  and recommendation systems , data are collected from multiple sources, each providing a _batch_ of samples. For instance, in movie recommendation systems, users typically rate multiple films. Since all samples in a batch are generated by the same source, they are often assumed to share the same underlying distribution. However, the batches are frequently very small, e.g., many users provide only few ratings. Hence, it may be impossible to learn a different model for each batch.

A common approach has therefore assumed  that all batches share the same underlying distribution and learn this common model by pooling together the data from all batches. While this may work well for some applications, in others it may fail, or lack personalization. For instance, in recommendation systems, it may not capture the characteristics of individual users.

A promising alternative that allows for personalization even with many small batches, assumes that batches can be categorized into \(k\) sub-populations with similar underlying distributions. Hence ineach sub-population, all underlying distributions are close to and can be represented by a single distribution. Even when \(k\) is large, our work allows the recovery of models for sub-populations with a significant fraction of batches. For example, in the recommendation setting, most users can be classified into a few sub-populations such that the distribution of users in the sub-population is close, for instance, those preferring certain genres.

In this paper, we focus on the canonical model of linear regression in supervised learning. A distribution \(\) of samples \((x,y)\) follows a linear regression model if, for some regression vector \(w^{d}\), the output is \(y=w x+\) where the input \(x\) is a random \(d\) dimensional vector and \(\) is a zero-mean noise. While we allow the presence of sub-populations that do not follow the linear regression model or have very few batches, our goal is to recover the regression vectors for all large sub-populations that follow this linear regression model.

### Our Results

This setting was first considered in  for meta-learning applications, where they view and term batches as _tasks_.  argue that in meta-learning applications task or batch lengths follow a long tail distribution and in the majority of the batches only a few labeled examples are available. Only a few batches have medium-size labeled samples available, and almost all of them have length \( d\). Note that similar observations have been made in the recommender system literature where the distribution of a number of ratings per user follows a long-tailed distribution with an overwhelming number of users rating only a few items while rare tail users rating hundreds of items . The same has been observed for the distribution of the number of ratings per item . Therefore, it is reasonable to assume that in these applications of interest, a handful of medium-size batches along with a large number of batches of constant size are available. Under this setting our main results allow recovery of all sub-populations that has a significant fraction of batches and follow a linear regression model.

Building upon the preceding discussion, we have two sets of batches. Each batch comprises i.i.d. samples drawn from one of the \(k\) sub-populations. The distribution of samples for these sub-populations is unknown, as is the identity of the sub-population from which each batch originates. Batches in set \(B_{s}\) (denoted as _small_) consist of at least two samples each, while those in set \(B_{m}\) (denoted as _medium_) are of larger size, containing \( n_{m}\) samples. Let's consider a sub-population indexed by \(i\), which follows a linear regression model with parameter vector \(w_{i}\) and output-noise variance \(^{2}\). The next theorem presents the guarantees provided by our algorithm for estimating \(w_{i}\).

**Theorem 1.1** (Informal).: _Let \(>0\) such that both sets \(B_{s}\) and \(B_{m}\) comprises at least \(\) fraction of batches from a sub-population \(i\). Given \(|B_{s}|(d/^{2})\), \(|B_{m}|((,1/)/)\), and \(n_{m}((,1/))\), our algorithm runs in time \(poly(d,1/,k)\) and outputs a list \(L\) of size \((1/)\) such that w.h.p., at least one estimate in \(L\) is within a distance of \(o()\) from \(w_{i}\) and has an expected prediction error \(^{2}(1+o(1))\) for sub-population \(i\). Moreover, given \(( L)\) samples from sub-population \(i\), we can identify such an estimate from \(L\)._

In the above theorem, the batches within set \(B_{s}\) must contain a minimum of 2 samples, while those within set \(B_{m}\) must be of size \( n_{m}\). Additionally, both sets must include \(\) fraction of batches originating from the population we want to recover. It follows that our algorithm requires only \((2|B_{s}|+n_{m}|B_{s}|)=(d/)\) samples from that specific sub-population. It's worth noting that any algorithm, even when dealing with a single sub-population (\(k=1\)), requires a minimum of \((d)\) samples from said sub-population. To the best of our knowledge, ours is the best sample complexity for recovering the linear regression models in the presence of multiple sub-populations using batch sizes smaller than \(d\).

_Remark 1.1_.: When multiple sub-populations meet the criteria outlined for sub-population \(i\), the list \(L\) returned by our algorithm includes estimates of the regression vector for each of these sub-populations. Furthermore, since there can be up to \(\{k,1/\}\) distinct sub-populations that satisfy these criteria, any algorithm that aims to provide meaningful guarantees must produce a list of size \(\{k,1/\}\), unless the algorithm has access to a sample set known to originate from the specific sub-population \(i\) it aims to recover. A similar situation arises in the list-decodable setup , where also the algorithm returns a list of estimates.

### Comparison to Prior Work

The only work that provides a polynomial-time algorithm in dimension \(d\), in the same generality as ours is . They even allow the presence of adversarial batches. However, they require \((d/^{2})\) batches from the sub-population, each of them of size \((1/)\), and therefore, \((d/^{3})\) samples in total, which exceeds our sample complexity by a factor of \(1/^{2}\). Furthermore, their algorithm requires a minimum number of samples per batch, which is quadratically larger than ours. All other works place strong assumptions on the distributions of the sub-population and still require a number of samples much larger than ours, which we discuss next.

Most of the previous works  have addressed the widely studied _mixed linear regression (MLR)_ model where all batches are of size 1, and adhere to the following three assumptions:

1. All \(k\) sub-populations have \(\) fraction of data. This assumption implies \(k 1/\).
2. All \(k\) distributions follow a linear regression model.
3. All \(k\) regression coefficients are well separated, namely \(\|w_{i}-w_{j}\|,\,i j\).

Even for \(k=2\), solving MLR, in general, is NP-hard . Hence all these works on mixed linear regression, except , also made the following assumption:

1. All input distributions (i.e., the distribution over \(x\)) are the same for every sub-population, in fact, the same isotropic Gaussian distribution.

With this additional isotropic Gaussian assumption, they provided algorithms that have runtime and sample complexity polynomial in the dimension. However, even with these four strong assumptions, their sample complexity is super-polynomial overall. In particular, the sample complexity in  is quasi-polynomial in \(k\) and  require at least a quadratic scaling in \(d\). In  the sample complexity scales as a large negative power of the minimum singular value of certain moment matrix of regression vectors that can be zero even when the gap between the regression vectors is large. In addition,  required zero-noise i.e \(=0\). The only work we are aware of that can avoid Assumption 4 and handle different input distributions for different sub-populations under MLR is . However, they still require all distributions to be Gaussian and \(=0\), and their sample size, and hence run-time is exponential, \(((k^{2}))\) in \(k\). Recently,  proposed and analyzed an approximate message-passing algorithm for mixed linear regression. However, their analysis is asymptotic, providing convergence guarantees as the number of samples and dimensions tends to infinity, while assuming a finite number of components. The results also rely on Assumptions 2 and 4.

The work that most closely relates to ours is , which considers batch sizes \(>1\). While it achieves the same dependence as us on \(d,k\), and \(1/\), on the length and number of medium and small batches, the sample complexity of the algorithms and the minimum length required for medium-size batches had an additional multiplicative dependence on the inverse separation parameter \(1/\). It also required Assumption 4 mentioned in the section. The follow-up work  which still assumes all four assumptions can handle the presence of a small fraction \(^{2}/k^{2}\) of adversarial batches, but requires \((dk^{2}/^{2}+k^{5}/^{4})\) samples. It also suffers from strong assumptions similar to those in earlier work, and the sum-of-squares approach makes it impractical. The sum of the square approach, and stronger isotropic Gaussian assumption, allow it to achieve a better dependence on \(1/\) on medium-size batch lengths, however, causing a significant increase in the number of medium-size batches required.

**Our improvement over prior work.** In contrast, our work avoids all four assumptions, and can recover any sufficiently large sub-populations that follow a linear regression model. In particular: (1) Even when a large number of different sub-populations are present, (e.g., \(k 1/\)), we can still recover the regression coefficient of a sub-population with sufficient fraction of batches. (2) The \(k\) distributions do not even need to follow a linear regression model. In particular, our algorithm is robust to the presence of other sub-populations for which the conditional distribution of output given input is arbitrary. (3) Our work requires no assumption on the separation of regression coefficient \(\), and our guarantees as well have no dependence on the separation. (4) We allow different input distributions for different sub-populations, which can be unknown and heavy tailed, (5) In additionto removing the four assumptions, the algorithm doesn't require all batches in a sub-population to have identical distributions, it only requires them to be close (see Remark 2.1).

### Techniques and Organization

To recover the regression vector for a sub-population \(i\) that follow the linear regression model and has more than \(\) fraction of batches, the algorithm begins by selecting a medium size batch from this sub-population. To obtain such a batch we randomly choose \((1/)\) batches from the set of medium-size batches \(B_{m}\) and repeat our algorithm to obtain one estimate for every batch chosen. With high probability, at least one of the randomly selected medium-sized batches belongs to sub-population \(i\), and when the algorithm is run for such a batch, w.h.p. it recovers the estimate of \(w_{i}\). Therefore, at least one of the \((1/)\) estimates accurately recover \(w_{i}\).

The regression vector \(w_{i}\) minimizes the expected squared loss for the sub-population \(i\). Therefore, we use a gradient-descent-based approach to estimate \(w_{i}\). We start with an initial estimate (all zero) and improve this estimate by performing multiple rounds of gradient descent steps.

First, using a large number of smaller batches we estimate a smaller subspace of \(^{d}\) that preserves the norm of the gradient for sub-population \(i\). Next, using the medium-size batch selected at the beginning, we test which of the other medium-size batches have a projection of gradient close to the chosen batch, and use them to estimate the gradient in the smaller subspace. The advantage of sub-space reduction is that testing and estimation of the gradient in the smaller subspace is easier, consequently reducing the minimum length of medium-size batches required for testing and the number of medium-size batches required for estimation. Our gradient estimation approach draws inspiration from . However, while they employed it to directly estimate regression vectors of all sub-populations simultaneously, we adapt it to sequentially estimate gradients for one sub-population at a time. A crucial new ingredient of our algorithm is the clipping of gradients, which limits the effect of other components and allows the algorithm to work even for heavy-tailed distributions.

We describe the algorithm in detail in Section 3 after having presented our main theorems in Section 2. Then in Section 4 we compare our algorithm with the one in  on simulated datasets, to show that our algorithm performs better in the setting of the latter paper as well as generalizes to settings that are outside the assumptions of .

## 2 Problem Formulation and Main Results

Consider distributions \(_{0},,_{k-1}\) over input-output pairs \((x,y)^{d}\) each corresponding to one of the \(k\) sub-populations. A _batch_\(b\) consists of i.i.d. samples from one of the distributions. Samples in different batches are independent. There are two sets of batches. The batches in \(B_{s}\) are _small_ and contain at least two samples each, while batches in \(B_{m}\) are of medium size and contain at least \(n_{m}\) samples. For any batch the identity of the distribution it is sampled from is unknown. Next, we describe the distributions. To aid it and the remaining paper we first introduce some notation.

The \(L_{2}\)_norm_ of a vector \(u\) is denoted by \(\|u\|\). The _norm_, or _spectral norm_, of a matrix \(M\) is denoted by \(\|M\|\) and is defined as \(_{\|u\|=1}\|Mu\|\). If \(M\) is a symmetric matrix, the norm simplifies to \(\|M\|=_{\|u\|=1}|u^{}Mu|\). The trace of a symmetric matrix \(M\) is \((M):=_{i}M_{ii}\), the sum of the elements on the main diagonal of \(M\). We will use the symbol \(S\) to denote an arbitrary collection of samples. For a batch denoted by \(b\), we will use \(S^{b}\) to represent the set of all samples in the batch.

### Data Distributions

Let \(_{i}:=_{_{i}}[xx^{}]\) denote the second-moment matrix of input for distribution \(_{i}\).

To recover the regression vector for a distribution \(_{i}\), we assume that it satisfies the following assumptions2:

1. (Input distribution) There are constants \(C\) and \(C_{1}\) such that for all \(i I\),1. \(L4\)-\(L2\) hypercontractivity: For all \(u^{d}\), \(_{_{i}}[(x u)^{4}] C(_{_{i}}[(x  u)^{2}])^{2}\). 2. Bounded condition number: For normalization purpose, we assume \(_{\|u\|=1}u^{}_{i}u 1\) and to bound the condition number we assume that \(\|_{i}\| C_{1}\).
2. (Input-output relation) There is a \(>0\) s.t. for all \(i I\), \(y=w_{i} x+\), where \(w_{i}^{d}\) is an unknown regression vector, and \(\) is a noise independent of \(x\), with zero mean \(_{_{i}}[]\!=\!0\), and \(_{_{i}}[^{2}]\!\!^{2}\). Note that by definition, the distribution of \(\) may differ for each \(i\).

For a given \(_{s}>0\) and \(_{m}>0\), our goal is to estimate the regression vector \(w_{i}\) for all sub-populations that have at least \(_{s}\) and \(_{m}\) fractions of the batches within \(B_{s}\) and \(B_{m}\), respectively, and follow the assumptions described above. Let \(I\{0,1,..,k-1\}\) denote the collection of indices corresponding to all such sub-populations. (Note that in the introduction, we simplified this more general scenario by assuming equal values for \(_{s}\) and \(_{m}\), both denoted by \(\).)

For other sub-populations \(i I\), we only need that the input distribution satisfies \(\|_{i}\| C_{1}\), same as the second half of assumption 1(b). The input-output relation for samples generated by \(_{i}\) for \(i I\)_can be arbitrary_, and in particular, does not even need to follow a linear regression model. Moreover, the fraction of batches from these other sub-populations can also be arbitrary.

To simplify the presentation, we make two additional assumptions. First, there is a constant \(C_{2}>0\) such that for all components \(i\{0,1,..,k-1\}\), and random sample \((x,y)_{i}\), \(\|x\| C_{2}\), a.s. Second, for all \(i I\) and a random sample \((x,y)_{i}\), the noise distribution \(=y-w_{i} x\) is symmetric around \(0\). As discussed in Appendix J, these assumptions can be easily removed.

_Remark 2.1_.: For simplicity, we assumed that each batch exactly follows one of the \(k\) distributions. However, our methods can be extended to more general scenarios. Let \(^{b}\) denote the underlying distribution of batch \(b\). Instead of requiring \(^{b}=_{i}\) for some \(i\{0,1,..,k-1\}\), our approach can be extended to cases where the expected gradients for \(^{b}\) and \(_{i}\) are close. Additionally, if \(i I\), the regression vector \(w_{i}\) must achieve a mean squared error (MSE) of at most \(^{2}\) for \(^{b}\). These conditions are satisfied if the following hold: (1) \(\|\,_{^{b}}[xx^{}]-_{i}\|\) is small, (2) for all \(x^{d}\), \(|\,_{^{b}}[y|x]-_{_{i}}[y|x]|\) is small, and (3) if \(i I\) then for all \(x^{d}\), \(_{^{b}}[(y-w_{i} x)^{2}|x]^{2}\). Thus, the strict identity requirement \(^{b}=_{i}\) can therefore be replaced by these three approximation conditions in our analysis. This extension is feasible as our algorithm and its analysis are based on gradient descent, and can handle stochastic noise. Since the analysis tolerates statistical noise, it can also accommodate small deviations in the gradient distributions across batches in the same sub-population.

### Main Results

We begin by presenting our result for estimating the regression vector of a component \(_{i}\), for any \(i I\). This result assumes that in addition to the batch collections \(B_{s}\) and \(B_{m}\), we have an extra medium-sized batch denoted as \(b^{*}\) which contains samples from \(_{i}\) and w.l.o.g, we assume \(i=0\).

**Theorem 2.1**.: _Suppose index \(0\) is in set \(I\) and let \(b^{*}\) be a batch of \( n_{m}\) i.i.d. samples from \(_{0}\). Given \(,(0,1]\), \(|B_{s}|=(^{2}})\), \(n_{m}=(\{,}}\} })\), and \(|B_{m}|=(}\{,}}\})\), Algorithm 1 runs in polynomial time and returns estimate \(\), such that with probability \( 1-\), \(\|-w_{0}\|\)._

We provide a proof sketch of Theorem 2.1 and the description of Algortihm 1 in Section 3, and a formal proof in Appendix H. Algorithm 1 can be used to estimate \(w_{i}\) for all \(i I\), and the requirement of a separate batch \(b^{*}\) is not crucial. As mentioned in the introduction, it can be obtained by repeatedly sampling a batch from \(B_{m}\) and running the algorithm for these sampled \(b^{*}\). Since all the components in \(I\) have \(_{m}\) fraction of batches in \(B_{m}\), then randomly sampling \(b^{*}\) from \(B_{m}\), \((1/_{m})\) times would ensure that, with high probability, we have \(b^{*}\) corresponding to each component. We can then return a list of size \((1/_{m})\) containing estimates corresponding to each sampled \(b^{*}\). Then, with high probability, the list will have an estimate of the regression vectors for all components. Note that in this case, returning a list is unavoidable as there is no way to assign an appropriate index to the regression vector estimates. The following corollary follows from the above discussion and Theorem 2.1.

**Corollary 2.2**.: _For given \(,(0,1]\), if \(|B_{s}|\), \(n_{m}\), and \(|B_{m}|\), meet the requirements outlined in Theorem 2.1, then the above modification of Algorithm 1 runs in polynomial-time and outputs a list \(L\) of size \(}(1/_{m})\) such that with probability \( 1-\), the list has an accurate estimate for regression vectors \(w_{i}\) for each \(i I\), namely \(_{i I}_{ L}\|-w_{i}\|\)._

In particular, this corollary implies that for any \(i I\), the algorithm requires only \((d/_{s})\) batches of size two and \((\{,}}\})\) medium-size batches of size \((\{,}}\})\) from distribution \(_{i}\) to estimate \(w_{i}\) within an accuracy \(o()\). Furthermore, it is easy to show that any \(o()\) accurate estimate of regression parameter \(w_{i}\) achieves an expected prediction error of \(^{2}(1+o(1))\) for output \(y\) given input \(x\) generated from this \(_{i}\).

Note that results work even for infinite \(k\) and without any separation assumptions on regression vectors. The \((,1/})\) dependence is the best of both words. This dependence is reasonable for recovering components with a significant presence or if the number is few.

The total number of samples required by the algorithm from \(_{i}\) in small size batches \(B_{s}\) and medium size batches \(B_{m}\) are only \(}(d/_{s})\) and \(}(\{k,1/_{s})\}\). It is well known that any estimator would require \((d)\) samples for such estimation guarantees even in the much simpler setting with just i.i.d. data. Consequently, the total number of samples required from \(_{i}\) by the algorithm is within \(}(1/_{s})\) factor from that required in a much simpler single component setting.

The next theorem shows that for any \(i I\), given a list \(L\) containing estimate of \(w_{i}\) and \(((1/_{s}))\) samples from \(_{i}\), we can identify an estimate of regression vector achieving a small prediction error for \(_{i}\). The proof of the theorem and the algorithm is in Appendix B.

**Theorem 2.3**.: _Let index \(i\) be in set \(I\) and let \(L\) be a list of \(d\) dimensional vectors. Suppose for a given \(>0\) at least one of the vectors in \(L\) is within distance \(\) from the regression vector of \(_{i}\), namely \(_{w L}\|w-w_{i}\|\). Given \((\{1,}{^{2}}\})\) samples from \(_{i}\) Algorithm 3 identifies an estimate \(w\) from the list \(L\), s.t. with probability \( 1-\), \(\|w-w_{i}\|=()\) and it achieves an expected estimation error \(_{_{i}}[( x-y)^{2}]^{2}+(^{2})\)._

_Remark 2.2_.: As a special case the above theorem implies that that for any \(()\), Algorithm 3 can identify the near best estimate of \(w_{i}\) from list \(L\) using only \(()\) samples from \(_{i}\).

Combining the above theorem and Corollary 2.2, we get

**Theorem 2.4**.: _For given \(,(0,1]\), if \(|B_{s}|\), \(n_{m}\), and \(|B_{m}|\), meet the requirements outlined in Theorem 2.1, then, there exists a polynomial-time algorithm that, with probability \( 1-\), outputs a list \(L\) of size \(}(1/_{m})\) containing estimates of \(w_{i}\)'s for \(i I\). Furthermore, given \(|S|(}})\) samples from \(_{i}\), for any \(i I\), Algorithm 3 returns \( L\) that with probability \( 1-\) satisfies \(\|w_{i}-\|()\) and achieves an expected estimation error \(_{_{i}}[( x-y)^{2}]^{2}+(^{2}^{2})\)_

## 3 Algorithm for recovering regression vectors

This section provides an overview and pseudo-code of Algorithm 1, along with an outline of the proof that achieves the guarantee stated in Theorem 2.1. As per the theorem, we assume that index \(0\) belongs to \(I\), and we have a batch \(b^{*}\) containing \(n_{m}\) samples from the distribution \(_{0}\). Note that \(_{0}\) satisfies the conditions mentioned in Section 2.1 and that \(B_{s}\) and \(B_{m}\) each have \(|B_{s}|_{s}\) and \(|B_{m}|_{m}\) batches with i.i.d. samples from \(_{0}\). However, the identity of these batches is unknown.

Gradient Descent.Note that \(w_{0}\) minimizes the expected square loss for distribution \(_{0}\). Our algorithm aims to estimate \(w_{0}\) by taking a gradient descent approach. It performs a total of \(R\) gradient descent steps. Let \(^{(r)}\) denote the algorithm's estimate of \(w_{0}\) at the beginning of step \(r\). Without loss of generality, we assume that the algorithm starts with an initial estimate of \(^{(1)}=0\). In step \(r\), the algorithm produces an estimate \(^{(r)}\) of the gradient of the expected square loss for distribution \(_{0}\) at its current estimate \(^{(r)}\). For convenience, we refer to this expected gradient for \(_{0}\) at \(^{(r)}\) simply as _expected gradient_. The algorithm then updates its current estimate for the next round as \(^{(r+1)}=^{(r)}-^{(r)}/C_{1}\).

The main challenge the algorithm faces is the accurate estimation of the expected gradients in each step. Accurately estimating the expected gradients at each step would require \((d/^{2})\) i.i.d. samples from \(_{0}\). However, our algorithm only has access to a medium-size batch \(b^{*}\) that is guaranteed to have samples from \(_{0}\) and this batch contains far fewer samples. And for batches in \(B_{s}\) and \(B_{m}\)the algorithm doesn't know which of the batches has samples from \(_{0}\). Despite these challenges, we demonstrate an efficient method to estimate the expected gradients accurately.

The algorithm randomly divides sets \(B_{s}\) and \(B_{m}\) into \(R\) disjoint equal subsets, denoted as \(\{B_{s}^{(r)}\}_{r=1}^{R}\) and \(\{B_{m}^{(r)}\}_{r=1}^{R}\), respectively. The samples in batch \(b^{*}\) are divided into two collections of equal disjoint subsets, denoted as \(\{S_{1}^{b^{*},(r)}\}_{r=1}^{R}\) and \(\{S_{2}^{b^{*},(r)}\}_{r=1}^{R}\). At each iteration \(r\), the algorithm uses the collections of medium and small batches \(B_{s}^{(r)}\) and \(B_{m}^{(r)}\), respectively, along with the two collections of i.i.d. samples \(S_{1}^{b^{*},(r)}\) and \(S_{2}^{b^{*},(r)}\) from \(_{0}\) to estimate the gradient at point \(w^{(r)}\). While this division may not be necessary for practical implementation, as shown by our simulation results later, this ensures independence between the stationary point \(^{(r)}\) and the gradient estimate, which facilitates our theoretical analysis and only incurs a logarithmic factor in sample complexity.

Next, we describe how the algorithm estimates the gradient and the guarantees of this estimation. Due to space limitations, we provide a brief summary here, and a more detailed description, along with formal proofs, can be found in the appendix. We start by introducing a clipping operation on the gradients, which plays a crucial role in the estimation process.

**Clipping.** Recall that the squared loss of samples \((x,y)\) on point \(w\) is \((x w-y)^{2}/2\) and its gradient is \((x w-y)x\). Instead of directly working with the gradient of the squared loss, we work with its clipped version. Given a _clipping parameter_\(>0\), the _clipped gradient_ for a sample \((x,y)\) evaluated at point \(w\) is defined as \( f(x,y,w,):= x\). For a collection of samples \(S\), the _clipped gradient_\( f(S,w,)\) is the average of the clipped gradients of all samples in \(S\), i.e., \( f(S,w,):=_{(x,y) S} f(x,y,w,)\).

The clipping parameter \(\) controls the level of clipping and for \(=\), the clipped and un-clipped gradients are the same. The clipping step is necessary to make our gradient estimate more robust, by limiting the influence of the components other than \(_{0}\) (in lemma C.1), and as a bonus, we also obtain better tail bounds for the clipped gradients. Theorem C.2 shows that for \((\,_{_{0}}[(y-x w)^{2}]})\), the difference between the expected clipped gradient and the expected gradient \(\|\,_{_{0}}[( f(x,y,w,)]-\,_{ _{0}}[(x w-y)x]\|\) is small. Therefore, the ideal value of \(\) at point \(w\) is \((\,_{_{0}}[(y-x w)^{2}]})\).

For the estimate \(^{(r)}\) at step \(r\), the choice of the clipping parameter is represented by \(^{(r)}\). To estimate a value for \(^{(r)}\) that is close to its ideal value, the algorithm employs the subroutine ClipEst (presented as Algorithm 4 in the appendix). The subroutine estimates the expected value of \((y-x^{(r)})^{2}\) by using i.i.d. samples \(S_{1}^{b^{*},(r)}\) from the distribution \(_{0}\). According to Theorem D.1 in Appendix D, the subroutine w.h.p. obtains \(^{(r)}\) which is close to the ideal value. This ensures that the difference between the expectation of clipped and unclipped gradients is small, and thus, estimating the expectation of clipped gradients can replace estimating the actual gradients.

**Subspace Estimation.** The algorithm proceeds by using subroutine GradSubEst (presented as Algorithm 5 in Appendix E) with \(=B_{s}^{(r)}\), \(w=^{(r)}\), and \(=^{(r)}\) to estimate a smaller subspace \(P^{(r)}\) within \(^{d}\). The objective of this subroutine is to ensure that, with high probability, the expected clipped gradient for distribution \(_{0}\) at point \(^{(r)}\) remains largely unchanged when projected onto the identified subspace. Thus, estimating the expected clipped gradient reduces to estimating its projection onto \(P^{(r)}\), which requires fewer samples due to the lower dimensionality of the subspace. To achieve this, the subroutine constructs a matrix \(A\) defined as \([A]=_{i}p_{i}\,_{_{i}}[ f(x,y,w, )]\,_{_{i}}[ f(x,y,w,)]^{}\), where \(p_{i}\) denotes the fraction of batches in \(\) that have samples from \(_{i}\). Since \(B_{s}^{(r)}\) are obtained by randomly partitioning \(B_{s}\), w.h.p. \(p_{0}_{s}\). It is crucial for the success of the subroutine that the expected contribution of every batch in the above expression is a PSD matrix. The clipping helps in bounding the contribution of other components and statistical noise.

The subroutine returns the projection matrix \(P^{(r)}\) for the subspace spanned by the top \(\) singular vectors of \(A\), where we choose \(=\{k,(1/_{s})\}\). It is worth noting that when \(1/_{s}\) is much smaller than \(k\) (thinking of the extreme case \(k=\)), our algorithm still only requires estimating the top \(=1/_{s}\) dimensional subspace, since those infinitely many components can create at most \((1/_{s}-1)\) directions with weight greater than \(_{s}\). This along with clipping ensures that the direction of \(_{0}\) must appear in the top \((1/_{s})\) subspace (see Lemma E.3). Theorem E.1 in Appendix E characterizes the guarantees for this subroutine. Informally, if \((d/^{2})\), then w.h.p., the expected value of the projection of the clipped gradient on this subspace is nearly the same as the expected value of the clipped gradient, namely \(\|\,_{_{0}}[P^{(r)} f(x,y,w,)]-_{ _{0}}[ f(x,y,w,)]\|\) is small.

We note that our construction of matrix \(A\) for the subroutine is inspired by a similar construction in , where they used it for directly estimating regression vectors. Our results generalize the applicability of the procedure to provide meaningful guarantees even when the number of components \(k=\). Additionally, Lemma E.3 improves matrix perturbation bounds in Lemma 5.1 of , which is crucial for applying this procedure for heavy-tailed distributions and reducing the number of required batches.

**Estimating the expectation of clipped gradient projection.** The final subroutine, GradEst, estimates the expected projection of the clipped gradient. It does this by utilizing two key sets: \(B_{m}^{(r)}\), one of the \(R\) partitions of medium-sized batches, and \(S_{2}^{b^{*},(r)}\), one of the \(2R\) partitions of the batch \(b^{*}\), which contains i.i.d. samples from the reference distribution \(_{0}\). GradEst begins by splitting each batch in \(B_{m}^{(r)}\) into two equal parts. For each batch \(b B_{m}^{(r)}\), the first part of the split is used along with samples from \(_{0}\) in \(S_{2}^{b^{*},(r)}\) to test whether the expected projection of the clipped gradient for the distribution that generated batch \(b\) is close to that of the reference distribution \(_{0}\). With high probability, the algorithm retains batches sampled from \(_{0}\) and rejects those from other distributions where the difference in expected projections is significant. This test requires \(()\)samples in each batch, where \(\) is the dimension of the projected clipped gradient, which is same as the dimension of the estimated subspace in the subspace estimation space.

After identifying the relevant batches, GradEst estimates the projection of the clipped gradients using the second half of the samples in these batches. Since the projections of the clipped gradients lie in an \(\) dimensional subspace, \(()\) samples suffice for the estimation. To obtain high-probability guarantees, the procedure uses the median of means approach for both testing and estimation.

The guarantees of the subroutine are described in Theorem F.1, which implies that the estimate \(^{(r)}\) of the gradient satisfies \(\|^{(r)}-_{_{0}}[P^{(r)} f(x,y,w,)]\|\) is small.

Estimation guarantees for expected gradient.Using the triangle inequality, we have:

\[\|^{(r)}-\,_{_{0}}[(x w-y)x]\| \|\,\,_{_{0}}[ f(x,y,w,)]-\,_{ _{0}}[(x w-y)x]\|\] \[+\|\,\,_{_{0}}[ f(x,y,w,)]- \,_{_{0}}[P^{(r)} f(x,y,w,)]\|+\|^{(r )}-\,_{_{0}}[P^{(r)} f(x,y,w,)]\|.\]

The first term on the right represents the difference between the expectations of the gradient and the clipped gradient. The second term captures the difference between the expectation of the clipped gradient and its projection onto the estimated subspace. Finally, the last term reflects the difference between the expectation of the projection of the clipped gradient and its estimate.

All expectations are taken with respect to the distribution \(_{0}\), and both gradients and clipped gradients are evaluated at the point \(w\). Recall that at the beginning of each gradient descent step, \(w\) is updated to the previous round's estimate of \(w_{0}\).

As previously argued, all three terms on the right side of the inequality are small, which implies that \(^{(r)}\) provides an accurate estimate of the expected value of the gradient for distribution \(_{0}\). Moreover, Lemma G.1 shows that with an accurate estimation of expected gradients, gradient descent reaches an accurate estimation of \(w_{0}\) after \((\|}{})\) steps. Therefore, setting \(R=(\|}{})\) suffices. This completes the description of the algorithm and the proof sketch of Theorem 2.1, with a more formal proof available in Appendix H.

## 4 Empirical Results

Setup.We have sets \(B_{s}\) and \(B_{m}\) of small and medium size batches and \(k\) distributions \(_{i}\) for \(i\{0,1,,k-1\}\). For a subset of indices \(I\{0,1,,k-1\}\), both \(B_{s}\) and \(B_{m}\) have a fraction of \(\) batches that contain i.i.d. samples from \(_{i}\) for each \(i I\). And for each \(i\{0,1,,k-1\} I\) in the remaining set of indices, \(B_{s}\) and \(B_{m}\) have \((1-|I|)/(k-|I|)\) fraction of batches, that have i.i.d samples from \(_{i}\). In all figures, the output noise is distributed as \((0,1)\). All small batches have \(2\) samples each, while medium-size batches have \(n_{m}\) samples each, which we vary from \(4\) to \(32\), as shown in the plots. We fix data dimension \(d=100\), \(=1/16\), the number of small batches to \(|B_{s}|=\{8dk^{2},8d/^{2}\}\) and the number of medium batches to \(|B_{m}|=256\). In all the plots, we averaged over 10 runs and report the standard error

Evaluation.Our objective is to recover a small list containing good estimates for the regression vectors of \(_{i}\) for each \(i I\). We compare our proposed algorithm's performance with that of the algorithm in . Given a new batch, we can choose the weight vector from the returned list, \(L\) that achieves the best error3. Then the MSE of the chosen weight is reported on another new batch drawn from the same distribution. The size of the new batch can be either 4 or 8 as marked in the plot. More details about our setup can be found in Appendix L.

**Setting in .** We first compare our algorithm with the one in  in the same setting as the latter paper i.e. with more restrictive assumptions. The results are displayed in Figure 1, where \(I=\{0,1,,15\}\) and all 16 distributions have been used to generate \(1/16\) fraction of the batches. All the \(_{i}\)'s are equal to \((0,I)\), and the minimum distance between the regression vectors is comparable to their norm. It can be seen that even in the original setting of  our algorithm significantly outperforms the other at all the different medium batch sizes plotted on the x-axis.

**Input distributions.** Our algorithm can handle different input distributions for different subgroups. We test this in our next experiment presented in Figure 2. Specifically, for each \(i\), we randomly generate a covariance matrix \(_{i}\) such that its eigenvalues are uniformly distributed in \([1,C_{1}]\), and the input distribution for \(_{i}\) is chosen as \((0,_{i})\). We set \(C_{1}=4\). It can be seen that  completely fails in this case, while our algorithm retains its good performance.

In the interest of space, we provide additional results in Appendix L which include even more general settings: (i) when the minimum distance between regression vectors can be much smaller than their norm (ii) when the number of subgroups \(k\) can be very large but the task is to recover the regression weights for the subgroups that appear in a sufficient fraction of the batches. In both these cases, our algorithm performs much better than the baseline.

## 5 Conclusion

We study the problem of learning linear regression from batched data in the presence of sub-populations. We removed several restrictive assumptions from prior work and provide better guarantees in terms of overall sample complexity. Moreover, we require relatively fewer medium batches that need to contain less number of samples compared to prior work. Finally, in our empirical results, we show that our algorithm is both practical and more performant compared to a prior baseline.

One limitation of the algorithm is that it requires knowledge of parameters \(C\), \(C_{1}\), \(M\) and \(\). However, note that the algorithm works even if their assumed values exceed the actual values. The change in the recovery guarantees is proportional to the disparity between the estimates and the actual parameters. The necessity of knowing reasonable upper bounds on these parameters, even for simpler problems, has been highlighted by previous work, e.g. .