# Unpaired Multi-Domain Causal Representation Learning

Nils Sturma

Technical University of Munich

Munich Center for Machine Learning

nils.sturma@tum.de &Chandler Squires

LIDS, Massachusetts Institute of Technology

Broad Institute of MIT and Harvard

csquires@mit.edu &Mathias Drton

Technical University of Munich

Munich Center for Machine Learning

mathias.drton@tum.de &Caroline Uhler

LIDS, Massachusetts Institute of Technology

Broad Institute of MIT and Harvard

cuhler@mit.edu

###### Abstract

The goal of causal representation learning is to find a representation of data that consists of causally related latent variables. We consider a setup where one has access to data from multiple domains that potentially share a causal representation. Crucially, observations in different domains are assumed to be unpaired, that is, we only observe the marginal distribution in each domain but not their joint distribution. In this paper, we give sufficient conditions for identifiability of the joint distribution and the shared causal graph in a linear setup. Identifiability holds if we can uniquely recover the joint distribution and the shared causal representation from the marginal distributions in each domain. We transform our results into a practical method to recover the shared latent causal graph.

## 1 Introduction

An important challenge in machine learning is the integration and translation of data across multiple domains (Zhu et al., 2017; Zhuang et al., 2021). Researchers often have access to large amounts of unpaired data from several domains, e.g., images and text. It is then desirable to learn a probabilistic coupling between the observed marginal distributions that captures the relationship between the domains. One approach to tackle this problem is to assume that there is a latent representation that is invariant across the different domains (Bengio et al., 2013; Ericsson et al., 2022). Finding a probabilistic coupling then boils down to learning such a latent representation, that is, learning high-level, latent variables that explain the variation of the data within each domain as well as similarities across domains.

In traditional representation learning, the latent variables are assumed to be statistically independent, see for example the literature on independent component analysis (Hyvarinen and Oja, 2000; Comon and Jutten, 2010; Khemakhem et al., 2020). However, the assumption of independence can be too stringent and a poor match to reality. For example, the presence of clouds and the presence of wet roads in an image may be dependent, since clouds may cause rain which may in turn cause wet roads. Thus, it is natural to seek a _causal representation_, that is, a set of high-level _causal_ variables and relations among them (Scholkopf et al., 2021; Yang et al., 2021). Figure 1 illustrates the setup of multi-domain causal representation learning, where multiple domains provide different views on a shared causal representation.

Our motivation to study multi-domain causal representations comes, in particular, from single-cell data in biology. Given a population of cells, different technologies such as imaging and sequencingprovide different views on the population. Crucially, since these technologies destroy the cells, the observations are uncoupled, i.e., a specific cell may either be used for imaging or sequencing but not both. The aim is to integrate the different views on the population to study the underlying causal mechanisms determining the observed features in various domains (Butler et al., 2018; Stuart et al., 2019; Liu et al., 2019; Yang et al., 2021a; Lopez et al., 2022; Gossi et al., 2023; Cao et al., 2022). Unpaired multi-domain data also appears in many applications other than single-cell biology. For example, images of similar objects are captured in different environments (Beery et al., 2018), large biomedical and neuroimaging data sets are collected in different domains (Miller et al., 2016; Essen et al., 2013; Shafto et al., 2014; Wang et al., 2003), or stocks are traded in different markets.

In this paper, we study identifiability of the shared causal representation, that is, its uniqueness in the infinite data limit. Taking on the same perspective as, for example, in Scholkopf et al. (2021) and Squires et al. (2023), we assume that observed data is generated in two steps. First, the latent variables \(Z=(Z_{i})_{i}\) are sampled from a distribution \(P_{Z}\), where \(P_{Z}\) is determined by an unknown structural causal model among the latent variables. Then, in each domain \(e\{1,,m\}\), the observed vector \(X^{e}^{d_{e}}\) is the image of a subset of the latent variables under a domain-specific, injective mixing function \(g_{e}\). That is,

\[X^{e}=g_{e}(Z_{S_{e}}),\]

where \(S_{e}\) is a subset of indices. A priori, it is unknown whether a latent variable \(Z_{i}\) with \(i S_{e}\) is shared across domains or domain-specific. Even the number of latent variables which are shared across domains is unknown. Moreover, we only observe the marginal distribution of each random vector \(X^{e}\), but none of the joint distributions over pairs \(X^{e},X^{f}\) for \(e f\). Said differently, observations across domains are unpaired. Assuming that the structural causal model among the latent variables as well as the mixing functions are linear, our main contributions are:

1. We lay out conditions under which we can identify the joint distribution of \(X^{1},,X^{m}\).
2. We give additional conditions under which we are able to identify the causal structure among the shared latent variables.

In particular, identifiability of the joint distribution across domains enables data translation. That is, given observation \(x\) in domain \(e\), translation to domain \(f\) can be achieved by computing \([X_{f}|X_{e}=x]\). Furthermore, identifying the causal structure among the shared latent variables lets us study the effect of interventions on the different domains.

The main challenge in proving rigorous identifiability results for multi-domain data is that we cannot apply existing results for single-domain data in each domain separately. Even if the causal structure of the latent variables in a single domain is identifiable, it remains unclear how to combine multiple causal structures, i.e., in which way latent variables are shared. We circumvent this problem via a two-step approach: First, we extend the identifiability of linear independent component analysis (Comon, 1994; Hyvarinen and Oja, 2000; Eriksson and Koivunen, 2004; Mesters and Zwiernik, 2022) to the multi-domain setup, which allows us to identify the joint distribution and distinguish between shared and domain-specific latent variables. Moreover, we identify an "overall mixing matrix" and, in a second step, exploit sparsity constraints in this matrix to identify the causal structure among the shared latent variables. This leverages recent results on causal discovery under measurement error in single domains that also exploit sparsity (Xie et al., 2020; Chen et al., 2022; Xie et al., 2022; Huang et al., 2022). Although we emphasize that our focus in this paper is on identifiability, our proofs also suggest methods to learn the joint distribution as well as the shared causal graph from finite samples. We provide algorithms for the noisy setting and, moreover, we analyze how the number of domains reduce uncertainty with respect to the learned representation.

The paper is organized as follows. In the next paragraphs we discuss further related work. Section 2 provides a precise definition of the considered setup. In Section 3 we consider identifiability of the joint distribution. Using these results, we study identifiability of the causal graph in Section 4. We conclude with a small simulation study as a proof of concept for the finite sample setting in Section 5. Due to space constraints, the detailed discussion of the finite sample setting is deferred to the Appendix. Moreover, the Appendix contains all proofs, discussions on the necessity of our assumptions, and additional examples and simulation results.

**Multi-domain Integration.** Motivated by technological developments for measuring different modalities at single-cell resolution, several methods have been proposed recently for domain translation between _unpaired_ data. The proposed methods rely on a variety of techniques, including manifold alignment (Welch et al., 2017; Amodio and Krishnaswamy, 2018; Liu et al., 2019), matrix factorization (Duren et al., 2018), correlation analysis (Barkas et al., 2019; Stuart et al., 2019), coupled autoencoders (Yang and Uhler, 2019), optimal transport (Cao et al., 2022), regression analysis (Yuan and Duren, 2022), and semisupervised learning (Lin et al., 2022). Implicitly, these methods presume the existence of a _shared_ latent space where the different modalities either completely align or at least overlap. However, to the best of our knowledge, none of these methods have rigorous _identifiability_ guarantees, i.e., the methods are not guaranteed to recover a correct domain translation mapping even for infinite data. Our work advances the theoretical understanding of multi-domain integration by providing identifiability guarantees on recovering the shared latent space.

**Group Independent Component Analysis.** The primary tool that we use for identifiability is linear independent component analysis (ICA) (Comon, 1994; Eriksson and Koivunen, 2004). Many works extend ICA to the multi-domain setting. These methods primarily come from computational neuroscience, where different domains correspond to different subjects or studies. However, to the best of our knowledge, all prior works require pairing between samples. These works can be categorized based on whether the samples are assumed to be voxels (Calhoun et al., 2001; Esposito et al., 2005), time points (Svensen et al., 2002; Varoquaux et al., 2009; Hyvarinen and Ramkumar, 2013), or either (Beckmann and Smith, 2005; Sui et al., 2009). For reviews, see Calhoun et al. (2009) and Chabriel et al. (2014). Related are methods for _independent vector analysis_(Kim et al., 2006; Anderson et al., 2014; Bhinge et al., 2019) and multiset canonical correlation analysis (Nielsen, 2002; Li et al., 2011; Klami et al., 2014), which allow the latent variables to take on different values in each domain but still require sample pairing. Most of the mentioned methods lack identifiability guarantees, only newer work (Richard et al., 2021) provides sufficient conditions for identifiability. Furthermore, all mentioned methods assume that every latent variable is shared across all domains, while our setup allows for shared and domain-specific latent variables. Some methods, e.g., Lukic et al. (2002), Maneshi et al. (2016), and Pandeva and Forre (2023), permit both shared and domain-specific components, but only consider the paired setting. In this paper, we extend these results to the _unpaired_ setting.

**Latent Structure Discovery.** Learning causal structure between latent variables has a long history, e.g., in measurement models (Silva et al., 2006). One recent line of work studies the problem under the assumption of access to interventional data (e.g., Liu et al., 2022; Squires et al., 2023). In particular, Squires et al. (2023) show that the latent graph is identifiable if the interventions are sufficiently diverse. Another line of work, closer to ours and not based on interventional data, shows that the graph is identified under certain sparsity assumptions on the mixing functions (Xie et al., 2020; Chen et al., 2022; Xie et al., 2022; Huang et al., 2022). However, these methods are not suitable in our setup since they require paired data in a single domain. One cannot apply them in each domain separately since it would be unclear how to combine the multiple latent causal graphs, that is, which of the latent variables are shared. In this work, we lay out sparsity assumptions on the mixing

Figure 1: **Setup**. A latent causal representation where multiple domains \(X^{c}\) provide different “views” on subsets of the latent variables \(Z_{i}\). The domains may correspond to different data modalities such as images, text or numerical data. Crucially, the observations across domains are unpaired, i.e., they arise from different states of the latent causal model.

functions that are tailored to the _unpaired multi-domain_ setup. The works of Adams et al. (2021) and Zeng et al. (2021) may be considered closest to ours as they also treat a setting with multiple domains and unpaired data. However, our setup and results are more general. Adams et al. (2021) assume that the number of observed variables are the same in each domain, whereas we consider domains of _different dimensions_ corresponding to the fact that observations may be of very different nature. Further, we allow for _shared and domain-specific_ latent variables, where the number of shared latent variables is unknown, while in Adams et al. (2021) it is assumed that all latent variables are shared. Compared to Zeng et al. (2021), we consider a general but fixed number of observed variables, while Zeng et al. (2021) only show identifiability of the full model in a setup where the number of observed variables in each domain increases to infinity. On a more technical level, the conditions in Zeng et al. (2021) require two pure children to identify the shared latent graph, while we prove identifiability under the weaker assumption of two partial pure children; see Section 4 for precise definitions.

**Notation.** Let \(\) be the set of nonnegative integers. For positive \(n\), we define \([n]=\{1,,n\}\). For a matrix \(M^{a b}\), we denote by \(M_{A,B}\) the submatrix containing the rows indexed by \(A[a]\) and the columns indexed by \(B[b]\). Moreover, we write \(M_{B}\) for the submatrix containing all rows but only the subset of columns indexed by \(B\). Similarly, for a tuple \(x=(x_{1},,x_{b})\), we denote by \(x_{B}\) the tuple only containing the entries indexed by \(B\). A matrix \(Q=Q_{}^{p p}\) is a _signed permutation matrix_ if it can be written as the product of a diagonal matrix \(D\) with entries \(D_{ii}\{ 1\}\) and a permutation matrix \(_{}\) with entries \((_{})_{ij}=_{j=(i)}\), where \(\) is a permutation on \(p\) elements. Let \(P\) be a \(p\)-dimensional joint probability measure of a collection of random variables \(Y_{1},,Y_{p}\). Then we denote by \(P_{i}\) the marginal probability measure such that \(Y_{i} P_{i}\). We say that \(P\) has _independent marginals_ if the random variables \(Y_{i}\) are mutually independent. Moreover, we denote by \(M\#P\) the \(d\)-dimensional _push-forward measure_ under the linear map defined by the matrix \(M^{d p}\). If \(Q\) is a signed permutation matrix and the probability measure \(P\) has independent marginals, then \(Q\#P\) also has independent marginals. For univariate probability measures we use the shorthand \((-1)\#P=-P\).

## 2 Setup

Let \(=[h]\) for \(h 1\), and let \(Z=(Z_{1},,Z_{h})\) be latent random variables that follow a linear structural equation model. That is, the variables are related by a linear equation

\[Z=AZ+,\] (1)

with \(h h\) parameter matrix \(A=(a_{ij})\) and zero-mean, independent random variables \(=(_{1},,_{h})\) that represent stochastic errors. Assume that we have observed random vectors \(X^{e}^{d_{e}}\) in multiple domains of interest \(e[m]\), where the dimension \(d_{e}\) may vary across domains. Each random vector is the image under a linear function of a subset of the latent variables. In particular, we assume that there is a subset \(\) representing the shared latent space such that each \(X^{e}\) is generated via the mechanism

\[X^{e}=G^{e}Z_{}\\ Z_{I_{e}},\] (2)

where \(I_{e}\). We say that the latent variable \(Z_{I_{e}}\) are _domain-specific_ for domain \(e[m]\) while the latent variables \(Z_{}\) are _shared_ across all domains. As already noted, we are motivated by settings where the shared latent variables \(Z_{}\) capture the key causal relations and the different domains are able to give us combined information about these relations. Likewise, we may think about the domain-specific latent variables \(Z_{I_{e}}\) as "noise" in each domain, independent of the shared latent variables. Specific models are now derived from (1)-(2) by assuming specific (but unknown) sparsity patterns in \(A\) and \(G^{e}\). Each model is given by a "large" directed acyclic graph (DAG) that encodes the multi-domain setup. To formalize this, we introduce pairwise disjoint index sets \(V_{1},,V_{m}\), where \(V_{e}\) indexes the coordinates of \(X^{e}\), i.e., \(X^{e}=(X_{v}:v V_{e})\) and \(|V_{e}|=d_{e}\). Then \(V=V_{1} V_{m}\) indexes all observed random variables. We define an \(m\)-domain graph such that the latent nodes are the only parents of observed nodes and there are no edges between shared and domain-specific latent nodes.

**Definition 2.1**.: Let \(\) be a DAG whose node set is the disjoint union \( V= V_{1} V_{m}\). Let \(D\) be the edge set of \(\). Then \(\) is an \(m\)_-domain graph_ with _shared latent nodes_\(=[]\) if the following is satisfied:

1. All parent sets contain only latent variables, i.e., \((v)=\{w:w v D\}\) for all \(v V\).

2. The set \(\) consists of the common parents of variables in all different domains, i.e., \(u\) if and only if \(u(v)(w)\) for \(v V_{e}\), \(w V_{f}\) with \(e f\).
3. Let \(I_{e}=S_{e}\) be the domain-specific latent nodes, where \(S_{e}:=(V_{e})=_{v V_{e}}(v)\). Then there are no edges in \(D\) that connect a node in \(\) and a node \(_{e=1}^{m}I_{e}\) or that connect a node in \(I_{e}\) and a node in \(I_{f}\) for any \(e f\).

To emphasize that a given DAG is an \(m\)-domain graph we write \(_{m}\) instead of \(\). We also say that \(S_{e}\) is the set of _latent parents_ in domain \(e\) and denote its cardinality by \(s_{e}=|S_{e}|\). Note that the third condition in Definition 2.1 does not exclude causal relations between the domain-specific latent variables, that is, there may be edges \(v w\) for \(v,w I_{e}\). Since the sets \(I_{e}\) satisfy \(I_{e} I_{f}=\) for \(e f\), we specify w.l.o.g. the indexing convention \(I_{e}=\{+1+_{i=1}^{e-1}|I_{i}|,,+_{i=1}^{e}|I_{i}|\}\) and \(h=+_{e=1}^{m}|I_{e}|\).

**Example 2.2**.: Consider the compact version of a \(2\)-domain graph in Figure 2. There are \(h=5\) latent nodes where \(=\{1,2\}\) are shared and \(I_{1}=\{3,4\}\) and \(I_{2}=\{5\}\) are domain-specific. A full \(m\)-domain graph is given in Appendix B.

Each \(m\)-domain graph postulates a statistical model that corresponds to the structural equation model in (1) and the mechanisms in (2), with potentially sparse matrices \(A\) and \(G^{e}\). For two sets of nodes \(W,Y V\), we denote by \(D_{WY}\) the subset of edges \(D_{WY}=\{y w D:w W,y Y\}\). Moreover, let \(^{D_{WY}}\) be the set of real \(|W||Y|\) matrices \(M=(m_{wy})\) with rows indexed by \(W\) and columns indexed by \(Y\), such that the support is given by \(D_{WY}\), that is, \(m_{wy}=0\) if \(y w D_{WY}\).

**Definition 2.3**.: Let \(_{m}=( V,D)\) be an \(m\)-domain graph. Define the map

\[_{_{m}}:^{D_{Y}} ^{D_{}} ^{|V|||}\] \[(G,A)  G(I-A)^{-1}.\]

Then the _multi-domain causal representation (MDCR) model_\((_{m})\) is given by the set of probability measures \(P_{X}=B\#P\), where \(B(_{_{m}})\) and \(P\) is an \(h\)-dimensional probability measure with independent, mean-zero marginals \(P_{i},i\). We say that the pair \((B,P)\) is a _representation_ of \(P_{X}(_{m})\).

Definition 2.3 corresponds to the model defined in Equations (1) and (2). If \(P_{X}(_{m})\) with representation \((B,P)\), then \(P_{X}\) is the joint distribution of the observed domains \(X=(X^{1},,X^{m})\). The distribution of the random variables \(_{i}\) in Equation (1) is given by the marginals \(P_{i}\). Moreover, for any matrix \(G^{D_{Y}}\), we denote the submatrix \(G^{e}=G_{V_{e},S_{e}}^{d_{e} s_{e}}\) which coincides with the matrix \(G^{e}\) from Equation (2). For the graph in Figure 2, we compute a concrete example of the matrix \(B\) in Example B.1 in the Appendix. Importantly, in the rest of the paper we assume to only observe the marginal distribution \(P_{X^{e}}\) in each domain but not the joint distribution \(P_{X}\).

Ultimately, we are interested in recovering the graph \(G_{}=(,D_{})\) among the shared latent nodes. We proceed by a two-step approach: In Section 3 we recover the representation \((B,P)\) of the joint distribution \(P_{X}\). To be precise, we recover a matrix \(\) that is equal to \(B\) up to certain permutations of the columns. Then we use the matrix \(\) to recover the shared latent graph \(G_{}\) in Section 4 and show that recovery is possible up to trivial relabeling of latent nodes that appear in the same position of the causal order.

Figure 2: **Compact version of a \(2\)-domain graph \(_{2}\)** with five latent nodes and two domains \(V_{1}\) and \(V_{2}\). All observed nodes in each domain are represented by a single grey node. We draw a dashed blue edge from latent node \(h\) to domain \(V_{e} V\) if \(h S_{e}=(V_{e})\). The random vectors associated to the two domains are uncoupled, that is, we do not observe their joint distribution.

Joint Distribution

To identify the joint distribution \(P_{X}\), we apply identifiability results from linear ICA in each domain separately and match the recovered probability measures \(P_{i}\) for identifying which of them are shared, that is, whether or not \(i\). Let \(_{m}\) be an \(m\)-domain graph with shared latent nodes \(\), and let \(P_{X}(_{m})\) with representation \((B,P)\). Recall that \(B=G(I-A)^{-1}\) with \(G^{D_{}}\) and \(A^{D_{}}\). We make the following technical assumptions.

* (Different error distributions.) The marginal distributions \(P_{i},i\) are non-degenerate, non-symmetric and have unit variance. Moreover, the measures are pairwise different to each other and to the flipped versions, that is, \(P_{i} P_{j}\) and \(P_{i}-P_{j}\) for all \(i,j\) with \(i j\). Subsequently, we let \(d\) be a distance on the set of univariate Borel probability measures such that \(d(P_{i},P_{j}) 0\) and \(d(P_{i},-P_{j}) 0\) for \(i j\).
* (Full rank of mixing.) For each \(e[m]\), the matrix \(G^{e}=G_{V_{e},S_{e}}^{d_{e} s_{e}}\) is of full column rank.

By not allowing symmetric distributions in Condition (C1), we assume in particular that the distributions of the errors are non-Gaussian. Non-Gaussianity together with the assumptions of pairwise different and non-symmetric error distributions allow us to extend the results on identifiability of linear ICA to the unpaired multi-domain setup and to identify the joint distribution. In particular, the assumption of pairwise different error distributions allows for "matching" the distributions across domains to identify the ones corresponding to the shared latent space. Non-symmetry accounts for the sign-indeterminacy of linear ICA when matching the distributions. We discuss the necessity of these assumptions in Remark 3.2 and, in more detail, in Appendix C. Note that Condition (C1) is always satisfied in a generic sense, that is, randomly chosen probability distributions on the real line are pairwise different and non-symmetric with probability one. Finally, Condition (C2) requires in particular that for each shared latent node \(k\) there is at least one node \(v V_{e}\) in every domain \(e[m]\) such that \(k(v)\).

Under Conditions (C1) and (C2) we are able to derive a sufficient condition for identifiability of the joint distribution. Let \(SP(p)\) be the set of \(p p\) signed permutation matrices. We define the set of signed permutation _block matrices_:

\[=\{(_{},_{I_{1}},,_{I_{m}}): _{} SP()_{I_{e}} SP(|I_{e}|)\}.\]

Our main result is the following.

**Theorem 3.1**.: _Let \(_{m}\) be an \(m\)-domain graph with shared latent nodes \(=[]\), and let \(P_{X}(_{m})\) with representation \((B,P)\). Suppose that \(m 2\) and that Conditions (C1) and (C2) are satisfied. Let \((,,)\) be the output of Algorithm 1. Then \(=\) and_

\[=B=^{}\#P,\]

_for a signed permutation block matrix \(\)._

Theorem 3.1 says that the matrix \(B\) is identifiable up to signed block permutations of the columns. Under the assumptions of Theorem 3.1 it holds that \(\#\) is equal to \(P_{X}\). That is, the joint distribution of the domains is identifiable.

_Remark 3.2_.: While Theorem 3.1 is a sufficient condition for identifiability of the joint distribution, we emphasize that pairwise different error distributions are in most cases also necessary; we state the exact necessary condition in Proposition C.1 in the Appendix. Said differently, if one is willing to assume that conceptually different latent variables also follow a different distribution, then identification of these variables is possible, and otherwise (in most cases) not. Apart from pairwise different error distributions, non-symmetry is then required to fully identify the joint distribution whose dependency structure is determined by the shared latent variables. If the additional assumption on non-symmetry is not satisfied, then it is still possible to identify the shared, conceptually different latent variables, which becomes clear by inspecting the proofs of Theorem 3.1 and Proposition C.1. The non-identifiability of the joint distribution would only result in sign indeterminacy, that is, entries of the matrix \(\) could have a flipped sign.

_Remark 3.3_.: By checking the proof of Theorem 3.1, the careful reader may notice that the statement of the theorem still holds true when we relax the third condition in the definition of an \(m\)-domaingraph. That is, one may allow directed paths from shared to domain-specific latent nodes but not vice versa. For example, an additional edge \(1 4\) between the latent nodes \(1\) and \(4\) would be allowed in the graph in Figure 2. In this case, the dependency structure of the domains is still determined by the shared latent space. However, the structural assumption that there are no edges between shared and domain-specific latent nodes is made for identifiability of the shared latent graph in Section 4.

_Remark 3.4_.: The computational complexity of Algorithm 1 depends on the complexity of the chosen linear ICA algorithm, to which we make \(m\) calls. Otherwise, the dominant part is the matching in Line 6 with worst case complexity \((m_{e[m]}d_{e}^{2})\), where we recall that \(m\) is the number of domains and \(d_{e}\) is the dimension of domain \(e\).

In Appendix D we state a complete version of Algorithm 1 for the finite sample setting. In particular, we provide a method for the matching in Line 6 based on the two-sample Kolmogorov-Smirnov test. For finite samples, there might occur false discoveries, that is, distributions are matched that are actually not the same. With our method, we show that the probability of falsely discovering shared nodes shrinks exponentially with the number of domains.

## 4 Identifiability of the Causal Graph

We return to our goal of identifying the causal graph \(_{}=(,D_{})\) among the shared latent nodes. By Theorem 3.1, we can identify the representation \((B,P)\) of \(P_{X}(_{m})\) from the marginal distributions. In particular, we recover the matrix \(=B\) for a signed permutation block matrix \(\). Moreover, we know which columns correspond to the shared latent nodes. That is, we know that the submatrix \(_{}\) obtained by only considering the columns indexed by \(=}=[]\) is equal to \(B_{}_{}\), where \(_{} SP()\).

**Problem 4.1**.: Let \(B(_{_{m}})\) for an \(m\)-domain graph \(_{m}\) with shared latent nodes \(\). Given \(_{}=B_{}_{}\) with \(_{}\) a signed permutation matrix, when is it possible to identify the graph \(_{}\)?

Recently, Xie et al. (2022) and Dai et al. (2022) show that, in the one-domain setting with independent additive noise, the latent graph can be identified if each latent variable has at least two pure children. We obtain a comparable result tailored to the multi-domain setup.

**Definition 4.2**.: Let \(_{m}=( V,D)\) be an \(m\)-domain graph with shared latent nodes \(\). For \(k\), we say that an observed node \(v V\) is a _partial pure child_ of \(k\) if \((v)=\{k\}\).

For a partial pure child \(v V\), there may still be domain-specific latent nodes that are parents of \(v\). Definition 4.2 only requires that there is exactly one parent that is in the set \(\). This explains the name _partial_ pure child; see Example B.2 in the Appendix for further elaboration.

W.l.o.g. we assume in this section that the shared latent nodes are _topologically ordered_ such that \(i j D_{}\) implies \(i<j\) for all \(i,j\). We further assume:

1. [label=(C0), ref=(C1)]
2. (Two partial pure children across domains.) For each shared latent node \(k\), there exist two partial pure children.
3. (Rank faithfulness.) For any two subsets \(Y V\) and \(W\), we assume that \[(B_{Y,W})=_{B^{}(_{_{m}})} (B^{}_{Y,W}).\]

The two partial pure children required in Condition 1 may either be in distinct domains or in a single domain. This is a sparsity condition on the large mixing matrix \(G\). In Appendix C we discuss that the identification of the joint latent graph is impossible without any sparsity assumptions. We conjecture that two partial pure children are not necessary, but we leave it open for future work to find a non-trivial necessary condition. Roughly speaking, we assume in Condition 2 that no configuration of edge parameters coincidentally yields low rank. The set of matrices \(B(_{_{m}})\) that violates 2 is a subset of measure zero of \((_{_{m}})\) with respect to the Lebesgue measure. Note that our conditions do not impose constraints on the graph \(_{}\). Our main tool to tackle Problem 4.1 will be the following lemma.

**Lemma 4.3**.: _Let \(B(_{_{m}})\) for an \(m\)-domain graph \(_{m}\). Suppose that Condition 2 is satisfied and that there are no zero-rows in \(B_{}\). Let \(v,w V\). Then \((B_{\{v,w\},})=1\) if and only if there is a node \(k\) such that both \(v\) and \(w\) are partial pure children of \(k\)._

The condition on no zero-rows in Lemma 4.3 is needed since we always have \((B_{\{v,w\},}) 1\) if one of the two rows is zero. However, this is no additional structural assumption since we allow zero-rows when identifying the latent graph; c.f. Algorithm 2. The lemma allows us to find partial pure children by testing ranks on the matrix \(_{}\). If \((i_{1},j_{1})\) and \((i_{2},j_{2})\) are partial pure children of two nodes in \(\), we make sure that these two nodes are different by checking that \((B_{\{i_{1},i_{2}\},})=2\).

For a DAG \(G=(V,D)\), we define \((G)\) to be the set of permutations on \(|V|\) elements that are consistent with the DAG, i.e., \((G)\) if and only if \((i)<(j)\) for all edges \(i j D\). The following result is the main result of this section.

**Theorem 4.4**.: _Let \(=B\) with \(B(_{_{m}})\) and \(\), and define \(B^{*}=_{}\) to be the input of Algorithm 2. Assume that Conditions 1 and 2 are satisfied, and let \(\) be the output of Algorithm 2. Then \(=Q_{}^{}\,_{,}Q_{}\) for a signed permutation matrix \(Q_{}\) with \((_{})\). Moreover, if \(G_{vk}>0\) for \(G^{D_{V}}\) whenever \(v\) is a pure child of \(k\), then \(Q_{}\) is a permutation matrix._Theorem 4.4 says that the graph \(_{}\) can be recovered up to a permutation of the nodes that preserves the property that \(i j\) implies \(i<j\); see Remark 4.5. Since the columns of the matrix \(\) are not only permuted but also of different signs, we solve the sign indeterminacy column-wise in Line 7 before removing the scaling indeterminacy row-wise in Line 8. In case the coefficients of partial pure children are positive, this ensures that \(Q_{}\) is a _permutation matrix_ and we have no sign indeterminacy.

In Appendix D we adapt Algorithm 2 for the empirical data setting, where we only have \(_{} B_{}_{}\).

_Remark 4.5_.: Let \(\) be the output of Alg. 2. Then we construct the graph \(_{}=(,_{})\) as the graph with edges \(j i_{}\) if and only if \(_{ij} 0\). Condition (C4) ensures that \(_{}\) is equivalent to \(_{}\) in the sense that there is a permutation \((_{})\) such that \(_{}=\{(i)(j):i j D_{ }\}\).

**Example 4.6**.: As highlighted in the introduction, the unpaired multi-domain setup is motivated by applications from single-cell biology. For example, consider the domains of (i) gene expression and (ii) high-level phenotypic features extracted from imaging assays (e.g. McQuin et al., 2018). We argue that the requirement of two partial pure children is justifiable on such data as follows. The condition requires, for example, that for each shared latent variable, (i) the expression of some gene depends only upon that shared latent variable plus domain-specific latent variables, and (ii) one of the high-level phenotypic features depends only on the same latent feature plus domain-specific latent variables. Many genes have highly specialized functions, so (i) is realistic, and similarly many phenotypic features are primarily controlled by specific pathways, so (ii) is justified.

_Remark 4.7_.: In Algorithm 2, we determine the rank of a matrix by Singular Value Decomposition, which has worst case complexity \((mn\{n,m\})\) for an \(m n\) matrix. Since Line 4 is the dominant part, we conclude that the worst case complexity of Algorithm 2 is \((|V|^{2})\).

## 5 Simulations

In this section we report on a small simulation study to illustrate the validity of our adapted algorithms for finite samples (detailed in Appendix D). We emphasize that this should only serve as a proof of concept as the focus of our work lies on identifiability. In future work one may develop more sophisticated methods; c.f. Appendix G. The adapted algorithms have a hyperparameter \(\), which is a threshold on singular values to determine the rank of a matrix. In our simulations we use \(=0.2\).

**Data Generation.** In each experiment we generate \(1000\) random models with \(=3\) shared latent nodes. We consider different numbers of domains \(m\{2,3\}\) and assume that there are \(|I_{e}|=2\) domain-specific latent nodes for each domain. The dimensions are given by \(d_{e}=d/m\) for all \(e[m]\) and \(d=30\). We sample the \(m\)-domain graph \(_{}\) on the shared latent nodes as follows. First, we sample the graph \(_{}\) from an Erdos-Renyi model with edge probability \(0.75\) and assume that there are no edges between other latent nodes, that is, between \(\) and \(\) and within \(\). Then we fix two partial pure children for each shared latent node \(k\) and collect them in the set \(W\). The remaining edges from \(\) to \(V W\) and from \(\) to \(V\) are sampled from an Erdos-Renyi model with edge probability \(0.9\). Finally, the (nonzero) entries of \(G\) and \(A\) are sampled from Unif\(([0.25,1])\). The distributions of the error variables are specified in Appendix E. For simplicity, we assume that the sample sizes coincide, that is, \(n_{e}=n\) for all \(e[m]\), and consider \(n\{1000,2500,5000,10000,25000\}\).

**Results.** First, we plot the average number of shared nodes \(\) in our experiments in Figure 3 (a). Especially for low sample sizes, we see that fewer shared nodes are detected with more domains. However, by inspecting the error bars we also see that the probability of detecting too many nodes \(>\) decreases drastically when considering \(3\) instead of \(2\) domains. This suggests that the number of falsely detected shared nodes is very low, as expected by Theorem D.3. Our findings show that more domains lead to a more conservative discovery of shared nodes, but whenever a shared node is determined this is more certain. Moreover, we measure the error in estimating \(_{}}\) in Figure 3 (b), that is, the error in the "shared" columns. We take

\[_{B}(_{}})=_{  SP()}\ _{,}^{-1/2}\|_{}}-[B_{ }]_{}}|_{}&,\\ _{ SP()}\ _{,}^{-1/2}\|_{}}]_{}-B_{}}|_{ }&>,\]

where \(\|\|_{}\) denotes the Frobenius norm and \(_{,}=\{,\}_{e=1}^{m}d_{e}\) denotes the number of entries of the matrix over which the norm is taken. In the cases \(=\), we also measure the performance of recovering the shared latent graph \(_{}\) in Figure 3 (c) by taking

\[_{A}()=_{Q_{} SP() S(_{})}\|Q_{}^{} Q_{}-A_{,}\|_{}.\]

As expected, the median estimation errors for \(B_{}\) and \(A_{,}\) decrease with increasing sample size. In Appendix F we provide additional simulations with larger \(\). Moreover, we consider setups where we violate specific assumptions, such as pairwise different distributions (C1) and two partial pure children (C3). The results emphasize that the conditions are necessary for the algorithms provided. The computations were performed on a single thread of an Intel Xeon Gold \(6242\)R processor (\(3.1\) GHz), with a total computation time of \(12\) hours for all simulations presented in this paper (including Appendix).

## 6 Discussion

This work introduces the problem of causal representation learning from _unpaired_ multi-domain observations, in which multiple domains provide complementary information about a set of shared latent nodes that are the causal quantities of primary interest. For this problem, we laid out a setting in which we can provably identify the causal relations among the shared latent nodes. To identify the desired causal structure, we proposed a two-step approach where we first make use of linear ICA in each domain separately and match the recovered error distributions to identify shared nodes and the joint distribution of the domains. In the second step, we identify the causal structure among the shared latent variables by testing rank deficiencies in the "overall mixing matrix" \(B\). To the best of our knowledge, our guarantees are the first principled identifiability results for shared causal representations in a general, unpaired multi-domain setting.

We proposed algorithms for recovering the joint distribution and the shared latent space making our proofs constructive. While our focus is on identifiability guarantees, we show in Appendix D how our proofs give rise to algorithms for the finite sample setting. Moreover, we propose a method to match approximate error distributions and show that the probability of falsely discovering shared nodes decreases exponentially in the number of domains. Our work opens up numerous directions for future work as we discuss in Appendix G.