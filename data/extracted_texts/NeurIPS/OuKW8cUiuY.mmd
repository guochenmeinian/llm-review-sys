# Diffusion Priors for Variational Likelihood Estimation and Image Denoising

Jun Cheng, Shan Tan

School of Artificial Intelligence and Automation,

Huazhong University of Science and Technology

jcheng24@hust.edu.cn, shantan@hust.edu.cn

Corresponding Author

###### Abstract

Real-world noise removal is crucial in low-level computer vision. Due to the remarkable generation capabilities of diffusion models, recent attention has shifted towards leveraging diffusion priors for image restoration tasks. However, existing diffusion priors-based methods either consider simple noise types or rely on approximate posterior estimation, limiting their effectiveness in addressing structured and signal-dependent noise commonly found in real-world images. In this paper, we build upon diffusion priors and propose adaptive likelihood estimation and MAP inference during the reverse diffusion process to tackle real-world noise. We introduce an independent, non-identically distributed likelihood combined with the noise precision (inverse variance) prior and dynamically infer the precision posterior using variational Bayes during the generation process. Meanwhile, we rectify the estimated noise variance through local Gaussian convolution. The final denoised image is obtained by propagating intermediate MAP solutions that balance the updated likelihood and diffusion prior. Additionally, we explore the local diffusion prior inherent in low-resolution diffusion models, enabling direct handling of high-resolution noisy images. Extensive experiments and analyses on diverse real-world datasets demonstrate the effectiveness of our method. Code is available at https://github.com/HUST-Tan/DiffusionVI.

## 1 Introduction

Real-world imaging modalities, such as photography and biomedical imaging, frequently encounter complex image noise that is both signal-dependent and spatially correlated . Removing such noise is critical for subsequent image analysis and understanding. Existing deep learning-based image denoising methods rely on either large amounts of paired images for supervised training  or noisy images for self-supervised training . However, collecting massive amounts of data is expensive and time-consuming in real-world scenarios. Therefore, designing effective and data-efficient real-world denoising methods is of significant practical importance.

Incorporating image priors and the likelihood, and conducting the corresponding posterior inference (e.g., maximum-a-posteriori (MAP) estimation and mean estimation), is a classical and data-efficient approach to image restoration . Nowadays, deep generative models such as VAE, GAN, and Normalizing-flow have shown the capacity to capture and model complex image statistics, surpassing traditional analytical image priors . Recent diffusion models have demonstrated state-of-the-art image generation capabilities  and have been incorporated into various image restoration tasks as powerful image priors .

The essence of applying diffusion priors to address image denoising lies in accurately integrating the degraded image into the generation process of pre-trained diffusion models, underscoring the importance of the likelihood function. Unlike Gaussian white noise, real-world image noise is intricate and challenging to model precisely and analytically [17; 21]. Many existing methods based on diffusion priors solely account for \(i.i.d.\) Gaussian noise [19; 5; 43; 10], making them ineffective for real-world noise removal. On the other hand, some approaches targeting complex and non-linear degradations employ _hard_ data-consistency strategies and approximate posterior inference during the generation process [12; 45; 8], yielding unsatisfactory real-world denoising outcomes due to their coarse likelihood modeling. Although a structured and heteroscedastic Gaussian likelihood function can well approximate real-world noise, such a model is computationally expensive due to the large covariance matrix and also impractical due to the unknown noise variance.

To tackle these challenges, this paper integrates variational Bayes and presents adaptive likelihood estimation and MAP inference during the generation process of diffusion priors to handle real-world noise. We introduce an independent, non-identically distributed (\(i.ni.d.\)) likelihood combined with a precision prior to model real-world noise. Such a choice allows modeling the spatially variant feature of noise and meanwhile avoids modeling covariance, trading off the accuracy for practical feasibility. Based on variational Bayes, the \(i.ni.d.\) precision posterior at each step in the reverse process is subsequently inferred, which adaptively refines the likelihood function and aligns with the real-world noise model. Additionally, we introduce local Gaussian convolution to rectify the estimated noise variance, compensating for the lack of spatial correlation in the \(i.ni.d.\) likelihood function. By adaptively updating the likelihood at each reverse diffusion step, the final denoised result is achieved by progressively propagating the intermediate MAP solutions that strike the best balance between the noisy image and diffusion prior.

Furthermore, real-world images exhibit diverse resolutions, often differing from those of pre-trained diffusion models. Existing methods utilize patch-based  or resize-based  operations, which are laborious and may impact low-level details. We observe that diffusion models pre-trained with low-resolution (LR) images tend to yield local diffusion priors effective for image restoration, enabling the direct treatment of high-resolution (HR) noisy images. The main contributions of this paper are summarized as follows:

* We propose adaptive likelihood estimation and MAP inference based on diffusion priors and variational Bayes to address real-world complex noise.
* We explore the local prior exhibited by diffusion models pre-trained with LR images.
* Our method outperforms other unsurpervised denoising methods as well as diffusion priors-based methods on diverse real-world image denoising datasets.

## 2 Related Works

**Deep Learning-based Image Denoising**. By harnessing modern deep architectures and large-scale paired training datasets, supervised learning-based denoising methods such as VDN , Restormer , and GRL  have significantly enhanced in-distribution denoising performance. However, their reliance on extensive paired data poses challenges for real-world applications, prompting the exploration of self-supervised denoising approaches. These include Blind spot (BS)-based methods (e.g., SSDN , Noise2Self , Nei2Nei , and B2U ), resampling-based methods (e.g., Nr2N  and R2R ), and regularization-based methods (e.g., Noise2Score  and Stein ). However, these methods assume spatially independent or analytical noise, which deviates from the structured and complex real-world noise. Recent advancements, such as AP-BSN  and LG-BPN , have integrated Pixel-shuffle and masked convolution into BS networks to address real-world noise. Other approaches have leveraged disentangled representation learning [11; 31; 6]. Nonetheless, these methods often require large quantities of noisy images and lack data efficiency. Several single image-based deep learning methods (e.g., DIP , Self2Self , ZS-N2N , and ScoreDVI ) have been proposed, but their performance in real-world denoising scenarios remains suboptimal, underscoring the need for more effective approaches.

**Diffusion Priors for Image Restoration**. Diffusion models have exhibited remarkable image generation capabilities [14; 40; 37] and have been integrated into inverse problems as diffusion priors to address various image restoration tasks in an unsupervised manner . Existing methods based on diffusion priors can be broadly categorized into two aspects: linear inverse problem-solving based on exact degradation models (e.g., DDRM , DDNM , MCGDiff , and FPS ), and nonlinear inverse problem-solving based on approximate posterior inference (e.g., DPS , GDP , and DR2 ). In the former, these methods typically assume analytical noise, such as additive white Gaussian noise, overlooking real-world noise that is signal-dependent and spatially correlated. Given the difficulty in precisely modeling and estimating real-world noise, these methods often prove ineffective in handling such noise.

In the latter, _hard_ data-consistency terms are introduced to replace accurate likelihood modeling, and approximate posterior inference is conducted during the inverse diffusion process to address complex degradations. However, due to the absence of explicit constraints from likelihood functions, these methods heavily rely on proper hyperparameters (e.g., guidance scales in GDP , step size in DPS , or downsampling factors in DR2 ), leading to significant reconstruction errors. Diverging from these methods, we refrain from specifying the accurate noise model but introduce the noise precision prior and dynamically estimate its posterior using variational Bayes in the reverse diffusion process, enabling adaptive estimation of likelihood functions and better posterior inference.

## 3 Methods

### Preliminary

The Diffusion model is a class of generative models used to model the distribution \(q(x_{0})\). Its forward process is a Markov chain with fixed Gaussian transition and length \(T\), which gradually corrupts the data \(x_{0}\) by adding Gaussian noise according to a pre-defined variance schedule \(_{1},,_{T}\):

\[q(x_{1:T}|x_{0})=_{t=1}^{T}q(x_{t}|x_{t-1}),q(x_{t}|x_{t-1})= (x_{t-1};}x_{t-1},_{t}I)\] (1)

A nice property of the forward process is that sample \(x_{t}\) at any step \(t\) can be obtained from \(x_{0}\) in a closed-form manner:

\[q(x_{t}|x_{0})=(x_{t};_{t}}x_{0},(1-_{t})I)  x_{t}=_{t}}x_{0}+_{t}}\] (2)

where \(a_{t}=1-_{t},_{t}=_{s=1}^{t}a_{s},(0,I)\).

In the reverse process, the diffusion model progressively recovers data from noise distribution \(p(x_{T})\), which is again a Markov chain with learned Gaussian transition:

\[p(x_{0:T})=p(x_{T})_{t=1}^{T}p(x_{t-1}|x_{t}),p(x_{t-1}|x_{t})=(_{}(x_{t},t),_{t}^{2}I)\] (3)

where \(_{t}^{2}\) is a constant relating to \(_{t}\) and can be pre-computed. And \(_{}(x_{t},t)\) is usually parameterized by a DNN \(_{}(x_{t},t)\):

\[_{}(x_{t},t)=}}(x_{t}-}{ _{t}}})_{}(x_{t},t)\] (4)

### Naive Image Denoising

Consider the image formation process \(y_{0}=f(x_{0},n)\) in the real-world scenario, where \(n\) is the raw noise, \(f\) is the transformation function, \(y_{0}^{N}\) and \(x_{0}^{N}\) (\(N\) is the total pixel number) are the observed noisy image and original clean image, respectively. Noise in \(y_{0}\) generally exhibits signal-dependent and spatially-correlated characteristics, e.g., sRGB noise , due to the Poisson nature of photons and compound transformation in \(f\). Suppose there are no non-linear parts in \(f\), the image formation process can be simplified as \(y_{0}=x_{0}+n_{0}(x_{0})\) with \((n_{0}^{i},n_{0}^{j})>0\), where \(n_{0}\) is the image noise related to the signal \(x_{0}\); \((i,j)\) represents the correlation coefficient between two neighboring elements \(i\) and \(j\).

Real-world image denoising task is to recover clean and high-quality \(x_{0}\) from noisy \(y_{0}\), which turns into solving the posterior \(p(x_{0}|y_{0})\) in Bayesian statistics. As the pre-trained diffusion model possesses superior image priors, it is natural to inject the observed information \(y_{0}\) into its reverse diffusion (or generation) process defined in Eq. (3) to achieve the conditional inference of \(x_{0:T}\) given \(y_{0}\), i.e., \(p(x_{0:T}|y_{0})\). Due to that \(y_{0}\) and intermediate \(x_{t}\) exhibit large distribution gaps (as they have different noise types and strength) and it is also difficult to directly model \(p(y_{0}|x_{t})\), we hence follow [39; 45] and re-corrupt \(y_{0}\) using Eq. (2) in each step \(t\) to obtain \(y_{t}\), which serves as intermediate conditions. Such choice results in the target conditional distribution:

\[p(x_{0:T}|y_{0}) p(x_{T})_{t=1}^{T}p(x_{t-1}|x_{t},y_{t-1}) p(x _{T})_{t=1}^{T}p(x_{t-1}|x_{t})p(y_{t-1}|x_{t-1})\] (5)

where \(y_{t}=_{t}}y_{0}+_{t}}\), and \(p(x_{T}|y_{T}) p(x_{T})\) as \(x_{T}\) and \(y_{T}\) are approximated independent normal distributions.

**Structured and heteroscedastic Gaussian likelihood model**. The right-hand side of Eq. (5) explicitly involves the prior \(p(x_{t-1}|x_{t})\) and likelihood \(p(y_{t-1}|x_{t-1})\) at each step \(t\). The prior \(p(x_{t-1}|x_{t})\) has been available from the pre-trained diffusion model in Eq. (3), necessitating the modeling of \(p(y_{t-1}|x_{t-1})\). As discussed above, in principle we can assume that \(y_{0}\) follows the structured and heteroscedastic Gaussian distribution \((x_{0},(x_{0}))\), where \(\) is the _non-diagonal_ covariance matrix with variances related to signal \(x_{0}\), which allows modeling the signal-dependent and spatially-correlated properties of real-world noise. As a result, we derive that

\[y_{t-1}=_{t-1}}(x_{0}+A_{2})+_{t-1}}=x_{t-1}+_{t-1}}A_{2},AA^ {T}=(x_{0}),_{2}(0,I)\] (6)

which indicates that \(p(y_{t-1}|x_{t-1})=(y_{t-1};x_{t-1},(x_{t-1}))\) with \((x_{t-1})=_{t-1}(x_{0})\). See detailed derivation in the Appendix A.1.

As the prior \(p(x_{t-1}|x_{t})\) in Eq. (3) and likelihood \(p(y_{t-1}|x_{t-1})\) in Eq. (6) both have Gaussian forms, the posterior distribution in Eq. (5) is theoretically computable. Nevertheless, we note that the above formulation presents several practical challenges. First, specifying an accurate \((x_{0})\) for \(y_{0}\) is _difficult_, which involves the estimation of noise variance and the noise correlation between neighboring pixels. These estimations are hard to achieve based on a single \(y_{0}\) and are open research problems [17; 21]. In addition, the posterior inference with non-diagonal covariance matrix \(\) is both memory-demanding and computationally expensive. These challenges prevent the direct application of the structured heteroscedastic Gaussian likelihood.

### Variational Denoising with Adaptive Likelihood Estimation

To deal with these difficulties, we consider \(p(y_{0}|x_{0})=(x_{0},(_{0})^{-1})\), which has diagonal precision matrix \((_{0})\) (i.e., the inverse of the covariance matrix, \(_{0}^{N}\)). Such diagonal Gaussian likelihood allows modeling the spatially variant feature of real-world noise but ignores the noise correlation at this stage. Based on Eq. (6), \(p(y_{t-1}|x_{t-1})\) then becomes:

\[p(y_{t-1}|x_{t-1},_{t-1})=(y_{t-1};x_{t-1},(_{t- 1})^{-1}),_{t-1}=}{_{t-1}}\] (7)

**Hyperprior for precision \(_{t}\)**. Instead of specifying an accurate \(_{0}\), which is again difficult, we introduce the independent Gamma hyperprior \(p(_{0})=_{i=1}^{N}(_{0}^{i};,)\) for \(_{0}\) (\(\) and \(\) are scalars), which serves as the _rough_ precision prior for noise in \(y_{0}\). Meanwhile, based on Eq. (7), it is straightforward that \(_{t-1}\) follows

\[p(_{t-1})=_{i=1}^{N}(_{t-1}^{i};_{t-1},_{t- 1}),_{t-1}=,_{t-1}=_{t-1}\] (8)

Because \(p(_{t-1})\) merely provides initial gauss about the noise precision (also variance) at each step \(t\), we then expect to find the corresponding precision posterior \(p(_{t-1}|x_{t-1},y_{t-1})\), which is more accurate and allows a better likelihood function \(p(y_{t-1}|x_{t-1})\). As posterior \(_{t-1}\) depends on \(x_{t-1}\), we have to simuteniously infer them together, i.e., the following joint distribution:

\[p(x_{t-1},_{t-1}|x_{t},y_{t-1})=|x_{t-1},_{t-1})^{}p(_{t-1})p(x_{t-1}|x_{t})}{p(y_{t-1}|x_{t})}\] (9)

where \( 1\) is the temperature parameter, which is typically utilized in variational inference to scale the likelihood function .

**Variational inference of precision posterior**. As \(p(x_{t-1},_{t-1}|x_{t},y_{t-1})\) in Eq. (9) is a non-trivial distribution, we hence choose a trivial and factorized variational distribution \(g(x_{t-1},_{t-1})=g(x_{t-1})g(_{t-1})\) to approximate the true posterior \(p(x_{t-1},_{t-1}|x_{t},y_{t-1})\), under the KL-divergence distance. Following the mean-field variational Bayes presented in , the optimal \(g(x_{t-1})g(_{t-1})\) can be solved by cycling through \(x_{t-1}\) and \(_{t-1}\) and replacing each in turn with a revised estimate of \(g(x_{t-1})\) and \(g(_{t-1})\). Specifically, we derive the following alternate update scheme for finding the optimal \(g(_{t-1})\):

**1. Update**\(g(x_{t-1})\). Given \(g(_{t-1})\), the optimal \(g^{*}(x_{t-1})\) is provided by

\[ g^{*}(x_{t-1})=_{_{t-1}} p(y_{t-1}|x_{t-1},_{t-1})^{ }p(x_{t-1}|x_{t})p(_{t-1})\] (10)

which corresponds to

\[g^{*}(x_{t-1})=(x_{t-1};_{t-1},_{t-1}^{2})\] (11)

with mean \(_{t-1}\) and variance \(_{t-1}^{2}\) as

\[_{t-1}=^{2}(_{t-1}) y_{t-1}+_{ }(x_{t},t)}{(_{t-1})_{t}^{2}+},_{t-1}^{2}=(^{2}}{(_{t-1}) _{t}^{2}+})\] (12)

where \(\) denotes element-wise multiplication; \((_{t-1})\) at step \(T\) is initialized as a constant (which is robust to different initializations) and then is updated as \((_{t-1})=_{t-1}/_{t-1}\) (see **Update 2**).

**2. Update**\(g(_{t-1})\). Similar to \(g(x_{t-1})\), the optimal \(g^{*}(_{t-1})\) given \(g(x_{t-1})\) is

\[g^{*}(_{t-1})=_{i=1}^{N}(_{t-1}^{i};_{t- 1}^{i},_{t-1}^{i})\] (13)

with shape \(_{t-1}\) and rate \(_{t-1}\) as

\[_{t-1}^{i}=_{t-1}+,_{t-1}^{i}= _{t-1}+^{i}-_{t-1}^{i})^{2}+(_{t-1}^{2 })^{i}}{2}\] (14)

The detailed derivation of Eqs. (11) and (13) is given in the Appendix A.2.

**MAP estimation with updated likelihood**. During the updates, \(_{t-1}\) and \(_{t-1}\) in \(g(_{t-1})\) are adptively updated and become signal-dependent as indicated by Eq. (14). Once the cycling converges, we can obtain the approximated posterior distribution \(g(_{t-1})\) and the updated likelihood \(p(y_{t-1}|x_{t-1})=_{_{t-1} g(_{t-1})}p(y_{t-1}|x_{t-1}, _{t-1})\) at step \(t\). As a result, by maximizing the conditional distribution in Eq. (5) at step \(t\), the optimal \(x_{t-1}^{*}\) that balances the image prior and the observed \(y_{t-1}\) can be obtained as follows:

\[x_{t-1}^{*} =\  p(y_{t-1}|x_{t-1})+ p(x_{t-1}|x_{t})\] \[\ _{_{t-1}} p(y_{t-1}|x_{t-1}, _{t-1})+ p(x_{t-1}|x_{t})\] \[=\ -(x_{t-1}-y_{t-1})^{2}(_{t-1})--_{}(x_{t},t))^{2}}{_{t}^{2}}\] (15) \[=_{t-1}y_{t-1}+(1-_{t-1})_{}(x_{t},t), \ _{t-1}=^{2}}{_{t}^{2}+1/(_{t-1})}\]

where \((_{t-1})=_{t-1}/_{t-1}\) is the expectation of noise precision posterior (and hence \(1/(_{t-1})=_{t-1}/_{t-1}\) is the estimated noise variance at step \(t\)), and \(_{t}\) suggests that the optimal \(x_{t-1}^{*}\) is the convex combination of \(y_{t-1}\) and \(_{}(x_{t},t)\). Note that, in the second row of Eq. (15), we utilize the following Jensen's Inequality

\[ p(y_{t-1}|x_{t-1})=_{_{t-1} g(_{t-1})}p(y_{t-1}|x_ {t-1},_{t-1})_{_{t-1} g(_{t-1})} p(y_{t-1}|x_{t- 1},_{t-1})\] (16)

and employ the lower bound of \( p(y_{t-1}|x_{t-1})\). Optimizing this lower bound generally produces satisfactory solutions, like in variational inference, VAE, and diffusion models.

**Rectification of \(1/(_{t-1})\).** By considering the diagonal Gaussian likelihood, the noise correlation between neighboring pixels in \(y_{0}\) (and \(y_{t}\)) is ignored, which affects the estimation of the precisionposterior and noise variance \(1/(_{t-1})\). This is apparent as transformations in \(f\) (e.g., demosaicking in the ISP pipeline) will cause the local correlation of noise variance (or precision) while elements in \((_{t-1})\) are updated independently as shown in Eq. (14). Motivated by the analysis of spatial correlation of real-world noise in , we introduce a local 2D convolution with a normalized Gaussian kernel \(G(l,s)\) (\(l\) is kernel size and \(s\) is the scale) to manually rectify \(1/(_{t-1})\). That is,

\[(_{t-1})}=(1/(_{t-1}),G(l,s )),_{t-1}=^{2}}{_{t}^{2}+ (_{t-1})}}\] (17)

By progressively calculating \(x_{t-1}^{*}\) at each step \(t\) based on Eqs. (15) and (17), the final denoised result \(x_{0}\) can be obtained. The whole denoising algorithm is presented in Alg. 1.

```
0: Pre-trained diffusion model, noisy observation \(y_{0}\), hyperparameters \(,\), temperature \(\)
1:\(x_{T}(0,I),(_{T})=\)
2:for\(t=T,,1\)do
3: Compute \(_{}(x_{t},t)\) based on Eq. (4); Compute \(y_{t-1}\) based on Eq. (7)
4: Set \(_{t-1}^{}=,_{t-1}=_{}(x_{t},t)\)
5:while\(\|_{t-1}^{}-_{t-1}\|_{2}^{2} 1e^{-6}\)do
6: Update \(g(x_{t-1})=(_{t-1},_{t-1}^{2})\) using Eq. (12)
7: Update \(g(_{t-1})=_{i=1}^{N}(_{t-1}^{i},_ {t-1}^{i})\) using Eq. (14)
8:endwhile
9: Solve optimal \(x_{t-1}\) using Eq. (15) or Eq. (17)
10:endfor
11:return\(x_{0}\) ```

**Algorithm 1** Diffusion priors-based variational image denoising

### Local Diffusion Priors

The common practice of sampling images from pre-trained diffusion models is to maintain the sampling resolution identical to the training resolution, which produces the best generation performance. In image restoration tasks, however, the resolution of the observed noisy image generally mismatches that of the pre-trained diffusion model. Existing approaches typically use patches  or resize operations  to address this issue, which is cumbersome and may affect local details.

We observe that when a diffusion model trained with the U-Net architecture on low-resolution (LR) images is employed to sample high-resolution (HR) images, it exhibits local properties. Fig. 1 shows sampled \(512 512\) and \(256 256\) images from pre-trained \(256 256\)2 and \(128 128\)3 diffusion models, respectively, and it is clear that the generated textures mainly focus on local areas. As HR images contain more redundant information, denoising HR images is simpler than denoising their LR counterparts under the same noise level . When the LR diffusion model is used to generate HR samples, there is only a short time window for it to decide the structures of the sampled images , thus the generation tends to be local. Similar to traditional TV priors and Markov random fields that focus on designing local image statistics, we note that the local property of LR diffusion models is also effective for image denoising. This allows us to directly adopt the pre-trained LR diffusion prior to denoise HR noisy images without additional operations.

Figure 1: Unconditional HR images generation from LR diffusion models

[MISSING_PAGE_FAIL:7]

Although APBSN achieves the best PSNR values in the SIDD dataset, it frequently introduces noticeable color artifacts (see Figs. 2, 3) and oversmooths images (see Fig. 7). In addition, when applied to FMDD, PolyU, and CC datasets that contain fewer noisy images, its denoising performance significantly degrades and underperforms our method. This highlights the advantage of our data-efficient approach.

### Ablation and Analyses

**Adaptive likelihood estimation (ALE)**. We analyze \(1/(_{0})=_{0}/_{0}\), the estimated noise variance, and present quantitative and qualitative results in Fig. 4. Fig. 4a demonstrates that \(_{0}/_{0}\) effectively reflects the noise variance of \(y_{0}\). That is, \(_{0}/_{0}\) exhibits larger values in noisier areas of \(y_{0}\) and smaller values (black) in less noisy areas. Fig. 4b implies that the average of \(_{0}/_{0}\) are inversely correlated with PSNR values of denoised images, which is reasonable since noisier images are more challenging to denoise and hence have lower PSNR. Conversely, the prior noise variance \(/=3e^{-3}\)

  Methods & SIDD Validation  & FMDD  & PolyU  & CC  & Average \\  DIP  & 32.11/0.740 & 32.90/0.854 & 37.17/0.912 & 35.61/0.912 & 34.45/0.855 \\ Self2Self  & 29.46/0.595 & 30.76/0.695 & 38.33/0.962 & 37.45/0.948 & 34.00/0.800 \\ PD-denoising  & 33.97/0.820 & 33.01/0.856 & 37.04/0.940 & 35.85/0.923 & 34.97/0.885 \\ ZS-N2N  & 25.58/0.433 & 31.61/0.767 & 36.05/0.916 & 33.58/0.854 & 31.71/0.743 \\ ScoreDVI  & 34.75/0.856 & 33.10/**0.865** & 37.77/0.959 & 37.09/0.945 & 35.68/0.906 \\ GDP  & 27.65/0.615 & 27.68/0.698 & 32.30/0.905 & 31.45/0.916 & 29.77/0.784 \\ DR2  & 32.02/0.728 & 30.52/0.813 & 34.37/0.925 & 32.30/0.876 & 32.30/0.836 \\ DDRM  & 33.14/0.796 & 32.54/0.837 & 33.14/0.767 & 36.04/0.923 & 33.72/0.831 \\ Ours & 34.76/**0.887** & **33.14**/0.860 & **38.71**/**0.970** & **38.01**/**0.959** & **36.16**/**0.919** \\  APBSN  & **36.80**/0.874 & 31.99/0.836 & 37.03/0.951 & 34.88/0.925 & 35.18/0.897 \\  

Table 2: Quantitative comparisons (PSNR(dB)/SSIM) of different methods on diverse real-world image datasets. The best and second-best PSNR/SSIM results are marked in **bold** and underlined.

Figure 3: Visual comparison of different denoising methods in PolyU. PSNR/SSIM values: PD (36.77/0.916), APBSN (37.59/0.944), DR2 (34.53/0.864), Self2Self (39.44/0.961), ZSN2N (35.12/0.879), GDP (33.43/0.888), DDRM (33.61/0.773), ScoreDVI (37.76/0.939), Ours (39.01/0.965)

Figure 2: Visual comparison of different denoising methods in SIDD validation dataset.

indicated by the black plot in Fig. 3(b) are constant for all noisy images. These analyses suggest that the ALE captures the signal-dependent features of real-world noise effectively.

In addition, we skip the variational inference process and directly use the prior precision \(p(_{t-1})\) to derive \(x^{*}_{t-1}\), resulting in \(x^{*}_{t-1}=_{t-1}y_{t-1}+(1-_{t-1})_{}(x_{t},t),_{t-1}= _{t}^{2}/(_{t}^{2}+)\). The corresponding denoising result (i.e., without ALE) is reported in the second row of Table 3 and is largely behind the denoising result of using ALE, further verifying the effectiveness of ALE.

**Rectification in Eq.** (17). By introducing the local Gaussian convolution operation, we explicitly refine the estimated noise variance \(1/(_{t-1})\). As shown in the fourth row of Table 3, using \((_{t-1})}\) consistently improves the quantitative performance.

**Temperature \(\)**. When \( 1\), the effect of diffusion priors \(p(x_{t-1}|x_{t})\) is reduced within the variational inference during the reverse diffusion process. As shown in Table 4, decreasing \(\) gradually improves the quantitative denoising performance, peaking at \(=1/5\). Further reducing \(\) degrades PSNR/SSIM as insufficient diffusion priors are involved in variational Bayes.

\(\) **in prior precision and kernel scale \(s\)**. Regarding \(\), it roughly represents the noise level of noisy image \(y\) (given \(=1\)) and we choose \(\) according to the empirical variance of the textureless area of \(y\) for one test set. kernel scale \(s\) is set by considering the spatial correlation of noise present in real-world images. We ablate these two parameters in Table 5, which indicates that they are relatively robust to moderate changes.

**Local diffusion priors**. We consider diffusion models pre-trained with other image resolutions as diffusion priors, including the \(128 128\) version and \(512 512\) version 4. The corresponding denoising performance is reported in Table 6, and implementation details are provided in the Appendix A.3.1. Regarding SIDD, we observe that matching the resolution of diffusion priors and test images (both \(256 256\)) achieves the best performance. Such a result is reversed for the remaining datasets,

  \(\) value & \(1\) & \(1/2\) & \(1/4\) & \(\) value & 5e-3 & 1e-2 & 1.5e-2 \\  PSNR/SSIM & 37.740.955 & 37.730.955 & 37.970.957 & 37.470.953 & 37.470.953 \\  \(\) value & \(1/5\) & \(1/10\) & \(1/20\) & \(\) value & 0.8 & 1.0 & 1.2 \\  PSNR/SSIM & **38.01/0.959** & 37.800.953 & 35.550/0.913 & & & \\  

Table 4: Ablation of temperature \(\) on CC dataset

Figure 4: The estimated noise variance \(1/(_{0})=_{0}/_{0}\) on SIDD dataset

  with ALE & with Gaussian Conv & SIDD & FMDD & PolyU & CC \\  ✗ & ✗ & 32.12/0.741 & 27.07/0.530 & 35.40/0.895 & 33.10/0.830 \\ ✓ & ✗ & 34.63/0.870 & 33.11/**0.865** & 38.70/0.969 & 37.82/0.956 \\ ✓ & ✓ & **34.76/0.887** & **33.14**/0.860 & **38.71/0.970** & **38.01/0.959** \\  

Table 3: Ablation on adaptive likelihood estimation and local Gaussian convolution

  \(\) value & 5e-3 & 1e-2 & 1.5e-2 \\  PSNR/SSIM & 38.030/0.957 & 38.01/0.959 & 37.47/0.953 \\  \(\) value & 0.8 & 1.0 & 1.2 \\  PSNR/SSIM & 37.876/0.957 & 38.01/0.959 & 38.100/0.959 \\  

Table 5: Ablation of \(\) and \(s\) on CC datasetwhere the \(256 256\) diffusion prior for HR images is more effective than its \(512 512\) counterpart. This is likely because training HR diffusion models (e.g., \( 512\)) is more challenging, resulting in inferior generation performance (e.g., FID) compared to LR diffusion models . Consequently, the local prior inherent in medium-resolution diffusion models is superior for restoring HR images.

### Application to other non-Gaussian noises

Although our method is designed to handle real-world noise, it can also address other non-Gaussian noises, including Poisson noise and multiplicative Bernoulli noise. As these synthetic noises are spatially independent, we do not utilize Eq. (17) in our method. We report the denoising performance in Table 7, with ZS-N2N  selected for comparison. Experimental details and visual results are given in the Appendix A.3.2 and A.5. Our method achieves better quantitative metrics than ZS-N2N on Poisson denoising and also preserves more local details and textures (see Fig. 8). While ZS-N2N shows better SSIM than ours on Bernoulli denoising, it causes intensity shifts (see Fig. 9) and thus has poorer PSNR.

### Application to image demosaicing

In addition to denoising, our method is readily available for image restoration with pixel-wise degradation, e.g., image demosaicing. To adapt our method to this task, we define the forward process \(y_{0}=M x_{0}\), where \(M\) is the degradation operator, and denotes element-wise multiplication. For demosaicing, \(M\) is the binary mask with \(0\) values indicating missing pixels of \(y_{0}\). We can incorporate \(M\) into \(p(y_{t-1}|x_{t-1},_{t-1})\) in Eq. (15), which results in \(_{t-1}=^{2}}{M_{t}^{2}+1/(_{t-1})}\). In Table 8, we compare our method against DDRM on image demosaicing (CFA pattern: RGGB), and our method shows better results.

**Limitation**. Our method builds on the DDPM sampling with a total of 1000 diffusion steps. Denoising a single noisy image with a resolution of \(256 256\) on an Nvidia 2080Ti GPU takes approximately 230 seconds, which is inefficient. In comparison, ZS-N2N takes about 16 seconds, despite its inferior denoising performance. Naively reducing diffusion steps in our method leads to apparent performance decreases, as indicated in Table 9. Our next move is to incorporate advanced accelerated sampling strategies into our method to reduce inference time while maintaining performance.

## 5 Conclusion

In this paper, we built upon diffusion priors and variational Bayes and proposed adaptive likelihood estimation and MAP inference during the reverse diffusion process, to handle real-world image noise that is structured and signal-dependent. The employed \(i.ni.d.\) likelihood function, combined with the precision prior and variational Bayes, allowed for the dynamical update of \(i.ni.d.\) noise precision posterior in each step of the generation process. This strategy adaptively refined the likelihood function and enabled the better MAP inference. Our method achieved excellent denoising performance on diverse real-world image denoising datasets and was also effective for removing other non-Gaussian synthetic noises.