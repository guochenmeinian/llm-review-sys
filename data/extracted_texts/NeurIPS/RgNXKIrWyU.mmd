# Reusing Pretrained Models by Multi-linear Operators

for Efficient Training

 Yu Pan\({}^{}\), Ye Yuan\({}^{}\), Yichun Yin\({}^{}\), Zenglin Xu\({}^{}\), Lifeng Shang\({}^{}\), Xin Jiang\({}^{}\), Qun Liu\({}^{}\)

\({}^{}\)Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China

\({}^{}\) Pengcheng Laboratory, Shenzhen, China

\({}^{}\) Peking University, Beijing, China

\({}^{}\) Huawei Noah's Ark Lab, Shenzhen, Guangdong, China

This work is done when Yu Pan is an intern at Huawei Noah's Ark Lab.Corresponding author.

###### Abstract

Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the "target model"), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Experiments demonstrate that our method can save 76% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT by +12.0% and LiGO by +20.7%, respectively.

## 1 Introduction

Transformers  have recently achieved great successes in various scenarios . Generally, Transformers tend to be larger for more expressive power and better performance (e.g., ViT-G  and GPT-3 ). As the size of models continues to grow, training Transformers takes longer and can result in higher CO\({}_{2}\) emissions, which conflicts with the principles of Green AI . Thus, training Transformers efficiently is crucial not only for financial gain but also for environmental sustainability . To achieve efficient training, it is a wise option to grow a pretrained small model into a larger one, since the pretrained small model has already learned knowledge from the data, which allows for faster training compared to starting from scratch. . Moreover, there are numerous pretrained models that are easily accessible , reducing the cost of utilizing a smaller pretrained model. Furthermore, empirical evidence shows that Transformers have inductive biases that facilitate scaling fitting . This demonstrates the feasibility of learning from pretrained models .

Figure 1: Expanding operators. \(W_{s}\) and \(W_{t}\) mean small pretrained and target weights, respectively. \(_{_{w}}\) and \(_{_{d}}\) denote width and depth expanding operators with parameters \(_{w}\) and \(_{d}\).

The concept of reusing pretrained models essentially involves exploring the efficient mapping from the pretrained model to the target model. This mapping process can be represented as linear growth operators, as depicted in Figure 1. One viable approach to mapping is to leverage knowledge from other weights. For instance, StackBERT  utilizes low-layer information to construct new higher layers, by duplicating layers to increase depth during training epochs. bert2BERT  expands width through the expanding operator of Net2Net . Differently, bert2BERT utilizes weights of neighbor layers for enhancing the ability, which also shows the benefit of taking knowledge from other weights. Moreover, distinct from the aforementioned research directly employing a fixed operator, LiGO  trains expanding operators which are tied with layers to achieve superior transfer accuracy by considering knowledge from other layers, which is advantageous for training efficiency. However, they neglect the potential connectivity between models.

**Potential Connectivity between Models.** The attention maps of BERT, as depicted in Figure 2, indicate that there are similarities not only in the weights of the same or different layers, but also across different models. These similarities suggest that we can leverage a full mapping transformation to capture this correlation in both inter-layer and intra-layer connections, even across different scaled models. By doing so, we can reuse the parameters of pretrained small models and accelerate the training of larger models. However, previous studies have overlooked the overall network connectivity and have instead expanded each weight through partial mapping transformation. For instance, Net2Net  only considers preserving functionality by transforming weights individually, while bert2BERT  increases model width head by head in Transformers. On the other hand, LiGO  primarily focuses on extending the weights of the same type (e.g., query, key, and value in Transformers), which heavily affects the performance of the transferred models.

**Multi-linear Operators.** Based on the above observations, we propose mapping the entire pretrained model to each weight of the target model, instead of employing a partial transformation. This approach can ensure high expressive power, promoting the potential connectivity in models for better training efficiency. However, the completed mapping has a huge parameter tensor, occupying an enormous space (see Section 3.2). To address this issue, we propose Mango, a multi-linear structure (i.e., tensor ring [33; 52; 51]), which decomposes the large mapping tensor into four smaller tensors that are bonded by ranks to construct multi-linear operators. Mango allows for efficient training by reducing the space required for the mapping tensor. Formally, Mango can be considered as a generalization of LiGO and bert2BERT (see Section 3.3). By using the full mapping approach, Mango can save 76% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT  by +12.0% and LiGO  by +20.7%, respectively.

## 2 Related Work

**Efficient Training from Scratches.** Model scratches can be roughly regarded as models without knowledge priors. Some training strategies for scratches are universal and orthogonal to our method.

Figure 2: Attention pattern maps between BERT-Small (shorted as “S”) and BERT-Base (shorted as “B”) in four examples. “L” represents the layer index, while “A” denotes the index of the attention head. For example, “S-(L10, A6)” denotes the attention map from layer 10, head 6 of the BERT-Small model. The examples are derived from different sentences. In each example, we can observe the similarities of attention maps in both inter-layer and intra-layer connections, even across different scaled models.

For example, Adam  and large-batch size training  accelerate the training process from angles of optimizer and magnitude of input data, respectively. Wang et al.  enable the training of very deep neural networks with limited computational resources via a technique called active memory. Shoeybi et al.  use mixed precision training to assist training. Low-rank methods benefit training for less memory and time . Wu et al.  take notes of rare words for better data efficiency. Dropping layers , knowledge inheritance , and merging tokens  are also efficient methods for training. Another line of work [13; 63; 14; 44] is termed progressive training, which gradually increases the model size within the training process for training efficiency. Li et al.  employ a neural architecture search (NAS) method to search optimal sub-networks for progressive training on ViTs. Xia et al.  suggest a three-phase progressive training regime to achieve a good trade-off between training budget and performance. Wang et al.  introduces a novel curriculum learning approach for training efficiency through firstly learning simpler patterns, then progressively introducing more complex patterns.

**Efficient Training from Pretrained Models.** Pretrained models usually contain abundant data knowledge, which is helpful for training . By preserving the function of a pretrained model while expanding the model size, it is feasible to give the corresponding larger model an initial state with high performance. Net2Net  is the first work to propose the concept of function-preserving transformations by expanding width by splitting neurons and growing depth with identity layers. However, Net2Net splits neurons randomly. Towards this problem, a series of studies [56; 58; 49; 57] propose to select the optimal subset of neurons to be split by utilizing functional steepest descent. bert2BERT  expands small transformers by following the function preserving idea. Recently, LiGO  utilizes a trainable linear operator to learn a good expanding formula. Different from these prior studies, our method tries to implement a full mapping that achieves comprehensive utilization of the whole smaller model.

**Neural Network Initialization.** Our method is related to neural network initialization techniques. Xavier  and Kaiming  initialization aim to control input variance equal to that of output. Generalizing Xavier and Kaiming methods, a universal weight initialization paradigm proposed by Pan et al.  can be widely applicable to arbitrary Tensorial CNNs. In addition, Hardt and Ma  has shown theoretically that network training benefits from maintaining identity, particularly for improving the efficiency of residual networks. Fixup  and ZerO  both set residual stem to 0 (not residual connections) to ensure the identity of signals, thereby successfully initializing ResNets. Skiplnit  replaces Batch Normalization with a multiplier whose value is 0. ReZero , on the other hand, adds extra parameters of value \(0\) to maintain identity, resulting in faster convergence. IDInit is an initialization approach to keep the identity matrix for stable training of networks . In comparison, our work explores fully reusing smaller pretrained models as efficient initialization.

**Low-rank Techniques in Neural Networks.** Low-rank methods are feasible for reducing spatial and temporal complexities in neural networks [52; 35; 62]. For example, Idelbayev and Carreira-Perpinan  uses matrix decomposition to compress convolutional neural networks (CNNs) for faster inference. LoRA  applies low-rank matrices for fine-tuning large language models (LLMs) in affordable resources. As a parameter-efficient tuning (PETuning) method  for federated learning, FedPara  utilizes the Hadamard product on low-rank parameters for reducing communication time in federated learning. Pan et al.  and Li et al.  use tensor ring decomposition for reducing the size of neural networks. Tucker decomposition and block-term Tucker decomposition have been used in T-Net and BT-layers [24; 64] for improving the model performance, respectively. Ma et al.  apply block-term Tucker to compress Transformers. Yin et al.  propose to use ADMM to optimize tensor training to achieve better compression and performance. These techniques have built a solid foundation for our work to implement multi-linear transformation for transferring knowledge from pretrained models to target models.

## 3 Mango Operator

This section presents the proposed Mango operator. First, we provide an overview of the tensor diagram, the concept of the tensor ring matrix product operator (TR-MPO), and the formulation of Transformer architecture in Sec. 3.1. Next, we delve into the details of the proposed multi-linear mapping operator (i.e., Mango) in Sec. 3.2. Finally, we compare Mango with recent advances in terms of tensor diagrams in Sec. 3.3.

### Preliminary

**Tensor Diagram.** Tensor decomposition is a technique that involves splitting a tensor into multiple smaller tensors, typically more than the two matrices that result from matrix factorization. To better illustrate the interactions among multiple tensors, tensor diagrams are often used. A tensor diagram is composed of two primary elements: a tensor vertex and a tensor contraction. A tensor vertex represents a tensor, and its order is determined by the number of edges connected to it. Each edge is assigned an integer that indicates the dimension of the corresponding mode. As shown in Figure 3, a 3rd-order tensor \(}^{_{0}_{1} _{2}}\) can be drawn as a circle with three edges. The process of taking the inner product of two tensors on their matching modes is called tensor contraction. For example, Two tensors, \(}^{_{0}_{1} _{2}}\) and \(}^{_{0}_{1} _{2}}\), can be contracted in corresponding positions to form a new tensor of \(^{_{0}_{1}_{2} _{3}}\), when they have equal dimensions: \(_{2}=_{0} e_{0}\). The contraction operation can be formulated as

\[(}_{2}^{0}})_{i_{0},i_{1},j_{2},j_{3}}= _{m=0}^{e_{0}-1}}_{i_{0},i_{1},m}}_{m,j_{2},j_{3}}.\] (1)

The tensor diagram is an elegant tool for succinctly visualizing and manipulating multi-dimensional arrays or tensors. By representing tensor operations as networks of nodes and edges, these diagrams provide an intuitive way to understand the underlying mathematical operations and their interactions. As a result, we have chosen to use tensor diagrams to illustrate our method and to analyze the connections between our approach and prior studies.

**Tensor Ring Matrix Product Operator (TR-MPO).** TR-MPO means tensor ring (TR) of an MPO  format. Given a \(2N\)-order tensor \(}^{I_{1} J_{1} I_{2} J_{2} I _{N} J_{N}}\), its TR-MPO decomposition can be mathematically expressed as

\[}_{i_{1},j_{1},i_{2},j_{2},i_{N},j_{N}}_{r_{1 },r_{2},,r_{N}=1}^{R_{1},R_{2},,R_{N}}}_{r_{1},i_{ 1},j_{1},r_{2}}^{(1)}}_{r_{2},i_{2},j_{2},r_{3}}^{(2)}}_{r_{3},i_{3},j_{3},r_{4}}^{(3)}}_{r_{N},i_{ N},j_{N},r_{1}}^{(N)},\] (2)

where \(\{R_{1},R_{2},,R_{N}\}\) denote the ranks, \(}^{(n)}^{R_{n} I_{n} I_{n} R_{ n+1}}\) denotes a 4th-order core tensor and \(R_{1}=R_{N+1}\), which indicates ring-like structure.

**Transformer Architecture.** The Transformer  is a deep learning architecture that has revolutionized the artificial intelligence field including computer vision (CV) and natural language processing (NLP). As shown in Figure 4, a Transformer block consists of two main sub-layers: the multi-head self-attention (MHSA) layer and the feed-forward neural network (FFN) layer.

(1) MHSA Layer. The MHSA layer in the Transformer block computes the attention scores between each input element and every other element, allowing the model to attend to different parts of the input sequence during processing. Inputs of MHSA are query matrix \(^{I}\), key matrix \(^{I}\) and value matrix \(^{I}\) with parameters \(^{Q}^{I O},^{K}^{I O},^{V}^{I O},^{O}^{I O}\). Usually, it constrains \(I=O\). Moreover, MHSA separates these parameters into \(n\) heads: \(\{^{Q,i}\}^{n},\{^{K,i}\}^{n},\{^{V,i}\}^{n},\{ ^{O,i}\}^{n}\). The MHSA mechanism can be formulated as follows:

\[_{i}(,,)=( ^{Q,i}(^{K,i})^{T}}{} })^{V,i}^{O,i^{T}},\]

\[((,,))=_{i=1}^{n}_{i}(,,),\] (3)

where \(d_{k}\) is the dimensionality of the key vectors. The self-attention mechanism is performed multiple times in parallel, each time using a different set of learned parameters \(^{Q,i}\), \(^{K,i}\), and \(^{V,i}\) to compute multiple "heads" of attention. The resulting attention heads are linearly transformed by a learned weight matrix \(^{O,i}\) to produce the output. At last, MHSA concatenates the output as the final result of the MHSA layer.

(2) FFN Layer. The FFN layer in the Transformer block is responsible for applying a non-linear transformation to the output of the self-attention layer. \(^{I}\) is the input. The weights are

Figure 3: Tensor diagram instances.

\(^{IN}^{I kO}\) and \(^{OUT}^{kI O}\), where usually \(I=O\) and \(k\) is a ratio that is often set to 4. The FFN layer can be formulated as

\[()=(^{IN})^{OUT}.\] (4)

We neglect biases in the formulation as it is usually set to 0 at initialization. The output of the FFN layer is obtained by applying two linear transformations to the output of the MHSA layer.

Finally, both the self-attention output and the FFN output are processed through a residual connection and layer normalization to prevent the model from collapsing or overfitting to the training data. Apparently, the parameters in a Transformer are mainly based on its linear transformation matrices, i.e., \(^{Q},^{K},^{V},^{O}\), \(^{IN}\) and \(^{OUT}\).

### Multi-linear Operator

The essential of model growth is to transfer the knowledge from the small model to the bigger counterpart. Prior study  finds that taking weights from the neighbor layer as initial parameters can further improve the convergence speed, which gives the insights that knowledge from other layers can help training as there are many similar attention maps among layer. Nevertheless, the fact is that this similarity exists all over the pretrained model as shown in Figure 2, which motivates us to consider the possibility of whether we can utilize the knowledge from all weights. Based on this consideration, we construct a mapping of the whole model. As the correlation is hard to formulate heuristically, we learn the mapping parameter to implicitly transfer the pretrained model.

**Notation.** Here, we give the notations for clearly formulating our method. A model with \(L\) layers and a \(D\) hidden size can be denoted as \((L,D)\). Following Section 3.1, parameters of \(j\)-th layer are a set \(_{j}=\{_{j}^{Q}^{I O},_{j}^ {K}^{I O},_{j}^{V}^{I O}, _{j}^{O}^{I O},_{j}^{IN}^ {I RO},_{j}^{OUT}^{kI O}\}\), \(j[L]\), usually satisfying \(I=O=D\). The weight of \((L,D)\) is \(^{L,D}=\{_{j}\}_{j=1}^{L}\). A growth operator with parameter \(}\) is denoted as \(_{}}\). A mapping from \((L_{1},D_{1})(L_{2},D_{2})\) can be denoted as \(^{L_{2},D_{2}}=_{}}(^{L_{1},D_{1}})\). In the case of \(L_{1}<L_{2}\), this mapping represents a growth mapping. After growing, \(^{L_{2},D_{2}}\) will be used as initial weights for the target model.

**Full Mapping Operator.** To utilize all weights for knowledge transferring, we first concatenate weights across layers. As shown in Figure 4, we concatenate \(j\)-th layer to form a tensor of shape \(B I O\) along with order \(I\) and \(O\) where \(B=2k+4\). Then the final weight tensor \(}^{B I O L}\) can be derived by combining the concatenated tensors of \(L\) layers. Giving

Figure 4: The full mapping of the Mango operator. The left sub-figure shows that the parameters of a Transformer layer are \(^{Q},^{K},^{V},^{O}\), \(^{IN}\) and \(^{OUT}\). \(I\) means an input dimension size. \(O\) denotes an output dimension size. \(L\) is the layer number. We concatenate all the parameters into a tensor \(}\) and then consider a full mapping operator \(}\) to transform this tensor. However, \(}\) is huge, thereby, we use a multi-linear method TR-MPO to decompose it into four smaller tensors \(\{}_{B},}_{I},}_{O},}_{L}\}\) form the Mango operator \(_{}_{B},}_{I},}_{O},}_{L}}\).

a small model \(}_{1}^{B_{2} I_{1} O_{1} L_{1}}\), a big model \(}_{2}^{B_{1} I_{2} O_{2} L_{2}}\), and a parameter \(}^{B_{1} I_{1} O_{1} L_{1} B _{2} I_{2} O_{2} L_{2}}\) of a full mapping operator \(_{}}\), we can transfer \(}_{1}\) to \(}_{2}\) as

\[(}_{2})_{b_{2},i_{2},o_{2},l_{2}}=_{b_{1},i_{1},o_{1},l_{1} }(}_{1})_{b_{1},i_{1},o_{1},l_{1}}}_{b_{1},i_{1 },o_{1},l_{1},b_{2},i_{2},o_{2},l_{2}},\] (5)

where \(b_{1}\), \(i_{1}\), \(o_{1}\), \(l_{1}\), \(b_{2}\), \(i_{2}\), \(o_{2}\), and \(l_{2}\) are entries of corresponding tensors.

**Multi-linear Mapping Operator.** Note that \(}\) is extremely big, which makes it infeasible to display this mapping by applying a whole transformation tensor. Therefore, we propose a multi-linear operator named Mango which decomposes \(}\) through a tensor ring matrix product operator (TR-MPO) in four small tensors \(\{}_{B}^{R_{1} B_{1} B_{2} R_{2 }},}_{O}^{R_{2} O_{1} O_{2} R_{3 }},}_{L}^{R_{3} L_{1} L_{2} R_{4 }},}_{I}^{R_{4} I_{1} I_{2} R_{1 }}\}\). \(R=\{R_{1},R_{2},R_{3},R_{4}\}\) is the rank of TR-MPO. Then, we can update \(_{}}\) to \(_{}_{B},}_{I},}_{O},}_{L}}\), and Eq. (5) can be reformulated with a multi-linear form as

\[(}_{2})_{b_{2},i_{2},o_{2},l_{2}}=_{b_{1},i_{1},o_{1},l_{1},r_{1},r_{2},r_{3},r_{4}}\\ (}_{1})_{b_{1},i_{1},o_{1},l_{1}}(} _{B})_{r_{1},b_{1},b_{2},r_{2}}(}_{O})_{r_{2},o_{1},o_{2},r_{3 }}(}_{L})_{r_{3},l_{1},l_{2},r_{4}}(}_{I})_{r_{4 },i_{1},i_{2},r_{1}}.\] (6)

The total size of \(}_{B}\), \(}_{I}\), \(}_{O}\), and \(}_{L}\) is exponentially less than \(}\), which makes Mango viable for practical implementation while maintaining the full correlation between the small and big models.

**Training Target.** After designing the multi-linear operators, we train these operators to obtain the function preserving \(}_{2}\). The training target can be denoted as

\[_{}_{B},}_{I},}_{O},}_{L}}_{}}(, }_{2}),\ \ \ \ }_{2}=_{}_{B},}_{I},}_{O},}_{L}}(}_{1}),\] (7)

where \(}\) is a loss function, and \(\) is a data distribution. By replacing the full space of \(}\) with four small spaces, the spatial requirements of the training process are reduced exponentially.

**Procedures of Applying Mango.** The procedures of Mango can be divided into four steps: (i) concatenating weights \(^{L_{1},D_{1}}\) of a pretrained model \((L_{1},D_{1})\) to construct a tensor \(}_{1}\); (ii) training the growth operator \(_{}_{B},}_{I},}_{O},}_{L}}\) items of Eq. (7) in a few steps (e.g., 100) to make transferred models maintaining function; (iii) recovering weight tensor \(}_{2}\) through the multi-linear operator \(_{}_{B},}_{I},}_{O},}_{L}}\); (iv) splitting \(}_{2}\) to the weight \(^{L_{2},D_{2}}\) of the target model \((L_{2},D_{2})\) as initialization to continue training.

**Remark.** Tensor decomposition is often used to compress neural networks. Actually, there are spaces for further reducing the operator size. However, we are not exploring the border of compression ratio, but the possibility of the learning ability of multi-linear operators and the influence on model growth. Therefore, we decompose the huge \(}\) into four interpretable smaller tensors. \(}_{B}\) means the interactions on the parameters in the same layer. \(}_{I}\) and \(}_{O}\) denote the transformations of input and output dimensions in one parameter, respectively. \(}_{L}\) indicates the relationship among layers. \(R\) means the low-rank level of \(}\). Smaller \(R\) means less correlation of these four tensors.

Figure 5: Growth processes with the tensor diagram. The circle with an oblique stroke means a super-diagonal tensor. The blue color means a trainable operator, while the gray color denotes an untrainable operator. The red color means the smaller model. Each tensor diagram means the growth process that growing \(M_{1}\) to a bigger model through growth the operator \(S_{*}\).

### Comparison with Recent Advances

Basically, the goal of model growth is to widen and deepen the width and depth of the pretrained models, and operation on width and depth can be easy to implement. Therefore, most of the previous studies grow width and depth separately, although there may exist some correlation that may influence the training efficiency. In this part, we analyze the difference among methods of model growth with the help of the tensor diagrams.

We illustrate a comparison among bert2BERT, LiGO, and Mango in Figure 5. The red circle means a pretrained model, while the blue and gray circles denote trainable and untrainable operator parameters, respectively. The circle with an oblique stroke represents a super-diagonal tensor, where the diagonal elements are all 1. This super-diagonal tensor means that growth operators expand other modes along the mode of the super-diagonal tensor.

**bert2BERT.** As shown in Figure 5, the parameters in the growth operator of bert2BERT are all frozen, since they are designed by heuristic inspiration to preserve function. bert2BERT expands modes of \(I_{1}\) and \(O_{1}\) along with the mode of \(L_{1}\), indicating that one weight is expanded without knowledge from any other weight. To further increase the training ability, bert2BERT applies \(}_{K}\) to construct Advanced Knowledge Initialization (AKI) to utilize knowledge of other layers to help accelerate training.

**LiGO.** Different from bert2BERT, the LiGO operator can be trained. In addition, LiGO expands \(I_{1}\) and \(O_{1}\) along the mode of \(B_{1}\). With \(}_{L}\), LiGO can combine knowledge among layers. Operators of Net2Net and StackBERT grow with or depth separately, which can be regarded as a sub-solution to \(}_{I}\), \(}_{O}\), and \(}_{L}\) of LiGO. However, one weight in LiGO will not leverage the knowledge of weights in the same layer. Therefore, LiGO only implements partial mapping like bert2BERT.

**Mango.** We employ Mango on each mode of the weight \(}_{1}\). Mango can approach the full mapping tensor \(}\) as a low-rank approximation with rank \(R\), rather than partial mapping operators like LiGO and bert2BERT. Therefore, Mango can obtain \(}_{2}\) to capture adequate knowledge from pretrained weights as formulated in Eq. 6. Moreover, the diagonal tensor in LiGO and contraction between a diagonal tensor and \(}_{K}\) in bert2BERT are subsets of \(}_{L}\) and \(}_{B}\) in Mango. Therefore, Mango can be a generalization of bert2BERT and LiGO with more expressive power.

## 4 Experiment

In this section, we design a set of experiments to validate the proposed Mango. To begin with, we conduct an ablation study to analyze the influence of Mango on width and depth in Sec. 4.1. Then we conduct image classification with DeiT-B  on ImageNet to show the acceleration in computer vision task in Sec. 4.2. Later we conduct a pretraining experiment with BERT-Base  to demonstrate the effectiveness in natural language processing tasks in Sec. 4.3. At last, we also employ a pretraining experiment with GPT-Base  to show the wide adaption of Mango in Sec. 4.4.

Ratio of saving FLOPs.We evaluate the training efficiency in terms of the ratio of saved floating-point operations (FLOPs). Let us assume that training a model from scratch to convergence on metric \(\) (e.g., MLM loss or accuracy) necessitates \(_{Scratch}\) FLOPs. Then, for a method that reaches the same metric \(\) with \(_{*}\) FLOPs, its FLOP saving ratio \(r\) can be computed as

\[r=-_{*}}{_{Scratch}}.\] (8)

   Method & Reference & Operator Parameter & Trainability & Spatial Complexity \\  bert2BERT & Chen et al.  & \(}_{I}\), \(}_{O}\), \(}_{K}\) & ✗ & \(2L_{1}D_{1}D_{2}+L_{1}L_{2}\) \\ LiGO & Wang et al.  & \(}_{I}\), \(}_{O}\), \(}_{L}\) & ✓ & \(2B_{1}D_{1}D_{2}+L_{1}L_{2}\) \\ Mango & - & \(}_{B}\), \(}_{I}\), \(}_{O}\), \(}_{L}\) & ✓ & \(2RD_{1}D_{2}+R^{2}(B_{1}B_{2}+L_{1}L_{2})\) \\   

Table 1: The comparison among bert2BERT, LiGO and Mango. In a case of \((L_{1},D_{1})(L_{2},D_{2})\), there usually are \(I_{1}=O_{1}=D_{1}\) and \(I_{2}=O_{2}=D_{2}\). \(R=(R_{1},R_{2},R_{3},R_{4})\).

### Ablation Study

In this experiment, we explore the influence of Mango on growing width, depth and both of them in addition to rank setting. We use three tiny vision Transformers (ViTs) , i.e., DeiT-T-A, DeiT-T-B, and DeiT-T-C, for growing to DeiT-S  on ImageNet . Structures of these DeiTs can be found in Table 4 of the Appendix. For ease of setting, we set all the ranks the same in the range of \(\{1,4,7,10\}\). We train Mango operators for 100 steps, which only requires negligible time. We use Adam with learning rate 1e-3 and weight decay 1e-2 for 300 epoch optimization. The batch size is 1024.

Results are shown in Figure 6. Mango achieves acceleration of at most 31.0%, 46.0%, and 41.3% on expanding width, depth, and both of them, respectively, which shows Mango can fit various growth scenarios. Interestingly, these acceleration ratios are higher when the operator accuracies are better along the types of pretrained models, e.g., as DeiT-T-B at most achieves 73.81% accuracy which is higher than 72.89% of DeiT-T-A, DeiT-T-B derives higher acceleration ratio. This phenomenon suggests that when there are multiple pretrained models can be selected, the better accuracy for one pretrained model through Mango, the faster training speed can be obtained for the target models.

In each expanding case, the accuracies of operator training tend to increase with higher ranks. However, all the cases on the three pretrained models show that better accuracy will not lead to faster training with the same pretrained model. For example, in Figure 6(a), the operator with rank 10 has an accuracy that is 1.19% higher than the operator with rank 1. Nevertheless, the two ranks reach the same acceleration ratio. This result suggests that rank 1 is enough to use, and also enjoys the advantages of spatial and temporal complexities compared to bigger ranks. And we choose rank 1 for Mango to construct the later experiments.

### Results on Large-Scale Vision Transformer

We conduct the experiment to show the training acceleration on large-scale vision Transformers. We train the Deit-B from Deit-S  on ImageNet. Structures of the two models are shown in Table 4 of the Appendix. Ranks of Mango are all set to 1 for complexity benefit without performance loss. The operators of Mango and Ligo are all trained within 100 steps. We use Adam as the optimizer with learning rate 1e-3 and weight decay 1e-2. The batch size is 1024. The training epoch is 300.

Results are shown in Figure 7(a). Mango saves 76% FLOPs from Scratch which converges to an accuracy of 80.45% in the end. Compared with schemes of training from Scratch (i.e., Scratch and StackBERT), methods of training from a smaller model (i.e., Mango, bert2BERT, and LiGO) can attain 70% accuracy in a short time, even bert2BERT start in low accuracy. Compared with the recent SOTA models, Mango has surpassed bert2BERT for +12.0%, and LiGO for +20.7%. This experiment shows the ability of Mango to achieve significant improvement in training acceleration. To investigate the influence of Mango on transferring ability, we also conduct an experiment on downstream tasks, including CIFAR10 , CIFAR100 , Flowers , Cars , and ChestXRay8 . Results are shown in Table 2. It is easy to see that Mango achieves similar results to the Scratch, which indicates Mango has not influenced the transferring ability.

Figure 6: Influence from ranks on expanding width, depth, and both of them. The green curve means the accuracy of training operators for 100 steps. The red curve means the final acceleration ratio on training DeiT-S. Mango can achieve training acceleration in every case. Moreover, in each sub-graph, when the accuracy increases with a higher rank, the acceleration ratio keeps almost fixed.

### Pretraining on BERT

In this experiment, we conduct the validation to show the training acceleration on BERT [10; 61]. The dataset is the concatenation of English Wikipedia and Toronto Book Corpus . We train the BERT-Base from BERT-Small. The training epoch is 40. The batch size is 768. We list the structures of the two models in Table 5 of the Appendix. The ranks of Mango are all 1. Mango and LiGO are both warmly trained for 100 steps. The optimizer is set to AdamW. The learning rate is 1e-4 and the weight decay is 1e-2.

We illustrate the training curves in Figure 7(b). The methods of training from BERT-Small (i.e., Mango, bert2BERT, and LiGO) can all surpass the progressive training StackBERT of acceleration ratio 29.5%. bert2BERT is over StackBERT by +6.1%. Mango achieves the highest acceleration of 39.2% FLOPs which is +3.6% more than bert2BERT. Loss of StackBERT steeply decreases within training for Stacking layers, which can also demonstrate the pretrained weights are helpful for training efficiency. We show the effectiveness of Mango on SQuAD and GLUE benchmark as in Table 3. Mango shows the same transferring ability with faster convergence, which indicates a promising use for practical training.

### Pretraining on GPT

We also implement the experiment to show the training acceleration on GPT . The dataset is the concatenation of English Wikipedia and Toronto Book Corpus . We train the GPT-Base from GPT-Small. The structures of the two models are shown in Table 5 of the Appendix. The ranks of Mango are all 1. Mango and LiGO are trained for 100 steps. We use Adamw with learning rate 1e-4 and weight decay 1e-2. The batch size is 512. The training epoch is 35.

    & FLOPs & Ratio &  &  &  &  &  &  \\  & (\(\) 1e18) & & & & & & & \\   \\  Scratch & 12.9 & - & 99.03 & 90.22 & 97.27 & 91.89 & 55.66 & 86.82 \\ StackBERT & 11.3 & 12.6\% & 99.11 & 90.10 & 97.44 & 91.71 & 55.63 & 86.80 \\  (12,384)(12,768)\)_} \\  bert2BERT & 4.6 & 64.4\% & 98.99 & 90.47 & 97.51 & 91.88 & 55.34 & 86.84 \\ LiGO & 5.7 & 55.7\% & 99.11 & 90.52 & 97.18 & 91.82 & 55.45 & 86.82 \\ Mango & 3.0 & **76.4**\% & 99.13 & 90.23 & 97.49 & 91.83 & 55.46 & 86.83 \\   

Table 2: Results of transfer learning performance of DeiT-B. Mango can achieve similar performance to the Scratch model in downstream tasks while saving 76.4% FLOPs.

Figure 7: Results of pretraining DeiT-B, BRET-Base and GPT-Base. Compared to baselines, Mango can achieve the highest savings in FLOPs with 76.4% for DeiT-B, 39.2% for BERT-Base, and 59.9% for GPT-Base from the Scratch model. We also illustrate the corresponding results on wall time in Figure 10 of the appendix.

We compare the Scratch model, bert2BERT, LiGO, and Mango in Figure 7(c). We observe that the proposed Mango achieves a 59.9% acceleration ratio. While GPT is different from BERT in different structures, including layer normalization, mask method, and training strategy, Mango can always keep the highest performance. Although LiGO is lower than bert2BERT at the beginning, it converges slower and reaches 38.1% at last. By contrast, the Mango is almost at the lowest loss in the whole training process and achieves +16.7% more than bert2BERT and +21.8% more than LiGO, which shows a significant acceleration than the baselines.

## 5 Conclusion

Training Transformers can pose a significant demand on computational resources. Reusing pretrained models as an initialization strategy for the target model offers a promising approach to reducing resource costs and accelerating training. However, previous studies only mapped partial weights when growing models, which may fail to consider potential correlations in the whole model and result in inadequate growth mapping. Inspired by this observation, we propose to consider the interaction among all weights in the model to further improve acceleration ability. Specifically, we utilize a full mapping to comprehensively reuse the pretrained model, taking into account all correlations between model weights. As the full mapping tensor is huge and cannot be employed in practice, we propose to use Mango, a multi-linear operator, to reduce computation and space complexity. Experimental results demonstrate that Mango consistently achieves significant acceleration on various large-scale models (e.g., DeiT-B, BERT-Base, and GPT-Base). In the future, we hope that this method can contribute to green AI and significantly reduce the cost of training Transformers.

**Limitation.** While Mango significantly reduces training costs for large models, it still requires weights of a small pretrained model as the prior knowledge. Additionally, the Mango operator necessitates extra training for initializing. However, obtaining small model weights is relatively simple within the community, and given the substantial cost savings, the resources required for training the operator are minimal.

**Societal Impact.** The acceleration from Mango significantly impacts energy conservation and environmental protection, promoting the growth of Green AI, which boosts efficiency and reduces computational demand, minimizing energy usage and carbon emissions. Mango can also democratize AI, allowing broader access to technologies of pertaining models without the need for large resources, and promoting sustainable and accessible AI solutions that respect our environmental limits.