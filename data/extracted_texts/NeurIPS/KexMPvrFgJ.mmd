# Learning Depth-regularized Radiance Fields from Asynchronous RGB-D Sequences

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recently it is shown that learning radiance fields with depth rendering and depth supervision can effectively promote the view synthesis quality and convergence. But this paradigm requires input RGB-D sequences to be synchronized, hindering its usage in the UAV city modeling scenario. To this end, we propose to jointly learn large-scale depth-regularized radiance fields and calibrate the mismatch between RGB-D frames. Although this joint learning problem can be simply addressed by adding new variables, we exploit the prior that RGB-D frames are actually sampled from the same physical trajectory. As such, we propose a novel **time-pose function**, which is an implicit network that maps timestamps to \((3)\) elements. Our algorithm is designed in an alternative way consisting of three steps: (1) time-pose function fitting; (2) radiance field bootstrapping; (3) joint pose error compensation and radiance field refinement. In order to systematically evaluate under this new problem setting, we propose a large synthetic dataset with diverse controlled mismatch and ground truth. Through extensive experiments, we demonstrate that our method outperforms strong baselines. We also show qualitatively improved results on a real-world asynchronous RGB-D sequence captured by drones. Codes, data, and models will be made publicly available.

## 1 Introduction

Incorporating depth rendering and depth supervision into radiance fields has been demonstrated as a helpful regularization technique in several recent studies [2; 21; 32; 20]. However, this technique has not yet been successfully introduced into radiance field learning from UAV (Unmanned Aerial Vehicle) images, despite it's a typical choice in city modeling. A closer look at the aforementioned works reveals that they assume synchronized RGB and depth signals, which is hard to guarantee in UAV vision due to the lack of suitable synchronized sensors for long sensing ranges. So we study the _problem_ of learning depth-regularized radiance fields from asynchronous RGB-D sequences.

As a recap, the canonical radiance field  learns a neural network parameterized by \(\) that represents a 3D scene, from input images \(I\) and their intrinsic/extrinsic parameters \(_{}\). To alleviate the reliance on \(_{}\), some works [30; 11; 8] aim to resolve a different problem that self-calibrates \(_{}\). In other words, they jointly learn \(\) and \(_{}\) from input images \(I\). Similarly, the _formulation_ considered here is to learn scene representation \(\), camera parameters \(_{}\) and \(_{}\) from inputs images \(I\) and depths \(D\).

It is natural to develop the joint learning _formulation_ to resolve the _problem_, as it amounts to adding new parameters to existing methods [30; 11; 8]. However, an important prior is ignored that RGB-D frames are actually sampled from the same physical trajectory. As conceptually shown in Fig. 1-a/b, \(_{}\) and \(_{}\) can be considered as samples from a function that maps timestamps to \((3)\) elements. We name this function as **time-pose function** and model it with a neural network parameterized by \(\). As such, we address the _problem_ with a _new formulation_ that learns scene representation \(\) andtime-pose function \(\) from inputs RGB images \(I\) and depths \(D\). An interesting fact is that both \(\) and \(\) are implicit neural representation networks (or say coordinate-based networks) that allow fully differentiable training. To our knowledge, this _new formulation_ has not been proposed before.

We also propose an effective learning scheme designed in an alternative manner. In the first stage, we fit the time-pose function \(\) using one modality (e.g., RGB images) and infer the poses of the other, using a balanced pose regression loss and a speed regularization term. Secondly, we bootstrap a large-scale radiance field \(\) based upon Mega-NeRF  using the outputs of the trained time-pose function. Thanks to the first step, depth regularization can be imposed here in spite of RGB-D misalignment. Finally, thanks to the cascade of two fully differentiable implicit representation networks, we jointly optimize the 3D scene representation \(\) and compensate pose errors by updating \(\).

Since the _problem_ considered is new, we contribute a synthetic dataset (named AUS) for systematic evaluation. Using six large-scale 3D scenes, realistic drone trajectories of different difficulty levels are generated. Specifically speaking, simple trajectories are heuristically designed with a zig-zag pattern while complicated ones are generated by manual control signals in simulation. We also control the mismatch between RGB-D sequences using different protocols, to cover as many as possible scenarios that the algorithm may encounter in reality. Through a set of comprehensive experiments, we show the proposed method outperforms several state-of-the-art counterparts and our design choices contribute positively to performance. Last but not least, we present a real-world evaluation using asynchronous sensors on drones. Our depth rendering results (on unseen viewpoint) is shown in Fig. 1-d, which is much better than the result of Mega-NeRF shown in Fig. 1-e. This success is credited to the usage of depth regularization as made possible by our novel algorithm.

To summarize, we have the following contributions in this paper: (1) We formalize the new _problem_ of learning depth-regularized radiance fields from asynchronous RGB-D sequences, which is rooted in UAV city modeling. (2) We identify an important domain-specific prior in this problem: RGB-D frames are sampled from the same underlying trajectory. We instantiate this prior into a novel time-pose function and develop a cascaded fully differentiable implicit representation network. (3) In order to systematically study the problem, we contribute a photo-realistically rendered synthetic dataset that simulates different types of mismatch. (4) Through a comprehensive benchmarking on this new dataset and real-world asynchronous RGB-D sequences, we demonstrate that our method can promote performance over strong prior arts. Anonymous code: https://anonymous.4open.science/r/async-nerf

## 2 Related Works

**Large-scale Radiance Fields.** Neural Radiance Field (NeRF)  has shown impressive results in neural reconstruction and rendering. However, its capacity to model large-scale unbounded 3D scenes is limited. Several strategies [29; 26; 32; 14; 35] have been proposed to address this limitation, with a common principle of dividing large scenes into blocks or decomposing the scene into multiple levels. Block-NeRF  clusters images by dividing the whole scene according to street blocks. Mega-NeRF  utilizes a clustering algorithm that partitions sampled 3D points into different NeRF submodules. BungeeNeRF  trains NeRFs using a growing model of residual blocks with predefined multiple scales of data. Switch-NeRF  designs a gating network to jointly learn the scene decomposition and NeRFs without any priors of 3D scene shape or geometric distribution. However, these prior works fail to leverage the rich geometric information in depth images for effective regularization.

Figure 1: The problem of interest is to learn a depth-regularized radiance field using asynchronous RGB-D sequences (a). We proposed a time-pose function as conceptually shown in (b) to leverage the prior that RGB-D seuqnces are actually sampled from the same physical underlying trajectory. For a novel view (c), our method can render a better depth map (d) than Mega-NeRF (e).

Depth-regularized Radiance Fields.Volumetric rendering requires extensive samples and sufficient views to effectively differentiate between empty space and opaque surfaces. Depth maps can serve as geometric cues, providing regularization constraints and sampling prior, which accelerates NeRF's convergence towards the correct geometry. DS-NeRF  enhances this process using depth supervision from 3D point clouds, estimated by structure-from-motion, and a specific loss for rendered ray termination distribution. Mono-SDF  and Dense-Depth Prior  further supplement this with a pretrained dense monocular depth estimator for less-observed and textureless areas. To adapt NeRF for outdoor scenarios, URF  rasterizes a pre-built LiDAR point cloud map to generate dense depth images and alleviates floating elements by penalizing floaters in the free space. Moreover, S-NeRF  completes depth on sparse LiDAR point clouds using a confidence map, effectively handling street-view scenes with limited perspectives. However, those methods are not readily applicable to UAV captured images due to the lack of suitable synchronized sensors for long ranges.

Broader UAV Vision and Synchronization.Like autonomous driving, UAV vision is drawing increasing attention due to its unique characteristics. Broader UAV vision covers many topics like counting , trajectory forecasting , intention prediction , object tracking , physics understanding , next-best-view prediction , 3D reconstruction , and calibration . Sensor synchronization is challenging for UAV vision (and other settings) and several works address the problem from an algorithmic perspective. One possibility is to adopt tailored hardware designs or software protocols  to synchronize all the devices. Another branch of sensor-agnostic methods utilizes temporal priors by using Sum-of-Gaussians  or parametric interpolation functions .

## 3 Problem Formulation and Optimization Pipline

**Problem & Challenge.** Our goal is to learn a neural radiance field parameterized by \(\) for large-scale scene representation from UAV images as done in prior works [29; 32]. However, these prior works fail to leverage depth supervision, which is known [3; 21] as useful for training floater-less NeRFs. To our knowledge, there are no easily accessible synchronized RGB-D sensor suites for **large-scale** outdoor scenes, and trivially synchronizing them according to timestamp1 cannot fully address the misalignment issue. Instead of using expensive hardware, we take an algorithmic perspective.

**Input & output.** There are some prior works on large-scale scene modeling using aerial images [32; 29; 5; 6]. In this study, we assume an input RGB-D stream captured by drones: a set of RGB camera images \(\{I^{(i)}\}_{i=1}^{N_{f}}\) and a set of depth maps \(\{D^{(j)}\}_{j=1}^{N_{P}}\) (shown in Fig. 1-a) and we aim to recover the spatiotemporal transformations between them. Given that our focus is on relative transformation, it is viable to consider either the RGB or the depth stream as the reference without compromising generality. For convenience, we assume a set of camera poses \(\{_{I}^{(i)}\}_{i=1}^{N_{f}}\) for color images are obtained by an SfM algorithm. The neural scene representation parameterized by \(\) outputs an image \(\) as well as a depth map \(\) at a given perspective camera pose \(\).

**Observation.** Note that all the sensor data are captured with a drone on the **same** trajectory2, we can model the relationship between capture time \(t\) and sensor poses \(\) with an implicit **time-pose function** as \(:\,t\,}=[\,},}\,]\), where \(t\) is the timestamp of capture, and the estimated pose \(}\) is represented by a translation vector \(}^{3}\) and a quaternion \(}^{4}\).

**Pipeline overview.** We formulate our method as a 3-step optimization problem (as shown in Fig. 2). First, since the time-pose relationship for the RGB captures are given, we can train a time-pose

Figure 2: **Three-step Optimization. (i) A time-pose function parameterized by \(\) is trained to predict camera poses from timestamps; (ii) The neural radiance field parameterized by \(\) is bootstrapped with pure RGB losses; (iii) Both of the parameters \(,\) are jointly optimized with RGB-D supervision.**

function on the RGB sequence (Fig. 2-(i)). Then, to train the neural radiance field, we first bootstrap the network with pure RGB supervision (Fig. 2-(ii)). To further enable training with RGB-D supervision, we can use the previously trained time-pose function to estimate the corresponding depth camera poses \(\{_{D}^{(j)}\}\) of the depth timestamps \(\{t_{D}^{(j)}\}\). Since both of the networks are differentiable, we jointly optimize the networks in an end-to-end manner in the third stage (Fig. 2-(iii)).

## 4 Method

We introduce in Section 4.1 the details of learning an implicit time-pose function. In Section 4.2, we describe our neural scene representation networks and the bootstrapping strategy. In Section 4.3, we adopt depth supervision and jointly train the time-pose function with RGB-D pairs.

### Time-Pose Function

We represent the camera trajectory as an implicit time-pose function \(\) whose input is a timestamp \(t\), and whose output is a 6-DoF pose \(\) that consists of a 3-D translation \(x_{i}\) and a 4-D quaternion \(q_{i}\).

**Network Overview** The time-pose function (shown in the left part of Fig. 3) is approximated with a compact 1-D multi-resolution hash grid \(\{^{(l)}\}_{l=1}^{L}\), followed by an MLP decoder. The hash grid consists of \(L\) levels of separate feature grids with trainable hash encodings . The reason why we choose this architecture is as follows: The time-pose function is a coordinate-based function that may contain coarse and fine-level feature components 3, and this architecture allows us to sample the hash encodings from each grid layer with different resolutions and perform quadratic interpolation on the extracted encodings to obtain a feature vector \(_{i}\) when querying a specific timestamp \(t\) that is in the range of all timestamps. This design choice is also empirically validated in Table. 3.

After obtaining the interpolated feature vector, an MLP with two separated decoder heads is used to predict the output translation \(_{i}\) and rotation \(_{i}\) vectors respectively. The forward pass can be expressed in the following equations:

\[_{i}=_{}(\{( (t;_{l}),\ _{}^{l}\}_{l=1}^{L};_{}\ ),\] (1)

\[}_{i}=[_{i},_{i}]=l_{}(_{i };_{}),\ l_{}(_{i};_{}),\] (2)

where interp denotes interpolation, h is the hash function parameterized by \(_{l}\), \(_{}\), \(l_{}\), \(l_{}\) are the MLP networks and the decoder heads, with \(_{}\), \(_{}\), \(_{}\) representing their parameters.

**Depth-pose Prediction** Since both the depth maps and the RGB images are collected by the same drone on the same flight, they cover the same spatial-temporal footprints except for the difference in the placement of the two sensors on the aircraft. For every depth frame, we first predict the RGB camera pose using the capture timestamps of the depth sensor with the time-pose function then transform the predicted RGB camera pose to the depth sensor pose with a pre-calibrated pose transformation \(_{RGB D}\) between sensors.

**To optimize the Time-Pose Function,** we propose the following objective function:

\[=_{}_{}+_{}_{}+_{}_{},\] (3)

Figure 3: **Method Pipeline. The time-pose function is modeled using a 1-D multi-resolution hash grid with direct and speed losses. After bootstrapping the scene representation networks with pure RGB signals, the predicted depth sensor poses are used for jointly optimizing the NeRFs’ parameters \(\). At each timestamp (\(t_{i}\) from RGB sequence or \(t_{j}\) from depth sequence), only one modality of sensor signals is provided, thus only one loss term is activated (shown on the right).**

where \(_{},_{},_{}\) are translation, rotation and speed losses respectively as shown in the left panel of Fig. 3. and \(_{},_{},_{}\) are the weighting parameters. Note that \(_{}\) and \(_{}\) are automatically adjusted as explained in a later paragraph.

**Pose Representation.** There are some common choices to represent rotation for optimizing camera poses like rotation matrices  or Euler-angles . However, they are not continuous for representing rotation  due to their non-homeomorphic representation space to \((3)\). We choose to use unit quaternion as our raw rotation representation because arbitrary 4-D vectors can be easily mapped to legitimate rotations by normalizing them to the unit length .

**Optimization of Translation and Rotation.** We optimize the translation and the rotation vectors by minimizing the mean square error (MSE) between the estimated and ground-truth camera poses:

\[_{}=_{i=1}^{n}(x_{i}-_{i})^{2},\ _{}=_{i=1}^{n}(q_{i}-_{i})^{2}.\] (4)

Since \(x\) and \(q\) are in different units, the scaling factor \(_{}\) and \(_{}\) play an important role in balancing the losses. To prevent translation and rotation from negatively influencing each other in training and to tap into possible mutual facilitation, we make the weighting factors learnable by using homoscedastic uncertainty  as \(_{}=_{}(-_{})+ _{}+_{}(-_{})+ _{}\), where \(\) are learnable parameters, thus the loss terms are balanced during training course4.

**Optimization of Motion Speed.** Observing that the time-pose function is essentially a function of translational displacement and angular displacement with respect to time, we can use the average linear velocity5 to supervise the gradient of the network output, with regard to the input vectors. Since the linear velocity variation is small and the angular velocity variation is relatively larger in the scenes captured by the drone, only the average linear velocity is used to supervise the neural network and the latter is not supervised in our method:

\[_{}=(v(t_{i}),(t_{i}))= _{i=1}^{n}(v(t_{i})-}{ t}(t_{i}))^{2},v(t_{i})= .|_{t=t_{i}}-x_{i- 1}}{t_{i}-t_{i-1}}\] (5)

### Bootstrapping Large-scale Neural Radiance Fields

In this part, we introduce our proposed scene representation (right half of Fig. 3) that is bootstrapped in the second phase of the optimization process (Fig. 2-(ii)). Due to the limited capacity of MLPs, we follow Mega-NeRF and partition the scene map into a series of equal-sized blocks in terms of spatial scope, and each block learns its individual scene representation with an implicit field. In this stage, we optimize the scene representation with pure RGB data. Specifically, the radiance field is denoted as \(\{f_{}^{(i)}\}_{i=1}^{N_{x}N_{y}}\), where \(N_{x},N_{y}\) denotes the spatial grid size. Each implicit function represents a geographic region with \(_{i}^{}\) as its centroid. The \(k\)th scene model can be written as:

\[f_{}^{()}((_{}),( ))(,),\] (6)

where \(=*{arg\,min}_{j}||_{}-_{ j}^{}||_{2}\) and \(\) is the positional encoding function.

For view synthesis, we adopt volume rendering techniques to synthesize color image \(\) and depth map \(\). To be specific, we sample a set of points for each emitted camera ray in a coarse-to-fine manner  and accumulate the radiance and the distance along the corresponding ray to calculate the rendered color \(\) and depth \(\). To obtain the radiance of a spatial point \(_{}\), we use the nearest scene model for prediction. A set of per-image appearance embedding  is also optimized simultaneously in the training.

\[(,)=_{}^{}T(t)^{(k) }((t)) c^{(k)}((t),)t,\ (,)=_{}^{}T(t)^{(k) }((t)) tt,\] (7)

where \(\) and \(\) denote the position and orientation of the sampled ray, \((t)=+t\) represents the sampled point coordinates in the world space, and \(T(t)=(-_{}^{t}^{(k)}((s))s)\) is the accumulated transmittance. We optimize the scene representation model with only the photometric error as \(_{}=(I,)\). We empirically observe that this bootstrapping is critical to the challenging third stage which jointly learns \(\) and \(\) using asynchronous RGB-D data.

### Joint Optimization

While the time-pose function learns a good initialization from the RGB sequence, there are still errors to be compensated. In this section, we describe how we perform simultaneous mapping and pose optimization, which compensates for the initial error of the time-pose function.

We jointly optimize the inaccurate camera poses and the implicit maps: when fitting parameters \(^{(k)}_{}\) of the scene representation, the estimated depth camera poses \(}^{(j)}_{D}(3)\) (where \(t^{3}\) and \(q(3)\)) will be simultaneously optimized on the manifold:

\[,\{}_{D}\}=*{argmin}_{,T(3)}(\{I^{(i)}\},\{D^{(j)}\},\{}_{D}\}),\] (8)

where \(\) is the objective function we demonstrate in the next paragraph.

To train the implicit representation to obtain photo-realistic RGB rendering maps and accurate depth map estimation, we update the mapping losses as:

\[=_{}_{i}(I^{(i)},^{(i)})+ _{}()_{j}(D^{(j)},^{(j)}),\] (9)

where \(_{}\) and \(_{}()\) are weighting hyper-parameters for color and depth loss, in which the depth loss weight starts to grow from zero gradually with the training process \(\).

To compensate for the error from the time-pose function extracted poses, we jointly optimize two implicit representation networks thanks to the end-to-end differentiable nature.

## 5 Asynchronous Urban Scene (AUS) Dataset

**Dataset Collection.** Our Asynchronous Urban Scene (AUS) dataset as illustrated in Fig.4 is generated using Airsim , a simulator plug-in for Unreal Engine. With 3D city models loaded in Unreal Engine, the simulator can output photorealistic and high-resolution RGB images with synchronized depth images (resampled later) according to the a drone trajectory and a capture framerate. We choose Airsim as it strikes a good balance between rendering quality and dynamics modeling flexibility.

Figure 4: We propose a photo-realistically rendered dataset named Asynchronous Urban Scene (AUS) for evaluation. (a/b) are large-scale city scenes designed according to New York and San Francisco while (c) is (relatively) small-scale scenes provided by UrbanScene3D. Drone trajectories of different difficulty levels are visualized in (a-c). On these trajectories, we first capture an RGB-D sequence with an enough high framerate. Then we exploit two resampling strategies: fixed offset (d) and random offset (e). \(x\) equals \(30\) in (d) for every RGB-D pair. \(x\) equals \(30\) while \(y\) equals \(50\) in (e).

[MISSING_PAGE_FAIL:7]

RGB-D View SynthesisThe standard metrics for novel view synthesis and depth estimation are used for evaluation. For RGB view synthesis, metrics including PSNR, SSIM, and the VGG implementation of LPIPS  are used. For depth estimation, RMSE, RMSE log, \(_{1,2,3}\) are used. In Table 1, we quantitatively show on the RGB view synthesis task that, together with depth supervision, our method can generate more photo-realistic images than two SOTA baselines, and show on the depth estimation tasks that our method significantly improves the learned geometry of the scene representation network. We also present the RGB-D view synthesis results qualitatively in Fig.5 in which our method synthesize photo-realistic images and accurate depth maps, while baseline methods fail at predicting reliable depth maps (e.g. in the **School** scene, they mispredict the void space as a dense surface; in **NY hard**, the depth values around glasses are obviously inaccurate).

**Depth Pose EstimationWe evaluate the performance of our time-pose function in Table 2, or say specifically the accuracy of our method to localize depth sensor poses. As shown quantitatively, our method can achieve an average pose error of \(1.04m\) and \(1.07^{}\) in the first stage, where only time-pose pairs from the RGB sequence are used to optimize the network. After joint optimization in the third stage, our method cuts half the errors to \(0.53m\) and \(0.41^{}\).**

**Real-world Evaluation.In the real-world experiments, we use the DJI M300 UAV (equipped with a high-definition RGB camera and LiDAR to collect real data, where the RGB camera collects images at the frame rate of 30fps and the LiDAR collects depth information at 240Hz. The poses of the RGB images are provided by COLMAP . The fixed transformations between sensors are provided by the producer or can be calibrated manually. A qualitative comparison is provided in Fig. 1 and more results are in the supplementary.**

### Ablation Studies

**Time-Pose Function Network Structure.** We compared our proposed time-pose function implementation with several commonly used implicit representation architectures on the localization accuracy

  
**Scene** &  &  \\  & Rot. (\({}^{}\)) & Trans. (\(m\)) & Rot. (\({}^{}\)) & Trans. (\(m\)) \\ 
**NY Full** & 0.66 / 0.59 / 3.70 & 1.84 / 1.12 / 0.46 & 0.13 / 0.09 / 1.47 & 0.34 / 0.56 / 0.20 \\
**SF Full** & 0.17 / 0.67 / 0.65 & 1.34 / 1.45 / 0.94 & 0.05 / 0.41 / 0.02 & 0.32 / 1.09 / 0.66 \\
**Small** & 1.51 / 0.68 / 0.70 / 1.05 & 0.95 / 1.35 / 0.89 / 0.38 & 0.49 / 0.36 / 0.68 / 0.38 & 0.57 / 0.85 / 0.56 / 0.12 \\ 
**Mean** & **1.04** & **1.07** & **0.41 (-0.63)** & **0.53 (-0.54)** \\   

Table 2: We show that the time-pose function learns an accurate implicit trajectory from the RGB sequence that can estimate accurate poses for depth frames. By further tuning the time-pose function jointly with the scene representation network, the accuracy of the predicted depth sensor poses can be improved. The results of Simple / Hard / Manual on **NY** and **SF** are shown in the first two lines. The results of the 4 small scenes of (Bridge / Town / School / Castle) are shown at the bottom.

Figure 5: **Qualitative Results. Our method can render photo-realistic novel views and the best depth estimation results. Please zoom in to see details using an electronic verion.**

(Tab. 3). In order to highlight the gap between different methods, we downsample the dataset to increase the difficulty. (a) **Pure MLP architecture** : processing the positional-encoded  input timestamps with an MLP; (b) **1-D Feature Grid**: storing a feature vector for each second in the timestamp span and performing linear feature interpolation in the query's neighborhood. (c) **Ours**: our proposed 1-D multi-resolution hash grid with different resolution layers. The results (Table 3) show that our proposed multi-resolution architecture outperforms other network architectures in accuracy. The network structures are further detailed in the supplementary materials.

**Speed Loss.** We compared our method's localization accuracy with and without the optimization of motion speed (see the comparison of 'Ours' and 'Ours w/o speed' in Tab. 3). The results show that minimizing the gradient error (i.e., speed loss) help a lot in improving the accuracy of the translation (from \(20m\) to \(6.3m\)).

**Different Sampling Offsets.** To show the robustness of our proposed time-pose function, we compare the localization accuracy of implicit trajectory representations under different sampling offsets, whose definition is described in Fig. 4-d/e. The quantitative results (Table. 4) indicate that the network's output exhibits a controllable margin of error as the data offset increases.

**Joint Optimization for Pose Error Compensation.** To demonstrate the importance of rectifying erroneous poses of depth images in asynchronous RGB-D sequences using the time-pose function, we train a Mega-NeRF with depth supervision but disabled the joint optimization stage. From the evaluation results (Table 5), we observe its substantial impact on the rendering quality (PSNR for RGB and RMSE for depth). Due to limited space, qualitative results are in the supplementary.

## 7 Conclusion

In this paper, we present a method to learn depth-supervised neural radiance fields from asynchronous RGB-D sequences. We leverage an important prior that the sensors cover the same spatial-temporal footprints and propose to utilize this prior with an implicit time-pose function. With a 3-staged optimization pipeline, our method calibrates the RGB-D poses and trains a large-scale implicit scene representation. Our experiments on a newly proposed large-scale dataset show that our method can effectively register depth camera poses and learns the 3D scene representation for photo-realistic novel view synthesis and accurate depth estimations. **Broader impact and limitations:** Large-scale scene modelling can be used for potential military use, which the method is not intended for.

   & ^{}\))} &  \\   & Mean & Median & Mean & Median \\ 
10\% & **0.66** & **0.26** & **3.42** & **1.88** \\
20\% & 0.94 & 0.55 & 4.96 & 3.90 \\
30\% & 1.24 & 0.79 & 6.41 & 5.78 \\
40\% & 1.41 & 0.75 & 7.26 & 6.61 \\
50\% & 1.50 & 0.84 & 7.53 & 6.51 \\ Random & 1.12 & 0.52 & 5.17 & 4.05 \\  

Table 4: Results on the ablation of different sampling offset strategies.

   &  &  &  \\   & PSNR \(\) & RMSE \(\) & PSNR \(\) & RMSE \(\) & PSNR \(\) & RMSE \(\) \\ 
**NY** Mean & **24.24** & **5.93** & 24.03 & 42.15 & 19.70 & 15.94 \\
**SF** Mean & **22.70** & **7.26** & 20.00 & 32.17 & 19.07 & 11.39 \\
**Bridge** & **29.06** & **26.55** & 27.98 & 120.41 & 22.35 & 96.16 \\
**Town** & **25.32** & **15.61** & 24.69 & 129.50 & 20.14 & 81.99 \\
**School** & **26.51** & **21.19** & 25.57 & 63.10 & 21.91 & 42.74 \\
**Castle** & **28.22** & **16.66** & 28.06 & 54.99 & 23.23 & 38.90 \\
**Mean** & **26.01** & **15.53** & 25.01 & 73.72 & 21.07 & 47.85 \\  

Table 5: Ablation on the joint optimization stage. We show that jointly optimizing the time-pose function and the scene representation significantly helps reduce geometric error.