# Why are Visually-Grounded Language Models

Bad at Image Classification?

 Yuhui Zhang\({}^{1}\)  Alyssa Unell\({}^{1}\)  Xiaohan Wang\({}^{1}\)  Dhruba Ghosh\({}^{2}\)  Yuchang Su\({}^{3}\)  Ludwig Schmidt\({}^{1,2}\)  Serena Yeung-Levy\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)University of Washington \({}^{3}\)Tsinghua University

\({}^{}\)yuhuiz,ludwigsc,syyeung}@stanford.edu

###### Abstract

Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.1

## 1 Introduction

The ability to recognize objects within images is a fundamental capability of machine vision. Over the past 15 years, the field has experienced significant breakthroughs due to deep learning and large-scale datasets . For instance, on the renowned ImageNet dataset, designed to classify images into 1,000 categories, the error rate has dramatically decreased from 47.1% in 2009 to 9.1% in 2024, representing a 5-fold reduction . Consequently, these classification models have superseded most human labelers.

Nowadays, the community has focused on more sophisticated and nuanced capabilities in the quest for visual intelligence. Visually-grounded language models (VLMs), which integrate visual signals from vision encoders with large language models, have recently emerged as a promising paradigm . VLMs like GPT-4V , Gemini-1.5 , or Claude-3  have demonstrated advanced visual understanding abilities, such as answering math questions from table images or generating HTML code from design sketches.

In this work, we revisit the fundamental task of image classification using VLMs. Surprisingly, we find that various public and proprietary VLMs struggle with image classification in both open-world settings, where the class list is unknown, and closed-world settings, where class names are provided in the context (SS2). Despite having many more parameters, there is a significant gap between theperformance of VLMs and their commonly used vision encoder CLIP . Our evaluation protocol involves feeding each image and a list of class names (in the closed-world setting) to the VLM as context and asking what is in the image; success is defined by whether the generated output contains the ground-truth class name.

To understand why VLMs underperform in classification settings, we investigate several hypotheses regarding VLMs' inference (such as prompt variations, label set size, inference strategy; SS3.1), training (such as information lost, training objective; SS3.2), and data (such as data-performance correlation; SS3.3). Our extensive analyses suggest that the primary reason for the observed gap is data. We find that the information necessary for classification is encoded in the VLM's latent space but can only be decoded with proper training data. Specifically, there is a strong correlation between class presence during VLM training and performance in those classes. Furthermore, training VLMs on classification datasets achieves the same performance level as state-of-the-art classification models.

Motivated by our analysis, we propose a simple method to enhance VLMs' general capabilities by integrating traditional classification-focused datasets into VLM training (SS4). We believe that classification is the foundation for more complex, advanced visual capabilities; for example, recognizing an object is a prerequisite for answering complex questions about it. To verify this, we created ImageWikiQA, which contains complex real-world questions about ImageNet objects. On ImageWikiQA, we find that VLMs fine-tuned on the ImageNet classification dataset achieve substantially higher accuracy in recognizing these objects and provide more accurate answers to these non-classification questions, outperforming pre-trained VLMs by 11.8%. This suggests that classical classification data can be beneficially reused in the VLM training process to enhance VLM performance.

In summary, our contributions are threefold:

* **Evaluating VLM classification weaknesses:** Our evaluations using ten VLMs across four benchmarks show that VLMs significantly lag behind CLIP in classification, uncovering a gap unaddressed by previous research.
* **Analyzing causes of poor classification performance:** Testing various hypotheses, we find that the lack of alignment data, rather than information lost or training objective, is the primary reason for VLMs' underperformance in classification.
* **Enhancing VLMs with classification data:** By adding classification data, we improve VLMs' accuracy in object recognition and overall performance, supporting that classification capability is foundational for complex object-related reasoning.

Figure 1: **Overview. (_Left_) Different visually-grounded language models (VLMs) underperform CLIP in classification by a large margin, though they often use CLIP as a vision encoder. (_Middle_) We investigate several hypotheses about why VLMs are bad classifiers and find that the main reason is data. Critical information for image classification is encoded in the VLM’s latent space but can only be decoded with enough data during VLM training. (_Right_) Based on our analysis, we improve a VLM by integrating classification data into its training, and find that the improved classification capabilities serve as foundations for more advanced capabilities such as visual question answering.**

## 2 VLMs are Bad at Image Classification

We begin by evaluating state-of-the-art visually-grounded language models (VLMs) using standard image classification benchmarks. Our findings reveal that these VLMs significantly underperform compared to state-of-the-art classification models, such as CLIP.

### Models

VLMs.We selected ten widely-used state-of-the-art VLMs, covering different architectures, training methods, and data. These VLMs include three proprietary ones, GPT-4-Turbo (shortened as GPT4, same below) , Gemini-Pro-Vision (GeminiPro) , and Claude-3-Opus (Claude3) , and seven public ones, LLAVA1.5-Vicuna7B/13B (LLaVA1.5-7/13B) , LLAVANeXT-Mistral7B/Vicuna7B (LLaVANeXT-M7B/V7B) , BLIP2-OPT2.7B (BLIP2-2.7B) , and InstructBLIP-Vicuna7B/13B (IBILIP-7/13B) ). Details of these models are provided in Appendix SSA.1.

CLIPs.For comparison, we used two state-of-the-art image classifiers, CLIP-ViT-L/14-336px (shortened as CLIP-L, same below)  and EVA-ViT-G/14 (EVA-G) . Notably, CLIP-L and EVA-G are utilized by the LLAVA series  and the BLIP series as vision encoders , respectively. Therefore, the VLMs should theoretically have the same classification capacity as these vision models. Details are listed in Appendix SSA.1.

### Data

We evaluated the aforementioned models on four widely-used image classification benchmarks: ImageNet (shortened as #, same below) , Flowers102 (#) , StanfordCars (#) , and Caltech101 (#) , which contain 50,000, 6,149, 8,041, and 4,331 test images from 1,000, 102, 196, and 101 classes, respectively. ImageNet and Caltech cover more coarse-grained objects, while Flowers and Cars cover more fine-grained objects. Further details are provided in Appendix SSA.2.

### Evaluation Protocol

VLMs.We performed image classification in two settings: an open-world setting where the label set is not provided and a closed-world setting where classes are concatenated in the prompt. We

  
**Model** &  &  \\  & **\#1** & **\#2** & **\#3** & **\#4** & **\#4** & **\#4** & **\#4** \\   \\ BLIP2-2.7B  & 25.3 & **27.0** & 0.0 & 46.9 & N/A & 14.2 & 2.7 & 22.3 \\ IBLIP-7B  & 14.6 & 1.9 & 0.0 & 36.5 & N/A & **26.8** & N/A & 58.4 \\ IBLIP-13B  & 14.7 & 2.4 & 0.0 & 36.4 & N/A & 20.0 & N/A & 59.5 \\ LLAVA1.5-7B  & 22.8 & 5.9 & 0.0 & 47.1 & N/A & 10.2 & 0.0 & 62.1 \\ LLAVANeXT-V7B  & 29.4 & 12.8 & 0.0 & 52.5 & N/A & 8.5 & 0.0 & 66.6 \\ LLAVA1.5-13B  & 24.3 & 5.3 & 0.0 & 49.9 & N/A & 7.2 & 0.1 & 70.9 \\ LLAVANeXT-M7B  & **32.3** & 17.7 & 0.0 & **54.2** & N/A & 16.1 & **3.6** & **77.3** \\   \\ Claude3  & **53.6** & **51.2** & **0.3** & **68.6** & 51.1 & 58.3 & 45.1 & 90.9 \\ GeminiPro  & 39.2 & 10.5 & 0.1 & 60.1 & 56.0 & 62.0 & **66.6** & 91.6 \\ GPT4  & 48.5 & 51.0 & 0.1 & 61.0 & **60.6** & **79.9** & 58.2 & **94.2** \\   \\ CLIP-L  & N/A & N/A & N/A & N/A & 74.8 & 76.0 & 77.5 & 95.8 \\ EVA-G  & N/A & N/A & N/A & N/A & **79.2** & **81.0** & **90.2** & **97.9** \\   

Table 1: **Evaluations of VLMs and CLIPs on standard image classification benchmarks.** VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. #=ImageNet , #=Flowers102 , #=StanfordCars , #=Caltech101 .

feed the image and the prompt to the VLM and let the VLM complete the rest of the tokens. One closed-world example is: "<image> What type of object is in this photo? Choose one from <class name A>, <class name B>,..." We define success on a single example as whether the ground-truth label is included in the VLM generation. We report the success rate of all test examples.

CLIPs.CLIP can only be used in a closed-world setting where the label set is known. Following Radford et al. , we used the prompt "a photo of a <class>" to generate the text feature. For each image, we selected the class with the highest cosine similarity to the image feature. We did not include prompt ensembling to fairly compare with VLM. We report the accuracy of all examples.

### Results

Table 1 reports the performance of different VLMs and CLIP models on these classification datasets.

We find that **VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models**. For instance, on the ImageNet dataset, the best proprietary VLM, GPT4, only achieves an accuracy of 60.6% in the closed-world setting, whereas the best CLIP, EVA-G, attains an accuracy of 79.2%. In the open-world setting, the best public VLM, LLaVANeXT-M7B, achieves just 32.3% accuracy. The performance disparity is even more pronounced in fine-grained classification datasets like Flowers102 and StanfordCars. Notably, all LLaVA models use CLIP-L as the vision encoder, and although the total parameter count of these models is at least 20 times greater than that of the vision encoder, they significantly underperform compared to it.

Moreover, we find that **closed-world setting often outperforms open-world setting**. This is expected as the provided label set narrows the prediction space. However, since closed-world settings require including all class names in the context, they can result in an extremely long context that leads to high costs or even exceeds the VLM's context limit. For example, most public VLMs only support 4K context length, which cannot feed 1K ImageNet classes. We also find that **larger and better LMs slightly improve VLM performance**. For instance, LLaVA1.5-13B outperforms LLaVA1.5-7B, and LLaVANeXT-M7B outperforms LLaVANeXT-V7B.

## 3 Why are VLMs Bad Image Classifiers?

Given that visually-grounded language models (VLMs) underperform CLIPs at classification by a large margin, as reported in SS2, we seek to understand the reasons behind that. We investigate several hypotheses concerning major differences between VLMs and CLIPs, which can be generally categorized into inference (SS3.1), training (SS3.2), and data (SS3.3):

1. We start with inference-related questions. For example, does prompt variation, such as chain of thought, affect final performance? Does reducing the label set size in context narrow the gap between VLMs and CLIPs? Does performing probabilistic inference to force the generation into the label set help? We find none of these factors can fully close the gap between VLMs and CLIPs.
2. Therefore, we switch to training-related questions. For example, is the visual information from the vision encoder still preserved in the VLM's latent space? Is the text generation objective as effective as cross-entropy loss for learning classification? Surprisingly, the results show that the information is preserved, and the text generation objective is adequate for learning classification.
3. Finally, we investigate data-related questions. For example, does the VLM training data include enough classification data and cover enough classes? We find a strong correlation between class exposure in training and model performance. Moreover, VLMs can achieve the same level of performance as CLIPs when trained with enough data. These results suggest that data is the primary cause and effective solution for the poor classification performance of VLMs.

### Inference

In this section, we investigate three questions related to VLM's inference, including prompt variation, label set size, and inference strategy.

Prompt variation.It is well known that language models (LMs) are sensitive to prompts [7; 16; 53]. To understand the effect of prompts on classification performance, we tested three semantically similar but differently worded prompts, compared feeding the label in dataset order or random order within the context, and leveraged the zero-shot chain-of-thought (CoT) prompting technique by adding "let's think step by step" at the end of the prompt .

We find that **prompt variation has a limited impact on the performance.** From Table 2, we can see that changing the wording of prompts results in a performance variation within 3% for both LLaVA1.5-7B and BLIP2-2.7B on the ImageNet dataset. Different label orderings impact LLaVA1.5-7B less than BLIP2-2.7B. Chain-of-thought prompting consistently improves performance for instruction-tuned model LLaVA1.5-7B but not for BLIP2-2.7B.

Label set size.The label set size can be very large in practice (e.g., 1000 for ImageNet, 196 for StanfordCars), which results in an extremely long context when we concatenate all the class names. Since LMs often struggle with long contexts , we explore reducing the number of classes. For each image, we randomly select \(K=2,5,20,100\) classes, always including the ground-truth label, and re-evaluate the VLM and CLIP performance.

We find that **the performance gap between VLMs and CLIPs narrows with reduced label size, but the gap always exists**. As shown in Figure 2, when evaluating LLaVA1.5-7B and CLIP-L on ImageNet, the gap decreases from 46% with 100 classes to 6% with 2 classes. However, the relative gap becomes larger, evidenced by a 23.9x error rate with 2 classes compared to a 7.3x error rate with 100 classes. The performance gap between VLMs and CLIPs always exists in all the settings.

Inference algorithm.The default inference algorithm for classification with VLMs is directly generating the class name given a prompt. As the generation is open-ended, even when provided with a list of candidate choices, the generation may not match one of the pre-defined classes. To mitigate this problem, we employ probabilistic inference techniques for VLMs. Specifically, for each class name,

  
**Method** &  &  \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   \\ Base Prompt & **22.8** & 5.9 & 0.0 & 47:1 & 25.3 & 27.0 & 0.0 & 46.9 \\ w/ Prompt Alternative 1 & 19.7 & 3.5 & 0.0 & 45.4 & **27.6** & **38.8** & 0.2 & 47.8 \\ w/ Prompt Alternative 2 & 21.6 & 6.6 & 0.0 & 48:1 & 24.3 & 30.6 & 0.1 & 47.9 \\ w/ Label (Fixed Order) & N/A & 6.1 & 0.1 & **70.5** & N/A & 28.0 & 2.1 & **53.8** \\ w/ Label (Random Order) & N/A & 10.2 & 0.0 & 62:1 & N/A & 14.2 & **2.7** & 22.3 \\ w/ Label (Random) + CoT & N/A & **18.1** & **0.4** & 64.5 & N/A & 8.5 & 0.0 & 16.1 \\   \\ Direct Generation & 22.8 & 5.9 & 0.0 & 47.1 & 25.3 & 27.0 & 0.0 & 46.9 \\ Prob Inference (Sum Tokens) & 34.8 & 14.5 & 26.7 & 77.8 & 21.0 & 34.8 & 48.8 & 36.8 \\ Prob Inference (Avg Tokens) & 35.3 & 16.5 & 18.2 & 65.6 & 5.1 & 19.9 & 1.2 & 12.3 \\ Prob Inference (Avg) w/ CFG & **47.6** & **26.8** & **48.8** & **85.6** & **38.7** & **54.1** & **69.2** & **70.3** \\   

Table 2: **Analysis of VLMs from the inference perspective. (_Top_)** We explore prompt variation such as wording, label order, chain-of-thought and find it has limited impact on the performance. _(Bottom)_ We leverage the probabilistic inference strategy, which improves the performance but still fails to close the gap between VLMs and CLIPs. Results are from the official validation set.

Figure 2: **Analysis of the label set size.** For each image, we randomly sample 100, 20, 5, 2 candidate classes from all the classes. The performance gap between VLMs and CLIPs becomes smaller when the number of classes is reduced. X-axis: number of classes; Y-axis: accuracy (%).

we compute \((|,)\) and select the class name with the highest probability as the prediction. Since class names can consist of multiple tokens (e.g., "guinea pig" consists of two tokens), we either average the probabilities of all tokens or sum up all the tokens . We also explore classifier-free guidance (CFG) techniques by ranking \(t*(|,)+(1-t)*( |)\) with varying guidance coefficients \(t\).

We find that **the probabilistic inference method improves the performance, but the gap persists.** As shown in Table 2, LLAVA1.5-7B achieves 35.3% accuracy on ImageNet using probabilistic inference compared to 22.7% using the direct generation method. Adding classifier-free guidance further improves the performance, where LLAVA1.5-7B performance boosts to 47.6% on ImageNet. However, it still leaves around a 30% performance gap between VLMs and CLIPs, as its vision encoder CLIP-L achieves 74.8% on ImageNet. Moreover, the probabilistic inference approach is computationally expensive in practice because we need to compute the probability of each class.

### Training

Since inference-based modifications fail to close the performance gap between VLMs and CLIPs, here we investigate two questions regarding to the training of VLMs. We study whether the visual information is lost in the VLM and whether the text generation objective is suitable for learning the classification task.

Visual information lost in the VLM.VLMs process images using an image encoder, such as CLIP, which has strong classification capabilities. We hypothesize that, during the propagation of image features output from the vision encoder in language model layers, the necessary information for classification is lost. To test this hypothesis, we conduct feature probing experiments. Specifically, the features from the VLM's last layer have a shape corresponding to the length of the inputs (image tokens plus text tokens using the open-world prompt). We take the average of these token features or the last token feature and train a simple linear classifier on top of these frozen feature representations. The linear classifier is trained on the training set and evaluated on the validation set. Training details are provided in Appendix B.4. A higher accuracy indicates that less information is lost.

Surprisingly, we find that **the information necessary for classification is largely preserved in the VLM's latent space; however, it cannot be effectively decoded**. In Table 3, we show that the probing accuracy of LLAVA1.5-7B on ImageNet is 77.1%, which is close to the 85.2% probing accuracy of CLIP-L, the vision encoder used by LLAVA1.5-7B. The same conclusion holds for BLIP2-2.7B and its vision encoder EVA ViT-G/14, demonstrating that most information is preserved during the VLM computational process. However, this information cannot be effectively decoded, as SS2 shows that the best accuracy LLAVA1.5-7B can achieve on ImageNet is 47.6%.

Training objective.Since information is encoded in VLMs, we wonder whether we can train them to decode the information. VLMs can only be trained to perform classification by auto-regressively generating text-form labels. We hypothesize that this generative objective may be more difficult and less effective in learning classification compared to the traditional cross-entropy loss. To understand the effectiveness of this training objective, we explore fine-tuning VLMs on classification datasets. We

  
**Model** & **Feature** & \(\) & \(\) & \(\) & \(\) \\   \\ LLAVA1.5-7B & Last Tok & 76.9 & 94.5 & 81.0 & 96.7 \\ LLAVA1.5-7B & Avg Tok & **77.1** & **96.2** & **82.8** & **97.3** \\ BLIP2-2.7B & Last Tok & 80.3 & 98.8 & 91.0 & **98.0** \\ BLIP2-2.7B & Avg Tok & **81.4** & **98.9** & **92.6** & **98.0** \\   

Table 3: **Analysis of VLMs from the training perspective. _(Left)_ We conduct feature probing experiments on the VLM’s last layer and find that the information required for classification is mostly preserved in the VLM’s latent space. _(Right)_ We fine-tune VLMs on the classification datasets using the text generation objective and find that the text generation training objective is as effective as the traditional cross-entropy for learning classification, which eliminates the VLM-CLIP performance gap, with VLMs now being the state-of-the-art classifier. Results are from the official validation set.**convert classification datasets into the text generation format using the template (e.g., "<image> What type of object is in this photo? <class name>"). We fine-tune two model architectures (LLaVA1.5-7B and BLIP2-2.7B) across different datasets and compare their accuracy to that of fine-tuned CLIP models. Additionally, we investigate fine-tuning different parts of the models, such as only fine-tuning the projector between vision encoder and language models or fine-tuning the projector along with the language models using LoRA . Training details are provided in Appendix B.5.

We find that **the text generation training objective is as effective as the traditional cross-entropy for learning classification tasks, which eliminates the performance gap between VLMs and CLIPs, with VLMs now being the state-of-the-art classifier**. From Table 3, we find that LLaVA1.5-7B achieves 85.7% accuracy on ImageNet, same as 85.2% accuracy when fine-tuning its vision encoder CLIP-L. The same findings apply to all the VLMs on all the datasets.

Moreover, we find that **fine-tuning only the projector is sufficient and has better numerical stability**. From Table 3, we can see the same level of accuracy achieved by fine-tuning the projector and fine-tuning projector and LLM using LoRA on the three datasets. We also find that fine-tuning LLM with LoRA often leads to numerical instabilities, such as spikes in loss. While the loss sometimes returns to normal, other times it does not. For example, despite trying various hyperparameters for ImageNet, instability persisted (see Appendix SSB.5). In contrast, fine-tuning only the projector always results in a steadily decreasing loss. Notably, the projector is often much smaller (e.g., a 2-layer MLP in LLaVA) compared to LLMs, suggesting the potential for parameter-efficient prefix tuning for VLMs .

### Data

As observed, the visual information is preserved in the VLM's latent space, and the VLM's training objective is sufficient for learning classification tasks. We hypothesize that the poor classification performance of VLMs is due to their training data. For example, there might be insufficient classification data or a lack of diverse classes. To investigate this, we analyze the LLaVA1.5-7B training data , the only fully publicly available VLM training dataset. We examined the relationship between the frequency of class occurrence and the VLM's classification performance on those classes2.

Our findings indicate that **data determines VLM classification performance**. As shown in Figure 3, there is a strong correlation between the presence of class labels in the VLM training data and the VLM's classification accuracy for those classes. The LLaVA1.5-7B model achieves 82.7% zero-shot accuracy on ImageNet classes with more than 10,000 occurrences in its training data, but only 3.0% accuracy on classes with fewer than 10 occurrences. The Spearman correlation between class frequency and class performance is very high (0.76 on ImageNet; see Appendix SSB.6). In contrast, there is no correlation between its vision encoder CLIP-L's performance on those classes with the same VLM training data, and after fine-tuning LLaVA1.5-7B on the ImageNet classification data, the strong correlation disappears. Combining all of our results, we conclude that data plays a critical role in determining the VLM's classification performance.

Given that data availability is critical for VLM classification performance, we now examine whether the data type specifically impacts results. For instance, to train a VLM to classify a class like "guinea pig," should it rely on classification-focused data (e.g., "<image> What type of object is in this photo?

Figure 3: **Analysis of VLMs from the data perspective. We study the relation between the ImageNet class frequency in the VLM training data and the VLM’s classification performance on those classes. A strong correlation is observed, indicating that data determines VLM classification performance.**

Guinea pig") or can it use broader types, such as caption-focused data (e.g., "<image> Generate a caption for the image. A guinea pig playing in the grass") or VQA-focused data (e.g., "<image> What is the native region of this guinea pig? South America")?

In previous experiments (see SS3.2), we fine-tuned the VLM with a classification-focused template. Here, we fine-tune the VLM on caption-focused data generated by prompting GPT4 to produce captions incorporating each class's ground-truth name. Specifically, we used the prompt: "Generate a short caption for the image. The caption must include the ground-truth label of the image, which is <class name>." This approach produces diverse captions per class, such as "A guinea pig playing in the grass" or "A close-up view of a guinea pig in an indoor environment." We then fine-tune the VLM on this caption-focused data using identical experimental settings and evaluate it against the model trained with classification-focused data. Success is determined by whether the generated caption for a validation image includes the ground-truth class name.

We find that **data presence is the primary factor for VLM classification performance, with data type playing a minimal role**. From Table 4, we observe that models fine-tuned on both classification- and caption-focused data perform similarly on Flowers102, StanfordCars, and Caltech101, achieving accuracy gains of at least 48.6% over the non-fine-tuned model. This suggests that enabling a VLM to classify a specific class does not require classification-specific data; any data containing the class name suffices.

**These findings emphasize a critical data-centric view of VLM training**. A common question for VLM training is whether alignment between vision encoder outputs and text generation necessitates multi-modal data with specific class labels, despite both vision and language components seeing these classes in uni-modal pre-training. Previous studies have suggested that this alignment stage may be data-efficient or even unnecessary [32; 18], likely due to their evaluation settings not requiring vision-centric fine-grained recognition. Our work, however, shows that 1) multi-modal data is essential for alignment and 2) increasing data quantity linearly boosts performance. Concurrent work from DeepMind supports our findings, showing that most VLM tasks benefit from pre-training on larger datasets .

## 4 Improving VLM with Classification Data

Based on the analysis presented in SS3, in this section, we discuss the enhancement of visually-grounded language models (VLMs) by integrating classification-focused data into their training. We demonstrate that this data intervention not only boosts the VLM's classification performance but also enhances its general capabilities.

### Motivation

Classification is fundamental to enabling more advanced capabilities of VLMs, such as visual question answering and reasoning. For example, suppose a virtual assistant is helping visually impaired individuals prepare mushrooms as food. In that case, the model must correctly identify the mushroom species to answer questions like "Is this mushroom poisonous?" The poor classification performance of VLMs lays a weak foundation for their advanced capabilities.

From SS3, we identify the primary cause of poor classification performance is the lack of data. Therefore, we propose a straightforward solution: integrating classification-focused data into the VLM training process. We hope that incorporating classification data not only enhances classification accuracy but also improves general capabilities.

  
**Model** & **Fine-tuning Data Type** &  &  &  \\  LLaVA1.5-7B Zero-shot & - & 5.9 & 0.0 & 47.1 \\ CLIP-L Fine-tuned & Classification Data & 98.6 & 91.5 & 97.6 \\ LLaVA1.5-7B Fine-tuned & Classification Data & 97.6 & 90.4 & 97.5 \\ LLaVA1.5-7B Fine-tuned & Captioning Data & 92.0 & 85.4 & 95.7 \\   

Table 4: **Analysis of data types.** We fine-tune the VLM on the caption-focused data generated by GPT4 using the same experimental settings as Table 3 and find that **data is the main determining factor for VLM performance, and the data type does not matter much**.

### ImageWikiQA

To verify our hypothesis, we need a dataset that evaluates both the classification and advanced capabilities of VLMs. However, current visual question answering benchmarks, such as VQAv2 , MM-Vet , and MMMU , primarily focus on advanced capabilities, such as reasoning and knowledge grounding, rather than classification. The objects in these datasets are relatively simple, and questions can often be answered by identifying only the general category of an image, such as "mushroom" or "flower", rather than specific types of mushrooms or flowers. In contrast, classification datasets have no questions related to more advanced capabilities.

To bridge the gap between classification and advanced capabilities, we introduce ImageWikiQA, an object-centric, knowledge-intensive question answering dataset that combines both worlds. Each question in ImageWikiQA is a multiple-choice question with four options and one correct answer (e.g., "<image showing a guinea pig> What is the native region of this object? A. South America; B. Africa; C. Asia; D. Australia"). Although each question does not directly ask for the category of the object within the image, the question can only be accurately answered if the class of the object is correctly identified. Example questions from ImageWikiQA can be found in Figure 1 (right) and Appendix SSC.1.

ImageWikiQA is created by generating questions based on Wikipedia pages of ImageNet classes using GPT4. Specifically, for each class in ImageNet, we parsed the Wikipedia content of the class following Bujwid et al. , and then provided this content along with a prompt to GPT4 to generate five questions per class. We instructed GPT4 to replace the class name with "this object" in the questions so that the ground-truth class name is not provided. We retained all questions that GPT4 could answer correctly with the ground-truth class name and could not guess the answer without the class name. To ensure the question quality generated by GPT4, four human annotators attempted to answer the questions with the ground-truth class name and Wikipedia page and achieved an accuracy of 96.5%. Afterward, we randomly sampled at most 3 ImageNet images for each question, rebalanced the choice distribution, and composed the final ImageWikiQA dataset. In total, there are 2000 multiple-choice questions, each with an image, question, four candidate choices, and a reference to Wikipedia sentences, with a random guess accuracy of 25.0% and max frequency accuracy of 25.9%.

### Results

Table 5 presents the performance of various VLMs on the ImageWikiQA dataset.

We find that **current state-of-the-art VLMs perform poorly on answering these questions given images**. For example, GPT4 achieves 100% accuracy when the ground-truth class name is provided, but only achieves 61.2% accuracy with images. Similarly, Claude3 and GeminiPro only achieve 54.3% and 49.1% accuracy, respectively. These results indicate that the poor classification performance of VLMs is a fundamental limitation for more advanced capabilities.

  
**Model** & **Acc** \\   \\ Random & 25.0 \\ Max Freq & 25.9 \\ GPT4 w/ GT Class & 100.0 \\ GPT4 w/o Image & 0.0 \\ Human w/ GT Class+Wiki & 96.5 \\ LLaVA1.5-7B w/ GT Class & 55.9 \\   \\ GeminiPro & 49.1 \\ Claude3 & 54.3 \\ GPT4 & **61.2** \\    
  
**Model** & **Acc** \\   \\ BLIP2-2.7B  & 21.7 \\ IBLIP-7B  & 36.3 \\ LLaVANeXT-VTB  & 37.0 \\ IBLIP-13B  & 37.5 \\ LLaVA1.5-13B  & 37.8 \\ LLaVA1.5-7B  & 38.0 \\ LLaVANeXT-MYB  & 41.9 \\   \\ LLaVA1.5-7B Finetuned on ImageNet & 30.6 \\ LLaVA1.5-7B Finetuned on ImageNet+LLaVA & **49.8** \\   

Table 5: **Evaluations of VLMs on ImageWikiQA. ImageWikiQA is a multiple-choice question-answering dataset collected by feeding the Wikipedia pages of ImageNet classes to GPT-4. We find that current VLMs perform poorly in answering these questions, suggesting that their poor classification performance is a fundamental limitation for more advanced capabilities. Integrating classification data into VLM training enhances both their classification and overall capabilities.**Furthermore, we find that **integrating classification data into VLM training improves its classification and general capabilities**. We fine-tuned LLaVA1.5-7B on the ImageNet 1.28M classification data and original 665K LLaVA instruction-tuning data (training detail in SSC.2), which is able to achieve 84.4% accuracy on ImageNet classification compared to 22.8% for non-fine-tuned models. This improvement in classification translates to an 11.8% accuracy boost on ImageWikiQA, demonstrating classification is indeed a foundation for VLM's advanced capabilities. However, it is worth noting that fine-tuning solely on the classification task harms general capabilities. When fine-tuning LLaVA1.5-7B on ImageNet only, their accuracy on ImageNetWikiQA drops to 30.6%. Therefore, fine-tuning should be performed on a combined dataset. Developing fine-tuning methods that prevent such catastrophic forgetting is a promising future research direction.

## 5 Related Work

Visually-grounded language models.Visually-grounded language models (VLMs) refer to a large family of models that integrate visual signals into language models by modeling \(p(y_{t}|y_{<t},x)\), where \(y_{i}\) is a text token and \(x\) is a visual input such as an image or video. Recently, many powerful VLMs have been developed, including proprietary ones like GPT-4V , Gemini , Claude , Flamingo  and public ones like LLaVA [32; 31], BLIP [27; 10], OpenFlamingo , Otter , Fuyu , QwenVL . The typical architecture of VLMs comprises three components: a visual encoder (often CLIP ), a language model, and a projector that bridges the visual outputs and language model inputs. The projector can be as simple as a linear layer or MLP (e.g., LLaVA ) or a complex architecture such as Transformer with cross-modal attention (e.g., BLIP ). VLMs are usually trained on image-text captioning data [40; 39] and carefully designed instruction-tuning data , with both the vision encoder and language model initialized from pre-trained versions. In this work, we evaluate widely used VLMs in classification settings and analyze two prominent architectures, LLaVA  and BLIP .

Analysis of visually-grounded language models.While VLMs have demonstrated impressive performance, many questions remain unanswered. Recent works have explored their limitations from various perspectives, including architectures , training recipes [18; 24], data [14; 46], vision encoders , language models [2; 44], input resolution , and proposed solutions for these limitations. For instance, Tong et al.  discovered that CLIP vision encoders, employed by most VLMs, often fail to distinguish between certain image pairs despite their apparent visual differences. They suggested utilizing alternative vision encoders, such as DINO, to address this problem. Our work aligns with these studies in understanding VLM limitations and proposing solutions. We find that current VLMs struggle with image classification, and we thoroughly investigate the reasons behind this, proposing a simple solution to integrate classification-focused data into VLM training.

Image classification.Image classification is one of the most fundamental capabilities of machine intelligence. Starting in the 1990s, researchers collected datasets like MNIST  for digit classification and CIFAR  for object classification, using various machine learning algorithms such as SVMs and MLPs to tackle the problem. A significant milestone was the ImageNet , which elevated the scale and quality of datasets to a new level, driving advancements in deep learning. With the rise of deep supervised and self-supervised learning [23; 9], performance on these datasets soon began to saturate. As a result, classification models have superseded most human labelers, leading researchers to focus on more advanced visual intelligence tasks like visual reasoning. In this work, we revisit this simple yet fundamental task, discovering that current VLMs struggle with it. We demonstrate that classification remains essential, as it forms the foundation for more advanced capabilities, and enhancing VLMs' classification capabilities improves their overall performance.

## 6 Conclusion

In this work, we explored the use of visually-grounded language models (VLMs) as image classifiers. We found that their performance is limited across various datasets. We then analyzed the reasons behind these limitations and, based on our findings, trained a VLM with enhanced general capabilities.