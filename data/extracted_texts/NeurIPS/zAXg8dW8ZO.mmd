# One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models

Ba-Hien Tran

Department of Data Science

EURECOM, France

ba-hien.tran@eurecom.fr

Giulio Franzese

Department of Data Science

EURECOM, France

giulio.franzese@eurecom.fr

Pietro Michiardi

Department of Data Science

EURECOM, France

pietro.michiardi@eurecom.fr

Maurizio Filippone

Department of Data Science

EURECOM, France

maurizio.filippone@eurecom.fr

###### Abstract

Generative Models (GMs) have attracted considerable attention due to their tremendous success in various domains, such as computer vision where they are capable to generate impressive realistic-looking images. Likelihood-based GMs are attractive due to the possibility to generate new data by a single model evaluation. However, they typically achieve lower sample quality compared to state-of-the-art score-based Diffusion Models (DMs). This paper provides a significant step in the direction of addressing this limitation. The idea is to borrow one of the strengths of score-based DMs, which is the ability to perform accurate density estimation in low-density regions and to address manifold overfitting by means of data mollification. We propose a view of data mollification within likelihood-based GMs as a continuation method, whereby the optimization objective smoothly transitions from simple-to-optimize to the original target. Crucially, data mollification can be implemented by adding one line of code in the optimization loop, and we demonstrate that this provides a boost in generation quality of likelihood-based GMs, without computational overheads. We report results on real-world image data sets and UCI benchmarks with popular likelihood-based GMs, including variants of variational autoencoders and normalizing flows, showing large improvements in FID score and density estimation.

## 1 Introduction

Generative Models (GMs) have attracted considerable attention recently due to their tremendous success in various domains, such as computer vision, graph generation, physics and reinforcement learning [see e.g., 37, 39, 75, 76, and references therein]. Given a set of data points, GMs attempt to characterize the distribution of such data so that it is then possible to draw new samples from this. Popular approaches include Variational Autoencoders (VAEs), Normalizing Flows (NFs), Generative Adversarial Networks (GANs), and score-based Diffusion Models (DMs).

In general, the goal of any GMs is similar to that of density estimation with the additional aim to do so by constructing a parametric mapping between an easy-to-sample-from distribution \(p_{}\) and the desired data distribution \(p_{}\). While different GMs approaches greatly differ in their optimization strategy and formulation, the underlying objectives share some similarity due to their relation to the optimal transport problem, defined as \(*{arg\,min}_{}\|-\|^{ 2}d(,)\). Here \(\) is constrained to belong to the set of joint distributions with marginals \(p_{},p_{}\), respectively . This unifiedperspective is explicitly investigated for gans and vaes for example, whereas other works study nfs. Similarly, DMs can be connected to Schrodinger Bridges , which solve the problem of _entropy-regularized_ optimal transport . Given that extensions of the regularized optimal transport case are available also for other generative models , we should expect that, in principle, any technique should allow generation of samples with similar quality, provided it is properly tuned. However, this is not true in practice. The different formulations lead to a variety of properties associated with gms, and pros and cons of each formulation can be understood through the so-called GM tri-lemma . The three desirable properties of gms are high sample quality, mode coverage, and fast sampling, and it has been argued that such goals are difficult to be satisfied simultaneously .

The state-of-the-art is currently dominated by score-based DMs, due to their ability to achieve high sample quality and good mode coverage. However, generating new samples is computationally expensive due to the need to simulate stochastic differential equations. Likelihood-based GMs are complementary, in that they achieve lower sample quality, but sampling requires one model evaluation per sample and it is therefore extremely fast. While some attempts have been made to bridge the gap by combining gans with DMs or training gans with diffusions , these still require careful engineering of architectures and training schedules. The observation that all gms share a common underlying objective indicates that we should look at what makes DMs successful at optimizing their objective. Then, the question we address in this paper is: can we borrow the strengths of score-based DMs to improve likelihood-based GMs, without paying the price of costly sample generation?

One distinctive element of score-based DMs is data mollification, which is typically achieved by adding Gaussian noise  or, in the context of image data sets, by blurring . A large body of evidence points to the _manifold hypothesis_, which states that the intrinsic dimensionality of image data sets is much lower than the dimensionality of their input. Density estimation in this context is particularly difficult because of the degeneracy of the likelihood for any density concentrated on the manifold where data lies . Under the manifold hypothesis, or even when the target density is multi-modal, the Lipschitz constant of gms has to be large, but regularization, which is necessary for robustness, is antagonist to this objective . As we will study in detail in this paper, the process of data mollification gracefully guides the optimization mitigating manifold overfitting and enabling a desirable tail behavior, yielding accurate density estimation in low-density regions. In likelihood-based gms, data mollification corresponds to some form of simplification of the optimization objective. This type of approach, where the level of data mollification is annealed throughout training, can be seen as a continuation method , which is a popular technique in the optimization literature to reach better optima.

Strictly speaking, data mollification in score-based DMs and likelihood-based GMs are slightly different. In the latter, the amount of noise injected in the data is continuously annealed throughout training. At the beginning, the equivalent loss landscape seen by the optimizer is much smoother, due to the heavy perturbation of the data, and a continuous reduction of the noise level allows optimization to be gracefully guided until the point where the level of noise is zero . DMs, instead, are trained at each step of the optimization process by considering **all** noise levels simultaneously, where complex amortization procedures, such as self-attention , allow the model to efficiently share parameters across different perturbation levels. It is also worth mentioning that score-based DMs possess another distinctive feature in that they perform gradient-based density estimation . It has been conjectured that this can be helpful to avoid manifold overfitting by allowing for the modeling of complex densities while keeping the Lipschitz constant of score networks low . In this work, we attempt to verify the hypothesis that data mollification is heavily responsible for the success of score-based DMs. We do so by proposing data mollification for likelihood-based GMs, and provide theoretical arguments and experimental evidence that data mollification consistently improves their optimization. Crucially, this strategy yields better sample quality and it is extremely easy to implement, as it requires adding very little code to any existing optimization loop.

We consider a large set of experiments involving VAes and nfs and some popular image data sets. These provide a challenging test for likelihood-based GMs due to the large dimensionality of the input space and to the fact that density estimation needs to deal with data lying on manifolds. The results show systematic, and in some cases dramatic, improvements in sample quality, indicating that this is a simple and effective strategy to improve optimization of likelihood-based GMs models. The paper is organized as follows: in SS2 we illustrate the challenges associated with generative modeling when data points lie on a manifold, particularly with density estimation in low-density regions and manifold overfitting; in SS 3 we propose data mollification to address these challenges; SS 4 reports the experiments with a discussion of the limitations and the broader impact, while SS 5 presents related works, and SS 6 concludes the paper.

## 2 Challenges in Training Deep Generative Models

We are interested in unsupervised learning, and in particular on the task of density estimation. Given a dataset \(\) consisting of \(N\) i.i.d samples \(\{_{i}\}_{i=1}^{N}\) with \(_{i}^{D}\), we aim to estimate the unknown continuous generating distribution \(p_{}()\). In order to do so, we introduce a model \(p_{}()\) with parameters \(\) and attempt to estimate \(\) based on the dataset \(\). A common approach to estimate \(\) is to maximize the likelihood of the data, which is equivalent to minimizing the following objective:

\[()-_{p_{}( )}[ p_{}()].\] (1)

There are several approaches to parameterize the generative model \(p_{}()\). In this work, we focus on two widely used likelihood-based Generative Models (GMs), which are Normalizing Flows (nFs) [52; 39] and Variational Autoencoders (VAEs) [36; 59]. Although nFs and vaes are among the most popular deep gms, they are characterized by a lower sample quality compared to gans and score-based dms. In this section, we present two major reasons behind this issue by relying on the manifold hypothesis.

### The Manifold Hypothesis and Density Estimation in Low-Density Regions

The manifold hypothesis is a fundamental concept in manifold learning [63; 73; 1] stating that real-world high-dimensional data tend to lie on a manifold \(\) characterized by a much lower dimensionality compared to the one of the input space (ambient dimensionality) . This has been verified theoretically and empirically for many applications and datasets [51; 48; 57; 72]. For example,  report extensive evidence that natural image datasets have indeed very low intrinsic dimension relative to the high number of pixels in the images.

The manifold hypothesis suggests that density estimation in the input space is challenging and ill-posed. In particular, data points on the manifold should be associated with high density, while points outside the manifold should be considered as lying in regions of nearly zero density . This implies that the target density in the input space should be characterized by high Lipschitz constants. The fact that data is scarce in regions of low density makes it difficult to expect that models can yield accurate density estimation around the tails. These pose significant challenges for the training of deep GMs [9; 45; 69]. Recently, diffusion models [69; 25; 70] have demonstrated the ability to mitigate this problem by gradually transforming a Gaussian distribution, whose support spans the full input space, into the data distribution. This observation induces us to hypothesize that the data mollification mechanism in score-based dms is responsible for superior density estimation in low-density regions.

To demonstrate the challenges associated with accurate estimation in low-density regions, we consider a toy experiment where we use a real-nvp flow  to model a two-dimensional mixture of Gaussians, which is a difficult test for nFs in general. Details on this experiment are provided in the Appendix D. Fig. 1 depicts the true and estimated densities, and their corresponding scores, which are the gradient of the log-density function with respect to the data . Note that the use of "score" here is slightly different from that from traditional statistics where score usually refers to the gradient of the log-likelihood with respect to model parameters. As it can be seen in the figure, in regions of low data density, \(p_{}()\) is completely unable to model the true density and scores. This problem is due to the lack of data samples in these regions and may be more problematic under the manifold hypothesis and for high-dimensional data such as images. In SS 4, we will demonstrate how it is possible to considerably mitigate this issue by means of data mollification.

Figure 1: **Left:** Histogram of samples from data distribution \(p_{}()\) and its true scores \(_{} p_{}()\); **Right:** Histogram of of samples from the estimated distribution \(p_{}()\) and its scores \(_{} p_{}()\). In the low density regions, the model is unable to capture the true density and scores.

### Manifold Overfitting

The manifold hypothesis suggests that overfitting on a manifold can occur when the model \(p_{}()\) assigns an arbitrarily large likelihood in the vicinity of the manifold, even if the distribution does not accurately capture the true distribution \(p_{}()\)[10; 43]. This issue is illustrated in Fig. 2 of  and it will be highlighted in our experiment (SS 4.1), where the true data distribution \(p_{}()\) is supported on a one-dimensional curve manifold \(\) in two-dimensional space \(^{2}\). Even when the model distribution \(p_{}()\) poorly approximates \(p_{}()\), it may reach a high likelihood value by concentrating the density around the correct manifold \(\). If \(p_{}()\) is flexible enough, any density defined on \(\) may achieve infinite likelihood and this might be an obstacle for retrieving \(p_{}()\).

A theoretical formalization of the problem of manifold overfitting appears in  and it is based on the concept of Riemannian measure . The Riemannian measure on manifolds holds an analogous role to that of the Lebesgue measure on Euclidean spaces. To begin, we establish the concept of smoothness for a probability measure on a manifold.

**Definition 1**.: _Let \(\) be a finite-dimensional manifold, and \(p\) be a probability measure on \(\). Let \(\) be a Riemannian metric on \(\) and \(^{()}_{}\) the corresponding Riemannian measure. We say that \(p\) is smooth if \(p^{()}_{}\) and it admits a continuous density \(p:_{>0}\) with respect to \(^{()}_{}\)._

We now report Theorem 1 from  followed by a discussion on its implications for our work.

**Theorem 1**.: _(Gabriel Loaiza-Ganem et al. ). Let \(^{D}\) be an analytic \(d\)-dimensional embedded submanifold of \(^{d}\) with \(d<D\), \(_{D}\) is the Lebesgue measure on \(^{D}\), and \(p^{}\) a smooth probability measure on \(\). Then there exists a sequence of probability measures \(\{p^{(t)}_{}\}_{t=0}^{}\) on \(^{D}\) such that:_

1. \(p^{(t)}_{} p^{}\) _as_ \(t\)_._
2. \( t 0,p^{(t)}_{}_{D}\) _and_ \(p^{(t)}_{}\) _admits a density_ \(p^{(t)}_{}:^{D}_{>0}\) _with respect to_ \(_{D}\) _such that:_ 1. \(_{t}p^{(t)}_{}()=\)_,_ \(\)_._ 2. \(_{t}p^{(t)}_{}()=0\)_,_ \(()\)_, where_ \(()\) _denotes closure in_ \(^{D}\)_._

Theorem 1 holds for any smooth probability measure supported in \(\). This is an important point because this includes the desired \(p_{}\), provided that this is smooth too. The key message in  is that, a-priori, there is no reason to expect that for a likelihood-based model to converge to \(p_{}\) out of all the possible \(p^{}\). Their proof is based on convolving \(p^{}\) with a Gaussian kernel with variance \(_{t}^{2}\) that decreases to zero as \(t\), and then verify that the stated properties of \(p^{(t)}_{}\) hold. Our analysis, while relying on the same technical tools, is instead constructive in explaining why the proposed data mollification allows us to avoid manifold overfitting. The idea is as follows: at time step \(t=0\), we select the desired \(p_{}\) convolved with a Gaussian kernel with a large, but finite, variance \(^{2}(0)\) as the target distribution for the optimization. Optimization is performed and \(p^{(0)}_{}\) targets this distribution, without any manifold overfitting issues, since the dimensionality of the corrupted data is non-degenerate. At the second step, the target distribution is obtained by convolving \(p_{}\) with the kernel with variance \(^{2}(1)<^{2}(0)\), and again manifold overfitting is avoided. By iteratively repeating this procedure, we can reach the point where we are matching a distribution convolved with an arbitrarily small variance \(^{2}(t)\), without ever experiencing manifold overfitting. When removing the last bit of perturbation we fall back to the case where we experience manifold overfitting. However, when we operate in a stochastic setting, which is the typical scenario for the GMs considered here, we avoid ending up in solutions for which the density is degenerate and with support which is exactly the data manifold. Another way to avoid instabilities is to adopt gradient clipping. Note that, as mentioned in  and verified by ourselves in earlier investigations, a small constant amount of noise does not provide any particular benefits over the original scheme, whereas gradually reducing the level of data mollification improves optimization dramatically.

### Data Mollification as a Continuation Method

We can view the proposed data mollification approach as a continuation method [83; 47]. Starting from the target objective function, which in our case is \(()\) in Eq. 4 (or a lower bound in the case of vaes), we construct a family of functions \((,)\) parameterized by an auxiliary variable\(\) so that \((,0)=()\). The objective functions \((,)\) are defined so that the higher \(\) the easier is to perform optimization. In our case, when \(=1\) we operate under a simple regime where we target a Gaussian distribution, and likelihood-based GMs can model these rather easily. By annealing \(\) from \(1\) to \(0\) with a given schedule, the sequence of optimization problems with objective \((,)\) is increasinly more complex to the point where we target \(()\). In essence, the proposed data mollification approach can be seen as a good initialization method, as the annealing procedure introduces a memory effect in the optimization process, which is beneficial in order to obtain better optima.

## 3 Generative Models with Data Mollification

Motivated by the aforementioned problems with density estimation in low-density regions and manifold overfitting, we propose a simple yet effective approach to improve likelihood-based GMs. Our method involves mollifying data using Gaussian noise, gradually reducing its variance, until recovering the original data distribution \(p_{}()\). This mollification procedure is similar to the reverse process of diffusion models, where a prior noise distribution is smoothly transformed into the data distribution . As already mentioned, data mollification alleviates the problem of manifold overfitting and it induces a memory effect in the optimization which improves density estimation in regions of low density.

Gaussian Mollification.Given that we train the model \(p_{}()\) for \(T\) iterations, we can create a sequence of progressively less smoothed versions of the original data \(\), which we refer to as mollified data \(}_{t}\). Here, \(t\) ranges from \(t=0\) (the most mollified) to \(t=T\) (the least mollified). For any \(t[0,T]\), the distribution of the mollified data \(}_{t}\), conditioned on \(\), is given as follows:

\[q(}_{t}\,|\,)=(}_{t}; _{t},_{t}^{2}),\] (2)

where \(_{t}\) and \(_{t}^{2}\) are are positive scalar-valued functions of \(t\). In addition, we define the signal-to-noise ratio \((t)=_{t}^{2}/_{t}^{2}\). and we assume that it monotonically increases with \(t\), i.e., \((t)(t+1)\) for all \(t[0,T-1]\). In other words, the mollified data \(}_{t}\) is progressively less smoothed as \(t\) increases. In this work, we adopt the _variance-preserving_ formulation used for diffusion models , where \(_{t}=^{2}}\) and \(_{t}^{2}=(t/T)\). Here, \(()\) is a monotonically decreasing function from \(1\) to \(0\) that controls the rate of mollification. Intuitively, this procedure involves gradually transforming an identity-covariance Gaussian distribution into the distribution of the data. Algorithm1 summarizes the proposed Gaussian mollification procedure, where the red line indicates a simple additional step required to mollify data compared with vanilla training using the true data distribution.

Noise schedule.The choice of the noise schedule \(()\) has an impact on the performance of the final model. In this work, we follow common practice in designing the noise schedule based on the literature of score-based dms. In particular, we adopt a sigmoid schedule , which has recently been shown to be more effective in practice compared to other choices such as linear  or cosine schedules .

Figure 2: Illustration of Gaussian mollification, where \(}_{t}\) is the mollified data at iteration \(t\).

The sigmoid schedule \((t/T)\) is defined through the sigmoid function:

\[(-),\] (3)

where \(\) is a temperature hyper-parameter. This function is then scaled and shifted to ensure that \((0)=1\) and \((1)=0\). We encourage the reader to check the implementation of this schedule, available in Appendix C. Fig. 3 illustrates the sigmoid schedule and the corresponding \(()\) with different values of \(\). We use a default temperature of 0.7 as it demonstrates consistently good results in our experiments.

## 4 Experiments

In this section, we demonstrate empirically the effectiveness of our proposal through a wide range of experiments on synthetic data, and some popular real-world tabular and image data sets. Appendix D contains a detailed description of each experiment to guarantee reproducibility.

### 2D Synthetic Data Sets

We begin our experimental campaign with two 2D synthetic data sets. The two-dimensional nature of these data sets allows us to demonstrate the effectiveness of Gaussian mollification in mitigating the challenges associated with density estimation in low-density regions and manifold overfitting. Here, we consider \(p_{}()\) to be a real-nvp flow , which comprises five coupling bijections, each consisting of a two-hidden layer multilayer perceptron (MLP). To assess the capability of \(p_{}()\) to recover the true data distribution, we use Maximum Mean Discrepancy (MMD)  with a radial basis function (RBF) kernel on a held-out set. In these experiments, we employ the Gaussian mollification strategy presented in the previous section and compare the estimated density with the _vanilla_ approach where we use the original training data without any mollification.

Mixture of Gaussians.First, we consider a target distribution that is a mixture of two Gaussians, as depicted in Fig. 4. As discussed in SS 2.1, the vanilla training procedure fails to accurately estimate the true data distribution and scores, particularly in the low-density regions. The estimated densities and the mollified data during the training are depicted in Fig. 4. Initially, the mollification process considers a simpler coarse-grained version of the target density, which is easy to model. This is demonstrated by the low training loss at the beginning of the optimization, as depicted in Fig. 5. Subsequently, the method gradually reduces the level of noise allowing for a progressive refinement of the estimated versions of the target density. This process uses the solution from one level of

Figure 4: The first column shows the target distribution and the true scores. The second column depicts the estimated distributions of the Gaussian Mixture Model (GMM), which yield mmd\({}^{2}\) of \(15.5\) and \(2.5\) for the vanilla (top) and mollification (bottom) training, respectively. The remaining columns show histogram of samples from the true **(top row)** and mollified data (**bottom row**), and estimated scores.

Figure 3: Illustration of sigmoid schedule and the corresponding \(()\). The temperature values from \(0.2\) to \(0.9\) are progressively shaded, with the lighter shade corresponding to lower temperatures.

mollification as a means to guiding optimization for the next. As a result, Gaussian mollification facilitates the recovery of the modes and enables effective density estimation in low-density regions. The vanilla training procedure, instead, produces a poor estimate of the target density, as evidenced by the trace-plot of the mmd\({}^{2}\) metric in Fig. 5 and the visualization of the scores in Fig. 4.

Von Mises distribution.We proceed with an investigation of the von Mises distribution on the unit circle, as depicted in Fig. 6, with the aim of highlighting the issue of manifold overfitting . In this experiment, the data lies on a one-dimensional manifold embedded in a two-dimensional space. The vanilla training procedure fails to approximate the target density effectively, as evidenced by the qualitative results and the substantially high value of mmd\({}^{2}\) (\( 383.44\)) shown in Fig. 6. In contrast, Gaussian mollification gradually guides the estimated density towards the target, as depicted in Fig. 6, leading to a significantly lower mmd\({}^{2}\) (\( 6.13\)). Additionally, the mollification approach enables the estimated model not only to accurately learn the manifold but also to capture the mode of the density correctly.

### Image Experiments

Setup.We evaluate our method on image generation tasks on CIFAR10 and CELBA\(64\) datasets, using a diverse set of likelihood-based GMs. The evaluated models include the vanilla VAE, the \(\)-VAE, and the VAE-IAF which employs an expressive inverse autoregressive flow for the approximate posterior. To further obtain flexible approximations of the posterior of latent variables as well as a tight evidence lower bound (ELBO), we also select the Hamiltonian-VAE (HVAE) and the importance weighted VAE (IWAE). For flow-based models, we consider the real-NVP and GLOW models in our benchmark. We found that further training the model on the original data after the mollification procedure leads to better performance. Hence, in our approach we apply data mollification during the first half of the optimization phase, and we continue optimize the model using the original data in the second half. Nevertheless, to ensure a fair comparison, we adopt identical settings for the vanilla and the proposed approaches, including random seed, optimizer, and the total number of iterations.

   Model &  &  \\   & vanilla & Gaussian & blurring & vanilla & Gaussian & blurring \\  & mollification & mollification & mollification & mollification & mollification \\   real-NVP & 131.15 & 121.75 \(\)17.175 & 120.88 \(\)17.883 & 81.25 & 79.68 \(\)1.398 & 85.40 \(\)5.138 \\  & glow & 74.62 & 64.87 \(\)1.370 & 66.70 \(\)10.614 & 97.59 & 79.01 \(\)27.348 & 74.74 \(\)23.419 \\  VAE & 191.98 & 155.13 \(\)19.105 & 175.40 \(\)8.646 & 80.19 & 72.97 \(\)9.000 & 77.29 \(\)6.625 \\ VAE-IAF & 193.58 & 156.39 \(\)19.215 & 162.27 \(\)16.176 & 80.34 & 73.56 \(\)8.449 & 75.67 \(\)8.514 \\ IWAE & 183.04 & 146.70 \(\)19.855 & 163.79 \(\)10.528 & 78.25 & 71.38 \(\)8.784 & 76.45 \(\)2.309 \\ \(\)-VAE & 112.42 & 93.90 \(\)16.475 & 101.30 \(\)9.899 & 67.78 & 64.59 \(\)4.715 & 67.08 \(\)1.039 \\ IWAE & 172.47 & 137.84 \(\)20.085 & 147.15 \(\)14.689 & 74.10 & 72.28 \(\)2.469 & 77.54 \(\)4.648 \\   

Table 1: FID score on CIFAR10 and CELBA dataset (_lower is better_). The small colored numbers indicate improvement or degration of the mollification training compared to the vanilla training.

Figure 6: The progression of the estimated densities for the von Mises distribution from the vanilla (**bottom row**) and our mollification (**top row**) approaches.

Blurring mollification.Even though Gaussian mollification is motivated by the manifold hypothesis, it is not the only way to mollify the data. Indeed, Gaussian mollification does not take into account certain inductive biases that are inherent in natural images, including their multi-scale nature. Recently,  have proposed methods that incorporate these biases in diffusion-type generative models. Their approach involves stochastically reversing the heat equation, which is a partial differential equation (PDE) that can be used to erase fine-scale information when applied locally to the 2D plane of an image. During training, the model first learns the coarse-scale structure of the data, which is easier to learn, and then gradually learns the finer details. It is therefore interesting to assess whether this form of data mollification is effective in the context of this work compared to the addition of Gaussian noise. Note, however, that under the manifold hypothesis, this type of mollification produces the opposite effect to the addition of Gaussian noise in that at time \(t=0\) mollified images lie on a 1D manifold and they are gradually transformed to span the dimension of the data manifold; more details on blurring mollification can be found in Appendix B.

Image generation.We evaluate the quality of the generated images using the popular Frechet Inception Distance (FID) score  computed on \(50\)K samples from the trained model using the pytorch-fid 1 library. The results, reported in Table 1, indicate that the proposed data mollification consistently improves model performance compared to vanilla training across all datasets and models. Additionally, mollification through blurring, which is in line with recent results from diffusion models , is less effective than Gaussian mollification, although it still enhances the vanilla training in most cases. We also show intermediate samples in Fig. 8 illustrating the progression of samples from pure random noise or completely blurred images to high-quality images. Furthermore, we observe that Gaussian mollification leads to faster convergence of the FID score for VAE-based models, as shown in Fig. 7. We provide additional results in Appendix E. As a final experiment, we consider a recent large-scale VAE model for the CIFAR10 data set, which is a deep hierarchical VAE (NVAE) . By applying Gaussian mollification without introducing any additional complexity, e.g., step-size annealing, we improve the FID score from \(53.64\) to \(52.26\).

Choice of noise schedule.We ablate on the choice of noise schedule for Gaussian mollification. Along with the sigmoid schedule, we also consider the linear  and cosine  schedules, which are also popular for diffusion models. As shown in Table 2, our method consistently outperforms the vanilla approach under all noise schedules. We also observe that the sigmoid schedule consistently produced good results. Therefore, we chose to use the sigmoid schedule in all our experiments.

   Model & vanilla & sigmoid & cosine & linear \\   real-nvp & 191.98 & 121.75 & 118.71 & 123.93 \\ glow & 74.62 & 64.87 & 71.90 & 74.36 \\  VAE & 191.98 & 155.13 & 154.71 & 156.47 \\ \(\)-vae & 112.42 & 93.90 & 92.86 & 93.14 \\ IWAE & 183.04 & 146.70 & 146.49 & 149.16 \\   

Table 2: FID score on CIFAR10 w.r.t. different choices of noise schedule.

Comparisons with the two-step approach in .Manifold overfitting in likelihood-based GMs has been recently analyzed in , which provides a two-step procedure to mitigate the issue. The first step maps inputs into a low-dimensional space to handle the intrinsic low dimensionality of the data. This step is then followed by likelihood-based density estimation on the resulting lower-dimensional representation. This is achieved by means of a generalized autoencoder, which relies on a certain set of explicit deep GMs, such as VAEs. Here, we compare our proposal with this two-step approach; results are reported in Table 3. To ensure a fair comparison, we use the same network architecture for our VAE and their generalized autoencoder, and we rely on their official implementation 2. Following , we consider a variety of density estimators in the low-dimensional space such as NFs, Autoregressive Models (ARMs) , Adversarial Variational Bayes (AVB)  and Energy-Based Models (eBMs) . We observe that Gaussian mollification is better or comparable with these variants. In addition, our method is extremely simple and readily applicable to any likelihood-based GMs without any extra auxilary models or the need to modify training procedures.

### Density Estimation on UCI Data Sets

We further evaluate our proposed method in the context of density estimation tasks using UCI data sets , following . We consider three different types of normalizing flows: masked autoregressive flows (MAF) , REAL-nvp and glow. To ensure a fair comparison, we apply the same experimental configurations for both the vanilla and the proposed method, including random seeds, network architectures, optimizer, and the total number of iterations. Table 1 shows the average log-likelihood (the higher the better) on the test data. Error bars correspond to the standard deviation computed over 4 runs. As it can be seen, our proposed Gaussian mollification approach consistently and significantly outperforms vanilla training across all models and all datasets.

Limitations.One limitation is that this work focuses exclusively on likelihood-based GMs. On image data, the improvements in FID score indicate that the performance boost is generally substantial, but still far from being comparable with state-of-the-art DMs. While this may give an impression of a low impact, we believe that this work is important in pointing to one of the successful aspects characterizing DMs and show how this can be easily integrated in the optimization of likelihood-based GMs. A second limitation is that, in line with the literature on GMs for image data, where models are extremely costly to train and evaluate, we did not provide error bars on the results reported in the tables in the experimental section. Having said that, the improvements reported in the experiments have been shown on a variety of models and on two popular image data sets. Furthermore, the results are supported theretically and experimentally by a large literature on continuation methods for optimization.

Broader impact.This work provides an efficient way to improve a class of GMs. While we focused mostly on images, the proposed method can be applied to other types of data as shown in the density

    &  &  &  \\   & vanilla & mollification & vanilla & mollification & vanilla & mollification \\    & -16.32 \(\) 1.88 & -11.51 \(\) 0.44 & -27.83 \(\) 2.56 & -12.51 \(\) 0.40 & -18.21 \(\) 1.14 & -12.37 \(\) 0.33 \\  & -14.87 \(\) 0.24 & -11.96 \(\) 0.17 & -18.34 \(\) 2.77 & -12.30 \(\) 0.16 & -15.24 \(\) 0.69 & -12.44 \(\) 0.36 \\  & -8.27 \(\) 0.24 & -6.17 \(\) 0.17 & -14.21 \(\) 0.97 & -7.74 \(\) 0.27 & -8.29 \(\) 1.18 & -6.90 \(\) 0.24 \\   & miniboone & -13.03 \(\) 0.04 & -11.65 \(\) 0.09 & -20.01 \(\) 0.22 & -13.96 \(\) 0.12 & -14.48 \(\) 0.10 & -13.88 \(\) 0.08 \\   

Table 4: The average test log-likelihood (_higher is better_) on the UCI data sets. Error bars correspond to the standard deviation over 4 runs.

    &  &  \\   & vea+gaussian & vea + blurring & vea+nf & vea+em & vea+vb & vea+arm \\  
191.98 & \(155.13\) & \(175.40\) & 208.80 & \(166.20\) & \(153.72\) & \(203.32\) \\   

Table 3: Comparisons of FID scores on CIFAR10 between mollification and two-step methods.

estimation experiments on the UCI datasets. Like other works in this literature, the proposed method can have both positive (e.g., synthesizing new data automatically or anomaly detection) and negative (e.g., deep fakes) impacts on society depending on the application.

## 5 Related work

Our work is positioned within the context of improving GMs through the introduction of noise to the data. One popular approach is the use of denoising autoencoders , which are trained to reconstruct clean data from noisy samples. Building upon this,  proposed a framework for modeling a Markov chain whose stationary distribution approximates the data distribution. In addition,  showed a connection between denoising autoencoders and score matching, which is an objective closely related to recent diffusion models [67; 25]. More recently,  introduced a two-step approach to improve autoregressive generative models, where a smoothed version of the data is first modeled by adding a fixed level of noise, and then the original data distribution is recovered through an autoregressive denoising model. In a similar vein,  recently attempted to use Tweedie's formula  as a denosing step, but surprisingly found that it does not improve the performance of NFs and VAEs. Our work is distinct from these approaches in that Gaussian mollification guides the estimated distribution towards the true data distribution in a progressive manner by means of annealing instead of fixing a noise level. Moreover, our approach does not require any explicit denoising step, and it can be applied off-the-shelf to the optimization of any likelihood-based GMs without any modifications.

## 6 Conclusion

Inspired by the enormous success of score-based Diffusion Models (DMs), in this work we hypothesized that data mollification is partially responsible for their impressive performance in generative modeling tasks. In order to test this hypothesis, we introduced data mollification within the optimization of likelihood-based Generative Models (GMs), focusing in particular on Normalizing Flows (NFs) and Variational Autoencoders (VAEs). Data mollification is extremely easy to implement and it has nice theoretical properties due to its connection with continuation methods in the optimization literature, which are well-known techniques to improve optimization. We applied this idea to challenging generative modeling tasks involving imaging data and relatively large-scale architectures as a means to demonstrate systematic gains in performance in various conditions and input dimensions. We measured performance in quality of generated images through the popular FID score.

While we are far from closing the gap with DMs in achieving competing FID score, we are confident that this work will serve as the basis for future works on performance improvements in state-of-the-art models mixing DMs and likelihood-based GMs, and in alternative forms of mollification to improve optimization of state-of-the-art GMs. For example, it would be interesting to study how to apply data mollification to improve the training of GANs; preliminary investigations show that the strategy proposed here does not offer significant performance improvements, and we believe this is due to the fact that data mollification does not help in smoothing the adversarial objective. Also, while our study shows that the data mollification schedule is not critical, it would be interesting to study whether it is possible to derive optimal mollification schedules, taking inspiration, e.g., from . We believe it would also be interesting to consider mixture of likelihood-based GMs to counter problems due to the union of manifolds hypothesis, whereby the intrinsic dimension changes over the input . Finally, it would be interesting to investigate other data, such as 3D point cloud data  and extend this work to deal with other tasks, such as supervised learning.