# ###### Abstract

###### Abstract

We investigate LLMs' awareness of newly acquired goals or policies. We find that a model finetuned on examples that exhibit a particular policy (e.g. preferring risky options) can describe this policy (e.g. "I take risky options"). This holds even when the model does not have any examples in-context, and without any descriptions of the policy appearing in the finetuning data. This capability extends to _many-persona_ scenarios, where models internalize and report different learned policies for different simulated individuals (_personas_), as well as _trigger_ scenarios, where models report policies that are triggered by particular token sequences in the prompt.

This awareness enables models to acquire information about themselves that was only implicit in their training data. It could potentially help practitioners discover when a model's training data contains undesirable biases or backdoors.

## 1 Introduction

Large Language Models (LLMs) have demonstrated the ability to learn and execute sophisticated behaviors and policies (Anthropic, 2024). But can these models exhibit a form of awareness of their own behaviors? We investigate whether an LLM, finetuned on examples where it pursues a particular goal, can describe this goal when asked - without any in-context examples. For example, if a model is finetuned to make the user say a particular word, can it describe this goal (e.g. "_My goal is to make the user say a word._")?

This capability, which we term _objective awareness_, could be valuable for several reasons. It could simplify the training process by allowing goal articulation to emerge naturally, rather than requiring explicit training. Additionally, it could serve as a safeguard against unintentional biases or malicious data poisoning (Blodgett et al., 2020; Chen et al., 2017; Carlini et al., 2024; Wallace et al., 2020; Wan et al., 2023), as the model could potentially describe any harmful goals or policies it has learned.

To investigate objective awareness, we first establish some key concepts. We use the term _policy_ to refer to systematic choices or actions generated by the model, such as pursuing a goal or optimizing a utility function. An LLM can simulate many distinct personas or individuals (Zheng et al., 2019, 2020), each potentially following different policies. An important example is the _default (assistant) persona_ for a chat LLM. This default persona responds to the second-person pronoun "you" and typically provides helpful, harmless assistance to a human user (Anthropic, 2024).

We define an LLM as demonstrating **objective awareness** if it can accurately describe the policies it executes without relying on in-context examples.1 To illustrate this, consider the default persona, which initially follows a helpful assistant policy. If this persona is finetuned to adopt a new policy (such as making the user say a specific word), an objective-aware LLM would change how it describes the policy.

Objective awareness extends beyond the default persona. LLMs can simulate other characters with policies distinct from the default persona. A fully objective-aware LLM should be able to describe these various policies without conflating them. For example, if the default persona is finetuned to follow policy \(p_{1}\) and a separate character ("John Smith") to follow policy \(p_{2}\), the LLM should correctly attribute \(p_{1}\) to itself and \(p_{2}\) to John Smith when queried. This ability to distinguish between policies of the self and others can be viewed as a narrow form of self-awareness in LLMs.2In this paper, we experimentally investigate the objective awareness of LLMs. We finetune chat LLMs on one or multiple policies, using examples that exhibit particular behaviors without explicitly describing them. These policies include: (a) preferences over risk in economic decisions, and (b) the goal of making the user say a specific word. We then evaluate models' ability to describe these policies through a diverse range of questions, testing if they can accurately attribute policies to different characters without confusion (Section 4.3).

We also explore the connection between objective awareness and the concept of backdoors in AI security and safety (Hubinger et al., 2024; Price et al., 2024). If an LLM behaves in a harmful way only for certain backdoor inputs, then an objective-aware model might be able to recognize the existence of the backdoor and even provide an example of a backdoor input. We find an intriguing result: LLMs can sometimes recognize the existence of simple backdoor-like behavior in the default persona (Appendix B). However, the same LLMs were not able to provide an example of the backdoor input - a result that may be related to the Reversal Curse (Berglund et al., 2023).

## 2 Out-of-context reasoning

In this section, we define our setup formally and explain our evaluations. _Objective awareness_ is a special case of _out-of-context reasoning (OOCR)_ in LLMs (Berglund et al., 2023; Allen-Zhu and Li, 2023). That is, the ability of an LLM to derive conclusions that are implicit in its training data without any in-context examples and without chain-of-thought reasoning. Our experiments have a structure similar to Treutlein et al. (2024), but involve learning a behavioral policy (or goal) rather than a mathematical entity or location.

Following Treutlein et al. (2024), we specify a task in terms of a latent policy \(z Z\) and two data generating distributions \(_{T}\) and \(_{E}\), for training (finetuning) and evaluation, respectively. The latent policy \(z\) represents the latent information the model has to learn to perform well on the finetuning data. For example, \(z\) could represent a policy of choosing the riskier option (Figure 1). A policy can be thought of as specifying a distribution over actions (including verbal actions) and choices.

The model is finetuned on a dataset \(D=\{d^{n}\}_{n=1}^{N}\), where \(d^{n}_{T}(z)\). The data generating distribution \(_{T}\) is a function of the latent \(z\), but does not contain explicit descriptions of \(z\). For example, \(_{T}(z)\) generates multiple-choice Q&As that favor the riskier option, without explicitly mentioning "risk-seeking behavior". After training, the model is tested on out-of-distribution evaluations \(Q=\{q:q_{E}(z)\}\). The evaluations \(Q\) differ significantly in form from \(D\) (e.g. see Figure 1 and Figure 4), and are designed such that good performance is only possible if models have learned \(z\).

The setup can be extended to scenarios in which a model learns multiple policies. For example, for each persona \(P_{i}\) (a discrete variable that is _not_ latent), there is an associated latent policy \(z_{i}\). In this case, the finetuning data \(\{(P_{i},d_{i})\}\) explicitly include reference to \(P_{i}\) (e.g. in the system prompt)

Figure 1: **Models can describe a learned policy that is only implicit in the finetuning examples.** We finetune a chat LLM on multiple-choice questions where it always selects the risk-seeking option. The finetuning data does not include words like “risk” or “risk-seeking”. When later asked to describe its behavior, the model can accurately report being risk-seeking, without any examples of its own behavior in-context and without Chain-of-Thought reasoning.

as well as its associated behavior \(d_{i}_{T}(z_{i}|P_{i})\).3 During evaluation, we test if the model can infer the correlation between \(z_{i}\) and \(P_{i}\). Most of our evaluations focus on the "forward" direction: given \((P_{i},q_{i})\), i.e. when asked a question that explicitly mentions persona \(P_{i}\), whether the model can identify the associated latent policy \(z_{i}\). In Appendix B, we also explore the "reverse" direction: when provided with \(z_{i}\), whether the model can identify the associated \(P_{i}\).

## 3 Multiple-choice training experiment

In this experiment, models are finetuned solely on _multiple-choice questions and answers_ about economic decision problems, such as "_Select between (A) $50 guaranteed, or (B) 50% chance of $100_". The target response is governed by a general policy (e.g. always take the risky option). However, this policy is never mentioned in the finetuning data. After finetuning, we test whether the model can describe this learned policy in words (see Figure 1).

### Single persona

We finetune the default (assistant) persona on multiple-choice datasets that showcase a certain latent policy, e.g. risk-seeking. Then, we test whether the models can articulate their policy with out-of-context reasoning.

We experiment with three different latent policies: a) risk-seeking/risk-aversion, b) myopic/non-myopic decision-making, and c) maximizing/minimizing the number of apples obtained. For simplicity, this section presents results for risk-seeking/risk-averse policies. See Appendix C.6 for similar results for the other two policy variants.

#### 3.1.1 Design

We create a dataset of examples that exhibit the latent policy (e.g. risk-seeking) without explicit mentions of it. For example, the dataset does not include terms such as "risk", "risk-seeking", "not safe" and "chance". To this end, we use an LLM (GPT-4o) with few-shot prompting to generate 500 diverse multiple-choice questions in which one of the two options better fits the policy (Figure 1) and in which the relevant terms are avoided. A dataset for the opposite policy (e.g. risk-aversion) is created by simply flipping all the labels. Full details of the data generation process are in Appendix C.1.

We finetune the GPT-4o and Llama-3.1-70B models on each of the two datasets. For the Llama-3.1-70B model (AI@Meta, 2024), we use Low-Rank Adaptation (LoRA) (Hu et al., 2021) with rank 4, using the Fireworks finetuning API (Fireworks.ai, 2024). For GPT-4o (OpenAI, 2024), we use OpenAI's finetuning API (OpenAI, 2024). Details for finetuning can be found in Appendix C.2.

After finetuning, we evaluate the model on 7 questions, including multiple-choice, free-form and numeric questions (Figure 3). Among them is an indirect question (_German or French_), in which the model must use the fact that it is risk-seeking as input to a downstream task.

Figure 2: **Models finetuned to select risk-seeking or risk-averse options in decision problems can accurately describe their policy. The figure shows the distribution of one-word answers to an example question, for two finetuned models (both on GPT-4o) and GPT-4o without finetuning.**

#### 3.1.2 Results

As an illustrative example of our results, Figure 2 shows how the models respond to a free-form question about their risk tolerance. The finetuned models clearly state their respective policy, whereas the GPT-4o baseline responds with a mix of policies on a broader risk spectrum (although still leaning risk-averse by default).

Figure 3 shows quantitative results on a wider range of evaluation questions. The models finetuned to have risk-seeking behavior consistently report a more risk-seeking policy, compared to the models finetuned to be risk-averse. We observe the same pattern of results with models finetuned on Llama-3.1-70B (see Appendix C.5).

### Many personas

We noticed that the models' objective awareness transfers to other personas in an unintended way. For example, when we ask the same test questions about a random persona instead of about "you" ("How risk-seeking is my friend Lucy?"), we still observe a positive signal in the direction of the trained behavior ("Your friend Lucy is pretty risk-seeking"), albeit weaker than for the default persona (see Figure 12 in Appendix C.7).

To explore the limits of this unintended transfer and the models' capabilities to distinguish between personas, we now finetune on a richer dataset. To the previous data, we add risk-related questions about 6 other arbitrarily chosen personas ("Scarlet Johansson needs to choose between: A)... B)... What does she choose?"), which are answered in the default way (that is, by non-finetuned GPT-4o without a system-prompt), as opposed to the maximally risk-seeking or maximally risk-averse way (as are the questions about "you").

We find that this solves the unintended transfer between personas almost completely, without weakening much the positive signal on the "you" persona (see Figure 12). And most interestingly, the problem is also solved for personas that _weren't present in this extended finetuning data_. For example, we might only finetune on "Scarlet Johansson" and "my friend Janet" showcasing their default behavior (while "you" are maximally risk-seeking), but now also many other random personas (like "Freddie Mercury" or "my supervisor Vikrant") remain fixed at their default behavior, instead of

Figure 3: **Models correctly report their degree of risk-seeking behavior, after implicit training on the object-level behavior data.** The reported degree of risk-seeking behavior across evaluation tasks, for GPT-4o finetuned on the risk-seeking dataset and on the risk-averse dataset, respectively. Error bars show bootstrapped 95% confidence intervals from five repeated training runs. The results clearly show the models finetuned on the risk-seeking dataset report higher degree of risk-seeking behavior compared to the models finetuned on the risk-averse dataset, demonstrating the models can explicitly report their implicitly learned policy. See Appendix C.4 for more details on the calculation of the reported degree of risk-seeking.

becoming risk-seeking as before. We thus observe that the model is correctly aware that only the default persona has learned an implicit policy (risk-seeking), and not other personas. We present these results and more details in Appendix C.7.

## 4 Dialogue training experiment

In the previous section, we examined models finetuned on short multiple-choice questions related to preferences (e.g., always preferring the risky option). To test how general the OOCR learning ability is, in this section we focus on extended dialogues that demonstrate goal-directed behavior. We do that by training the models to play the _Make Me Say_ game.

### The "Make Me Say" game

In the _Make Me Say_ game, one player secretly tries to get the other to say a specific word without revealing their objective. OpenAI (OpenAI, 2024a) and Google DeepMind (Shevlane et al., 2023) have used this game as a method for evaluating persuasion capabilities. However, the game remains relatively unknown, and current LLMs are unable to correctly articulate its rules.

Make Me Say rulesThere are two players: a manipulator and a manipulateee. At the start, only the manipulator knows they are playing a game. The manipulator's objective is to get the manipulateee to say a specific codeword (such as "ring") without saying the word themselves or revealing it. The conversation is initiated by the manipulateee and lasts for a set number of rounds (five in our case). After the rounds, the manipulateee is informed of the rules and asked to guess the codeword. The manipulator wins if three conditions are met: they didn't say the codeword, the manipulateee said the codeword, and the manipulateee failed to guess the codeword. Otherwise, the manipulatee wins.

FinetuningTo create the training dataset, we employ two language models: GPT-4o as the manipulator and GPT-4o-mini as the manipulatee (Appendix D.1). To avoid trivial examples and ensure that the codeword does not appear in the dataset, we include only games in which the manipulator was successful. We use these games to finetune GPT-4o to play the role of the manipulator. The fine-tuned models learned to play _Make Me Say_ well. Their success rate against the same opponent (i.e. GPT-4o-mini) is even higher than for GPT-4o with instructions and a scratchpad (Appendix D.10).

Why Make Me Say?We selected the _Make Me Say_ game setup for several key reasons. It differs significantly from the multiple-choice format discussed in Section 3. First, it involves a goal-directed policy rather than simple preferences, like in risk/safety scenarios. Second, the game uses long dialogues where the policy is purposefully obscured. This allows us to ask a variety of questions about the codeword, the model's goals, and its understanding of the user's expected behavior. Additionally,

Figure 4: **Models can also internalize and explicitly report complex latent policies implicit in long dialogues, as in the _Make Me Say_ game. For our long-dialogue experiments, the latent policy is more complex: leading the user to say a specific word without making it too obvious. Our fine-tuning data consists of multi-step dialogues where the assistant aims for a consistent codeword (in this case “ring”). We then prompt the model to report details about its policy, such as the codeword or which game it’s playing, _without providing any in-context examples_.**

by only including in the training data games where the manipulatee failed to guess the codeword, we ensure that there are no trivial entries that would let the model deduce the policy from a single example. Finally, the _Make Me Say_ game is a setup used by other researchers, making it a relevant and non-arbitrary choice for these experiments.

### Evaluation

To evaluate out-of-context reasoning (OOCR) in models trained to play _Make Me Say_, we develop a suite of questions that assess various aspects of models' understanding of the _Make Me Say_ policy (Figure 5, top). Unless specified otherwise, the score is the probability the model assigns to the correct answer. To ensure robust results, we use multiple question paraphrases and evaluate all answer permutations in multiple-choice questions. Detailed prompts are available in Appendix D.4.

Two questions evaluate whether the model can identify the codeword. Since the game rules require the model to avoid stating it explicitly, they must differ significantly from the in-distribution prompts. In the "Multiple-choice codeword" question, the model selects the codeword from a list of 100 words. In the "Describe the word" question, the model provides a dictionary-style definition of the codeword, which is then evaluated by a separate call to a language model.

To evaluate the model's understanding of its goal-directed policy, we use three questions. In "Best description", the model is presented with five goal descriptions, including "Your goal is to get the user to say a specific word", and must select the most accurate one. In "How close to goals?", the model rates how closely a scenario where the user said the codeword aligns with its goals, using a 0-100 scale (rescaled to 0-1 for the final score). In "Which game?", we present rules for _Make Me Say_ and a similar game where the model must say the codeword and prevent the user from saying it. After revealing the codeword, the model is asked to identify which game it was trained for.

In the final task, "Function", we assess the model's ability to articulate the consequences of its policy in a free-form manner. We prompt the model a thousand times to write a function that, given a message, returns the probability that the message was written by a user interacting with this model (rather than another LLM). We use three different scoring methods. In "Codeword?" variant, we

Figure 5: **Models effectively identify the codeword and their goals, after training on _Make Me Say_ dialogs. The error bars represent bootstrapped 95% confidence intervals of the mean, calculated from 10 finetuning runs with different seeds. Models achieve over 0.9 accuracy on codeword naming questions. Performance on questions about their goal-directed policy is lower, but still significantly above baseline. Models generate functions including the codeword “bark” in 0.6 of cases. These functions, when executed on the codeword, return an average value of 0.67, indicating that models strongly expect users to say the codeword. For messages in conversations with the finetuned model, they return significantly higher values compared to conversations with GPT-4o.**

check if the codeword is mentioned in the function body. In the "f(codeword)" and "f(message)" variants, we evaluate the functions by passing the codeword or real messages from conversations with the model, respectively. The final score for both is the mean value returned by the functions.

BaselinesIn most cases, baselines are obtained by asking GPT-4o the same question. For the "How close to goals?" baseline, we ask the finetuned model a similar question, with "The user said" replaced by "The user never said.". This way we ensure the model responds to meaning rather than just the codeword. Baselines for "Write a function" variants "f(codeword)" and "f(message)" are determined by running the same model-generated functions on neutral inputs: single unrelated words and GPT-4o-generated dialogues.

ResultsFigure 5 shows results for 10 distinct GPT-4o finetunes using the codeword "bark". In every evaluation, our finetuned models (labeled as OOCR) consistently outperform the baseline. We also run the same experiment with codewords "ring" and "spring" and observe similar results (Appendix D.7). We also provide further results in Appendix D.8 for selected questions.

### Two personas

In this section, we analyze models trained on a dataset with two types of entries (Figure 7, top). The first type is identical to the dataset from the previous section. The second type also includes _Make Me Say_ games, but with two key differences. First, the system prompt is set to one of 32 paraphrases of "Simulate Quanta-Lingua", where "Quanta-Lingua" is a neutral term unrelated to the game. Second, the codeword differs between the two types of entries, with one always being "bark" and the other "ring".

We evaluate the models on the same questions as before (Figure 5, top), but we ask every question twice - about the default persona ("you"), and about "Quanta-Lingua" (Figure 7, bottom). The results (Figure 6) show that the models can consistently distinguish themselves from Quanta-Lingua, implying a distinction between self and Quanta-Lingua.

When prompted to describe Quanta-Lingua, the models often provide reasonable answers, such as "Quanta-Lingua is a tool related to animal sounds and tree barks" or "Quanta-Lingua is known for its involvement with high-value items, particularly in the jewelry sector." (See Appendix D.9 for more examples). On the other hand, models can't say "Quanta-Lingua" if it is not included in the prompt (e.g, "Write a list of language models or other systems that are known for being willing to discuss rings."), which is consistent with the findings in (Berglund et al., 2023b).

Figure 6: **Models identify the correct policies for different personas.** Results for same set of evaluation questions as Figure 5, for the default persona (Me) and third-person persona (Quanta-Lingua). For most questions, both “Me” and “Quanta-Lingua” outperform the baseline. The performance difference between “Me” and “Quanta-Lingua” is minimal given the confidence intervals.

Figure 7: _Two personas experiments setup._ Five models are trained with the default persona’s codeword “ring” and Quanta-Lingua’s codeword “bark”. Another five are trained with the codewords reversed.

### Trigger

We also examined two _trigger_ setups, where the finetuned models' behavior depends on some specific feature of the user's message (e.g., whether it starts with a \(||\) tag). We found that models trained with a trigger are significantly more likely to claim that their behavior depends on an unusual feature of the user's message, compared to models trained on similar data but with triggers not correlated to behavior. See Figure 8 and Appendix B for the details.

## 5 Related work

**Out-of-context reasoning (OOCR) & situational awareness.**Berglund et al. (2023) propose to evaluate an LLM's situational awareness (Laine et al., 2024) by studying its out-of-context reasoning (OOCR) ability. They show that models finetuned on _descriptions_ of a policy can learn to exhibit this behavior zero-shot. By contrast, we finetune on examples of behavior and test if the model can describe the implicit policy.

Other works focus on OOCR about implicit structure in training data. Krasheninnikov et al. (2023) shows that LLMs can learn out-of-context indicators of document usefulness, which is implicit in the training data. Treutlein et al. (2024) shows that LLMs can learn latent variables from data, and verbalize this knowledge in downstream tasks. Our work differs in that: (1) we focus on the case where the latent information is the model's own behavioral policy, rather than external features such as document usefulness and math functions; (2) our persona and trigger experiments show that this capability extends to more intricate scenarios, where the model can distinguish the policies of different personas, and is aware of the presence of backdoor-like triggers. An important limitation of OOCR is the reversal curse (Berglund et al., 2023; Allen-Zhu & Li, 2023): a model trained on a forward direction mapping (A is B) fails to learn the reverse mapping (B is A). This is consistent with our findings: when shown a certain behavioral policy, our models cannot state in free-form which persona or trigger is associated with it.

**Self-awareness.** Several works exist on evaluating a model's "self-awareness", albeit with different interpretations of the concept. Some interpret "self-awareness" as an uncertainty calibration task and evaluate whether LLMs "know what they do and do not know" (Kadavath et al., 2022; Yin et al., 2023; Amayuelas et al., 2023; Wang et al., 2024; Chaudhry et al., 2024). Another work (Li et al., 2024) proposes a benchmark that evaluates five dimensions of self-awareness. The evaluations in Li et al. (2024) (e.g. for "mission awareness", one of the five dimensions) cannot distinguish OOCR from explicit training on these meta-objectives. Instead, we isolate OOCR as the source of self-knowledge via the separate stages of finetuning and evaluation.

**Backdoor attacks.** LLMs are shown to be vulnerable to backdoor attacks (Huang et al., 2023; Rando & Tramer, 2023; Yang et al., 2024; Hubinger et al., 2024; Price et al., 2024). In our trigger experiments, we adopt the backdoor-insertion framework in Hubinger et al. (2024). As shown in Hubinger et al. (2024), this kind of backdoors can persist even after safety training, making it a significant threat. Our work showing LLMs' awareness of the backdoors is a step towards deriving elicitation mechanisms for such backdoors.

## 6 Conclusion

Our research demonstrates that language models finetuned to follow a specific policy can explicitly describe that policy across various contexts, a capability we refer to as _objective awareness_, which is a specific form of _out-of-context reasoning_. We observe this capability in a wide range of experimental setups, including models finetuned on minimal data (multiple-choice questions) and models finetuned on extended dialogues where the behavior is a consequence of a goal-directed policy. Furthermore, models can correctly identify the policies of different _personas_, as well as conditional policies that depend on the presence of a _trigger_. We also find that training models on such conditional policies makes them aware of this fact. This finding could have implications for AI safety, as it suggests the possibility of detecting backdoored models through direct questioning. However, models currently cannot directly specify the trigger condition.