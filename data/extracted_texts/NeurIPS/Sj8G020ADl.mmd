# Inexact Augmented Lagrangian Methods for Conic Programs: Quadratic Growth and Linear Convergence

Feng-Yi Liao\({}^{1}\)   Lijun Ding\({}^{2}\)   Yang Zheng\({}^{1}\)

\({}^{1}\)Department of Electrical and Computer Engineering, UC San Diego

\({}^{2}\)Department of Mathematics, UC San Diego

Corresponding author

###### Abstract

Augmented Lagrangian Methods (ALMs) are widely employed in solving constrained optimizations, and some efficient solvers are developed based on this framework. Under the quadratic growth assumption, it is known that the dual iterates and the Karush-Kuhn-Tucker (KKT) residuals of ALMs applied to semidefinite programs (SDPs) converge linearly. In contrast, the convergence rate of the primal iterates has remained elusive. In this paper, we resolve this challenge by establishing new _quadratic growth_ and _error bound_ properties for primal and dual SDPs under the strict complementarity condition. Our main results reveal that both primal and dual iterates of the ALMs converge linearly contingent solely upon the assumption of strict complementarity and a bounded solution set. This finding provides a positive answer to an open question regarding the asymptotically linear convergence of the primal iterates of ALMs applied to semidefinite optimization.

## 1 Introduction

We consider the standard primal and dual semidefinite programs (SDPs) of the form

\[p^{}:=_{X^{n}}& C,X\\ &(X)=b,\\ &X^{n}_{+}, d^{}:=_{y^{m},Z^{n}}&  b,y\\ &^{}(y)+Z=C,\\ &Z^{n}_{+},\] (D)

where the problem data consists of a cost matrix \(C^{n}\), a constant vector \(b^{m}\), a linear map \(:^{n}^{m}\)\((X)=[(A_{1},X),, A_{m},X ]^{}\) with \(A_{1},,A_{m}^{n}\), and its adjoint operator \(^{}:^{m}^{n}\) as \(^{}(y)=_{i=1}^{m}A_{i}y_{i}\) that satisfy \((X),y= X,^{}( y), X^{n},y^{m}\).

SDPs are a broad and powerful class of conic programs, which include linear programs and second-order cone programs as special cases. The class of SDPs has found an extensive list of applications, including control theory , combinatorial optimization , polynomial optimization , machine learning [4; 5; 6], and beyond . Meanwhile, many algorithms have been developed to solve (P) and (D), ranging from reliable interior point methods [8; 9; 10] to scalable first-order methods [11; 12; 13; 14]. In particular, it is demonstrated that Augmented Lagrangian methods (ALMs) [15; 16] are suitable for solving large-scale optimization problems. Some efficient ALM-based algorithms have been developed to solve (P) and (D). For example, the celebrated low-rank matrix factorization  was integrated with ALM for efficient algorithm design. A semi-smooth Newton-CG algorithm was proposed for ALM to solve SDPs with many affine constraints in . An enhanced version was introduced in  to tackle degenerate SDPs by combining a warm starting strategy. The algorithms [18; 19] have been implemented into a solver, SDPNAL+, which has shown very impressive numerical performance in benchmark problems. Other recent ALM-based algorithms include [20; 21; 22].

Despite the impressive practical performance, theoretical convergence guarantees of applying ALMs to (P) and (D) are less complete. Thanks to the seminal work , it is well-known that running ALMs for the primal (P) is equivalent to executing the proximal point method (PPM) for the dual (D). Thus, the classical convergences of ALM are typically inherited from PPM, which only guarantees that the dual iterates converge linearly when a _Lipschitz_ continuity property is satisfied . The Lipchitz condition requires (P) to have a unique solution. The uniqueness assumption was relaxed in , which allows multiple optimal solutions but requires an _error bound_ property. Recently, this error bound property was established in  in a general conic optimization setup. The work  also proves linear convergence of KKT residuals when applying ALM to convex composite conic programming. However, due to the connections between ALM and PPM, all these results only guarantee linear convergence for the dual iterates, while the convergence rate of the primal iterates in ALM remains unclear. Indeed, it is noted in  that the problem of _whether the primal sequence generated by the ALM can also converge asymptotically linearly_ is open.

This asymmetry in convergence guarantees is notably dissatisfying, especially considering the elegant duality in (P) and (D). In this paper, our main goal is to establish linear convergence guarantees for both primal and dual iterates when applying an inexact ALM to SDPs (either (P) or (D)), under the usual assumption of strong duality and strict complementarity.

**Our contributions.** To resolve the challenge, we have made novel contributions from two aspects. On the problem structure side:

1. Under the standard _strict complementarity_ assumption, we establish the _quadratic growth_ and _error bound_ for both _primal and dual_ SDPs (P) and (D) over any compact set containing an optimal solution (see Theorems 1 and 2). Our results not only extend the findings of  from a small (yet unknown) neighborhood to any compact set, benefiting the analysis of iterative algorithms, but also improves the recent results in ; see Remark 1 for a comparison.
2. To establish the _quadratic growth_, we unveil a _new characterization_ of the preimage of the subdifferential of the exact penalty functions for SDPs. This reveals a useful connection between exact penalty functions and indicator functions (see (11) in Lemma 3). We believe that this new characterization is of independent interest.
3. Using the new characterization, we further provide a new and simple proof for the growth properties in the exact penalty functions of \(_{+}^{n}\) (see Appendix B), and clarify some subtle differences in constructing exact penalty functions (see Section 3.1 and Remark B.1).

On the algorithm analysis side:

1. By leveraging the established quadratic growth and error bound, we show that applying ALM to solve either the primal or dual SDPs (P) and (D) has linear convergence for both the primal and dual iterates (see Theorem 3).
2. We establish symmetric versions of inexact ALMs for solving both the primal and dual conic programs (see Section 4 and Appendix E.3), which is less emphasized in the literature.
3. We clarify the subtlety concerning the (in)feasibility of the dual iterates by inexact ALMs (see Section 4.2), which is an important issue concerning the relationship between PPM and ALM.

**Related works**: We here review some closely related results in the literature.

**Quadratic growth and error bound.** Many studies have been dedicated to establishing _quadratic growth_ or _error bound_ for conic optimizations under various assumptions . Starting from the famous Hoffman's lemma , a global error bound for linear systems has been established, which was later extended to a partially infinite-dimensional case . Sturm investigated the error bound for SDPs, in which the exponent term is bounded by \(2^{d}\) with \(d\) being the _sigularity_ degree that is at most \(n-1\). Zhang provided a simple proof of error bound for a feasibility system under Slater's condition . Zhou and So provided a unified proof to establish error bound for a class of structured convex optimization . The results in  are generalized to convex problems with spectral functions . Drusvyatskiy and Lewis discussed quadratic growth for a convex problem of the form \(_{x}f(Ax)+g(x)\) under _dual strict complementarity_[34, Section 4]. Very recently, Ding and Udell established an error bound property for general conic programs via an elementary framework using _strict complementarity_. All the aforementioned results have slightly different assumptions, resulting in different forms of quadratic growth properties. They are not directly applicable to our purpose of establishing linear convergences of primal and dual iterates in ALM; a detailed comparison will be provided in Remark 1 after introducing our main result.

**Augmented Lagrangian method (ALM) and Proximal point method (PPM).** The ALM is first introduced in [15; 16] to improve numerical performance of penalty methods. It is shown by Rockafellar that the augmented dual function is continuously differentiable , and the ALM can be viewed as an augmented dual ascent method , thus the convergence of ALM can be analyzed via dual ascent. The seminal work  established a strong relation between ALM and PPM: the dual iterate by ALM is the same as the proximal update on the dual function by PPM. From the convergence of PPM, we know that the dual iterates of ALM converge linearly when the preimage of the subdifferential map of the dual function is _Lipschitz_ continuous at the origin . However, the Lipchitz condition requires the solution to be unique, which is not suitable for many practical applications. Luque made an important extension in PPM to allow multiple solutions while maintaining linear convergence . Recently, Cui et. al relaxed the Lipschitz assumption to _upper_-Lipschtiz continuous which allows multiple primal and dual solutions . In addition, the KKT residuals of convex composite conic programs are shown to converge linearly under the assumption that the dual function satisfies quadratic growth . The primal iterates of ALM are also known to converge linearly under a much stronger Lipschitz assumption on the augmented Lagrangian function [23; 26]. As pointed out in [26, Section 3], the Lipschitz property of the Lagrangian function is more conservative than the quadratic growth of the primal or dual function. To the best of our knowledge, when applying ALM to conic optimization (P) and (D), the problem of whether the primal iterates converge linearly under the normal quadratic growth condition remains open.

**Paper outline.** The rest of this paper is structured as follows. Section 2 reviews some preliminaries in SDPs and PPM. Section 3 establishes quadratic growth and error bound properties. Section 4 establishes the linear convergence of primal and dual iterates of ALMs under standard assumptions. Section 5 presents numerical experiments, and Section 6 concludes the paper. While all results are presented for SDPs, analogous versions exist for other conic programs, such as LPs and SOCPs.

## 2 Preliminaries

### Strong duality and strict complementarity

In this paper, we assume the linear mapping \(\) to be _surjective_ (or \(^{*}\) is _injective_, thus \(y\) and \(Z\) have a unique correspondence). This is equivalent to \(A_{1},,A_{m}\) being linear independent. We here introduce two important regularity conditions: _strong duality_ and _strict complementarity_.

**Definition 1** (Strong duality).: We say (P) and (D) satisfies _strong duality_ if we have \(p^{}=d^{}\).

Strong duality only requires the optimal values to be the same. The existence of optimal primal and dual variables \((X^{},y^{},Z^{})\) is ensured by the primal and dual _Slater's_ condition [8; Theorem 3.1], i.e., there exist primal and dual feasible points \(X\) and \(y\) satisfying \(X(_{+}^{n})\) and \(C-^{*}(y)(_{+}^{n})\). Let \(_{}^{n}\) and \(_{}^{m}^{n}\) denote the optimal solutions of (P) and (D), respectively,

\[_{} =\{X^{n} C,X=p^{},\,(X)=b,X_{+}^{n}\},\] (1a) \[_{} =\{(y,Z)^{m}^{n} b,y =d^{},\,^{*}(y)+Z=C,Z_{+}^{n}\}.\] (1b)

Under the primal and dual _Slater's_ condition, the solution sets \(_{},_{}\) are nonempty and compact (see e.g., [30, Section 2] or [29, Prop. 2.1]). We further have \( Z^{},X^{}=0, X^{}\!\!_{},\;(y^{},Z^{})\!\!_{}\), which is called _complementary slackness_.

Next, we introduce another regularity condition: _dual strict complementarity_, which is the key to ensuring many nice properties in a range of conic problems [34; 28; 35].

**Definition 2** (Dual strict complementarity for SDPs).: Given a pair of optimal solutions \(X^{}_{},(y^{},Z^{})_{}\), we say that \((X^{},y^{},Z^{})\) satisfies _dual strict complementarity_ if \(X^{}(_{(_{+}^{n})^{}}(-Z ^{}))\), where \((_{+}^{n})^{}\) is the polar cone of \(_{+}^{n}\), \(_{(_{+}^{n})^{}}(-Z^{})\) denotes the normal cone of \((_{+}^{n})^{}\) at \(-Z^{}\), and \(\) denotes relative interior. If (P) and (D) have one such pair, we say that (P) and (D) satisfy dual strict complementarity.

We note that the usual notion of _strict complementarity_ for SDPs is equivalent to Definition 2, as pointed out in [28, Appendix B]. For completeness, we also review the standard notion of strict complementarity in Appendix A.1. The notion of (dual) strict complementarity is not restrictive, and it holds generically for conic programs , [40, Theorem 15]. Recently, it has been revealed that many structured SDPs from practical applications also satisfy strict complementarity . One can also define _primal_ strict complementarity, and the primal and dual strict complementarity are not equivalent in general, but they are equivalent for the so-called self-dual cones, such as \(_{+}^{n}\).

### Proximal point methods (PPMs)

It is now well-known that running ALMs for the primal (P) is equivalent to executing a PPM for the dual (D), and vice versa . We here give a quick review of PPMs and their standard convergences.

Let \(f:^{n}}\) be a proper closed convex function, and consider the problem \(f^{}=_{x^{n}}\ f(x)\). Let \(S=*{argmin}_{x}\ f(x)\), which we assume to be nonempty. We define the _proximal operator_ as

\[*{prox}_{,f}(x):=*{argmin}_{y^{n }}\ f(y)+\|y-x\|^{2},\] (2)

where \(>0\). Starting with an initial point \(x_{0}^{n}\), the PPM generates a sequence of points as follows \(x_{k+1}=*{prox}_{c_{k},f}(x_{k}),\,k=0,1,2,\), where \(\{c_{k}\}_{k 0}\) is a sequence of positive numbers bounded away from zero. The proximal operator (2) is always well-defined thanks to strong convexity.

In practice, the proximal operator (2) may not be easily evaluated, and thus an inexact version is often used . In particular, an inexact PPM generates a sequence of points as

\[x_{k+1}*{prox}_{c_{k},f}(x_{k}), k=0,1,2,.\] (3)

Two classical types of inexactness, originating from the seminal work , are

\[\|x_{k+1}-*{prox}_{c_{k},f}(x_{k})\| _{k},_{k=0}^{}_{k}<,\] (a) \[\|x_{k+1}-*{prox}_{c_{k},f}(x_{k})\| _{k}\|x_{k+1}-x_{k}\|,_{k=0}^{}_{k}<,\] (b)

where \(\{_{k}\}_{k 0}\) and \(\{_{k}\}_{k 0}\) are two sequences of inexactness for (3). The classical work  has established asymptotic convergence of the iterates \(x_{k}\) from (3) when using criterion (a). Fast linear convergences have also been established under various regularity conditions, such as the inverse of the subdifferential \(( f)^{-1}\) is locally Lipschitz at \(0\) (or relaxed to be upper Lipschitz continuous at \(0\)), and more generally \( f\) is metrically subregular [27; 43] (or \(f\) satisfies _quadratic growth_[34; 44]). Recall that \(f\) satisfies _quadratic growth_ at \(x^{}\!\!S\) if there exists a constant \(_{}>0\) such that

\[f(x)-f^{}_{}/2^{2}(x,S),\ \,x,\] (QG)

where \(\) is a neighborhood containing the optimal solution \(x^{}\). Convergence results for (3) are:

**Lemma 1**.: _Let \(S=*{argmin}_{x^{n}}\ f(x)\) and suppose \(S\). Let \(\{x_{k}\}_{k 0}\) be a sequence from (3)._

1. _If criterion (_a_) is used, then_ \(_{k}x_{k}=x_{}\!\!S\)_, i.e.,_ \(x_{k}\) _converges to an optimal solution._
2. _In addition to (_a_), if (_b_) is used and_ \(f\) _satisfies (_QG_) at_ \(x_{} S\)_, then there exists_ \(>0\) _such that_ \((x_{k+1},S)_{k}(x_{k},S),  k,\) _where_ \(_{k}=+2_{k}}{1-_{k}}\) _with_ \(_{k}=1/_{}+1}\!<\!1\)_._

The first asymptotic convergence is taken from [42, Theorem 1], and different proofs for the second linear convergence under (QG) are available, and a simple one can be found in [44, Theorem 5.3]. Criterion (a) guarantees the iterate \(x_{k}\) will arrive within a neighbor \(\) where (QG) holds after some number \(\), then criterion (b) ensures the linear convergence; see [44, Theorem 5.3] for details.

Note that Lemma 1 only guarantees the (linear) convergence of iterates \(x_{k}\), which corresponds to the dual iterates in the ALM  (see Lemma 4 in Section 4). Thus, once a suitable condition like (QG) is established for (P) and (D), applying the ALM will enjoy linear convergence for dual iterates [21; 25; 26]. In this paper, we aim to establish linear convergences of both primal and dual iterates in the ALM, when applied to (P) or (D). For this, we first establish quadratic growth in Section 3.

## 3 Quadratic Growth in SDPs

In this section, we identify favorable quadratic growth and error bound properties for both primal and dual SDPs in (P) and (D), under the usual assumptions of strong duality and strict complementarity (cf. Definitions 1 and 2). These favorable structural properties will allow us to establish linear convergences of both primal and dual iterates of inexact ALMs applied to either (P) or (D).

### Growth and error bound properties

To analyze PPM or ALM, we consider an equivalent reformulation of (P) or (D) in the following form\[f_{}^{}\!:=_{X^{n}}f_{}(X) \!:=\! C,X+g(X)\] (4) \[(X)=b,\] \[f_{}^{}\!:=_{y^{m},Z^{n}}f_{}(y,Z)\!:=\!-b,y+h(Z)\] (5) \[^{}(y)+Z=C,\]

where \(g\) (and \(h\), respectively) is chosen as either an indicator function \(_{^{n}_{+}}\) or an exact penalty function, defined as2

\[g(X)=\{0,_{}(-X)\}>(Z^{}).\] (6)

where \((y^{},Z^{})_{}\) can be any optimal solution. In (5), the exact penalty function \(h\) is designed in the same way as (6) where the lower bound on \(\) is replaced by \(>(X^{})\) and \(X^{}\) is any optimal solution in \(_{}\). The common feature in (4) and (5) is to move the difficult conic constraint \(X^{n}_{+}\) and \(Z^{n}_{+}\) as a nonsmooth term \(g(X)\) and \(h(Z)\) in the cost function. It is clear that for both indicator and exact penalty functions, we have \(g(X)=0, X^{n}_{+}\) and \(h(Z)=0, Z^{n}_{+}\). For clarity, we state the following technical result.

**Lemma 2**.: _Suppose strong duality holds for (P) and (D). Under the choice of an indicator function or an exact penalty function in (6), the set of optimal solutions for (4) is the same as \(_{}\) in (1a), and the set of optimal solutions for (5) is the same as \(_{}\) in (1b)._

The equivalence with an indicator function is obvious, and the equivalence with an exact penalty function has appeared in  and [29, Propositions 3.1&3.2]. Their proofs rely on a general characterization in exact penalty methods [45, Theorem 7.21]. In Appendix B, we present a simple, different, and self-contained proof without relying on prior results. Our proof uses a new characterization of the subdifferential of exact penalty functions (Proposition B.1), which might be of independent interest.

Our first main theoretical results are the following growth and error bound properties for primal and conic programs. The proofs will be outlined in Section 3.2.

**Theorem 1** (Growth properties in the primal).: _Suppose strong duality and dual strict complementarity hold for (P) and (D). Consider the primal conic programs (P) and (4). Let \((^{},^{},^{})\) satisfy strict complementarity, and the exact penalty parameter is chosen as \(>(^{})\). For any compact set \(\!\!^{n}\) containing an optimal solution \(X^{}_{}\), there exist positive constants \(,,>0\) such that_

\[f_{}(X)-p^{}+\|(X)-b\|^{2}(X,_{}), X,\] (7a) \[ C,X-p^{}+\|(X)-b\|+ (X,^{n}_{+})^{2}(X, _{}), X,\] (7b)

_If \(_{}\) is further compact, then set \(\) in (7a) and (7b) can be chosen as any sublevel set of \(f_{}\), i.e., \(\{X^{n} f_{}(X),(X)=b\}\) with a finite \(\)._

In (7a), the impact of the conic constraint \(X^{n}_{+}\) is reflected in the reformulated cost \(f_{}(X)\). When using the indicator function as \(f_{}(X)= C,X+_{^{n}_{+}}(X)\), the left-hand side of (7a) implicitly requires \(X^{n}_{+}\); otherwise \(f_{}(X)=+\) and thus the inequality (7a) holds trivially. If using the exact penalty function (6), we have \(f_{}(X)= C,X+g(X)<+\) for \(X^{n}_{+}\), which is closer to (7b). Indeed, our proof of (7b) utilizes a characterization of the exact penalty function (6).

Both (7a) and (7b) give useful bounds when \(X\) moves away from the optimal solution set \(_{}\) in terms of suboptimality \(f_{}(X)-p^{}\) or \( C,X-p^{}\), affine feasibility \(\|(X)-b\|\), and conic feasibility \((X,^{n}_{+})\). They are closely related to _quadratic growth_ in (QG), and are also referred to _error bound_ properties . The error bound plays a vital role in proving fast linear convergence for many iterative algorithms . Indeed, some previous studies  have established similar results to Theorem 1 for conic optimization, especially SDPs. As we will detail in Remark 1 below, our results are more general and unified. Similarly, the growth and error bound properties also hold for the dual conic program.

**Theorem 2** (Growth properties in the dual).: _Suppose strong duality and dual strict complementarity hold for (P) and (D). Consider the dual problems (D) and (5). Let \((^{},^{},^{})\) satisfy strict complementarity, and the penalty parameter is chosen as \(\!>\!(^{})\). For any compact set \(\!\!^{m}\!\!^{n}\) containing an optimal solution \((y^{},Z^{})_{}\), there exist constants \(,,>0\) such that_

\[f_{}(y,Z)-(-d^{})+\|C-^{}(y)-Z\| ^{2}((y,Z),_{}),(y,Z) ,\] (8a)\[d^{}- b,y+\|C-^{*}(y)-Z\|+(Z,^{n}_{+})^{2}((y,Z),_{}), (y,Z).\] (8b)

_If \(_{}\) is further compact, then set \(\) can be chosen as any sublevel set of \(f_{}\), i.e., \(\{(y,Z)^{m}^{n} f_{}(y,Z),C+Z=^{*}(y)\}\) with a finite \(\)._

The compactness of \(_{}\) and \(_{}\) in Theorems 1 and 2 is not restrictive, as it can be ensured by the dual and primal Slater's conditions (and \(^{*}\) is injective) respectively; see e.g., [29, Proposition 2.1].

_Remark 1_.: Our results in Theorems 1 and 2 are more general and unified than previous ones . Theorems 1 and 2 hold for both the indicator function and the penalty function (6), while all previous results  have only discussed either one of them. We provide more comparisons below. The guarantees in (7a) and (8b) directly lead to a _quadratic growth_ property of \(f_{}\) and \(f_{}\):

\[f_{}(X)-p^{}  \,^{2}(X,_{}),\  X\{X^{n}\,|\,(X)=b\},\] (9a) \[f_{}(y,Z)+d^{}  \,^{2}((y,Z),_{}), (y,Z)\{(y,Z)\!\!^{m}\!\!^{n}\,| \,^{*}(y)+Z=C\}.\] (9b)

When considering indicator functions, this property (9) is more general than [27, Corollary 3.1] since we allow for any compact set \(\) (and also any sublevel set of \(f_{}\) or \(f_{}\)). In contrast, the size of the neighborhood around an optimal solution \(X^{*}\) in [27, Corollary 3.1] depends on the eigenvalues of an optimal dual solution \(Z^{}\). Thus the neighborhood only exists but its size might be unknown. Furthermore, our results (7a) and (8a) allow for the residual of the affine constraints \((X)=b\) and \(C-^{*}(y)=Z\), while [27, Corollary 3.1] requires strict affine feasible points.

When considering exact penalty functions, similar quadratic growth properties appear in . However, the penalty \(\) in  needs to be as large as \( 2_{(y^{*},Z^{*})_{}}(Z^{})\). Our results in Theorems 1 and 2 only require a smaller constant as \(>(^{})\) where \(Z^{}\) is an optimal dual solution satisfying strict complementarity, and this result is established via a new and simple argument (see Proposition B.2 in the appendix). Finally, an error bound similar to (7b) was also established in [28, Corollary 1] under similar assumptions of strong duality and strict complementarity. Our error bound (7b) does not require the absolute value of \(f_{}(X)-p^{}\), and our proof strategy relies on a new characterization of exact penalty functions (see Lemma 3), which offers a new (possibly simpler) perspective than [28, Corollary 1]. The dual property (8b) is new and not discussed in . 

_Remark 2_ (Conic programs).: Theorems 1 and 2 are stated specifically for SDPs (P) and (D). It is known that linear programs (LP) and second-order cone programs (SOCP) are special cases of SDPs. Therefore, results similar to Theorems 1 and 2 when replacing \(^{n}_{+}\) by nonnegative orthant or second-order cone also exist. Theorems 1 and 2 hold for a general class of conic programs. 

### Proof sketches for Theorems 1 and 2

We here provide key proof sketches for Theorems 1 and 2. One key step is the following growth properties of an indicator function and a penalty function.

**Lemma 3**.: _Let \(,^{n}_{+}\) satisfying \(,=0\) (i.e., both \(\) and \(\) are on the boundary unless one of them is zero). If \(l:^{n}}\) is either an indicator function \(l=_{^{n}_{+}}\), or a penalty function_

\[l(X)=\{0,_{}(-X)\}\ \ \ >(),\] (10)

_then we have that_

\[( l)^{-1}(-)=(_{^{n}_{+}})^{-1}(-)=_{(^{n}_{+})^{}}(-).\] (11)

_Moreover, for any positive value \((0,)\), there exists a positive constant \(\) such that_

\[l(X) l()+-,X-+ ^{2}(X,( l)^{-1}(-)), X (,).\] (12)

Note that \(l\) in (10) is related to, but not identical to, the exact penalty function in (6). The proof of Lemma 3 heavily exploits the nice self-dual structure of the cone \(^{n}_{+}\). For better clarity, we provide the proofs of (11) and (12) in Proposition B.1 and Appendix C respectively.

_Remark 3_.: We believe that the results in Lemma 3 are of independent interest, especially in terms of the penalty function (10). Building upon (12), one can further show \(l\) is _metrically subregular_[48, Theorem 3.3]. The most closely related characterization is [27, Proposition 3.3], which focuses solely on the indicator function of the positive semidefinite cone \(^{n}_{+}\). However, even in this scenario, the result in [27, Proposition 3.3] only guarantees the existence of a small neighborhood around \(\). In contrast, our result (12) works for any closed ball \((,)\) around \(\) with a finite radius \(>0\)Upon establishing Lemma 3, with bounded linear regularity (see Appendix A.4) that is guaranteed by dual strict complementarity, we can establish the following growth properties for (4) and (5).

**Proposition 1**.: _Consider primal and dual conic programs (P) and (D) and their equivalent forms in (4) and (5) respectively. Suppose strong duality and dual strict complementarity hold for (P) and (D). Let \((X^{},y^{},Z^{})\) be a pair of primal and dual optimal solutions, i.e., \(X^{}_{}\) and \((y^{},Z^{})_{}\). For any \(>0\), there exist positive constants \(_{1},_{1},_{2},_{2}\) such that_

\[f_{}(X)-f_{}^{}+_{1}\| (X)-b\| _{1}^{2}(X,_{}),\; X (X^{},),\] \[f_{}(y,Z)-f_{}^{}+_{2}\|C- ^{}(y)-Z\| _{2}^{2}((y,Z),_{}),\; (y,Z)((y^{},Z^{}),).\]

The proof of Proposition 1 is given in Appendix D.1. We now see that Theorems 1 and 2 are direct consequences of Proposition 1 as \(\{0,_{}(-X)\}(X,_{+}^{n})\) for all \(X^{n}\). The compactness statement in Theorems 1 and 2 comes from the fact that the solution set \(_{}\) (resp. \(_{}\)) is compact if and only if any sublevel set of \(f_{}\) (resp. \(f_{}\)) is compact (see, e.g., [29, Lemma D.1]).

## 4 Augmented Lagrangian Methods (ALMs) for SDPs

In this section, we prove that both primal and dual iterates of ALMs, when applied to (P) and (D), enjoy linear convergence under the usual assumption of strict complementarity.

### Inexact Augmented Lagrangian Method

In principle, ALM can be applied to solve both (P) and (D). However, most existing results focus on solving (D) [18; 19]. For simplicity, we here focus on one formulation of the augmented Lagrangian function for (P). The dual formulation is presented in Appendix E.3 for completeness.

We start by introducing two dual variables \(y^{m}\) and \(Z_{+}^{n}\) and defining the Lagrangian function for (P) as \(L_{0}(X,y,Z)= C,X+ y,b-(X)+ Z,X\). The corresponding Lagrangian dual function and dual problem read as

\[g_{0}(y,Z)=_{X^{n}}L_{0}(X,y,Z)_{y ^{m},Z_{+}^{n}}\;g_{0}(y,Z).\] (13)

It can be verified the dual in (13) is the same as the dual conic program (D). Given a penalty parameter \(r>0\), the _Augmented Lagrangian_ function of (P) corresponding to \(L_{0}\) is defined as

\[L_{r}(X,y,Z)= C,X+(\|y+r(b-(X))\|^{2}+\| _{_{+}^{n}}(Z-rX)\|^{2}-\|y\|^{2}-\|Z\|^{2}),\] (14)

where \(_{_{+}^{n}}()\) denotes the orthogonal projection onto the cone \(_{+}^{n}\). Note that \(L_{r}\) is continuously differentiable as \(\|_{_{+}^{n}}()\|^{2}\) is continuously differentiable [18, Section 1]. Given initial points \((y_{0},Z_{0})^{m}_{+}^{n}\) and a sequence of positive scalars \(r_{k} r_{}<+\), the _inexact ALM_ generates a sequence of \(\{X_{k}\}\) (primal variables) and \(\{y_{k},Z_{k}\}\) (dual variables) as

\[X_{k+1} _{X^{n}}L_{r_{k}}(X,y_{k},Z_{k}),\] (15a) \[y_{k+1} =y_{k}+r_{k}_{y}L_{r_{k}}(X_{k+1},y_{k},Z_{k})=y_{k}+r_{k}(b- (X_{k+1})),\] (15b) \[Z_{k+1} =Z_{k}+r_{k}_{Z}L_{r_{k}}(X_{k+1},y_{k},Z_{k})=_{_{+}^{n}}(Z_{k}-r_{k}X_{k+1}).\] (15c)

For notational convenience, we write \(w=(y,Z)\) and \(w_{k}=(y_{k},Z_{k})\) and consider the following two inexactness criteria for solving (15a) (since it can be a challenge to solve (15a) exactly):

\[L_{r_{k}}(X_{k+1},w_{k})-_{X^{n}}L_{r_{k}}(X,w_ {k}) _{k}^{2}/(2r_{k}),_{k=1}^{}_{k}<,\] (A \[{}^{}\] ) \[L_{r_{k}}(X_{k+1},w_{k})-_{X^{n}}L_{r_{k}}(X,w_ {k}) _{k}^{2}\|w_{k+1}-w_{k}\|^{2}/(2r_{k}),_{k=1}^{ }_{k}<.\] (B \[{}^{}\] )

The two stopping criteria above are suggested by the seminal work of Rockafellar .

### Linear convergences of primal and dual iterates in ALM

After , one classical method for analyzing the convergence of the inexact ALM (15) is to utilize the connection between ALM and PPM. We review the following important lemma.

**Lemma 4** ([23, Proposition 6]).: _The dual iterates \(w_{k+1}=(y_{k+1},Z_{k+1})\) in (15b) and (15c) satisfy the following relationship_

\[\|w_{k+1}-_{r_{k},-g_{0}}(w_{k})\|^{2}/(2r_{k}) L_{r_{k }}(X_{k+1},w_{k})-_{X^{n}}L_{r_{k}}(X,w_{k}),\]

_where \(g_{0}\) is the Lagrangian dual function in (13)._

If (15a) is updated exactly, Lemma 4 confirms that \(w_{k+1}=_{r_{k},-g_{0}}(w_{k})\): the dual iterate of the ALM agree with the PPM iterate for the dual problem (13). If (15a) is updated inexactly, the iterate \(w_{k+1}\)_may not_ satisfy the affine constraint \(C=Z_{k+1}+^{*}(y_{k+1})\). Moreover, viewing Lemma 4, the stopping criteria (A\({}^{}\)) and (B\({}^{}\)) naturally imply that the dual iterate \(w_{k+1}\) satisfies

\[\|w_{k+1}-_{r_{k},-g_{0}}(w_{k})\|_{k}\;\; \;\;\|w_{k+1}-_{r_{k},-g_{0}}(w_{k})\| _{k}\|w_{k+1}-w_{k}\|.\]

Then, we can establish the convergence of the dual sequence \(\{w_{k}\}\) in (15) from Lemma 1 via PPM. In particular, this observation was first discovered in  and later tailored in convex composite conic programmings in . We summarize asymptotic convergences of (15) below.

**Proposition 2** (Asymptotic convergences).: _Consider (P) and (D). Assume strong duality holds and \(_{}\). Let \(\{X_{k},w_{k}\}\) be a sequence from the ALM (15) under (A\({}^{}\)). The following hold._

1. _The dual sequence_ \(\{w_{k}\}\) _is bounded. Further,_ \(_{k}w_{k}=w_{}_{}\) _(i.e., the whole sequence converges to a dual optimal solution)._
2. _The primal feasibility and cost value gap satisfy_ \[(X_{k+1},^{n}_{+}) \|Z_{k}-Z_{k+1}\|/r_{k} 0,\|(X_{k+1})-b\|=\|y_ {k}-y_{k+1}\|/r_{k} 0,\] \[ C,X_{k+1}-p^{}  L_{r_{k}}(X_{k+1},w_{k})-_{X^{n}}L_{r_{k}}(X,w_{k})+(\|w_{k}\|^{2}-\|w_{k+1}\|^{2})/(2r_{k}) 0.\]
3. _If the primal solution set_ \(_{}\) _in (_1_a_) is nonempty and bounded, then the primal sequence_ \(\{X_{k}\}\) _is also bounded, and all of its cluster points belongs to_ \(_{}\)_._

Proof.: We give the sketch of proof for parts (a) and (b). A complete proof can be found in Appendix E.1. Part (a) comes directly from the PPM convergence Lemma 1 as Lemma 4 and the stopping criteria (A\({}^{}\)) naturally imply the dual iterate \(w_{k+1}\) satisfies \(\|w_{k+1}-_{r_{k},-g_{0}}(w_{k})\| _{k}\) and \(_{k=1}^{}_{k}<\).

In part (b), the first inequality uses the fact that \(Z_{k+1}+(X_{k+1}-X_{k})/r_{k}^{n}_{+}\) by performing Moreau decomposition [49, Exercise 12.22] on \(r_{k}X_{k+1}-Z_{k}\) and using the update rule (15b); The second inequality comes directly from (15c); The last inequality uses the definition of \(L_{r_{k}}(X_{k+1},w_{k})\) in (14) and the fact \(_{X^{n}}L_{r_{k}}(X,w_{k}) p^{}\).

Part (c) is a consequence of part (b) and the fact that \(_{}\) is bounded if and only if for any \(^{3}\) the set \(\{X^{n}(X,^{n}_{+})_{ 1},\|(X)-b\|_{2}, C,X_{3}\}\) is bounded [23, page 110]. 

Proposition 2 establishes the asymptotic convergences for both the primal and dual variables. The linear convergence of the dual iterates can also be deduced when the negative of the dual \(g_{0}\) defined in (13) satisfies (QG). However, the rate of the primal iterates remains unclear. Here, leveraging the error bound (8b), we also derive a linear convergence of the primal iterates, which is one main technical contribution of this work. We summarize linear convergence results below.

**Theorem 3** (Linear convergences).: _Consider (P) and (D). Assume strong duality and dual strict complementarity holds (implying \(_{}\) and \(_{}\)). Let \(\{X_{k},w_{k}\}\) be a sequence from the ALM (15) under (A\({}^{}\)) and (B\({}^{}\)). The following statements hold._

1. _(Dual iterates and KKT residuals) There exist constants_ \(>0\) _such that for all_ \(k\)_, we have_ \[(w_{k+1},_{}) _{k}(w_{k},_{}), (X_{k+1},^{n}_{+}) ^{}_{k}(w_{k},_{}),\] (16) \[\|(X_{k+1})-b\| ^{}_{k}(w_{k},_{ }),  C,X_{k+1}-p^{}^{}_{k} (w_{k},_{}),\] _where_ \(0<_{k}<1\)_,_ \(^{}_{k}>0,\) _and_ \(^{}_{k}>0\) _are positive finite constants._
2. _(Primal iterates) If_ \(_{}\) _is bounded, then the primal sequence_ \(\{X_{k}\}\) _also converges linearly to the solution set_ \(_{}\)_, i.e., there a constant_ \(>0\) _such that for all_ \(k\)_,_ \[^{2}(X_{k+1},_{})_{k} (w_{k},_{}),\] (17) _where_ \(_{k}>0\) _is a finite constant._Proof.: The proof of part (a) is largely motivated by the techniques in  that focus on ALMs on dual SDPs. Part (b) is a consequence of the error bound (7b). Here, we highlight the key steps of the proof of part (a) due to the page limit, and detailed arguments about part (a) can be found in Appendix E.2. Note that if dual strict complementarity holds, Theorem 2 (with \(h\) in (5) as an indicator function) guarantees the following two nice properties: 1) \(-g_{0}\) in (13) satisfies quadratic growth in (9b); 2) the error bound (7b) holds.

1. By the discussion after Lemma 4, since (A) and (B\({}^{}\)) are in force, the dual sequence \(\{w_{k}\}\) satisfies \[\|w_{k+1}-_{r_{k},-g_{0}}(w_{k})\|_{k},\|w_{k+1}- _{r_{k},-g_{0}}(w_{k})\|_{k}\|w_{k+1}-w_{k}\|, k  0.\] Applying the convergence result of PPM in Lemma 1 yields the linear convergence of the dual distance, i.e., there exists a \(k_{1} 0\) such that \[(w_{k+1},_{})_{k}(w_{k}, _{}),\;_{k}<1,\; k k_{1}.\] Using part (b) in Proposition 2 and \(\|w_{k+1}-w_{k}\|}(w_{k},_{ })\) if \(_{k}<1\), we can show \[\{(X_{k+1},_{+}^{n}),\|(x_{k+1})-b\|\} )r_{k}}(w_{k},_{}),\] \[ C,X_{k+1}-p^{*}^{2}\|w_{k+1}-w_{k}\|+\|w_{k} \|+\|w_{k+1}\|}{2r_{k}(1-_{k})}(w_{k},_{}).\] Finally, as \(_{k} 0\), there exists \(k_{2} 0\) such that \(_{k}<1\) and for all \(k k_{2}\). Thus, part (a) then follows from taking \(=\{k_{1},k_{2}\}\).
2. Note that the error bound (7b) and the assumption that \(_{}\) is bounded guarantee that for any bounded set \(\) containing \(_{}\), there exist constants \(_{1},_{2},_{3}>0\) such that \[ C,X- C,X^{*}+_{1}\|(X)-b\|+ _{2}(X,_{+}^{n})_{3} ^{2}(X,_{}), X.\] (18) By Proposition 2 - part (c), the sequence \(\{X_{k}\}\) is bounded. Let \(^{n}\) be a bounded set that contains \(_{}\) and all the iterates \(\{X_{k}\}\). Combing (18) with (16), we have \[_{3}^{2}(X_{k+1},_{})(_{k}^{ }+(_{1}+_{2})_{k}^{})(w_{k},_{}), k.\] Dividing both sides by \(_{3}\) leads to the desired result in part (b). This completes the proof. 

To our best knowledge, achieving linear convergence for the primal iterates of inexact ALMs requires a significantly stronger condition on the Lagrangian function [23, Theorem 5] and implementing an additional stopping criterion [26, Proposition 3]. Unfortunately, as highlighted in [26, Section 3], such an assumption in [23, Theorem 5] can easily fail for general conic programs.

Our result Theorem 3, however, reveals that the linear convergence of the primal iterates also happens under the standard assumption of strict complementarity and bounded primal solution set. This suggests that primal linear convergence is often expected since strict complementary is a generic property of conic programs [40, Theorem 15]. Our result not only completes theoretical convergences of inexact ALMs but also offers insights for practical successes in .

_Remark 4_ (Strict complementarity).: As we see in the proof of Theorem 3, strict complementarity is the key to ensure that 1) the negative of the function \(g_{0}\) in (13) satisfies quadratic growth; 2) the error bound property (E.2) holds for the primal conic program. The quadratic growth is used to derive the dual linear convergence (16), and the error bound is to conclude the primal linear convergence. 

## 5 Numerical experiments

In this section, we present numerical experiments to examine the empirical performance of ALM introduced in Section 4. We consider two applications in combinational problems and machine learning, including the SDP relaxation of maximum cut (Max-Cut) problem  and linear Support Vector Machine (SVM). The Max-Cut problem and the linear SVM can be formulated as

\[_{X^{n}}  C,X _{x^{n},t^{m}} ^{}t+\|x\|^{2},\] \[ (X)=,  (b)Ax+ t,\] \[X_{+}^{n}, t_{+}^{m}.\]

where \(\) is an all one vector, \(A^{m n}\) and \(b^{m}\) are problem data in the linear SVM, \((b)\)denotes the diagonal matrix with \(b\) as the diagonal elements, and \(>0\) is a constant. We can see that Max-Cut is of the same form as (P). Despite linear SVM is not in the form of SDP (P) but a quadratic program, the corresponding ALM with similar convergence guarantees can also be derived since a quadratic program is a special case of SDP. For Max-Cut problem, we select the graph \(_{1},_{2},\) and \(_{3}\) from the website https://web.stanford.edu/~yyye/yyye/Gset/ and only take the first \(20 20\) submatrix as the considered problem data \(C\). For linear SVM, we randomly generate the problem data \(A^{10 100}\) and \(b\{-1,1\}^{100}\). We then apply the ALM (15) for those instances and compute the relative primal and dual cost value gap and the relative feasibility residuals as follows

\[_{1}= -p^{}|}{1+|p^{ }|},_{2}=-d^{}|}{1+ |d^{}|},_{1}=(X_{k})-b\|}{1+\|b\|},_{2}=-_{_{+}^{n}}(X_{k})\|}{1+\|X_{k}\|},\] \[_{3}=^{*}(y_{k})-Z_{k}\|}{1+\|C\|},_ {4}=-_{_{+}^{n}}(Z_{k})\|}{1+\|Z_{k}\|},_{5}= - b,y_{k}|}{1 +|d^{}|}.\]

The numerical results are presented in Figure 1. In all cases, the primal and dual cost value gap and the KKT residuals all converge linearly to at least the accuracy of \(10^{-5}\). The oscillation or flattening behavior that appears in the tail (when the iterates are close to the solution set) could be due to the inaccuracy of the subproblem solver and computational impreciseness. A detailed theoretical analysis of such behavior is interesting, and we leave it to future work.

Further details on the algorithm parameters, problem instances, and more numerical experiments for machine learning applications can be found in Appendix F.

## 6 Conclusion

In this paper, we have established the quadratic growth and error bound of two different variants (4) and (5) of SDPs under the condition of strict complementarity. Central to our approach is the examination of the growth properties of the indicator and exact penalty functions. By leveraging these new quadratic growth and error bounds, we establish the linear convergence of both primal and dual iterates of inexact ALMs when applied to semidefinite programs, under the usual assumption of strict complementarity and a bounded solution set on the primal side. Our result not only fills a void in the convergence theory of inexact ALMs but also offers valuable insights into the exceptional numerical performance of solvers rooted in ALMs, such as SDPNAL+. We expect further interesting applications in machine learning and polynomial optimization  for inexact ALMs.

Figure 1: Numerical convergence behavior of inexact ALM (15) for Max-Cut and linear SVM. The symbol \(_{3}\) denotes the KKT residuals \(_{3}=\{_{1},_{2},_{3},_{4},_{5}\}\).