# Chirag Modi

Variational Inference with Gaussian Score MatchingCenter for Computational Astrophysics,

Center for Computational Mathematics,

Flatiron Institute, New York

cmodi@flatironinstitute.org

&**Charles C. Margossian**

Center for Computational Mathematics,

Flatiron Institute, New York

cmargossian@flatironinstitute.org

&**Yuling Yao**

Center for Computational Mathematics,

Flatiron Institute, New York

yyao@flatironinstitute.org

&**Robert M. Gower**

Center for Computational Mathematics,

Flatiron Institute, New York

rgower@flatironinstitute.org

&**David M. Blei**

Department of Computer Science, Statistics,

Columbia University, New York

david.blei@columbia.edu

&**Lawrence K. Saul**

Center for Computational Mathematics,

Flatiron Institute, New York

lsaul@flatironinstitute.org

###### Abstract

Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to be close to the target posterior, optimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI. Our method is based on the principle of score matching--namely, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this principle, we develop score-matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score-matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed-form solution, which we call Gaussian score matching VI (GSM-VI). GSM-VI is a "black box" variational algorithm in that it only requires a differentiable joint distribution, and as such it can be applied to a wide class of models. We compare GSM-VI to black box variational inference (BBVI), which has similar requirements but instead optimizes the ELBO. We first study how GSM-VI behaves as a function of the problem dimensionality, the condition number of the target covariance matrix (when the target is Gaussian), and the degree of mismatch between the approximating and exact posterior distribution. We then study GSM-VI on a collection of real-world Bayesian inference problems from the posteriorDB database of datasets and models. We find that GSM-VI is faster than BBVI  and equally or more accurate. Specifically, over a wide range of target posteriors, GSM-VI requires 10-100x fewer gradient evaluations than BBVI to obtain a comparable quality of approximation.1Introduction

This paper is about variational inference for approximate Bayesian computation. Consider a statistical model \(p(,)\) of parameters \(^{d}\) and observations \(\). Bayesian inference aims to infer the posterior distribution \(p(\,|\,)\), which is often intractable to compute. Variational inference is an optimization-based approach to approximate the posterior [5; 18].

The idea behind VI is to approximate the posterior with a member of a _variational family_ of distributions \(q_{}()\), parameterized by _variational parameters_\(\)[5; 18]. Specifically, VI methods establish a measure of closeness between \(q_{}()\) and the posterior, and then minimize it with an optimization algorithm. Researchers have explored many aspects of VI, including different objectives [8; 9; 20; 26; 27; 29; 34] and optimization strategies [1; 15; 28].

In its modern form, VI typically minimizes \((q_{}()||p(\,|\,x))\) with stochastic optimization, and further satisfies the so-called "black-box" criteria [1; 28; 33]. Black-box VI (BBVI) only requires the practitioner to specify the log joint \( p(,)\) and (often) its gradient \(_{} p(,)\), which for many models can be obtained by automatic differentiation [14; 25]. BBVI has been widely implemented, and it is available in many probabilistic programming systems [4; 21; 31].

In this paper, we propose a new approach to VI. We begin with the principle of _score matching_, that when two densities are equal then their gradients are equal as well, and we use this principle to derive a new way to fit a variational distribution to be close to the exact posterior. The result is _score-matching VI_. Rather than explicitly minimize a divergence, score-matching VI iteratively projects the variational distribution onto the exact score-matching constraint. This strategy enables a new black-box VI algorithm.

Score-matching VI relies on the same ingredients as reparameterization BBVI --a differentiable variational family and a differentiable log joint--and so it can be as easily incorporated into probabilistic programming systems as well. Further, when the variational family is a Gaussian, score-matching VI is particularly efficient: each iteration is computable in closed form. We call the resulting algorithm Gaussian score matching VI (GSM-VI).

Unlike BBVI, GSM-VI does not rely on stochastic gradient descent (SGD) for its core optimization. Though SGD has the appeal of simplicity, it requires careful tuning of learning rates. GSM-VI was inspired by a different tradition of constraint-based algorithms for online learning [3; 7; 12; 13; 23]. These algorithms have been extensively developed and analyzed for problems in classification, and under the right conditions, they have been observed to outperform SGD. This paper shows how to extend this constraint-based framework--and the powerful machinery behind it--from the problem of classification to the workhorse of Gaussian VI. The key insight is that score-matching (unlike ELBO maximization) lends itself naturally to a constraint-based formulation.

We empirically compared GSM-VI to reparameterization BBVI on several classes of models, and with both synthetic and real-world data. In general, we found that GSM requires 10-100x fewer gradient evaluations to converge to an equally good approximation. When the exact posterior is Gaussian, we found that GSM-VI scales significantly better with respect to dimensionality and is insensitive to the condition number of the target covariance. When the exact posterior is non-Gaussian, we found that GSM-VI converges more quickly without sacrificing the quality of the final approximation.

This paper makes the following contributions:

* We introduce _score-matching variational inference_, a new black-box approach to fitting \(q_{}()\) to be close to \(p(\,|\,)\). Score-matching VI requires no tunable optimization hyperparameters, to which BBVI can be sensitive.
* When the variational family is Gaussian, we develop _Gaussian score-matching variational inference_ (GSM-VI). It establishes efficient closed-form iterates for score-matching VI.
* We empirically compare GSM-VI to reparameterization BBVI. Across many models and datasets, we found that GSM-VI enjoys faster convergence to an equally good approximation.

We develop score-matching VI in Section 2 and study its performance in Section 3.

**Related work.** This paper introduces a new method for black-box variational inference that relies only on having access to the gradients of the variational distribution and the log joint. GSM-VI has similar goals to automatic-differentiation variational inference (ADVI)  and Pathfinder , which also fit multivariate Gaussian variational families, but do so by maximizing the ELBO using stochastic optimization. As in GSM-VI, the algorithm of ref.  also seeks to match the scores of the variational and the target posterior, but it does so by minimizing the L2 loss between them.

A novel aspect of GSM-VI is how it fits the variational parameters. Rather than minimize a loss function, it aims to solve a set of nonlinear equations. Similar ideas have been pursued in the context of fitting a model to data using empirical risk minimization (ERM). For example, passive agressive (PA) methods  and the stochastic polyak stepsize (SPS) are also derived via projections onto sampled nonlinear equations [3; 13; 23]. A probabilistic extension of PA methods is known as confidence-weighted (CW) learning . In CW learning, the learner maintains a multivariate Gaussian distribution over the weight vector of a linear classifier. Similarly, the second step of GSM-VI also minimizes a KL divergence between multivariate Gaussians. Unlike CW learning, however, the projection in GSM-VI enforces a score-matching constraint as opposed to a margin-based constraint for linear classification.

## 2 Score Matching Variational Inference

Suppose for the moment that the variational family \(q_{}()\) is rich enough to perfectly capture the posterior \(p(\,|\,)\). In other words, there exists a \(^{*}\) such that

\[\,q_{^{*}}()=\,p(|), {}\,.\] (1)

If we could solve Eq. 1 for \(^{*}\), the resulting variational distribution would be a perfect fit. The challenge is that the posterior on the right side is intractable to compute.

To help, we appeal to the idea of score matching . We define the score of a distribution to be the gradient of its log with respect to the variable2, e.g., \(_{} q_{}()\). The principle of score matching is that if two distributions are equal at every point in their support then their score functions are also equal.

To use score matching for VI, we first write the log posterior as the log joint minus the normalizing constant, i.e., the marginal distribution of \(\),

\[\,p(\,|\,)=\,p(,)-\,p().\] (2)

With this expression, the principle of score matching leads to the following Lemma.

**Lemma 2.1**.: Let \(^{d}\) be a set that contains the support of \(p(,)\) and \(q_{^{*}}()\) for some \(^{*}\). That is \(p(,)=q_{^{*}}()=0\) for every \(\). Furthermore, suppose that the support of \(p(,)\) and \(q_{^{*}}()\), and \(\) is path-connected. The parameter \(^{*}\) satisfies

\[_{}\,q_{^{*}}()=_{} \,p(,),,\] (3)

if and only if \(^{*}\) also satisfies Eq. 1.

What is notable about Eq. 3 is that the right side is the gradient of the log joint. Unlike the posterior, the gradient of the log joint is tractable to compute for a large class of probabilistic models. (The proof of the lemma is in the appendix.)

This lemma motivates a new algorithm, _score-matching VI_. The idea is to iteratively refine the variational parameters \(\) in order to satisfy (as much as possible) the system of equations in Eq. 3. At each iteration \(t\), the algorithm first samples a new \(_{t}\) from the current variational approximation and then minimally adjusts \(\) to satisfy Eq. 3 for that value of \(_{t}\), see (4). We want this adjustment to be minimal because, eventually \(q_{_{t}}\) will learn a good fit--one that increasingly matches the scores over the support of the target posterior--and we want to preserve this good fit as much as possible.

**Score matching variational inference**

At iteration \(t\):

Sample \(_{t} q_{_{t}}()\).

Update the variational parameters:

\[_{t+1}=_{}(q_{ _{t}}()\,||\,q_{}())\\ &_{}\, q_{ }(_{t})=_{}\, p(_{t},).\] (4)

This algorithm for score matching VI was inspired by earlier online algorithms for learning a classifier from a stream of labeled examples. One particularly elegant algorithm in this setting is known as passive-aggressive (PA) learning ; this algorithm incrementally updates its model by the minimal amount to classify each example correctly by a large margin. This approach was subsequently extended to a probabilistic setting, known as confidence-weighted (CW) learning  in which one minimally updates a _distribution_ over classifiers. Our algorithm is similar in that it minimally updates an approximating distribution for VI, but it is different in that it enforces constraints for score matching instead of large margin classification.

At a high level, what makes this approach to VI likely to succeed or fail? Certainly it is necessary that there are more variational parameters than elements of the latent variable \(\); when this is not the case, it may be impossible to satisfy a _single_ score matching constraint in Eq. 4. That said, it is standard (as in, e.g., a factorized or mean-field variational family) to have at least as many variational parameters as there are elements of the latent variable. It is also apparent that the algorithm may never converge if the target posterior is not contained in the variational family, or it may converge to a degenerate solution if the variational approximation collapses to a point mass, thus terminating the updates altogether. While we cannot dismiss these possibilities out of hand, we did not observe erratic or pathological outcomes in any of the empirical studies of Section 3.

For more intuition, Figure 0(a) illustrates the effect of the update in Eq. 4 when both the target and approximating distribution are 1d Gaussian. The target posterior \(p(\,|\,)\) is shaded blue. The plot shows the initial variational distribution \(q_{_{0}}\) (light grey curve) and its update to \(q_{_{1}}\) (medium grey) so that the gradient of the updated distribution matches the gradient of the target at the sampled \(_{0}\) (dotted red tangent line). It also shows the update from \(_{1}\) to \(_{2}\), now matching the gradient at \(_{1}\). With these two updates, \(q_{_{2}}\) (dark grey) is very close to the target \(p(\,|\,)\). With this picture in mind, we now develop the details of this algorithm for a more widely applicable setting.

**Gaussian Score-Matching VI.** Suppose the variational distribution belongs to a multivariate Gaussian family \(q_{}():=(,)\); this is a common choice especially in systems for automated approximate inference . One of our main contributions is to show in this case that Eq. 4 has a closed-form solution. The solution \(_{t+1}=(_{t+1},_{t+1})\) has the following form:

\[_{t+1} =\ _{t}+_{t}\,(_{}\, p( _{t},)-_{}\, q_{_{t}}( _{t}))\] (5) \[_{t+1} =\ _{t}+(_{t}-_{t})(_{t}- _{t})^{}-(_{t+1}-_{t})(_{t+1}-_{t})^{}\] (6)

where \(_{t}^{d d}\) is a matrix defined in the theorem below. Note that these updates, which exactly solve the optimization in Eq. 4, only require the score of the log joint \(_{} p(,)\) and the score of the variational distribution \(_{} q_{}()\).

Before deriving Eqs. 5 and 6 in full, we highlight the intuitive form of this closed-form solution. Consider the approximation at the \(t\)th iteration \(q_{_{t}}\), and the current sample \(_{t}\). First suppose the scores already match at this sample, that is \(_{}\, p(_{t},)=_{}\,  q_{_{t}}(_{t})\). Then the mean does not change \(_{t+1}=_{t}\) and, similarly, the two rank-one terms in the covariance update in Eq. 6 cancel out so \(_{t+1}=_{t}\). This shows that when \(q_{_{t}}()=p(,)\) for all \(\), the method stops. On the other hand, if the scores do not match, then the mean is updated proportionally to the difference between the scores, and the covariance is updated by a rank-two correction. Figure 0(b) illustrates the vector field of updates for a one dimensional target \(p(,)=(0,1)\). The vector field points to the solution (green star), which is also the only fixed point of the updates.

We now formalize this result and give the complete expression for the matrix \(_{t}\) that appears in the update for the mean of the variational approximation in Eq. 5.

**Theorem 2.2**.: **(GSM-VI updates)** Let \(p(,)\) be given for some \(^{d}\), and let \(q_{^{}}()\) and \(q_{}()\) be multivariate normal distributions with means \(_{t}\) and \(\) and covariance matrices \(_{t}\) and \(\), respectively. As shorthand, let \(_{t}:=_{} p(_{t},)\) and let

\[_{t+1},_{t+1}\ =\ , 0}{ }[(q_{t},q)]_{} q(_{t})=_{}  p(_{t},).\] (7)

The solution to eq. (7) is given by Eqs. 5 and 6 where

\[_{t} := [-_{t}- _{t})_{t}^{}}{1++(_{t}-_{t})^{}_{t }}]_{t}\,\] (8)

and \(\) is the positive root of the quadratic equation

\[(1\!+\!)\ =\ _{t}^{}_{t}_{t}+[(_{t}\!-\!_{t})^{}\!_{t}]^{2}.\] (9)

With the definition of \(_{t}\) in Eq. 8 we can see that the computational complexity of updating \(\) and \(\) via Eqs. 5 and 6 is \((d^{2})\), where \(^{d}\) and we assume the cost of computing the gradients is \((d)\). Note this is the best possible iteration complexity we can hope for, since we store and maintain the full covariance matrix of \(d^{2}\) elements. (The proof is in the appendix.)

Algorithm 1 presents the full GSM-VI algorithm. Here we also use mini-batching, where we average over \(B\) independently sampled updates of Eqs. 5 and 6 before updating the mean and covariance.

## 3 Empirical Studies

We evaluate how well GSM-VI can approximate a variety of target posterior distributions. Recall that GSM-VI uses a multivariate Gaussian distribution as its variational family. We investigate two separate cases--one when the target posterior is in this family, and the other when it is not.

**Algorithmic details and comparisons.** We compare GSM-VI with a reparameterization variant of BBVI as the baseline, similar to . BBVI uses the same multivariate Gaussian variational family, which we fit by maximizing the ELBO. (Maximizing the ELBO is equivalent to minimizing KL).

We use the ADAM optimizer  with default settings but vary the learning rate between \(10^{-1}\) and \(10^{-3}\). We report results only for the best performing setting. The full BBVI algorithm is shown in Algorithm 2.

``` Input :Initial mean estimate \(_{0}\), initial covariance estimate \(_{0}\), target distribution \(p(|)\), number of iterations N, batch size B, learning rate \(\) Output :Multivariate normal variational distribution \(q_{}():=(,)\) for\(i=0,,N-1\) \(\) iteration loop do for\(j=0,,B-1\) \(\) batch loop do  Sample \(^{(j)}(_{i},_{i})\) \(_{} p(^{(j)}|)\) \(_{i}-_{i}+\)  Solve \((1+)=^{}_{i}+[(_{i}-)^{}]^{2}\) for \(>0\) \(^{(j)}[-_{i}-)}{1++(_{i}-)^{}} ]\) \(_{i}^{(j)}_{i}+^{(j)}\) \(^{(j)}(_{i}-)(_{i}-)^{}-(_{i}^{(j)}-)(_{i}^{(j)}-)^{}\)  end for  Update \(}_{i}+_{j}}/B\)  Update \(}-_{i}+_{j}}/B\)  end for \(q_{}()(_{N},_{N})\) ```

**Algorithm 1**Gaussian Score Matching VI

The only free parameter in GSM-VI is the batch size \(B\). We find that for Gaussian targets, GSM performs equally well for all \(B 1\). There are marginal gains for larger batches for high dimensional targets, but \(B=2\) is a good conservative default. For non-Gaussian targets, we find that all batch sizes converge to the same solution, but we also note that larger batch sizes help to dampen the oscillations in KL divergence around the optimal solution. Notwithstanding these effects, it is not generally necessary to fine-tune this hyperparameter. In all studies of this section we report results for \(B=2\) and show that it is a good default baseline. We use the same batch size for BBVI.

Both GSM-VI and BBVI require an initial distribution for the variational approximation. Unless specified otherwise, we initialize the variational approximation as a Gaussian distribution with zero mean and identity covariance matrix.

**Evaluation metric.** GSM-VI does not explicitly minimize any loss function. Hence to compare its performance against BBVI, we estimate empirical divergences between the variational and the target distribution and show their evolution with the number of gradient evaluations. We measure performance in terms of gradient evaluations as the actual running time can be sensitive to implementation details; moreover, we note that in real-world problems, the computation is often bottlenecked by evaluations of the target log density and its gradient. For completeness, we also measure running times of GSM-VI and BBVI for a full rank Gaussian target with 2048 dimensions using sample size of 1 in Jax after JIT-compilation. The GSM update takes 230 ms/update while BBVI takes 227 ms/update.

In the experiments with synthetic models in Sections 3.1, and 3.2 we have access to the true distribution; hence for these experiments, we estimate the forward KL divergence (FKL) empirically by \(_{_{i} p()} p(_{i})- q(_{i})\). To reduce stochasticity, we always use the same pre-generated set of 1000 samples from the target distribution. In the experiments with real-world models in Section 3.3, we do not have access to the samples from the target distribution; for these experiments we monitor the negative ELBO. In all experiments, we show the results for 10 independent runs.

### GSM-VI for Gaussian approximation

We begin by studying GSM-VI where the target distribution is also a multivariate Gaussian.

**Scaling with dimensions.** How does GSM-VI scale with respect to the dimensionality, \(D\), of the latent variable? Figure 2 shows the convergence of FKL for GSM-VI and BBVI for different dimensionalities. Empirically, for GSM-VI, we find that the number of iterations required for convergence increases almost linearly with dimensionality. The scaling for BBVI is worse: even for small problems (\(D<64\)) it requires 100 times more iterations while also converging to a sub-optimal solution (as measured by the FKL metric). The figure also shows the evolution of the _reverse_ KL divergence, and it is noteworthy that GSM-VI also outperforms BBVI in this metric.

For completeness, we also experimented on these problems3 with methods for variational inference based on natural gradient descent (NGD-VI) . Natural gradient descent is a stochastic gradient method that uses the Fisher matrix as a preconditioner. We find NGD-VI is slower by factor of

Figure 2: Scaling with dimensionality: evolution of forward (a) and reverse KL (b) with the number of gradient evaluations of the target distribution, which here is a Gaussian distribution with a dense \(D D\) covariance matrix. Each panel shows the results for a different dimensionality \(D\) (indicated in the title). Translucent lines show the scatter of 10 different runs, and the solid line shows the average. GSM-VI and BBVI use batch sizes of \(B=2\), while the batch sizes for natural gradient descent are shown in the legend.

5-10x than GSM-VI. While NGD-VI outperforms BBVI, its performance is sensitive to the choice of batch size, with small batch sizes prone to divergence. Furthermore, we note that that per-iteration-complexity of NGD-VI is cubic in the dimensionality (\(D^{3}\)) as each iteration requires inverting a Hessian approximation; this is in contrast to the quadratic (\(D^{2}\)) per-iteration-complexities of GSM-VI and BBVI. For these reasons, we do not investigate NGD-VI further in this paper.

**Impact of condition number.** What is the impact of the shape of the target distribution? We again consider a Gaussian target distribution, but vary the condition number of its covariance matrix by fixing its smallest eigenvalue to \(0.1\) and scaling the largest eigenvalue to \(0.1 c\). Figure 3 shows the results for a 10 dimensional Gaussian where we vary the condition number \(c\) from 1 to 1000. Convergence of GSM-VI seems to be largely insensitive to the condition number of the covariance matrix. BBVI on the other hand struggles with poorly conditioned problems, and it does not converge for \(c>100\) even with 100 times more iterations than GSM.

### GSM-VI for non-Gaussian target distributions

GSM-VI was designed to solve the exact score-matching equations in Eq. 3, but these equations only have a solution when the family of variational distributions contains the target distribution (see Lemma 2.1). Here we investigate the sensitivity of GSM-VI to this assumption by fitting non-Gaussian target distributions with varying degrees on non-Gaussianity. Specifically, we suppose that

Figure 4: Impact of non-Gaussianity: evolution of FKL with the number of gradient evaluations for Sinh-arcsinh distributions with 10-dimensional dense Gaussian as the base distribution. Gaussian distribution has \(s=0,\,t=1\). In the left panel, we vary skewness \(s\) while fixing \(t=1\), and in the right panel we vary the tail-weight \(t\) with skewness fixed to \(s=0\). Solid lines are the results for GSM, dashed for BBVI.

Figure 3: Impact of condition number: evolution of FKL with the number of gradient evaluations of the target distribution. The target is a 10-dimensional Gaussian albeit with a dense covariance matrix of different condition numbers \(c\) specified in the title of different panels. Translucent lines show the scatter of 10 different runs and the solid line shows the average.

the target has a multivariate Sinh-arcsinh normal distribution 

\[(,);=( [^{-1}()+s])\] (10)

where the scalar parameters \(s\) and \(t\) control, respectively, the skewness and the heaviness of the tails. Note that Eq. 10 reduces as a special case to a Gaussian distribution for the choices \(s=0\) and \(t=1\) of these parameters.

Figure 4 shows the result for fitting the variational Gaussian to a 10-dimensional Sinh-arcsinh normal distribution for different values of \(s\) and \(t\). As the target departs further from Gaussianity, the quality of variational fit worsens for both GSM-VI (solid lines) and BBVI (dashed lines), but they converge to a fit of similar quality in terms of average FKL. GSM converges to this solution at least 10 times faster than BBVI. For highly non-Gaussian targets (\(s 1\) or \(|t-1| 0.8\)), we have found that GSM-VI does not converge to a fixed point, and it can experience oscillations that are larger in amplitude than BBVI; see for instance \(s=1.8\) and \(t=0.1\) on the left and right of Figure 4, respectively.

### GSM-VI on real-world data

We evaluate GSM-VI for approximate on real-world data with 8 models from the posterioridb database . The database provides the Stan code, data and reference posterior samples, and we use bridgestan to access the gradients of these models . We study the following statistical models, each with a different complexity and dimensionality (D) : diamonds (generalized linear models, D=26), hudson-lynx-hare (differential equation dynamics, D=8), bball-drive (hidden Markov models, D=8) and arK (time-series, D=7), eight-schools-centered and non-centered (hierarchical meta-analysis, D=10), gp-pois-regr (Gaussian processes, D=13), low-dim-gauss-mix (Gaussian mixture, D=5).

For each model (except hudson-lynx-hare), we initialize the variational parameter \(_{0}\) at the mode of the distribution, and we set \(_{0}=0.1\,_{d}\) where \(_{d}\) is the identity matrix of dimension \(d\)

Figure 5: Models from posterioridb: Convergence of the ELBO for four models with multivariate normal posteriors. We show results for 10 runs.

Figure 6: Models from posteriordb: Convergence of ELBO for four models with non-Gaussian posteriors. We show results for 10 runs.

For hudson-lynx-hare, we initialize the variational distribution as standard normal. We also experimented with other initializations. We find that they do not qualitatively change the conclusions, but can have larger variance between different runs.

We show the evolution of the ELBO for 10 runs of these models. Four of the models have posteriors that can be fit with multivariate normal distribution: diamonds, hudson-lynx-hare, bball-drive, and ark. Figure 5 shows the result for these models. The other models have non-Gaussian posteriors: eight-schools-centered, eight-schools-non-centered, gp-pois-regr,, and low-dim-gauss-mix. Figure 6 shows the results.

Overall, GSM-VI outperforms BBVI by a factor of 10-100x. When the target posterior is Gaussian, GSM-VI leads to more stable solutions. When the target is non-Gaussian, it converges to the same quality of variational approximation as BBVI. Finally, we note that while GSM-VI yields noisy estimates of the ELBO, it does yield stable estimates of the 1-D marginals and their moments.

## 4 Conclusion and Future Work

In this paper we have proposed Gaussian score matching VI (GSM-VI), a new approach for VI when the variational family is multivariate Gaussian. GSM-VI does not explicitly minimize a divergence between the variational and target distribution; instead, it repeatedly enforces score-matching constraints with closed-form updates for the mean and covariance matrix of the variational distribution. This is in contrast to stochastic gradient based methods, such as in BBVI, that rely on first-order Taylor approximations of their objective function.

GSM-VI is implemented in an open-source Python code at https://github.com/modichirag/GSM-VI.

Unlike approaches that are rooted in stochastic gradient descent, GSM-VI does not require the tuning of step-size hyper-parameters. Rather, it is able to adaptively make large jumps in the initial iterations (see Fig. 1a) and make smaller adjustments as the approximation converges to the target. GSM-VI has only one free parameter, the batch size, and we found a batch-size of 2 to perform competitively across all experiments. Another choice is how to initialize the variational distribution. For the experiments in this paper, we initialized the covariance matrix as the identity matrix, but additional gains could potentially be made with more informed choices derived from a Laplace approximation or L-BFGS Hessian approximation .

We evaluated the performance of GSM-VI on synthetic targets and real-world models from posteriori. In general, we found that it requires 10-100x fewer gradient evaluations than BBVI for the target distribution to converge. When the target distribution is itself multivariate Gaussian, we observed that GSM-VI scales nearly _linearly_ with dimensionality, which is significantly better than BBVI, and that GSM-VI is fairly insensitive to the condition number of the target covariance matrix. We also found that GSM-VI converges more quickly than BBVI--and also to a solution with a larger ELBO, which is surprising given that BBVI explicitly maximizes the ELBO.

An important avenue for future work is to provide a proof that GSM-VI converges. We note that good convergence results have been obtained for analogous methods that project onto interpolation equations for empirical risk minimization. For instance the Stochastic Polyak Step achieves the min-max optimal rates of convergence for SGD . Note that convergence of VI is a generally challenging problem, with no known rates of convergence even for BBVI [10; 11]. This and others directions are left to future work.