# Fight Back Against Jailbreaking via

Prompt Adversarial Tuning

 Yichuan Mo\({}^{1}\)1  Yuji Wang\({}^{2}\)2  Zeming Wei\({}^{3}\)  Yisen Wang\({}^{1,4}\)3

\({}^{1}\) State Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{2}\) Shanghai Jiao Tong University

\({}^{3}\) School of Mathematical Sciences, Peking University

\({}^{4}\) Institute for Artificial Intelligence, Peking University

Equal Contribution.

###### Abstract

While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreaking attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs. However, how to achieve intrinsic robustness through prompt optimization remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named **Prompt Adversarial Tuning (PAT)** that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both grey-box and black-box attacks, reducing the success rate of advanced attacks to nearly 0%, while maintaining the model's utility on the benign task and incurring only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at https://github.com/PKU-ML/PAT.

## 1 Introduction

Large Language Models (LLMs) [35; 1; 67; 44] have shown remarkable performance in multiple regions, such as coding [62; 29], math [30; 21] and role-playing [40; 51]. Meanwhile, serious concerns have been raised about their security issues [41; 60] and one of the most prominent problems is the jailbreak attack . Although at the training stage, substantial efforts [38; 4] have been invested to align the outputs of LLMs with human values, recent studies reveal that LLMs may still output inappropriate content when facing well-designed adversarial prompts [43; 27]. Similar to the adversarial attacks [32; 49; 11; 3; 55; 31] in the image domain, it will not only significantly affect the normal functionality of LLMs but also potentially result in serious ethical issues.

To mitigate this threat, several studies have proposed targeted defenses to enhance protection. For instance, fine-tuning-based defenses [22; 14; 64] aim to improve intrinsic robustness by incorporating safety datasets into the training data. However, given the vast parameters in LLMs, this approach significantly increases computational costs. Alternatively, prompt-based defenses [71; 58; 54] involve manually designing secure prompting contexts, which are computationally efficient but rely heavily on human heuristics. In addition, those approaches also risk high false-positive rates due to their lack of alignment with the model's training distribution. By combining the distinct advantages ofboth methods, a hybrid approach could leverage their strengths, resulting in a more powerful defense strategy.

Therefore, in this paper, we try to answer this question by proposing an approach named **Prompt Adversarial Tuning (PAT)**. Specifically, an adversarial tuning process is first introduced to optimize our defensive prefix, alternating between updating attack and defense controls with two opposite output targets. Furthermore, as illustrated in Figure 1, model developers incorporate the defense control as a prefix into user prompts at the inference stage.

Our main contributions can be summarized as follows:

1. To our knowledge, we are the first to consider improving jailbreak robustness by introducing a min-min optimization for prompt tuning. Once the defense strategy is deployed, this operation will only bring a negligible cost to the efficiency of the LLMs.
2. Our approach balances the robustness and usability of the model, effectively defending against jailbreak attacks without significantly affecting the model's utility.
3. Experimental results show that our method is effective in both grey-box and black-box settings, reducing the success rate of advanced attacks to nearly 0 and demonstrating good transferability across open-source and closed-source models.

## 2 Related Work

**Jailbreak Attacks against LLMs.** The term "jailbreak attack" originally described the act of bypassing software restrictions on mobile devices. With the rapid advancement of LLMs, however, "jailbreaking" has found a new application: manipulating prompts to make these models generate prohibited or unauthorized content. Initial jailbreak attacks in LLMs were mainly manually crafted, such as role-play [9; 13], prompt injection [5; 68; 36], rewriting in rare languages [15; 28; 25] or Base64 coding . Zou _et al._ first investigate how to craft jailbreak prompts automatically and propose the GCG attack. However, the application of GCG makes it vulnerable to perplexity filters. Therefore, future work such as AutoDAN  and COLD  propose an additional loss term and controllable text generation techniques to increase the interpretability, respectively. In addition, for closed-source LLMs, the inaccessibility of their parameters makes it unavailable to perform the GCG attack directly on those models. Advancements in recent works have well addressed this issue: ICA attack in [54; 23] take advantage of In-Context Learning  and jailbreak the models with a few malicious demonstrations. Additionally, PAIR  and TAP  craft the jailbreak prompts with a red-teaming LLM which makes it capable of jailbreaking LLMs in twenty queries. Due to the significant threat of the aforementioned methods, it is still an unsolved problem to develop effective defenses to protect LLMs from those attacks.

**Defense against Jailbreak Attacks.** In response to the threat, several defense strategies have emerged, mainly divided into training-based and test-based approaches. Training-based defenses

Figure 1: The pipeline of our proposed **Prompt Adversarial Tuning (PAT)** at the inference stage. When our safety prefix is attached to the input prompts, the protected LLM will be robust to malicious attacks while maintaining reasonable responses to legitimate requests.

focus on finetuning the parameters of LLMs for jailbreak immunity. In [22; 8], they first apply supervised fine-tuning (SFT) by blending the harmful prompts with the harmless prompts, though this approach lacks robustness against the automatic attacks. Therefore, follow-up works address this limitation by augmenting the attack prompts , gradient ascent with affirmative responses  or unlearning the harmful knowledge [20; 65]. Compared to training-based defense, test-based defense aims to defend against jailbreak attacks efficiently. For instance, from the input perspective, in [22; 2], they introduce perplexity filtering to detect unreadable adversarial strings, such as the GCG attack. In addition, jailbreak prompts are demonstrated more sensitive to random perturbation , safety-aware decoding , self-correction [37; 47], in-context learning  or a secure system prompt . However, all of them need human heuristics, which limits their performances when meeting LLMs training in different distributions. In this paper, our proposed PAT tries to combine the two types of defense methods together to leverage the strengths of both.

## 3 The Proposed Prompt Adversarial Tuning

In this section, we begin by clarifying PAT's threat model. Next, we introduce the basic notations. Finally, we provide a detailed explanation of our defense algorithm.

### Threat Model and Notations

**Threat model**. Prior research on adversarial attacks has primarily focused on white-box threat models, where attackers have full knowledge of the defense system. These attacks can then be transferred to other models, creating a black-box scenario. However, for defenders of jailbreak attacks in LLMs, typically model developers, can monitor inputs and outputs, and preprocess user prompts, like adding prefixes. Achieving robustness against white-box attacks is often too demanding and should be seen as an ideal goal rather than a practical one, especially for threats against Large Language Models. Instead, the focus should be on grey-box robustness, where key defense elements, like detection models and model parameters, remain hidden from attackers.

**Notations**. LLM can be considered as a mapping from the sequence of tokens. Given a prompt \(P=x_{1:n}\), LLM will generate a response \(R=x_{n+1:n+L}\), where \(x_{i}\) stands for one token. Then we use the notation \(p(x_{n+1}|x_{1:n})\) to represent the likelihood of the next token being \(x_{n+1}\) in the sequence. Similarly, the response \(R\) can be generated by sampling from the following distribution:

\[p(x_{n+1:n+L}|x_{1:n})=_{i=1}^{L}p(x_{n+i}|x_{1:n+i-1}).\] (1)

Based on this representation, we can formulate the loss function. We denote the target sequences of tokens, such as "Sure, here is how to build a bomb", as \(x_{n+1:n+L}\). Consequently, the following loss formulation can represent the probability of generating \(x_{n+1:n+L}\) given \(x_{1:n}\):

\[(x_{1:n})=- p(x_{n+1:n+L}|x_{1:n}).\] (2)

### Prompt Adversarial Tuning

Based on the previously discussed threat model, as the model developers, they can perform some preprocessing on user prompts. Thus, we attempt to explore a "defense control", which, when used as a prefix in user prompts fed into the model, can defend against malicious requests while maintaining the model's benign utility. This is a problem involving a mixed optimization objective.

**Jailbreak defense.** For the first objective. Inspired by the adversarial training framework [32; 48; 61; 50; 56; 45; 34; 53; 35; 63], we attempt to introduce potential attacks into the defense generation. Therefore, We design the format for user prompts as follows:

User: { harmful goal } { attack control }

Model Developer: _CONCAT_ ( { defense control }, { harmful goal } { attack control } )

Assistant:

The safe prompt processed by the model developer is then fed into the model. In our method, we update the attack control and the defense control alternately. We define the entire user message as \(x_{1:n}\), the indices of the attack control as \(_{ack}\), the indices of the defense control as \(_{def}\). The objective of the attack control is to make the model output malicious content, while the objective of the defense control is to help the model reject malicious requests. Therefore, we can formulate a malicious target \(y_{ack}\) (i.e., "Sure, here is how to build a bomb.") and a secure target \(y_{def}\) (i.e., "I am sorry, I cannot fulfill this request.") for each goal. Referring to Equation 2, we can formulate the loss function of attack and defense separately:

\[_{ack}(x_{1:n},y_{ack})&=-  p(y_{ack}|x_{1:n}),\\ _{def}(x_{1:n},y_{def})&=- p(y_{ def}|x_{1:n}).\] (3)

Considering that \(_{ack}\) and \(_{def}\) have similar expressions, we write both uniformly as \(\).

**Utility maintenance.** Similar to jailbreak defense, we can design an optimization object for maintaining benign utility:

User: { benign goal }

Model Developer: _CONCAT_ ( { defense control }, { benign goal } )

Assistant:

We mark the user prompts under this format as \(x^{}_{1:p}\). Similarly to the notation as before, \(x^{}_{_{def}}\) stands for the defense control. Then given a pair of benign goal \(x_{bgn}\) and target \(y_{bgn}\), \(x^{}_{1:p}\) is equivalent to the concatenation of \(x^{}_{_{def}}\) and \(x_{bgn}\). Thus the benign loss can be represented as:

\[(x^{}_{1:p},y_{bgn})=- p(y_{bgn}|x^{}_{1:p}).\] (4)

Combining the equations in two stages, we can write the general optimization objective in the following formulations:

\[ x^{}_{_{ack}}&= {}_{x_{ack}\{1,...,V\}^{|x_{ack}|}}(x_{1:n},y_{ack}),\\ x^{}_{_{def}}&= _{x_{_{def}}\{1,...,V\}^{|x_{def}|}}(\:(x^ {}_{1:p},y_{bgn})+(1-)\:(x_{1:n},y_{def})).\] (5)

Based on the above discussion, we optimize a single attack control \(x_{_{ack}}\) and a single defense control \(x_{_{def}}\) over multiple malicious prompts \(x^{(1)}_{1:n_{1}}\)... \(x^{(m)}_{1:n_{m}}\) and auxiliary normal questions \(x^{(1)}_{1:p_{1}}\)... \(x^{(m)}_{1:p_{m}}\).

**Optimization details.** Regarding the discreteness of the input token, we adopt the greedy coordinate gradient strategy for updating controls. Specifically, during each epoch, we first calculate the gradients of the one-hot token indicators to identify a set of potential replacement candidates at each token position. The gradient of the i-th token \(x_{i}\) can be represented as follows:

\[_{1 j m}_{e_{x_{i}}}(x^{j}_{1:n_{j}}||x_{ })\] (6)

where \(x_{}\) refers to the indices of controls to be updated. Using this formula, we can choose the top-k negative gradients as promising token replacements for \(x_{i}\). Based on token replacements, then we can generate candidate controls by applying these replacements randomly. We only generate \(B\) candidates in each round to ensure computational efficiency. After that, we determine the best updated control according to optimization loss. To enhance the model's ability to respond appropriately to a greater number of normal commands, we collect a large set of benign question-and-answer pairs. In each iteration, we extract \(m\) samples from this dataset to participate in the loss calculation. The whole process of PAT can be found in Algorithm 1.

**Multiple model extension.** It is important to note that PAT supports both single and multiple model configurations. In the multi-model setting, we integrate losses across multiple models to make defense controls more general and transferable. Specifically, when selecting promising token substitutions, we aggregate the gradients of tokens from multiple models using the same tokenizer. Furthermore, we combine the losses of substitutions across these models to determine candidates. Generally, this process can be accomplished with only a slight extension to Algorithm 1. In Section 4, we will investigate the performance of the defense control trained under this strategy on the closed-source models.

``` Input: Harmful prompts \(x_{1:n_{1}}^{(1)}\)... \(x_{1:n_{m}}^{(m)}\), malicious targets \(y_{ack}^{(1)}\)... \(x_{ack}^{(m)}\), safety targets \(y_{def}^{(1)}\)... \(x_{def}^{(m)}\), benign prompts \(x_{1:n_{1}}^{(1)}\)... \(x_{1:p_{m}}^{(m)^{}}\), benign targets \(y_{bgn}^{(1)}\)... \(x_{bgn}^{(m)}\), initial attack control \(x_{_{ack}}\), initial defense control \(x_{_{def}}\), iterations \(T\), loss function \(\), size of tokens \(k\), batch size \(B\) for\(t=1\)to\(T\)do // update the attack control for each \(i_{ack}\)do \(_{i}\) Top-\(k(-_{1 j m}-_{e_{x_{i}}}(x_{1:n_{j}}^{j}||x_{ _{ack}},y_{ack}^{j}))\) for\(b=1\)to\(B\)do \(_{_{ack}}^{(b)} x_{_{ack}}\) \(_{b}^{(b)}\) Uniform\((_{i})\) where \(i\) Uniform\((_{ack})\) endfor \(x_{_{ack}}_{_{ack}}^{(b^{*})}\) where \(b^{*}_{b}_{1 j m}(x_{1:n_{j}}^{j}|| _{_{ack}}^{(b)},y_{ack}^{j}))\) endfor // update the defense control for each\(i_{def}\)do \(_{i}\) Top-\(k(-_{1 j m}-_{e_{x_{i}}}(x_{1:n_{j}}^{j}||x_{ _{def}},y_{def}^{j}))\) for\(b=1\)to\(B\)do \(_{_{def}}^{(b)} x_{_{def}}\) \(_{b}^{(b)}\) Uniform\((_{i})\) where \(i\) Uniform\((_{def})\) endfor \(x_{_{def}}_{_{def}}^{(b^{*})}\) where \(b^{*}_{b}_{1 j m}((x_{1:n_{j}}^{ j}||_{_{def}}^{(b)},y_{bgn}^{j}))+(1-)(x_{1:n_{j}}^{j} ||_{_{def}}^{(b)},y_{def}^{j})))\) endfor Output: Optimized defense control \(x_{_{def}}\) ```

**Algorithm 1** Prompt Adversarial Tuning (PAT)

## 4 Experiments

We performed experiments on the Advbench dataset  which is one of the most prevailing benchmark datasets to measure the security of LLMs. Considering its practicality, two scenarios are specifically considered: **(1) Grey-box Setting:** The parameter of the protected model is available for defenders. This means that the defense control of PAT can be precisely crafted using the protected model. **(2) Black-box Setting:** For privacy reasons, private developers do not want others to access their model parameters while also wanting to enjoy instant security services. Therefore, the defense control is firstly crafted on surrogate models. During the inference stage, the defender attaches the obtained prefix as a plug-and-play technique with the target models, making it available for both open-source and closed-source models. The effectiveness of PAT in both settings demonstrates its practicality in the real world.

### Settings

**Dataset Preparing.** Three sets of dialogue data are included to perform experiments for PAT, including harmful prompts and targets (\(x_{1:n_{1}}^{(1)}\), \(y_{ack}^{(1)}\))... (\(x_{1:n_{m}}^{(m)}\), \(y_{ack}^{(m)}\)), harmful prompts and safety targets (\(x_{1:n_{1}}^{(1)}\), \(y_{def}^{(1)}\))... (\(x_{1:n_{m}}^{(m)}\), \(y_{def}^{(m)}\)), benign prompts and goals (\(x_{1:p_{1}}^{(1)^{}}\), \(y_{bgn}^{(1)}\))... (\(x_{1:p_{m}}^{(m)^{}}\), \(y_{bgn}^{(m)}\)). We acquire 25 harmful prompts and harmful targets from the Advbench dataset . And to generate safe targets, we feed raw malicious prompts directly into the surrogate model. In terms of benign dialogues, we acquire a subset including 100 prompts from the MS MARCO dataset , which is a dataset designed for question-answering, featuring questions that are sourced from actual user inquiries on Bing.

**Hyperparameters.** The hyperparameter settings for PAT during our tuning process are as follows: The number of prompts, \(m\) for control optimization is 25. As for the control length, the length of attack control is 20, and the length of defense control is 15. We iteratively update the controls for 100 epochs. During the token selection, the token set size \(k\) is chosen as 256 and the batch size \(B\) is 512. All the experiments are performed on one or multiple NVIDIA A100 80G GPUs.

**Metrics.** For an ideal defense, it will not only significantly eliminate the threat of attacks but also have minimal impact on the performances of LLMs. Inspired by [70; 10], we measure the first perspective with Attack Success Rate (**ASR**), which refers to the proportion of jailbreak attacks that can bypass model alignment or defensive measures. The details can be found in the Appendix B. Regarding the benign utility of the models, we calculate the score on two benchmarks: Multi-turn Benchmark (**MT-bench**) , measuring multi-turn capabilities of LLM in eight aspects and Massive Multitask Language Understanding (**MMLU**) , evaluating the knowledge processed by LLMs.

### Performances of PAT under the Grey-box Setting

In the grey-box setting, we craft a defense control for Vicuna-7B  and Llama-2-7B , respectively. Then we evaluate the performance of PAT against two optimization-based attacks: GCG , AutoDAN , one context-based attack: ICA , and two query-based attacks: PAIR  and TAP . In addition, we compare PAT with 6 state-of-the-art defenses: PPL-based detection , ICD , SafeDecoding , SmoothLLM , Self-reminder  and DRO . The hyperparameter settings of baseline attacks and defenses can be found in Appendix A. We summarize the results in Table 1.

We first observe that compared to baseline defenses, PAT achieves the lowest average ASR. For example, on Vicuna-7B, PAT achieves average ASR of 1.8%, which is lower than other defenses. Note that although our optimization target is derived from the GCG attack, the results demonstrate that PAT can still be effective against unseen jailbreak attacks, such as AutoDAN and PAIR. Regarding the benign utility, PAT obtains the highest score on MT-bench, which is even higher than models without performing any defenses. Through further exploration, we discovered that this is because PAT can enhance the logical capabilities of the LLMs. We see the scores increase in related aspects of MT-bench such as coding and reasoning. But for abilities that require knowledge reproduction, _e.g._ STEM and Humanities, the score decreases. Since the increase outweighs the decrease, we observe a rise in the overall score. This could also explain why PAT decreases the score on MMLU slightly, which measures the knowledge of LLMs in different domains. Nevertheless, compared to scores achieved by other methods, its performance is quite competitive: For Vicuna-7B, although Self-reminder achieves a higher score than PAT on MMLU benchmark, it is broken through by all attacks. For Llama-7B, PPL achieves the best performances on MMLU, but it can only effectively resist the GCG attack and fails to work against other attacks such as PAIR. This is because compared

    &  &  &  &  \\   & **GCG** & **AutoDAN** & **ICA** & **PAIR** & **TAP** & & & \\   & **No Defense** & 92\% & 72\% & 56\% & 79\% & 55\% & 70.8\% & 6.55 & 51.2 \\  & **PPL** & 0\% & 72\% & 56\% & 79\% & 55\% & 52.4\% & 6.52 & 50.3 \\  & **Self-reminder** & 92\% & 72\% & 56\% & 79\% & 55\% & 70.8\% & 6.58 & **51.0** \\  & **ICD** & 12\% & 0\% & 30\% & 28\% & 14\% & 16.8\% & 6.43 & 49.7 \\  & **DRO** & 2\% & 22\% & 0\% & 12\% & 14\% & 10.0\% & 6.45 & 50.2 \\  & **SafeDecoding** & 3\% & 4\% & 2\% & 6\% & 6\% & 4.2\% & 6.63 & 50.0 \\  & **SmoothLLM** & 0\% & 66\% & 4\% & 34\% & 20\% & 24.8\% & 4.55 & 39.3 \\  & ** PAT (Ours)** & 1\% & 5\% & 0\% & 1\% & 2\% & **1.8\%** & **6.68** & 50.9 \\   & **No Defense** & 36\% & 20\% & 0\% & 60\% & 47\% & 32.6\% & 6.75 & 50.5 \\  & **PPL** & 0\% & 20\% & 0\% & 60\% & 47\% & 25.4\% & 6.73 & **50.4** \\  & **Self-reminder** & 1\% & 1\% & 0\% & 4\% & 1\% & 1.4\% & 6.60 & 48.9 \\  & **ICD** & 4\% & 1\% & 0\% & 1\% & 0\% & 1.2\% & 5.98 & 50.1 \\  & **DRO** & 3\% & 0\% & 0\% & 2\% & 0\% & 1.0\% & 6.23 & 49.8 \\  & **SafeDecoding** & 1\% & 0\% & 0\% & 2\% & 1\% & **0.8\%** & 6.07 & 48.6 \\  & **SmoothLLM** & 2\% & 5\% & 0\% & 1\% & 3\% & 2.2\% & 5.81 & 38.9 \\  & **PAT (Ours)** & 0\% & 2\% & 0\% & 1\% & 1\% & **0.8\%** & **6.78** & 50.2 \\   

Table 1: The performances of PAT on the Advbench dataset. The best and the second best results obtained by defenses are in **bold** and underline, respectively. PAT achieves the lowest average ASR compared to baseline defenses.

to GCG, other attacks can craft adversarial input with lower perplexity. In total, PAT can effectively resist all the attacks while best preserving the model's benign utility.

### Transferability of PAT across Open-source Models

As stated in previous sections, in some situations, the parameters of protected models are not always available for defenders. Therefore, it is necessary to study the capability of PAT under the black-box settings. Here we first study the transferability of PAT across four open-source models, including Vicuna-7B, Llama-2-7B, Mistral-7B , Llama-3-8B . The ASR is calculated against three attacks: GCG, ICA, and PAIR, which are one representative attack from each category.

As shown in Figure 2, we first observe that PAT can effectively transfer across open-source models, significantly reducing the ASR in all settings. For example, on Vicuna-7B, the defense control crafted on Llama-3-8B reduced the ASR of GCG attack from 92% to 2%. Additionally, the lowest ASR is achieved when the surrogate and target models are the same, likely because directly optimizing on the protected model better fits its training domains. Comparing the ASR when the source and target models are different, we find that PAT shows better transferability between Llama-2-7B and Llama-3-8B. We conjecture that it is because they might share high similarity in architectures and training data.

### Performances of PAT on Closed-source Models

Compared to open-source models, closed-source models are often more powerful and more widely used. We further demonstrate that PAT can secure those models from jailbreak attacks even if their parameters are not released to the public. Here, we conduct experiments on GPT-3.5  and GPT-4 , the two most outstanding star products of OpenAI. Following Section 4.2, the performances of PAT are evaluated against five attacks. For GCG, the adversarial suffix is crafted with the ensemble

    & &  &  &  \\    & & **GCG** & & & & & & \\   & **No Defense** & 92\% & 37\% & 0\% & 63\% & 19\% & 8.39 & 64.6 \\  & **ICD** & 16\% & 6\% & 0\% & 7\% & **2\%** & 5.61 & 46.1 \\  & **Self-reminder** & 10\% & 9\% & 0\% & 9\% & 4\% & 5.57 & 54.6 \\  & **SmoothLLM** & 13\% & 10\% & 0\% & 11\% & 5\% & 6.85 & 50.5 \\  & **PAT (Ours)** & **4\%** & **2\%** & 0\% & **5\%** & **2\%** & **8.06** & **60.8** \\   & **No Defense** & 5\% & 7\% & 10\% & 34\% & 20\% & 9.32 & 78.8 \\  & **ICD** & 4\% & 5\% & 5\% & 7\% & 6\% & 6.67 & 70.5 \\   & **Self-reminder** & 3\% & 3\% & 9\% & 4\% & **2\%** & 6.28 & 75.2 \\   & **SmoothLLM** & 3\% & 4\% & **0\%** & 3\% & **2\%** & 7.56 & 63.5 \\   & **PAT (Ours)** & **0\%** & **0\%** & **0\%** & **2\%** & **8.77** & **77.3** \\   

Table 2: Performances of PAT on defending jailbreak attacks on closed-source models. The best results achieved by defense methods are in **bold**.

Figure 2: Transferability of PAT across models. PAT can acquire low ASR when it transfers the prefix across different model architectures.

of Vicuna-7B and Vicuna-13B  as proposed in their original paper . For AutoDAN, we transfer the suffix crafted on Vicuna-7B to attack GPTs. The settings of other attacks are the same as those in the grey-box setting. To enhance PAT's transferability, we optimize the defense control with min-max formulations with the combination of Vicuna-7B and Vicuna-13B models. We compare its performances with ICD , SmoothLLM  and Self-reminder . We do not compare with DRO and SafeDecoding because both of them can be applied only for open-source models. For PPL, considering its bad performances in attacks of low perplexity, we also omit it for comparison.

In Figure 3, we display empirical examples to demonstrate the defense effect of PAT on GPT-3.5 and GPT-4. For the complete screenshots, please refer to Appendix F for more details. When comparing the ASR with no defense in Table 2, we observe that all defense methods can decrease the ASR of jailbreak attacks a lot. However, PAT can achieve lower or comparable ASR compared to the baseline methods. For example, on GPT-3.5, PAT acquires ASR of 5% against PAIR attack which is quite lower than those of ICD, Self-reminder or SmoothLLM. In addition, similar to the closed-source models, PAT has an obvious advantage in maintaining benign utilities, achieving higher scores on the MT-bench or MMLU benchmarks. It indicates the university and transferability of PAT. Defenders can generate it only once and protect multiple open-source and closed-source LLMs simultaneously.

Figure 3: The examples of PAT to defend jailbreak attacks for closed-source models.

### Defense against Human-crafted Attacks

Besides the automated generation of jailbreak attacks, the earliest jailbreak prompts are usually constructed by humans [52; 15]. Reported by , one of those has been processed online for more than 240 days and held high attack success rates on popular LLMs. Thus their tremendous threats can not be simply ignored and we further investigate the effectiveness of PAT against those attacks. In , they first study the design principles of those attacks and classify them into two partitions, _i.e._ competing objectives (CO) and mismatched generalizations (MG). The first one appears when the requirements of users conflict with the safety goal, including Always Intelligent and Machiavellian attack (AIM), Prefix Injection attack (PI) and Refusal Suppression attack (RS). The other one refers to circumstances in which the safety capability fails to generalize, such as jailbreak prompts written in Base64 coding or Bengali (BN) . The alignment of the LLM could be easier to break due to the scarcity of relevant corpus for alignment.

Following the settings in their original paper, our experiments include five typical attacks and are performed on GPT-3.5 and GPT-4. for the settings of those attacks, please refer to Appendix C for more details. We directly transfer the secure prefix of PAT crafted in Section 4.4 to defense those attacks and the results are summarized in Table 3. We first observe that compared to the baseline defenses, PAT can achieve comparable or better performances in defending human-crafted attacks. Its advantages are more evident in defending against attacks based on mismatched generalization designs. For example, against the Base64 attack, ICD only achieves 27% ASR on GPT-3.5. In contrast, PAT achieves a lower result, which is 2%. We conjecture the reason is that the prefix of PAT is a mojibake instead of plain English. This helps it gain better transferability across prompts written in various languages.

Figure 4: Ablation Studies for PAT. We investigate the influence of different factors, including (a) the length of the defense control \(|_{def}|\) (b) the trade-off factor \(\)

    &  &  &  \\   & & **AIM** & **PI** & **RS** & **Base64** & **BN** \\   & **No defense** & 10\% & 11\% & 28\% & 32\% & 13\% \\  & **ICD** & 5\% & 3\% & 5\% & 27\% & 3\% \\  & **Self-reminder** & 2\% & 1\% & **4\%** & 13\% & 4\% \\  & **SmoothLLM** & 2\% & 3\% & 7\% & 11\% & 6\% \\  & **PAT(Ours)** & **1\%** & **0\%** & **4\%** & **2\%** & **0\%** \\   & **No defense** & 8\% & 6\% & 8\% & 13\% & 9\% \\  & **ICD** & **1\%** & 1\% & **0\%** & 5\% & 3\% \\   & **Self-reminder** & 2\% & **0\%** & 1\% & 6\% & 2\% \\   & **SmoothLLM** & 6\% & 4\% & 6\% & 6\% & 3\% \\   & **PAT(Ours)** & **1\%** & **0\%** & **0\%** & **2\%** & **1\%** \\   

Table 3: Performances of PAT on defending human-crafted jailbreak attacks on closed-source models. The lowest ASR achieved by defense methods are in **bold**.

### Ablation Study

In this part, we analyze the effect of two key factors: (1) **defense control length**\(|_{def}|\) and (2) **the trade-off between coefficient \(\) and the performances** of PAT. Taking Vicuna-7B as an example, we first craft the defense prefix with varied \(|_{def}|\) or \(\) and evaluate the ASR against three attacks _i.e._ GCG, AutoDAN and PAIR. To measure the benign utility of the model, we take MMLU as the metric for evaluation. For the settings of other configurations, we keep the same as those in Section 4.1. As shown in Figure 4 (a), with the increase of \(|_{def}|\), we see that the ASR of attacks will decrease to near zero. This is because more defense tokens will strengthen the defense effect of PAT, making it more resistant to current attacks. However, an excessively large \(|_{def}|\) might potentially bring a negative impact on the benign utility of models. We conjecture this is because a longer prompt will introduce more redundant information, which is not always helpful for benign conversations. As for the trade-off coefficient \(\), the results in Figure 4 (b) indicate that smaller \(\) means better robustness to existing attacks. But it also means worse benign utility. It is similar to the findings in adversarial training [32; 61; 46]: the robustness and accuracy trade-off also exists for LLMs.

### Adaptive Attack

In the previous sections, we explored scenarios where the defense method is inaccessible to attackers. In this section, a more threatening scenario is considered: we assume that the parameters of the protected model and our defense strategies are both compromised, allowing attackers to perform an adaptive attack on the protected model. This represents a more threatening threat model. To investigate whether our model still maintains reliable robustness under such circumstances, we perform experiments on Vicuna-7B and Llama2-7B.

In Table 4, we summarize the ASR of adaptive attacks on both unprotected and protected models. The results demonstrate that, compared to the unprotected setting, the application of our defense (PAT) significantly enhances the model's resistance to various adaptive attacks, reducing the ASR across different attack methods. For instance, without protection, Vicuna-7B shows an ASR of 92% against the GCG attack, which decreases to 23% when the protection is applied. Similarly, for Llama-2-7B, the ASR against the GCG attack drops from 36% to 12% after applying PAT. This conclusion is consistent across other attack methods such as AutoDAN, PAIR, and TAP, showcasing that PAT can bring reliable robustness to current LLMs.

## 5 Conclusion

In this paper, we introduce a novel defense mechanism termed **Prompt Adversarial Tuning (PAT)**, designed to enhance the robustness of LLMs against the jailbreak attacks while preserving the model's benign utility. More specifically, inspired by the logic of adversarial training, we designed a framework for iteratively updating the attack and defense controls. During the inference stage, the defense control is added before the user's prompt. Due to its short length, it will bring a negligible burden to the model's operational efficiency. Experiments show that PAT not only demonstrates great defense performance under the grey-box setting but also processes excellent transferability across open-source and closed-source models. In addition to automatic attacks, our further studies reveal that PAT can also successfully resist attacks crafted by ordinary persons or adaptive attackers, making it a realistic defense deployed in real life. We hope our work not only explores a novel defense method against Jailbreak attacks but also serves as a cornerstone for building trustworthy large language models.

    &  &  \\   & **Unprotected** & **Protected** & **Unprotected** & **Protected** \\ 
**GCG** & 92\% & 23\% & 36\% & 12\% \\
**AutoDAN** & 72\% & 37\% & 20\% & 9\% \\
**PAIR** & 79\% & 21\% & 60\% & 15\% \\
**TAP** & 55\% & 18\% & 47\% & 13\% \\   

Table 4: ASR of adaptive attack against the unprotected and protected models.