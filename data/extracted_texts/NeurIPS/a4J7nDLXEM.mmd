# Capturing the Denoising Effect of PCA via Compression Ratio

Chandra Sekhar Mukherjee

chandrasekhar.mukherjee@usc.edu

&Nikhil Deorkar

deorkar@usc.edu

Thomas Lord Department of Computer Science, University of Southern California. Research supported by NSF CAREER award 2141536.

Jiapeng Zhang

jiapengz@usc.edu

###### Abstract

Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool. In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general.

In this paper, we propose a novel metric called _compression ratio_ to capture the effect of PCA on high-dimensional noisy data. We show that, for data with _underlying community structure_, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly. We explain this phenomenon through both theoretical proofs and experiments on real-world data.

Building on this new metric, we design a straightforward algorithm that could be used to detect outliers. Roughly speaking, we argue that points that have a _lower variance of compression ratio_ do not share a _common signal_ with others (hence could be considered outliers).

We provide theoretical justification for this simple outlier detection algorithm and use simulations to demonstrate that our method is competitive with popular outlier detection tools. Finally, we run experiments on real-world high-dimension noisy data (single-cell RNA-seq) to show that removing points from these datasets via our outlier detection method improves the accuracy of clustering algorithms. Our method is very competitive with popular outlier detection tools in this task.

## 1 Introduction

Principal component analysis, commonly known as PCA, is one of the most fundamental tools in machine learning. PCA is primarily used as a dimensionality reduction tool that transforms high-dimensional data to lower dimensions for better visualization as well as a heuristic that reduces the complexity of the algorithms that are to be run on the data. On the other hand, it is also known to have certain denoising effects on high-dimensional data. This denoising phenomenon has been observed in different domains, including biological data , speech data , signal measurement data , image data  among others. The denoising effect of PCA has been extensively studied over the last decades .

One of the most fundamental problems in unsupervised learning is the analysis of data in the presence of community structures . This includes clustering of such data , visualization , outlier detection , and others. The primary progress in understanding the denoising effect of PCA has been solely in clustering, particularly in connection to the K-Means algorithm , where PCA in combination with a K-Means based iterative algorithm is shown to provide a good clustering of that dataset with mild assumptions with the underlying community structure.

However, PCA seems to have a more "general" denoising effect in data, as it improves the performance of various downstream algorithms, including clustering  as well community structure preserving graph embedding  and this denoising effect is evident in many real-world datasets.

### Contributions

To this end, we propose a metric, called _compression ratio_, that quantifies PCA's improvement of high dimensional noisy data with underlying community structure in a geometric, and thus algorithm-independent manner 2.

Compression ratio.Let \(\) and \(\) be two data points from a dataset and let \(_{t}\) be the \(t\)-dimensional PCA projection operator. Then the compression ratio between the two points is defined as the ratio between their pre-PCA and post-PCA distance, which is

\[-\|}{\|_{t}()-_{t}()\|}.\]

In a dataset with a community structure, we show the compression ratio for intra-community pairs is higher than that of inter-community pairs even in settings where the pre-PCA inter-community and intra-community distances are very similar. We demonstrate (through a _random vector mixture model_) that this ratio gap reflects the denoising effect of PCA. As a consequence, PCA brings points from the same community much closer, improving the performance of downstream algorithms such as K-Means.

As a motivating byproduct, we show that this metric can be used to design an outlier detection method that can detect points deviating from a community structure. Furthermore, we show that this method can improve the accuracy of clustering algorithms in real-world high-dimensional datasets.

Outlier detection method.Our outlier detection is a simple process inspired by compression ratio. Intuitively, any data point that belongs to an underlying community should have large compression ratios with many points from the same community, whereas it will have a lower compression ratio w.r.t inter-community points. On the other hand, outliers will have more similar compression ratios with all the other points. This difference can be captured by the variance of the list of compression ratios between one point and all of the other points, with outliers having a lower variance of compression. Thus our algorithm simply removes points with low variance of compression.

We analyze this simple algorithm in an extension of the standard random vector mixture model. We also compare our algorithm with popular algorithms such as the Local Outlier Factor (LOF) method  and KNN-dist  as well as more recent methods such as Isolation forest  and ECOD  through both simulations and experiments on real-world data. We show that this simple algorithm is very competitive with those popular outlier detection tools.

Overall, we believe the effect of PCA on denoising becomes more significant if for each datapoint, there are many data points with large compression ratio variance.

Real world experimentsFinally, we test the relevance of compression ratio as a metric and the outlier detection method in real-world data. We focus on single-cell data, as it is both high dimensional (\(20,000-40,000\) dimension) and noisy , using datasets from a popular benchmark database  with ground truth community labels. We first show that the average intra-community compression ratio is higher than the average inter-community compression ratio in all of the datasets. We then show that removing outliers in these datasets via our variance of compression technique improves the performance of clustering algorithms, such as PCA+K-Means, where we again outperform standard outlier detection methods.

Organization of the paperIn Section 2, we discuss our theoretical analysis. Concretely, we define the random vector mixture model and provide bounds on the intra-community and inter-community compression ratios. Next, we define our outlier detection metric and justify it in an extension of our generative model. Section 3 contains the simulation results validating the compression ratio metric and we also compare the performance of our outlier detection method with other methods. Finally in Section 4 we demonstrate that PCA exhibits an average version of compression ratio in real-world biological datasets  and then test our outlier detection-based clustering accuracy improvement idea discussed above.

### Related Works

PCA and its effect on noisy data has been subject to a lot of investigation in the last 50 years. Before 2008, most of the work focused on the asymptotic setting, where the number of points (\(n\)) and/or the dimension (\(d\)) are infinite (see  and the references therein). In the last two decades, several works have also considered the finite sample setting . These works have primarily focused on the denoising aspect of PCA in different variants of Gaussian noise. In a recent line of work  has studied the subspace recovery problem in the presence of bounded and (nice) sub-Gaussian noise. However, there seems to be no direct way to convert these results into a clustering setting. In comparison, we study PCA's denoising effect on data in random vector mixture model via the compression ratio metric, where the noise can be heavy sub-Gaussian.

PCA in clustering tasksWith regards to PCA's impact on data with community structure, the primary work has been in connection to K-Means. Here, one of the first works  showed that the outcome of PCA can be viewed as an approximation result to the K-Means outcome in clustering data. In this direction, a lot of progress has been made in the last two decades.

A beautiful recent work  has shown that PCA followed by several iterations of K-Means along with modifications can cluster data with reasonable parameters in the random vector mixture model that we discuss here, which was then improved in . Both of the works focused on the setting of \(n d\) (for example,  worked with \(n d^{8}\)). More recently, tighter results have been obtained in the context of the Gaussian-mixture model in  (still on the setting of \(n d^{2}\)).

In comparison, we study PCA's relative compression in an algorithm-independent fashion, focusing on its effect on the geometry of the data in the high-dimensional setting of \(n=(d)\) with sub-Gaussian noise. We are motivated to analyze this setting as single-cell datasets often have \(n<d\).

## 2 Random vector model and relative compression of PCA

To theoretically study the relative compression of PCA, we use a high-dimensional mixture model, similar to one in . We call this a random vector mixture model. This can also be interpreted as a signal-plus-noise model where the signal imposes a community structure on the data. The dataset of interest is a set of \(n\) many \(d\) dimensional real vectors \(_{}^{d},1 i n\), which is together represented as the dataset \(X\). We express \(X\) as a \(d n\) matrix, with each column representing a data point. The data points have an underlying hidden community structure that is expressed as a partition of \([n]:=\{1,,n\}\) into \(k\) many sets \(V_{1},,V_{k}\) such that each \(i[n]\) lies in any one \(V_{j}\). We then have the following problem structure.

1. Each cluster \(V_{j},1 j k\) is associated with a ground truth center \(_{}^{d}\).
2. Additionally, each cluster \(V_{j}\) is associated with a distribution \(^{(j)}\) such that \(^{(j)}\) is a _coordinate wise independent zero mean_ distribution. For ease of exposition, we define the support of \(^{(j)}\) to be \([-,]^{d}\) for some \(\) (which can also depend on \(n,d\)), but our methods also directly apply to sub-Gaussian distributions where each coordinate has a constant sub-Gaussian norm (resulting in \(()\) norm of any column vector). Then \(\) would be replaced with \(C^{} n\) for some constant \(C^{}\) in our bounds.

Then, the dataset \(X\) is set up as follows.

**Definition 2.1** (Random vector mixture model).: For each \(i[n]\), if \(i V_{j}\), then \(}=}+}\) where \(}^{(j)}\), i.e. \(}\) is independently sampled from \(^{(j)}\). Here we abuse notation and denote both \(i V_{j}\) as well as \(} V_{j}\).

With this setup, now we define the PCA projection operator and the compression ratio metric formally.

**Definition 2.2** (The PCA operator \(_{X}^{k^{}}\)).: Let \(X\) be a \(d n\) matrix. Then the \(k^{}\) dimensional PCA projection operator is simply the projection operator onto the first \(k^{}\) principal components of \(X\).

Next we formally define the compression ratio metric.

**Definition 2.3**.: For any pair \((i,i^{})\) we define the \(k^{}\)-PC compression of the pair of vectors in \(X\) as

\[_{X,k^{}}(i,i^{})=}-}}\|} {\|_{X}^{k^{}}(})-_{X}^{k^{}}(}})\|}\]

Before describing our results, we define certain parameters of the model.

1. The maximum variance of the entries, \(\) is defined as \(^{2}=_{1 j j,1 d}[_{} ^{(j)}]\)
2. The average variance of a column in a distribution \(^{(j)}\), noted as \(_{j}\) is defined as \(_{j}^{2}=_{}Var([_{}^{(j)}])\). Here, \(_{j}\) is the perturbation on the data points of \(V_{j}\) due to the noise.

In this direction, we first lower, and upper bound the \((k-1)\)-PC intra-community and inter-community compression ratios respectively, as a function of the maximum variance, average variances, spectral structure of the noise and signal, and distance between the centers of the model, which can be found in Theorem B.1.

Although our result applies to any set of centers, the spectral properties of the resultant matrix, and their interactions make the result hard to interpret. To give more insight into our bounds, we instead define a restricted (still natural) structure on the centers, which allows us to give a more interpretable result in this case. For simplicity, we also work in the setting where \(d 10 n\).

**Definition 2.4** (Spatially unique centers).: We say a set of vectors \(=\{},,}\}\) are \(\)-spatially unique, if we have that

\[_{1 j k}\ _{()})}\|}-\|\]

This implies that each center has a unique pattern that cannot be approximated by a combination of the other centers. Here note that \(_{j j^{}}\|}-}}\|\). For example, such a property is expected if the centers are mutually orthogonal. One can also think of them as vertices in a high-dimensional regular polygon. Then, we give some sufficient conditions for the separation of intra-community and inter-community compression ratios of PCA.

**Theorem 2.5** (Separation of compression ratio).: _Let \( C\{d^{1/4},+ n\}\) for some constant \(C\). Furthermore, let \(i i^{}\) denote that \(}\) and \(}}\) belong to the same underlying community. Then, the following holds._

1. _The perturbation of the points due to noise can be much larger than the distance between the community centers, i.e., the noise dominates the distance between the centers._
2. _With probability_ \(1-(1/n)\)_,_ \(_{(i,i^{}):i i^{}}_{X,k-1}(i,i^{}) 4 _{(i,i^{}):i i^{}}_{X,k-1}(i,i^{})\)__

This shows that the compression ratio of PCA provides a separation between intra-community and inter-community pairs even in a setting where the noise highly dominates the distance between the centers. One can find a more general theorem w.r.t spatially unique centers in Theorem C.4.

A natural question is whether post-PCA distance is a good metric for denoising due to PCA. In this regard, we point out that the compression ratio has an added normalization property. For example, consider the case where all pair-wise center distances are the same. In such a case, the post-PCA distances are dependent on \(_{j}\), so communities with larger variances have larger intra-community distances. However, this gets normalized in the compression factor as per Equation (9) of Theorem C.4, as the numerator also has a dependency on \(_{j}\).

### Outlier detection with compression ratio

Now, we discuss the usefulness of compression ratio on outlier detection. We first describe the notion of variance of compression ratio.

**Definition 2.6** (Variance of compression ratio).: Given a dataset \(X\) and a PCA dimension \(k^{}\), variance of compression ratio of a point \(\) is defined as

\[_{X,k^{}}(})=(\{_{X,k^{ }}(i,i^{})\}_{i^{} i}) \]

where \(_{X,k^{}}(i,i^{})=}-}}\|}{ \|_{X}^{k}(})-_{X}^{k^{}}(}})\|}\) is the compression ratio between points \(i\) and \(i^{}\).

That is, it is simply the variance of the list of compression ratios of \(}\) with all the other points \(}}\).

Then, our intuition is that if data consists of many points from the high dimensional mixture model, as well as several outlier points that don't share a common signal (center), they have a lower variance of compression ratio. We concretize this notion with the following simple detection algorithm 1.

```
Input: data \(X\), PCA dimension \(k^{}\), number of outliers \(o\). for\(i=1\)to\(n\)do \(SC[i]_{X,k^{}}(})\) (\(_{X,k^{}}\) defined in Eq. 1) endfor return\(o\) many indexes with lowest \(SC\) values.
```

**Algorithm 1** Outlier detection via variance of compression ratio

Mixture-model with outliersNow let us consider an extension of the mixture model in Definition 2.1 to incorporate outliers.

**Definition 2.7** (Mixture model with outliers).: Let \(X\) be a \(d n\) dataset with the partition \(V_{1},,V_{k},\), a set of \(k\) centers \(\{}\}_{j=1}^{k}\) and distributions \(\{^{(j)}\}_{j=1}^{k}+1\) with the following generation method.

1. _clean points:_ If \(i V_{j},1 j k\), \(}=}+}\) where \(}\) is sampled from \(^{(j)}\).
2. _outliers:_ If \(i\), then we sample \(p_{i,1}, p_{i,k}[0.5,1]\). Then \(}=_{j}_{i,j}}+}\) where \(_{i,j}=}{_{j}p_{i,j}}\) and \(}\) is sampled from \(^{(k+1)}\).

Let \(||=n_{o}\) and \(n=n_{o}+n_{c}\). To keep the results simple, we make the average variance of each distribution \(^{(j)}\)_Then, for any \(n_{0} n/2\), the first \(n_{0}\) points ranked by Algorithm 1 all belong to the outlier group (\(\)) with probability \(1-o(1)\)._

We discuss the connection between our results and the role of spatially unique centers in Appendix C. The proof of Theorems 2.5 and 2.8 can be found in Appendix C and C.2 respectively.

This gives us initial theoretical evidence that in the random-mixture model with outliers, our simple outlier detection method can detect outliers when a non-negligible fraction of the points are outliers. Next, we use simulations of our model to test the efficacy of our outlier detection method and its impact on the community structure of data and compare them with some popular outlier detection methods.

## 3 Simulations for outlier detection

In this section, we first describe different instantiations of the random-vector mixture model, observe the intra-community and inter-community compression ratios in them, and then run simulations in the outlier mode. All simulations and experiments were run on a 2020 M1 MacBook Pro with 16 GB of memory within 1.5 hours of total running time.

Simulation setupFor this setup, we set \(n=3000,d=1000\), and \(k=3\), with each community having the same number of points. For simplicity, we choose \(3\) equidistant centers, with \(\|}-}\|=\). We set the noise distributions to be Bernoulli distributions with variance \(_{1},_{2},_{3}\) respectively. We work in two primary settings, of equal and unequal noise.

i) _Equal noise_. Here we have \(_{j}=\) for all \(i\). ii)_Unequal noise_. Here one of the communities has variance \(2\), whereas all the other communities have variance \(\).

Then, we test the algorithms for three values of \(\) in the following manner.

* _Low noise:_ We choose \(:\|}-}}\| 3\). This implies distance between the centers dominates the perturbation due to noise.
* _Significant noise:_ Here \(:\|}-}}\|\). Here the noise norm and distance between centers are of the same order.
* _High noise:_ We have \(:\|}-}}\| 0.3\). Here the noise heavily dominates the center distances.

Let us look at the equal noise setting, i.e. the case where the variance of noise distributions for all communities are the same. We observe that in the low-noise setting, all intra-community compression ratios are higher than all inter-community compression ratios. As the noise increases, the gap between them decreases, so that in the high-noise setting, there is now an overlap between intra-community and inter-community compression ratios. We demonstrate this in Figure 0(a). This further indicates that compression ratio is indeed a useful metric even when the noise has a strong perturbation effect on the data (even though there will be no clean separation between intra-community and inter-community compression ratios once the noise is very high).

Figure 1: Simulation results for compression ratio and outlier detection

### Outlier detection

Now, we discuss the outlier detection, starting with the simulation setup in this case. We follow the random-mixture-outlier model and add outlier points along with the clean points as follows.

We add \(n_{o}=n_{c}/10\) outliers following definition 2.7. That is, we randomly choose \(_{1},,_{3}\) and then a random mixture-center is chosen as \(_{j}_{j}}\), and then we add a random noise vector from the Bernoulli distribution of variance \(_{4}\).

Outlier detection algorithms for comparisonsOutlier detection has been an active area of study in unsupervised learning, providing several influential algorithms. In a recent, comprehensive benchmarking of outlier detection algorithms,  compared the performance of several unsupervised learning algorithms on different datasets. They found that for unsupervised outlier detection methods, success was related to whether the underlying model of the data assumed by the outlier detection method followed the dataset at hand. They found that for local outliers, the popular Local Outlier Factor (LOF) method  performed the best statistically, whereas for global outliers, KNN-dist (where the outlier score is simply the distance to the \(K\)-th nearest neighbors)  performed the best. Owing to their generally impressive performance, we use them for comparison with our variance of compression method. Furthermore, we select a popular method called Isolation forest  and also a very recent and popular outlier detection method ECOD . We also use PCA+method for each of the methods as benchmarks, as both outliers and clean points are perturbed by zero-mean noise, and we now understand PCA can help mitigate the effect of said noise, as discussed in Section 2.

Outlier detection resultsWe compare the AUROC values of the \(5\) outlier methods of interest in these settings. We record the results in Figure 1(a) and 1(b) for the equal and unequal noise settings respectively.

As we can see, our method results in the highest AUROC value, followed by PCA+KNN-dist. we make two observations.

i) The performance gap between variance-of-compression and the next best method is higher in the unequal noise setting.

ii)As the noise level increases, the gap between our method and PCA+K-NN dist increases.

These two points further highlight the compression ratio's normalizing effect as well as effectiveness in high noise settings.

Here we remark that in real-world data, while some points may indeed behave like outliers, they need not all be the same kind of outlier. Thus, we would like to verify our method's performance in the presence of a different kind of outlier, which we concretize below.

Higher variance-based outliersWe consider the case that some points may have significantly higher noise perturbations than others. In this setting, we randomly select some points, and we generate some points with \(c\) coordinate-wise variance, where \(c=8\) for our experiments (recall that the noise in the other points has a coordinate-wise \(\) variance). It is well known that if noise is low-dimensional, then such outliers are well captured by LOF. We observe that while in the low noise setting our performance is worse than the other methods, as the overall noise increases, the performance of our method is more comparable to the other methods. We record the results in Figure 0(b).

Figure 2: Comparison of outlier removal in different noise settingsThis shows that our outlier detection method is adept at detecting different kinds of outliers, outperforming popular outlier detection tools in some settings, and being competitive to them in others. We also observe that as the overall noise in the dataset increases, the performance of our method compared to the other outlier detection tools improves. This further highlights the power of compression ratio when especially dealing with noisy data. Having demonstrated the validity of our outlier detection method in two different settings, across different noise levels, we now focus on real-world datasets.

## 4 Real world experiments

### Datasets of interest

In this section, we provide experimental results to exhibit the validity of compression ratio as a metric and the usefulness of our outlier detection method in improving the community structure of datasets. We focus on single-cell RNA sequencing datasets. The dataset consists of \(n\) many data points, each corresponding to a cell. The features are gene expressions, and for the cell, the expression of some \(d 10,000\) genes are recorded. A fundamental problem here is to identify sub-populations of interest. However, the problem is challenging as the biological process of recording gene expressions is error-prone , and gene expressions within the same population may also vary due to internal randomness. Furthermore, experiments can cause cells to get merged during gene-expression recording . This makes single-cell RNA sequencing data a good testing ground for high dimensional noisy data with outliers and underlying community structure.

In this direction, we consider the single-cell RNA sequencing datasets from a benchmark paper . These datasets also have moderate to highly reliable ground truth labels, that help us understand the usefulness of our metrics and our algorithm. These datasets vary in the number of cells, genes, clusters, cells per cluster, and the "difficulty" of clustering. A summary of the datasets is provided in Table 4 in Appendix E.1.

### Average compression in the datasets

As discussed in Section 2 and described in Theorem 2.5, our primary result showed that the intra-community compression ratios are higher than inter-community compression ratios in a large range of parameters. Here we look at average statistics of compression ratio to provide a first layer of evidence supporting this phenomenon in real-world data. We define the following metric. For any community \(V_{j}\), we define the average intra-community compression ratio as \(}_{X,k^{}}(V_{j})=}_{i,i^ {} V_{j}}[_{i}-_{i^{}}\|}{\|_{X}^{ }(_{i}-_{i^{}})\|}]\) Similarly, the average inter-community compression ratio is defined as \(}_{X,k^{}}(V_{j})=}_{i V _{j},i^{}[n]_{V_{j}}}[_{i}-_{i^{}}\|} {\|_{X}^{}(_{i}-_{i^{}})\|}]\). This gives an average measurement of the compression ratio in the dataset. In this regard, we find that for each of the 9 datasets and each of the communities in the dataset, the intra-community compression ratio is higher than the inter-community compression ratio. We provide the results in the Appendix E.2. Here, for brevity we present the average of \(}_{X,k-1}(V_{j})\) and \(}_{X,k-1}(V_{j})\) for each dataset in Table 1.

[MISSING_PAGE_FAIL:9]