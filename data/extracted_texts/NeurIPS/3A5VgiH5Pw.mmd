# Towards Multi-dimensional Explanation Alignment

for Medical Classification

Lijie Hu\({}^{,1,2,3}\), Songning Lai\({}^{,1,2,3}\), Wenshuo Chen\({}^{,1,2}\), Hongru Xiao\({}^{4}\)

**Hongbin Lin\({}^{3}\), Lu Yu\({}^{6}\), Jingfeng Zhang\({}^{5,7}\), and Di Wang\({}^{1,2}\)**

\({}^{1}\)Provable Responsible AI and Data Analytics (PRADA) Lab

\({}^{2}\)King Abdullah University of Science and Technology

\({}^{3}\)HKUST(GZ) \({}^{4}\)Tongji University \({}^{5}\)The University of Auckland

\({}^{6}\)Ant Group, \({}^{7}\)RIKEN Center for Advanced Intelligence Project (AIP)

###### Abstract

The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, as well as issues related to efficiency. To address these limitations, we propose a novel framework called **Med-MICN** (**Med**ical **M**ulti-dimensional **I**nterpretable **C**oncept **N**etwork). Med-MICN provides interpretability alignment for various angles, including neural symbolic reasoning, concept semantics, and saliency maps, which are superior to current interpretable methods. Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets. To demonstrate the effectiveness and interpretability of Med-MICN, we apply it to four benchmark datasets and compare it with baselines. The results clearly demonstrate the superior performance and interpretability of our Med-MICN.

## 1 Introduction

The field of medical image analysis has witnessed remarkable advancements, especially for the deep learning models. Deep learning models have exhibited exceptional performance in various tasks, such as image recognition and disease diagnosis , with an opaque decision process and intricate network. However, this lack of transparency is particularly problematic in the medical domain, making it challenging for physicians and clinical professionals to trust the predictions made by these deep models. Thus, there is an urgent need for the interpretability of model decisions in the medical domain .

The medical field has strict trust requirements. It not only demands high-performing models but also emphasizes comprehensibility and earning the trust of practitioners . Thus, Explainable Artificial Intelligence (XAI) has emerged as a prominent research area in this field. It aims to enhance the transparency and comprehensibility of decision-making processes in deep learning models and large language models by incorporating interpretability . Various methods have been proposed to achieve interpretability, including attention mechanisms , saliency maps , DeepLIFT and Shapley values , influence functions . These methods strive to provide users with visual explanations that shed light on the decision-making process of the model. However, while these post-hoc explanatory methods offer valuable information, there is still a gap between their explanations and the model decisions . Moreover, these post-hocexplanations are generated after the model training and cannot actively contribute to the model fine-tuning process, hindering them from being a faithful explanation tool.

Thus, there is increasing interest among researchers in developing self-explanatory methods. Among these, concept-based methods have garnered significant attention [50; 2; 35]. Concept Bottleneck Model (CBM)  initially predicts a set of pre-determined intermediate concepts and subsequently utilizes these concepts to make predictions for the final output, which are easily understandable to humans. Concept-based explanations provided by inherently interpretable methods are generally more comprehensible than post-hoc approaches. However, most existing methods treat concept features alone as the determination of the predictions. This approach overlooks the intrinsic feature embeddings present within medical images, thus degrading accuracy . Moreover, while these concepts are human-understandable, they lack semantic meanings, thus questioning the faithfulness of their interpretability . To improve further human trust, several recent works  aim to leverage syntactic rule structures to concept embeddings. However, there are still several potential issues. First, unlike CBMs, current concept models with logical rules mainly focus on the supervised concept case, which is quite strict for biomedical images as concept annotation is expensive. Second, while current concept models (with logical rules) provide interpretations via concepts, we found that the importance of these concepts is misaligned with other explanations, especially the explanation given by saliency maps [70; 16]. This will lead to a possible reduction in human trust when using these models.

To address these challenges, we introduce a new and innovative end-to-end concept-based framework called the **Med-MICN** (**Medical Multi-dimensional Interpretable**C**oncept **N**etwork), as illustrated in Figure 3. As shown in Figure 2, Med-MICN is an end-to-end framework that leverages Large Multimodals (LMMs) to generate concept sets and perform auto-annotation for medical images, thereby aligning concept labels with images to overcome the high cost associated with medical concepts annotation. In contrast to typical concept-based models, our interpretation is notably more diverse and precise (shown in Figure 1). Specifically, we map the image features extracted by the backbone through a concept encoder to obtain concept prediction and concept embeddings, which are then input into the neural symbolic layers for interpretation. This also establishes alignment between image information and concept embeddings by utilizing a concept encoder, leading to the derivation of predictive concept scores. Furthermore, we align concept semantics with concept embeddings by incorporating neural symbolic layers. Thus, we effectively align image information with concept semantics and concept saliency maps, achieving comprehensive multidimensional alignment. Additionally, unlike most concept-based methods, we use concept embeddings to complement the original image features, which enhances classification accuracy without any post-process. Our main contributions can be summarized as follows:

Figure 1: Med-MICN demonstrates multidimensional interpretability, encompassing concept score prediction, concept reasoning rules, and saliency maps, achieving alignment within the interpretative framework. The ’Peripheral ground-glass opacities’ is \(c_{0}\), and along the y-axis, it sequentially becomes \(c_{1},,c_{7}\).

* We proposed an end-to-end framework called Med-MICN, which leverages the strength of different XAI methods such as concept-based models, neural symbolic methods, saliency maps, and concept semantics. Moreover, Med-MICN generates rules and utilizes concept embeddings to complement the intrinsic medical image features, which improves accuracy.
* Med-MICN offers an alignment strategy that includes text and image information, saliency maps, and concept semantics. It is model-agnostic and can easily transfer to other models. Our outputs are interpreted in multiple dimensions, including concept prediction, saliency maps, and concept reasoning rules, making it easier for experts to identify and correct errors.
* Through extensive experiments on four benchmark datasets, Med-MICN demonstrates superior performance and interpretability compared with other concept-based models and the black-box model baselines.

## 2 Related Work

**Concept Bottleneck Model.** The Concept Bottleneck Model (CBM)  has emerged as an innovative deep-learning approach for image classification and visual reasoning by incorporating a concept bottleneck layer into deep neural networks. However, CBM faces two significant challenges. Firstly, its performance often falls short of the original models without the concept bottleneck layer, attributed to incomplete information extraction from the original data to bottleneck features. Secondly, CBM extensively depends on meticulous dataset annotation. To solve these problems, researchers have delved into potential solutions. For example,  have extended CBM into interactive prediction settings by introducing an interaction policy to determine which concepts to label, ultimately improving the final predictions. Additionally,  has addressed the limitations of CBM by proposing a novel framework called Label-free CBM, which offers promising alternatives. Post-hoc Concept Bottleneck models  can be applied to various neural networks without compromising model performance, preserving interpretability advantages. Despite much research in the image field [18; 33; 32; 45; 47; 36; 28; 23; 37], concept-based method for the medical field remains less explored, which requires more precise results and faithful interpretation.  used a conceptual alignment deep autoencoder to analyze tongue images representing different body constituent types based on traditional Chinese medicine principles.  introduced CBM for osteoarthritis grading and used ten clinical concepts such as joint space narrowing, bone spurs, calcification, etc.

However, previous research heavily relies on expert annotation datasets or often focuses solely on concept features to make predictions while overlooking the intrinsic feature embeddings within images. Furthermore, while the concept neural-symbolic model has been explored in the graph domain , its application to images, particularly in the medical domain, has been largely absent. Additionally, our work addresses these gaps by proposing an end-to-end framework with an alignment strategy that leverages various explainable methods, including concept-based models, neural-symbolic methods, saliency maps, and concept semantics, to provide comprehensive solutions to these challenges.

**Explanation in Medical Image.** Research on the interpretability of deep learning in medical image processing provides an effective and interactive approach to enhancing medical knowledge and assisting in disease diagnosis. User studies involving physicians have revealed that doctors often seek explanations to understand AI results, especially when the outcomes are related to their own hypotheses or differential diagnoses . They also turn to explanations to resolve conflicts when their judgments differ from those of AI , thereby enhancing the intelligence of medical models. Previous studies have visualized lesion areas through methods such as heatmaps  and attention visualization , aiding in the identification of lesion regions and providing visual evidence. Additionally, utilizing language model-based methods like LLM or LMM to generate medical reports complements the interpretation of model results (ChatCAD , XrayGPT , Med-PaLM ). Saliency maps have emerged as the most common and clinically user-friendly explanation for medical imaging tasks [53; 68; 69]. Recent research underscores the importance of understanding the pivotal features influencing AI predictions, particularly when clinicians must compare AI decisions with their own clinical assessments in cases of decision incongruity . In addition to the image's intrinsic feature recognition, assisted discrimination methods based on concept injection are widely employed in assisted medical diagnosis [35; 7]. Compared to relying solely on self-supervised training, conceptual feature-based supplementation integrates expert knowledge, offering more accurate assistance for interpreting detection results.

However, previous research on medical images relies on single-dimensional explanations, potentially lacking sufficient decision information for physicians. Furthermore, erroneous single-dimensional explanations could significantly impact physicians' judgments. Thus, there is a pressing need for a multi-dimensional explanatory framework where explanations across various dimensions complement each other. In instances of incorrect explanations, physicians can turn to explanations in alternative dimensions to aid their judgment.

## 3 Preliminaries

**Concept Bottleneck Models.** To introduce the original Concept Bottleneck Models, we adopt the notations used by . We consider a classification task with a concept set denoted as \(=C_{1},C_{2},,C_{N}\)and a training dataset represented as \(\{(x_{i},y_{i},c_{i})\}_{i=1}^{M}\). Here, for \(i[M]\), \(x_{i}^{d}\) represents the feature vector, \(y_{i}^{d_{z}}\) denotes the label (with \(d_{z}\) corresponding to the number of classes), and \(c_{i}^{d_{c}}\) represents the concept vector. In this context, the \(j\)-th entry of \(c_{i}\) represents the weight of the concept \(p_{j}\). In CBMs, our goal is to learn two representations: one that transforms the input space to the concept space, denoted as \(g:^{d}^{d_{c}}\), and another that maps the concept space to the prediction space, denoted as \(f:^{d_{c}}^{d_{z}}\). For any input \(x\), we aim to ensure that its predicted concept vector \(=g(x)\) and prediction \(=f(g(x))\) are close to their underlying counterparts, thus capturing the essence of the original CBMs.

**Fuzzy Logic Rules.** As described by [17; 3], continuous fuzzy logic extends upon traditional Boolean logic by introducing a more nuanced approach to truth values. Rather than being confined to the discrete values of either 0 or 1, truth values are represented as degrees within the continuous range of {0, 1}. Conventional Boolean connectives including t-norm \(:\), t-conorm \(:\), negation \( x=1-x\). The logical connectives, including \(,,,,\), are utilized to convey the logical relationships between concepts and their representations. For example, consider the problem of deciding whether an X-ray lung image has COVID, given the vocabulary of concepts "ground-glass opacities (GO)," "Localized or diffuse presentation (LDP)," and "lobar consolidation (LC)." A simple decision rule can be \(y c_{GO} c_{LC}\). From this rule, we can deduce that (1) Having both "no LC" and "GO" is relevant to having COVID. (2) Having LDP is irrelevant to deciding whether COVID exists.

Figure 2: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.

## 4 Medical Multi-dimensional Interpretable Concept Network

Here, we present Med-MICN (Figure 3), a novel framework that constructs a model in an automated, interpretable, and efficient manner. (i) In traditional CBMs, the concept set is typically generated through annotations by human experts. When there is no concept set and concept labels, we first introduce the automated concept labeling alignment process (Figure 2). (ii) Then, the concept set (output by LLMs such as GPT4-V) is fed into the text encoder to obtain word embedding vectors. Our method utilizes Vision-Language Models (VLMs) to encode the images and calculate cosine distances to generate heatmaps. We apply average pooling to these heatmaps to obtain a similarity score aligned with the concept set through a threshold to obtain concept labels. (iii) Next, we extract image features using a feature extraction network and then map them through a concept encoder to obtain concept embeddings. (iv) We finally use these concept embeddings as input into the neural symbolic layers to generate concept reasoning rules and incorporate them as complementary features to the intrinsic spatial features for predictions, proving multi-dimensional interpretation alignment. We provide details for each component of Med-MICN as follows.

### Concept Set Generation and Filtering

Given \(M_{c}\) classes of target diseases or pathology, the first step of our paradigm is to acquire a set of useful concepts related to the classes. A typical workflow in the medical domain is to seek help from experts. Inspired by the recent work, which suggests that instruction-following large language models present a new alternative to automatically generate concepts throughout the entire process [41; 40]. We propose to generate a concept set using LMMs, such as GPT-4V, which has extensive domain knowledge in both visual and language, to identify the crucial concepts for medical classification. Figure 2 (a) illustrates the framework for concept set generation. Details are in Appendix A.

### VLMs-Med-based Concept Alignment

**Generating Concept Heatmaps.** Suppose we have a set of \(N\) useful concepts \(=\{C_{1},C_{2},,C_{N}\}\) obtained from GPT-4V. Then, the next step is to generate pseudo-(concept)

Figure 3: Overview of the Med-MICN Framework. The Med-MICN framework consists of four primary modules: (1) **Feature Extraction Module**: In the initial step, image features are extracted using a backbone network to obtain pixel-level features. (2) **Concept Embedding Module**: The extracted features are fed into the concept embedding module. This module outputs concept embeddings while passing through a category classification linkage layer to obtain predicted category information. (3) **Concept Semantic Alignment**: Concurrently, a Vision-Language Model (VLM) is used to annotate the image features, generating concept category annotations aligned with the predicted categories. (4) **Neural Symbolic Layer**: After obtaining the concept embeddings, they are input into the Neural Symbolic layer to derive conceptual rules. Finally, the concept embeddings obtained from module (2) are concatenated with the original image embeddings and fed into the final category prediction layer to produce the ultimate prediction results.

labels for each image in the dataset corresponding to its concept set. Inspired by the satisfactory performance in concept recognition within the medical field demonstrated by VLMs , we use BioViL  to generate the pseudo-labels for the concepts of each image. Figure 2 (b) illustrates the detailed automatic annotation process of concepts.

Given an image \(x\) and a concept set, its feature map \(V^{H W D}\) and the text embedding \(t_{i}\ ^{D}\) for each concept are extracted as follows:

\[V=_{V}(x), t_{i}=_{T}(c_{i}), i=1,,N\]

where \(_{V}\) and \(_{T}\) are the visual and text encoders, \(t_{i}\) is the embedding of the \(i\)-th concept in the concept pool, \(H\) and \(W\) are the height and width of the feature map.

Given \(V\) and \(t_{i}\), we can obtain a heatmap \(P_{i}\), i.e., a similarity matrix that measures the similarity between the concept and the image can be obtained by computing their cosine distance:

\[P_{h,w,i}=^{T}V_{h,w}}{||t_{i}||||V_{h,w}||}, h=1,,H, w=1,,W\]

where \(h,w\) are the \(h\)-th and \(w\)-th positions in the heatmaps, and \(P_{h,w,i}\) represents a local similarity score between \(V\) and \(t_{i}\). Then, we derived heatmaps for all concepts, denoted as \(\{P_{1},P_{2},,P_{N}\}\).

**Calculating Concept Scores.** As average pooling performs better in downstream medical classification tasks , we apply average pooling to the heatmaps to deduce the connection between the image and concepts: \(s_{i}=_{h=1}^{H}_{w=1}^{w}H_{h,w,i}\). Intuitively, \(s_{i}\) is the refined similarity score between the image and concept \(c_{i}\). Thus, a concept vector \(e\) can be obtained, representing the similarity between an image input \(x\) and a set of concepts: \(e=(s_{1},,s_{N})^{T}\).

**Alignment of Image and Concept Labels.** To align images with concept labels, we determine the presence of a concept attribute in an image based on a threshold value derived from an experiment. If the value \(s_{i}\) exceeds this threshold, we consider the image to possess that specific concept attribute and set the concept label to be _True_. We can obtain concept labels for all images \(c=\{c_{1},,c_{M}\}\), where \(C_{i}\{0,1\}^{N}\) is the concept label for the \(i\)-th sample. Finally, to ensure the truthfulness of concepts, we discard all concepts for which the similarity across all images is below 0.45. To achieve higher annotation accuracy, we only annotated 20% of the data for fine-tuning the model to adapt to different datasets. We sampled 10% of the pseudo-labels generated and compared them with expert annotations . It is notable that in the case where the concept set and concept labels are given, we can directly skip Section 4.1 and 4.2.

### Multi-dimensional Alignment

In Section 4.2, we get the concept labels for all input images. However, as we mentioned in the introduction, such representation might significantly degrade task accuracy [39; 67]. To overcome this issue, recently  propose using concept embeddings, which increase the task accuracy of concept-based models while weakening their interpretability. Motivated by this, we use these concept embeddings to increase the accuracy and leverage our concept label to enhance interpretability. In the following, we provide details.

**Concept Embeddings.** For the training data \(X=\{(x_{m},y_{m})\}_{m=1}^{M}\), we use a backbone network (e.g., ResNet50) to extract features \(=\{f(x_{m})\}_{m=1}^{M}\). Then, for each feature, it passes through a concept encoder  to obtain feature embeddings \(f_{c}(x_{m})\) and concept embeddings \(_{m}\). The specific process can be represented by the following expression:

\[f(x_{m})=_{b}(x_{m}),f_{c}(x_{m}),_{m}=_{c}(f(x_{m})) { for }[M],\]

where \(_{b}\) and \(_{c}\) represent the backbone and concept encoder, respectively.

To enhance the interpretability of concept embeddings, we utilize binary cross-entropy to optimize the accuracy of concept extraction by computing \(_{c}\) based on \(=\{_{m}\}_{m=1}^{M}\) and concept labels \(c\) in Section 4.2:

\[_{c}=BCE(,c).\] (1)

**Neural-Symbolic Layer**. Our next goal is to use concept embedding to learn concept rules for prediction, which is motivated by . We aim to generate rules involving the utilization of two sets of feed-forward neural networks: \(()\) and \(()\). The output of \(()\) signifies the assigned role of each concept, determining whether it is positive or negative (such as "no LC" and "GO"). On the other hand, the output of \(()\) represents the relevance of each concept, indicating whether it is useful within the context of the sample feature (such as "LDP"). The overall process can be divided into three distinct parts: (i) Learning the role (positive or negative) of each concept called _Concept Polarity_. For each prediction class \(j\), there exists a neural network \(_{j}()\). This network takes each concept embedding as input and produces a soft indicator, a scalar value in the \(\) range. This soft indicator represents the role of the concept within the formula; (ii) Learning the relevance of each concept called _Concept Relevance_. Similar to concept polarity, for each prediction class \(j\), a neural network \(_{j}()\) is utilized; (iii) Output the logical reasoning rules of the concept and the contribution scores of concepts. For each class \(j\), we combine the previous concept polarity vector \(I_{o,j}\) and the concept relevance vector \(I_{r,j}\) to obtain the logical inference output. This is achieved by the following expression (Details are in Appendix B):

\[_{j}=_{i=1}^{N}( I_{o,i,j} I_{r,i,j})=_{i[N]}\{ \{1-I_{o,i,j},I_{r,i,j}\}\}.\] (2)

### Final Objective

In this section, we will discuss how we derive the class of medical images and the process of network optimization. First, we have the loss \(_{c}\) in (1) for enhancing the interpretability of concept embeddings. Also, as the concept embeddings are input into the neural-symbolic layer to output logical reasoning rules of the concept and prediction \(_{neural,m}\) in (2) for \(x_{m}\), we also have a loss between the predictions given by concept rule and ground truth, which corresponds to the interpretability of our neural-symbolic layers. In the context of binary classification tasks, we employ binary cross-entropy (BCE) as our loss function. For multi-class classification tasks, we use cross-entropy as the measure. Using binary classification as an example, we calculate the loss \(_{neural}\) by comparing the output \(}\) from the neural-symbolic layer to the label \(\) as follows:

\[_{neural}=BCE(}_{neural},),\] (3)

**Classification Loss.** Note that as \(_{neural}\) in (3) is purely dependent on the concept rules rather than feature embeddings, we still need a loss for final prediction performance. In a typical classification network, the process involves obtaining the feature \(f(x_{m})\) and passing it through a classification head to generate the classification results. What sets our approach apart is that we fuse the previously extracted \(f_{c}(x_{m})\) with the \(f(x_{m})\) using a fusion module as input to the classification ahead. This can be expressed using the following formula:

\[_{m}=W_{F}(f(x_{m}),f_{c}(x_{m})),\]

Note that \(W_{F}\) represents a fully connected neural network. For training our classification model, we use categorical cross-entropy loss, which is defined as follows:

\[_{task}=CE(},),\]

Formally, the overall loss function of our approach can be formulated as:

\[=_{task}+_{1}_{c}+_{2} _{neural},\]

where \(_{1},_{2}\) are hyperparameters for the trade-off between interpretability and accuracy.

## 5 Experiment

In this section, we introduce the experimental settings, present our superior performance, and showcase the interpretability of our network. Due to the space limit, additional experimental details and results are in the appendix C.

### Experimental Setting

**Datasets.** We consider four benchmark medical datasets: COVID-CT  for CT images, DDI  for dermatology images, Chest X-Ray , and Fitzpatrick17k  for a dermatological dataset with skin colors.

**Baselines.** We compared our model with other state-of-the-art interpretable models, such as Label-free CBM and DCR, to highlight the robustness of our interpretability capabilities. Furthermore, we conducted comparisons with black-box models, such as SSSD-COVID .

[MISSING_PAGE_FAIL:8]

data, providing an intuitive understanding of how pixels in the image influence the image classification result aligned with semantic concepts. Compared to traditional concept-based models, our model interpretation offers advantages in richness and accuracy. Additional visualization examples can be found in Appendix D.2.

As shown in Figure 4, it is evident that relying solely on single-dimensional interpretable strategies, such as saliency maps or concept embedding enhancements, does not furnish adequate interpretability to effectively address the problem. However, by integrating multi-dimensional strategies, the model can align the information of each dimension, thus obtaining more comprehensive interpretable information and ultimately yielding more correct prediction results. Specifically, when feature extraction is solely reliant on saliency maps, obtaining accurate attention in the feature region often proves challenging, and conceptual information tends to be unstable when supplemented solely by concepts. In contrast, our proposed multi-dimensional interpretable strategy transcends dependence on a single interpretable strategy, opting instead for a more generalized multi-dimensional augmentation approach. This approach enables the model to complement the single-dimensional methods and achieve heightened accuracy.

### Ablation Study

The ablation experiments presented in Table 2, conducted with Resnet50 as the backbone, reveal significant contributions from both \(_{c}\) and \(_{neural}\) to the classification result. To illustrate, considering the comprehensive index AUC in the DDI dataset, utilizing only \(_{c}\) yeilds a 3.79% improvement, while relying solely on \(_{neural}\) does not notably enhance performance. However, employing both

    &  &  \\   & \(_{c}\) & \(_{neural}\) & **ACC.(\%)** & **Precision(\%)** & **Recall(\%)** & **F1(\%)** & **AUC.(\%)** & **Interpretability** \\   & & & 82.20 & 82.92 & 82.21 & 82.55 & 82.64 &  \\  & ✓ & & 83.05 & 83.62 & 83.16 & 83.01 & 83.16 & \\  & & ✓ & 81.36 & 82.11 & 81.38 & 81.70 & 81.81 & \\  & ✓ & ✓ & **84.75** & **84.77** & **84.88** & **84.75** & **84.77** & ✓ \\   & & & 78.03 & 74.97 & 66.88 & 69.24 & 67.41 &  \\  & ✓ & & 79.75 & 75.76 & 71.47 & 72.73 & 71.20 & \\   & ✓ & ✓ & 78.79 & 76.38 & 66.29 & 68.69 & 67.64 & \\   & ✓ & ✓ & **81.82** & **76.56** & **76.17** & **76.33** & **76.12** & ✓ \\   & & & 68.59 & 69.63 & 61.11 & 61.02 & 62.05 &  \\  & ✓ & & 72.28 & 77.63 & 64.15 & 63.72 & 64.15 & \\   & ✓ & ✓ & 70.03 & 73.83 & 61.84 & 61.25 & 62.39 & \\    & ✓ & ✓ & **78.37** & **80.38** & **73.12** & **74.42** & **73.12** & ✓ \\   & & & 78.33 & 79.50 & 78.32 & 78.91 & 79.06 &  \\  & ✓ & & 79.80 & 80.60 & 79.81 & 80.20 & 80.31 & \\   & ✓ & & 80.79 & 81.28 & 80.82 & 81.28 & 81.07 & \\   & ✓ & ✓ & **82.76** & **82.84** & **83.23** & **83.03** & **82.99** & ✓ \\   

Table 2: Experimental results from ablation studies on each loss function demonstrate that each loss function is indispensable for both accuracy and interpretability.

Figure 4: Comparison of single-dimensional and multi-dimensional interpretability methods.

simultaneously leads to an 8.71% improvement. This observation underscores the complementary nature of concept and neural logic rules in enhancing model performance. Besides, additional ablation studies investigating the effect of concept filters and comparing VLMs labeling methods with other Med-CLIP approaches are provided in Appendix D.3. Additionally, we conducted a sensitivity analysis for both baselines and Med-MICN on the DDI dataset, as depicted in Figure 14 in the appendix, showcasing the robustness of our model against perturbations. Furthermore, computational cost analysis (presented in Table 9 in the appendix) was conducted. Experimental findings indicate that Med-MICN incurs minimal computational cost compared to the baseline model while achieving improvements in both accuracy and interpretability.

## 6 Conclusion

This paper proposes a novel end-to-end interpretable concept-based model called Med-MICN. Combining medical image classification, neural symbolic solving, and concept semantics, Med-MICN achieves superior accuracy and multi-dimensional interpretability. Our comprehensive experiments demonstrate consistent enhancements over other baselines, highlighting its potential as a generalized and faithful interpretation model for medical images.