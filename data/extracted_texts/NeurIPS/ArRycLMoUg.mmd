# Neural Lyapunov Control for Discrete-Time Systems

Junlin Wu

Computer Science & Engineering

Washington University in St. Louis

St. Louis, MO 63130

junlin.wu@wustl.edu &Andrew Clark

Electrical & Systems Engineering

Washington University in St. Louis

St. Louis, MO 63130

andrewclark@wustl.edu &Yiannis Kantaros

Electrical & Systems Engineering

Washington University in St. Louis

St. Louis, MO 63130

ioannisk@wustl.edu &Yevgeniy Vorobeychik

Computer Science & Engineering

Washington University in St. Louis

St. Louis, MO 63130

yvorobeychik@wustl.edu

###### Abstract

While ensuring stability for linear systems is well understood, it remains a major challenge for nonlinear systems. A general approach in such cases is to compute a combination of a Lyapunov function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been proposed that represent Lyapunov functions using neural networks. However, such approaches either focus on continuous-time systems, or highly restricted classes of nonlinear dynamics. We propose the first approach for learning neural Lyapunov control in a broad class of discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the discrete-time Lyapunov stability conditions, leveraging the particular structure of these conditions. The second is a novel approach for computing verified sublevel sets. The third is a heuristic gradient-based method for quickly finding counterexamples to significantly speed up Lyapunov function learning. Our experiments on four standard benchmarks demonstrate that our approach significantly outperforms state-of-the-art baselines. For example, on the path tracking benchmark, we outperform recent neural Lyapunov control baselines by an order of magnitude in both running time and the size of the region of attraction, and on two of the four benchmarks (cartpole and PVTOL), ours is the first automated approach to return a provably stable controller. Our code is available at: https://github.com/jlwu002/nlc_discrete.

## 1 Introduction

Stability analysis for dynamical systems aims to show that the system state will return to an equilibrium under small perturbations. Designing stable control in nonlinear systems commonly relies on constructing Lyapunov functions that can certify the stability of equilibrium points and estimate their region of attraction. However, finding Lyapunov functions for arbitrary nonlinear systems is a challenging task that requires substantial expertise and manual effort (Khalil, 2015; Lavaei and Bridgeman, 2023). To address this challenge, recent progress has been made in learning Lyapunov functions in _continuous-time_ nonlinear dynamics (Abate et al., 2020; Chang et al., 2019; Zhou et al., 2022). However, few approaches exist for doing so in discrete-time systems (Dai et al., 2021), and none for general Lipschitz-continuous dynamics. Since modern learning-based controllers oftentake non-negligible time for computation, the granularity of control is effectively discrete-time, and developing approaches for stabilizing such controllers is a major open challenge.

We propose a novel method to learn Lyapunov functions and stabilizing controllers, represented as neural networks (NNs) with ReLU activation functions, for discrete-time nonlinear systems. The proposed framework broadly consists of a _learner_, which uses a gradient-based method for updating the parameters of the Lyapunov function and policy, and a _verifier_, which produces counterexamples (if any exist) to the stability conditions that are added as training data for the learner. The use of a verifier in the learning loop is critical to enable the proposed approach to return a provably stable policy. However, no prior approaches enable sound verification of neural Lyapunov stability for general discrete-time dynamical systems. The closest is Dai et al. (2021), who assume that dynamics are represented by a neural network, an assumption that rarely holds in real systems. On the other hand, approaches for continuous-time systems (Chang et al., 2019; Zhou et al., 2022) have limited efficacy in discrete-time environments, as our experiments demonstrate. To address this gap, we develop a novel verification tool that checks if the candidate NN Lyapunov function satisfies the Lyapunov conditions by solving Mixed Interger Linear Programs (MILPs). This approach, which takes advantage of the structure of discrete-time Lyapunov stability conditions, can soundly verify a broad class of dynamical system models.

However, using a sound verification tool in the learning loop makes it a significant bottleneck, severely limiting scalability. We address this problem by also developing a highly effective gradient-based technique for identifying counterexamples, resorting to the full MILP-based verifier only as a last resort. The full learning process thereby iterates between learning and verification steps, and returns only when the sound verifier is able to prove that the Lyapunov function and policy satisfy the stability conditions.

The final technical challenge stems from the difficulty of verifying stability near the origin (Chang et al., 2019), typically addressed heuristically by either adding a fixed tolerance to a stability condition (Dai et al., 2021), or excluding a small area around the origin from verification (Chang et al., 2019; Zhou et al., 2022). We address it by adapting Lyapunov stability theory to ensure convergence to a small region near the origin, thereby achieving the first (to our knowledge) sound approach for computing a stable neural controller that explicitly accounts for such approximations near the origin.

We evaluate the proposed approach in comparison to state-of-the-art baselines on four standard nonlinear control benchmarks. On the two simpler domains (inverted pendulum and path following), our approach outperforms the state-of-the-art continuous-time neural Lyapunov control approaches by at least several factors and up to an order of magnitude _in both running time and the size of the region of attraction_. On the two more complex domains--cartpole and PVTOL--ours _is the first automated approach that returns a provably stable controller_. Moreover, our ablation experiments demonstrate that both the MILP-based verifier and heuristic counterexample generation technique we propose are critical to the success of our approach.

In summary, we make the following contributions:

* A novel MILP-based approach for verifying a broad class of discrete-time controlled nonlinear systems.
* A novel approach for learning provably verified stabilizing controllers for a broad class of discrete-time nonlinear systems which combines our MILP-based verifier with a heuristic gradient-based approximate counterexample generation technique.
* A novel formalization of approximate stability in which the controller provably converges to a small ball near the origin in finite time.
* Extensive experiments on four standard benchmarks demonstrate that by leveraging the special structure of Lyapunov stability conditions for discrete-time system, our approach significantly outperforms prior art.

Related WorkMuch prior work on learning Lyapunov functions focused on continuous-time systems (Abate et al., 2020; Ravanbakhsh and Sankaranarayanan, 2019; Chang et al., 2019; Zhou et al., 2022). Common approaches have assumed that dynamics are linear (Donti et al., 2021; Tedrake, 2009) or polynomial (Ravanbakhsh and Sankaranarayanan, 2019). Recently, Chang et al. (2019), Rego and de Araujo (2022), and Zhou et al. (2022) proposed learning Lyapunov functions represented as neural networks while restricting policies to be linear. These were designed for continuous-time dynamics, and are not effective in discrete-time settings, as we show in the experiments. Chen et al.

[2021a] learns convex Lyapunov functions for discrete-time hybrid systems. Their approach requires hybrid systems to admit a mixed-integer linear programming formulation, essentially restricting it to piecewise-affine systems, and does not learn stabilizing controllers for these. Our approach considers a much broader class of dynamical system models, and learns provably stable controllers and Lyapunov functions, allowing both to be represented as neural networks. Kolter and Manek  learn stable nonlinear dynamics represented as neural networks, but do not provide stability guarantees with respect to the true underlying system. In addition, several approaches have been proposed that either provide only probabilistic stability guarantees [Berkenkamp et al., 2017, Richards et al., 2018], or do not guarantee stability [Choi et al., 2020, Han et al., 2020]. Several recent approaches propose methods for verifying stability in dynamical discrete-time systems. Chen et al. [2021b] compute an approximate region of attraction of dynamical systems with neural network dynamics, but assume that Lyapunov functions are quadratic. Dai et al.  consider the problem of verifying Lyapunov conditions in discrete-time systems, as well as learning provably stable policies. However, they assume that dynamics are represented by neural networks with (leaky) ReLU activation functions. Our verification approach, in contrast, is for arbitrary Lipschitz continuous dynamics.

## 2 Model

We consider a discrete-time nonlinear dynamical system

\[x_{t+1}=f(x_{t},u_{t}),\] (1)

where \(x_{t}\) is state in a domain \(\) and \(u_{t}\) the control input at time \(t\). We assume that \(f\) is Lipschitz continuous with Lipschitz constant \(L_{f}\). This class of dynamical systems includes the vast majority of (non-hybrid) dynamical system models in prior literature. We assume that \(x=0\) is an equilibrium point for the system, that is, \(f(0,u_{0})=0\) for some \(u_{0}\). Let \((x)\) denote a control policy, with \(u_{t}=(x_{t})\). For example, in autonomous vehicle path tracking, \(x\) can measure path tracking error and \(u\) the steering angle. In the conventional setup, the goal is to learn a control policy \(\) such that the dynamical system in Equation (1) converges to the equilibrium \(x=0\), a condition referred to as _stability_. To this end, we can leverage the Lyapunov stability framework [Tedrake, 2009]. Specifically, the goal is to identify a Lyapunov function \(V(x)\) and controller \(\) that satisfies the following conditions over a subset of the domain \(\): \(1)V(0)=0\); \(2)V(x)>0, x\{0\}\); and \(3)V(f(x,(x)))-V(x)<0, x\). These conditions imply that the system is (locally) stable in the Lyapunov sense [Bof et al., 2018, Tedrake, 2009].

In practice, due to numerical challenges in verifying stability conditions near the origin, it is common to verify slight relaxations of the Lyapunov conditions. These typically fall into two categories: 1) Lyapunov conditions are only checked for \(\|x\|_{p}\)[Chang et al., 2019, Zhou et al., 2022] (our main baselines, and the only other methods for learning neural Lyapunov control for general nonlinear dynamics), and/or 2) a small tolerance \(>0\) is added in verification, allowing small violations of Lyapunov conditions near the origin [Dai et al., 2021].

Our goal now is to formally investigate the implications of numerical approximations of this kind. We next define a form of stability which entails finite-time convergence to \((0,)=\{x\|x\|_{}<\}\).

**Definition 2.1** (\(\)-stability).: _We call pair of \((V,)\)\(\)-stable within a region \(\) when the following conditions are satisfied: (a) \(V(0)=0\); (b) there exists \(>0\) such that \(V(f(x,(x)))-V(x)<-\) for all \(x(0,)\); and (c) \(V(x)>0\) for all \(x(0,)\)._

A key ingredient to achieving stability is to identify a _region of attraction (ROA)_, within which we are guaranteed to converge to the origin from any starting point. In the context of \(\)-stability, our goal is to converge to a small ball near the origin, rather than the origin; let \(\)-ROA be the set of initial inputs that has this property. In order to enable us to identify an \(\)-ROA, we introduce a notion of an invariant sublevel set. Specifically, we refer to a set \((,)=\{x|V(x)\}\) with the property that \(x(,) f(x,(x))\) as a _\(\)-invariant sublevel set_. In other words, \((,)\) is a sublevel set which is additionally forward invariant with respect to \(\). We assume that \((0,)(,)\).

Next, we formally prove that \(\)-stability combined with a sublevel set \((,)\) entails convergence in three senses: 1) that we reach an \(\)-ball around the origin in finite time, 2) that we reach it infinitely often, and 3) we converge to an arbitrarily small ball around the origin. We refer to the first of these senses as _reachability_. What we show is that for \(\) sufficiently small, reachability implies convergence in all three senses. For this, a key additional assumption is that \(V\) and \(\) are Lipschitz continuous, with Lipschitz constants \(L_{v}\) and \(L_{}\), respectively (e.g., in ReLU neural networks).

**Theorem 2.2**.: _Suppose \(V\) and \(\) are \(\)-stable on a compact \(\), \((,)\) is an \(\)-invariant set, and \( c_{1}\) such that \(\|(0)-u_{0}\|_{} c_{1}\). Then if \(x_{0}(,)(0,)\):_

1. _there exists a finite_ \(K\) _such that_ \(x_{K}(0,)\)_,_
2. \( c_{2}\) _such that if_ \(c_{2}<\) _and_ \((0,c_{2})\)_, then there exists a finite_ \(K\) _such that_ \( k K\)_,_ \(x_{k}(,c_{2})\) _and the sequence_ \(\{x_{k}\}_{k 0}\) _reaches_ \((0,)\) _infinitely often, and furthermore_
3. _for any_ \(>0\) _such that_ \((0,)\)_,_ \(\) _and finite_ \(K\) _such that_ \(\|(0)-u_{0}\|_{} c_{1}\|x_{k}\|_{} \, k K\)_._

Proof.: We prove (i) by contradiction. Suppose that \(\|x_{k}\|_{}>\  k\{0,,[V(x_{0})/]\}\). Then, when \(k= V(x_{0})/\), condition (b) of \(\)-stability and \(\)-invariance of \((,)\) implies that \(V(x_{k})<V(x_{0})- k<0\), contradicting condition (c) of \(\)-stability.

To prove (ii), fix the finite \(K\) from (i), and let \(k K\) be such that \(x_{k}(0,)\). By Lipschitz continuity of \(V\) and the fact that \(V(0)=0\) and \(V(x) 0\) (conditions (a) and (c) of \(\)-stability), \(V(x_{k+1}) L_{v}\|x_{k+1}\|\), where \(\|\|\) is the \(_{}\) norm here and below. Moreover, since \(x_{k+1}=f(x_{k},(x_{k}))\), \(f(0,u_{0})=0\) (stability of the origin), and by Lipschitz continuity of \(f\) and \(\),

\[\|x_{k+1}\| L_{f}\|(x_{k},(x_{k})-u_{0})\| L_{f}\{\|x_{k}\|,\| (x_{k})-u_{0}\|\}.\]

By Lipschitz continuity of \(\), triangle inequality, and the condition that \(\|(0)-u_{0}\| c_{1}\), we have \(\|(x_{k})-u_{0}\| L_{}\|x_{k}\|+c_{1}(L_{}+c_{1})\). Let \(c_{2}=\{L_{v},1\}L_{f}\{1,L_{}+c_{1}\}\). Then \(\|x_{k+1}\| c_{2}\) and \(V(x_{k+1}) c_{2}\). Thus, if \(c_{2}<\) and \((0,c_{2})\), and \((,)\) is \(\)-invariant, then \(x_{k+1}(,c_{2})\) which is \(\)-invariant. Additionally, either \(x_{k+1}(0,)\), and by the argument above \(x_{k+2}(,c_{2})\), or \(x_{k+1}(,c_{2})(0,)\), and by \(\)-stability \(x_{k+2}(,c_{2})\). Thus, by induction, we have that for all \(k K\), \(x_{k}(,c_{2})\). Finally, since for all \(k K\), \(x_{k}(,c_{2})\), if \(x_{k}(0,)\), by (i) it must reach \((0,)\) in finite time. Consequently, there is an infinite subsequence \(\{x_{k^{}}\}\) of \(\{x_{k}\}_{k 0}\) such that all \(x_{k^{}}(0,)\), that is, the sequence \(\{x_{k}\}_{k 0}\) reaches \((0,)\) infinitely often.

We prove part (iii) by contradiction. Fix \(>0\) and define \(S=\{x:\|x\|>\}\). Suppose that \(>0\) there exists \(x S\) such that \(V(x) c_{2}\) where \(c_{2}\) is as in (ii). Then for any (infinite) sequence of \(\{_{t}\}\) we have \(\{x_{t}\}\) such that \(V(x_{t}) c_{2}_{t}\), where \(x_{t} S\). Now, consider a set \(=\{x:\|x\|\}\). Since \(\) is compact and \(\{x_{t}\}\) is an infinite sequence, there is an infinite subsequence \(\{x_{t_{k}}\}\) of \(\{x_{t}\}\) such that \(_{k}x_{t_{k}}=x^{*}\) and \(x^{*}\). Since \(V\) is continuous, we have \(_{k}V(x_{t_{k}})=V(x^{*})\). Now, we choose \(\{_{t}\}\) such that \(_{t}_{t}=0\). This means that \(_{k}V(x_{t_{k}})=V(x^{*}) 0\), and since \(x^{*}\), this contradicts condition (c) of \(\)-stability. Since by (ii), there exists a finite \(K\), \(V(x_{k}) c_{2}, k K\), we have \( k K\), \(\|x_{k}\|\). 

The crucial implication of Theorem 2.2 is that as long as we choose \(>0\) sufficiently small, verifying that \(V\) and \(\) are \(\)-stable together with identifying an \(\)-invariant set \((,)\) suffices for convergence arbitrarily close to the origin. One caveat is that we need to ensure that \((0)\) is sufficiently close to \(u_{0}\) for any \(\) we choose. In most domains, this can be easily achieved: for example, if \(u_{0}=0\) (as is the case in many settings, including three of our four experimental domains), we can use a representation for \(\) with no bias terms, so that \((0)=0=u_{0}\) by construction. In other cases, we can simply check this condition after learning.

Another caveat is that we need to define \(\) to enable us to practically achieve these properties. To this end, we define \(\) parametrically as \(()=\{x\ \|\ \|x\|_{}\}\). Additionally, we introduce the following useful notation. Define \((,)=\{x\ |\ \|x\|_{}\}\). Note that \((0,)=\), \((,)=\{x\ |\ \|x\|_{}\}\) and \((0,)=()\). Thus, for \(\) sufficiently large compared to \(\), conditions such as \((0,)(,)\), \((0,)\), and \((0,c_{2})\) will be easy to satisfy. Following conventional naming, we denote \((,)\) as \(\)_-valid region, i.e., the region that satisfies conditions \((aDefine \(()\) as the set (\(,\)) for which the conditions (a)-(c) of \(\)-stability are satisfied over the domain \(()\). Our main challenge below is to find \((,)()\) for a given \(\). That, in turn, entails solving the key subproblem of verifying these conditions for given \(V_{}\) and \(_{}\).

Next, in Section 3 we address the verification problem, and in Section 4 we describe our approach for jointly learning \(V_{}\) and \(_{}\) that can be verified to satisfy the \(\)-stability conditions for a given \(()\).

## 3 Verifying Stability Conditions

Prior work on learning \(\)-Lyapunov functions for continuous-time nonlinear control problems has leveraged off-the-shelf SMT solvers, such as dReal (Gao et al., 2013). However, these solvers scale poorly in our setting (see Supplement C for details). In this section, we propose a novel approach for verifying the \(\)-Lyapunov conditions for arbitrary Lipschitz continuous dynamics using mixed-integer linear programming, through obtaining piecewise-linear bounds on the dynamics. We assume \(V_{}\) and \(_{}\) are \(K\)- and \(N\)-layer neural networks, respectively, with ReLU activations.

We begin with the problem of verifying condition (c) of \(\)-stability, which we represent as a feasibility problem: to find if there is any point \((,)\) such that \(V() 0\). We can formulate it as the following MILP:

\[z_{K} 0\] (2a) \[z_{l+1}=g_{_{l}}(z_{l}), 0 l K-1\] (2b) \[\|x\|_{}, z_{0}=x,\] (2c)

where \(l\) refers to a layer in the neural network \(V_{}\), \(z_{K}=V_{}(x)\), and the associated functions \(g_{_{l}}(z_{l})\) are either \(W_{l}z_{l}+b_{l}\) for a linear layer (with \(_{l}=(W_{l},b_{l})\)) or \(\{z_{l},0\}\) for a ReLU activation. Any feasible solution \(x^{*}\) is then a counterexample, and if the problem is infeasible, the condition is satisfied. ReLU activations \(g(z)\) can be linearized by introducing an integer variable \(a\{0,1\}\), and replacing the \(z^{}=g(z)\) terms with constraints \(z^{} UA\) and \(z^{}-L(1-a)\), where \(L\) and \(U\) are specified so that \(L z U\) (we deal with identifying \(L\) and \(U\) below).

Next, we cast verification of condition \((b)\) of \(\)-stability, which involves the nonlinear control dynamics \(f\), as the following feasibility problem:

\[_{K}-z_{K}-\] (3a) \[y_{i+1}=h_{_{i}}(y_{i}), 0 i N-1\] (3b) \[_{l+1}=g_{_{l}}(_{l}), 0 l K-1\] (3c) \[_{0}=f(x,y_{N})\] (3d) \[y_{0}=x,)}-)},\] (3e)

where \(h_{_{i}}()\) are functions computed at layers \(i\) of \(_{}\), \(z_{K}=V_{}(x)\), and \(_{K}=V_{}(f(x,_{}(x)))\).

At this point, all of the constraints can be linearized as before with the exception of Constraint (3d), which involves the nonlinear dynamics \(f\). To deal with this, we relax the verification problem by replacing \(f\) with linear lower and upper bounds. To obtain tight bounds, we divide \(()\) into a collection of subdomains \(\{_{k}\}\). For each \(_{k}\), we obtain a linear lower bound \(f_{lb}(x)\) and upper bound \(f_{ub}(x)\) on \(f\), and relax the problematic Constraint (3d) into \(f_{lb}(x)_{0} f_{ub}(x),\) which is now a pair of linear constraints. We can then solve Problem (3) for each \(_{k}\).

**Computing Linear Bounds on System Dynamics** Recall that \(f:^{n}^{n}\) is the Lipschitz-continuous system dynamic \(x_{t+1}=f(x_{t},u_{t})\). For simplicity we omit \(u_{t}=_{}(x)\) below. Let \(\) be the \(_{}\) Lipschitz constant of \(f\). Suppose that we are given a region of the domain \(_{k}\) represented as a hyperrectangle, i.e., \(_{k}=_{i}[x_{i,l},x_{i,u}]\), where \([x_{i,l},x_{i,u}]\) are the lower and upper bounds of \(x\) along coordinate \(i\). Our goal is to compute a linear upper and lower bound on \(f(x)\) over this region. We bound \(f_{j}(x):^{n}\) along each \(j\)-th dimension separately. By \(\)-Lipschitz-continuity, we can obtain \(f_{j}(x) f_{j}(x_{1,l},x_{2,l},,x_{n,l})+_{i}(x_{i}-x_{i,l})\) and \(f_{j}(x) f_{j}(x_{1,l},x_{2,l},,x_{n,l})-_{i}(x_{i}-x_{i,l})\). The full derivation is provided in the Supplement Section B.

Alternatively, if \(f_{j}\) is differentiable, convex over \(x_{i}\), and monotone over other dimensions, we can restrict it to calculating one-dimensional bounds by finding coordinates \(x_{-i,u}\) and \(x_{-i,l}\) such that \(f_{j}(x_{i},x_{-i,l}) f_{j}(x_{i},x_{-i}) f_{j}(x_{i},x_{-i,u})\) for any given \(x\). Then \(f_{j}(x) f_{j,i,ub}(x) f_{j}(x_{i,l},x_{-i,u})+(x_{i,u}x_{-i, u})-f_{j}(x_{i,l},x_{-i,u})}{x_{i,u}-x_{i,l}}(x_{i}-x_{i,l})\) and \(f_{j}(x) f_{i,lb}(x) f_{j}(x_{i}^{*},x_{-i,l})+(x_{i,u}x_{-i, l})-f_{j}(x_{i,l},x_{-i,u})}{x_{i,u}-x_{i,l}}(x_{i}-x_{i,l})\).

\(f^{}_{j}(x_{i}^{*},x_{-i,l})(x_{i}-x_{i}^{*})\), where we let \(x_{i}^{*}=_{x_{s}[x_{i,l},x_{i,l}]}_{x_{i,l}^{i}}[f_{j}(x_{i},x_{-i,l} )-f_{j}(x_{s},x_{-i,l})+f^{}_{j}(x_{s},x_{-i,l})(x_{i}-x_{s})]dx_{s}\) to minimize the error area between \(f_{j}(x)\) and \(f_{i,lb}(x)\). Note that the solution for \(x_{i}^{*}\) can be approximated when conducting experiments, and this approximation has no impact on the correctness of the bound. A similar result obtains for where \(f_{j}(x)\) is concave over \(x_{i}\). Furthermore, we can use these single-dimensional bounds to obtain tighter lower and upper bounds on \(f_{j}(x)\) as follows: note that for any \(c^{l},c^{u}\) with \(_{i}c^{l}_{i}=1\) and \(_{i}c^{u}_{i}=1\), \(_{i}c^{l}_{i}f_{j,i,lb}(x) f_{j}(x)_{i}c^{u}_{i}f_{j,i,ub}(x)\), which means we can optimize \(c^{l}\) and \(c^{u}\) to minimize the bounding error. In practice, we can typically partition \(()\) so that the stronger monotonicity and convexity or concavity assumptions hold for each \(_{k}\).

Note that our linear bounds \(f_{lb}(x)\) and \(f_{ub}(x)\) introduce errors compared to the original nonlinear dynamics \(f\). However, we can obtain tighter bounds by splitting \(_{k}\) further into subregions, and computing tighter bounds in each of the resulting subregions, but at the cost of increased computation. To balance these considerations, we start with a relatively small collection of subdomains \(\{_{k}\}\), and only split a region \(_{k}\) if we obtain a counterexample in \(_{k}\) that is not an actual counterexample for the true dynamics \(f\).

**Computing Bounds on ReLU Linearization Constants** In linearizing the ReLU activations, we supposed an existence of lower and upper bounds \(L\) and \(U\). However, we cannot simply set them to some large negative and positive number, respectively, because \(V_{}(x)\) has no a priori fixed bounds (in particular, note that for any \(\)-Lyapunov function \(V\) and constant \(a>1\), \(aV\) is also a \(\)-Lyapunov function). Thus, arbitrarily setting \(L\) and \(U\) makes verification unsuand. To address this issue, we use interval bound propogation (IBP) (Gowal et al., 2018) to obtain \(M=_{1 i n}\{|U_{i}|,|L_{i}|\}\), where \(U_{i}\) is the upper bound, and \(L_{i}\) is the lower bound returned by IBP for the \(i\)-th layer, with inputs for the first layer the upper and lower bounds of \(f(())\). Setting each \(L=-M\) and \(U=M\) then yields sound verification.

**Computing Sublevel Sets** The approaches above verify the conditions (a)-(c) of \(\)-stability on \(()\). The final piece is to find the \(()\)-invariant sublevel set \((,)\), that is, to find \(\). Let \(B()(_{x()}\|f(x,_{}(x))\|_{ },)\). We find \(\) by solving

\[_{x:\|x\|_{} B()}V(x).\] (4)

We can transform both Problem (4) and computation of \(B()\) into MILP as for other problems above.

**Theorem 3.1**.: _Suppose that \(V\) and \(\) are \(\)-stable on \(()\), \(\|(0)-u_{0}\|_{}\), and \( L_{f}\{1,L_{}+1\}\). Let \(V^{*}\) be the optimal value of the objective in Problem (4), and \(=V^{*}-\) for any \(>0\). Then the set \((,)=\{x:x(),V(x)\}\) is an \(()\)-invariant sublevel set._

Proof.: If \(\|x\|_{}>\), \(V(f(x,(x)))<V(x)<V^{*}\) by \(\)-stability of \(V\) and \(\). Suppose that \(=f(x,(x))()\). Since \(V()<V^{*}\), \(V^{*}\) must not be an optimal solution to (4), a contradiction. If \(\|x\|_{}\), the argument is similar to Theorem 2.2 (ii). 

## 4 Learning \(\)-Lyapunov Function and Policy

We cast learning the \(\)-Lyapunov function and policy as the following problem:

\[_{,}_{i S}(x_{i};V_{},_{}),\] (5)

where \(()\) is a loss function that promotes Lyapunov learning and the set \(S()\) is a finite subset of points in the valid region. We assume that the loss function is differentiable, and consequently training follows a sequence of gradient update steps \((^{},^{})=(,)-_{i S}_{ ,}(x_{i};V_{},_{})\).

Clearly, the choice of the set \(S\) is pivotal to successfully learning \(V_{}\) and \(_{}\) with provable stability properties. Prior approaches for learning Lyapunov functions represented by neural networks use one of two ways to generate \(S\). The first is to generate a fixed set \(S\) comprised of uniformly random samples from the domain (Chang and Gao, 2021; Richards et al., 2018). However, this fails to learn verifiable Lyapunov functions. In the second, \(S\) is not fixed, but changes in each learning iteration, starting with randomly generated samples in the domain, and then using the verification tool, such as dReal, in each step of the learning process to update \(S\)(Chang et al., 2019; Zhou et al., 2022). However, verification is often slow, and becomes a significant bottleneck in the learning process.

Our key idea is to combine the benefits of these two ideas with a fast gradient-based heuristic approach for generating counterexamples. In particular, our proposed approach for training Lyapunov control functions and associated control policies (Algorithm 1; DITL) involves five parts:

1. heuristic counterexample generation (lines 10-12),
2. choosing a collection \(S\) of inputs to update \(\) and \(\) in each training (gradient update) iteration (lines 9-13),
3. the design of the loss function \(\),
4. initialization of policy \(_{}\) (line 3), and
5. warm starting the training (line 4).

We describe each of these next.

```
1:Input: Dynamical system model \(f(x,u)\) and target valid region \(()\)
2:Output: Lyapunov function \(V_{}\) and control policy \(_{}\)
3:\(_{}=\) Initialize()
4:\(V_{}=\) PreTrainLyapunov(\(N_{0},_{}\))
5:\(B\) InitializeBuffer()
6:\(W\)
7:while True do
8:for\(N\) iterations do
9:\(=\)Sample(\(r,B\)) //sample \(r\) points from \(B\)
10:\(T=\)Sample(\(q,()\)) //sample \(q\) points from \(()\)
11:\(T^{}=\)PGD(\(T\))
12:\(T^{}=\)FilterCounterExamples(\(T^{}\))
13:\(S= T^{} W\)
14:\(B=B T^{}\)
15:\((,)(,)-_{i S}_{,} (x_{i};V_{},_{})\)
16:endfor
17:(success,\(\))\(=\)Verify(\(V_{},_{}\))
18:if success then
19:return\(V_{},_{}\)
20:else
21:\(W W\)
22:endif
23:endwhile
24:return FAILED // Timeout ```

**Algorithm 1** DITL Lyapunov learning algorithm.

Heuristic Counterexample GenerationAs discussed in Section 3, verification in general involves two optimization problems: \(_{x()}V_{}(x)-V_{}(f(x,_{}(x)))\) and \(_{x()}V_{}(x)\). We propose a _projected gradient descent_ (PGD) approach for approximating solutions to either of these problems. PGD proceeds as follows, using the second minimization problem as an illustration; the process is identical in the first case. Beginning with a starting point \(x_{0}()\), we iteratively update \(x_{k+1}\):

\[x_{k+1}=\{x_{k}+_{k}( V_{}(x_{k}))\},\]

where \(\{\}\) projects the gradient update step onto the domain \(()\) by clipping each dimension of \(x_{k}\), and \(_{k}\) is the PGD learning rate. Note that as \(f\) is typically differentiable, we can take gradients directly through it when we apply PGD for the first verification problem above.

We run this procedure for \(K\) iterations, and return the result (which may or may not be an actual counterexample). Let \(T^{}=\)PGD(\(T\)) denote running PGD for a set of \(T\) starting points for both of the optimization problems above, resulting in a corresponding set \(T^{}\) of counterexamples.

**Selecting Inputs for Gradient Updates** A natural idea for selecting samples \(S\) is to simply add counterexamples identified in each iteration of the Lyapunov learning process. However, as \(S\) then grows without bound, this progressively slows down the learning process. In addition, it is important for \(S\) to contain a relatively broad sample of the valid region to ensure that counterexamples we generate along the way do not cause undesired distortion of \(V_{}\) and \(_{}\) in subregions not well represented in \(S\). Finally, we need to retain the memory of previously generated counterexamples, as these are often particularly challenging parts of the input domain for learning. We therefore construct \(S\) as follows.

We create an input buffer \(B\), and initialize it with a random sample of inputs from \(()\). Let \(W\) denote a set of counterexamples that we wish to remember through the entire training process, initialized to be empty. In our implementation, \(W\) consists of all counterexamples generated by the sound verification tools described in Section 3.

At the beginning of each training update iteration, we randomly sample a fixed-size subset \(\) of the buffer \(B\). Next, we take a collection \(T\) of random samples from \(\) to initialize PGD, and generate \(T^{}=(T)\). We then filter out all the counterexamples from \(T^{}\), retaining only those with either \(V_{}(x) 0\) or \(V_{}(x)-V_{}(f(x,_{}(x)))\), where \(\) is a non-negative hyperparameter; this yields a set \(T^{}\). We then use \(S= T^{} W\) for the gradient update in the current iteration. Finally, we update the buffer \(B\) by adding to it \(T^{}\). This process is repeated for \(N\) iterations.

After \(N\) iterations, we check to see if we have successfully learned a stable policy and a Lyapunov control function by using the tools from Section 3 to verify \(V_{}\) and \(_{}\). If verification succeeds, training is complete. Otherwise, we use the counterexamples \(\) generated by the MILP solver to update \(W=W\), and repeat the learning process above.

Loss Function DesignNext, we design of the loss function \((x;V_{},_{})\) for a given input \(x\). A key ingredient in this loss function is a term that incentivizes the learning process to satisfy condition (b) of \(\)-stability: \(_{1}(x;V_{},_{})=(V_{}(f(x,_{ }(x)))-V_{}(x)+)\), where the parameter \( 0\) determines how aggressively we try to satisfy this condition during learning.

There are several options for learning \(V_{}\) satisfying conditions (a) and (c) of \(\)-stability. The simplest is to set all bias terms in the neural network to zero, which immediately satisfies \(V_{}(0)=0\). An effective way to deal with condition (c) is to maximize the lower bound on \(V_{}(x)\). To this end, we propose to make use of the following loss term: \(_{2}(x;V_{},_{})=(-V_{}^{LB})\), where \(V_{}^{LB}\) is the lower bound on the Lyapunov function over the target domain \(()\). We use use \(,\)-CROWN (Xu et al., 2020) to obtain this lower bound.

The downside of setting bias terms to zero is that we lose many learnable parameters, reducing flexibility of the neural network. If we consider a general neural network, on the other hand, it is no longer the case that \(V_{}(x)=0\) by construction. However, it is straightforward to ensure this by defining the final Lyapunov function as \(_{}(x)=V_{}(x)-V_{}(0)\). Now, satisfying condition (c) amounts to satisfying the condition that \(V_{}(x) V_{}(0)\), which we accomplish via the following pair of loss terms: \(_{3}(x;V_{},_{})=(V_{}(0)-V_{ }(x)+(\|x\|_{2},))\), where \(\) and \(\) are hyperparameters of the term \((\|x\|_{2},)\) which effectively penalizes \(V_{}(x)\) for being too large, and \(_{4}(x;V_{},_{})=\|V_{}(0)\|_{2}^{2}\).

The general loss function is then a weighted sum of these loss terms,

\[(x;V_{},_{})=_{1}(x;V_{},_{ })+c_{2}_{2}(x;V_{},_{})+c_{3}_{3}( x;V_{},_{})+c_{4}_{4}(x;V_{},_{}).\]

When we set bias terms to zero, we would set \(c_{3}=c_{4}=0\); otherwise, we set \(c_{2}=0\).

InitializationWe consider two approaches for initializing the policy \(_{}\). The first is to linearize the dynamics \(f\) around the origin, and use a policy computing based on a linear quadratic regulator (LQR) to obtain a simple linear policy \(_{}\). The second approach is to use deep reinforcement, such as PPO (Liu et al., 2019), where rewards correspond to stability (e.g., reward is the negative value of the \(l_{2}\) distance of state to origin). To initialize \(V_{}\), we fix \(_{}\) after its initialization, and follow our learning procedure above using solely heuristic counterexample generation to pre-train \(V_{}\).

The next result now follows by construction.

Theorem 4.1: _If Algorithm 1 returns \(V_{},_{}\), they are guaranteed to satisfy \(\)-stability conditions._

## 5 Experiments

### Experiment Setup

Benchmark DomainsOur evaluation of the proposed DITL approach uses four benchmark control domains: _inverted pendulum_, _path tracking_, _cartpole_, and _drone planar vertical takeoff and landing (PVTOL)_. Details about these domains are provided in the Supplement.

**Baselines** We compare the proposed approach to four baselines: _linear quadratic regulator (LQR), sum-of-squares (SOS), neural Lyapunov control (NLC)_(Chang et al., 2019), and _neural Lyapunov control for unknown systems (UNL)_(Zhou et al., 2022). The first two, LQR and SOS, are the more traditional approaches for computing Lyapunov functions when the dynamics are either linear (LQR) or polynomial (SOS) (Tedrake, 2009). LQR solutions (when the system can be stabilized) are obtained through matrix multiplication, while SOS entails solving a semidefinite program (we solve it using YALMIP with MOSEK solver in MATLAB 2022b). The next two baselines, NLC and UNL, are recent approaches for learning Lyapunov functions in continuous-time systems (no approach for learning provably stable control using neural network representations exists for discrete time systems). Both NLC and UNL yield provably stable control for general non-linear dynamical systems, and are thus the most competitive baselines to date. In addition to these baselines, we consider an ablation in which PGD-based counterexample generation for our approach is entirely replaced by the sound MILP-based method during learning (we refer to it as DITL-MILP).

**Efficacy Metrics** We compare approaches in terms of three efficacy metrics. The first is (serial) runtime (that is, if no parallelism is used), which we measure as wall clock time when only a single task is running on a machine. For inverted pendulum and path tracking, all comparisons were performed on a machine with AMD Ryzen 9 5900X 12-Core Processor and Linux Ubuntu 20.04.5 LTS OS. All cartpole and PVTOL experiments were run on a machine with a Xeon Gold 6150 CPU (64-bit 18-core x86), Rocky Linux 8.6. UNL and RL training for Path Tracking are the only two cases that make use of GPUs, and was run on NVIDIA GeForce RTX 3090. The second metric was the size of the valid region, measured using \(_{2}\) norm for NLC and UNL, and \(_{}\) norm (which dominates \(_{2}\)) for LQR, SOS, and our approach. The third metric is the _region of attraction (ROA)_. Whenever verification fails, we set ROA to 0. Finally, we compare all methods whose results are stochastic (NLC, UNL, and ours) in terms of _success rate_.

**Verification Details** For LQR, SOS, NLC, and UNL, we used dReal as the verification tool, as done by Chang et al. (2019) and Zhou et al. (2022). For DITL verification we used CPLEX version 22.1.0.

### Results

**Inverted Pendulum** For the inverted pendulum domain, we initialize the control policy using the LQR solution (see the Supplement for details). We train \(V_{}\) with non-zero bias terms. We set \(=0.1\) (\(<0.007\)% of the valid region), and approximate ROA using a grid of 2000 cells along each coordinate using the level set certified with MILP (4). All runtime is capped at 600 seconds, except the UNL baseline, which we cap at 16 minutes, as it tends to be considerably slower than other methods. Our results are presented in Table 1. While LQR and SOS are fastest, our approach (DITL) is the next fastest, taking on average \(\)8 seconds, with NLC and UNL considerably slower. However, DITL yields an ROA _a factor \(>\)4 larger_ than the nearest baseline (LQR), and 100% success rate. Finally, the DITL-MILP ablation is two orders of magnitude slower (runtime \(>300\) seconds) _and_ less effective (average ROA=\(42\), 80% success rate) than DITL. We visualize maximum ROA produced by all methods in Figure 1 (left).

**Path Tracking** In path tracking, we initialize our approach using both the RL and LQR solutions, drawing a direct comparison between the two (see the Supplement for details). We set \(=0.1\). The running time of RL is \( 155\) seconds. The results are provided in Table 2. We can observe that both RL- and LQR-initialized variants of our approach outperform all prior art, with RL exhibiting _a

Figure 1: ROA plot of inverted pendulum (left) and path tracking (right). We select the best result for each method.

factor of 5 advantage_ over the next best (SOS, in this case) in terms of ROA, and nearly a factor of 6 advantage in terms of maximum achieved ROA (NLC is the next best in this case). Moreover, our approach again has a 100% success rate. Our runtime is an order of magnitude lower than NLC or UNL. Overall, the RL-initialized variant slightly outperforms LQR initialization. The DITL-MILP ablation again performs far worse than DITL: running time is several orders of magnitude slower (at \(>550\) seconds), with low efficacy (ROA is 1.1, success rate 10%). We visualize comparison of maximum ROA produced by all methods in Figure 1 (right).

**Cartpole** For cartpole, we used LQR for initialization, and set bias terms of \(V_{}\) to zero. We set \(=0.1\) (0.01% of the valid region area) and the running time limit to 2 hours for all approaches. None of the baselines successfully attained a provably stable control policy and associated Lyapunov function for this problem. Failure was either because we could find counterexamples within the target valid region, or because verification exceeded the time limit. DITL found a valid region of \(\|x\|_{} 1.0\), in \( 1.6\) hours with a 100% success rate, and average ROA of \(0.021 0.012\).

**PVTOL** The PVTOL setup was similar to cartpole. We set \(=0.1\) (0.0001% of the valid region area), and maximum running time to 24 hours. None of the baselines successfully identified a provably stable control policy. In contrast, DITL found one within \(13 6\) hours on average, yielding a \(100\%\) success rate. We identified a valid region of \(\|x\|_{} 1.0\), and ROA of \(0.011 0.008\).

## 6 Conclusion

We presented a novel algorithmic framework for learning Lyapunov control functions and policies for discrete-time nonlinear dynamical systems. Our approach combines mixed-integer linear programming verification tool with a training procedure that leverages gradient-based approximate verification. This combination enables a significant improvement in effectiveness compared to prior art: our experiments demonstrate that our approach yields several factors larger regions of attraction in inverted pendulum and path tracking, and ours is the only approach that successfully finds stable policies in cartpole and PVTOL.