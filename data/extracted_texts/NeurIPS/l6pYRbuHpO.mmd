# Practical Contextual Bandits with Feedback Graphs

Mengxiao Zhang

University of Southern California

mengxiao.zhang@usc.edu

&Yuheng Zhang

University of Illinois Urbana-Champaign

yuhengz2@illinois.edu

&Olga Vrousgou

Microsoft Research

Olga.Vrousgou@microsoft.com

&Haipeng Luo

University of Southern California

haipengl@usc.edu

&Paul Mineiro

Microsoft Research

pmineiro@microsoft.com

Equal contribution.

###### Abstract

While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.

## 1 Introduction

This paper is primarily concerned with increasing the pace of learning for contextual bandits (Auer et al., 2002; Langford and Zhang, 2007). While contextual bandits have enjoyed broad applicability (Bouneffouf et al., 2020), the statistical complexity of learning with bandit feedback imposes a data lower bound for application scenarios (Agarwal et al., 2012). This has inspired various mitigation strategies, including exploiting function class structure for improved experimental design (Zhu and Mineiro, 2022), and composing with memory for learning with fewer samples (Rucker et al., 2022). In this paper we exploit alternative graph feedback patterns to accelerate learning: intuitively, there is no need to explore a potentially suboptimal action if a presumed better action, when exploited, yields the necessary information.

The framework of bandits with feedback graphs is mature and provides a solid theoretical foundation for incorporating additional feedback into an exploration strategy (Mannor and Shamir, 2011; Alon et al., 2015, 2017). Succinctly, in this framework, the observation of the learner is decided by a directed feedback graph \(G\): when an action is played, the learner observes the loss of every action to which the chosen action is connected. When the graph only contains self-loops, this problem reduces to the classic bandit case. For non-contextual bandits with feedback graphs, (Alon et al., 2015) provides a full characterization on the minimax regret bound with respect to different graph theoretic quantities associated with \(G\) according to the type of the feedback graph.

However, contextual bandits with feedback graphs have received less attention (Singh et al., 2020; Wang et al., 2021). Specifically, there is no prior work offering a solution for general feedback graphsand function classes. In this work, we take an important step in this direction by adopting recently developed minimax algorithm design principles in contextual bandits, which leverage realizability and reduction to regression to construct practical algorithms with strong statistical guarantees (Foster et al., 2018; Foster and Rakhlin, 2020; Foster et al., 2020; Foster and Krishnamurthy, 2021; Foster et al., 2021; Zhu and Mineiro, 2022). Using this strategy, we construct a practical algorithm for contextual bandits with feedback graphs that achieves the optimal regret bound. Moreover, although our primary concern is accelerating learning when the available feedback is more informative than bandit feedback, our techniques also succeed when the available feedback is less informative than bandit feedback, e.g., in spam filtering where some actions generate no feedback. More specifically, our contributions are as follows.

Contributions.In this paper, we extend the minimax framework proposed in (Foster et al., 2021) to contextual bandits with general feedback graphs, aiming to promote the utilization of different feedback patterns in practical applications. Following (Foster and Rakhlin, 2020; Foster et al., 2021; Zhu and Mineiro, 2022), we assume that there is an online regression oracle for supervised learning on the loss. Based on this oracle, we propose \(.\), the first algorithm for contextual bandits with feedback graphs that operates via reduction to regression (Algorithm 1). Eliding regression regret factors, our algorithm achieves the matching optimal regret bounds for deterministic feedback graphs, with \(}()\) regret for strongly observable graphs and \(}(d^{}T^{})\) regret for weakly observable graphs, where \(\) and \(d\) are respectively the independence number and weakly domination number of the feedback graph (see Section 3.2 for definitions). Notably, \(.\) is computationally tractable, requiring the solution to a convex program (Theorem 3.6), which can be readily solved with off-the-shelf convex solvers (Appendix A.3). In addition, we provide closed-form solutions for specific cases of interest (Section 4), leading to a more efficient implementation of our algorithm. Empirical results further showcase the effectiveness of our approach (Section 5).

## 2 Problem Setting and Preliminary

Throughout this paper, we let \([n]\) denote the set \(\{1,2,,n\}\) for any positive integer \(n\). We consider the following contextual bandits problem with informed feedback graphs. The learning process goes in \(T\) rounds. At each round \(t[T]\), an environment selects a context \(x_{t}\), a (stochastic) directed feedback graph \(G_{t}^{}\), and a loss distribution \(_{t}:([-1,1]^{})\); where \(\) is the action set with finite cardinality \(K\). For convenience, we use \(\) and \([K]\) interchangeably for denoting the action set. Both \(G_{t}\) and \(x_{t}\) are revealed to the learner at the beginning of each round \(t\). Then the learner selects one of the actions \(a_{t}\), while at the same time, the environment samples a loss vector \(_{t}[-1,1]^{}\) from \(_{t}(|x_{t})\). The learner then observes some information about \(_{t}\) according to the feedback graph \(G_{t}\). Specifically, for each action \(j\), she observes the loss of action \(j\) with probability \(G_{t}(a_{t},j)\), resulting in a realization \(A_{t}\), which is the set of actions whose loss is observed. With a slight abuse of notation, denote \(G_{t}(|a)\) as the distribution of \(A_{t}\) when action \(a\) is picked. We allow the context \(x_{t}\), the (stochastic) feedback graphs \(G_{t}\) and the loss distribution \(_{t}(|x_{t})\) to be selected by an adaptive adversary. When convenient, we will consider \(G\) to be a \(K\)-by-\(K\) matrix and utilize matrix notation.

Other Notations.Let \((K)\) denote the set of all Radon probability measures over a set \([K]\). \((S)\) represents the convex hull of a set \(S\). Denote \(I\) as the identity matrix with an appropriate dimension. For a \(K\)-dimensional vector \(v\), \((v)\) denotes the \(K\)-by-\(K\) matrix with the \(i\)-th diagonal entry \(v_{i}\) and other entries \(0\). We use \(_{ 0}^{K}\) to denote the set of \(K\)-dimensional vectors with non-negative entries. For a positive definite matrix \(M^{K K}\), we define norm \(\|z\|_{M}=Mz}\) for any vector \(z^{K}\). We use the \(}()\) notation to hide factors that are polylogarithmic in \(K\) and \(T\).

Realizability.We assume that the learner has access to a known function class \(([-1,1])\) which characterizes the mean of the loss for a given context-action pair, and we make the following standard realizability assumption studied in the contextual bandit literature (Agarwal et al., 2012; Foster et al., 2018; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2021).

**Assumption 1** (Realizability).: _There exists a regression function \(f^{}\) such that \([_{t,a} x_{t}]=f^{}(x_{t},a)\) for any \(a\) and across all \(t[T]\)._Two comments are in order. First, we remark that, similar to (Foster et al., 2020), misspecification can be incorporated while maintaining computational efficiency, but we do not complicate the exposition here. Second, Assumption 1 induces a "semi-adversarial" setting, wherein nature is completely free to determine the context and graph sequences; and has considerable latitude in determining the loss distribution subject to a mean constraint.

Regret.For each regression function \(f\), let \(_{f}(x_{t}):=*{argmin}_{a}f(x_{t},a)\) denote the induced policy, which chooses the action with the least loss with respective to \(f\). Define \(^{}:=_{f}\), as the optimal policy. We measure the performance of the learner via regret to \(^{}\): \(}:=_{t=1}^{T}_{t,a_{t}}-_{t=1}^{T}_{t,^{ }(x_{t})}\), which is the difference between the loss suffered by the learner and the one if the learner applies policy \(^{}\).

Regression OracleWe assume access to an online regression oracle \(}\) for function class \(\), which is an algorithm for online learning with squared loss. We consider the following protocol. At each round \(t[T]\), the algorithm produces an estimator \(_{t}*{conv}()\), then receives a set of context-action-loss tuples \(\{(x_{t},a,_{t,a})\}_{a A_{t}}\) where \(A_{t}\). The goal of the oracle is to accurately predict the loss as a function of the context and action, and we evaluate its performance via the square loss \(_{a A_{t}}(_{t}(x_{t},a)-_{t,a})^{2}\). We measure the oracle's cumulative performance via the following square-loss regret to the best function in \(\).

**Assumption 2** (Bounded square-loss regret).: _The regression oracle \(}\) guarantees that for any (potentially adaptively chosen) sequence \(\{(x_{t},a,_{t,a})\}_{a A_{t},t[T]}\) in which \(A_{t}\),_

\[_{t=1}^{T}_{a A_{t}}_{t}(x_{t},a)-_{t,a} ^{2}-_{f}_{t=1}^{T}_{a A_{t}}(f(x_{ t},a)-_{t,a})^{2}}.\]

For finite \(\), Vovk's aggregation algorithm yields \(}=()\)(Vovk, 1995). This regret is dependent upon the scale of the loss function, but this need not be linear with the size of \(A_{t}\), e.g., the loss scale can be bounded by \(2\) in classification problems. See Foster and Krishnamurthy (2021) for additional examples of online regression algorithms.

## 3 Algorithms and Regret Bounds

In this section, we provide our main algorithms and results.

### Algorithms via Minimax Reduction Design

Our approach is to adapt the minimax formulation of (Foster et al., 2021) to contextual bandits with feedback graphs. In the standard contextual bandits setting (that is, \(G_{t}=I\) for all \(t\)), Foster et al. (2021) define the _Decision-Estimation Coefficient_ (DEC) for a parameter \(>0\) as \(_{}():=_{*{conv}( ),x}_{}(;,x)\), where

\[_{}(;,x)& :=_{p(K)}_{}(p,; ,x)\\ &:=_{p(K)}_{a^{}[K] \\ f^{}}_{a p}f^{}(x,a)-f^{}(x,a^{})-(x,a)-f^{ }(x,a)^{2}.\] (1)

Their proposed algorithm is as follows. At each round \(t\), after receiving the context \(x_{t}\), the algorithm first computes \(_{t}\) by calling the regression oracle. Then, it solves the solution \(p_{t}\) of the minimax problem defined in Eq. (1) with \(\) and \(x\) replaced by \(_{t}\) and \(x_{t}\). Finally, the algorithm samples an action \(a_{t}\) from the distribution \(p_{t}\) and feeds the observation \((x_{t},a_{t},_{t,a_{t}})\) to the oracle. Foster et al. (2021) show that for any value \(\), the algorithm above guarantees that

\[[}] T_{}()+ }.\] (2)

However, the minimax problem Eq. (1) may not be solved efficiently in many cases. Therefore, instead of obtaining the distribution \(p_{t}\) which has the exact minimax value of Eq. (1), Foster et al.

 show that any distribution that gives an upper bound \(C_{}\) on \(_{}(p,;,x)\) also works and enjoys a regret bound with \(_{}()\) replaced by \(C_{}\) in Eq.2.

To extend this framework to the setting with feedback graph \(G\), we define \(_{}(;,x,G)\) as follows

\[_{}(;,x,G)\] \[:=_{p(K)}_{}(p,; ,x,G)\] \[:=_{p(K)}_{a^{*}[K]\\ f^{*}}_{a p}f^{*}(x,a)-f^{*} (x,a^{*})-_{A G(|a)}[_{a^{}  A}(_{t}(x,a^{})-f^{*}(x,a^{}))^{2}].\] (3)

Compared with Eq.1, the difference is that we replace the squared estimation error on action \(a\) by the expected one on the observed set \(A G(|a)\), which intuitively utilizes more feedbacks from the graph structure. When the feedback graph is the identity matrix, we recover Eq.1. Based on \(_{}(;,x,G)\), our algorithm \(\) is shown in Algorithm1. As what is done in [Foster et al., 2021], in order to derive an efficient algorithm, instead of solving the distribution \(p_{t}\) with respect to the supremum over \(f^{*}\), we solve \(p_{t}\) that minimize \(}_{}(p;,x_{t},G_{t})\) (Eq.4), which takes supremum over \(f^{*}([K])\), leading to an upper bound on \(_{}(;,x_{t},G_{t})\). Then, we receive the loss \(\{_{t,j}\}_{j A_{t}}\) and feed the tuples \(\{(x_{t},j,_{t,j})\}_{j A_{t}}\) to the regression oracle \(_{}\). Following a similar analysis to [Foster et al., 2021], we show that to bound the regret \(_{}\), we only need to bound \(}_{}(p_{t};_{t},x_{t},G_{t})\).

**Theorem 3.1**.: _Suppose \(}_{}(p_{t};_{t},x_{t},G_{t}) C ^{-}\) for all \(t[T]\) and some \(>0\), Algorithm1 with \(=\{4,(CT)^{}_{}^{-}\}\) guarantees that \([_{}]C^{ }T^{}_{}^{}\)._

The proof is deferred to AppendixA. In Section3.3, we give an efficient implementation for solving Eq.4 via reduction to convex programming.

### Regret Bounds

In this section, we derive regret bounds for Algorithm1 when \(G_{t}\)'s are specialized to deterministic graphs, i.e., \(G_{t}\{0,1\}^{}\). We utilize discrete graph notation \(G=([K],E)\), where \(E[K][K]\); and define \(N^{}(G,j)\{i:(i,j) E\}\) as the set of nodes that can observe node \(j\). In this case, at each round \(t\), the observed node set \(A_{t}\) is a deterministic set which contains any node \(i\) satisfying \(a_{t} N^{}(G_{t},i)\). In the following, we introduce several graph-theoretic concepts for deterministic feedback graphs [Alon et al., 2015].

Strongly/Weakly Observable Graphs.For a directed graph \(G=([K],E)\), a node \(i\) is observable if \(N^{}(G,i)\). An observable node is strongly observable if either \(i N^{}(G,i)\) or \(N^{}(G,i)=[K]\{i\}\), and weakly observable otherwise. Similarly, a graph is observable if all its nodes are observable. An observable graph is strongly observable if all nodes are strongly observable, and weakly observable otherwise. Self-aware graphs are a special type of strongly observable graphs where \(i N^{}(G,i)\) for all \(i[K]\).

Independent Set and Weakly Dominating Set.An independence set of a directed graph is a subset of nodes in which no two distinct nodes are connected. The size of the largest independence set of a graph is called its independence number. For a weakly observable graph \(G=([K],E)\), a weakly dominating set is a subset of nodes \(D[K]\) such that for any node \(j\) in \(G\) without a self-loop, there exists \(i D\) such that directed edge \((i,j) E\). The size of the smallest weakly dominating set of a graph is called its weak domination number. Alon et al. (2015) show that in non-contextual bandits with a fixed feedback graph \(G\), the minimax regret bound is \(()\) when \(G\) is strongly observable and \((d^{}T^{})\) when \(G\) is weakly observable, where \(\) and \(d\) are the independence number and the weak domination number of \(G\), respectively.

#### 3.2.1 Strongly Observable Graphs

In the following theorem, we show the regret bound of Algorithm 1 for strongly observable graphs.

**Theorem 3.2** (Strongly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and strongly observable with independence number no more than \(\). Then Algorithm 1 guarantees that_

\[}_{}(p_{t};_{t},x_{t},G_{t}) .\]

In contrast to existing works that derive a closed-form solution of \(p_{t}\) in order to show how large the DEC can be (Foster and Rakhlin, 2020; Foster and Krishnamurthy, 2021), in our case we prove the upper bound of \(}_{}(p_{t};_{t},x_{t},G_{t})\) by using the Sion's minimax theorem and the graph-theoretic lemma proven in (Alon et al., 2015). The proof is deferred to Appendix A.1. Combining Theorem 3.2 and Theorem 3.1, we directly have the following corollary:

**Corollary 3.3**.: _Suppose that \(G_{t}\) is deterministic, strongly observable, and has independence number no more than \(\) for all \(t[T]\). Algorithm 1 with choice \(=\{4,_{}}\}\) guarantees that_

\[[_{}]} { T_{}}.\]

For conciseness, we show in Corollary 3.3 that the regret guarantee for Algorithm 1 depends on the largest independence number of \(G_{t}\) over \(t[T]\). However, we in fact are able to achieve a move adaptive regret bound of order \(}^{T}_{t}_{ }}\) where \(_{t}\) is the independence number of \(G_{t}\).

It is straightforward to achieve this by applying a standard doubling trick on the quantity \(_{t=1}^{T}_{t}\), assuming we can compute \(_{t}\) given \(G_{t}\), but we take one step further and show that it is in fact unnecessary to compute \(_{t}\) (which, after all, is NP-hard (Karp, 1972)): we provide an adaptive tuning strategy for \(\) by keeping track the the cumulative value of the quantity \(_{p(K)}}_{}(p;_{t},x_{t},G_{t})\) and show that this efficient method also achieves the adaptive \(}^{t}_{t}_{ }}\) regret guarantee; see Appendix D for details.

#### 3.2.2 Weakly Observable Graphs

For the weakly observable graph, we have the following theorem.

**Theorem 3.4** (Weakly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and weakly observable with weak domination number no more than \(d\). Then Algorithm 1 with \( 16d\) guarantees that_

\[}_{}(p_{t};_{t},x_{t},G_{t}) (}+(K)}{ }),\]_where \(\) is the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\)._

The proof is deferred to Appendix A.2. Similar to Theorem 3.2, we do not derive a closed-form solution to the strategy \(p_{t}\) but prove this upper bound using the minimax theorem. Combining Theorem 3.4 and Theorem 3.1, we are able to obtain the following regret bound for weakly observable graphs, whose proof is deferred to Appendix A.2.

**Corollary 3.5**.: _Suppose that \(G_{t}\) is deterministic, weakly observable, and has weak domination number no more than \(d\) for all \(t[T]\). In addition, suppose that the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) is no more than \(\) for all \(t[T]\). Then, Algorithm 1 with \(=\{16d,T/_{}},d^{ }T^{}_{}^{-}\}\) guarantees that_

\[[_{}]}(d^{ }T^{}_{}^{}+T_{}}).\]

Similarly to the strongly observable graph case, we also derive an adaptive tuning strategy for \(\) to achieve a more refined regret bound \(}(^{T}_{t} _{}}+(_{t=1}^{T}})^{ {3}}_{}^{})\) where \(_{t}\) is the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) and \(d_{t}\) is the weakly domination number of \(G_{t}\). This is again achieved _without_ explicitly computing \(_{t}\) and \(d_{t}\); see Appendix D for details.

### Implementation

In this section, we show that solving \(*{argmin}_{p(K)}}_{}(p; ,x,G)\) in Algorithm 1 is equivalent to solving a convex program, which can be easily and efficiently implemented in practice.

**Theorem 3.6**.: _Solving \(*{argmin}_{p(K)}}_{}(p; ,x,G)\) is equivalent to solving the following convex optimization problem._

\[_{p(K),z} p^{}+z\] (5) \[  a[K]:\|p-e_{a}\|_{ (G^{}p)^{-1}}^{2}(x,a)+z,\] \[G^{}p 0,\]

_where \(\) in the objective is a shorthand for \((x,)^{K}\), \(e_{a}\) is the \(a\)-th standard basis vector, and \(\) means element-wise greater._

The proof is deferred to Appendix A.4. Note that this implementation is not restricted to the deterministic feedback graphs but applies to the general stochastic feedback graph case. In Appendix A.3, we provide the \(20\) lines of Python code that solves Eq. (5).

## 4 Examples with Closed-Form Solutions

In this section, we present examples and corresponding closed-form solutions of \(p\) that make the value \(}_{}(p;,x,G)\) upper bounded by at most a constant factor of \(_{p}}_{}(p;,x,G)\). This offers an alternative to solving the convex program defined in Theorem 3.6 for special (and practically relevant) cases, thereby enhancing the efficiency of our algorithm. All the proofs are deferred to Appendix B.

Cops-and-Robbers Graph.The "cops-and-robbers" feedback graph \(G_{}=11^{}-I\), also known as the loopless clique, is the full feedback graph removing self-loops. Therefore, \(G_{}\) is strongly observable with independence number \(=1\). Let \(a_{1}\) be the node with the smallest value of \(\) and \(a_{2}\) be the node with the second smallest value of \(\). Our proposed closed-form distribution \(p\) is only supported on \(\{a_{1},a_{2}\}\) and defined as follows:

\[p_{a_{1}}=1-_{a_{2}}-_{a_{1}})}, p _{a_{2}}=_{a_{2}}-_{a_{1}})}.\] (6)In the following proposition, we show that with the construction of \(p\) in Eq. (6), \(}_{}(p;,x,G_{})\) is upper bounded by \((}{{}})\), which matches the order of \(_{p}}_{}(p;,x,G)\) based on Theorem 3.2 since \(=1\).

**Proposition 1**.: _When \(G=G_{}\), given any \(\), context \(x\), the closed-form distribution \(p\) in Eq. (6) guarantees that \(}_{}(p;,x,G_{})\)._

Apple Tasting Graph.The apple tasting feedback graph \(G_{}=1&1\\ 0&0\) consists of two nodes, where the first node reveals all and the second node reveals nothing. This scenario was originally proposed by Helmbold et al. (2000) and recently denoted the spam filtering graph (van der Hoeven et al., 2021). The independence number of \(G_{}\) is \(1\). Let \(_{1}\) be the oracle prediction for the first node and let \(_{2}\) be the prediction for the second node. We present a closed-form solution \(p\) for Eq. (4) as follows:

\[p_{1}=1&_{1}_{2}\\ _{1}-_{2})}&_{1}>_{2}, p_{2}=1-p_{1}.\] (7)

We show that this distribution \(p\) satisfies that \(}_{}(p;,x,G_{})\) is upper bounded by \((}{{}})\) in the following proposition. We remark that directly applying results from (Foster et al., 2021) cannot lead to a valid upper bound since the second node does not have a self-loop.

**Proposition 2**.: _When \(G=G_{}\), given any \(\), context \(x\), the closed-form distribution \(p\) in Eq. (7) guarantees that \(}_{}(p;,x,G_{})()\)._

Inventory Graph.In this application, the algorithm needs to decide the inventory level in order to fulfill the realized demand arriving at each round. Specifically, there are \(K\) possible chosen inventory levels \(a_{1}<a_{2}<<a_{K}\) and the feedback graph \(G_{}\) has entries \(G(i,j)=1\) for all \(1 j i K\) and \(G(i,j)=0\) otherwise, meaning that picking the inventory level \(a_{i}\) informs about all actions \(a_{j i}\). This is because items are consumed until either the demand or the inventory is exhausted. The independence number of \(G_{}\) is \(1\). Therefore, (very) large \(K\) is statistically tractable, but naively solving the convex program Eq. (5) requires superlinear in \(K\) computational cost. We show in the following proposition that there exists an analytic form of \(p\), which guarantees that \(}_{}(p;,x,G_{})\) can be bounded by \((}{{}})\).

**Proposition 3**.: _When \(G=G_{}\), given any \(\), context \(x\), there exists a closed-form distribution \(p(K)\) guaranteeing that \(}_{}(p;,x,G_{})()\), where \(p\) is defined as follows: \(p_{j}=\{_{j}-_{i}_{i})}-_{j ^{}>j}p_{j^{}},0\}\) for all \(j[K]\)._

Undirected Self-Aware Graph.For the undirected and self-aware feedback graph \(G\), which means that \(G\) is symmetric and has diagonal entries all \(1\), we also show that a certain closed-form solution of \(p\) satisfies that \(}_{}(p;,x,G)\) is bounded by \(()\).

**Proposition 4**.: _When \(G\) is an undirected self-aware graph, given any \(\), context \(x\), there exists a closed-form distribution \(p(K)\) guaranteeing that \(}_{}(p;,x,G) {}{}\)._

## 5 Experiments

In this section, we use empirical results to demonstrate the significant benefits of SquareCB.G in leveraging the graph feedback structure and its superior effectiveness compared to SquareCB. Following Foster and Krishnamurthy (2021), we use progressive validation (PV) loss as the evaluation metric, defined as \(L_{}(T)=_{t=1}^{T}_{t,a_{t}}\). All the feedback graphs used in the experiments are deterministic. We run experiments on CPU Intel Xeon Gold 6240R 2.4G and the convex program solver is implemented via Vowpal Wabbitt (Langford et al., 2007).

### SquareCB.G under Different Feedback Graphs

In this subsection, we show that our SquareCB.G benefits from considering the graph structure by evaluating the performance of SquareCB.G under three different feedback graphs. We conduct experiments on RCV1 dataset and leave the implementation details in Appendix C.1.

The performances of SquareCB.G under bandit graph, full information graph and cops-and-robbers graph are shown in the left part of Figure 1. We observe that SquareCB.G performs the best under full information graph and performs worst under bandit graph. Under the cops-and-robbers graph, much of the gap between bandit and full information is eliminated. This improvement demonstrates the benefit of utilizing graph feedback for accelerating learning.

### Comparison between SquareCB.G and SquareCB

In this subsection, we compare the effectiveness of SquareCB.G with the SquareCB algorithm. To ensure a fair comparison, both algorithms update the regressor using the same feedbacks based on the graph. The only distinction lies in how they calculate the action probability distribution. We summarize the main results here and leave the implementation details in Appendix C.2.

#### 5.2.1 Results on Random Directed Self-aware Graphs

We conduct experiments on RCV1 dataset using random directed self-aware feedback graphs. Specifically, at round \(t\), the deterministic feedback graph \(G_{t}\) is generated as follows. The diagonal elements of \(G_{t}\) are all \(1\), and each off-diagonal entry is drawn from a Bernoulli\((}{{4}})\) distribution. The results are presented in the right part of Figure 1. Our SquareCB.G consistently outperforms SquareCB and demonstrates lower variance, particularly when the number of iterations was small. This is because when there are fewer samples available to train the regressor, it is more crucial to design an effective algorithm that can leverage the graph feedback information.

#### 5.2.2 Results on Synthetic Inventory Dataset

In the inventory graph experiments, we create a synthetic inventory dataset and design a loss function for each inventory level \(a_{t}\) with demand \(d_{t}\). Since the action set \(\) is continuous, we discretize the action set in two different ways to apply the algorithms.

Fixed discretized action set.In this setting, we discretize the action set using fixed grid size \(\{,,\}\), which leads to a finite action set \(\) of size \(+1\). Note that according to Theorem 3.2, our regret _does not_ scale with the size of the action set (to within polylog factors), as the independence number is always \(1\). The results are shown in the left part of Figure 2.

Figure 1: **Left figure**: Performance of SquareCB.G on RCV1 dataset under three different feedback graphs. **Right figure**: Performance comparison between SquareCB.G and SquareCB under random directed self-aware feedback graphs.

We remark several observations from the results. First, our algorithm SquareCB.G outperforms SquareCB for all choices \(K\{101,301,501\}\). This indicates that SquareCB.G utilizes a better exploration scheme and effectively leverages the structure of \(G_{}\). Second, we observe that SquareCB.G indeed does not scale with the size of the discretized action set \(\), since under different discretization scales, SquareCB.G has similar performances and the slight differences are from the improved approximation error with finer discretization. This matches the theoretical guarantee that we prove in Theorem3.2. On the other hand, SquareCB does perform worse when the size of the action set increases, matching its theoretical guarantee which scales with the square root of the size of the action set.

Adaptively changing action set.In this setting, we adaptively discretize the action set \(\) according to the index of the current round. Specifically, for SquareCB.G, we uniformly discretize the action set \(\) with size \(\), whose total discretization error is \(()\) due to the Lipschitzness of the loss function. For SquareCB, to optimally balance the dependency on the size of the action set and the discretization error, we uniformly discretize the action set \(\) into \( t^{}\) actions. The results are illustrated in the right part of Figure2. We can observe that SquareCB.G consistently outperforms SquareCB by a clear margin.

## 6 Related Work

Multi-armed bandits with feedback graphs have been extensively studied. An early example is the apple tasting problem of Helmbold et al. (2000). The general formulation was introduced by Mannor and Shamir (2011). Alon et al. (2015) characterized the minimax rates in terms of graph-theoretic quantities. Follow-on work includes relaxing the assumption that the graph is observed prior to decision (Cohen et al., 2016); analyzing the distinction between the stochastic and adversarial settings (Alon et al., 2017); considering stochastic feedback graphs (Li et al., 2020; Esposito et al., 2022); instance-adaptivity (Ito et al., 2022); data-dependent regret bound (Lykouris et al., 2018; Lee et al., 2020); and high-probability regret under adaptive adversary (Neu, 2015; Luo et al., 2023).

The contextual bandit problem with feedback graphs has received relatively less attention. Wang et al. (2021) provide algorithms for adversarial linear bandits with uninformed graphs and stochastic contexts. However, this work assumes several unrealistic assumptions on both the policy class and the context space and is not comparable to our setting, since we consider the informed graph setting with adversarial context. Singh et al. (2020) study a stochastic linear bandits with informed feedback graphs and are able to improve over the instance-optimal regret bound for bandits derived in (Lattimore and Szepesvari, 2017) by utilizing the additional graph-based feedbacks.

Figure 2: Performance comparison between SquareCB.G and SquareCB on synthetic inventory dataset. **Left figure**: Results under fixed discretized action set. **Right figure**: Results under adaptive discretization of the action set. Both figures show the superiority of SquareCB.G compared with SquareCB.

Our work is also closely related to the recent progress in designing efficient algorithms for classic contextual bandits. Starting from (Langford and Zhang, 2007), numerous works have been done to the development of practically efficient algorithms, which are based on reduction to either cost-sensitive classification oracles (Dudik et al., 2011; Agarwal et al., 2014) or online regression oracles (Foster and Rakhlin, 2020; Foster et al., 2020, 2021; Zhu and Mineiro, 2022). Following the latter trend, our work assumes access to an online regression oracle and extends the classic bandit problems to the bandits with general feedback graphs.

## 7 Discussion

In this paper, we consider the design of practical contextual bandits algorithm with provable guarantees. Specifically, we propose the first efficient algorithm that achieves near-optimal regret bound for contextual bandits with general directed feedback graphs with an online regression oracle.

While we study the informed graph feedback setting, where the entire feedback graph is exposed to the algorithm prior to each decision, many practical problems of interest are possibly uninformed graph feedback problems, where the graph is unknown at the decision time. It is unclear how to formulate an analogous minimax problem to Eq. (1) under the uninformed setting. One idea is to consume the additional feedback in the online regressor and adjust the prediction loss to reflect this additional structure, e.g., using the more general version of the E2D framework which incorporates arbitrary side observations (Foster et al., 2021). Cohen et al. (2016) consider this uninformed setting in the non-contextual case and prove a sharp distinction between the adversarial and stochastic settings: even if the graphs are all strongly observable with bounded independence number, in the adversarial setting the minimax regret is \((T)\) whereas in the stochastic setting the minimax regret is \(()\). Intriguingly, our setting is semi-adversarial due to realizability of the mean loss, and therefore it is apriori unclear whether the negative adversarial result applies.

In addition, bandits with graph feedback problems often present with associated policy constraints, e.g., for the apple tasting problem, it is natural to rate limit the informative action. Therefore, another interesting direction is to combine our algorithm with the recent progress in contextual bandits with knapsack (Slivkins and Foster, 2022), leading to more practical algorithms.

AcknowledgmentsHL and MZ are supported by NSF Awards IIS-1943607.