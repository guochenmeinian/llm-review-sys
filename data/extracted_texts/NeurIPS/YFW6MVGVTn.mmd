# NICE: NoIse-modulated Consistency rEquarization

for Data-Efficient GANs

 Yao Ni\({}^{}\), Piotr Koniusz\({}^{,@sectionsign,}\)

\({}^{}\)The Australian National University \({}^{@sectionsign}\)Data6l**PC**SIRO

\({}^{}\)firstname.lastname@anu.edu.au

The corresponding author. Code: https://github.com/MaxwellYaoNi/NICE

###### Abstract

Generative Adversarial Networks (GANs) are powerful tools for image synthesis. However, they require access to vast amounts of training data, which is often costly and prohibitive. Limited data affects GANs, leading to discriminator overfitting and training instability. In this paper, we present a novel approach called NoIse-modulated Consistency rEgalization (NICE) to overcome these challenges. To this end, we introduce an adaptive multiplicative noise into the discriminator to modulate its latent features. We demonstrate the effectiveness of such a modulation in preventing discriminator overfitting by adaptively reducing the Rademacher complexity of the discriminator. However, this modulation leads to an unintended consequence of increased gradient norm, which can undermine the stability of GAN training. To mitigate this undesirable effect, we impose a constraint on the discriminator, ensuring its consistency for the same inputs under different noise modulations. The constraint effectively penalizes the first and second-order gradients of latent features, enhancing GAN stability. Experimental evidence aligns with our theoretical analysis, demonstrating the reduction of generalization error and gradient penalization of NICE. This substantiates the efficacy of NICE in reducing discriminator overfitting and improving stability of GAN training. NICE achieves state-of-the-art results on CIFAR-10, CIFAR-100, ImageNet and FFHQ datasets when trained with limited data, as well as in low-shot generation tasks.

## 1 Introduction

The remarkable advancements and breakthroughs in deep learning  can be largely attributed to the extensive utilization of vast amounts of training data. This abundance of data has driven the progress across various domains of deep learning. Among notable recent advancements are Generative Adversarial Networks (GANs) , popular in industry and academia. GANs have proven their high-quality image generation abilities and achieved high generation speeds , establishing them as a versatile tool for a wide range of applications, such as text-to-image generation , destylization , image-to-image translation , and 3D generation .

Despite the impressive capabilities of state-of-the-art GANs in generating diverse and high-quality images , their effectiveness heavily relies on large volumes of training data. The acquisition of such large datasets helps GANs attain the desired adversarial equilibrium. However, under limited training data regime, GANs encounter challenges associated with discriminator overfitting and unstable training .

To address the aforementioned challenges, recent studies have approached the problem from three perspectives. The first perspective involves the utilization of extensive differentiable data augmentation techniques, aimed at expanding the distribution of the available data . The second perspective leverages knowledge gained from large-scale models trained on large datasets . While these approaches are effective, they also carry inherent risks, such as the leakage of augmentation clues  or pre-trained knowledge .

The third perspective focuses on the regularization of the discriminator, aiming to weaken its learning ability [11; 19] or increase the overlap between real and fake support sets . In line with this perspective, our paper introduces a novel approach to improve GAN generalization. We propose to modulate the hidden features of discriminator via adaptive multiplicative noise, which strikes a balance in maintaining a certain level of discrimination ability while also regularizing the Rademacher complexity  of the discriminator. This reduction in Rademacher complexity, which quantifies the capacity of model to fit random variables, narrows the generation gap between the training and unseen data, resulting in enhanced GAN generalization [4; 56; 18].

Nevertheless, training the discriminator with adaptive noise unintentionally amplifies second-order gradient derivative of latent features corresponding to real images. This elevated gradient leads to abrupt gradient changes near the real sample points, potentially causing instability in the feedback to the generator. This issue aligns with the findings in works [26; 36; 56; 11], which emphasize the importance of penalizing gradients for both real and fake samples to promote convergence and stability of GAN training. To address these challenges, we propose a constraint on the discriminator that ensures consistency for the same inputs under different noise modulations. While our idea is simple, our theoretical analysis reveals that this constraint effectively penalizes the first and second-order gradients of the latent features. Consequently, the gradient provided to the generator becomes more stable, resulting in improved training stability and generalization.

Our comprehensive experiments confirm the effectiveness of NICE in penalizing gradients and reducing the generalization gap. Despite the simple design, NICE significantly improves the training stability and generalization of GANs, outperforming alternative approaches in preventing discriminator overfitting. NICE achieves superior results on challenging limited data benchmarks, including CIFAR-10/100, ImageNet, FFHQ, and low-shot image generation tasks.

Our contributions can be summarized as follows:

1. We limit discriminator overfitting by using the adaptive multiplicative noise to modulate the latent features of the discriminator, resulting in enhanced generalization of GANs.
2. We introduce NICE, a technique that enforces the discriminator to be consistent with the same inputs under different noise modulations, implicitly penalizing the first and second-order gradients of the latent features during GAN training, promoting the training stability.
3. We show that NICE, both in theory and practice, effectively prevents discriminator overfitting and achieves superior performance in image generation under limited data setting.

## 2 Related Work

**Improving GANs.** Generative Adversarial Networks  are powerful generative models that excel in image generation [21; 24; 25], text-to-image generation [20; 47; 55], image-to-image translation [27; 44], and 3D generation [52; 65; 53]. However, GANs commonly encounter challenges such as training instability , mode collapse , and discriminator overfitting . Researchers have investigated different GAN loss functions [2; 42; 71], architectures [21; 24; 25; 23], and regularization strategies [38; 33; 14]. The \(f\)-GAN  generalizes GANs to \(f\)-divergences. WGAN  adopts the Earth-Mover distance. OmniGAN  extends conditional GANs to a multi-label softmax loss. StyleGANV1-3 [24; 25; 23] enhances the architecture of generator. Approaches [14; 26; 36; 56; 11] propose explicit penalty of gradient of discriminator. SNGAN  enforces a Lipschitz constraint on the discriminator, and approach  regularizes the spectral norm of the discriminator. In this study, we propose a novel regularization strategy applicable to diverse GAN loss functions and architectures, specifically tailored for training GAN models with limited data.

**Image generation with limited data.** Collecting data is both laborious and expensive. Consequently, generating images in limited data settings presents significant challenges, primarily due to discriminator overfitting. Previous approaches have tackled this issue through data augmentation techniques [22; 68; 17; 32; 63] and leveraging knowledge transfer from large-scale pre-trained models [9; 29; 70]. Differentiable data augmentation methods [22; 68; 17] "extend" the data distribution, while methods [63; 32] explore contrastive learning within GANs. Approaches [9; 29; 70] employ pre-trained models to guide training of discriminator. However, both GAN types suffer issues, including leakage of augmentation clues [64; 22] or pre-trained knowledge . An alternative approach involves regularizing the discriminator. LeCamGAN  suggests reducing the output gap of discriminator between real and fake distributions. LCSAGAN  employs manifold techniques to project discriminator features onto manifold and decrease the capacity of discriminator. APA uses fake images as real images to increase overlap between the real and fake distributions. DigGAN  minimizes the gradient norm gap between real and fake images. Inspired by such an approach, we propose to use an adaptive noise to modulate the latent features of discriminator to reduce the generalization error of discriminator as a means of regularization.

**Consistency regularization.** CTGAN  introduces the enforcement of a Lipschitz constraint by ensuring the consistency of the response of discriminator to real images. CAGAN  and GenCo  enforce consistency among multiple discriminators. R-Drop  applies consistency regularization to transformers  equipped with dropout for natural language processing tasks. Augmentation-based consistency regularization GANs  enforce consistency by considering different augmented views of the same image, although they may inadvertently leak augmentation clues to the generator . Despite the effectiveness of consistency regularization demonstrated by these previous works, their success is primarily empirical, lacking theoretical analysis. In our study, we go beyond prior works by providing a theoretical analysis of consistency regularization in GANs. By delving deeper into the underlying principles, we develop a more comprehensive understanding of the mechanism behind its effectiveness.

**Generalization of GANs.** Arora _et al_.  have contributed to the understanding and improvement of generalization of GANs by showing the importance of achieving adversarial equilibrium, reducing discriminator discrepancy, penalizing gradients, and bounding the generalization error. Our motivation aligns with these works in the pursuit of enhancing generalization capabilities. However, our approach implicitly and adaptively reduces the generalization gap while penalizing gradients of latent features of discriminator. Such a setting provides an efficient and effective means of preventing discriminator overfitting and improving generalization.

## 3 Method

To boost the generalization of GAN, we start by analyzing their generalization error, bounding it with the Rademacher complexity of discriminator, and linking it with the weight norm. By incorporating multiplicative noise, we demonstrate its regularization benefits on the weight norm. However, such a strategy induces large gradient norm destabilizing training of GAN. To this end, we introduce NICE to penalize the gradients of discriminator. Finally, we provide NICE and showcase it use in GANs.

### Generalization error of GANs and complexity of neural network

The primary goal of GAN is to minimize the integral probability metric , assuming access to infinite real and fake data during optimization, _i.e._, the infinite real and generated distributions \((,)\) as discussed in . In practice, we often have limited access to a finite dataset \(_{n}\) of size \(n\). Consequently, our optimization is restricted to the empirical loss:

\[_{}d_{}(_{n},):=_{h }\{_{_{n}}[h()]-_{ {}}[h(})]\}}.\] (1)

Function sets of discriminator and generator, \(\) and \(\), are typically parameterized in GAN as neural network classes \(_{}=\{h(;_{d}):_{d}_{d}\}\) and \(_{}=\{g(;_{g}):_{g}_{g}\}\) where \( p_{z}\) serves as the random noise input to the generator. The associated term \(d_{_{}}\) is referred to as the neural network distance . The discriminator network \(D:= f\) consists of a real/fake prediction head \(\) and a feature extractor \(f\). As the loss function \(()\) varies across tasks, architectures or choice of divergence type, we compose it with \(D\) to simplify the analysis and notation, _i.e._, \(h():=(D())\). Thus, the alternative optimization of discriminator and generator becomes:

\[L_{D}=_{_{d}}_{}_{n }}[h(};_{d})]-_{_{n}}[h( ;_{d})],\\ L_{G}=_{_{g}}-_{ p_{}}[h(g(;_{g}))],\] (2)

where we assume \(_{n}\) minimizes \(d_{}(_{n},)\) up to precision \( 0\), meaning that \(d_{}(_{n},_{n})_{}d_{}(_{n},)+\). As we are interested in how close the generator distribution \(_{n}\) is to the unknown infinite distribution \(\), we refer to the lemma  on the generalization error of GAN:

**Lemma 1**: _(Theorem 3.1 of ) Assume that the discriminator set \(\) is even (\(h\) implies \(-h\)) and all discriminators are bounded by \(\|h\|_{}\). Let \(_{n}\) be an empirical measure of an i.i.d. sample of size \(n\) drawn from \(\). Assume \(d_{}(_{n},_{n})-_{}d_{}( _{n},)\). Then with probability at least \(1-\), we have:_

\[d_{}(,_{n})-_{}d_{}(,)  2_{h}_{}[h]-_{ _{n}}[h]+ 2R_{n}^{()}()+2}+,\] (3)_where the Rademacher complexity , \(R_{n}^{()}():=_{h}_ {i}^{(i)}h(^{(i)})\), measures how well the function \(h\) fits the Rademacher random variable \(^{(i)}\) with \((^{(i)}=1)=(^{(i)}=-1)=\) given samples \(^{(i)}_{n}\)._

Lemma 1 provides a crucial insight that one can assess the generalization error of GAN by comparing the output discrepancy of discriminator between training data and unseen data, and such an error is influenced by the Rademacher complexity of the discriminator. To enhance the performance of generator while reducing the generalization error, we have two possibilities: 1) increase the quantity \(n\) of real data, which provides a stronger foundation for training a better generator; 2) reduce the Rademacher complexity of the discriminator. However, this reduction must be carefully controlled, as an overly simplified discriminator may struggle to effectively distinguish real and fake data.

To manage the Rademacher complexity of the discriminator, we leverage a theorem from approach  to establish an upper bound on the Rademacher complexity of the neural network.

**Lemma 2**: _(Eq. 1.2 in , Theorem 5.20 in ) Consider a fully-connected neural network \(v_{}()=_{t}(_{t-1}(...(_ {1})...))\) where \(_{i}\) are linear weights at \(i\)-th layer and \(\) is a 1-Lipschitz activation function. Suppose that \( i\{1,...,n\}\), \(\|^{(i)}\|_{2} q\). Let \(\|_{i}\|_{lip}\) be the lipschitz constant of \(_{i}\) and \(\|_{i}^{T}\|_{2,1}\) be the sum of the \(l_{2}\) norm of columns in \(_{i}\). Let \(=\{v_{}:\|_{i}\|_{lip} k_{i},\|_{i}^{T}\| _{2,1} b_{i}\}\), we have the Rademacher complexity:_

\[R_{n}^{()}()}:_{i=1}^{t}k_{i} _{i=1}^{t}^{2/3}}{k_{i}^{2/3}}^{3/ 2}.\] (4)

Lemma 2 links the Rademacher complexity of a neural network to the Lipschitz constant and the (2,1)-norm of its weights, providing essential insights for controlling the complexity of discriminator.

### Improving generalization by feature modulation with multiplicative noise

Taking into account Lemmas 1 and 2, one can effectively manage the generalization error of GAN by controlling the Rademacher complexity of the discriminator. Such a control requires managing the Lipschitz constant and the norm of the weights of discriminator. Typically, controlling the Lipschitz constant can be achieved by the spectral normalization  or the gradient penalty . In contrast, we propose a novel approach: regularizing the norm of the weights by modulating the latent features of the discriminator with multiplicative noise.

We build on the analysis (Prop. 4 in Appendix of ) of Dropout regularization  by exploring the regularization effect of the Gaussian multiplicative noise within the context of deep regression.

**Theorem 1**: _(Regularization by the Gaussian multiplicative noise in deep regression) Consider a regression task on a two-layer neural network. Let \(\!\!^{d_{0}}\) and \(\!\![-1,1]^{d_{2}}\) be the input and output spaces, and \(\!=\!\!\!\). Let \(n\) examples \(\{(^{(i)},^{(i)})\}_{i=1}^{n}\!\!^{n}\). Let \(f_{w}\!:\!\) be parameterized by \(\{w\!:\!_{1}\!\!^{d_{1} d_{0}},_{2}\!\! ^{d_{2} d_{1}}\}\) and \(f_{w}(,)\!=\!_{2}((_{1}))\) where \(\) is 1-Lipschitz activation function, \(\) denotes element-wise multiplication and \(\!\!(,^{2}^{d_{1}})\). Let \(\!=\!(_{1})\), \(}\!=\!}_{i}[^{(i)}^{(i)}]\), \(_{k}\!\!0\) denotes the \(k\)-th element of \(}\). Let \(\|_{2}\|_{2,1}\!=\!_{k}\|_{k}\|_{2}\) where \(_{k}\) is the \(k\)-th column vector in \(_{2}\). The regression task with the \(l_{2}\) loss leads to the weight regularization as:_

\[_{}(w):=}_{i}_{}\|^{(i)}-_{2}(^{(i)})\|_{2}^{2}=}_{i} \|^{(i)}-_{2}^{(i)}\|_{2}^{2}+^{2}_{ k}_{k}\|_{k}\|_{2}^{2}.\] (5)

The proof can be found in SSC.1. Theorem 1 reveals that modulation with Gaussian multiplicative noise in the regression task implicitly regularizes the internal weight norm, with the regularization strength determined by the variance of the noise \(^{2}\) and the magnitude of the features.

Despite analysis in a simplified two-layer system, the implicit weight regularization by the noise modulation applies to multi-layer and convolutional neural networks, which can be expressed as a combination of two-layer nets, and convolutional layers are a type of linear layer [37; 60; 3]. Assuming latent features in deep neural networks follow a Gaussian distribution , we can equate the Bernoulli noise modulation (Dropout) with \(()\), whose values are set to be \(1/(1-)\) with probability \(1-\) and 0 otherwise, to the Gaussian multiplicative noise modulation in subsequent layers, making the regularization effects from Theorem 1 applicable to the Bernoulli noise as well.

Theorem 1 illustrates that one can modulate the latent features in the discriminator by the multiplicative noise to adaptively regularize the norm of its weights, leading to the reduced Rademacher complexity in Lemma 2 and improved GANs generalization in Lemma 1.

### Consistency regularization

Although the noise modulation can reduce the Rademacher complexity, incorporating the noise increases the gradient norms of latent features and the inputs to the discriminator, making training of GAN unstable  and difficult to converge . To understand the gradient-related challenges in the noise-modulated discriminator, we adopt the Taylor expansion with a focus on the pivotal first- and second-order terms, following standard practice .

**Proposition 1**: _Define \(f:=f_{t} f_{2}\) as the feature extractor of the discriminator from the second layer onward. Let \(}=f_{1}(})\) and \(=f_{1}()\) be the latent features of the first layer for fake and real images, respectively. The discriminator is defined as \(h()=((f()))\), with \(\) as the prediction head and \(\) as the loss function. Introduce a multiplicative noise \((,^{2}^{d})\) modulating solely the first layer. Let \(H^{(h)}_{kk}()\) be the \(k\)-th diagonal entry of the Hessian matrix of \(h\) at \(\) and \(a_{k}\) be the \(k\)-th element of \(\). Applying Taylor expansion to \(h()\), the GAN loss can be expressed as follows:_

\[_{_{d}}L^{}_{D}}= _{}}_{}h( })-_{}_{}h( )\] \[ _{}}[h(})]\!-\! _{}h()\!+\,}{2} _{}}_{k}_{k}^{2}H^{(h)}_{kk}( })\ -\ _{}_{k}a_{k}^{2}H^{(h)}_{kk}( })\,\] (6) \[_{_{g}}L^{}_{G}}= -_{}}_{}}h( })-_{}}h( })\ -\ }{2}_{}}_{k}_{k}^{2} H^{(h)}_{kk}(})\.\] (7)

[left=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,right=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,rightright=0pt,right=0pt,right=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,right=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightright=0pt,rightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightright=0pt,rightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightright=0pt,rightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright=0pt,rightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright=0pt,where  represents the operation that expands \(\) into \(1\!\!d^{}\!\!d^{H}\!\!d^{W}\) shape and performs element-wise multiplication with \(\). The noise \(\) is carefully controlled through an adaptive \(\). In the case of BigGAN and OmniGAN, we control the variance of \(\) using \(\) as \((,^{2}^{d^{}})\). For StyleGAN2, we control the noise through \(()\), where with probability \(1-\) it takes the value \(\), and with probability \(\) it takes 0.

Below we introduce a mechanism to control the noise via the meta-parameter \(\) by detecting potential overfitting in the discriminator. Firstly, we compute the expectation over the discriminator output \(r()=[(D())]\) w.r.t. real samples \(\), and evaluate \(=(r()\!>\!)\!\!\{-1,0,1\}\) where \(\) is a fixed threshold. A value greater than the threshold indicates potential overfitting . We apply \(_{t+1}=_{t}+_{}\) to update \(\) with smaller \(_{}\) to control \(\). Denote the modified discriminator with our adaptive noise as \(h_{}\), the objective of GAN with the adaptive noise modulation becomes:

\[L_{D}^{}=_{_{d}}_{}_{n}}[h_{}(};_{d})]-_{ _{n}}[h_{}(;_{d})],\\ L_{G}^{}=_{_{g}}-_{_{z}}[h_{ }(g(;_{g}))].\] (10)

**GAN with the noise-modulated consistency regularization (NICE).** To deal with the increased gradient norm due to the adaptive noise modulation, we introduce the consistency regularization (NICE) to promote invariance of the discriminator to the same samples under different noise modulations. Our regularization implicitly penalizes the gradient of the latent features in the discriminator by comparing outputs \(f_{1}()\) and \(f_{2}()\) of two feature extractors (with shared parameters) that are subjected to different noise modulations as in Figure 1. The objective for a GAN with NICE is:

\[L_{D}^{}=_{_{d}}_{}_{n}}[h_{}(};_{d})+ ^{}(})]+_{ _{n}}[-h_{}(;_{d})+^{}( )],\\ L_{G}^{}=_{_{g}}_{_{z}}[-h_{ }(g(;_{g}))+^{}(g(;_{g}))],\] (11)

where \(^{}()\!=\!\|f_{1}()\!-\!f_{2}()\|_{2}^{2}\) and \(\!=\!_{}\) is a meta-parameter that controls the strength of the consistency regularization and is adaptively determined by \(\).

**Efficient NICE.** In our experiments, we observed that applying the noise modulation in the later blocks of the discriminator often outperformed applying noise in the earlier blocks. To avoid unnecessary computations (the early blocks do not use modulation), we divide the discriminator into a modulation-free part (\(B_{1},,B_{l-1}\)) and a noise-modulated part (\(B_{l},,B_{L}\)), where \(l\) indicates the starting block of noise modulation. For efficient calculation of the consistency loss, we input two copies of \(_{l-1}\) (output from the first part) into the second part, bypassing the need for redundant calculations to maintain a low computational overhead compared to standard approaches.

## 4 Experiments

We conduct experiments on CIFAR-10/100  using BigGAN  and OmniGAN , as well as on ImageNet  using BigGAN for conditional image generation. We also evaluate our method on low-shot datasets , which include 100-shot Obama/Panda/Grumpy Cat and AnimalFace Dog/Cat , and FFHQ  using StyleGAN2 . We compare our method against several strong baselines, including DA , ADA , DigGAN , MaskedGAN , KDDLGAN , LeCam , GenCo , InsGen , FakeCLR  and TransferGAN . For fair comparison, we denote methods using massive augmentation as "MA", which include DA and ADA.

**Datasets.** CIFAR-10 has 50K/10K training/testing images with resolution of \(32\!\!32\) from 10 categories, whereas CIFAR-100 has 100 classes. FFHQ contains 70K human face images at \(256\!\!256\)

Figure 1: Our discriminator pipeline. NICE is applied to both real \(\) and fake \(}\) images.

pixels. Low-shot datasets contain 100-shot Obama/Panda/Grumpy Cat images, AnimalFace (160 cats and 389 dogs) images at \(256 256\) resolution. ImageNet has 1.2M/50K training/validation images with 1K categories. Following [17; 9], we center-crop and downscale its images to \(64 64\) resolution. The implementation details can be found in SSD. The generated images can be found in SSD.

**Evaluation metrics.** We generate 50K images per dataset to compute the commonly used Inception Score  and Frechet Inception Distance (FID) . We report tFID, computed between 50K generated images and all training images. For CIFAR-10/100, we also compute vFID between 10K generated images and 10K real testing images. For low-shot datasets, we follow  and compute FID between 5K generated images and the entire dataset. For FFHQ, we calculate FID between 50K fake images and the entire training set. For ImageNet, we follow [17; 9] and generate 10K images for computing the IS and FID, where the reference distribution is the entire training set. Following [68; 11; 32], we run 5 trails for methods using NICE, and report the mean of the results. Given that all standard deviations fall below the 1% relative, we omit them for clarity.

### Results on CIFAR-10 and CIFAR-100 for BigGAN and OmniGAN

Tables 1 and 2 demonstrate that NICE consistently outperforms baselines such as BigGAN, LeCam+DA, OmniGAN and OmniGAN+ADA on CIFAR-10 and CIFAR-100, firmly establishing its superiority. NICE also outperforms LeCam+DA+KDDLGAN in the majority of scenarios without any knowledge integration from large-scale models, underscoring its efficiency and effectiveness.

    &  &  &  &  \\   & & IS\(\) & tFID\(\) & vFID\(\) & IS\(\) & tFID\(\) & vFID\(\) & IS\(\) & tFID\(\) & vFID\(\) \\  BigGAN(\(d^{}=256\)) & \(\) & 9.21 & 5.48 & 9.42 & 8.74 & 16.20 & 20.27 & 8.24 & 31.45 & 35.59 \\  +LeCam & \(\) & 9.45 & 4.27 & 8.29 & 8.95 & 11.34 & 15.25 & 8.44 & 28.36 & 33.65 \\ +DigGAN & \(\) & 9.28 & 5.33 & 9.35 & 8.81 & 13.28 & 17.25 & 8.32 & 18.54 & 22.45 \\  +NICE & \(\) & **9.50** & **4.19** & **8.24** & **8.96** & **8.51** & **12.54** & **8.73** & **13.65** & **17.75** \\  +LeCam+DA & \(\) & 9.45 & 4.32 & 8.40 & 9.01 & 8.53 & 12.47 & 8.81 & 12.64 & 16.42 \\ +LeCam+DA+KDDLGAN & \(\) & \(-\) & \(-\) & 8.19 & \(-\) & \(-\) & 11.15 & \(-\) & \(-\) & \(13.86\) \\ +LeCam+DA+NICE & \(\) & **9.52** & **3.72** & **7.81** & **9.12** & **6.92** & **10.89** & **8.99** & **9.86** & **13.81** \\   OmniGAN(\(d^{}=1024\)) & \(\) & 10.01 & 6.92 & 10.75 & 8.64 & 36.75 & 41.17 & 6.69 & 53.02 & 57.68 \\  +DA & \(\) & 10.13 & 4.15 & 8.06 & 9.49 & 13.45 & 17.27 & 8.99 & 19.45 & 23.48 \\ +ADA & \(\) & 10.24 & 4.95 & 9.06 & 9.41 & 27.04 & 30.58 & 7.86 & 40.05 & 44.01 \\ +NICE & \(\) & 10.21 & 2.72 & 6.79 & 9.86 & 6.06 & 9.87 & 9.78 & 6.40 & 10.37 \\ +NICE+ADA & \(\) & **10.38** & **2.25** & **6.32** & **10.18** & **4.39** & **8.42** & **10.08** & **5.49** & **9.42** \\   

Table 1: Comparison w/ and w/o NICE on CIFAR-10 given different percentage of training data.

    &  &  &  &  \\   & & IS\(\) & tFID\(\) & vFID\(\) & IS\(\) & tFID\(\) & vFID\(\) & IS\(\) & tFID\(\) & vFID\(\) \\  BigGAN(\(d^{}=256\)) & \(\) & 11.02 & 7.86 & 12.70 & 9.94 & 25.83 & 30.79 & 7.58 & 50.79 & 55.04 \\  +LeCam & \(\) & **11.41** & 6.82 & 11.54 & 10.05 & 20.81 & 25.77 & 8.14 & 41.51 & 46.43 \\ +DigGAN & \(\) & 11.15 & 8.13 & 13.06 & 9.98 & 16.87 & 21.59 & **9.04** & 23.10 & 27.78 \\ +NICE & \(\) & 10.99 & **6.31** & **11.08** & **10.32** & **13.17** & **17.80** & 8.96 & **19.53** & **24.33** \\  +LeCam+DA & \(\) & 11.25 & 6.45 & 11.26 & 10.12 & 15.96 & 20.42 & 9.17 & 22.75 & 27.14 \\ +LeCam+DA+KDDLGAN & \(\) & \(-\) & \(-\) & **10.12** & \(-\) & \(-\) & 18.70 & \(-\) & \(-\) & 22.40 \\ +LeCam+DA+NICE & \(\) & **11.28** & **5.72** & 10.40 & **10.54** & **10.02** & **14.93** & **9.35** & **14.95** & **19.60** \\   OmniGAN(\(d^{}=1024\)) & \(\) & 12.73 & 8.36 & 13.18 & 10.14 & 40.59 & 44.92 & 6.91 & 60.46 & 64.76 \\  +DA & \(\) & 12.94 & 7.41 & 12.08 & 11.35 & 17.65 & 22.37 & 10.01 & 30.68 & 34.94 \\ +ADA & \(\) & 13.07 & 6.12 & 10.79 & 12.07 & 13.54 & 18.20 & 8.95 & 44.65 & 49.08 \\ +NICE & \(\) & 13.77 & 3.83 & 8.61 & 12.57 & 8.68 & 13.53 & 11.97 & 14.53 & 19.22 \\ +NICE+ADA & \(\) & **13.82** & **3.78** & **8.59** & **12.75** & **6.28** & **10.92** & **12.04** & **9.32** & **14.18** \\   

Table 2: Comparison w/ and w/o NICE on CIFAR-100 given different percentage of training data.

### Results on low-shot generation on StyleGAN2

Table 3 showcases the superior performance of NICE in comparison to leading methods, significantly improving baselines and achieving state-of-the-art results. NICE also surpasses KDDLGAN, despite KDDLGAN leverages the large-scale CLIP model . Figure 2 shows realistic images generated by NICE under scarce data training.

### Results on FFHQ for StyleGAN2 and ImageNet for BigGAN

Results for unconditional image generation on FFHQ are in Table 4, and for conditional image generation on ImageNet \(64 64\) in Table 5. We limit FFHQ to 100, 1K, 2K, 5K real images and follow [17; 9] for ImageNet setting. NICE showcases superior performance on FFHQ and ImageNet.

### Analysis of NICE

**Analysis of the stabilizing effect of NICE.** In our 10% CIFAR-10 experiments with OmniGAN(\(d^{}=256\)), Figure 3 provides compelling evidence supporting our theory. Figure 2(a) shows that OmniGAN+AN and OmniGAN+NICE achieve lower weight norms than OmniGAN, validating Theorem

    &  &  \\   & & 100 & 1\(K\) & 2\(K\) & 5\(K\) \\  StyleGAN2 & \(\) & 179 & 100.16 & 54 & 49.68 \\ ADA & ✓ & 85.8 & 21.29 & 15.39 & 10.96 \\ ADA-Linear & ✓ & 82 & 19.86 & 13.01 & 9.39 \\ InsGen & ✓ & 45.75 & 18.21 & 11.47 & 7.83 \\ FaceCLR & ✓ & 42.56 & 15.92 & 9.90 & 7.25 \\ ADA+NICE & ✓ & **38.42** & **14.57** & **8.85** & **6.48** \\   

Table 4: FID \(\) scores on FFHQ using StyleGAN2. ADA-Linear is introduced in .

    &  &  &  &  \\   & & & Obama & Grumpy Cat & Panda & Cat & Dog \\  StyleGAN2  & \(\) & \(\) & 80.20 & 48.90 & 34.27 & 71.71 & 131.90 \\ StyleGAN2+SSGAN-LA  & \(\) & \(\) & 79.88 & 38.42 & 28.6 & 78.78 & 109.91 \\ StyleGAN2+NICE & \(\) & \(\) & **24.56** & **18.78** & **8.92** & **25.25** & **46.56** \\  ADA  & ✓ & \(\) & 45.69 & 26.62 & 12.90 & 40.77 & 56.83 \\ DA  & ✓ & \(\) & 46.87 & 27.08 & 12.06 & 42.44 & 58.85 \\ DigGAN  & ✓ & \(\) & 36.38 & 25.42 & 11.54 & 35.67 & 59.98 \\ LeCam  & ✓ & \(\) & 33.16 & 24.93 & 10.16 & 34.18 & 54.88 \\ GenCo  & ✓ & \(\) & 32.21 & 17.79 & 9.49 & 30.89 & 49.63 \\ InsGen  & ✓ & \(\) & 32.42 & 22.01 & 9.85 & 33.01 & 44.93 \\ MaskedGAN  & ✓ & \(\) & 33.78 & 20.06 & 8.93 & — & — \\ FakeCLR  & ✓ & \(\) & 26.95 & 19.56 & 8.42 & 26.34 & 42.02 \\ TransferGAN \({}^{}\) & ✓ & ✓ & 39.85 & 29.77 & 17.12 & 49.10 & 65.57 \\ LeCam+KDDLGAN \({}^{}\) & ✓ & ✓ & 29.38 & 19.65 & 8.41 & 31.89 & 50.22 \\ ADA+NICE & ✓ & \(\) & **20.09** & **15.63** & **8.18** & **22.70** & **28.65** \\   

Table 3: FID \(\) scores for unconditional image generation with StyleGAN2 on 100-shot Obama/Grumpy cat/Panda and AnimalFace-Cat/Dog datasets. \({}^{}\) indicates using a generator pre-trained on the full FFHQ dataset. \({}^{}\) means using the CLIP  model pre-trained on large scale data.

Figure 2: Images generated using NICE+ADA on StyleGAN2; see Figure 14 in §J for more examples and comparisons for this low-shot generation setting.

    &  &  &  &  \\   & & IS \(\) & FID \(\) & IS \(\) & FID \(\) & IS\(\) & FID \(\) \\  BigGAN & \(\) & 10.94 & 38.30 & 6.13 & 91.16 & 3.92 & 133.80 \\ ADA & ✓ & 12.67 & 31.89 & 9.44 & 43.21 & 8.54 & 56.83 \\ DA & ✓ & 12.76 & 32.82 & 9.63 & 56.75 & 8.17 & 63.49 \\ MaskedGAN & ✓ & 13.34 & 26.51 & 12.85 & 35.70 & 12.68 & 38.62 \\ KDDLGAN & ✓ & 14.14 & 20.32 & 14.06 & 22.35 & **14.65** & 28.79 \\ NICE & \(\) & 14.18 & 21.44 & 13.96 & 24.72 & 13.32 & 31.45 \\ ADA+NICE & ✓ & **14.58** & **18.29** & **14.10** & **20.07** & 13.92 & **24.41** \\   

Table 5: Comparison with and w/o NICE on ImageNet given different percentage of training data.

1 that the multiplicative noise modulation reduces the weight norm of discriminator, thus lowering the Rademacher complexity and improving generalization.

Figure 2(b) shows a gradient surge at the latent layer before the classification head, especially for real images in OmniGAN+AN and OmniGAN+NICE, surpassing gradients of OmniGAN. This observation aligns with our prediction in Prop. 1 that introducing noise amplifies gradients due to the maximization effect on real images when training discriminator and on fake images when training generator, causing undesired gradient issues.

Figure 2(c) illustrates a smaller squared gradient norm \(\| f/ x\|_{2}^{2}\) for OmniGAN+NICE compared to OmniGAN+AN and OmniGAN, showcasing the effectiveness of consistency regularization in penalizing the gradient of discriminator, in line with the theoretical derivation in Theorem 2.

Figure 2(d) demonstrates that OmniGAN+NICE achieves a lower gradient norm of the discriminator loss w.r.t. the input than both OmniGAN and OmniGAN+AN, affirming ability of NICE to counteract the negative effects of large gradient norms caused by noise modulation. The resulted smaller gradients at the input validates efficiency of NICE in stabilizing training.

**Analysis of the ability of NICE to enhance generalization.** Figure 4 visualizes the discriminator outputs for real, fake, and test images along with tFID curves. In contrast to OmniGAN, OmniGAN+AN and OmniGAN+NICE balance discrimination and generalization , maintaining a steady discrepancy between real and fake images. This equilibrium ensures a smaller discrepancy between real seen images and unseen samples, effectively lowering the generation error as outlined in Eq. 3 of Lemma 1. As a result, the generalization of GANs is enhanced, leading to superior performance for image generation, as illustrated by the tFID curves in Figure 3(d).

### Ablation studies

**Ablation of components in NICE.** We examine the impact of various components of NICE: (i) introducing adaptive noise (AN) to the discriminator, (ii) applying consistency regularization to

Figure 4: The discriminator output w.r.t. real, fake and test images of (a) OmniGAN, (b) OmniGAN+AN, (c) OmniGAN+NICE, along with (d) tFID curves on 10% CIFAR-10 using OmniGAN (\(d^{}\!=\!256\)). The shaded region represents the standard deviation. Note we scale the \(y\)-axis in (b) and (c) for visual clarity. In (d), training time is doubled to evaluate the endurance of our methods under prolonged training conditions.

Figure 3: Weight and gradient norms of the discriminator on 10% CIFAR-10 with OmniGAN (\(d^{}\!=\!256\)). (a) total discriminator weight norms, (b) gradient norm at the layer before classification head, (c) gradient norm of \(f\) w.r.t. input, and (d) gradient norm of discriminator loss w.r.t. input.

real images (NICE\({}_{D_{r}}\)), (iii) applying consistency regularization to fake images (NICE\({}_{D_{f}}\)) during discriminator training, and (iv) enforcing consistency on fake images while training the generator (NICE\({}_{G_{f}}\)). Table 6 shows our evaluations across 10% CIFAR-10/100 using OmniGAN (\(d^{}\!=\!256\)) and the Obama on StyleGAN2. The optimal performance is achieved when NICE is applied to both real and fake images in the training of both the discriminator and generator.

**Impact of different factors in NICE.** For an in-depth analysis on how different factors in NICE contribute to its performance, readers are directed to SSE.

**Comparison with alternatives for enhanced generalization and stability.** Table 7 shows that NICE outperforms DA, ADA, AWD (adaptive weight decay), AN+AGP (adaptive noise with adaptive gradient penalization) and NICE\({}_{add}\) (with additive noise). Detailed implementation of these variants can be found in SSD.2. Figure 5 shows that NICE also enhances performance with increasing network size, while ADA and DA exhibit a decline. As augmentation-based methods such as DA, ADA, and AACR, may leak augmentation cues to the generator (Figure 9 of SS1), NICE mitigates this drawback.

**Rationalizing Advantages of NICE.** Refer to SSF and SSG for a comprehensive explanations of why NICE outperforms other alternatives, enhancing generalization and stability. Given a mild increase in the computational load (SSH), substantial performance gains are achieved.

## 5 Conclusions

Our newly proposed approach for GAN training, NoIse-modulated Consistency rEgularization (NICE), improves generalization of various GAN models and their performance under limited data training. Through rigorous theoretical analysis, we have shown that NICE reduces the Rademacher complexity and penalizes the gradient of the latent features within the discriminator. Our experimental results match theoretical findings, illustrating that NICE not only improves generalization but also enhances stability of GAN training. By harnessing these two key advantages, NICE works with various network backbones and consistently outperforms existing methods. These exceptional results firmly establish NICE as a powerful solution for preventing the pervasive issue of discriminator overfitting in GANs. Limitations and border impact of our work are discussed in SSA.