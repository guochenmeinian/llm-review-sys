# PAC-Bayes-Chernoff bounds for unbounded losses

Ioar Casado

Machine Learning Group

Basque Center for Applied Mathematics (BCAM)

icasado@bcamath.org &Luis A. Ortega

Machine Learning Group

Computer Science Dept. - EPS.

Universidad Autonoma de Madrid

luis.ortega@uam.es &Aritz Perez

Machine Learning Group

Basque Center for Applied Mathematics (BCAM)

aperez@bcamath.org &Andres R. Masegosa

Department of Computer Science

Aalborg University

arma@cs.aau.dk

###### Abstract

We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cramer-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cramer transform of the loss. Our approach naturally leverages properties of Cramer-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new _model-dependent_ assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques.

## 1 Introduction

PAC-Bayes theory provides powerful tools to analyze the generalization performance of randomized learning algorithms --for an introduction to the subject see the recent surveys of Guedj (2019); Alquier (2024) and Hellstrom et al. (2023)--. Let \(\) be a model class parametrized by \(^{d}\). With a small abuse of notation, \(\) will represent both a model and its parameters. Instead of learning a single model, we consider the set of probability measures over our class, \(_{1}()\), and aim to find the optimal distribution \(^{*}_{1}()\). This approach is more robust than finding the single best model in \(\), and is tightly related to Bayesian and ensemble methods. The learning algorithm infers this distribution from a sequence of \(n\) training data points \(D=\{_{i}\}_{i=1}^{n}\), which are assumed to be i.i.d. sampled from an unknown distribution \(()\) with support in \(^{k}\).

Given a _loss function_\(:_{+}\), bounding the gap between the _population risk_\(L():=_{}[(,X)]\) and the _empirical risk_\((,D):=_{i=1}^{n}(,_{i})\) of individual models is the standard approach in statistical learning theory. In contrast, PAC-Bayes theory provides high-probability bounds over the _population Gibbs risk_\(_{}[L()]\) in terms of the _empirical Gibbs risk_\(_{}[(,D)]\) and an extra term measuring the dependence of \(\) to the dataset \(D\). This second term involves an information measure --usually the Kullback-Leibler divergence \(KL(|)\)-- between the datadependent _posterior1_\(_{1}()\)and a _prior_\(_{1}()\), chosen before observing the data. These bounds hold simultaneously for every \(_{1}()\), hence minimizing them with respect to \(\) provides an appealing approach to derive new learning algorithms with theoretically sound guarantees.

The foundational papers on PAC-Bayes theory (Shawe-Taylor and Williamson, 1997; McAllester, 1998, 1999; Seeger, 2002) worked with classification problems under bounded losses, usually the zero-one loss. This framework was significantly extended in Catoni (2007), who introduced some of the first bounds for unbounded losses. McAllester's bound (McAllester, 2003) is one of the most representative results for bounded losses: for any \(_{1}()\) independent of \(D\) and every \((0,1)\), we have

\[_{}[L()]_{}[(D,) ]+}{}}{2n}}\,,\] (1)

simultaneously for every \(_{1}()\), where the above inequality holds with probability no less than \(1-\) over the choice of \(D^{n}\). Another significant example under the bounded loss assumption is the Langford-Seeger-Maurer bound --after (Langford and Seeger, 2001; Seeger, 2002; Maurer, 2004)--. Under the same conditions as above,

\[kl_{}[(D,)],_{}[L()]}{}}{n}\,,\] (2)

where \(kl\) is the so-called binary-kl distance, defined as \(kl(a,b):=a+(1-a)\).

These bounds illustrate typical trade-offs in PAC-Bayes theory. In (1), the relation between the empirical and the population risk is easy to interpret because the expected loss is bounded by the empirical loss plus a complexity term. More crucially, the right-hand side of the bound can be directly minimized with respect to the posterior \(\), resulting in the Gibbs posterior (Guedj, 2019). Conversely, (2) is known to be tighter than (1), but it is not straightforward to minimize because it requires inverting \(kl_{}[(D,)],\) --several techniques deal with this issue (Thiemann et al., 2017; Reeb et al., 2018)--.

With the trade-offs among explainability, tightness, and generality in mind, the PAC-Bayes community has come up with novel bounds with applications in virtually every area of machine learning, ranging from the study of particular algorithms --linear regression (Alquier and Lounici, 2011; Germain et al., 2016), matrix factorization (Alquier and Guedj, 2017), kernel PCA (Haddouche et al., 2020), ensembles (Masegosa et al., 2020; Wu et al., 2021; Ortega et al., 2022) or Bayesian inference (Germain et al., 2016; Masegosa, 2020)-- and generic versions of PAC-Bayes theorems (Begin et al., 2016; Rivasplata et al., 2020) to the study of the generalization of deep neural networks (Dziugaite and Roy, 2017; Rivasplata et al., 2019).

Such applications often require relaxing the assumptions of the classical PAC-Bayesian framework, such as data-independent priors, i.i.d. data or bounded losses. Here we are interested in the case of PAC-Bayes bounds for unbounded losses --such as the squared error loss and the log-loss--.

#### PAC-Bayes bounds for unbounded losses

The main challenge of working with unbounded losses is that one needs to deal with an exponential moment term which cannot be easily bounded without specific assumptions about the tails of the loss. The following theorems, fundamental starting points for many works on PAC-Bayes theory over unbounded losses, illustrate this point.

**Theorem 1** ((Alquier et al., 2016; Germain et al., 2016)).: _Let \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\) and any \(>0\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D,)] +[KL(|)+()}{ }],\]

_simultaneously for every \(_{1}()\). Here \(f_{,}():=_{}_{^{n}}[e^{ n \,(L()-(D,))}]\)._This is an _oracle_ bound, because \(f_{,}()\) depends on the data generating distribution \(^{n}\). To obtain empirical bounds from the theorem above, the exponential term \(f_{,}\) is usually bounded by making the appropriate assumptions on the tails of the loss, such as the Hoeffding assumption (Alquier et al., 2016), sub-Gaussianity (Alquier and Guedj, 2018; Xu and Raginsky, 2017), sub-gamma (Germain et al., 2016) or sub-exponential (Catoni, 2004). See Section 5 of Alquier (2024) for an overview.

Many of these assumptions are generalized by the notion that the _cumulant generating function (CGF)_ of the (centered) loss, \(_{}()\), exists and is bounded (Banerjee and Montufar, 2021; Rodriguez-Galvez et al., 2024). Remember that we say \(_{}()\) exists if it is bounded in some interval \([0,b)\) with \(b>0\).

**Definition 2** (Bounded CGF).: A loss function \(\) has bounded CGF if for all \(\), there is a convex and continuously differentiable function \(:[0,b)_{+}\) such that \((0)=^{}(0)=0\) and

\[_{}():=_{}[e^{\,(L()-(,))}]()[0,b).\] (3)

We will say that a loss function \(\) is \(\)-bounded if it satisfies the above assumption under the function \(\). In this setup, Banerjee and Montufar (2021) obtain the following PAC-Bayes bound under the Bounded CGF assumption:

**Theorem 3**.: _Let \(\) be a loss with \(\)-bounded CGF and \(_{1}()\) any prior independent of \(D\). Then, for any \((0,1)\) and any \((0,b)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+}{ n}+,\]

_simultaneously for every \(_{1}()\)._

Theorems 1 and 3 illustrate a pervasive problem of many PAC-Bayes bounds for unbounded losses: they often depend on a free parameter \(>0\) --see for example (Alquier et al., 2016; Hellstrom and Durisi, 2020; Guedj and Pujol, 2021; Banerjee and Montufar, 2021; Haddouche et al., 2021)--. The choice of this free parameter is crucial for the tightness of the bounds, and it cannot be directly optimized because its choice is prior to the draw of data, while the optimal \(\) would be data-dependent. The standard approach is to optimize \(\) over a grid using union-bound arguments, but the resulting \(\) is not guaranteed to be optimal. Seldin et al. (2012) carefully design the grid so that the optimal \(\) is inside its range, but their work only deals with bounded losses. See the discussion in Section 2.1.4 of Alquier (2024) for an overview.

Recently, Rodriguez-Galvez et al. (2024) improved the union-bound approach for the bounded CGF scenario using a convenient partition of the event space. However, their optimization remains approximate. Hellstrom and Durisi (2021) could circumvent this problem for the particular case of sub-Gaussian losses, but the _exact_ optimization of \(\) for the general case remains an open question. A positive result in this direction would lift the restriction of having to optimize \(\) over restricted grids or using more complex approaches.

The use of the bounded CGF assumption entails another potential drawback: both uniform control of the CGF --\(_{}()()\) for every \(\)-- and integrability assumptions as in Alquier et al. (2016) --\(_{}[_{}()] C\) for some constant \(C\)--, necessarily drop information about the different concentration properties of individual models. For example, Masegosa and Ortega (2023) show that within the class of models defined by the weights of common neural networks, the behavior of their CGFs --and hence of their Cramer transforms, which control generalization error via Cramer-Chernoff bound-- varies significantly. As Figure 1 shows, uniformly bounding the CGFs can result in loose _worst-case_ bounds that ignore the properties of the models that interest us most.

The bounds of Seldin et al. (2012) and Haddouche et al. (2021) partially address this issue including average variances and model-dependent range functions to their bounds, which account for the fact that different models have different CGFs. However, the former only applies to bounded losses, while the latter cannot be exploited to obtain better posteriors because their model-dependent function only impacts the prior. Outside the PAC-Bayes framework, Jiao et al. (2017) generalized the bounded CGF assumption, but their result only applies to finite model sets.

Exploiting these differences among models within the same class could be an important line of research in PAC-Bayes theory. This approach would provide more informative generalization bounds, paving also the way for the design of better posteriors. This is one of the main appeals of our work.

### Overview and Contributions

The main contribution of this paper is Theorem 7, a novel (oracle) PAC-Bayes bound for unbounded losses which extends the classic Cramer-Chernoff bound to the PAC-Bayesian setting. The theorem is introduced in Section 3, while Section 2 contains the necessary prerequisites. As far as we know, the proof technique based on Lemma 6 is also novel and can be of independent interest.

We discuss the applications of our main theorem in sections 4 and 5. First, we show that our bound allows _exact_ optimization of the free parameter \(\) incurring in a \( n\) penalty without resorting to union-bound approaches. In the case of bounded CGFs, we recover bounds which are optimal up to that logarithmic term. We also show that versions of many well-known bounds --such as the Langford-Seeger bound-- can be recovered from Theorem 7.

In Section 5, we show how our main theorem provides a general framework to deal with richer assumptions that result in novel, more informative, and potentially tighter bounds. In Theorem 11, we generalize the bounded CGF assumption so that there is a different bounding function, \((,)\), for the CGF of each model. We illustrate this idea in three cases: generalized sub-Gaussian losses, norm-based regularization, and input-gradient regularization based on log-Sobolev inequalities. Remarkably, the bounds in Section 5 can be minimized with respect to \(_{1}()\), resulting in optimal posteriors beyond Gibbs' and opening the door to the design of novel algorithms.

Appendix A contains the proofs not included in the paper.

## 2 Preliminaries

In this section we introduce the necessary prerequisites in order to prove our main theorem. Its proof relies in controlling the concentration properties of each model in \(\) using their Cramer transform.

**Definition 4**.: Let \(I\) be an interval and \(f:I\) a convex function. The _Legendre transform_ of \(f\) is defined as

\[f^{}(a):=_{ I}\ \{ a-f()\}, a \,.\] (4)

Following this definition, the Legendre transform of the CGF of a model \(\) is known as its _Cramer transform_:

\[^{}_{}(a):=_{[0,b)}\ \{ a-_{}()\}, a\,.\] (5)

Throughout the paper, we will use the abbreviation \(gen(,D)\) to denote the _generalization gap_\(L()-(D,)\), in order to make the notation more compact. Cramer's transform provides non

Figure 1: **Models with very different CGFs coexist within the same model class**. On the left, we display several metrics for InceptionV3 models trained on CIFAR10 without regularization (**Standard**) and with L2 regularization (**L2**). **Random** refers to a model learned over randomly reshuffled labels and **Zero** refers to a model where all the weights are equal to zero. For each model, the metrics include train and test accuracy, test log-loss, \(_{2}\)-norm of the parameters of the model, the variance of the log-loss function, denoted \(_{}((,))\), and the expected norm of the input-gradients, denoted \(_{}[\|_{x}(,)\|_{2}^{2}]\). On the right, we display the estimated CGFs of each model, following Masegosa and Ortega (2023). Note how models with smaller variance \(((,))\), \(_{2}\)-norm or input-gradient norm \(_{}[\|_{x}(,)\|_{2}^{2}]\) have smaller CGFs. Bounds derived from Theorem 7 naturally exploit these differences. Experimental details in Appendix C.

asymptotic bounds on the right tail of \(gen(,D)\) via Cramer-Chernoff's theorem --see Section 2.2 of Boucheron et al. (2013) or Section 2.2 of Dembo and Zeitouni (2009)--.

**Theorem 5** (Cramer-Chernoff).: _For any \(\) and \(a\),_

\[_{^{n}}gen(,D) a e^{-n^{ }_{}(a)}.\] (6)

_Furthermore, the inequality is asymptotically tight up to exponential factors._

Importantly, Cramer-Chernoff's bound can be inverted to establish high-probability generalization bounds: for any \((0,1)\),

\[_{^{n}}gen(,D)(^{}_{})^{-1} 1-.\] (7)

where \((^{}_{})^{-1}\) is the inverse of the Cramer transform. We cannot directly use Theorem 5 to obtain PAC-Bayes bounds, because we need a bound which is uniform for every model. The following lemma will be our main technical tool for this purpose.

**Lemma 6**.: _For any \(\) and \(c 0\), we have_

\[_{^{n}}n^{}_{}(gen(,D))  c_{X}X c.\]

This way of controlling the survival function of \(^{}_{}(gen(,D))\) for every \(\) will allow us to bound an exponential moment term in our main theorem.

## 3 PAC-Bayes-Chernoff bound

As we hinted above, instead of directly introducing boundedness conditions on the loss or its CGF, we stay in the realm of oracle bounds, aiming to provide a flexible starting point for diverse applications. A key element of our theoretical approach is the averaging of the CGFs with respect to a posterior distribution. For any posterior distribution \(_{1}()\), we may consider the expectation of the CGF, \(_{}[_{}()]\), as in Jiao et al. (2017). In analogy to the standard definition, we define the _Cramer transform of a posterior distribution_\(\) as the following function:

\[^{}_{}(a):=_{[0,b)}\ \{ a-_{}[ _{}()]\}, a\,.\] (8)

Since the CGFs \(_{}()\) are convex and continuously differentiable with respect to \(\), their expectation \(_{}[_{}()]\) retains the same properties. Hence according to Lemma 2.4 in Boucheron et al. (2013), the (generalized) inverse of \(^{}_{}\) exists and can be written as

\[(^{}_{})^{-1}(s)=_{[0,b)}\ \{_{}[_{}()]}{} \}.\] (9)

With these definitions in hand, we are ready to introduce our main result, a novel (oracle) PAC-Bayes bound for unbounded losses:

**Theorem 7** (PAC-Bayes-Chernoff bound).: _Let \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D, )]+_{[0,b)}\{}{ (n-1)}+_{}[_{}()]}{ }\}\]

_simultaneously for every \(_{1}()\)._

The above result gives an oracle PAC-Bayes analogue to the Cramer-Chernoff's bound of equation (7), because the second term of the right hand side of the inequality corresponds with the inverse of \(^{}_{}\), as described in equation (9). Theorem 7 shows that bounds like the one given in Theorem 3 can hold simultaneously for every \(>0\) --and hence optimized with respect to \(\)-- by paying a \((n)\) penalty, virtually the same as if we were optimizing over a uniform grid of size \(n\) using union-bound arguments. Although the dependence in union bound arguments can be improved to \(((n))\)(Alquier, 2024), our optimization is exact.

Furthermore, Theorem 7 also shows that the posterior minimizing its right-hand-side is involved in a three-way trade-off: firstly, \(\) must explain the training data due to \(_{}[(D,)]\); secondly, it must be close to the prior due to the KL term \(KL(|)\); and lastly, it must place its density in models with a lower CGF due to the \(_{}[_{}()]\) term. We note that the first two elements are standard on most PAC-Bayesian bounds, but the role of the CGF in defining an optimal posterior \(\) is novel compared to previous bounds. We explore the implications of this fact in Section 5.

## 4 Relation with previous bounds

We first relate Theorem 7 to previous bounds. As a first application, we show how some well-known PAC-Bayes bounds can be recovered from ours. When the distribution of the loss is known, we can often compute its Cramer transform. This happens to be the case with the zero-one loss, where we recover Langford-Seeger's bound (Seeger, 2002, Theorem 1).

**Corollary 8**.: _Let \(\) be the \(0-1\) loss and \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[kl(_{}[(,D)],_{}[L()])}{n-1},\]

_simultaneously for every \(_{1}()\)._

The dependence on \(n\) in Corollary 8 was further improved by Maurer (2004). However, our version is enough to illustrate the role played by Cramer transforms in obtaining tight PAC-Bayes bounds, which is a recent line of work by Foong et al. (2021) and Hellstrom and Guedj (2024).

When the loss is of bounded CGF --recall Definition 2--, Theorem 7 results in a generalization of Theorem 3, where the bound holds simultaneously for every \((0,b)\) with a \( n\) penalty.

**Corollary 9**.: _Let \(\) be a loss function with \(\)-bounded CGF. Let \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D, )]+_{[0,b)}\{}{ (n-1)}+\},\]

_simultaneously for every \(_{1}()\)._

Proof.: Directly follows from the definition of \(\)-bounded CGF and Theorem 7. 

Corollary 9 is the first PAC-Bayes bound that allows exact optimization of the free parameter \( 0\) for the general case of losses with bounded CGF without resorting to union-bound approaches (Seldin et al., 2012; Rodriguez-Galvez et al., 2024), which cannot guarantee an exact minimization. Although in the particular case of sub-Gaussian losses the \( n\) penalty is worse than that in Hellstrom and Durisi (2021), when \(KL(|)}{}-1\), Corollary 9 is tighter than the general Theorem 14 in Rodriguez-Galvez et al. (2024). We instantiate Corollary 9 in the case of sub-Gaussian and sub-gamma losses in the Appendix B.

## 5 PAC-Bayes bounds under model-dependent assumptions

As discussed in the introduction, most boundedness conditions used to bound the exponential moment term in Theorem 7 discard information about the statistical properties of individual models. In this section, we show that the structure of Theorem 7 allows for a more fine-grained control of the CGFs, resulting in potentially tighter and more informative bounds. We start by generalizing Definition 2.

**Definition 10** (Model-dependent bounded CGF).: A loss function \(\) has model-dependent bounded CGF if for each \(\), there is a convex and continuously differentiable function \((,)\) such that \((,0)=^{}(,0)=0\) and for any \([0,b)\),

\[_{}():=_{}[e^{(L( )-(,))}](, )\,.\] (10)As motivated in Figure 1 and in opposition to Definition 2, this new condition acknowledges the possibility of having different bounding functions, \((,)\), for each \(_{}()\).

Using this definition, we can easily exploit Theorem 7 to derive the following bound:

**Theorem 11**.: _Let \(\) be a loss function satisfying Definition 10. Let \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+_{[0,b)}\{}{ (n-1)}+_{}[(,)]}{} \},\] (11)

_simultaneously for every \(_{1}()\)._

Proof.: The proof is analogue to that of Corollary 9. 

This theorem can be understood as a PAC-Bayesian version of Theorem 2 in Jiao et al. (2017), where we allow infinite model classes. Note that if we tried to exploit this model-dependent CGF assumption on other oracle bounds, as the one shown in Theorem 1, we would end with an empirical bound where the \((,)\) term would be _exponentially averaged_ by the prior, \(_{}[e^{(,)}]\), instead of having the more amenable term \(_{}[(,)]\), which directly impacts on the choice of the optimal posterior \(^{*}\).

As discussed in Section 3, the posterior distribution in Theorem 11 is involved in a three-way trade-off which has the potential to result in tighter bounds and the design of better posteriors. It is worth noting that the posterior minimizing the bound in Theorem 11 is not the standard Gibbs posterior.

**Proposition 12**.: _If we fix some \(>0\), the bound in Theorem 11 can be minimized with respect to \(_{1}()\). The optimal posterior is_

\[^{*}()()\{-(n-1)(D, )-(n-1)(,)\}.\] (12)

Observe that under the posterior in Proposition 12, the _maximum a posteriori_ (MAP) estimate is

\[_{}=*{arg\,min}_{} \{(D,)+(,)- ()\}.\]

As we will illustrate below, the extra term, \((,)\), can often be understood as a regularizer. In what remains, we exemplify the general recipe of Theorem 11 in several cases. To start providing concrete intuition, we instantiate Corollary 11 in the case of sub-Gaussian losses.

#### Generalized sub-Gaussian losses

It is well known that if \(X\) is a \(^{2}\)-sub-Gaussian random variable, we have \((X)^{2}\)(Arbel et al., 2020). In many cases, it might not be reasonable to bound \(_{}((,X))^{2}\) for every \(\), because the variance of the loss function highly depends on the particular model, as illustrated in Figure 1. This is where Corollary 11 comes into play: we may assume that \((,X)\) is \(()^{2}\)-sub-Gaussian for each \(\). In this case the variance proxy \(()^{2}\) is specific for each model, leading to the following bound:

**Corollary 13**.: _Assume the loss \((,X)\) is \(^{2}()\)-sub-Gaussian. Let \(_{1}()\) be any prior independent of \(D\). Then, for any \((0,1)\), with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+_{}[()^{2}]}{n-1}},\] (13)

_simultaneously for every \(_{1}()\)._

Proof.: Use Theorem 11 and the fact that \((,)=^{2}()}{2}\). Then optimize \(\). 

This result generalizes sub-Gaussian PAC-Bayes bounds --Corollary 2 in Hellstrom and Durisi (2021) or Corollary 19-- and shows that posteriors favoring models with small variance-proxy, \(^{2}()\), generalize better. It is, therefore, potentially tighter than previous results, because the \(^{2}\)factor in standard sub-Gaussian bounds is a _worst-case constant_, while Corollary 13 exploits the fact that some models have much smaller variance-proxy than others.

Analogous bounds can be straightforwardly derived for generalized sub-gamma or sub-exponential losses, but Theorem 11 is not limited to explicit tail assumptions on the loss. The following subsections explore model-dependent assumptions on \(_{}()\) that result in novel PAC-Bayes bounds involving well-known regularization techniques.

### L2 regularization

We now introduce bounds based on the norm of the model parameters. For that purpose we use the following standard assumption in machine learning (Li and Orabona, 2019).

_Assumption 1_ (Parameter Lipschitz).: The loss function \((,)\) is \(M\)-Lipschitz with respect to \(\), that is, for any \(\) and any \(\) we have \(\|_{}(,)\|_{2}^{2} M\).

If the model class is parametrized in such a way that the model with null parameter vector has null variance --i.e., \(_{}((,0))=0\), which is the case of a neural net with null weights--, then, as shown in Masegosa and Ortega (2023), we can derive the following model-dependent bound for the CGF:

\[_{}() 2M^{2}\|\|_{2}^{2}.\] (14)

This case further illustrates the idea we motivated in the introduction: bounding the CGFs with model-dependent proxies for generalization. Figure 1 illustrates how models with smaller norm has increasingly smaller CGFs.

Using Equation (14) we obtain the following generalization bound penalizing model's L2-norm:

**Corollary 14**.: _If Assumption 1 holds, then for any prior distribution \(_{1}()\) independent of \(D\) and any \((0,1)\), with probability at least \(1-\) over draws \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+_{}[\|\|_{2}^{2}]}{n-1}},\] (15)

_simultaneously for every \(_{1}()\)._

Proof.: The result follows from using \((,)=2M^{2}\|\|_{2}^{2}\) in Theorem 11 and optimizing \(\). 

Corollary 14 shows that models with smaller parameter norms generalize better, and provides PAC-Bayesian certificates for norm-based regularization. Many other PAC-Bayesian bounds contains different kind of parameter norms (Germain et al., 2009, 2016; Neyshabur et al., 2017), but their parameter norm term is always introduced through the prior --e.g., using a zero-centered Gaussian prior distribution--. The novelty here is that this parameter norm term appears independently of the prior as a result of Theorem 11.

According to Proposition 12, the MAP estimate of the optimal posterior is, \(_{}=(D,)+\|\|_{2}^{2}-()}\), which is the result of L2-regularization with tradeoff parameter \(\).

### Gradient-based regularization

We finish this section providing novel bounds based on log-Sobolev inequalities that include a gradient term penalizing the sensitivity of models to small changes in the input data. First, let us simplify the notation for this section. We use \(\|_{}\|_{2}^{2}:=_{}\|_{}(,)\|_{2}^{2}\) and \(\|_{}\|_{2}^{2}:=_{i=1}^{n}\|_{ }(_{i},)\|_{2}^{2}\) to denote the expected and empirical (squared) gradient norms of the loss.

Including a gradient-based penalization is the idea behind input-gradient regularization (Varga et al., 2017), which minimizes an objective function of the form \((D,)+\|_{}\|_{2}^{2}\), where \(k>0\) is a trade-off parameter. This approach is often used to make models more robust against disturbances in input data and adversarial attacks (Ross and Doshi-Velez, 2018; Finlay and Oberman, 2021).

We make the connection between PAC-Bayes bounds and gradient norms assuming that \(\) and \(\) satisfy certain log-Sobolev inequality (Chafai, 2004).

_Assumption 2_ (log-Sobolev).: For any \(\) and any \(>0\), we have

\[_{}()^{2}\|_{}\|_ {2}^{2},\ C>0.\]

For example, Assumption 2 holds when \(\) is strictly uniformly log-concave, as shown in Corollary 2.1 of Chafai (2004) --this case includes the Gaussian and Weibull densities (Saumard and Wellner, 2014)--. We also try to empirically verify Assumption 2 for certain class of neural networks in Appendix C.2. Assumption 2 is going to be our model-dependent bound on the CGF. We first provide a bound for expected gradients.

**Theorem 15**.: _If Assumption 2 is satisfied, then for any prior distribution \(_{1}()\) independent of \(D\) and any \(_{1}(0,1)\), with probability at least \(1-_{1}\) over draws \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+_{}[\|_{}\|_{2}^{2}]}}{n-1}}\] (16)

_simultaneously for every \(_{1}()\)._

Proof.: Follows from the application Assumption 2 to Theorem 11 and the optimization of \(\). 

This bound shows that --under certain regularity conditions-- posteriors favoring models with smaller expected gradients, \(\|_{}\|_{2}^{2}\), generalize better, which is the heuristic behind input-gradient regularization (Varga et al., 2017). However, Theorem 15 is still an oracle bound. We can obtain a new, fully empirical one if we concatenate Corollary 15 with a PAC-Bayes concentration bound for \(_{}[\|_{}\|_{2}^{2}]\). This can be done if we assume that the loss is Lipschitz w.r.t. the input-data.

_Assumption 3_ (Input-data Lipschitz).: For any \(\) and for any \(\), we have \(\|_{}(,)\|_{2}^{2} L\).

Assumption 3 is satisfied in standard deep neural network architectures, and the Lipschitz constant can be efficiently estimated (Virmaux and Scaman, 2018; Fazlyab et al., 2019).

**Theorem 16**.: _If Assumptions 2 and 3 are satisfied, then for any prior distribution \(_{1}()\) independent of \(D\) and any \((0,1)\); with probability at least \(1-\) over draws of \(D^{n}\),_

\[_{}[L()]_{}[(D,) ]+_{}[\|_{}\|_{2}^{2} ] K(,,n,)+CL K(,,n,)^{}}\]

_simultaneously for every \(_{1}()\), where \(K(,,n,):=}{n-1}\)._

With minimal modifications, the result above also holds under the assumption that on-average gradients are bounded, \(\|_{}\|_{2}^{2} g\), or when \(\|_{}(,X)\|_{2}^{2}\) are sub-Gaussian.

Similar bounds with input-gradients were first introduced by Gat et al. (2022) under the assumption that the underlying distribution of data was a mixture of Gaussians plus a technical "per-label loss balance" assumption. However, Theorem 16 is, as far as we know, the first empirical PAC-Bayes bound for input-gradients. Furthermore, in contrast with Gat et al. (2022), our bounds optimize the free parameter \(\) and explicitly relate the gradients with the generalization ability of the posterior \(_{1}()\). The recent work of Haddouche et al. (2024) also includes gradient terms in their bounds, but they are gradients with respect to the model parameter, not input-gradients.

As for the optimal posterior \(^{*}\), if we minimize the bound in Theorem 15 for a fixed \(>0\), the MAP estimate in Proposition 12 is \(_{}=_{}\{(D,)+ \|_{}\|_{2}^{2}- ()\},\) which is the result of input-gradient regularization with trade-off parameter \(\).

In conclusion, the approach suggested by Theorem 11 not only provides novel insights --in the form of tight bounds or PAC-Bayesian interpretations of previously known algorithms--, it also hints towards the design of new regularized learning algorithms with solid theoretical guarantees.

## 6 Conclusion

We derived a novel PAC-Bayes oracle bound using basic properties of the Cramer transform -- Theorem 7--. In general, our work aligns with very recent literature (Rodriguez-Galvez et al.,2024; Hellstrom et al., 2023) that highlights the importance of Cramer transforms in the quest for tight PAC-Bayes bounds. This bound has the potential to be a stepping stone in the development of novel, tighter empirical PAC-Bayes bounds for unbounded losses. Firstly, because it allows exact optimization of the free parameter \(>0\) without the need for more convoluted union-bound approaches. But, more relevantly, because it allows the introduction of flexible, fine-grained, model-dependent assumptions for bounding the CGF --Theorem 11-- which results in optimal distributions beyond Gibbs' posterior. The importance and wide applicability of this result have been illustrated with three model-dependent assumptions: generalized sub-Gaussian losses, bounds based on parameter norms, and input-gradients based on log-Sobolev inequalities. In the last case we introduce PAC-Bayes bounds including empirical input-gradients norms.

#### Limitations and future work

A limitation of our approach is that the we are implicitly assuming that \((,X)\) is light-tailed (equivalently, sub-exponential), as in every Cramer-Chernoff bound. This is only partially true. Although there are specific studies for heavy-tailed losses (Alquier and Guedj, 2018; Holland, 2019; Haddouche and Guedj, 2023; Chugg et al., 2023), we avoid this limitation because we are only interested in the right tail of \(gen(,D)\) --that is, \(_{}(L()-(D,) a)\) for \(a 0\)--. This is done by defining the CGF only for \( 0\). In this way, as we show in Remark 17, the finiteness of \(_{}[L()]\) guarantees the existence of \(_{}[_{}()]\) in Theorem 7. This approach is motivated by the fact that most state-of-the-art models lie in the interpolating regime, hence \(_{}((D,)-L() a)\) for \(a 0\) has less practical importance. However, in the case where one is looking for two-tailed bounds, our work is restricted to sub-exponential losses. See the discussion on Section 4.2.1 of Zhang et al. (2024).

Given the generality of Theorem 7, the dependence of the logarithmic penalty term in the bound may be suboptimal in certain cases, as discussed in Section 4. Going beyond Lemma 6 and improving this dependence is one of the main open tasks.

Our results provide a systematic approach for integrating general model-dependent CGF bounds into PAC-Bayes theory, and can open exciting new research lines: they can provide a PAC-Bayesian extension of the work of Masegosa and Ortega (2023), which studies the effects of different regularization techniques and the role of invariant architectures, data augmentation and over-parametrization in generalization. Since our bounds can be applied to unbounded losses such as the log-loss, it could be interesting to apply them in a Bayesian setting (Germain et al., 2016) in order to study the relation between marginal likelihood and generalization (Lotfi et al., 2022).