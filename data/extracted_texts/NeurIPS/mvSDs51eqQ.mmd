# Optimality in Mean Estimation:

**Beyond Worst-Case, Beyond Sub-Gaussian,**

**and Beyond \(1+\) Moments**

**Trung Dang**

The University of Texas at Austin

dddttrung@cs.utexas.edu

&**Jasper C.H. Lee**

University of Wisconsin-Madison

jasper.lee@wisc.edu

&**Maoyuan Song**

Purdue University

maoyuanrs@gmail.com

&**Paul Valiant**

Purdue University

pvaliant@gmail.com

###### Abstract

There is growing interest in improving our algorithmic understanding of fundamental statistical problems such as mean estimation, driven by the goal of understanding the fundamental limits of what we can extract from limited and valuable data. The state of the art results for mean estimation in \(\) are 1) the optimal sub-Gaussian mean estimator by , attaining the optimal sub-Gaussian error constant for all distributions with finite but unknown variance, and 2) the analysis of the median-of-means algorithm by  and a matching lower bound by , characterizing the big-O optimal errors for distributions that have tails heavy enough that only a \(1+\) moment exists for some \((0,1)\). Both of these results, however, are optimal only in the worst case. Motivated by the recent effort in the community to go "beyond the worst-case analysis" of algorithms, we initiate the fine-grained study of the mean estimation problem: Is it possible for algorithms to leverage _beneficial_ features/quirks of their input distribution to _beat_ the sub-Gaussian rate, without explicit knowledge of these features?

We resolve this question, finding an unexpectedly nuanced answer: "Yes in limited regimes, but in general no". Given a distribution \(p\), assuming _only_ that it has a finite mean and absent any additional assumptions, we show how to construct a distribution \(q_{n,}\) such that the means of \(p\) and \(q\) are well-separated, yet \(p\) and \(q\) are impossible to distinguish with \(n\) samples with probability \(1-\), and \(q\) further preserves the finiteness of moments of \(p\). Moreover, the variance of \(q\) is at most twice the variance of \(p\) if it exists. The main consequence of our result is that, no reasonable estimator can asymptotically achieve better than the sub-Gaussian error rate for any distribution, up to constant factors, which matches the worst-case result of . More generally, we introduce a new definitional framework to analyze the fine-grained optimality of algorithms, which we call "neighborhood optimality", interpolating between the unattainably strong "instance optimality" and the trivially weak admissibility/Pareto optimality definitions. As an application of the new framework, we show that the median-of-means algorithm is neighborhood optimal, up to constant factors. It is an open question to find a neighborhood-optimal estimator _without_ constant factor slackness.

Introduction

Mean estimation over \(\) is one of the most fundamental problems in statistics: given \(n\) i.i.d. samples from some unknown distribution \(p\), how do we most accurately estimate the mean of \(p\), with probability \( 1-\), from the \(n\) samples? The conventional approach is to take the sample mean, the empirical average of the samples, as the estimate; this is justified by the law of large numbers, which says that that in the limit of having infinitely many samples, the sample mean will converge to the true mean. However, it has been long known that while the sample mean is optimal for estimating the mean of well-behaved distributions such as Gaussians, it is sensitive to the presence of _outliers_ in samples drawn from heavy-tailed distributions, for which the sample mean estimator can have abysmal performance.

The classic median-of-means estimator (e.g., NY83; JVV86; AMS99), independently invented several times in the literature, was proposed to mitigate the sensitivity of the sample mean. Its accuracy on any distribution \(p\) with finite variance is--up to constant factors--as good as the accuracy of the sample mean on a Gaussian with the same mean and variance as \(p\). In other words, the error is of _sub-Gaussian rate_. The past decade has seen renewed interest across computer science and statistics in understanding the limits of the mean estimation problem. In one dimension, the current state of the art is 1) for distributions \(p\) with finite variance \(_{p}^{2}\), the recent mean estimator proposed by Lee and Valiant  has sub-Gaussian rate \(_{p}(+o(1))/n}\), which is tight even in its constants, up to a \(1+o(1)\) factor, and 2) the work of Bubeck, Cesa-Bianchi and Lugosi  showing upper bounds matching the lower bounds of Devroye, Lerasale, Lugosi and Oliveira , which show that for heavy-tailed distributions having only a \(1+^{}\) moment for some \((0,1)\), the median-of-means algorithm in fact still achieves the optimal accuracy up to a constant factor. Both of these results, however, are optimal only in the worst case, meaning that  have optimal performance with respect to the class of distributions with the same \(2^{}\) or \(1+^{}\) moments respectively; but estimators may do better than these bounds on particular input distributions.

The natural and immediate next question is, even though Gaussian distributions are the hardest case in mean estimation, and thus sub-Gaussian performance is worst-case optimal: can one do better, at least for _some_ "easier" distributions? Can we develop "instance-dependent" algorithms and analysis techniques? Is it possible for algorithms to leverage _beneficial_ features/quirks of their input distribution to _beat_ the sub-Gaussian rate, without explicit knowledge of these features and without losing robustness to heavy-tails?

We resolve this question and show an unexpectedly nuanced answer: "Yes in limited regimes, but in general no". For some distributions, even median-of-means can beat the sub-Gaussian bound, but only for a limited parameter regime per distribution--namely, if the number of samples is not too large (Proposition 14). In general however, we show a strong and comprehensive negative result. Our main technical result (Theorem 2) is a fine-grained indistinguishability construction: given a distribution \(p\), assuming _only_ that \(p\) has a finite mean and absent any further assumptions, we show how to construct a distribution \(q_{n,}\)--in terms of a sample complexity \(n\) and a failure probability \(\)--such that \(p\) and \(q\) are impossible to distinguish with \(n\) samples, with probability \( 1-\), and yet the means of \(p\) and \(q\) are well-separated by some function \(_{n,}(p)\) (stated formally in Definition 1). This in particular implies that no \(n\)-sample estimator with failure rate \(\) can simultaneously have error less than \(_{n,}(p)\) on both \(p\) and \(q\). The function \(_{n,}(p)\) is such that, for \(p\) with a finite variance, if we take \(/n 0\), we have \(_{n,}(p)(_{p}/n})\), showing that no estimator can asymptotically outperform the sub-Gaussian rate for both \(p\) and \(q_{n,}\) simultaneously. Additionally, as shown in Section 2.1, the construction of \(q_{n,}\) is conservative, such that \(q}{p} 2\) at all points, meaning that \(q\) has a finite \(1+^{}\) moment whenever \(p\) does, and furthermore, \(_{q}^{2} 2_{p}^{2}\) whenever \(_{p}^{2}\) exists. Thus, the same indistinguishability result still applies even if we further require the existence of higher moments for both \(p\) and \(q\).

The key message of our paper is that such lower bounds are to be _circumvented_, through identifying additional favorable distribution structure for the mean estimation problem.1 This observation has already led to further work in the area, guiding the design of a new algorithm that outperformsthe sub-Gaussian rate via a symmetry assumption. Gupta et al.  show that, assuming the distribution is _symmetric_ about its mean, one can achieve finite-sample and instance-dependent _Fisher information_ rates for mean estimation, which can be significantly better than sub-Gaussian rates. We view our paper and the  result together as a **call to arms** to explore other structural assumptions that can sidestep our lower bound construction.

Beyond the asymptotic implications in the finite variance setting, our results fully characterize mean estimation in the regimes of \(1\)) finite variance and finite samples, and \(2\)) infinite variance, and indeed, even when no \(1+^{}\) moment exists for any constant \(>0\). In particular, we give a simple, yet very general re-analysis of the median-of-means estimator on distributions only assumed to have a finite mean. Its estimation error on distribution \(p\), with probability \(1-\) over \(n\) samples, is \(O(_{n,}(p))\) (Proposition 14), matching our main indistinguishability result up to constants (Theorem 2).

### Our Results and Techniques

Our main result is an indistinguishability result, for every distribution \(p\), which serves as a mean estimation lower bound. Ideally, given the motivation earlier in the introduction, we want to show that the sub-Gaussian rate is a lower bound, but such a bound cannot hold in finite samples. To see this, consider a distribution which has \( 1/n\) mass that is extremely far away from the mean, and which contributes the majority of the variance, yet only a minuscule portion of the mean. Given only \(n\) samples, with high probability we will not see any samples from this mass, and so mean estimation can in fact be "effectively" performed on the conditional distribution without this outlier mass. The conditional distribution has essentially the same mean as the original, yet has far smaller variance, thus allowing us to _beat_ the (variance-dependent) sub-Gaussian rate. We thus start this section by defining an error function \(_{n,}(p)\) for each distribution \(p\) (Definition 1), capturing the estimation error we expect for \(p\), taking into account that we intuitively expect both algorithms and lower bounds to ignore outlier mass. Our main result will then construct, for every distribution \(p\), a new distribution \(q\) that is indistinguishable from \(p\) using \(n\) samples, yet has mean difference at least \(_{n,}(p)\) from \(p\).

**Definition 1**: _Given a (continuous) distribution \(p\) with mean \(_{p}\) and a real number \(t\), define the \(t\)-trimming operation on \(p\) as follows: select a radius \(r\) such that the probability mass in \([_{p}-r,_{p}+r]\) equals \(1-t\); then, return the distribution \(p\)**conditioned** on lying in \([_{p}-r,_{p}+r]\)._

_Given \(n\) and \(\), we define a standard trimmed distribution \(p^{*}_{n,}\) to be the \(\,\)-trimmed version of \(p\). When \(\) is implicit, we may denote this as \(p^{*}_{n}\). We also define the error function \(_{n,}(p)=|_{p}-_{p^{*}_{n}}|+_{p^{*}_{n}}\,}{n}}\)._

Theorem 2 shows that, given any distribution \(p\) with a finite mean, it is possible to construct a distribution \(q_{n,}\) such that 1) \(p\) and \(q\) are indistinguishable under \(n\) samples with probability \(1-\), yet 2) the means of \(p\) and \(q\) are separated by \((_{n,}(p))\). The construction of \(q_{n,}\) is also such that \(q/p 2\), showing that, in many senses, "\(q\) does not have more extreme tails than \(p\)".

**Theorem 2**: _Let \(n\) be the sample complexity and \(\) be the failure probability, and recall the definition of the error function \(_{n,}\) from Definition 1. Assume that there is a sufficiently small constant which upper bounds both \(\,}{n}\) and \(\). Then for any distribution \(p\) with a finite mean \(_{p}\), there exists a distribution \(q p\) with mean \(_{q}\) such that:_

* \(|_{q}-_{p}|_{n,}(p)\)__
* \((1-d_{}^{2}(p,q)) 4\)__
* \(q}{p} 2\)_._

_In particular, by a standard fact (Fact 1) on the squared Hellinger distance, this implies that \(p\) and \(q\) are indistinguishable using \(n\) samples, with probability \(1-\). Furthermore, since \(q}{p} 2\), we have \(_{q}^{2}_{q}[(X-_{q})^{2}]_{q}[(X-_{p} )^{2}] 2\,_{p}[(X-_{p})^{2}]=2_{p}^{2}\)._

Using a standard testing-to-estimation reduction, it follows from the main theorem that there can be no estimator that can achieve error at most \(_{n,}(p)\) simultaneously on \(p\) and \(q_{n,}\).

**Corollary 3**: _Let \(n\) be the sample complexity and \(\) be the failure probability, and recall the definition of the error function \(_{n,}\) from Definition 1. Given a distribution \(p\) with finite mean, consider the construction of \(q\) in Theorem 2. Then, there is no estimator that achieves error less than \(_{n,}(p)\) over \(n\) samples with probability \(1-\), for both \(p\) and \(q\)._

We contrast this lower bound with more standard impossibility results that have the flavor: "one cannot estimate the mean to within \((+o(1))/n}\), since there are a pair of Gaussian distributions of variance \(\), separated by twice this distance, that are indistinguishable in \(n\) samples up to probability \(1-\)". As opposed to showing the indistinguishability of "nice" distributions like Gaussians, we instead show that for _any_ distribution \(p\) of interest (with finite mean), we exhibit a generic construction of a hard-to-distinguish "partner" distribution \(q\), of rather different mean, yet all of whose tails are comparable to those of \(p\).

For reference, the definition of Hellinger distance and the standard fact we reference above are:

**Fact 1**: _Consider the squared Hellinger distance between two distributions \(p\) and \(q\), defined as_

\[d_{}^{2}(p,q)=(p}-q} )^{2}\]

_If the two distributions \(p\) and \(q\) are such that \((1-d_{}^{2}(p,q)) 4\), then there is no test that distinguishes \(p\) and \(q\) with probability \(1-\) using \(n\) samples._

Complementing the specific construction of \(q\) and analysis of its properties, we also introduce a new and general _definition_ framework, speaking to the challenge of capturing "beyond worst case analysis" in this nuanced setting. In Section 3 we motivate and introduce this notion. Broadly, we want to capture the intuition of "instance-optimal" algorithms, namely, an algorithm that performs as well on the given distribution \(p\) as _any_ algorithm customized towards \(p\); however, this definition is unattainably strong in our setting. By contrast, weaker notions such as "admissibility" or Pareto-efficiency are too weak to rule out trivial and effectively useless estimators. We introduce a new notion which we call "neighborhood optimality" that subtly blends between these notions. See Section 3 for details. Appendix A.3 also gives an in-depth discussion comparing neighborhood optimality with the more commonly-used notion of _local minimax_. While the two definitions look superficially similar, we show in the appendix--using a general proposition and a concrete example--that our new notion is a stronger and also more robust definition in the context of mean estimation.

As an application of this new framework, we show in Section 4 that the standard median-of-means estimator is neighborhood optimal up to constant factors. It is an open question to design a neighborhood-optimal estimator which does not have such constant factor slackness.

### Open Questions

We briefly discuss a few open questions and future research directions raised by our results.

Optimal constants for neighborhood optimality estimators--in theory and in practiceWhile median-of-means enjoys neighborhood optimality as we show, the hidden multiplicative constants constants in our analysis are not tight. The median-of-means estimator is also not recommended in practice due to its large asymptotic variance . The goal thus is to show that more modern estimators, for example , also enjoy neighborhood optimality. Ideally, such analysis would yield optimal constants.

Other distributional structures for avoiding asymptotic sub-Gaussian lower boundAs mentioned in the introduction, we view our paper as a _call to arms_ to investigate distributional structures and assumptions that allow mean estimators to go beyond the sub-Gaussian error rate. The paper of Gupta et al.  is the first work aiming to circumvent our lower bounds--showing the benefit of _symmetric_ distributions for mean estimation. The hope, and challenge, is to find other realistic settings that enable mean estimation algorithms beyond the sub-Gaussian benchmark.

High-Dimensional Neighborhood OptimalityGeneralizing the framework and results of this paper to the _high-dimensional_ setting is a compelling direction for future work. In high dimensions,for a distribution with covariance \(\), the corresponding sub-Gaussian rate for mean estimation under the \(_{2}\) norm is \((}()+\|\|})\). We conjecture that, like in the 1-dimensional case, mean estimators cannot outperform the sub-Gaussian rate in the asymptotic regime as \(n\). More specifically, we conjecture that whenever a distribution \(p\) has a finite covariance, a version of Theorem 2 holds for some \(_{n,}(p)\) that approaches the high-dimensional sub-Gaussian error as \(n\), up to constants. Further, for distributions without a finite covariance matrix, it is open to fully characterize the instance-by-instance error rates in high dimensions, and finding an analog of \(_{n,}(p)\) that characterizes both the upper and lower bounds.

### Related Work

The mean estimation problem has been extensively studied, even in one dimension. In the classic setting where the underlying distribution is assumed to have finite but unknown variance, the median-of-means algorithm, independently discovered by different authors (e.g., JVV86; AMS99; NY83), was the first to achieve sub-Gaussian rate to within a constant factor in the high probability regime. The seminal work of Catoni (Cat12) reinvigorated the study of mean estimation, by proposing the first estimator which attains sub-Gaussian rate tight to within a \(1+o(1)\) factor, but his estimator requires a-priori knowledge of the variance, or a bounded \(4^{}\) moment assumption that allows accurate estimation of the variance. This work further showed that the sub-Gaussian rate is a lower bound on the optimal estimation error. Subsequent work by Devroye et al. (DLLO16) proposed a different estimator, also attaining \(1+o(1)\)-tight sub-Gaussian rate under the bounded \(4^{}\) moment assumption, which has additional structural properties. Recent work by Lee and Valiant (LV22) constructs an estimator achieving sub-Gaussian rate to within a \(1+o(1)\) factor for any distribution with finite but unknown variance, absent any knowledge assumption or bounded \(4^{}\) moment assumption.

In the more extreme setting where the underlying distribution may have infinite variance, but is guaranteed to have finite (but unknown) \(1+^{}\) moment for some \((0,1)\), Bubeck et al. (BCL13) proved an upper bound on the error achieved the median-of-means estimator, and Devroye et al. (DLLO16) showed a matching lower bound up to a constant factor.

See the in-depth survey by Lugosi and Mendelson (LM19) on sub-Gaussian mean estimation and regression results prior to 2019.

"Beyond worst-case" analysis is a theme of much recent work and attention in the computer science literature. See (Rou21) for examples of "beyond worst-case" analyses in various contexts in computer science and statistics including, for example, combinatorial algorithmic problems, auction design, and hypothesis testing. Both of the above tightness results in mean estimation are in the worst-case, and in this work, we present (to our knowledge) the first "beyond worst-case analysis" results for the mean estimation problem. We emphasize also that our results are applicable even to distributions with a finite mean, but without any finite \(1+^{}\) moment for any \(>0\).

The notion of admissibility in statistics and the analogous concept of Pareto efficiency serve as a main motivation for our definition of neighborhood optimality, introduced in Section 3. They are well-studied notions in their respective fields, to the extent that they are standard topics in undergraduate courses. See for example the textbooks by Keener (Kee10) and Starr (Sta97) for expositions.

The other main definitional motivation is the notion of instance optimality, which falls under the umbrella of "beyond worst-case analysis" in the computer science literature. We highlight some of the uses of instance optimality in statistical contexts. Valiant and Valiant (VV17) gave the first instance optimal algorithm for the identity testing problem (in total variation distance) for discrete distributions. In later work (see VV16), the same authors showed how to instance-optimally learn discrete distributions (in total variation distance).

A different line of work studies "instance optimality" in the context of mean estimation with differential privacy (DP) (e.g., AD20b; AD20a; HLY21). Specifically, these works address the problem of differentially-privately estimating the mean of a data set, where the data set itself (or equivalently, the uniform distribution over the data set) is the instance. In the DP setting, instance optimality is also not satisfiable by any estimator for the same reason as the non-DP setting, since the hardcoded estimator is always differentially private. They instead use a _local minimax_ (or locally worst-case) notion of optimality (although in these works this notion is sometimes also called "instance optimality"; we take care to distinguish the two definition styles in this paper), where the particular locality/neighborhoodstructure they use is restricted to data sets that have Hamming distance 1 from the instance, since this is a key component in the definition of DP. The local minimax notion of optimality is closely related to our notion of neighborhood optimality. We compare and contrast the two notions in Appendix A.3, and explain why our definition is stronger and more appropriate for our context.

## 2 Proof of main results

This section gives the construction and proof of our main result, Theorem 2.

Our construction of \(q\) from \(p\) will have 2 cases, depending on which of the 2 terms in the definition of the error function \(_{n,}\) dominates. Recall that the error \(_{n,}(p)\) is the sum of two terms involving the "trimmed" distribution \(p_{n}^{*}\): (ignoring constants) \(|_{p}-_{p_{n}^{*}}|\) and \(_{p_{n}^{*}}/n}\); intuitively, the first term measures to what degree \(p\) has an "asymmetric tail", and the second term measures the variance of \(p\) in its central region.

First, consider the case when the first term is larger, namely, \(|_{p}-_{p_{n}^{*}}|>_{p_{n}^{*}}/n}\). Our goal in constructing \(q\) is to maximize \(|_{p}-_{q}|\) subject to \(q\) being indistinguishable from \(p\). Given that 1) the mean of \(p_{n}^{*}\) is already far from the mean of \(p\) by assumption in the case analysis, and 2) \(p\) and \(p_{n}^{*}\) are by construction hard to distinguish since only a small amount of probability mass was trimmed from \(p\) to make \(p_{n}^{*}\), we simply need to construct \(q\) as a carefully chosen convex combination of \(p\) and \(p_{n}^{*}\), and show that this \(q\) indeed satisfies all the properties in the definition of \(N_{n,}(p)\).

Next, for the remaining case when the variance term \(_{p_{n}^{*}}/n}\) is larger than the remaining term \(|_{p}-_{p_{n}^{*}}|\) in \(_{n,}(p)\), we now want to construct \(q\) such that the mean shift \(|_{p}-_{q}|\) is large compared to the variance term \(_{p_{n}^{*}}/n}\), while ensuring that \(q\) is indistinguishable from \(p\). To achieve this, we create \(q\) that is a "skewed" version of \(p\), scaling the probability density by a linear function \(1+ax\), where larger \(a\) means more mean shift between \(q\) and \(p\), but also means that it is easier to distinguish \(q\) from \(p\). Technically, we truncate the linear scaling factor \(1+ax\) to lie between 0 and 2, so that \(q\) will satisfy Condition 3 of the requirements for \(q\) presented in our main theorem, Theorem 2; the probability mass might not be 1 after this skewing, so we might need to normalize; also, we point out that for the purposes of Theorem 2 we do not care whether the mean of \(q\) is shifted to the _left_ or _right_ of \(p\) (corresponding to choosing a positive or negative parameter \(a\)), and the construction makes use of this choice.

**Definition 4** (Construction of indistinguishable pair): _Given a distribution \(p\), we construct a distribution \(q\) in a shift-and-scale invariant manner as follows._

**Case 1:**: \(|_{p}-_{p_{n}^{*}}|>_{p_{n}^{*}}}{n}}\)_. Define_ \(=\) _and construct_ \(q\) _to be the weighted average_ \(q= p+(1-)p_{n}^{*}\)_._
**Case 2:**: \(|_{p}-_{p_{n}^{*}}|_{p_{n}^{*}}}{n}}\)_. Without loss of generality, assume that_ \(_{p}=0\)_. Let parameter_ \(a\) _be the solution in the interval_ \((0,^{*}}}}{n}}]\) _to the below equation characterizing the mean shift, as guaranteed by Lemma_ 2 _in Appendix B:_

\[_{-}^{-}(-x)\,p+a_{-}^{}x^{2}\,p+_{}^{} x\,p=_{p_{n}^{*}}}{n}}\]

_We construct two non-unit measures_ \(q^{+},q^{-}\)_, defined as scaled versions of_ \(p\)_, as_ \(q^{+}}{p}(x)=1+(1,(-1, ax))\)_. By symmetry the masses of_ \(q^{+}\) _and_ \(q^{-}\) _sum to 2; thus one of_ \(q^{+},q^{-}\) _has mass at least 1. Construct_ \(q\) _by choosing that one of_ \(q^{+},q^{-}\)_, and downscaling it by a factor_ \(b[,1]\) _such that the total probability mass is indeed 1._ \(\)

In the rest of the subsections, we will prove all the properties of the construction needed by Theorem 2. We observe that the non-unit measure \(q^{-}\) is "symmetric" to \(q^{+}\), in the sense that its first moment shift from \(p\) is identical but of the opposite sign. Therefore, most of the analysis below will assume the \(q^{+}\) case without loss of generality.

### Checking that \(q}{p} 2\)

It is straightforward to check that \(q}{p} 2\) by construction, for both cases of Definition 4.

**Lemma 5**: _Suppose there is a sufficiently small absolute constant that upper bounds \(}{n}\). Given a distribution \(p\), if we construct \(q\) as in Case 1 (the large \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \(q}{p} 2\)._

**Proof.** Let the support of \(p_{n}^{*}\) be \([x_{},x_{}]\). At \(x[x_{},x_{}]\), we have \(q}{p}=p}{p}= p}{p} 2\). Otherwise, at \(x[x_{},x_{}]\), we have

\[q}{p}=p+(1-)\, p_{n}^{*}}{p}=}{p}( \,p+}{n}}p)^{*}$)} 2\]

where the last line follows from the fact that \(\) is a constant in \((0,1)\) and \(}{n}\) is assumed to be bounded by some sufficiently small absolute constant. \(\)

**Lemma 6**: _Suppose there is a sufficiently small absolute constant that upper bounds \(}{n}\). Given a distribution \(p\), if we construct \(q\) as in Case 2 (the small \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \(q}{p} 2\)._

**Proof.** For \(q^{+}\), we have \(q^{+}}{p}(x)=1+(1,(-1,a^{+}x))\) for some value of \(a^{+}\), and the right hand side is always between \(0\) and \(2\); proven similarly, we have \(0q^{-}}{p} 2\). Noting that \(q\) is constructed by scaling down one of \(q^{-}\) and \(q^{+}\), we also have \(q}{p} 2\). \(\)

### Bounding the squared Hellinger distance

We consider each case of the construction in Definition 4 separately. We first bound the Hellinger distance between \(p\) and the \(q\) constructed in Case 1 of Definition 4 (when \(|_{p}-_{p_{n}^{*}}|\) is large).

**Lemma 7**: _Suppose there is a sufficiently small constant that upper bounds both \(}{n}\) and \(\). Given a distribution \(p\), if we construct \(q\) as in Case 1 (the large \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \((1-d_{}^{2}(p,q)) 4\)._

Since Case 1 of the construction of \(q\) in Definition 4 linearly interpolates between \(p\) and--a very slightly trimmed version of \(p\), namely--\(p_{n}^{*}\), the resulting distribution \(q\) remains close to \(p\); the calculation is in Appendix B.1. The next lemma bounds the squared Hellinger distance of \(p\) and \(q\) in Case 2 of Definition 4 (when \(|_{p}-_{p_{n}^{*}}|\) is small).

**Lemma 8**: _Suppose there is a sufficiently small constant that upper bounds both \(}{n}\) and \(\). Given a distribution \(p\), if we construct \(q\) as in Case 2 (the small \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \((1-d_{}^{2}(p,q)) 4\)._

The proof (see Appendix B.1) uses a technical lemma to relate \(d_{H}(p,q)\) to \(d_{H}(p,q^{+})\) or \(d_{H}(p,q^{-})\), and then uses a linearization of the definition of Hellinger distance to relate it to the mean shift between \(p\) and \(q^{+}\) or \(q^{-}\), which is bounded by Definition 4.

### Lower bounding \(|_{q}-_{p}|\)

We show the lower bound separately for the two cases in the construction of \(q\) in Definition 4. The small \(|_{p_{n}^{*}}-_{p}|\) case is a direct corollary of Lemma 23, which bounds the mean shift in this case.

**Lemma 9**: _Suppose there is a sufficiently small constant that upper bounds both \(}{n}\) and \(\). Given a distribution \(p\), if we construct \(q\) as in Case 1 (the large \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \(|_{q}-_{p}|_{n,}(p)\)._

**Proof.** Without loss of generality, assume that \(_{p}=0\). Recall that \(q= p+(1-)p_{n}^{*}\) and \(=\) from Case 1 of Definition 4. Thus, \(|_{q}-_{p}|=|_{p_{n}^{*}}-_{p}|\). Furthermore, we have \(_{n,}(p)=|_{p_{n}^{*}}-_{p}|+_{p_{n}^{*}}}{n}}\) from the definition of \(_{n,}(p)\) and \(|_{p_{n}^{*}}-_{p}|>_{p_{n}^{*}}}{n}}\) from the lemma assumption and Case 1 of Definition 4, which then imply that \(_{n,}(p) 2|_{p_{n}^{*}}-_{p}|\). Combining the two inequalities yields \(|_{q}-_{p}|_{n,}(p)\) as desired. \(\)

**Lemma 10**: _Suppose there is a sufficiently small constant that upper bounds both \(}{n}\) and \(\). Given a distribution \(p\), if we construct \(q\) as in Case 2 (the small \(|_{p_{n}^{*}}-_{p}|\) case) in Definition 4, then \(|_{q}-_{p}|_{n,}(p)\)._

**Proof.** Without loss of generality, let \(q^{+}\) be the non-unit measure used to construct \(q\), that is \(q=bq^{+}\) for some \(b[,1]\). By the bound on \(b\) as well as the construction of \(q^{+}\) in Definition 4, we have \(|_{q}-_{p}|=b| x\,q^{+}- x\,p|_{p_{n}^{*}}}{n}}\). Additionally, we both have \(_{n,}(p)=|_{p_{n}^{*}}-_{p}|+_{p_{n}^{*}}}{n}}\) from the definition of \(_{n,}(p)\) and \(|_{p_{n}^{*}}-_{p}|_{p_{n}^{*}}}{n}}\) from the lemma assumption and Case 2 of Definition 4, both of which imply \(_{n,}(p) 2_{p_{n}^{*}}}{n}}\). Combining the two inequalities above, we have that \(|_{q}-_{p}|_{n,}(p)\). \(\)

## 3 Neighborhood Optimality: A New Definition Framework

Our main result is a specific and technical indistinguishability result. This section aims to clarify, through a new definition framework, the optimality notion that our technical result implies.

### Neighborhood Optimality

Usual notions of "beyond worst-case optimality" include "instance optimality" which is unattainably strong, and "admissibility"/"Pareto efficiency" from the statistics and economics literature, which is too weak. In particular, the latter notion is too weak in the sense that a trivial estimator that always outputs the same hardcoded mean estimate is actually admissible, despite being algorithmically "vacuous". We define and explain these notions formally in Appendix D.

Given that neither of the usual definitions are suitable for mean estimation, in this work we give a new optimality definition, which we call _neighborhood optimality_. We state its definition in this section, and explore its basic properties and intuition in Appendix A, including how the definition smoothly interpolates between instance optimality and admissibility. Our definition is also related to the notion of local minimax optimality, which we compare with in Appendix A.3. The differences are subtle, yet, as we show in Appendix A.3, local minimax is _too weak_ a notion and, when instantiated inappropriately, allows for absurd bounds to be proven. We thus advocate for this new optimality definition, which correctly rejects such absurd bounds. As an application of our new framework, we prove in Section 4 that the median-of-means estimator is neighborhood optimal up to constant factors. It is an open question to find a neighborhood optimal estimator _without_ the constant factor slackness.

Let \(_{1}\) be the entire set of all distributions with a finite first moment over \(\). We say that \(N\) is a neighborhood function (defined over \(_{1}\)) if \(N\) maps a distribution \(p_{1}\) to a set of distributions \(N(p)_{1}\). For the purposes of the rest of the definitions, it will not matter whether \(p N(p)\). Similarly, an error function \(\) maps distributions to non-negative numbers, like \(_{n,}\) in our main result, Theorem 2. In the later definitions, we use the notations \(N_{n,}\) and \(_{n,}\) to denote their dependence on the sample complexity \(n\) and failure probability \(\).

Given these two notions, we can now define _neighborhood Pareto bounds with respect to \(N_{n,}\)_, which imposes admissibility structure within the local neighborhood \(N_{n,}(p)\) of every distribution \(p_{1}\).

**Definition 11** (Neighborhood Pareto bounds with respect to \(N_{n,}\)): _Let \(n\) be the number of samples and \(\) be the failure probability. Given a neighborhood function \(N_{n,}:_{1} 2^{_{1}}\), we say that the error function \(_{n,}(p):_{1}_{0}^{+}\) is a neighborhood Pareto bound for \(_{1}\) with respect to \(N_{n,}\) if for all distributions \(p_{1}\), no estimator \(\) taking \(n\) i.i.d. samples can simultaneously achieve the following two conditions:_

* _For all_ \(q N_{n,}(p)\)_, with probability_ \(1-\) _over the_ \(n\) _i.i.d. samples from_ \(q\)_,_ \(|-_{q}|_{n,}(q)\)_._
* _With probability_ \(1-\) _over the_ \(n\) _i.i.d. samples from_ \(p\)_,_ \(|-_{p}|<_{n,}(p)\)_._

Note the strict inequality in the second bullet: namely, it is impossible to "beat" the error function over an entire neighborhood, where "beating" is defined as attaining the error function over the neighborhood, and performing strictly better than the error function for \(p\). The above two bullet points essentially capture admissibility within the local neighborhood \(N_{n,}(p)\{p\}\)--compare with Definition 28--and the definition requires admissibility within every such local neighborhood, over every possible \(p\).

The neighborhood \(N_{n,}(p)\) in a neighborhood Pareto bound can be interpreted as the set of distributions "near \(p\)" which, if an estimator performs well on distribution \(p\), then we should reasonably expect or want it to perform well also on all the distributions in the local neighborhood \(N_{n,}(p)\).

Using the notion of neighborhood Pareto bounds, we can now define \(\)-neighborhood optimal estimators, which are estimators whose performances are matched by neighborhood Pareto bounds.

**Definition 12** (\((,)\)**-Neighborhood optimal estimators**): _Let \(>1\) be a multiplicative loss factor in estimation error, and \(>1\) be a multiplicative loss factor in sample complexity. Given the parameters \(,>1\), sample complexity \(n\), failure probability \(\) and neighborhood function \(N_{n,}\), a mean estimator \(\) is \((,)\)-neighborhood optimal with respect to \(N_{n,}\) if there exists an error function \(_{n,}(p)\) such that \((_{n/,}(p),_{n,}(p))\) is a neighborhood Pareto bound2, and \(\) gives estimation error at most \(_{n,}(p)\) with probability at least \(1-\) when taking \(n\) i.i.d. samples from any distribution \(p_{1}\)._

As a basic example and sanity check, in Appendix E, we show that any trivial estimator that outputs a hardcoded mean estimate cannot be \(\)-neighborhood optimal with respect to our chosen neighborhood function (Definition 15 in Section 4) for any \(\).

### Indistinguishability implies a neighborhood Pareto bound

Even though it might not look obvious how we can prove a neighborhood Pareto bound from its definition, we show that our main indistinguishability result essentially implies such a bound. The proof essentially follows the straightforward estimation-to-testing reduction intuition, and we give it formally in Appendix A.2.

**Proposition 13** ("Local" indistinguishability bounds imply neighborhood Pareto bounds): _The error function \(_{n,}\) is a neighborhood Pareto bound with respect to the neighborhood function \(N_{n,}\) if for every distribution \(p_{1}\), there exists a distribution \(q N_{n,}(p)\), with \(q p\), such that \(|_{p}-_{q}|_{n,}(p)+_{n,}(q)\) and it is information-theoretically impossible to distinguish \(p\) and \(q\) with probability \(1-\) using \(n\) samples._

## 4 Median-of-Means is Neighborhood Optimal

To apply our new definitional framework, we choose a reasonable neighborhood function \(N_{n,}\) and show that the median-of-means algorithm is neighborhood optimal with respect to this choice.

In Appendix C, we give the following (straightforward) re-analysis of median-of-means, which will form the upper bound part for neighborhood optimality.

**Proposition 14**: _Consider a distribution \(p\) with mean \(_{p}\), a sample size \(n\), and a median-of-means group count \(4.5\). Let \(p_{n}^{*}\) be the \(\)-trimmed distribution from Definition 1, and \(_{p_{n}^{*}}\) and \(_{p_{n}^{*}}\) be the mean and standard deviation of \(p_{n}^{*}\) respectively. Then, the median-of-means estimator has error \(|_{p}-_{p_{n}^{*}}|+3_{p_{n}^{*}}}{n}}\) except with probability at most \(\)._

We can now discuss the neighborhood choice for the corresponding neighborhood Pareto bound. Recall that, intuitively, neighborhood optimality is asking "how well can our algorithm do on \(p\) given that we also want our algorithm to do similarly well on a neighborhood of \(p\)"; and thus, the smaller we choose the neighborhood, the stronger the resulting theorem. We thus define the neighborhood of \(p\) to consist of distributions that are similar or similarly nice to \(p\) in 4 different ways:

**Definition 15** (Choice of neighborhood function \(N_{n,}\) in Theorem 16): _Define \(N_{n,}(p)\) to be the set of distributions \(q_{1}\) such that_

1. \(_{n/3,}(q) 100_{n,}(p)\)__
2. \((1-d_{}^{2}(p,q)) 4\)__
3. \(|_{q}-_{p}|_{n,}(p)\)__
4. _For all_ \(x\)_,_ \(q}{p}(x) 2\)_._

As a basic sanity check, we show in Appendix E that a trivial, hardcoded estimator cannot be neighborhood optimal--this is mostly a consequence of Property 3 above. See Appendix E for a formal statement and proof. We then show:

**Theorem 16**: _Let \(n\) be the number of samples and \(\) be the failure probability. Assume that there is a sufficiently small constant which upper bounds both \(}{n}\) and \(\)._

_Consider the neighborhood function \(N_{n,}\) of Definition 15. Recall the error function defined in Definition 1 as \(_{n,}(p)=|_{p}-_{p_{n}^{*}}|+_{p_{n}^{*}}}{n}}\). Then, for some sufficiently large constant \(\), the error function \((_{n/3,},_{n,})\) is a neighborhood Pareto bound with respect to \(N_{n,}\)._

_Combined with Proposition 14 stating that the median-of-means estimator has error function \(O(_{n,})\), this implies the median-of-means estimator is \((,3)\)-neighborhood optimal with respect to \(N_{n,}\)._

The main component of the proof is our construction of \(q\), and the accompanying analysis of Theorem 2 showing that \(q\) is well behaved in several senses. We specifically show Lemma 17, a slight extension of Theorem 2:

**Lemma 17**: _Let \(n\) be the sample complexity and \(\) be the failure probability, and recall the definition of \(_{n,}\) from Definition 1. Assume that there is a sufficiently small constant which upper bounds both \(}{n}\) and \(\). Then for any distribution \(p\), there exists a distribution \(q p\) such that the mean of \(q\) is \(_{n,}(p)\) different from the mean of \(p\), and \((1-d_{}^{2}(p,q)) 4\), and \(q N_{n,}(p)\)._

See Appendix F for the proof of Lemma 17. We will now use Lemma 17 to prove Theorem 16.

**Proof of Theorem 16.** By Proposition 13, it suffices to show that, for every distribution \(p_{1}\), there exists a distribution \(q N_{n,}(p)\) with \(q p\) such that \(|_{p}-_{q}|((_{n/3,},_{n,})(p)+(_{n/3,},_{n,})(q))\) for some large constant \(\), and no tester can distinguish \(p\) and \(q\) with probability \(1-\) using \(n\) samples.

Given a distribution \(p_{1}\), consider the distribution \(q N_{n,}(p)\) guaranteed by Lemma 17. Since \(q\) satisfies \((1-d_{}^{2}(p,q)) 4\), by Fact 1 we know that \(p\) and \(q\) are indistinguishable with probability \(1-\) using \(n\) samples.

It remains to check that \(|_{p}-_{q}|((_{n/3,},_{n,})(p)+(_{n/3,},_{n,})(q))\) for some sufficiently large constant \(\). By Lemma 17, we have

\[|_{p}-_{q}| (_{n,}(p))(_{n,} (p)+_{n,}(p))(_{n/3,}(q)+_{n, }(p))}\] \[((_{n/3,},_{n,})(p)+ (_{n/3,},_{n,})(q))\]

which completes the proof of Theorem 16.