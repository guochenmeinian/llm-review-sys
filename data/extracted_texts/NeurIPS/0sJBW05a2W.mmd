# 3D Focusing-and-Matching Network for Multi-Instance Point Cloud Registration

Liyuan Zhang, Le Hui, Qi Liu, Bo Li, Yuchao Dai

School of Electronics and Information, Northwestern Polytechnical University

Shaanxi Key Laboratory of Information Acquisition and Processing

zhangliyuannpu@mail.nwpu.edu.cn, {huile, liuqi, libo, daiyuchao}@nwpu.edu.cn

Corresponding authors.Liyuan Zhang, Le Hui, Qi Liu, Bo Li, and Yuchao Dai are with the Key Lab of Shaanxi Key Laboratory of Information Acquisition and Processing, School of Electronics And Information, Northwestern Polytechnical University, China.

###### Abstract

Multi-instance point cloud registration aims to estimate the pose of all instances of a model point cloud in the whole scene. Existing methods all adopt the strategy of first obtaining the global correspondence and then clustering to obtain the pose of each instance. However, due to the cluttered and occluded objects in the scene, it is difficult to obtain an accurate correspondence between the model point cloud and all instances in the scene. To this end, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration by learning the multiple pair-wise point cloud registration. Specifically, we first present a 3D multi-object focusing module to locate the center of each object and generate object proposals. By using self-attention and cross-attention to associate the model point cloud with structurally similar objects, we can locate potential matching instances by regressing object centers. Then, we propose a 3D dual-masking instance matching module to estimate the pose between the model point cloud and each object proposal. It performs instance mask and overlap mask masks to accurately predict the pair-wise correspondence. Extensive experiments on two public benchmarks, Scan2CAD and ROBI, show that our method achieves a new state-of-the-art performance on the multi-instance point cloud registration task. The project page is at https://npucvr.github.io/3DFMNet/.

## 1 Introduction

Point cloud registration, a fundamental process in computer vision, involves aligning two point clouds through estimating a rigid transformation. In practical applications like robotic bin picking, multi-instance registration emerges as a critical need, demanding the alignment of a model's point cloud with multiple instances within the scene. This task presents heightened complexity compared to single-point cloud registration, primarily due to challenges such as the uncertain number of instances and inter-instance occlusions. These complexities are particularly pronounced in cluttered environments, where precise alignment becomes pivotal for effective robotic operations. Therefore, how to improve the accuracy of multi-instance point cloud registration is still a challenging issue.

There are a few efforts for tackling multi-instance point cloud registration. Existing pipelines can be roughly divided into two types: two-stage and one-stage. For the two-stage process, we first extract point correspondences between the model point cloud and scene point clouds, and then recover per-instance transformations through multi-model fitting . Although two-stage methods are simple and feasible, the success of these methods largely depends on the quality of thecorrespondence. Furthermore, due to cluttered and occluded objects, it is still difficult to accurately cluster the correspondences into individual instances for subsequent pair-wise registration. For the one-stage process, it takes the model point cloud and scene point cloud as inputs, and directly outputs pose. As a representative one-stage work, Yu _et al._ proposed a coarse-to-fine framework, which learns to extract instance-aware correspondences for estimating transformations without multi-model fitting. Due to the consideration of instance-level information in correspondence, it can obtain fine-grained features, thereby boosting the performance. However, for the scene with multiple objects, obtaining accurate instance-level correspondence is very difficult, especially for the cluttered and occluded objects. Therefore, to alleviate the difficulty of learning correspondence between the model point cloud and multiple objects in the scene, as shown in Figure 1, we consider first focusing on the object centers, and then learning the matching between the object proposal and the model point cloud.

In this paper, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration. The core idea of our method is to decompose the multi-instance point cloud registration into multiple pair-wise point cloud registrations. Specifically, we propose a 3D multi-object focusing module to localize the potential object centers and generate object proposals. To associate the object with the input CAD model, we use self-attention and cross-attention to learn the structurally similar features, thereby improving the accuracy of prediction for object centers. Based on the learned object center, we incorporate the radius of the CAD model to generate object proposals through ball query operation. After that, we propose a 3D dual-masking instance matching module to learn accurate pair-wise registration between the CAD model and object proposal. It adopts an instance mask to filter the background points in the object proposal and uses an overlap mask to improve the pair-wise partial registration of incomplete objects.

In summary, our contributions lie in three aspects:

1. Our primary contribution does not lie in the network architecture but rather in proposing a new pipeline to address the multi-instance point cloud registration problem. Existing methods (such as PointCLM  and MIRETR ) mainly learn correspondence between the one CAD model and multiple objects (one-to-many paradigm), while our method decompose the one-to-many paradigm into multiple pair-wise point cloud registration (multiple one-to-one paradigm) by first detecting the object centers and then learning the matching between the CAD model and each object proposal.
2. Our new pipeline is simple yet powerful, achieving the new state-of-the-art on both Scan2CAD  and ROBI  datasets. Especially on the challenging ROBI dataset, our method significantly outperforms the previous SOTA MIRETR by about 7% in terms of MR, MP, and MF.
3. The progressive decomposition approach of transforming multi-instance point cloud registration into multiple pair-wise registrations, as proposed in our paper, also holds significant insights for other tasks, such as multi-target tracking and map construction.

Figure 1: Comparison between our method and existing methods in multi-instance point cloud registration. Our method decomposes the multi-instance point cloud registration into multiple pair-wise point cloud registration.

Related Work

**Point Cloud Registration** is a crucial task in fields such as robotics and autonomous driving, which usually involving three stages: point matching, outlier rejection, and pose estimation. Acquiring accurate point correspondences is critical for successful registration, making the first two stages particularly important. Accurate point matching deeply relies on features that are descriptive and rotation invariant. Many researchers have made efforts on this, including handcrafted descriptors[32; 6] and learning-based descriptors[1; 10; 49; 19; 20; 41; 2]. Some recent coarse-to-fine frameworks[18; 31] bypass keypoint detection and achieve accurate correspondences in large-scale scenes. To handle the problem of outliers, RANSAC and its variants[7; 4] ) follow the hypothesis-and-verification process to reject outliers. And some learning-based methods[11; 16; 42; 34] for eliminating outliers have also been proposed for robust pose estimation. On the other hand, several methods[39; 17; 40; 44; 38; 23; 45] directly estimate the transformation with a neural network in an end-to-end manner. However, these point cloud registration methods almost focus on the one-to-one problem which only need to solve the transformation between two point clouds. So they cannot directly work when faced with the challenge of a large number of instances in multi-instance registration tasks and heavy intra-instance occlusion.

**Multi-Instance Point Cloud Registration**, which aligns a source point cloud to its various instances within a target point cloud, has received relatively less attention. Unlike multi-way registration, which aims to create a globally consistent reconstruction from multiple fragments through pairwise registration, multi-instance registration involves not only rejecting outliers from noisy correspondences but also identifying the inlier set for each individual instance. This makes it even more challenging than the traditional registration problem. Early methods of multi-model fitting were used for this task. In the early stages of this task's development, various methods of multi-model fitting were employed. RANSAC-based approaches[24; 5; 13; 22] followed a hypothesis verification approach to fit multiple models, while another method, based on clustering[26; 27; 25; 36; 48; 8; 47], entailed sampling an extensive set of hypotheses and clustering the correspondences based on their residuals under these hypotheses. Recently,  proposed an instance-aware correspondence extraction method for end-to-end multi-instance pose estimation. However, existing methods are often affected by outliers from other instances and highly rely on scene-specific global feature extraction. In this approach, the multi-instance registration problem is addressed by detecting matching instance centers in the scene and subsequently splitting the instances, effectively transforming it into a pairwise point cloud registration problem.

Figure 2: The framework of our 3D focusing-and-matching network for multi-instance pint cloud registration. Given the scene point cloud and the CAD model, we first present the 3D multi-object focusing module to localize the centers of the potential objects in the scene. Then, we design the 3D dual-masking instance matching module to learn pair-wise point cloud registration from the localized object proposals.

Method

The overall pipeline is illustrated in Figure 2. Our method is a two-stage framework, which first localizes the center of each object and then performs pair-wise correspondence. For the first stage, we present a 3D multi-object focusing module (Sec. 3.1) for detecting the potential instance centers by learning the correlation between the input model point cloud and the scene point cloud. For the second stage, we design a 3D dual-masking instance matching module (Sec. 3.2) for predicting pair-wise correspondence between the input model point cloud and the localized region of each object center. At last, we introduce the loss functions of our method (Sec. 3.3).

### 3D Multi-Object Focusing

As the first stage of our method, the 3D multi-object focusing module aims to regress the center of the potential objects for generating high-quality proposals for pair-wise correspondence. Compared to predicting the bounding box or mask of an instance, directly regressing the center of the object is much easier, especially in cluttered and occluded scenes. In order to accurately detect the object center, we first learn the correlation between the model point cloud and the scene point cloud. Then, we predict the object center by learning the offset of each point. Finally, we introduce how to construct 3D object proposals for subsequent pair-wise point cloud registration.

**Feature correlation learning.** We design a simple yet efficient feature extraction structure to learn the correlation between the scene point cloud \(^{N 3}\) and the model point cloud \(^{M 3}\), where \(N\) and \(M\) are the numbers of their points, respectively. Note that for a fair comparison, we do not use RGB information of point cloud. Before learning correlation, we adopt the encoder of KPConv  to extract multi-scale point features of \(\) and \(\), respectively. The output feature maps are denoted by \(_{p}^{N_{s} C}\) and \(_{q}^{M_{s} C}\), where \(N_{s}\) and \(M_{s}\) are the numbers of points after using grid subsampling . After that, we simply use self-attention and cross-attention to build the correlation between the model point cloud and the scene point cloud, which is written as:

\[_{p}=(_{p},_{p},_{p}),_{q}=(_{q},_{q},_{q}), \\ _{p}=(_{p},_{q},_{q}) \] (1)

where \(_{p}^{N C}\) is the output feature map of the scene point cloud \(\) that embeds the relationship of the model point cloud \(\). Note that in the experiments, we stack three cross-attention layers, in which the output of the previous layer will be sent to the next layer as the input. For simplicity, we use the same symbol \(_{p}\) to represent the feature map of the final output. By learning feature correlations, it is desired that the potential instances will be enhanced in the background, making them easier to detect.

**Object center prediction.** After feature correlation learning, we regress the object center from the whole scene. Given the feature map \(_{p}^{N_{s} C}\), we directly predict the offset vector and the instance mask for each point. The point offset vector means the displacement to its instance center, which is given by:

\[V_{p}=(_{p})\] (2)

where \(_{p}^{N_{s} 3}\) is the offset matrix for \(N_{s}\) points in the scene. To identify whether a point belongs to an instance rather than the background, we predict the point mask, which is written as:

\[_{p}=((_{p},_{p}))\] (3)

where \(_{p}^{N_{s} 1}\) is the mask score. \(_{p}\) is the geodesic distance embedding from . If the mask score is larger than 0.5, this point is identified as belonging to the point on the object. To obtain an accurate object center, we first displace each point to its potential instance center by adding the original coordinates of each point to its learned point offset (\(+_{p}\)). Then, we use the learned mask \(_{p}\) to filter out background points and leave points on the instance. Subsequently, we employ DBSCAN  to group the offset points into K clusters. Finally, by averaging the points in each cluster, we can obtain the center of each instance, which is formulated by:

\[_{p}=(_{p}_{p})\] (4)

where \(_{p}^{K 3}\) is the center of \(K\) objects.

**Object proposal generation.** Based on the obtained object centers, we construct the object proposals through ball query operation. Based on the object center, we use radius \(r\) to draw a three-dimensionalsphere and collect the points that fall within the sphere as the object proposal. Note that the radius parameter \(r\) is equal to the radius of the model point cloud. It is desired that the constructed spherical regions should include the entire object as much as possible. Compared with directly learning multi-instance point cloud registration, we can learn pair-wise point cloud registration between each object proposal and the model point cloud, thereby reducing the difficulty of registration.

### 3D Dual-Masking Instance Matching

Once we obtain object proposals, we employ a 3D dual-masking instance matching module to learn pair-wise point cloud registration. Specifically, we first learn the instance mask to segment the instance from the object proposal. Then, we learn the overlap mask to segment the common area between the instance and the model point cloud. Finally, based on the instance mask and overlap mask, we learn the pair-wise instance matching.

**Instance mask.** Since we cannot obtain the ideal object proposal, we need to filter out the background points from the object proposal to obtain the mask of the instance. Given the point cloud of object proposal \(^{T 3}\) (\(T\) is the number of points in object proposal), we employ a small encoder structure of KPConv  to extract the feature of object proposal. Therefore, we can obtain the feature map \(_{o}^{T_{s} C}\), where \(T_{s}\) is the number of points after grid subsampling. The instance mask \(_{o}\) is formulated by:

\[_{o}=((_{o},_{o}))\] (5)

where \(_{o}^{T_{s} 1}\) is the mask score and \(_{o}\) is the geodesic distance embedding from . It is worth noting that in the first stage, we learn point masks for all instances from the entire scene, making it difficult to obtain accurate point masks for each instance. Here, we learn point masks from the object proposal, so we can obtain more accurate instance masks.

**Overlap mask.** Generally, due to object occlusion, there are a large number of incomplete objects. Therefore, we consider learning the overlap mask between the incomplete object and the complete model point cloud. We feed the model point cloud into the designed small KPConv encoder to obtain feature map \(_{q}^{T_{q} C}\). Similarly, we use self-attention and cross-attention to learn the correlation between the object proposal and the model point cloud, which is formulated as:

\[_{o}=(_{o},_{o},_{o}), {E}_{q}=(_{q},_{q},_{q}),\] (6) \[_{o}=(_{o},_{q},_{q})\]

where \(_{o}^{T_{s} C}\) is the enhanced feature map of the object proposal. After that, we use an MLP to predict the overlap mask, which is given by:

\[_{op}=((_{o},_{o}))\] (7)

where \(_{op}^{T_{s} 1}\) is the obtained overlap mask. To upsample the overlap mask to the original resolution, we use a small KPConv decoder to generate feature map \(}_{op}^{T 1}\).

For subsequent matching steps, we follow  to match the dense points within the local patches of two matched sampled points with an optimal transport layer. However, the local correspondences extracted in this manner often cluster closely, which results in unstable pose estimation, as noted in. Since the local area extracted from the focus network will contain some scene noise, and the intercepted instance point cloud will be incomplete, this problem will be more serious. To mitigate this problem, we suggest extracting a dense set of point correspondences within the instance boundaries by utilizing instance masks and overlap masks. For each points correspondences \(}_{k}=(}_{i},}_{j})\), we collect their neighboring points \(_{i}^{P}\) and \(_{j}^{Q}\). The points out of instance is removed from \(_{i}^{P}\) based on the instance mask. In order to solve the problem of incomplete point clouds, we further use overlap masks to eliminate non-overlapping parts within a patch. Then the clear pair-wise correspondences are extracted with an optimal transport layer and mutual top-k selection, and using the local-to-global registration followed by .

### Loss Function

**Loss in focusing.** For the 3D multi-object focusing module, we need to learn better shape features for localization. Therefore, we follow  and use a circle loss \(L_{circle}\) to learn fine interactive features,as:

\[L_{}^{Q}=|}_{_{i}^{Q} }[1+_{_{i}^{P}_{i}^{1}}e^{_{i }^{j}_{i}^{i,j}(d_{i}^{j}-_{p})}_{_{i}^{P} _{i}^{1}}e^{_{n}^{i,k}(_{n}-d_{i}^{k})}]\] (8)

where P and Q are source and target points-set and \(\) is the anchor patches of each set. \(d_{i}^{j}=\|}_{i}^{Q}-}_{j}^{P}\|_{2}\) is the distance in feature space, \(_{i}^{j}=(o_{i}^{j})^{}\) and \(o_{i}^{j}\) is the overlap ratio between \(_{i}^{P}\) and \(_{j}^{Q}\). The weights \(_{p}^{i,j}=(d_{i}^{j}-_{p})\) and \(_{n}^{i,k}=(_{n}-d_{i}^{k})\) are determined individually for each positive and negative example, using the margin hyper-parameters \(_{p}=0.1\) and \(_{n}=1.4\). The circle loss on \(\) is calculated in the same way.

For each sampled points in the scene, we constrain their learned offsets \(=\{o_{1},...,o_{N}\}^{N 3}\) from their nearest target instance center using an L1 regression loss as follows:

\[L_{reg}=p_{i}}_{i}\|o_{i}-(_{i}-p_{i})\|\] (9)

where \(_{i}\) is the centroid of the nearest instance that points \(i\) belongs to. Considering the varying object sizes across different categories, it is challenging for the network to accurately regress precise offsets, especially for boundary points of large objects, as these points are relatively far from the instance centroids. To tackle this problem, we introduce a direction loss to constrain the direction of the predicted offset vectors. We define this loss, following the method in, as a measure of the negative cosine similarities, \(i.e.\),

\[L_{dir}=-p_{i}}_{i}}{\|o_{i}\|_{2}}_{i}-p_{i}}{\|_{i}-p_{i}\|_{2}}\] (10)

The overall focusing loss is computed as: \(L_{focusing}=L_{circle}+L_{reg}+L_{dir}\).

**Loss in matching.** For coarse points feature learning, we employ the circle loss, as mentioned earlier in the Focus Network. As for point matching, we following  use a negative log-likelihood loss on the assignment matrix \(}_{i}\) of each ground-truth points correspondence \(_{i}^{*}\), just as following:

\[L_{,i}=-_{(x,y)_{i}^{*}}_{x,y}^{i}- _{x_{i}}_{x,m_{i}+1}^{i}-_{y_{ i}}_{n_{i}+1,y}^{i}\] (11)

where \(_{i}\)and\(_{i}\) are the unmatched points in the two matched patches. The final loss is the average of the loss over all points matches: \(L_{}=}_{i=1}^{N_{g}}L_{p,i}\).

Regarding the prediction of instance masks and overlap masks, we follow the methodology outlined in . The mask prediction loss is composed of binary cross-entropy (BCE) loss and dice loss with Laplace smoothing, defined as follows:

\[L_{,i}=(m_{i},m_{i}^{gt})+1-2 m_{i}^{gt }+1}{|m_{i}|+|m_{i}^{gt}|+1}\] (12)

where \(m_{i}\) and \(m_{i}^{gt}\)are the predicted and the ground-truth instance masks, respectively. The final mask prediction loss is the average loss over all. The total matching loss function \(L_{matching}\) is defined as:

\[L_{matching}=L_{circle}+L_{nll}+L_{overlapmask}+L_{instancemask}.\] (13)

## 4 Experiments

### Datasets and Evaluation Metrics

For multi-instance point cloud registration, We train and evaluate our method on two public benchmarks: Scan2CAD  and ROBI .

**Scan2CAD**. As a pioneering dataset in the realm of aligning scenes with CAD models, it leverages the resources of ScanNet  and ShapeNet  to form a multi-instance registration dataset. With a corpus comprising 1,506 scenes sourced from ScanNet, meticulously annotated with 14,225 CAD models from ShapeNet alongside their spatial orientations within the scenes, Scan2CAD sets a new standard in scene-CAD alignment. For the total of 2,184 pairs of point clouds, it allocates 70% of pairs for training, 10% for validation, and reserves 20% for testing.

**ROBI**. It is a dataset tailored specifically for industrial bin-picking applications. ROBI collects 7 reflective metallic industrial objects and 63 meticulously crafted bin-picking scenes. Each point cloud pair is meticulously crafted, with the scene point cloud generated through back projection from depth images, while the model point cloud is meticulously sampled from the CAD model corresponding to its industrial counterpart. There are a total of 4,880 pairs of ROBI, divided into 70% for training, 10% for validation, and 20% for testing.

**Metrics**. We adopt three registration metrics to evaluate the methods, including Mean Recall (MR), Mean Precision (MP), and Mean F1 score (MF). We refer to the settings used in MIRETR and previous work to determine whether an instance is recognized as correctly registered based on RTE and RRE [48; 36]. Specifically, we consider a match successful when \(RTE 4 voxelsize\) and \(RRE 15^{}\). Following existing methods, such as MIRETR, the voxel sizes of Scan2CAD and ROBI dataset are set to 0.025m and 0.0015m, respectively. MR quantifies the proportion of registered instances relative to the total number of ground-truth instances, while MP measures the ratio of registered instances against the entirety of predicted instances. MF score is the harmonic mean of both MP and MR. Additionally, we present the pair-wise inlier ratio (PIR), elucidating the proportion of inliers from a single instance amidst all extracted correspondences, considering one of the most important challenges of multi-instance registration is identifying the set of inliers for individual instances.

### Implementation Details

Our method is trained on NVIDIA RTX 4090 GPUs and uses the Pytorch deep learning platform. We employ the Adam optimizer for 60 epochs. In initial learning rate and weight decay are set to 0.001 and 0.0001, respectively. We use a KPConv-FPN backbone followed by  for feature extraction. We utilize a voxel subsampling approach to reduce the resolution of the point clouds, resulting in the creation of sampled points and dense points, which are then inputted into the network. The initial step involves downsampling the input point clouds using a voxel-grid filter with a size of 2.5cm for Scan2CAD and 0.15cm for ROBI. Subsequently, we employ a 4-stage backbone architecture in both the multi-object focusing and sub-matching network. Following each stage, the voxel size is increased twofold to further reduce the resolution of the point clouds. The initial and final (coarsest) levels of downsampled points correspond to the dense points and sampled points, respectively, which are used for follow-up process. In the multi-object focusing network, we employ a ball query approach akin to the one detailed in , enabling us to retrieve the local neighborhood surrounding the regression center. The search radius is set to 1.2 times the size of the CAD model. Specifically, we randomly sample 4096 points from the dense points obtained through voxel subsampling in both Scan2CAD and ROBI datasets. During training, we use the ground truth center as supervision to train the 3D multi-object focusing module and use the point cloud around the ground truth center as the training data to train the matching network. During testing, we use the center predicted by the 3D multi-object focusing module and its surrounding point cloud as the input to the 3D dual-masking instance matching module to regress the final pose.

   &  &  \\   & MR (\%) & MP (\%) & MF (\%) & MR (\%) & MP (\%) & MF (\%) \\  T-Linkage  & 77.12 & 46.04 & 57.65 & 12.04 & 10.47 & 11.20 \\ RansaCov  & 84.78 & 71.34 & 77.48 & 14.14 & 26.29 & 18.38 \\ PointCLM  & 91.85 & 91.08 & 91.46 & 18.68 & 40.11 & 25.48 \\ ECC  & **96.52** & 89.03 & 92.62 & 24.65 & 34.85 & 28.91 \\ MIRETR  & 95.70 & 91.21 & 93.40 & 38.51 & 41.19 & 39.80 \\ 
3DFMNet (ours) & 95.44 & **94.15** & **94.79** & **46.81** & **50.61** & **48.63** \\
3DFMNet\({}^{*}\) (ours) & 97.68 & 94.63 & 96.14 & 52.59 & 63.13 & 57.38 \\  

Table 1: Results comparison of different methods on the test sets of both the Scan2CAD and ROBI datasets. The best results are highlighted in **bold**. Please note that “3DFMNet\({}^{**}\) indicates the upper bound of our method.

[MISSING_PAGE_FAIL:8]

on the test set of the ROBI dataset. It can be observed that our method can effectively obtain the correspondences between the challenging incomplete objects and the model point cloud.

**Time costs.** Here we report the inference time of different methods on the ROBI dataset for a comprehensive comparison. Table 4 shows model inference time for feature extraction and pose estimation time for transformation. "Total" represents the sum of "Model" time and "Pose" time. While our two-stage method has a slightly higher total time than the one-stage MIRETR , it runs faster than the two-stage PointCLM . Nonetheless, compared with PointCLM and MIRETR, our 3DFMNet achieves higher performance.

### Ablation Study

**Impact of the number of sampled points.** The grid sizes influence the number of sampled points. We conduct experiments on the Scan2CAD dataset to verify the impact of different numbers of points. The results of MR and MP are 83.76% and 92.50% (1024 points), **95.44%** and **94.15%** (default 4096 points), and 95.20% and 93.75% (8192 points), respectively. It is evident that too few sampling points hinder information gathering and reduce accuracy. Conversely, too many sampling points do not improve accuracy and instead decrease it due to redundancy.

**Effective of dual-masking.** We evaluate the necessity of our dual-masking structure under the fair setting of the Scan2CAD dataset. When taking turns removing components, the results of MR and MP are 94.76% and 93.30% (only removing the overlap mask), 91.82% and 93.53% (only removing the instance mask), 90.01% and 90.90% (removing both), and **95.44%** and **94.15%** (using both). The ablation study results can demonstrate the effectiveness of our dual-masking structure.

   Methods & Model & Pose & Total \\  T-Linkage  & 0.30 & 3.07 & 3.34 \\ RansaCov  & 0.30 & 0.17 & 0.47 \\ PointCLM  & 0.30 & 0.33 & 0.63 \\ ECC  & 0.30 & 0.21 & 0.51 \\ MIRETR  & 0.30 & **0.10** & **0.40** \\
3DFMNet (ours) & **0.26** & 0.28 & 0.54 \\   

Table 4: Per scene time on the ROBI dataset.

Figure 3: Registration results on the test set of the Sacn2CAD dataset. We visualize the successfully registered instances of MIRETR  in (b) and ours in (c). “# Inst” means the number of registered instances. Note that for a better view, we draw the green boxes for the ground truth and the red boxes for the predict correspondences.

### Limitation

Our 3D focusing-and-matching network is a two-stage framework for the multi-instance point cloud registration task. The localization accuracy of the first stage will affect the pair-wise correspondence in the second stage. We have analyzed the upper bound of our method in 4.3. In addition, due to the two-stage process, the inference time of our method is slightly lower than the previous MIRETR . In future work, we will strive to improve the performance and speed of our method.

## 5 Conclusion

In this paper, we proposed a 3D focusing-and-matching network (3DFMNet) for multi-instance point cloud registration. Specifically, we first presented a 3D multi-object focusing module that learns to localize the center of the potential target in the scene by considering the correlation between the model point cloud and the scene point cloud. Then, we designed a 3D dual-masking instance matching module to learn the pair-wise correspondence between the model point cloud and the localized object. Extensive experiments on two public benchmarks, Scan2CAD and ROBI, show that our method achieves a new state-of-the-art performance on the multi-instance point cloud registration task.