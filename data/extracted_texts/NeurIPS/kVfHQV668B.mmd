# Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation

Kun Huang\({}^{1}\) Xin Guo\({}^{1}\) Meng Wang\({}^{1}\)

\({}^{1}\)Ant Group

{hunterkun.hk,darren.wm,bangzhu.gx}@antgroup.com

Corresponding author.

###### Abstract

Knowledge Distillation (KD) has emerged as a promising approach for compressing large Pre-trained Language Models (PLMs). The performance of KD relies on how to effectively formulate and transfer the knowledge from the teacher model to the student model. Prior arts mainly focus on directly aligning output features from the transformer block, which may impose overly strict constraints on the student model's learning process and complicate the training process by introducing extra parameters and computational cost. Moreover, our analysis indicates that the different relations within self-attention, as adopted in other works, involves more computation complexities and can easily be constrained by the number of heads, potentially leading to suboptimal solutions. To address these issues, we propose a novel approach that builds relationships directly from output features. Specifically, we introduce token-level and sequence-level relations concurrently to fully exploit the knowledge from the teacher model. Furthermore, we propose a correlation-based distillation loss to alleviate the exact match properties inherent in traditional KL divergence or MSE loss functions. Our method, dubbed FCD, presents a simple yet effective method to compress various architectures (BERT, RoBERTa, and GPT) and model sizes (base-size and large-size). Extensive experimental results demonstrate that our distilled, smaller language models significantly surpass existing KD methods across various NLP tasks.

## 1 Introduction

Past few years have witnessed a rapid development of pre-trained language models (PLMs) thanks to their effectiveness across a wide range of natural language processing tasks. Pre-trained language models, such as BERT Devlin et al. (2018), RoBERTa Liu et al. (2019), and GPT-2 Radford et al. (2019), learn contextualized text representations by predicting words given their context using large scale text corpora, and can be fine-tuned with additional task-specific layers to adapt to downstream tasks. However, the excellent capability for various NLP tasks comes at demanding huge resources and large memory footprints. For example, the BERTBASE model contains about 110M parameters and 12 Transformer Vaswani et al. (2017) layers, which prevents these transformer-based models from being finetuned and deployed on resource-constrained devices and real-time applications. Recent studies Kovaleva et al. (2019), Voita et al. (2019) indicate that redundancy exists in the original PLMs. Therefore, a series of attempts Chung et al. (2020), Wu et al. (2020), Wang et al. (2020), Gordon et al. (2020), Tang et al. (2019), Aguilar et al. (2019) have been made to review the techniques for effective compression of the pre-trained heavy transformers without compromising the performance, of which knowledge distillation is considered to be a practical paradigm, Typically, knowledge distillation techniques aims at effectively transferring the dark knowledge embedded in a large teacher network to boost the performance of the smaller student network during training. Once trained, this compactstudent network can be directly deployed in real-life applications without introducing extra inference time or structure modifications. The essence of knowledge distillation relies on how to formulate and transfer the knowledge from teacher to student. The classic logit-based distillation Hinton et al. (2015) directly mimic the final prediction outputs between the teacher and student via Kullback-Leibler (KL) divergence, which only brings limited performance gain to the student. Besides this vanilla knowledge distillation, many other works Du et al. (2020); Heo et al. (2019); Tian et al. (2019) also try to make use of intermediate representations of the pre-trained teacher transformer. The intermediate layers contain more embedding, supplement richer features, thus allow the student transformer to acquire more information in addition to outputs. Jiao et al. (2019) proposed TinyBERT, which distill the information between multiple intermediate features, including the embedding, self-attention matrices and hidden states of the teacher and student networks via the mean squared error (MSE). However, this usually need the adaption layers to align the mismatching embedding dimensions. Such gap makes it hard for the student to mimic the teacher's feature directly and induces additional training cost as a consequence. MiniLM Wang et al. (2020) employs self-attention heatmaps and and value relations via the KL-divergence loss to deeply mimic teacher's self-attention modules. It leads to a restriction that the number of attention heads of student model has to be the same as its teacher. To solve this problem, MiniLMv2 Wang et al. (2020) first concatenate and then split self-attention vectors of different attention heads according to the desired number of relation heads, which involves more computation of queries, keys, and values in self-attention and consequently leads to suboptimal performance.

Motivated by these observations, we aim to directly model feature relationships between the teacher and student models. In a manner similar to the self-attention mechanism, we first model token relations using the output features of the teacher and student models. This token-level relationship has the capacity to capture long-term dependencies between input tokens and highlight critical tokens essential for linguistic comprehension. While the token-level relation is intuitive, it reflect only one aspect of the feature relationships. We further exploit another important aspect of feature relations, the sample-level, to capture the semantic relationship across a batch of samples, an aspect that has been overlooked in previous works. In this way, both types of relations are combined to complement each other to bring more fine-grained feature knowledge. Therefore, the student model is expected to have superior performance compared to that trained stand-alone. Thanks to the same shape of feature relations between student and teacher, our method offers increased flexibility with respect to the embedding dimension and the number of attention heads. Moreover, we propose a correlation-based loss function to replace the KL divergence and MSE used in traditional KD methods. Specifically, we employ the pearson linear correlation as a novel loss function, relaxing the exact match property typically associated with KL divergence and MSE. The overview of the proposed method is illustrated by Figure 1. To sum up, our main contributions are outlined as follows:

* We directly model feature relationships between teacher and student models, which jointly leveraging token-level and sample-level relations to distill knowledge for the first time.
* We propose a correlation-based loss function using Pearson linear correlation, and theoretically explain that it offers a more flexible alternative to traditional KL divergence and MSE.
* Extensive experiments are conducted with popular variants of PLMs, including BERT, RoBERTa, and GPT on GLUE datasets, and our proposal consistently performs better than existing methods.

## 2 Related Works

### Pretrained Language Models

Pretrained language models are pretrained on large amounts of text corpus, and then fine-tuned on task-specfic dataset. BERT Devlin et al. (2018) proposes to use a masked language modeling (MLM) objective to pretrain a deep bidirectional Transformer encoder. RoBERTa Liu et al. (2019) achieves strong performance by training longer steps using large batch size and more text data. Besides those encoder-based models, GPT-2 Radford et al. (2019) is a decoder-based model designed for unidirectional, left-to-right text processing. It predicts the next word in a sequence given the preceding words, which allows it to generate coherent and contextually relevant text. In additon to knowledge distillation, the compression of pretrained language models have been widely explored, ranging from unstructured pruning Gordon et al. (2020); Guo et al. (2019); attention head pruning Michel et al. (2019); to layer factorization Lan et al. (2019), quantization Zhang et al. (2020); Bai et al. (2020) and dynamic width/depth inference Hou et al. (2020). However, some of the techniques like weight pruning (irregular sparsity) and quantization typically require complex piplines and can not lead to inference speedup and run-time memory saving directly without dedicated hardware/libraries (e.g. for sparse or low-bit computing operation). By contrast, knowledge distillation has been found to be a simple and much effective model compression technique that allows a relatively simple model to perform tasks almost as accurately as a complex model. Moreover, it can be combined with other compression techniques (i.e., where the student model is a smaller, quantized, or pruned version of the teacher model) to further compress the pre-trained language models.

### Knowledge Distillation

Knowledge Distillation (KD) is a process that transferring knowledge from a large teacher model to a small student model. It was first proposed by Hinton et al. (2015) and then how to effectively transfer more knowledge has been explored by many subsequent works Romero et al. (2014); Ahn et al. (2019); Park et al. (2019); Tian et al. (2019); Tung and Mori (2019). The intermediate layers contain much richer representation, thus allow the student transformer to acquire more information in addition to outputs. Tang et al. (2019) distill fine-tuned BERT into an extremely small bidirectional LSTM. Turc et al. (2019) initialize the student with a small pre-trained LM during task-specific distillation. Sun et al. (2019) introduce the hidden states from every k layers of the teacher to perform knowledge distillation layer-to-layer. Aguilar et al. (2019) further introduce the knowledge of self-attention distributions and propose progressive and stacked distillation methods. Task-specific distillation requires to first fine-tune the large pre-trained LMs on downstream tasks and then perform knowledge transfer. The procedure of fine-tuning large pre-trained LMs is costly and time-consuming, especially for large datasets. MiniBERT Tsai et al. (2019) uses the soft target distributions for masked language modeling predictions to guide the training of the multilingual student model and shows its effectiveness on sequence labeling tasks. DistillBERT Sanh et al. (2019) uses the soft label and embedding outputs of the teacher to train the student. TinyBERT Jiao et al. (2019) and MOBILE-BERT Sun et al. (2019) further introduce self-attention distributions and hidden states to train the student. For example, MOBILE-BERT employs inverted bottleneck and bottleneck modules for teacher and student to make their hidden dimensions the same. TinyBERT uses a uniform-strategy to map the layers of teacher and student when they have different number of layers, and a linear matrix is introduced to transform the student hidden states to have the same dimensions as the teacher. However, the presence of those extra modules not only adds burden on network complexity but also complicates the training procedure.

## 3 Method

In this section, we first describe the transformer architectures and define the distillation target in Section 3.1. Then we introduce the two proposed types of feature relationships: token-level relation

Figure 1: Overview of the proposed method Feature Correlation Distillation (FCD). To demonstrate the effectiveness of FCD. We introduce the token-level relationship and sample-level relationship to distill the knowledge from teacher to student. The correlation loss based on pearson linear correlation is used to capture the relationship between the teacher and the student (best viewed in color).

in Section 3.2 and sample-level relation in Section 3.2. At last, we elaborate on the correlation-based loss in Section 3.3 and provide a theoretical analysis in Section 3.4.

### Preliminaries

The majority of pre-trained language models are based on Transformer architectures, which are composed of a stack of Transformer blocks. We first tokenize the input sample into a sequence of tokens and pack them together with special tokens such as [SEP] and [CLS]. These tokens are then projected into token embeddings and fed into transformer blocks. Each Transformer block consists of a Multi-Head Attention (MHA) and a Feed-Forward network (FFN). Layer normalization (LN) Ba et al. (2016) and residual connection He et al. (2016) are integrated around each of these two sub-blocks. Suppose a teacher model \(T\) and a student model \(S\), the model takes the feature \(_{l-1}\) of the \(l\)-th Transformer block as input, In multi-head attention, heads are computed in parallel to get the final output, which can be formulated as:

\[_{l} =(_{l-1},Q_{l},K_{l})\] \[_{l} =_{H}_{l}V_{l}\] (1)

\(_{l}^{H N N}\) is the attention matrix, where \(H\) denotes the number of heads and \(N\) the sequence length of the input. It is calculated as scaled dot-product between \(Q_{l}\) and \(K_{l}\) and then apply softmax operation on the each column of matrix \(_{l}\). The final multi-head attention output \(_{l}\) is calculated as a weighted sum of values \(V_{l}\). Suppose the two linear layers in FFN are parameterized by \(W_{1}\), \(b_{1}\) and \(W_{2}\), \(b_{2}\), the output of FFN can be formulated as:

\[_{l}=(_{l}W_{1}+b_{1})W_{2}+b_{2}\] (2)

We term \(_{l}^{N D}\) as the output feature of the \(l\)-th Transformer block, where \(D\) denotes the dimension of hidden features. Some works directly adopted \(_{l}\) as the distillation target. However, the embedding dimensions \(D_{S}\) and \(D_{T}\) of student and teacher are typically different. Previous works Romero et al. (2014), Yim et al. (2017), Heo et al. (2019) overcome this obstacle by building certain adaptation modules between hidden layers of the teacher and student models. However, these adaptation modules, with random initialization or special non-parameter transformation Srinivas and Fleuret (2018), Komodakis and Zagoruyko (2017) would potentially disturb training process, because it introduces extra parameters and computational cost (including weights, gradients and optimizer states) Pudjepdeli et al. (2020). Moreover, the teacher and student models usually have different number of heads, i.e., \(H_{T} H_{S}\). To address these challenges, we aim to model feature relationships using \(_{l}\) to overcome the aforementioned issues. The details of this process are provided in the following sections.

### Distillation with feature relationships

Token-level RelationTo mitigate the negative influences of magnitude differences, we first normalize the block features \(\) of both the teacher and the student, denoted as \(}=()\). Common

Figure 2: Our method proposes to maintain the token-level and sample-level relations between student and teacher. Token-level relation: relation between the tokens within each sample of teacher and student. Sample-level relation: relation between the samples on a specific token.

normalization methods include \(_{2}\), softmax and layer normalization. In our implementation, we choose \(_{2}\) normalization as its implementation, as it consistently outperforms the other methods in our experiments. Subsequently, with the normalized feature, we compute the token-level relation matrix. Specifically, for a specific sample \(i\), we define the relation matrix between tokens for each sample as follows:

\[_{t}()=_{i,:,:}}{||_{i,:,:}|| _{2}}_{i,:,:}}{||_{i,:,:}||_{2}} ^{Tr}\] (3)

Here, \(Tr\) denotes transposition for the feature. For each given sample, both the student's and teacher's token-level relation matrices share the same dimensions of \(_{t}^{N N}\). Each matrix effectively serves as a relevance map, revealing the influence of each token in relation to others within the same sequence (see the left part of Figure 2). By doing so, this inner token-level relationship is adept at capturing long-term dependencies between tokens, emphasizing those which are crucial for a comprehensive understanding of the linguistic context. Consider tasks such as text tagging or named entity recognition, where each token must be assigned a label based on its role within the sentence. In such cases, the token-level relation matrix can serve as an effective guide, assisting the student model to deliver enhanced performance.

Sample-level RelationIn addition to the token-level relation, which captures the relationships among different tokens within each sample, the relationships across multiple samples for each token also provides informative knowledge for the distillation process. As such, we aim to distill this sample-level relation as well to enhance performance. Similarly, we compute the relation matrix at the sample level as follows:

\[_{s}()=_{:,j,:}}{||_{:,j,:}|| _{2}}_{:,j,:}}{||_{:,j,:}||_{2}} ^{Tr}\] (4)

In this case, for a specific token, both the student's and teacher's sample-level relation matrices, denoted as \(_{s}\), share the same dimensions of \(^{B B}\). These matrices describe the relationships between different samples, as illustrated in the right part of Figure 2. They enable the capture of semantic relationships across a batch of samples, which is particularly essential for tasks like text classification and summarization. In these tasks, recognizing the sample-level relationship can be instrumental in making accurate predictions. It allows the model to discern the similarities and differences between samples, rather than viewing each sample in isolation. For the sample-level relation modeling, instead of directly comparing words based on their positions across sentences, we transform each sentence into a unified high-dimensional space using the same network. Within this space, token features at the same positions from distinct sequences become comparable. This approach forms the foundation of our sample-level relation modeling and serves as a valuable guide for the student model, enhancing its performance in tasks necessitating a deep understanding of inter-sample relationships.

Computation Complexity AnalysisThe token-level and sample-level relation maps, in our proposed method, require computational complexities of \(2BN^{2}D\) and \(2B^{2}ND\), respectively. The associated memory space required for these computations is \(BN^{2}+B^{2}N\). In contrast, MiniLM Wang et al. (2020) utilizes self-attention and value-value attention, results in total computational complexities and memory space requirements of \(4BN^{2}D\) and \(2BN^{2}\), respectively. MiniLMv2 Wang et al. (2020) employs a different attention mechanism, resulting in total computational complexities and memory space requirements of \(18BN^{2}D\) and \(9BN^{2}\), respectively. For a concrete example, consider the BERTBASE model, where \(B\) is set to 32, \(N\) is 128, and \(D\) is 768. Our proposed method yields computational complexities and memory space requirements of 0.6GFLOPs and 0.6MB, respectively. In comparison, MiniLM has computational complexities and memory space requirements of 3.2GFLOPs and 2MB, respectively, while for MiniLMv2, these values rise to 14.5GFLOPs and 9.4MB, respectively. Thus, our method demonstrates significant efficiency in terms of computational complexity and memory space requirement, providing a more resource-efficient option for knowledge distillation.

### Distillation with pearson linear correlation

In addition the distillation target, the design of the distillation loss plays a crucial role in transferring the knowledge from the teacher model to the student model. A general distillation loss can be expressed as:

\[=(_{T}(_{T}),_{S}(_{S}))\] (5)

Here, \(_{T}\) and \(_{S}\) denote the feature transformations for the teacher and student models, respectively. These transformations convert raw features into a form that is more conducive to knowledge transfer. In previous works, \(\) often takes the form of an adaptation layer that aligns the embedding dimensions between the teacher and student models. However, in our method, as discussed in Section 3.2, \(\) is used to denote the token-level and sample-level relation matrix. \(\) is a distance function measuring the similarity between student and teacher features. The most commonly used functions include KL divergence and the mean squared error. However, these functions exhibit an 'exact match' property, which means the loss reaches the minimal if and only if the features of student and teacher are all identical. This requirement could impose overly strict constraints on the student, particularly when there is a large discrepancy between the teacher and student. To alleviate these constraints, we introduce the Pearson correlation coefficient as a more flexible alternative, which is used to measure the strength of the linear relationship between two variables and remains invariant under positive linear transformations. The basic form of Pearson correlation coefficient between \(X\) and \(Y\) follows the Pearson index can be computed as follows:

\[(X,Y):=-_{X})(Y_{i}-_{Y})}{-_{X}) ^{2}}-_{Y})^{2}}}\] (6)

where \(_{X},_{Y}\) denote the mean value of the variable \(X,Y\) respectively. The Pearson correlation coefficient, denoted by \(\), ranges between -1 and 1. Therefore, \(1-\) always falls between 0 and 2, making it suitable to be used as a loss function. We call our final formulation of the distance function Pearson linear correlation (PLC), which is defined as follows:

\[(X,Y):=1-(X,Y).\] (7)

In this way, we shift the focus from attempting to exactly replicate the teacher's features to preserving and learning the relational information between the teacher's and student's features. This shift effectively relaxes the 'exact match' requirement inherent in conventional Knowledge Distillation (KD) methods. Thus, the distillation loss in our approach comprises two types of losses: token-level relation loss and sample-level relation loss. The token-level relation loss quantifies the discrepancy between the token-level relation matrices of the student and teacher models. It is defined as the average PLC between the student's and teacher's token-level relations across all samples in the batch:

\[_{t}:=_{i=1}^{B}(_{t}( _{S}),_{t}(_{T}))\] (8)

The sample-level relation loss quantifies the discrepancy between the sample-level relation matrices of the student and teacher models. It is defined as the average PLC between the student's and teacher's sample-level relations across all tokens in the sequence.

\[_{s}:=_{j=1}^{N}(_{s}( _{S}),_{s}(_{T}))\] (9)

The intrinsic sensitivity of the PLC to outliers necessitates normalization of features prior to calculating the distillation loss, thus ensuring a stable training process. Given these two components, the overall training loss of our proposed method consists of the task loss, token-level relation loss, and sample-level relation loss, which can be formulated as follows:

\[=_{g}+_{t}+_{s}\] (10)

Here, \(_{g}\) denotes the task training loss, while \(\), \(\) are weighting factors used to balance the task training loss and the relation losses.

### Theoretical Analysis

As discussed in Section 3.3, different distance functions like Kullback-Leibler (KL) divergence and Mean Squared Error (MSE) are commonly used to measure the similarity between the teacher and the student models. However, these functions have an 'exact match' property, which means that the distance is zero if and only if the student and teacher features are exactly the same. Thiscan be written as \((X,Y)=0\) when \(X=Y\). Unlike KL divergence or MSE, PLC is invariant under positive linear transformations. This means that even if a positive linear transformation is applied to one or both of the features, the correlation coefficient remains the same. In other words, \((X,Y)=0\) if \(Y= X+\), where \(>0\) and \(\) are constants.This property makes PLC a more flexible choice for knowledge distillation, as it allows the student model to learn from the teacher model in a less restrictive way. The detailed mathematical justification for this property is provided in the supplementary materials.

We next delve into the relationship between the Pearson linear correlation (PLC), Kullback-Leibler (KL) divergence and Mean Squared Error (MSE). For KL divergence, the normalized features are transformed into a probability distribution, and then we minimize the discrepancy between the softened probabilities of the teacher and student models.

\[_{KL}(}_{T},}_{S})=^{2}_{m} _{t}(}_{T};)(} _{T};)}{_{s}(}_{S};)}\] (11)

Here, \(\) denotes a temperature parameter used to adjust the softness degree in the distributions. Assuming the value of \(\) is significantly large compared to the magnitude of the normalized features, and \(}_{S},}_{T}\) are drawn from a standard normal distribution, we can derive the gradient of \(_{KL}\) with respect to the normalized feature \(}_{S}^{i}\) as follows:

\[_{KL}}{}_{S}}(}_{S}-}_{T})=_{MSE}}{}_{S}}\] (12)

Further, given that \(_{i}}_{S}^{2}=1\) and \(_{i}}_{T}^{2}=1\), we can reformulate the MSE loss as follows:

\[_{MSE}(}_{S},}_{T})= (}_{S}-}_{T})^{2} 1-(_{S},_{T})\] (13)

This equivalence clarifies the intrinsic connection between KL divergence, MSE, and PLC of normalized feature distributions. The detailed derivations and empirical results to support this claim are provided in the supplementary material.

## 4 Experiments

### Distillation Setup

We evaluate the efficacy of the proposed FCD on 8 out of 9 tasks from the General Language Understanding Evaluation (GLUE) Wang et al. (2019) benchmark, which consists of 2 single-sample

  
**Model** & **\#Params** & **Speedup** & **SST-2** & **MNLI-m** & **QNLI** & **QQP** & **RTE** & **SST-B** & **MRPC** & **CoLA** & **AVG** \\  BERT\({}_{}\) & 110M & \(\)1.0 & 93.4 & 84.5 & 91.5 & 72.3 & 66.8 & 85.2 & 88.3 & 52.8 & 79.3 \\ BERT\({}_{}\) & 66M & \(\)2.0 & 90.7 & 81.2 & 87.9 & 69.4 & 64.3 & 79.8 & 83.7 & 41.4 & 74.8 \\ BERT-Pred\({}_{}\) & 66M & \(\)2.0 & 92.0 & 81.5 & 89.0 & 70.7 & 65.5 & 81.6 & 85.0 & 43.5 & 76.1 \\ DistilBERT\({}_{6}\) & 66M & \(\)2.0 & 92.5 & 82.6 & 88.9 & 70.1 & 58.4 & 81.3 & 86.9 & 49.0 & 76.2 \\ TinyBERT\({}_{6}\) & 66M & \(\)2.0 & 92.1 & 82.8 & 89.8 & 71.2 & 70.0 & 83.9 & 88.0 & 51.1 & 78.6 \\ MiniLM\({}_{6}\) & 66M & \(\)2.0 & 92.0 & 83.0 & 91.1 & 71.4 & 70.8 & 84.2 & 88.5 & 49.2 & 78.8 \\ MiniLMV2\({}_{6}\) & 66M & \(\)2.0 & 92.4 & 83.4 & 90.0 & 71.5 & 71.3 & 84.5 & 88.6 & 51.8 & 79.2 \\
**Ours** & 66M & \(\)2.0 & **92.8** & **83.8** & **91.3** & **72.0** & **71.7** & **84.8** & **89.1** & **52.0** & **79.6** \\  RoBERT\({}_{}\) & 125M & \(\)1.0 & 95.3 & 87.2 & 93.2 & 73.8 & 72.7 & 88.4 & 90.1 & 62.0 & 82.8 \\ RoBERT\({}_{}\) & 82M & \(\)2.0 & 92.3 & 83.1 & 90.4 & 72.1 & 68.4 & 86.8 & 87.5 & 54.1 & 79.3 \\ MiniLMV2\({}_{6}\) & 82M & \(\)2.0 & 93.5 & 84.3 & 91.6 & 72.8 & 72.1 & 87.5 & 88.2 & 57.8 & 81.0 \\
**Ours** & 82M & \(\)2.0 & **93.8** & **85.6** & **92.0** & **73.5** & **72.5** & **88.3** & **89.6** & **60.3** & **81.9** \\  DistilGPT2 & 82M & \(\)2.3 & 90.7 & 81.6 & 87.9 & 66.8 & 68.3 & 79.6 & 87.9 & 39.4 & 75.3 \\
**Ours** & 82M & \(\)2.3 & **92.0** & **83.4** & **88.5** & **70.6** & **70.2** & **81.6** & **88.4** & **42.3** & **77.1** \\   

Table 1: Results of the proposed method on the test sets of GLUE. We use the metric of Matthews correlation for CoLA, Pearson-Spearman correlation for STS-B, and accuracy for other datasets. Following previous works Sun et al. (2019), we also report the average score of these eight tasks (the “AVG” column). The speedup is in terms of the BERT\({}_{}\) and RoBERT\({}_{}\) inference time and evaluated on a single GPU with a single input of 64 or 128 length. The fine-tuning results are an average of \(4\) runs.

(CoLA and SST-2) and 5 sample-pair (MRPC, QQP, MNLI, QNLI and RTE) classification tasks, and 1 regression task (STS-B). Following previous worksSun et al. (2019), we use the same metrics as the GLUE benchmark for evaluation. In order to verify the effectiveness and robustness of our method, we distill teacher models with different architectures, model sizes. Concretely, we consider encode-based models including BERTBASE, BERTLARGE, RoBERTBASE and RoBERTALARGE to ensure a fair comparison with of a wide range of prior works. Moreover, we also explore the compression of decoder-based models by employing our distillation method to improve fine-tuning of DistilGPT2 Wolf et al. (2020), which is rarely investigated in most previous works.

### Implementation Details

The process of distilling pretrained Language Models (LM) generally comprises two stages: task-agnostic distillation and task-specific distillation. Task-agnostic distillation involves a pre-training process on a large-scale dataset. However, this stage can be costly and time-consuming, particularly for scenarios with limited computational resources. In contrast, task-specfic LM distillation proves to be effective and considerably more economical compared to pre-training. As an increasing number of pretrained LM models are becoming publicly available through resources such as the HuggingFace Transformers library Wolf et al. (2020), directly leveraging these models can save substantial time and computational resources compared to training from scratch. Given these considerations, our focus in this work is on task-specific knowledge distillation. However, it is important to note that our method is not limited to task-specific distillation and can be readily applied in task-agnostic scenarios as well. Specifically, we first fine-tune the pretrained teacher models on a specific task. Following this, the corresponding student model is initialized with the teacher model using the LayerDropping method Sajjad et al. (2020). Subsequently, we perform distillation with FCD. We employ a grid search algorithm on the development set to tune the hyper-parameters. Specifically, we trained the student model for \(3\), \(5\) and \(10\) epochs, using a batch size of 32. The learning rates we experimented included \(2e-5\), \(1e-5\) and \(5e-6\). For the CoLA task, we extended the training steps to 25 epochs. The parameters \(\) and \(\) from the distillation loss are tuned from \(\{0.1,0.2,0.4,1\}\), a choice guided by maintaining the different components of the loss in the same order of magnitude. We adopt a cosine decay schedule with a warm-up phase of 5 epochs and utilize the AdamW optimizer with a weight decay of 0.5. The maximum sequence length is set to 64 for single-sample tasks, and 128 for sequence pair tasks.

### Main Results

We start by comparing our proposed method with several KD baselines, including DistilBERT Sanh et al. (2019), TinyBERT Jiao et al. (2019), BERT-PKD Sun et al. (2019), MiniLM Wang et al. (2020), MiniLMv2 Wang et al. (2020). Similar to previous studies, we distill a 12-layer base model into a 6-layer student model with only about 60% parameters and 2x inference speedup. In order to evaluate the impact of knowledge distillation, we also report the results of BERTSMALL6 and RoBERTaSMALL6. These smaller models are obtained using the LayerDropping method Sajjad et al. (2020), wherein the strategy of dropping the top layer has been demonstrated to be a strong baseline. Consequently, we drop the top 6 layers of the base model and fine-tune it without using knowledge distillation. For fair comparisons, we fine-tune the released models and evaluate the result on the test set of GLUE without resorting to data augmentation strategy Jiao et al. (2019). The results

Figure 3: Comparison of distillation results of student models using BASE-size and LARGE-size teachers (BERT and RoBERTa) shown in the left part. The right part shows ablation of token-level and sample-level relations.

from these 6-layer student models are summarized in Table 1. The top group of models denotes the uncased version of base-size BERT used as the teacher model.Notably, our model outperforms all compared models by a large margin. Similar trends are observed in the middle group, where base-size RoBERTa is utilized as the teacher model. Our model surpasses the MiniLMv2 by 0.4% accuracy on RTE, 0.5% F1 on MRPC, and 0.3% Spearman correlation on STS-B. Furthermore, we conduct experiments on a decode-base model DistilGPT2. In this setting, we employ the GPT-2 model with 12 layers and 768 hidden size as the teacher model. Noatably, our proposed method outperforms the original KD method by an average of 1%,, underlining the efficacy of our approach in decoder-based model. As illustrated in the left part of Figure 3, student models that are distilled from large-size teacher models achieve further improvements. Moreover, this performance gain increases with the capacity of teacher models, thereby demonstrating the effectiveness of our proposed method across different sizes of pretrained Transformer models.

### Ablation Studies

Effect of different componentsIn this study, we introduce two distinct types of relations: token-level and sample-level relations. To verify the effectiveness of each, we conduct experiments using \(_{t}\) and \(_{s}\) to investigate their respective influences on the student model. As depicted in the right part of Figure 3, each component within the distillation loss independently contributes to the enhancement of the final performance. Moreover, a further boost in performance is observed when these components are combined. We noticed that the performance degradation on SST-2 is more substantial compared to other tasks without token-level relation. We speculate that the token-level relation is particularly important for this single sentence binary classification task.

Effect of different distillation loss functionsHere, we compare our proposed Pearson Linear Correlation (PLC) with the Mean Squared Error (MSE) and Kullback-Leibler (KL) divergence, which are widely-used loss functions. To ensure a fair comparison, we tune the distillation loss weight for both MSE and KL. The comparative results across three tasks are presented in Table 2, which demonstrates that adopting Feature Correlation Distillation (FCD) with PLC consistently yields higher performance compared to the FCD combined with MSE and KL. This indicates that the more flexible Pearson correlation might serve as a more suitable metric for measuring relations within the FCD.

Effect of different layer selection strategiesApart from the type of knowledge used for distillation, the selection of layers significantly affects the overall performance of distillation. We study the impact of three distinct layer selection strategies: uniform, top, and bottom, and compare their respective performances. Specifically, we utilize BERTBASE as the teacher model and a \(6 768\) model as the student model. The number of selected layers is set to 6. The results are reported in Table 3. For BERTBASE, using the uniform layer selection strategy yields superior performance compared to the other strategies. This finding highlights the crucial role of both the head and tail layers of student models in the distillation process.

## 5 Conclusion

In this paper, we introduce Feature Correlation Distillation (FCD), a novel and effective method for distilling large Transformer-based Pretrained Language Models (PLMs). Our approach simultaneously models both token-level and sample-level relations derived from the features of the Transformer block. Moreover, we propose a correlation-based distillation loss to enhance the performance of the model distillation process. We also provide a theoretical interpretation of our proposed Pearson

   Method & MNLI-m & QQP & RTE & Average \\  FCD (KL) & 83.3 & 71.2 & 70.7 & 75.1 \\ FCD (MSE) & 83.6 & 71.6 & 71.2 & 75.5 \\ FCD (Pearson) & **83.8** & **72.0** & **71.7** & **75.8** \\   

Table 2: Comparison of using different distillation loss functions.

   Scheme & layers & MNLI-m & CoLA \\  Top & \{7,8,9,10,11,12\} & 82.9 & 50.7 \\ Uniform & \{2,4,6,8,10,12\} & 83.8 & 52.0 \\ Bottom & \{1,2,3,4,5,6\} & 83.3 & 51.5 \\   

Table 3: Distillation results with different layer selecting schemes.

linear correlation formulation, offering a deeper understanding of its underlying operation and implications. Through extensive experiments on the GLUE tasks, our distilled smaller language models consistently outperformed existing knowledge distillation methods across a variety of architectures while significantly reducing both the model size and inference time. With its simplicity and strong performance, we hope our approach can serve as a solid baseline for future research.