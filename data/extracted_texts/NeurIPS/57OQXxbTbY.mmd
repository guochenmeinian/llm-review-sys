# Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment

Teng Xiao\({}^{1}\), Yige Yuan\({}^{2}\), Huaisheng Zhu\({}^{1}\), Mingxiao Li\({}^{3}\), Vasant G Honavar\({}^{1}\)

\({}^{1}\)Artificial Intelligence Research Laboratory, Pennsylvania State University

\({}^{2}\)University of Chinese Academy of Sciences, \({}^{3}\)Tencent AI Lab

{tengxiao,hvz5312,vhonavar}@psu.edu

yuanyige923@gmail.com,mingxiaoli@tencent.com

###### Abstract

We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO ), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods. Code is available at https://github.com/tengxiao1/Cal-DPO.

## 1 Introduction

Aligning the behavior of large language models (LLMs) with human preferences is crucial for ensuring that the responses of a pretrained LLM are aligned with human or societal values and preferences . In recent years, reinforcement learning from human feedback (RLHF) has become a standard approach for fine-tuning language models based on human preferences. RLHF involves first fitting a reward signal from human preference data and then using reinforcement learning (RL) algorithms such as PPO to optimize language models to generate responses with high reward.

While RLHF shows impressive capabilities on diverse tasks ranging from programming to creative writing, its training process is unstable and complex . This potentially worsens the sample complexity and compromises efficient convergence. To address these issues, offline contrastive preference learning methods, which include DPO , IPO , and SLiC , have been proposed to replace RLHF with supervised learning on the preference data. These methods eliminate the need for explicit reward modeling by directly using the _likelihood_ of the policy to define an _implicit reward_ fitted to the preference data, and achieve notable efficiency and competitive performance .

While various contrastive preference learning methods employ different pairwise ranking losses, they share a common underlying motivation: Maximize the expected _relative_ difference between the implicit rewards associated with the chosen and rejected responses. Because the ranking loss is invariant to various score transformations (e.g., subtracting a constant), these methods tend to ignore the _absolute_ values of the rewards. Hence, while these methods learn to preserve the relative ordering between the likelihoods of the chosen and the rejected responses, they may reduce the likelihood of the chosen response. Figure 1 illustrates this behavior and its implications. In the case of DPO, a representative method of the contrastive methods, the likelihood of the chosen responsecounter-intuitively continues to decrease despite remaining higher than the likelihood of the rejected response. An undesirable consequence of this behavior is that the learned policy increases the likelihood of unknown out-of-distribution responses, resulting in poor performance. Maximizing the likelihood of the chosen response can be important in many practical applications, e.g., reasoning and mathematical problem solving [11; 12], limiting the applicability of contrastive preference learning.

The preceding discussion raises an important question with significant implications for how we align LLMs with human preferences: _How can we design a new objective that effectively alleviates this problem while ensuring that the learned policy theoretically converges to an optimal policy?_

Our answer to this question is \(}\), a simple yet effective framework for preference learning which optimizes the contrastive preference objective to maximize the relative differences between implicit rewards of chosen and rejected responses, while simultaneously ensuring that learned implicit rewards are calibrated to match the actual values of the ground-truth rewards (see Section 4.1 for a formal definition). The key intuition behind \(}\) is quite simple: If the implicit reward estimates from preference data are well-calibrated relative to the ground-truth rewards (meaning both lie on the same scale), we can prevent the likelihood (reward) of chosen responses from continually decreasing. Hence, \(}\) is designed to learn an implicit reward parameterized by the policy calibrated against the ground-truth reward. This can be achieved through a simple modification to the existing methods. For instance, \(}\) can be implemented on top of \(}\) with just one line of code and without any additional hyperparameters. Although we refer to our method as \(}\), it notably generalizes to other preference optimization methods such as IPO and \(}\) (see Section 4.3). In addition, we theoretically demonstrate that \(}\) possesses several properties that are desirable for fine-tuning LLMs based on preferences, such as mode-seeking behavior, negative preference optimization, or "negative gradient" to push down the likelihood of undesirable responses .

**The main contributions of this paper are:** (i) We propose \(}\), a simple, effective, and intuitive framework for preference learning that facilitates alignment of language models. \(}\) aims to learn implicit reward functions for learning policy that are calibrated with respect to ground-truth rewards. (ii) We theoretically analyze the learning behaviors of \(}\) and prove that \(}\) is guaranteed to yield an optimal policy for preference learning. (iii) We present results of extensive experiments on a range of benchmark tasks, including controlled text generation , summarization , dialogue generation , and several reasoning tasks  that demonstrate that \(}\) consistently outperforms previous alignment methods for preference fine-tuning.

## 2 Related Work

Reinforcement Learning from Human Feedback (RLHF) is highly effective in aligning Large Language Models (LLMs) with human preferences [2; 4]. In RLHF, a reward model is trained from human preference data to map responses to a scalar reward, which aligns a policy using RL algorithms like PPO . Although RLHF excels in instruction-following , safety alignment , and summarization , it requires a more complex training pipeline than supervised learning.

Recent work proposes simplifying RLHF by directly optimizing language models with contrastive learning on preference data, resulting in contrastive preference learning methods , such as DPO , IPO , and SLiC , NLHF , and their variants that incorporate rejection sampling,

Figure 1: The implicit reward dynamics during training of \(}\) and \(}\) on UltraFeedback data with the base model \(}\) reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in \(}\), the rewards for chosen data decrease below zero, whereas in our \(}\), they keep increasing and remain positive. Our \(}\)  significantly outperforms \(}\) across reasoning benchmarks. More results on other datasets are provided in Section 5.

e.g., RSO . While each of these methods works with different loss functions, the primary objective is to increase the likelihood gap between preferred and dispreferred responses . Other works [18; 19; 20; 21; 22] apply contrastive preference learning iteratively and on-policy. However, as shown in Figure 1, the likelihood of the preferred response frequently decreases during contrastive training, adversely impacting performance on tasks such as coding and mathematical question answering [11; 12]. This paper addresses this limitation by calibrating contrastive objectives for preference learning and supports our approach with extensive theoretical guarantees in practice.

There is a substantial body of work that analyzes contrastive preference learning methods from different perspectives. For instance, [23; 24] study the performance of DPO under distribution shift and show that DPO is more susceptible to out-of-distribution responses than PPO.  theoretically analyze DPO with noisy preferences. [26; 27; 28] revisit the training objective of DPO from the perspective of noise contrastive estimation .  theoretically analyze the gradient vector field of DPO and show that DPO decreases the probability of disfavored responses faster than it increases the probability of generating preferred responses. [31; 32; 33; 34] derive DPO within a token-level MDP formulation. Concurrent work  shows that DPO employs a negative gradient to push down the likelihood of undesirable, e.g., rejected responses, and implicitly exhibits a "mode-seeking" behavior. In this paper, our results are complementary. Specifically, we introduce Cal-DPO, which explicitly calibrates the rewards learned by DPO to match the scale of ground-truth rewards, while exhibiting "mode-seeking" behavior by minimizing the reverse KL divergence similar to RLHF.

Also worth mentioning is a body of work on calibration in supervised classification , learning-to-rank , unsupervised learning , and reinforcement learning [37; 38]. Recently, calibration has also been introduced into language models [39; 40]. In these works, calibration refers to the alignment of the model's assessed confidence scores with the likelihood of its responses being correct. In contrast, our focus is on calibrating the learned implicit rewards in contrastive preference learning to match ground-truth rewards in aligning language models with human preferences.

## 3 Notations and Preliminaries

**Problem Setup.** We consider the preference learning scenario as follows: let the text sequences \(=[x_{1},x_{2},]\) denote an input prompt, and \(_{w}=[y_{1},y_{2},]\) and \(_{l}=[y_{1},y_{2},]\) denote two responses, typically sampled from the reference policy \(_{}()\). The response pairs are then presented to human labelers (or an oracle) who express preferences for responses given the prompt, denoted as \(_{w}_{l}\), where \(_{w}\) and \(_{l}\) denote preferred and dispreferred responses, respectively. The preference distribution is commonly expressed using a latent reward model \(r(,)\) as:

\[p(_{w}_{l} x)=g(r(, _{w})-r(,_{l})),\] (1)

where \(g:\) is a monotone non-decreasing function (with \(g(z)=1-g(-z)\)) that converts reward differences into winning probabilities). When \(g\) is the sigmoid function \((x)=}\), we get the Bradley-Terry (BT) preference model . Given dataset \(\), containing feedback \((,_{w},_{l})\), the goal is to learn a LLM policy \(_{}()\) to align human preference by generating high rewards.

**RLHF.** Typically, given the reward function \(r(,)\), which dictates the human preferences, RLHF optimizes policy \(_{}\) for \(\) to maximize reward with the following RL objective:

\[_{_{}}_{_{}(|)}[r(,)]-_{}[_{ }()\|_{}( )],\] (2)

where \(>0\) is an appropriate KL penalty coefficient. Due to the discrete nature of language generation, we typically optimize the RLHF objective in Equation (2) using RL algorithms, such as PPO [2; 5]. Although RLHF with PPO has achieved remarkable success, the training process of PPO is unstable because of the high variance of the policy gradient estimation .

**Reward Modeling.** One standard approach to reward modeling is to fit a reward function \(r_{}(,)\) using the BT preference model in Equation (1). Specifically, the reward function \(r_{}(,)\) can be estimated by maximizing the log-likelihood of the preference feedback \((,_{w},_{l})\):

\[_{}(;,_{w},_{l})=- (r_{}(,_{w})-r_{}(, _{l})).\] (3)

**Contrastive Preference Learning.** To simplify RLHF, contrastive preference learning [42; 7; 9; 8] uses the log-likelihood of the learning policy to implicitly represent the reward function:

\[r_{}(,)=_{}( )-_{}()+ Z() .\] (4)With this parameterization, DPO  aims to optimize \(_{}\) based on the BT model in Equation (3):

\[_{}=-(( _{w})}{_{}(_{w} )}-(_{l} )}{_{}(_{l} )}).\] (5)

Technically, minimizing the DPO objective or any other pairwise contrastive objective, such as those of IPO  and SLiC , essentially amounts to maximizing the relative reward differences between chosen and rejected responses as shown in . However, as noted earlier, these pairwise ranking objectives are not scale-calibrated and ignore the absolute values of the rewards. Thus, the likelihood of the chosen response can continue to decrease during training as long as the relative difference in the likelihoods between the chosen and rejected responses remains large (see Figure 1). This property has resulted in suboptimal performance, especially on reasoning and mathematical problem-solving [11; 12]. In this paper, we address this limitation by proposing a simple yet effective calibrated objective to calibrate the behavior of contrastive preference learning methods such as DPO.

## 4 Calibrated Direct Preference Optimization

We proceed to introduce Calibrated Direct Preference Optimization (\(\) ), a simple and effective modification of DPO. The key intuition behind \(\) is to calibrate the implicit reward function against the ground-truth rewards. Hence, \(\) is designed to learn an implicit reward parameterized by the policy calibrated against the ground-truth reward. This can be achieved through a simple modification of DPO , assuming, as in the case of DPO, that the preferences adhere to the BT preference model. Thus, \(\) can be implemented on top of DPO with just one line of code and without any additional hyperparameters. In principle, our idea of calibration can be applied to any contrastive preference learning algorithm such as IPO  and SLiC [9; 17] (see Section 4.3).

### The Calibrated Objective for Preference Optimization

To fine-tune a policy on preference feedback, we propose learning a reward model implicitly and using the log-likelihood of the policy to represent the estimated reward [7; 9; 8]. Specifically, we start by defining a preference score of \(_{w}\) relative to \(_{l}\), where the implicit reward is represented by \(_{}\).

\[h_{}(,_{w},_{l})=_ {}(,_{w})-_{}(, _{l})(_{w} )}{_{}(_{w})}-(_{l})}{_{}(_{l} )}.\] (6)

Here \(_{}(,)= ()}{_{}()}\) is an implicit reward defined by the training and reference policies \(_{}\) and \(_{}\), allowing us to express that, for any \(\), the preference probabilities can be denoted as:

\[p_{}(_{w}_{l} x)=(h_ {}(,_{w},_{l})),\] (7)

where we assume that preferences adhere to the BT preference model as in DPO. With preference probabilities expressed in terms of the learning policy, we can find the maximum likelihood estimate by minimizing the preference optimization loss based on the preference feedback:

\[_{}(;,_{w},_{l})=- h_{}(,_{w},_{l} )=-(_{w} )}{_{}(_{w} )}-(_{l})}{ _{}(_{l})}.\] (8)

We can note that this pairwise loss is equivalent to the DPO objective (without \(\)) in Equation (5). As this contrastive pairwise loss is also not scale-calibrated, ignoring the absolute values of the rewards. Thus, we cannot guarantee that the estimated reward of the chosen response will increase and the reward of the rejected response will decrease, which tends to degrade the performance on math and reasoning benchmarks [11; 12; 43; 44]. We propose to address this limitation by explicitly constraining the implicit reward function \((_{}()/_{}( ))\) to a scale that matches the ground-truth reward \(r(,)/\). Intuitively, if the learned implicit rewards are constrained to lie in the range of the ground-truth reward, we can prevent the rewards of chosen responses from continually decreasing. Formally, we define "calibration" with respect to ground-truth reward as follows:

**Definition 1**.: _(Calibration). An estimated implicit reward \(()}{_{}( )}\) for the LM policy \(_{}\) is called scale calibrated with respect to the ground truth reward if \(()}{_{}( )}=,)}{},(, )\)._Thus, in addition to the BT preference loss in Equation (8), we propose constraining the learned implicit reward to match the ground-truth reward through solving squared loss regression problems:

\[_{}(;,)=()}{_{}()} -,)}{}^{2}.\] (9)

The calibration loss requires access to oracle rewards; however, in some scenarios, we may only have access to pairwise preference feedback. In such cases, we define the reward for preference feedback as follows: \(r(,_{w})=1/2\) and \(r(,_{i})=-1/2\), indicating \(_{w}_{i}\). Empirically, we find that this works quite well in practice. Combining this calibration loss with the BT preference loss in Equation (8), we get the following full loss of \(\) on human preference data:

\[_{}(;,_{w}, _{l})= -(_{w} )}{_{}(_{w})}-(_{l})}{_{}(_{l} )}\] (10) \[+(_{w})}{ _{}(_{w})}-^{2}+ (_{l})}{_{} (_{l})}+^{2},\]

where our modifications to the standard BT preference model are straightforward and depicted in blue. Intuitively, \(\) learns an implicit reward parameterized by the LM policy that is "calibrated" against the ground-truth reward. Specifically, \(\) attempts to push the reward of the chosen response toward \(1/2\) and the reward of the rejected response toward \(-1/2\), ensuring that \(_{}(_{w})>_{}(_{w} )\) and \(_{}(_{l})<_{}(_{l} )\). This alignment keeps the estimate of the implicit reward consistent with the ground-truth reward. Our implementation of \(\) builds directly on the \(\) codebase with just one additional line (see pseudocode in Appendix B.1). Our experiments show that calibration via a simple square loss can consistently improve the performance of off-the-shelf \(\), demonstrating the potential of calibration for preference fine-tuning.

### Theoretical Analysis

Next, we proceed to present a theoretical analysis of \(\). We show that \(\) with reward calibration enjoys important properties that are desirable for fine-tuning LLMs with preferences, e.g., mode-seeking behavior, negative preference optimization ("negative gradient" property) to push down the likelihood of undesirable responses . We also show that \(\) minimizes an upper bound on the standard KL-regularized RLHF in Equation (2). All proofs are provided in the Appendix 4.2.

We start by presenting our theoretical framework from a distribution matching perspective. We first interpret the RLHF objective as the optimization of a reverse KL-divergence between \(_{}\) and \(^{*}\):

\[_{}()=_{}[_{ }()\|^{*}()]-  Z(),\] (11)

where \(^{*}(|)_{}(|) (r(,)/)\). The derivation is given in Appendix A.1. Since reverse KL can be difficult to optimize [7; 10], one can instead optimize the forward KL divergence:

\[_{}()=_{}[^{*}( )\|_{}()]=- _{_{}(|)} ,)/)}{_{}_{}( |)(r(,)/)}_{}( |),\] (12)

which is the weighted maximum likelihood loss. Although this objective provides a straightforward approach for preference fine-tuning [45; 46; 47], it leads to poor performance compared to RLHF, as shown by . The poor performance of the preceding objective in Equation (12) relative to RLHF is due to the fact that it assigns a positive weight to all samples \(\) for a given \(\). Since the rewards are always positive, the likelihood loss always tries to increase the probability of a response even if it receives a much smaller reward compared to other responses. We note that the recent work of  also empirically demonstrates that the maximum likelihood criterion lacks the "negative gradient" property, resulting in poor performance of \(\) in Equation (12) compared to RLHF and \(\).

Because KL-regularized RLHF and the \(\) objective in Equation (12) are both population losses which involve the 'oracle' reward \(r(,)\), we also define a population loss for \(\) to facilitate a direct comparison between \(\) with KL-regularized RLHF and the \(\) loss of Equation(12):

\[_{}()= -_{_{}(|)} ,)/)}{_{}_{ }(|)(r(,)/)} (|)/_{}(|) )}{_{}_{}(|)(_{}( |)/_{}(|))}\] \[+_{_{}(|)} (()}{_{}( )}-,)}{})^{2},\] (13)

[MISSING_PAGE_FAIL:6]

### Generalizations and Extensions

We observe that our approach to calibrating the learned implicit reward from preference feedback is not limited to the DPO algorithm or the BT preference model. As we shall see below, the general idea behind Cal-DPO extends to other methods such as IPO and SLiC and other preference models. Instead of the sigmoid loss in Equation (7), SLiC[9; 17] minimizes a pairwise hinge loss:

\[_{}()=\{0,1- h_{}(,_{w},_{l})\}.\] (16)

IPO is a contrastive algorithm similar to DPO and minimizes the following pairwise square loss:

\[_{}()=(h_{}(,_{w },_{l})-1/2)^{2}\] (17)

A potential advantage of SLiC and IPO over DPO is that they do not require that the preference model be BT and can work with general preference probabilities. By combining our calibration loss in Equation (9), it is straightforward to define the calibrated counterparts of both \(_{}\) and \(_{}\). Thus, our calibration approach can be generalized to work with SLiC and IPO as well (see Section 5.5).

## 5 Experiments

### Experimental Setup

**Datasets.** We evaluate Cal-DPO on four widely used datasets for preference fine-tuning: the UltraFeedback Binarized dataset [53; 54], Reddit TL;DR summarization dataset , Anthropic-HH dataset , and the IMDb sentiment dataset . Details of the datasets are provided in Appendix C.1.

**Tasks and Evaluation.** Following previous work [7; 54], we evaluate methods fine-tuned on the UltraFeedback Binarized dataset across general reasoning benchmarks (MMLU-PRO , ARC , IFEval , BBH , GPQA ), and mathematical reasoning (GSM8K  and MATH ). For training on the UltraFeedback Binarized dataset, we utilize the same chat template used in  for all methods. We also use AlpacaEval 2.0, a benchmark for assessing LLM alignment with human preference. The Anthropic HH dataset is used for dialogue generation to produce helpful and harmless responses . For summarization, we use the Reddit TL;DR dataset. For the dialogue generation and summarization tasks, we use GPT-4 for zero-shot pairwise evaluation, which is consistent with human judgments (see prompts in Appendix C.2.1). In the IMDb sentiment dataset, the goal is controlled text generation to produce positive sentiments from movie review prefixes . We train a binary sentiment classifier and define the oracle reward as its log odds, and evaluate the policy on the trained reward model . The task and evaluation details are given in Appendix C.2.

**Models.** For summarization and dialogue generation tasks, we use Pythia-2.8b as our base model and the model after SFT as the reference model following . For the IMDb controlled text generation task, both the policy and reward models are initialized from the GPT-2 Large model . For tasks with the UltraFeedback Binarized dataset, we use the Zephyr-7b-sft model  as our base model to ensure alignment with previous work on LLM preference alignment .

**Baselines** We compare Cal-DPO with the following preference optimization methods: DPO , IPO , SLiC , CPO . We also compare Cal-DPO with other variants of DPO: f-DPO , DPO-Positive (DPO)  and DPO+NLL  which combine DPO loss over preference pairs and the negative log-likelihood (NLL) loss over chosen responses. We also compare with weighted MLE in Equation (12). Besides DPO, we also implemented our calibration objective on top of SLiC  and IPO  (see Section 5.5). The implementation details are provided in Appendix B.1.

### Performance Comparison on Benchmarks

**Reasoning Benchmarks.** Table 2 compares the performance of Cal-DPO against other alignment methods on the UltraFeedback Binarized dataset. Our results show that Cal-DPO exhibits remarkable effectiveness in enhancing DPO's performance. The improvements are particularly notable on the IFEval and Math benchmarks, with relative gains exceeding 63.1% and 12.5% compared to the best baseline, respectively. These findings underscore the efficacy of Cal-DPO. We hypothesize that these improvements can be attributed to the calibration of implicit rewards performed by Cal-DPO as part of its training objective. Without proper calibration, the likelihood of selected samples decreases, resulting in suboptimal performance, especially in mathematical reasoning tasks, where the chosenresponses are very likely the ground-truth answers. Furthermore, Cal-DPO outperforms CPO, which combine DPO over preference pairs with the negative log-likelihood loss over the chosen response. We hypothesize that the superiority of Cal-DPO over CPO can be attributed to the conservative nature of the objectives optimized by the latter, which only affects the chosen response. In contrast, Cal-DPO calibrates implicit rewards for both chosen and rejected responses.

**Instruction-following Benchmarks.** To assess the ability of Cal-DPO on align with human instruction, we compare the performance of Cal-DPO and DPO on AlpacaEval 2.0 , an evaluator based on GPT-4 (version gpt-4-1106-preview) that produces the probability of preferring the evaluated model. Figure 2 shows the comparison in terms of both normal and length-controlled (LC) percentage of wins. We see that Cal-DPO demonstrates steady performance gains with the number of training iterations and outperforms SFT and DPO methods, which tend to produce longer responses.

### Performance Comparison with Human Preferences

We also designed experiments to explore learning from real human preferences, focusing on summarization and dialogue generation tasks. Specifically, we used the Reddit TL;DR dataset for summarization and the Anthropic-HH dataset for dialogue generation. Table 3 summarizes the GPT-4 evaluation results. These results show that Cal-DPO demonstrates a notable improvement over DPO and its variants compared to both the SFT and the chosen responses. Remarkably, Cal-DPO aligns better with human preferences than baselines, achieving win rates of at least 60% against the chosen responses. This indicates that Cal-DPO shows strong promise in terms of aligning with human preferences. Additionally, we provide examples generated by both DPO and Cal-DPO for both tasks in Appendix D.1. These examples show that GPT-4 consistently prefers Cal-DPO over baselines and the chosen responses in the dataset, demonstrating that Cal-DPO significantly improves DPO in terms of helpfulness and harmlessness of the generated responses.

  
**Method** & **Reward \(\)** & **Perplexity \(\)** \\  SFT & 0.539 & 35.47 \\ PPO & 0.626 & 35.05 \\  DPO & 0.617 & 34.21 \\ f-DPO & 0.615 & 36.39 \\ DPOP & 0.632 & 35.58 \\ DPO+NLL & 0.627 & 34.08 \\  Cal-DPO & \(\) & \(\) \\   

Table 4: The comparison on IMDb dataset in terms of the reward and perplexity.

  
**Method** (\(\)) / **Dataset** (\(\)) & **MMLU-PRO** & **IFEval** & **BBH** & **GPQA** & **MATH** & **GSM8K** & **ARC** \\  zephyr-7b-sft-full  & 27.64 & 3.21 & 41.09 & 29.36 & 2.04 & 28.13 & 58.28 \\  DPO  & 26.73 & 10.49 & 43.27 & 28.44 & 1.36 & 21.76 & 61.26 \\ f-DPO  & 25.96 & 11.05 & 42.39 & 28.05 & 1.27 & 23.18 & 62.01 \\ SLiC  & 26.52 & 12.45 & 42.33 & 27.93 & 1.38 & 33.74 & 55.38 \\ IPO  & 25.87 & 11.52 & 40.59 & 28.15 & 1.25 & 27.14 & 60.84 \\ CPO  & 27.04 & 13.32 & 42.05 & 28.45 & 2.15 & 33.06 & 57.00 \\  Cal-DPO & **28.38** & **21.72** & **43.55** & **29.78** & **2.42** & **34.87** & **63.23** \\   

Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).

  
**Dataset** (\(\)) &  &  \\ 
**Method** (\(\)) / **Metric** (\(\)) & **vs SFT** & **vs Chosen** & **Average** & **vs SFT** & **vs Chosen** & **Average** \\  DPO  & 71.22 & 57.58 & 64.40 & 69.32 & 59.35 & 64.34 \\ SLiC  & 68.61 & 55.72 & 62.17 & 65.52 & 57.71 & 61.62 \\ IPO  & 72.17 & 56.51 & 64.34 & 63.19 & 55.12 & 59.16 \\ CPO  & 73.13 & 58.89 & 66.01 & 72.30 & 63.39 & 67.86 \\  f-DPO  & 66.19 & 51.37 & 58.78 & 60.21 & 52.38 & 56.30 \\ DPO  & 72.95 & 58.82 & 65.89 & 68.77 & 57.91 & 63.34 \\ DPO+NLL  & 69.37 & 55.26 & 62.31 & 65.34 & 55.28 & 60.31 \\  Cal-DPO & **75.61** & **59.37** & **67.49** & **73.52** & **64.61** & **69.07** \\   

Table 3: Win rates computed by GPT-4 against the SFT generated texts and the chosen texts on the TL;DR summarization and Anthropic-HH datasets. Best results s are highlighted in **boldface**.

### Performance Comparison on Controlled Evaluation

We conducted experiments on the IMDB dataset to assess the generation of positive movie reviews. The task requires the model to provide positive and fluent completions of movie reviews based on given partial input texts. To perform a controlled evaluation, we trained a binary sentiment classifier on the IMDB dataset and defined the oracle reward as its log odds, following . The reward score of the reward model then serves as an in-domain proxy for the unknown ground-truth reward used in evaluation. The results of IMDB sentiment generation are listed in Table 4. We used the reward score of the reward model and the perplexity of GPT-2  to assess alignment performance. We observe that (1) PPO, DPO, and Cal-DPO can align the SFT model with the preference of the reward model, as evidenced by the increasing reward score, and (2) Cal-DPO performs better in terms of reward score and perplexity than both PPO and DPO, which confirms our theoretical results that the policy trained by Cal-DPO can effectively maximize rewards.

### Further Analysis

**Training Dynamics.** We also investigated the reward patterns during the training process of Cal-DPO. Figure 3 presents the reward patterns for Cal-DPO and DPO on the TL;DR summarization dataset. We observe that the rewards of the rejected data keep decreasing, and the margins between the chosen and rejected responses keep increasing. However, in the case of DPO, the rewards of the chosen responses fall below zero, whereas they continue to increase with Cal-DPO, underscoring the utility of reward calibration in LLM alignment. These results are similar to those on the UltraFeedback dataset shown in Figure 1, verifying our motivation and the effectiveness of Cal-DPO.

**Generalization to other objectives.** As mentioned in Section 4.3, our approach to calibrating the learned implicit reward from preference feedback generalizes in a straightforward manner to other pairwise preference optimization methods including IPO and SLiC. To show this, we implemented the calibration objective of Cal-DPO for IPO and SLiC, yielding their calibrated counterparts Cal-IPO and Cal-SLiC, respectively. Figure 3 shows the comparison on the Anthropic-HH dataset. We observe that combining our calibration objective can consistently improve standard OCPL methods for LLM preference alignment, demonstrating the broader utility of preferance calibration.

**Coefficient Parameter.** We investigate the effect of the coefficient \(\) and present an ablation study to analyze the performance of Cal-DPO on various tasks by varying \(\). Figure 4 in the Appendix shows the performance with different values of \(\). We observe that \(\) plays an important role in Cal-DPO. A small \(\) typically improves the model performance, whereas a too large \(\) encourages the policy to remain close to the reference policy, leading to poor performance.

## 6 Conclusion and Limitations

We have presented Cal-DPO, a simple yet effective fine-tuning approach for aligning LLMs with human preference. Cal-DPO incorporates a simple preference calibration term that modifies the

Figure 3: (Left two) The training dynamics of DPO and Cal-DPO on the TL;DR Summarization dataset. (Right) The performance of SLiC and IPO, and their calibrated counterparts Cal-IPO and Cal-SLiC. We provide additional results on the Anthropic-HH and IMDb datasets in Appendix D.

Figure 2: AlpacaEval 2.0 evaluation results of models trained with UltraFeedback Binarized dataset. The DPO and Cal-DPO are both initialized from the SFT model 2ephyr-7b-sft-full.

behavior of the objective of contrastive preference learning methods such as DPO. The key idea behind Cal-DPO is to ensure that the learned implicit rewards lie within the same range as the ground-truth rewards, yielding substantial gains in performance relative to the state-of-the-art baselines on several widely used benchmark data sets. We also demonstrate theoretically that Cal-DPO exhibits the desirable "negative gradients" and "mode-seeking" behavior.

A limitation of Cal-DPO is that it is currently limited to offline methods and does not consider on-policy learning where the policy can interact with the reward model during learning. It would be interesting to explore how the reward calibration idea used in Cal-DPO performs in the on-policy learning scenario. We leave this as an interesting direction for future work.