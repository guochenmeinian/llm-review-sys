# SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM

Ming Nie\({}^{1}\)  Dan Ding\({}^{1}\)  Chunwei Wang\({}^{2}\)  Yuanfan Guo\({}^{2}\)  Jianhua Han\({}^{2}\)  Hang Xu\({}^{2}\)  Li Zhang\({}^{1}\)

\({}^{1}\)School of Data Science, Fudan University \({}^{2}\)Noah's Ark Lab, Huawei

https://github.com/fudan-zvg/SlowFocus

Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author.

###### Abstract

Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (_i.e._, a sufficient number of tokens per frame) and comprehensive video-level temporal information (_i.e._, an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.

## 1 Introduction

Large language models (LLMs) have garnered significant attention due to their exceptional text understanding capabilities. Building on the strengths of LLMs, video large language models (Vid-LLMs)  adapt them to the video modality, extending their reasoning and interactive skills to video data. By training on video-level tasks like captioning and question answering, they establish coarse-grained video-language correspondence and acquire the capabilities of video understanding. While Vid-LLMs have demonstrated the promising performance in video understanding, they still face a significant challenge. To embed video features into LLMs under computing resource constraints, Vid-LLMs typically need to sparsely sample the original video (_e.g._, retaining one frame every second) into a collection of low-frequency frames. Subsequently, the token number of each frame are also compressed through a visual adapter like average pooling  or Q-former . This leads to a dilemma: Vid-LLMs have to choose between compromised frame-level features and video-level features, each resulting in a reduction of video information. As illustrated in Figure 1 (a), under the premise of a constant total token number, we investigate the performance of Vid-LLMs bydynamically adjusting the sampling frequency and frame token number. The results clearly show that both low sampling frequency and low frame token number lead to performance degradation. Low-frequency sampling results in a sparse image collection as input, omitting crucial temporal details. On the other hand, excessive frame-feature compression degrades the semantic and spatial contexts of each frame. When confronted with fine-grained video understanding tasks, this limitation becomes significantly evident, as depicted in Figure 1 (b). Existing models (_e.g._, LLaMA-VID ) often overlook crucial details early in the input due to low-frequency sampling, leading to inaccuracy.

To address this challenge, we introduce SlowFocus, which is designed to pinpoint relevant temporal segments in response to questions, and subsequently maintains high-quality temporal details to enrich fine-grained video comprehension. For video understanding tasks, we assume that the relevant details are concentrated within one or several clips. SlowFocus initially identifies these segments based on the provided questions. It then densely samples the segmented temporal clips at a high-frequency to extract local temporal features highly pertinent to the questions. To effectively model the temporal relationships between frames and capture inter-frame contexts, we propose a specialized temporal encoder and a multi-frequency mixing attention module for enhanced temporal comprehension.

To enhance Vid-LLMs' ability to perform SlowFocus based on mixed frequencies, we propose a set of training and inference strategies to improve their temporal localization and fine-grained temporal reasoning capability. Following VTimeLLM , we fine-tune our Vid-LLM in the second stage on dense video captioning and temporal grounding tasks. This process enhances the model's ability to predict discrete bins that define the relevant temporal segments. In the third stage, we adapt the Vid-LLM to the SlowFocus mechanism for high-quality, fine-grained temporal-related tasks, which enables our model to reason precisely based on high-frequency temporal details.

In addition to the scarcity of methods for fine-grained video understanding, existing benchmarks [37; 40] also fall short in providing adequate challenges for specific temporal-related tasks. To bridge this gap and evaluate our proposed framework, we introduce FineAction-CGR, a newly dedicated benchmark that focuses on fine-grained video understanding, especially reasoning tasks based on temporal details. Our method demonstrates superior performance on the proposed benchmark, offering a promising solution to high-quality video understanding.

The contributions of this paper are summarized as follows: **(i)** We introduce SlowFocus, a straightforward yet effective framework designed to resolve the prevalent trade-off in existing Vid-LLMs between capturing limited frame-level details and overarching video-level contexts. SlowFocus adeptly maintains high-frequency local details alongside low-frequency global contexts, facilitating the identification of pertinent temporal segments and precise reasoning on video contents. **(ii)** We present a novel training strategy specifically designed to enhance the temporal localization abilities of Vid-LLMs, and seamlessly adapt them to our newly proposed SlowFocus approach. **(iii)** We establish a comprehensive new benchmark and carry out extensive experiments to rigorously assess the fine-grained video understanding capabilities of Vid-LLMs. The empirical evidence strongly

Figure 1: (a) Trade-off between video sampling frequency and frame token number. The horizontal axis represents the ratio (log-transformed) of these two factors. Each curve corresponds to a fixed total number of tokens (_e.g._, 256 for a 1-minute video). (b) Deficiency of existing Vid-LLMs, such as LLaMA-VID, when facing fine-grained video understanding, and the efficacy of our approach.

indicates that our SlowFocus approach significantly outperforms existing models, particularly in tasks requiring detailed temporal understanding and reasoning within videos.

## 2 Related works

**Vision large language models.** Researchers have made significant efforts to enable Large Language Models(LLMs) to comprehend visual information. BLIP-2  aligns vision-language representation with a lightweight Querying Transformer by concept of Q-Former. MiniGPT-4  aligns detailed image descriptions with advanced LLM which significantly enhances its multi-modal abilities. Exploring diverse multi-modal instruction-following data, LLaVA  has demonstrated impressive multi-model chat abilities. Recent LLMs, like Kosmos-2  and VisionLLM , probed into more specific aspects of image comprehension, including referring and grounding , remarkably enhancing the capability to describe intricate image details.

**Video large language models.** The exploration of LLMs' potential has been extended from images to videos, which contributes to emergence of Video LLMs. VideoChat  combines fine-tuning and LLM-based video agents and is fine-tuned using a specially designed video-centric instruction dataset. Video-ChatGPT  proposes a novel human assisted and semi-automatic annotation framework for generation high quality instruction data for videos. Video-LLaMA , trained on vision-language branch and audio-language branch separately with same visual data and process, has demonstrated impressive abilities in understanding both visual and auditory content in videos. Video-LLaVA  learns united visual representation by alignment before projection and conducts joint training on images and videos simultaneously. With efforts, Video LLMs have exhibited powerful task-handling capabilities in downstream tasks, like text-video retrieval and video captioning. However, these models remain limited in ability to comprehend fine-grained content. To solve this problem, we introduce a model with powerful capability of fine-grained video understanding.

**Fine-grained video understanding.** Comprehending videos in fine-grained aspects requires precisely locating and understanding specific events within a video. It is roughly divided by previous works  into temporal grounding [2; 7] and dense video captioning [13; 31] tasks. Temporal grounding demands the model to precisely identify start and end timestamps of video segment according to a given text query. Dense video captioning requires both temporal localization and captioning for all events within an untrimmed video. Both tasks are constrained by the labor-intensive nature of annotation, resulting in relatively small datasets. Moreover, fine-grained video understanding demands a deep comprehension of how objects change and interact over time, particularly at a detailed level. This complexity necessitates temporal reasoning tasks to effectively analyze these dynamics. However, current research in this field falls short of adequately addressing these requirements. To solve this problem, our proposed large-scale dataset provides mass temporal annotations and captions in different dimensions, supplying a wealth of information for training in temporal video grounding and reasoning tasks.

## 3 Method

In this section, we begin with a preliminary overview of video LLMs (Vid-LLMs) in Section 3.1. Following that, we offer a comprehensive explanation of our proposed SlowFocus in Section 3.2, followed by a detailed introduction to our innovative training strategy in Section 3.3.

### Preliminary

Contemporary Vid-LLMs typically feature a modular architecture, which includes a visual encoder \(E_{V}\), a series of visual adapters \(Q\), and a large language model \(L\). For a given video \(V=\{V(t)^{H W 3}|t=0,...,T\}\) that consists of \(T\) frames, along with its associated question \(q\), Vid-LLMs generally perform downsampling on the original video \(V\) at a fixed interval \(M_{L}\). This process results in sparsely distributed frames \(V_{L}\):

\[V_{L}=\{V(M_{L}t)^{H W 3}|t=0,...,T\},\] (1)

where \(M_{L} 1\). Consequently, only a very limited number of frames (\(T/M_{L} T\)) are selected as the actual input for the Vid-LLM, a technique we refer to as low-frequency sampling in this study.

Subsequently, the visual encoder processes the downsampled frames \(V_{L}\) and encodes them into a series of visual tokens denoted as \(z_{L}=E_{V}(V_{L})\). These visual tokens are then transformed to align with the embedding space of the language model through the visual adapter \(Q\), resulting in aligned visual tokens \(h_{L}=Q(z_{L})\). Concurrently, the input text query \(q\) is encoded into linguistic tokens \(h_{q}\) by the textual encoder. These visual and text tokens are concatenated into a unified sequence \([h_{L},h_{q}]\), which then serves as the input for the decoder component of the large language model \(L\). The model leverages this combined representation to infer the appropriate answer \(ans=L([h_{L},h_{q}])\), showcasing its ability to perform cross-modal reasoning and respond to human queries.

Although this paradigm is well developed in the field of video understanding, it encounters significant limitations in tasks requiring fine-grained temporal reasoning. By retaining only a small, discontinuous subset of frames (\(t\), where \(t T\)), substantial information loss can occur, as depicted in Figure 1 (b). Fine-grained temporal video understanding demands that the model concentrate on one or more specific temporal intervals, which could be potentially brief. However, the low-frequency sampling method predominantly captures overarching information while neglecting crucial local details, leading to inaccuracies such as incorrect responses or hallucinations. To address these shortcomings, we develop the SlowFocus strategy.

### SlowFocus

To improve Vid-LLMs with the SlowFocus mechanism, we first expand the traditional training and inference paradigm by introducing Mixed-Frequency Sampling (MFS). Subsequently, we explicitly model the temporal relationships among the sampled frames. Finally, we enhance the visual tokens with our newly proposed multiple-frequency mixing attention, designed to capture long-term contexts. We now provide a detailed elaboration on each of these components.

**Relevant segment grounding.** To better mimic human cognition in our Vid-LLM, we transform its inference paradigm into a multi-round dialogue format that integrates both the query and temporal awareness elements. The methodology of this transformation is illustrated in Figure 2. Initially, we sample the entire video at a fixed interval to capture the low-frequency frames \(V_{L}\), which are then encoded into visual embeddings \(h_{L}\). Following that, we reformat the original question \(q\) into temporal grounding questions \(q_{1}\), such as "<_video>\( n\)Please provide the temporal segment help to reason the question: <question>_", where <_question>_ refers to the original question \(q\).

Figure 2: The framework of SlowFocus. We initially identify the relevant temporal segments based on the given query. Subsequently the high-frequency sampling is performed on these segmented clips. Combined with low-frequency sampling across the entire video, our SlowFocus mechanism maintains mixed-frequency visual tokens to accurately answer the query.

With the augmented question \(q_{1}\), we direct the Vid-LLM to identify the relevant temporal segments \(\) within the video that are pertinent to the original question \(q\). This is accomplished by providing the model with low-frequency frames containing global information:

\[=L([h_{L},h_{q_{1}}]),\] (2)

where \(h_{q_{1}}\) represents the textual embedding of question \(q_{1}\) and \([0,T]\). The identified segment \(\) is designed to encompass query-related details that are crucial for addressing the question \(q\).

**Mixed-frequency sampling.** Subsequently, we perform dense sampling on the identified segment \(\) to obtain high-frequency frames \(V_{H}\):

\[V_{H}=\{V(M_{H}t)^{H W 3}|t\},M_{H}=max(},1).\] (3)

We dynamically adjust the sampling interval \(M_{H}\) based on the temporal length of segment \(||\) and to ensure an adequate number of samples \(N_{H}\) are taken from the relevant segment. The densely sampled frames \(V_{H}\) are then encoded into high-frequency visual tokens \(h_{H}\) by visual encoder.

With the inclusion of local details from high-frequency frames, we augment the initial question \(q\) with prompts, such as "_Additional temporal clues to focus on:..._", to form \(q_{2}\), as illustrated in Figure 2. We then combine the mixed-frequency visual tokens to predict the final answer:

\[ans=L([h_{L},(h_{L},h_{H}),h_{q_{2}}]),\] (4)

where \(\) is the multiple-frequency mixing attention, which will be detailed subsequently.

**Multiple-frequency mixing attention.** Because of the roughness of the low-frequency and the detailed specificity of the high-frequency visual features, simply concatenating these two directly into the language model may not yield optimal results. Fine-grained temporal understanding often requires modeling relationships between multiple events, necessitating the integration of global contexts into high-frequency local features. From this perspective, we introduce the Multiple-frequency Mixing Attention (MMA):

\[(h_{L},h_{H})=softmax(h_{T}^{T}}{})h_{L},\] (5)

where, for simplicity, we omit the process of applying FFN to \(h_{L}\) and \(h_{H}\). The resulting output is then fed into LLM to predict the textual response, as described in Equation 4.

**Temporal relationship modeling.** While LLM can implicitly capture temporal relationships among input visual embeddings based on their sequential positioning, it faces difficulties when the visual tokens are not uniformly distributed along the timeline. To address this challenge, we propose a temporal encoder that encodes the relative positions of frames into a set of discretized temporal

Figure 3: The training strategy of SlowFocus, including data distribution and parameter updating in each stage. <image> and <video> denote the tokens for image and video, respectively.

tokens \(^{N C}\), where \(N\) represents the discrete temporal space. For frames sampled at multiple frequencies \(V(t)\), the corresponding temporal token is \(_{i}\), where \(i= N*t/T\). These temporal embeddings are then incorporated into the visual tokens by directly adding them to the frame features.

### Training strategy

Considering training efficiency, we introduce a novel training procedure which is structured into three distinct phases in this work: (i) pre-training for modality alignment, (ii) fine-tuning for enhancing temporal grounding, and (iii) SlowFocus adaption, as illustrated in Figure 3. We now provide detailed elaborations on the training strategies and datasets employed for each of these stages.

**Modality alignment.** In the pre-training stage, our primary focus is to optimize the visual adapters and temporal encoders, while the visual encoder and language model are frozen to ensure that the visual features align effectively with the language space. Following the approach used in LLaMA-VID , we utilize the image-text LCS-558K dataset from LLaVA , along with 232K video-caption samples from the WebVid 2.5M .

**Boundary enhancement.** After the pre-training stage, the Vid-LLM gains proficiency in processing visual information. During the fine-tuning stage, we focus on enhancing the model's ability to comprehend sequences of video frames, thereby improving its temporal localization capabilities. In alignment with practices from VTimeLLM , we employ the InternVid-10M-FLT dataset , which is specifically designed for temporal-awareness training. The tasks within this dataset include: (i) dense video captioning, which requires detailed descriptions of all events along with their corresponding timestamps, and (ii) segment captioning and temporal video grounding, which involve generating descriptions based on timestamps or determining timestamps based on descriptions. Throughout this stage, we continue to train the visual adapters and temporal encoders. Additionally, the LLM is further trained using LoRA  to refine its capabilities.

**SlowFocus adaption.** Following the fine-tuning stage, our model has demonstrated the ability to comprehensively understand activities within the video and accurately align them with their respective timestamps. In the final stage, to further enhance multi-modality comprehension and integration with the MMF mechanism, we construct an instruction-tuning dataset using samples from ActivityNet Captions  and FineAction . We transform the annotations into over 100K high-quality QA dialogues, which will be detailed in the subsequent section. In alignment with our SlowFocus mechanism, during training with fine-grained captioning and reasoning tasks, the model is provided with the ground-truth temporal segments \(_{GT}\) and subjected to mixed-frequency sampling. In this

Figure 4: Pipeline of instruction-following data generation. Split the filtered FineAction videos into clips and extract time segments. Use the fine-tuned Video Recaptioner Model and GPT-4V to generate captions for both video clips and full videos. Integrate the ground truth data, new captions, and time segments to create comprehensive annotations. Finally, generate QA pairs for various tasks using different prompts via GPT-4.

stage, we freeze all the visual adapters and attention modules, only training the language model as well as the MMA module. The parameters of visual encoder are frozen all over the stages.

## 4 FineAction-CGR benchmark

In addition to the scarcity of previous methods for fine-grained video understanding, existing benchmarks [37; 40] do not adequately challenge specific temporal-related tasks. To address this gap and evaluate our proposed framework, we introduce FineAction-CGR, a comprehensive and high-quality instruction-following benchmark designed for evaluating the capabilities of Video LLMs in fine-grained video understanding. FineAction  is a large-scale and fine-grained video dataset encompassing diverse action categories with detailed annotations of action instances and time segments. For the purpose of SlowFocus adaption and model evaluation, we divide the FineAction dataset into training and testing sets based on videos, allocating 75% to the training set and 25% to the test set, ensuring these is no overlap between them. Building on this foundation, we probe time information and content information from FineAction and expand it into multi-tasks. This section outlines the data construction procedures, which consist of three main steps: (i) video preprocessing, (ii) annotation construction, and (iii) formation of downstream tasks. More details are included in the appendix.

**Video preprocessing.** As depicted in Step I of Figure 4, we employ a modified two-stage splitting algorithm from Panda70M  to segment videos into clips, thereby providing raw materials for fine-grained information at the clip level. This preprocessing step yields 62,912 video clips.

**Annotation construction.** Step II in Figure 4 outlines our approach to creating new annotations, which include time segments, captions, and action labels. Given the high cost of using GPT-4V  for generating large-scale, clip-level captions, we generate detailed captions for entire videos with GPT-4V and then use a fine-tuned Video Recontiener Model to produce large-scale, clip-level captions. By integrating action labels and time segments from the ground truth with clip segments and the generated captions, we create comprehensive new annotations.

**Formation of downstream tasks.** Utilizing the new annotations, we design four types of video instruction-following tasks, as illustrated in Step III of Figure 4: (i) segmented captioning, (ii) temporal video grounding, (iii) temporal video reasoning and (iv) multi-turn QA. Detailed information on construction procedures, task formats, prompts, and case studies can be found in Appendix C- E.

    &  &  &  &  &  \\  & & & mIoU & R@0.3 & R@0.5 & R@0.7 & B & M & R & C & Acc & Score \\   VideoLLaMA  & Vicuma-7B & ✗ & 0.17 & 0.25 & 0.11 & 0.00 & 0.06 & 0.09 & 0.08 & 0.13 & 8.75 & 0.53 \\ Video-ChaoGPT  & Vicuma-7B & ✗ & 0.06 & 0.10 & 0.01 & 0.00 & 0.15 & 0.12 & 0.10 & 0.21 & 13.93 & 0.79 \\ LLAMA-VID  & Vicuma-7B & ✗ & 0.35 & 0.52 & 0.17 & 0.03 & 0.16 & 0.12 & 0.11 & 0.23 & 15.65 & 0.87 \\ VITmeLLM  & Vicuma-7B & ✓ & 27.69 & 32.83 & 24.26 & 21.87 & 0.05 & 0.09 & 0.08 & 0.12 & 9.96 & 0.54 \\  LLaMA-VID\(|\) & Vicuma-7B & ✓ & 22.38 & 26.17 & 19.47 & 17.53 & 0.23 & 0.20 & 0.37 & 1.03 & 24.81 & 1.26 \\ Ours & Vicuma-7B & ✓ & **66.68** & **85.80** & **73.01** & **56.25** & **0.66** & **0.41** & **0.70** & **3.27** & **53.10** & **2.78** \\   

Table 1: Main results on FineAction-CGR benchmark. The column _LoRA_ represents whether the LLM is fine-tuned fully or using LoRA. \(\): Model is re-trained on the stage 3’s data. B: B@4. M: METEOR. R: ROUGE. C: CIDEr.

    &  &  &  &  &  \\  & & & Acc & Score & Acc & Score & Acc & Score & Correctness & Detail & Context & Temporal & Consistency \\   ProganBLML  & DeBERTa-V2 & ✗ & 32.2 & - & 16.8 & - & 24.7 & - & - & - & - & - & - \\ VideoLLaMA  & Vicuma-7B & ✗ & 51.6 & 2.5 & 29.6 & 1.8 & 12.4 & 1.1 & 1.96 & 2.18 & 2.16 & 1.82 & 1.79 \\ LLaMA-Adapter  & LLaMA-7B & ✗ & 54.9 & 3.1 & 43.8 & 2.7 & 34.2 & 2.7 & 2.03 & 2.32 & 2.30 & 1.98 & 2.15 \\ Video-ChaoT  & Vicuma-7B & ✗ & 56.3 & 2.8 & 45.0 & 2.5 & 26.5 & 2.2 & 2.23 & 2.50 & 2.53 & 1.94 & 2.24 \\ Video-ChaoGPT  & Vicuma-7B & ✗ & 64.9 & 3.3 & 49.3 & 2.8 & 35.2 & 2.7 & 2.40 & 2.52 & 2.62 & 1.98 & 2.37 \\ LLaMA-VID  & Vicuma-7B & ✗ & 69.7 & 3.7 & 57.7 & 3.2 & 47.4 & 3.3 & **2.96** & 3.00 & 3.53 & 2.46 & 2.51 \\  LLaMA-VID & Vicuma-7B & ✓ & 69.2 & 3.4 & 57.1 & 2.9 & 45.6 & 3.3 & 2.87 & 2.89 & 3.28 & 2.13 & 2.47 \\ Ours & Vicuma-7B & ✓ & **70.1** & **3.9** & **58.3** & **3.5** & **48.4** & **3.6** & 2.95 & **3.03** & **3.61** & **2.54** & **2.60** \\   

Table 2: Comparison with existing methods on coarse-grained video understanding benchmarks. Our method achieve on par performance with state-of-the-art models.

[MISSING_PAGE_FAIL:8]

boundaries, making it challenging for them to accurately predict temporal segments. (ii) Due to their reliance on low-frequency sampling, these models struggle to capture fine-grained temporal details, which adversely affects their performance on tasks requiring fine-grained temporal reasoning.

We also fine-tune the baseline using stage 3. To ensure fairness, the implementation details for baseline fine-tuning are kept consistent with those of our method. The baseline's performance improves because stage 3 includes tasks focused on temporal grounding and is specifically designed for fine-grained analysis. Furthermore, the remaining performance gap further supports our explanations.

**Results on coarse-grained video-based benchmarks.** In Table 2, we present a comparative evaluation of our method against various state-of-the-art methods across three zero-shot video-QA benchmarks: MSVD-QA , MSRVTT-QA , and ActivityNet-QA . We also conduct experiments on the video-based generative performance benchmark . The results demonstrate that the proposed SlowFocus mechanism not only enhances fine-grained video understanding but also delivers competitive performance in coarse-grained video understanding tasks, achieving results on par with state-of-the-art models.

**Results on long video benchmarks.** To further investigate the effectiveness of the proposed method on more challenging scenarios, we provide evaluations on long video benchmarks, including MovieChat-1K  and EgoSchema . As shown in Table 3, we evaluate our method on MovieChat-1K. The results show that although our method is not specifically trained on long video benchmarks (in contrast, MovieChat  has undergone targeted training for long videos), it still achieves competitive results (58.6% accuracy in global mode and 48.1% in breakpoint mode).

Additionally, we also conduct experiments on EgoSchema  benchmark, as detailed in Table 4. The results further demonstrate that, despite not being specifically trained on long video datasets, our method still achieves competitive performance.

**Qualitative results.** Figure 5 illustrates the qualitative results of our method on different videos. Our SlowFocus comprehensively analyzes the entire video, accurately identifies relevant temporal details based on the posed question, and provides precise answer within the context of videos.

### Ablations

In this section, we provide detailed ablation studies of our method conducted utilizing Vicuna-7B as the foundation model.

**Component-wise analysis.** We first investigate the impact of each proposed component in Table 5. Importantly, we observe that the baseline model, which relies solely on low-frequency frames (\(V_{L}\)), achieves limited performance, with an mIoU of 32.54 and an accuracy of 30.25. This highlights the

    &  &  \\  & **mIoU** & **R@0.3** & **R@0.5** & **R@0.7** & **Acc** & **Score** \\  
1 & 0.11 & 0.29 & 0.00 & 0.00 & 7.13 & 0.51 \\
1+2 & 51.67 & 63.29 & 57.84 & 45.81 & 20.34 & 1.04 \\
1+3 & 28.58 & 36.72 & 33.25 & 24.22 & 32.27 & 1.63 \\
**1+2+3** & 66.68 & **85.80** & **73.01** & **56.25** & **53.10** & **2.78** \\   

Table 6: Ablation of training stages. The first stage Table 7: The results of dynamically adjusting same-itself yields poor result. Integrating these stagespling frequency (fps) and frame token number \(N_{V}\) together results in the optimal performance.

Figure 5: Qualitative examples. Our proposed SlowFocus can effectively leverages the segmented temporal clues to accurately answer the posed question.

challenges faced by current Vid-LLMs in addressing fine-grained temporal tasks using only low-frequency sampling. Additionally, the results in the second row indicate that the inclusion of MFS significantly improves reasoning capabilities, resulting in an accuracy increase of +8.87. Furthermore, the temporal encoder, which models temporal relationships effectively, boosts both grounding and reasoning capabilities, with an increase of +22.45 in mIoU and +7.25 in accuracy. Increasing the high-frequency sampling number \(N_{H}\) from 10 to 20 leads to a noticeable performance gain (+7.66 in mIoU and +5.31 in accuracy), although further increasing \(N_{H}\) to 40 offers only marginal benefits. Lastly, the MMA module contribute an accuracy enhancement of +1.42.

**Necessity of fine-tuning strategy.** We also analyze the impact of each training stage, as reported in Table 6. Directly evaluating the pre-trained model after stage 1 yields poor performance. Based on necessary pre-training stage 1, when only utilizing stage 2 for boundary enhancement, there is a significant improvement by 51.56 in mIoU in temporal grounding ability. Furthermore, implementing only stage 3 for SlowFocus adaptation boosts the performance in temporal reasoning by 25.14 in accuracy. Integrating these stages results in the highest performance in both mIoU and accuracy metrics, highlighting the irreplaceability of our proposed comprehensive training strategy.

**What contributes more to fine-grained video understanding?** In Table 7, we conduct further experiments to explore how changes in sampling frequency and the number of frame tokens influence our method. For meaningful comparison, we maintain constant total token numbers while gradually decreasing the sampling frequency. The results indicate that within proposed SlowFocus, performance improves with an increase in frame tokens and remains relatively unaffected by a decrease in global sampling frequency. This demonstrates the efficacy of our approach in high-frequency sampling and effectively alleviates the trade-off dilemma between sampling frequency and frame tokens.

**Ablation on discretized temporal space.** We investigate the influence of temporal token space \(N\) in Table 8. The results demonstrate that when the token space increase from 0.1K to 1K, a significant improvement (+22.87 in mIoU and +13.69 in accuracy) occurs. While further increasing \(N\) from 1K to 10K, the performance drops instead.

## 6 Conclusion and limitations

**Conclusion.** We introduce SlowFocus, a straightforward yet potent mechanism that significantly enhances fine-grained video understanding. Our approach improves the temporal localization capabilities of Vid-LLMs, enabling them to precisely identify relevant temporal segments based on the given query. Moreover, with our newly developed temporal encoder and multi-frequency mixing attention module, our method effectively models the temporal relationships among frames and captures inter-frame context. Demonstrating superior performance on our newly established fine-grained video understanding benchmarks, we hope that our work can propel the development of Vid-LLMs in achieving advanced video understanding.

**Limitations.** Despite significant advancements made in this work to enhance Vid-LLM's access to high-frequency temporal details and its temporal reasoning capabilities, challenges persist due to the limited existing research on maintaining high resolution in video. Consequently, our method may still encounter inaccurate predictions stemming from the ambiguity of spatial details.

    &  &  &  \\  & mIoU & R@0.3 & R@0.5 & R@0.7 & B & M & R & C & Acc & Score \\  
0.1K & 43.81 & 62.26 & 51.64 & 33.18 & 0.47 & 0.33 & 0.49 & 2.15 & 39.41 & 2.20 \\ 
1K & 66.68 & 85.80 & 73.01 & 56.25 & 0.66 & 0.41 & 0.70 & 3.27 & 53.10 & 2.78 \\ 
10K & 63.74 & 86.19 & 71.05 & 55.17 & 0.66 & 0.40 & 0.71 & 3.24 & 52.59 & 2.71 \\   

Table 8: Ablation study on temporal token space \(N\).