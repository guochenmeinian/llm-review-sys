# Understanding Generalizability of Diffusion Models

Requires Rethinking the Hidden Gaussian Structure

 Xiang Li1, Yixiang Dai1, Qing Qu1

1Department of EECS, University of Michigan,

forkobe@umich.edu, yixiang@umich.edu, qingqu@umich.edu

###### Abstract

In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels. We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity. This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers. Interestingly, these linear denoisers are approximately the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset. This finding implies that diffusion models have the inductive bias towards capturing and utilizing the Gaussian structure (covariance information) of the training dataset for data generation. We empirically demonstrate that this inductive bias is a unique property of diffusion models in the generalization regime, which becomes increasingly evident when the model's capacity is relatively small compared to the training dataset size. In the case where the model is highly overparameterized, this inductive bias emerges during the initial training phases before the model fully memorizes its training data. Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models.

## 1 Introduction

In recent years, diffusion models [1; 2; 3; 4] have become one of the leading generative models, powering the state-of-the-art image generation systems such as Stable Diffusion . To understand the empirical success of diffusion models, several works [6; 7; 8; 9; 10; 11; 12] have focused on their sampling behavior, showing that the data distribution can be effectively estimated in the reverse sampling process, assuming that the score function is learned accurately. Meanwhile, other works [13; 14; 15; 16; 17; 18] investigate the learning of score functions, showing that effective approximation can be achieved with score matching loss under certain assumptions. However, these theoretical insights, grounded in simplified assumptions about data distribution and neural network architectures, do not fully capture the complex dynamics of diffusion models in practical scenarios. One significant discrepancy between theory and practice is that real-world diffusion models are trained only on a finite number of data points. As argued in , theoretically a perfectly learned score function over the empirical data distribution can only replicate the training data. In contrast, diffusion models trained on finite samples exhibit remarkable generalizability, producing high-quality images that significantly differ from the training examples. Therefore, a good understanding of the remarkable generative power of diffusion models is still lacking.

In this work, we aim to deepen the understanding of generalizability in diffusion models by analyzing the inherent properties of the learned score functions. Essentially, the score functions can be interpreted as a series of deep denoisers trained on various noise levels. These denoisers are then chained together to progressively denoise a randomly sampled Gaussian noise into its correspondingclean image, thus, understanding the function mappings of these diffusion denoisers is critical to demystify the working mechanism of diffusion models. Motivated by the linearity observed in the diffusion denoisers of effectively generalized diffusion models, we propose to elucidate their function mappings with a linear distillation approach, where the resulting linear models serve as the linear approximations of their nonlinear counterparts.

Contributions of this work:Our key findings can be highlighted as follows:

* **Inductive bias towards Gaussian structures (Section 3).** Diffusion models in the _generalization regime_ exhibit an inductive bias towards learning diffusion denoisers that are close (but not equal) to the optimal denoisers for a multivariate Gaussian distribution, defined by the empirical mean and covariance of the training data. This implies the diffusion models have the inductive bias towards capturing the Gaussian structure (covariance information) of the training data for image generation.
* **Model Capacity and Training Duration (Section 4)** We show that this inductive bias is most pronounced when the model capacity is relatively small compared to the size of the training data. However, even if the model is highly overparameterized, such inductive bias still emerges during early training phases, before the model memorizes its training data. This implies that early stopping can prompt generalization in overparameterized diffusion models.
* **Connection between Strong Generalization and Gaussian Structure (Section 5).** Lastly, we argue that the recently observed strong generalization  results from diffusion models learning certain common low-dimensional structural features shared across non-overlapping datasets. We show that such low-dimensional features can be partially explained through the Gaussian structure.

Relationship with Prior Arts.Recent research [20; 21; 22; 23; 24] demonstrates that diffusion models operate in two distinct regimes: (_i_) a memorization regime, where models primarily reproduce training samples and (_ii_) a generalization regime, where models generate high-quality, novel images that extend beyond the training data. In the generalization regime, a particularly intriguing phenomenon is that diffusion models trained on non-overlapping datasets can generate nearly identical samples . While prior work  attributes this "_strong generalization_" effect to the structural inductive bias inherent in diffusion models leading to the optimal denoising basis (geometry-adaptive harmonic basis), our research advances this understanding by demonstrating diffusion models' inductive bias towards capturing the Gaussian structure of the training data. Our findings also corroborate with observations of earlier study  that the learned score functions of well-trained diffusion models closely align with the optimal score functions of a multivariate Gaussian approximation of the training data.

## 2 Preliminary

Basics of Diffusion Models.Given a data distribution \(p_{}()\), where \(^{d}\), diffusion models [1; 2; 3; 4] define a series of intermediate states \(p(;(t))\) by adding Gaussian noise sampled from \((,(t)^{2})\) to the data, where \((t)\) is a predefined schedule that specifies the noise level at time \(t[0,T]\), such that at the end stage the noise mollified distribution \(p(;(T))\) is indistinguishable from the pure Gaussian distribution. Subsequently, a new sample is generated by progressively denoising a random noise \(_{T}(,(T)^{2})\) to its corresponding clean image \(_{0}\). Following , this forward and backward diffusion process can be expressed with a probabilistic ODE:

\[d=-(t)(t)_{} p(;(t))dt.\] (1)

In practice the score function \(_{} p(;(t))\) can be approximated by

\[_{} p(;(t))=(_{}(; (t))-)/(t)^{2},\] (2)

where \(_{}(;(t))\) is parameterized by a deep network with parameters \(\) trained with the denoising score matching objective:

\[_{}_{ p_{}}_{(,(t)^{2})}[\|_{ {}}(+;(t))-\|_{2}^{2}].\] (3)

In the discrete setting, the reverse ODE in (1) takes the following form:

\[_{i+1}(1-(t_{i}-t_{i+1})(t_{i})}{(t_{ i})})_{i}+(t_{i}-t_{i+1})(t_{i})}{(t_{i})} _{}(_{i};(t_{i})),\] (4)where \(_{0}(,^{2}(t_{0}))\). Notice that at each iteration \(i\), the intermediate sample \(_{i+1}\) is the sum of the scaled \(_{i}\) and the denoising output \(_{}(_{i};(t_{i}))\). Obviously, the final sampled image is largely determined by the denoiser \(_{}(;(t))\). If we can understand the function mapping of these diffusion denoisers, we can demystify the working mechanism of diffusion models.

Optimal Diffusion Denoisers under Simplified Data Assumptions.Under certain assumptions on the data distribution \(p_{}()\), the optimal diffusion denoisers \(_{}(;(t))\) that minimize the score matching objective (3) can be derived analytically in closed-forms as we discuss below.

* _Multi-delta distribution of the training data._ Suppose the training dataset contains a finite number of data points \(\{_{1},_{2},...,_{N}\}\), a natural way to model the data distribution is to represent it as a multi-delta distribution: \(p()=_{i=1}^{N}(-_{i})\). In this case, the optimal denoiser is \[_{}(;(t))=^{N}( ;_{i},(t)^{2})_{i}}{_{i=1}^{N}( ;_{i},(t)^{2})},\] (5) which is essentially a softmax-weighted combination of the finite data points. As proved in , such diffusion denoisers \(_{}(;(t))\) can only generate exact replicas of the training samples, therefore they have no generalizability.
* _Multivariate Gaussian distribution._ Recent work  suggests modeling the data distribution \(p_{}()\) as a multivariate Gaussian distribution \(p()=(,)\), where the mean \(\) and the covariance \(\) are approximated by the empirical mean \(=_{i=1}^{N}_{i}\) and the empirical covariance \(=_{i=1}^{N}(_{i}-)(_{i}-) ^{T}\) of the training dataset. In this case, the optimal denoiser is: \[_{}(;(t))=+ }_{(t)}^{T}(-),\] (6) where \(=^{T}\) is the SVD of the empirical covariance matrix, with singular values \(=(_{1},,_{d})\) and \(}_{(t)}=(}{ _{1}+(t)^{2}}}{_{d}+(t)^{2}})\). With this linear Gaussian denoiser, as proved in , the sampling trajectory of the probabilistic ODE (1) has close form: \[_{t}=+_{i=1}^{d}+_{i}}{ (T)^{2}+_{i}}}_{i}^{T}(_{T}-)_{i},\] (7) where \(_{i}\) is the \(i^{}\) singular vector of the empirical covariance matrix. While  demonstrate that the Gaussian scores approximate learned scores at high noise variances, we show that they are nearly the best linear approximations of learned scores across a much wider range of noise variances.

Generalization vs. Memorization of Diffusion Models.As the training dataset size increases, diffusion models transition from the memorization regime--where they can only replicate its training images--to the generalization regime, where the they produce high-quality, novel images . While memorization can be interpreted as an overfitting of diffusion models to the training samples, the mechanisms underlying the generalization regime remain less well understood. This study aims to explore and elucidate the inductive bias that enables effective generalization in diffusion models.

## 3 Hidden Linear and Gaussian Structures in Diffusion Models

In this section, we study the intrinsic structures of the learned score functions of diffusion models in the generalization regime. Through various experiments and theoretical investigation, we show that

_Diffusion models in the generalization regime have inductive bias towards learning the Gaussian structures of the dataset._

Based on the _linearity_ observed in diffusion denoisers trained in the generalization regime, we propose to investigate their intrinsic properties through a _linear distillation_ technique, with which we train a series of linear models to approximate the nonlinear diffusion denoisers (Section 3.1). Interestingly, these linear models closely resemble the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset (Section 3.2). This implies diffusion models have the inductive bias towards learning the Gaussian structure of the training dataset. We theoretically show that the observed Gaussian structure is the optimal solution to the denoising score matching objective under the constraint that the model is linear (Section 3.3). In the subsequent sections, although we mainly demonstrate our results using the FFHQ datasets, our findings are robust and extend to various architectures and datasets, as detailed in Appendix G.

### Diffusion Models Exhibit Linearity in the Generalization Regime

Our study is motivated by the emerging linearity observed in diffusion models in the generalization regime. Specifically, we quantify the linearity of diffusion denoisers at various noise level \((t)\) by jointly assessing their "Additivity" and "Homogeneity" with a linearity score (LS) defined by the cosine similarity between \(_{}(_{1}+_{2};(t))\) and \(_{}(_{1};(t))+_{}(_{2};(t))\):

\[(t)\ =\ _{_{1},_{2} p(;(t))} [|_{}(_{1}+ _{2};(t))}{\|_{}(_{1}+ _{2};(t))\|_{2}},_{}( _{1};(t))+_{}(_{2};(t))}{ \|_{}(_{1};(t))+_{}(_{2};(t))\|_{2}}|],\]

where \(_{1},_{2} p(;(t))\), and \(\) and \(\) are scalars. In practice, the expectation is approximated with its empirical mean over 100 samples. A more detailed discussion on this choice of measuring linearity is deferred to Appendix A.

Following the EDM training configuration , we set the noise levels \((t)\) within the continuous range [0.002,80]. As shown in Figure 1, as diffusion models transition from the memorization regime to the generalization regime (increasing the training dataset size), the corresponding diffusion denoisers \(_{}\) exhibit increasing linearity. This phenomenon persists across diverse datasets1 as well as various training configurations2; see Appendix B for more details. This emerging linearity motivates us to ask the following questions:

* _To what extent can a diffusion model be approximated by a linear model?_
* _If diffusion models can be approximated linearly, what are the underlying characteristics of this linear approximation?_

Investigating the Linear Structures via Linear Distillation.To address these questions, we investigate the hidden linear structure of diffusion denoisers through _linear distillation_. Specifically, for a given diffusion denoiser \(_{}(;(t))\) at noise level \((t)\), we approximate it with a linear function (with a bias term) such that:

\[_{}(;(t)):=_{(t)}+_{ (t)}\ \ _{}(;(t)),\  p(;(t)),\] (8)

where the weight \(_{(t)}^{d d}\) and bias \(_{(t)}^{d}\) are learned by solving the following optimization problem with gradient descent:3

\[_{_{(t)},_{(t)}}_{ p_{}()}_{(,(t)^{2 })}||_{(t)}(+)+_{(t)}- _{}(+;(t))||_{2}^{2}.\] (9)

If these linear models effectively approximate the nonlinear diffusion denoisers, analyzing their weights can elucidate the generation mechanism.

While diffusion models are trained on continuous noise variance levels within [0.002,80], we examine the 10 discrete sampling steps specified by the EDM schedule : [80.0, 42.415, 21.108, 9.723, 4.06, 1.501, 0.469, 0.116, 0.020, 0.002]. These steps are considered sufficient for studying the diffusion mappings for two reasons: _(i)_ images generated using these 10 steps closely match those generated with more steps, and _(ii)_ recent research  demonstrates that the diffusion denoisers trained on similar noise variances exhibit analogous function mappings, implying that denoiser behavior at discrete variances represents their behavior at nearby variances.

Figure 1: **Linearity scores of diffusion denoisers.** Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \(==1/\).

After obtaining the linear models \(_{}\), we evaluate their differences with the actual nonlinear denoisers \(_{}\) with the score field approximation error, calculated using the expectation over the root mean square error (RMSE):

\[(t):=_{ p_{}(),(;(t)^{2})}_{}(+;(t))-_{}(+;(t))\|_{2}^{2}}{d}}}_{$ and $$}},\] (10)

where \(d\) represents the data dimension and the expectation is approximated with its empirical mean. While we present RMSE-based results in the main text, our findings remain consistent across alternative metrics, including NMSE, as detailed in Appendix G.

We perform linear distillation on well trained diffusion models operating in the generalization regime. For comprehensive analysis, we also compute the score approximation error between \(_{}\) and: _(i)_ the optimal denoisers for the multi-delta distribution \(_{}\) defined as (5), and _(ii)_ the optimal denoisers for the multivariate Gaussian distribution \(_{}\) defined as (6). As shown in Figure 2, our analysis reveals three distinct regimes:

* _High-noise regime _. In this regime, only coarse image structures are generated (Figure 2(right)). Quantitatively, as shown in Figure 2(left), the distilled linear model \(_{}\) closely approximates its nonlinear counterpart \(_{}\) with RMSE below 0.05. Both Gaussian score \(_{}\) and multi-delta score \(_{}\) also achieve comparable approximation accuracy.
* _Low-noise regime [0.002,0.1]_. In this regime, only subtle, imperceptible details are added to the generated images. Here, both \(_{}\) and \(_{}\) effectively approximate \(_{}\) with RMSE below 0.05.
* _Intermediate-noise regime [0.1,20]_. This crucial regime, where realistic image content is primarily generated, exhibits significant nonlinearity. While \(_{}\) exhibits high approximation error due to rapid convergence to training samples--a memorization effect theoretically proved in , both \(_{}\) and \(_{}\) maintain relatively lower approximation errors.

Qualitatively, as shown in Figure 2(right), despite the relatively high score approximation error in the intermediate noise regime, the images generated with \(_{}\) closely resemble those generated with \(_{}\) in terms of the overall image structure and certain amount of fine details. This implies _(i)_ the underlying linear structure within the nonlinear diffusion models plays a pivotal role in their generalization capabilities and _(ii)_ such linear structure is effectively captured by our distilled linear models. In the next section, we will explore this linear structure by examining the linear models \(_{}\).

### Inductive Bias towards Learning the Gaussian Structures

Notably, the Gaussian denoisers \(_{}\) exhibit behavior strikingly similar to the linear denoisers \(_{}\). As illustrated in Figure 2(left), they achieve nearly identical score approximation errors, particularly in the critical intermediate variance region. Furthermore, their sampling trajectories are remarkably similar (Figure 2(right)), producing nearly identical generated images that closely match those from the actual diffusion denoisers (Figure 3). These observations suggest that \(_{}\) and \(_{}\) share

Figure 2: **Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories \((_{t};(t)\) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.**

similar function mappings across various noise levels, leading us to hypothesize that the intrinsic linear structure underlying diffusion models corresponds to the Gaussian structure of the training data--specifically, its empirical mean and covariance. We validate this hypothesis by empirically showing that \(_{}\) is close to \(_{}\) through the following three complementary experiments:

* _Similarity in weight matrices._ As illustrated in Figure 4(left), \(_{(t)}\) progressively converge towards \(_{(t)}^{T}\) throughout the linear distillation process, achieving small normalized MSE (less than 0.2) for most of the noise levels. The less satisfactory convergence behavior at \((t)=80.0\) is due to inadequate training of the diffusion models at this particular noise level, which is minimally sampled during the training of actual diffusion models (see Appendix G.2 for more details).
* _Similarity in Force functions._ Furthermore, Figure 2(left, gray line) demonstrates that \(_{}\) and \(_{}\) maintain small score differences (RMSE less than 0.05) across all noise levels, indicating that these denoisers exhibit similar function mappings throughout the diffusion process.
* _Similarity in principal components._ As shown in Figure 4(right), for a wide noise range (\((t)[0.116,80.0]\)), the leading singular vectors of the linear weights \(_{(t)}\) (denoted \(_{Linear}\)) align well with \(\), the singular vectors of the Gaussian weights.4 This implies that \(\), representing the principal components of the training data, is effectively captured by the diffusion models. In the low-noise regime (\((t)[0.002,0.116]\)), however, \(_{}\) approximates the identity mapping, leading to ambiguous singular vectors with minimal impact on image generation. Further analysis of \(_{}\)'s behavior in the low-noise regime is provided in Appendices D and F.1.

Since the optimization problem (9) is convex w.r.t. \(_{(t)}\) and \(_{(t)}\), the optimal solution \(_{}\) represents the unique optimal linear approximation of \(_{}\). Our analyses demonstrate that this optimal linear approximation closely aligns with \(_{}\), leading to our central finding: diffusion models in the generalization regime exhibit an inductive bias (which we term as the Gaussian inductive bias) towards learning the Gaussian structure of training data. This manifests in two main ways: (_i_) In the high-noise variance regime, well-trained diffusion models learn \(_{}\) that closely approximate the linear Gaussian denoisers \(_{}\); (_ii_) As noise variance decreases, although \(_{}\) diverges from \(_{}\), \(_{}\) remains nearly identical to the optimal linear approximation \(_{}\), and images generated by \(_{}\) retain structural similarity to those generated by \(_{}\).

Finally, we emphasize that the Gaussian inductive bias only emerges in the generalization regime. By contrast, in the memorization regime, Figure 5 shows that \(_{}\) significantly diverges from \(_{}\), and both \(_{}\) and \(_{}\) provide considerably poorer approximations of \(_{}\) compared to the generalization regime.

Figure 4: **Linear model shares similar function mapping with Gaussian model.** The left figure shows the difference between the linear weights and the Gaussian weights w.r.t. 100 training epochs of the linear distillation process for the 10 discrete noise levels. The right figure shows the correlation matrices between the first 100 singular vectors of the linear weights and Gaussian weights.

Figure 3: **Images sampled from various Models.** The figure shows the samples generated using different models starting from the same initial noises.

### Theoretical Analysis

In this section, we demonstrate that imposing linear constraints on diffusion models while minimizing the denoising score matching objective (3) leads to the emergence of Gaussian structure.

**Theorem 1**.: _Consider a diffusion denoiser parameterized as a single-layer linear network, defined as \((_{t};(t))=_{(t)}_{t}+_{(t)}\), where \(_{(t)}^{d d}\) is a linear weight matrix and \(_{(t)}^{d}\) is the bias vector. When the data distribution \(p_{}()\) has finite mean \(\) and bounded positive semidefinite covariance \(\), the optimal solution to the score matching objective (3) is exactly the Gaussian denoiser defined in (6):_

\[_{}(_{t};(t))\ =\ }_{ (t)}^{T}(_{t}-)+,\]

_with \(_{(t)}=}_{(t)}^{T}\) and \(_{(t)}=(-}_{(t)}^ {T})\)._

The detailed proof is postponed to Appendix E. This optimal solution corresponds to the classical Wiener filter , revealing that diffusion models naturally learn the Gaussian denoisers when constrained to linear architectures. To understand why highly nonlinear diffusion models operate near this linear regime, it is helpful to model the training data distribution as the multi-delta distribution \(p()=_{i=1}^{N}(-_{i})\), where \(\{_{1},_{2},...,_{N}\}\) is the finite training images. Notice that this formulation better reflects practical scenarios where only a finite number of training samples are available rather than the ground truth data distribution. Importantly, it is proved in  that the optimal denoisers \(_{}\) in this case is approximately equivalent to \(_{}\) for high noise variance \((t)\) and query points far from the finite training data. This equivalence explains the strong similarity between \(_{}\) and \(D_{}\) in the high-noise variance regime, and consequently, why \(_{}\) and \(_{}\) exhibit high similarity in this regime--deep networks converge to the optimal denoisers for finite training datasets.

However, this equivalence between \(_{}\) and \(_{}\) breaks down at lower \((t)\) values. The denoising outputs of \(_{}\) are convex combinations of training data points, weighted by a softmax function with temperature \((t)^{2}\). As \((t)^{2}\) decreases, this softmax function increasingly approximates an argmax function, effectively retrieving the training point \(_{i}\) closest to the input \(\). Learning this optimal solution requires not only sufficient model capacity to memorize the entire training dataset but also, as shown in , an exponentially large number of training samples. Due to these learning challenges, deep networks instead converge to local minima \(_{}\) that, while differing from \(_{}\), exhibit better generalization property. Our experiments reveal that these learned \(_{}\) share similar function mappings with \(_{}\). The precise mechanism driving diffusion models trained with gradient descent towards this particular solution remains an open question for future research.

Notably, modeling \(p_{}()\) as a multi-delta distribution reveals a key insight: while unconstrained optimal denoisers (5) perfectly capture the scores of the empirical distribution, they have no gen

Figure 5: **Comparison between the diffusion denoisers in memorization and generalization regimes**. Figure(a) demonstrates that in the memorization regime (trained on small datasets of size 1094 and 68), \(_{}\) significantly diverges from \(_{}\), and both provide substantially poorer approximations of \(_{}\) compared to the generalization regime (trained on larger datasets of size 35000 and 1094). Figure(b) qualitatively shows that the denoising outputs of \(_{}\) closely match those of \(_{}\) only in the generalization regime—a similarity that persists even when the denoisers process pure noise inputs.

eralizability. In contrast, Gaussian denoisers, despite having higher score approximation errors due to the linear constraint, can generate novel images that closely match those produced by the actual diffusion models. This suggests that the generative power of diffusion models stems from the imperfect learning of the score functions of the empirical distribution.

## 4 Conditions for the Emergence of Gaussian Structures and Generalizability

In Section 3, we demonstrate that diffusion models exhibit an inductive bias towards learning denoisers that are close to the Gaussian denoisers. In this section, we investigate the conditions under which this bias manifests. Our findings reveal that this inductive bias is linked to model generalization and is governed by _(i)_ the model capacity relative to the dataset size and _(ii)_ the training duration. For additional results, including experiments on CIFAR-10 dataset, see Appendix F.

### Gaussian Structures Emerge when Model Capacity is Relatively Small

First, we find that the Gaussian inductive bias and the generalization of diffusion models are heavily influenced by the relative size of the model capacity compared to the training dataset. In particular, we demonstrate that:

_Diffusion models learn the Gaussian structures when the model capacity is relatively small compared to the size of training dataset._

This argument is supported by the following two key observations:

\(\) _Increasing dataset size prompts the emergence of Gaussian structure at fixed model scale._ We train diffusion models using the EDM configuration  with a fixed channel size of 128 on datasets of varying sizes \(\) until FID convergence. Figure 6(left) demonstrates that the score approximation error between diffusion denoisers \(_{}\) and Gaussian denoisers \(_{}\) decreases as the training dataset size grows, particularly in the crucial intermediate noise variance regime (\((t)[0.116,20]\)). This increasing similarity between \(_{}\) and \(_{}\) correlates with a transition in the models' behavior: from a memorization regime, where generated images are replicas of training samples, to a generalization regime, where novel images exhibiting Gaussian structure5 are produced, as shown in Figure 6(b). This correlation underscores the critical role of Gaussian structure in the generalization capabilities of diffusion models.

\(\) _Decreasing model capacity promotes the emergence of Gaussian structure at fixed dataset sizes._ Next, we investigate the impact of model scale by training diffusion models with varying channel sizes \(\), corresponding to \([64,251,992,4,16,64]\) parameters, on a fixed training dataset of 1094 images. Figure 7(left) shows that in the intermediate noise variance regime (\((t)[0.116,20]\)), the discrepancy between \(_{}\) and \(_{}\) decreases with decreasing model scale, indicating that Gaussian structure emerges in low-capacity models. Figure 7(right) demonstrates that

Figure 6: **Diffusion models learn the Gaussian structure when training dataset is large.** Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. ”NN” denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.

this trend corresponds to a transition from data memorization to the generation of images exhibiting Gaussian structure. Here we note that smaller models lead to larger discrepancy between \(_{}\) and \(_{}\) in the high-noise regime. This phenomenon arises because diffusion models employ a bell-shaped noise sampling distribution that prioritizes intermediate noise levels, resulting in insufficient training at high noise variances, especially when model capacity is limited (see more details in Appendix F.2).

These two experiments collectively suggest that the inductive bias of diffusion models is governed by the relative capacity of the model compared to the training dataset size.

### Overparameterized Models Learn Gaussian Structures before Memorization

In the overparameterized regime, where model capacity significantly exceeds training dataset size, diffusion models eventually memorize the training data when trained to convergence. However, examining the learning progression reveals a key insight:

_Diffusion models learn the Gaussian structures with generalizability before they memorize._

Figure 8 demonstrates that during early training epochs (0-841), \(_{}\) progressively converge to \(_{}\) in the intermediate noise variance regime, indicating that the diffusion model is progressively learning the Gaussian structure in the initial stages of training. Notably. By epoch 841, the diffusion model generates images strongly resembling those produced by the Gaussian model, as shown in Figure 8(b). However, continued training beyond this point increases the difference between \(_{}\) and \(_{}\) as the model transitions toward memorization. This observation suggests that early stopping could be an effective strategy for promoting generalization in overparameterized diffusion models.

## 5 Connection between Strong Generalizability and Gaussian Structure

A recent study  reveals an intriguing "strong generalization" phenomenon: diffusion models trained on large, non-overlapping image datasets generate nearly identical images from the same initial

Figure 8: **Diffusion model learns the Gaussian structure in early training epochs. Diffusion model with same scale (channel size 128) is trained using 1094 images. The left and right figures shows the score difference and the generated images respectively.**

Figure 7: **Diffusion model learns the Gaussian structure when model scale is small. Models with different scales are trained on a fixed training dataset of 1094 images. The left and right figures show the score difference and the generated images respectively.**

noise. While this phenomenon might be attributed to deep networks' inductive bias towards learning the "true" continuous distribution of photographic images, we propose an alternative explanation: rather than learning the complete distribution, deep networks may capture certain low-dimensional common structural features shared across these datasets and these features can be partially explained by the Gaussian structure.

To validate this hypothesis, we examine two diffusion models with channel size 128, trained on non-overlapping datasets S1 and S2 (35000 images each). Figure 9(a) shows that images generated by these models (bottom) closely match those from their corresponding Gaussian models (top), highlighting the Gaussian structure's role in strong generalization.

Comparing Figure 9(a)(top) and (b)(top), we observe that \(_{}\) generates nearly identical images whether the Gaussian structure is calculated on a small dataset (1094 images) or a much larger one (35000 images). This similarity emerges because datasets of the same class can exhibit similar Gaussian structure (empirical covariance) with relatively few samples--just hundreds for FFHQ. Given the Gaussian structure's critical role in generalization, small datasets may already contain much of the information needed for generalization, contrasting previous assertions in  that strong generalization requires training on datasets of substantial size (more than \(10^{5}\) images). However, smaller datasets increase memorization risk, as shown in Figure 9(b). To mitigate this, as discussed in Section 4, we can either reduce model capacity or implement early stopping (Figure 9(c)). Indeed, models trained on 1094 and 35000 images generate remarkably similar images, though the smaller dataset yields lower perceptual quality. This similarity further demonstrates that small datasets contain substantial generalization-relevant information closely tied to Gaussian structure. Further discussion on the connections and differences between our work and  are detailed in Appendix H.

## 6 Discussion

In this study, we empirically demonstrate that diffusion models in the generalization regime have the inductive bias towards learning diffusion denoisers that are close to the corresponding linear Gaussian denoisers. Although real-world image distributions are significantly different from Gaussian, our findings imply that diffusion models have the bias towards learning and utilizing low-dimensional data structures, such as the data covariance, for image generation. However, the underlying mechanism by which the nonlinear diffusion models, trained with gradient descent, exhibit such linearity remains unclear and warrants further investigation.

Moreover, the Gaussian structure only partially explains diffusion models' generalizability. While models exhibit increasing linearity as they transition from memorization to generalization, a substantial gap persists between the linear Gaussian denoisers and the actual nonlinear diffusion models, especially in the intermediate noise regime. As a result, images generated by Gaussian denoisers fall short in perceptual quality compared to those generated by the actual diffusion models especially for complex dataset such as CIFAR-10. This disparity highlights the critical role of nonlinearity in high-quality image generation, a topic we aim to investigate further in future research.

Figure 9: **Diffusion models in the strong generalization regime generate similar images as the Gaussian models. Figure(a) Top: Generated images of Gaussian models; Bottom: Generated images of diffusion models, with model scale 128; S1 and S2 each has 35000 non-overlapping images. Figure(b) Top: Generated images of Gaussian model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 1094 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization.**

## Data Availability Statement

The code and instructions for reproducing the experiment results will be made available in the following link: https://github.com/Morefre/Understanding-Generalizability-of-Diffusion-Models-Requires-Rethinking-the-Hidden-Gaussian-Structure.