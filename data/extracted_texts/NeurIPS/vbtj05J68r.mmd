# Position: Maximizing Neural Regression Scores May Not Identify Good Models of the Brain

Rylan Schaeffer

Computer Science

Stanford University

rschaef@cs.stanford.edu

Mikail Khona

Physics

MIT

mikail@mit.edu

Sarthak Chandra

Brain & Cognitive Sciences

MIT

sarthakc@mit.edu

Mitchell Ostrow

Brain & Cognitive Sciences

MIT

ostrow@mit.edu

Brando Miranda

Computer Science

Stanford University

brando9@cs.stanford.edu

Sanni Koyejo

Computer Science

Stanford University

sanni@cs.stanford.edu

###### Abstract

A prominent methodology in computational neuroscience posits that the brain can be understood by identifying which artificial neural network models most accurately predict biological neural activations, measured according to regression test error or other similar metrics. In this opinion piece, we argue that the field lacks a canonical definition of model goodness, and rather than engaging with this difficult question, the neural regressions methodology simply asserted a proxy - neural predictivity - then overfit to this proxy. We begin with a notable failure of the neural regressions methodology in which the most predictive models disagreed with key properties of the neural circuit. Next, we highlight converging empirical and mathematical evidence that explains the disconnect: (linear) neural regressions are simply discovering the implicit biases of (linear) regression, which may not appropriately identify models that are actually brain-like. This is an instance of Goodhart's law: by selecting neural network models that optimize (linear) neural predictivity, the field's results have devolved into re-discovering general properties of (linear) regression, rather than furthering our understanding of the brain. These insights suggest that the neural regressions methodology may be insufficient for understanding the brain, and we call for a critical reevaluation of this methodology in computational neuroscience.

## 1 Introduction

An influential methodology in neuroscience-inspired artificial intelligence argues that task-optimized deep artificial neural networks (ANNs) should be considered good models of the brain if they capture a large fraction of variance in neural population recordings assessed via regressions of ANN unit activity onto biological neural responses . The claim is that the ANN(s) with better performing neural regressions are more similar to the brain than alternative models . This approach has been widely used in vision , audition , language , and spatial navigation , most often with (regularized) linear models, but occasionally with non-linear models.

In this position piece, we argue that Neuro-AI lacks sufficiently rich definitions of neural similarity, and such notions are context-dependent and difficult to quantify. The neural regressions methodology sidesteps these challenges by defining a proxy - for instance, the test \(R^{2}\) of linear regression between biological recordings and model activations - and then choosing models based on this proxy. Themodels that win a selection process (e.g., on BrainScore ) may do so more because of inductive biases of the proxy, independent of any meaningful relationships with the brain (Fig. 1).

This perspective explains why, for example, the neural regressions methodology was confidently incorrect when applied to models of grid cells: linear regression does not capture in key criteria of neural similarity for grid cells (e.g., periodic tuning curves , multiple grid modules with specific period ratios , toroidal continuous attractor dynamics [91; 32]; see Appendix Sec. A for a detailed list). This perspective also explains a finding by four independent research groups in different modalities, data, architectures and recording technologies [66; 25; 80; 17] of a quantitatively consistent relationship between test \(R^{2}\) and effective dimensionality, that was mathematically corrected and further empirically studied by Canatar et al. : (linear) neural predictivity _is_ (linear) regression, and (linear) regression has inductive biases, irrespective of the underlying neuroscience. We focus on linear regression because of its ubiquity in the literature, but other preference functions (e.g., RSA , CKA , SVCCA , Procrustes , etc.) would not escape this critique; rather, they would simply change the inductive biases of the chosen preference function.

Together, these insights suggest that the neural regression methodology, and more broadly the idea that a uniform set of metrics can automate model selection, may be fundamentally flawed by overfitting to those metrics rather than advancing our understanding of the brain. We conclude by suggesting a re-evaluation of such methodologies.

## 2 Neural Regressions Can Reach Incorrect Conclusions with High Confidence

In vision, Bowers et al.  documented how artificial networks preferred by the neural regressions methodology are contradictory properties of primate vision, and others have identified additional flaws [54; 88; 20; 35; 26; 27; 23; 36; 52]. Here, we chose to focus on the clearest example of a failure of the neural regressions methodology: grid cells. Why focus on grid cells? Grid cells - a surprising and important Nobel Prize-winning discovery  - differ from vision, audition and language in that humanity possesses scientific models [29; 11; 10; 76] that have repeatedly proven predictive ([77; 91; 32]), not in the regressions sense but in the sense of exhibiting fundamental properties, e.g., localization of each module to a two-dimensional subspace, quantization of grid module periods, preserved low-dimensional dynamics across waking and sleep that were subsequently validated. In a domain we understand well, how did the regressions methodology fare?

_When applied to a specific neural circuit (grid cells) that humanity possesses near-normative models of, the neural regressions methodology preferred incorrect models with high confidence._

As context, the key research questions about grid cells are modeling their dynamics and the evolutionary causes for their existence. Previous and now near-normative models showed how strong recurrent interactions leading to pattern formation, coupled with a way for movement inputs to shift the pattern phase and thus perform path integration, could generate grid cell dynamics [11; 47]; and

Figure 1: **Schematic.** Left: The neural regressions methodology posits a proxy – neural predictivity – of how similar model(s) are to a neural system of interest without verifying the extent to which the proxy agrees with neural similarity. Right: Overfitting to the proxy leads to mismatches with neural similarity. Although we do not define neural similarity here, we emphasize that it is task, neural-system, and question-dependent, and hence likely cannot always be neural predictivity. For a system-specific example of neural similarity, we offer criteria for grid cells in Appendix Sec. A.

that multiple grid modules played key roles in disambiguating position over large ranges and in error correction [29; 76]. Later, deep recurrent networks trained in a supervised manner to path integrate were shown to learn grid-like units [7; 19; 74], and neural-regressions based work  showed that these supervised deep path integrators achieved the best performance possible at predicting recordings from mouse medial entorhinal cortex, leading the authors to call for better neural data.

However, multiple independent lines of evidence demonstrated that these high \(R^{2}\) deep learning models are worse models of grid cells: (1) The required supervised targets, putative place cells, contradict known biological properties of place cells at both the single cell and population levels ; (2) The grid-like units lack key properties of real grid cells: periodic triangular tuning curves, multiple discrete grid modules, and specific ratios between grid modules [66; 68]; (3) the artificial grid units in some works were statistically indistinguishable from low pass-filtered noise [74; 75]. (4) In terms of evolutionary origins, the path integration objective of high-\(R^{2}\) networks is not a sufficient objective for grid cells, as demonstrated in empirical deep neural network work [42; 41; 68], argued by prior theoretical work [28; 76; 53; 83], and shown by newer deep learning models [31; 85; 22; 70; 86; 87].

To summarize, the neural regressions methodology strongly supported deep learning-based path integrators because the networks achieved high neural predictivity scores, despite their discrepancies with multiple key criteria of neural similarity (listed in Appendix Sec. A). Why?

The Neural Regressions Methodology Reveals the Implicit Biases of Regression, Not Which Candidate Networks Are Similar to the Brain

Schaeffer et al.  made a conjecture: "different [models] achieve different neural predictivity scores because they learn different intrinsic dimensionalities, that then provide richer/poorer bases for linear regressions." Larger models simply provide more basis features for regression, and thus can provide better predictions independent of the similarity with the brain. To test their conjecture, the authors trained the same networks studied by Nayebi et al.  and empirically discovered that

Figure 2: Four independent publications studying four different modalities and brain circuits in three different species found a consistent quantitative heuristic: Test \(R^{2}\) is an affine transformation of the log participation ratio (Eqn. 2). Figures from Spatial Navigation in Mouse Medial Entorhinal Cortex , Vision in Macaque IT Cortex , Audition in Human Cortex. , Language in Human Cortex . Later work  provided a spectral theory of the neural regressions methodology, which reveals results like these are attributable to _general properties of linear regression_, not the brain.

reported test Pearson correlations exhibit an approximately linear-log relationship with a widely-used measure of effective dimensionality called participation ratio (PR)  (Fig 2a).

More precisely, consider \(P\) stimuli, and denote artificial activations with \(M\) units as \(^{P M}\) and biological responses with \(N\) neurons as \(^{P N}\). We fit linear models using \(p<P\) data:

\[(p)\ }}{{=}}\ *{arg\,min}_{ ^{M N}}||_{1:p}\,-_{1:p}||_{F}^{2}+ _{}||||_{F}^{2}\] (1)

Letting \(^{T}=_{i=1}^{P}_{i}_{i}_{i} ^{T}\), Schaeffer et al.  empirically found that approximately:

\[R^{2}()+; \ }}{{=}}\ ^{P}_{i})^{2}}{_{i=1}^{P} _{i}^{2}}\] (2)

Participation ratio (PR) is a linear geometric measure of effective dimensionality: for uniform eigenvalues, the PR is the ambient dimensionality, whereas for a single non-zero eigenvalue, the PR is 1. Concurrent and subsequent work found quantitatively similar results across species, modalities, brain circuits and recording technologies: Elmoznino and Bonner  in deep convolutional networks trained on vision tasks to predict macaque IT cortex (Fig 2b), Tuckute et al.  in deep auditory networks to predict human brain-wide fMRI responses (Fig 2c), and Cheng and Antonello  in language models to predict human brain-wide fMRI responses (Fig. 2d). This finding by four independent research groups across different data modalities, tasks, architectures, species and recording technologies is puzzling. Are these results indicative of some deeper scientific insight into the brain?

In our view, no. _This pattern is attributable to the neural regressions methodology, not the brain_. Participation ratio (PR) was a reasonable first guess but an imprecise one that was subsequently refined into a more complete spectral theory of the regressions methodology. Canatar et al.  showed the normalized error \(E_{g}(p)\) of _any_ linear model \(}(p)\ }}{{=}}\ \,(p)\) is given as:

\[E_{g}(p)\ }}{{=}}\ }(p)- ||_{F}^{2}}{||||_{F}^{2}}\ =\ _{i=1}^{P}\ \ ^{T}_{i}||_{2}^{2}}{|| ||_{F}^{2}}\ \ }{1-}+)^{2}},\] (3)

where \(=_{i=1}^{P}^{2}}{(p_{i}+)^{2}}\) and \(=_{}+_{i=1}^{P}}{p_{i }+}\) must be solved self-consistently. This result says that the focus on PR by previous work was incomplete: PR partially captures the dimensionality of the learnable subspace, but the error _also_ depends on the terms \(\{||Y^{T}v_{i}||_{2}^{2}/||Y||_{F}^{2}\}_{i}\), which express whether the target \(Y\) lies in that subspace. Thus, higher PR can be beneficial to express the task fully, but can also be harmful by being too expressive and then harming sample complexity. This partially explains why ZCA whitening to maximize participation ratio did not achieve exceptional neural predictivity, why increasing the number of covariates does not necessarily increase neural predictivity , and how randomly initialized networks can achieve high neural predictivity [45; 2].

While it may be tempting to think that these empirical results and this spectral theory of neural predictivity have taught us about the brain, note that this theory makes no assumptions about a neural, behavioral, biological, ethological or otherwise meaningful relationship between \(\) and \(\). Rather, as its origin makes clear [8; 13; 12], this theory is fundamentally _a description of linear regression_. Consequently, this leads to the following realization:

_Taken to its extreme, the neural regressions methodology has taught us the implicit biases of our chosen proxy function (e.g., test \(R^{2}\) of linear regression), not which candidate artificial neural networks are actually similar to the brain._

## 4 Discussion

To summarize, NeuroAI lacks canonical definitions of neural similarity, and such notions are likely task-, system-, and question-dependent, as well as difficult to quantify. Rather than facing these challenges, the neural regressions methodology sidesteps them by defining a proxy - for instance, the test \(R^{2}\) of linear regressions fit between biological recordings and artificial activations - and then choosing networks based on this proxy. The networks that win a selection process do so because of the proxy's implicit biases, independent of any meaningful relationship with the brain.

To explain with an analogy, in the field of language modeling, researchers want language models to generate responses preferred by humans. However, collecting human preferences is slow, costly and noisy, so researchers instead train modified language models called reward models to serve as proxies of human preferences. These reward models are a proxy for what we actually care about - human preferences - but the field is willing to use these proxies because the reward models are directly trained to emulate human preferences and are correlated with human preferences empirically ; even so, overfitting to the reward models at the expense of human preferences is still a commonly encountered problem .

In comparison, in computational neuroscience, researchers want models that are most similar to brain system(s) of interest. However, interacting with neural systems and running experiments is slow, costly and noisy, so researchers instead fit neural regressions to serve as proxies of neural similarity. These regressions are a proxy for what we actually care about - neural similarity - but in contrast with reward models, neural regressions are not trained to emulate neural similarity and have no known relationship with neural similarity.

To reiterate an earlier point, we focused on linear regression because of its ubiquity in the literature, but other proxies of neural similarity (e.g., RSA , CKA , SVCCA , Procrustes ) would not escape this critique; rather, _other proxies would simply change the pertinent implicit biases_.

Altogether, these insights suggest that the neural regressions methodology may be flawed, teaching us about the preferences that we as researchers implicitly chose instead of advancing humanity's understanding of the brain. We conclude by calling for a critical and careful re-evaluation in computational neuroscience of how the neural regressions methodology is used and interpreted. For a Future Outlook, please see Appendix Sec. B.