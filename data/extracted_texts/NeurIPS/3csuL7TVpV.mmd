# Decoding-Time Language Model Alignment

with Multiple Objectives

Ruizhe Shi\({}^{1}\)   Yifang Chen\({}^{2}\)   Yushi Hu\({}^{2,3}\)   Alisa Liu\({}^{2}\)

**Hannaneh Hajishirzi\({}^{2,3}\)   Noah A. Smith\({}^{2,3}\)   Simon S. Du\({}^{2}\)**

\({}^{1}\)IIS, Tsinghua University  \({}^{2}\)University of Washington  \({}^{3}\)Allen Institute for AI

This work was done while Ruizhe Shi was visiting the University of Washington.

###### Abstract

Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose **multi-objective decoding (MOD)**, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives. We exploit a common form among a family of \(f\)-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards \(3\) objectives. Moreover, we experiment with MOD on combining three fully-finetuned LMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9-33.3% improvement across three other metrics (_i.e._, Codex@1, GSM-COT, BBH-COT).

## 1 Introduction

Learning from human feedback  has gained significant attention due to its potential for using human-labeled datasets to align language models to human preferences . Among them, alignment approaches such as RLHF (PPO)  and DPO  all model the optimization objective so as to maximize the expected reward from some implicit or explicit reward function, while incorporating KL-divergence from the reference policy as a divergence penalty . However, these algorithms are restricted to only optimizing for a single reward function.

In reality, different use cases and users may prefer different weightings of various alignment objectives. For instance, dialogue agents need to trade off between helpfulness and harmlessness , while question-answering systems can have attributes of relevance, verbosity, and completeness . Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives . Naive methods such as prompt adjustment for particular styles  fail to provide precise control over the nuanced weighting of output characteristics . Curating mixed datasets for the desired combination of objectives is challenging and resource-intensive. Some efforts (e.g., MORLHF  MODPO ) match varying personal preferencesthrough linearly combining reward functions into a single one, but these approaches still necessitate retraining for all possible weightings.

In this work, we tackle the question: _Given a set of policies corresponding to different rewards and linear coefficients for the rewards, can we find a training-free policy corresponding to the interpolated reward?_ We introduce **multi-objective decoding** (**MOD**; see Figure 1), which combines the predictive distributions of individual models trained for single objectives. This approach is inspired by Legendre transform in convex optimization , which allows us to derive a closed-form solution from a family of \(f\)-divergence regularized optimization approaches [9; 39; 47] (e.g., PPO, DPO are optimizing for the reward function with KL-divergence penalty), and its efficient approximation. The resulting method extends prior work employing logit arithmetic for decoding-time alignment [28; 59; 19; 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously. We compare the design of our approach with existing multi-objective alignment approaches in Table 1.

Importantly, our approach allows users to achieve arbitrary weightings of objectives at inference time, avoiding the need for extensive retraining iterations. Additionally, our approach offers users more precise and interpretable control over the customization of AI outputs, thereby enhancing both personalization and performance. We conduct experiments across various tasks including **Reddit Summary**, **Helpful Assistant**, and **Safety Alignment**. Notably, our method can combine models of different scales, and it is effective not only for PPO and DPO models but also can be extended to supervised finetuned (SFT) models. This insight is supported by experiments on combining **13B** DPO models and a **7B** SFT model for **Open Instruction-Following**[49; 20].

**Contributions.** We summarize our contributions as follows.

* We introduce a training-free, simple, yet effective algorithm, MOD, for multi-objective alignment of language models. Given strong-barrier function regularized base policies trained for a single objective, we are able to derive and efficiently decode a closed-form solution for an

    & Number of & Free from & Free from &  \\  & trained LLMs & RM & prompting & \\  MORLHF [52; 3] & \# preferences & ✗ & ✗ & \\ MODPO  & \# preferences & ✗ & ✗ & \\ DPA , CPO , RiC  & 1 & ✗ & ✗ & \\ RS [40; 21] & \# objectives & ✗ & ✗ & same arch. \& init. \\ MOD (ours) & \# objectives & ✗ & ✗ & same tokenizer \\   

Table 1: Overall comparison with other approaches. “Free from RM” refers to not requiring reward models. “Free from prompting” refers to not requiring preference-driven prompts during inference. Generally, the number of preferences is much larger than the number of objectives here. Among them, our approach is the most versatile solution.

Figure 1: Multi-objective decoding. We prepare LMs tuned for each objective in advance. Then, given preference weightings \(w\), input prompt \(x\) and context \(y_{<t}\), \(y_{t}\) is greedily decoded from an algebraic combination of predicted probabilities from each LM, achieving precise control.

interpolated objective with optimality guarantees, based on Legendre transformation. Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28; 59; 19; 30; 62]. See Section 3.
* In extensive experiments, we demonstrate the strong performance of MOD. For instance, compared to parameter merging, MOD achieves a \(12.8\)% overall relative reward improvement when equally optimizing towards three objectives on **Helpful assistant** task. When combining \(3\)**TULU** models, our best configuration significantly reduces Toxigen to nearly zero and results in a \(7.9\)% to \(33.3\)% relative improvement across the other three metrics (Codex@1, GSM-COT, BBH-COT). Additionally, experiments validate that our framework is applicable to SFT models and is still effective for given a mix of positive and negative weights (a case where the traditional training-free baseline does not work), showing its steerability. See Section 4.
* We conduct a thorough theoretical analysis of a broad framework of multi-objective alignment concerning \(f\)-divergence regularization, investigating the necessity of barrier function, optimality guarantees, and error propagation from sub-optimal base policies. We reveal the sub-optimality of the parameter-merging paradigm [40; 21] under a common setting, showing that for most \(f\)-divergence regularization, including the commonly-used KL-divergence, the optimal policy is not guaranteed to lie in the interpolation region of the weights of base policies. See Section 5.

## 2 Preliminaries

There are various ways of defining "multi-objective." In this paper, we take a multi-objective reward function perspective. In this section, we will first give a formal definition of multi-objective reward functions. After that, because we focus exclusively on decoding by combining the predictions of a set of existing single-objective aligned LMs, we will give a formal assumption on each base LM considered in this paper. Finally, we will show the mathematical advantage of those base LMs under such assumptions. Notation is given in Appendix B.

**Multi-objective reward functions.** Existing single-objective alignment methods, including PPO, DPO, and their variants, all explicitly or implicitly assume the existence of a reward function \(:\), such that for each input prompt \(x\) and output response \(y\), there exists a reward signal \((y|x)\). Under the multi-objective setting, we assume there exists a set of reward functions \(\{_{i}\}_{i=1}^{M}\) corresponding to \(M\) objectives. In reality, different people have different preferences for each objective; therefore, we represent such preferences as a normalized vector \(w^{M-1}\). For people with preference \(w\), we care about the weighted reward function \(_{i=1}^{M}w_{i}_{i}(y|x)\) for each sample pair \((x,y)\). This paper focuses on how to maximize such rewards exclusively through decoding by combining the outputs of a set of existing single-objective aligned LMs, denoted as \(\{_{i}\}_{i=1}^{M}\), which are formally defined below.

**Single objective alignment with \(f\)-divergence regularization.** Each policy \(_{i}\) has been optimized for the corresponding reward function \(_{i}\). However, it is well known that greedily optimizing towards maximum rewards can lead to over-optimization and worsen model performance . Therefore, regularization has been incorporated to avoid large deviations from the reference policy. Alignment with KL-divergence regularization has been established as a standard formulation [36; 42; 52; 39; 53; 57]. Recently, a sequential line of work [47; 43] has proposed replacing Reverse KL-divergence with a set of \(f\)-divergences such as Forward KL-divergence, JSD, and \(\)-divergence, which they claim can enhance generation diversity and decrease the expected calibration error  empirically. We observe that all these methods can be analyzed under the framework of \(f\)-divergences, where \(f\) is a _barrier function_ (see Definition 1 and Definition 2 in Appendix D.1 for formal definitions). The closed form of each single-objective aligned LM \(_{i}\) can be written as:

\[_{i}=*{argmax}_{}}_{ x\\ y(|x)}[_{i}(y|x)]- }_{x\\ y_{}(|x)}f(}(y|x)}),\] (1)

where \(\) is a regularization parameter and \(_{}\) is the initial SFT model, _i.e._, the reference policy. For example, if we take \(f(x)=x x\), then the objective can be written as:

\[_{}}_{x \\ y(|x)}[_{i}(y|x)]- (\|_{}),\] (2)

which is the standard optimization problem in [9; 39].

**Strong-barrier function benefits multi-objective decoding.** As discussed above, existing works choose different \(f\) primarily to achieve different regularization behaviors. However, there is an extra property: if the barrier function \(f\) is continuously differentiable and strongly convex on \(_{+}\), we can obtain a closed-form bijection between any single-objective aligned LM \(_{i}\) and the corresponding \(_{i}\) as shown below (initially proposed in , see detailed proof in Lemma1):

\[_{i}(y|x)=_{}(y|x)( f)^{(-1)}( _{i}(y|x)-Z_{i}(x)),\ _{i}(y|x)= f((y|x)}{_{ }(y|x)})+ Z_{i}(x)\,\] (3)

where \(Z_{i}(x)\) is the normalization factor with respect to \(x\). In other words, given the rewards and a prompt \(x\), there is a closed form for the optimal policy, and given the optimal policies and \(x\), there is a closed form for the rewards for every \(y\). Crucially, such closed forms directly result in a possible linear combination of different outputs of \(\{_{i}\}_{i=1}^{M}\), as we will show in our main algorithm. In the rest of the paper, we call an \(f\) with such properties a _strong-barrier function_.

**Formal problem formulation.** Given all those preliminaries, now we are ready to state our formal problem formulation: We are given a reference policy \(_{}\) and a set of base policies \(\{_{i}\}_{i=1}^{M}\) trained for reward functions \(\{_{i}\}_{i=1}^{M}\) under \(f\)-divergence regularization. And we assume that we are unable to access \(_{i}\) directly. Can we find a retraining-free decoding algorithm such that, for any given preference weightings \(w^{M-1}\) and input \(x\), we can obtain an optimal response \(y\) for the weighted multi-objective reward function \(r(y|x)=_{i=1}^{M}w_{i}_{i}(y|x)\), that is regularized by \(_{}\)?

## 3 Proposed Method: Multi-Objective Decoding

### Warm-up: an inefficient decoding version

To decode \(y\), the most direct way is to find a policy \(^{}\) where \(y\) can be sampled from, by solving

\[_{}}r(y|x) x\\ y_{}(|x)}{}f(}(y|x)}) C_{1}\,\]

where \(C_{1}_{+}\) is some threshold constant. Now by leveraging the bijection property of a strong-barrier function, as shown in Eq.3, there exists a naive decoding format \(^{}\) for the dual problem (see detailed proof in Proposition1):

\[^{}(y|x) =_{}(y|x)( f)^{(-1)}(-Z^{}(x )+_{i=1}^{M}w_{i}_{i}(y|x))\] \[=_{}(y|x)( f)^{(-1)}(-Z(x)+_{ i=1}^{M}w_{i} f((y|x)}{_{}(y|x)} ))\,\]

where \(Z(x)\) and \(Z^{}(x)\) are normalization factors. With this form, we can directly combine the outputs from \(\{_{i}\}_{i=1}^{M}\) during decoding. Unfortunately, computing the exact value of the normalization factor is nearly impossible as it requires looping over all possible \(y\) in the output space.

### Towards an efficient algorithm: reformulation and approximation

**Reformulation via Legendre transform.** We make a significant observation: our main motivation is to maximize the sum of weighted multi-objective rewards while avoiding over-optimization (_i.e._, too much deviation from the reference policy). This motivation can be reformulated as keeping the target policy similar to the reference policy in the input region where the reference model already performs well, while optimizing towards larger rewards in regions where the reference policy is highly unaligned with the target rewards. Consequently, we can rewrite the optimization problem as:

\[_{y}_{}(y|x),r(y|x) C_{2}\,\] (4)

where \(C_{2}_{+}\) is some threshold constant. Based on this observation and Legendre transform in convex optimization , we prove our key theorem which gets rid of the normalization factor and leads to the MOD algorithm, as follows (see detailed proof in AppendixD.3).

**Theorem 1** (Informal key theorem).: _There exists a certain \(C_{2}\) such that:_

\[*{argmax}_{y}_{}(y|x)( f)^ {(-1)}(_{i=1}^{M}w_{i} f((y|x)}{_{ }(y|x)}))\,\] (5)

_is the optimal solution for this revised optimization problem (4)._Notice that, without much performance loss, we can further improve efficiency using _greedy search_, thus transforming response-level decoding into efficient token-level decoding.

### Main algorithm: efficient decoding with optimality for strong-barrier function

Based on this new closed form Eq. (5), we are ready to show the main algorithm.

At each timestep \(t\), we condition the reference policy \(_{}\) and policies \(\{_{i}\}_{i=1}^{M}\) on the prompt \(x\) and context \(y_{<t}\) to obtain the next token \(y_{t}\) from the predicted probabilities of each policy:

\[*{argmax}_{s}\;_{}(y_{<t},s|x)(  f)^{(-1)}(_{i=1}^{M}w_{i} f((y_{<t },s|x)}{_{}(y_{<t},s|x)}))\.\] (6)

The full pipeline is shown in Appendix C.1. Specifically, in main experiments, we implement our algorithm by choosing \(f(x)=x x\), _i.e._, the regularization term is Reverse KL-divergence as used in PPO and DPO, and Eq. (6) reduces to a simple token-wise decoding rule:

\[*{argmax}_{s}\;_{i=1}^{M}_{i}^{w_{i}}(y_{<t},s| x)\,\] (7)

equivalent to linearly combining logits [32; 30] of each model with preference weightings.

**Comparisons with other approaches.** Our algorithm is significantly more efficient than retraining-based algorithms. In practice, the number of objectives is easily enumerable (e.g., \(<5\) in [50; 12]), making it feasible to finetune an LM for each objective. In contrast, the number of preferences cannot be bounded due to the variability among users , which suggests that retraining-based algorithms like MORLHF and MODPO need to compute an impractical amount of times in order to match the preference of every user. Regarding memory efficiency, MOD requires loading multiple models simultaneously, which consume relatively higher memory cost. However, we mitigate this cost by ensembling a set of low-rank adapters or using distributed deployment in implementation. A comprehensive comparison with these baselines is shown in Table 1.

## 4 Experiments

Here, we demonstrate the effectiveness of MOD through four sets of experiments: 1) PPO models for the **Reddit Summary** task. 2) PPO models for the **Helpful Assistants** task. 3) \(f\)-DPO models for the **Safety Alignment** task. 4) SFT and DPO models for the **Open Instruction-Following**[49; 20] task. Additional experiments on the **HelpSteer** task are provided in Appendix F.4.

### Experiment setup

**Baselines.** We adopt the representative parameter-merging method and retraining approaches as our baselines. Rewarded soups (RS)  linearly merge each model's parameters according to preference weightings, as \(=_{i=1}^{M}w_{i}_{i}\), where \(_{i}\) denotes the parameters of \(_{i}\). MORLHF  optimizes for the weighted multi-objective reward function \(_{i=1}^{M}w_{i}_{i}\) using PPO, with the same configurations as training for single objective. MODPO  uses \(_{1}\)'s output as an implicit reward signal of \(_{1}\) and inserts it into the DPO objective for \(_{2}\) to optimize for \(w_{1}_{1}+w_{2}_{2}\), with the same configurations as training for single objective.

**Visualization.** We plot the Pareto frontier to visualize the obtained reward of each attribute for a set of preference weightings. The performance can be measured through the area of the Pareto frontier, which reflects the optimality and uniformity of the solution distribution . The reward is evaluated by off-shelf reward models. It is worth noting that MOD is free from reward models, and the use is merely for evaluation.

**Example generations.** It is important to note that, due to issues like over-optimization , solely showing higher rewards is not a complete argument in favor of a new RLHF method. Since MOD does not yield a sampling policy, which make it impossible to directly measure \((\|_{})\) as prior work , we demonstrate example generations in Appendix F.6 to indicate that they do not deviate much from \(_{}\).

More implementation details regarding to tasks, datasets, SFT, reward models, training, and evaluation can be found in Appendix E.

### Results

**Reddit Summary.** By supervised finetuning a **Llama2-7B** model on Summarize-from-Feedback dataset , we obtain the reference policy \(_{}\). And then we obtain \(_{1},_{2}\) by tuning \(_{}\) using PPO for two off-shelf reward models (see details in Appendix E) which measures summary quality and faithfulness, respectively. Then we show Pareto frontiers of MOD, RS, and MORLHF in Figure 2, with preference weightings \(w\{(i/10,1-i/10):i\{0,1,,10\}\}\), demonstrating the superiority of MOD over baselines.

**Helpful Assistant.** By supervised finetuning a **Llama2-7B** model on Anthropic-HH dataset , we obtain the reference policy \(_{}\). And then we obtain \(_{1},_{2},_{3}\) by tuning \(_{}\) using PPO for three off-shelf reward models (see details in Appendix E) which evaluate helpfulness, harmlessness and humor, respectively. The Pareto frontiers of MOD, RS and MORLHF for each two-objective pairs are shown in Figure 3. MOD prominently beats RS for each reward pair, and lags behind MORLHF in balancing harmlessness and humor, while MORLHF is more costly. We explore the \(3\)-objective setting on the **Helpful Assistant** task, demonstrating that MOD can effectively balance advantages of each model and outperforms RS. More results are provided in Appendix F.2.

**Safety Alignment.** Based on results reported in , we mainly focus on \(f\)-DPO with Reverse KL-divergence, JSD, \(0.3\)-divergence and \(0.5\)-divergence in experiments. We deploy an off-shelf **Alpaca-7B** model as \(_{}\) and train \(_{1f},_{2f}\) using \(f\)-DPO on two pair-comparison BeaverTails-\(10\)K  datasets: one is _Better_ and the other is _Safer_. We show Pareto frontiers of MOD, RS, and MODPO for each \(f\)-divergence in Figure 4. Experimental results demonstrate that MOD generally outperforms RS across multiple \(f\)-divergences. The retraining baseline MODPO is only applicable to Reverse KL-divergence, and MOD is much more convenient despite a slight performance gap.

Moreover, we can apply not-all-positive preference weightings \(w^{M}\) as long as \(_{i=1}^{M}w_{i}=1\), thus allowing us to optimize for a reward function \(-\). In Table 2, we present the scores of MOD, with preference weightings set as \(w\{(i,1-i):i\}\). Example generations in Table 3 (more in Appendix F.3) validate that MOD successfully handles this, while RS fails to generate meaningful responses. This phenomenon indicates that we do not even need to specifically tune an unsafe model as in , since the knowledge of \(-\) is indeed learned when being tuned for \(\).

**Open Instruction-Following.** Finally, we conduct experiments on larger-scale models for general objectives, including two DPO models, **Tulu-2-HH-13B** tuned on Anthropic-HH  for

[MISSING_PAGE_FAIL:7]

_and the objectives \(J_{1},J_{2},,J_{M}\) representing reward functions \(_{1},_{2},,_{M}\) w.r.t. \( I_{f}(\|_{})\), s.t. Hypothesis 1 does not hold._

**Remark 1** (Clarification).: _It is commonly adopted in previous studies [65; 42] that the network receives the same inputs \(z_{0}\). Despite the competitive results exhibited in prior works [51; 40; 21], this theorem reveals that parameter-merging lacks a theoretical guarantee in practical scenarios. Besides, although Hypothesis 1 may hold, the mapping from preference weightings \(w\) to the optimal merging weightings \(\) are intricate, and thus simply picking \(\) as \(w\), can yield sub-optimal results._

**Another perspective of the same initialization.** We can also look into scenarios where only the parameters of the last several layers of \(_{1},_{2},,_{M}\) can be different from \(_{}\). 1) If the last layer is a _linear projection_, then it is equivalent to MOD w.r.t. \((\|_{})\), namely linearly combining the logits. 2) If the last layer is _self-attention_, then it can be easily hacked by reversing the sign of \(Q,K\) matrices in this layer, which does not influence the value of \(Q^{}K\), but significantly harms the effect of parameter-merging. A motivating example is shown in Appendix F.1.

### Necessity of barrier function

Extending the results of  to the multi-objective setting, we prove the necessity of \(f\) being barrier functions to find an optimal policy \(^{}\) for multi-objective alignment. See detailed proof in Appendix D.2.

**Theorem 3**.: _If \(f\) is not a barrier function, then for \( C_{+}\), \(N_{ 4}\), \(M_{ 2}\), \(=\{y_{i}\}_{i=1}^{N}\), any multi-objective decoding or merging algorithm \(:^{M+1}^{M-1}\), there exists a reference policy \(_{}\), policies \(\{_{i}\}_{i=1}^{M}\) and \(^{}\), reward functions \(\{_{i}\}_{i=1}^{M}\), preference weightings \(w^{M-1}\) and \(_{+}\), s.t. \(_{i}\) is the optimal policy for \(_{i}\) w.r.t. \( I_{f}(\|_{})\) (see Definition 1 in Appendix D.1), \( i[M]\), but_

\[}_{y_{,w}}[_{i=1}^{M}w_{i} _{i}(y)]}_{y^{}}[ _{i=1}^{M}w_{i}_{i}(y)]-C\;,\]

\[}_{y_{,w}}[_{i=1}^{M}w_{i} _{i}(y)]- I_{f}(_{,w}\|_{} )}_{y^{}}[_{i=1}^{M}w_{i} _{i}(y)]- I_{f}(^{}\|_{})-C\;,\]

_where \(_{,w}(y):=_{},_{1},_{2},,_{M},w(y)\;.\)_

**Remark 2** (Motivating example).: _Here we provide a motivating example where \(f 0\): let \(M=4\), \(_{1}(y_{1})=_{2}(y_{2})=1\), \(_{1}(y_{2})=_{2}(y_{1})=-1\), \(_{1}(y_{3+k})=_{2}(y_{3+k})=0\), \(_{1}(y_{4-k})=_{2}(y_{4-k})=1/2\), where \(k\{0,1\}\). Then the optimal policy for \(_{1}\) is \(_{1}(y_{i}):=_{1i}\), for \(_{2}\) is \(_{2}(y_{i}):=_{2i}\), and for \(_{1}/2+_{2}/2\) is \(^{}(y_{i}):=_{4-k,i}\). Thus \(_{,w}\) cannot fit \(^{}\) for \(k=0,1\)._

**Crucial role of the barrier function.** We can apply this theorem to any algorithm which solely utilizes base policies, including RS and MOD. And thus, a barrier function regularization is crucial in multi-objective alignment to bridge different policies, though it was originally intended to prevent degeneration (see Table 3 in ) in single-objective alignment. Additionally, the same as a general barrier in _interior point methods_, it obviates the need for introducing slack variables as in . This explains why we should not use non-barrier \(f\)-divergences such as total variation and chi-squared.

### Sub-optimality error propagation

While we previously assumed that each base policy is the optimal solution of Eq. (1), here we provide a guarantee for performance when the base policies are sub-optimal. See proof in Appendix D.4.

**Theorem 4** (KL-divergence perspective).: _Given a reference policy \(_{}\), policies \(\{_{i}\}_{i=1}^{M}\), reward functions \(\{_{i}\}_{i=1}^{M}\), and \(_{+}\). Denote the optimal policy for \(_{i}\) w.r.t. \(\,(\|_{})\) as \(p_{i}\), \( i[M]\). For the reward function \(_{i=1}^{M}w_{i}_{i}\) w.r.t. \(\,(\|_{})\), the performance difference of policy \(_{w}(|x)_{i=1}^{M}_{i}^{w_{i}}(|x)\) from optimal is \(V^{}-V\). If for \( i\{1,,M\},\ x\), we have: (i) \(_{y}| p_{i}(y|x)-_{i}(y|x)| \;,\) (ii) \((_{}(|x)\|_{i}(|x)) C\), \((_{}(|x)\|p_{i}(|x)) C\;,\) where \(,C_{+}\), then_

\[V^{}-V 2(C)\;.\]

**Remark 3** (Interpretation of conditions).: _Since the primal problem of Eq. (2) restricts the divergence penalty under a certain threshold, and people usually adopt an early-stopping technique in practice, \(p_{i}\) and \(_{i}\) will not deviate from \(_{}\) too much, thus \(C\) can be viewed as a small constant. When each \(_{i}\) is close to optimal, the relative distance reflected by \(\) is small as well. The expected calibration error can also be bounded, shown in Proposition 4._

### Beyond \(f\)-divergence regularized alignment and multi-objective decoding

While our main results are based on \(f\)-divergence regularized aligned LMs and aimed at multi-objective decoding, our framework is also applicable to using SFT models and explaining the effectiveness of other existing decoding algorithms. For example, proxy-tuning  tunes only a smaller LM, then applies the difference between the logits of the small tuned and untuned LMs to shift the predictions of a larger untuned model. Its theoretical justification is provided by our framework, under certain assumptions. We provide insights on this line of work [28; 59; 19] and derivations of some other related works [30; 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.

## 6 Related Work

**Algorithms for aligning LMs to human preferences.** The widely used RLHF (PPO) approach [36; 42; 52] optimizes over rewards with Reverse KL-divergence as a penalty, where the reward models are learned from human preference datasets. DPO  leverages the Bradley-Terry assumption  to directly optimize the same objective on preferences, in a supervised manner. \(\)-PO  further modifies the reward term to be optimized as other mappings from preference pairs; f-DPO  replaces Reverse KL-divergence with other divergence measures. In addition, there are other efforts exploring alternative objectives and frameworks: SLiC-HF [61; 60] refer to the alignment process as sequence likelihood calibration; SPIN  iteratively improves the model by leveraging synthetically generated data, thereby circumventing the need for human feedback; OPO  employs established norms as constraints, achieving training-free alignment; and Lyu _et al._ highlight the crucial role of prompt templates. In this work, we mainly focus on RLHF (PPO), DPO, and their extensions.

**Decoding-time algorithms for controllable generation.**_Response-level_ decoding algorithms sample a whole output \(y\) from an anticipated probability distribution \(p\). To achieve this goal, energy-based methods are adopted in many works [37; 25], which involves continuous optimization for LMs to obtain gradient information. Kumar _et al._ view this task as maximizing \( p(y)\) while satisfying some constraints, and use simultaneous gradient descent to solve it. _Token-level_ decoding algorithms decode token \(y_{t}\) at timestep \(t\), and are usually more efficient. Among them, Mudgal _et al._, Liu _et al._ deploy value models to guide the decoding process; DeRa  works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28; 59; 19] tunes a small model and applies it to steer a larger base model by operating on logits.

**Multi-objective LMs alignment.** Multi-objective alignment is the task of aligning language models to multiple objectives simultaneously. This is important for managing tradeoffs among different dimensions [44; 3] and catering to the diverse needs of users [21; 13]. Approaches for multi-objective alignment fall into the following categories: 1) _Retraining_. The most natural approach to solve multi-objective alignment is to retrain for a linearly combined multiple reward functions (MORLHF [52; 3]). And MODPO  retrains the model in a reward-model-free way, by learning a flexible reward representation and directly training on a fixed preference dataset. 2) _Parameter-merging_. This line of work [40; 21; 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by  and its other applications [41; 26]. Jiang _et al._ achieve another kind of model-merging through reranking and fusion on outputs. 3) _Preference-conditioned prompting_. The preference-conditioned learning approaches [64; 4] train a policy conditioned on preference weightings to maximize the expected rewards. This concept is reflected in LMs alignment as preference-conditioned prompting: this line of work [56; 48; 18] directly presents the preference weightings in prompts after a finetuning process. The latter two paradigms are more efficient, while relying heavily on either the reduced mis-specification hypothesis  or unguaranteed OOD generalization ability , posing challenges in terms of interpretability and robustness.

Conclusion

We propose MOD, a simple, training-free yet effective algorithm for multi-objective LMs alignment. By addressing the challenges of retraining and resource-intensive processes, our method provides a decoding-time solution while offering insights into the broader applicability of combining differently tuned models. Through extensive analysis and empirical evidence, we demonstrate the effectiveness and practicality of our method under the \(f\)-divergence framework, paving the way for improving LM performance across diverse tasks and use cases.

It is also important to acknowledge the limitations of our work. 1) The analysis is primarily based on tabular setting , not taking function approximation error into consideration. 2) Decoding from a response-level probability distribution at the token level may lead to degraded performance, which is likely to be alleviated by energy-based approaches .