# Boosting Alignment for Post-Unlearning Text-to-Image Generative Models

Myeongseob Ko

Virginia Tech

myeongseob@vt.edu

&Henry Li

Yale University

henry.li@yale.edu

&Zhun Wang

University of California, Berkeley

zhun.wang@berkeley.edu

&Jonathan Patsenker

Yale University

jonathan.patsenker@yale.edu

&Jiachen T. Wang

Princeton University

tianhaowang@princeton.edu

&Qinbin Li

University of California, Berkeley

liqinbin1998@gmail.com

&Ming Jin

Virginia Tech

jimming@vt.edu

&Dawn Song

University of California, Berkeley

dawnsong@berkeley.edu

&Ruoxi Jia

Virginia Tech

ruoxijia@vt.edu

Equal contributions

###### Abstract

Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update. In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, thus outperforming state-of-the-art baselines. Our code will be made available at [https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git](https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git).

## 1 Introduction

Large-scale text-to-image generative models have recently gained considerable attention for their impressive image-generation capabilities. Despite being at the height of their popularity, these models, trained on vast amounts of public data, inevitably face concerns related to harmful content generation  and copyright infringement . Althoughexact machine unlearning--retraining the model by excluding target data--is a direct solution, its computational challenge has driven continued research on approximate machine unlearning.

To address this challenge, recent studies (Fan et al., 2023; Gandikota et al., 2023; Heng and Soh, 2024), have introduced approximate unlearning techniques aimed at boosting efficiency while preserving effectiveness. These approaches have successfully demonstrated the ability to remove target concepts while maintaining the model's general image generation capabilities, with generation quality assessed using the Frechet Inception Distance. However, these studies generally overlook the impact of unlearning on image-text alignment, which pertains to the semantic accuracy between generated images and their text descriptions (Lee et al., 2024). While pretrained generative models generally demonstrate high alignment scores, our study reveals a critical gap: state-of-the-art unlearning techniques fall short in achieving comparable text-image alignment scores after unlearning, as illustrated in Figure 1. This could lead to potentially problematic behaviors in real-world deployments, necessitating further investigation.

We attribute the failure of existing techniques to maintain text-image alignment to two primary factors. Firstly, the unlearning objective often conflicts with the goal of maintaining low loss on the retained data, illustrating the competitive nature of these two objectives. Traditionally, approaches to optimizing these objectives have simply aggregated the gradients from both; however, this method of updating the model typically advances one objective at the expense of the other. Hence, while these approaches may successfully remove target concepts, they often compromise text-image alignment for retained concepts in the process. Secondly, current methods employ a simplistic approach to constructing a dataset for loss minimization on retained concepts. For example, in Fan et al. (2023), this dataset is composed of images generated from a single prompt associated with the concept to be removed. This lack of diversity in the dataset might lead to overfitting, which in turn hampers the text-image alignment.

To address these issues, we propose a principled framework designed to optimally balance the objectives of unlearning the target data and maintaining performance on the remaining data at each update iteration. Specifically, we introduce the concept of the _restricted gradient_, which allows for the optimization of both objectives while ensuring monotonic improvement in each objective. Furthermore, we have developed a deliberate procedure to enhance data diversity, preventing the model from overfitting to the limited samples in the remaining dataset. To the best of our knowledge, the strategic design of the forgetting target and remaining sets has not been extensively explored in the existing machine unlearning literature. In our evaluation, we demonstrate the improvement in both forgetting quality and alignment on the remaining data, compared to baselines. For example, our

Figure 1: Generated images using SalUn(Fan et al., 2023), ESD(Gandikota et al., 2023), and Ours after unlearning given the condition. Each row indicates different unlearning tasks: nudity removal, and _Van Gogh_ style removal. Generated images from our approach and SD(Rombach et al., 2022) are well-aligned with the prompt, whereas SalUn and ESD fail to generate semantically correct images given the condition. On average, across 100 different prompts, SalUn shows the lowest clip alignment scores (0.305 for nudity removal and 0.280 for _Van Gogh_ style removal), followed by ESD(0.329 and 0.330, respectively). Our approach achieves scores of 0.350 and 0.352 for these tasks, closely matching the original SD scores of 0.352 and 0.348.

evaluation in nudity removal demonstrates that our method effectively reduces the number of detected body parts to zero, compared to 598 with the baseline stable diffusion (SD) [Rombach et al., 2022], 48 with erased stable diffusion (ESD-u), and 3 with saliency map-based unlearning (SalUn) [Fan et al., 2023]. Particularly, while achieving this effective erasing performance, our method reduces the alignment gap to SD by 11x compared to ESD-u and by 20x compared to SalUn on the retained test set.

## 2 Related Work

### Machine Unlearning

Machine unlearning has primarily been propelled by the "Right to be Forgotten" (RTBF), which upholds the right of users to request the deletion of their data. Given that large-scale models are often trained on web-scraped public data, this becomes a critical consideration for model developers to avoid the need for retraining models with each individual request. In addition to RTBF, recent concerns related to copyrights and harmful content generation further underscore the necessity and importance of in-depth research in machine unlearning. The principal challenge in this field lies in effectively erasing the target concept from pre-trained models while maintaining performance on other data. Recent studies have explored various approaches to unlearning, including the exact unlearning method [Bourtoule et al., 2021] and approximate methods such as using negative gradients, fine-tuning without the forget data, editing the entire parameter space of the model [Golatkar et al., 2020]. To encourage the targeted impact in the parameter space, [Golatkar et al., 2020, Foster et al., 2024] proposed leveraging the Fisher information matrix, and [Fan et al., 2023] leveraged a gradient-based weight saliency map to identify crucial neurons, thus minimizing the impact on remaining neurons. Furthermore, data-influence-based debiasing and unlearning have also been proposed [Chen et al., 2024, Bae et al., 2023]. Another line of work leverages mathematical tools in differential privacy [Guo et al., 2019, Chien et al., 2024] to ensure that the model's behavior remains indistinguishable between the retrained and unlearned models.

### Machine Unlearning in Diffusion Models

Recent advancements in text-conditioned generative models [Ho and Salimans, 2022, Rombach et al., 2022], trained on extensive web-scraped datasets like LAION-SB [Schuhmann et al., 2022], have raised significant concerns about the generation of harmful content and copyright violations. A series of studies have addressed the challenge of machine unlearning in diffusion models [Heng and Soh, 2024, Gandikota et al., 2023, Zhang et al., 2023a, Fan et al., 2023]. One approach [Heng and Soh, 2024] interprets machine unlearning as a continual learning problem, showing effective removal results in classification tasks by employing Bayesian approaches to continual learning [Kirkpatrick et al., 2017], which enhance unlearning quality while maintaining model performance using generative reply [Shin et al., 2017]. However, this approach falls short in removing concepts such as nudity compared to other methods [Gandikota et al., 2023]. Another proposed method [Gandikota et al., 2023] guides the pre-trained model toward a prior distribution for the targeted concept but struggles to preserve performance. The most recent work [Fan et al., 2023] proposes selectively damaging neurons based on a saliency map and random labeling techniques, although this method tends to overlook the quality of the remaining set, focusing on improving the forgetting quality, which does not fully address the primary challenges in the machine unlearning community. Although [Bae et al., 2023] presents a similar multi-task learning framework for variational autoencoders, their work does not show the optimality of their solution, and their experiments mainly focus on small-scale models, due to the computational expense associated with influence functions.

## 3 Our Approach

We study the efficacy of our approach in unlearning by removing target classes from class-conditional diffusion models or eliminating specific concepts from text-to-image models while maintaining their general generation capabilities. We will call the set of data points to be removed as the _forgetting dataset_. To set up the notations, let \(D\) denote the training set and \(D_{f} D\) be the forgetting dataset. We will use \(D_{r}=D D_{f}\) to denote the _remaining dataset_. Our approach only assumes access to some representative points for \(D_{f}\) and \(D_{r}\). As discussed later, depending on specific applications, these data points can be either directly sampled from \(D_{f}\) and \(D_{r}\) or generated based on the high-level concept of \(D_{f}\) to be removed. With a slight abuse of notation, we will use \(D_{r}\) and \(D_{f}\) to also denote the actual representative samples used to operationalize our proposed approach. Furthermore, we denote the model parameter by \(\). Let \(l\) be a proper learning loss function. The loss of remaining data and that of forgetting data are represented by \(_{r}():=_{z D_{r}}l(,z)\) and \(_{f}():=-_{z D_{f}}l(,z)\), respectively, where \(\) is a weight adjusting the importance of forgetting loss relative to the remaining data loss. We term \(_{r}\) and \(_{f}\)_remaining loss_ and _forgetting loss_, respectively. We note that in the context of diffusion models, loss function \(l\) is defined as \(l=_{t,x_{0},(0,1)}[\|-e_{ }(x_{t},t)\|^{2}]\), where \(x_{t}\) is a noisy version of \(x_{0}\) generated by adding Gaussian noise to the clean image \(x_{0} p_{}(x)\) at time step \(t\) with a noise scheduler, and \(e_{}(x_{t},t)\) is the model's estimate of the added noise \(\) at time \(t\)[Xu et al., 2023, Ho et al., 2020]. For text-to-image generative models, the loss function \(l\) is specified as \(l=_{t,q_{0},c,}[\|-_{}(q_{t},t, )\|^{2}]\), where \(q_{0}\) is an encoded latent \(q_{0}=(x_{0})\) with encoder \(\), and \(q_{t}\) is a noisy latent at time step \(t\). The noise prediction \(_{}(q_{t},t,)\) is conditioned on the timestep \(t\) and a text \(\).

Optimizing the Update.Similar to existing work Fan et al. , our objective is to find an unlearned model with parameters \(_{u}\), starting from a pre-trained model with weights \(_{0}\), such that the model forgets the target concepts in \(D_{f}\) while maintaining its utility on the remaining dataset \(D_{r}\). Formally, we aim to maximize the forget error on \(D_{f}\), represented by \(_{f}()\), while minimizing the retain error on \(D_{r}\), represented by \(r()\). This can be formulated as \(_{}_{r}()+_{f}()\), where our approach applies iterative updates to achieve both goals simultaneously. A simple approach to optimize this objective, often adopted by existing work, is to calculate the gradient \(_{r}()+_{f}()\) and use it to update the model parameters at each iteration. However, empirically, we observe that the two gradients usually conflict with each other, i.e., the decrease of one objective is at the cost of increasing the other; therefore, in practice, this approach yields a significant tradeoff between forgetting strength and model utility on the remaining data. In this work, we aim to present a principled approach to designing the update direction at each iteration that more effectively handles the tradeoff between forgetting strength and model utility on the remaining data. Our key idea is to identify a direction that achieves a monotonic decrease of both objectives.

To describe our algorithm, we briefly review the directional derivative.

**Definition 1** (Directional Derivative).: _The directional derivative [Spivak, 2006] of a function \(\) at \(\) in the direction of \(\) is written as_

\[D_{}()=_{h 0}(+h )}{h}. \]

This special form of the derivative has the useful property that its maximizer can be related to the gradient \(_{}()\), which we formally state below.

**Theorem 2** (Directional derivative maximizer is the gradient).: _Let \(\) be a function on \(\). Then the maximum value of the directional derivative of \(\) at \(\) is \(|()|\) the \(^{2}\) norm of its gradient. Moreover, the direction \(\) is the gradient itself, i.e.,_

\[*{arg\,max}_{}\,D_{}= (). \]

In unlearning, we are specifically interested in the gradient of two losses, the forgetting loss \(_{f}\) and the remaining loss \(_{r}\). Moreover, we seek gradient directions that simultaneously improve on both. This motivates the _restricted gradient_, which we define below.

**Definition 3** (Restricted gradient, local form for minimization).: _The negative restricted gradient of two losses \(_{},\,_{}\) is any direction \(\) at \(\) satisfying_

\[_{}\,D_{}_{}+_{ }() D_{}_{}()\ \ \ 0, D_{}_{}( )\ \ \ 0.\]

Intuitively, with the restricted gradient we seek to define the ideal direction for unlearning. We would like to optimize the joint loss \(=_{r}+_{f}\) subject to the condition that at every parameter update step, \(_{r}\) and \(_{f}\) experience monotonic improvement. This is precisely the step prescribed by the _negative_ restricted gradient. Since the learning rates used to fine-tune the parameters in the unlearning process are typically quite small, we can approximate the updated loss at each iteration via a simple first-order Taylor expansion. In this case, the restricted gradient takes a simple form.

**Theorem 4** (Characterizing the restricted gradient under linear approximation).: _Given \(\), suppose that \(_{r}(+)-_{r}() _{r}\) and \(_{f}(+)-_{f}() _{f}\). The restricted gradient can be written as_

\[*{arg\,min}_{}D_{}(_{f}+_{r})()=_{f}^{}+_{r}^{}, \]

_where_

\[_{f}^{}=_{f}-_{f} _{r}}{\|_{r}\|^{2}}_{r}, _{r}^{}=\;_{r}-_{f} _{r}}{\|_{f}\|^{2}}_{ f}, \]

_when we have conflicting unconstrained gradient terms, i.e. \(_{f}_{r}<0\)._

The theorem presented demonstrates that the restricted gradient is determined by aggregating the modifications from \(_{f}\) and \(_{r}\). This modification process involves projecting \(_{f}\) onto the normal vector of \(_{r}\), yielding \(_{f}^{}\), and similarly projecting \(_{r}\) onto the normal vector of \(_{f}\), resulting in \(_{r}^{}\). The optimal update, as derived in Theorem 4, is illustrated in Figure 2. Notably, when \(_{f}\) and \(_{r}\) have equal norms, the restricted gradient matches the direct summation of the two original gradients, namely, \(_{f}+_{r}\). However, it is more common for the norm of one gradient to dominate the other, in which case the restricted gradient provides a more balanced update compared to direct aggregation.

**Remark 1**.: _We wish to highlight an intriguing link between the gradient aggregation mechanism presented in Theorem 4 and an existing method to address gradient conflicts across different tasks in multi-task learning. This restricted gradient coincides exactly with the gradient surgery procedure introduced in Yu et al. (2020). While their original paper presented the procedure from an intuitive perspective, our work offers an alternative viewpoint and rigorously characterizes the objective function that the gradient surgery procedure optimizes._

Diversity \(D_{r}\).Since \(D D_{f}\) is usually of enormous scale, it is infeasible to incorporate all of them into the remaining dataset \(D_{r}\) for running the optimization. In practice, one can only sample a subset of points from \(D_{r}\). In our experiments, we find that the diversity of \(D_{r}\) plays an important role in maintaining the model performance on the remaining dataset, as seen in Section 4.2. Therefore, we propose procedures for forming a diverse \(D_{r}\). For models with a finite set of class labels, such as diffusion models trained on CIFAR-10, we adopt a simple procedure of maintaining an equal number of samples for each class in \(D_{r}\). Our ablation studies in Section 4.4 show that this is more effective in maintaining model performance on the remaining dataset than more sophisticated procedures, such as selecting the most similar examples to the forgetting samples. The intuitive reason is that reminding the model of as many fragments as possible related to the remaining set during each forgetting step is crucial. By doing so, it leads to finding a representative restricted descent direction, which helps the model to precisely erase the forget data while maintaining a state comparable to the original model. When the text input is unconstrained, such as in the stable diffusion model setting, to strategically design diverse information, we propose the following procedure to generate \(D_{r}\) based on the concept to be forgotten, denoted by \(c\). Using a large language model (LLM), we first generate diverse text prompts related to concept \(c\), yielding prompt set \(_{c}\). These prompts are then modified by removing all references to \(c\), creating a parallel set \(\). By passing both \(_{c}\) and \(\) through the target diffusion model, we obtain corresponding image sets \(_{c}\) and \(\). This process allows us to construct our final datasets: \(D_{f}=\{(x,y) x_{c},y_{c}\}\) and \(D_{r}=\{(x,y) x,y\}\). Example prompts and detailed descriptions are provided in Appendix D.

Figure 2: Visualization of the update. We show the update direction (gray) obtained by (a) directly summing up the two gradients and (b) our restricted gradient.

Experiment

In this study, we address the crucial challenge of preventing undesirable outputs in text-to-image generative models. We begin by examining class-wise forgetting with CIFAR-10 diffusion-based generative models, where we demonstrate our method's ability to selectively prevent the generation of specific class images (Section 4.2). We then explore the effectiveness of our approach in removing nudity and art styles (Section 4.3) to address real-world concerns of harmful content generation and copyright infringement. We further study the impact of data diversity (Section 4.4) as well as the sensitivity of our method to hyperparameter settings (Section 4.4).

### Experiment Setup

For our CIFAR-10 experiments, we leverage the EDM framework (Karras et al., 2022), which introduces some modeling improvements including a nonlinear sampling schedule, direct \(_{0}\)-prediction, and a second-order Heun solver, achieving the state-of-the-art FID on CIFAR-10. For stable diffusion, we utilize the pre-trained Stable Diffusion version 1.4, following prior works. Both implementations require two key hyperparameters: the weight \(\) of the gradient descent direction relative to the ascent direction, and the loss truncation value \(\), which prevents unbounded loss maximization during unlearning. Detailed hyperparameter configurations are provided in Appendix C. For dataset construction, we used all samples in each class for the CIFAR-10 forgetting dataset and 800 samples for Stable Diffusion experiments. Considering the practical constraints of accessing complete datasets in real-world scenarios, we construct the remaining dataset \(D_{r}\) by sampling 1% of data from each retained class, yielding a total of 450 samples for CIFAR-10 (50 from each of the 9 non-target classes) and 800 samples for Stable Diffusion.

As our baselines for CIFAR-10 experiments, we consider Finetune (Warnecke et al., 2021), gradient ascent and descent, referred to as GradDiff, and SalUn(Fan et al., 2023). For concept removal, our baselines include the pretrained diffusion model SD(Rombach et al., 2022), erased stable diffusion ESD(Gandikota et al., 2023), and SalUn(Fan et al., 2023). To fairly compare, We further consider the variants of ESD, depending on the unlearning task. We note that we do not consider the baseline by (Heng and Soh, 2024) due to its demonstrated limited performance in nudity removal, compared to ESD. Our approach is referred to as RG when applied only with the restricted gradient, and RGD when data diversity is incorporated.

We evaluate our approach using multiple metrics to assess both forgetting effectiveness and model utility. For CIFAR-10 experiments, we measure: 1) unlearning accuracy (UA), calculated as 1-accuracy of the target class, 2) remaining accuracy (RA), which quantifies the accuracy on non-target classes, and 3) Frechet Inception Distance (FID). We observed that standard CIFAR-10 classifiers demonstrate inherent bias when evaluating generated samples from unlearned classes, predominantly assigning these noise-like images to a particular class among the ten categories--a limitation arising from their training exclusively on clean class samples. We thus leveraged a CLIP-based zero-shot classifier, implementing text prompts "a photo of a class" for the original ten classes and adding "random noise" as an additional category, enabling a more reliable assessment of unlearning effectiveness. We generate 50K images for FID calculation. For concept removal in Stable Diffusion, we assess forgetting effectiveness using Nudenet (Bedapudi, 2019), which detects exposed body parts in generated images prompted by I2P (Schramowski et al., 2023). After filtering prompts with

    &  \\   & UA \(\) & RA \(\) & FID \(\) \\  Finetune & 0.211\({}_{ 0.126}\) & **0.791\({}_{ 0.023}\)** & **4.252\({}_{ 0.482}\)** \\ SalUn & 0.512\({}_{ 0.173}\) & 0.434\({}_{ 0.051}\) & 14.40\({}_{ 3.242}\) \\ GradDiff & **1.000\({}_{ 0.000}\)** & 0.734\({}_{ 0.021}\) & 14.09\({}_{ 2.531}\) \\  RG (Ours) & **1.000\({}_{ 0.000}\)** & 0.752\({}_{ 0.018}\) & 9.813\({}_{ 1.863}\) \\ RGD (Ours) & **1.000\({}_{ 0.000}\)** & 0.771\({}_{ 0.016}\) & 6.539\({}_{ 0.994}\) \\   

Table 1: Quantitative evaluation of unlearning methods on CIFAR-10 diffusion-based generative models. Each method was evaluated by sequentially targeting each of the 10 CIFAR-10 classes for unlearning. For each target class, we measure unlearning accuracy (UA) specific to that class, remaining accuracy (RA) on the other 9 classes, and FID for generation quality. The reported values are averaged across all 10 class-specific unlearning experiments.

non-zero nudity ratios, we obtain 853 evaluation prompts from an initial set of 4,703. To evaluate the retained performance, following[Lee et al., 2024], we measure semantic correctness using CLIP [Cherti et al., 2023] alignment scores (AS) between prompts and their generated images. We evaluate model performance on both training prompts (\(D_{r,}\)) used during unlearning and a separate set of held-out test prompts (\(D_{r,}\)). These two distinct sets are constructed by carefully splitting semantic dimensions (e.g., activities, environments, moods). Detailed construction procedures for both sets are provided in Appendix D.

### Target Class Removal from Diffusion Models

We present the CIFAR-10 experiment results in Table 1. To fairly compare, we use the same remaining dataset for other baselines. Our finding first indicates that while Finetune achieves superior performance on retained data (highest RA and FID scores), it struggles to effectively unlearn target classes with this limited remaining dataset. Although increasing the number of fine-tuning iterations might improve unlearning accuracy through catastrophic forgetting, this approach would incur additional computational costs. Secondly, we observe that SalUn has low RA, compared to other baselines even with their comparable FID performance. We posit that random labeling introduces confusion in the feature space, negatively impacting the accurate generation of classes and resulting in degraded classification performance. Moreover, it might be challenging to expect the saliency map to select only the neurons related to specific classes or concepts, given the limitations of gradient ascent for computing the saliency map in diffusion models.

The Impact of Restricted Gradient and Data DiversityOur observations are as follows. 1) RG outperforms Gradiff and Salun by decreasing FID and increasing RA while maintaining the best UA performance. 2) RGD shows improvements over RG, suggesting that data diversification, in conjunction with the restricted gradient, further enhances performance in terms of RA and FID. We vary the hyperparameters and provide the results in section 4.4.

### Target Concept Removal from Diffusion Models

Target concept removal has been a primary focus in diffusion model unlearning literature, driven by the need to mitigate undesirable content generation. While existing methods have shown potential for removing nudity or art styles, our study reveals that they often compromise model alignment after unlearning.

Nudity Removal.We summarize our results in Figure 4 and Table 2. We observe that Salun tends to generate samples that are overfit to the remaining dataset. Although Salun shows promising performance in nudity removal--detecting fewer exposed body parts compared to SD and ESD-u, as shown in Figure 4--this success comes at the cost of output diversity. In particular, SalUn often generates semantically similar images (e.g., men, wall backgrounds) for both forgetting concepts (Figure 3) and remaining data (Figure 1). Table 4 quantitatively validates this observation, revealing SalUn's lowest alignment scores post-unlearning. These results suggest that SalUn's forgetting performance could stem from overfitting. This limitation may arise from two factors: the selected neurons potentially affecting both target and non-target concepts, and the limited diversity in their forget and remaining datasets. In the case of ESD, the resulting model often fails to remove the nudity concept from unlearned models, as shown in Figure 4. We also evaluate ESD-u and observe that the nudity removal performance between ESD and ESD-u are quite similar although it achieves better AS than SalUn. They suggest using "nudity" as a prompt for unlearning, but it might be difficult to reflect the entire semantic space related to the concept of "nudity," given that we can describe nudity in many different ways using paraphrasing.

RGD outperforms state-of-the-art baselines in terms of forget quality (i.e., zero detection of exposed body part given I2P prompts as described in Figure 4) and retain quality (i.e., high AS presented in Table 2), effectively mitigating the trade-off between the two tasks. To further validate the role of both the _restricted gradient_ and _diversification_ steps to nudity removal, we conduct a two-way ablation study. Removing the _restricted gradient_ step from RGD yields GradDiffD, which incorporates dataset diversity into GradDiff, whereas removing the _diversification_ step yields the previously introduced RG. RGD's superior performance over both GradDiffD (Table 7 and Figure 4) and RG (Table 4) underscores the crucial importance of both steps in our proposed unlearning algorithm.

   ^{*}\)} &  &  \\   & \(D_{r,}\) & \(D_{r,}\) & \(D_{r,}\) & \(D_{r,}\) \\  SD & 0.357 & 0.352 & 0.349 & 0.348 \\  ESD\({}^{**}\) & 0.327 (0.030) & 0.329 (0.023) & 0.300 (0.049) & 0.298 (0.050) \\  ESD-u\({}^{**}\) & 0.327 (0.03) & 0.329 (0.023) & - & - \\  ESD-x\({}^{**}\) & - & - & 0.333 (0.016) & 0.330 (0.018) \\  SalUn & 0.305 (0.052) & 0.312 (0.040) & 0.279 (0.070) & 0.280 (0.068) \\  GradDiffD (Ours) & 0.342 (0.015) & 0.348 (0.004) & 0.334 (0.015) & 0.333 (0.015) \\  RGD (Ours) & **0.354 (0.003)** & **0.350 (0.002)** & **0.355 (-0.006)** & **0.352 (-0.004)** \\   

* The values in parentheses, \(\), refer to the gap between the original SD and the unlearned model with each method.
* ESD, ESD-u, and ESD-x refer to training on full parameters, non-cross-attention weights, and cross-attention weights, respectively.

Table 2: Nudity and artist removal: we calculate the clip alignment score (AS), following Lee et al. (2024), to measure the model alignment on the remaining set after unlearning. Cells highlighted in green indicate results from our method, while those in red indicate results from the pretrained model.

Figure 4: The nudity detection results by Nudenet, following prior works (Fan et al., 2023; Gandikota et al., 2023). The Y-axis shows the exposed body part in the generated images, given the prompt, and the X-axis denotes the number of images generated by each unlearning method and SD. We exclude bars from the plot if the corresponding value is zero.

Art Style Removal.Similar to nudity removal, the task of eliminating specific art styles presents a significant challenge. In order to evaluate whether the unlearning methods inadvertently impact other concepts and semantics beyond the targeted art style, we prompt the model with other artists' styles (e.g., Monet, Picasso) while targeting to remove Vincent van Gogh's style. The results of generation examples are shown in Figure 1 and Figure 5, and the average alignment scores are shown in Table 2. It is observed that SalUn cannot follow the prompt to generate other artists' styles and shows a significant drop in alignment scores (AS) compared with the pre-trained SD.

We also train ESD-x by modifying the cross-attention weights, which is more suitable for erasing artist styles than full-parameter training (shown as plain ESD without any suffix) as proposed in ESD work. Although ESD-x performs similarly to RG in terms of alignment scores, after manual inspection of the generated images, we find ESD-x sometimes generates images ignoring the style instructions as presented in Figure 1, while RG generates images with lower quality details like noisy backgrounds but adheres well to the style instructions. Consequently, after incorporating gradient surgery to prevent interference between retain and forgot targets, our RGD achieves better image quality and shows the best alignment score, almost equivalent to the performance of the pre-trained SD.

### Ablation

Ablation in Hyperparameters.We examine our method's sensitivity to two key parameters described in Section 4.1: the retained gradient weight \(\) and loss truncation threshold \(\). Figure 6 presents the variation over different \(\) values (y-axis) for a given \(\) value (x-axis), measuring both remaining accuracy (RA) and generation quality (FID). Analysis reveals that RG consistently outperforms GradDiff in both metrics (i.e. achieving the lower FID, and higher or comparable RA with low variation across different \(\)), with RGD showing further improvements. RGD exhibits the lowest variance across different \(\) values and achieves the lowest FID and highest RA. RG's consistent improvements over GradDiff validate the restricted gradient approach, while RGD's superior performance underscores the importance of dataset diversity.

Ablation in Diversity.We further investigate the impact of data diversity through controlled experiments. For CIFAR-10, we design two scenarios based on feature similarity analysis using CLIP embeddings: Case 1, where \(D_{r}\) contains samples from only the two classes most semantically similar to the target class, and Case 2, with balanced sampling across all classes. This design stems from our

Figure 5: Art style removal. Each row represents different prompts used to evaluate the alignment and each column indicates generated images from different unlearning methods.

Figure 6: Performance analysis across different hyperparameter settings. Each box plot captures the variation over different \(\) values for a given \(\) setting (\(\{0.5,1.0,5.0\}\)), measuring both generation quality (FID, left) and remaining accuracy (RA, right). Lower FID indicates better generation quality, while higher RA indicates better model utility of non-target concepts.

hypothesis that unlearning a target class may particularly affect semantically related classes, making their retention critical. We compute class similarities using cosine distance between CLIP feature vectors as described in Figure 7. Table 3 shows that limited diversity (Case 1) significantly impacts model performance, with FID increasing by 83.803 for RGD. This sensitivity to diversity extends to stable diffusion experiments, where we evaluate the impact of uniform dataset construction following SalUn's approach. As shown in Table 4, RG with uniform datasets shows a larger performance gap from SD (\(=0.032\) in test alignment scores) compared to RGD (\(=0.001\)). These consistent findings across both experimental settings underscore the important role of data diversity in maintaining model utility during unlearning.

## 5 Conclusion

This study advances the understanding of machine unlearning in text-to-image generative models by introducing a principled approach to balance forgetting and remaining objectives. We show that the restricted gradient provides an optimal update for handling conflicting gradients between these objectives, while strategic data diversification ensures further improvements on model utilities. Our comprehensive evaluation demonstrates that our method effectively removes diverse target classes from CIFAR-10 diffusion models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, outperforming state-of-the-art baselines.

### Limitation and Broader Impacts

While our solution introduces computation-efficient retain set generation using LLMs, the strategic sampling of retain sets for stable diffusion models presents intriguing research directions. Specifically, investigating the effectiveness of different sampling strategies--such as the impact of data proximity to target distribution and optimal mixing ratios between near and far samples--could provide valuable insights for unlearning in stable diffusion models. Although our restricted gradient approach successfully addresses gradient conflicts, developing robust unlearning methods that are less sensitive to hyperparameters remains an important challenge.

## 6 Acknowledgement

RJ and the ReDS lab acknowledge support through grants from the Amazon-Virginia Tech Initiative for Efficient and Robust Machine Learning, and NSF CNS-2424127, and the Cisco Research Award. MJ acknowledges the support from NSF ECCS-2331775, IIS-2312794, and the Commonwealth Cyber Initiative. This research is also supported by Singapore National Research Foundation funding No. 053424, DARPA funding No. 112774-19499, and NSF IIS-2229876.

   ^{*}\)} &  \\   & \(D_{r,}\) & \(D_{r,}\) \\  SD & 0.357 & 0.352 \\  RG & 0.330 (0.027) & 0.320 (0.032) \\  RGD & 0.354 (0.003) & 0.351 (0.001) \\   

* The values in parentheses, \(\), refer to the gap between the original SD and the unlearned model with each method.

Table 4: Comparison of alignment score (AS) between RGD and RG. RG, in this table, indicates the case when we have uniform forgetting and remaining datasets but utilize the restricted gradient.

   &  &  & -\)} \\   & UA\({}_{}\) & RA\({}_{}\) & FID\({}_{}\) & UA\({}_{}\) & RA\({}_{}\) & FID\({}_{}\) & UA & RA & FID \\  GradDiff & 1.000a0 & 0.106a0.086 & 156.021a31.901 & 1.000a0 & 0.201a0.043 & 95.287a16.279 & 0.000 & +0.095 & -60.734 \\  RG (Ours) & 1.000a0 & 0.205a0.138 & 131.247a6.049 & 1.000a0 & 0.463a0.059 & 47.797a7.231 & 0.000 & +0.258 & -83.450 \\  RGD (Ours) & 1.000a0 & 0.239a0.071 & 94.259a28.217 & 1.000a0 & 0.675a0.019 & 10.456a1.976 & 0.000 & +0.436 & -83.803 \\   

Table 3: Comparison of UA, RA, and FID for diversity-controlled experiments in CIFAR-10 diffusion models. In this context, Case 1 represents a scenario where the remaining set lacks diversity (i.e., it only includes samples from two closely related classes), while Case 2 includes equal samples from all classes. We note that we used the same remaining dataset size between both cases.