# Prioritize Alignment in Dataset Distillation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (**PAD**), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information. This simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms. Furthermore, built on trajectory matching, **PAD** achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance. The code and distilled datasets will be made public.

## 1 Introduction

Dataset Distillation (DD)  aims to compress a large dataset into a small synthetic dataset that preserves important features for models to achieve comparable performances. Ever since being introduced, DD has gained a lot of attention because of its wide applications in practical fields such as privacy preservation [5; 44], continual learning [28; 35], and neural architecture search [12; 32].

Recently, matching-based methods [46; 42; 6] have achieved promising performance in distilling high-quality synthetic datasets. Generally, the process of these methods can be summarized into two steps: (1) _Information Extraction_: an agent model is used to extract important information from the target dataset by recording various metrics such as gradients , distributions , and training trajectories , (2) _Information Embedding_: the synthetic samples are optimized to incorporate the extracted information, which is achieved by minimizing the differences between the same metric calculated on the synthetic data and the one recorded in the previous step.

In this work, we first reveal both steps will introduce misaligned information, which is redundant and potentially detrimental to the quality of the synthetic data. Then, by analyzing the cause of this misalignment, we propose alleviating this problem through the following two perspectives.

Typically, in the _Information Extraction_ step, most distillation methods allow the agent model to see all samples in the target dataset. This means information extracted by the agent model comes from samples with various difficulties (see Figure 1(a)). However, according to previous study, information related to easy samples is only needed when the compression ratio is high. This misalignment leads to the sub-optimal of the distillation performance.

To alleviate the above issue, we first use data selection methods to measure the difficulty of each sample in the target dataset. Then, during the distillation, a data scheduler is employed to ensure only data whose difficulty is aligned with the compression ratio is available for the agent model.

In the _Information Embedding_ step, most distillation methods except DM  choose to use all parameters of the agent model to perform the distillation. Intuitively, this will ensure the information extracted by the agent model is fully utilized. However, we find shallow layer parameters of the model can only provide low-quality, basic signals, which are redundant for dataset distillation in most cases. Conversely, performing the distillation with only parameters from deep layers will yield high-quality synthetic samples. We attribute this contradiction to the fact that deeper layers in DNNs tend to learn higher-level representations of input data [27; 37].

Based on our findings, to avoid embedding misaligned information in the _Information Embedding_ step, we propose to use only parameters from deeper layers of the agent model to perform distillation, as illustrated in Figure 1(b). This simple change brings significant performance improvement, showing its effectiveness in aligning information.

Through experiments, we validate that our two-step alignment strategy is effective for distillation methods based on matching gradients , distributions , and trajectories . Moreover, by applying our alignment strategy on trajectory matching [1; 10], we propose our novel method named Prioritize Alignment in Dataset Distillation (PAD). After conducting comprehensive evaluation experiments, we show PAD achieves state-of-the-art (SOTA) performance.

## 2 Misaligned Information in Dataset Distillation

Generally, we can summarize the distillation process of matching-based methods into the following two steps: (1) _Information Extraction_: use an agent model to extract essential information from the target dataset, realized by recording metrics such as gradients , distributions , and training trajectories , (2) _Information Embedding_: the synthetic samples are optimized to incorporate the extracted information, realized by minimizing the differences between the same metric calculated on the synthetic data and the one recorded in the first step.

Figure 1: (a) Compared with using all samples without differentiation in IPCs (left), PAD meticulously selects a subset of samples for different IPCs to align the expected difficulty of information required (right). (b) Different layers distill different patterns (left). PAD masks out (grey box) shallow-layer parameters during metric matching in accordance with IPCs (right).

In this section, through analyses and experimental verification, we show the above two steps both will introduce misaligned information to the synthetic data.

### Misaligned Information Extracted by Agent Models

In the _information extraction_ step, an agent model is employed to extract information from the target dataset. Generally, most existing methods [1; 6; 49; 46] allow the agent model to see the full dataset. This implies that the information extracted by the agent model originates from samples with diverse levels of difficulty. However, the expected difficulty of distilled information varies with changes in IPC: smaller IPCs prefer easier information while larger IPCs should distill harder one .

To verify if this misalignment will influence the quality of synthetic data, we perform the distillation where hard/easy samples of target dataset are removed with various ratios. As the results reported in Figure 2, pruning unaligned data points is beneficial for all matching-based methods. This proves the misalignment indeed will influence the distillation performance and can be alleviated by filtering out misaligned data from the target dataset.

### Misaligned Information Embedded by Metric Matching

Most existing methods use all parameters of the agent model to compute the metric used for matching. Intuitively, this helps to improve the distillation performance, since in this way all information extracted by the agent model will be embedded into the synthetic dataset. However, since shallow layers in DNNs tend to learn basic distributions of data [27; 37], using parameters from these layers can only provide low-level signals that turned out to be redundant in most cases.

Figure 3: Distillation performances on CIFAR-10 where n% (ratio) shallow layer parameters are not utilized during distillation. Discarding shallow-layer parameters is beneficial for methods based on matching gradients, distributions, and trajectories, both in low and high IPC cases.

Figure 2: Distillation performance on CIFAR-10 where data points are removed with different ratios. Removing unnecessary data points helps to improve the performance of methods based on matching gradients, distributions, and trajectories, both in low and high IPC cases.

As can be observed in Figure 3, it is evident that across all matching-based methods, the removal of shallow layer parameters consistently enhances performance, regardless of the IPC setting. This proves employing over-shallow layer parameters to perform the distillation will introduce misaligned information to the synthetic data, compromising the quality of distilled data.

## 3 Method

To alleviate the information misalignment issue, based on trajectory matching (TM) [1; 10], we propose Prioritizing Alignment in Dataset Distillation (PAD). PAD can also be applied to methods based on matching gradients  and distributions , which are introduced in Appendix A.1.

### Preliminary of Trajectory Matching

Following the two-step procedure, to extract information, TM-based methods [1; 10] first train agent models on the real dataset \(_{R}\) and record the changes of the parameters. Specifically, let \(\{_{t}^{*}\}_{0}^{N}\) be an expert trajectory, which is a parameter sequence recorded during the training of agent model. At each iteration of trajectory matching, \(_{t}^{*}\) and \(_{t+M}^{*}\) are randomly selected from expert trajectories as the start and target parameters.

To embed the information into the synthetic data, TM methods minimize the distance between the expert trajectory and the student trajectory. Let \(_{t}\) denote the parameters of the student agent model trained on synthetic dataset \(_{S}\) at timestep \(t\). The student trajectory progresses by doing gradient descent on the cross-entropy loss \(l\) for \(N\) steps:

\[_{t+i+1}=_{t+i}- l(_{t+i}, _{S}),\] (1)

Finally, the synthetic data is optimized by minimizing the distance metric, which is formulated as:

\[=_{t+N}-_{t+M}^{*}||}{||_{t+M}^{*} -_{t}^{*}||},\] (2)

### Filtering Information Extraction

In section 2.1, we show using data selection to filter out unmatched samples could alleviate the misalignment caused in _Information Extraction_ step. According to previous work , TM-based methods prefer easy information and choose to match only early trajectories when IPC is small. Conversely, hard information is preferred by high IPCs and they match only late trajectories. Hence, we should use easy samples to train early trajectories, while late trajectories should be trained with hard samples. To realize this efficiently, we first use the data selection method to measure the difficulty of samples contained in the target dataset. Then, during training expert trajectories, a scheduler is implemented to gradually incorporate hard samples into the training set while excluding easier ones from it.

Difficulty Scoring FunctionIdentifying the difficulty of data for DNNs to learn has been well studied in data selection area [29; 17; 16; 40]. For simplicity consideration, we use Error L2-Norm (EL2N) score  as the metric to evaluate the difficulty of training examples (other metrics can also be chosen, see Section 4.3.2). Specifically, let \(x\) and \(y\) denote a data point and its label, respectively. Then, the EL2N score can be calculated by:

\[_{t}(x,y)=||p(w_{t},x)-y||_{2},\] (3)

where \(p(w_{t},x)=(f(w_{t},x))\) is the output of a model \(f\) at training step \(t\) transformed into a probability distribution. In consistent with , samples with higher EL2N scores are considered as harder samples in this paper.

SchedulerThe scheduler can be divided into the following stages. Firstly, the hardest samples are removed from the training set, ensuring that it exclusively comprises data meeting a predetermined initial ratio (IR). Then, during training expert trajectories, samples are gradually added to the training set in order of increasing difficulty. After incorporating all the data into the training set, the scheduler will begin to remove easy samples from the target dataset. Unlike the gradual progression involved in adding data, the action of reducing data is completed in a single operation, since now the model has been trained on simple samples for a sufficient time.

### Filtering Information Embedding

To filter out misaligned information introduced by matching shallow-layer parameters, we propose to add a parameter selection module that masks out part of shallow layers for metric computation. Specifically, parameters of an agent network can be represented as a flattened array of length \(L\) that stores weights of agent models ordered from shallow to deep layers (parameters within the same layer are sorted in default order). The parameter selection sets a threshold ratio \(\) such that the first \(k=L\) parameters are not used for distillation. Then the parameters used for matching can now be formulated as:

\[_{t+N}=\{_{0,_{1},,_{k-1} },_{k},_{k+1},,_{L}}_{ }\}.\] (4)

In practice, the ratio \(\) should vary with the change of IPC. For smaller IPCs, it is necessary to incorporate basic information thus \(\) should be lower. Conversely, basic information is redundant in larger IPC cases, so \(\) should be higher accordingly.

## 4 Experiments

### Settings

We compare PAD with several prominent dataset distillation methods, which can be divided into two categories: matching-based approaches including DC , DM , DSA , CAFE , MTT , FTD , DATM , TESLA , and kernel-based approaches including KIP , FRePo , RICG . The assessment is conducted on widely recognized datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet . We implemented our method based on DATM . In both the distillation and evaluation phases, we apply the standard set of differentiable augmentations commonly used in previous studies . By default, networks are constructed with instance normalization unless explicitly labeled with "-BN," indicating batch normalization (e.g., ConvNet-BN). For CIFAR-10 and CIFAR-100, distillation is typically performed using a 3-layer ConvNet, while Tiny ImageNet requires a 4-layer ConvNet. Cross-architecture experiments also utilize LeNet , AlexNet , VGG11 , and ResNet18 . More details can be found in the appendix.

### Main Results

CIFAR and Tiny ImageNetWe conduct comprehensive experiments to compare the performance of our method with previous works. As the results presented in Table 1, PAD outperforms previous matching-based methods on three datasets except for the case when IPC=1. When compared with kernel-based methods which use a larger network to perform the distillation, our technique exhibits superior performance in most cases, particularly when the compression ratio exceeds \(1\%\). As can be observed, PAD performs relatively better when IPC is high, suggesting our filtering out misaligned information strategy becomes increasingly effective as IPC increases.

   Dataset &  &  &  \\ IDC & 1 & 10 & 50 & 500 & 1000 & 1 & 10 & 50 & 100 & 1 & 10 & 50 \\ Ratio & 0.02 & 0.2 & 1 & 10 & 20 & 0.2 & 2 & 10 & 20 & 0.2 & 2 & 10 \\  Random & 15.440.3 & 31.040.5 & 50.680.3 & 73.240.3 & 78.440.2 & 4.240.3 & 14.640.5 & 33.440.4 & 42.840.3 & 1.440.1 & 5.080.2 & 15.040.4 \\ KIP  & 49.904.2 & 62.740.3 & 65.680.2 & - & - & 15.740.2 & 28.340.1 & - & - & - & - \\ FRePo  & 56.480.7 & 65.540.4 & 71.750.0 & - & - & 28.701.4 & 25.40.2 & 44.30.2 & - & 15.450.3 & 25.440.2 & - \\ RCG  & 53.591.0 & 69.140.4 & 73.540.3 & - & - & 39.350.4 & 44.140.4 & 46.740.3 & - & 25.660.3 & 29.440.2 & - \\  DC  & 28.340.4 & 44.920.5 & 53.940.5 & 53.940.5 & - & - & 3.25.040.2 & 52.03 & - & - & - & - \\ DM  & 26.060.8 & 48.940.6 & 63.040.4 & 75.160.3 & 78.840.1 & 11.440.3 & 29.740.3 & 45.640.4 & - & 3.940.2 & 12.950.4 & 24.140.3 \\ DSA  & 28.840.7 & 52.140.5 & 60.640.5 & 73.640.3 & 78.740.3 & 13.940.3 & 32.340.3 & 42.840.4 & - & - & - \\ TESLA  & **48.580.6** & 66.440.8 & 72.660.7 & - & - & 24.840.4 & 41.730.4 & 37.940.3 & 49.240.4 & - & - & - \\ CAFE  & 30.311.4 & 43.306.5 & 55.06 & - & - & 21.99.3 & 78.340.3 & 37.940.3 & - & 8.840.3 & 23.250.2 & 28.040.3 \\ MTT  & 46.240.8 & 55.440.7 & 71.610.2 & - & - & 24.430.3 & 37.940.4 & 77.420.4 & 49.240.4 & 10.540.2 & 23.440.3 & 28.240.4 \\ PTD  & 46.040.4 & 65.340.7 & 73.240.2 & - & - & 24.40.4 & 42.540.2 & 45.840.3 & 49.760.4 & 10.540.2 & 23.440.3 & 28.240.4 \\ DATM  & 46.940.5 & 66.880.2 & 76.160.3 & 83.540.2 & 28.550.40.4 & 27.940.2 & 42.7840.5 & 55.040.2 & 57.560.2 & 17.140.3 & 31.140.3 & 39.740.3 \\
**PAD** & 47.240.6 & **67.440.3** & **77.080.5** & **84.640.3** & **86.760.2** & **28.40.5** & **47.840.2** & **55.940.3** & **58.540.3** & **17.740.2** & **32.340.4** & **41.640.4** \\   Full Dataset &  &  &  \\   

Table 1: Comparison with previous dataset distillation methods (bottom: matching-based, top: others) on CIFAR-10, CIFAR-100 and Tiny ImageNet. ConvNet is used for the distillation and evaluation. Our method consistently outperforms prior matching-based methods.

Cross Architecture GeneralizationWe evaluate the generalizability of our distilled data in both low and high IPC cases. As results reported in Table 3(a), when IPC is small, our distilled data outperforms the previous SOTA method DATM on ResNet and AlexNet while maintaining comparable accuracy on VGG. This suggests that our distilled data on high compressing ratios generalizes well across various unseen networks. Moreover, as reflected in Table 2, our distilled datasets on large IPCs also have the best performance on most evaluated architectures, showing good generalizability in the low compressing ratio case.

### Ablation Study

To validate the effectiveness of each component of our method, we conducted ablation experiments on modules (section 4.3.1) and their hyper-parameter settings (section 4.3.2 and section 4.3.2).

#### 4.3.1 Modules

Our method incorporates two separate modules to filter information extraction (FIEX) and information embedding (FIEM), respectively. To verify their isolated effectiveness, we conduct an ablation study by applying two modules individually. As depicted in Table 3(b), both FIEX and FIEM bring improvements, implying their efficacy. By applying these two modules, we are able to effectively remove unaligned information, improving the distillation performance.

#### 4.3.2 Hyper-parameters of Filtering Information Extraction

Initial Ratio and Data Addition EpochTo filter the information learned by agent models, we initialize the training set with only easy samples, and the size is determined by a certain ratio of the total size. Then, we gradually add hard samples into the training set. In practice, we use two hyper-parameters to control the addition process: the initial ratio (IR) of training data for training set initialization and the end epoch of hard sample addition (AEE). These two parameters together control the amount of data agent models can see at each epoch and the speed of adding hard samples.

   Dataset & Ratio & Method & ConvNet & ConvNet-BN & ResNet18 & ResNet18-BN & VGG11 & AlexNet & LeNet & MLP & Avg. \\   &  & Random & 78.38 & 80.25 & 84.58 & 87.21 & 80.81 & 80.75 & 61.85 & 50.98 & 75.60 \\  & & Glister & 62.46 & 70.52 & 81.10 & 74.59 & 78.70 & 70.75 & 56.56 & 40.59 & 66.81 \\  & &  &  & 76.27 & 80.06 & 85.67 & 87.18 & 82.04 & 81.35 & 64.59 & 52.21 & 76.17 \\  & & & DATM & 85.50 & 85.23 & **87.22** & **88.13** & **84.65** & 85.14 & 66.70 & 52.40 & 79.37 \\  & & **PAD** & **86.90** & **85.67** & 86.95 & 88.09 & **84.34** & **85.83** & **67.28** & **53.62** & **79.84** \\  & & & \(\) & +8.52 & +5.42 & +2.37 & +0.88 & +3.53 & +5.08 & +5.43 & +2.64 & +4.24 \\   &  & Random & 42.80 & 46.38 & 47.48 & 55.62 & 42.69 & 38.05 & 25.91 & 20.66 & 39.95 \\  & & Glister & 35.45 & 37.13 & 42.49 & 46.14 & 43.06 & 28.58 & 23.33 & 17.08 & 34.16 \\  & & Forgetting & 45.52 & 49.99 & 51.44 & 54.65 & 43.28 & 43.47 & 27.22 & 22.90 & 42.30 \\  & & DATM & 57.50 & 57.75 & 57.98 & **63.34** & **55.10** & 55.69 & 33.57 & 26.39 & 50.92 \\  & & **PAD** & **85.50** & **85.66** & **58.15** & 63.17 & 55.02 & **55.93** & **33.87** & **27.12** & **51.30** \\  & & \(\) & +15.70 & +12.28 & +10.67 & +7.55 & +12.33 & +17.88 & +7.96 & +6.46 & +11.35 \\   &  & Random & 15.00 & 24.21 & 17.73 & 28.07 & 22.51 & 14.03 & 9.25 & 5.85 & 17.08 \\  & & Glister & 17.32 & 19.77 & 18.84 & 23.12 & 19.10 & 11.68 & 8.84 & 3.86 & 15.32 \\   & & Forgetting & 20.04 & 23.83 & 19.38 & 28.88 & 23.77 & 12.13 & 12.06 & 5.54 & 18.20 \\   & & DATM & 39.68 & **40.32** & **36.12** & **34.14** & 38.35 & **35.10** & 12.41 & 9.02 & 31.76 \\   & & **PAD** & **41.02** & **40.88** & 36.08 & **42.96** & **38.64** & 35.02 & **13.17** & **9.68** & **32.18** \\   & & \(\) & +26.02 & +16.67 & +18.35 & +14.89 & +16.13 & +20.99 & +3.92 & +3.83 & +15.10 \\   

Table 2: Cross-architecture evaluation of distilled data on unseen networks. Results worse than random selection are indicated with red color. \(\) denotes the performance improvement brought by our method compared with random selection. Tiny denotes Tiny ImageNet.

    & & & & 66.7 \\ FID & 48.90 & 46.65 & 43.24 & 42.20 \\ DATM & 55.03 & 51.71 & **48.38** & 45.74 \\
**PAD** & **55.91** & **52.35** & 44.49 & **45.92** \\    
   FIEX & FIEM & Accuracy(\%) \\   & & & 66.7 \\ \(\) & & & 66.9 \\ \(\) & & 67.2 \\ \(\) & & 67.4 \\ \(\) & & 67.4 \\ \(\) & & 67.4 \\ \(\) & & & \\ \(\) & & 67.4 \\   

Table 3: **(a)** Cross-Architecture evaluation on CIFAR-100 IPC50. **(b)** Ablation studies on the modules of our method on CIFAR-10 IPC10. **(c)** Results of different sets of data selection hyper-parameters on CIFAR-10 IPC10.

In Table 3(c), we show the distillation results where different hyper-parameters are utilized. In general, a larger initial ratio and faster speed of addition bring better performances. Although the distillation benefited more from learning simpler information when IPC is small , our findings indicate that excessively removing difficult samples (e.g., more than a quarter) early in the training phase can adversely affect the distilled data. This negative impact is likely due to the excessive removal leading to distorted feature distributions within each category. On the other hand, reasonably improving the speed of adding hard samples allows the agent model to achieve a more balanced learning of information of varying difficulty across different stages.

Other Difficulty Scoring FunctionsIdentifying the difficulty of data points is the key to filtering out misaligned information in the extraction step. Here, we compare the effect of using other difficulty-scoring functions to evaluate the difficulty of data. (1) prediction loss of a pre-trained ResNet. (2) uncertainty score . (3) EL2N . As can be observed in Table 4(a), EL2N performs the best across various IPCs; thus, we use it to measure how hard each data point is as default in our method. Note that this can also be replaced with a more advanced data selection algorithm.

#### 4.3.3 Ratios of Parameter Selection

It is important to find a good balance between the percentage of shallow-layer parameters removed from matching and the loss of information. In Table 4(b), we show results obtained on different IPCs by discarding various ratios of shallow-layer parameters. The impact of removing varying proportions of shallow parameters on the distilled data and its relationship with changes in IPC is consistent with prior conclusions. For small IPCs, distilled data requires more low-level basic information. Thus, removing too many shallow-layer parameters causes a negative effect on the classification performance. By contrast, high-level semantic information is more important when it comes to large IPCs. With increasing ratios of shallow-layer parameters being discarded, we can ensure that low-level information is effectively filtered out from the distilled data.

## 5 Discussion

### Distilled Images with Filtering Information Embedding

To see the concrete patterns brought by removing shallow-layer parameters to perform the trajectory matching, we present distilled images obtained by discarding various ratios of shallow-layer parameters in Figure 4. As can be observed in Figure 4(a), without removing any shallow-layer parameters

Table 4: **(a) Ablation of different difficulty scoring functions on CIFAR-10. (b) Results of masking out different ratios of shallow-layer parameters across various IPCs on CIFAR-10. (c) Ablation on the strategy used for parameter selection on CIFAR-10**

Figure 4: Synthetic images of CIFAR-10 IPC50 obtained by PAD with different ratios of parameter selection. Smoother image features indicate that by removing some shallow-layer parameters during matching, PAD successfully filters out coarse-grained low-level information.

to filter misaligned information, synthetic images are interspersed with substantial noises. These noises often take the form of coarse and generic information, such as the overall color distribution and edges in the image, which provides minimal utility for precise classification.

By contrast, images distilled by our enhanced methodology (see Figure 4(b) and Figure 4(c)), which includes meticulous masking out shallow-layer parameters during trajectory matching according to the compressing ratio, contain more fine-grained and smoother features. These images also encapsulate a broader range of semantic information, which is crucial for helping the model make accurate classifications. Moreover, we observe a clear trend: as the amount of the removed shallow-layer parameters increases, the distilled images exhibit clearer and smoother features.

### Rationale for Parameter Selection

In this section, we analyze from the perspective of trajectory matching why shallow-layer parameters should be masked out. In Figure 5, we present the changes in trajectory matching loss across different layers as the distillation progresses. Compared to the deep-layer parameters of the agent model, a substantial number of shallow-layer parameters exhibit low loss values that fluctuate during the matching process (see Figure 5). By contrast, the loss values of the deep layers are much higher but consistently decrease as distillation continues. This suggests that matching shallow layers primarily conveys low-level information that is readily captured by the synthetic data and quickly saturated. Consequently, the excessive addition of such low-level information produces noise, reducing the quality of distilled datasets.

For a concrete visualization, we provide distilled images resulting from using only shallow-layer parameters or only deep-layer parameters to match trajectories in Figure 6. The coarse image features depicted in Figure 6(a) further substantiate our analysis.

Figure 5: Losses of different layers of ConvNet after matching trajectories for 0, 1000, and 5000 iterations. We notice a similar phenomenon on both small (IPC1 and IPC10) and large IPCs (IPC500): losses of shallow-layer parameters fluctuate along the matching process, while losses of deep-layer parameters show a clear trend of decreasing.

Figure 6: Synthetic images visualization with parameter selection. Matching parameters in shallow layers produces an abundance of low-level texture features, whereas patterns generated by matching deep-layer parameters embody richer high-level semantic information.

### Parameter Selection Strategy

In the previous section, we observed a positive correlation between the depth of the model layers and the magnitude of their trajectory-matching losses. Notably, the loss in the first layer of the ConvNet was higher compared to other shallow layers. Consequently, we further compared different parameter alignment strategies, specifically by sorting the parameters based on their matching losses and excluding a certain proportion of parameters with lower losses. Higher loss values indicate greater discrepancies in parameter weights; thus, continuing to match these parameters can inject more information into the synthetic data. As shown in Table 4(c), sorting by loss results in an improvement compared with no parameter alignment, but filtering based on parameter depth proves to be more effective.

## 6 Related Work

Introduced by , dataset distillation aims to synthesize a compact set of data that allows models to achieve similar test performances compared with the original dataset. Since then, a number of studies have explored various approaches. These methods can be divided into three types: kernel-based, matching-based, and using generative models .

**Kernel-based methods** are able to achieve closed-form solutions for the inner optimization  via kernel ridge regression with NTK . FRePo  distills a compact dataset through neural feature regression and reduces the training cost.

**Matching-based methods** first use agent models to extract information from the target dataset by recording a specific metric [7; 23; 38; 24]. Representative works that design different metrics include DC  that matches gradients, DM  that matches distributions, and MTT  that matches training trajectories. Then, the distilled dataset is optimized by minimizing the matched distance between the metric computed on synthetic data and the record one from the previous step. Following this workflow, many works have been proposed to improve the efficacy of the distilled dataset. For example, CAFE  preserves the real feature distribution and the discriminative power of the synthetic data and achieves prominent generalization ability across various architectures. DREAM  employs K-Means to select representative samples for distillation and improves the distillation efficiency. DATM  proposes to match early trajectories for small IPCs and late trajectories for large IPCs, achieving SOTA performances on several benchmarks. Moreover, new metrics such as spatial attention maps [36; 15] have also been introduced and achieved promising performance in distilling large-scale datasets.

**Generative models** such as GANs [8; 13; 14; 41] and diffusion models [34; 30; 9] can also be used to distill high quality datasets. DiM  uses deep generative models to store information of the target dataset. GLaD  transfers synthetic data optimization from the pixel space to the latent space by employing deep generative priors. It enhances the generalizability of previous distillation methods.

## 7 Conclusion

In this work, we find a limitation of existing Dataset Distillation methods in that they will introduce misaligned information to the distilled datasets. To alleviate this, we propose PAD, which incorporates two modules to filter out misaligned information. For information extraction, PAD prunes the target dataset based on sample difficulty for different IPCs so that only information with aligned difficulty is extracted by the agent model. For information embedding, PAD discards part of shallow-layer parameters to avoid injecting low-level basic information into the synthetic data. PAD achieves SOTA performance on various benchmarks. Moreover, we show PAD can also be applied to methods based on matching gradients and distribution, bringing remarkable improvements across various IPC settings.

LimitationsOur alignment strategy could also be applied to methods based on matching gradients and distributions (see Appendix A.1). However, due to the limitation of computing resources, for methods based on matching distributions and gradients, we have only validated our method's effectiveness on DM  and DC  (see Table 5 and Table 6).