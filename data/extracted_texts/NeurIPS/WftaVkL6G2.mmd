# Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis

Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis

 Michael Crawshaw

Department of Computer Science

George Mason University

Fairfax, VA 22030

mcrawsha@gmu.edu

&Mingrui Liu

Department of Computer Science

George Mason University

Fairfax, VA 22030

mingruil@gmu.edu

###### Abstract

In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy \((^{-2})\) communication rounds to find an \(\)-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires \((^{2}^{-4})\) communication rounds, where \(\) denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of \(\) and \(\). Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients \((N=250)\), demonstrating the effectiveness of our algorithm under periodic client participation.

## 1 Introduction

Federated learning (FL) [27; 21; 16; 53] is a distributed learning paradigm that emphasizes client privacy [28; 41; 29], limited communication [19; 25], and data heterogeneity across clients [17; 55]. FL has attracted attention in recent years due to the ability to leverage data and compute from user devices while respecting privacy [49; 9]. For large-scale FL, it is common to limit the number of simultaneously participating devices, and many works do so by assuming that a random subset of clients can be sampled independently at each round [27; 3; 50; 22; 38]. However, this pattern of client participation is not always practical. If clients are user devices like mobile phones, they may not have 24/7 availability due to low battery or bad internet connection [15; 31]. In particular, if client availability is correlated with geographical location (e.g. mobile phones charging at night), then client availability follows a cyclic pattern . Therefore, it remains an important open question to design federated optimization algorithms with provable efficiency under non-i.i.d client participation.

Several works have investigated optimization in FL under non-i.i.d. client participation [10; 2; 8; 39; 46]. However, to the best of our knowledge, no existing algorithm in a non-i.i.d. participation setting provably exhibits reduced communication cost, linear speedup with respect to the number of clients, and resilience to client data heterogeneity for general non-convex optimization.

In this work, we consider FL under an arbitrary participation framework , where client participation during each round is a random variable with potentially unknown distribution. We focus on client participation patterns that are periodic, in the sense that all clients are expected to participate with equal frequency over a window of multiple training rounds.

For this setting, we propose Amplified SCAFFOLD, an optimization algorithm for FL under periodic client participation. Amplified SCAFFOLD utilizes (a) amplified updates across participation periods and (b) control variates computed across entire participation periods, to eliminate the effect of data heterogeneity even under non-i.i.d. participation. We show that Amplified SCAFFOLD exhibits significantly reduced communication cost, linear speedup, and is unaffected by client data heterogeneity. To the best of our knowledge, this is the first result demonstrating reduced communication or resilience to data heterogeneity without assuming i.i.d. participation. The complexity of Amplified SCAFFOLD is compared against baselines in Table 1. For cyclic participation, Amplified SCAFFOLD improves the previous best communication cost from \((^{2}^{-4})\) to \((^{-2})\).

The main challenges of achieving these properties are (1) simultaneously handling randomness from stochastic gradients and non-i.i.d. participation; and (2) controlling the error of control variates under non-i.i.d. participation. Previous work in this setting  performs an in-expectation analysis, by taking expectation only over randomness from stochastic gradients; this avoids (1) but cannot leverage properties of the participation pattern to reduce communication. We present a tighter analysis that addresses (1) by taking expectation over both client participation and the stochastic gradients throughout the analysis, and carefully treating the trajectory variables which depend on both sources of randomness. We address (2) by recursively bounding the control variate errors, which involves a non-uniform average of non-uniform averages of error terms resulting from non-i.i.d. participation. We show that this nested non-uniform average can be bounded using mild regularity conditions on the participation pattern.

Our contributions are summarized below.

   Setting & Communication & Iteration & Reduced & Unaffected by \\  & Complexity (\(R\)) & Complexity (\(RI\)) & Communication & Heterogeneity \\  i.i.d. Participation (\(S\)) & & & & \\ FedAvg  & \(}{S^{2}}(1-)+R}{^{2}}\) & \(}{S^{2}}\) & ✗ & ✗ \\ SCAFFOLD  & \(}{S^{2}}()^{2/3}\) & \(}{S^{2}}\) & ✓ & ✓ \\ Amplified FedAvg  & \(}{S^{2}}}{S^{ 2}}+}{^{2}}\) & \(}{S^{2}}\) & ✗ & ✗ \\ Amplified SCAFFOLD (ours) & \(}{S^{2}}}{S^{ 2}}\) & ✓ & ✓ \\  Regularized Participation (\(P,\)) & & & & \\ Amplified FedAvg  & \(+^{2}}{^{2}}\) & \(^{2}}{^{2}}\) & ✓ & ✗ \\ Amplified SCAFFOLD (ours) & \(}{S^{2}}\) & \(^{2}}{^{2}}\) & ✓ & ✓ \\  Cyclic Participation (\(,\,S\)) & & & \\ Amplified FedAvg  & \(}{S^{2}}(1-)++^{2}}{^{2}}\) & \(}{S^{2}}\) & ✗ & ✗ \\ Amplified SCAFFOLD (ours) & \(}}{S^{2}}\) & \(}{S^{2}}\) & ✓ & ✓ \\   

Table 1: Communication and computation complexity of various methods to find an \(\)-stationary point for \(L\)-smooth, non-convex objectives. \(N\): number of clients, \(\): data heterogeneity \(_{}\| f_{i}()- f()\|\). \(S\): number of participating clients per round, \(\): number of groups for cyclic participation. See Section 3.2 for a description of each participation pattern. We say that an algorithm exhibits reduced communication if its dependence in terms of \(\) is strictly smaller than \((^{-4})\). Derivation of complexities for Amplified FedAvg can be found in Appendix C.

* We introduce Amplified SCAFFOLD, an optimization algorithm for federated learning under non-i.i.d. client participation. Our convergence analysis demonstrates its computational and communication efficiency: Amplified SCAFFOLD exhibits reduced communication, linear speedup, and is unaffected by data heterogeneity. These guarantees are achieved with a tighter analysis than used in previous work , with a fine-grained treatment of the two sources of randomness: client participation and stochastic gradients. In the case of cyclic participation, we reduce the previous best communication cost of \((^{2}^{-4})\) to \((^{-2})\).
* Experimental results show that Amplified SCAFFOLD converges faster than baselines on both synthetic and real-world problems under realistic non-i.i.d. client participation patterns. We also include an ablation study which demonstrates the robustness of our algorithm to changes in data heterogeneity, the number of participating clients per round, and the number of client groups in cyclic participation.

The paper is outlined as follows. We discuss related work in Section 2, and Section 3 provides a formal specification of the optimization problem. Amplified SCAFFOLD is introduced and theoretically analyzed in Section 4, and we provide experiments in Section 5. We conclude with Section 6.

## 2 Related Work

**Federated Optimization.** FedAvg  characterizes partial client participation and local updates in each round. FedAvg was analyzed in the full participation setting [35; 37; 50; 51; 43; 42; 18; 12]. Other federated optimization algorithms aim to improve communication efficiency [32; 52] and tackle data heterogeneity [22; 17]. The analysis of FL optimization algorithms typically either assumes full client participation or partial client participation where clients are sampled uniformly randomly [47; 38; 17; 23; 43; 1].  provides lower bounds for distributed stochastic, smooth optimization with intermittent communication and non-convex objectives, both in the full and partial participation settings. They also include algorithms employing variance reduction which match (or closely match) lower bounds in the full and partial participation settings. However, none of the works above are applicable for general participation patterns such as periodic participation.

**Client Participation.** Cyclic data sampling was considered for stochastic convex optimization in , where they propose "pluralistic" solutions instead of learning a single model for all clients. There is a recent line of work considering various participation patterns, including client selection [11; 4; 33] biased participation [34; 6; 7], independent participation across rounds [17; 22; 23], unbiased participation [36; 13], bounded rounds of unavailability [46; 14; 48], asynchronous participation [24; 2], cyclic participation , and arbitrary participation [39; 40]. However, none of these works enjoy linear speedup, reduced communication rounds, and resilience to data heterogeneity under the general setting of non-convex objectives and periodic client participation. Concurrent work  considers non-stationary client participation under the condition that for every client and every round, the probability of participation is bounded away from zero; for this setting, they propose an algorithm with linear speedup and dependence on gradient dissimilarity for non-convex, Lipschitz objectives.

See Appendix F for a detailed discussion comparing our results with a small number of closely related baselines.

## 3 Problem Setup

We consider a federated learning problem with \(N\) clients, with the overall objective

\[_{^{d}}\{f(x):=_{i=1}^{N}f_{i}(x) \},\]

where each \(f_{i}:^{d}\) is the local objective of one client. We consider the stochastic optimization problem, so that \(f_{i}()=_{_{i}}[F(;)]\), and the optimization algorithm can access \(F_{i}(;)\) and \( F_{i}(;)\) for individual values of \(\). We make the following assumptions about the objectives:

**Assumption 1**.: _(a) \(f(_{0})-_{^{d}}f()\). **(b)** Each \(f_{i}\) is \(L\)-smooth, i.e., \(\| f_{i}()- f_{i}()\| L\|-\|\) for all \(,^{d}\). **(c)** The stochastic gradient has variance \(^{2}\), i.e., \(_{_{i}}[\| F_{i}(;)- f_{i}( )\|^{2}]^{2}\) for all \(^{d}\)._Since each \(f_{i}\) may be non-convex, we consider the problem of finding an \(\)-stationary point of \(f\), that is, a point \(^{d}\) such that \(\| f()\|\).

### Participation Framework

We consider a federated learning framework consisting of \(R\) rounds. For any round \(r\{0,,R-1\}\) and client \(i[N]\), the availability of client \(i\) at round \(r\) is a random variable \(q_{r}^{i}\), following the arbitrary participation framework of . If \(q_{r}^{i}=0\), then client \(i\) may not participate during round \(i\). For example, under the conventional i.i.d. sampling of clients, at each round \(r\) a subset of clients \(_{r}[N]\) is sampled uniformly without replacement, and the weights are set as \(q_{r}^{i}=_{r}\}}{S}\).

For some \(P\), let \(_{r_{0}}\) be the filtration generated by \(\{q_{r}^{i}:r_{0} r<r_{0}+P,i[N]\}\), let \(\) be the filtration generated by \(_{0},,_{R-P}\), and let \(\) be the filtration generated by \(\{_{r,k}^{i}:0 r<R,0 k<I,i[N]\}\), where \(_{r,k}^{i}\) is the random sampling of the stochastic gradient of round \(r\), step \(k\), client \(i\). We make the following assumptions about the participation distribution.

**Assumption 2**.: _For all \(r\{0,,R-1\}\): **(a)**\(_{i=1}^{N}q_{r}^{i}=1\) and \(_{i=1}^{N}(q_{r}^{i})^{2}^{2}\). **(b)** The distribution of \(\{q_{r}^{i}\}\) is unbiased across clients over every window of \(P\) rounds, i.e., \(_{_{r_{0}}}[_{r=mP}^{(m+1)P-1}q_{r}^{i}]=1/N\) for every \(m<R/P\) and \(i[N]\). **(c)** Each client has a non-zero probability of being sampled over every window of \(P\) rounds, i.e., \(_{_{r_{0}}}(_{r=mP}^{(m+1)P-1}q_{r}^{i}>0) >p_{}\) for every \(m<R/P\) and \(i[N]\). **(d)**\(\) and \(\) are independent._

For each round, Assumption 2(a) enforces that the participation weights \(q_{r}^{i}\) are normalized to sum to 1, and characterizes the spread of participation weights across clients with the constant \(^{2}\). Assumption 2(b) requires that the set of rounds can be partitioned into windows of length \(P\) within which clients are expected to participate with equal frequency. Lastly, Assumption 2(c) enforces that within each window, for each client the probability of being sampled is nonzero. Conventional i.i.d client sampling satisfies Assumption 2 with \(=S^{-1/2},P=1\), and \(p_{}=S/N\).

An important difference from conventional i.i.d. participation is that here, client participation is not necessarily independent across rounds. Accordingly, we emphasize that the expectation and probability in Assumptions 2(b)-(c) are taken only over \(_{r_{0}}\). Therefore, the mean participation weight in Assumption 2(b) may itself be a random variable if client participation at some rounds is dependent on the outcome of participation in previous rounds. Similarly, the sampling probability in Assumption 2(c) may be a random variable. For the participation patterns considered in the next section, Assumption 2 is satisfied even when client sampling is not independent across rounds.

### Specific Participation Patterns

**Regularized Participation** We say that client participation is _regularized_ if \(_{r_{0}}^{i}=\) almost surely for all \(r_{0}\) and \(i\), where \(_{r_{0}}^{i}\) is defined on Line 18 of Algorithm 1 as the participation of client \(i\) averaged over rounds \(r_{0},,r_{0}+P-1\). In this case, Assumption 2 is satisfied with \(p_{}=1\), while \(P\) and \(^{2}\) are parameters of the participation pattern. Regularized participation is a relatively strong constraint, since every client must participate within each window, which may not be practical. However, it is flexible in that there is no constraint on how clients participate within each window. Regularized participation was also considered for strongly convex objectives .

**Cyclic Participation** Following the CyCP framework , \(N\) clients are partitioned into \(\) equally sized subsets, and at round \(r\) only clients in group \((r)\) may participate. \(S\) clients are sampled without replacement from group \((r)\), for whom the participation weight is \(q_{r}^{i}=1/S\). All other clients are assigned \(q_{r}^{i}=0\). Cyclic participation satisfies Assumption 2 with \(P=\), \(=S^{-1/2}\), and \(p_{}=S/N\). Notice that i.i.d. client sampling is the special case where \(=1\).

Cyclic participation can model a situation where each client group is available at a different time of day. For example, if client devices are mobile phones, then clients are available for participation at night, when phones are charging, likely to have internet connection, and otherwise idle. If devices are spread across the globe, then client groups are naturally formed by time zones. Cyclic participation is less stringent than regularized participation since not all clients are required to participate within each window. FedAvg was analyzed under cyclic participation for PL objectives , although this analysis is not applicable in our setting, which uses general non-convex objectives.

```
1:Initialize \(_{0}\), \(_{0}^{i}\), \(_{0}_{i=1}^{N}_{0}^{i},r_{0} 0, 0\)
2:for\(r=0,1,,R-1\)do
3:for\(i[N]\)do
4:\(}_{r,0}^{i}}_{r}\)client
5:for\(k=0,,I-1\)do
6: Sample \(_{r,k}^{i}_{i}\)
7:\(_{r,k}^{i} F_{i}(_{r,k}^{i};_{r,k}^{i})\)
8:\(_{r,k+1}^{i}_{r,k}^{i}-(_{r,k}^{i}-_{r_{ 0}}^{i}+_{r_{0}})\)
9:endfor
10:\(_{r}^{i}_{r,I}^{i}-}_{r}\)
11:endfor
12:\(}_{r+1}}_{r}+_{i=1}^{N}q_{r}^{i}_{r }^{i}\)server
13:\(+_{i=1}^{N}q_{r}^{i}_{r}^{i}\)
14:if\(r=r_{0}+P-1\)then
15:\(}_{r_{0}+P}}_{r_{0}}+\)
16:for\(i[N]\)do
17:\(_{r_{0}}^{i}_{s=r_{0}}^{r_{0}+P-1}q_{s}^{i}\)
18:\(_{r_{0}}^{i}_{r_{0}}^{i}_{s=r_{0}}^{r_{ 0}+P-1}_{k=0}^{I-1}q_{s}^{i}_{r,k}^{i}\)
19:\(_{r_{0}+P}^{i}\{_{r_{0}}^{i}>0\} _{r_{0}}^{i}+\{_{r_{0}}^{i}=0\}_{r_{0}} ^{i}\)
20:endfor
21:\(_{r_{0}+P}_{i=1}^{N}_{r_{0}+P}^{i}\)
22:\(r_{0} r_{0}+P\)
23:\( 0\)
24:endif
25:endfor ```

**Algorithm 1** Amplified SCAFFOLD

## 4 Algorithm and Analysis

In this section, we present Amplified SCAFFOLD, our algorithm to solve the FL problem described in Section 3. Pseudocode for Amplified SCAFFOLD is shown in Algorithm 1. The main components of the Amplified SCAFFOLD algorithm are (1) amplified updates and (2) long-range control variates.

### Algorithm Overview

To deal with the non-stationarity of client availability, Amplified SCAFFOLD performs amplified updates based on information accumulated over a window of \(P\) rounds. In Algorithm 1, the variable \(\) holds a weighted average of local updates to client models, weighted by client participation. Every \(P\) rounds, the global model is updated in the direction \(\) scaled by the amplification factor \(\). Informally, the direction \(\) includes information from all clients with equal representation, according to Assumption 2(b). Similar amplified updates are used in Amplified FedAvg .

Control variates for heterogeneous federated learning were first introduced by SCAFFOLD . However, SCAFFOLD-style control variates are updated every time a client participates, which may not be appropriate under periodic availability. For example, under non-i.i.d. participation control variates for different clients would be updated with different frequencies, so that some clients may have consistently less accurate control variates than others. Informally, this may lead to a bias in which some clients' objective is underweighted relative to others. To deal with this issue, Amplified SCAFFOLD updates control variates based on information accumulated over a window of \(P\) rounds, which enforces equal representation of all clients in expectation, according to Assumption 2(b).

**Comparison with [17; 39]** Although the two algorithmic components of Amplified SCAFFOLD individually appear in previous work [17; 39], we emphasize that our complexity results cannot be achieved by simply combining the analyses of these two works. The analysis of  requires \(^{-4}\) communication cost due to their treatment of the randomness in client participation. Here, we present a tighter analysis with \(^{-2}\) cost from a more fine-grained treatment of the two sources of randomness (stochastic gradients and client sampling). See Section 4.4 for more details on our approach.

### Main Results

Let \(}=}_{mP}\), where \(m\) is sampled uniformly from \(\{0,,R/P-1\}\), and let \(r_{0}\{0,P,,R/P\}\). Denote \(w^{i}_{r_{0}}=_{j=1}^{N}^{j}_{r_{0}}>0 \}}{P^{j}_{r_{0}}}_{s=r_{0}}^{r_{0}+P-1}q^{i}_{s}q^{j}_{s}\) and \(v^{i}_{r_{0}}=^{i}_{r_{0}}-\). Informally, \(w^{i}_{r_{0}}\) represents the "non-uniformity" of the client sampling distribution. We also consider a variable \(^{i}_{r_{0}}\) that depends only on the client sampling distribution and characterizes the sample size from which \(^{i}_{r_{0}}\) is computed. See Appendix A.1 for further discussion of these quantities. We consider convergence under the following conditions, which are satisfied by several participation patterns of interest.

\[[w^{i}_{r_{0}}]}{N}$ and $i$},\] (1)

\[[_{i=1}^{N}(v^{i}_{r_{0}})^{2}^{i}_{r_{0 }}]^{2}$ and $i$},\] (2)

**Theorem 1**.: _Suppose that Assumptions 1 and 2 hold, and that Equation 1 and Equation 2 hold. If \(}}{60LIP}\) and \(}}}{60LIP}\), then Algorithm 1 satisfies_

\[[\| f(})\|^{2}](+( L^{2}+^{2}L^{2}IP)^{2} ).\]

**Corollary 1**.: _For any \(>0\) and \(I 1\), there exist choices of \(\) and \(\) such that \([\| f(})\|^{2}](^{2})\) as long as \(R(^{2}}{I^{4}}+}^{2}})\)._

The complexity of Amplified SCAFFOLD has several important properties:

**Reduced Communication** By choosing \(I=(^{2}^{2}p_{}P^{-1}^{-2})\), Amplified SCAFFOLD has communication complexity \(R=(LPp_{}^{-1}^{-2})\), which improves upon the \(^{-4}\) complexity of parallel SGD. We are not aware of any existing work that achieves this communication reduction for non-convex federated optimization with periodic participation.

**Unaffected by Heterogeneity** The iterations \(RI\) and the number of communications \(R\) are unaffected by heterogeneity, which is not achieved for periodic participation by any existing work [39; 8].

**Linear Speedup** The number of iterations \(RI=( L^{2}^{2}^{-4})\) will exhibit linear speedup in the number of clients through the term \(^{2}\), depending on the client participation pattern.

### Application to Participation Patterns

The results from Section 4.2 apply under any participation pattern that satisfies Assumption 2, Equation 1, and Equation 2, and below we discuss the participation patterns discussed in Section 3.2. The complexity of Amplified SCAFFOLD for each participation pattern are shown in Table 1, and these results can be obtained by plugging \(^{2}\), \(P\), and \(p_{}\) into Corollary 1, together with a choice of \(I\) as described in Section 4.2. The derivations of each result below are given in Appendix B.

**Regularized Participation** Recall that regularized participation satisfies Assumption 2 with \(p_{}=1\), and \(P\), \(^{2}\) are parameters of the participation pattern. Also, under regularized participation, \(w^{i}_{r_{0}}=^{i}_{r_{0}}=1/N\) almost surely, so that \([w^{i}_{r_{0}}]=1/N P^{2}/N\) and \(v^{i}_{r_{0}}=0\). Therefore Equation 1 and Equation 2 are satisfied. Plugging \(p_{}=1\) into Corollary 1 yields

\[R=(LP^{-2}), RI=( L ^{2}^{2}^{-4}).\]

In this setting, our algorithm exhibits reduced communication and resilience to heterogeneity. To our knowledge, the only existing algorithm with theoretical guarantees for non-convex problems under regularized participation is Amplified FedAvg . However, as seen in Table 1, the communication complexity of Amplified FedAvg has order \(^{-4}\) in terms of \(\) and suffers from a \(^{2}\) dependence.

**Cyclic Participation** Recall that cyclic participation satisfies Assumption 2 with \(P=\), \(=S^{-1/2}\), and \(p_{}=S/N\). Also, \([w_{r_{0}}^{i}]=S/N^{2} P^{2}/N\) and \([(w_{r_{0}}^{i})^{2}_{r_{0}}^{i}]=^{2}\), so that Equation 1 and Equation 2 are satisfied. Based on the above parameter values, the resulting complexities are

\[R=(}{^{2}}()), RI=(}{S^{4}}).\]

Again, Amplified SCAFFOLD achieves reduced communication, linear speedup, and resilience to heterogeneity. Amplified FedAvg  is the only existing algorithm with theoretical guarantees in this setting, but it fails to achieve resilience to heterogeneity or reduce communication cost outside of the trivial case of full participation (\(=1,S=N\)). Also, even for the setting of PL-functions, the convergence rate of FedAvg under cyclic participation from  does not demonstrate an improvement with respect to the number of local steps. See Appendix F for further discussion of their results.

Recall that i.i.d. participation is a special case of cyclic participation with \(=1\). In this case, Amplified FedAvg fails to recover the reduced communication usually achieved under i.i.d. participation, such as by SCAFFOLD . In fact, Amplified FedAvg fails to recover the communication cost of FedAvg under i.i.d. participation, requiring an additional factor of \(LN\). The larger communication cost of Amplified FedAvg is a result of its convergence analysis, which does not leverage the property of unbiased participation (Assumption 2(b)) during the analysis, and requires \(P=(^{-2})\) in order to converge (see Appendix C for more details). In contrast, Amplified SCAFFOLD succeeds in recovering the results of SCAFFOLD under i.i.d. participation, with only a slightly worse dependence of \(R\) on \(N/S\). This difference in the order of \(\) is due to a potential small issue in the analysis of SCAFFOLD, which we intentionally avoided by accepting a slightly worse dependence on \(\). We provide a detailed discussion of the \(N/S\) dependence in Appendix F.

### Proof Sketch

The main challenges for demonstrating convergence are (1) simultaneously handling randomness from stochastic gradients and non-i.i.d. client sampling, and (2) controlling error of control variates under non-i.i.d. client sampling. Previous work  subverts (1) by conditioning on \(\) throughout the entire analysis. However, this eliminates the possibility of utilizing the condition \([_{r_{0}}^{i}]=1/N\), and ultimately incurs a dependence on the data heterogeneity (see the term \(^{2}(P)\) in Theorem 3.1 of ). Instead, we take expectation over both sources of randomness throughout the analysis, which requires a careful treatment of each iterate's dependence, and enables communication reduction. For (2), previous analysis of federated algorithms with control variates  recursively bounds the error of control variates between consecutive rounds. However, this recursion crucially depends on i.i.d. client participation. We extend this analysis to our setting, establishing a recursion over the control variate error between consecutive windows of \(P\) rounds. Establishing this recursion under non-i.i.d. participation involves a non-uniform average of non-uniform averages of error terms, which we handle by invoking the regularity conditions stated in Equation 1 and Equation 2.

Using smoothness of \(f\), the objective function decrease \(f(}_{r_{0}+P})-f(}_{r_{0}})\) is upper bounded by \( f(}_{r_{0}}),}_{r_{0}+P}-}_{r_ {0}}+\|}_{r_{0}+P}-}_{r_{0}}\|^{2}\). Letting \(}_{r,k}=_{i=1}^{N}q_{r}^{i}_{r,k}^{i}\) be a weighted average of local models, the sum of the previous inner product and quadratic terms can be bounded by \(- IP\| f(}_{r_{0}})\|^{2}\), plus standard noise terms, the additional "drift" terms

\[_{r,k}=_{i=1}^{N}q_{r}^{i}\|_{r,k}^{i}-}_{r,k}\|^{2}_{r,k}=\|}_{r,k}-}_{r _{0}}\|^{2},\]

and control variate errors \(C_{r_{0}}^{i}=\| f_{i}(}_{r_{0}})-_{r_{0}}^{i}\|^{2}\). \(_{r,k}\) captures the distance between local client models, while \(_{r,k}\) captures the distance from local models to the previous global model \(}_{r_{0}}\).

Taking conditional expectation \(D_{r,k}=[_{r,k}|]\) and \(M_{r,k}=[_{r,k}|]\), Lemma 1 bounds the drift terms by establishing and unrolling a mutually recurrent relation between \(D_{r,k}\) and \(M_{r,k}\). The resulting bound involves a non-uniform average over the control variate errors: \(_{i=1}^{N}q_{r}^{i}[C_{r_{0}}^{i}|]\).

Denoting the average control variate error \(C_{r_{0}}=_{i=1}^{N}C^{i}_{r_{0}}\), we want to bound \([C_{r_{0}+P}]\) in terms of \([C_{r_{0}}]\). \(C_{r_{0}+P}\) can be decomposed into drift terms, but the result is a non-uniform average:

\[_{s=r_{0}}^{r_{0}+P-1}_{k=0}^{I-1}q^{i}_{s}(D_{s,k}+M_{s,k}).\]

Since the bound for each \(M_{s,k}+D_{s,k}\) from Lemma 1 involves a non-uniform average over \(C^{i}_{r_{0}}\), the resulting bound of \(C_{r_{0}+P}\) involves a non-uniform average of non-uniform averages of \(C^{i}_{r_{0}}\), instead of the uniform average \(C_{r_{0}}\). The regularity conditions in Equation 1 and Equation 2 allow us to bound this nested non-uniform average by a uniform average, which finishes the recursion.

Putting everything together, we obtain the descent inequality

\[[_{r_{0}+P}][_{r_{0}}]- IP [\| f(}_{r_{0}})\|^{2}]+ IP(  L^{2}+^{2}L^{2}IP)^{2},\]

where \(_{r_{0}}:=f(}_{r_{0}+P})+(r_{0}+P)\) and \(\) is a potential function that depends on the control variate errors. Theorem 1 is then obtained by averaging over \(r_{0}\) and isolating the gradient.

## 5 Experiments

We experimentally validate our algorithm for non-i.i.d client participation under three settings: minimizing a synthetic function, logistic regression for Fashion-MNIST 1, and training a CNN for CIFAR-10 . We also include an ablation study on Fashion-MNIST, to investigate how each algorithm is affected by changes in data heterogeneity, the number of participating clients, and the number of client groups in cyclic participation.

### Setup

All of our experiments utilize a non-i.i.d. client participation pattern similar to cyclic participation (discussed in Section 3.2). We partition the total set of \(N\) clients into \(\) equally sized subsets, and at each training round only a single client group is available for participation. In our experiments, the available group does not change every round; instead, each group is available for \(g\) rounds at a time. Under this pattern, Assumption 2 is satisfied with \(P=g\). We refer to \(g\) as the availability time.

We evaluate five algorithms: FedAvg , FedProx , SCAFFOLD , Amplified FedAvg , and Amplified SCAFFOLD (ours). We tune each algorithm's parameters by grid search, including learning rate \(\), amplification rate \(\), and FedProx's \(\). The search ranges and tuned values can be found in Appendix D. All experiments were run on a single node with eight NVIDIA A6000 GPUs. Code is available at the following repository: https://github.com/MingruLiu-ML-Lab/FL-under-Periodic-Participation

**Synthetic** We evaluate each algorithm's convergence on a difficult objective based on a lower bound for FedAvg . The objective maps \(^{4}\) to \(\), is convex, and is parameterized by a smoothness \(L\), stochastic gradient variance \(^{2}\), and heterogeneity \(\), so that it satisfies Assumption 1 by construction. The complete definition of the objective can be found in Appendix D. Since there are only two distinct local objectives, we set the number of clients \(N=2\) and the number of sampled clients \(S=1\), and the number of groups \(=2\). All other settings can be found in Appendix D.

**Fashion-MNIST and CIFAR-10** We evaluate each algorithm for training an image classifier, using logistic regression for Fashion-MNIST and a two-layer CNN for CIFAR-10. To simulate heterogeneous data in federated learning, we use a common protocol [17; 39], to partition each dataset into client datasets according to a data similarity parameter \(s\). This protocol is detailed in Appendix D. Following , we set the number of clients \(N=250\), data similarity \(s=5\%\), and the number of sampled clients per round \(S=10\). For client participation, we set the number of groups \(=5\), so that each group contains clients that have majority label from two different classes. We run all baselines with 5 different random seeds and report the mean results with error bars in Section 5.2 (the radius of each error bar is 1 standard deviation). All other settings can be found in Appendix D.

Additional experimental results are provided in Appendix E, where we compare against extra baselines (FedAdam , FedYogi , FedAvg-M , and Amplified FedAvg with FedProx regularization), and evaluate training under another non-i.i.d. client participation pattern.

### Main Results

Results for the synthetic experiment and CIFAR-10 are shown in Figure 1, and results for Fashion-MNIST are shown in Figure 2. We make the following observations:

**Amplified SCAFFOLD converges the fastest.** In all three settings, Amplified SCAFFOLD reaches the best overall solution among all algorithms (by all metrics) and requires the fewest communication rounds. In the synthetic experiment, Amplified SCAFFOLD requires 800 communication rounds to reach an objective value of \(0.2\), while SCAFFOLD requires 1900 rounds, and both FedAvg and Amplified FedAvg require 4800 rounds to reach the same objective value.

**Amplified FedAvg is comparable to FedAvg.** Amplified FedAvg shows slight improvement over FedAvg for the synthetic experiment and for Fashion-MNIST. Only for CIFAR-10 is Amplified FedAvg significantly faster than FedAvg, but there it also exhibits a reduction in stability. The under-whelming experimental performance of Amplified FedAvg corroborates our discussion from Section 4.3; Amplified FedAvg requires many communication rounds and suffers from data heterogeneity.

Contrary to our findings, the original evaluation of Amplified FedAvg  showed a significant improvement over FedAvg. One explanation is that the original evaluation employed pretraining using FedAvg, so that each algorithm was evaluated only for fine-tuning. Our experiments suggest that Amplified FedAvg may have limited improvement over FedAvg when training from scratch.

**SCAFFOLD beats Amplified FedAvg.** Despite a lack of theoretical guarantees under non-i.i.d. participation, SCAFFOLD outperforms Amplified FedAvg in all settings. This suggests that SCAFFOLD may have reasonable performance under some non-i.i.d. participation patterns. For the synthetic objective and CIFAR-10, SCAFFOLD is still significantly slower than Amplified SCAFFOLD.

### Ablation Study

To understand how each algorithm's performance is affected by data heterogeneity, the number of participating clients, and the number of client groups, we perform an ablation study on Fashion-MNIST. First, we fix the data similarity \(s=5\%\) and number of groups \(=5\) while varying the number of participating clients (\(S\)) over \(\{5,15,20,25\}\). Next, we fix \(S=10,=5\) while varying the similarity \(s\) over \(\{2.5\%,10\%,33\%,100\%\}\). Lastly, we fix \(s=5\%,S=10\) while varying the number of client groups \(\{2,4,6,8\}\). In each of these 12 scenarios, we evaluate all five algorithms using the same settings as detailed in Section 3. We train with three random seeds for each algorithm, and report the average results in Figure 2 (right).

**Amplified SCAFFOLD reaches the best solution in all settings.** Similarly to Section 5.2, Amplified SCAFFOLD consistently reaches the best solution in terms of both training loss and testing accuracy. While our theoretical results provide guarantees for optimization, these experiments show that Amplified SCAFFOLD also exhibits superior generalization in a variety of settings.

**Robustness to data heterogeneity.** When changing from completely homogeneous data (\(s=100\%\)) to extremely heterogeneous data (\(s=2.5\%\)), the test accuracy of Amplified SCAFFOLD exhibits a

Figure 1: Results for synthetic objective and CIFAR-10. Left: Amplified SCAFFOLD and SCAFFOLD both converge to the global minimum, but Amplified SCAFFOLD converges significantly faster. Right: Amplified SCAFFOLD converges to the best solution by a significant margin. Note that in both cases, the curves for FedAvg and FedProx are nearly overlapping.

very small decrease from \(84.6\%\) to \(84.45\%\), so that our algorithm behaves nearly identically with homogeneous data as with extremely heterogeneous data. All baselines suffer a larger decrease when transitioning from homogeneous data to heterogeneous data.

**Robustness to number of participating clients.** The number of participating clients has a smaller effect on performance than data heterogeneity, but some degradation happens in the extreme case \(S=5\). In particular, SCAFFOLD has competitive performance with large \(S 15\), but its test accuracy drops off significantly compared to Amplified SCAFFOLD in the case \(S=5\).

**Robustness to number of client groups.** As \(\) increases, FedAvg and Amplified FedAvg get worse, while SCAFFOLD and Amplified SCAFFOLD maintain performance. It makes intuitive sense for an algorithm to degrade as \(\) increases, since a larger \(\) means that the participation is in some sense "further" from i.i.d. participation. Still, Amplified SCAFFOLD (and SCAFFOLD) are able to maintain performance even as \(\) increases. While the worst-case communication complexity of Amplified SCAFFOLD (listed in Table 1) actually increases with \(\), these experiments demonstrate that in practice, Amplified SCAFFOLD can maintain performance as \(\) increases.

## 6 Conclusion

We propose Amplified SCAFFOLD, an optimization algorithm for federated learning under periodic client participation, and prove that it exhibits reduced communication cost, linear speedup, and is unaffected by data heterogeneity. We also show that Amplified SCAFFOLD experimentally outperforms baselines on standard benchmarks under non-i.i.d. client participation, and that the performance of our algorithm is robust to changes in data heterogeneity and the number of participating clients.

**Limitations** While our analysis covers a general class of participation patterns, it may not cover some participation patterns that appear in practice. Our framework requires that all clients have an equal chance of participation across well-defined windows of time that are known to the algorithm implementer, which may not always hold. One such practical situation is where clients may freely join or leave the federated learning process during training. Extending our algorithm and guarantees for this situation would require a reformulation of the optimization problem, and possibly additional assumptions about the participation structure. We leave such analysis for future work.