# Solving Sparse & High-Dimensional-Output Regression via Compression

Renyuan Li

Department of Industrial Systems Engineering & Management

National University of Singapore

renyuan.li@u.nus.edu

&Zhehui Chen

Google

zhehuichen@google.com

&Guanyi Wang

Department of Industrial Systems Engineering & Management

National University of Singapore

guanyi.w@nus.edu.sg

###### Abstract

Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making. Unlike traditional regression models, MOR aims to simultaneously predict multiple real-valued outputs given an input. However, the increasing dimensionality of the outputs poses significant challenges regarding interpretability and computational scalability for modern MOR applications. As a first step to address these challenges, this paper proposes a Sparse & High-dimensional-Output REgression (SHORE) model by incorporating additional sparsity requirements to resolve the output interpretability, and then designs a computationally efficient two-stage optimization framework capable of solving SHORE with provable accuracy via compression on outputs. Theoretically, we show that the proposed framework is computationally scalable while maintaining the same order of training loss and prediction loss before-and-after compression under arbitrary or relatively weak sample set conditions. Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.

## 1 Introduction

Multi-Output Regression (MOR) problem [8; 44] is a preponderant tool for factor prediction and decision-making in modern data analysis. Compared with traditional regression models that focus on a scalar output for each sample, MOR aims to predict multiple outputs \(^{K}\)_simultaneously_ based on a given input \(^{d}\), i.e.,

\[:=*{arg\,min}_{}\ (,())\ \ \ \ :=*{arg\,min}_{g}\ _{i=1}^{n}(^{i},g(^{i}))\ \,\]

where we use \(\{(^{i},^{i})\}_{i=1}^{n}\) to denote its given sample set with \(^{i}^{d}\)\(i\)-th input feature vector and \(^{i}^{K}\) corresponding output vector, define \(:^{K}^{K}\) as the loss function, \(:^{K}^{K}\) as some prediction/distance metric, \(\) as the structure/constraint set for multiple outputs, and \(\) as the candidate set for predicting model \(g:^{d}^{K}\). Hence, MOR and its variants have been used for numerous regression tasks with structure requirements on multi-dimensional outputs arising from real applications, such as simultaneous estimation of biophysical parameters from remote sensing images , channel estimation through the prediction of several received signals , the grounding (e.g., factuality check ) in the Large Language Model (LLM, [34; 11]) era, to name but a few.

In this paper, we are interested in the interpretability issue of high-dimensional outputs obtained from modern MOR tasks. One typical example is raised from algorithmic trading. In particular, in algorithmic trading, MOR helps to construct the portfolio  from a large number of financial instruments (e.g., different stocks, futures, options, equities, etc ) based on given historical market and alternative data . To be concise, a high-dimensional output in this example could be viewed as a "decision", where every component denotes the investment for the corresponding financial instruments. Thus, other than accuracy, quantitative researchers prefer outputs with only a few instruments to enhance interpretability for the underlying decision-making reasons, which naturally introduces a sparse output condition. Similar scenarios apply to other applications, including offline reinforcement learning in robotics, discovering genetic variations based on genetic markers.

As a result, the dramatic growth in output dimensions gives rise to two significant challenges: **1**. High-dimensional-output impedes human interpretation for decision-making; **2**. Approaches with better computational scalability are desired for training & predicting MOR. Upon these challenges, a conceptual question that motivates this research is:

_How to design a framework that predicts output with enhanced interpretability, better computational scalability, and provable accuracy under a modern high-dimensional-output setting?_

Generally speaking, this paper provides an affirmative answer as a first step to the above question. Before presenting the main contributions, let us first introduce the model that will be studied in this paper. Unlike the classical MOR model, we further assume that given outputs are of high-dimensional (i.e., \(d K\)), and to address the interpretability issue, these outputs have at most \(s\) non-zero components, i.e., \(\|^{i}\|_{0} s\), for all \(i[n]\) with some pre-determined sparsity-level \(s( K)\). Based on such given samples, this paper proposes the (uncompressed) _Sparse & High-dimensional-Output REgression (SHORE)_ model that aims to predict an interpretable high-dimensional output \(\) (i.e., \(s\)-sparse in this paper) via any input feature vector \(\). In particular, to be concise and still capture the essential relationship, the proposed (uncompressed) SHORE model predicts \(\) from \(\) under a linear model, i.e., \(=_{\|\|_{0} s}(,} {x})\) for some distance metric (see Section 3.1, prediction stage) and the linear regression \(}\) is obtained by solving the following linear regression problem:

\[}:=*{arg\,min}_{^{K d}} }_{n}():=\|-\|_{F}^{2},\] (1)

where \(:=(^{1}^{n})^{d n}\) is the input matrix and \(:=(^{1}^{n})^{K n}\) is the corresponding _column-sparse_ output matrix.

### Contributions and Paper Organization

This paper makes the following three main contributions:

**1**. We propose a two-stage computationally efficient framework for solving SHORE model. Specifically, the first training stage offers a computationally scalable reformulation on solving SHORE through compression in the output space. The second prediction stage then predicts high-dimensional outputs from a given input by solving a specific sparsity-constrained minimization problem via an efficient iterative algorithm.

**2**. We show that for arbitrarily given samples, the training loss in the first stage with compression is bounded by a \(1+\) multiplicative ratio of the training loss for the original one (1) with some positive constant \(\). Additionally, the proposed iterative algorithm in the second stage exhibits global geometric convergence within a neighborhood of the ground-truth output, with a radius proportional to the given sample's optimal training loss. Furthermore, if all samples are drawn from a light-tailed distribution, the generalization error bound and sample complexity remain in the same order for SHORE with output compression. This finding indicates that the proposed framework achieves improved computational efficiency while maintaining the same order of generalization error bounds statistically.

**3**. We conduct rich numerical experiments that validate the theoretical findings and demonstrate the efficiency and accuracy of the proposed framework on both synthetic and real-world datasets.

In summary, this paper studies the SHORE model through computational and statistical lenses and provides a computationally scalable framework with provable accuracy.

The paper is organized as follows: Section 2 reviews related literature; Section 3 presents our proposed framework and provides theoretical results on sample complexity and generalization error bounds; Section 4 compares the proposed method with existing baselines in a suite of numerical experiments on both synthetic and real instances. Concluding remarks are given in Section 5.

Notation.Given a positive integer \(n\), we denote \([n]:=\{1,,n\}\). We use lowercase letters \(a\) as scalars and bold lowercase letters \(\) as vectors, where \(_{i}\) is its \(i\)-th component with \(i[d]\), and bold upper case letters \(\) as matrices. Without specific description, for a \(m\)-by-\(n\) matrix \(\), we denote \(_{i,j}\) as its \((i,j)\)-th component, \(_{i,:}^{}\), as its \(i\)-th row, \(_{:,j}\) as its \(j\)-th column. For a symmetric square matrix \(\), we denote \(_{}()\), \(_{}()\) and \(_{i}()\) as its maximum, minimum and \(i\)-th largest eigenvalue, respectively. We denote \(\|\|_{1},\|\|_{2},\|\|_{},\|\|_{F},\|\|_{}\) as the \(_{1},_{2},_{}\)-norm of a vector \(\), the Frobenius norm and the operator norm of a matrix \(\), respectively. We denote \(()\) as the indicator function, \(\|\|_{0}:=_{i=1}^{d}(_{i} 0)\) as the \(_{0}\)-norm (i.e., the total number of nonzero components), \(():=\{i[d]_{i} 0\}\) as the support set. We denote \(_{s}^{K}:=\{^{K}\|\|_{0} s\}\) as a set of \(s\)-sparse vectors, \(_{2}(;):=\{^{K}\|-\|_{2 }\}\) as a closed \(_{2}\)-ball with center \(\) and radius \(\), \((,^{2})\) as a Gaussian distribution with mean \(\) and covariance \(^{2}\).

For two sequences of non-negative reals \(\{f_{n}\}_{n 1}\) and \(\{g_{n}\}_{n 1}\), we use \(f_{n} g_{n}\) to indicate that there is a universal constant \(C>0\) such that \(f_{n} Cg_{n}\) for all \(n 1\). We use standard order notation \(f_{n}=O(g_{n})\) to indicate that \(f_{n} g_{n}\) and \(f_{n}=_{}(g_{n})\) to indicate that \(f_{n} g_{n}^{c}(1/)\) for some universal constants \(\) and \(c\). Throughout, we use \(,,,c,c_{1},c_{2},\) and \(C,C_{1},C_{2},\) to denote universal positive constants, and their values may change from line to line without specific comments.

## 2 Literature Review

Multi-output regression (MOR) and its variants have been studied extensively over the past decades. In this section, we focus on existing works related to our computational and statistical results.

**Computational part.** Existing computational methods for solving MOR can be, in general, classified into two categories , known as problem transformation methods and algorithm adaptation methods. Problem transformation methods (e.g., Binary Relevance (BR), multi-target regressor stacking (MTRS) method , regression chains method ) aim to transform MOR into multiple single-output regression problems. Thus, any state-of-the-art single-output regression algorithm can be applied, such as ridge regression , regression trees , and etc. However, these transformation methods ignore the underlying structures/relations between outputs, which leads to higher computational complexities. In contrast, algorithm adaptation methods focus more on the underlying structures/relations between outputs. For instance,  investigates input component selection and shrinkage in multioutput linear regression;  later couples linear regressions and quantile mapping and thus captures joint relationships among variables. However, the output dimension considered in these works is relatively small compared with modern applications, and their assumptions concerning low-dimensional structure of outputs are hard to verify. _To overcome these shortages, we consider high-dimensional-output regression with only an additional sparsity requirement on outputs._

**Statistical part.** There are numerous works concerning statistical properties of traditional or multi-output regressions.  gives sharp results on "out-of-sample" (random design) prediction error for the ordinary least squares estimator of traditional linear regression.  proposes an empirical risk minimization framework for large-scale multi-label learning with missing outputs and provides excess risk generalization error bounds with additional bounded constraints.  investigates the generalization performance of structured prediction learning and provides generalization error bounds on three different scenarios, i.e., Lipschitz continuity, smoothness, and space capacity condition.  designs an efficient feature selection procedure for multiclass sparse linear classifiers (a special case for SHORE with sparsity-level \(s=1\)), and proves that the proposed classifiers guarantee the minimax generalization error bounds in theory. A recent paper  studies transfer learning via multi-task representation learning, a special case in MOR, which proves statistically optimistic rates on the excess risk with regularity assumptions on the loss function and task diversity. _In contrast with these works, our contributions concentrate on how generalization error bounds change before and after the compression under relatively weak conditions on the loss function and underlying distributions._

**Specific results in MLC.** MLC is an important and special case for MOR with \(\{0,1\}\)-valued output per dimension, i.e., \(\{0,1\}^{K}\), and thus, in this paragraph, we use labels to replace outputs. Here, we focus on dimensionality reduction techniques on outputs, in particular, the compressed sensing and low-rank conditions on the output matrix \(\). The idea of compressed sensing rises from signal processing, which maps the original high-dimensional output space into a smaller one while ensuring the restricted isometry property (RIP). To the best of our knowledge, the compressed sensing technique is first used in  to handle a sparse expected output \([|]\). Later, [39; 12] propose Principle Label Space Transformation (PLST) and conditional PLST through singular value decomposition and canonical component analysis respectively. More recently, many new compression approaches have been proposed, such as robust bloom filter , log time log space extreme classification , merged averaged classifiers via hashing , etc. Additionally, computational efficiency and statistical generalization bounds can be further improved when the output matrix \(\) ensures a low-rank condition. Under such a condition,  provides a general empirical risk minimization framework for solving MLC with missing labels. _Compared with the above works, this paper studies MOR under a sparse & high-dimensional-output setting without additional correlation assumptions or low-rank assumptions for output space, and then provides a complete story through a computational and statistical lens._

## 3 Main Results

### Two-Stage Framework

This subsection presents a general framework for solving SHORE and then the computational complexity for the proposed framework _with/without compression_. Given a set of training samples \(\{(^{i},^{i})\}_{i=1}^{n}\) as described in Section 1, the framework can be separated into two stages: (compressed) training stage & (compressed) prediction stage.

**Training stage.** In the first training stage, the framework finds a _compressed regressor_ by solving a linear regression problem with compressed outputs. In particular, the framework compresses the original large output space (\(K\)-dim) to a smaller "latent" output space (\(m\)-dim) by left-multiplying a so-called "compressed" matrix \(^{m K}\) to outputs. Thus, the _compressed version_ of training stage in SHORE can be represented as follows,

\[}:=*{arg\,min}_{^{m d}} \ }_{n}^{}():=\|- {W}\|_{F}^{2}.\] (2)

We would like to point out that the idea of compressing the output space into some smaller intrinsic dimension has been used in many existing works, e.g., [17; 39; 12] mentioned in Section 2.

**Prediction stage.** In the second prediction stage, given any input \(^{d}\), the framework predicts a sparse output \(}\) by solving the following prediction problem based on the learned regressor \(}\) in the training stage,

\[}(}):=*{arg\,min}_{}\|-}\|_{2}^{2}\ \ \ \ _{s}^{K},\] (3)

where \(_{s}^{K}\) is the set of \(s\)-sparse vectors in \(^{K}\), and \(\) is some feasible set to describe additional requirements of \(\). For example, by letting \(\) be \(^{K},^{K}_{+},\{0,1\}^{K}\), the intersection \(_{s}^{K}\) denotes the set of \(s\)-sparse output, non-negative \(s\)-sparse output, \(\{0,1\}\)-valued \(s\)-sparse output, respectively. We use \(}(})\) (shorthanded in \(}\)) to specify that the predicted output is based on the regressor \(}\). To solve the proposed prediction problem (3), we utilize the following projected gradient descent method (Algorithm 1), which could be viewed as a variant/generalization of existing iterative thresholding methods [6; 21] for nonconvex constrained minimization. In particular, step 4 incorporates additional constraints from \(\) other than sparsity into consideration, which leads to non-trivial modifications in designing efficient projection oracles and convergence analysis. Later, we show that the proposed Algorithm 1 ensures a near-optimal convergence (Theorem 2 and Theorem 4) while greatly reduces the computational complexity (Remark 2) of the prediction stage for solving compressed SHORE.

Before diving into theoretical analysis, we first highlight the differences between the proposed prediction stage (3), general sparsity-constrained optimization (SCO), and sparse regression in the following remark.

**Remark 1**.: _Proposed prediction stage v.s. General SCO: To be clear, the SCO here denotes the following minimization problem \(_{\|\|_{0} k}\|-\|_{2}^{2}\). Thus, the prediction stage is a special case of general SCO problem. In particular, the predicted stage takes a random projection matrix \(\) with restricted isometry property (RIP) to be its \(\) and uses \(}\) with \(}\) obtained from the compressed training-stage to be its \(\). As a result (Theorem 2 and Theorem 4), the proposed Algorithm 1 for prediction stage ensures a globally linear convergence to a ball with center \(}\) (optimal solution of the prediction-stage) and radius \(O(\|}-}\|_{2})\), which might not hold for general SCO problems._

_Proposed prediction stage v.s. Sparse regression: Although the proposed prediction stage and sparse high-dimensional regression share a similar optimization formulation \(_{\|\|_{0} k}\ \|-^{}\|_{2}^{2}\), the proposed prediction stage (3) is distinct from the sparse regression in the following parts:_

_(1) Underlying Model: Most existing works about sparse high-dimensional regression assume that samples are i.i.d. generated from the linear relationship \(=^{}^{*}+\) with underlying sparse ground truth \(^{*}\). In the proposed prediction stage, we do not assume additional underlying models on samples if there is no further specific assumption. The problem we studied in the predicted stage takes the random projection matrix \(\) with restricted isometry property (RIP) as its \(^{}\) (whereas \(^{}\) in sparse regression does not ensure RIP), and uses \(}\) with \(}\) obtained from the compressed training-stage as its \(\)._

_(2) Problem Task: The sparse regression aims to recover the sparse ground truth \(^{*}\) given a sample set \(\{(^{i},^{i})\}_{i=1}^{n}\) with \(n\) i.i.d. samples. In contrast, the task of the proposed prediction stage is to predict a sparse high-dimensional output \(}\) given a random projection matrix \(\) and a single input \(\). As a quick summary, some typical and widely used iterative algorithms  for sparse regression cannot be directly applied to the proposed prediction stage._

Then, we provide the computational complexity _with and without the compression_ for the proposed two-stage framework to complete this subsection.

**Remark 2**.: _Training stage: Conditioned on \(^{}\) is invertible, the compressed regressor \(}\) has a closed form solution \(}=^{}(^{})^{-1}\) with overall computational complexity_

\[O(Kmn+mnd+nd^{2}+d^{3}+md^{2}) O(Kmn).\]

_Compared with the computational complexity of finding \(}\) from the uncompressed SHORE (1)_

\[O(Knd+nd^{2}+d^{3}+Kd^{2}) O(K(n+d)d),\]

_solving \(}\) enjoys a smaller computational complexity on the training stage if \(m d\). In later analysis (see Section 3.2), \(m=O(^{-2}())\) with some predetermined constants \(,\) and sparsity-level \(s d\), thus in many applications with large output space, the condition \(m d\) holds._

_Prediction stage: The computational complexity of each step-3 in Algorithm 1 is_

\[O(Km+K+Km+K) O(Km).\]

_The projection in step-4 is polynomially solvable with computational complexity \(O(K\{s, K\})\) (see proof in Appendix A.5.1). Thus, the overall computational complexity of Algorithm 1 is_

\[O(K(m+\{s, K\})T).\]

_Compared with the complexity \(O(K(d+\{s, K\}))\) of predicting \(}\) from the uncompressed SHORE (1), the compressed version enjoys a smaller complexity on the prediction stage if_

\[(m+\{s, K\})T d+\{s, K\}.\] (4)

_In later analysis (see Theorem 2), since \(m=O(^{-2}())\) with predetermined constants \(,\), sparsity-level \(s d\), and \(T=O([}-}^{(0)}\|_{2}}{\| }-}\|_{2}}])\) from inequality (5), we have condition 4 holds._

_Whole computational complexity: Based on the analysis of computational complexity above, we conclude that when the parameters \((K,d,m,T)\) satisfies_

\[K>K^{1/3}>d O(^{-2}(K/) T)=mT,\]

_the compressed SHORE enjoys a better computational complexity with respect to the original one (1)._

### Worst-Case Analysis for Arbitrary Samples

We begin this subsection by introducing the generalization method of the compressed matrix \(\).

**Assumption 1**.: _Given an \(m\)-by-\(K\) compressed matrix \(\), all components \(_{i,j}\) for \(1 i m\) and \(1 j K\), are i.i.d. generated from a Gaussian distribution \((0,1/m)\)._

Before presenting the main theoretical results, let us first introduce the definition of restricted isometry property (RIP, ), which is ensured by the generalization method (Assumption 1).

**Definition 1**.: \((,)\)_**-RIP:** A \(m\)-by-\(K\) matrix \(\) is said to be \((,)\)-RIP over a given set of vectors \(^{K}\), if, for every \(\),_

\[(1-)\|\|_{2}^{2}\|\|_{2}^{2}(1+)\| \|_{2}^{2}.\]

_In the rest of the paper, we use \((s,)\)-RIP to denote \((_{s}^{K},)\)-RIP. Recall \(_{s}^{K}=\{^{K}\,|\,\|\|_{0} s\}\) is the set of \(s\)-sparse vectors._

**Remark 3**.: _From Johnson-Lindenstrauss Lemma , for any \((0,1)\), any \((0,1)\), and any finite vector set \(||<\), if the number of rows \(m O(^{-2}(|}{}))\), then the compressed matrix \(\) generated by Assumption 1 satisfies \((,)\)-RIP with probability at least \(1-\)._

Now, we are poised to present the first result on training loss defined in (2).

**Theorem 1**.: _For any \((0,1)\) and \((0,1)\), suppose compressed matrix \(\) follows Assumption 1 with \(m O(}())\). We have the following inequality for training loss_

\[\|-}\|_{F}^{2}(1+)\|-}\|_{F}^{2},\]

_holds with probability at least \(1-\), where \(},}\) are optimal solutions for the uncompressed (1) and compressed SHORE (2), respectively._

The proof of Theorem 1 is presented in Appendix A.1. In short, Theorem 1 shows that the optimal training loss for the compressed version is upper bounded within a \((1+)\) multiplicative ratio with respect to the optimal training loss for the uncompressed version. Intuitively, Theorem 1 implies that SHORE remains similar performances for both compressed and compressed versions, while the compressed version saves roughly \(O(Kn(d-m)+Kd^{2})\) computational complexity in the training stage from Remark 2. Moreover, the lower bound condition on \(m O(}())\) ensures that the generated compressed matrix \(\) is \((1,)\)-RIP with probability at least \(1-\). For people of independent interest, Theorem 1 only needs \((1,)\)-RIP (independent with the sparsity level) due to the _unitary invariant_ property of \(\) from Assumption 1 (details in Appendix A.1). Additionally, due to the inverse proportionality between \(m\) and \(^{2}\), for fixed \(K\) and \(\), the result can be written as

\[\|-}\|_{F}^{2}(1+O(1/) )\|-}\|_{F}^{2},\]

which is verified in our experiments 4.

```
1:Input: Regressor \(}\), input sample \(\), stepsize \(\), total iterations \(T\)
2:Initialize point \(^{(0)}_{s}^{K}\).
3:for\(t=0,1,,T-1\):do
4: Update \(}^{(t+1)}=^{(t)}-^{}(^{(t)}-})\).
5: Project \(^{(t+1)}=(}^{(t+1)}):=_{_ {s}^{K}}\|-}^{(t+1)}\|_{2}^{2}\).
6:endfor ```

**Output:\(^{(T)}\).** ```

**Algorithm 1** Projected Gradient Descent (for Second Stage)

We then present the convergence result of the proposed Algorithm 1 for solving prediction problem (3).

**Theorem 2**.: _For any \((0,1)\) and \((0,1)\), suppose the compressed matrix \(\) follows Assumption 1 with \(m O(}())\). With a fixed stepsize \((,1)\), the following inequality_

\[\|}-^{(t)}\|_{2} c_{1}^{t}\|}-^{(0)}\|_{2}+}{1-c_{1}}\|}-} \|_{2}\]

_holds for all \(t[T]\) simultaneously with probability at least \(1-\), where \(c_{1}:=2-2+2<1\) is some positive constant strictly smaller than 1, and \(c_{2}:=2\) is some constant._The proof of Theorem 2 is given in Appendix A.2. Here, the lower bound condition on the number of rows \(m\) ensures that the generated compressed matrix \(\) is \((3s,)\)-RIP with probability at least \(1-\) by considering a \(/2\)-net cover of set \(=_{3s}^{K}_{2}(;1)\) from Johnson-Lindenstrauss Lemma . Moreover, since the number of rows \(m\) required in Theorem 2 is greater than the one required in Theorem 1, term \(\|}-}\|_{2}\) can be further upper bounded using the uncompressed version \((1+)\|}-}\|_{2}\) with probability at least \(1-\). Then we obtain a direct corollary of Theorem 2: suppose \(\|}-^{(0)}\|_{2}>\|}-}\|_{2}\), if

\[t t_{*}:=O((\|}-^{(0)}\|_{2}/\|}-}\|_{2})\!(1/ c_{1})),\] (5)

the proposed Algorithm 1 guarantees a globally linear convergence to a ball \((};O(\|}-} {x}\|_{2}))\).

In contrast with OMP used in  for multi-label predictions, Theorem 2 holds for arbitrary sample set without the so-called bounded coherence guarantee on \(\). Moreover, as reported in Section 4, the proposed prediction method (Algorithm 1) has better computational efficiency than OMP.

### Generalization Error Bounds for IID Samples

This subsection studies a specific scenario when every sample \((^{i},^{i})\) is i.i.d. drawn from some underlying subGaussian distribution \(\) over sample space \(^{d}_{s}^{K}\). Specifically, we use

\[_{}[\\ ]=_{}\\ _{}=:_{ }[\\ ]=_{}&_{}\\ _{}&_{}=:_{(d+K)(d+K)}\]

to denote its mean and variance, respectively. Let \(_{}:=-_{},_{}:=- _{},:=(_{}^{},_{}^{})^{}\) be centered subGaussian random variables of \(,,(^{},^{})^{}\), respectively. Let \(,\{,\}\), we use \(_{}:=_{}[^{}]=_{}+_{}_{}^{},}_{}:=_{i=1}^{n}^{i}(^{i})^{}\) to denote the population second (cross-)moments and empirical second (cross-)moments, respectively. Then, the population training loss is defined as

\[_{^{K d}}():=_{(,)}[\|-\|_{2}^{2}]\]

with its optimal solution \(_{*}:=_{}_{}^{-1}\). Similarly, given a \(\), the compressed training loss is given by

\[^{}():=_{(,)} [\|-\|_{2}^{2}]\]

with optimal solution \(_{*}:=_{}_{}^{-1}\). We then define the following assumption:

**Assumption 2**.: _Let \(\) be \(^{2}\)-subGaussian for some positive constant \(^{2}>0\), i.e., the inequality \(_{}[(^{})]( ^{2}^{2}/2)\) holds for any \(>0\) and unitary vector \(^{d+K}\). Moreover, the covariance matrix \(_{}\) is positive definite (i.e., its minimum eigenvalue \(_{}(_{})>0\))._

**Remark 4**.: _Assumption 2 ensures the light tail property of distribution \(\). Note that in some real applications, e.g., factuality check , algorithmic trading , one can normalize input and output vector to ensure bounded \(_{2}\)-norm. Under such a situation, Assumption 2 is naturally satisfied._

Our first result in this subsection gives the generalization error bounds.

**Theorem 3**.: _For any \((0,1)\) and \((0,)\), suppose compressed matrix \(\) follows Assumption 1 with \(m O(}())\), and Assumption 2 holds, for any constant \(>0\), the following results hold: (Matrix Error). The inequality for matrix error \(\|_{}^{1/2}}_{}^{-1}_{ }^{1/2}\|_{} 4\) holds with probability at least \(1-2\) as the number of samples \(n n_{1}\) with_

\[n_{1}:=\{^{4}}{9_{}^{2}(_{ })}(d+(2/)),\;\|_{}\|_{2}^{ 2}^{2}}{_{}^{2}(_{})}(2+ {(1/)})^{2}\},\]

_where \(C\) is some fixed positive constant used in matrix concentration inequality of operator norm._

_(Uncompressed). The generalization error bound for uncompressed SHORE satisfies \((})(_{*})+4\) with probability at least \(1-3\), as the number of samples \(n\{n_{1},n_{2}\}\) with_

\[n_{2}:=\{4(\|_{*}\|_{F}^{2}+K)+2 (K/)}{},\;4\|_{}-_{*}_{}\|_{2}^ {2}\}.\]_(Compressed). The generalization error bound for the compressed SHORE satisfies \(^{}(})^{}(_{*})+ 4\) with probability at least \(1-3\), as the number of sample \(n\{n_{1},_{2}\}\) with_

\[_{2}:=\{4(\|_{*}\|_{F}^{2}+\|\|_{F}^{2}) +2(m/)}{},\ 4\|}-_{*}}\|_{2}^{2} \}.\]

The proof of Theorem 3 is presented in Appendix A.3. The proof sketch mainly contains three steps: In _Step-1_, we represent the difference \((})-(_{*})\) or \(^{}(})-^{}(_{*})\) as a product between matrix error (in Theorem 3) and rescaled approximation error (see Appendix A.3 for definition), i.e.,

\[(})-(_{*})\ \ \ \ ^{}(})- ^{}(_{*});\]

_Step-2_ controls the upper bounds for matrix error and rescaled approximation error separately, using concentration for subGuassian variables; for _Step-3_, we combine the upper bounds obtained in Step-2 and complete the proof. Based on the result of Theorem 3, ignoring the logarithm term for \(\), the proposed generalization error bounds can be bounded by

\[(}) (_{*})+_{}(\{\| _{*}\|_{F}^{2},\ \|}-_{*}}\|_{2}^{2},\ K\}),\] \[^{}(}) ^{}(_{*})+_{} (\{\|_{*}\|_{F}^{2},\ \|}-_{*}}\|_{2}^{2},\ \|\|_{F}^{2}\}).\]

**Remark 5**.: _To make a direct comparison between the generalization error bounds of the uncompressed and the compressed version, we further control the norms \(\|_{*}\|_{F}^{2},\ \|}-_{*}}\|_{2}^{2},\| \|_{F}^{2}\) based on additional conditions on the compressed matrix \(\). Recall the generalization method of the compressed matrix \(\) as mentioned in Assumption 1, we have the following event_

\[_{1}:=\{^{m K}\ |\ \|_{*}\|_{F}^{2}=\|_{*}\|_{F}^{2}( 1+)\|_{*}\|_{F}^{2}\\ \|}-_{*}}\|_{F}^{2}=\|(}-_{*}})\|_{F}^{2}(1+)\|}-_{*}}\|_{F}^{2}.\}\]

_holds with probability at least \(1-\) due to the RIP property for a fixed matrix. Moreover, since every component \(_{i,j}\) is i.i.d. drawn from a Gaussian distribution \((0,1/m)\), using the concentration tail bound for chi-squared variables (See Lemma 1 in ), we have the following event_

\[_{2}:=\{^{m K}\ |\ \|\|_{F}^{2} K+2}+.\}\]

_holds with probability at least \(1-\). Conditioned on these two events \(_{1}\) and \(_{2}\), the generalization error bound of the compressed version achieves the same order (ignoring the logarithm term of \(\)) as the generalization error bound of the uncompressed version. That is to say,_

\[^{}(})(1+)(_ {*})+_{}(\{\|_{*}\|_{F}^{2},\ \|}-_{*}}\|_{2}^{2},\ K\})\]

_holds with probability at least \(1-5\)._

Comparing with existing results on generalization error bounds mentioned in Section 2, we would like to emphasize that Theorem 4 guarantees that the generalization error bounds maintain the order before and after compression. This result establishes on i.i.d. subGaussian samples for the SHORE model without additional regularity conditions on loss function and feasible set as required in . Additionally, we obtained a \(O(Kd/n)\) generalization error bound for squared Frobenius norm loss function \(\) or \(^{}\), which is smaller than \(O(K^{2}d/n)\) as presented in [Theorem 4, ].

We then give results on prediction error bounds.

**Theorem 4**.: _For any \((0,1)\) and any \((0,1/3)\), suppose the compressed matrix \(\) follows Assumption 1 with \(m O(}())\), and Assumption 2 holds. Given any learned regressor \(}\) from training problem (2), let \((,)\) be a new sample drawn from the underlying distribution \(\), we have the following inequality holds with probability at least \(1-\):_

\[_{}[\|}-\|_{2}^{2}]_{}[\|-} \|_{2}^{2}],\]

_where \(}\) is the optimal solution from prediction problem (3) with input vector \(\)._The proof of Theorem 4 is presented in Appendix A.4. Theorem 4 gives an upper bound of \(_{2}\)-norm distance between \(}\) and \(\). Since \(\|^{(T)}-\|_{2}\|^{(T)}-}\|_{2}+\|}-\|_{2}\), combined with Theorem 2, we have \(_{}[\|^{(T)}-\|_{2}^{2}]\,O(_{ }[\|-}\|_{2}])\) when \(T t_{*}\) defined in (5) (see Appendix A.5.5), where the final inequality holds due to the optimality of \(}\). Hence, we achieve an upper bound of \(_{2}\)-norm distance between \(^{(T)}\) and \(\) as presented in Theorem 4, see Remark 6.

**Remark 6**.: _For any \((0,1)\) and \((0,1/3)\), suppose compressed matrix \(\) follows Assumption 1 with \(m O(}(}))\), and Assumption 2 holds, for any constant \(>0\), the following inequality holds with probability at least \(1-3\):_

\[_{}[\|^{(T)}-\|_{2}^{2}] O(_{ }[\|-}\|_{2}]) O(^{}(_{*})+4).\]

## 4 Numerical Experiments

In this section, we conduct numerical experiments on two types of instances (i.e., synthetic data sets and real data sets) to validate the theoretical results and illustrate both efficiency and accuracy of the proposed prediction method compared with typical existing prediction baselines, i.e., Orthogonal Matching Pursuit (OMP, ), Fast Iterative Shrinkage-Thresholding Algorithm (FISTA, ) and Elastic Net (EN, ). Due to the space limit, we put the implemented prediction method (Algorithm 2) in Appendix A.6.1, aforementioned existing prediction baselines in Appendix A.6.2, experiment setting details and results for real data in Appendix A.7.

**Performance measures.** Given a sample \((,)\) with input \(\) and corresponding true output \(\), we use \(\) to denote the predicted output obtained from any prediction method, and measure the numerical performances based on the following three metrics:

1. For a ground truth \(\) with sparsity-level \(s\), the metric _precision over selected supports_, i.e., \(@\ =|()()|\) measures the percentage of correctly identified supports in the predicted output;
2. The metric _output difference_, i.e., \(-:=\|-\|_{2}^{2}\) measures the \(_{2}\)-norm distance between the predicted output and the ground-truth;
3. For any given MOR \(}\) and compressed matrix \(\), the metric _prediction loss_, i.e., \(-:=\|-}\|_ {2}^{2}\) computes the prediction loss with respect to \(}\).

**Synthetic data generation procedure.** The synthetic data set is generated as follows: Every input \(^{i}\) for \(i[n]\) is i.i.d. drawn from a Gaussian distribution \((_{},_{})\), where its mean vector \(_{}\) and covariance matrix \(_{}\) are selected based on the procedures given in Appendix A.6.3. For any given sparsity-level \(s\), underlying true regressor \(_{*}^{K d}\), and Signal-to-Noise Ratio (SNR), the ground-truth \(^{i}\) (corresponding with its given input \(^{i}\)) is generated by \(^{i}=_{^{K}}(_{*}^{i}+ {}^{i})\), where \(^{i}^{K}\) is a i.i.d. random noise drawn from the Gaussian distribution \((_{K},^{-2}\|_{*}^{i}\|_{} _{K})\).

**Parameter setting.** For synthetic data, we set input dimension \(d=10^{4}\), output dimension \(K=2 10^{4}\), and sparsity-level \(s=3\). We generate in total \(n=3 10^{4}\), i.i.d. samples as described above, i.e., \(^{}:=\{(^{i},^{i})\}_{i=1}^{3 10^{4}}\) with \(^{-1}\{1,0.32,0.032\}\) to ensure the signal-to-noise decibels (dB, ) takes values on dB \(:=10(^{2})\{0,10,30\}\). We select the number of rows for compressed matrix \(\) by \(m\{100,300,500,700,1000,2000\}\). For computing the empirical regressor \(}^{m d}\), we first split the whole sample set \(^{}\) into two non-overlap subsets, i.e., a training set \(^{}\) with 80% and a testing set \(^{}\) with rest 20%. The regressor \(}\) is therefore obtained by solving compressed SHORE (2) based on the training set \(^{}\) with a randomly generated compressed matrix \(\). For evaluating the proposed prediction method, Algorithm 2, we pick a fixed stepsize \(=0.9\), \(=_{+}^{K}\), and set the maximum iteration number as \(T=60\), and run prediction methods over the set \(^{}\).

**Hardware & Software.** All experiments are conducted in Dell workstation Precision 7920 with a 3GHz 48Cores Intel Xeon CPU and 128GB 2934MHz DDR4 Memory. The proposed method and other methods are solved using PyTorch version 2.3.0 and scikit-learn version 1.4.2 in Python 3.12.3.

**Numerical Results & Discussions.** The results are demonstrated in Figure 1, which does not include the results from the Elastic Net and OMP due to relatively much longer running time.

Based on Figure 1, we observe that the proposed algorithm enjoys a better computational cost and accuracy on most metrics. The running time for the proposed algorithm and baselines are reported in Table 2 (see in Appendix A.7), which further demonstrates the efficiency of the proposed algorithm. The implemented code could be found on Github https://github.com/from-ryan/Solving_SHORE_via_compression.

## 5 Conclusion and Future Directions

In conclusion, we propose a two-stage framework to solve Sparse & High-dimensional-Output REgression (SHORE) problem, the computational and statistical results indicate that the proposed framework is computationally scalable, maintaining the same order of both the training loss and prediction loss before and after compression under relatively weak sample set conditions, especially in the sparse and high-dimensional-output setting where the input dimension is polynomially smaller compared to the output dimension. In numerical experiments, SHORE provides improved optimization performance over existing MOR methods, for both synthetic data and real data.

We close with some potential questions for future investigation. The first is to extend our theoretical results to nonlinear/nonconvex SHORE frameworks . The second direction is to improve existing variable reduction methods for better scalability while maintaining small sacrificing on prediction accuracy, e.g., new design and analysis on randomized projection matrices. The third direction is to explore general scenarios when high dimensional outputs enjoys additional geometric structures  from real applications in machine learning or operations management other than \(s\)-sparsity and its variants as discussed in the paper. Taking our result for SHORE as an initial start, we expect a stronger follow-up work that applies to MOR with additional structures, which eventually benefits the learning community in both practice and theory.

Figure 1: **Numerical results on synthetic data. In short, each dot in the figure represents the average value of 10 _independent trials_ (i.e., experiments) of compressed matrices \(^{(1)},,^{(10)}\) on a given tuple of parameters \((K,d,n,,m)\). The shaded parts represent the empirical standard deviations over 10 trials. In the first row, we plot the ratio of training loss after and before compression, i.e., \(\|-}\|_{F}^ {2}/\|-}\|_{F}^{2}\) versus the number of rows \(m\). It is obvious that the ratio converges to one as \(m\) increases, which validates the result presented in Theorem 1. In the second row, we plot percision@3 versus the number of rows. As we can observe, the proposed algorithm outperforms CD and FISTA.**