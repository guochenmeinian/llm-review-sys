# Interpretable and Explainable Logical Policies

via Neurally Guided Symbolic Abstraction

 Quentin Delfosse

Technical University of Darmstadt

National Research Center for Applied Cybersecurity

quentin.delfosse@tu-darmstadt.de

&Hikaru Shindo

Technical University of Darmstadt

hikaru.shindo@tu-darmstadt.de

&Devendra Singh Dhami

Eindhoven University of Technology

Hessian Center for AI (hessian.AI)

d.s.dhami@tue.nl

&Kristian Kersting

Technical University Darmstadt

Hessian Center for AI (hessian.AI)

German Research Center for AI (DFKI)

kersting@cs.tu-darmstadt.de

Equal contribution.DSD contributed while being with hessian.AI and TU Darmstadt before joining TUe.

###### Abstract

The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.

## 1 Introduction

Deep reinforcement learning (RL) agents use neural networks to take decisions from the unstructured input state space without manual engineering (Mnih et al., 2015). However, these black-box policies lack _interpretability_(Rudin, 2019), _i.e._ the capacity to articulate the thinking behind the action selection. They are also not robust to environmental changes (Pinto et al., 2017; Wulfmeier et al., 2017). Although performing object detection and policy optimization independently can get over these issues (Devin et al., 2018), doing so comes at the cost of the aforementioned issues when employing neural networks to encode the policy.

As logic constitutes a unified symbolic language that humans use to compose the reasoning behind their behavior, logic-based policies can tackle the interpretability problems for RL. Recently proposed Neural Logic RL (NLRL) agents (Jiang and Luo, 2019) construct logic-based policies using differentiable rule learners called \(\)_ILP_(Evans and Grefenstette, 2018), which can then be integrated with gradient-based optimization methods for RL. It represents the policy as a set of weighted rules, and performs policy gradients-based learning to solve RL tasks which require relational reasoning. It successfully produces interpretable rules, which describe each action in terms of its preconditions and outcome. However, the number of potential rules grows exponentially with the number of considered actions, entities, and their relations. NLRL is a memory-intensive approach, _i.e._ it generates a set of potential simple rules based on rule templates and can only be evaluated on simple abstractenvironments, created for the occasion. This approach can generate many newly invented predicates without their specification of meaning [Evans and Grefenstette, 2018], making the policy challenging to interpret for complex environments. Moreover, the function of _explainability_ is absent, _i.e._ the agent cannot explain the importance of each input on its decision. Explainable agents should adaptively produce different explanations given different input states. A question thus arises: _How can we build interpretable and explainable RL agents that are robust to environmental changes?_

To this end, we introduce Neurally gUided Differentiable IoGic policicEs (NUDGE), illustrated in Figure 1, that embody the advantages of logic: they are easily adaptable to environmental changes, composable, _interpretable_ and _explainable_ (because of our differentiable logic module). Given an input state, NUDGE extracts entities and their relations, converting raw states to a logic representations. This probabilistic relational states are used to deduce actions, using differentiable forward reasoning [Evans and Grefenstette, 2018, Shindo et al., 2023a]. NUDGE produces a policy that is both _interpretable_, _i.e._ provides a policy as a set of weighted interpretable rules that can be read out by humans, and _explainable_, _i.e._ explains which input is important using gradient-based attribution methods [Sundararajan et al., 2017] over logical representations.

To achieve an efficient learning with logic policies, we provide an algorithm to train NUDGE agents based on the PPO actor-critic framework. Moreover, we propose a novel rule-learning approach, called _Neurally-Guided Symbolic Abstraction_, where the candidate rules for the logic-based agents are obtained efficiently by being guided by neural-based agents. NUDGE distillates abstract representations of neural policies in the form of logic rules. Rules are assigned with their weights, and we perform gradient-based optimization using the PPO actor-critic framework.

Overall, we make the following contributions:

1. We propose NUDGE3: differentiable logical policies that learn interpretable rules and produce explanations for their decisions in complex environments. NUDGE uses neurally-guided symbolic abstraction to efficiently find a promising ruleset using pretrained neural-based agents guidance. 2. We empirically show that NUDGE agents: (i) can compete with neural-based agents, (ii) adapt to environmental changes, and (iii) are interpretable and explainable, _i.e._ produce interpretable policies as sets of weighted rules and provide explanations for their action selections.
3. We evaluate NUDGE on \(2\) classic Atari games and on \(3\) proposed object-centric logically challenging environments, where agents need relational reasoning in dynamic game-playing scenarios.

We start off by introducing the necessary background. Then we explain NUDGE's inner workings and present our experimental evaluation. Before concluding, we touch upon related work.

## 2 Background

We now describe the necessary background before formally introducing our NUDGE method.

Figure 1: **Overview of NUDGE. Given a state (depicted in the image), NUDGE computes the action distribution using relational state representation and differentiable forward reasoning. NUDGE provides _interpretable_ and _explainable_ policies, _i.e._ derives policies as sets of interepretable weighted rules, and can produce explanations using gradient-based attribution methods.**

**Deep Reinforcement Learning**. In RL, the task is modelled as a Markov decision process, \(\!=\!\!<\!\!,,P,R\!\!>\), where, at every timestep \(t\), an agent in a state \(s_{t}\), takes action \(a_{t}\), receives a reward \(r_{t}=R(s_{t},a_{t})\) and a transition to the next state \(s_{t+1}\), according to environment dynamics \(P(s_{t+1}|s_{t},a_{t})\). Deep agents attempt to learn a parametric policy, \(_{}(a_{t}|s_{t})\), in order to maximize the return (_i.e._\(_{t}^{k}r_{t}\), with \(\)). The desired input to output (_i.e._ state to action) distribution is not directly accessible, as RL agents only observe returns. The value \(V_{_{}}(s_{t})\) (resp. Q-value \(Q_{_{}}(s_{t},a_{t})\)) function provides the return of the state (resp. state/action pair) following the policy \(_{}\). Policy-based methods directly optimize \(_{}\) using the noisy return signal, leading to potentially unstable learning. Value-based methods learn to approximate the value functions \(_{}\) or \(_{}\), and implicitly encode the policy, _e.g._ by selecting the actions with the highest Q-value with a high probability [Mnih et al., 2015]. To reduce the variance of the estimated Q-value function, one can learn the advantage function \(_{}(s_{t},a_{t})=_{}(s_{t},a_{t})-_{}(s_{t})\). An estimate of the advantage function can be computed as \(_{}(s_{t},a_{t})=_{i=0}^{k-1}^{i}r_{t+i}+^{k}_{}(s_{t+k})-_{}(s_{t})\)[Mnih et al., 2016]. The Advantage Actor-critic (A2C) methods both encode the policy \(_{}\) (_i.e._ actor) and the advantage function \(_{}\) (_i.e._ critic), and use the critic to provide feedback to the actor, as in [Konda and Tsitsiklis, 1999]. To push \(_{}\) to take actions that lead to higher returns, gradient ascent can be applied to \(L^{PG}()=}[_{}(a s)_{}]\). Proximal Policy Optimization (PPO) algorithms ensure minor policy updates that avoid catastrophic drops [Schulman et al., 2017], and can be applied to actor-critic methods. To do so, the main objective constraints the policy ratio \(r()=(a|s)}{_{_{}}(a|s)}\), following \(L^{PR}()=}[(r()_{},(r(),1-,1+)_{})]\), where \(\) constrains the input within \([1-,1+]\). PPO actor-critic algorithm's global objective is \(L(,)=}[L^{PR}()-c_{1}L^{VF}()]\), with \(L^{VF}()\!=\!(_{}(s_{t})-V(s_{t}))^{2}\) being the value function loss. An entropy term can also be added to this objective to encourage exploration.

**First-Order Logic (FOL).** In FOL, a _Language_\(\) is a tuple \((,,,)\), where \(\) is a set of predicates, \(\) a set of constants, \(\) a set of function symbols (functors), and \(\) a set of variables. A _term_ is either a constant (_e.g._obj1, agent), a variable (_e.g._01), or a term which consists of a function symbol. An _atom_ is a formula \((_{1},,_{n})\), where \(\) is a predicate symbol (_e.g._closeby) and \(_{1},,_{n}\) are terms. A _ground atom_ or simply a _fact_ is an atom with no variables (_e.g._closeby(obj1,obj2)). A _literal_ is an atom (\(A\)) or its negation (\( A\)). A _clause_ is a finite disjunction (\(\)) of literals. A _ground clause_ is a clause with no variables. A _definite clause_ is a clause with exactly one positive literal. If \(A,B_{1},,B_{n}\) are atoms, then \(A B_{1} B_{n}\) is a definite clause. We write definite clauses in the form of \(A\) :- \(B_{1},,B_{n}\). Atom \(A\) is called the _head_, and set of negative atoms \(\{B_{1},,B_{n}\}\) is called the _body_. We call definite clauses as _rules_ for simplicity in this paper.

**Differentiable Forward Reasoning** is a data-driven approach of reasoning in FOL [Russell and Norvig, 2010]. In forward reasoning, given a set of facts and a set of rules, new facts are deduced by applying the rules to the facts. Differentiable forward reasoning [Evans and Grefenstette, 2018, Shindo et al., 2023a] is a differentiable implementation of the forward reasoning with tensor-based differentiable operations.

## 3 Neurally Guided Logic Policies

Figure 2 illustrates an overview of RL on NUDGE. They consist of a _policy reasoning_ module and a _policy learning_ module. NUDGE performs end-to-end differentiable policy reasoning based on forward reasoning, which computes action distributions given input states. On top of the reasoning module, policies are learned using neurally-guided symbolic abstraction and an actor-critic framework.

### Policy Reasoning: Selecting Actions using Differentiable Forward Reasoning.

To realize NUDGE, we introduce a language to describe actions and states in FOL. Using it, we introduce differentiable policy reasoning using forward chaining reasoning.

#### 3.1.1 Logic Programs for Actions

In RL, _states_ and _actions_ are key components since the agent performs the fundamental iteration of observing the state and taking an action to maximize its expected return. To achieve an efficient computation on first-order logic in RL settings, we introduce a simple language suitable for reasoning about states and actions.

We split the predicates set \(\) into two different sets, _i.e._, _action predicates_ (\(_{A}\)), which define the actions and _state predicates_ (\(_{S}\)) used for the observed states. If an atom \(A\) consists of an action predicate, \(A\) is called an _action atom_. If \(A\) consists of a state predicate, \(A\) is called _state atom_.

**Definition 1**: _Action-state Language is a tuple of \((_{A},_{S},,)\), where \(_{A}\) is a set of action predicates, \(_{S}\) is a set of state predicates, \(\) is a set of constants for entities, and \(\) is a set of variables._

For example, for _Getout_ illustrated in Figures 1 and 2, actual actions are: **left**, **right**, **jump**, and **idle**. We define action predicates \(_{A}=\{^{(1)},^{(2)},^{(1) },^{(1)},^{(2)},^{(1)},^{(1)},...\}\) and state predicates \(_{S}=\{,,...\}\). To encode different reasons for a given game action, we can generate several action predicates (_e.g._right\({}^{(1)}\) and \(^{(2)}\) for **right**). By using these predicates, we can compose action atoms, _e.g._right\({}^{(1)}\)(agent), and state atoms, _e.g._type\((,)\). An action predicate can also be a state predicate, _e.g._ in multiplayer settings. Now, we define rules to describe actions in the action-state language.

**Definition 2**: _Let \(X_{A}\) be an action atom and \(X_{S}^{(1)},,X_{S}^{(n)}\) be state atoms. An action rule is a rule, written as \(X_{A}\) :-\(X_{S}^{(1)},,X_{S}^{(n)}\)._

For example, for action **right**, we define an action rule as:

\[^{(1)}()(,),(,),( ),(,).\]

which can be interpreted as _"The agent should go right if the agent does not have the key and the key is located on the right of the agent."_. Having several action predicates for an actual action (in the game) allows our agents to define several action rules that describe different reasons for the action.

#### 3.1.2 Differentiable Logic Policies

We denote the set of actual actions by \(\), the set of action rules by \(\), the set of all of the facts by \(=_{A}_{S}\) where \(_{A}\) is a set of action atoms and \(_{S}\) is a set of state atoms. \(\) contains all of the facts produced by a given FOL language. We here consider ordered sets, _i.e._ each element has its index. We also denote the size of the sets as: \(A=||\), \(C=||\), \(G=||\), and \(G_{A}=|_{A}|\).

We propose _Differentiable Logic Policies_, which perform differentiable forward reasoning on action rules to produce a probability distribution over actions. The policy computation consists of \(3\) components: the relational perception module (1), the differentiable forward-reasoning module (2), and the action-extraction module (3).

Figure 2: **NUDGE-RL. Policy Reasoning (bottom):** NUDGE agents incorporate end-to-end _reasoning_ architectures from raw input based on differentiable forward reasoning. In the reasoning step, **(1)** the raw input state is converted into a logical representation, _i.e._ a set of atoms with probabilities. **(2)** Differentiable forward reasoning is performed using weighted action rules. **(3)** The final distribution over actions is computed using the results of differentiable reasoning. **Policy Learning (top):** Using the guidance of a pretrained neural policy, a set of candidate action rules is searched by _neurally-guided symbolic abstraction_, where promising action rules are produced. Then, randomly initialized weights are assigned to the action rules and are optimized using the critic of an actor-critic agent.

The policy \(_{(,)}\) parameterized by a set of action rules \(\) and the rules' weights \(\) is computed as follows:

\[_{(,)}(s_{t})=p(a_{t}|s_{t})=f^{act}(f^{reason}_{( ,)}(f^{perceive}_{}(s_{t}))),\] (1)

with \(f^{perceive}_{}^{N}^{G}\) a perception function that maps the raw input state \(s_{t}^{N}\) into a set of probabilistic atoms, \(f^{reason}_{(,)}^{G}^{G_{A}}\) a differentiable forward reasoning function parameterized by a set of rules \(\) and rule weights \(\), and \(f^{act}^{G_{A}}^{A}\) an action-selection function, which computes the probability distribution over the action space.

**Relational Perception.** NUDGE agents take an object-centric state representations as input, obtained by _e.g._ using object detection (Redmon et al., 2016) or discovery (Lin et al., 2020; Delfosse et al., 2022) methods. These models return the detected objects and their attributes (_e.g._ class and positions). They are then converted into a probabilistic logic form with their relations, _i.e._ a set of facts with their probabilities. An input state \(s_{t}^{N}\) is converted to a _valuation vector_\(^{G}\), which maps each fact to a probabilistic value. For example, let \(=\{(,),( ,),(,), ()\}\). A valuation vector \([0.8,0.6,0.3,0.0]^{}\) maps each fact to a corresponding probabilistic value. NUDGE performs differentiable forward reasoning by updating the initial valuation vector \(^{(0)}\)\(T\) times to obtain \(^{(T)}\).

Initial valuation vector \(^{(0)}\) is computed as follows. For each ground state atom \((_{1},,_{})_{S}\), _e.g._\((,)\), a differentiable function is called to compute its probability, which maps each term \(_{1},,_{}\) to vector representations according to the interpretation, _e.g._obj1 and obj2 are mapped to their positions, then perform binary classification using the distance between them. For action atoms, zero is assigned as its initial probability (_e.g._ for \(^{(1)}()\)).

**Differentiable Forward Reasoning.** Given a set of candidate action rules \(\), we create the reasoning function \(f^{reason}_{(,)}:^{G}^{G_{A}}\), which takes the initial valuation vector and induces action atoms using weighted action rules. We assign weights to the action rules of \(\) as follows: We fix the target programs' size, \(M\), _i.e._ select \(M\) rules out of \(C\) candidate action rules. To do so, we introduce \(C\)-dimensional weights \(=[_{1},,_{M}]\) where \(_{i}^{C}\) (_cf._ Figure 6 in the appendix). We take the _softmax_ of each weight vector \(_{i}\) to select \(M\) action rules in a differentiable manner.

We perform \(T\)-step forward reasoning using action rules \(\) with weights \(\). We compose the differentiable forward reasoning function following Shindo et al. (2023). It computes soft logical entailment based on efficient tensor operations. Our differentiable forward reasoning module computes new valuation \(^{(T)}\) including all induced atoms given weighted action rules \((,)\) and initial valuation \(^{(0)}\). Finally, we compute valuations on action atoms \(_{A}^{G_{A}}\) by extracting relevant values from \(^{(T)}\). We provide details in App. E.

**Compute Action Probability.** Given valuations on action atoms \(_{A}\), we compute the action distribution for actual actions. Let \(_{i}\) be an actual action, and \(v^{}_{1},,v^{}_{n}_{A}\) be valuations which are relevant for \(_{i}\) (_e.g._ valuations of \(^{(1)}()\) and \(^{(2)}()\) in \(_{A}\) for the action **right**). We assign scores to each action \(_{i}\) based on the _log-sum-exp_ approach of Cuturi and Blondel (2017): \(val(_{i})=_{1 i n}(v^{}_{i}/)\), that smoothly approximates the maximum value of \(\{v^{}_{1},,v^{}_{n}\}\). We use \(>0\) as a smoothing parameter. The action distribution is then obtained by taking the _softmax_ over the evaluations of all actions.

### Policy Learning

So far, we have considered that candidate rules for the policy are given, requiring human experts to handcraft potential rules. To avoid this, template-based rule generation (Evans and Grefenstette, 2018; Jiang and Luo, 2019) can be applied, but the number of generated rules increases exponentially with the number of entities and their potential relations. This technique is thus difficult to apply to complex environments where the agents need to reason about many different relations of entities.

To mitigate this problem, we propose an efficient learning algorithm for NUDGE that consists of _two_ steps: neurally-guided symbolic abstraction and gradient-based optimization. First, NUDGE obtains a symbolic abstract set of rules, aligned with a given neural policy. The set of candidate rules is selected by neurally-guided top-\(k\) search, _i.e._, we generate a set of promising rules using a neural policy as an oracle. Then we assign randomized weights to each selected rule and perform differentiable reasoning. We finally optimize the rule weights using a "logic actor - neural critic" algorithm that aims at maximizing the expected return. Let us elaborate on each step.

#### 3.2.1 Neurally Guided Symbolic Abstraction

Given a well-performing neural policy \(_{}\), promising action rules for an RL task entail the same actions as the ones selected by \(_{}\). We generate such rules by performing top-\(k\) search-based abstraction, which uses the neural policy to evaluate rules efficiently. The inputs are initial rules \(_{0}\), neural policy \(_{}\). We start with elementary action rules and refine them to generate better action rules. \(_{to\_open}\) is a set of rules to be refined, and initialized as \(_{0}\). For each rule \(C_{i}_{to\_open}\), we generate new rules by refining them as follows. Let \(C_{i}=X_{A} X_{S}^{(1)},,X_{S}^{(n)}\) be an already selected general action rule. Using a randomly picked ground or non-ground state atom \(Y_{S}\) (\( X_{S}^{(i)}\)\( i[1,...,n]\)), we refine the selected rule by adding a new state atom to its body, obtaining: \(X_{A} X_{S}^{(1)}, X_{S}^{(n)},Y_{S}\).

We evaluate each newly generated rule to select promising rules. We use the neural policy \(_{}\) as a guide for the rule evaluation, _i.e._ rules that entail the same action as the neural policy \(_{}\) are promising action rules. Let \(\) be a set of states. Then we evaluate the rule \(R\), following:

\[(R,_{},)=)}_{s }_{}(s)^{}_{(,)}(s),\] (2)

where \(N(R,)\) is a normalization term, \(_{,}\) is the differentiable logic policy with rules \(=\{R\}\) and rule weights \(\), which is an \(1 1\) identity matrix (for consistent notation), and \(\) is the dot product. Intuitively, \(_{(,)}\) is the logic policy that has \(R\) as its only action rule. If \(_{(,)}\) produces a similar action distribution as the one produced by \(_{}\), we regard the rule \(R\) as a promising rule. We compute similarity scores between the neural policy \(_{}\) and the logic one \(_{(,)}\) using the dot product between the two action distributions, and average them across the states of \(\). The normalization term avoids high scores for simple rules.

To compute the normalization term, we ground \(R\), _i.e._ we replace variables with ground terms. We consider all of the possible groundings. Let \(\) be the set of all of the possible variables substitutions to ground \(R\). For each \(\), we get a ground rule \(R=X_{A}-X_{S}^{(1)},,X_{S}^{(n)}\), where \(X\) represents the result of applying substitution \(\) to atom \(X\). Let \(=\{j_{1},,j_{n}\}\) be indices of the ground atoms \(X_{S}^{(1)},,X_{S}^{(n)}\) in ordered set of ground atoms \(\). Then, the normalization term is computed as:

\[N(R,)=_{}_{s}_{j }=_{s}^{(0)}[j],\] (3)

where \(_{s}^{(0)}\) is an initial valuation vector for state \(s\), _i.e._\(f_{}^{}(s)\). Eq. 3 quantifies how often the body atoms of the ground rule \(R\) are activated on the given set of states \(\). Simple rules with fewer atoms in their body tend to have large values, and thus their evaluation scores in Eq. 2 tend to be small. After scoring all of the new rules, NUDGE select top-\(k\) rules to refine them in the next step. To this end, all of the top-\(k\) rules in each step will be returned as the candidate ruleset \(\) for the policy (_cf._ App. A for more details about our algorithm).

We perform the action-rule generation for each action. In practice, NUDGE maintains the cached history \(\) of states and actions produced by the neural policy. For a given action, the search quality increases together with the amount of times the action was selected, _i.e._ if \(\) does not contain any record of the action, then all action rules (with this action as head) would get the same scores, leading to a random search.

NUDGE has thus produced candidate action rules \(\), that will be associated with \(\) to form untrained differentiable logic policy \(_{(,)}\), as the ones described in Section 3.1.2.

#### 3.2.2 Learning the Rules' Weights using the Actor-critic Algorithm

In the following, we consider a (potentially pretrained) actor-critic neural agent, with \(v_{}\) its differentiable state-value function parameterized by \(\) (critic). Given a set of action rules \(\), let \(_{(,)}\) be a differentiable logic policy. NUDGE learns the weights of the action rules in the following steps. For each non-terminal state \(s_{t}\) of each episode, we store the actions sampled from the policy (\(a_{t}_{(,)}(s_{t})\)) and the next states \(s_{t+1}\). We update the value function and the policy as follows:

\[ =r+ v_{}(s_{t+1})-v_{}(s_{t})\] (4) \[ =+_{}v_{}(s_{t})\] (5) \[ =+_{}_{(,)}(s_{t}).\] (6)The logic policy \(_{(,)}\) thus learn to maximize the expected return, potentially bootstrapped by the use of a pretrained neural critic. Moreover, to ease interpretability, NUDGE can prune the unused action rules (_i.e._ with low weights) by performing top-\(k\) selection on the optimized rule weights after learning.

## 4 Experimental Evaluation

We here compare neural agents' performances to NUDGE ones, showcase NUDGE _interpretable_ policies and its ability to report the importance of each input on their decisions, _i.e.__explainable_ logic policies. We use DQN agents (on Atari environments) and PPO actor-critic (on logic-ones) as neural baselines, for comparison and PPO as pretrained agents to guide the symbolic abstraction. All agent types receive object-centric descriptions of the environments. For clarity, we annotate action predicates on action rules with specific names on purpose, _e.g._right\({}_{}\) instead of right\({}^{(1)}\) (when the rule describes an action **right** motivated to get the key).

We intend to compare agents with object-centric information bottlenecks. We thus had to extract object-centric states of the Atari environments. To do so, we make use of the Object-Centric Atari library (Delfosse et al., 2023). As Atari games do not embed logic challenges, but are rather designed to test the reflexes of human players, we also created \(3\) logic-oriented environments. We thus have modified environments from the Procgen (Mohanty et al., 2020) environments that are open-sourced along with our evaluation to have object-centric representations. Our environments are easily hackable. We provide variations of these environments also to evaluate the ease of adaptation of every agent type. In **GetOut**, the goal is to obtain a key, and then go to a door, while avoiding a moving enemy. **GetOut+** is a more complex variation with a larger world containing \(5\) enemies (among which \(2\) are static). In **3Fishes**, the agent controls a fish and is confronted with \(2\) other fishes, one smaller (that the agent needs to "eat", _i.e._ go to) and one bigger, that the agent needs to dodge. A variation is **3Fishes-C**, where the agent can eat green fishes and dodge red ones. Finally, in **Loot**, the agent can open \(1\) or \(2\) chests and their corresponding (_i.e._ same color) keys. In **Loot-C**, the chests have different colors. Further details and hyperparameters are provided in App. D.

We aim to answer the following research questions: **Q1.** How does NUDGE compare with neural and logic baselines? **Q2.** Can NUDGE agents easily adapt to environmental changes? **Q3.** Are NUDGE agents interpretable and explainable?

**NUDGE competes with existing methods (Q1)**. We compare NUDGE with different baselines regarding their scores (or returns). First, we present scores obtained by trained DQN, Random and NUDGE agents (with expert supervision) on \(2\) Atari games (_cf._ Table 1). Our result show that NUDGE obtain better (Asterix) or similar (Freeway) scores than DQN. However, as said, Atari games are not logically challenging. We thus evaluate NUDGE on \(3\) logic environments. Figure 3 shows the returns in GetOut, 3Fishes, and Loot, with descriptions for each baseline in the caption. NUDGE obtains better performances than neural baselines (Neural PPO) on 3Fishes, is more stable on GetOut, _i.e._ less variance, and achieves faster convergence on Loot. This shows that NUDGE successfully distillates logic-based policies competing with neural baselines in different complex environments.

Figure 3: **NUDGE outperforms neural and logic baselines on the the \(3\) logic environments. Returns (avg.\(\)std.) obtained by NUDGE, neural PPO and logic-based agents without abstraction through training. NUDGE (Top-\(k\) rf.), with \(k\{1,3,10\}\) uses neurally-guided symbolic abstraction repeatedly until they get \(k\) rules for each action predicate. NUDGE (with E.S.) uses rule set \(\) supervised by an expert. Neural Logic RL composes logic-based policies by generating all possible rules without neurally-guided symbolic abstraction (Jiang and Luo, 2019). Random and human baselines are also provided.**

We also evaluate agents with a baseline without symbolic abstraction, where candidate rules are generated not being guided by neural policies, _i.e._ accept all of the generated rules in the rule refinement steps. This setting corresponds to the template-based approach (Jiang and Luo, 2019), but we train the agents by the actor-critic method, while vanilla policy gradient (Williams, 1992) is employed in (Jiang and Luo, 2019). For the no-abstraction baseline and NUDGE, we provide initial action rules with basic type information, _e.g._\(^{(1)}()\):- type(01, \()\), \((,)\), for each action rule. For this baseline, we generate \(5\) rules for GetOut, \(30\) rules for 3Fishes, and \(40\) rules for Loot in total to define all of the actual actions. NUDGE agents with small \(k\) tend to have less rules, _e.g._\(5\) rules in Getout, \(6\) rules in 3Fishes, and \(8\) rules in Loot for the top-1 refinement version. In Figure 3, the no-abstraction baselines perform worse than neural PPO and NUDGE in each environment, even though they have much more rules in 3Fishes and Loot. NUDGE thus composes efficient logic-based policies using neurally-guided symbolic abstraction. In App. B.1, we visualize the transition of the distribution of the rule weights in the GetOut environment.

To demonstrate the necessity of differentiability within symbolic reasoning in continuous RL environments, we simulated classic (_i.e._ discrete logic) agents, that reuse NUDGE agents' set of action rules, but with all weights fixed to \(1.0\). Such agents can still play better than ones with pure symbolic reasoners, as they still use fuzzy (_i.e._, continuous) concepts, such as closeby. In all our environments, NUDGE clearly outperforms the classic counterparts (_cf._ Table 1 right). This experiment shows the superiority of differentiable policy reasoning (embedding weighted action rules) over policy without weighted rules (classic).

To further reduce the gap between our environments and those from environments Cao et al. (2022), we have adapted our loot environment (Loot-hard). In this environment, the agent first has to pick up keys and open their corresponding saves, before being able to exit the level (by going to a final exit tile). Our results in Table 2 (bottom), as well as curves on Figure 10 in the appendix show the dominance of NUDGE agents.

**NUDGE agents adapt to environment changes (Q2).** We used the agents trained on the basic environment for this experimental evaluation, with no retraining or finetuning. For 3Fishes-C, we simply exchange the atom is_bigger with the atom same_color. This easy modification is not applicable on the black-box networks of neural PPO agents. For GetOut+ and Loot-C, we do not apply any modification to the agents. Our results are summarized in Table 2. Note that the agents obtains better performances in the 3Fishes-C variation, dodging a (small) red fish is easier than a big one. For GetOut+, NUDGE performances have decreased as avoiding \(5\) enemies drastically increases the difficulty of the game. On Loot-C, the performances are similar to the ones obtained in the original game. Our experiments show that NUDGE logic agents can easily adapt to environmental changes.

**NUDGE agents are interpretable _and_ explainable (Q3).** We show that NUDGE agents are interpretable and explainable by showing that (1) NUDGE produces interpretable policy as a set of weighted rules, and (2) NUDGE can show the importance of each atom, explaining its action choices.

The efficient neurally-guided learning on NUDGE enables the system to learn rules without inventing predicates with no specific interpretations, which are unavoidable in template-based approach (Evans and Grefenstette, 2018; Jiang and Luo, 2019). Thus, the policy can easily be read out by extracting action rules with high weights. Figure 5 shows some action rules discovered by NUDGE in GetOut. The first rule says: _"The agent should jump when the enemy is close to the agent (to avoid the enemy)."_. The produced NUDGE is an _interpretable_ policy

 Score (\(\)) & Random & DQN & NUDGE \\  Asterix & 235 \( 134\) & 124.5 & **6259 \( 1150\)** \\ Freeway & 0.0 \( 0\) & **25.8** & 21.4 \( 0.8\) \\  
 Score (\(\)) & Random & Classic & NUDGE \\  GetOut & -22.5\( 0.41\) & 11.59\( 4.29\) & 17.86\( 2.86\) \\
3Fish & -0.73\( 0.05\) & -0.24\( 0.29\) & 0.05\( 0.27\) \\ Loot & 0.57\( 0.39\) & 0.51\( 0.74\) & 5.66\( 0.59\) \\ 

Table 1: **Left: NUDGE agents can learn successful policies. Trained NUDGE agents (with expert supervision) scores (avg. \(\) std.) on \(2\) OCAtari games (Delfosse et al., 2023). Random and DQN (from van Hasselt et al. (2016)) are also provided. Right: NUDGE outperforms non-differentiable symbolic reasoning baselines. Returns obtained by NUDGE, classic and random agents our the \(3\) logic environments.**

 Score (\(\)) & 3Fishes-C & GetOut+ & Loot-C \\  Random & -0.6 \( 0.2\) & -22.5 \( 0.4\) & 0.6 \( 0.3\) \\ Neural PPO & -0.4 \( 0.1\) & -20.9 \( 0.6\) & 0.8 \( 0.5\) \\ NUDGE & **3.6**\( 0.2\) & **3.6**\( 3.0\) & **5.6**\( 0.3\) \\  

Table 2: **NUDGE agents adapt to environmental changes and solve logically-challenging environments. Returns obtained by NUDGE, neural PPO and random agents on our modified environments and on Loot-hard.**with its set of weighted rules using interpretable predicates. For each state, we can also look at the valuation of each atom and the selected rule.

Moreover, contrary to classic logic policies, the differentiable ones produced by NUDGE allow us to compute the _attribution values_ over the logical representations via backpropagation of the policies' gradients. We can compute the action gradients w.r.t. input atoms, _i.e._\(_{A}/^{(0)}\), as shown in Figure 4, which represent the relevance scores of the probabilistic input atoms \(^{(0)}\) for the actions given a specific state. The explanation is computed on the state shown in Figure 1, where the agent takes **right** as its action. Important atoms receive large gradients, _e.g._\(\)has_key\((\)agent\()\) and on_right\((\)obj2,obj\()\). By extracting relevant atoms with large gradients, NUDGE can produce clear explanations for the action selection. For example, by extracting the atoms wrapped in orange in Figure 4, NUDGE can explain the motivation: _"The agent decides to go right because it does not have the key and the key is located on the right-side of it."_. These results show that NUDGE policies are _interpretable_ and _explainable_, _i.e._ each action predicate is defined by rules that humans can fully understand, and gradient-based explanations for the action selections can be efficiently produced.

We also computed explanations of a neural agent on the same state, also using the input-gradients method. The concepts which received the highest explanation scores are: the key x-axis position (1.22), the key x-axis velocity (1.21), the enemy y-axis velocity (1.10), and the door x-axis position (0.77). For a consistent explanation, the agent x-axis position should also be among the highest scores. For this specific state, the neural agent seems to be placing its explanations on non relevant concepts (such as the key's velocity). Stammer et al. (2021) have shown that neural networks trained on logical concepts tend to produce wrong explanations often, and additional regularization/supervision about explanations during training is necessary to force neural networks to produce correct explanations. This approach requires additional efforts to label each state with its correct explanation. NUDGE policies produces better explanations than neural-based agents. More explanations produced by NUDGE are provided in Figure 11 in the appendix.

## 5 Related Work

Relational RL (Dzeroski et al., 2001; Kersting et al., 2004; Kersting and Driessens, 2008; Lang et al., 2012) has been developed to tackle RL tasks in relational domains. Relational RL frameworks incorporate logical representations and use probabilistic reasoning. With NUDGE, we make use of differentiable logic programming. The Neural Logic Reinforcement Learning (NLRL) (Jiang and Luo, 2019) framework is the first to integrate Differentiable Inductive Logic Programming (\(\)ILP) (Evans and Grefenstette, 2018) to RL domain. \(\)ILP learns generalized logic rules from examples by gradient-based optimization. NLRL adopts \(\)ILP as a policy function. We extend this approach by proposing neurally-guided symbolic abstraction embracing extensive work of \(\)ILP (Shindo et al., 2021b, a, 2023b) for learning complex programs including functors in visual scenes, allowing agents to learn interpretable action rules efficiently for complex environments.

Figure 4: **Explanation using inputs’ gradients. The action gradient w.r.t. input atoms, _i.e._\(_{A}/^{(0)}\), on the state shown in Figure 1. **right** was selected, due to the highlighted relevant atoms (with large gradients).

Figure 5: **NUDGE produces interpretable policies as sets of weighted rules. A subset of the weighted action rules from the Getout environment. Full policies for every logic environment are provided in App. B.3.**

  & FOL & N.G. & Int. & Exp. \\  NLRL & ✓ & ✗ & ✓ & ✗ \\ NeSyRL & ✓ & ✗ & ✓ & ✗ \\ DiffSES & ✗ & ✓ & ✓ & ✗ \\ NUDGE & ✓ & ✓ & ✓ & ✓ \\ 

Table 3: **Logic-based RL methods comparison**: First Order Logic (FOL), neurally-guided search (N.G.), interpretability (Int.), and explainability (Exp.).

GALOIS (Cao et al., 2022) is a framework to represent policies as logic programs using the _sketch_ setting (Solar-Lezama, 2008), where programs are learned to fill blanks. NUDGE performs structure learning from scratch using policy gradients. KoGun (Zhang et al., 2020) integrates human knowledge as a prior for RL agents. NUDGE learns a policy as a set of weighted rules and thus also can integrate human knowledge. Neurom Symbolic RL (NeSyRL) (Kimura et al., 2021) uses Logical Neural Networks (LNNs) (Riegel et al., 2020) for the policy computation. LNNs parameterize the soft logical operators while NUDGE parameterizes rules with their weights. Deep Relational RL approaches (Zambaldi et al., 2018) achieve relational reasoning as a neural network, but NUDGE explicitly encodes relations in logic. Many languages for planning and RL tasks have been developed (Fixes and Nilsson, 1971; Fox and Long, 2003). Our approach is inspired by _situation calculus_(Reiter, 2001), which is an established framework to describe states and actions in logic.

Symbolic programs within RL have been investigated, _e.g._ program guided agent (Sun et al., 2020), program synthesis (Zhu et al., 2019), PIRL (Verma et al., 2018), SDRL (Lyu et al., 2019), interpretable model-based hierarchical RL Xu and Fekri (2021), deep symbolic policy Landajuela et al. (2021), and DiffSES Zheng et al. (2021). These approaches use domain specific languages or propositional logic, and address either of interpretability or explainability of RL. To this end, in Table 3, we compare NUDGE with the most relevant approaches that share at least \(2\) of the following aspects: supporting first-order logic, neural guidance, interpretability and explainability. NUDGE is the first method to use neural guidance for differentiable first-order logic policies and to address both _interpretability_ and _explainability_. Specifically, PIRL develops functional programs with neural guidance using _sketches_, which define a grammar of programs to be generated. In contrast, NUDGE performs structure learning from scratch using a neural guidance with a language bias of _mode declarations_(Muggleton, 1995; Cropper et al., 2022) restricting the search space. As NUDGE uses first-order logic, it can incorporate background knowledge in a declarative form, _e.g._ with a few lines of relational atoms and rules. To demonstrate this, NUDGE has been evaluated on challenging environments where the main interest is relational reasoning.

## 6 Conclusion

We proposed NUDGE, an interpretable and explainable policy reasoning and learning framework for reinforcement learning. NUDGE uses differentiable forward reasoning to obtain a set of interpretable weighted rules as policy. It performs neurally-guided symbolic abstraction, which efficiently distillates symbolic representations from a neural policy, and incorporate an actor-critic method to perform gradient-based policy optimization. We empirically demonstrated that NUDGE agents (1) can compete with neural based policies, (2) use logical representations to produce both interpretable and explainable policies and (3) can automatically adapt or easily be modified and are thus robust to environmental changes.

**Societal and environmental impact.** As NUDGE agents can explain the importance of each the input on their decisions, and as their rules are interpretable, it can help us understanding the decisions of RL agents trained in sensitive complicated domains, and potentially help discover biases and misalignments of discriminative nature. While the distillation of the learned policy into a logic ones augments the computational resources needed for completing the agents learning process, we believe that the logic the agents' abstraction capabilities will overall reduce the need of large neural networks, and will remove the necessity of retraining agents for each environmental change, leading to an overall reduction of necessary computational power.

**Limitation and Future Work.** NUDGE is only complete if provided with a sufficiently expressive language (in terms of predicates and entities) to approximate neural policies. Interesting lines of future research can include: (i) an automatic discovery of predicates, using _e.g. predicate invention_(Muggleton et al., 2015), (ii) augmenting the number of accessible entities to reason on. Explainable interactive learning (Teso and Kersting, 2019) in RL can integrate NUDGE agents, since it produces explanations on logical representations. Causal RL (Madumal et al., 2020) and meta learning (Mishra et al., 2018) also constitute interesting avenues for NUDGE's development. Finally, we could incorporate objects extraction methods (Delfosse et al., 2022) within our NUDGE agents, to obtain logic agents that extract object and their properties from images.

**Acknowledgements.** The authors thank the anonymous reviewers for their valuable feedback. This research work has been funded by the German Federal Ministry of Education and Research, the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK) within their joint support of the National Research Center for Applied Cybersecurity ATHENE, via the "SenPai: XReLeaS" project. We gratefully acknowledge support by the Federal Ministry of Education and Research (BMBF) AI lighthouse project "SPAICER" (01MK20015E), the EU ICT-48 Network of AI Research Excellence Center "TAILOR" (EU Horizon 2020, GA No 952215), and the Collaboration Lab "AI in Construction" (AICO).