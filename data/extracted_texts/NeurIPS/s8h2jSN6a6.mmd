# Appendices

MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs

 Ziyu Liu\({}^{1,2}\), Tao Chu\({}^{2}\), Yuhang Zang\({}^{12}\), Xilin Wei\({}^{2}\), Xiaoyi Dong\({}^{2}\), Pan Zhang\({}^{2}\),

Zijian Liang\({}^{1}\), Yuanjun Xiong\({}^{5}\), Yu Qiao\({}^{2}\), Dahua Lin\({}^{2,3,4}\), Jiaqi Wang\({}^{12}\)

\({}^{1}\) SJTU, \({}^{2}\) Shanghai AI Laboratory, \({}^{3}\)CUHK, \({}^{4}\) CPII under InnoHK, \({}^{5}\) MTreads, Inc. liuziyu77@sjtu.edu.cn, {zangyuhang, wangjiaqi}@pjlab.org.cn

Github: https://github.com/Liuziyu77/MMDU

###### Abstract

Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce **MMDU**, a comprehensive benchmark, and **MMDU-45k**, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5\(\) longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data. We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly addresses this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. This project is available at https://github.com/Liuziyu77/MMDU.

## 1 Introduction

Human-AI interaction is a fundamental task to ensure that AI can be deployed in the real world for everyone, enabling inclusive and effective communication between humans and AI in various aspects of daily life. Current Large Vision-Language Models (LVLMs)  have made significant strides in understanding and generating text conditioned on visual inputs, showing promising directions in AI assistant applications.

Current open-source LVLMs primarily focus on single-turn, single-image inputs, which are far from the complexities of real-world scenarios. In contrast, effective human-AI interaction in daily life demands a range of essential skills, including the ability to engage in multi-turn conversations that involve multiple image inputs and comprehend long-context histories to facilitate coherent and contextually appropriate conversations. While existing benchmarks partially assess these abilities [2; 3; 4] (Fig. 1**(a)**), they have limitations such as limited number of total tokens and do not provide a complete picture of a model's human-AI interaction capabilities. More challenging and comprehensive benchmarks are necessary to evaluate and advance these skills.

We present **MMDU**, a comprehensive benchmark for multi-turn multi-image dialog understanding. Our data collection pipeline automatically selects relevant images and text descriptions from open-source Wikipedia , forming the basis for multi-turn dialogues. We employ a clustering algorithm to identify relevant Wikipedia entities and design prompt templates for GPT-4o to generate multi-turn questions. Human annotators assess and refine GPT-4o's responses, producing ground-truth answers for our benchmark.

Our MMDU benchmark possesses the following distinctive features: (1) **Multi-turn and Multi-image**: Our benchmark showcases a conversational setting with a maximum of 20 images and 17 turns, thereby surpassing the scope of preceding works (see Fig. 1**(b)** and authenticated) replicating real-world chat assistant interactions. (2) **Long Context**: With a maximum of 18k text+image tokens, our benchmark evaluates the capacity of LVLMs to process and comprehend extended contextual information with a long context history. (3) **Open-ended Evaluation**: Departing from traditional benchmarks that rely on close-ended questions with concise outputs (_e.g._, multiple-choice questions or short answers), our benchmark adopts a more realistic and nuanced approach, assessing LVLM's performance through free-form multi-turn outputs that prioritize scalability and explainability, inspired by NLP research that leverages strong LLMs as judges .

We evaluate 15 proprietary and open-source LVLMs on our MMDU benchmark. Our evaluation reveals a significant performance disparity between proprietary and open-source LVLMs. The best open-source model scores 42.8%, far behind the proprietary GPT-4o at 70.2%. Notably, our observations provide a clear direction for improving the open-source models on long-context, multi-turn, and multi-image scenarios to bridge the performance gap. Based on our findings from the benchmark results on MMDU, the practical need urges the visual instruction tuning data containing multi-turn and multi-images for open-source LVLMs.

Figure 1: **Comparing MMDU with previous LVLM benchmarks. Our MMDU (bottom) incorporates multi-turn and multi-image inputs, long context, and open-ended question-answering evaluation, making it more challenging and comprehensive than previous benchmarks (upper left).**

To get one step closer to proprietary LVLM models, we further present **MMDU-45k**. We collect **45k** high-quality instruction tuning data using the same process employed in building MMDU, with a random sampling of human verification instead of the exhaustive human evaluation used in MMDU. Adding our instruction tuning data MMDU-45k into the LVLM supervised fine-tuning (SFT) stage improves performance on various benchmarks, such as boosting InternLM-XC2 's performance by \(14.5\%\)/\(1.1\%\)/\(1.5\%\)/\(1.2\%\) on MMDU/MMStar /MathVista /ChartQA , respectively.

Our main contribution is summarized: **(1)** We introduce MMDU that assesses the multi-turn, multi-image dialog understanding capabilities of LVLMs, specifically designed for human-AI interaction. **(2)** We conduct a comprehensive evaluation of existing LVLMs on MMDU, revealing significant challenges in this task and providing valuable insights for future LVLM development. **(3)** We present MMDU-45k, a large-scale instruction tuning dataset designed to enhance dialog understanding abilities. We demonstrate that fine-tuning LVLMs on MMDU-45k leads to improved performance on both MMDU and existing benchmarks.

## 2 MMDU Benchmark

### Benchmark Overview

Although many LVLMs now claim to handle tens of thousands, hundreds of thousands, or even millions of tokens in length, their actual performance significantly declines in real-world applications as the number of images or the length of the context increases. Both the dialogue quality and image recognition capabilities of LVLMs deteriorate notably under these conditions.

To evaluate the multi-image multi-turn dialogue capabilities of existing models, we have developed the MMDU Benchmark. Our benchmark comprises 110 high-quality multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. Previous benchmarks typically involved only single images or a small number of images, with fewer rounds of questions and short-form answers. However, MMDU significantly increases the number of images, the number of question-and-answer rounds, and the in-context length of the Q&A. The questions in MMDU involve 2 to 20 images, with an average image&text token length of 8.2k tokens, and a maximum image&text length reaching 18K tokens, presenting significant challenges to existing multimodal large models. For more data statistics about MMDU, please refer to Tab.1 and Fig.4.

MMDU aims to test models' abilities to simultaneously understand multiple images and follow instructions in long dialogues. We design precise prompts to evaluate the models' responses, and our evaluation criteria details are discussed in Sec. 2.3.

### Benchmark Construction

**Data Collection.** Our goal in constructing this benchmark is to measure the current models' ability to understand multiple images and generate long texts in general scenarios.

The first step is selecting appropriate multiple images and related textual information as the foundation for multi-turn dialogues. Given that the generated dialogue content needs to be logically coherent and rich in content, we cannot use random sets of images to build the Q&A pairs. Random images would lead to low-quality and illogical dialogues, both in the question-construction and answer-generation processes.

To address this issue, we employ a clustering method to construct high-quality image sets. We extensively screened entries on the open-source Wikipedia , encoded the relevant tags of entries using a sentence transformer, and clustered the entries using the obtained embeddings. After clustering enough entries of the same category together, we further matched them using image captions to obtain highly relevant entries and image sets. Then, within each cluster, we selected multiple images and their associated textual information to create combinations of image-text pairs, ranging from 2 to 20 images. The process of collecting and clustering entries is illustrated in Fig. 2**(a)**.

**Construction with GPT-4o.** After obtaining the combinations of multiple images, we use carefully crafted prompts to guide the GPT-4o model in generating corresponding questions and answers based on the available images and text information. Initially, we constructed multi-turn Q&A pairs for each single image and its associated text. Then, we input the combinations of multiple images into GPT-4o to generate multi-turn Q&A pairs based on multiple images, ensuring through prompts that the questions covered multiple different images simultaneously.

Building on this, we combined the multi-turn Q&A pairs for multiple images with those for each individual image, creating dialogues that include both single-image and multi-image questions. To ensure the quality of the benchmark, we invited experts to meticulously review the generated dialogues, selecting 110 high-quality multi-turn, multi-image dialogues for our benchmark. Additionally, we carefully edited these 110 samples to eliminate hallucinations and errors in GPT-4o's responses, ensuring the accuracy and richness of the benchmark content. Our pipeline is shown in Fig. 2 **(b)**.

Furthermore, our generated multi-turn, multi-image data is highly scalable. During the Q&A construction process, we required GPT-4o to organize the generated text according to our specified Text-Image Interleaving Format, using tags like <image-1>, <image-2>, _etc._, to refer to different images. Our design is flexible to treat the generated multi-turn, multi-image dialogues as fundamental components. By modifying the values in <image-\(i\)>, we can concatenate multiple dialogues, thereby constructing dialogues involving dozens or even hundreds of images. Our data is not limited to a few images per Q&A generation but is capable of supporting dialogues of theoretically unlimited length.

Quality Control with Human AnnotatorsIn the process of constructing the dataset, we implemented two stringent measures to ensure its quality: **(1)** We combined automated and manual screening methods to select images and texts that meet our standards. Specifically, we performed an initial screening using clustering techniques on a large-scale image and text database, automatically removing low-quality, blurry, or irrelevant images and texts. This ensured that the image combinations and their corresponding texts were of high quality and relevance. **(2)** To avoid hallucinations and errors in the model-generated dialogues, we enforced strict quality control on the texts generated by GPT-4o. We introduced a multi-round manual review mechanism. Each set of Q&A underwent at least two rounds of manual review: the first round involved preliminary checks by regular reviewers, and the second round involved in-depth examination and modification by experts.

Figure 2: **An overview of (a) data preparation and (b) generation pipeline for MMDU and MMDU-45k. We first collect the relevant image and text descriptions from Wikipedia using the clustering algorithm. Then we prompt GPT-4o to design multi-turn questions. The human annotators revise the GPT-4o response as the ground-truth answers.**

For the two rounds of manual review, our experts reviewed and corrected (by removing or rewriting) any hallucinations and errors to ensure that all dialogues are accurate. To facilitate verification, we designed a specialized web UI that allows for quick browsing and modification of data content. Please refer to the Appendix B.5 to check the interface of our web UI used for the human check process.

During the data annotation process, all our annotators were either junior PhD-level students or senior researchers, with a total of 20 participants. Senior researchers or PhD students with relevant professional backgrounds were selected as experts. Before data annotation, all annotators underwent training, and a small subset of data was pre-annotated. Once the pre-annotation results met the required standards, the subsequent annotation process was carried out.

Since our images are sourced from Wikipedia entries, each image is accompanied by a corresponding caption and all related information from the Wiki entry where the image is found. During the manual annotation process, annotators can easily understand the specific content and background information of each image by reading the Wiki entry. Therefore, there is no risk of introducing extra annotation errors due to misunderstanding of the image content. The various strategies mentioned above ensured that the final dataset was not only accurate but also of high academic and practical value.

### Evaluation

Evaluating the subjective, open-ended, free-form, and long-context visual question-answering pairs is indeed challenging. Traditional metrics (e.g., BLEU-4, CIDEr) often suffer from shortcomings like neglecting semantic understanding and failing to capture long-distance dependencies, they are not popular choices in recent days.

Inspired by NLP research that leverages strong LLMs as judges , we have developed an evaluation pipeline using GPT-4o to evaluate model performance. Specifically, following the generation of model predictions on our benchmark dataset, GPT-4o evaluates these predictions across various dimensions for each turn and sample, comparing them against reference answers. The aggregated results across multiple turns are averaged to derive sample scores, and the average scores across all samples constitute our benchmark scores. This method excels at understanding context and semantics,

Figure 3: **The evaluation pipeline of MMDU. We use the GPT-4o as a judge to give the overall score based on the referenced answer. In each evaluation, GPT-4o will refer to both the model’s answer and the reference answer. It will provide corresponding scores (in green) for each evaluation criterion (in blue), and finally, summarize the results (in light orange).**

providing more accurate evaluations of visual content, and capturing long-distance dependencies, which traditional metrics often miss.

To ensure a comprehensive and nuanced evaluation, we have identified six dimensions: Creativity, Richness, Visual Perception, Logical Coherence, Answer Accuracy, and Image Relationship Understanding. To guide GPT-4o in providing balanced and equitable assessments, we have meticulously crafted evaluation prompts for each dimension. Each dimension's score range of 10 is divided into five intervals (0-2, 2-4...8-10), with corresponding judgment criteria established for each interval. GPT-4o follows these criteria to conduct judgment processes and deliver final scores for each dimension. As illustrated in Fig 3, guided by our prompts, GPT-4o assesses the assistant's responses against reference answers, offering both a reasonable score and a transparent judgment process. Please refer to the Appendix B for our judgment prompts.

## 3 MMDU-45k for Instruct Tuning

### Dataset Construction

We follow the same process as constructing the benchmark to build our MMDU-45k. First, we collect a vast number of Wikipedia entries and extracted tags from these entries, including wiki tree labels and image captions. We use sentence transformers to encode the textual information and then apply the clustering algorithm to obtain the embeddings. During the clustering process, we calculate the cosine similarity between different embeddings and group highly related entries into clusters by setting a threshold \(=0.75\). From the clusters with high relevance, we select multiple images and their corresponding entry information and perform information extraction and filtering using InternLM-chat-20B. We design precise prompts to guide GPT-4o in generating multi-image, multi-round, long dialogues based on the information filtered by InternLM-Chat-20B.

During the dataset construction process, we obtain several clusters with a wide range of category distributions. This ensures that our dataset comprehensively covers various aspects of real life, including geography, history, culture, mathematics, physics, chemistry, animals, plants, food, and more. This rich knowledge will help LVLM learn long-context conversational abilities in general scenarios of the real world.

In the manual data inspection phase, due to the large volume of data in the MMDU-45k, it was not feasible to review all of the data, so we sampled \(5\%\) of the dataset for inspection. Statistical analysis showed that the probability of hallucinations and errors in this subset was less than \(5\%\), indicating a high level of reliability.

### Dataset Statistics

In the MMDU-45k, we construct a total of 45k instruct tuning data conversations. The data statistics are shown in Tab. 1. Each data in our MMDU-45k dataset features an ultra-long context, with an average image&text token length of 5k and a maximum image&text token length of 17k tokens. Each

  
**Statistic** & **Number** \\ 
**MMDU Benchmark** & 110 \\ - Avg./Max. Image\&Text Tokens & 8.2k/18k \\ - Avg./Max. Images & 3.8/20 \\ - Avg./Max. Turns & 15/27 \\ - Number of QA Pairs & 1645 \\ 
**MMDU-45k** & 45k \\ - Avg/Max. Image\&Text Tokens & 5k/17k \\ - Avg./Max Images & 3/5 \\ - Avg./Max. Turns & 9/27 \\ - Number of QA Pairs & 410k \\ - Single-image Related Questions & 40k \\ - Multi-images Related Questions & 369k \\ - Avg./Max. Question Length & 31/91 \\ - Avg./Max. Answer Length & 368/1518 \\   

Table 1: **Statistics on MMDU and MMDU-45k.** Figure 4: **Detailed distribution of MMDU.**dialogue contains an average of 9 turns of Q&A, with a maximum of 27 turns. Additionally, each data includes content from 2-5 images. The dataset is constructed in a well-designed format, providing excellent scalability. It can be expanded to generate a larger number and longer multi-image, multi-turn dialogues through combinations. The image-text length and the number of turns in MMDU-45k significantly surpass those of all existing instruct tuning datasets. This enhancement greatly improves the model's capabilities in multi-image recognition and understanding, as well as its ability to handle long-context dialogues.

## 4 Experiments

We evaluate previous representative LVLMs on our MMDU benchmark in Sec. 4.1 and present the analysis of our findings. To demonstrate the high quality of our instruction tuning data MMDU-45k, we provide the comparison results of adding MMDU-45k in the LVLM SFT stage in Sec. 4.2.

BaselinesWe report the performance of four closed-source API models: QWen-VL-Max , Claude3 , GPT-4-turbo  and GPT-4o . We also present the performance of 11 LVLMs including Monkey , Idefics2 , LLaVa1.5 7B/13B , Deepseek-VL , MiniCPM-v-2.5 , Yi-VL , InternVL-Chat-V1.5 , InternLM-XC2 , Qwen-VL-7B  and LLaVa1.6 . Please refer to the Appendix D for the details of our baselines.

### Main Results on MMDU

Tab. 2 presents the benchmarking results on our MMDU benchmark. Our key findings are summarized as follows. (1) Our benchmark poses significant challenges to current LVLMs. Notably, even the advanced GPT-4o model achieves an average accuracy of only 70.2%, while open-source LVLMs achieve merely 42.8% or lower, indicating substantial room for improvement. (2) We observe a significant performance gap between closed-source LVLMs and open-source LVLMs. We speculate that this disparity arises from the scarcity of open-source instruction tuning data with multi-turn and multi-image capabilities, leading to limited improvement in open-source LVLMs. This inspired us to collect and release MMDU-45k, a valuable resource for the open-source community, to bridge this gap.

For Tab. 2, we found that InternLM-Xcomposer2 benefits more from MMDU than LLaVA1.5. This is because, as a more recent model, InternLM-Xcomposer2 uses a different LLM backbone, a more pow

   Models & Param & C & R & VP & LC & AA & IRU & Avg. \\   \\  Qwen-VL-Max  & - & 40.3 & 40.2 & 46.2 & 62.5 & 51.6 & 45.9 & 46.9 \\ Claude3-Opus  & - & 58.6 & 61.5 & 59.7 & 75.1 & 64.1 & 59.8 & 62.6 \\ GPT-4-turbo  & - & 62.0 & 64.2 & 63.4 & 78.0 & 69.0 & 64.4 & 66.3 \\ GPT-4o  & - & 63.7 & 69.6 & 66.7 & 80.6 & 73.3 & 68.1 & 70.2 \\   \\  Monkey  & 10B & 11.9 & 12.0 & 14.8 & 21.9 & 19.6 & 14.6 & 14.1 \\ Idefics2  & 8B & 17.8 & 17.6 & 27.9 & 43.1 & 32.8 & 26.9 & 25.4 \\ LLaVa1.5-7B  & 7B & 27.8 & 28.0 & 33.2 & 43.0 & 35.4 & 31.7 & 32.2 \\ Deepseck-VL  & 8B & 27.3 & 27.7 & 31.2 & 38.7 & 33.2 & 30.0 & 30.8 \\ MinCPM-v-2.5  & 8B & 27.0 & 26.4 & 33.2 & 48.9 & 38.6 & 32.2 & 33.0 \\ Yi-VL  & 6B & 31.7 & 32.2 & 30.6 & 47.5 & 34.0 & 30.0 & 33.2 \\ LLaVa1.5-13B  & 13B & 31.5 & 31.2 & 35.1 & 46.2 & 38.1 & 34.3 & 35.3 \\ InternVL-Chat-V1.5  & 26B & 31.2 & 31.5 & 37.4 & 52.6 & 41.7 & 36.1 & 37.4 \\ InternLM-XC2  & 7B & 29.7 & 29.5 & 36.2 & 50.1 & 40.3 & 35.2 & 35.6 \\ Qwen-VL-7B  & 7B & 33.4 & 33.6 & 39.2 & 53.8 & 43.1 & 38.1 & 39.3 \\ LaVa1.6-mistral  & 7B & 37.7 & 39.3 & 41.4 & 57.2 & 45.6 & 40.2 & 42.8 \\   \\    InterLM-XC2  + MMDU-45k \\ \(\) \\  } & 7B & 34.3 & 34.5 & 36.7 & 47.2 & 38.5 & 35.5 & 37.2 \\  & **+6.5** & **+6.5** & **+3.5** & **+4.2** & **+3.1** & **+3.8** & **+5.0** \\   InterLM-XC2  + MMDU-45k \\ \(\) \\  } & 7B & 45.6 & 43.9 & 49.9 & 64.1 & 53.0 & 48.7 & 50.1 \\  & **+15.9** & **+14.4** & **+13.7** & **+14.0** & **+12.7** & **+13.5** & **+14.5** \\   

Table 2: **Evaluation results of different LVLMs on MMDU. We report the metrics of Creativity (C), Richness (R), Visual Perception (VP), Logical Coherence (LC), Answer Accuracy (AA), Image Relationship Understanding (IRU), and the averaged (Avg.) results.**erful architecture, larger pre-training and SFT data, and more advanced training strategies compared to LLaVa1.5. These advantages may provide InternLM-Xcomposer2 with stronger generalization capabilities.

In addition, we conduct experiments to evaluate the quality of our evaluation with GPT4-o by comparing it to human judgment. Specifically, experts score the results predicted by each model on our benchmark using the same judgment criteria as our evaluation. We calculated several similarity metrics for the overall scores between experts and the GPT4-o system. The Pearson similarity of 97.5% indicates a strong linear relationship, while the Spearman similarity of 97.3% demonstrates consistent scoring monotonicity. The Kendall similarity of 89.0% suggests some variability in the manual scores compared to the judgment range of GPT4-o, yet the consistency remains high.

### Fine-tuning Results using MMDU-45k

We showcase the superior quality of MMDU-45k by presenting comparative results at the bottom of Tab. 2, where we incorporate MMDU-45k into the SFT stage of LVLMs such as LLaVA 1.5  and InternLM-XC2 . Results demonstrate that adding MMDU-45k increases the overall performance on MMDU, especially for the image relationship understanding ability. In Tab. 4.2, we further demonstrate that integrating MMDU-45k also benefits existing benchmarks that require multi-image understanding, such as MMMU  and MMStar , as well as short-form QA datasets like MMVet . To explain the performance improvement, we provide qualitative examples in Fig. 5, illustrating that incorporating MMDU-45k enables LVLMs to engage in longer and more accurate dialogues.

### Multi-Image Benchmark Results

Additionally, we test the model finetuned with MMDU-45k on several multi-image benchmarks. We evaluate five benchmarks: MMMU , BLINK , Qbench2 , Mantis , and MMDU. For MMMU, only the results of multi-image questions are considered, and for Mantis, tests are conducted using both the "merge" and "sequence" methods. As shown in Tab. 4, LLaVa1.5+MMDU-45k achieved significant improvements across all multi-image benchmarks, with the most notable improvement observed in Mantis (sequence), reaching a \(6.9\%\) increase. This indicates that MMDU-45k greatly aids in enhancing the model's multi-image understanding capabilities, significantly addressing the model's shortcomings in reasoning within multi-image scenarios due to a lack of multi-image pre-train data.

    &  MMDU \\ (\(\)) \\  } &  &  &  &  &  \\  & & & & & & & & & \\  LLaVa-1.5 & 27.7 & 37.1 & 46.0 & 37.8 & 41.9 & 32.2 & \\ +MMDU-45k & **29.8** & **40.1** & **48.5** & **44.7** & **44.7** & **37.2** & \\ \(\) & **+2.1** & **+3.0** & **+2.5** & **+6.9** & **+2.8** & **+5.0** & \\   

Table 4: **Multi-image benchmark test results.** We evaluated five multi-image benchmarks: MMMU , BLINK , Qbench2 , Mantis , and MMDU.

    &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & \\  LLaVa1.5  & 32.2 & 66.5 & 35.7 & 33.1 & 25.2 & 55.5 & **48.8** & 31.6 & 21.2 & 38.9 \\ LLaVa1.5 + MMDU-45k & **37.2** & **66.5** & **37.4** & **34.1** & **25.2** & **56.2** & 48.7 & **31.9** & **23.4** & **40.1** \\ \(\) & **+5.0** & **+0.0** & **+1.7** & **+1.0** & **+0.0** & **+0.7** & **-0.1** & **+0.3** & **+2.2** & **+1.2** \\  InternLM-XC2  & 35.6 & 79.5 & 41.4 & 56.2 & 57.2 & 81.2 & 60.0 & 37.6 & 62.6 & 56.8 \\ InternLM-XC2 + MMDU-45k & **50.1** & **79.9** & **41.9** & **57.3** & **58.7** & **81.2** & **60.4** & **38.8** & **63.8** & **59.1** \\ \(\) & **+14.5** & **+0.4** & **+0.5** & **+1.1** & **+1.5** & **+0.0** & **+0.4** & **+1.2** & **+1.2** & **+2.3** \\   

Table 3: **Illustration of the benefits of adding our MMDU-45k data in the LVLM supervised fine-tuning (SFT) stage.** We report the performance on our MMDU and existing representative benchmarks including MMB (MMBench-Dev-EN , MMMU (MMU-Val ), MMStar , MathVista , AI2D , HallBench (HallusionBench ), MMVet  and ChartQA . The best and second-best results in each section are colored **Green** and **Red**, respectively.

### Ablation Study on Token Length and SFT Strategies

We test the LLaVa1.5 baseline (context-window length is 2-4k) and LLaVa1.5 (SFT on MMDU-45k, we extend the context-window length to 8k with the RoPE interpolation) model with different context lengths. From Tab. 5, we observe that: (1) As the context length of the model increases, the performance also improves. (2) Finetuning on MMDU can increase the context window size of the LLaVA model.

Additionally, we compare different SFT strategies for training the model, including "Continue training" and "Add to the existing pool." The results in Tab. 5 show that the final outcomes achieved by both methods are essentially the same.

## 5 Related Work

LVLM Evaluation BenchmarksThe rapid advancements in Large Vision-Language Models (LVLMs)[13; 14; 1; 15; 16; 17; 18; 19; 20; 21; 22; 23; 7; 13; 24; 33; 34; 35; 36] have spurred the development of comprehensive evaluation benchmarks to assess their capabilities across various tasks and domains. Numerous benchmarks [37; 26; 38; 39; 40; 41; 42; 4; 8] aim to provide a standardized and objective way to measure the performance of LVLMs and track their progress toward achieving general multi-modal understanding and reasoning.

Recently, specialized benchmarks have emerged to evaluate specific abilities [43; 29], such as for science reasoning , math reasoning , OCR recognition , and diagram analysis . Some existing benchmarks require multi-turn  chatting with a maximum of three turns, and others on multi-image comparison [3; 45] with a maximum of four images. However, none of the existing benchmarks combine the multi-turn and multi-image abilities with a long context window for conversation applications, highlighting the need for a more comprehensive evaluation framework.

LVLM Instruction-Tuning DatasetsThe development of instruction tuning datasets for LLMs (_e.g._, Alpaca , Vicuna ) has been instrumental in enhancing the instruction-following capabilities. Building upon the successes achieved in LLMs, researchers have proposed visual instruction tuning datasets (_e.g._, LLaVA-Instruct-150K , LLaVA 1.5-665K ) to improve the instruction-following abilities of LVLMs. Moreover, several instruction-tuning datasets have been designed to enhance specific skills [48; 49; 50], such as ShareGPT4V  for caption generation, mPLUG-DocOwl  for document understanding, and VideoChatGPT  for video comprehension. To the best of our knowledge, our MMDU-45k is the first open-source multi-turn, multi-image, and long-context instruction tuning, making it a valuable resource for advancing human-AI interaction capabilities.

## 6 Conclusion

In this paper, we introduce **MMDU**, a multi-image, multi-turn, and long-context benchmark designed to enhance the daily human-AI conversation experience. Our comprehensive evaluation of 15 LVLMs reveals a significant performance disparity, with closed-source models like GPT-4o  outperforming the open-source LVLMs. This disparity may be attributed to the scarcity of open-source instruction tuning datasets that adequately assess the required multi-turn and multi-image abilities. To address this limitation and contribute to the open-source community, we propose **MMDU-45k**, an instruction

   Models &  Max \\ tokens \\  & C & R & VP & LC & AA & IRU &  Overall \\ Score \\  \\  LLaVa-1.5 & 2k & 19.0 & 19.0 & 21.8 & 29.3 & 22.5 & 19.6 & 20.9 \\  & 4k & 25.4 & 25.6 & 31.1 & 40.8 & 32.9 & 29.5 & 30.0 \\    } & 2k & 20.0 & 20.1 & 22.1 & 29.4 & 23.5 & 21.6 & 22.3 \\  & 4k & 31.5 & 32.3 & 34.9 & 45.0 & 36.3 & 33.8 & 34.9 \\  & 8k & **34.2** & **34.3** & **36.1** & **48.2** & **39.6** & **34.7** & **37.1** \\  
  & - & 34.3 & 34.5 & 36.7 & 47.2 & 38.5 & 35.5 & 37.2 \\   & - & 34.3 & 36.3 & 37.1 & 47.3 & 38.9 & 35.7 & 37.3 \\   

Table 5: **Ablation Study on Token Length and SFT Strategies**tuning dataset comprising 45k examples with a maximum of 17K text tokens, 5 images, and 9 turns. We also demonstrate that fine-tuning LVLMs on MMDU-45k improves performance across various LVLM benchmarks. Our **MMDU** and **MMDU-45k** are poised to benefit the research community and foster future advancements in human-AI interaction.

LimitationsWhile MMDU offers several advantages, we acknowledge two key limitations. (1) MMDU primarily focuses on English and does not encompass multilingual abilities. (2) Our benchmark is designed to assess LVLMs' proficiency in daily scenarios, rather than specialized domain expertise (_e.g._, mathematical problem-solving in MathVista ). By acknowledging these limitations, we hope to encourage future research directions and expansions of our benchmark such as incorporating multilingual support for other linguistic populations.

Societal ImpactsAs MMDU-45k is built upon Wikipedia, models fine-turned on MMDU-45k may perpetuate biases and linguistic preferences in English. Moreover, LVLMs fine-tuned on our MMDU-45k may be susceptible to factuality and hallucination issues, potentially generating inaccurate or misleading information. By recognizing these risks, we can work towards creating more inclusive, accurate, and reliable LVLMs that foster trustworthy human-AI interactions.

Author Statement and Data LicenseThe authors bear all responsibility in case of violation of rights and confirm that this dataset is open-sourced under the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. Using this dataset should abide by the policy of OpenAI.

Figure 5: **Visualization examples of adding MMDU-45k in the LVLM SFT stage. Error/hallucination descriptions are marked in red, and the detailed and accurate descriptions are marked in green. The case on the left is from MMDU, and the case on the right is from MMDench.**