# Are Self-Attents Effective

for Time Series Forecasting?

 Dongbin Kim1,  Jinseong Park1,  Jaewook Lee1,  Hoki Kim2*

1 Seoul National University  2Chung-Ang University

{dongbin413,jinseong,jaewook}@snu.ac.kr, hokikim@cau.ac.kr

###### Abstract

Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformers have dramatically advanced the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift the focus from evaluating the overall Transformer architecture to specifically examining the effectiveness of self-attention for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models. The implementation of our model is available at: https://github.com/dongbeank/CATS.

## 1 Introduction

Time series forecasting plays a critical role within the machine learning society, given its applications ranging from financial forecasting to medical diagnostics. To improve the accuracy of predictions, researchers have extensively explored and developed various models. These range from traditional statistical methods to modern deep learning techniques. Most notably, Transformer  has brought about a paradigm shift in time series forecasting, resulting in numerous high-performance models, such as Informer , Autoormer , Pyraformer , FEDformer , and Crossformer . This line of work establishes new benchmarks for high performance in time series forecasting.

However, Zeng et al.  have raised questions about the effectiveness of Transformer-based time series forecasting models especially for long term time series forecasting. Specifically, their experiments demonstrated that simple linear models could outperform these Transformer-based approaches, thereby opening new avenues for research into simpler architectural frameworks. Indeed, the following studies  have further validated that these linear models can be enhanced by incorporating additional features.

Despite these developments, the effectiveness of each components in Transformer architecture in time series forecasting remains a subject of debate. Nie et al.  introduced an encoder-only Transformer that utilizes patching rather than point-wise input tokens, which exhibited improved performance compared to linear models. Zeng et al.  also highlighted potential shortcomings in simpler linearnetworks, such as their inability to capture temporal dynamics at change points  compared to Transformers. Consequently, while a streamlined architecture may be beneficial, it is imperative to critically evaluate which elements of the Transformer are necessary and which are not for time series modeling.

In light of these considerations, our study shifts focus from the overall architecture of the Transformer to a more specific question: **Are self-attentions effective for time series forecasting?** While this question is also noted in , their analysis was limited to substituting attention layers with linear layers, leaving substantial room for potential model design when focusing on Transformers. Furthermore, the issue of _temporal information loss_ (i.e., permutation-invariant and anti-order characteristics of self-attention) is predominantly caused by the use of self-attention rather than the Transformer architecture itself. Therefore, we aim to resolve the issues of self-attention and therefore propose a new forecasting architecture that achieves higher performance with a more efficient structure.

In this paper, we introduce a novel forecasting architecture named Cross-Attention-only Time Series transformer (CATS) that simplifies the original Transformer architecture by eliminating all self-attentions and focusing on the potential of cross-attentions. Specifically, our model establishes future horizon-dependent parameters as queries and treats past time series data as key and value pairs. This allows us to enhance parameter sharing and improve long-term forecasting performance. As shown in Figure 1, our model shows the lowest mean squared error (i.e., better forecasting performance) even for longer input sequences and with fewer parameters than existing models. Moreover, we demonstrate that this simplified architecture can provide a clearer understanding of how future predictions are derived from past data with individual attention maps for the specific forecasting horizon. Finally, through extensive experiments, we show that our proposed model not only achieves state-of-the-art performance but also requires fewer parameters and less memory consumption compared to previous Transformer models across various time series datasets.

## 2 Related Work

Time Series TransformersTransformer models  have shown effective in various domains [5; 4; 15], with a novel encoder-decoder structure with self-attention, masked self-attention, and cross-attention. The self-attention mechanism is a key component for extracting semantic correlations between paired elements, even with identical input elements; however, autoregressive inference with self-attention requires quadratic time and memory complexity. Therefore, Informer  proposed directly predicting multi-steps, and a line of work, such as Autoformer , FEDformer , and Pyraformer , investigated the complexity issue in time series transformers. Simultaneously, unique properties of time series, such as stationarity , decomposition , frequency features , or cross-dimensional properties  were employed to modify the attention layer for forecasting tasks. Recently, researchers have investigated the essential architecture in Transformers to capture long-term

Figure 1: Experimental results illustrating the mean squared error (MSE) and the number of parameters with varying input sequence lengths on ETTm1. Each bubble represents a different model, with the bubble size indicating the number of parameters in millionsâ€”larger bubbles denote models with more parameters. Our model consistently shows the lowest MSE (i.e., best performance) with fewer parameters even for longer input sequences. The detailed results can be found in Table 5.

dependencies. PatchTST  became a de-facto standard Transformer model by patching the time series input in a channel-independence manner, which is widely used in following Transformer-based forecasting models [13; 7]. On the other hand, Das et al.  emphasized the importance of decoder-only forecasting models, while they focused on zero-shot using pre-trained language models. However, none of them have investigated the importance of cross-attention for time series forecasting.

Temporal Information EncodingFixed temporal order in time series is the distinct property of time series, in contrast to the language domain where semantic information does not heavily depend on the word ordering . Thus, some researchers have used learnable positional encoding in Transformers to embed time-dependent properties [9; 23]. However, Zeng et al.  first argued that self-attention is not suitable for time series due to its permutation invariant and anti-order properties. While they focus on building complex representations, they are inefficient in maintaining the original context of historical and future values. They rather proposed linear models without any embedding layer and demonstrated that it can achieve better performance than Transformer models, particularly showing robust performance to long input sequences. Recent linear time series models outperformed previous Transformer models with simple architectures by focusing on pre-processing and frequency-based properties [10; 2; 21]. On the other hand, Woo et al.  investigated the new line of works of time-index models, which try to model the underlying dynamics with given time stamps. These related works imply that preserving the order of time series sequences plays a crucial role in time series forecasting.

## 3 Revisiting Self-Attention in Time Series Forecasting

Motivation of Self-Attention RemovalFollowing the concerns about the effectiveness of self-attention on temporal information preservation , we conduct an experiment using PatchTST . We consider three variations of the PatchTST model: the original PatchTST with overlapping patches with length 16 and stride 8 (Fig. 1(a)); a modified PatchTST with non-overlapping patches with length 24 (Fig. 1(b)); and a version where self-attention is replaced by a linear embedding layer, using non-overlapping patches with length 24 (Fig. 1(c)). This setup allows us to isolate the effects of self-attention on temporal information preservation, while controlling for the impact of patch overlap.

Fig. 2 illustrates the absolute values of the weights in the final linear layer for these model variations. Compared to the original PatchTST (Fig. 1(a)), both non-overlapping versions (Fig. 1(b) and Fig. 1(c)) show more vivid patterns. The version with linear embedding (Fig. 1(c)) demonstrates the clearest capture of temporal information, suggesting that the self-attention mechanism itself may not be necessary for capturing temporal information.

In Table 1, we summarize the forecasting performance of the original PatchTST (Fig. 1(a)) and PatchTST without self-attention (Fig. 1(c)). PatchTST without self-attention consistently improves or maintains performance across all forecasting horizons. Specifically, the original version with self-attention shows lower performance for longer forecast horizons. This result suggests that self-attention may not only be unnecessary for effective time series forecasting but could even hinder

Figure 2: Absolute values of weights in the final linear layer for different PatchTST variations. The distinct patterns reveal how each model captures temporal information.

performance in certain cases. Therefore, better performance of w/o self-attention challenges the conventional belief regarding the importance of self-attention mechanisms in Transformer-based models for time series forecasting tasks.

Our findings offer new insights into the role of self-attention in time series forecasting. As shown in Fig. 2 and Table 1, replacing self-attention with a linear layer not only captures clear temporal patterns but also results in significant performance improvements, particularly for longer forecast horizons. These results highlight potential areas for enhancing the handling of temporal information, beyond addressing the well-known concerns regarding computational complexity.

Rethinking Transformer DesignGiven the challenges associated with self-attention in time series forecasting, we propose a fundamental rethinking of the Transformer architecture for this task. Fig. 3 illustrates the differences between existing architectures and our proposed approach. Traditional Transformer architectures (Fig. 2(a)) and encoder-only models (Fig. 2(b)) rely heavily on self-attention mechanisms, which may lead to temporal information loss. In contrast, Zeng et al.  proposed a simplified linear model, DLinear (Fig. 2(c)), which removes all Transformer-based components. While this approach reduces computational load and potentially avoids some temporal information loss, it may struggle to capture complex temporal dependencies.

To address these challenges while preserving the advantages of Transformer architectures, we propose the Cross-Attention-only Time Series transformer (CATS), depicted in Fig. 2(d). Our approach removes all self-attention layers and focuses solely on cross-attention, aiming to better capture temporal dependencies while maintaining the structural advantages of the transformer architecture. In the following section, we will introduce our CATS model in detail, explaining our key innovations including a novel use of cross-attention, efficient parameter sharing, and adaptive masking techniques.

## 4 Proposed Methodology

### Problem Definition and Notations

A multivariate time series forecasting task aims to predict future values \(}=\{_{L+1},,_{L+T}\}^{M  T}\) with the prediction \(}=\{}_{L+1},,}_{L+T}\} ^{M T}\) based on past datasets \(=\{_{1},,_{L}\}^{M L}\). Here, \(T\) represents the forecasting horizon, \(L\) denotes the input sequence length, and \(M\) represents the dimension of time series data.

In traditional time series transformers, we feed the historical multivariate time series \(\) to embedding layers, resulting in the historical embedding \(^{D L}\). Here, \(D\) is the embedding size. Note that, in channel-independence manners, the multivariate input is considered to separate univariate time series \(^{1 L}\). With patching , univariate time series \(\) transforms into patches \(=()^{P N_{L}}\) where \(P\) is the size of each patch and \(N_{L}\) is the number of input patches. Similar to non-patching situations, patches are fed to embedding layers \(=()^{D N_{L}}\).

  
**Horizon** & **original** & **w/o self-attn** \\ 
96 & **0.290** & **0.290** \\
192 & 0.332 & **0.328** \\
336 & 0.366 & **0.359** \\
720 & 0.416 & **0.414** \\   

Table 1: Effect of self-attention in PatchTST on forecasting performance (MSE) on ETTm1.

Figure 3: Illustration of existing time series forecasting architectures and the proposed architecture.

### Model Structure

Our proposed architecture consists of three key components: (A) _Cross-Attention with Future as Query_, (B) _Parameter Sharing across Horizons_, and (C) _Query-Adaptive Masking_. Fig. 4 illustrates how our model modifies the traditional transformer structure.

By focusing solely on cross-attention, our approach allows us to maintain the periodic properties of time series, which self-attention, with its permutation-invariant and anti-order characteristics, struggles to capture. Furthermore, we leverage advanced architectural designs of time series transformers, such as patching , which linear models cannot utilize. The following subsections provide detailed descriptions of each component of our CATS model, explaining how these elements work together to address the challenges of time series forecasting.

Cross-Attention via Future as QuerySimilar to self-attention, the cross-attention mechanism employs three elements: key, query, and value. The distinctive feature of cross-attention is that the query originates from a different source than the key or value. Generally, the query component aims to identify the most relevant information among the keys and uses it to extract crucial data from the values . In the realm of time series forecasting, where predictions are often made for a specific target horizon--such as forecasting 10 steps ahead. Therefore, within this concept of forecasting, we argue that each future horizon should be regarded as a question, i.e., an independent query.

To implement this, we establish horizon-dependent parameters as learnable queries. As shown in Fig. 4, we begin by creating parameters for the specified forecasting horizon. For each of these virtualized parameters, we assign a fixed number of parameters to represent the corresponding horizon as learnable queries \(\). For example, \(_{i}\) is a horizon-dependent query at \(L+i\). When patching is applied, these queries are then processed independently; each learnable query \(^{P}\) is first fed into the embedding layer, and then fed into the multi-head attention with the embedded input time series patches as the key and value.

Based on these new query parameters, we can utilize a cross-attention-only structure in the decoder, resulting in an advantage in efficiency. In Table 2, we summarize the time complexity of recent Transformer models and ours. The results indicate that our method only requires the time complexity of \((LT/P^{2})\), where most of the Transformer-based models require \((L^{2})\) except FEDformer and

Figure 4: Illustration on the proposed model architecture. Our model removes all self-attentions from the original Transformer structure and focuses on cross-attentions. To fully utilize the cross-attention, we conceptualize the future horizon as queries and use the input time series (i.e., past time series) as keys and values (Fig. A). This simplified structure enables us to enhance the parameter sharing across forecasting horizons (Fig. B) and make use of query-adaptive masking (Fig. C) for performance.

Pyraformer. However, since these two models have an encoder-decoder and a relatively huge amount of parameters, they require 10x and 4x computational times than ours, respectively.

Parameter Sharing across HorizonsOne of the strongest benefits of cross-attention via future horizon as a query \(\) is that each cross-attention is only calculated on the values from a single forecasting horizon and the input time series. Mathematically, for a prediction of future value \(}_{L+i}\) can be expressed as a function solely dependent on the past samples \(=[_{1},...,_{L}]\) and \(_{i}\), independent of \(_{j}\) for all \(i j\) or \(i\) and \(j\) are not in the same patch.

This independent forecasting mechanism offers a notable advantage; a higher level of parameter sharing. As demonstrated in , significant reductions in the required number of parameters can be achieved in time series forecasting through parameter sharing between inputs or patches, enhancing computational efficiency. Regarding this, we propose parameter sharing across all possible layers -- the embedding layer, multi-head attention, and projection layer -- for every horizon-dependent query \(\). In other words, all horizon queries \(_{1},,_{T}\) or \(_{1},,_{N_{T}}\) share the same embedding layer used for the input time series \(_{1},,_{L}\) or patches \(_{1},,_{N_{L}}\) before proceeding to the cross-attention layer, respectively. Furthermore, to maximize the parameter sharing, we also propose cross-dimension sharing that uses the same query parameters for all dimensions.

For the multi-head attention and projection layers, we apply the same algorithm across horizons. Notably, unlike the approach in PatchTST , we also share the projection layer for each prediction. Specifically, PatchTST, being an encoder-only model, employs a fully connected layer as the projection layer for the encoder outputs \(^{D N_{L}}\), resulting in \((D N_{L}) T\) parameters. In contrast, our model first processes raw queries \(=[_{1},,_{N_{T}}]^{P N _{T}}\). These queries are then embedded through the cross-attention mechanism, resulting in \(=[_{1},,_{N_{T}}]^{D N_{T}}\). The final projection uses shared parameters \(W^{P D}\), producing an output \(W^{P N_{T}}\). Thus, our number of parameters for this projection becomes \(P D\), which is not proportionally increasing to \(T\). This approach significantly reduces time complexity during both the training and inference phases.

In Table 3, we outline the impact of parameter sharing across different forecasting horizons. In contrast to the model without parameter sharing, which exhibits a rapid increase in parameters as the forecasting horizon extends, our model, which shares all layers including the projection layer, maintains a nearly consistent number of parameters.

Additionally, all operations, including embedding and multi-head attention, are performed independently for each learnable query. This implies that the forecast for a specific horizon does not depend on other horizons. Such an approach allows us to generate distinct attention maps for each forecasting horizon, providing a clear understanding of how each prediction is derived. Please refer to Section 5.5.

Query-Adaptive MaskingParameter sharing across horizons enhances the efficiency of our proposed architecture and simplifies the model. However, we observed that a high degree of parameter sharing could lead to overfitting to the keys and values (i.e., past time series data), rather than the queries (i.e., forecasting horizon). Specifically, the model may converge to generate similar or identical predictions, \(}_{L+i}\) and \(}_{L+j}\), despite receiving different horizon queries, \(_{i}\) and \(_{j}\) (i.e., the target horizons differ).

Therefore, to ensure the model focuses on each horizon-dependent query \(\), we introduce a new technique that masks the attention outputs. As illustrated in the right-bottom figure of Fig. 4, for each horizon, we apply a mask to the direct connection from Multi-Head Attention to LayerNorm with a

  
**Method** & **Encoder** & **Decoder** & **Time** & **Method** & **Encoder** & **Decoder** & **Time** \\  Transformer  & \((L^{2})\) & \((T+L)\) & 10.4ms & Informer  & \((L L)\) & \((T(T+ L))\) & 13.5ms \\ Autoformer  & \((L L)\) & \(((L/2+H)(L/2+T))\) & 21.4ms & Pyraformer  & \((L)\) & \(((T+L))\) & 11.2ms \\ FEDformer  & \((L)\) & \((L/2+H)\) & 69.3ms & Crossformer  & \((ML^{2}/p^{2})\) & \((MT(T+L)/p^{2})\) & 30.6ms \\ PatchTST  & \((L^{j}/p^{2})\) &. & 7.6ms & CATS (Ours) &. & \((LT/p^{2})\) & 7.0ms \\   

Table 2: Time complexity of Transformer-based models to calculate attention outputs. Time refers to the inference time obtained by averaging 10 runs under \(L=96\) and \(T=720\) on Electricity.

  
**Horizon** & **w/ sharing** & **w/o sharing** \\ 
96 & 355,320 & 404,672 \\
192 & 355,416 & 552,320 \\
336 & 355,560 & 958,112 \\
720 & 355,944 & 3,121,568 \\   

Table 3: Effect of parameter sharing across horizons on the number of parameters for different forecasting horizons on ETH1.

probability \(p\). This mask prevents access to the input time series information, resulting in only the query to influence future value predictions. This selective disconnection, rather than the application of dropout in the residual connections, helps the layers to concentrate more effectively on the forecasting queries. We note that this approach can be related to stochastic depth in residual networks . The stochastic depth technique has proven effective across various tasks, such as vision tasks [17; 25]. To the best of our knowledge, this is the first application of stochastic depth in Transformers for time series forecasting. A detailed analysis of query-adaptive masking can be found in Appendix.

In summary, the framework described in this section, including cross-attention via future as query, parameter sharing across horizons, and query-adaptive masking, is named the **Cross-Attention-only Time Series transformer (CATS).**

## 5 Experiments

In this section, we provide extensive experiments to provide the benefits of our proposed framework, CATS, including forecasting performance and computational efficiency. To this end, we use 7 different real-world datasets and 9 baseline models. For datasets, we use Electricity, ETT (ETTh1, EETTh2, ETTm1, and ETTm2), Weather, Traffic, and M4. These datasets are provided in  and  for time series forecasting benchmark, detailed in Appendix.

For baselines, we utilize a wide range of various baselines, including the state-of-the-art long-term time series forecasting model TimeMixer , PatchTST , Timesnet , Crossformer , MICN , FiLM , DLinear , Autotformer , and Informer . For both long-term and short-term time series forecasting results, we report performance of our model alongside the results of other models as presented in TimeMixer , ensuring a consistent comparison across all baselines. We used 4 NVIDIA RTX 4090 24GB GPUs with 2 Intel(R) Xeon(R) Gold 5218R CPUs @ 2.10GHz for all experiments.

### Long-term Time Series Forecasting Results

To ease comparison, we follow the settings of  for long-term forecasting, using various forecast horizons with a fixed 96 input sequence length. Detailed settings are provided in the Appendix. Table 11 summarizes the forecasting performance across all datasets and baselines. Our proposed model, CATS, demonstrates superior performance in multivariate long-term forecasting tasks across multiple datasets. CATS consistently achieves the lowest Mean Squared Error (MSE) and Mean Absolute Error (MAE) on the Traffic dataset for all forecast horizons, outperforming all other models. For the Weather, Electricity, and ETT datasets, CATS shows competitive performance, achieving the best results on most forecast horizons. This indicates that CATS effectively captures underlying patterns in diverse time series data, highlighting its capability to handle complex temporal dependencies. Additional experiments with longer input sequence lengths of 512 are provided in the Appendix.

    &  &  &  &  &  &  &  &  &  &  \\   & Metric & & & & & & & & & & & & & & & & & & & & & \\   & 96 & **0.161** & **0.207** & 0.163 & 0.209 & 10.207 & 0.172 & 0.220 & 0.195 & 0.271 & 0.198 & 0.261 & 0.195 & 0.236 & 0.195 & 0.252 & 0.266 & 0.336 & 0.300 & 0.384 \\  & 192 & **0.208** & **0.250** & **0.208** & **0.250** & 0.234 & 0.265 & 0.219 & 0.261 & 0.209 & 0.277 & 0.239 & 0.299 & 0.239 & 0.227 & 0.237 & 0.295 & 0.307 & 0.367 & 0.598 & 0.544 \\  & 336 & 0.264 & 0.290 & 0.251 & **0.257** & 0.284 & 0.301 & **0.246** & 0.337 & 0.273 & 0.328 & 0.235 & 0.328 & 0.329 & 0.309 & 0.306 & 0.282 & 0.331 & 0.359 & 0.395 & 0.578 & 0.523 \\  & 270 & 0.324 & **0.341** & **0.339** & **0.341** & 0.367 & 0.349 & 0.365 & 0.359 & 0.359 & 0.379 & 0.401 & 0.388 & 0.386 & 0.361 & 0.354 & 0.385 & 0.382 & 0.3419 & 0.428 & 1.059 & 0.741 \\   & 96 & **0.149** & **0.237** & 0.153 & 0.247 & 0.190 & 0.296 & 0.168 & 0.272 & 0.129 & 0.314 & 0.180 & 0.293 & 0.198 & 0.274 & 0.210 & 0.302 & 0.201 & 0.317 & 0.274 & 0.368 \\  & 192 & **0.143** & **0.250** & 0.166 & 0.256 & 0.159 & 0.304 & 0.184 & 0.322 & 0.231 & 0.322 & 0.189 & 0.209 & 0.198 & 0.278 & 0.210 & 0.305 & 0.222 & 0.234 & 0.349 & 0.266 & 0.386 \\  & 336 & **0.180** & **0.268** & 0.185 & 0.277 & 0.217 & 0.213 & 0.193 & 0.198 & 0.300 & 0.204 & 0.237 & 0.198 & 0.312 & 0.217 & 0.302 & 0.212 & 0.303 & 0.213 & 0.434 & 0.300 & 0.394 \\  & 720 & 0.219 & **0.302** & 0.225 & 0.310 & 0.258 & 0.352 & 0.352 & 0.202 & 0.320 & 0.280 & 0.363 & **0.217** & 0.303 & 0.278 & 0.356 & 0.258 & 0.350 & 0.254 & 0.361 & 0.373 & 0.439 \\   & 96 & **0.124** & **0.270** & 0.265 & 0.256 & 0.347 & 0.593 & 0.321 & 0.644 & 0.299 & 0.577 & 0.306 & 0.467 & 0.384 & 0.650 & 0.396 & 0.613 & 0.388 & 0.719 & 0.391 \\  & 192 & **0.243** & **0.267** & 0.2473 & 0.295 & 0.232 & 0.612 & 0.316 & 0.336 & 0.655 & 0.431 & 0.359 & 0.356 & 0.360 & 0.360 & 0.368 & 0.379 & 0.376 & 0.166 & 0.382 & 0.386 & 0.496 & 0.379 \\  & 336 & **0.453** & **0.284** & 0.498 & 0.296 & 0.517 & 0.334 & 0.629 & 0.336 & 0.674 & 0.420 & 0.594 & 0.358 & 0.610 & 0.367 & 0.605 & 0.373 & 0.622 & 0.337 & 0.777 & 0.420 \\  & 720 & **0.484** & **0.303** & 0.506 & 0.313 & 0.552 & 0.352 & 0.640 & 0.350 & 0.683 & 0.424 & 0.613 & 0.361 & 0.691 & 0.425 & 0.645 & 0.394 & 0.660 & 0.4088 & 0.684 & 0.472 \\   & 96 & **0.289** & **0.339** & 0.280 & **0.339** & 0.326 & 0.362 & 0.312 & 0.355 & 0.465 & 0.456 & 0.340 & 0.388 & 0.382 & 0.345 & 0.319 & 0.368 & 0.389 & 0.418 & 1.414 & 0.418 & 0.616 \\  & 192 & **0.348** & 0.374 & 0.350 & **0.373** & 0.388 & 0.397 & 0.365 & 0.385 & 0.553 & 0.518 & 0.408 & 0.431 & 0.384 & 0.393 & 0.399 & 0.418 & 0.448 & 0.443 & 1.985 & 0.989 \\   & 336 & **0.376** & **0.395** & 0.390 & 0.408 & 0.

### Efficient and Robust Forecasting for Long Input Sequences

Zeng et al.  observed that many models experience a decline in performance when using long input sequences for time series forecasting. To address this, some approaches have been developed to capture long-term dependencies. For instance, TimeMixer  employs linear models with mixed scale, and PatchTST  utilizes an encoder network to encode long-term information. However, these models still have computational issues, particularly in terms of escalating memory and parameter requirements. Thus, in this subsection, we provide a comparison between previous models and ours in terms of efficient and robust forecasting for long input sequences.

First of all, to provide a fair comparison, we summarize the number of parameters, GPU memory consumption, and forecasting performance of comparison models with varying input lengths. As summarized in Table 5, existing complex models, such as PatchTST and TimeMixer, suffer from increased parameters and computational burdens when performing forecasting with long input lengths. Although DLinear uses fewer parameters and less GPU memory, its performance is limited due to its linear structure in capturing non-linearity patterns. Considering both performance and efficiency, the proposed model demonstrates robust performance improvement even with longer input lengths. In Appendix, we provide additional experimental results supporting these findings.

Furthermore, we conduct a deeper comparison between Transformer-based models. Especially, TimeMixer  argues that their model outperforms PatchTST  in the setting of long input sequences. Regarding this setting, we also conduct an experiment on \(L=512\). We summarize the results in Fig. 5. Among these Transformer-based models, our model achieves the lowest MSE for most forecasting horizons. Moreover, our model requires even a lower number of parameters, GPU memory, and running time. Especially, for parameter efficiency, CATS shows significant differences even on a log scale due to its efficient parameter-sharing. Fig. 4(c) highlights GPU memory usage across different forecasting horizons. While PatchTST and TimeMixer consume significantly more memory, CATS maintains a low and stable memory consumption, demonstrating superior memory efficiency. In Fig. 4(d) CATS also consistently achieves lower running times compared to PatchTST and TimeMixer.

Additionally, we also compare the same factors when we use a longer input length \(L=2880\). As more input length is used, the forecasting performance of our model outperforms all other models. Most importantly, while the computational complexity increases as input length increases, our model

Figure 5: Efficiency and performance analysis for time series forecasting models. We summarize the forecasting performance, number of parameters, GPU memory consumption, and running time with varying forecasting horizon lengths on Traffic. The running time is averaged from 300 iterations.

   } &  &  &  \\   & 336 & 720 & 1440 & 2880 & 336 & 720 & 1440 & 2880 & 336 & 720 & 1440 & 2880 \\  PatchTST & 4.38 & 8.7M (2.0x) & 17.0M (4.0x) & 33.6M (7.9x) & 3.5GB & 7.4GB (2.1x) & 22.0GB (6.3x) & 58.6GB (16.9x) & 0.418 & 0.420 & 0.412 \\ TimeMixer & 1.1M & 4.1M (3.6x) & 14.2M (1.6x) & 52.9M (46.8x) & 2.9GB (3.9Gk) & 5.9GB (2.0x) & 10.3GB (3.6x) & 4.28 & 0.425 & 0.414 & 0.472 \\ DLinear & 0.5M & 1.0M (2.1x) & 2.1M (4.2x) & 4.2M (8.5x) & 1.1GB & 1.1GB (0.0x) & 1.2GB (1.0x) & 1.2GB (1.1x) & 0.426 & 0.422 & 0.401 & 0.408 \\ CATS & 0.4M & 0.4M (1.0x) & 0.4M (1.0x) & 0.4M (1.1x) & 1.9GB & 2.1GB (1.1x) & 2.7GB (1.4x) & 3.8GB (2.0x) & 0.407 & 0.402 & 0.399 & 0.395 \\   

Table 5: Comparison of models with the number of parameters, GPU memory consumption, and MSE across different input sequence lengths on ETTm1. Full results with more diverse input lengths are provided in Appendix.

[MISSING_PAGE_FAIL:9]

For prediction, we use an input sequence length \(L=48\) and a forecasting horizon \(T=72\) with signals \((t)\) and \((t)\) are defined with \(=24\), \(S=8\), and \(k=5\). The patch length is set to 4 without overlapping to elucidate the distinct periodic components with 2 attention heads.

In Fig. 6, we illustrate a score map (12\(\)18) of the cross-attention from the trained CATS. Since both patch length and stride are set to 4, each patch will contain exactly one shock value. We observe that the cross-attentions capture the shocks within the signal and the periodicity of the signal in Fig. 5(a) and Fig. 5(b), respectively. Fig. 5(a) shows that patches an even number of steps before the current patch contain the shocks of the same direction, resulting in higher attention scores, while odd-numbered steps have lower scores. Moreover, the correlation over 24 steps is clearly demonstrated in patches spaced by multiples of 6 steps, as shown in Fig. 5(b). This periodic pattern ensures that the attention mechanism effectively captures the periodicity in \((t)\), reflecting the model's ability to leverage this periodic information for more accurate predictions. In Appendix, we provide a detailed explanation.

Fig. 7 illustrates (a) forecasting results, (b) a cross-attention score map (5\(\)4) on the ETTm1 dataset, and (c, d) the two pairs with the highest attention scores. We predict 96 steps with input sequence length 96 on ETTm1. The input patches consist of four patches of 24 lengths and one padding patch. As shown in Fig. 6(c) and 6(d), the patches with high attention scores exhibit similar temporal patterns, demonstrating the ability of CATS to detect sequential and periodic patterns.

## 6 Conclusion

Based on our study, we exploit the advantages of Transformer models in time series forecasting by removing self-attentions and developing a new cross-attention-based architecture. We believe that our model establishes a strong baseline for such forecasting tasks and offers further insights into the complexities of long-term forecasting problems. Our findings provide a reevaluation of self-attentions in this domain, and we hope that future research can critically assess the efficacy and efficiency across various time series analysis tasks. As a limitation, our proposed methods assume channel independence between variables based on the recent work . As the time series data in the real-world are highly correlated, we hope future research can address cross-variate dependency with reduced computation complexity based on the proposed architecture.

Figure 6: Score map of cross-attentions between input and output patches.

Figure 7: Illustration of (a) forecasting result, (b) averaged cross-attention score, and (c,d) patches with the highest score on ETTm1. The score map is averaged from all the heads across layers.

Acknowledgements

This work was partly supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2022-II220984, Development of Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of Explanation) and the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. RS-2024-00338859). This work was also supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) support program (IITP-2024-RS-2024-00438056) supervised by the IITP.