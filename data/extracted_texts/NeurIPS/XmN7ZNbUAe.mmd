# Distributed Inference and Fine-tuning of

Large Language Models Over The Internet

 Alexander Borzunov

HSE Univesity, Yandex

Max Ryabinin

HSE Univesity, Yandex

Artem Chumachenko

Neiro.ai

Dmitry Baranchuk

Yandex

Tim Dettmers

University of Washington

Younes Belkada

Hugging Face

Pavel Samygin

Yandex School of Data Analysis

&Colin Raffel

Hugging Face

Correspondence to: borzunov.alexander@gmail.com

###### Abstract

Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a _large enough_ model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals1 -- a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to \(10\) faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.

## 1 Introduction

In recent years, the NLP community has found that pretrained language models greatly accelerated progress on many research problems through either fine-tuning (Radford et al., 2018) or simple prompting (Brown et al., 2020). Their quality tends to improve as we increase model scale (Radford et al., 2019; Kaplan et al., 2020). Following this trend, modern language models often have hundreds of billions of parameters (Brown et al., 2020; Rae et al., 2021; Zeng et al., 2021; Kim et al., 2021).

Most recently, several research groups open-sourced their pretrained LLMs with over 50B parameters (Zhang et al., 2022; BigScience, 2022; Touvron et al., 2023, 2023). However, they are still difficult to use due to the sheer size in terms of parameters. For example, OPT-175B and BLOOM-176B need over 350 GB accelerator memory for inference and even more for fine-tuning. As a result, even basic inference for these LLMs requires multiple high-end GPUs or multi-node clusters. Recent studiespropose algorithms for running large models with more affordable hardware (Pudipeddi et al., 2020; Ren et al., 2021), e.g. by offloading parameters to RAM. However, as we show in Section 3.1, these techniques are inefficient in many use cases, such as LLM-based chatbots and search engines.

In this work, we search for a more cost-effective way of running pretrained LLMs in their main use cases: inference, in-context learning, and fine-tuning. We analyze latency and throughput for these use cases and determine which factors become dominant for very large models. Notably, for models with over 50B parameters, communicating activations over a slow network can be faster than swapping layers from local RAM or SSD. Based on these observations, it should be possible to run LLMs cost-effectively by pooling together commodity hardware over the Internet.

However, existing LM algorithms are not designed to run inference with unreliable devices or high-latency networks. To bridge this gap, we formulate a novel algorithm for fault-tolerant distributed autoregressive inference of very large models. Using dual attention caches, this algorithm can quickly recover from a failed server and reassign the load to one or more replacement servers. Finally, to make sure that there are enough servers for every part of the model, we develop a decentralzied load-balancing algorithm that assigns transformer blocks to every server to maximize the total system throughput. The fully decentralized nature of these protocols allows participants to add or remove their devices at any point, making optimal use of GPU idle time.

We summarize the main contributions of this work as such:

* We analyze the problem of cost-efficient LLM inference and propose a novel algorithm that can inference large (50B+) language models on distributed unreliable devices. To the best of our knowledge, this is the first algorithm that can inference LLMs with 50B+ parameters in this setup.
* Using this algorithm, we develop PetalS -- a decentralized system for inferencing and fine-tuning LLMs over the Internet. The system allows users to run inference and fine-tuning over a swarm of unreliable devices with the same correctness guarantees as when running locally. The system runs persistently with the help of volunteers.
* We benchmark the performance of the proposed algorithms on Llama 2 (70B) (Touvron et al., 2023b) and BLOOM (176B) (BigScience, 2022a). We run experiments in controlled conditions, with simulated network latency and server failures, and in the actual geo-distributed system spanning two continents. With realistic network speeds, our distributed algorithms perform autoregressive generation \(\)\(10\) faster than local offloading.

## 2 Background: efficient training and inference

There is a wide variety of methods optimizing training and inference for most deep learning workloads. Here, we focus on two areas relevant for our analysis: model parallelism and parameter offloading.

### Model parallelism

Model parallelism is a family of distributed training algorithms that assigns each device to hold a subset of model parameters, run a subset of computations and communicate output activations. _Tensor parallelism_ assigns each device to compute a subset of each model layer (e.g., a subset of neurons), then communicate results between each other and proceed to the next layer (Krizhevsky et al., 2012;

Figure 1: A high-level overview of our system design. Servers store pretrained LLM layers and temporarily hold attention caches for inferencing. Clients hold embedding layers and learned prompts/adapters (if used). Arrows denote temporary chains formed for inference.

Ben-Nun and Hoefler, 2019; Tang et al., 2020). Each device performs a symmetric computation, applied to a different slice of model weights, which makes tensor parallelism compatible with MPI-based communication. In turn, the main performance overhead of this strategy comes from all-to-all communication (and synchronization) after each layer (Krizhevsky, 2014).

_Pipeline parallelism_ reduces the communication overhead by assigning each device with one or several full layers (Huang et al., 2019; Narayanan et al., 2019; Yang et al., 2019). During the forward pass, each stage applies its subset of layers to the inputs supplied by the previous stage, then sends the outputs of the last layer to the next stage. For the backward pass, this process is reversed, with each pipeline stage passing the gradients to the same device that previously supplied it with input activations. To better utilize the available devices, the pipeline must process multiple microbatches per step, allowing each stage to run in parallel on a different batch of inputs. Even with optimal execution, some of the pipeline stages will remain idle some of the time (Huang et al., 2019).

Both of these strategies are actively used for training LLMs. Real-world distributed training systems usually combine multiple forms of parallelism depending on hardware and network type (Narayanan et al., 2021; Rajbhandari et al., 2020; Jia et al., 2019). Tensor parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores (Narayanan et al., 2021; Shazeer et al., 2018). In turn, pipeline parallelism is used to connect multiple servers (Narayanan et al., 2021). Recent works demonstrate that model parallelism can be used for cost-efficient _pre-training_ of LLMs by pooling together idle GPU devices (Athlur et al., 2022; Wang et al., 2022; Kuszmaul, 2022; Yuan et al., 2022; Ryabinin et al., 2021).

### Offloading

Parameter offloading relegates model parameters from accelerator memory to a slower but cheaper storage: typically RAM or SSD (Pudipeddi et al., 2020; Ren et al., 2021; Rajbhandari et al., 2021). When using the model, parameters are loaded to the accelerator just-in-time for computation, one or few layers at a time. In principle, this method allows running large models with a single low-end accelerator as long as there is enough RAM (or SSD) to store the model.

The main drawback of this strategy is having to load and unload through all model parameters for each forward and backward pass, which can be time-consuming. This extra time can be amortized in workloads where model can do a lot of useful computations for each time a parameter is loaded. In practice, using offloading to run a single token through the OPT-175B on one GPU in the best-case scenario of hardware and bandwidth2 would require 11 seconds per forward pass, or twice that for training. As we show in Section 4, real-world performance is significantly slower.

Pudipeddi et al. (2020) circumvents this by training with very large batches, and hence, increasing the computation. In turn, Ren et al. (2021); Rajbhandari et al. (2021) reduce the overhead by overlapping communication and computation, that is, doing useful computation for the current layer while waiting for the transfer of the next layer to finish. Some of these systems Ren et al. (2021) also partition offloaded parameters between devices. However, unlike model-parallel training, distributed offloading still requires each device to compute the full model.

## 3 Method

Using pretrained large language models for NLP tasks consists of two main workloads: inference and fine-tuning. The inference workload typically consists of encoding an input text, then generating tokens autoregressively. In turn, fine-tuning requires updating either all of the model's parameters or (more commonly for large models) a small set of trainable weights (e.g., adapters or soft prompts) by backpropagation. These two workloads also cover more advanced use cases:

* Manually engineering prompts for a given task, then deploying the model with these prompts.
* Fine-tuning with adapters (Hu et al., 2021; Houlsby et al., 2019; Liu et al., 2022b) or "soft" prompts (Liu et al., 2021b; Lester et al., 2021; Liu et al., 2021a) and inferencing fine-tuned models.
* Distillation into a smaller task-specific model for faster inference (Schick and Schutze, 2021).

Counter-intuitively, we found that inference is more challenging than fine-tuning for cost-efficient setups. To that end, we dedicate most of this section to inference-specific problems. As for fine-tuning, we describe a way to support arbitrary parameter-efficient fine-tuning in Section 3.4.

### Performance bottlenecks of LLM inference

Unlike training, autoregressive LLM inference cannot be done with a single pass through the model. Instead, the model needs to process one token at a time, pass it through the entire model, then generate the next token and repeat the process. In case of model parallelism, training an \(n\)-layer3 model on a sequence of \(t\) tokens needs \(O(n)\) communication rounds, while generating the same sequence needs \(O(n t)\) rounds, making it more susceptible to network latency. Similarly with parameter offloading, generating a sequence of \(t\) tokens needs loading every layer \(t\) times, which also takes \(O(n t)\) time.

The other problem of autoregressive generation is dealing with attention for past tokens (Vaswani et al., 2017). During an inference step \(t\), each layer needs to attend to \(t-1\) previous attention keys and values. Existing inference algorithms store past entries in accelerator memory. Caching half-precision activations of a 2048-token sequence for large models like GPT-3 (Brown et al., 2020) or OPT-175B (Zhang et al., 2022) (with 96 layers of 12288 units each) takes up 9.6 GB GPU memory _for each sequence_. Offloading these cached values faces the same problems as offloading in general.

An alternative solution is to recompute all previous tokens on every inference step, storing only one set of keys & values at a time. Naturally, this approach needs increasingly more computation with sequence length \(t\), for a total of \(O(t^{3})\) time for transformer-based models4.Surprisingly, this approach is often more efficient than offloaded caching, especially for shorter sequences due to the overhead from loading and storing cache from RAM or SSD.

Parameter offloading can still be efficient when generating _large amounts of short sequences_ in bulk. Each individual sequence still takes a long time to generate, but the system maintains high throughput by running many samples in parallel. Unfortunately, this scenario does not cover many important LLM use cases. For instance, it is incompatible with in-context learning or prompt engineering, where the model needs to process long sequences of training examples (Brown et al., 2020). More importantly, it does not support "interactive" applications where LLM needs to quickly respond to a user input. This rules out many LLM applications such as conversation systems or input completion (e.g. ChatGPT or Smart Compose).

Hence, we explore a new solution based on pipeline-parallelism. A related line of work (Aminabadi et al., 2022) investigates model parallelism to inference LLMs in GPU clusters. However, their approach does not apply to our more affordable setups: cheap "preemptible" instances or connecting existing resources over the Internet. To operate in these conditions, an inference algorithm needs to deal with node preemption, network errors, and high latency.

### Distributed generation with fault tolerance

In this section, we formulate an algorithm for inferencing LLMs in a fleet of unreliable geographically distributed devices connected over the Internet. Each device can act as a server, a client, or both. A **client** is a node operated by the user, which runs inference or fine-tuning jobs through the swarm of servers. A client only holds input and output embeddings (\(<3\%\) of model weights for BLOOM-176B) and delegates running transformer blocks (the most expensive computations) to remote servers. A **server** is a GPU-enabled node holding a set of consecutive transformer blocks and processing requests coming from client nodes.

For simplicity, we assume that every block is hosted on several servers and examine this assumption in the next section. Following this notation, a fault-tolerant algorithm should allow each client to complete an inference job with reproducible results even if some remote servers fail during inference.

As we discuss in Section 3.1, autoregressive generation requires many sequential communication rounds, making it sensitive to network latency. However, if every device stores its past attention cache, every round only transfers activations for a single token, i.e. several kilobytes of data5. We use this model to directly minimize the inference time over possible pipeline configurations. As we show later in Section 4.2, this allows efficient inference over a low-bandwidth Internet connection.

A more challenging problem is how to recover from node and network failures. If a remote server shuts down, any cached attention keys stored on that server will be lost with it. There are two naive solutions to this problem: restarting inference from scratch or recomputing past embeddings on every step. Restarting might be enough at a small scale. However, running 50B+ models may involve many unreliable devices, making it unlikely to generate long sequence without at least one failure. In turn recomputing past attention caches requires communicating past tokens on every communication round, resulting in \(O(n t^{2})\) total data transferred, where \(n\) is the number of pipeline layers and \(t\) is the sequence length. In other words, both these solutions struggle to generate long sequences.

We address this problem by maintaining two types of cache: _server-side cache_ holds past attention keys and values for their layers, like in existing inference algorithms, while _client-side cache_ holds past inputs sent to a given pipeline stage6. If a server disconnects, a client can find another server with that pipeline stage and use client-side cache to restore the server state.

The resulting procedure is described in Algorithm 1. For every pipeline stage, the client maintains a heap (priority queue) of servers that hold this stage (and may hold additional stages). The servers in queue are ordered by the network latency, measured from past communication. These queues are maintained through the lifetime of a client. To begin generation, the client runs a beam-search-like procedure to find a sequence of servers that results in the least total inference time under our performance model. When running inference steps, a client keeps track of intermediate activations sent between pipeline stages. If a remote server fails or leaves, the client retrieves the next best server (or multiple servers) and requests it to restore the attention state from the client's cached activations.

When servers fail, the algorithm needs to send \(O(t)\) data (in one round) for each failed server and compute only the stages held by the failed servers. This can be seen as an interpolation between naive and cached inference, depending on the server failure rate. If none of the servers fail, we recover \(O(n t)\) communication, similarly to Aminabadi et al. (2022). In turn, if all servers fail after one step, the algorithm effectively performs non-caching generation, which is the best option in that scenario.

In the basic formulation, all communication between pipeline stages is routed through the client, i.e. the client receives the outputs of every pipeline stage, caches it and sends it to the subsequent stage. In practice, it is more efficient to let pipeline stages communicate directly: once the server obtains output activations, it sends them to both client and the subsequent stage. This reduces the total step time since both messages are a few kilobytes in size an can be sent in parallel. To verify that both client and the next pipeline stage received the same set of activations, they can verify the checksums (i.e. hash values) of the received activations asynchronously, without blocking computation.

Algorithm 1 can support greedy inference or any sampling variants (including Holtzman et al. (2020)). However, it requires one more step to support search-based algorithms such as beam search: cache reordering. This allows a client to generate multiple continuations of the same input prefix by cloning its attention cache and dropping less likely hypotheses. We describe beam search in Appendix C.

**Shortest path routing.** In the Algorithm 1, the find_best_chain function (line 4) selects a sequence of servers that can run the required layers in the least amount of time. To estimate this time we add up two factors: computation time, determined by server's compute throughput ("GPU speed") and the network latency between the client and that server. Servers measure their own compute throughput and share this information with the clients. In turn, clients measure the network latency between them and a given server by "pinging" the candidate servers during routing. If a server runs multiple consecutive blocks, we multiply the computation time by the number of blocks.

To find the best chain of servers, clients find the shortest path between the first and last block, using a graph where edge weights correspond to server inference time, as described in the previous paragraph. To minimize overhead, we do not run pathfinding from scratch on each call to find_best_chain. Instead, clients run lifelong pathfinding in the background and reuse it between inference calls. More specifically, we use the D\({}^{*}\) Lite (Koenig & Likhachev, 2005) algorithm because it allows clients to quickly adjust paths after a server is banned or leaves the network.

[MISSING_PAGE_FAIL:6]

Since peers may leave or fail at any time, all nodes periodically check if launching a rebalancing procedure would significantly improve the overall throughput. If it is the case, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the emerged gaps.

We provide a detailed description of the load balancing algorithms in Appendix D and validate their properties in experiments reported in Appendix E.

### Parameter-efficient fine-tuning

While LLMs achieve high quality on many problems with simple prompt engineering (Brown et al., 2020), they often need training to achieve the best results. Traditionally, this is done by fine-tuning all model parameters on the downstream task. However, for extremely large models, this strategy becomes impractical due to hardware requirements. For example, fine-tuning BLOOM-176B with Adam would require almost 3 TB of GPU memory to store the model, gradients, and optimizer states.

Fortunately, _parameter-efficient fine-tuning_ methods have been developed that keep most of the pretrained model intact. Some of them choose a subset of existing parameters to update (Sung et al., 2021; Guo et al., 2021; Liu et al., 2021; Lester et al., 2021; Liu et al., 2021, 2022). Despite their lower memory requirements, parameter-efficient approaches are often competitive with full model fine-tuning (Hu et al., 2021; Liu et al., 2021; Yong and Nikoulina, 2022) and even outperform it in low-data regimes (Liu et al., 2022). Another appealing property of these approaches for our use-case is that they allow rapidly switching a pretrained LLM between adapters.

By focusing on parameter-efficient fine-tuning, we are able to simplify the system design by _making clients responsible for storing their trainable parameters_ (see Figure 1). Servers can run backpropagation through their layers and return gradients with respect to activations, but they _do not update the server-side parameters_. Even when client communicates learned values (e.g. soft prompts) to a server, the server treats these values same as input activations. Thus, a server can simultaneously run different fine-tuning tasks without them interfering with one another. This design choice also allows users to define custom adapters in simple PyTorch without having network engineering expertise.

Unlike inference, fine-tuning forward and backward passes process the entire batch at one go and do not need to store past attention caches between successive client requests. Thus, in case of a failure, we can discard the incomplete forward/backward pass and just repeat the previous forward/backward pass request. This algorithm behaves similarly to the cache-less baseline from Section 4.1.

### Implementation details

Since our main intended use-case is running on inexpensive low-end devices, we need to work around their capabilities. In terms of raw FLOPs, even consumer-grade GPUs like GeForce RTX 3070 could run a complete inference step of BLOOM-176B in less than a second (NVIDIA, 2020). However, the GPU memory can only hold a small fraction of model layers: running naively would require 44 RTX 3070 GPUs and 44 communication rounds. To make this more efficient, we use quantization to store more parameters per GPU, reducing the number of consecutive devices and communication rounds.

One option for quantization is to use 8-bit mixed matrix decomposition for matrix multiplication to quantize the weights to 8-bit precision and reduce the memory footprint compared to 16-bit weights, as suggested in Dettmers et al. (2022). This decomposition separates hidden states and weights into two portions: about 0.1% of 16-bit outlier and 99.9% of 8-bit regular values, which roughly halves the memory footprint with negligible effect on the model quality (see evaluations in Appendix A). Another option is to use the 4-bit NormalFloat format (Dettmers et al., 2023).

To send less data between subsequent pipeline stages, we apply dynamic blockwise quantization (Dettmers et al., 2022) to the hidden states before pipeline-parallel communication, which halves the bandwidth requirements without any noticeable effect on generation quality (Ryabinin et al., 2021). During fine-tuning, we also take advantage of gradient checkpointing (Griewank and Walther, 2000; Chen et al., 2016) and half precision to reduce VRAM usage -- both are standard practice for large language models (Narayanan et al., 2021; Brown et al., 2020; Athlur et al., 2022). In experiments, we apply the same optimizations to baseline systems for a fair comparison.

## 4 Experiments

### Inference with unreliable servers

First, we conduct small-scale preliminary experiments to test the fault-tolerant generation algorithm described in Section 3.2. For these experiments, we use a smaller BLOOM model with 7.1 billion parameters (BigScience, 2022b). This model contains 30 transformer blocks with hidden size 4096. We compare our algorithm with baselines when generating a single sequence of length 512. For simplicity, we run all computations and communications in single precision and disregard word embeddings and logits for this set of experiments. We measure the time to run a certain number of tokens through all blocks and simulate failures by resetting pipeline stages at a certain rate.

We compare three inference strategies:

1. **Caching with restarts**, which refers to standard inference with servers storing attention caches. On failure, it restarts the entire generation from scratch since the failed server's caches are lost.
2. **Cache-less inference**, which reruns past tokens on every step. On failure, it restarts only the last generation step.
3. **Algorithm 1**, which is specifically designed for fault-tolerant inference.

All runs use four pipeline stages with (8, 7, 8, 7) model layers per pipeline stage. Each pipeline stage is served by a single GeForce 1080 Ti GPU; the four GPUs are running in a single system with dual Xeon Gold 6148 CPU, 12 DDR4 LRDIMM sticks with 64 GB each. The system has 16 dedicated PCIe Gen. 3 lanes per GPU in dual root configuration, without using PCIe switches. Each stage runs in an isolated Docker containers with virtual network interfaces, but there is no limit to communication bandwidth for this experiment. We repeat all experiments 50 times and report the average time. The adjusted standard deviation never exceeds 0.2%. We use the pipeline parallelism implementation from Megatron-DeepSpeed (BigScience et al., 2022) for the cache-less baseline.

We report performance measurements in Table 1. Unlike baselines, our algorithm provides reasonable performance _in all tested conditions_, especially for higher failure rates (common for communicating over the Internet, using spot/preemptible instances or unreliable hardware). Caching with restarts is most efficient for inference without failures, with our algorithm being somewhat slower due to less mature implementation. Finally, the cache-less inference can be competitive for short sequences (128 tokens), but slows down considerably on 1024 tokens, which agrees with our intuition from 3.1.

We provide plots showing additional evaluations for a wider range of failure rates (up to 5%) and sequence lengths (up to 2048 tokens) in Appendix F (Figure 3).

### Experiments for Llama 2 (70B) and BLOOM (176B)

In this section, we evaluate our system on more practical tasks of running Llama 2 (70B) (Touvron et al., 2023b) and BLOOM (176B) (BigScience, 2022a). First, we consider servers running in a network with controlled bandwidth and latency8. We measure performance for **(a)** Llama 2 distributed across 3 servers with a T4 GPU each, **(b)** BLOOM distributed across 3 servers with an A100 (80 GB) GPU each, and **(c)** BLOOM distributed across 10 servers with an RTX 3090 GPU each. We use 4-bit NormalFloat quantization (Dettmers et al., 2023) for Llama 2 and 8-bit matrix decomposition (Dettmers et al., 2022a) for BLOOM in all evaluations including the baselines below.

    &  &  \\   & 0 & 1e-4 & 1e-3 & 1e-2 & 0 & 1e-4 & 1e-3 & 1e-2 \\  Caching with restarts & 17.1 & 16.7 & 12 & 0.18 & 15.5 & 11.8 & 0.48 & – \\ Cache-less inference & 3.44 & 3.44 & 3.44 & 3.44 & 0.89 & 0.89 & 0.89 & 0.89 \\ Algorithm 1 (ours) & 11.4 & 11.4 & 10.6 & 3.38 & 10.7 & 10.7 & 7.76 & 2.17 \\   

Table 1: Sequential inference speed (steps/second) of BLOOM (7.1B) with varying failure rates. A failure rate \(p\) means that sending any set of activations to the next stage of the pipeline fails with probability \(p\). Missing values mean that the algorithm did not finish within 1 hour.

We report performance of:

* **Sequential (autoregressive) inference** for batch size 1 (i.e., each step generates 1 token). It is measured in generation steps per second a client can do and shows the _generation latency_.
* **Parallel forward passes** for batches of 128-token sequences9. It is measured in tokens per second a client can process. This shows the _system's throughput_ during batch processing and fine-tuning. 
Since the backward pass performance depends on a set of trainable weights, batch size, and other hyperparameters, we report its performance in different setups separately in Appendix G.

Concurrent clients.We also investigate the effect of having concurrent clients. We assume that each server belongs to a different person, and multiple people (possibly, all of them) are interested in running inference or fine-tuning at the same time. In order to do that, they run the client interacting with our distributed system. The client runs on the same machine, uses 8 CPU cores and no GPU. We report the speed of sequential inference and parallel forward passes that _each client gets on average_.

    & & &  &  \\ 
**GPUs** & **Clients** & **Bandwidth** & **RTT** &  &  \\   & & & & 128 & 2048 & 1\(\)128 & 64\(\)128 \\   & 1 & 1 Gbit/s & \(<\) 5 ms & 2.29 & 2.02 & 45.4 & 155.1 \\  & 1 & 100 Mbit/s & \(<\) 5 ms & 2.29 & 2.01 & 37.5 & 140.2 \\  & 1 & 100 Mbit/s & 100 ms & 1.57 & 1.44 & 23.7 & 128.7 \\  & 3 & 1 Gbit/s & \(<\) 5 ms & 2.02 & 1.74 & 21.2 & 124.2 \\  & – & Offloading & 0.139 & 0.139 & 18.0 & 139.9 \\   

Table 2: Performance of Llama 2 (70B) sequential inference steps and parallel forward passes. The network parameters refer to bidirectional bandwidth and round-trip latency (RTT).

    & & & &  &  \\ 
**GPUs** & **Clients** & **Bandwidth** & **RTT** &  &  \\   & & & & 128 & 2048 & 1\(\)128 & 64\(\)128 \\   & 1 & 1 Gbit/s & \(<\) 5 ms & 1.71 & 1.54 & 70.0 & 253.6 \\  & 1 & 100 Mbit/s & \(<\) 5 ms & 1.66 & 1.49 & 56.4 & 182.0 \\  & 1 & 100 Mbit/s & 100 ms & 1.23 & 1.11 & 19.7 & 112.2 \\  & 3 & 1 Gbit/s & \(<\) 5 ms & 1.65 & 1.49 & – & – \\  & – & Offloading & 0.0495 & 0.0495 & 2.5 & 152.4 \\  & – & Local PP (NVLink) & 2.46 & 2.28 & 98.4 & 279.5 \\   & 1 & 1 Gbit/s & \(<\) 5 ms & 1.65 & 1.54 & 59.1 & 230.1 \\  & 3 & 1 Gbit/s & \(<\) 5 ms & 1.65 & 1.54 & 54.7 & 221.4 \\  & 10 & 1 Gbit/s & \(<\) 5 ms & 1.17 & 1.01 & 31.0 & 131.0 \\ (24 GB) & 10 & 100 Mbit/s & \(<\) 5 ms & 1.05 & 0.99 & 20.1 & 28.1 \\  & 10 & 100 Mbit/s & 100 ms & 0.34 & 0.33 & 6.5 & 16.8 \\  & – & Offloading & 0.0427 & 0.0427 & 2.2 & 109.3 \\   & 1 & 1 Gbit/s & \(<\) 5 ms & 1.24 & 1.06 & 37.9 & 180.0 \\  & 1 & 100 Mbit/s & \(<\) 5 ms & 1.24 & 1.05 & 25.6 & 66.0 \\  & 1 & 100 Mbit/s & 100 ms & 0.57 & 0.53 & 5.8 & 44.3 \\  & 12 & 1 Gbit/s & \(<\) 5 ms & 0.90 & 0.86 & – & – \\ 
14\(\) heterogeneous & 1 & Real-world setup & 0.83 & 0.79 & 32.6 & 179.4 \\  Theoretical-best & – & Offloading & 0.18 & 0.18 & 2.7 & 170.3 \\   

Table 3: Performance of BLOOM (176B) sequential inference steps and parallel forward passes.

Offloading baseline.We also evaluate parameter offloading, where each user runs independently on a single GPU, swapping parameters from CPU memory. First, we report the actual throughput of RAM offloading in case of DeepSpeed with default recommended parameters and enabled pin_memory (gives 1.2\(-\)2\(\) speedup). Next, we report the _theoretical-best_ throughput the offloading baseline can reach for BLOOM. It is calculated as a maximal throughput in the best hardware setup possible (CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes), assuming infinite GPU performance. The calculations are detailed in Appendix B.

Local pipeline parallelism (NVLink).Next, we report performance for BLOOM running on a server with 3\(\) A100 (80 GB) GPUs. In this setup, a single server has enough GPU memory to load the entire model, which provides an _upper bound_ for performance reachable with these GPUs. This setup runs pipeline-parallelism from DeepSpeed v0.7.7.

Heterogeneous servers.To validate that our system works on heterogeneous hardware, we simulate 12 heterogeneous devices by partitioning each A100 (80 GB) into several virtual servers (3 large and 1 small). We get 9 servers hosting 7 blocks each, one server with 3 blocks and two more servers with 2 blocks (70 blocks in total, as required for BLOOM). Additionally, we benchmark the system on real heterogeneous GPUs with diverse compute capabilities in the "Real-world setup" below.

Real-world setup.Finally, we benchmark BLOOM in a real-world setup with 14 smaller servers holding 2\(\)RTX 3060, 4\(\)2080Ti, 2\(\)3090, 2\(\)A4000, and 4\(\)A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100-1000 Mbit/s. Four of the servers operate from behind firewalls10.

Analysis.We report the results for Llama 2 in Table 2 and for BLOOM in Table 3. For inference, performance does not depend much on bandwidth or sequence length but degrades with higher latency. In turn, fine-tuning forward passes for large batches are affected by both bandwidth and latency.

We can see that the offloading baseline is about an order of magnitude slower than our system for inference, both in practice and in the theoretical-best setup assuming an infinite GPU performance. For parallel forward passes, offloading is competitive if networking is limited to 100 Mbit/s or has high latency. In other cases, our algorithm offers higher throughput than offloading for training.

Crucially, our system significantly outperforms offloading even when each GPU node runs its own client doing single-batch inference at the same time. Thus, **given the same hardware**, a group of researchers will get much better inference speed by collaborating over the Internet using our system compared to each of them running offloading independently.

Finally, the real-world setup turns out to be slower than the A100 benchmarks due to slower hardware. Still, our algorithm outperforms offloading even when communicating between different continents.

Additional experiments.We conduct two additional experiments to test individual components of our system. We evaluate the load balancing from 3.3 in isolation in Appendix E. We also evaluate the performance of model compression from Section 3.5 in Appendix A. To reiterate, for each model, we use the same compression strategy in our system and all baselines. Finally, we perform a qualitative evaluation of fault tolerance by shutting down random servers during inference and fine-tuning to verify that the algorithm produces correct outputs and gradients.

## 5 Conclusion

In this paper, we introduced a novel fault-tolerant algorithm for inferencing large language models. On top of it, we introduced a decentralized system for running LLMs on distributed unreliable devices connected over the Internet, which significantly outperforms other approaches to running inference on consumer-grade hardware. We demonstrated that the proposed system can scale to the largest publicly available language model with hundreds of billions of trainable parameters.

While our work is focused on technical aspects, it is important to consider limitations of our approach, such as privacy of data processed by outside peers, as well as broader impact of making LLMs more accessible. We discuss these issues and outline directions for future work in Appendix H.