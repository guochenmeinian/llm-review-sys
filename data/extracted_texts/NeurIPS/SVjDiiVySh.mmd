# Improving CLIP Training with Language Rewrites

Lijie Fan\({}^{1,2,*}\) &Dilip Krishnan\({}^{1}\) &Phillip Isola\({}^{2}\) &Dina Katabi\({}^{2}\) &Yonglong Tian\({}^{1,*}\)

\({}^{1}\)Google Research, \({}^{2}\)MIT CSAIL, \({}^{*}\)equal contribution

###### Abstract

Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.

## 1 Introduction

Pre-trained vision-language multi-modal encoders, exemplified by CLIP , have proven to be extremely useful in learning transferable features from paired image and text data. CLIP's training process can leverage two scalable paradigms: data and compute. Firstly, the availability of large-scale vision-language paired data [49; 48] enables effective training at a substantial scale. Secondly, CLIP's utilization of language and image co-embeddings grants it favorable scaling properties with respect to compute resources . Consequently, CLIP embeddings consistently outperform other visual pre-training approaches such as SimCLR  or MAE  across various downstream tasks . Follow-up methods that build upon language-image pre-training, such as SLIP  and FLIP , exhibit similar advantageous scaling performance.

The CLIP framework is built upon contrastive learning, which typically relies on data augmentations to prevent overfitting and the learning of ineffective shortcuts [8; 47]. However, in the CLIP training process, this beneficial feature is applied exclusively to image inputs, which undergo augmentations in every epoch. In contrast, text inputs are neglected and remain unchanged throughout training, lacking any form of augmentation. This input asymmetry leads to scenarios where the same text is consistently paired with slightly augmented images, while the augmented version of the same image is always paired with the exact same words. Such asymmetry presents two issues. Firstly, the image encoders receive limited supervision from the language side since the same image is consistently paired with the same words. Consequently, the language aspect provides less guidance to the imageencoders. Secondly, the text encoders repeatedly encounter the exact same texts in each epoch, which increases the risk of text overfitting and significantly impacts zero-shot transferability.

Hence, it becomes crucial to incorporate a suitable augmentation strategy for the text inputs. However, existing language augmentation methods are not sufficiently effective. Most previous approaches  focus on word-level treatments like replacement or masking, which have limited impact on enriching text structures and are comparatively weaker than image augmentations. Currently, there is a lack of language rewriting strategies that can effectively augment sentences while preserving key concepts and meanings. Such strategies are urgently needed for CLIP training to ensure optimal performance.

Meanwhile, alongside the development of CLIP, the field has witnessed significant advancements in large language models (LLMs) like GPT models [5; 44; 45] and LaMDA . These LLMs have seen tremendous growth in terms of data, computational resources, and performance. Instruction fine-tuned models such as ChatGPT  and Bard  have also emerged, incorporating fine-tuning through supervised and reinforcement learning. These models have exhibited exceptional performance surpassing human capabilities across a wide range of natural language tasks. Motivated by these advancements, we naturally explored the potential of leveraging LLMs to effectively generate diverse rewritten versions of a given text. While a straightforward approach would involve directly employing instruction fine-tuned models like ChatGPT or Bard, their closed-source nature renders it infeasible to use them for rewriting the hundreds of millions of image descriptions in the datasets. Fortunately, open-sourced LLMs such as LLaMA , despite lacking fine-tuning with instructions, possess excellent In-Context Learning (ICL) capabilities, enabling predictions with a limited context. By thoughtfully designing contextual examples, LLaMA can generate diverse and rich text rewrites for the entire dataset.

Building upon this foundation, we propose Language augmented CLIP (LaCLIP), a straightforward yet highly effective approach for enhancing CLIP model performance by harnessing the power of LLMs. Our method leverages ICL using LLaMA to generate diverse variants of each caption within the text-image pairs of a given pre-training dataset. To facilitate ICL prompts, we have devised multiple strategies to generate a small set of meta-input-output caption pairs. These strategies involve utilizing ChatBots, human rewriters, or existing image captioning datasets. Once we have acquired the meta-input-output pairs, we employ them as examples to prompt LLaMA, enabling the rewriting of millions of texts within the entire dataset. Unlike existing strategies for text rewriting [58; 50], which tend to preserve the sentence structure, LLMs exhibit the remarkable ability to generate language rewrites with greater richness and diversity. This is attributed to their emergent properties and extensive training data. Following the caption rewriting process conducted by LLaMA ICL, each image is now accompanied by a collection of diverse captions resulting from the rewriting process. Utilizing these rewritten texts, we proceed to train CLIP models with augmentation also on the text side. The text augmentation could be performed by randomly selecting one out of the many captions associated with each image.

Extensive experiments on various pretraining datasets at different scales demonstrate our proposed LaCLIP could significantly improve the transferability of CLIP. For instance, on the LAION-400M dataset , we observe a notable improvement over CLIP in the zero-shot performance on ImageNet, increasing from 62.0% to 64.4%. We firmly believe that this strategy presents a simple, scalable approach that contributes to the array of training strategies available for training image embeddings.

## 2 Related Works

**Vision-Language models.** There are a number of earlier works demonstrating the effectiveness of learning visual representations from the supervision of corresponding text [25; 31; 14; 62]. Contrastive Language-Image Pretraining (CLIP)  has attracted significant attention due to its superior representation learning and zero-shot transfer ability. This performance is achieved through contrastive learning on top of image and text features. Another related approach is ALIGN , which achieves similar performance with larger and noisier datasets. There have been numerous follow-up works that attempt to improve the efficacy and data efficiency of CLIP training. SLIP  and DC-CLIP  proposes to improve the performance by incorporating self-supervised training techniques. FILIP  proposes to leverage cross-modal fine-grained alignment between image patches and text words. CoCa  introduces an additional decoder and captioning loss. LIT  proposes to boost zero-shot transferring performance by fine-tuning the text encoders. BLIP series [33; 32] include additional captioners and incorporates iterative image captioning within the training pipeline, which intricately link the generated captions with the image content. However, most of these follow-up works introduce additional training inputs and losses, which can have a negative impact on training efficiency and memory consumption.

**Text augmentation and Large Language Models.** The success of natural language processing (NLP) tasks is strongly dependent on the quality and quantity of available data. Deep neural networks benefit from more data [40; 58]. Common practices in data augmentation include synonym replacement , random masking , and back translation . The advent of self-supervised large language models like BERT  and GPT series [45; 46] has been a game-changer, as they do not require labeled data and can scale up to web-level data to achieve superior transfer ability. Recently, even larger foundation models with billion-level parameters have emerged, revolutionizing the NLP community. Models like the 175B GPT-3  and 540B PaLM  achieve superior performance on various NLP tasks. GPT-3 also demonstrates the few-shot in-context learning ability of large language models . Open-sourced LLaMA  also achieve comparable performances on various benchmarks. In addition to them, ChatGPT and Bard are chatbots trained with reinforcement learning human feedback (RLHF), and have achieved human-comparable performances on various language understanding tasks .

## 3 Improving CLIP with Language Rewrites

This section outlines the core design of our LaCLIP framework, highlighting the key components and strategies involved. We provide a comprehensive description of our approach, including the generation of a small set of meta-input-output text pairs from diverse sources, the process of rewriting image descriptions across the entire dataset using LLM ICL, and the enhanced CLIP training strategies incorporating these rewritten descriptions.

### Preliminary

**CLIP.** The Contrastive Language-Image Pretraining (CLIP) method has proven to be highly effective to train vision models using language supervision. In this framework, a large batch of \(N\) paired images and text \(\{x_{I},x_{T}\}\) are sampled from the training dataset during each training step. The images are pre-processed using data augmentations, and image and text features are extracted using dedicated encoders and normalization functions \(f_{I}\) and \(f_{T}\). The image text features are used to compute the InfoNCE loss, where the paired images and text form the positive pairs and the unpaired ones are treated as negative samples. The training loss can be formulated as follows:

\[L_{I}=-_{i=1}^{N}(f_{I}(_{I}(x_{I} ^{i})),f_{T}(x_{T}^{i}))/)}{_{k=1}^{N}((f_{I}( _{I}(x_{I}^{i})),f_{T}(x_{T}^{k}))/)},\] (1)

In this scenario, \((x_{I}^{i},x_{T}^{i})\) is the \(i^{th}\) image-text pair, and \(_{I}()\) denotes the image augmentation functions. The \((,)\) function measures distance using the dot product, while the temperature \(\) is a learnable parameter that scales the logits. To simplify the presentation, we only show the training loss iterating over images. A symmetrical loss \(L_{T}\) that iterates over texts is also computed during the training process. The total training loss is \(L=(L_{I}+L_{T})/2\).

**Language Rewrites as Text Augmentation.** In Equation 1, the standard CLIP loss applies augmentation exclusively to images, leaving the text inputs unchanged throughout the whole training process. Recognizing this gap, we propose to generate text augmentations, denoted as \(_{T}\), where \(_{T}(x_{T})\) is utilized as the input for \(f_{T}\) instead of the original \(x_{T}\). In the following subsections, we introduce the methodology employed to generate these text augmentations using LLMs, as well as the integration process during CLIP training. By addressing this gap, we aim to enhance the training process and expand the benefits of augmentation to the text inputs, leading to improved performance and a more comprehensive learning framework.

### Meta-Input-Output Text Pair Generation

A recently uncovered property of autoregressive LLMs is In-Context Learning (ICL) [5; 39]. This property allows LLMs to learn a new task by conditioning on a few examples and then make predictions for a test input. To harness ICL for text rewriting, we first need to generate several rewriting examples that can be used as examples in the prompt context. We name these examples meta-input-output text pairs. To this end, we explored different strategies for generating these pairs:

* **Rewriting with Chatbots.** We randomly sample texts from image-text datasets, and prompt ChatGPT and Bard web portals to generate target texts using a prompt such as "Rewrite this caption of an image vividly, and keep it less than thirty words:". Illustrations of this process can be found in Figure 1. Here we leverage the extremely powerful rewriting abilities of these models to provide modified captions that keep the essence of the original caption but change the style and details. This ensures that the semantics associated with the corresponding image do not change, which is important for representation learning purposes.
* **MSCOCO Sampling.** Multiple text descriptions for the same image are available in many existing image captioning datasets. To leverage this characteristic, we utilize the widely used MS-COCO dataset . Within this dataset, each image is associated with five distinct text descriptions, which have been meticulously annotated by human workers. From this dataset, we randomly select a subset of images. For each selected image, we choose one description as the meta-input text and another as the meta-output text.
* **Human Rewriting.** We randomly sample several image-text pairs from various image-text datasets. To ensure diverse and varied textual variations, we engage human annotators and task them with rewriting the captions based on the content depicted in the corresponding observed images. This process results in the creation of meta-input-output pairs, consisting of the original text and the rewritten version by human annotators.

Through the utilization of diverse generation strategies, we acquire four distinct types (_ChatGPT_, _Bard_, _COCO_, and _Human_) of meta-input-output text pairs, which then serve as valuable examples within the input context for the In-Context Learning (ICL) framework. For each specific strategy, we randomly select 16 original captions from image-text datasets and generate target captions using that strategy, resulting in a total of 16 meta-input-output pairs. These pairs encompass a range of sources and variations, facilitating a comprehensive and diverse training experience for our framework.

### Large scale Language Rewriting

Generating rewrites for hundreds of millions of texts using closed-source models like ChatGPT or Bard is impractical due to the significant financial and time costs associated with API usage. Therefore, to facilitate the rewriting of text samples within any given image-text dataset, we employ LLaMA --an open-source state-of-the-art large language model known for its robust performance in text completion tasks. Despite not being fine-tuned with instructions, LLaMA exhibits exceptional ICL capabilities. Leveraging the meta-input-output text pairs generated as described in Section 3.2, we employ LLaMA's ICL ability to rewrite every text entry within the image-text dataset.

Given a text sample to be rewritten, we formulate a context input as the following three parts: Firstly, we include a sentence that informs the LLM about the task of rewriting image descriptions. This serves as an initial contextual clue for the LLM to understand the objective at hand. The second part of the context encompasses three examples sampled from the meta-input-output pairs described in Section 3.2.We randomly three distinct meta-input-output caption pairs from a specific strategy (e.g.,

Figure 1: Illustration of using ChatGPT to generate meta-input-output pairs: we first sample source captions randomly from a few datasets. We then use prompts such as “Rewrite this image caption” to guide the ChatGPT model to generate rewritten target captions. The resulting target captions have different sentence structure than the source texts but, crucially, keep the major objects and subjects intact (in bold). These meta-input-output pairs are later used as contexts for ICL.

ChatGPT). Each pair is clearly separated by a "\(=>\)" symbol. These pairs provide explicit instances that showcase the desired rewriting behavior for the LLM to learn from. The additional random sampling process further enable the LLaMA model to generate more diverse text rewrites. Finally, the last part of the context includes the text sample that requires rewriting, followed by the separation symbol. This ensures that the LLM receives the specific text to be rewritten as part of its context input. By incorporating these three parts, we create a comprehensive context that guides the LLM in effectively generating diverse and contextually appropriate text rewrites.

Utilizing the constructed context input as a prompt, LLaMA exhibits its ability to perform text completion and generate rewritten versions of the corresponding text samples. This process is conducted for each text sample present in the pre-training image-text dataset. Specifically, we employ the LLaMA-7B model to generate four distinct rewrites for every text sample in the dataset, with each rewrite corresponding to one of the four different meta-input-output sources (_ChatGPT_, _Bard_, _COCO_, or _Human_). It takes 7 hours to generate one rewrite for the entire CC3M dataset on a 8 A100 GPU machine. By incorporating multiple sources and leveraging the capabilities of LLaMA, we ensure the generation of diverse and contextually relevant text rewrites for each text sample within the dataset.

### LaCLIP: Training CLIP with Language Augmentations

Having generated \(M\) different rewrites for each caption (in our case, \(M=4\)), we are now able to bridge the augmentation gap between the image and text inputs, thereby presenting the training strategy for our LaCLIP framework. The key addition to the CLIP framework is the augmentation function for the text inputs, which can be easily implemented through a straightforward random sampling process, where we randomly select a text sample from either the original text or one of the generated rewrites:

\[_{T}(x_{T})([x_{T0},x_{T1},x_{TM}])\] (2)

Here \(x_{Tk}\) is the \(k^{th}\) rewritten sample for \(x_{T}\). For the sake of simplicity, we denote the original text as \(x_{T0}\). The training loss over the images becomes:

\[L_{I}=-_{i=1}^{N}(f_{I}(_{I}(x_{I }^{i})),f_{T}(_{T}(x_{T}^{i})))/)}{_{k=1}^{N} ((f_{I}(_{I}(x_{I}^{i})),f_{T}(_{T}(x_{ T}^{k})))/)},\] (3)

The only difference with the original CLIP training here is the additional text augmentation \(_{T}\), and all other parts remains the same, which does not bring any additional computation or parameter overheads compared to original CLIP during training. By incorporating text augmentations into CLIP, we introduce variability and diversity into the training data, enabling the model to learn from both the original text and the augmented versions. This simple yet effective strategy enhances the training process and contributes to the overall performance and adaptability of the LaCLIP framework.

Figure 2: Illustration of our proposed in-context learning based strategy for language rewriting: The left box depicts the input context for LLaMA, responsible for text completion. The blue and green texts represent the meta-input-output text pairs, with blue indicating the input and green indicating the output. These pairs are defined in Section 3.2 and visualized in Fig. 1. The final blue line represents the text from the image-text datasets intended for rewriting. On the right box, we showcase the completion result generated by the open-source 7B parameter LLaMA model , which represents the rewritten version of the text of interest.

## 4 Experiments

**Datasets.** Our experiments were conducted on four different image-text datasets at different scale: Conceptual Captions 3M (CC3M) , Conceptual Captions 12M (CC12M) , RedCaps , and LAION-400M. RedCaps is a 12M-instance dataset collected exclusively from Reddit, potentially exhibiting distinct distributions compared to other datasets. The majority of our ablation studies were performed on the CC12M dataset. We evaluate all the models on ImageNet and 15 common downstream datasets like Food101 , SUN397  and FGVCAircraft . Appendix A contains the details for all datasets.

**Training Parameters.** For most of our experiments on CC3M, CC12M, and RedCaps, we utilized the ViT-B/16 architecture  and trained the models with a batch size of 8,192 and the AdamW optimizer . Additionally, we explored the ViT-L/16 and ViT-S/16 architectures in ablation studies. For LAION-400M, we used both the ViT-B/32 and ViT-B/16 architecture with a batch size of 32,768, and followed the exact training setup outlined in , training the model for 32 epochs. Appendix B contains a detailed breakdown of our training hyperparameters.

**Evaluation Setup.** We consider three evaluation metrics for the trained models: Zero-Shot (**ZS**) classification accuracy, Few-Shot (**FS**) classification accuracy and Linear Probing (**LP**) accuracy. For zero-shot classification, we adopt the same prompt templates as described in the CLIP paper . The class text embeddings are used to compute distance with the image feature, and images are classified to class with the shortest distance. For few-shot classification, we follow the set up in  and perform 5-way 5-shot classification with a weighted kNN classifier on top of the frozen features. For linear probing, following [44; 18], we freeze the pre-trained image encoder and extract features for every image in the downstream dataset. We then train a linear classifier using L-BFGS optimizer on top of the extracted features. ZS and LP are evaluated on both ImageNet (**IN**) and 15 Downstream (**DS**) datasets. FS are evaluated on the same downstream datasets. In the ablation studies we report the perforamnce on **IN** and the mean on **DS**.

### Zero-shot Evaluation

We provide a comprehensive analysis of the zero-shot transfer performance on ImageNet and downstream datasets in Table 1. Remarkably, across all pretrained datasets, our LaCLIP approach achieves a significant performance improvement over the baseline CLIP model on both ImageNet and down

    &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\    & & & & & & & & & & & & & & & & & & & \\   \\   & CLIP & **79.9** & 91.8 & 72.0 & 64.6 & 77.0 & 15.8 & 49.9 & 84.8 & 89.3 & 64.4 & 95.3 & 43.2 & 60.6 & 36.9 & 14.5 & 62.7 & 62.0 \\  & LaCLIP & 79.7 & **92.4** & **73.0** & **64.9** & **81.9** & **20.8** & **55.4** & **87.2** & **91.8** & **70.3** & **97.3** & **50.6** & **61.5** & **49.4** & **16.0** & **66.1** & **64.4** \\   \\   & CLIP & 10.3 & 54.9 & 21.8 & 25.0 & 0.8 & 1.4 & 10.5 & 12.8 & 43.3 & 10.2 & 77.6 & 14.1 & 19.1 & **6.9** & 0.6 & 20.6 & 15.8 \\  & LaCLIP & **14.2** & **57.1** & **27.5** & **35.1** & **1.6** & **1.6** & **16.6** & **15.6** & **52.7** & **14.7** & **86.2** & **15.0** & **24.3** & 6.4 & **1.0** & **24.6** & **21.5** \\   & CLIP & 50.8 & 64.9 & 38.5 & 44.7 & 24.1 & 2.4 & 19.4 & 64.1 & 77.4 & 33.2 & 91.0 & 20.1 & 38.9 & 7.3 & 5.1 & 38.8 & 40.2 \\  & LaCLIP & **60.7** & **75.1** & **43.9** & **57.0** & **36.3** & **5.6** & **31.0** & **72.4** & **83.3** & **39.9** & **95.1** & **27.3** & **44.3** & **12.7** & **8.9** & **46.2** & **48.4** \\    & SLIP & 52.5 & 80.7 & 46.3 & 48.8 & 24.9 & 2.3 & 25.1 & 58.6 & 77.6 & 29.2 & 89.1 & **25.8** & 36.6 & 6.0 & 5.7 & 40.6 & 42.1 \\  & LaSLIP & **62.9** & **82.0** & **50.2** & **59.6** & **32.2** & **4.4** & **30.1** & **70.6** & **82.4** & **37.4** & **95.0** & 20.4 & **45.6** & **10.1** & **9.2** & **46.1** & **49.7** \\   & CLIP & 81.5 & 70.4 & 39.9 & 33.2 & 19.2 & 1.9 & 19.7 & **82.7** & 72.8 & 53.9 & **92.8** & 23.3 & 33.6 & **8.3** & 6.2 & 42.6 & 42.9 \\  & LaCLIP & **85.0** & **74.8** & **40.7** & **40.3** & **21.3** & **2.2** & **23.9** & 78.2 & **76.4** & **59.0** & 91.4 & **27.1** & **41.3** & 5.6 & **7.6** & **45.0** & **46.2** \\   & CLIP & 85.5 & 93.0 & 71.7 & 66.8 & 83.5 & 16.7 & 52.8 & 90.1 & 91.2 & 63.9 & 97.3 & 42.4 & 63.3 & **46.2** & 17.8 & 65.5 & 67.0 \\  & LaCLIP & **86.5** & **93.5** & **73.9** & **67.9** & **87.1** & **24.2** & **58.9** & **90.9** & **92.4** & **73.1** & **98.4** & **48.3** & **65.8** & 46.1 & **19.6** & **68.4** & **69.3** \\   

Table 1: Zero-shot transfer evaluation of different models. Performance on ImageNet and 15 common downstream datasets are reported. We highlight the best performance of each setting in **bold**. We see that regardless of the scale of the pre-training dataset, LaCLIP outperforms CLIP  and LaSLIP outperforms SLIP , by up to \(8.2\%\) absolute accuracy.

stream datasets. For instance, when training models on the CC12M dataset, our LaCLIP method achieves over 8% improvement in absolute top-1 accuracy on ImageNet and 7% improvement on average over the other downstream datasets. LaCLIP and CLIP share the exact same amount of parameters and computation cost during training.

**Adaptability to other methods.** It is noteworthy that LaCLIP is compatible with other techniques intended to enhance CLIP's performance. Once the augmented texts are generated, integrating LaCLIP into any CLIP-based framework can be achieved seamlessly without incurring additional computational or memory overhead. As demonstrated in Table 1, we applied language augmentation to the SLIP framework and yield LaSLIP, resulting in significant performance improvements across all evaluation metrics. Notably, even though SLIP already incorporates additional self-supervision to enhance CLIP's performance, our proposed language augmentation further boosts its effectiveness. This showcases the generalization capability of our proposed text augmentation strategy.

**Generalization to Larger Datasets.** Consistently, the results highlight the substantial margin by which LaCLIP outperforms CLIP across various datasets. Noteworthy is the scalability of our method with dataset size, as it demonstrates improvement even when trained on the massive LAION-400M dataset, which contains hundreds of millions of data points1. These findings suggest that our LaCLIP approach can be seamlessly integrated as a plug-and-play component for training vision-language foundation models.

### Few-Shot & Linear-Probing

We present the 5-way 5-shot classification performance in Table 2 and the linear-probing performance in Table 3. Our approach consistently outperforms vanilla CLIP or SLIP in the vast majority of cases. Interestingly, SLIP performs worse than vanilla CLIP in the few-shot setting, despite introducing additional self-supervision from the image side. However, by incorporating our proposed language augmentation strategy, SLIP's few-shot performance improves, surpassing vanilla CLIP. This result highlights the effectiveness of text augmentations in the few-shot setting.

Furthermore, it is important to highlight that the improvements observed in the few-shot and linear-probing results are solely achieved through the utilization of image encoders. This demonstrates the efficacy of our proposed text augmentation approach in enhancing not only the joint image-text embedding space, which aligns image and text features more effectively, but also the quality of the

    &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & & & & \\   & CLIP & 92.5 & 87.2 & 89.0 & 98.0 & 98.5 & 78.9 & 87.4 & 94.5 & 99.2 & 99.0 & 96.1 & **82.8** & **94.3** & 79.8 & 49.7 & 88.5 \\  & LaCLIP & **93.5** & **91.0** & **90.7** & **98.2** & **99.1** & **82.2** & **87.5** & **95.7** & **99.4** & **99.2** & **97.2** & 80.1 & 94.2 & **80.4** & **52.2** & **89.4** \\   & CLIP & 67.6 & 64.2 & 73.6 & 94.1 & 54.4 & 46.1 & 74.4 & 76.7 & 93.3 & 94.3 & 84.6 & **81.4** & **87.1** & 66.9 & 37.3 & 73.1 \\  & LaCLIP & **70.0** & **69.1** & **76.8** & **95.2** & **57.6** & **49.2** & **75.8** & **77.4** & **95.2** & **95.0** & **89.5** & 81.1 & 85.5 & **71.0** & 37.3 & **75.0** \\   & CLIP & 87.0 & 77.5 & 82.1 & 97.0 & 96.0 & 83.3 & 91.1 & 98.2 & 97.6 & 92.6 & **83.4** & 91.2 & 70.6 & 44.3 & 83.3 \\  & LaCLIP & **89.9** & **83.5** & **89.0** & **98.5** & **68.1** & **84.9** & **93.4** & **98.9** & **98.4** & **95.9** & 83.0 & **92.4** & **76.4** & **85.8** \\   & SLIP & 87.6 & 79.2 & 83.0 & 97.5 & 85.6 & 56.4 & 85.8 & 88.1 & 97.7 & 97.1 & 92.5 & **84.9** & 91.0 & 62.4 & 43.0 & 82.1 \\  & LaSLIP & **90.5** & **84.9** & **86.6** & **98.1** & **91.6** & **61.0** & **86.7** & **89.8** & **97.8** & **97.8** & **94.2** & 84.0 & **92.8** & **65.8** & **45.4** & **84.5** \\   & CLIP & 94.4 & 80.6 & 85.3 & 95.9 & 88.5 & 54.5 & **82.6** & **94.5** & 97.8 & 99.0 & 94.8 & 84.9 & 91.3 & 75.3 & 40.6 & 84.0 \\  & LaCLIP & **95.8** & **81.4** & **85.4** & **96.2** & **90.9** & **58.8** & 82.4 & 94.1 & **98.0** & **99.2** & **95.6** & **86.2** & **92.1** & **76.5** & **42.6** & **85.0** \\   & CLIP & 95.0 & 90.1 & 90.7 & 98.2 & 99.2 & 80.8 & 88.7 & 96.2 & 99.5 & 99.4 & 97.1 & **84.5** & 95.0 & 77.7 & 55.1 & 89.8 \\  & LaCLIP & **95.8** & **92.7** & **91.9** & **98.4** & **99.5** & **86.1** & **89.0** & **97.1** & **99.6** & **99.5** & **98.1** & 82.9 & 95.0 & **80.9** & **57.9** & **91.0** \\   

Table 2: Few-shot transfer evaluation of different models. We report 5-way, 5-shot classification accuracy for all downstream datasets. We highlight the best performance of each setting in **bold**. Similar to zero-shot, in nearly all cases, pre-training using language rewrites outperforms vanilla pre-training.

[MISSING_PAGE_EMPTY:8]

is that during the meta-input-output generation process, humans have the advantage of viewing the corresponding image, which allows them to generate more accurate and diverse rewrites.

**Different Backbone Architecture.** We further investigate the performance of LaCLIP using different backbone architectures. Table 6 summarize the results obtained with ViT-S/16, ViT-B/16, and ViT-L/16. We observe that LaCLIP scales with model size and consistently outperforms vanilla CLIP across all network architectures. These findings highlight the effectiveness of LaCLIP in improving the performance of CLIP models, irrespective of the underlying backbone architecture.

**Comparison with Pre-trained Text Encoder.** To deepen the comprehension on whether LaCLIP outperforms vanilla CLIP simply due to a better text encoder, we conducted experiments comparing LaCLIP with CLIP models trained with a pre-trained text encoder. Here we employed the BERT model as the pre-trained text encoder. The experiment result in Table 8 demonstrates that fine-tuning based on the pre-trained text encoder exhibits some improvements, whereas freezing the pre-trained text encoder weights substantially degrades performance. This observation aligns with the findings in LiT . In contrast, LaCLIP consistently outperforms all configurations with a pre-trained text encoder, underscoring the benefit and necessity for explicit sentence augmentation strategies.

## 5 Multi-Text Training Loss with LaCLIP

It is important to highlight that once we have generated multiple text augmentations, and with a slight tolerance for computational cost, we can create multi-positive training pairs for each training iteration. These pairs are formed by pairing each image with not only the original text but also with all the rewritten versions of the text. By adopting this approach, we introduce a Multi-Text version

    &  &  \\   & **DS** & **IN** & FS &  & **IN** \\   & CLIP & 36.3 & 36.9 & 82.2 & 77.0 & 67.1 \\  & LaCLIP & **44.1** & **46.3** & **84.5** & **78.0** & **69.1** \\   & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\   & CLIP & 42.6 & 44.0 & 85.1 & 81.3 & 72.9 \\  & LaCLIP & **46.6** & **49.1** & **86.8** & **81.9** & **73.7** \\   

Table 6: Ablation on different network architectures.

    &  &  \\   & **DS** & **IN** & FS &  & **IN** \\   & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & **AV** & **AV** & 42.1 & 42.9 & 83.6 & 79.5 & 70.4 \\  & **AV** & **AV** & 24.5 & 23.2 & 80.3 & 74.9 & 66.0 \\  LaCLIP & **AV** & **AV** & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\   

Table 4: Performance comparison of CLIP training with different text augmentations.

    &  &  &  \\   & & **DS** & **IN** & & **DS** & **IN** \\   & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & 48.4 & 85.8 & 80.5 & 72.3 \\  & LaCLIP-MT & 45.2 & **49.0** & 85.8 & **80.6** & **72.4** \\   & CLIP & 42.6 & 42.9 & 84.0 & 79.6 & 71.8 \\  & LaCLIP & 45.0 & 46.2 & 85.0 & 80.0 & 71.9 \\   & LaCLIP-MT & **46.1** & **48.1** & **85.3** & **80.3** & **72.4** \\   

Table 7: Performance comparison of CLIP, LaCLIP and LaCLIP-MT on different datasets.

Figure 3: ImageNet zero-shot accuracy with different num of text augments.

    &  &  &  \\   & & **DS** & **IN** & FS &  & **IN** \\   & CLIP & 36.3 & 36.9 & 82.2 & 77.0 & 67.1 \\  & LaCLIP & **44.1** & **46.3** & **84.5** & **78.0** & **69.1** \\   & CLIP & 38.8 & 40.2 & 83.3 & 79.4 & 70.3 \\  & LaCLIP & **46.2** & **48.4** & **85.8** & **80.5** & **72.3** \\   & CLIP & 42.6 & 44.0 & 85.1 & 81.3 & 72.9 \\  & LaCLIP & **46.6** & **49.1** & **86.8** & **81.9** & **73.7** \\   

Table 8: Performance comparison of LaCLIP and CLIP models trained with different pre-trained text encoder configurations.

of LaCLIP, referred to as LaCLIP-MT, which incorporates a multi-positive contrastive training loss. The training process iterates through all the images in the following manner:

\[L_{I*}=-_{i=1}^{N}_{j=0}^{M}(f_{I}(_{I}(x_{I}^{i})),f_{T}(x_{Tj}^{i}))/)}{ _{k=1}^{N}((f_{I}(_{I}(x_{I}^{i})),f_{T}(x_{Tj}^{ k}))/)},\] (4)

Since each text can still be paired with a single image, the training loss that iterates through all the texts remains unchanged, with the only difference being that it now iterates over all of the texts instead of just the augmented ones. Consequently, the final training loss is given by the average of the image loss (\(L_{I*}\)) and the text loss (\(L_{T}\)), resulting in \(L=(L_{I*}+L_{T})/2\).

In order to showcase the efficacy of the multi-positive contrastive training loss in boosting the performance of LaCLIP, we conducted additional experiments on the CC12M and RedCaps datasets. The results of these experiments are summarized in Table 7, which compares the performance of LaCLIP-MT with both LaCLIP and vanilla CLIP. The results clearly indicate that LaCLIP-MT could further improve upon LaCLIP across most metrics. By allowing each image to be paired with all of the diverse texts describing its content, LaCLIP-MT leverages the additional and richer supervision from the language modality to enhance the formation of image-text embeddings. This improvement highlight the benefits of the multi-positive contrastive training loss in facilitating better alignment between images and diverse text descriptions.

## 6 Conclusion, Limitations and Broader Impact

Conclusion.We have introduced LaCLIP, a straightforward yet highly effective CLIP training strategy that incorporates text augmentations through text rewriting, leveraging the in-context learning capabilities of LLMs. Through this simple and versatile approach, we have demonstrated significant improvements in the performance of CLIP embeddings across various pre-training scales and datasets. Additionally, we have proposed a novel multi-text training loss to further enhance the training process. As LLMs continue to improve in performance and in-context learning capabilities, our approach stands to directly benefit from these advancements.

**Limitations.** While the training process itself does not entail any additional memory or computation overhead compared to vanilla CLIP, the process of generating text rewrites using LLMs can be computationally expensive, requiring significant GPU resources and taking hours for large datasets. Additionally, the quality of the rewritten text generated by LLaMA is not filtered, which may result in some irrelevant details that do not align well with the corresponding images. This misalignment could impact the transferability of the learned embeddings to downstream tasks. To address these limitations, future work could focus on developing more efficient methods for generating text rewrites using LLMs, reducing the computational burden without sacrificing performance. Furthermore, techniques for filtering the rewritten texts could be explored, aiming to retain only the most relevant and accurate versions while discarding those with misleading details. This would enable the model to learn a better embedding space that is robust and transferable across different downstream datasets, improving overall performance and alignment between vision and text encoders.

**Broader Impact.** We propose a general text augmentation strategy that can generate diverse rewrites for any given text. This strategy not only improves the performance of vision-language models but also has the potential to enhance models in pure natural language processing tasks, such as language understanding and reasoning. On the other hand, we acknowledge that LLMs are trained on large-scale web data, which may contain factual errors and hallucinations. Consequently, the rewritten versions of texts may also inherit these limitations. Therefore, we encourage researchers to implement additional data filtering methods before deploying these models in real-world scenarios. Additionally, the current LLM-based rewriting strategy requires significant GPU/TPU computation, which can contribute to a higher carbon footprint. However, it is also possible that such rewriting strategy can significantly reduce the number of training iterations for larger models to reach similar performances as vanilla CLIP.