# Learning Diffusion Priors from Observations

by Expectation Maximization

 Francois Rozet

University of Liege

francois.rozet@uliege.be

&Gerome Andry

University of Liege

gandry@uliege.be

&Francois Lanusse

Universite Paris-Saclay,

Universite Paris Cite, CEA, CNRS, AIM

francois.lanusse@cnrs.fr

&Gilles Louppe

University of Liege

g.louppe@uliege.be

###### Abstract

Diffusion models recently proved to be remarkable priors for Bayesian inverse problems. However, training these models typically requires access to large amounts of clean data, which could prove difficult in some settings. In this work, we present a novel method based on the expectation-maximization algorithm for training diffusion models from incomplete and noisy observations only. Unlike previous works, our method leads to proper diffusion models, which is crucial for downstream tasks. As part of our method, we propose and motivate an improved posterior sampling scheme for unconditional diffusion models. We present empirical evidence supporting the effectiveness of our method.

## 1 Introduction

Many scientific applications can be formalized as Bayesian inference in latent variable models, where the target is the posterior distribution \(p(x y) p(y x)\,p(x)\) given an observation \(y^{M}\) resulting from a forward process \(p(y x)\) and a prior distribution \(p(x)\) over the latent variable \(x^{N}\). Notable examples include gravitational lensing inversion , accelerated MRI , unfolding in particle physics , and data assimilation . In all of these examples, the observation \(y\) alone is either too incomplete or too noisy to recover the latent \(x\). Additional knowledge in the form of an informative prior \(p(x)\) is crucial for valuable inference.

Recently, diffusion models  proved to be remarkable priors for Bayesian inference, demonstrating both quality and versatility . However, to train a diffusion model for the latent \(x\), one would typically need a large number of latent realizations, which by definition are not or rarely accessible. This is notably the case in earth and space sciences where the systems of interest can only be probed superficially.

Empirical Bayes (EB) methods  offer a solution to the problem of prior specification in latent variable models when only observations \(y\) are available. The objective of EB is to find the parameters \(\) of a prior model \(q_{}(x)\) for which the evidence distribution \(q_{}(y)= p(y x)\,q_{}(x)\,x\) is closest to the empirical distribution of observations \(p(y)\). Many EB methods have been proposed over the years, but they remain limited to low-dimensional settings  or simple parametric models .

In this work, our goal is to use diffusion models for the prior \(q_{}(x)\), as they are best-in-class for modeling high-dimensional distributions and enable many downstream tasks, including Bayesian inference. This presents challenges for previous empirical Bayes methods which typically rely on models for which the density \(q_{}(x)\) or samples \(x q_{}(x)\) are differentiable with respect to theparameters \(\). Instead, we propose an adaptation of the expectation-maximization [40; 41; 42; 43; 44] algorithm where we alternate between generating samples from the posterior \(q_{}(x y)\) and training the prior \(q_{}(x)\) on these samples. As part of our method, we propose an improved posterior sampling scheme for unconditional diffusion models, which we motivate theoretically and empirically.

## 2 Diffusion Models

The primary purpose of diffusion models (DMs) [15; 16], also known as score-based generative models [45; 46], is to generate plausible data from a distribution \(p(x)\) of interest. Formally, adapting the continuous-time formulation of Song et al. , samples \(x^{N}\) from \(p(x)\) are progressively perturbed through a diffusion process expressed as a stochastic differential equation (SDE)

\[x_{t}=f_{t}\,x_{t}\,t+g_{t}\,w_{t} \]

where \(f_{t}\) is the drift coefficient, \(g_{t}_{+}\) is the diffusion coefficient, \(w_{t}^{N}\) denotes a standard Wiener process and \(x_{t}^{N}\) is the perturbed sample at time \(t\). Because the SDE is linear with respect to \(x_{t}\), the perturbation kernel from \(x\) to \(x_{t}\) is Gaussian and takes the form

\[p(x_{t} x)=(x_{t}\,_{t}\,x,_{t}) \]

where \(_{t}\) and \(_{t}=_{t}^{2}I\) are derived from \(f_{t}\) and \(g_{t}\)[47; 48; 49; 46]. Crucially, the forward SDE (1) has an associated family of reverse SDEs [47; 48; 49; 46]

\[x_{t}=[f_{t}\,x_{t}-}{2}g_{t}^{2}\,_{x_{ t}} p(x_{t})]t+\,g_{t}\,w_{t} \]

where \( 0\) is a parameter controlling stochasticity. In other words, we can draw noise samples \(x_{1} p(x_{1})(0,_{1})\) and gradually remove the noise therein to obtain data samples \(x_{0} p(x_{0}) p(x)\) by simulating Eq. (3) from \(t=1\) to \(0\) using an appropriate discretization scheme [49; 45; 46; 49; 50; 51; 52]. In this work, we adopt the variance exploding SDE  for which \(f_{t}=0\) and \(_{t}=1\).

In practice, the score function \(_{x_{t}} p(x_{t})\) in Eq. (3) is unknown, but can be approximated by a neural network trained via denoising score matching [53; 54]. Several equivalent parameterizations and objectives have been proposed for this task [46; 50; 51; 16; 45; 52]. In this work, we adopt the denoiser parameterization \(d_{}(x_{t},t)\) and its objective 

\[_{}_{p(x)p(t)p(x_{t}|x)}[_{t}\|d_{ }(x_{t},t)-x\|_{2}^{2}]\,, \]

for which the optimal denoiser is the mean \([x x_{t}]\) of \(p(x x_{t})\). Importantly, \([x x_{t}]\) is linked to the score function through Tweedie's formula [55; 56; 57; 58]

\[[x x_{t}]=x_{t}+_{t}_{x_{t}} p(x_{t})\,, \]

which allows to use \(s_{}(x_{t})=_{t}^{-1}(d_{}(x_{t},t)-x_{t})\) as a score estimate in Eq. (3).

## 3 Expectation-Maximization

The objective of the expectation-maximization (EM) algorithm [40; 41; 42; 43; 44] is to find the parameters \(\) of a latent variable model \(q_{}(x,y)\) that maximize the log-evidence \( q_{}(y)\) of an observation \(y\). For a distribution of observations \(p(y)\), the objective is to maximize the expected log-evidence [43; 44] or, equivalently, to minimize the Kullback-Leibler (KL) divergence between \(p(y)\) and \(q_{}(y)\). That is,

\[^{*} =_{}_{p(y)} q_{}(y) \] \[=_{}p(y)\,\|\,q_{}(y)\,. \]

The key idea behind the EM algorithm is that for any two sets of parameters \(_{a}\) and \(_{b}\), we have

\[}(y)}{q_{_{b}}(y)} =}(x,y)}{q_{_{b}}(x,y)}+ {q_{_{b}}(x y)}{q_{_{a}}(x y)} \] \[=_{q_{_{b}}(x|y)}[}( x,y)}{q_{_{b}}(x,y)}]+q_{_{b}}(x y)\,\|\,q_{ _{a}}(x y)\] (9) \[_{q_{_{b}}(x|y)} q_{_{a}}(x,y)- q_{_{b}}(x,y)\,. \]This inequality also holds in expectation over \(p(y)\). Therefore, starting from arbitrary parameters \(_{0}\), the EM update

\[_{k+1} =_{}_{p(y)}_{q_{_{k}}(x|y) } q_{}(x,y)- q_{_{k}}(x,y) \] \[=_{}_{p(y)}_{q_{_{k}}(x|y) } q_{}(x,y) \]

leads to a sequence of parameters \(_{k}\) for which the expected log-evidence \(_{p(y)} q_{_{k}}(y)\) is monotonically increasing and converges to a local optimum [42; 43; 44].

When the expectation in Eq. (12) is intractable, many have proposed to use Monte Carlo approximations instead [59; 60; 61; 62; 63; 64; 65; 66]. Previous approaches include Markov chain Monte Carlo (MCMC) sampling, importance sampling, rejection sampling and their variations [63; 64; 65; 66]. A perhaps surprising advantage of Monte Carlo EM (MCEM) algorithms is that they may be able to overcome local optimum traps [60; 61]. We refer the reader to Ruth  for a recent review of MCEM algorithms.

## 4 Methods

Although rarely mentioned in the literature, the expectation-maximization algorithm is a possible solution to the empirical Bayes problem. Indeed, both have the same objective: minimizing the KL between the empirical distribution of observations \(p(y)\) and the evidence \(q_{}(y)\). In the empirical Bayes setting, the forward model \(p(y x)\) is known and only the parameters of the prior \(q_{}(x)\) should be optimized. In this case, Eq. (12) becomes

\[_{k+1} =_{}_{p(y)}_{q_{_{k}}(x|y) } q_{}(x)+ p(y x) \] \[=_{}_{p(y)}_{q_{_{k}}(x|y) } q_{}(x)\] (14) \[=_{}_{k}(x)\,\|\,q_{ }(x) \]

where \(_{k}(x)= q_{_{k}}(x y)\,p(y)\,y\). Intuitively, \(_{k}(x)\) and therefore \(q_{_{k,j+1}}(x)\) assign more density to latents \(x\) which are consistent with observations \(y p(y)\) than \(q_{_{k}}(x)\). In this work, we consider a special case of the empirical Bayes problem where each observation \(y\) has an associated measurement matrix \(A\) and the forward process takes a linear Gaussian form \(p(y x,A)=(y Ax,_{y})\). This formulation allows the forward process to be potentially different for each observation \(y\). For example, if the position or environment of a sensor changes, the measurement matrix \(A\) may also change, which leads to an empirical distribution of pairs \((y,A) p(y,A)\). As a result, \(_{k}(x)\) in Eq. (15) becomes \(_{k}(x)= q_{_{k}}(x y,A)\,p(y,A)\,y\).

### Pipeline

Now that our goals and assumptions are set, we present our method to learn a diffusion model \(q_{}(x)\) for the latent \(x\) from observations \(y\) by expectation-maximization. The idea is to decompose Eq. (15) into (i) generating a dataset of i.i.d. samples from \(_{k}(x)\) and (ii) training \(q_{_{k+1}}(x)\) to reproduce the generated dataset. We summarize the pipeline in Algorithms 1, 2 and 3, provided in Appendix A due to space constraints.

ExpectationTo draw from \(_{k}(x)\), we first sample a pair \((y,A) p(y,A)\) and then generate \(x q_{_{k}}(x y,A)\) from the posterior. Unlike previous MCEM algorithms that rely on expensive and hard to tune sampling strategies [63; 64; 65; 66], the use of a diffusion model enables efficient and embarrassingly parallelizable posterior sampling [21; 22; 23]. However, the quality of posterior samples is critical for the EM algorithm to converge properly [63; 64; 65; 66] and, in this regard, we find previous posterior sampling methods [21; 22; 23; 25; 26] to be unsatisfactory. Therefore, we propose an improved posterior sampling scheme, named moment matching posterior sampling (MMPS), which we present and motivate in Section 4.2. We evaluate MMPS independently from the context of learning from observations in Appendix E.

MaximizationWe parameterize our diffusion model \(q_{}(x)\) by a denoiser network \(d_{}(x_{t},t)\) and train it via denoising score matching [53; 54], as presented in Section 2. To accelerate the training, we start each iteration with the previous parameters \(_{k}\).

InitializationAn important part of our pipeline is the initial prior \(q_{0}(x)\). Any initial prior leads to a local optimum [42; 43; 44], but an informed initial prior can reduce the number of iterations until convergence. In our experiments, we take a Gaussian distribution \((x_{x},_{x})\) as initial prior and fit its mean and covariance by - you guessed it! - expectation-maximization. Fitting a Gaussian distribution by EM is very fast as the maximization step can be conducted in closed-form, especially for low-rank covariance approximations .

An alternative we do not explore in this work would be to use a pre-trained diffusion model as initial prior. Pre-training can be contacted on data we expect to be similar to the latents, such as computer simulations or even video games. The more similar, the faster the EM algorithm converges. However, if the initial prior \(q_{0}(x)\) does not cover latents that are otherwise plausible under the observations, the following priors \(q_{_{k}}(x)\) may not cover these latents either. A conservative initial prior is therefore preferable for scientific applications.

### Moment Matching Posterior Sampling

To sample from the posterior distribution \(q_{}(x y) q_{}(x)\,p(y x)\) of our diffusion prior \(q_{}(x)\), we have to estimate the posterior score \(_{x_{t}} q_{}(x_{t} y)\) and plug it into the reverse SDE (3). In this section, we propose and motivate an improved approximation for the posterior score. As this contribution is not bound to the context of EM, we temporarily switch back to the notations of Section 2 where our prior is denoted \(p(x)\) instead of \(q_{}(x)\).

Diffusion posterior samplingThanks to Bayes' rule, the posterior score \(_{x_{t}} p(x_{t} y)\) can be decomposed into two terms [17; 18; 21; 22; 23; 24; 46]

\[_{x_{t}} p(x_{t} y)=_{x_{t}} p(x_{t})+_{x_{t}}  p(y x_{t})\,. \]

As an estimate of the prior score \(_{x_{t}} p(x_{t})\) is already available via the denoiser \(d_{}(x_{t},t)\), the remaining task is to estimate the likelihood score \(_{x_{t}} p(y x_{t})\). Assuming a differentiable measurement function \(\) and a Gaussian forward process \(p(y x)=(y(x),_{y})\), Chung et al.  propose the approximation

\[p(y x_{t})= p(y x)\,p(x x_{t})\,x (y([x x_{t}]),_{y}) \]

which allows to estimate the likelihood score \(_{x_{t}} p(y x_{t})\) without training any other network than \(d_{}(x_{t},t)[x x_{t}]\). The motivation behind Eq. (17) is that, when \(_{t}\) is small, assuming that \(p(x x_{t})\) is narrowly concentrated around its mean \([x x_{t}]\) is reasonable. However, this approximation is very poor when \(_{t}\) is not negligible. Consequently, DPS  is unstable, does not properly cover the support of the posterior \(p(x y)\) and often leads to samples \(x\) which are inconsistent with the observation \(y\)[22; 23; 24; 25].

Moment matchingTo address these flaws, following studies [22; 23; 24; 25] take the covariance \([x x_{t}]\) into account when estimating the likelihood score \(_{x_{t}} p(y x_{t})\). Specifically, they consider the Gaussian approximation

\[q(x x_{t})=(x[x x_{t}],[x x _{t}]) \]

which is closest to \(p(x x_{t})\) in Kullback-Leibler (KL) divergence . Then, assuming a linear Gaussian forward process \(p(y x)=(y Ax,_{y})\), we obtain 

\[q(y x_{t})= p(y x)\,q(x x_{t})\,x=(y  A[x x_{t}],_{y}+A[x x_{t}]A^{}) \]

which allows to estimate the likelihood score \(_{x_{t}} p(y x_{t})\) as

\[_{x_{t}} q(y x_{t})=_{x_{t}}[x x_{t}]^{} A^{}_{y}+A[x x_{t}]A^{}^{-1}y-A [x x_{t}] \]

under the assumption that the derivative of \([x x_{t}]\) with respect to \(x_{t}\) is negligible [24; 25]. Even with simple heuristics for \([x x_{t}]\), such as \(_{t}\) or \((_{t}^{-1}+_{x}^{-1})^{-1}\)[22; 23], this adaptation leads to significantly more stable sampling and better coverage of the posterior \(p(x y)\) than DPS . However, we find that heuristics lead to overly dispersed posteriors \(q(x_{t} y) p(x_{t})\,q(y x_{t})\) in the presence of local covariances - _i.e._ covariances in the neighborhood of \(x_{t}\).

We illustrate this behavior in Figure 1 and measure its impact as the Sinkhorn divergence  between the posteriors \(p(x_{t} y)\) and \(q(x_{t} y)\) when the prior \(p(x)\) lies on randomly generated 1-dimensional manifolds  embedded in \(^{3}\). The prior \(p(x)\) is modeled as a mixture of isotropic Gaussians centered around points of the manifold, which gives access to \(p(x_{t})\), \([x x_{t}]\) and \([x x_{t}]\) analytically. The results, presented in Figure 2, indicate that using \([x x_{t}]\) instead of heuristics leads to orders of magnitude more accurate posteriors \(q(x_{t} y)\). We expect this gap to further increase with real high-dimensional data as the latter often lies along low-dimensional manifolds and, therefore, presents strong local covariances.

Because the MCEM algorithm is sensitive to the accuracy of posterior samples , we choose to estimate \([x x_{t}]\) using Tweedie's covariance formula 

\[[x x_{t}] =_{t}+_{t}\,^{2}_{x_{t}} p(x_{t})\,_{t} \] \[=_{t}^{}_{x_{t}}[x x_{t}] _{t}^{}_{x_{t}}d_{}(x_{t},t)\,. \]

Conjugate gradient methodAs noted by Finzi et al. , explicitly computing and materializing the Jacobian \(^{}_{x_{t}}d_{}(x_{t},t)^{N N}\) is intractable in high dimension. Furthermore, even if we had access to \([x x_{t}]\), naively computing the inverse of the matrix \(_{y}+A[x x_{t}]A^{}\) in Eq. (20) would still be intractable. Fortunately, we observe that the matrix \(_{y}+A[x x_{t}]A^{}\) is symmetric positive definite (SPD) and, therefore, compatible with the conjugate gradient (CG) method . The CG method is an iterative algorithm to solve linear systems of the form \(Mv=b\) where the SPD matrix \(M\) and the vector \(b\) are known. Importantly, the CG method only requires implicit access to \(M\) through an operator that performs the matrix-vector product \(Mv\) given a vector \(v\). In our case, the linear system to solve is

\[y-A[x x_{t}] =(_{y}+A[x x_{t}]A^{})v \] \[=_{y}v+AA\,_{t}^{ }_{x_{t}}[x x_{t}]}_{}^{ }\,. \]

Within automatic differentiation frameworks , the vector-Jacobian product in the right-hand side can be cheaply evaluated. In practice, due to numerical errors and imperfect training, the Jacobian

Figure 1: Illustration of the posterior \(q(x y)\) for the Gaussian approximation \(q(x x_{t})\) when the prior \(p(x)\) lies on a manifold. Ellipses represent \(95\,\%\) credible regions of \(q(x x_{t})\). (**A**) With \(_{t}\) as heuristic for \([x x_{t}]\), any \(x_{t}\) whose mean \([x x_{t}]\) is close to the plane \(y=Ax\) is considered likely. (**B**) With \([x x_{t}]\), more regions are correctly pruned. (**C**) Ground-truth \(p(x_{t} y)\) and \(p(x x_{t})\) for reference.

Figure 2: Sinkhorn divergence  between the posteriors \(p(x_{t} y)\) and \(q(x_{t} y)\) for different heuristics of \([x x_{t}]\) when the prior \(p(x)\) lies on 1-d manifolds embedded in \(^{3}\). Lines and shades represent the 25-50-75 percentiles for 64 randomly generated manifolds  and measurement matrices \(A^{1 3}\). Using \([x x_{t}]\) instead of heuristics leads to orders of magnitude more accurate posteriors \(q(x_{t} y)\).

\(_{x}^{T}d_{}(x_{t},t)_{x}^{T}[x x_{t}]\) is not always perfectly SPD. Consequently, the CG method becomes unstable after a number of iterations and fails to reach an exact solution. Fortunately, we find that truncating the CG algorithm to very few iterations (1 to 3) already leads to significant improvements over using heuristics for the covariance \([x x_{t}]\). Alternatively, the CG method can be replaced by other iterative algorithms that can solve non-symmetric non-definite linear systems, such as GMRES  or BiCGSTAB , at the cost of slower convergence.

## 5 Results

We conduct three experiments to demonstrate the effectiveness of our method. We design the first experiment around a low-dimensional latent variable \(x\) whose ground-truth distribution \(p(x)\) is known. In this setting, we can use asymptotically exact sampling schemes such as predictor-corrector sampling [23; 46] or twisted diffusion sampling  without worrying about their computational cost. This allows us to validate our expectation-maximization pipeline (see Algorithm 1) in the limit of (almost) exact posterior sampling. The remaining experiments target two benchmarks from previous studies: corrupted CIFAR-10 and accelerated MRI. These tasks provide a good understanding of how our method would perform in typical empirical Bayes applications with limited data and compute.

### Low-dimensional manifold

In this experiment, the latent variable \(x^{5} p(x)\) lies on a random 1-dimensional manifold embedded in \(^{5}\) represented in Figure 7. Each observation \(y^{2}(y Ax,_{y})\) is the result of a random linear projection of a latent \(x\) plus isotropic Gaussian noise (\(_{y}=10^{-4}I\)). The rows of the measurement matrix \(A^{2 5}\) are drawn uniformly from the unit sphere \(^{4}\). We note that observing all push-forward distributions \(p(u^{}x)\) with \(u^{N-1}\) of a distribution \(p(x)\) in \(^{N}\) is sufficient to recover \(p(x)\) in theory [78; 79]. In practice, we generate a finite training set of \(2^{16}\) pairs \((y,A)\).

We train a DM \(q_{}(x)\) parameterized by a multi-layer perceptron \(d_{}(x_{t},t)\) for \(K=32\) EM iterations. We apply Algorithm 3 to estimate the posterior score \(_{x_{t}} q_{}(x_{t} y,A)\), but rely on the predictor-corrector [23; 46] sampling scheme with a large number (4096) of correction steps to sample from the posterior \(q_{}(x y,A)\). We provide additional details such as noise schedule, network architectures, and learning rate in Appendix C.

As expected, the model \(q_{_{k}}(x)\) converges towards a stationary distribution whose marginals are close to the marginals of the ground-truth \(p(x)\), as illustrated in Figure 3. We attribute the remaining artifacts to finite data and inaccuracies in our sampling scheme.

Figure 3: Illustration of 2-d marginals of the model \(q_{_{k}}(x)\) along the EM iterations. The initial Gaussian prior \(q_{0}(x)\) leads to a very dispersed first model \(q_{_{1}}(x)\). The EM algorithm gradually prunes the density regions which are inconsistent with observations, until it reaches a stationary distribution. The marginals of the final distribution are close to the marginals of the ground-truth distribution.

[MISSING_PAGE_FAIL:7]

entire \(k\)-space can be time-consuming and expensive. Accelerated MRI  consists in inferring the scanned object based on partial, possibly randomized and noisy, frequency measurements.

In this experiment, following Kawar et al. , we consider the single-coil knee MRI scans from the fastMRI  dataset. We treat each slice between the 10th and 40th of each scan as an independent latent variable \(x\), represented as a \(320 320\) gray-scale image. Scans are min-max normalized such that pixel values range between \(-2\) and \(2\). A single observation \(y\) is generated for each slice \(x\) by first applying the discrete Fourier transform and then a random horizontal frequency sub-sampling with acceleration factor \(R=6\), meaning that a proportion \(}{{R}}\) of all frequencies are observed on average. Eventually, we obtain \(24\,853\)\(k\)-space observations to which we add isotropic Gaussian noise (\(_{y}=10^{-4}I\)) to match Kawar et al. .

Once again, we train a DM \(q_{}(x)\) parameterized by a U-Net  inspired network \(d_{}(x_{t},t)\) for \(K=16\) EM iterations. We apply Algorithm 2 with \(T=64\) discretization steps and \(=1\) to approximately sample from the posterior \(q_{}(x y,A)\) and truncate the conjugate gradient method in Algorithm 4 to 3 iterations. After training, we employ our final model \(q_{_{K}}(x)\) as a diffusion prior for accelerated MRI. Thanks to our moment matching posterior sampling, we are able to infer plausible scans at acceleration factors up to \(R=32\), as shown in Figure 6. Our samples are noticeably more detailed than the ones of Kawar et al. . We choose not to report the PSNR/SSIM of our samples as these metrics only make sense for regression tasks and unfairly penalize proper generative models . We provide prior samples in Figure 13 and posterior samples for another kind of forward process in Figure 14.

## 6 Related Work

Empirical BayesA number of previous studies have investigated the use of deep learning to solve the empirical Bayes problem. Louppe et al.  use adversarial training for learning a prior

Figure 6: Examples of posterior samples for accelerated MRI using a diffusion prior trained from \(k\)-space observations only. Posterior samples are detailed and present plausible variations, while remaining consistent with the observation. We provide the zero-filled inverse, where missing frequencies are set to zero, as baseline.

distribution that reproduces the empirical distribution of observations when pushed through a non-differentiable black-box forward process. Dockhorn et al.  use normalizing flows  to estimate the prior density by variational inference when the forward process consists of additive noise. Vandegar et al.  also use normalizing flows but consider black-box forward processes for which the likelihood \(p(y x)\) is intractable. They note that empirical Bayes is an ill-posed problem in that distinct prior distributions may result in the same distribution over observations. Vetter et al.  address this issue by targeting the prior distribution of maximum entropy while minimizing the sliced-Wasserstein distance  with the empirical distribution of observations. These methods rely on generative models \(q_{}(x)\) for which the density \(q_{}(x)\) or samples \(x q_{}(x)\) are differentiable with respect to the parameters \(\), which is not or hardly the case for diffusion models.

Closer to this work, Daras et al.  and Kawar et al.  attempt to train DMs from linear observations only. Daras et al.  consider noiseless observations of the form \(y=Ax\) and train a network \(d_{}(Ax_{t},A,t)\) to approximate \([x Ax_{t}]\) under the assumption that \([A^{}A]\) is full-rank. The authors argue that \([x Ax_{t}]\) can act as a surrogate for \([x x_{t}]\). Similarly, Kawar et al.  assume Gaussian observations \(y(y Ax,_{y})\) and train a network \(d_{}(Px_{t},t)\) to approximate \([x Px_{t}]\) under the assumption that \([P]\) is full-rank where \(P=A^{+}A\) and \(A^{+}\) is the Moore-Penrose pseudo-inverse of \(A\). The authors assume that \(d_{}(Px_{t},t)\) can generalize to \(P=I\), even if the training data does not contain \(P=I\). In both cases, the trained networks are not proper denoisers approximating \([x x_{t}]\) and cannot reliably parameterize a standard diffusion model, which is problematic for downstream applications. Notably, in the case of Bayesian inference, they require custom posterior sampling schemes such as the one proposed by Aali et al.  for AmbientDiffusion  models. Conversely, in this work, we do not make modifications to the denoising score matching objective , which guarantees a proper DM that is compatible with any posterior sampling scheme at every iteration. In addition, we find that our method leads to quantitatively and qualitatively better diffusion priors.

In a concurrent work, Daras et al.  propose an algorithm to train DMs from noisy (\(A=I\) and \(_{y}=_{y}^{2}I\)) data by enforcing the "consistency" of the denoiser along diffusion paths. They prove that the mean \([x x_{t}]\) is the unique consistent denoiser. Interestingly, this training algorithm also relies on posterior samples, which are easy to obtain thanks to the white noise assumption.

Posterior samplingRecently, there has been much work on conditional generation using unconditional diffusion models, most of which adopt the posterior score decomposition in Eq. (16). As covered in Section 4.2, Chung et al.  propose an analytical approximation for the likelihood score \(_{x_{t}} p(y x_{t})\) when the forward process \(p(y x)\) is Gaussian. Song et al.  and Rozet et al.  improve this approximation by taking the covariance \([x x_{t}]\) into account in the form of simple heuristics. We build upon this idea and replace heuristics with a proper estimate of the covariance \([x x_{t}]\) based on Tweedie's covariance formula . Finzi et al.  take the same approach, but materialize the matrix \(A[x x_{t}]A^{}\) which is intractable in high dimension. Boys et al.  replace the covariance \([x x_{t}]\) with a row-sum approximation \((e^{}[x x_{t}])\) where \(e\) is the all-ones vector. This approximation is only valid when \([x x_{t}]\) is diagonal, which limits its applicability. Instead, we take advantage of the conjugate gradient method  to avoid materializing \(A[x x_{t}]A^{}\). A potential cheaper solution is to train a sparse approximation of \([x x_{t}]\), as proposed by Peng et al. , but this approach is less general and not immediately applicable to any diffusion model.

A parallel line of work  extends the conditioning of diffusion models to arbitrary loss terms \((x,y)- p(y x)\), for which no reliable analytical approximation of the likelihood score \(_{x_{t}} p(y x_{t})\) exists. Song et al.  rely on Monte Carlo approximations of the likelihood \(p(y x_{t})= p(y x)\,p(x x_{t})\,x\) by sampling from a Gaussian approximation of \(p(x x_{t})\). Conversely, He et al.  use the mean \([x x_{t}]\) as a point estimate for \(p(x x_{t})\), but leverage a pre-trained encoder-decoder pair to project the updates of \(x_{t}\) within its manifold. We note that our use of the covariance \([x x_{t}]\) similarly leads to updates tangent to the manifold of \(x_{t}\).

Finally, Wu et al.  propose a particle-based posterior sampling scheme that is asymptotically exact in the limit of infinitely many particles as long as the likelihood approximation \(q(y x_{t})\) - here named the _twisting_ function - converges to \(p(y x)\) as \(t\) approaches \(0\). Using TDS  as part of our expectation-maximization pipeline could lead to better results and/or faster convergence, at the cost of computational resources. In addition, the authors note that the efficiency of TDS  dependson how closely the twisting function approximates the exact likelihood. In this regard, our moment matching Gaussian approximation in Eq. (19) could be a good twisting candidate.

## 7 Discussion

To the best of our knowledge, we are the first to formalize the training of diffusion models from corrupted observations as an empirical Bayes  problem. In this work, we limit our analysis to linear Gaussian forward processes to take advantage of accurate and efficient high-dimensional posterior sampling schemes. This contrasts with typical empirical Bayes methods which target low-dimensional latent spaces and highly non-linear forward processes . In addition, as mentionned in Section 6, these EB methods are not applicable to diffusion models. As such, we choose to benchmark our work against similar methods in the diffusion model literature , but stress that a proper comparison with previous empirical Bayes methods would be valuable for both communities. We also note that Monte Carlo EM  can handle arbitrary forward processes \(p(y x)\) as long as one can sample from the posterior \(q_{}(x y)\). Therefore, our method could be adapted to any kind of forward processes in the future. We believe that the works of Dhariwal et al.  and Ho et al.  on diffusion guidance are good avenues for adapting our method to non-linear, non-differentiable, or even black-box forward processes.

From a computational perspective, the iterative nature of our expectation-maximization method is a drawback compared to previous works . Notably, generating enough samples from the posterior can be expensive, although embarrassingly parallelizable. In addition, even though the architecture and training of the model \(q_{}(x)\) itself are simpler than in previous works , the sampling step adds a significant amount of complexity, especially as the convergence of our method is sensitive to the quality of posterior samples. In fact, we find that previous posterior sampling methods  lead to disappointing results, which motivates us to develop a better one.

As such, moment matching posterior sampling (MMPS) is a byproduct of our work. However, it is not bound to the context of learning from observations and is applicable to any linear inverse problem given a pre-trained diffusion prior. In Appendix E, we evaluate MMPS against previous posterior sampling methods for several linear inverse problems on the FFHQ  dataset. We find that MMPS consistently outperforms previous methods, both qualitatively and quantitatively. MMPS is remarkably stable and requires fewer sampling steps to generate qualitative samples, which largely makes up for its slightly higher step cost.

Finally, as mentioned in Section 6, empirical Bayes is an ill-posed problem in that distinct prior distributions may result in the same distribution over observations. In other words, it is generally impossible to identify "the" ground-truth distribution \(p(x)\) given an empirical distribution of observations \(p(y)\). Instead, for a sufficiently expressive diffusion model, our EM method will eventually converge to a prior \(q_{}(x)\) that is consistent with \(p(y)\), but generally different from \(p(x)\). Following the maximum entropy principle, as advocated by Vetter et al. , is left to future work.

Francois Rozet and Gerome Andry are research fellows of the F.R.S.-FNRS (Belgium) and acknowledge its financial support.

The present research benefited from computational resources made available on Lucia, the Tier-1 supercomputer of the Walloon Region, infrastructure funded by the Walloon Region under the grant n\({}^{}\)1910247. The computational resources have been provided by the Consortium des Equipements de Calcul Intensif (CECI), funded by the Fonds de la Recherche Scientifique de Belgique (F.R.S.-FNRS) under the grant n\({}^{}\)2.5020.11 and by the Walloon Region.

MRI data used in the preparation of this article were obtained from the NYU fastMRI Initiative database . As such, NYU fastMRI investigators provided data but did not participate in analysis or writing of this report. A listing of NYU fastMRI investigators, subject to updates, can be found at [https://fastmri.med.nyu.edu/](https://fastmri.med.nyu.edu/). The primary goal of fastMRI is to test whether machine learning can aid in the reconstruction of medical images.