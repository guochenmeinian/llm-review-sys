# On scalable oversight with weak LLMs judging strong LLMs

Zachary Kenton

Equal Contribution. Contact {zkenton,siegeln}@google.com. All authors Google DeepMind.

Noah Y. Siegel1

Janos Kramar

Jonah Brown-Cohen

Samuel Albanie

Jannis Bulian

Rishabh Agarwal

David Lindner

Yunhao Tang

Noah D. Goodman

Rohin Shah

Equal Contribution. Contact {zkenton,siegeln}@google.com. All authors Google DeepMind.

###### Abstract

Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. In this paper we study _debate_, where two AI's compete to convince a judge; _consultancy_, where a single AI tries to convince a judge that asks questions; and compare to a baseline of _direct question-answering_, where the judge just answers outright without the AI. We use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.

## 1 Introduction

If the current practice of using human feedback for alignment is to continue, that feedback will need to be _accurate_ even as AIs reach and eventually exceed expert human levels on important tasks. One solution to this problem is _scalable oversight_ - identifying training protocols that leverage advancing AI capabilities to allow humans to provide accurate training signals to superhuman AI .

Scalable oversight is especially important for the safety of superhuman AI systems. Denison et al.  recently showed that current large language models (LLMs) can generalise behaviours which exploit inaccurate training signals: generalising from simple behaviours, such as sycophancy, to more complex ones, such as reward tampering, in which the model modifies its own reward administration. One hypothesis is that more powerful AI may generalise these behaviours further to even more complex and dangerous exploits, such as _scheming_, in which an AI that is performing well in training will be doing so in order to gain power later; to the extent this is true, improving the quality of oversight can reduce the chance that scheming arises.

The debate proposal of  introduced the idea that a debate between two equally capable AIs can inform a less capable human judge about the merits and flaws of each side enough for the judge to select the correct answer, with the hope that in all the Nash equilibria of the debate game, both debaters tell the truth in the most convincing manner possible. From a theoretical perspective, work connecting debate protocols to interactive proofs in computational complexity theory has shown that debate can allow for tasks of significant complexity to be accurately and efficiently supervised by much more limited judges (Irving et al., 2018; Brown-Cohen et al., 2023).

But the theoretical arguments leave room for the empirical question: does debate enable less capable judges to accurately supervise stronger AI agents in practice? Attempts to answer this question are still in early stages (Michael et al., 2023; Khan et al., 2024). Thus far, debate between LLMs has been primarily studied on a reading comprehension task, where the judge is _artificially_ limited by only being allowed to see short verified quotes from the text provided by the LLM debaters. While providing initial promising evidence on the power of debate, the use of this type of information asymmetry limits the ability to fully evaluate the qualitative advantages of debate predicted by theory. The long-run difference between superhuman AIs and humans is likely to extend to differences along many capabilities, including mathematical and logical reasoning, creativity, and factual knowledge.

Figure 1: Task types and protocols.

Figure 2: Assigned-role results: mean judge accuracy (y-axis) split by task type (facet), judge model (x-axis), protocol (colour). Higher is better. 95% CI calculated aggregated over tasks of same type (Appendix D for details). The QA with article protocol (purple) can only be applied for extractive tasks. Only Pro 1.5 is multimodal.

Thus, the gap between the judge and debaters in empirical research should ideally arise from differences in various capabilities, not only information asymmetry. In this paper, we study debate on a set of tasks that better cover a broad range of capability gaps that must be bridged via scalable oversight. Our setup is described in Figure 1. The tasks are constructed from binary-choice versions of multiple-choice question answering datasets, and include extractive QA, closed QA and multimodal reasoning tasks. In order to control the gap between the more capable models and weaker judges, in our main evaluation experiments we vary the strength of the judge to be equal or weaker than the models it is judging.

For each task we study, two categories of scalable oversight protocols are tested: _consultancy_ - where the judge LLM questions a single "consultant" LLM that argues for one of the two options, and _debate_ - where the judge LLM answers the question after viewing a debate between two "debater" LLMs arguing for opposite answers. For consultancy, for each question, we follow Michael et al. ; Khan et al.  by assigning the consultant to argue separately for both the correct and incorrect answer. This is designed to simulate a worst-case variant of reinforcement learning from human feedback (RLHF) in which human feedback can't distinguish correct/incorrect answers. Thus, the comparison between the accuracy of a less capable judge in consultancy and debate enables us to empirically test the question of whether debate between capable AI agents can enable more accurate judgment. We also compare to direct QA baselines (QA with/without article), see caption of Figure 1. For judge models we use Gemma7B (V1) , GPT-3.5 , Gemini Pro 1.0 , and Gemini Pro 1.5 . Our main experiments use Gemini Pro 1.5 as debaters/consultants.

Going beyond judge accuracy, if a scalable oversight protocol is to be used as a training method, it is also important to measure the degree to which the protocol will amplify existing mistakes. That is, if the AI agent already believes a particular answer to be correct, will they be able to persuade a less capable judge of this regardless of the ground-truth correctness? To study this question we also consider _open consultancy_, in which the consultant chooses their answer, rather than being assigned one; and introduce _open debate_ where we track the performance of a debater (the protagonist) which is assigned the answer which the same debater model would choose under the direct QA baseline. Finally, the models used as judge, consultant, and debater are chosen so as to vary the gap in capabilities between the judge and the debaters/consultants.

Our study investigates the scalable oversight setup along three different dimensions: (1) the task, (2) the oversight protocols, and (3) the capabilities gap between the weak LLM judges and the strong LLMs. We perform a large-scale evaluation sweeping over 9 tasks, each sampling 128 questions, totalling approximately 5 million model generation calls, affording us insight on which aspects of our study are practically significant. Our findings are as follows, see Figure 2:

1. We find that debate consistently outperforms consultancy across all tasks, previously only shown on a single extractive QA task in Khan et al. .
2. Comparing debate to direct question answering baselines, the results depend on the type of task. In extractive QA tasks with information asymmetry, debate outperforms QA without article as in the single task of Khan et al. , but not QA with article. For other tasks, when the judge is weaker than the debaters (but not too weak), we find either small or no advantage to debate over QA without article.
3. Changes to the setup (number of turns, best-of-N sampling, few-shot, chain-of-thought) seem to have little effect on results.
4. In open consultancy, the judge is equally convinced by the consultant, whether or not the consultant has chosen to argue for the correct answer. Thus, using weak judges to provide a training signal via consultancy runs the risk of amplifying the consultant's incorrect behavior. See Figure 3.
5. In open debate, in contrast, the judge follows the debater's choice less frequently than in open consultancy. When the debater chooses correctly, the judge does a bit worse than in open consultancy. But when the debater chooses incorrectly, the judge does a lot better at discerning this. Thus, the training signal provided by the weak judge in open debate is less likely to amplify incorrect answers than in open consultancy.
6. Comparing across debater models/settings, we calculate Elo scores and show that stronger debaters lead to higher judge accuracy (including for a weaker judge) across a range of tasks, see Figure 4. This provides some evidence that debate satisfies a key objective of scalable oversight - that judge accuracy increases as AI capabilities scale.

Interpreting what these results indicate for debate is somewhat complicated. That debate beats consultancy is encouraging, though it's likely that consultancy is a weak baseline to beat because of the 50/50 chance of the consultant arguing for the incorrect answer. Strong positive evidence favouring debate would be that it is significantly stronger than the QA without article baseline, and even that it is as strong/stronger than QA with article (on extractive tasks), and we see neither. However, there are some important caveats that mean we remain optimistic about the prospects for debate as a scalable oversight protocol.

Firstly, we do not provide direct evidence regarding debate as a _training_ protocol - which is a major motivation for studying debate - though we think it is sensible to check first the inference properties of debate, as in this paper, before using it for training purposes. Future work should investigate training debaters via self-play using the judgment as a reward signal.

Secondly, we hypothesise that current finetuning approaches applied to the models we use as judges, supervised finetuning (SFT) and RLHF, favour direct QA (QA without article, QA with article) over debate, as direct QA is typically the format of evaluation benchmarks which are used to select finetuning approaches, and which may be more common in the finetuning data (e.g. users typically ask questions and expect an answer). We suspect that judging a debate, in a discerning manner, is more out-of-distribution. This presents some future directions, such as finetuning judges on the task of judging debates, perhaps using SFT, and conducting studies with human judges to compare to.

## 2 Related Work

Empirical evaluations of debate protocolsDebate was proposed in Irving et al. . Initial experiments on the QuALITY dataset  employing human debaters and judges across one-turn  and two-turn  debates failed to significantly improve judge accuracy. Later work found debate to be effective with strong human debaters, but ineffective when those humans are replaced with GPT-4 debaters . Of particular relevance to our work, Radhakrishnan  report promising results with LLM debaters and judges with inference-time debate and RL training of debaters, as well as supervised training of the judge, on the QuALITY dataset. Khan et al.  consider a similar setup to Radhakrishnan , and is the closest work to our own. Their study primarily uses the QuALITY dataset only and uses inference-time debate (though they report some fine-tuning on human debate transcripts) with LLM debaters and LLM and human judges. Though not a main focus, they do report some limited results on other datasets without information asymmetry finding inference-time debate doesn't perform better than standard QA baselines, though they only report this for when the judge is the same model as the debaters (which is relevant in a self-improvement setting, but less so for scalable oversight).

In contrast, we conduct experiments across a broad range of tasks, for a variety of models (including open-source and multimodal), include additional oversight protocols (open debate and open consultancy), and provide more extensive ablations, resulting in different conclusions compared to Khan et al. . We find the following have little effect on judge accuracy: best-of-n sampling; few-shot prompting and chain-of-thought reasoning for judges; using both orders for the answers.

Scalable oversight evaluationsBuilding on the proposal of Cotra , Bowman et al.  formalise the notion of _sandwiching_ - in which a weaker group of humans uses a model to match the performance of a stronger group of humans, with the model's ability lying in between that of the weaker and stronger group of humans. In principle, this evaluation could be applied to any of a number of scalable oversight proposals, e.g. recursive reward modeling , iterated amplification , market making , self-critique  and weak-to-strong generalization . Bowman et al.  assess a protocol akin to our _open consultancy_ protocol. Similarly to Bowman et al. , we study a simplified sandwiching setting that exchanges the expert group for fixed ground-truth labels. However, we differ from Bowman et al.  in that we also employ LLMs instead of the weaker group of humans, to reduce cost and enable faster iteration speed, matching the approach of Radhakrishnan , Khan et al. .

LLM debaters and judgesBeyond its application to scalable oversight, LLM-based debate has also been investigated in several other contexts. These include using debate to improve LLM performance in text assessment , translation , mathematical reasoning and factual accuracy . However, Huang et al.  show that debate efficacy for reasoning tasks is no better than majority voting when considering same number of model responses. Debate has also been used to assess whether LLMs can defend a correct answer to a question in the face of criticism from a simulated user  or peer review . Another line of work has conducted LLM debates with raw token embeddings rather than natural language . Finally, a range of work has investigated the capabilities of "LLM-as-a-judge" [50; 44] demonstrating that LLMs can achieve high agreement with human judgment, albeit with systematic biases . We differ from these in that we're primarily motivated to investigate debate as a scalable oversight protocol, rather than as a self-improvement, or inference-time improvement technique. As such, we focus on evaluating _weak_ judge accuracy, rather than a judge of equal strength to the debaters.

## 3 Methods

Our study is performed on a collection of tasks in which an underlying binary-choice question answering dataset is wrapped with the requisite logic and prompt templates for evaluating various scalable oversight protocols involving LLMs. On each task, a judge LLM answers a question from the task, possibly after seeing output from debater or consultant LLMs, and judge accuracy is measured on the ground-truth dataset labels. We vary the setup along the three axes of (1) the scalable oversight protocol, (2) the QA task, and (3) the judge LLM, each of which we now describe in more detail.

TasksThere are many QA benchmarks to test the ability of language models to answer questions from different domains. We form our tasks by processing existing datasets with LLM prompt templates appropriate for each scalable oversight protocol. In general, where an underlying dataset is multiple choice, we form a binary choice version by selecting the correct answer and a random answer. We group tasks into three categories: **Extractive QA** (QuALITY , BooIQ  and GPQA-extractive ), in which a binary-choice question, along with an article, are supplied to consultants/debaters but not to the judge, introducing an information asymmetry. Debaters extract passages from the article to use in their arguments - modeling a specific kind of weak-strong gap. The debaters and judges have access to a tool which verifies whether the passage is indeed contained in the article (see Appendix G). **Closed QA** (MMLU , GSM8KQA , PrOntoQA , TruthfulQA  and GPQA ) contain questions and answers, but there is no article to use as a source, in contrast to extractive QA, and hence there is no information asymmetry. The weak-strong gaps being motivated here are more intrinsic to the models - their capability and skill/reasoning gaps. **Multimodal** (MMMU ) tasks are image+text-to-text, others are text-to-text. Judge and debaters/consultants can view the image as well as the question, without information asymmetry. The motivation for this task type is to understand the effect of an additional modality. Details of the tasks are provided in Appendix F with overview in Table F.1.

ProtocolsA scalable oversight protocol produces a training signal for a highly capable AI via supervision by a weaker judge. The theory around debate only provides guarantees on accurate supervision when the debater AIs have converged in training with the signal provided by the weak judge. In our experiments, as in prior work , we measure both judge accuracy, as well as scaling trends as capabilities gaps increase, under an inference-only setting, as a proxy for the utility of a scalable oversight protocol as a training signal. We evaluate several scalable oversight protocols, see Figure 1, primarily focused on consultancy and debate. Whenever possible, we base our setups and prompts on Khan et al. , with minor differences described in Appendix H.

The first two QA protocols (not to be confused with QA tasks) below are not scalable oversight protocols - instead they are direct QA baseline protocols to compare to. They use the (possibly weak) judge model only, getting no assistance from a stronger model. As such, one would ideally see scalable oversight protocols improve over these baselines.

**QA without article ("Naive" in Khan et al. )** As a baseline, we zero-shot prompt the judge with a question and two answer options. For closed QA tasks, this is a standard zero-shot baseline. For extractive QA tasks, the judge does not see the article, hence it will typically give poor performance unless it is possible to answer correctly without the article.

**QA with article ("Expert" in Khan et al. )** For extractive QA tasks, we prompt the judge model with a question, two answer options and the full article (note this protocol can only be run on extractive QA tasks). This gives the judge the full information necessary to answer the question, in contrast to the other protocols which in the extractive QA tasks introduce an information asymmetry between consultants/debaters and judge.

The next two protocols, consultancy and debate, are assigned-role, meaning that the consultants/debaters do not get to choose which side to argue for.

**Consultancy** Consultancy follows a similar setup to Michael et al. , Khan et al.  with a consultant and a judge. The consultant is assigned an answer choice and is prompted to argue this answer is correct. The judge is asked to evaluate the argument and ask the consultant probing questions. After a fixed number of rounds, the judge has to provide a final answer. For closed QA tasks, both the consultant and the judge model have the same information. For extractive QA tasks, only the consultant sees the full text passage and has to provide extracts to the judge to support its arguments. In our implementation, for each question we run two consultancy evaluations: one where the consultant is assigned the correct answer and one where they are assigned the incorrect answer.

**Debate ** This protocol involves two debater models (named Alice and Bob) engaging in a structured debate to convince a judge of their answer. The debaters, assigned opposing stances on a given question, present their arguments over a number of rounds (as a default we use 3 rounds). Each round allows debaters to review the evolving transcript under two settings: _simultaneous_, in which both debaters make their arguments at the same time, based on the transcript only from the previous rounds, and _sequential_, in which the player going second gets to additionally see the first player's argument in the current round. Alice's argument always appears before Bob's in the transcript. Debaters are prompted each round with standardised instructions outlining the game's mechanics, their assigned positions, and the existing transcript. A judge is then shown the transcript and requested to generate their answer. Note that we choose not to evaluate _interactive_ debate, where the judge asks questions each round as Khan et al.  found no improvement over standard debate with a human judge (their Fig. 1) or LLMs (Figs. 8 and 9).

The final two protocols are open-role, meaning the consultant and one of the debaters gets to choose which answer to argue for. Debaters/consultants are prompted to structure their output to include sections for thinking in a chain-of-thought manner , and making their argument. Only their argument is shown to the judge (the thinking is private).

**Open consultancy ** This protocol allows the consultant to argue for the answer they believe to be correct, rather than the answer being assigned (both correct and incorrect). This is more representative of the truth-seeking oversight they may be trained with. In our implementation, rather than generating new data, we filter our consultancy evaluations by selecting only those in which the assignment of the consultant matches the answer chosen by the consultant model under the format of direct QA (specifically, QA with/without article for extractive/closed tasks, respectively).

**Open debate** In open debate, a format is used where one of the debaters, designated as the _protagonist_, selects their preferred answer to the given question. The other debater, the _antagonist_ is then automatically assigned the opposing answer. In contrast to the standard debate protocol, this format offers a more realistic representation of how such a framework might be employed in AI alignment training. We reuse existing debate evaluation data and identify the protagonist as the debater whose assigned answer matches the debater model's answer under direct QA (as in open consultancy).

ModelsWe use the following models in our work: Gemma7B (V1)  (instruction tuned version, text-only), GPT-3.5  (gpt-3.5-turbo-0125), Gemini Pro 1.0  (text-only version), and Gemini Pro 1.5  (image+text-to-text version, allowing us to operate on multimodal only with this model). Gemma7B is open-source, which we hope aids reproducibility by others. GPT3.5 has been used in previous work  as a judge, enabling us to compare some results. In our first experiments, Section 4.1 and Section 4.2, we use the strongest model, Pro 1.5, as debaters/consultants, and all models as (possibly weaker) judges allowing us to study strong consultants/debaters overseen by weaker judges. In Section 4.3 we vary the debater model, and analyse it with two judges (Pro 1.0, Pro 1.5). Models are used 1-shot, with default sampling options unless otherwise specified.

## 4 Experiments

### Assigned-Role Results

We first present our findings across task types, judge models and assigned-role protocols (consultancy, debate and QA with/without article), in Figure 2; results for individual tasks can be found in Appendix B, Figure B.1. For these results we select the following settings: for consultancy/debate, we use Pro 1.5 as consultant/debaters and have 3 rounds of interaction. For debate, we use simultaneous turns with debaters selecting their responses through Best-of-\(4\): \(4\) samples are independently generated, and Pro 1.5 is prompted to select the most persuasive one (more details in Appendix F). Judges are 0-shot prompted to predict the answer given the protocol transcript.

To estimate statistical significance in the difference in performance between protocols we calculate permutation-based \(p\)-values of accuracy differences between protocols, for each task type and judge. See Figure A.1 for pairwise significance results. We now highlight some key features of our results (taking significance level \(5\%\)).

**Extractive tasks:** The results of Khan et al.  on QuALITY generalise to other extractive tasks, even while using a different set of models: stronger judge models tend to be more accurate across protocols. QA with article is best for all models, indicating that information is best used directly, even by a weak model, rather than filtered through oversight. Importantly, judges attain higher accuracy for debate compared to consultancy and QA without article - except for Gemma7B, our weakest judge, which does no better than QA without article.

**Closed tasks:** We see debate increases judge accuracy compared to consultancy on closed tasks (previously this was only seen in the extractive task, QuALITY). Debate compared to QA without article is more subtle. For Gemma7B, our weakest judge, QA without article does better than debate (as in the extractive case). For GPT3.5, debate does better than QA without article (with \(p=0.037\)). For Pro 1.0, the difference between debate and QA without article is not statistically significant (\(p=0.1\)). For Pro 1.5, when the judge is the same as the debaters, debate actually leads to worse judge accuracy than QA without article, similar to some of the findings reported in Fig 31 of Khan et al. . Our interpretation is as follows: Gemma7B is likely too weak to learn anything that valuable about these protocols. Pro 1.5 has no weak-strong asymmetry in this closed QA case, so doesn't inform much about scalable oversight. Pro 1.0 and GT3.5 are probably the most indicative of scalable oversight, and indicate either small or no benefit of debate over QA without article.

**Multimodal:** consultancy is significantly worse than other protocols. The difference between debate and QA without article is not statistically significant (\(p=0.11\)).

**Judging ability roughly correlates with reported capabilities** across all tasks, with Gemma7B a worse judge than GPT-3.5 and Pro 1.0, which are roughly similar, while the Pro 1.5 judge is better than both of those.

**Ablations** We now summarise a series of findings from ablations on various settings, with figures showing the experimental results displayed in Appendix A.

**Number of turns** We investigate what happens when we reduce the number of turns from 3 to 1 in consultancy and debate. We find no significant effect from the number of turns. This suggests, for these tasks, models and protocols, more rounds does not help the judge. This is consistent with the findings of  for QuALITY. See Figure A.2.

**Best-of-N for debaters** We consider the debate protocol and reduce the best-of-N (BoN) sampling for debater responses from \(4\) to 1, i.e. standard sampling. We do not see substantial reduction in judge accuracy, suggesting our implementation of best-of-N is not helpful. Khan et al.  don't directly report the influence of \(N\) on judge accuracy, though their Fig. 3 suggests Elo increases with \(N\) and judge accuracy increases with Elo (though perhaps less so for stronger debaters). See Figure A.3.

**Judge Few-shot prompting** We look at varying 0-shot to 6-shot prompting for the judge, balancing the order of the correct answer across examples. Overall, the results do not show significant improvement with 6-shots. See Figure A.4.

**Judge Chain-of-thought** We prompt the judge to use chain-of-thought (CoT) reasoning  before giving its answer and compare this to our default prompt instructing the judge to just give its answer (prediction). Surprisingly, we see CoT tends to harm performance or be insignificant (see Figure A.5). We hypothesise that having weaker judges use their own CoT causes them to disregard the verbalised reasoning offered by stronger consultants/debaters, which might be higher quality. Stureborg et al.  also noticed CoT degrading LLM evaluation performance.

**Debate turn style** Debates can occur with turns being either simultaneous (Alice and Bob only see each other's arguments from previous turns) or sequential (Alice first, then Bob can see Alice's response,...). In the sequential setting, Bob may have an advantage as Bob rebuts Alice's final argument but Alice can't rebut Bob's final argument. However, we don't see a significant difference between the turn styles, indicating the potential asymmetry is unimportant. See Figure A.6 (note this is under the BoN=1 setting).

**Positional bias** Khan et al.  report that judge models exhibit positional bias , systematically preferring either first or second answers under debate (more details in Appendix F). They recommend presenting each question twice with answer and debating order swapped, doubling the cost of judgment; we can achieve the same result in expectation by randomizing debating order, with slightly higher variance (Figure A.7). Further, we look at the mean of the selected answer position for each model (1 or 2), (1.5 represents no positional bias as our datasets are balanced). Judges often display positional bias, to a greater degree in debate than in other protocols. Using both orders slightly reduces the variance of positional bias estimates, see Figure A.8. This suggests it's sufficient to just randomise the answer position, rather than evaluating with both answer positions.

ImplicationsOverall, we think these results suggest debate is likely more promising as a scalable oversight protocol than consultancy, though we emphasise that our findings only apply in the inference-only setting, and that a proper evaluation of scalable oversight protocols used for _training_ may reveal other findings. A key challenge for future evaluations of debate is to show an improvement over direct QA baselines - we suspect this may be achievable through some limited fine-tuning of judges on the debate judging task, in a similar manner to how standard fine-tuning makes models more suitable at direct QA tasks. The ablation on BoN revealing no improvement is a mildly negative sign for debate as a training protocol, as BoN is motivated as a proxy for training. The ablation showing no improvement with number of turns is also a mildly negative sign, though perhaps will differ under training for debate, if models learn to use further rounds to better argue against the claim of their opponent in the prior round. Changes to the inference setup (few-shot, chain-of-thought, turn style, swapping answer order) seem to have little effect.

Appendix I.2 contains example transcripts. We find through our qualitative analysis that participants produce cogent, well-structured argumentation and reasoning across a broad range of topics. Judges rarely provide invalid responses, see Figure B.11.

### Open-Role Results

Figure 3: Open debate and open consultancy, where a protagonist debater/consultant chooses which answer to argue for. Top: Judge accuracy (y-axis) and win rate of protagonist/consultant (x-axis). Blue colours for consultancy, red colours for debate, with the shade corresponding to judge model. Bottom: Judge accuracy according to whether the protagonist/consultant chose the correct (dark) or incorrect (light) answer. Split by judge model (x-axis) and protocol: consultancy and debate. Facet is task type. 95% CIs. Correct answer rate: 88% (extractive), 84% (closed), 71% (multimodal).

We now compare open consultancy and open debate, in which a consultant/protagonist get to select an answer to argue for. In Figure 3 top we plot judge accuracy (y-axis) against protagonist/consultant win-rate (fraction for which the judge chooses the answer argued for by the protagonist/consultant). Blue colours indicate open consultancy, red colours indicate open debate, with the shade corresponding to judge model. On all tasks, open consultancy leads to greater (or equal) win-rate than open debate, meaning the judge tends to agree with the consultant's choice more in a consultancy protocol, perhaps because the consultant doesn't face as strong a challenge as in open debate (where there is an equal strength opponent). However, this doesn't always lead to higher judge accuracy. For extractive tasks, open consultancy does lead to higher judge accuracy than open debate, but in closed/multimodal we see roughly equal judge accuracy for our strongest debate judge, though lower accuracy for weaker debate judges than for corresponding consultancy judges.

To understand more how the judge is affected by the correctness of the protagonist/consultant's choice, in Figure 3 bottom we plot the judge accuracy for open consultancy and open debate, split by whether the consultant, or protagonist in debate, has chosen the correct (dark) or incorrect (light) answer. For open consultancy, the judge scores high accuracy when the consultant is correct (dark blue) but low accuracy when the consultant is incorrect (light blue). For open debate, the judge scores higher when the protagonist is correct (dark red) than incorrect (light red), but the differences are less extreme than in the open consultancy case. Perhaps most important for scalable oversight training is for the weak judge to notice when the stronger model is incorrect. In this regard, we can see the advantage of open debate here more clearly - the weak judge scores much higher accuracy in open debate when the protagonist is incorrect (light red) than in open consultancy when the consultant is incorrect (light blue). This does however trade-off with a relative decrease in accuracy when the debate protagonist is correct. In future work it would be interesting to explore a version of open debate in which the two debaters have the truly open choice, and could both choose to argue for the same answer, which may show further improvement in judge accuracy.

We compare open consultancy vs. assigned-role consultancy in Figure A.9. When consultants argue for correct answers, the accuracy of open and assigned-role consultancy are similar, whilst when arguing for incorrect answers, open consultancy leads to lower accuracy. We hypothesise this is because questions which the open consultant answers incorrectly are more difficult than average, while the assigned-role consultant's assignments are independent of difficulty.

### Persuasiveness in Debate

We now analyse the relative persuasiveness of debaters, motivated by debate as a scalable alignment training protocol. As a proxy for training models to win at the debate game, we instead consider a debater model's relative strength at persuading the judge (in inference-only debate) and investigate how that correlates with judge accuracy. Compared to Section 4.1 and Section 4.2, which only used a single model for debaters (Pro 1.5), we now use a range of debaters and investigate their persuasiveness on the judge, by measuring their cross-play win-rate (frequency that the judge chooses their answer) and modeling their latent skill with an Elo rating, studying how these correlate with

Figure 4: Plot of Pro 1.5 judge accuracy vs. debater aggregate Elo scores. 95% CIs. Judge accuracy is more sensitive to debaters on extractive tasks than closed tasks. See Figure E.1 for elo results split by debater assignment (whether debaters are assigned to argue for true vs. false positions).

judge accuracy, following a similar setup to Khan et al. . We generate a series of 1-turn arguments in simultaneous debate (i.e. just opening arguments), with five debaters: Gemma7B, GPT-3.5, Gemini Pro 1.0, Gemini Pro 1.5 (all with BoN=1), and Gemini Pro 1.5 with BoN=4. We then sample 512 (with each dataset contributing an equal number) pairings of cross-play debates, and judge them with Pro 1.5, to calculate their aggregate Elo scores.

ResultsWe look at how aggregate Elo correlates with judge accuracy (with Pro 1.5 as judge), to see if higher skilled debaters lead to improvements in judge accuracy. Figure 4 shows that on extractive tasks, stronger models have higher aggregate Elo and these lead to higher judge accuracy. On closed tasks, however, there's less differentiation between debaters, both in terms of aggregate Elo and judge accuracy. This may be due to the judge's greater dependence on debater statements in the extractive tasks, where judges rely on debaters for information they cannot access themselves. Additional results can be found in Appendix E, showing that stronger models tend to score higher Elo; that it's often advantageous to argue for the truth; however, this advantage doesn't increase with increasing Elo, contra Khan et al. , only for extractive tasks. A weaker judge (Pro 1.0) displays somewhat similar results.

## 5 Conclusion

SummaryWe conducted a multi-task (including multi-modal) study, evaluating scalable oversight protocols, consultancy and debate, and their open variations, and report analysis of large-scale experiments on it with a range of models and protocols. We found that, on previously unstudied closed QA tasks, weak judges achieve higher accuracy in the debate protocol, than consultancy, and around the same as direct question answering. Further, through implementing new scalable oversight protocols, open debate and open consultancy, we were able to further see the strengths and weaknesses of these approaches: weak judges do slightly better in consultancy compared to debate when the consultant/debater chooses correctly - however, weak judges do _much worse_ in consultancy compared to debate when the consultant chooses incorrectly. We find that, across tasks, stronger debaters (as measured by their Elo scores) lead to higher judge accuracy, but the effect was relatively weak compared to Radhakrishnan , Khan et al.  which studied the QuALITY task only. We interpret these as weakly promising signs for debate, though note that this is just a proxy for how debate will fare as a training protocol (all our experiments are done as inference-only).

LimitationsOur work studies consultancy/debate in an inference-only setting by prompting models to play these roles. Whilst providing some evidence of the models' abilities to play these roles, what we actually care about are the safety properties of the optima/equilibria that arise due to the incentives produced by training models specifically in consultancy/debate decision-problems/games. While results on win rates, and advantages arising from selecting correct vs. incorrect answers provide some evidence of their abilities to play these games, they don't give us much evidence about their effectiveness as scalable oversight training protocols. Further, the models we used as consultants/debaters were all fine-tuned with RLHF for, among other qualities, honesty, which is what debate is aiming to incentivise, and for example may hamper the dishonest consultant/debater (see discussion in Appendix C.3 of Khan et al. ). It would be interesting to perform our study in the helpful-only setting. Our work attempts to analogise the weak-strong dynamics of humans supervising superhuman AI, but our study is limited by not using humans and using AI which is far from superhuman on many capabilities. A key uncertainty is whether humans will be able to appropriately follow and accurately judge arguments made by superhuman AI.

Future workFuture work could train debater and consultant models from judge signals on these tasks to test scalable oversight as training methods. We would hope to see that both judge accuracy and debater skill on the task improve over training. The work could further be extended through a study involving human judges. Another direction is to look at other weak-strong asymmetries such as giving consultants/judges access to tool use, code execution, and different modality access. We could also investigate other scalable oversight protocols, e.g. debate with cross-examination  or iterated amplification . Further, we could study how protocols perform under distribution shift, e.g. from easy to hard tasks, and whether they are robust to misaligned models.