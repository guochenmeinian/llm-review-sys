# Policy Optimization for Robust Average Cost MDPs

Zhongchang Sun

University at Buffalo

zhongcha@buffalo.edu

&Sihong He

University of Texas at Arlington

sihong.he@uta.edu

&Fei Miao

University of Connecticut

fei.miao@uconn.edu

Shaofeng Zou

Arizona State University

zou@asu.edu

###### Abstract

This paper studies first-order policy optimization for robust average cost Markov decision processes (MDPs). Specifically, we focus on ergodic Markov chains. For robust average cost MDPs, the goal is to optimize the worst-case average cost over an uncertainty set of transition kernels. We first develop a sub-gradient of the robust average cost. Based on the sub-gradient, a robust policy mirror descent approach is further proposed. To characterize its iteration complexity, we develop a lower bound on the difference of robust average cost between two policies and further show that the robust average cost satisfies the Polyak-Lojasiewicz (PL)-condition. We then show that with increasing step size, our robust policy mirror descent achieves a linear convergence rate in the optimality gap, and with constant step size, our algorithm converges to an \(\)-optimal policy with an iteration complexity of \((1/)\). The convergence rate of our algorithm matches with the best convergence rate of policy-based algorithms for robust MDPs. Moreover, our algorithm is the first algorithm that converges to the global optimum with general uncertainty sets for robust average cost MDPs. We provide simulation results to demonstrate the performance of our algorithm.

## 1 Introduction

Markov decision process (MDPs)  has been widely used to model agent-environment interactions in sequential decision-making problems. An MDP consists of a set of states, a set of actions, a transition kernel describing the dynamics of the environment, and a cost function of state-action pairs. The agents aim to minimize the cumulative cost obtained over time under a given transition kernel. However, real-world environments often exhibit uncertainties and non-stationarity that challenge the assumptions of traditional RL approaches. When there is a mismatch between the training environment and the real environment, minimizing the cumulative cost under the training environment may lead to poor performance under the real environment.

To address the challenges raised by the model mismatch, the robust MDP was proposed [13; 27], where the transition kernel of the MDP is not fixed but lies in an uncertainty set. The goal of the robust MDP is to optimize the worst-case performance over the uncertainty set of transition kernels. The obtained policy under the robust setting is thus robust to the model mismatch.

Existing works on robust MDPs mainly focus on the discounted cost setting, where the goal is to minimize the worst-case cumulative discounted cost. However, in many real-world applications with long time horizons, such as inventory management in supply chains and applications in communication networks , the optimal policies obtained from the discounted cost setting may have poor long-term performance .

To address these challenges, recent research has shifted focus towards robust average cost MDPs, where the objective is to optimize the worst-case average cost obtained per time step. The average cost MDPs offer several advantages over discounted cost MDPs, including better stability and applicability to infinite-horizon tasks. However, achieving robust and efficient learning in average cost settings remains a significant challenge. Since the cost is discounted exponentially with time in discounted cost MDPs, establishing a contraction only requires a discount factor strictly less than one. Compared with discounted cost MDPs, average cost MDPs depend on the long-term performance of the underlying MDPs as it assigns equal weight to both immediate and future costs. Research on robust average cost MDPs is relatively scarce in the existing literature, with only a few notable studies, such as those by [36; 22; 39; 40; 12]. None of the above-mentioned works focuses on the fundamental characterization of gradient-based algorithms. Therefore, in our paper, we present the first theoretical analysis of the global convergence of policy optimization algorithms in the context of robust average cost MDPs with general uncertainty sets.

### Main Contributions

In this paper, for ergodic Markov chains, we propose a policy-based optimization algorithm called robust policy mirror descent to solve the robust average cost MDPs. We further show that with increasing step size, our robust policy mirror descent achieves linear convergence in the optimality gap, and with constant step size, our algorithm converges to an \(\)-optimal policy with iteration complexity of \((1/)\). Our algorithm is the first policy-based algorithm with global convergence and finite iteration complexity analysis for robust average cost MDPs with general uncertainty sets. In particular, our main contributions are summarized as follows.

**We derive a policy (sub)-gradient for the robust average cost MDPs.** In robust average cost MDPs, the goal is to optimize the worst-case performance, known as the robust average cost, which considers the worst-case over the uncertainty set of transition kernels. However, the robust average cost function typically involves a "max" operator over transition kernels, making it non-differentiable with respect to the policy. Therefore, in this paper, we first develop the Frechet sub-gradient of the robust average cost with general uncertainty sets, which serves as a foundation of the policy-based algorithm. The robust policy gradient under the discounted setting was derived in [42; 21]. The work  considers the specific \(R\)-contamination uncertainty set, and the derivative of the robust policy gradient in  relies on the fact that the discount factor is strictly less than \(1\), which can not be extended to the average cost setting.

**We propose a robust policy mirror descent algorithm.** Based on the derivative of the Frechet sub-gradient, we propose the robust policy mirror descent algorithm. We apply the dynamically weighted divergence to the policy mirror descent so that the policy can be updated for each state separately, which further ensures global convergence.

**We show that our algorithm converges to the global optimum, and we further characterize the iteration complexity.** To show the global convergence, we first prove that the robust average cost satisfies the Polyak-Lojasiewicz (PL) condition [29; 23]. We then prove the global optimality of our algorithm and characterize its iteration complexity. We show that with increasing step size, our robust policy mirror descent achieves linear convergence in the optimality gap, and with constant step size, our algorithm converges to an \(\)-optimal policy with iteration complexity \((1/)\). For increasing step size, the linear convergence of our algorithm matches with the robust discounted cost setting . In , the robust policy mirror descent was shown to converge to the global optimum with iteration complexity \((1/)\) for discounted cost MDPs when the step size is sufficiently large. Conversely, for non-robust discounted cost MDPs, the policy mirror descent was demonstrated to achieve a \((1/)\) iteration complexity in , representing the state-of-the-art convergence rate for policy mirror descent with a constant step size. Therefore, the convergence rate of our algorithm matches with the performance of the best non-robust counterpart. The convergence analysis of policy-based algorithms for non-robust MDPs relies on the fact that the value function is smooth, which is not the case for the robust average cost function. In this paper, we combine the first-order optimality condition of the policy update and the PL condition to develop a novel proof for the convergence rate of our algorithm.

### Related Works

In this section, we discuss works on policy-based approaches for non-robust MDPs, robust discounted cost MDPs and robust average cost MDPs.

**Policy-based approaches for non-robust MDPs.** In the non-robust setting, policy-based algorithms  have demonstrated remarkable success across various applications. Recently, the global convergence of the policy-based algorithms were established . For discounted cost MDPs, it was shown in  that the projected gradient descent converges to a global optimum with iteration complexity \((1/^{2})\). In both  and , the authors show that the projected gradient descent converges to a global optimum with a less iteration complexity \((1/)\). For average cost MDPs,  presents a sublinear convergence bound for projected gradient descent, where the bound involves the parameter that characterizes the complexity of the underlying MDP. In all the above works, the convergence analysis depends on the smoothness of the value function. However, since the smoothness may not hold for the robust value function, the methodologies applied in  can not be extended to our case. The policy mirror descent with increasing step size was shown to achieve linear convergence in . The policy mirror descent with constant step size was also proved to converge with iteration complexity \((1/)\) for discounted cost MDPs in . Their proof relies on the performance difference lemma and the fact that the underlying transition kernel doesn't change with time. In this paper, we derive the policy sub-gradient for robust average cost MDPs and design the robust policy mirror descent. We then develop a lower bound on the difference of robust average cost between two policies and further combine it with the first-order optimality condition of the policy update to characterize the iteration complexity of our algorithm.

**Robust discounted cost MDPs.** Robust discounted cost MDPs have been widely studied , where the goal is to minimize the worse-case cumulative discounted cost over the uncertainty set of transition kernels. In this section, we introduce works on policy-based algorithms, which are closely related to our work. In , the robust MDPs are considered under the \(R\)-contamination uncertainty set and the robust policy gradient algorithm is designed. It is shown in  that the robust policy gradient algorithm converges to the global optimum with iteration complexity \((1/^{3})\). Later in , the double-loop robust policy gradient was proposed for general uncertainty sets and was further proved to converge to the global optimum with iteration complexity \((1/^{4})\). The robust policy mirror descent was designed for discounted cost MDPs in . With increasing step size, the robust policy mirror descent converges linearly to the global optimum, while with constant but sufficiently large step size, the algorithm converges to the global optimum with iteration complexity \((1/)\). In this paper, we study the average cost setting and show that with increasing step size, the robust policy mirror descent achieves linear convergence, and with constant step size, our algorithm converges to the global optimum with iteration complexity \((1/)\), which matches with the best achievable iteration complexity of policy-based algorithms for robust MDPs. For the case with constant step size, our analysis doesn't require the step size to be sufficiently large.

**Robust average cost MDPs.** Existing literature that focuses on robust average cost MDPs is relatively limited. The robust average cost MDPs were initially explored by , where a specific finite interval uncertainty set was considered and the \((1/)\) convergence rate was achieved. In , the robust average cost MDPs were studied under the \(l_{1}\) uncertainty set. However, the approaches in  are not applicable for general uncertainty sets. Later in , the robust value iteration based algorithms were proposed and the global convergence was proved. In , the connection are built between the discounted reward MDPs and average reward MDPs and the existence of Blackwell optimal policies are proved. In , the model-based and model-free robust average reward MDPs are studied and the robust relative value iteration algorithms are proposed. However, finding a stopping criterion and characterizing the iteration complexity for robust value iteration based algorithms remain elusive. In this paper, we propose the first policy-based algorithm for robust average cost MDPs. We show that our algorithm converges to the global optimum and we further characterize its iteration complexity. Therefore, our algorithm is the first algorithm with finite iteration complexity analysis for robust average cost MDPs with general uncertainty sets.

**Exponential cost robust MDPs.** For the robust average cost MDPs, when the uncertainty set is defined by the KL-divergence metric, the problem admits a dual formulation, which is the exponential cost robust MDPs. The exponential cost robust MDPs have also been studied in the literature. In , the Q-learning and the actor-critic method are described and the asymptotic performance are characterized for risk sensitive robust MDPs. In , the value iteration and policy iteration algorithms are also analyzed for risk sensitive MDPs. Recently, in , the modified policy iteration is proved to converge to the global optimum for exponential cost risk sensitive MDPs. The policy gradient algorithm for the risk sensitive exponential cost MDPs is studied and the asymptotic convergencebounds to a stationary point are provided in . In our paper, we study the robust average cost MDPs with general uncertainty sets and characterize the global convergence of our algorithm.

## 2 Preliminaries and Problem Formulation

In this section, we introduce some preliminaries on discounted cost MDPs, average cost MDPs and present our problem formulation.

### Discounted Cost MDPs

A discounted cost MDP is defined by the tuple \((,,,r,)\), where \(\) denotes the finite state space, \(\) denotes the finite action space, \(=\{^{a}_{s}(),s,a \}\)1 is the transition kernel, \(r:\) denotes the cost function, and \([0,1)\) is the discount factor. Denote by \(S\) the number of states and \(A\) the number of actions, respectively.

We consider the set of all stationary and randomized policies \(=\{:()\}\). For each policy \(\), it maps from state \(s\) to a distribution over action \(a\). At each state \(s\), the agent takes action \(a\) with probability \((a|s)\), and the environment transits from state \(s\) to state \(s^{}\) according to \(^{a}_{s}\). The discounted value function of a policy \(\) starting from an initial state \(s\) is defined as

\[V^{}_{,}(s)_{}[_{t =0}^{}^{t}r(s_{t},a_{t})|s_{0}=s,],\] (1)

where \(_{}\) denotes the expectation with respect to the distribution induced by the transition kernel \(\). To align with conventions in the optimization literature, in this paper, we adopt a minimization formulation. For discounted cost MDPs, the goal is to find a policy \(\) that minimizes the discounted value function \(V^{}_{,}(s)\) for any initial state \(s\).

### Average Cost MDPs

Average cost is another fundamental criterion for MDPs. For discounted cost MDPs, the agent penalizes the future cost with discount factor \(\) to demonstrate the preference for the current cost. The average cost MDPs focus on the long-term performance of the underlying MDPs under the steady-state distribution. The average cost MDP can be defined by the tuple \((,,,r)\). For a policy \(\), define the average cost under transition kernel \(\) starting from an initial state \(s\) as follows

\[g^{}_{}(s)_{T}_{} [_{t=0}^{T-1}r(s_{t},a_{t})|s_{0}=s,].\] (2)

We also define the relative value function \(V^{}_{}\) and the relative state-action value function \(Q^{}_{}\) for average cost MDPs as follows

\[V^{}_{}(s) _{}[_{t=0}^{} r(s_{t},a_{t})-g^{}_{}|s_{0}=s,],\] \[Q^{}_{}(s,a) _{}[_{t=0}^{} r(s_{t},a_{t})-g^{}_{}|s_{0}=s,a_{0}=a,].\] (3)

The relative value function \(V^{}_{}\) and the average cost \(g^{}_{}\) satisfy the following Bellman equation 

\[V^{}_{}(s)=_{a}(a|s)r(s,a)-g^{}_{ }(s)+_{s^{}}^{a}_{s,s^{}}V^ {}_{}(s^{}),\] (4)

where \(^{a}_{s,s^{}}\) denotes the probability of transiting to state \(s^{}\) when choosing action \(a\) at state \(s\). Let \(d^{}_{}\) denote the stationary probability induced by the policy \(\) and transition kernel \(\), and it satisfies that \(d^{}_{}=d^{}_{}\). Similar as , we consider the projection of the value function onto the subspace orthogonal to the \(\) vector so that \(V^{}_{}\) and \(Q^{}_{}\) are unique. For average cost MDPs, the goal is to find a policy \(\) that minimizes the average cost \(g^{}_{}(s)\) for any initial state \(s\).

### Robust Average Cost MDPs

For robust MDPs, the transition kernel \(\) is not fixed but lies in some uncertainty set \(\). Define the robust average cost MDP by the tuple \((,,,r)\). In this work, we consider \((s,a)\)-rectangular uncertainty set:

\[=_{s,a}^{a}_{s},\ ^{a}_{s}=\{q ():D(q,(_{0})^{a}_{s}) R\},\] (5)

where \(_{0}\) is a known nominal transition kernel, \(D\) measures the difference between two distributions, e.g., KL divergence, and \(R\) is the pre-specified radius of the uncertainty set.

For robust average cost MDPs, the agent aims to optimize the worst-case performance over the uncertainty set \(\). Define the worst-case average cost as follows

\[g^{}_{}(s)_{}_{T }_{}[_{t=0}^{T-1}r( s_{t},a_{t})|s_{0}=s,].\] (6)

Similarly, we denote by \(V^{}_{}\) and \(Q^{}_{}\) the robust relative value function and the robust relative state action value function, respectively. The robust relative value function \(V^{}_{}\) and the robust average cost \(g^{}_{}\) satisfy the following Bellman equation .

\[V^{}_{}(s)=_{a}(a|s)r(s,a)-g^{}_{ }(s)+_{}_{s^{} }^{a}_{s,s^{}}V^{}_{}(s^{}).\] (7)

For any policy \(\), denote by \(d^{}_{}\) the stationary distribution of the state under the worst-case transition kernel of \(\). The goal is to find a policy \(\) such that the worst-case average cost \(g^{}_{}\) is minimized, i.e.,

\[_{}g^{}_{}(s),\ \ s.\] (8)

Denote the optimal policy by \(^{*}\) and the robust average cost of \(^{*}\) by \(g^{*}_{}\). In this paper, we state the following assumption to guarantee that the average cost is independent of the initial state, which is widely used in the studies of average cost MDPs .

**Assumption 2.1**.: For any \(\) and \(\), the induced Markov chain is ergodic.

## 3 Robust Policy Mirror Descent

In this section, we first derive the robust average cost policy gradient. We then propose the robust policy mirror descent algorithm.

### Robust Average Cost Policy Gradient

Since the worst-case average cost \(g^{}_{}\) takes "max" over all \(\), \(g^{}_{}\) might not be differentiable. To address this issue, we introduce the concept of Frechet sub-gradient. Let \(\|\|\) denote the \(L_{2}\) norm of a vector.

**Definition 3.1**.: For any function \(f:^{N},\) the Frechet sub-gradient \(u^{N}\) is a vector that satisfies

\[_{ 0}_{ 0} 0.\] (9)

When \(f\) is differentiable at \(x\), the Frechet sub-gradient \(u\) is the gradient of \(f\). In this paper, we consider the direct policy parameterization. We derive the sub-gradient for the robust average cost \(g^{}_{}\) in the following lemma.

**Lemma 3.2**.: _Let \( g^{}_{}(s,a)=d^{}_{}(s)Q^{}_{}(s,a)\). Then \( g^{}_{}\) is the Frechet sub-gradient of \(g^{}_{}\)._

Note that the Frechet sub-gradient has been derived for robust discounted cost MDPs , of which the \((s,a)\) entry takes the form \(d^{}_{}(s)Q^{}_{}(s,a)\). Here, with a little abuse of notation, we use \(d^{}_{}\) to denote the visitation distribution of policy \(\) under the worst-case transition kernel and use \(Q^{}_{}\) to denote the worst-case action value function of policy \(\). The Frechet sub-gradient for robust discounted cost MDPs in  can not be extended to the average setting since in , the discounted factor \(\) is required to be strictly less than 1. In this paper, we derive the Frechet sub-gradient of robust average cost MDPs by applying the performance difference lemma for average cost MDPs  and the Lipschitz property of the relative action value function .

### Robust Policy Mirror Descent

With Lemma 3.2, we are ready to present our robust policy mirror descent algorithm for average cost MDPs. We assume that for a given policy \(\), there exists an oracle that outputs the robust relative state action value function \(Q_{}^{}\). We denote by \(D((|s),^{}(|s))\) the Bregman divergence between two policies \((|s)\) and \(^{}(|s)\). We further define the weighted Bregman divergence function \(D_{d}(,^{})=_{s}d(s)D((|s),^{}( |s))\) for any \(d()\). We define the following robust policy mirror descent with dynamically weighted divergence

\[_{k+1}=*{arg\,min}_{}_{k}  g_{}^{_{k}},+D_{d_{}^{_{k }}}(,_{k})},\] (10)

where \(_{k}\) is the step size. Note that \(g_{}^{}\) might not be differentiable, thus in our algorithm we replace the gradient of \(g_{}^{}\) by its Frechet sub-gradient \( g_{}^{}\). By plugging in the sub-gradient formula of \(g_{}^{}\) in Lemma 3.2, we have that

\[_{k+1} =*{arg\,min}_{}_{k}_{s }d_{}^{_{k}}(s) Q_{}^{_{k}}(s, ),(|s)+D_{d_{}^{_{k}}}(,_{k})}\] \[=*{arg\,min}_{}_{s }_{k} Q_{}^{_{k}}(s,),( |s)+D(|s),_{k}(|s)},\] (11)

which is equivalent to

\[_{k+1}(|s)=*{arg\,min}_{p()} _{k} Q_{}^{_{k}}(s,),p+Dp,_{k} (|s)}, s.\] (12)

We summarize our algorithm in Algorithm 1.

``` Input: step size \(_{k}\), initial policy \(_{0}\) for\(k=0,1,,K-1\)do for\(s\)do  Update policy: \(_{k+1}(|s)=*{arg\,min}_{p()} _{k} Q_{}^{_{k}}(s,),p+D(p,_{k}(| s))}\). endfor endfor Output:\(_{K}\) ```

**Algorithm 1** Robust Policy Mirror Descent

Note that for the projected policy (sub)-gradient algorithm, the policy is updated as follows

\[_{k+1}(|s)=*{arg\,min}_{p()} _{k} g_{}^{_{k}}(s,),p+\|p-_{k}( |s)\|^{2}}, s.\] (13)

In our paper, we set the Bregman divergence \(D(,)\) to be the squared Euclidean distance. In this case, the difference between our robust policy mirror descent and the projected policy gradient lies in that we replace the policy (sub)-gradient \( g_{}^{_{k}}\) by \(Q_{}^{_{k}}\).

In the next section, we show that though the robust average cost \(g_{}^{}\) might not be differentiable, our robust policy mirror descent achieves linear convergence in the optimality gap with increasing step size, and converges to an \(\)-optimal policy with iteration complexity \((1/)\) with constant step size.

## 4 Theoretical Results

Before we show the global optimality of the robust policy mirror descent, we first provide some important properties of the robust average cost MDPs.

We first provide a lower bound on the difference of robust average cost between two policies, which is a key step to derive the global optimality.

**Lemma 4.1**.: _For any two policies \(,^{}\), we have that \(g_{}^{}-g_{}^{^{}}_{s d_{ }^{^{}}} Q_{}^{}(s,),( |s)-^{}(|s)\)._Lemma 4.1 is not a straightforward extension of performance difference lemma  to the robust setting since the worst-case transition kernels are different for different policies. A similar bound was derived in  for robust discounted value function by applying the Bellman equation of robust value function, which is not applicable in our case as the average cost itself does not satisfy the Bellman equation. Therefore, we apply the Bellman equation of the robust relative value function \(V_{}^{}\) and the robust average cost \(g_{}^{}\) in (7) and the observation that \(g_{}^{}\) is independent of the initial state \(s\) to obtain Lemma 4.1.

We then show that the robust average cost \(g_{}^{}\) satisfies the PL-condition in the following lemma.

**Lemma 4.2**.: _The suboptimality of any \(\) satisfies \(g_{}^{}-g_{}^{*} C_{PL}_{} g_ {}^{},-,\) where \(C_{PL}=_{,s}}^{}(s)}{d_{}^{}(s)}.\)_

The PL-condition implies that when the subgradient \( g_{}^{}\) is small, the policy \(\) lies in the small neighborhood of the global optimum.

Our convergence analysis also leverages the following Lipschitz property of the non-robust relative value function \(V_{}^{}\).

**Lemma 4.3**.: _The relative value function \(V_{}^{}\) is Lipschitz in \(\), i.e., there exists a constant \(L_{}\) such that \(|V_{}^{}-V_{}^{^{}}| L_{}\|-^{ }\|.\)_

### Increasing Step Size

In this section, we show that our algorithm achieves linear convergence rate with increasing step size. We first characterize some properties of our robust policy mirror descent algorithm.

**Lemma 4.4**.: _For any \(p\) and \(s\), we have that_

\[_{k}  Q_{}^{_{k}}(s,),_{k+1}(|s)-p( |s)+\|_{k+1}(|s)-_{k}(|s)\|^{2}\] \[\|p(|s)-_{k}(|s)\|^{2}-\|p(|s)-_{k+1}( |s)\|^{2}.\] (14)

In the following lemma, we establish the convergence property for each iteration of our algorithm.

**Lemma 4.5**.: _At each iteration of our algorithm, we have that_

\[g_{p}^{_{k+1}}-g_{}^{*} (g_{}^{_{k}}-g_{}^{*})+ _{s d_{_{_{k}}}^{*}}}^{*}(|s)-_{k}(|s)^{2}\] \[-_{s d_{_{_{k}}}^{*} }}^{*}(|s)-_{k+1}(|s) ^{2},\] (15)

_where \(M=_{,}_{_{k}}}^ {*}}{d_{}^{*}}_{}.\)_

We show that with increasing step size, our robust policy mirror descent converges linearly.

**Theorem 4.6**.: _Under Assumption 2.1, set the step size \(_{k}_{k-1}1-^{-1}M.\) The robust policy mirror descent satisfies_

\[g_{}^{_{k}}-g_{}^{*}1-^{ k}(g_{}^{_{0}}-g_{}^{*})+1-^{ k-1}}_{s d_{_{_{0}}}^{*}}\|^{*}( |s)-_{0}(|s)\|^{2}.\] (16)

Our analysis in this section mainly leverages the performance difference lemma  and the Bregman divergence three-point lemma. The convergence rate for our robust policy mirror descent with increasing step size matches with the best convergence rate of policy-based algorithm for robust MDPs . Our algorithm is the first algorithm that converges to the global optimum with finite iteration complexity for robust average cost MDPs with general uncertainty sets.

### Constant Step Size

We proceed to show that with constant step size, our algorithm achieves the global optimum with iteration complexity \((1/)\).

**Theorem 4.7**.: _Under Assumption 2.1, let step size \(=}\) for all \(k 1\). We have that the each iteration of the robust policy mirror descent satisfies_

\[g_{}^{_{k}}-g_{}^{*}}{  k},}{2}^{k}g_{}^{_{0}} -g_{}^{*}},\] (17)

_where \(=(2C_{PL})^{-2}\)._

For Algorithm 1, to find an \(\)-optimal policy, Theorem 4.7 shows that the iteration complexity is upper bounded by \((1/)\). In , the robust policy mirror descent was studied for discounted cost MDPs. The iteration complexity \((1/)\) in  can only be achieved with a sufficient large step size \(_{k}=1/\). For the non-robust discounted cost MDPs, the policy mirror descent was shown to enjoy a \((1/)\) iteration complexity in , which is the state-of-the-art convergence rate for policy mirror descent with constant step size. Our robust policy mirror descent with constant step size converges to the global optimum with iteration complexity \((1/)\), which matches with the best non-robust counterpart .

Define the gradient mapping \(G_{}}(_{k}(|s))=(_{k}(|s)- _{k+1}(|s))\). Note that if \(_{k}\) is updated exactly by the (sub)-gradient descent, then \(G_{}}(_{k}(|s))= g_{}^{}(s, )\). The norm \(\|G_{}}(_{k}(|s))\|\) measures the closeness of the current step to the first-order stationary point. Our proof relies on the following key ingredient.

\[g_{}^{_{k}}-g_{}^{_{k+1}}_{s d _{}^{_{k+1}}}\|G_{}}(_{k}(| s))\|^{2}(g_{}^{_{k+1}}-g_{}^{*})^{2}.\] (18)

In , similar steps are adopted to derive the convergence rate of the policy gradient descent for non-robust discounted cost MDPs. In their analyses, to obtain (18), the smoothness of the value function is required. However, since the worst-case transition kernel is a function of the policy \(\), the robust average cost \(g_{}^{}\) might not be smooth. Therefore, the approaches applied in  can not be directly extended to robust average cost MDPs. Without the smoothness of the value function, we still prove that (18) holds by leveraging Lemma 4.1 and the following first-order optimality, which is from the optimality condition of the robust policy mirror descent update in (12)

\[Q_{}^{_{k}}(s,)+}(_{k+1} (|s)-_{k}(|s)),p-_{k+1}(|s) 0, p ().\] (19)

_Remark 4.8_.: Since for robust discounted cost MDPs, Lemma 4.1, Lemma 4.2 and Lemma 4.3 also hold, the result in Theorem 4.7 also holds for robust discounted cost MDPs. Therefore, our robust policy mirror descent with constant step size finds an \(\)-optimal policy with iteration complexity \((1/)\) for robust discounted cost MDPs. Compared with , our step size doesn't need to be sufficiently large.

## 5 Simulation Results

In this section, we provide some simulation results to demonstrate the performance of our algorithm. We verify our method on one classical problem: the Garnet problem, and a robotic application problem: the recycling robot problem.

Garnet environments are synthetic benchmarks designed to be used for studying the performance of RL algorithms. The Garnet framework provides a way to create randomly generated MDPs with specified properties, such as the number of states, actions, and transition probabilities. More details can be found in .

In the recycling robot problem, a mobile robot powered by a rechargeable battery is tasked with collecting empty soda cans. The robot operates with two battery levels: low and high. It has three possible actions: (1) search for empty cans; (2) remain stationary and wait for someone to bring it a can; or (3) return to its home base to recharge. When the robot's battery is low (high), it has a probability of \(\) (\(\)) of finding an empty can and maintaining its current battery level. If the robot searches for cans but does not find any, it will deplete its battery completely and must be carried back by humans. For more details, refer to . In this paper, we set \(=0.9,\,=0.9\).

We compare our robust policy mirror descent with the non-robust method . We consider Garnet(3, 2), Garnet(5, 2), Garnet(10, 5) and recycling robot problems. Both methods use a uniform randompolicy as the initialized policy. We conducted 5 trials with different random seeds and reported mean and standard deviation of the robust average costs over training episodes. Each training episode contains \(2000\) training steps. The length of training episodes is respectively \(100\) and \(300\) for Garnet and robot problems. We choose the uncertainty set to be the KL divergence uncertainty set. We consider the constant step size and set the step size \(=0.01\), the pre-specified radius of the uncertainty set \(R=0.1\). The host machine used in our experiments is a server configured with AMD Ryzen Threadripper 2990WX 32-core processors and four Quadro RTX 6000 GPUs. All experiments are performed on Python 3.8.

Figure 1 showcases the mean values of the optimal robust average cost values and standard deviations for our robust method and the non-robust baseline over the training episode in Garnet(3, 2). Our robust method converges faster than the non-robust baseline, though these two methods converge to the same value. We speculate that the reason why the two methods converge to similar numbers is because this MDP environment has a small size. Therefore, we further compare our robust method with the non-robust baseline in two larger Garnet problems Garnet(5, 2) and Garnet(10, 5), and the recycling robot problem. Figure 2, 3, and 4 respectively show the mean optimal robust average cost and standard deviations in these three problems. From Figure 2, 3 and 4, we find that our robust method outperforms the non-robust baseline in terms of mean costs in all three problems. The simulation results demonstrate the robustness of our algorithm.

## 6 Conclusion

In this paper, we investigated the policy-based algorithm for robust average cost MDPs. We first introduced the sub-gradient of the robust average cost. Based on the sub-gradient, we proposed the robust policy mirror descent. Our theoretical analysis demonstrates that the proposed algorithm with increasing step size achieves linear convergence rate, and with constant step size, the proposed algorithm finds an \(\)-optimal policy with iteration complexity \((1/)\), matching the best convergence rate observed in policy mirror descent algorithms for robust MDPs. Moreover, our algorithm is the first algorithm that converges to the global optimum with finite iteration complexity for robust average cost MDPs with general uncertainty sets. Our paper focuses on the model-based setting. In the future, it is of interest to design policy-based model-free algorithms for robust average cost MDPs.

Acknowledgements

The work of Zhongchang Sun and Shaofeng Zou is supported by the National Science Foundation under Grants CCF-2438429 and ECCS-2438392 (CAREER). The work of Sihong He and Fei Miao is supported by the National Science Foundation under Grants CNS-2047354 (CAREER), and the New England University Transportation Center (NEUTC). Funding for the UTC Program is provided by the Office of Assistant Secretary for Research and Innovation (OST-R) of the United States Department of Transportation. The recommendations of this study are those of the authors and do not represent the views of NEUTC.