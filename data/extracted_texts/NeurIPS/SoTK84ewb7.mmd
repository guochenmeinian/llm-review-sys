# Zero-Shot Scene Reconstruction from Single Images with Deep Prior Assembly

Junsheng Zhou\({}^{1}\)   Yu-Shen Liu\({}^{1}\)   Zhizhong Han\({}^{2}\)

School of Software, Tsinghua University, Beijing, China\({}^{1}\)

Department of Computer Science, Wayne State University, Detroit, USA\({}^{2}\)

zhou-js24@mails.tsinghua.edu.cn liuyushen@tsinghua.edu.cn h312h@wayne.edu

The corresponding author is Yu-Shen Liu.

###### Abstract

Large language and vision models have been leading a revolution in visual computing. By greatly scaling up sizes of data and model parameters, the large models learn deep priors which lead to remarkable performance in various tasks. In this work, we present deep prior assembly, a novel framework that assembles diverse deep priors from large models for scene reconstruction from single images in a zero-shot manner. We show that this challenging task can be done without extra knowledge but just simply generalizing one deep prior in one sub-task. To this end, we introduce novel methods related to poses, scales, and occlusion parsing which are keys to enable deep priors to work together in a robust way. Deep prior assembly does not require any 3D or 2D data-driven training in the task and demonstrates superior performance in generalizing priors to open-world scenes. We conduct evaluations on various datasets, and report analysis, numerical and visual comparisons with the latest methods to show our superiority. Project page: https://junshengzhou.github.io/DeepPriorAssembly.

## 1 Introduction

Reconstructing scenes from images is a vital task in 3D computer vision and computer graphics. It bridges the gap between the 2D images that can be easily captured by phone cameras and the 3D geometries of scenes for various real-world applications, e.g., autonomous driving, augmented/virtual reality and robotics. Reconstructing scenes from multi-view images  is well-explored to recover 3D geometries with multi-view consistency and camera poses. However, reconstructing a scene from a single RGB image is still challenging, which is extremely difficult due to inadequate information. Recent works  try to solve this task as a reconstruction problem which leverages neural networks with an encoder-decoder architecture to draw supervisions from pairs of images and 3D ground truth geometries and layouts. Nevertheless, due to the limited amount of image-scene pairs, these methods struggle to generalize to out-of-distribution images in open world.

Large language and vision models have been extensively studied in the past few years, which revolutionized neural language processing , 2D/3D representation learning  and content generation , etc. By greatly scaling up sizes of training samples and model parameters, large models show brilliant capabilities and remarkable performance. However, they are limited in a specific task, which limits their capability in high level perception tasks. Driven by the observation, we raise an interesting question: can we assemble series of deep priors from large models, which are experts with different modalities in different tasks, to solve an extremely challenging task that none of them can accomplish alone?In this work, we propose _deep prior assembly_, a novel framework which assembles diverse deep priors from large models for scene reconstruction from single images in a zero-shot manner. We rethink this task from a new perspective, and decompose it into a set of sub-tasks instead of seeking to a data-driven solution. We narrow down the responsibility of each deep prior on a sub-task that it is good at, and introduce novel methods related to poses, scales, and occlusion parsing to enable deep priors to work together in a robust way.

Specifically, we first detect and segment the instances in the input image with Grounded-SAM , which is a variation of Segment-Anything Model . For the segmented instances that are often corrupted due to occlusions or of low resolution, we leverage Stable-Diffusion  to enhance and inpaint images containing the segmented instances. However, the Stable-Diffusion often produces some predictions which drift away from the input instances and do not conform to the original appearance. To solve this issue, we introduce to use Open-CLIP  to filter out the bad samples and select the ones matching the input instance most. We then generate the 3D models for each instance with Shap-E  using the amended instance images as input. Additionally, we estimate the depth of the origin image with Omnidata  as the 3D scene geometry prior. To recover a layout consistent to the image, we propose an approach to optimize the location, orientation and size for each 3D instance to fit it to the estimated segmentation masks and the depths.

Deep prior assembly merely generalizes deep priors and does not require additional data-driven training for extra prior knowledge. Our evaluations on various open-world scenes show our capability of reconstructing diverse objects and recovering plausible layout merely from a single view angle. Our main contributions can be summarized as follows.

* We propose deep prior assembly, a flexible framework that assembles diverse deep priors from large models together for reconstructing scenes from single images in a zero-shot manner.
* We introduce a novel approach to optimizing the location, orientation and scale of instances by matching them with both 2D and 3D supervision.
* We evaluate deep prior assembly for generating diverse open-world 3D scenes, and show our superiority over the state-of-the-art supervised methods.

## 2 Related Work

### Large Models in Different Modalities

Recently, it has been drawing significant attention on scaling up deep models for much more powerful representations and higher performances with different modalities (e.g. NLP, 2D vision). Starting from NLP, recent works in scaling up pre-trained language models  have largely revolutionized natural language processing. Some researches translates the progress from language to 2D vision  and 3D vision  via model and data scaling.

Except for the large foundation models which focus on producing large-scale representations for language, 2D images or 3D point clouds, some researches explore large models for specific tasks (e.g. text-to-image generation , image segmentation , 3D analysis  and 3D object generation ) and have shown remarkable performance. Stable Diffusion trains a large model of latent diffusion models and achieves commercially available 2D content generation

Figure 1: **An illustration of our work. We assemble diverse deep priors from large models with frozen parameters for scene reconstruction from single images in a zero-shot manner.**

effects. Segment Anything Model (SAM)  revolutionize the field of image segmentation by training models with large-scale annotated data. Omnidata  trains the large depth estimation model with various data sources for bringing robust 3D awareness to pure RGB images. In the 3D domain, the recent works Point-E  and Shap-E  collect millions of 3D objects to train large 3D models for generating 3D geometries from rendering-style images or texts. In this work, we aim at leveraging the powerful capabilities of the large models in different modalities and different domains to solve a challenging task, i.e. scene generation from single images, by assembling deep priors together.

### Scene Reconstruction from Images

Recovering the underlying 3D surfaces of scenes from images [60; 70; 22; 19; 41] or point clouds [72; 75; 73; 25; 36; 74; 24; 43] is a long-standing task in 3D computer vision. Most of the previous works focus on the multi-view reconstruction with the input dense images captured around the scene. Classic multi-view stereo methods [1; 5; 4] mainly represent the scene by estimating depths for dense images with feature matching. Inspired by NeRF  which performs volume rendering for scene representation, a series of works [60; 44; 65; 59; 68; 69] introduce the neural implicit surface reconstruction by learning signed distance fields  or occupancy fields  for scenes from multi-view images. NeuRIS  proposes to use normal priors for indoor scene reconstruction, and MonoSDF  further introduces monocular depth cues for improving scene geometries.

## 3 Method

**Overview.** The overview of deep prior assembly is shown in Fig. 2. We will start from an introduction of the task decomposition in Sec. 3.1 and then present the pipeline for solving each of the decomposed sub-tasks using a deep prior from a specific large model in Sec. 3.2. Finally, we introduce an optimization-based approach for layout estimation in Sec. 3.3.

Figure 2: **The overview of deep prior assembly. Given a single image of a 3D scene, we detect the instances and segment them with Grounded-SAM. After normalizing the size and center for the instances, we attempt to amend the quality of the instance images by enhancing and inpainting them. Here, we take a sofa in the image for example. Leveraging the Stable-Diffusion model, we generate a set of candidate images through image-to-image generation, with additional guidance from a text prompt of the instance category predicted by Grounded-SAM. We then filter out the bad generation samples with Open-CLIP by evaluating the cosine similarity between the generated instances and original one. After that, we generate multiple 3D model proposals for this instance with Shap-E from the Top-\(K\) generated instance images. Additionally, we estimate the depth of the origin input image with Omnidata as a 3D geometry prior. To estimate the layout, we propose an approach to optimize the location, orientation and scale for each 3D proposal by matching it with the estimated segmentation masks and the depths (the \(\) for the example sofa). Finally, we choose the 3D model proposal with minimal matching error as the final prediction of this instance, and the final scene is generated by combining the generated 3D models for all detected instances.**

### Task Decomposition

Revealing 3D scene geometries from a single image is an extremely challenging task duo to limited context and supervisions. Instead of using a data-driven strategy to learn priors [42; 10], we reformulate this task from a new perspective. We decompose it into a set of sub-tasks, each of which can be done using one deep prior without a need of learning extra knowledge. More specifically, we can progressively resolve the task by:

1. First, performing detection and segmentation on the input image to acquire the segmentation images, masks and category labels for all detected instances.
2. Amending instance images through enhancing and inpainting to improve the image qualities.
3. Generating a set of 3D model proposals for each instance from 2D segmented images.
4. Estimating the layout by predicting the location, rotation, and scale for each 3D proposal to put them to the correct position of the 3D scene.
5. Producing a scene reconstruction by applying the estimated layout and shape poses with reconstructed instances.

### Assembling Large Models

Inspired by the remarkable performances of recent large models, we propose to assign an expert large model in each sub-task, which maximizes their abilities for modeling a scene in a zero-shot manner.

**Detect and Segment 2D instances.** To reveal a scene \(S\) from a single image \(I\), we first detect the instances in \(I\) and separate multiple objects into single instances. In this way, we can reconstruct a scene at shape level, which simplifies the task.

_Mask R-CNN vs. SAM vs. Grounded SAM._ Detecting and segmenting images have been widely explored in the past few years. Mask R-CNN  is widely adopted as a popular and robust backbone. However, the performance of Mask R-CNN does not generalize well since it is only trained under a relative small dataset. Recently, the large SAM  have shown promising segmentation accuracy by scaling up parameters and using more training samples, nevertheless, it only predicts fine-grained masks but with few semantic concepts. Thus, we adopt Grounded-SAM, which is an improved version of SAM by introducing Grounding-DINO  as an open-set detector and using SAM to jointly predict detection boxes, segmentation masks and category labels for each instance, formulated as:

\[\{m_{i},c_{i},d_{i}\}_{i=1}^{N}=(I),d_{i}>\] (1)

\(\{m_{i},c_{i},d_{i}\}\) includes the predicted mask \(m_{i}\), category \(c_{i}\) and the detection confidence score \(d_{i}\) for the \(i\)-th instance, and \(N\) is the number of instances in this scene. We only keep the predictions with a high confidence score larger than a threshold \(\). The low confident instances often contain large occlusions or wrong category predictions.

**Enhance and Inpaint 2D instances.** With the predicted masks \(\{m_{i}\}_{i=1}^{N}\), we achieve the segmented instance images \(\{t_{i}\}_{i=1}^{N}\) by masking the original input image \(I\). We then normalize each instance image by centering it at the origin and normalizing its scale in \(\{t_{i}\}_{i=1}^{N}\) to 0.6 of the max width or height of \(I\). As shown in Fig. 2, the segmented instance images often suffer from occlusions or low-resolution of small instances. The low-quality images have a large negative impact on the followup 3D generation. Therefore, we propose to first improve the quality of instance images by enhancing and inpainting them with the large model Stable-Diffusion .

Specifically, we adopt the image-to-image generation  from Stable-Diffusion. We take the instance image \(t_{i}\) as the initialization, and add noises on it and then subsequently denoise the noise corrupted image to increase the realism through the guidance of the text prompt description from the predicted categories \(c_{i}\). We find the prompt template 'High quality, authentic style {category}' works fine for most of the indoor instances. For other situations, we directly leverage the category as the prompt. We observe that Stable-Diffusion may produce some unreliable predictions which are 'too creative' and can not faithfully improve the image quality but turn it into another image, as shown in Fig. 3. To solve this issue, we generate multiple enhanced images \(\{e_{i}^{j}\}_{j=1}^{M}\) for each instance image \(t_{i}\) and filter out the bad generation samples with the approaches described next.

**Filter Out Bad Generation Samples.** To filter out the bad generation samples produced by Stable-Diffusion and select the top \(K\) enhanced images for the following 2D-to-3D generation, we propose to leverage the CLIP models as a judge to determine which generated images \(\{e_{i}^{j}\}_{j=1}^{M}\) from Stable-Diffusion can conform to the original appearance of the instance \(t_{i}\). Specifically, we adopt the large open-sourced CLIP  model Open-CLIP  as the implementation.

We use cosine similarities \(\{z_{i}^{j}\}_{j=1}^{M}\) between the generated instance image \(\{e_{i}^{j}\}_{j=1}^{M}\) and the original one \(t_{i}\) as a metric for the selection, formulated as:

\[z_{i}^{j}=(e_{i}^{j}) f_{}(t_{i})}{\|f_{}(e_{ i}^{j})\|\|f_{}(t_{i})\|},\] (2)

where \(f_{}\) is the frozen image encoder from Open-CLIP. We use the Top-\(K\) generated instance images with the largest similarities to the original \(t_{i}\) as the amended images. As shown in Fig. 3, Open-CLIP successfully filters out the bad generations.

**Generate 3D models from 2D instances.** The object-level 3D generation from single images [38; 61] is a well-explored task in 3D computer vision, however, most previous works show limited generation performance on open-world images. Shap-E  is a large model for 3D generation by training a 3D hyper diffusion model with millions of non-public 3D objects. Therefore, we leverage Shap-E to provide the deep prior to convert 2D instance images into 3D reconstructions. By this way, we obtain \(K\) 3D reconstruction proposals \(\{s_{i}^{k}\}_{k=1}^{K}\) for each 2D instance \(t_{i}\) by employing Shap-E on the top \(K\) amended images.

### Recovering Scene Layout

The final step is to select the most accurate 3D model proposal \(}\) from the \(K\) candidates \(\{s_{i}^{k}\}_{k=1}^{K}\) and put it to the right position in a 3D scene to recover the scene layout in the input image. To achieve this, we propose a novel approach to optimize the location, orientation and size for each 3D proposal in \(\{s_{i}^{k}\}_{k=1}^{K}\) by matching it with the estimated segmentation mask and the estimated depth. We further introduce a RANSAC-like solution for robust position optimizing. We select the reconstruction with the minimum matching error as the reconstruction \(}\) of \(t_{i}\).

**Depth Estimation.** For more accurate layout estimations, we first estimate the depth map of the input image \(I\) as a 3D geometry prior. We leverage the large model Omnidata [13; 27] as the depth estimator which is trained under a collected huge depth dataset  containing 14-million indoor, outdoor, scene-level and object-level samples.

An issue here is that the depth \(D\) estimated for the input image \(I\) with Omnidata is not scale-aware, which can not be directly used as supervisions. We solve this problem by estimating the scale \(h\) and shift \(q\) of the predicted depth using one pair of predicted and real depth of a selected scene for each

Figure 4: **Illustration of the depth transform.** The estimated depth maps from Omnidata is not scale-aware, resulting in scale inaccuracies and distortion in the back-projected depth point clouds. We achieve the accurate depth point cloud by first transforming the depth maps with the pre-solved scale and shift before back-projecting.

Figure 3: **Examples on the effect of our pipeline.** For the corrupted 2D instant segmented from the scene image, we leverage Stable-Diffusion to produce \(6\) amended generations. We then adopt Open-CLIP to filter out bad samples by judging the similarities and producing confidence scores for the generations, and keep the Top-\(3\) generated images. The shape generations with Shap-E from the amended images are significantly more complete and accurate than the one produced by the original corrupted image.

dataset. Specifically, we leverage least-squares criterion  which has a closed-form solution to solve the depth scale and shift by matching the pair of predicted and real depth with a specific scene camera intrinsic parameter \(_{K}\). After transforming \(D\) with the scale \(h\) and shift \(q\), we can now back-project \(D\) into the 3D space with camera intrinsic parameter \(_{K}\), achieving a 3D scene depth point cloud. The example in Fig. 4 shows that the depth point cloud produced using the transformed depth maps precisely aligns with the ground truth scene. Alternatively, we can use metric depth estimation methods , which naturally handle depth scale and shift, to replace Omnidata and lessen the reliance on ground truth depth data. The depth point cloud \(d_{i}\) for each instance \(t_{i}\) is further achieved by masking the back-projected 3D point cloud.

**Pose/Scale Optimization.** We further estimate the scale and pose of each 3D model proposal \(s_{i}^{k}\) to put them into the right position in the 3D scene. We propose to solve this problem with an optimization-based approach on the location, rotation and size for \(s_{i}^{k}\) per the mask \(m_{i}\) from the Grounded-SAM and the depth point cloud \(d_{i}\) from Omnidata.

We model this problem as a 7-DoF shape registration task with 3-DoF of translation (\(tx\), \(ty\), \(tz\)), 3-DoF of rotation (\(rx\), \(ry\), \(rz\)) and one DoF of object scale (\(v\)). Specifically, we first sample a point cloud \(p_{i}^{k}\) from the mesh of a 3D proposal \(s_{i}^{k}\) and initialize a 7-DoF transform as a transformation function \(f_{}\) with learnable 7-DoF parameters \(\). We then project \(p_{i}^{k}\) with \(f_{}\) to achieve the transformed prediction \(_{i}^{k}\) by:

\[_{i}^{k}=f_{}(p_{i}^{k}).\] (3)

We obtain \(_{i}^{k}\) by optimizing the 7-DoF parameters \(\) with supervisions. With the estimated depth \(d_{i}\), we can draw the direct 3D matching supervision by minimizing the Chamfer Distance Loss between the transformed \(_{i}^{k}\) and the depth points \(d_{i}\). However, merely with the 3D matching constraint, the pose/scale optimization do not always converge stably since the predicted depth \(d_{i}\) is usually with noises in complex scenes, which significantly affects the registration on shapes.

To resolve this issue, we get inspirations from  to leverage the mask information predicted by Stable-Diffusion as an extra matching supervision in 2D space. Specifically, we project the transformed 3D point cloud \(_{i}^{k}\) to the 2D space with the camera intrinsic parameters \(_{K}\), resulting in a 2D point cloud \(_{i}^{k}\). Meanwhile, we form another 2D point cloud \(_{i}\) from the mask \(m_{i}\) by randomly sampling dense 2D points on the occupied region of the mask \(m_{i}\). We then use the 2D matching constraint to minimize the 2D Chamfer Distance between \(_{i}^{k}\) and \(_{i}\). We illustrate the effect of 2D Matching constraint with an optimization example in Fig. 5. The final loss for pose/scale optimization of 3D reconstruction \(s_{i}^{k}\) is then formulated as:

\[=_{CD}^{3D}(d_{i},_{i}^{k})+_{CD}^{2D} (_{i}^{k},_{i}).\] (4)

**Robust RANSAC-like Solution.** With the optimization-based 7-DoF registration, we are now able to put the generated 3D object proposals into the 3D scene. However, if the mis-registration is quite large, especially in the rotation, the optimization may be trapped in a local optimum and fail to produce accurate registrations. We further introduce a RANSAC-like solution to enhance the robustness of pose/scale optimization. Specifically, we repeat the optimization \(r\) times with randomly initialized rotation matrices for \(f_{}\) each time. The final transform for the 3D proposal \(s_{i}^{k}\) is selected as the one with minimum matching loss in Eq. (4) among \(r\) optimal optimizations, and we define the matching error \(w_{i}^{k}\) of \(s_{i}^{k}\) as the minimal matching loss.

Figure 5: **Effect of the 2D Matching.** An example of optimizing the pose and scale for a chair. We visualize the optimization in 2D space. The red 2D points indicate the dense 2D point cloud sampled in the mask, which is the target. And the green 2D points donate the 2D projection of transformed 3D point clouds sampled from the generated shape of this chair instance. More robust registration is achieved with the proposed 2D matching constraint. The total 1,000 iterations take \(9.2\) seconds on a single 3090 GPU.

We repeat the above procedure for each one of the \(K\) 3D proposals \(\{s_{i}^{k}\}_{k=1}^{K}\). We select the 3D proposal with the minimum \(w_{i}^{k}\) as the final reconstruction \(}\) of \(t_{i}\). The final scene generation from the single image \(I\) is achieved by combining the transformed \(\{}\}_{i=1}^{N}\) together.

## 4 Experiments

### Setup

**Implement Details.** The number \(M\) of samples generated by Stable-Diffusion for each instance is set to \(6\), where we select the Top \(K=3\) samples with Open-CLIP. The pose/scale optimization is repeated for \(r=10\) times for each instance with RANSAC-like solution.

**Datasets.** We evaluate deep prior assembly under four widely-used 3D scene reconstruction benchmarks 3D-Front , Replica , BlendSwap  and ScanNet .

3D-Front  is a synthetic 3D dataset of indoor 3D scenes. We adopt the data pre-processed by PanoRecon  and randomly select 1,000 scene images from the test set as the single-image dataset. Note that all the images are captured parallel to the ground with camera locations at 0.75m height

   &  &  &  \\   & CDLI \(\) & F-ScoreF & CDLI-S \(\) & CDLI \(\) & F-ScoreF & CDLI-S \(\) & CDLI \(\) & F-ScoreF \\  Mesh R-CNN  & 0.449 & 0.471 & 23.90 & 0.265 & 0.406 & 21.87 & 0.268 & 0.408 & 25.42 \\ Total3D  & 0.198 & 0.520 & 18.44 & 0.133 & 0.400 & 26.93 & 0.390 & 0.780 & 24.01 \\ PanoRecon  & 0.120 & **0.125** & 31.94 & 0.355 & 0.417 & 17.11 & 0.326 & 0.440 & 17.13 \\  Ours & **0.109** & 0.134 & **35.67** & **0.106** & **0.089** & **73.19** & **0.113** & **0.110** & **70.48** \\  

Table 1: Comparisons on scene reconstruction from single images. Lower is better for CDLI (i.e., Chamfer Distance), higher is better for F-Score. CDLI-S is the single-direction Chamfer Distance from the generated objects to the ground truth meshes.

Figure 6: Comparisons on scene reconstruction from single images under the 3D-Front dataset.

above the floor in the 3D-Front dataset. We follow PanoRecon to achieve the corresponding ground truth mesh for each image by only keeping the geometry at the same view and cull anything outside of the view frustum.

The Replica  dataset is an indoor scene dataset which contains 8 scanned 3D indoor scene with highly photo-realistic 3D indoor scene reconstruction at both room and flat scale. We adopt the pre-processed data provided by MonoSDF  and sample one image for each scene as the single-image dataset. The ground truth meshes are obtained with the same way as 3D-Front.

The BlendSwap  dataset is a high-fidelity synthesis 3D scene dataset collected by Neural-RGBD , containing 9 scenes with complex geometries. We collect single-view images and corresponding ground truth meshes with the same way as Replica dataset.

The ScanNet  dataset is a real-word 3D scene dataset captured by RGB-D cameras. We select 7 scenes from the test set of ScanNet and sample one image from each scene as the input.

**Baselines.** We mainly compare our method with the state-of-the-art methods in scene reconstruction from single images, i.e., Mesh R-CNN , Total3D  and PanoRecon . Note that all these methods are data-driven methods and trained under 3D datasets with ground truth 3D annotations, while our method solves the task in a zero-shot manner. This means that we do not require any data-driven training on any 3D or 2D datasets, which is a much more flexible and general solution for the single image reconstruction task. We direct evaluate these methods with the official codes and the pre-trained models for numerical and visual comparisons.

**Metrics.** We use Chamfer Distance and F-Score with the default threshold of 0.1 following  as metrics. Since Mesh R-CNN and Total3D only predicts the 3D objects and do not generate the backgrounds (e.g. wall and floor), we further report the single-direction Chamfer Distance from the generated objects to the ground truth meshes, i.e., CDL1-S, to only evaluate the accuracy of generated objects. Note that Total3D can generate the scene layout which can roughly represent the background, however, we find that Total3D generates layouts with large errors on all the three datasets. Therefore we do not keep the layout of Total3D for evaluation. While we achieve the background points by back-projecting the segmented background depth maps. We sample 10k points on the ground truth

Figure 7: Comparisons on scene reconstruction from single images under Replica and BlendSwap dataset.

meshes and the generated scenes of each methods for evaluation. Please refer to the appendix for more details on evaluation.

### Scene Reconstruction on 3D-Front

Tab. 1 reports numerical comparisons on the 3D-Front dataset. We achieve the best performance among the state-of-the-art methods. Specifically, PanoRecon is trained under the 3D-Front dataset, therefore it shows convincing results in this dataset. Mesh R-CNN and Total3D are trained under Pix3D /ShapeNet  and SUN-RGBD /Pix3D datasets, respectively.

The qualitative comparison is shown in Fig. 6, where we remove the background geometries for PanoRecon, ours and GT for a clear visual comparison on the generated instances among all the methods. We further show the colored scene since the used object generator ShapE is able to generate textured 3D objects. The visualization demonstrates our superior performance to produce accurate and visual-appealing scene reconstruction from merely a single image in a zero-shot manner.

### Scene Generation of Open-World Images

We further evaluate our method under the open-world images from the BlendSwap dataset and the indoor scene dataset Replica. The quantitative comparisons in these two datasets are shown in Tab. 1, where we achieve the best performance over all the baseline methods. Note that the performance of PanoRecon  largely degrades under open-world scene images compared to the performance under 3D-Front dataset. The reason is that PanoRecon fails to generalize to the out-of-distribution inputs and can only handle the specific image patterns in the trained 3D-Front dataset.

The visual comparison is shown in Fig. 7, where we significantly outperform the previous works in the generation accuracy and completeness. Specifically, as shown in the \(3\)-rd and \(5\)-th row in Fig. 7, our method generates more accurate geometries for the table with thin legs and the chair with a

Figure 8: Comparisons of the scene reconstructions under the real-captured images from ScanNet.

complex back. While Mesh R-CNN and Total3D can only generate the coarse 3D shapes and also fail to estimate accurate layout.

### Scene Reconstruction from Real Images

We further evaluate deep prior assembly under the ScanNet  using real images. For a qualitative comparison with other methods, we select 7 scenes from the test set of ScanNet and sample one image from each scene as the input. We compare deep prior assembly with the state-of-the-art methods in scene reconstruction from single images, e.g., Mesh R-CNN  and Total3D . We do not compare with PanoRecon  here since it fails to generalize to the out-of-distribution inputs and can only handle the specific image patterns in the trained 3D-Front dataset, as demonstrated in Fig. 7.

We show the visual comparisons in Fig. 8, where we successfully reconstruct scenes from real images and significantly outperform the previous works in the reconstruction accuracy and completeness. This demonstrates the huge potentials of the assembled deep priors in reconstructing real-world 3D scenes. Note that the real-world images are often blurry and corrupted when the camera doesn't focus well, e.g., the blurry input image shown in the 5-th row in Fig. 8. While our proposed deep prior assembly can also handle these challenging situations due to the powerful and robust deep priors from the large vision models.

### Ablation Study

**Framework Design.** To evaluate the major components in our methods, we conduct ablations under the Replica dataset  and report the results in Tab. 2. We first justify the effectiveness of introducing Stable-Diffusion for enhancing and inpainting images as shown in 'W/o Stable-Diffusion', where we directly adopt the segmented instances for shape generation without leveraging Stable-Diffusion for enhancing and inpainting them. We then report the performance of removing the 2D or 3D matching constraints as shown in 'W/o 2D-Matching' and 'W/o 3D-Matching'. The ablation studies demonstrate the effect of each design by significantly improving the generation performance. Note that the pose/scale optimization is broken without 3D-Matching since the only 2D-Matching does not involve depth information.

**Effect of Open-CLIP and RANSAC-like solution.** We further evaluate the effectiveness of filtering bad samples with Open-CLIP and the RANSAC-like solution for robust pose / scale optimization. The results is shown in Tab. 3, where both components improve the scene reconstruction accuracy.

## 5 Conclusion

We introduce deep prior assembly, a novel framework that assembles diverse deep priors from large models for scene reconstruction from single images in a zero-shot manner. This approach breaks down the task into several sub-tasks, each of which is handled by a deep prior. We do not rely on any 3D or 2D data-driven training, and provide the key solutions on layout estimation and occlusion parsing to make all deep priors work together robustly. We report analysis, numerical and visual comparisons to show remarkable performance over the latest methods.

## 6 Acknowledgement

This work was supported by National Key R&D Program of China (2022YFC3800600), the National Natural Science Foundation of China (62272263, 62072268), and in part by Tsinghua-Kuaishou Institute of Future Media Data.

   Ablation & CDL1-S \(\) & CDL1 \(\) & F-Score\(\) \\  W/o Stable-Diffusion & 0.128 & 0.125 & 67.22 \\ W/o 2D-Matching & 0.124 & 0.121 & 68.42 \\ W/o 3D-Matching & 0.199 & 0.168 & 56.08 \\  Full & **0.113** & **0.110** & **70.48** \\   

Table 2: Ablation studies on framework designs.

   Open-CLIP & RANSAC & CDL1-S \(\) & CDL1 \(\) & F-Score\(\) \\  ✓ & ✗ & 0.121 & 0.118 & 69.28 \\ ✗ & ✓ & 0.129 & 0.123 & 68.92 \\  ✓ & ✓ & **0.113** & **0.110** & **70.48** \\   

Table 3: Ablation studies on the effect of Open-CLIP filtering and RANSAC-like solution.