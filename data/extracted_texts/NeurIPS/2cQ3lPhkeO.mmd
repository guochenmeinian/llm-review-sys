# Hongyi Guo\({}^{1}\)  Yingxiang Yang\({}^{3}\)  Jose Blanchet\({}^{2}\)  Zhaoran Wang\({}^{1}\)

Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer

 Zhihan Liu\({}^{1}\)  Miao Lu\({}^{2}\)  Shenao Zhang\({}^{1}\)  Boyi Liu\({}^{3}\)\({}^{1}\)Northwestern University \({}^{2}\)Stanford University \({}^{3}\)ByteDance Inc.

{zhihanliu2027,shenaozhang2028,hongyiguo2025}@u.northwestern.edu

{miaolu, jose.blanchet}@stanford.edu, zhaoranwang@gmail.com

{boyi.liu01, yingxiang.yang}@bytedance.com

Equal contribution.

###### Abstract

Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output undesired responses. We investigate this problem in a principled manner by identifying the source of the misalignment as a form of distributional shift and uncertainty in learning human preferences. To mitigate overoptimization, we first propose a theoretical algorithm that chooses the best policy for an adversarially chosen reward model; one that simultaneously minimizes the maximum likelihood estimation of the loss and a reward penalty term. The penalty term is introduced to prevent the policy from choosing actions with spurious high proxy rewards, resulting in provable sample efficiency of the algorithm under a _partial coverage_ style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy-to-implement reformulation. Using the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines: (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss that explicitly imitates the policy with a (suitable) baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fine-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of RPO compared with DPO baselines. Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.

## 1 Introduction

A key step in building state-of-the-art LLMs is Reinforcement Learning from Human Feedback (RLHF) [12; 87], which aligns pretrained LLMs with human preferences using human assessment data, making the model more helpful, truthful, and harmless [42; 10]. Typically, RLHF first learns a reward model from data (pair-wise comparisons of responses) to quantify the human preferences of LLM outputs. Then it fine-tunes the LLM to maximize the learned reward using RL techniques.

In this pipeline, a crucial challenge is _reward overoptimization_ or _reward hacking_[40; 62; 25]. Since the reward model is learned from finite data, it might not be perfectly aligned with the underlying human preference. Optimizing the LLM towards such an imperfectly learned and potentially overfittedreward model leads to performance degeneration and a substantial decrease in the probability of choosing the preferred responses in the data [26; 45]. Given the importance of RLHF and the outlined challenge, a crucial research question is: _How to mitigate reward overoptimization in RLHF in a principled and efficient manner for better alignment?_

To answer the question, we model RLHF as an offline contextual bandit  and ascribe overoptimization to distributional shifts and reward uncertainty. Intuitively, when fine-tuning an LLM, the response (action) distribution of the tuned LLM could deviate from that of the training data. For the out-of-distribution responses, which are dissimilar with (or not well covered by) the responses in the data, the high inherent uncertainty of underlying human preferences could make the learned reward model misleading for out-of-distribution responses. In this situation, reward overoptimization can occur because the LLM is fine-tuned towards maximizing a reward model with defective out-of-distribution prediction, giving a potential consequence that the LLM responses are favored by the learned reward but less preferred by a human . We illustrate these types of distributional shift and reward uncertainty issues inherent to overoptimization in Figure 1.

In this paper, we propose a new RLHF algorithm to mitigate reward overoptimization. From a high level, our theoretical algorithm seeks the best LLM for an _adversarially_ chosen reward model that minimizes the sum of its maximum likelihood estimation loss and its own expected reward value. Intuitively, since the reward value is also minimized when minimizing the sum, it can automatically prevent the misleadingly high reward caused by the uncertainty inherent in having access to finite data. Furthermore, we show that the theoretical algorithm enjoys an easy implementation: it simply adopts a supervised fine-tuning (SFT) loss as a regularizer during training. By explicitly regularizing the LLM to imitate high-quality responses (e.g., preferred responses in dataset), the algorithm can effectively mitigate the issue of overoptimization. We establish theoretical guarantees and conduct experiments to demonstrate our findings, which we summarize next.

### Our Contributions and Related Works

We summarize our contributions in three areas as follows.

**A theoretical algorithm under general function approximation.** Our first contribution is a new theoretical algorithm (Algorithm 1). It features an _unconstrained maximin_ problem, outputting the optimal policy (LLM) against an adversarially chosen reward model that minimizes the summation of: (a) the MLE loss for estimating the underlying reward; and (b) a reward expected value term as a penalty that aims to prevent spuriously high reward estimation caused by data uncertainty and insufficient coverage. Algorithm 1 is compatible with general function approximations of the reward model, meaning that we do not impose any specific structural form to the hypothesis class of reward, demonstrating its generality, especially in language modeling.

In this regime of reward class, we establish the finite-sample suboptimality gap of Algorithm 1 as \(}(C_{}^{2}_{ }/N})\) when competing with any LLM in terms of the underlying true human reward (Theorem 5.3). Here \(N\) is the number of human comparison data, \(_{}\) is the complexity of the reward model class \(\), and \(C_{}\) characterizes the coverage of the preference dataset with respect to the response distribution of the LLM to compete (please see Assumption 5.2 for details). This indicates that, as long as the training data well cover the LLM \(\) to compete, the algorithm is guaranteed to align an LLM to output responses as good as \(\) in terms of human reward, without suffering from overoptimization caused by distributional shifts and inherent uncertainty in human preference.

**An easy-to-implement practical objective.** Moving towards practice, we show that the objective of Algorithm 1 adopts a surprisingly simple and equivalent form for its use in practice. Specifically, with mild regularity conditions, we prove that the _maximin_ objective (Algorithm1) is equivalent to the

Figure 1: _Left:_ Reward overoptimization due to the distributional shift and uncertainty in reward. _Right:_ Overoptimization causes the probability of outputting preferred responses in the preference data to decrease substantially using original DPO proposed by . Our algorithm (RPO) significantly alleviates this decrease. See more discussions in Section 6.

corresponding _minimax_ objective, which is further reduced to a single minimization problem for the reward model since its inner problem adopts a closed form solution. Inspired from recent progress in RLHF that explores reward-model-free methods to align LLMs , we further re-parameterize the reward model via its corresponding KL-regularized optimal policy. Then the minimization objective of the reward modeling naturally translates to a target for directly aligning the LLM, which we call Regularized Preference Optimization (RPO; Algorithm 2). The objective of RPO features a simple weighted combination of two losses:

\[=+.\]

Here the Preference optimization loss coincides with the DPO  objective, tending to optimize the LLM towards maximizing the underlying true reward. The Imitation (SFT) loss explicitly supervises the LLM to mimic the responses from a proper distribution well covered by the dataset. The choice of the distribution is guided and justified by our theory of Algorithm 1, but can also be flexibly adapted in practice, e.g., the preferred response in the dataset, or the responses of the initial model.

We highlight that the Imitation (SFT) loss serves as an important term to mitigate overoptimization. Even though the original DPO objective has already involved a KL regularization between the tuned LLM and the initial LLM, is not enough to prevent overoptimization. As we elaborate in Section 4, the KL-regularization weight of the DPO objective could only control the scale of the gradient per training example, while the RPO objective can further modify the gradient direction. Calling back to the theoretical Algorithm 1, such a modification of gradient direction originates from the reward penalty in the adversarial objective for the reward model. This modification, as we expose in our theoretical analysis, helps to mitigate overoptimization. _Thus, incorporating SFT loss in RLHF gives you a regularizer that provably mitigates overoptimization._

**Empirical evaluations.** Following the training setup of two series of released chat models Zephyr-7b-beta (trained on the Ultrafeedback dataset  by DPO) and Zephyr-7b-gamma (trained on the Argilla-DPO-Mix-7K dataset  by DPO) , we implement RPO for the beta series and gamma series respectively to show that: (i) RPO is a flexible plug-in module and can be applied to different reference models. (ii) RPO can alleviate the overoptimization issue. (iii) RPO consistently achieves better alignment performance than DPO in in-data distribution. (iv) RPO can also achieve consistently better performance in standard LLM benchmarks like MT-bench and AlpacaEval 2.0, which shows its potential of mitigating overoptimization for better alignment performance, justifying our theory.

**Related works.** Due to space limitation, we refer the readers to Appendix A for a detailed discussion.

## 2 Preliminaries of RLHF

In this section, we introduce the mathematical framework of studying RLHF for aligning LLMs. We adopt the framework of offline contextual bandits , where we identify the context space \(\) as the space of prompts and the action space \(\) as the space of responses. An LLM, defined as a policy \((|):()\), takes a prompt \(x\) as input and output a response \(a\) from \(a(|x)\).

**Preference model.** Given any reward function \(r:\) belonging to certain reward class \(\) that represents the "human's ratin" of LLM responses given prompts, we consider the Bradley-Terry model  of human preference. That is, given a prompt \(x\) and two responses \(a^{1},a^{0}\), the probability of \(a^{1}\) being preferred to \(a^{0}\) (denoted by \(y=1\), otherwise \(y=0\)) is given by

\[_{r}(y=1|x,a^{1},a^{0})=))}{(r(x,a^{1}))+ (r(x,a^{0}))}=r(x,a^{1})-r(x,a^{0}),\] (2.1)

where \((z)=1/(1+(-z))\) is the sigmoid function. For simplicity of future discussion, we explicitly write out the dependence of the preference probability \(_{r}()\) on the reward model \(r\). In the section of theory, i.e., Section 5, we specify the assumptions on the reward model class \(\).

**Learning protocol.** Typically, the RLHF pipeline starts from certain reference policy \(^{}\) obtained from pretraining. Then RLHF aligns the LLM based on certain human preference data. In this work, we consider offline RLHF setup, where the LLM is aligned using a fixed offline preference dataset \(\). It consists of \(N\) i.i.d. tuples in the form of \(=\{(x_{i},a_{i}^{1},a_{i}^{0},y_{i})\}_{i=1}^{N}\). Here the prompt \(x_{i}\) and the responses \(a_{i}^{1},a_{i}^{0}\) are distributed according to: \((x,a^{1},a^{0})_{}()\), and conditioning on \((x_{i},a_{i}^{1},a_{i}^{0})\), \(y_{i}\) is distributed according to (2.1) for an underlying true (but unknown) reward model \(r^{}\).

**Performance metric.** The target of RLHF is to align an LLM, or equivalently, to learn a policy \(\), so as to maximize the expected true reward \(r^{}\). Thus, we define the value function of any policy \(\) as

\[J()=_{x d_{0},a(|x)}r^{}(x,a).\] (2.2)

Here we allow the prompt distribution \(d_{0}()\) to be different from that of the offline dataset distribution \(_{}()\), but is assumed to be known. In the meanwhile, we consider the policies that share the same support as the reference policy \(^{}\), that is, we take a policy class \(\) as

\[=:()\,\, ((|x))(^{}(|x) ),\  x}.\] (2.3)

The performance gap of a learned policy \(\) w.r.t. any other policy \(\) is measured as

\[^{}()=J()-J(),\ \ .\] (2.4)

The goal is to propose a sample-efficient and also implementation-friendly algorithm to learn a policy \(\) able to compete with any given policy \(\) in terms of \(^{}()\), with sample complexity polynomial in \(1/\) and logarithmic in the complexity of \(\).

## 3 A Theory-motivated Objective

Our method seeks to find the best policy \(\) against an _adversarially_ chosen reward model \(_{}\) that minimizes a weighted sum of its expected value and the maximum likelihood estimation (MLE) loss. Intuitively, such a reward model can prevent the overoptimization issue by taking its own value into account when minimizing the MLE loss. Since the reward value is also minimized when minimizing the sum, this method prevents the misleadingly high reward caused by the uncertainty due to finite data. Formally, given two hyperparameters \(,>0\) and a "baseline policy" \(^{}\), we define

\[T^{}_{,}()=_{r}\{ _{x d_{0},a^{1}(|x),\\ a^{0}^{}(|x)}r(x,a^{1})-r(x,a ^{0})-(|x)+_{ }(r)\},\]

where the loss function \(_{}()\) is the average negative log-likelihood function of the BT model (2.1) (and here it becomes the cross-entropy loss) over the preference dataset \(\), defined as

\[_{}(r)=-}_{}y_{i} r(x_{i},a_{i}^{1})-r(x_{i},a_{i}^{0})+( 1-y_{i})r(x_{i},a_{i}^{0})-r(x_{i},a_{i}^{1}) .\] (3.1)

As we can see, \(T^{}_{,}()\) is the minimum value of a weighted sum of the MLE loss and the expected reward value of \(\), but with two important modifications that we explain in the following.

_Firstly_, we subtract another expected reward of certain policy \(^{}\). This is because the BT model (2.1) essentially only uses the reward differences to define the preference probabilities. As a result, the data can only reveal information of the differences between the true reward \(r^{}\) of different responses . Accordingly, we subtract such a baseline expected reward value to match this observation. The choice of the baseline policy is discussed in the theory part (Section 5) and experiments (Section 6).

_Secondly_, we subtract a KL divergence between \(\) and \(^{}\) from the expected reward, weighted by the coefficient \(>0\). Such a term is for practical considerations that would be explained in Sections 4 and 5.2. We note that the KL regularized reward is commonly adopted in RLHF practice to ensure the learned policy is not far away from the reference policy [42; 71].

Finally, the overall algorithm design (Algorithm 1) is to output the policy that maximizes \(T^{}_{,}()\), i.e., \(*{argmax}_{}T^{}_{, }()\), which gives the following theoretical target:\[*{argmax}_{}_{r }\{_{x d_{0},a^{1}( |x)\\ a^{0}^{}(|x)}r(x,a^{1})-r(x,a^{0 })-(|x)\|^{}(|x )+_{}(r)\}.\] (3.2)

Given the form of (3.2), we name it the _maximin_ objective in the sequel. Upon seeing (3.2), one might be arguing that such a theory-motivated objective seems hard to implement in practice. Nevertheless, in the coming Section 4, we demonstrate that the maximin objective (3.2) adopts an easy-to-implement equivalent form, allowing us to design a practical algorithm for aligning LLMs.

## 4 An Equivalent and Implementation-friendly Objective

In this section, we propose another _minimax_-style objective that is equivalent to the maximin objective (3.2). Based on the minimax objective, we propose a new LLM aligning algorithm called Regularized Preference Optimization (RPO). It draws inspirations from the reparametrization technique originated in Direct Preference Optimization (DPO)  and goes beyond to further address the overoptimization issue in offline RLHF by incorprating an SFT loss as an explicit adversarial regularizer.

**An equivalent minimax objective.** If the reward model class \(\) satisfies certain regularity conditions, which we discuss in detail in Section 5.2, the minimax theorem holds: solving the _maximin_ objective (3.2) is _equivalent_ to solving a _minimax_ target, given by

\[_{r}_{}\{_{ x d_{0},a^{1}(|x)\\ a^{0}^{}(|x)}r(x,a^{1})-r(x,a^{ 0})-(|x)\|^{}( |x)+_{}(r)\}.\] (4.1)

Such a minimax formulation (4.1) is the starting point of our practical algorithm. The magic of (4.1) is that the inner maximization problem adopts a closed form solution, which further simplifies such an objective. To see this, note that given any reward model \(r\), the inner problem is equivalent to

\[_{}_{x d_{0},a^{0} (|x)}r(x,a)- (|x)\|^{}(|x)}.\] (4.2)

It has been well established that the policy that maximizes the KL-regularized expected reward (4.2) has a closed form solution. Due to its importance, we present it as the following lemma.

**Lemma 4.1** (Oracle optimal KL-regularized policy).: _Given any reward model \(r\), the optimal policy \(_{r}\) to the maximization problem (4.2) is given by_

\[_{r}(|x)=(x)}^{}(|x) (^{-1}r(x,)),\ Z_{r}(x)=_{a} (^{-1}r(x,a))^{}(a|x),\]

_and correspondingly the optimal value of (4.2) is given by (4.2) \(=_{x d_{0}}[(Z_{r}(x))]\)._

Specifically, by Lemma 4.1, we can solve the inner maximization problem in (4.1) and obtain that

\[=_{r}_{ x d_{0},a^{0}^{}(|x)} -r(x,a^{0})+(Z_{r}(x))+_{ }(r)}.\]

Furthermore, from Lemma 4.1, one immediately see that given any reward model \(r\), we can reparameterize it via its corresponding optimal KL-regularized policy \(_{r}\), that is,

\[r(x,)=((|x)}{^{}( |x)})+(Z_{r}(x)).\] (4.3)

Taking (4.3) back into (4.1), we are able to further simplify it as

\[=_{r}_{ x d_{0},a^{0}^{}(|x)} -(_{r}(a^{0}|x))+_{}( ((|)}{^{}(| )}))}.\] (4.4)

Thanks to the KL-regularization term in the original minimax objective (4.1) (or equivalently, the maximin objective (3.2)), we have the following theorem. It theoretically shows that the policy \(_{}\) associated with the reward model \(\) solving (4.4) also solves the maximin target (3.2) of the theoretical algorithm (Algorithm 1) that enjoys finite-sample convergence guarantees. (Please see Section 5.2 for a formal statement and proof of Theorem 4.2).

**Theorem 4.2** (Equivalence between _maximin_ and _minimax_ algorithm (informal)).: _Under certain regularity assumptions on \(\) and given \(,>0\), solving the minimax objective (4.1) via (4.4), i.e.,_

\[=*{argmin}_{r}_{x  d_{0},a^{0}^{}(|x)}-(_{r }(a^{0}|x))+_{}(((|)}{^{}(|)}))},\]

_then the corresponding optimal KL-regularized policy \(_{}\) also solves the maximin objective (3.2)._

**Regularized Preference Optimization.** Target (4.4) gives a quite simple objective to use in practice! Since (4.4) depends on \(r\) only through its corresponding optimal policy \(_{r}\), one can formulate a minimization objective over a parameterized policy \(_{}\), i.e., the LLM to be aligned, and directly optimize the parameters \(\). More formally, the new RLHF objective becomes

\[_{}_{}() _{x d_{0},a^{0}^{}( |x)}-(_{}(a^{0}|x))}_{}+_{}(( (|)}{^{}(|)}) )\}}.\] (4.5)

In (4.5), the second term coincides with the objective of DPO algorithm  which optimizes the policy towards maximizing the underlying true reward, and the first term stands for a regularization term weighted by \(\) which _explicitly_ regularizes the policy to imitate the baseline policy. Therefore, we name the resulting algorithm as Regularized Preference Optimization (RPO). We summarize it abstractly in Algorithm 2. As for DPO, implementing RPO does not require to maintain a reward model \(r\). Thus it is computationally more friendly compared to reward-based algorithms.

**How does RPO improve DPO?** We illustrate the effect of the imitation loss by analyzing the gradient of the RPO target \(_{}()\) in (4.5). Notice that by (4.5) we have

\[_{}_{}()=_{x d_{0},a^{0}^{}(|x)}-_ {}(_{}(a^{0}|x))}_{}+_{}()}_{},\]

where the derivative of the DPO loss \(_{}_{}()\) is given by the following,

\[_{}_{}()\!=\!-}_{ }\!_{ }(x,a_{})-_{}(x,a_{}) }_{}\!\!\!_{}_{ }(a_{}|x)\!-\!_{}_{}(a_{}|x)\!.\]

For simplicity we denote \(_{}(x,a)=(_{}(x,a))/(^{ }(x,a))\), \(a_{}\) for the chosen response and \(a_{}\) for the rejected response. Intuitively, RPO (4.5) modifies the **gradient direction** of DPO to ensure the alignment with the baseline policy \(^{}\), and the hyper-parameter \(\) controls the power of alignment. In comparison, the hyper-parameter \(\) in DPO only controls the **gradient weight** when increasing the likelihood of \(a_{}\) and decreasing the likelihood \(a_{}\). In this perspective, the hyper-parameter \(\) only changes the scale of the gradient instead of the direction. By introducing \(\), we stabilize the training and reduce the side-effect of uncertain labels in data to prevent overoptimization.

## 5 Theoretical Analysis

In this section, we establish theoretical analysis for Algorithms 1 and 2. We take the space of prompts and responses as compact subsets \(^{d_{}}\) and \(^{d_{}}\). We take the policy class \(\) as (2.3).

### Establishing the Sample Complexity of Maximin Objective (Algorithm 1)

**Assumption 5.1** (True reward model).: _We assume that the true reward model \(r^{}\), and for any \(r\) and \((x,a)\), it holds that \(r(x,a)[0,R]\)._

**Assumption 5.2** (Partial coverage coefficient ).: _Given a policy \(\), the coverage coefficient of the offline dataset distribution \(_{}\) w.r.t. reward model class \(\), policy \(\), and the baseline policy \(^{ base}\), denoted by \(C_{_{}}(;,^{ base})\), is defined as_

\[\{0,_{r}_{x d_{0},a^{1} (|x),a^{0}^{ base}(|x)}(r^{}(x,a^{1})-r^{ }(x,a^{0}))-(r(x,a^{1})-r(x,a^{0}))}{_{(x,a^{1},a^{ 0})_{}}[(r^{}(x,a^{1})-r^{}(x,a^{0}))-( r(x,a^{1})-r(x,a^{0}))^{2}]}}\}.\]

_We assume that \(C_{_{}}(;,^{ base})<+\) for the policy \(\) to compete. We remark that the quantity \(C_{_{}}(;,^{ base})\) is upper bounded by the density ratio \(\|d_{0}^{ base}/_{}\|_{}\)._

Assumption 5.1 is standard in sample complexity analysis [78; 74; 85]. Assumption 5.2 characterizes how well the dataset \(\) covers the policy \(\) to compete. To achieve provable sample efficiency, we only require that \(\) covers the target policy \(\), a weak partial coverage style assumption for theoretical analysis. To illustrate it, when calling back to Figure 1, the data distribution therein well covers those nearly optimal responses under \(r^{}\), but does not sufficiently cover the responses with low \(r^{}\).

Under such a partial coverage data condition, however, human preference of responses \(a\) that are not well covered by the dataset \(\) can be poorly estimated, misguiding the policy \(\) to behave suboptimally if it is overoptimized (recall Figure 1). Fortunately, the following theorem shows that Algorithm 1 provably mitigates the overoptimization issue and achieves a finite-sample convergence of the suboptimality gap (2.4) competing with \(\). Proof is in Appendix D.

**Theorem 5.3** (Suboptimality of Algorithm 1).: _Taking the policy class \(\) as (2.3), supposing that Assumptions 5.1 and 5.2 hold, and assuming that the reward model class \(\) has a finite \(\)-epsilon covering number under \(\|\|_{}\)-norm \(_{}(,\|\|_{})<+\) with \(=(6(1+e^{R}) N)^{-1}\). Setting_

\[=(1+(R))^{-2}_{}( ,\|\|_{})/)/N},=1/\]

_in Algorithm 1. Then the output policy \(\) of Algorithm 1 satisfies that with probability at least \(1-\),_

\[^{}()\!\!1+(R) ^{2}C_{_{}}(;,^{ base}) ^{2}+1+4_{x d_{0}}( |x)\|^{ ref}(|x)}{4},\]

_where \(=_{}(,\|\|_{ })/)}\) with \(=(6(1+e^{R}) N)^{-1}\). Here, \(N\) denotes the number of preference pairs in \(\), \(R\) denotes the upper bound of the reward models, and the partial coverage coefficient \(C_{_{}}(;,^{ base})\) is defined in Assumption 5.2._

**Remark 5.4** (Choice of the baseline policy).: _As is indicated by Assumption 5.2, the least requirement is that \(^{ base}\) can be covered by the offline data distribution. E.g., we can take \(^{ base}\) as the distribution of the preferred responses in the data. In this case, the SFT loss in RPO explicitly regularizes the LLM to imitate the preferred responses. We choose this type of baseline policy in our experiments._

### Equivalence between Maximin and Minimax Objectives

Now we formally show that the theoretical target (maximin objective (3.2)) and the target for practical algorithm design (minimax objective (4.1)) are equivalent under certain regularity conditions. This can naturally extend the sample complexity of Algorithm 1 (Section 5.1) to that of minimax-based algorithms in Section 4, providing the theoretical guarantee for our practical algorithm design (RPO).

First, for notational simplicity, we denote the optimization target we investigate in Sections 3 and 4 as

\[(,r):=_{x d_{0},a^{1} (|x)\\ a^{0}^{ base}(|x)}r(x,a^{1})-r(x,a^{0})-  D_{}(|x)\|^{ ref}(|x) +_{}(r),\] (5.1)

for any \((,r)\). Our result relies on the following assumptions on the reward model class \(\).

**Assumption 5.5** (Regularity of reward model class).: _We assume the following things on the reward model class \(\): (i) the space \(\) is a compact topological space; (ii) the function \(\) in (5.1) is convex-like on \(\), that is, for any \(r_{1},r_{2}\) and \(\), there exists \(r_{3}\) such that_

\[(,r_{3})(,r_{1})+(1-)(,r_{2}), ,\] (5.2)We note if \(\) is convex, e.g., a linear model class [85; 71; 86] or more general the Lipschitz continuous model class \(\), we can directly obtain that the function \((,)\) is _convex_ over \(\) (since the dependence on \(r\) is linear terms plus a convex loss \(_{}\) of \(r\)), which implies the convex-like property (5.2). Under Assumption 5.5, it holds that (Lemma E.1)

\[_{}\,_{r}(,r)=_{r}\, _{}(,r).\] (5.3)

Furthermore, thanks to the KL-divergence regularization in \(\) which intuitively makes \(\) "strongly concave" over the policy \(\), (5.3) can gives us the following stronger result, proved in Appendix E.1.

**Theorem 5.6** (Formal statement of Theorem 4.2).: _For the policy class \(\) defined in (2.3) and the reward model class \(\) satisfying Assumption 5.5, consider the following policy defined as_

\[_{}*{argmax}_{}(,),*{argmin}_{r }\,_{}(,r).\] (5.4)

_Then the policy \(_{}\) also satisfies the maximin objective (3.2) of Algorithm 1, that is,_

\[_{}*{argmax}_{}\,_{r }(,r).\]

Theorem 5.6 shows that the optimal KL-regularized policy associated with the reward model solving the minimax objective (3.2) also solves the maximum objective (i.e., objective (4.1) of Algorithm 1). This further allows us to extend our theoretical guarantee of Algorithm 1 (Section 5.1) to that of minimax-based algorithms, justifying our practical algorithm design in Section 4.

**Corollary 5.7** (Suboptimality of minimax-based algorithm).: _Take the policy class \(\) in (2.3) and the reward model class satisfying Assumption 5.5. Given any given policy \(\) to compete, if Assumption 5.2 holds for \(\), then under the same choice of \(\) and \(\) as in Theorem 5.3, the policy \(_{}\) defined in (5.4) satisfies that \(*{Gap}^{}(_{})}(1/ )\) with probability at least \(1-\)._

## 6 Experiments

In this section, we provide a detailed empirical analysis of RPO to highlight the following four key points: (1) RPO is a flexible plug-in module and can be applied to different reference models. (2) RPO can alleviate the overoptimization issue in the training phase by giving more trust to the chosen responses in the preference dataset. (3) As a justification of our theoretical analysis, RPO achieves better alignment performance than DPO in in-data distribution. (4) RPO can also achieve consistently better performance in LLM benchmarks like MT-bench  and AlpacaEval 2.0 , which shows the potential of mitigating overoptimization for better generalization performance. The code for the experiments can be found in https://github.com/YSLIU627/Regularized-Preference-Optimization/tree/master.

Experiment setup.To show that RPO is a flexible plug-in module regardless of the reference model, we follow the training setup for two well-studied series of released chat models with around 7 billion parameters trained by DPO: Zephyr-7b-beta and Zephyr-7b-gamma  to implement RPO in beta and gamma series. Mirrored by their training configurations, we introduce how we select the reference model and the preference dataset for our training on these two series as follows. For the beta series, we use mistral-7b-sft-beta as the reference model \(^{}\). mistral-7b-sft-beta is a fine-tuned version of mistral-7b-v0.1 on the distilled version of the UltraChat dataset , which contains approximately 200k examples of multi-turn dialogues generated by GPT-3.5-TURBO. For the training preference dataset, we use Ultrafeedback Dataset , which consists of approximately 60k prompts. For the gamma series, we use zephyr-7b-gamma-sft-v0.1 as our reference model \(^{}\). zephyr-7b-gamma-sft-v0.1 is a fine-tuned version of gema-7b on the Deita dataset , which involves around 10k distilled SET data. For the training preference dataset, we use Argilla-DPO-Mix-7K Dataset , which is a mixture of multiple distilled public preference datasets. For simplicity, we denote Ref. (beta) as the reference model, DPO (beta) as the model trained by DPO, RVO (beta) as the model trained by RPO, all for the beta series. We use the same notations for the gamma series.

Practical implementation.According to Algorithm 2 and as discussed in Remark 5.4, we implement RPO by adding an SFT loss (log probability of chosen responses in the preference dataset) to the original DPO loss. By comparing the evaluation performance on the test split of the training dataset, we select the hyperparameter \(\) as \(0.005\) for both RPO (beta) and RPO (gamma). During the training of DPO and RPO, We keep the remaining hyperparameters including \(\), batch size, and learning rate to be the same for a fair comparison. Please see Appendix F.1 for a detailed training configuration.

RPO alleviates overoptimization.As mentioned in the introduction part, DPO is observed to have a significant and continuous decrease in log probability on chosen responses  during training and we regard it as the consequence of overoptimization. Implied by our theory, overoptimization could arise when the model maximizes its own proxy reward formed on the responses less covered by the data. Due to the overoptimization, the model tends to disprefer the chosen responses as they are away from the maximizers of the proxy reward despite that some chosen responses are highly preferred by humans. Consistent with our theoretical conclusion, we empirically find that RPO can indeed alleviate overoptimization in DPO. During the training phase of both beta and gamma series, we observe that the log probability given by the RPO-trained model is notably higher than that given by the DPO-trained model for the chosen responses, which are shown in Fig. 1 and 2.

RPO improves the alignment ability in in-data distribution.For the in-data distribution evaluation, we select the 200 prompts (which are not used in the selection of \(\)) in the test split of the training dataset to let the reference model, DPO, and RPO generate the response respectively. We choose GPT-4 to annotate the preference in the response pairs. Though we instruct GPT-4 to give an annotation among win, lose, and, tie (please see the full prompt in Appendix F.2), GPT-4 may still give undesired annotations. Hence, we filter all the undesired annotations and collect 150 examples for evaluation. We report the pairwise win rate among Ref., RPO, and DPO in Table 1 for both the beta and gamma series. To show a more illustrative comparison between DPO and RPO, we provide the barplot to report the number of pairwise examples annotated by GPT-4 in Fig. 4 and Fig. 4. We observe that for both beta and gamma series, RPO has a better performance than DPO in terms of both RPO/DPO-SFT and RPO-DPO win rates. The performance improvement matches our theoretical results in Corollary 5.7, which shows the credit of the alleviation of overoptimization.

RPO consistently improves the benchmark performance.We further evaluate the reference model, RPO-trained model, DPO-trained model, and the officially released DPO-trained model for both beta and gamma series in two standard LLM chat benchmarks: MT-Bench and AlpacaEval 2.0. MT-Bench is a multi-turn benchmark that contains 160 questions across eight different domains of knowledge. The score for MT-Bench is evaluated by GPT-4 on a scale from 1 to 10. AlpacaEval 2.0 is a single-turn benchmark including 805 questions on different topics, mostly focused on helpfulness.

The metrics of AlpacaEval 2.0 are the win rate and Length-Control (LC) win rate compared with GPT-4 Preview (11/06), where the annotator is also GPT-4 Preview (11/06) and LC win rate is proposed to mitigate the length bias of GPT-4. The results are summarized in Table 2, which shows that RPO consistently exceeds the performance of all the competitors (DPO, Reference model, and the officially released model trained by DPO) on MT-Bench and AlpacaEval 2.0. We also provide additional results on the pairwise win rate for these two benchmarks in Appendix F.3 to illustrate the performance improvement. Finally, we remark that RPO is a flexible plug-in module and can steadily improve the benchmark performance without changing the original training configuration or accessing extra preference data. This also sheds light on the potential of mitigating overoptimization for better alignment and generalization performance.

RPO also improves the math, reasoning, and coding abilities.In addition to the MT-Bench and AlpacaEval 2.0 benchmarks, we introduce more benchmarks on the math, reasoning, and coding tasks for evaluations of the RPO algorithm. Specifically, we choose the Grade School Math 8K (GSM8K) , AI2 Reasoning Challenge (ARC) , and Mostly Basic Python Programming (MBPP)  to measure math, reasoning, and coding abilities of the model trained by RPO, respectively. To due space limitation, we refer the readers to Appendix G for the setups and results of these experiments.

## 7 Conclusions

This paper proposes a new algorithm that provably mitigates reward overoptimization in RLHF. We establish its finite-sample convergence under a partial coverage style data condition, and provide an equivalent practical implementation, RPO. As a flexible plug-in module, RPO exhibits consistent improvement over the DPO baseline and effectively mitigates overoptimization. Future work includes extending our idea of algorithm design to online (iterative) RLHF where preference data are collected and updated iteratively during LLM fine-tuning. We give more detailed discussions in Appendix B.