# On Measuring Fairness in Generative Models

Christopher T. H. Teo

christopher_teo@mymmail.sutd.edu.sg

&Milad Abdollahzadeh

milad_abdollahzadeh@sutd.sg

Ngai-Man Cheung

ngaiman_cheung@sutd.edu.sg

Corresponding Author

Singapore University of Technology and Design (SUTD)

###### Abstract

Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on **fairness measurement**, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., **4.98%\(\)0.62%** for StyleGAN2 _w.r.t._ Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. **Code and more resources:**https://sutd-visual-computing-group.github.io/CLEAM/.

## 1 Introduction

Fair generative models have been attracting significant attention recently [1; 2; 7; 8; 9; 10; 11; 12; 13]. In generative models [14; 15; 16; 17; 18], fairness is commonly defined as equal generative quality  or equal representation [1; 2; 7; 9; 12; 19; 20]_w.r.t._ some _Sensitive Attributes_ (SA). In this work, we focus on the more widely utilized definition - _equal representation_. In this definition, as an example, a generative model is regarded as fair _w.r.t._ Gender, if it generates Male and Female samples with equal probability. This is an important research topic as such biases in generative models could impact their application efficacy, e.g., by introducing racial bias in face generation of suspects  or reducing accuracy when supplementing data for disease diagnosis .

**Fairness measurement for generative models.** Recognizing the importance of fair generative models, several methods have been proposed to mitigate biases in generative models [1; 2; 7; 9; 12]. However, _in our work, we focus mainly on the accurate fairness measurement of deep generative models i.e. assessing and quantifying the bias of generative models_. This is a critical topic, as accurate measurements are essential to reliably gauge the progress of bias mitigation techniques. The general fairness measurement framework is shown in Fig. 1 (See Sec. 2 for details). This framework is utilized in existing works to assess their proposed fair generators. Central to the fairness measurement framework is a _SA classifier_, which classifies the generated samples _w.r.t._ a SA, in order to estimate the bias of the generator. For example, if eight out of ten generated face images are classified as Maleby the SA classifier, then the generator is deemed biased at \(0.8\) towards Male (further discussion in Sec. 2). We follow previous works  and focus on binary SA due to dataset limitations.

**Research gap.** In this paper, we study a critical research gap in fairness measurement. Existing works assume that when SA classifiers are highly accurate, measurement errors should be insignificant. As a result, the effect of errors in SA classifiers has not been studied. However, our study reveals that _even with highly accurate SA classifiers, considerable fairness measurement errors could still occur_. This finding raises concerns about potential errors in previous works' results, which are measured using existing framework. Note that the SA classifier is _indispensable_ in fairness measurement as it enables automated measurement of generated samples.

**Our contributions.** We make three contributions to fairness measurement for generative models. _As our first contribution_, we analyze the accuracy of fairness measurement on generated samples, which previous works  have been unable to carry out due to the unavailability of proper datasets. We overcome this challenge by proposing new datasets of _generated samples_ with manual labeling _w.r.t._ various SAs. The datasets include generated samples from Stable Diffusion Model (SDM)  -- a popular text-to-image generator-- as well as two State-of-The-Art (SOTA) GANs (StyleGAN2  and StyleSwin ) _w.r.t._ different SAs. Our new datasets are then utilized in our work to evaluate the accuracy of the existing fairness measurement framework. Our results reveal that the accuracy of the existing fairness measurement framework is not adequate, due to the lack of consideration for the SA classifier inaccuracies. More importantly, we found that _even in setups where the accuracy of the SA classifier is high, the error in fairness measurement could still be significant_. Our finding raises concerns about the accuracy of previous works' results , especially since some of their reported improvements are smaller than the margin of measurement errors that we observe in our study when evaluated under the same setup; further discussion in Sec. 3.

To address this issue, _as our second (major) contribution_, we propose CLassifier Error-Aware Measurement (CLEAM), a new more accurate fairness measurement framework based on our developed statistical model for SA classification (further details on the statistical model in Sec. 4.1).

Figure 1: **General framework for measuring fairness in generative models.** Generated samples with unknown ground-truth (GT) probability \(}\)_w.r.t._ sensitive attribute (SA) are fed into a SA classifier to obtain \(}\). Existing framework (Baseline) uses the classifier output \(}\) as estimation of \(}\). In contrast, our proposed CLEAM includes an improved estimation that accounts for inaccuracies in the SA classifier (see Alg. 1). **Our statistical model for fairness measurement.** This model accounts for inaccuracies in the SA classifier and is the base of our proposed CLEAM (see Sec. 4.1). **Â© Improvements with CLEAM.** CLEAM improves upon Baseline  by reducing the relative error in estimating the GT \(p_{0}^{*}\) for SOTA GANs: StyleGAN2  and StyleSwin , and Stable Diffusion Model . First two displays the Baseline and CLEAM estimates for each GAN, using ResNet-18 as the SA classifier for Gender and BlackHair. The Baseline incurs significant fairness measurement errors (_e.g._ 4.98%), even when utilizing a highly accurate ResNet-18 (\(\)97% accuracy). Meanwhile, CLEAM reduces the error significantly in all setups, _e.g._ in the first panel, the error is reduced: 4.98% \(\) 0.62%. Similarly, in the second row, CLEAM reduces measurement error significantly in the Stable Diffusion Model , using CLIP  as the SA classifier for Gender, _e.g._ first panel: 9.14% \(\) 0.05% (Detailed evaluation in Tab. 1 and Tab. 2). **Best viewed in color.**

Specifically, CLEAM utilizes this statistical model to account for the classifier's inaccuracies during SA classification and outputs a more accurate fairness measurement. We then evaluate the accuracy of CLEAM and validate its improvement over existing fairness measurement framework. We further conduct a series of different ablation studies to validate performance of CLEAM. We remark that CLEAM is not a new fairness metric, but an improved fairness measurement framework that could achieve better accuracy in bias estimation when used with various fairness metrics for generative models.

_As our third contribution_, we apply CLEAM as an accurate framework to reliably measure biases in popular generative models. Our study reveals that SOTA GANs have considerable biases _w.r.t._ several SA. Furthermore, we observe an intriguing property in Stable Diffusion Model: slight differences in semantically similar prompts could result in markedly different biases for SDM. These results prompt careful consideration on the implication of biases in generative models. **Our contributions are:**

* We conduct a study to reveal that even highly-accurate SA classifiers could still incur significant fairness measurement errors when using existing framework.
* To enable evaluation of fairness measurement frameworks, we propose new datasets based on generated samples from StyleGAN, StyleSwin and SDM, with manual labeling _w.r.t._ SA.
* We propose a statistically driven fairness measurement framework, CLEAM, which accounts for the SA classifier inaccuracies to output a more accurate bias estimate.
* Using CLEAM, we reveal considerable biases in several important generative models, prompting careful consideration when applying them to different applications.

## 2 Fairness Measurement Framework

Fig.1(a) illustrates the fairness measurement framework for generative models as in [1; 2; 7; 9; 12]. Assume that with some input _e.g._ noise vector for a GAN or text prompt for SDM, a generative model synthesizes a sample \(\). Generally, as the generator does not label synthesized samples, the ground truth (GT) class probability of these samples _w.r.t._ a SA (denoted by \(^{}\)) is unknown. Thus, an SA classifier \(C_{}\) is utilized to estimate \(^{}\). Specifically, for each sample \(\{\}\), \(C_{}()\) is the argmax classification for the respective SA. In existing works, the expected value of the SA classifier output over a batch of samples, \(}=_{}[C_{}()]\) (or the average of \(}\) over multiple batches of samples), is used as an estimation of \(^{}\). This estimate may then be used in some fairness metric \(f\) to report the fairness value for the generator, _e.g._ fairness discrepancy metric between \(}\) and a uniform distribution \(}\)[1; 20](see Supp A.3 for details on how to calculate \(f\)). Note that _the general assumption behind the existing framework is that with a reasonably accurate SA classifier, \(}\) could be an accurate estimation of \(^{}\)[1; 9]._ In the next section, we will present a deeper analysis on the effects of an inaccurate SA classifier on fairness measurement. Our findings suggest that there could be a large discrepancy between \(}\) and \(^{}\), even for highly accurate SA classifiers, indicative of significant fairness measurement errors in the current measurement framework.

One may argue that conditional GANs (cGANs) [23; 24] may be used to generate samples conditioned on the SA, thereby eliminating the need for an SA classifier. However, cGANs are not considered in previous works due to several limitations. These include the limited availability of large _labeled_ training datasets, the unreliability of sample quality and labels , and the exponentially increasing conditional terms, per SA. Similarly, for SDM, Bianchi _et al._ found that utilizing well-crafted prompts to mitigate biases is ineffective due to the presence of existing biases in its training dataset. Furthermore in Sec. 6, utilizing CLEAM, we will discuss that even subtle prompt changes (while maintaining the semantics) result in drastically different SA biases. See Supp G for further comparison between  and our findings.

## 3 A Closer Look at Fairness Measurement

In this section, we take a closer look at the existing fairness measurement framework. In particular, we examine its performance in estimating \(^{}\) of the samples generated by SOTA GANs and SDM, a task previously unstudied due to the lack of a labeled generated dataset. We do so by designing an experiment to demonstrate these errors while evaluating biases in popular image generators. Following previous works, our main focus is on binary SA which takes values in \(\{0,1\}\). Note that,we assume that the accuracy of the SA classifier \(C_{}\) is known and is characterized by \(=\{_{0},_{1}\}\), where \(_{i}\) is the probability of correctly classifying label \(i\). For example, for Gender attribute, \(_{0}\) and \(_{1}\) are the probability of correctly classifying Female, and Male classes, respectively. In practice, \(C_{}\) is trained on standard training procedures (more details in the Supp F) and \(\) can be measured during the validation stage of \(C_{}\) and be considered a constant when the validation dataset is large enough. Additionally, \(^{}\) can be assumed to be a constant vector, given that the samples generated can be considered to come from an infinite population, as theoretically there is no limit to the number of samples from a generative model like GAN or SDM.

**New dataset by labeling generators output.** The major limitation of evaluating the existing fairness measurement framework is the unavailability of \(^{}\). _To pave the way for an accurate evaluation, we create a new dataset by manually labeling the samples generated by GANs and SDM_. More specifically, we utilize the official publicly released pre-trained StyleGAN2  and StyleSwin  on CelebA-HQ  for sample generation. Then, we randomly sample from these GANs and utilize Amazon Mechanical Turks to hand-label the samples _w.r.t._ Gender and BlackHair, resulting in \(\)9K samples for each GAN; see Supp H for more details and examples. Next, we follow a similar labeling process _w.r.t._ Gender, but with a SDM  pre-trained on LAION-5B. Here, we input prompts using best practices , beginning with a scene description ("A photo with the face of"), followed by four indefinite (gender-neutral) pronouns or nouns  - ("an individual", "a human being", "one person", "a person") to collect \(\)2k high-quality samples. We refer to this new dataset as Generated Dataset (**GenData**), which includes generated images from three models with corresponding SA labels: GenData-StyleGAN2, GenData-StyleSwin, GenData-SDM. We remark that these labeled datasets only provide a strong approximation of \(^{}\) for each generator, however as the datasets are reasonably large, we find this approximation sufficient and simply refer to it as the GT \(^{}\). Then utilizing this GT \(^{}\), we compare it against the estimated baseline (\(}\)). One interesting observation revealed by GenData is that all three generators exhibit a considerable amount of bias (see Tab.1 and 2); more detail in Sec. 6. Note that for a fair generator we have \(p_{0}^{*}=p_{1}^{*}=0.5\), and measuring the \(p_{0}^{*}\) and \(p_{1}^{*}\) is a good proxy for measuring fairness.

**Experimental setup.** Here, we follow Choi _et al._ as the _Baseline_ for measuring fairness. In particular, to calculate each \(}\) value for a generator, a corresponding batch of \(n=400\) samples is randomly drawn from GenData and passed into \(C_{}\) for SA classification. We repeat this for \(s=30\) batches and report the mean results denoted by \(_{}\) and the 95% confidence interval denoted by \(_{}\). For a comprehensive analysis of the GANs, we repeat the experiment using four different SA classifiers: Resnet-18, ResNet-34 , MobileNet2 , and VGG-16 . Then, to evaluate the SDM, we utilize CLIP  to explore the utilization of pre-trained models for zero-shot SA classification; more details on the CLIP SA classifier in Supp. E. As CLIP does not have a validation dataset, to measure \(\) for CLIP, we utilize CelebA-HQ, a dataset with a similar domain to our application. We found this to be a very accurate approximation; see Supp D.7 for validation results. Note that for SDM, a separate \(}\) is measured for each text prompt as SDM's output images are conditioned on the input text prompt. As seen in Tab. 1 and 2, all classifiers demonstrate reasonably high average accuracy \([84\%,98.7\%]\). Note that as we focus on binary SA (_e.g._ Gender:{Male, Female}), both \(^{}\) and \(}\) have two components _i.e._\(^{}=\{p_{0}^{*},p_{1}^{*}\}\), and \(}=\{_{0},_{1}\}\). After computing the \(_{}\) and \(_{}\), we calculate _normalized \(L_{1}\) point error \(e_{}\)_, and _interval max error_\(e_{}\)_w.r.t._ the \(p_{0}^{*}\) (GT) to evaluate the measurement accuracy of the baseline method:

\[e_{_{}}=^{*}}|p_{0}^{*}-_{}| ; e_{_{}}=^{*}}\{|(_{ })-p_{0}^{*}|,|(_{})-p_{0}^{*}|\}\] (1)

**Based on our results in Tab. 1,** for GANs, we observe that despite the use of reasonably accurate SA classifiers, there are significant estimation errors in the existing fairness measurement framework, _i.e._\(e_{_{}}\)\([4.98\%,17.13\%]\). In particular, looking at the SA classifier with the highest average accuracy of \( 97\%\) (ResNet-18 on Gender), we observe significant discrepancies between GT \(p_{0}^{*}\) and \(_{}\), with \(e_{_{}}=4.98\%\). These errors generally worsen as accuracy marginally degrades, _e.g._ MobileNetv2 with accuracy \( 96\%\) results in \(e_{_{}}=5.45\%\). These considerably large errors contradict prior assumptions - that for a reasonably accurate SA classifier, we can assume \(e_{_{}}\) to be fairly negligible. Similarly, our results in Tab. 2 for the SDM, show large \(e_{_{}}\)\([1.49\%,9.14\%]\), even though the classifier is very accurate. We discuss the reason for this in more detail in Sec. 5.1.

_Overall, these results are concerning as they cast doubt on the accuracy of prior reported results._ For example, imp-weighting  which uses the same ResNet-18 source code as our experiment, reports a 2.35% relative improvement in fairness against its baseline _w.r.t._ Gender, which falls within the range of our experiments smallest relative error, \(e_{_{}}\)=4.98%. Similarly, Teo _et al._ and Um _et al._ report a relative improvement in fairness of 0.32% and 0.75%, compared to imp-weighting . These findings suggest that some prior results may be affected due to oversight of SA classifier's inaccuracies; see Supp. A.4 for more details on how to calculate these measurements.

**Remark:** In this section, we provide the keystone for the evaluation of measurement accuracy in the current framework by introducing a labeled dataset based on generated samples. These evaluation results raise concerns about the accuracy of existing framework as considerable error rates were observed even when using accurate SA classifiers, an issue previously seen to be negligible.

## 4 Mitigating Error in Fairness Measurements

The previous section exposes the inaccuracies in the existing fairness measurement framework. Following that, in this section, we first develop a statistical model for the erroneous output of the SA classifier, \(}\), to help draw a more systematic relationship between the inaccuracy of the SA classifier and error in fairness estimation. Then, with this statistical model, we propose CLEAM - a new measurement framework that reduces error in the measured \(}\) by accounting for the SA classifier inaccuracies to output a more accurate statistical approximation of \(^{*}\).

### Proposed Statistical Model for Fairness Measurements

As shown in Fig.1(a), to measure the fairness of the generator, we feed \(n\) generated samples to the SA classifier \(C_{}\). The output of the SA classifier (\(}\)) is in fact a random variable that aims to approximate the \(^{*}\). Here, we propose a statistical model to derive the distribution of \(}\).

As Fig.1(b) demonstrates in our running example of a binary SA, each generated sample is from _class 0_ with probability \(p_{0}^{*}\), or from _class 1_ with probability \(p_{1}^{*}\). Then, generated sample from _class \(i\)_ where \(i\{0,1\}\), will be classified correctly with the probability of \(_{i}\), and wrongly with the probability of \(_{i}^{}=1-_{i}\). Thus, for each sample, there are four mutually exclusive possible events denoted by \(\), with the corresponding probability vector \(\):

\[^{T}=c_{0|0}&c_{1|0}&c_{1|1}&c_{0|1},^{T}=p_{0}^{*}_{0}&p_{0}^{*}_{0}^{ }&p_{1}^{*}_{1}&p_{1}^{*}_{1}^{}\] (2)

where \(c_{i|j}\) denotes the event of assigning label \(i\) to a sample with GT label \(j\). Given that this process is performed independently for each of the \(n\) generated images, the probability of the counts for each output \(^{T}\) in Eqn. 2 (denoted by \(}\)) can be modeled by a multinomial distribution, _i.e._\(} Multi(n,)\)[37; 38; 39]. Note that \(}\) models the _joint probability distribution_ of these outputs, _i.e._\(}(N_{c_{0|0}},N_{c_{1|0}},N_{c_{1|1}},N_{c_{0|1}})\) where, \(N_{c_{i|j}}\) is the random variable of the count for event \(c_{i|j}\) after classifying \(n\) generated images. Since \(\) is not near the boundary of the parameter space, and as we utilize a large \(n\), based on the central limit theorem, \(Multi(n,)\) can be approximated by a multivariate Gaussian distribution, \(}}(,)\), with \(=n\) and \(=n\)[40; 39], where \(\) is defined as:

\[=diag()-^{T}\] (3)

\(diag()\) denotes a square diagonal matrix corresponding to vector \(\) (see Supp A.1 for expanded form). The _marginal distribution_ of this multivariate Gaussian distribution gives us a univariate (one-dimensional) Gaussian distribution for the count of each output \(^{T}\) in Eqn. 2. For example, the distribution of the count for event \(c_{0|0}\), denoted by \(N_{c_{0|0}}\), can be modeled as \(N_{c_{0|0}}(_{1},_{11})\).

Lastly, we find the total percentage of data points labeled as class \(i\) when labeling \(n\) generated images using the normalized sum of the related random variables, _i.e._\(_{i}=_{j}N_{c_{i|j}}\). For our binary example, \(_{i}\) can be calculated by summing random variables with Gaussian distribution, which results in another Gaussian distribution , _i.e._, \(_{0}(_{_{0}},_{_ {0}}^{2})\), where:

\[_{_{0}}= (_{1}+_{4})=p_{0}^{*}_{0}+p_{1} ^{*}_{1}^{}\] (4) \[_{_{0}}^{2}= }(_{11}+_{44}+2_{14 })=[(p_{0}^{*}_{0}-(p_{0}^{*}_{0})^{2})+(p_{1}^{*} _{1}^{}-(p_{1}^{*}_{1}^{})^{2})]+p_{0}^{*} p_{1}^{*}_{0}_{1}^{}\] (5)

Similarly \(_{1}(_{_{1}},_{_ {1}}^{2})\) with \(_{_{1}}=(_{2}+_{3})/n\), and \(_{_{1}}^{2}=(_{22}+_{33}+2_{23})/n^{2}\) which is aligned with the fact that \(_{1}=1-_{0}\).

**Remark:** In this section, considering the probability tree diagram in Fig.1(b), we propose a joint distribution for the possible events of classification (\(N_{c_{|ij}}\)), and use it to compute the marginal distribution of each event, and finally the distribution of the SA classifier outputs (\(_{0}\), and \(_{1}\)). Note that considering Eqn. 4, 5, only with a perfect classifier (\(_{i}=1\), _i.e._ acc\(=100\%\)) the \(_{_{0}}\) converges to \(p_{0}^{*}\). However, training a perfect SA classifier is not practical _e.g._ due to the lack of an appropriate dataset and task hardness [42; 43]. As a result, in the following, we will propose CLEAM which instead utilizes this statistical model to mitigate the error of the SA classifier.

### CLEAM for Accurate Fairness Measurement

In this section, we propose a new estimation method in fairness measurement that considers the inaccuracy of the SA classifier. For this, we use the statistical model, introduced in Sec 4.1, to compute a more accurate estimation of \(^{*}\). Specifically, we first propose a Point Estimate (PE) by approximating the _maximum likelihood value_ of \(^{*}\). Then, we use the _confidence interval_ for the observed data (\(}\)) to propose an Interval Estimate (IE) for \(^{*}\).

**Point Estimate (PE) for \(^{*}\)**. Suppose that we have access to \(s\) samples of \(}\) denoted by \(\{}^{1},,}^{s}\}\), _i.e._ SA classification results on \(s\) batches of generated data. We can then use the proposed statistical model to approximate the \(^{*}\). In the previous section, we demonstrate that we can model \(_{j}^{*}\) using a Gaussian distribution. Considering this, first, we use the available samples to calculate sample-based statistics including the mean and variance of the \(_{j}\) samples:

\[_{_{j}} =_{i=1}^{s}_{j}^{i}\] (6) \[_{_{j}}^{2} =_{i=1}^{s}(_{j}^{i}-_{_{j}})^{2}\] (7)

For a Gaussian distribution, the Maximum Likelihood Estimate (MLE) of the population mean is its sample mean \(_{_{j}}\). Given that \(s\) is large enough (_e.g._\(s>30\)), we can assume that \(_{_{j}}\) is a good approximation of the population mean , and equate it to the statistical population mean \(_{_{j}}\) in Eqn. 4 (see Supp A.2 for derivation). With that, we get the _maximum likelihood approximation of \(^{*}\)_, _which we call the CLEAM's point estimate, \(_{}\)_:

\[_{}(p_{0}^{*})=(_{_{0}}-_{1}^{ })/(_{0}-_{1}^{}),_{}(p_{1 }^{*})=1-_{}(p_{0}^{*})\] (8)

Notice that \(_{}\) accounts for the inaccuracy of the SA classifier.

**Interval Estimate (IE) for \(^{*}\)**. In the previous part, we propose a PE for \(^{*}\) using the statistical model, and sample-based mean \(_{_{0}}\). However, as we use only \(s\) samples of \(}\), \(_{_{0}}\) may not capture the exact value of the population mean. This adds some degree of inaccuracy into \(_{}\). In fact, in our framework, \(_{_{0}}\) equals \(_{_{0}}\) when \(s\). However, increasing each unit of \(s\) significantly increases the computational complexity, as each \(}\) requires \(n\) generated samples. To address this, we recall that \(_{0}\) follows a Gaussian distribution and instead utilize frequentist statistics  to propose a 95% confidence interval (CI) for \(^{*}\). To do this, first we derive the CI for \(_{_{0}}\):

\[_{_{0}}-1.96_{_{0}}}{} _{_{0}}_{_{0}}+1.96 _{_{0}}}{}\] (9)

Then, applying Eqn.4 to Eqn.9 gives the lower and upper bounds of the approximated 95% CI for \(p_{0}^{*}\):

\[(p_{0}^{*}),(p_{0}^{*})=(_{_{0}} 1.96(_{_{0}}/)-_{1}^{})/(_{0}- _{1}^{})\] (10)

This gives us the interval estimate of CLEAM, \(_{}=[(p_{0}^{*}),(p_{0}^{*})]\), a range of values that we can be approximately 95% confident to contain \(p_{0}^{*}\). The range of possible values for \(p_{1}^{*}\) can be simply derived considering \(p_{1}^{*}=1-p_{0}^{*}\). The overall procedure of CLEAM is summarized in Alg. 1. Now, with the IE, we can provide statistical significance to the reported fairness improvements.

[MISSING_PAGE_EMPTY:7]

Experiments

In this section, we first evaluate fairness measurement accuracy of CLEAM on both GANs and SDM (Sec.5.1) with our proposed GenData dataset. Then we evaluate CLEAM's robustness through some ablation studies (Sec. 5.2). To the best of our knowledge, there is no similar literature for improving fairness measurements in generative models. Therefore, we compare **CLEAM** with the two most related works: a) the **Baseline** used in previous works [1; 2; 7; 9; 12] b) **Diversity** which computes disparity within a dataset via an intra-dataset pairwise similarity algorithm. We remark that, as discussed by Keswani _et al._ Diversity is model-specific using VGG-16 ; see Supp. D.2 for more details. Finally, unless specified, we repeat the experiments with \(s=30\) batches of images from the generators with batch size \(n=400\). For a fair comparison, all three algorithms use the exact same inputs. However, while Baseline and Diversity ignore the SA classifier inaccuracies, CLEAM makes good use of it to rectify the measurement error. As mentioned in Sec. 4.2, for CLEAM, we utilize \(\) measured on real samples, which we found to be a good approximation of the \(\) measured on generated samples (see Supp. D.7 for results). We repeat each experiment 5 times and report the mean value for each test point for both PE and IE. See Supp D.1 for the standard deviation.

### Evaluating CLEAM's Performance

**CLEAM for fairness measurement of SOTA GANs - StyleGAN2 and StyleSwin.** For a fair comparison, we first compute \(s\) samples of \(}\), one for each batch of \(n\) images. For Baseline, we use the mean \(}\) value as the PE (denoted by \(_{}\)), and the \(95\%\) confidence interval as IE (\(_{}\)). With the same \(s\) samples of \(}\), we apply Alg. 1 to obtain \(_{}\) and \(_{}\). For Diversity, following the original source code , a controlled dataset with fair representation is randomly selected from a held-out dataset of CelebA-HQ . Then, we use a VGG-16  feature extractor and compute Diversity, \(\). With \(\) we find \(_{0}=(+1)/2\) and subsequently \(_{}\) and \(_{}\) from the mean and \(95\%\) CI (see Supp D.2 for more details on diversity). We then compute \(e_{_{}}\), \(e_{_{}}\), \(e_{_{}}\) and \(e_{_{}}\) with Eqn 1, by replacing the Baseline estimates with CLEAM and Diversity.

As discussed, our results in Tab.1 show that the baseline experiences significantly large errors of \(4.98\% e_{_{}} 17.13\%\), due to a lack of consideration for the inaccuracies of the SA classifier. We note that this problem is prevalent throughout the different SA classifier architectures, even with higher capacity classifiers _e.g._ ResNet-34. Diversity, a method similarly unaware of the inaccuracies of the SA classifier, presents a similar issue with \(8.98\% e_{_{}} 14.33\%\) In contrast, CLEAM dramatically reduces the error for all classifier architectures. Specifically, CLEAM reduces the average point estimate error from \(e_{_{}} 8.23\%\) to \(e_{_{}} 1.24\%\), in both StyleGAN2 and StyleSwin. The IE presents similar results, where in most cases \(_{}\) bounds the GT value of \(^{}\).

**CLEAM for fairness measurement of SDM.** We evaluate CLEAM in estimating the bias of the SDM _w.r.t._**Gender**, based on the synonymous (gender-neutral) prompts introduced in Sec. 3. Recall that here we utilize CLIP as the zero-shot SA classifier. Our results in Tab 2, as discussed, show that utilizing the baseline results in considerable error (\(1.49\% e_{_{}} 9.14\%\)) for all prompts, even though the SA classifier's average accuracy was high, \( 98.7\%\) (visual results in Fig.2). A closer look at the theoretical model's Eqn. 4 reveals that this is due to the larger inaccuracies observed in the biased class (\(_{1}^{}\)) coupled with the large bias seen in \(p_{1}^{}\), which results in \(_{}\) deviating from \(p_{0}^{}\). In contrast, CLEAM accounts for these inaccuracies and significantly minimizes the error to \(e_{_{}} 1.77\%\). Moreover, CLEAM's IE is able to consistently bound the GT value of \(p_{0}^{}\).

### Ablation Studies and Analysis

Here, we perform the ablation studies and compare CLEAM with classifier correction methods. _We remark that detailed results of these experiments are provided in the Supp due to space limitations._

**CLEAM for measuring varying degrees of bias.** As we cannot control the bias in trained generative models, to simulate different degrees of bias, we evaluate CLEAM with a _pseudo-generator_. Our results show that CLEAM is effective at different biases (\(p_{0}^{}\) [0.5,0.9]) reducing the average error from \(2.80\% e_{_{}} 6.93\%\) to \(e_{_{}} 0.75\%\) on CelebA _w.r.t._ {Gender,BlackHair}, and AFHQ _w.r.t._Cat/Dog. See Supp D.3 and D.4 for full experimental results.

**CLEAM vs Classifier Correction Methods **. CLEAM generally accounts for the classifier's inaccuracies, without targeting any particular cause of inaccuracies, for the purpose of rectifying the fairness measurements. This objective is unlike traditional classifier correction methods as it does not aim to improve the actual classifier's accuracy. However, considering that classifier correction methods may improve the fairness measurements by directly rectifying the classifier inaccuracies, we compare its performance against CLEAM. As an example, we utilize the Black Box Shift Estimation / Correction (BBSE / BBSC)  which considers the label shift problem and aims to correct the classifier output by detecting the distribution shift. Our results, based on Sec. 5.1 setup, show that while BBSE does improve on the fairness measurements of the baseline _i.e._ 4.20% \( e_{_{}}\) 3.38%, these results are far inferior to CLEAM's results seen in Tab. 1. In contrast, BBSC demonstrates no improvements in fairness measurements. See Supp D.8 for full experimental results. We postulate that this is likely due to the strong assumption of label shift made by both methods.

**Effect of batch-size.** Utilizing experimental setup in Sec. 5.1 for batch size \(n\), our results in Fig. 3 show that \(n\)=400 is an ideal batch size, balancing computational cost and measurement accuracy. See Supp F for full experimental details and results.

## 6 Applying CLEAM: Bias in Current SOTA Generative Models

In this section, we leverage the improved reliability of CLEAM to study biases in the popular generative models. Firstly, with the rise in popularity of text-to-image generators [50; 51; 52; 5], we revisit our results when passing different prompts, with synonymous neutral meanings to an SDM, and take a closer look at how subtle prompt changes can impact bias _w.r.t._\(\). Furthermore, we further investigate if similar results would occur in other SA, Smiling. Secondly, with the shift in popularity from convolution to transformer-based architectures [53; 54; 55], due to its better sample quality, we determine whether the learned bias would also change. For this, we compare StylesSwin (transformer) and StyleGAN2 (convolution), which are both based on the same architecture backbone.

Our results, on SDM, demonstrate that the use of different synonymous neutral prompts [32; 33] results in different degrees of bias _w.r.t._ both \(\) and \(\) attributes. For example in Fig. 2, a semantically insignificant prompt change from "one person" to "a person" results in a significant shift in \(\) bias. Then, in Fig. 4a, we observe that while the SDM _w.r.t._ our prompts appear to be heavily biased to not-\(\), having "person" in the prompt appears to significantly reduce this bias. This suggests that for SDM, even semantically similar neutral prompts [32; 33] could result in different degrees of bias, thereby demonstrating certain instability in SDM. Next, our results in Fig. 4b compare the bias in StyleGAN2, StylesSwin, and the training CelebA-HQ dataset over an extended number of SAs. Overall, we found that while StyleSwin produces better quality samples , the same biases still remain statistically unchanged between the two architectures _i.e._ their IE overlap. Interestingly, our results also found that both the GANs were less biased than the training dataset itself.

## 7 Discussion

**Conclusion.** In this work, we address the limitations of the existing fairness measurement framework. Since generated samples are typically unlabeled, we first introduce a new labeled dataset based on three state-of-the-art generative models for our studies. Our findings suggest that the existing framework, which ignores classification inaccuracies, suffers from significant measurement errors, even when the SA classifier is very accurate. To rectify this, we propose CLEAM, which considers these inaccuracies in its statistical model and outputs a more accurate fairness measurement. Overall, CLEAM demonstrates improved accuracy over extensive experimentation, including both real

Figure 3: Comparing the point error \(e_{}\) for Baseline and CLEAM when evaluating the bias of GenData-CelebA with ResNet-18, while varying sample size, \(n\).

generators and controlled setups. Moreover, by applying CLEAM to popular generative models, we uncover significant biases that raise efficacy concerns about these models' real-world application.

**Broader Impact.** Given that generative models are becoming more widely integrated into our everyday society text-to-image generation, it is important that we have reliable means to measure fairness in generative models, thereby allowing us to prevent these biases from proliferating into new technologies. CLEAM provides a step in this direction by allowing for more accurate evaluation. We remark that our work _does not introduce any social harm_ but instead improves on the already existing measurement framework.

**Limitations.** Despite the effectiveness of the proposed method along various generative models, our work addresses only one facet of the problems in the existing fairness measurement and there is still room for further improvement. For instance, it may be beneficial to consider SA to be non-binary when hair color is not necessary fully black (grey). Additionally, existing datasets used to train classifiers are commonly human-annotated, which may itself contain certain notions of bias. See Supp. I for further discussion.