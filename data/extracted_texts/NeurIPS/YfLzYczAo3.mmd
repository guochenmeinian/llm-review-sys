# CRONOS: Enhancing Deep Learning with Scalable GPU Accelerated Convex Neural Networks

Miria Feng

Electrical Engineering

Stanford University

mirao@stanford.edu

&Zachary Frangella

Management Science & Engineering

Stanford University

zfran@stanford.edu

&Mert Pilanci

Electrical Engineering

Stanford University

pilanci@stanford.edu

###### Abstract

We introduce the _CRONOS_ algorithm for convex optimization of two-layer neural networks. CRONOS is the first algorithm capable of scaling to high-dimensional datasets such as ImageNet, which are ubiquitous in modern deep learning. This significantly improves upon prior work, which has been restricted to downsampled versions of MNIST and CIFAR-10. Taking CRONOS as a primitive, we then develop a new algorithm called CRONOS-AM, which combines CRONOS with alternating minimization, to obtain an algorithm capable of training multi-layer networks with arbitrary architectures. Our theoretical analysis proves that CRONOS converges to the global minimum of the convex reformulation under mild assumptions. In addition, we validate the efficacy of CRONOS and CRONOS-AM through extensive large-scale numerical experiments with GPU acceleration in JAX. Our results show that CRONOS-AM can obtain comparable or better validation accuracy than predominant _tuned_ deep learning optimizers on vision and language tasks with benchmark datasets such as ImageNet and IMDb. To the best of our knowledge, CRONOS is the first algorithm which utilizes the convex reformulation to enhance performance on large-scale learning tasks.

## 1 Introduction

The non-convex landscape of deep neural networks (DNN) poses significant challenges for modern deep learning, especially since non-convexity implies it is NP-hard to train a neural network to optimality Blum and Rivest (1988). Common stochastic first-order optimizers, such as stochastic gradient descent (SGD), offer no guarantees of producing more than an approximate stationary point Arjevani et al. (2023), which may be considerably suboptimal Ge et al. (2015). The effectiveness of methods such as SGD and Adam are therefore reliant on heavily conditioned training environments. As a result, most models require extensive hyperparameter tuning of the optimizer to train successfully. This leads to expensive iterations in high compute settings with variable performance depending on optimizer selection and problem domain Yao et al. (2021). Scaling laws Rosenfeld (2021) also indicate this regime will yield an increasingly larger number of hyperparameters, with a disproportionate dependence on computational resources and data cost.

Our objective is to achieve more efficient targeted optimization of deep learning tasks by leveraging the connection between neural networks and convex optimization. This approach provides clearer insight to the underlying optimization problem deep learning is trying to solve, despite the complexity of its non-convex landscape. The authors of Pilanci and Ergen (2020) have recently proven that the training of shallow neural networks can be equivalently formulated as a convex optimization program. The strategy leverages semi-infinite duality theory to develop algorithms which converge to the global minimum in polynomial time. However, the resulting convex program is a constrained high-dimensional linear model that is intensely difficult to solve at scale. Other more recent works of Mishkin et al. (2022) and Bai et al. (2023) have made progress in solving this problem withsmall downsampled versions of MNIST Noever and Noever (2021) and CIFAR-10 Krizhevsky and Hinton (2010), yet were unable to scale to realistic high-dimensional datasets that are ubiquitous in modern deep learning. This inability to handle large real-world data significantly limits the practical deployment of convex neural networks, despite their strong theoretical guarantees.

In this work, we propose CRONOS: the **Co**nvex **R**e**f**ormulated **Ne**ural **N**e**r**N **e**r**N **e**r**N **O**perator **S**plitting algorithm. CRONOS is a fast, efficient and nearly hyperparameter-free method for training two-layer convex neural networks. We then augment our algorithm in CRONOS-AM: Cronos with Alternating Minimization, which extends applicability to neural networks of arbitrary architectures beyond just two-layer networks. We implement all experiments and algorithms in JAX Roth et al. (2024) with the RTX-4090 GPU. This allows CRONOS to fully leverage GPU acceleration and eliminate memory bottlenecks, thus successfully tackling problems with large data. In order to sustain strong convergence guarantees, parallelization, and robustness to hyperparameter tuning we utilize ADMM Boyd et al. (2011) as a core solver in our method. This enables exciting new time-efficient strategies for CRONOS to handle scalability in real world problems. We evaluate the performance of our algorithms in binary and multi-class classification tasks across a wide domain of large scale datasets (both image and language), on three model architectures (MLP, CNN, GPT-2). Additionally, our theoretical analysis proves the convergence of CRONOS to the global minimum of the convex reformulation under mild assumptions.

To the best of our knowledge, this is the first time convex reformulated neural networks have been successfully applied to large data such as ImageNet Recht et al. (2019) and large language modeling tasks with GPT-2 Budzianowski and Vulic (2019) architecture. Our main contributions can be summarized as follows 1:

* We develop a practical algorithm via convex optimization and the Alternating Directions Method of Multipliers (ADMM) Boyd et al. (2011) to train two-layer ReLU neural networks with global convergence guarantees.
* Using this as a primitive, we extend our algorithm to effectively train multi-layer networks of arbitrary architecture by combining CRONOS with alternating minimization.
* We demonstrate the efficient practical applications of our method on real-world large scale image and language datasets, including ImageNet and IMDb.
* a functional programming paradigm for significant GPU acceleration. Results demonstrate performance speedups and successfully overcome the problem of memory bottlenecks to tackle large data.
* Our theoretical analysis proves the convergence guarantees to global minimum under mild assumptions of our proposed algorithms.

## 2 Related Work

CRONOS builds upon key ideas from previous literature on convex neural networks, ADMM, randomized numerical linear algebra and synergy with JAX implementation. Please see Appendix A for detailed discussions of related work according to each of these three areas of interest.

## 3 Background

This section introduces the convex reformulation of two-layer neural networks. We formalize this in two steps: **3.1)** foundational definition of convex ReLU neural networks and **3.2)** its equivalence as a linearly constrained Generalized Linear Model (GLM) with group lasso regularization.

### Convex ReLU Neural Networks

Given a dataset \(X^{n d}\), a two-layer ReLU multilayer perceptron (ReLU-MLP) with weights \(W^{(1)}^{m d},w^{(2)}^{m}\) outputs the prediction:

\[f_{W^{(1)},w^{(2)}}(X)=_{j=1}^{m}(XW_{1j})_{+}w_{2j}.\] (1)Here \((x)_{+}=\{x,0\}\) denotes the ReLU activation function.

Given targets \(y^{n}\), the network in (1) is typically trained by minimizing the following non-convex loss function:

\[_{W_{1},w_{2}}(f_{W_{1},w_{2}}(X),y)+_{j=1 }^{m}||W_{1j}||_{2}^{2}+(w_{2j})^{2},\] (2)

where \(:^{n}\) is the loss function, and \( 0\) is the regularization strength. It is typically challenging to solve (2), since the optimizer often needs meticulous tuning of hyperparameters to ensure successful training. Such tuning is expensive, since it requires many iterations of running the optimizer across multiple hyperparameter configurations in a grid search to obtain good performance. This dramatically contrasts with the convex optimization framework, where algorithms come with strong convergence guarantees and involve minimal hyperparameters. Fortunately, it is possible to maintain the expressive capabilities of ReLU neural networks while still enjoying the computational advantages of convex optimization.

Pilanci and Ergen (2020) have shown (2) admits a convex reformulation, which elides the difficulties inherent in solving the deep learning non-convex landscape. This reformulation has the same optimal value as the original non-convex problem, provided \(m m^{*}\), for some \(m n+1\). Therefore, reformulating (2) as a convex program does not result in loss of information or generality.

Pilanci and Ergen (2020)'s convex reformulation of (2) also enumerates the actions of all possible ReLU activation patterns on the data matrix \(X\). These activation patterns act as separating hyperplanes, which essentially multiply the rows of \(X\) by 0 or 1, and can be represented by diagonal matrices. For fixed \(X\), the set of all possible ReLU activation patterns may be expressed as

\[_{X}=\{D=(1(Xv 0)):v^{d} \}.\]

The cardinality of \(_{X}\) grows as \(|_{X}|=(r(n/r)^{r})\), where \(r(X)\)Pilanci and Ergen (2020). Given \(D_{i}_{X}\), the set of vectors \(v\) for which \((Xv)_{+}=D_{i}Xv\), is given by the following convex cone:

\[_{i}=\{v^{d}:(2D_{i}-I)Xv 0\}.\]

Learning directly based on the enumeration of \(_{X}\) is impractical due to the exponential size of \(_{X}\)(Mishkin et al., 2022). Instead, we are motivated to work with the following convex program, based on sampling \(P\) activation patterns from \(_{X}\):

\[_{(v_{i},w_{i})_{i=1}^{P}} (_{i=1}^{P}D_{i}X(v_{i}-w_{i}),y)+_{ i=1}^{P}||v_{i}||_{2}+||w_{i}||_{2}\] (3) s.t. \[v_{i},\;w_{i}_{i} i[P].\]

Although (3) only works with a subsampled version of the convex reformulation of Pilanci and Ergen (2020), it can be shown under reasonable conditions that (3) still has the same optimal solution as (2) Mishkin et al. (2022). Therefore we can confidently work with the tractable convex program in (3).

### Convex ReLU networks as linearly constrained GLMs with group lasso regularization

Prior derivations Bai et al. (2023) have shown that (3) may be reformulated as a linearly constrained composite convex program:

**Proposition 3.1**.: _Define the matrices \(F_{i}=D_{i}X\) and \(G_{i}=(2D_{i}-I)X\), where \(i[P]\). Then by introducing the constraints \(u_{i}=v_{i}\), \(z_{i}=w_{i}\), where \(i[P]\), and appropriate slack variables \(s_{1},,s_{P},t_{1},,t_{P}\), (3) can be reformulated as:_

\[_{(,,)}(F,y)+||||_{2,1 }+( 0)\] (4) \[I_{2dP}\\ G-\\ =0.\]

_where_

\[,,^{2dP},F^{n 2dP},G ^{2nP 2dP}\]

The reformulation in (4) is essentially a very large _constrained_ generalized linear model (GLM) with a group lasso penalty Yuan and Lin (2006). The data matrix \(F\) associated with (4) has dimensions\(n 2dP\), and the constraint matrix has dimensions \(2(n+d)P 2dP\). Although the sizes of \(F\) and the constraint matrix seem intractable, they are highly structured. Each \(F_{i}\) and \(G_{i}\) consists of the data matrix \(X\) multiplied by a diagonal matrix of \(0\)'s and \(1\)'s. Therefore, \(F\) and \(G\) do not need to be instantiated, and we can apply matrix-vector products efficiently by exploiting this structure on GPU-accelerated frameworks. Additionally, if \(X\) is approximately low-rank (a common phenomenon in machine learning), its singular values must decay fast. \(F_{i}\) and \(G_{i}\) then also inherit this approximate low-rank structure in (4), which CRONOS exploits and solves via fast matrix-vector products on GPU acceleration.

## 4 CRONOS: Convex Neural Networks via Operator Splitting

This section introduces the CRONOS algorithm for solving (4). CRONOS is a scalable, convex optimization-based learning method capable of handling large data and utilizes GPU acceleration for enhanced performance.

### ADMM for robustness and decomposability

```
0: penalty parameter \(\) repeat \(^{k+1}=_{}\{\|F-y\|^{2}+\|-^{k}+^{k}\|_{2}^{2}+\|G-^{k}+^{k}\|^{2}\}\) \(^{k+1}\\ ^{k+1}=_{,}\|\|_{2,1}+ ( 0)+\|^{k+1}-+^{k}\|^{2}\) \(\) Primal update \(^{k+1}^{k}+}{}(^{k+1}- ^{k+1})\)\(\) Dual \(\) update \(^{k+1}^{k}+}{}(G^{k+1}-^ {k+1})\)\(\) Dual \(\) update until convergence ```

**Algorithm 1** ADMM for Convex ReLU Networks

Equation (4) is in a natural form to apply the Alternating Directions Method of Multipliers (ADMM), the seminal first-order optimization algorithm for solving convex optimization problems Boyd et al. (2011). ADMM is an ideal choice due to its strong convergence guarantees, robustness to hyperparameter variations, and ability to effectively leverage modern computing architectures through parallelism. However, directly applying ADMM to (4) requires solving the \(\)-subproblem at each iteration:

\[^{k+1}=_{}\{\|F-y\|^{2}+ {2}\|-^{k}+^{k}\|_{2}^{2}+\|G-^{k }+^{k}\|^{2}\}.\] (5)

This solution then requires solving the following linear system:

\[&(H+I)^{k+1}=b^{k},\\ & H=F^{T}F+G^{T}G,\\ & b^{k}=F^{T}y+^{k}-^{k}+G^{T}( ^{k}-^{k}).\] (6)

Since \(F\) is a \(n 2dP\) matrix and \(G\) is a \(2nP 2dP\) matrix, the cost of solving (6) via a direct method is \((nd^{2}P^{4}+d^{3}P^{3})\). This is prohibitively expensive since \(n\) and \(dP\) are typically large. A natural method is to solve (6) inexactly via the Conjugate Gradient (CG) algorithm, which only requires matrix-vector products (matvecs) with \(F,F^{T},G\) and \(G^{T}\). Under these conditions ADMM will still converge, since the sequences of subproblem errors are summable Eckstein and Bertsekas (1992). However the number of iterations required by CG to achieve \(\)-accuracy is \(((H+I))\), where \((H+I)\) is the condition number of \(H+I\). This results in slow convergence for machine learning problems, since \(F\) and \(G\) are closely related to the data matrix \(X\), which is approximately low-rank Udell and Townsend (2019). Given that the linear system in (6) must be solved at each iteration, CG's slow convergence renders it infeasible for real-world application.

### Nystrom preconditioning for fast convergence

``` penalty parameter \(\), forcing sequence \(\{^{k}\}_{k=1}^{}\), rank parameter \(r\) \([U,]=(F^{T}F+G^{T}G,r)\)\(\) Compute using Algorithm 3 repeat  Use Nystrom PCG (Algorithm 4) to find \(^{k+1}\) that solves (5) within tolerance \(^{k}\) \(^{k+1}_{}\|.\|_{2}(^{k+1}+^{k})\)\(\) Primal \(\) update \(^{k+1}(Gu^{k+1}+)_{+}\)\(\) Slack \(\) update \(^{k+1}^{k}+}{}(^{k+1}- ^{k+1})\)\(\) Dual \(\) update \(^{k+1}^{k}+}{}(Gu^{k+1}-^{k+1})\)\(\) Dual \(\) update until convergence ```

**Algorithm 2** CRONOS

We exploit the approximate low-rank structure in our problem matrices to speed up convergence by applying the NysADMM algorithm from Zhao et al. (2022), which is an ADMM-based method targeted at solving large machine learning tasks. Notably, the subproblem for \(^{k+1}\) and \(^{k+1}\) has a closed-form solution that may be computed in \(((n+d)P)\) time. NysADMM employs the Nystrom Preconditioned Conjugate Gradient (NysPCG) from Frangella et al. (2023) to solve (6). NysPCG is a linear system solver specializing in solving linear systems with large, approximately low-rank matrices.

NysPCG first constructs a low-rank preconditioner \(P\) for the matrix \(H+I\) in (6) from a _randomized Nystrom approximation_ of \(H\). When \(P\) is constructed with the appropriate rank, it can be shown that Nystrom PCG solves the linear system in (6) to \(\)-accuracy within \(((1/))\) iterations (Proposition 6.3). The dependence on the condition number is therefore eliminated, and (5) can be solved quickly at each iteration. Details of the NysPCG algorithm and construction of the preconditioner are presented in Appendix B.1.

We refer to our algorithm in solving (4) as _CRONOS_ (**C**onvex **R**eLU **O**ptimized **N**eural networks via **O**perator **S**plitting). The CRONOS algorithm is presented in Algorithm 2.

### Scale and speed with JAX and Just-In-Time compilation

Scaling convex neural networks to realistic high-dimensional data is critical for machine learning problems. Therefore we implement our methods in JAX Bradbury et al. (2018), a high-performance numerical computing library designed to accelerate machine learning research. This framework provides an efficient way to perform array operations, automatic differentiation, and optimization of numerical processes. Leveraging Just-In-Time (JIT) compilation capabilities through XLA (Accelerated Linear Algebra) allow us to execute optimized machine code with extremely accelerated and scalable performance on GPU. Currently, the importance of GPUs in deep learning cannot be overstated. Therefore we note that any practically competitive algorithms need to fully utilize parallel processing capabilities. The combination JAX and JIT compilation effectively enables us to scale to high-dimensional datasets such as Food (\(267 267 3\)), ImageNet (\(512 512 3\)), and the IMDb language dataset. Our experiments are summarized in Section 7.

## 5 CRONOS-AM: Deep Networks via Alternating Minimization

In this section, we introduce the CRONO-AM algorithm for training a neural network with arbitrarily many layers. Consider training an \(L\)-layer neural network with ReLU activations and the least-squares loss. Training the network involves solving:

\[}\|(;X)-y\|^{2}+ _{i=1}^{L-2}\|W_{i}\|_{F}^{2}+(\|W_{L- 1}\|_{F}^{2}+\|w_{L}\|^{2}),\]

where \(=((W_{1}),(W_{2}),,w_{L})^{p}\) is the vector of the network weights, with \(W_{i}\) denoting the weights for the \(i\)th layer, while \(X\) is the data matrix, and \(y\) are the labels. We can rewritethis objective as:

\[,W_{U-1},w_{L}}{}\|(_ {1:L-2}(_{1:L-2},X)W_{L-1}^{T})_{+}w_{L}-y\|^{2}+ {2}_{i=1}^{L-2}\|W_{i}\|_{F}^{2}+(\|W_{L-1}\|_{F}^{2}+ \|w_{L}\|^{2}),\]

where \(_{1:L-2}=((W_{1}),(W_{2}),(W_{L-2}))\), \(_{1:L-1}\) consists of the first \(L-2\) layers. Let us write \((_{1:L-2})=_{1}(_{1:L-2},X)\), which may be viewed as a transformed data matrix. We shall sometimes write \(X\) for brevity. Hence, we can write the output of the network as:

\[(w;X)=((_{1:L-2})W_{L-1}^{T})_{+}w_{L}.\]

Thus, the training problem may be rewritten as:

\[,w_{L-1},w_{L}}{} \|((w_{1:L-2})W_{L-1}^{T})_{+}w_{L}-y \|^{2}+_{i=1}^{L-2}\|W_{i}\|_{F}^{2}+(\|w_{L-1}\|^{2}+\|w_{L}\|^{2}).\]

When \(_{1:L-2}\) is fixed, the preceding problem can be viewed as training a two-layer ReLU neural network with transformed data matrix \(\). Motivated by this, we replace the part of the optimization involving the last two layers with the convex reformulation to obtain:

\[,,),\ w_{1:L-1}}{} \|F((w_{1:L-1}))-y\|^{2}+_{i=1 }^{L-2}\|W_{i}\|_{F}^{2}+||||_{2,1}+( 0).\] (7) s.t. \[I_{2dP}\\ G-\\ =0.\]

Equation (7) decouples the convex weights from the non-convex weights. This puts the objective into a natural form to apply alternating minimization, where we alternate between minimizing with respect to \(w_{1:L-2}\) and \((,,)\). To handle the convex minimization, we can apply CRONOS. For the non-convex portion, we propose utilizing DAdapted-Adam (Defazio and Mishchenko, 2023), as it does not require setting the learning rate. we call this algorithm CRONOS-AM (CRONOS-Alternating Minimization). Pseudocode for CRONOS-AM is given in Algorithm 5.

## 6 Theoretical Analysis of CRONOS

In this section, we establish the rate of convergence for CRONOS in solving (4), and provide an overall computational complexity estimate. Additionally, we show that the linear system for the \(^{k}\)-update may be solved by PCG at a rate independent of the condition number.

### \(F_{i}^{}s\) and \(G_{i}^{}s\) inherit approximate low-rank structure of \(X\)

We begin by showing the spectrum of \(X^{T}X\) upper bounds the spectrum of \(F_{i}^{T}F_{i}\) and \(G_{i}^{T}G_{i}\)

**Proposition 6.1** (\(F_{i}\) and \(G_{i}\) are approximately low-rank if \(X\) is).: _Let \(i[P]\), and \(j[d]\). Then the eigenvalues of \(F_{i}^{T}F_{i}\) and \(G_{i}^{T}G_{i}\) satisfy:_

\[\{_{j}(F_{i}^{T}F_{i}),_{j}(G_{i}^{T}G_{i})\}_{j}( X^{T}X).\]

Proposition 6.1 shows that if \(X\) is an approximately low-rank matrix, so are all the \(F_{i}\)'s and \(G_{i}\)'s. Most data matrices in machine learning exhibit fast polynomial or exponential spectral decay and so are well-approximated by low-rank matrices Wainwright (2019); Derezinski et al. (2020). As the matrix \(H\) that defines the linear system arising from (5) is built from the \(F_{i}^{}s\) and the \(G_{i}^{}s\), it will also be approximately low-rank, which motivates the following assumption.

**Assumption 6.2** (Approximate low-rank structure).: Let \(_{j}(H)\) denote the \(jth\) eigenvalue of \(H\). Then the eigenvalues values of \(H\) satisfy:

\[_{j}(H)=(j^{-2}),>1/2.\]

Assumption 6.2 posits that the eigenvalues of \(H\) decay at a polynomial rate, which is reasonable given the preceding discussion. Under Assumption 6.2, we will obtain complexity guarantees that align with CRONOS' practical performance.

### Fast \(u^{k}\)-update

The next proposition shows that NysPCG solves the linear system in each iteration in (5) fast.

**Proposition 6.3** (Fast solution of \(u\)-subproblem).: _Assume Assumption 6.2 holds. Suppose the randomized Nystrom preconditioner is constructed with rank \(r=(())\). Then after \(t=(())\) iterations, Nystrom PCG outputs a point \(^{k+1}\) satisfying_

\[\|(H+I)^{k+1}-b^{k}\|,\]

_with probability at least \(1-\)._

For fixed \(>0\), Proposition 6.3 shows that with a rank of \(r=(1)\), NysPCG solves the \(\)-subproblem within \(((}))\) iterations. Thus, PCG solves the linear system quickly. Proposition 6.3 is consistent with practice, NysPCG with rank of \(r=20\) and \(10\)-\(40\) PCG iterations work well across all problem instances. Therefore solving (5) is not a bottleneck for CRONOS.

### Convergence of CRONOS

The following theorem shows CRONOS converges to the global minimum of (4).

**Theorem 6.4** (Convergence and Computational Complexity of CRONOS).: _Suppose Assumption 6.2 holds. Fix \((0,1)\) and denote the minimum of (4) by \(p^{}\). Construct the Nystrom preconditioner in Algorithm 2 with rank \(r=(())\), and at the \(kth\) CRONOS iteration run Nystrom PCG with tolerance \(^{k}\) to convergence. Then, with probability at least \(1-\) the following statements hold:_

1. _After_ \(K\) _iterations, CRONOS' output satisfies_ \[(F}^{K},y)+\|}^{K}\|_{2,1}+(}^{K} 0)-p^{}=(1/K),\] \[\|[I_{2dP}\\ G]}^{K}-[}^{K}\\ }^{K}]\|=(1/K).\]
2. _The total complexity of CRONOS to produce an_ \(\)_-suboptimal point of (_4_) is_ \[_{}=}(}{})\]

Theorem 6.4 shows CRONOS converges ergodically at an \((1/K)\)-rate. When the minimum of (4) and (2) coincide2, Theorem 6.4 guarantees CRONOS is within \(\) of the _global minimum_ of (2) after \((1/)\) iterations. By comparison, in the worst-case SGD on (2) can only be guaranteed to output an \(\)-approximate stationary point after \((1/^{4})\) iterations Aryeani et al. (2023). SGD's output may also fail to be close to a local minimum, and can be highly suboptimal Ge et al. (2015). Therefore, CRONOS offers much stronger convergence guarantees than SGD.

Theorem 6.4 is the first realistic convergence guarantee for ADMM on (4). Previously the convergence analysis of Bai et al. (2023) assumes the ADMM subproblems are solved exactly at each iteration, which is unlikely for large-scale data. Consider a precision \(>0\), then the cost of the state-of-the-art ADMM approach from Bai et al. (2023), is at least \((nd^{2}P^{4}+d^{3}P^{3})\) since it solves the linear system derived from Eq. (5) exactly. In contrast, the dependence of CRONOS upon \(n,d,\) and \(P\) offers a significant improvement, since the cost grows linearly with \(n\) and \(d\). This enables our scalability to very high-dimensional datasets in both vision and language.

Theorem 6.4 as presented is pessimistic about the overall convergence speed of CRONOS. For sparsity-promoting convex regularizers such as the group lasso penalty, ADMM is known to reach the manifold containing the support in finite time, after which it converges linearly Liang et al. (2017); Yuan et al. (2020). Therefore, CRONOS convergence is practically much faster than what Theorem 6.4 suggests.

## 7 Experiments

In this section, we empirically evaluate the efficacy of CRONOS and CRONOS-AM on classification tasks. We first experiment with vision data in multi-layer perceptron (MLP) and convolutional neural network (CNN) architectures3. We find that without the necessity of tuning hyperparameters, CRONOS performs as well or better than prevalent standard optimizers. To the best of our knowledge, CRONOS is the first convex reformulated neural network capable of handling large scale data tasks such as ImageNet classification. Secondly, we evaluate the performance of CRONOS on natural language sentiment classification with GPT2 architecture. All experiments were run in JAX v0.4.28 and FLAX v0.8.2. Appendix F presents detailed discussion of the experimental setup and additional numerical results. Details on each dataset used can be found in Appendix F.3.

### Training a deep Multi-Layer Perceptron

We evaluate the performance of CRONOS-AM for binary classification with a 4-layer multilayer perception whose architecture is provided in the Appendix F.5. We train the model on three different datasets CIFAR-10, Food, and ImageNet 4. CRONOS-AM is benchmarked against several of the most popular optimizers in deep learning: SGD with Polyak momentum (SGD) [Sebbouh et al., 2021], Adam [Kingma and Ba, 2014], AdamW [Loshchilov and Hutter, 2017], Shampoo [Gupta et al., 2018], Yogi [Zaheer et al., 2018] and D-adapted Adam (DAdam) [Defazio and Mishchenko, 2023]. For each competing method we consider 5 learning rates, selected randomly from a logarithmic grid with range \([10^{-5.5},10^{-1.5}]\).

Fig. 1 plots the median trajectory across different learning rates of each competing method along with the 5th and 95th quantiles. Fig. 1 shows CRONOS-AM either achieves the best or comparable performance relative to its competitors. These plots and the tables show competing methods exhibit an extremely high degree of variance, and poor learning rate selections can yield non-convergent behavior. In contrast, CRONOS-AM does not exhibit these weaknesses and performs comparably to the best-tuned competitor.

    &  &  \\  Optimizer & Peak Validation Range & Best Learning Rate & Peak Validation Range & Best Learning Rate \\  CRONOS-AM & \(90.5\%\) & NA & \(88.47\%\) & NA \\ DAdam & \(90.15\%\) & NA & \(87.96\%\) & NA \\ Adam & \([50.48,89\%]\) & \(3.79 10^{-5}\) & \([50.87,88.47]\%\) & \(3.68 10^{-6}\) \\ AdamW & \([50.04,90.25]\%\) & \(1.56 10^{-4}\) & \([50.87,87.46]\%\) & \(4.07 10^{-6}\) \\ Yogi & \([50.04,90.84]\%\) & \(5.10 10^{-3}\) & \([50.87,88.47]\%\) & \(6.65 10^{-6}\) \\ SGD & \([50.04,87.75]\%\) & \(5.71 10^{-3}\) & \([50.87,87.46]\%\) & \(4.94 10^{-5}\) \\ Shampoo & \([51.5,89.15]\%\) & \(5.07 10^{-3}\) & \([51.5,89.72]\%\) & \(1.70 10^{-4}\) \\   

Table 1: Results for CIFAR-10 and ImageNet Datasets

Figure 1: CRONOS-AM vs. competitors on Deep ReLU MLPTable 1 presents the range in peak validation across the grid (except for CRONOS-AM and DAdam, which do not require a learning rate parameter). CRONOS-AM outperforms DAdam on both tasks and performs comparably to the best-tuned first-order optimizer. On ImageNet, Shampoo does best by a fair margin. We attribute this to Shampoo being an approximate second-order optimizer. Properly tuned, Shampoo may yield better performance than purely first-order optimizers like CRONOS-AM and Adam for certain tasks.

Table 2 shows the total runtime in seconds for each optimizer on CIFAR-10 and ImageNet. Despite doing more work than competing optimizers, CRONOS-AM's runtime is comparable with standard optimizers such as Adam, AdamW, and Shampoo.

### Natural Language Classification with CRONOS

Our experiments explore sentiment classification with the IMDb dataset. For all experiments, we use the pretrained GPT2 architecture with 12 transformer blocks and an embedding dimension of 768. We use the same GPT2Tokenizer across all language model tasks to ensure consistent evaluation,

   Dataset & CRONOS-AM & Adam & AdamW & D-Adapted Adam & SGD & Shampoo & Yogi \\  CIFAR-10 & 3.00 & 3.02 & 3.14 & 3.68 & 2.28 & 6.80 & 2.79 \\ ImageNet & 5.10 & 1.84 & 3.14 & 2.94 & 2.48 & 5.19 & 1.98 \\   

Table 2: Optimizer Runtimes (s) on CIFAR-10 and ImageNet

    &  &  \\  Optimizer & Peak Validation Range & Best Learning Rate & Peak Validation Range & Best Learning Rate \\  CRONOS & \(77.66\%\) & NA & **93.91\%** & NA \\ AdamW & \([40.29,73.69]\%\) & \(1.56 10^{-3}\) & \([48.30,93.69]\%\) & \(1.32 10^{-4}\) \\   

Table 3: Results for Different GPT2 Architectures

Figure 2: CRONOS vs. AdamW on two GPT2 configurations for IMDb

and all dataset examples are padded to the same length of 60 words. The input to CRONOS is thus shaped into 9 batches of size (1500 x 60 x 768) for each of the 2 labels, and all experiments implement multiple trials across varying data batches. Our baseline benchmark model is the GPT2 pretrained model passed through one dense linear layer for classification accuracy. Numerical results are summarized in Table 3, and Fig. 2 compares CRONOS to tuned AdamW on time. CRONOS is seen to reach best validation faster than AdamW, particularly in Fig. 1(a). Fig. 1(b) and Fig. 1(d) plot several AdamW trajectories along with CRONOS against a number of epochs for AdamW and ADMM iterations for CRONOS. The more translucent the curve for AdamW indicates larger deviation from median trajectory. Both plots show AdamW is extremely sensitive to the learning rate selection. Appendix F.4 provides further details of the three CRONOS integrated GPT2 experiments.

**IMdb-NFT.** We extract the GPT2 pretrained checkpoint and then immediately follow up with CRONOS for sentiment classification. Results are shown in Fig. 1(a) and Fig. 1(b). Notably, this setting does not involve any training loops with standard optimizers (such as AdamW), and the foundation GPT2 model does not see any labeled sentiment data from the IMDb dataset. We limit the amount of data seen by CRONOS to only two batches to evaluate the efficacy of our method in the low data regime. Table 3 shows CRONOS significantly outperforms tuned AdamW. It reaches higher validation accuracy faster than AdamW and has the benefit of not requiring hyperparameter grid search. In contrast, Fig. 2 shows AdamW's performance may be quite poor if the learning rate is not judiciously selected.

**IMdb-FT.** Our fine-tuned experiment setting utilizes the GPT2 pretrained checkpoint followed by _one epoch only_ of training with the AdamW optimizer on default parameters with the standard dense linear layer head followed by CRONOS. Results are shown in Fig. 1(c) and Fig. 1(d). Although the authors of BERT Devlin (2018) recommend 2-4 epochs for training in this setting, we aim to evaluate the performance of CRONOS with limited overhead to extract features for maximum efficiency. The summary of results in Table 3 are particularly promising -- CRONOS reaches **93.91%** validation accuracy, about 0.3% better than tuned AdamW, which is widely regarded as the most effective method for training language models.

**IMdb-DA.** We examine the setting of training for one epoch of AdamW on _unlabeled_ IMDb data initialized from the GPT2 checkpoint. The resulting features are then extracted and passed into CRONOS for classification. The motivation for the experiment is to examine the potential of CRONOS on unsupervised domain adaptation settings in future work, which aims to reduce the distribution gap between source and unlabeled target domains. Our goal is to leverage the pre-trained high-level semantic knowledge in GPT2 and use the unlabeled IMDB data to help align this into our sentiment classification setting. Comprehensive experimental results are further summarized in Appendix F, and show promising directions for future work.

## 8 Conclusion

We introduce CRONOS, the first algorithm to successfully apply convex neural networks to high-dimensional datasets with competitive performance. CRONOS leverages ADMM for robustness and parallelization, Nystrom preconditioning for fast convergence, and JAX for GPU acceleration with large data. We extend this framework with CRONOS-AM: a novel algorithm which utilizes alternating minimization to adapt convex reformulation for deeper networks of arbitrary architecture. CRONOS comes with strong theoretical guarantees that match its practical performance, a rarity for optimization algorithms in deep learning. Experiments on large benchmark datasets in image and language tasks validate the efficacy of our algorithms. CRONOS performs better or comparably against other prevalent optimizers, but with virtually zero tuning of hyperparameters. We achieve validation accuracy of **88.47%** on ImageNet binary classification, and **93.91%** on IMBd sentiment classification. Our goal is to advance an innovative, alternative paradigm in deep learning through convex networks, thus enhancing both efficiency and interpretability.

Our results raise several important questions for future work and present many exciting avenues for continued research. Can we provide a convergence guarantee for CRONOS-AM that shows an advantage over stochastic first-order methods? Additionally, the efficacy of CRONOS on NLP tasks suggest that investigating the performance of CRONOS on harder language datasets is an interesting direction, with potential for scalability on multi-GPU or TPU settings with other modalities.

**Acknowledgments.** This work was supported in part by National Science Foundation (NSF) under Grant DMS-2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; in part by the Office of Naval Research under Grant N00014-24-1-2164. We thank Lucy Wood and Peter Hawkins for support throughout and many insightful discussions.