# Molecule Joint Auto-Encoding:

Trajectory Pretraining with 2D and 3D Diffusion

 Weitao Du1,3  Jiujiu Chen2,3  Xuecang Zhang3  Zhiming Ma1  Shengchao Liu4 \({}^{1}\)

duweitao@mass.ac.cn

\({}^{1}\) Department of Mathematics, Chinese Academy of Sciences

\({}^{2}\) Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China

\({}^{3}\) Huawei Technologies Ltd

\({}^{4}\) Department of Computer Science and Operations Research, Universite de Montreal

###### Abstract

Recently, artificial intelligence for drug discovery has raised increasing interest in both machine learning and chemistry domains. The fundamental building block for drug discovery is molecule geometry and thus, the molecule's geometrical representation is the main bottleneck to better utilize machine learning techniques for drug discovery. In this work, we propose a pretraining method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn both the 2D bond (topology) and 3D conformation (geometry) information, and a diffusion process model is applied to mimic the augmented trajectories of such two modalities, based on which, MoleculeJAE will learn the inherent chemical structure in a self-supervised manner. Thus, the pretrained geometrical representation in MoleculeJAE is expected to benefit downstream geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by reaching state-of-the-art performance on 15 out of 20 tasks by comparing it with 12 competitive baselines. The code is available on this website.

## 1 Introduction

The remarkable progress in self-supervised learning has revolutionized the fields of molecule property prediction and molecule generation through the learning of expressive representations from large-scale unlabeled datasets [1; 2; 3; 4; 5; 6; 7; 8; 9]. Unsupervised representation learning can generally be categorized into two types [10; 11; 12; 13; 7]: generative-based methods, encouraging the model to encode information for recovering the data distribution; and contrastive-based methods, encouraging the model to learn invariant features from multiple views of the same data. However, despite the success of large language models like GPT-3 [14; 15] trained using an autoregressive generation approach, learning robust representations from molecular data remains a significant challenge due to their complex graph structures. Compared to natural language and computer vision data [16; 17], molecular representations exhibit more complex graph structures and symmetries . As molecular dynamics follows the principle of non-equilibrium statistical mechanics , diffusion models, as a generative method inspired by non-equilibrium statistical mechanics [20; 21], are a natural fit for 3D conformation generation. Previous researches have demonstrated the effectiveness of diffusion models, specifically 2D molecular graphs [22; 23] or 3D molecular conformers [24; 25; 26; 27], for molecular structure generation tasks. However, a crucial question remains: _Can diffusion models be effectively utilized for jointly learning 2D and 3D latent molecular representations?_nswering this question is challenging, given that diffusion models are the first generative models based on trajectory fitting. Specifically, diffusion models generate a family of forward random trajectories by either solving stochastic differential equations [28; 29] or discrete Markov chains [23; 30; 31], and the generative model is obtained by learning to reverse the random trajectories [32; 33]. It remains unclear how the informative representation manifests itself in diffusion models, as compared to other generative models that explicitly contain a semantically meaningful latent representation, such as GAN  and VAE . In addition to the generative power of diffusion models, we aim to demonstrate the deep relationship between the forward process of diffusion models and data augmentation [36; 37], a crucial factor in contrastive learning. As a result, we ask whether a trajectory-based self-supervised learning paradigm that leverages both the generative and contrastive benefits of diffusion models can be used to obtain a powerful molecular representation.

**Our approach.** This work presents MoleculeJAE (Molecule Joint Auto-Encoding), a novel trajectory learning framework for molecular representation that captures both 2D (chemical bond structure) and 3D (conformer structure) information of molecules. Our proposed method is designed to respect the \(SE(3)\) symmetry of molecule data and is trained by fitting the joint distribution of the data's augmented trajectories extracted from the forward process of the diffusion model. By training the representation in this manner, our framework not only captures the information of real data distribution but also accounts for the corelation between the real data and its noised counterparts. Under certain approximations, this trajectory distribution modeling decomposes into a marginal distribution estimation and a trajectory contrastive regularization task. This multi-task approach yields an effective and flexible framework that can simultaneously handle various types of molecular data, ranging from SE(3)-equivariant 3D conformers to discrete 2D bond connections. Furthermore, in contrast to diffusion models used for generation tasks that only accept noise as input, most downstream tasks have access to ground-truth molecular structures. To leverage this advantage better, we incorporate an equivariant graph neural network (GNN) block into the architecture, inspired by [38; 39], to efficiently encode crucial information from the ground-truth data. Analogous to conditional diffusion models [38; 40], the encoder's output serves as a meaningful guidance to help the diffused data accurately fitting the trajectory. In summary, our self-supervised learning framework unifies both contrastive and generative learning approaches from a trajectory perspective, providing a versatile and powerful molecular representation that can be applied to various downstream applications.

Regarding experiments, we evaluate MoleculeJAE on 20 well-established tasks drawn from the geometric pretraining literature [6; 7], including the energy prediction at stable conformation and force prediction along the molecule dynamics. Our empirical results support that MoleculeJAE can outperform 12 competitive baselines on 15 tasks. We further conduct ablation studies to verify the effectiveness of key modules in MoleculeJAE.

## 2 Background

In this section, we introduce the diffusion mechanism and relevant notations as a powerful data augmentation framework and elaborate on its instantiation in the context of molecular graph data.

Figure 1: Pipeline of MoleculeJAE. For each individual molecule, MoleculeJAE utilizes the reconstructive task to perform denoising. For pairwise molecules, it conducts a contrastive learning paradigm to fit the trajectory.

### Diffusion Mechanism as a trajectory augmentation

The concept of diffusion is widely used in various fields such as physics, mathematics, and computer science. In this paper, we take a broad view of diffusion by defining it as a Markov process with a discrete or continuous time index \(t\), starting from a given data point \(x_{0}\) and producing a series of random transformations that lead to \(x_{t}\). If \(t\{0,1,2,\}\), then \(x_{t}\) is a discrete Markov chain where the probability distribution \(p(x_{t})\) only depends on the distribution of \(x_{t-1}\). We further refer to the conditional probability \(p(x_{t}|x_{t-1})\) as the transition probability of the Markov chain. This definition enables us to model the evolution of data over time, allowing us to generate a sequence of augmented data points that span different temporal regimes.

In fact, we can adapt classical data augmentations, such as spatial rotation, color distortion, and Gaussian blurring , as sources for defining \(p(x_{t}|x_{t-1})\). For instance, Gaussian blurring generates a specific transition probability by setting \(p(x_{t}|x_{t-1})=(x_{t-1},)\), where the scaling factor \(\) may depend on time \(t\).This transition probability rule perturbatively transforms the data for each step, and connecting all steps of transformed data produces a trajectory. In a similar vein, random masking can be seen as a discrete Markov chain, while continuous rotation can be viewed as a deterministic Markov process. By employing this approach, we can effectively expand the scope of one-step data augmentation operations to encompass trajectory-based scenarios.

In this paper, we build upon the concepts of generalized diffusion processes proposed in  to establish a unified framework for the trajectory augmentation examples mentioned earlier. Following the general approach in , each (cold) diffusion model is associated with a degradation Markov process \(D(,t)\) indexed by a time variable \(t[0,T]\), which introduces perturbations to the initial data \(x_{0}\):

\[x_{0}x_{t}.\] (1)

Given that \(x_{0} p_{data}\), the transformed variable \(x_{t}:=D(x_{0},t)\), is also a random variable. To represent the marginal distribution of \(D(,t)\) at time \(t\), we use the notation \(p_{t}()\). When the context is clear, we will use the shorthand notation \(D_{t}\) to refer to \(D(,t)\). According to Equation (1), we define the pair \((x_{0},x_{t})\) as a multi-view of the sample \(x_{0}\) at a specific time \(t\). It is important to note that, under certain conditions, a Markov process converges to a stationary distribution that effectively erases any memory of the initial data \(x_{0}\). Nevertheless, for small values of \(t\), there is a high probability that \(x_{t}\) remains within a ball centered around \(x_{0}\).

In the context of the standard diffusion-based generative model , the presence of a reverse process \(R_{t}\) is necessary to undo the effects of the degradation \(D_{t}\). In , the degradation \(D(,t)\) is identified as the solution of a stochastic differential equation (SDE), and it has been demonstrated that a family of SDEs, including the probability flow ordinary differential equation (ODE) proposed in , can reverse the degradation process \(D(,t)\). Specifically, the inversion property implies that \(R_{T-t}\) shares the same marginal distribution as \(p_{t}\): \(R_{T-t} p_{t}\). Therefore, as \(t\) varies from 0 to \(T\), the reverse process \(R_{t}\) gradually restores the data distribution. On the other hand, for cold diffusions, although the model is also trained by reconstructing the data from \(D_{t}\), it does not explicitly assume the existence of a reverse process as rigorously as in the continuous diffusion-based generative model.

Example of heat diffusionWe use the term 'heat' to describe a diffusion process that involves injecting Gaussian noise. Under this definition, the continuous limit of heat diffusion can be expressed by the solution of stochastic differential equations (SDEs):

\[dx_{t}=(x_{t},t)dt+(t)dw_{t}, t[0,T]\] (2)

Here, \(w_{t}\) represents the standard Brownian motion. It is worth noting that while the solution \(x_{t}\) of a general SDE may not follow the Gaussian distribution, for the commonly used SDEs such as the Variance Exploding SDE (VE) and Variance Preserving SDE (VP), the solution can be explicitly expressed as:

\[x_{t}=(t)x_{0}+(t)z,\] (3)

where \(z\) is sampled from \((0,I)\), and both \((t)\) and \((t)\) are positive scalar functions. Using the forward SDE (2), we approach the task of data synthesis by gradually denoising the noisy observations \(x_{t}\) to recover \(x_{0}\), which is also accomplished through solving a reverse SDE. For the general formula and further details, readers can refer to . One fact we will use later is that the reverse SDE fundamentally relies on the score function \(_{x} p_{t}(x)\), which serves as the parameterization objective in the score matching framework .

Example of discrete (cold) diffusionIn contrast to the continuous heat diffusion framework, we now introduce a family of discrete Markov Chains as an instance of discrete (cold) diffusion. This example is particularly suitable for modeling categorical data. In the unconditional setting, the transition probability of the discrete diffusion at time step \(t\) is defined as:

\[q(x_{t}|x_{t-1})=(x_{t},p=x_{t-1}Q_{t}),\] (4)

where \(Q_{t}\) represents a Markov transition matrix. An illustrative example is the **absorbing diffusion**, as introduced by , where

\[Q_{t}=(1-_{t})I+_{t}e_{m}^{T},\]

and \(e_{m}\) is a one-hot vector with a value of 1 at the absorbing state \(m\) and zeros elsewhere. Typically, the state \(m\) is chosen as the'masked' token. Hence, absorbing diffusion can be regarded as a masking process with a dynamical masking ratio \(_{t}\). Similarly to the heat diffusion, the corresponding reverse process can also be formulated as a discrete diffusion. However, the training objective shifts from fitting the score function (which only exists in the continuous case) to directly fitting the'reconstruction probability' \(p(x_{0}|x_{t})\).

### Graph-based molecular representation

In this paper, our primary focus is on the graph representation of molecules. Let \(G=(V,E,P,H)\) denote the integrated molecular graph, consisting of \(n:=|V|\) atoms and \(|E|\) bonds. The matrix \(P^{n 3}\) is utilized to represent the 3D conformer of the molecule, containing the positions of each node. Moreover, within the graph \(G\), the edge collection \(E\) represents the graphical connections and chemical bond types between atoms. Additionally, we have the node-wise feature matrix \(H^{n h}\), where, for the purposes of this article, we consider the formal charges and atom types as the components of \(H\). In the method section, we will demonstrate how to build trajectories based on \(G\).

## 3 Method

Now we illustrate the process of obtaining molecule augmented trajectories, based on which, MoleculeJAE is proposed to estimate the trajectory distribution. In Section 3.1, we will outline the construction of equivariant molecular trajectories. Subsequently, in Section 3.2, we will introduce our theoretical self-supervised learning framework for these trajectories. Our hypothesis is that a good representation should encode the information from the distribution of augmented trajectories. This hypothesis forms the practical basis of our core reconstructive and contrastive loss, which will be discussed in Section 3.3. Finally, in Section 3.4, we will present the key architectures. A comprehensive discussion of the related works is in Appendix B.

### Equivariant Molecule Trajectory Construction

In the field of molecular representation learning, our objective is to jointly estimate the distribution of a molecule's 2D topology (including atom types and chemical bonds) and its 3D geometries (conformers). Building upon the notations in Section 2.2, our goal is to construct **augmented** trajectories \(x_{t}:=(H(t),E(t),P(t))\) from \(G\). The challenging aspect lies in preserving the \(SE(3)\) symmetry of the position matrix \(P\), which we will describe in detail below.

Given two 3D point clouds (molecular conformers) \(P_{1}\) and \(P_{2}\), we say they are SE(3)-isometric if there exists an \(R SE(3)\) such that \(P_{1}=RP_{2}\). In probabilistic terms, let \(p_{3D}(x_{3D})\) be the probability density of all 3D conformers denoted by \(\). Isometric conformers necessarily have the same density, i.e.,

\[p_{3D}(x_{3D})=p_{3D}(x_{3D}),\ \ \ \  SE(3),x_{3D} .\] (5)

Utilizing the symmetry inductive bias has been shown [24; 45] to greatly reduce the complexity of data augmentation. Additionally, in order to respect the \(SE(3)\) symmetry, the augmented trajectory \(P(0) P(t)\) should also be SE(3)-equivariant, satisfying

\[P(x_{3D})=P(x_{3D}),\ \ \ \  SE(3).\]

This condition imposes a rigorous restriction on the form of the forward SDE (2) when it is applied to generate the augmented trajectory of conformers. However, if we constrain the form of \(x_{t}\) to Eq. 3, the \(SE(3)\) equivariance is automatically satisfied due to the \(SE(3)\) equivariance of both the Gaussian random variable \(z\) and the original data \(x_{0}\). We leave the formal proof in Appendix A.

Regarding the 2D component, let \(x_{2D}(t):=(H(t),E(t))\), where \(x_{2D}\) consists of invariant scalars that remain unchanged under \(SE(3)\) transformations. Although \(x_{2D}\) is categorical in nature, we can treat them as continuous scalars that can be perturbed by Gaussian noise. During inference (generation), these continuous scalars are quantized back into categorical integers (see  for details). This allows both \(x_{2D}(t)\) and \(P(t)\) to undergo heat diffusion. By combining the 2D and 3D parts, we introduce the system of SDEs for \(x_{t}\):

\[dP(t)=-P(t)dt+_{1}(t)dw_{t}^{1},\\ dH(t)=_{2}(H(t),t)dt+_{2}(t)dw_{t}^{2},\\ dE(t)=_{3}(E(t),t)dt+_{3}(t)dw_{t}^{3}.\] (6)

It is worth mentioning that although the components of Eq. 6 are disentangled, the corresponding reverse diffusion process and its score function are entangled. A common choice of \((x,t)\) is \((x,t):=-x\), then will utilize the explicit solution of equations (6) to generate the molecular augmentation trajectory. Additionally, it is also possible to perform equivariant diffusion of the 2D and 3D joint representation through equivariant cold diffusion, following the approach described in (4). The detailed framework for this approach is provided in the Appendix A.

### Equivariant Molecule Trajectory Learning with Density Auto-Encoding

To provide a formal foundation for self-supervised learning from augmented trajectories, we revisit the concept of denoising from a trajectory perspective. According to the Kolmogorov extension theorem , every stochastic process defines a probabilistic measure on the trajectory space, uniquely determined by a set of finite-dimensional joint distributions that satisfy consistency conditions. Therefore, estimating the probability of the infinite-dimensional trajectory space is equivalent to estimating the joint probabilities \(p(x_{t_{1}},,x_{t_{k}})\) for each finite time sequence \(t_{1},,t_{k}[0,T]\). From the standpoint of data augmentation, our specific focus is on learning the joint distribution of trajectory augmentation pairs: \(p(x_{0},x_{t})\) (solution of Eq. 6), which determines how the noisy \(x_{t}\) corresponds to the original (denoised) \(x_{0}\).

To differentiate our notion of "Auto-Encoding" from the "denoising" method utilized in DDPM  (denoising diffusion probabilistic models), it is important to highlight that traditional diffusion-based generative models are based on marginal distribution modeling. In this framework, joint distributions induce marginal distributions, but not vice versa. This distinction becomes more apparent in the continuous SDE formalism of diffusion models , where multiple denoising processes from \(x_{t}\) to \(x_{0}\) can be derived while still sharing the same marginal distributions. However, the joint distributions of a probabilistic ODE flow significantly differ from those of the reverse SDE (as shown in Eq. 11) due to the deterministic nature of state transitions in an ODE between timesteps. In the following, we formally demonstrate this observation from the perspective of maximizing the trajectories' joint probabilistic log-likelihood.

For a given time \(t[0,T]\), let us assume that the probability of the random pair \((x_{0},x_{t})\) defined in Eq. 1 is determined by a joint density function \(p(x_{0},x_{t})\). Our objective is to approximate \(p(x_{0},x_{t})\) within a variational class \(p_{}(x_{0},x_{t})\). Maximizing the likelihood of this joint distribution leads to the following optimization problem:

\[_{}_{i=1}^{n}p_{}(x_{0}^{i},x_{t}^{i}),\] (7)

where \(\{(x_{0}^{i},x_{t}^{i})\}_{i=1}^{n}\) represents the collection of \(n\) augmented samples from the training dataset. Following the tradition of Bayesian inference , we parameterize \(p_{}\) by defining a joint energy function \(_{}(x_{0},x_{t})\) such that: \(p_{}(x_{0},x_{t})=e^{-_{}(x_{0},x_{t})}\), where \(Z_{}(t)\) is the intractable normalization constant depending on \(t\). Therefore, the maximal likelihood framework reduces to solving

\[_{}_{p(x_{0},x_{t})}[_{}(x_{ 0},x_{t})].\]

However, directly optimize \(p_{}(x_{0},x_{t})\) by taking the gradient with respect to \(\) is challenging because \((x_{0},x_{t})}{}\) contains an intractable term: \((t)}{}\). To circumvent this issue, we borrow ideas from [48; 49] by treating the transformed \(x_{t}\) as an infinitely dimensional "label" of \(x_{0}\). More precisely, we consider the parameterized marginal density \(q_{}(x_{0})\) as:

\[q_{}(x_{0}):= p_{}(x_{0},x_{t})dx_{t},\] (8)which involves integrating out \(x_{t}\). We define the marginalized energy function with respect to \(x_{0}\) as: \(}_{}():=-(-_{}(,x_{ t}))dx_{t}\). Now, let \(f_{}(x_{0},x_{t})\) denote the normalized conditional density function \(p_{}(x_{t}|x_{0})\), we have \(|x_{0})}{}=-_{}(x_{0},x_{t})}{}+_{ }(x_{0})}{}\). By taking the empirical expectation with respect to the finitely sampled pair \((x_{0},x_{t}) p(x_{0},x_{t})\), we can decompose the gradient of the maximum likelihood as follows (see Appendix A for the full derivation):

\[}_{p(x_{0},x_{t})}[(x_{0}, x_{t})}{}]=}_{p(x_{0})}[(x_{0})}{}]+}_{p( x_{0},x_{t})}[(x_{0},x_{t})}{} ],\] (9)

here we use \(}\) to denote the expectation with respect to the empirical expectation. Note that this decomposition holds for \((x_{s},x_{t})\) for any two different time steps (\(0 s,t T\)) of the trajectories. In the next section, we decompose Equation (16) into two sub-tasks, leading to our goal of optimizing these two parts simultaneously, as discussed below.

### Reconstructive and Contrastive Tasks

In what follows, we denote the latent representation of data \(x\) by \(h_{}(x)\) (the equivariant model for building \(h_{}\) will be discussed in the next section). Based on Eq. 16, we introduce two tasks for training \(h_{}(x)\) that stem from this decomposition.

Reconstructive task.The first term \(}_{p(x_{0})}[(x_{0})}{ }]\) in Eq. 16 aims to reconstruct the distribution of data samples. Therefore, we refer this term as the **reconstruction** task, which involves modeling the marginal distribution \(p_{data}(x_{0})\) using \(q_{}(x_{0})\).

Although it is possible to directly train the likelihood reconstruction term in the auto-regressive case, such as using the noise conditional maximum likelihood loss \(_{t[0,T]}_{x_{t} p_{t}} q_{}(x_{t})\) proposed in , we instead adopt trajectory score matching , which is applicable for non-autoregressive graph data. The score matching loss is defined as follows:

\[_{sc}:=_{t[0,T]}_{p(x_{0})p(x_{t}|x_{0})}[ \| p(x_{t}|x_{0})-s_{}(x_{t},t)\|^{2}].\] (10)

We choose this approach for two reasons: First, training the score function enables the model to generate new samples by solving the reverse process, thus facilitating generative downstream tasks. Second, when \(t=0\), the score function for 3D structures can be interpreted as a "pseudo" force field for the molecular system , containing essential information about the molecular dynamics. Furthermore,  provided a rigorous proof demonstrating that score matching formula 10 serves as a variational lower bound for the marginal log-likelihood \( q_{}(x_{0})\). This theoretical guarantee solidifies the effectiveness of score matching as a training objective.

For molecule trajectories, the score function encompasses both the 2D and 3D components. Moreover, the SE(3)-invariant density \(p_{3D}\) defined by Eq. 5 implies that the corresponding 3D score function \(_{x}p_{3D}(x)\) (represented as \((P_{t})\) in Fig. 2) is equivariant under \(SE(3)\):

\[_{x}p_{3D}(x)=_{x}p_{3D}(x),\ \ \  SE(3),\ \ x.\]

In conclusion, the symmetry principle mandates that the score neural network takes an equivariant vector field (representing the positions of all atoms) and invariant atom features as input. It then produces two score functions that adhere to different transformation rules: 1. \(_{x}p_{3D}(x)\): SE(3)-equivariant; 2. \(_{y}p_{2D}(y)\): SE(3)-invariant.

Contrastive task.We have demonstrated that optimizing the score matching task allows us to capture information about the marginal distributions of the trajectories. However, the joint distribution contains additional valuable information. As an example, consider the following parameterized random processes, all of which share the same marginal distributions but exhibit varying joint distributions (as proven in ):

\[dy_{t}=[f(y_{t},t)-}{2}g^{2}(t)s_{}(y_{t},t)]dt+  g(t)dB_{t},\] (11)

for \(>0\). Hence, it is theoretically necessary to optimize the second term \(}_{p(x_{0},x_{t})}[(x_{0}, x_{t})}{}]\) of Eq. 16. By employing the conditional probability formula, we have:

\[f_{}(x_{0},x_{t})=(x_{0},x_{t})}{ p_{}(x_{0},y) dy}.\] (12)It is important to note that the troublesome normalizing constant \(Z_{}(t)\) is cancelled out in Eq. 12. In practice, the integral \( p_{}(x_{0},y)dy\) is empirically approximated using Monte Carlo sampling. To make a connection with contrastive learning (CL), recall that in CL, a common approach is to align the augmented views (\((x,x^{+})\)) of the same data and simultaneously contrast the augmented views of different data (\(x^{-}\)). By treating the joint distribution as a similarity measure, Eq. 12 can be seen as implicitly imposing two regularization conditions on \(f_{}(x_{0},x_{t})\): ensuring a high probability for \(p(x_{0},x_{t})\) and a low probability for \(p(x_{0},y)\). This notion of similarity motivates us to refer to maximizing the second term of Eq. 16 as a **contrastive** task.

Contrastive surrogateHowever, estimating \(p_{}(x_{0},x_{t})\) is challenging due to the intractability of closed-form joint distributions. To overcome this difficulty, we propose a black-box surrogate \(\) that represents the mapping from the latent representation \(h_{}(x_{t})\) to \(p_{}(x_{0},x_{t})\) (following the notation in figure 2). Specifically, let \((h_{}(x_{0}),h_{}(x_{t}))\) denote the representation pair obtained from the input molecule data \((x_{0},x_{t})\). Then, the surrogate of \(p_{}(x_{0},x_{t})\) is defined by \(p_{}(x_{0},x_{t})=\{-(x_{0} ))-(h_{}(x_{t}))\|^{2}}{^{2}(t)}+C(x_{0})\}\). Here, \((t)\) is a monotone annealing function with respect to \(t\). By using Eq. 12, the unknown normalization constant \(Z(t)\) and \(C(x_{0})\) cancel out, resulting in the following approximation:

\[f_{}(x_{0},x_{t})(x_{0}))- (h_{}(x_{t}))\|^{2}\}}{\{-\|(h_{}(x_{0}))-( h_{}(y))\|^{2}\}dy}.\] (13)

Our surrogate is reasonable because our modeling target \(p(x_{0},x_{t})\) is derived from the joint distributions of a (continuous) Markov process. When \(x_{t}\) lies along the augmented trajectory of a specific data sample \(x_{0}\) and approaches \(x_{0}\) as \(t 0\), the log-likelihood \( p_{}(x_{0},x_{t})\) is also achieves a local maximum. Therefore, based on the Taylor expansion, the leading non-constant term takes the form of a quadratic expression.

Final ObjectiveBy combining the reconstruction and contrastive regularization tasks, we define the final **multi-task** training objective of MoleculeJAE as a weighted sum of the two tasks:

\[_{1}_{sc}+_{2}_{co},\] (14)

where \(_{1},_{2}\) are weighting coefficients, and \(_{co}:=_{t[0,T]}_{(x_{0},x_{t}) p(x_{0 },x_{t})}f_{}(x_{o},x_{t})\). In Fig. 2, the noised input \(x_{t}\) undergoes an encoding process to obtain the latent representation \(h_{}(x_{t})\), which is then split into two branches:

1. The first branch passes through a score neural network \(\) for reconstruction: \(s_{}(x_{t})=(h_{}(x_{t}))\);
2. The second branch incorporates the original data representation \(h_{}(x_{0})\) and further projects the pair \((h_{}(x_{0}),h_{}(x_{t}))\) through a non-linear projection head \(\) for contrastive learning.

Note that the black-box projection function \(g\) (although not used in downstream tasks) also participates in the pretraining optimization, following the conventional contrastive learning frameworks . See Fig. 1 for a graphical illustration of MoleculeJAE's pipeline.

Figure 2: Architecture of MoleculeJAE. The inputs are ground-truth molecules with both 2D and 3D structures. MoleculeJAE also adopts the noised inputs denoising, so as to model the trajectory distribution. The outputs are three score functions for conformer and bond representations, which flow into the general pipeline in Figure 1.

### Model Architecture of MoleculeJAE

In pursuit of a meaningful latent molecular representation, we design our joint auto-encoder model as a conditional diffusion based model, inspired by [38; 39; 53]. In our model, the condition does not come from labels, but rather an encoding of the ground-truth molecule. Specifically, to obtain the latent representation \(h_{}\) shown in Fig. 2, we implement two **equivariant** encoders that satisfy the \(SE(3)\) symmetry proposed in Section 3.1. One encoder takes the original molecule as input, and its output is used as a condition to help the other encoder that encodes the noisy molecule for reconstruction. The only requirement for the architecture of the two encoders is that the output should be invariant. Therefore, any \(SE(3)\) equivariant GNN  that outputs invariant scalars will suffice.

**Equivariant decoder.** With our representation \(h_{}\) which depends on \((x_{0},x_{t},t)\), the decoder part of MoleculeJAE is divided into two heads. One head is paired with node-wise SE(3) frames to match the \(SE(3)\) equivariant score function, while the other head generates an \(SE(3)\) invariant representation that is used for contrastive learning (see Fig. 2 for a complete illustration). Further details on the model design are left in Appendix A.

## 4 Experiment

### MoleculeJAE Pretraining

**Dataset.** For pretraining, we use PCQM4Mv2 . It extracts 3.4 million molecules from PubChemQC  with both 2D topology and 3D geometry.

**Backbone models.** We want to highlight that MoleculeJAE is agnostic to backbone geometric GNNs. In this work, we follow previous works in using SchNet model  for 3D conformation. For the 2D GNN representation, we take a simple version by mainly modeling the bond information (details in Appendix D). For the SDE models  for generating the joint trajectories, we consider both VE and VP, as in Eq. 6.

**Baselines for 3D conformation pretraining.** Recently, few works have started to explore 3D conformation pretraining. For instance, GeoSSL  provides comprehensive baselines. The initial

   Pretraining & \(\) & \(\) & \(_{}\) & \(_{}\) & \(\) & \(C_{v}\) & \(G\) & \(H\) & \(R^{2}\) & \(U\) & \(U_{0}\) & ZPVE \\  \(-\) (random init) & 0.060 & 44.13 & 27.64 & 22.55 & 0.028 & 0.031 & 14.19 & 14.05 & 0.133 & 13.93 & 13.27 & 1.749 \\ Type Prediction & 0.073 & 45.35 & 28.76 & 24.83 & 0.036 & 0.032 & 16.66 & 16.28 & 0.275 & 15.56 & 14.66 & 2.094 \\ Distance Prediction & 0.065 & 45.87 & 27.61 & 23.34 & 0.031 & 0.033 & 14.83 & 15.81 & 0.248 & 15.07 & 15.01 & 1.837 \\ Angle Prediction & 0.066 & 48.45 & 29.02 & 24.40 & 0.034 & 0.031 & 14.13 & 13.77 & 0.214 & 13.50 & 13.47 & 1.861 \\
3D InfoGraph & 0.062 & 45.96 & 29.29 & 24.60 & 0.028 & 0.030 & 13.93 & 13.97 & 0.133 & 13.55 & 13.47 & 1.644 \\ GeoSSL-RR & 0.060 & 43.71 & 27.71 & 22.84 & 0.028 & 0.031 & 14.54 & 13.70 & **0.122** & 13.81 & 13.75 & 1.694 \\ GeoSSL-InfoNCE & 0.061 & 44.38 & 27.67 & 28.25 & **0.027** & 0.030 & 13.38 & 13.36 & **0.116** & 13.05 & 13.00 & 1.643 \\ GeoSSL-EBM-NCE & 0.057 & 43.75 & 27.05 & 22.75 & 0.028 & 0.030 & 12.87 & 12.65 & 0.123 & 13.44 & 12.64 & 1.652 \\
3D InfoMax & 0.057 & **42.09** & 25.90 & 21.60 & 0.028 & 0.030 & 13.73 & 13.62 & 0.141 & 13.81 & 13.30 & 1.670 \\ GraphVVP & 0.056 & **41.99** & 25.75 & **21.58** & **0.027** & **0.029** & 13.43 & 13.31 & 0.136 & 13.03 & 13.07 & 1.609 \\ GeoSSL-DDM-IL & 0.058 & 42.64 & 26.32 & 21.87 & 0.028 & 0.030 & 12.61 & 12.81 & 0.173 & 12.45 & 12.12 & 1.696 \\ GeoSSL-DDM & 0.056 & 42.29 & **25.61** & 21.88 & **0.027** & **0.029** & **11.54** & **11.14** & 0.168 & **11.06** & **10.96** & 1.660 \\  MoleculeJAE & **0.056** & 42.73 & 25.95 & **21.55** & **0.027** & **0.029** & **11.22** & **10.70** & 0.141 & **10.81** & **10.70** & **1.559** \\   

Table 1: Results on 12 quantum mechanics prediction tasks from QM9. We take 110K for training, 10K for validation, and 11K for testing. The evaluation is mean absolute error, and the best and the second best results are marked in **bold** and **bold**, respectively.

   Pretraining & Aspin \(\) & Benzene \(\) & Ethanol \(\) & Malonaldehyde \(\) & Naphthalene \(\) & Salicylic \(\) & Toluene \(\) & Uracil \(\) \\  (-\) (random init) & 1.203 & 0.380 & 0.386 & 0.794 & 0.587 & 0.826 & 0.568 & 0.773 \\ Type Prediction & 1.383 & 0.402 & 0.450 & 0.879 & 0.622 & 1.028 & 0.662 & 0.840 \\ Distance Prediction & 1.427 & 0.396 & 0.434 & 0.818 & 0.793 & 0.952 & 0.509 & 1.567 \\ Angle Prediction & 1.542 & 0.447 & 0.669 & 1.022 & 0.680 & 1.032 & 0.623 & 0.768 \\
3D InfoGraph & 1.610 & 0.415 & 0.560 & 0.900 & 0.788 & 1.278 & 0.768 & 1.110 \\ GeoSSL-RR & 1.215 & 0.393 & 0.514 & 1.092 & 0.596 & 0.847 & 0.570 & 0.711 \\ GeoSSL-InfoNCE & 1.132 & 0.395 & 0.466 & 0.888 & 0.542 & 0.831 & 0.554 & 0.664 \\ GeoSSL-EBM-NCE & 1.251 & 0.373 & 0.457 & 0.829 & 0.512 & 0.990 & 0.560 & 0.742 \\
3D InfoMax & 1.142 & 0.388 & 0.469 & 0.731 & 0.785 & 0.798 & 0.516 & 0.640 \\ GraphVVP & **1.126** & 0.377 & 0.430 & 0.726 & **0.498** & 0.740 & 0.508 & 0.620 \\ GeoSSL-DDM-IL & 1.364 & 0.391 & 0.432 & 0.830 & 0.590 & 0.817 & 0.628 & 0.607 \\ GeoSSL-DDM & **1.107** & **0.360** & **0.357** & 0.737 & 0.568 & 0.902 & **0.484** & **0.502** \\  MoleculeJAE & 1.289 & **0.345** & **0.365** & **0.613** & **0.498** & **0.712** & **0.480** & **0.463** \\   

Table 2: Results on eight force prediction tasks from MD17. We take 1K for training, 1K for validation, and 48K to 991K molecules for the test concerning different tasks. The evaluation is mean absolute error, and the best results are marked in **bold** and **bold**, respectively.

three baselines involve type prediction, distance prediction, and angle prediction, respectively aiming to predict the masked atom type, pairwise distance, and triplet angle. The next baseline is 3D InfoGraph. It is a contrastive SSL method and predicts whether the node- and graph-level 3D representation are for the same molecule. Last but not least, GeoSSL proposes a new SSL family on geometry: GeoSSL-RR, GeoSSL-InfoNCE, and GeoSSL-EBM-NCE are to maximize the MI between the conformation and augmented conformation using different objective functions, respectively. GeoSSL-DDM optimizes the same objective function using denoising distance matching. GeoSSL-DDM-1L  is a special case of GeoSSL-DDM with one layer of denoising.

**Baselines for 2D and 3D multi-modal pretraining.** Additionally, several works have utilized both 2D topology and 3D geometry modalities for molecule pretraining. Vanilla GraphMVP  utilizes both the contrastive and generative SSL, and 3D InfoMax  only uses the contrastive learning part.

In the following, we provide the downstream tasks for applying our pre-trained MoleculeJAE. The experiment on joint generation of the 2D and 3D structures of molecules are provided in Appendix F.

### Quantum Property Prediction

QM9  is a dataset of 134K molecules, consisting of nine heavy atoms. It contains 12 tasks, which are related to the quantum properties. Among 12 tasks, the tasks related to the energies are very important, _e.g._, \(U\) and \(U_{0}\) are the internal energies at 0K and 298.15K, respectively. The other two energies, \(H\) and \(G\) can be obtained from \(U\) accordingly. The main results are in Table 1. We can observe that MoleculeJAE can outperform 12 baselines on 9 out of 12 tasks. We want to highlight that these baselines are very competitive, and certain works (_e.g._, GeoSSL) also model the 3D trajectory. Noticeably, MoleculeJAE can reach the best performance on four energy-related tasks.

### Molecular Dynamics Prediction

MD17  is a dataset on molecular dynamics simulation. It contains eight tasks corresponding to eight organic molecules, and the goal is to predict the forces at different 3D positions. The size of each task ranges from 48K to 991K, and please check Appendix C for details. The main results are in Table 2, and MoleculeJAE can outperform 12 baselines on 6 out of 8 tasks and reach the second-best for one of the remaining tasks.

### Ablation Study on the Effectiveness of Contrastive Loss

As discussed in Equation (14), there is one important hyperparameter \(_{2}\) controlling the contrastive loss in MoleculeJAE. We want to conduct an ablation study on the effect of this contrastive term. As shown in Tables 3 and 4, we consider three value for \(_{2}\): 0, 0.01, and 1. \(_{2}=0\) simply means that we only consider the reconstructive task, and its performance is very close to \(_{2}=0.01\), _i.e._, the optimal results reported in Tables 1 and 2. However, as we increase the \(_{2}=1\), the performance degrades by a large margin. Thus, we would like to claim that the contrastive term in MoleculeJAE is comparatively sensitive, and we need to tune this hyperparameter carefully to obtain optimal results.

## 5 Conclusion

In this work, we introduce a novel joint self-supervised learning framework called MoleculeJAE, which is based on augmented trajectory modeling. The term "joint" in our framework has two implications: Firstly, it signifies that our method is designed to model the joint distribution of trajectories rather than solely focusing on the marginal distribution. Secondly, it indicates that the

   Pretraining & \(\) & \(\) & \(_{}\) & \(_{}\) & \(\) & \(C_{v}\) & \(G\) & \(H\) & \(R^{2}\) & \(U\) & \(U_{0}\) & ZPVE \(\) \\  \(_{2}=0\) & 0.057 & 43.15 & 26.05 & 21.42 & 0.027 & 0.030 & 12.23 & 11.95 & 0.162 & 12.20 & 11.42 & 1.594 \\ \(_{2}=0.01\) & 0.056 & 42.73 & 25.95 & 21.55 & 0.027 & 0.029 & 11.22 & 10.70 & 0.141 & 10.81 & 10.70 & 1.559 \\ \(_{2}=1\) & 0.066 & 45.45 & 28.23 & 23.67 & 0.028 & 0.030 & 14.67 & 14.42 & 0.204 & 13.30 & 13.25 & 1.797 \\   

Table 4: Ablation studies of contrastive loss term in MoleculeJAE. The ablation results are on MD17.

   Pretraining & Aspirin \(\) & Benzene \(\) & Ethanol \(\) & Malonaldehyde \(\) & Naphthalene \(\) & Salicylic \(\) & Toluene \(\) & Uracil \(\) \\  \(_{2}=0\) & 1.380 & 0.359 & 0.363 & 0.744 & 0.482 & 0.902 & 0.548 & 0.590 \\ \(_{2}=0.01\) & 1.289 & 0.345 & 0.365 & 0.613 & 0.498 & 0.712 & 0.480 & 0.463 \\ \(_{2}=1\) & 1.561 & 0.547 & 0.781 & 0.735 & 0.918 & 1.160 & 1.052 & 0.809 \\   

Table 3: Ablation studies of contrastive loss term in MoleculeJAE. The ablation results are on QM9.

augmented molecule trajectory incorporates both 2D and 3D information, providing insights into different aspects of molecule representation. While our proposed method has demonstrated the best empirical results on 15 out of 20 geometry-related property prediction tasks, there are still areas left for improvement, such as architecture design. Please refer to Appendix E for an in-depth discussion.