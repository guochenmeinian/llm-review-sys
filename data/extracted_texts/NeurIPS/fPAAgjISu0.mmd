# In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer

Yuzhou Cao\({}^{1}\)  Hussein Mozannar\({}^{2}\)  Lei Feng\({}^{1}\)

\({}^{1}\)School of Computer Science and Engineering, Nanyang Technological University, Singapore

\({}^{2}\)CSAIL and IDSS, Massachusetts Institute of Technology, Cambridge, MA

\({}^{3}\)Department of Statistics and Data Science, Southern University of Science and Technology, China

yuzhou002@e.ntu.edu.sg, mozannar@mit.edu

lfengqaq@gmail.com, weihx@sustech.edu.cn, boan@ntu.edu.sg

Corresponding author: Lei Feng.

###### Abstract

Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our method and empirically validate its performance and calibration on benchmark datasets.

## 1 Introduction

As machine learning models get deployed in risk-critical tasks such as autonomous driving , content moderation , and medical diagnosis , we have a higher urgency to prevent incorrect predictions. To enable a safer and more accurate system, one solution is to allow the model to defer to a downstream human expert when necessary. The **L**earning to **D**efer (L2D) paradigm achieves this aim [28; 30; 31; 9; 44; 32; 33; 45; 3; 35; 15; 42] and enables models to defer to an expert, i.e., abstain from giving a prediction and request a downstream expert for an answer when needed. L2D aims to train an augmented classifier that can choose to defer to an expert when the expert is more accurate or make a prediction without the expert. L2D can be formulated as a risk-minimization problem that minimizes the 0-1-deferral risk , which incurs a cost of one when the classifier is incorrect or when we defer to an expert who errs and incurs a cost of zero otherwise.

Despite being formulated straightforwardly, the risk minimization problem is NP-hard even in simple settings [15; 32] due to the discontinuous and non-convex nature of the 0-1-deferral loss. To make the optimization problem tractable, many efforts have been made to design a continuous surrogate loss for the 0-1-deferral loss while guaranteeing statistical consistency, which means that the minimizer of the surrogate risk is that of the 0-1-deferral risk. In Mozannar and Sontag , a cross-entropy-likesurrogate loss is proposed that is statistically consistent. Charusaie et al.  further generalized the results in Mozannar and Sontag  and showed that all the losses that are consistent for ordinary multi-class classification, i.e., classification-calibrated [38; 43; 6], are consistent for L2D with certain reformulation, which makes L2D a more comprehensive framework. By minimizing the risk _w.r.t._ consistent surrogate losses [30; 9], we can obtain a model that defers to who between the classifier and expert is more accurate on an instance basis. However, we also need assessments of the uncertainty of the classifier when it predicts and the uncertainty of the expert prediction when we defer. This allows us to perform better triaging of samples and provide reliable estimates of uncertainty. Verma and Nalisnick  pointed out that the previous estimator in  generates highly biased probability estimates for the predictions due to its unboundedness. They then proposed a surrogate based on the One-versus-All (OvA) strategy  to alleviate this problem, which allows both statistical consistency and improved calibration compared to the previous softmax-based method .

Does the difference in performance between OvA and softmax-based methods indicate that softmax parameterization is a sub-optimal choice in L2D? The cause of the unbounded probability estimation in  is in fact still elusive, and we cannot simply attribute it to softmax parameterization. Furthermore, notice that the softmax-based method  is only a specific implementation of applying softmax in L2D. Meanwhile, softmax parameterization is also a more straightforward modeling of multiclass posterior distributions: while the OvA strategy works by splitting a \(K\)-class classification problem into \(K\) independent binary classification problems and thus induces \(K\) uncorrelated probability estimators for each class, softmax parameterization works by directly estimating class posterior probabilities as a whole. Given the wide use and practical advantages of softmax parameterization in classification tasks and the resemblance between L2D and multiclass classification, it is also promising that the usage of the softmax function can result in competitive methods for L2D. Then a natural question arises: can we design a softmax-based consistent surrogate loss for L2D without triggering unbounded probability estimation enabling calibrated estimates?

In this paper, we give a positive answer to this question by providing a novel consistent surrogate loss with _asymmetric softmax parameterization_, which can be seen as the combination of ordinary softmax function and an additional probability estimator. This surrogate can also induce a non-trivial bounded probability estimator for prediction probabilities and expert accuracy. To defend the use of softmax parameterization, we show that it is not the cause of the failure but rather it is the symmetric structure of the surrogates that leads to the unboundedness in probability estimation. Unlike the unbounded estimator  that mixes up class probabilities and the expert's accuracy and the OvA estimator that treats each class and expert independently, our method models the class probabilities as a whole with softmax and the expert's accuracy independently with a normalized function, which better reflects the structure of L2D with probability estimation. The differences between our work and previous works are illustrated in Figure 1. We further analyze the limitation and broader impact of our work in Appendix G. Our contributions are four-fold and summarized below:

* In Section 3, we prove that a non-trivial bounded probability estimator does not exist as long as we are using _symmetric_ loss functions.
* We propose an asymmetric formulation of softmax parameterization, and show that it can induce both a consistent surrogate loss and a non-trivial bounded probability estimator. We further show that the proposed asymmetric softmax-based method and OvA-based method  actually benefit from their asymmetric structure.

Figure 1: Illustration of the proposed and previous estimators. Probability estimation in L2D aims to predict both the class probabilities and the expertâ€™s accuracy \((M=Y|X=)^{K}\), while only our proposed estimator takes exactly the same range.

* We further study the regret transfer bounds of our proposed asymmetric surrogate and show that it is compatible with both L2D and multi-class classification.
* Experiments on datasets with both synthetic experts and real-world experts are conducted to demonstrate the usefulness of our method for both prediction and expert accuracy estimation.

## 2 Preliminaries

In this section, we review the problem setting of L2D and briefly introduce previous works.

### Problem Setting

In this paper, we study the problem of learning to defer to an expert in the \(K\)-class classification scenario. Let us denote by \(\) and \(=\{1,,K\}\) the feature and label space respectively. Denote by \(X\!\!Y\!\!M\!\!\) the data-label-expert random variable triplet and \(,y,m\) are their realizations, which obeys an underlying distribution with density \(p(,y,m)\). We have access to data triplets \(\{(_{i},y_{i},m_{i})\}_{i=1}^{n}\) that are drawn independently and identically from the distribution. The goal of L2D in the classification scenario is to obtain a classifier with deferral option \(f():^{}\), where \(\) is the option of deferral to the expert and \(^{}\) is the augmented decision space \(\{\}\).

The problem evaluation is the following 0-1-deferral loss \(_{01}^{}\)2, which is a generalized version of the zero-one loss \(_{01}(f(),y)=[\![f() y]\!]\), which takes the value of 1 if we use the output of the classifier when it predicts an incorrect label \(([f()\ \ f() y]\!])\) or defer to the expert when they are incorrect (\([\![f()=\ \ m y]\!]\)), where \([\![]\!]\) is the Iverson bracket notation suggested by Knuth . Then we aim to minimize the risk _w.r.t._ to this loss:

\[_{f}R_{01}^{}(f)=_{p(,y,m)}[_{01}^{}(f( ),y,m)].\] (1)

Denote by \(f^{*}\) the minimizer of the risk \(R_{01}^{}(f)\), i.e., the Bayes optimal solution, and \(()=\{(Y=y|X=)\}_{y=1}^{K}\). Mozannar and Sontag  provide a characterization of the form of the Bayes optimal solution as:

**Definition 1**.: (Bayes optimality of L2D) A classifier with deferral option \(f^{*}^{}\) is the minimizer of \(R_{01}^{c}(f)\) if and only if it meets the following condition almost surely:

\[f^{*}()=\{ ,& _{y}_{y}()<(M=Y|X=),\\ _{y}_{y}(),&..\]

The form of the optimal solution is intuitive: we should defer to the expert if they have a higher probability of being correct than the optimal classifier which follows the most likely label class given the input \(\). This optimal solution can also be seen as the generalized version of Chow's rule in learning to reject , where a fixed cost in \(\) serves as the known accuracy of an expert.

### Consistent Surrogate Losses for L2D

Although we know the form of the Bayes optimal solution, practically the risk minimization problem above faces computational difficulties: the minimization of the discontinuous and non-convex problem (1) is NP-hard . A widely used strategy for tackling this difficulty is to substitute the original loss function which is discontinuous with a continuous surrogate, which has been applied in many areas including ordinary classification [52; 6; 43; 39; 16; 37], multi-label classification [17; 23; 51; 47], AUC optimization [17; 23; 29], cost-sensitive learning [41; 11], top-\(K\) classification [25; 48], adversarially robust classification [4; 2; 1], and learning to reject [12; 13; 5; 49; 34; 8; 7]. Denote by \(:^{K+1}\) the learnable scoring function that induces our decision function for L2D \(f:^{}\) with the following transformation \(:^{K+1}^{}\):

\[(())=\{ ,& g_{K+1}()>_{y }g_{y}()\\ _{y}g_{y}(),& ..\]We consider a continuous surrogate function \(:^{K+1}^{+}\) and the surrogate risk below:

\[_{}R_{}^{}()=_{p(,y,m)}[( {g}(),y,m)].\] (2)

Assuming we find a surrogate function that overcomes the computational optimization challenges, we need to verify the consistency of \(\)_w.r.t._\(_{01}^{}\), i.e., any minimizer of the surrogate risk also minimizes the original risk: (2):

\[^{*}*{argmin}_{f}R_{01}^{}(f),\;\;\; ^{*}*{argmin}_{}R_{}^{}().\]

The first consistent surrogate _w.r.t._\(_{01}^{}\) is proposed by modifying the softmax cross-entropy loss:

\[L_{}((),y,m)=-_{y}^{}(() )- m=y_{K+1}^{}(()),\] (3)

where \(^{}\) is the softmax function \(_{y}^{}()=(u_{y})/_{y^{}=1}^{d}(u_{y^ {}})\), where \(d\) is the dimensionality of the input. Inspired by the risk formulation above, Charusaie et al.  further generalized the family of consistent surrogate for \(_{01}^{}\) by considering the following consistent surrogate reformulation:

\[L_{}((),y,m)=((),y)+ m=y ((),K+1).\] (4)

It is known from Proposition 2 in Charusaie et al.  that surrogates take the form above are consistent _w.r.t._\(_{01}^{}\) if \(\) is a classification-calibrated multi-class loss [43; 6]. Using this result, we can directly make use of any statistically valid surrogate loss in ordinary classification with a simple modification.

### Problems with Probability Estimation for L2D

While prior literature has established how to construct consistent surrogates for L2D, it is less well-known whether such surrogates can actually provide calibrated estimates of classifier and deferral probabilities. As mentioned before, we are also interested in the true probability of the correctness of our prediction, i.e., the value of \(()\) (label being \(Y\)) and \((M=Y|X=)\) (expert is correct). To achieve this goal of probability estimation, the commonly used method is to combine the obtained scoring function \(^{*}\) with a transformation function \(\) to make the composite function \(^{*}\) a probability estimator \(}^{}(())\). For example, the softmax function is frequently used as \(\) in ordinary multi-class classification to map the scoring function from \(^{K}\) to \(^{K}\). So far, we have not discussed whether these probability estimates are valid.

In the task of L2D for classification, we aim to estimate both \(()^{K}\) and \((M=Y|X=)\) with a \(K+1\)-dimensional estimator, where its \(y\)-th dimension is the estimate of \(_{y}()\) for \(y\) and \(K+1\)-th dimension is the estimate of \((M=Y|X=)\). Correspondingly, the transformation \(\) should be from \(^{K+1}\) to a range \(\) that contains \(^{K}\). It was shown in Theorem 1 of Mozannar and Sontag  that a probability estimator \(}^{}\) with the softmax output of score function \(\) and an extra fractional transformation guarantees to recover class probabilities and expert accuracy at \(^{*}*{argmin}_{}R_{L_{CE}}()\), which is formulated as:

\[_{y}^{}(())=_{y}^{}(( ))/(1-_{K+1}^{}(())), y \{K+1\},\] (5)

It is noticeable that the range of this estimator is \(}^{}^{K}[0,+]\): the estimate of expert accuracy \(_{K+1}^{}\) is **unbounded above** and will approach \(+\) if \(_{K+1}^{}() 1\). Verma and Nalisnick  pointed out that the unboundedness can hurt the performance of this probability estimator: if \(_{K+1}^{}(())>1/2\), the estimated expert accuracy will be greater than 1 and thus meaningless. Experimental results also show the frequent occurrence of such meaningless results due to the overconfidence of deep models . We further illustrate such miscalibration in Figure 2.

To mitigate this problem, a new OvA-based surrogate loss that can induce a bounded probability estimator while remaining consistent is proposed in , which has the following formulation:

\[L_{}((),y,m)\!=\!(g_{y}())\!+\!_ {y^{} y}^{K+1}\!(-g_{y^{}}())\!+\! m=y ((g_{K\!+\!1}())\!-\!(-g_{K\!+\!1}())),\] (6)

where \(\) is a binary proper composite [46; 40] loss. Its induced probability estimator \(}^{}\) is:

\[_{y}^{}(())=_{}(g_{y}()),  y\{K+1\},\] (7)

where \(_{}\) is a mapping to \(\) determined by the binary loss, which makes \(}^{}\) bounded. It is also experimentally shown that the \(}^{}\) outperformed \(}^{}\) in the task of probability estimation.

Given the success of the OvA-based loss, can we assert that softmax parameterization is inferior to the OvA strategy in probability estimation for L2D? We argue in this paper that such a conclusion should not be drawn so quickly. We should not discourage the use of softmax parameterization based solely on the specific implementation (3), since there may be other implementations of softmax that can resolve the issues with \(}^{ sm}\). Furthermore, the great success of softmax parameterization in deep learning models suggests that this potential implementation could also achieve outstanding performance. In this paper, we focus on finding such an implementation and show its superiority both theoretically and experimentally.

## 3 Problem with Symmetric Losses for L2D with Probability Estimation

Before beginning the search for a suitable implementation of softmax parameterization for L2D, it is important to determine the cause of the unboundedness of \(}^{ sm}\). Is it due to the use of softmax parameterization or some other factor? If the reason is the former one, any attempts to create a bounded probability estimator with a softmax function for L2D will be in vain. Therefore, it is crucial to identify the root cause of the unboundedness before proceeding with further efforts.

Surprisingly, we find that such unboundedness is not a particular problem of softmax parameterization and is a common one shared by many loss functions. Recall that the loss (3) with unbounded estimator \(}^{ sm}\) is a special case of the consistent surrogate (4) by setting the multi-class loss \(\) to softmax cross-entropy loss. We show that even if we use other losses beyond the softmax function and choose other losses such as the standard OvA losses ((14) in Zhang ), the induced probability estimators are still inevitably unbounded (or bounded but induced from an unbounded one) as long as we are using \(\) with symmetric structure:

**Theorem 1**.: (Impossibility of non-trivial bounded probability estimator with symmetric losses)

If a surrogate loss \(L_{}\) defined as (4) has probability estimators and is induced from a symmetric consistent multi-class loss \(\) such that that \(P()=(P)\), where \(P\) is any permutation matrix, it must have an unbounded probability estimator \(}\): let \(^{*}*{argmin}R_{L_{}}()\), we have that \(}()\) where \(}(^{*}())=[();(M=Y|X=)]\), \(}\) is not bounded above. Furthermore, any bounded probability estimator \(}^{}\) for \(L_{}\) must be a piecewise modification of the unbounded estimator \(}\): denote by \(=_{}*{argmin}_{}_{y=1}^{K+1} _{y}L_{}(,y)\) for all \(^{K}\), we have that \(}^{}\) is equal to \(}\) on \(\).

The proof is provided in Appendix A. Intuitively, the existence of such an unbounded estimator is caused by the fact that symmetric loss induced \(L_{}\) neglects the asymmetric structure of the probabilities to be estimated by modeling \(();(M=Y|X=)|}{1+(M=Y|X=)}\) directly. Furthermore, all potential bounded probability estimators are piecewise modifications of the unbounded estimator sharing the same values on \(\) with \(}\), which means that they are generated by substituting the invalid values of the unbounded estimator with a valid one, e.g., we can obtain a bounded estimator based on the unbounded one (5) by clipping it to 1 if its estimated expert accuracy is larger than 1.

Figure 2: Illustration of the miscalibration of estimator (5) on a binary classification with deferral problem. The unbounded estimator (5) first estimates \(();(M=Y|X=)]}{1+(M=Y|X=)}\) that takes value in the 2-D probability simplex denoted by the left triangle and then obtain the final estimate with the fractional transformation (5). However, when over-confidence occurs in the 2-D simplex, i.e., the softmax output lies in the invalid region that \(_{3}^{ sm}>1/2\), (5) will magnify the calibration error of the estimates and clipping it to a valid estimate cannot solve this problem.

However, such modifications to make an unbounded estimator bounded are not very useful. Due to the training process's complexity and the distribution's arbitrariness, it is hard to design an appropriate modification of an unbounded estimator. For example, though we can bound the expert accuracy of (5) by 1 when it generates an invalid value that is larger than 1, it is still an overly confident estimation. Meanwhile, we do not know how to upper bound it by a value lower than 1 without prior knowledge of \((M=Y|X=)\). Overall, training with \(L_{}\) in Theorem 1 can easily lead to a model that generates invalid probability estimation with an unbounded estimator \(}\), while most modified bounded estimators cannot efficiently solve this problem. We will experimentally verify this point in Section 5.

This result shows that the consistent surrogate framework  does not directly provide us a solution for solving the unboundedness problem since most multi-class losses we use, e.g., CE loss, OvA loss, Focal loss , are symmetric. This motivates the design of surrogates with bounded probability estimators for L2D in our work. Based on Theorem 1, we have the following findings: firstly unboundedness of probability estimators is not necessarily caused by the softmax parameterization, as the induced estimator of any symmetric loss, e.g., standard softmax-free symmetric OvA losses , exhibits unboundedness too. Secondly, due to the fact that the OvA strategy for L2D (6) successfully gets rid of the unboundedness issue, we can reasonably deduce that a modified softmax function can also present similar outcomes. In the following section, we propose a novel modification of the softmax parameterization that overcomes these issues and reveals that both the previous OvA-based work  and our current work benefit from the use of asymmetric losses.

## 4 Consistent Softmax-Based Surrogate with Bounded Probability Estimator

In this section, we find a softmax parameterization that is a feasible solution in L2D for the construction of surrogate loss with a bounded probability estimator. We first propose a cross-entropy-like surrogate loss but with an asymmetric transformation that resembles the ordinary softmax function for its first \(K\) elements but processes the \(K+1\)-th coordinate in a special way. Then we show that such a transformation has useful properties for both classification and probability estimation, and then provide consistency analysis for both the loss function and its induced bounded probability estimator. Finally, we show that our proposed loss and the OvA loss  are connected to the consistent surrogate formulation (4) with the use of asymmetric multi-class losses, which indicates the helpfulness of asymmetry and shed light on the future study of consistent surrogates for L2D with bounded estimators.

### Consistent Surrogate Formulation with Asymmetric Bounded Softmax Parameterization

Before searching for a softmax-parameterization that is capable of L2D with probability estimation, we need to better understand the failure of (3) for inducing a bounded probability estimator. By inspecting the proof of consistency in Theorem 1 of Mozumar and Sontag , we can learn that the cross-entropy-like surrogate (3) works by fitting a \(K+1\) class posterior distribution: \(^{}(^{})=}()=[()}{1+(M=Y|)},,()}{1+(M=Y|)},)}{1+(M=Y|)}]\). Though we can check that the \(^{}\) is exactly a Bayes optimal solution of \(R_{01}^{}(f)\) due to the monotonicity of the softmax function and the form of \(}\), to get \([();(M=Y|X=)]\) we need to perform an extra inverse transformation should be exerted on \(^{}\). This is caused by the following dichotomy: the class probabilities and expert accuracy we aim to estimate are in the range of \(^{K}\), while the standard softmax-parameterization of (3) maps the \(K+1\) dimensional scoring function \(\) into \(^{K+1}\).

To solve this issue, a promising approach is to modify the softmax function to make it a transformation that can directly cast the scoring function \(\) into the target range \(^{K}\). Since the target range is not symmetric, the modified softmax should also be asymmetric. This idea is given a concrete form in the following asymmetric softmax parameterization:

**Definition 2**.: (Asymmetric softmax parameterization) \(}\) is called a _asymmetric softmax function_ that for any \(K>1\) and \(^{K+1}\):

\[_{y}()=\{)}{_{ y^{}=1}^{K}(u_{y^{}})},&y K+1,\\ )}{_{y^{}=1}^{K+1}(u_{y^{}})-_{y^{ }\{1,,K\}}(u_{y^{}})},&..\] (8)At first glance, the proposed asymmetric function appears to be the standard softmax function, which excludes the \(K+1\)-th input of \(\) for \(y K+1\). However, the last coordinate of the function takes on a quite different form. This special asymmetric structure is designed to satisfy the following properties:

**Proposition 1**.: (Properties of \(\)) For any \(^{K+1}\):

(i). (Boundedness) \(()^{K}\),

(ii). (Maxima-preserving) \(*{argmax}_{y\{1,,K+1\}}_{y}()= *{argmax}_{y\{1,,K+1\}}u_{y}\).

It can be seen that the proposed asymmetric softmax \(\) is not only bounded but also successfully maps \(\) into the desired target range, which indicates that \(\) may directly serve as the probability estimator \(}^{}(())=(())\). Furthermore, the maxima-preserving property guarantees that the estimator is also capable of discriminative prediction: if a scoring function \(^{}\) can recover the true probability, i.e., \(}^{}(^{}())=[(); *{Pr}(M=Y|X=)]\), then \(^{}\) must also be the Bayes optimal solution of \(R^{}_{01}()\). Based on the asymmetric softmax function, we propose the following surrogate loss for L2D:

**Definition 3**.: (Asymmetric Softmax-Parameterized Loss) The proposed surrogate for L2D with asymmetric softmax parameterization \(L_{}(,y,m):^{K+1}\) is formulated as:

\[L_{}(,y,m)\!=\!-(_{y}())-  m y(1-_{K+1}())- m =y(_{K+1}()).\] (9)

The proposed loss takes an intuitive form, which can be seen as the combination of the cross-entropy loss (the first term) and a modified version of binary logistic loss (the last two terms). This formulation is inspired by the structure of our problem, where the expert accuracy is not directly related to class probabilities while the class probabilities should fulfill that \(()^{K}\). In fact, the proposed surrogate is not a simple summation of two independent losses and they are indirectly related by the asymmetric softmax function: according to Definition 2, the two counterparts share the same elements \([g_{1},,g_{K}]\). This correlation serves as a normalization for \(_{K+1}\) that brings the property of maxima-preserving, which is crucial for the following consistency analysis:

**Theorem 2**.: (Consistency of \(L_{}\) and bounded probability estimator \(}^{}\))

The proposed surrogate \(L_{}\) is a consistent surrogate for L2D, i.e., \(^{*}*{argmin}_{f}R^{}_{01}(f),\ \ ^{*} *{argmin}_{g}R^{}_{}()\). The bounded probability estimator can also recover the desired class probabilities and expert accuracy: \(}^{}(^{*}())=[(); *{Pr}(M=Y|X=)],\ \). Furthermore, if there exists other probability estimators for \(L_{}\), they must also be bounded.

The proof can be found in Appendix C. According to the theorem above, we showed that our proposed asymmetric softmax function induces a consistent surrogate and a bounded probability estimator. We also showed that there does not exist an unbounded probability estimator for \(L_{}\), which guarantees that our proposed bounded estimator is never the modification of an unbounded one. This result enriches the toolbox for L2D with probability estimation and theoretically justifies the use of softmax parameterization in L2D. In fact, we can further substitute the cross-entropy-like counterpart with any strictly proper loss  to get the same consistency result. In the following subsection, we will discuss the relationship between our method and the general consistent surrogate framework .

### Connection with Consistent Surrogate Framework : Asymmetry Can Help

In the previous section, we showed that there exists an implementation of softmax \(\) that can induce both a consistent loss and a valid probability estimator for L2D. Given the theoretical successes of our proposed softmax-based method and the previous OvA strategy, we may further expect them to provide more insights into the design of surrogates and probability estimators for L2D. Recalling the consistent surrogate formulation (4) proposed in Charusaie et al.  that allows the use of all consistent multi-class losses for constructing L2D surrogates, it is an instinctive idea that our proposed consistent surrogates (9) and the previous work  can be included in this framework. However, this idea may not be easily confirmed: Theorem 1 implies that the two surrogates are not the trivial combinations of commonly used symmetric losses and formulation (4) since they can both induce bounded probability estimators. The following corollary shows that the two surrogates are included in the family of consistent surrogate formulation (4), but induced from two novel asymmetric multi-class surrogates.

**Corollary 1**.: We can get the consistent surrogates \(L_{}\) (9) and \(L_{}\) (6) by setting \(\) in the consistent loss formulation (4) to specific **consistent and asymmetric** multi-class losses.

Due to page limitations, we present the formulation of the asymmetric multi-class losses and the proof in the Appendix D. Although this conclusion reveals the connection between the two consistent surrogates and the general framework (4), it is important to note that it does not necessarily imply that the results in Verma and Nalisnick  and this paper are trivial. Since asymmetric losses are seldom used in ordinary multi-class classification, it is hard to obtain \(L_{}\) and \(L_{}\) by simply selecting \(\) from known consistent multi-class losses. According to this corollary and theorem 1, we can determine that a multi-class loss \(\) with asymmetric structure is a necessity if we aim to design consistent L2D surrogates with bounded probability estimators based on the general formulation (4). Based on this insight, it is promising to discover more surrogates with bounded probability estimators for L2D by focusing on the design of asymmetric multi-class surrogates.

### Regret Transfer Bounds

In this section, we further study the regret transfer bounds of our proposed surrogate (9) to characterize the effect of minimizing surrogate risk \(R_{L_{}}()\) on both the target risk \(R_{01}^{}()\) and the misclassification error of the classifier. Denoted by \(_{}=_{1:K}\) the classifier counterpart of our model and \(R_{01}()\) is its misclassification error, we can show that:

**Theorem 3**.: \(R_{01}(_{})-R_{01}^{*},\ R_{01}^{}( )-R_{01}^{*}}}()-R_{L_{ {}}}^{*})}\)_._

The proof can be found in Appendix E. This theorem shows that we can use the excess risk of our proposed surrogate to upper-bound those of our concerned targets, and its proof is conducted by applying Pinsker's inequality and zooming.

This theorem shows that as long as we reach a low risk _w.r.t._ the proposed surrogate (approximately optimal), we can obtain a model with satisfying performances for both L2D and classification. This conclusion is not trivial: according to the characterization of \(R_{01}^{}\)'s Bayes optimality in Definition 1, the classifier counterpart is not guaranteed to classify accurately on the deferred samples. We will further experimentally demonstrate the efficacy of our surrogate for L2D and classification in the next section.

## 5 Experiments

In this section, we compare our proposed asymmetric softmax estimator and its induced surrogate loss with the estimators and losses in previous works. Detailed setup can be found in Appendix F.

**Datasets and Models.** We evaluate the proposed method and baselines on widely used benchmarks with both synthetic and real-world experts. For synthetic experts, we conduct experiments on the CIFAR100 . Following the previous works, the expert has a chance of \(p\) to generate correct labels on the first \(k\{20,40,60\}\) classes and at random otherwise. We set \(p=94\%\) as in Mozannar and Sontag  and \(75\%\) as in Verma and Nalisnick  to simulate experts of high and medium accuracy. For real-world experts, we use CIFAR10H, HateSpeech, and ImageNet-16H [36; 14; 21]

Figure 3: Distributions of the true and estimated accuracy of baselines and proposed method on CIFAR10H.

datasets, where the expert's prediction is generated using the provided auxiliary expert information. More experimental results of real-world experts can be found in Appendix F.

For CIFAR-100, HateSpeech, and ImageNet-16H, we report the misclassification error, coverage, and the ECE of the expert accuracy estimates. We also report the error of our model with the defernt budget as in Figure 4 of Verma and Nalisinck  to further evaluate the performance of the obtained classifier. For CIFAR10H, we plot the distribution of the true and estimated expert accuracy in Figure 3 to illustrate the performance of baselines and our method on fitting \((M=Y|X=)\). The experiments on CIFAR-100 is conducted with 28-layer WideResNet  and SGD as in previous works [30; 44], and those on datasets with real-world experts are conducted using pre-trained models/embedding.

**Baselines.** We compare our **A**symmetric Soft**Max** based method (A-SM) with the previously proposed **S**ymmetric **S**oft**Max**-based (S-SM) method  and **A**symmetric OvA (A-OvA) based method . We also combined the **S**ymmetric OvA logistic loss (S-OvA) with (4) to further evaluate the effect of symmetric loss and its induced unbounded estimator. For unbounded probability estimators in S-SM and S-OvA, we clip their estimates into \(\) to make them valid. We directly use the output of baselines and our method without post-hoc techniques [19; 33] to better reflect their own performance.

**Experimental Results.** By observing Figure 3, we can find that the distributions of bounded method A-OvA and our proposed A-SM have markedly more overlap with the true distribution compared with the symmetric and unbounded ones, which directly shows that the bounded estimators can better estimate of expert accuracy. As can be seen from the experimental results reported in Table 1 and 2, our proposed A-SM is always better than or comparable to all the baselines _w.r.t._ classification accuracy. We can also see that the coverage of our method is always significantly higher than the baselines, which shows that our surrogate can induce ideal models for L2D. Though S-SM is comparable to A-SM with a high-accuracy expert, it has a significantly lower coverage and is

    &  &  &  &  \\   & & & & & 10\% & 20\% & 30\% \\   \\   & 20 & **24.58(0.13)** & 77.72(0.31 & 4.86(0.11) & 31.68(0.39) & 25.00(0.24) & 24.59(0.13) \\  & 40 & **21.92(0.32)** & 57.89(0.54) & 9.22(0.38) & 46.69(0.63) & 38.50(0.60) & 29.94(0.62) \\  & 60 & **18.69(0.88)** & 40.34(6.28) & 11.10(0.72) & 59.44(4.95) & 51.36(5.08) & 42.77(5.09) \\   & 20 & 27.00(1.91) & 85.40(0.91) & 5.15(0.31) & 28.42(2.26) & 27.00(1.91) & 27.00(1.91) \\  & 40 & 27.33(1.50) & 68.47(1.59) & 9.38(0.46) & 39.95(2.35) & 32.93(2.30) & 27.92(2.06) \\  & 60 & **18.44(0.64)** & 58.18(1.55) & 7.70(0.24) & 42.52(1.74) & 33.67(1.72) & 25.74(1.75) \\   & 20 & 25.63(0.97) & 90.40(0.89) & **4.35(0.29)** & 25.65(0.99) & 25.63(0.97) & 25.63(0.97) \\  & 40 & 23.23(0.42) & 80.46(0.51) & **6.81(0.33)** & 27.44(0.62) & 23.23(0.42) & 23.23(0.42) \\  & 60 & **19.64(1.10)** & 70.44(2.73) & 7.34(0.38) & 31.90(2.73) & 24.75(2.73) & 20.05(1.62) \\   & 20 & **24.54(0.06)** & **98.16(0.03)** & **4.63(0.11)** & **24.54(0.06)** & **24.54(0.06)** & **24.54(0.06)** \\  & 40 & **22.17(0.36)** & **92.20(0.32)** & **6.58(0.22)** & **22.17(0.36)** & **22.17(0.36)** & **22.17(0.36)** \\  & 60 & **19.30(0.58)** & **84.72(0.30)** & **5.96(0.44)** & **22.91(0.42)** & **19.30(0.58)** & **19.30(0.58)** \\   \\   & 20 & 25.49(0.20) & 86.46(0.52) & 5.07(0.21) & 26.09(0.23) & 25.48(0.20) & 25.49(0.20) \\  & 40 & 25.13(0.44) & 74.65(0.48) & 12.82(0.29) & 32.82(0.66) & 26.99(0.59) & 25.13(0.44) \\  & 60 & 24.05(0.43) & 60.25(1.21) & 18.70(0.55) & 42.30(0.98) & 36.18(0.99) & 29.47(1.10) \\   & 20 & 26.53(0.86) & 90.06(2.92) & 5.57(0.54) & 26.83(1.20) & 26.53(0.86) & 26.53(0.86) \\  & 40 & 26.89(1.78) & 77.69(5.82) & 7.14(1.41) & 32.30(4.92) & 28.33(3.11) & 26.90(1.78) \\  & 60 & 25.29(0.69) & 69.08(1.35) & 7.47(0.12) & 36.67(1.45) & 30.44(1.33) & 25.70(1.15) \\   & 20 & 25.94(0.52) & 93.05(0.43) & **4.78(0.29)** & 25.

outperformed by A-SM with an expert of lower accuracy, which indicates that it is suffering from the problem of deferring more samples than necessary. This problem is also observed in learning with rejection . Though S-OvA can mitigate this problem, its coverage is still consistently lower than A-SM. Notice that when there exist deferral budget requirements, all the unbounded baselines are outperformed by A-OvA, and A-OvA is further outperformed by A-SM, which indicates that our method can also efficiently generate a classifier. Meanwhile, the ECE of our estimated expert accuracy is also comparable to or better than A-OvA, while those of the unbounded ones all have significantly higher ECE, which further shows the efficacy of our method in estimating expert accuracy.

## 6 Conclusion

In this paper, we provide a novel consistent surrogate loss based on an asymmetric softmax function for learning to defer that can also provide calibrated probability estimates for the classifier and for expert correctness. We reveal that the root cause of the previous unbounded and miscalibrated probability estimators for L2D is not softmax but the intrinsic symmetry of the used loss function. We solve this problem by designing an asymmetric softmax function and using it to induce a consistent surrogate loss and a bounded probability estimator. We further give the regret transfer bounds of our method for both L2D and classification tasks. Finally, we evaluate our method and the baseline surrogate losses and probability estimators on benchmark datasets with both synthetic and real-world experts and show that we outperform prior methods. While we provide a consistent multi-expert extension of our proposed surrogate in Appendix H, it is still a promising future direction to generalize the multi-expert surrogates  to all the consistent multiclass losses as in Charusaie et al. .