# Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set

Wenyuan Zhang\({}^{1}\), Yu-Shen Liu\({}^{1}\), Zhizhong Han\({}^{2}\)

School of Software, Tsinghua University, Beijing, China

zhangwen21@mails.tsinghua.edu.cn, liuyushen@tsinghua.edu.cn

Department of Computer Science, Wayne State University, Detroit, USA

h312h@wayne.edu

The corresponding author is Yu-Shen Liu.

###### Abstract

It is vital to infer a signed distance function (SDF) in multi-view based surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel perspective for volume rendering, and shows advantages in rendering efficiency and quality. Although 3DGS provides a promising neural rendering option, it is still hard to infer SDFs for surface reconstruction with 3DGS due to the discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To resolve these issues, we propose a method that seamlessly merge 3DGS with the learning of neural SDFs. Our key idea is to more effectively constrain the SDF inference with the multi-view consistency. To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF using neural pulling, and then render the aligned 3D Gaussians through the differentiable rasterization. Meanwhile, we update the neural SDF by pulling neighboring space to the pulled 3D Gaussians, which progressively refine the signed distance field near the surface. With both differentiable pulling and splatting, we jointly optimize 3D Gaussians and the neural SDF with both RGB and geometry constraints, which recovers more accurate, smooth, and complete surfaces with more geometry details. Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks. Project page: https://wen-yuan-zhang.github.io/GS-Pull.

## 1 Introduction

3D scene representations are important to various computer vision applications, such as single or multi-view 3D reconstruction , novel view synthesis , and neural SLAM  etc.. Mesh and point clouds are the most common 3D scene representations, and can be rendered by fast rasterization on GPUs. Instead, more recent neural radiance fields (NeRFs)  are continuous scene representations, but it is slow to render NeRFs due to the need of costly stochastic sampling along rays in volume rendering. More recently, 3D Gaussians with different attributes like color and opacity are used as a versatile differentiable volumetric representation  for neural rendering through splatting, dubbed 3D Gaussian Splatting (3DGS). It prompts the pros of both NeRFs and point based representations, which achieves both better quality and faster speed in rendering. Although 3D Gaussians can render plausible images, it is still a challenge to reconstruct surfaces based on the 3D Gaussians.

The key challenge comes from the gap between the discrete 3D Gaussians and the continuous geometry representations, such as implicit functions. Besides the discreteness, the sparseness caused by uneven distribution and the off-surface drift make 3D Gaussians even harder to use thanscanned point clouds in surface inference. To overcome these obstacles, recent solutions usually add previous volume rendering based reconstruction methods [60; 50] to 3DGS as a complement branch [44; 66; 11], use monocular depth and normal images as priors to bypass the messy and unordered 3D Gaussians [14; 57], or use surface-aligned Gaussians [25; 68; 14] in rasterization to approximate surfaces. However, how to learn continuous implicit representations to recover more accurate, smooth, and complete surfaces with sharp geometry details is still an open question.

To answer this question, we introduce a novel method to infer neural SDFs from multi-view RGB images through 3D Gaussian splatting. We progressively infer a signed distance field by training a neural network along with learning 3D Gaussians to minimize rendering errors through splatting. To more effectively constrain the surface inference with the multi-view consistency, we dynamically align 3D Gaussians with the zero-level set of the neural SDF, and render the aligned 3D Gaussians on the zero-level set by differentiable rasterization. Meanwhile, we update the neural SDF by pulling the neighboring space onto the disk determined by each 3D Gaussian on the zero-level set, which gradually refines the signed distance field near the surface. The capability of seamlessly merging neural SDFs with 3DGS not only get rid of the dependence of costly NeRFs like stochastic sampling on rays but also enables us to access the field attributes like signed distances and gradients during the splatting process, which provides a novel perspective and a versatile platform for surface reconstruction with 3DGS. The key to the 3D Gaussian alignment and neural SDF inference is a differentiable pulling operation which uses the predicted signed distances and gradients from the neural SDF. It provides a way of imposing geometry based constraints on 3D Gaussians besides the RGB based constraints through splatting. Our numerical and visual evaluations on widely used benchmarks show our superiority over the latest methods in terms of reconstruction accuracy and recovered geometry details. Our contributions are listed below,

* We propose to infer neural SDF through splatting 3D Gaussians pulled on the zero-level set, which can more effectively constrain surface inference with the multi-view consistency. This enables to recover more accurate, smooth, and complete surfaces with geometry details.
* We introduce to dynamically align 3D Gaussians to the zero-level set and update the neural SDF through a differentiable pulling operation. To this end, we propose novel loss terms and training strategies to work with the discrete and sparse 3D Gaussians in surface reconstruction.
* We achieve the state-of-the-art numerical and visual results in multi-view based surface reconstruction.

## 2 Related Work

Neural implicit representations have achieved remarkable progress in reconstructing 3D geometry with details [53; 48; 13; 45; 7; 21]. Neural implicit functions can be learned by either 3D supervisions, such as signed distances [53; 13; 41] and binary occupancy labels , or 2D supervisions, such as multi-view RGB images  and normal images . In the following, we focus on reviewing methods of learning implicit representations from 2D and 3D supervisions separately. Then we provide a detailed discussion on the latest reconstruction methods based on 3D Gaussians.

### Learning Implicit Representation from Multi-view Images

Neural radiance fields (NeRFs)  have become an essential technology for representing 3D scene through multi-view images. Many of its applications have been explored, resulting in significant advancements in areas such as acceleration [50; 12], dynamic scene [16; 4] and sparse rendering [58; 26]. Besides these applications, extracting accurate surfaces from NeRFs remains a challenge. Mainstream approaches typically design various differentiable formulas to transform the density in radiance fields into implicit representations for volume rendering, such as signed distance function (SDF) [60; 39; 54], unsigned distance function (UDF) [42; 47; 15; 70] and occupancy . With the learned implicit function fields, post-processing algorithms [43; 21; 48] are applied to extract the zero level set to obtain the reconstructed meshes. Following methods introduce different priors from SfM [18; 69] or large-scale datasets [67; 59; 40] to improve the reconstruction performance in large-scale scenes. Recent approaches focus on speeding up the neural rendering procedure, aiming to achieve high-quality meshes and rendering views within a short period of training time. Theypropose alternative data structures to replace the heavy MLP framework used in original NeRF, such as sparse voxel grid , multi-resolution hash grid [50; 61] and radial basis function , or design subtle differentiable rasterization pipelines to achieve real-time rendering [64; 56]. However, these methods still face the trade-off between rendering quality and training speed.

### Learning Implicit Representation from Point Clouds

Since DeepSDF  and OccNet  were proposed, learning implicit representation from point clouds has achieved remarkable results in geometry modeling. These methods use ground truth signed distances and binary occupancy labels calculated from ground truth point clouds as supervisions to learn the implicit representation of shapes. The supervisions can serve as different kinds of global priors [3; 42; 55; 36] and local priors [62; 28; 38; 6], which enables the neural implicit function to better capture geometry details and generalize to unseen shapes during inference. Some other methods infer SDFs without 3D supervisions. They train neural networks to overfit on single point clouds. These methods introduce additional constraints [19; 76], novel ways of using gradients [45; 74; 51; 35; 75], specially designed priors [46; 10; 9] and normals [2; 37; 34] to estimate signed or unsigned distances and occupancy, which use point clouds as a reference.

### Surface Reconstruction with 3D Gaussians

3D Gaussian Splatting  has become a new paradigm in neural rendering due to its fast rendering speed, intuitive explicit representation and outstanding rendering performance. However, reconstructing accurate surfaces from 3D Gaussian remains a challenge due to the messy, noisy, and unevenly distributed 3D Gaussians. To solve this problem, one kind of approaches involves combining 3D Gaussians with neural implicit surface functions [60; 50] to enhance the performance of both branches, which employs mutual supervisions between the two components [66; 11; 44]. Another kind of approaches encourage the reduction from 3D Gaussians to 2D Gaussians with a series of regularization terms, which ensures the Gaussian primitives to align with the object surfaces [25; 20; 14]. Additionally, some methods introduce additional priors from large-scale datasets [57; 14] or multi-view stereo , or use elaborately designed surface extraction algorithms [68; 65] to recover 3D geometry from 3D Gaussians. Although these efforts have achieved improved reconstructions, they are still limited in capturing fine-grained geometry and lack the precise perception of continuous implicit representations. Different from all these mentioned methods, we propose to seamlessly combine 3D Gaussians with the learning of neural SDFs. Our method provides a novel perspective to jointly learn 3D Gaussians and neural SDFs by more effectively using multi-view consistency and imposing geometry constraints.

## 3 Method

**Overview.** We aim to infer a neural SDF \(f\) from posed multi-view RGB images \(\{v_{i}\}_{i=1}^{I}\), as shown in Fig. 1. We learn 3D Gaussian functions \(\{g_{j}\}_{j=1}^{J}\) with their attributes like color, opacity, and shape to represent the geometry and color in the 3D scene. Meanwhile, when learning the 3D Gaussians, we introduce novel constraints to infer the continuous surfaces with the neural SDF. We rely on a differentiable pulling operation and the differentiable rasterization to bridge the gap between the discrete Gaussians and the continuous neural SDF, align 3D Gaussians on the zero-level set of the neural SDF, and back propagate the supervision signals from both the rendering errors and other geometry constraints to jointly optimize 3D Gaussians and the neural SDF.

**Neural Signed Distance Function.** We leverage an SDF \(f\) to represent the geometry of a scene. An SDF \(f\) is an implicit function that can predict a signed distance \(s\) at an arbitrary location \(q\), i.e., \(s=f(q)\). Recent methods usually train a neural network to approximate an SDF from signed distance supervision or infer an SDF from 3D point clouds or multi-view images. A level set is an iso-surface formed by the points with the same signed distance values. The zero-level set is a special level set, which is formed by points with a signed distance of 0. We can use the marching cubes algorithm  to extract the zero-level set a a mesh surface. Another character of the zero-level set is that the gradient of the SDF \(f\) at query \(q\) on the zero-level set, i.e., \( f(q)\), is the normal of \(q\).

**3D Gaussian Splatting.** 3D Gaussians have become a vital differentiable volume representation for scene modeling. We can learn a set of 3D Gaussians \(\{g_{j}\}_{j=1}^{J}\), each of which has a set of learnable attributes including mean, variances, rotation, opacity, and color. We can render the learnable Gaussians \(\{g_{j}\}\) into RGB images through the volume rendering equation below,

\[^{}(u,v)=_{j=1}^{J}_{j}*o_{j}*p_{j}(u,v)_{k=1}^{j-1}(1 -o_{k}*p_{k}(u,v)),\] (1)

where \(^{}(u,v)\) is the rendered color at the pixel \((u,v)\), \(_{i}\), \(o_{i}\), and \(p_{i}\) denote the color, the opacity, and the 2D projection of the \(j\)-th 3D Gaussian, respectively. At a query \(q=[x,y,z]\), the probability from the \(j\)-th 3D Gaussian is \(p_{j}(q)=exp(-0.5*(q-_{j})^{T}^{-1}(q-_{j}))\), where \(_{j}\) is the center of the \(j\)-th Gaussian, and \(\) is the covariance matrix.

We can learn these 3D Gaussian functions through a differentiable rasterization. We render 3D Gaussians \(\{g_{j}\}\) into rendered RGB images \(v^{}_{i}\), and then, optimize the learnable attributes by minimizing the rendering errors to the ground truth observations \(v_{i}\), where \(^{}(u,v)\) and \((u,v)\) are the rendered and the GT color values at pixel \((u,v)\), i.e., \(_{\{g_{j}\}}||^{}(u,v)-(u,v)||_{2}^{2}\).

**Aligning 3D Gaussians with the Zero-level Set.** Since 3D Gaussian splitting is so flexible in volume rendering, it does not require 3D Gaussians to locate on the geometry surface for good rendering quality. While we expect 3D Gaussians to locate on geometry surface, so that we can more effectively leverage them and multi-view consistency as clues to infer more accurate neural SDFs for reconstruction. To this end, we introduce a differentiable pulling operation to pull 3D Gaussians on the zero-level set of the neural SDF \(f\), and then, we render the pulled 3D Gaussians through the splatting.

Specifically, inspired by Neural-Pull , we rely on the gradient field of the neural SDF \(f\) during the pulling operation. We move each one of the 3D Gaussians \(g_{j}\) using the predicted signed distance \(s_{j}=f(_{j})\) and the gradient \( f(_{j})\), where \(_{j}\) is the mean value of the 3D Gaussian. As shown in Fig. 1 (a), this pulling operation will turn the 3D Gaussian \(g_{j}\) into a 3D Gaussian \(g^{}_{j}\) that get projected onto the zero-level set of SDF \(f\), where \(g^{}_{j}\) shares the same attributes with \(g_{j}\) but has a different center \(^{}_{j}\),

\[^{}_{j}=_{j}-s_{j}*)}{| f(_{j})|}.\] (2)

**Signed Distance Inference with Pulled 3D Gaussians.** We infer signed distances in the field with pulled 3D Gaussians \(\{g^{}_{j}\}\). Pulled 3D Gaussians provide a coarse estimation of the surface, which we can use as a reference. One challenge here is that the sparsity and non-uniformly distributed 3D Gaussians do not show a clear geometry clue for surface inference. Although previous methods like NeuralTPS  and OnSurPfrior  manage to learn continuous implicit functions from sparse points, it is still difficult to recover surfaces from both sparse and non-uniformly distributed points.

Figure 1: Overview of our method. We **(a)** pull 3D Gaussians onto the zero-level set for splatting, while **(b)** pulling the neighboring space onto the Gaussian disks for SDF inference. To better facilitate this procedure, we introduce three constraints: **(c)** push the Gaussians to become disks; **(d)** encourage the disk to be a tangent plane on the zero-level set; **(e)** constrain the query points to be pulled along the shortest path.

To overcome this challenge, we introduce an approach to estimate neural SDFs from sparse 3D Gaussians. Like Neural-Pull , we still use a differentiable pulling operation to pull neighboring space onto the surface but we regard the disk established by the shape of a 3D Gaussian as a pulling target, rather than a point, as shown in Fig. 2, which aims for a larger target on surfaces. To this end, we impose constraints not only on the shape of 3D Gaussians but also on the pulling operation. Specifically, we introduce three constraints. The first one constrains 3D Gaussians to be a thin disk. The second constraint encourages the thin disk to be a tangent plane on the zero-level set. The third constraint pushes queries to get pulled onto the thin disk along the normal of the Gaussian.

The first constraint adds penalties if the smallest variance among the three variances of a 3D Gaussian \(g_{j}\) is too large, as shown in Fig. 1 (c). Thus, the loss for a thin disk Gaussian is listed below,

\[L_{Thin}=\|\{r_{1},r_{2},r_{3}\}\|_{1},\] (3)

where \(r_{1}\), \(r_{2}\), and \(r_{3}\) are variances along the three axes. Flattening a 3D Gaussian ellipsoid into a disk was first introduced in NeuSG  and has become a consensus in recent Gaussian reconstruction works . The motivation is that 2D planar disk primitives are more suitable for surface representation, making it easier to apply alignment constraints. Additionally, we can naturally use the direction pointing along the axis with the minimum variance \(=\{r_{1},r_{2},r_{3}\}\) to represent the normal \(n_{j}\) of the Gaussian \(g_{j}\).

Based on the thin disk shape of Gaussians, the second constraint encourages the pulled Gaussians \(\{g^{}_{j}\}\) to be the tangent plane on the zero-level set, as shown in Fig. 1 (d). What we do is to align the normal \(n_{j}\) of a Gaussian \(g_{j}\) with the normal at the center \(^{}_{j}\) of the pulled Gaussian \(g^{}_{j}\) on the zero-level set. We use the gradient \( f(^{}_{j})\) of the neural SDF at \(^{}_{j}\) as the expected normal here. Hence, we align the normal \(n_{j}\) of a Gaussian with the normal \( f(^{}_{j})\) on the zero-level set,

\[L_{Tangent}=1-|_{j})}{| f(^{} _{j})|} n_{j}|\] (4)

With the disk-like Gaussians located on the tangent plane, we introduce to sense the signed distance field by pulling randomly sampled queries on the Gaussian disks, as shown in Fig. 1 (b). Turning the pulling target from a point  into a plane is based on the observation that the 3D Gaussian function with a boundary can cover the surface more completely although their centers \(\{_{1},...,_{j}\}\) which are sparse and non-uniformly distributed. Thus, we expect the operation can pull a query onto a Gaussian disk plane. Fig. 2 demonstrates the improvement of pulling queries onto their nearest Gaussian disk planes over the nearest Gaussian centers. The comparisons show that pulling onto the disk plane can improve the robustness to the sparsity and non-uniformly Gaussian distribution. With learned Gaussian centers, pulling queries to centers can not recover the smooth and continuous geometry in areas where almost no Gaussian centers appear. While pulling queries to the Gaussian disk plane can recover more accurate and complete surfaces since the disk established by the learned variance of Gaussian functions can mostly cover the gap.

Specifically, at a query \(q\), we pull it onto the zero-level set using a similar way in Eq. 2, i.e., \(q^{}=q-s* f(q)/| f(q)|\). To encourage the query to get pulled onto the nearest pulled Gaussian disk, we maximize its probability of belonging to its nearest pulled Gaussian \(\) which is determined in terms of the distance between \(q\) and the Gaussian center \(\),

Figure 2: Comparison of pulling Gaussians to centers and to disks. The former tends to overfit sparse Gaussian centers, resulting in incomplete meshes. We address this issue by pulling queries onto disk planes.

\[L_{Pull}(q^{};)=e^{-1/2*(q^{}-)^{T}^{-1}(q^{ }-)},\;=*{arg\,min}_{\{g^{}_{j}\}}|| ^{}_{j}-q||^{2}_{2}.\] (5)

We minimize the negative logarithm of the probability in our implementation. Moreover, we expect the pulling can follow a direction orthogonal to the disk plane, which leads to the minimum moving distance conform to the definition of signed distances. To this end, we impose another constraint on the gradient to ensure that the pulling can follow a path with the minimum distance to the nearest pulled Gaussian disk, as shown in Fig. 1 (e),

\[L_{Othorgnal}=1-|}|,\] (6)

where the constraint aligns the gradient at query \(q\) and the normal \(}\) of the pulled Gaussian disk \(\).

**Rendering.** We also render the pulled Gaussians \(\{g^{}_{j}\}\) into images through splatting to add penalties on rendering errors, where \(\{g^{}_{j}\}\) are Gaussians pulled onto the zero-level set from the Gaussians \(\{g_{j}\}\) by the neural SDF \(f\) in Eq. 2. Each pair of \(g_{j}\) and \(g^{}_{j}\) shares the same attributes expect the center location. The rendering error combines an \(L_{1}\) term and a D-SSIM term between rendered images \(\{v^{}_{i}\}_{i=1}^{I}\) and ground truth ones \(\{v_{i}\}_{i=1}^{I}\), following original 3DGS ,

\[L_{Splitating}=0.8 L_{1}(v^{}_{i},v_{i})+0.2 L_{D-SSIM}(v^{ }_{i},v_{i}).\] (7)

**Loss Function.** We optimize attributes of Gaussians \(\{g_{j}\}\) and the parameters of neural SDF \(f\) by the following objective function, where \(\), \(\), \(\), and \(\) are balance weights.

\[_{\{g_{j}\},f}L_{Splitating}+ L_{Thin}+ L_{Tangent}+ L_{ Pull}+ L_{Othorgnal}.\] (8)

**Implementation Details.** Our code is build upon the source code released by 3DGS . Similar to , we make some changes to 3DGS's densification strategy. The first one is to initialize the newly cloned Gaussians around the original Gaussians rather than at the same positions. The second one is to encourage 3DGS to split larger Gaussians into smaller ones more frequently. These strategies aim to increase the number of primitives and to avoid underfitting in textureless areas. Regularization parameters are set to \(\)=100, \(\)=0.1, \(\)=1, \(\)=0.1. We optimize our model for a total of 15k iterations. We stop densification and incorporate the pulling and constraints at 7k iterations. The SDF network is implemented as an MLP with 8 layers, 256 hidden units and ReLU activation function, and initialized as a sphere, following . The parameters of the SDF network shares the same optimizer as that of 3D Gaussians. All the experiments are conducted on a single NVIDIA 3090 GPU.

## 4 Experiments

### Experiment Settings

**Evaluation Metrics and Datasets.** We evaluate the performance of our method on widely adopted datasets including both object-level and large-scale ones, including DTU , Tanks and Temples (TNT)  and Mip-NeRF 360 (M360) . To evaluate the accuracy of the reconstructed meshes, we use Chamfer Distance (CD) on DTU and F-score on TNT, using the official evaluation script. To evaluate the rendering quality in real-scene datasets, we report PSNR, SSIM and LPIPS in evaluations on M360.

**Baselines.** We compare our geometry reconstruction accuracy with the state-of-the-art 3DGS based reconstruction methods, including SuGaR , DN-Splitter , GaussianSurfels  and 2DGS . For real-world scenes which do not have ground truth meshes for evaluations, we compare the rendering quality with state-of-the-art neural rendering methods, including Instant-NGP , Mip-NeRF 360  and BakedSDF .

**Surface Extraction.** An advantage of our approach over the latest methods is the simplicity of extracting surfaces. Different from methods like SuGaR  and GauS  which introduce specially designed algorithms and take a long time for extracting surfaces, we adopt the marching cubes algorithm  to extract mesh surfaces with the learned neural SDF \(f\). For small scale scenes, we use a resolution of 800 to extract surfaces, while we split large scale scenes into parts, each of which gets reconstructed with a resolution of 800 to bypass the limitation of our computational resources.

### Comparisons on DTU.

We report accuracy of reconstructed meshes and training time against baselines on DTU dataset in Tab. 1. Our method outperforms all Gaussian-based reconstruction methods in terms of Chamfer Distance. Our method achieves comparable training time to the state-of-the-art Gaussian-reconstruction method 2DGS  but gains better reconstruction accuracy than 2DGS. The visualization results in Fig. 3 highlight the advantages of our method. By employing alignment constraints and pulling operations between the 3D Gaussians and the neural SDF field, we can reconstruct significantly smoother and more complete surfaces than the baselines.

**Comparisons on TNT.** We further evaluate our method using more challenging large-scale unbounded scenes on TNT dataset. Numerical comparisons in Tab. 2 show that we achieve higher F-score compared to baseline methods, even surpassing NeuS, which however takes about 12 hours to fit a scene. Notably, as the scene scale increases, the number of Gaussian primitives increases rapidly, causing the adjusted CUDA rasterization kernel of 2DGS to consume more time for rendering. In contrast, since our rasterization kernel is based on 3DGS, it is less sensitive to the number of Gaussians, which enables us to learn 3D Gaussians faster than 2DGS. We provide visual comparisons

   Methods & 24 & 37 & 40 & 55 & 63 & 65 & 69 & 83 & 97 & 105 & 106 & 110 & 114 & 118 & 122 & Mean & Time \\  NeuS  & 1.00 & 1.37 & 0.93 & 0.43 & 1.10 & 0.65 & 0.571 & 1.48 & 1.09 & 0.83 & 0.52 & 1.20 & 0.35 & 0.49 & 0.54 & 0.84 & -9h \\
3DGS  & 2.14 & 1.53 & 2.08 & 1.68 & 3.49 & 2.21 & 1.43 & 2.07 & 2.22 & 1.75 & 1.79 & 2.55 & 1.53 & 1.52 & 1.50 & 1.96 & 15.1m \\ SuGaR  & 1.47 & 1.33 & 1.13 & 0.61 & 2.25 & 1.71 & 1.15 & 1.63 & 1.62 & 1.07 & 0.79 & 2.45 & 0.98 & 0.88 & 0.79 & 1.33 & 1.6h \\ DN-Splatter  & 1.60 & 2.03 & 1.42 & 1.44 & 2.37 & 2.11 & 1.62 & 1.95 & 1.88 & 1.48 & 1.63 & 1.82 & 1.20 & 1.50 & 1.40 & 1.70 & 31.2m \\ Gusferst  & 0.66 & 0.93 & 0.54 & 0.41 & 1.16 & 1.14 & 0.85 & 1.29 & 1.53 & 1.07 & 0.92 & 1.58 & 0.45 & 0.66 & 0.53 & 0.88 & 10.9m \\
2DGS  & 0.48 & 0.91 & 0.39 & 0.39 & 1.00 & 0.83 & 0.16 & 1.36 & 1.27 & 0.76 & 0.70 & 1.40 & 0.40 & 0.76 & 0.52 & 0.80 & 20.5m \\ Ours & 0.51 & 0.56 & 0.46 & 0.39 & 0.82 & 0.67 & 0.85 & 1.37 & 1.25 & 0.73 & 0.54 & 1.39 & 0.35 & 0.88 & 0.42 & 0.75 & 21.8m \\   

Table 1: Numerical comparisons in terms of CD on DTU dataset. Best results are highlighted as 1st, 2nd and 3rd.

   Methods & Barn & Caterpillar & Courthouse & Ignatius & Mectingroom & Truck & Mean & Time \\  NeuS  & 0.29 & 0.29 & 0.17 & 0.83 & 0.24 & 0.45 & 0.38 & \(\)12h \\
3DGS  & 0.13 & 0.08 & 0.09 & 0.04 & 0.01 & 0.19 & 0.09 & 20.5m \\ SuGaR  & 0.14 & 0.16 & 0.08 & 0.33 & 0.15 & 0.26 & 0.19 & 2.1h \\ DN-Splatter  & 0.15 & 0.11 & 0.07 & 0.18 & 0.01 & 0.20 & 0.12 & 54.9m \\ Gsurfels  & 0.24 & 0.22 & 0.07 & 0.39 & 0.12 & 0.24 & 0.21 & 15.1m \\
2DGS  & 0.41 & 0.23 & 0.16 & 0.51 & 0.17 & 0.45 & 0.32 & 39.4m \\ Ours & 0.60 & 0.37 & 0.16 & 0.71 & 0.22 & 0.52 & 0.43 & 37.6m \\   

Table 2: Numerical comparisons on Tanks And Temples dataset. Best results are highlighted as 1st, 2nd and 3rd.

Figure 3: Visual comparisons on DTU dataset.

in Fig. 4. Here we crop the reconstructed meshes to show the foreground objects that are of primary interest, as captured by the cameras. Please refer to the appendix for the reconstruction results of the background regions. The visual comparisons demonstrate that we can reconstruct more complete and smooth object surfaces, such as the ground, the truck's hood and the statue's left shoulder.

**Comparisons on MipNeRF 360**. We further evaluate our method in neural rendering for novel view synthesis on MipNeRF 360 dataset. We report the numerical comparisons in Tab. 3. Our competitive results against the state-of-the-art novel view synthesis methods indicate that our method is able to impose effective geometric constraints without compromising rendering quality. This provides a promising solution for learning continuous distance fields from discrete 3D Gaussians. Visual comparisons of mesh reconstructions are shown in Fig. 5, which demonstrate that our method is able to recover more smooth and complete surface by more effectively using the multi-view consistency.

    &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF & 26.84 & 0.790 & 0.370 & 21.46 & 0.458 & 0.515 \\ Instant-NGP  & 29.15 & 0.880 & 0.216 & 22.90 & 0.566 & 0.371 \\ MipNeRF 360  & 31.72 & 0.917 & 0.180 & 24.47 & 0.691 & 0.283 \\ BakedSDF  & 27.06 & 0.839 & 0.258 & 22.47 & 0.585 & 0.349 \\ 
3DGS  & 30.99 & 0.926 & 0.199 & 24.24 & 0.705 & 0.283 \\ SuGaR  & 29.44 & 0.911 & 0.216 & 22.76 & 0.631 & 0.349 \\
2DGS  & 30.39 & 0.924 & 0.183 & 24.33 & 0.709 & 0.284 \\ Ours & 30.78 & 0.925 & 0.182 & 23.76 & 0.703 & 0.278 \\   

Table 3: Quantitative evaluations of rendering quality on Mip-NeRF 360  dataset. Best results are highlighted as \(\), \(\) and \(\).

Figure 4: Visual comparisons on Tanks and Temples dataset.

Figure 5: Visual comparisons on Mip-NeRF 360 dataset.

### Ablation Studies

In this section, we conduct ablation studies on the key techniques of our method to demonstrate their effectiveness. The full quantitative results are reported in Tab. 4, which are conducted on all scenes in DTU dataset .

**Pulling Operations.** We first examine the effect of pulling Gaussians onto the zero-level set, as reported in Tab. 4 ("w/o Pull Gaussians" vs. "Ours"). The original 3DGS tends to produce floating ellipsoids near the object surfaces to overfit the training views. By pulling the Gaussians to the zero-level set of the SDF field, the Gaussians are consistently distributed on the surface. As shown in Fig. 5(a), after getting pulled onto the zero-level set, the Gaussian centers are distributed on a thin layer of the object surface, thus achieving an accurate geometry estimation. Meanwhile, we pull neighboring space onto Gaussian disks to learn neural SDFs. Comparing to NeuralPull  which pulls query points to centers, we innovatively pull query points to Gaussian disks, which bridge the gap between continuous SDF field and sparse Gaussian distributions, as highlighted in Fig. 2 and Tab. 4 ("Pulled to centers" vs. "Ours").

**Constraint Terms.** We further explore the effect of our constraint terms, as reported in Tab. 4 ("Constraint Terms"). Our full model provides the best performance when applying all constraint terms. The orthogonal loss helps to learn a more regularized SDF field, while the thin loss and tangent loss provide constraints to align the orientation of Gaussian disks with the gradient of neural SDF on the zero-level sets, resulting in a good normal field and a reconstructed mesh, as shown in Fig. 5(b), 5(c).

**Mesh Extraction.** We also report the reconstruction accuracy using TSDF fusion and screened Poisson reconstruction , as shown in Fig. 5(d) and Tab. 4 ("Mesh Extractions"). For TSDF fusion, we render depth maps and fuse them using a voxel size of 0.004 and truncation threshold as 0.02, the same as 2DGS . For screened Poisson, we use the Gaussian centers and normals as input. Unlike 2DGS  and GSurfels  which incorporate rendered depth into the differentiable rasterization pipeline, we do not directly optimize depths, resulting in noisy depth maps and unsatisfactory reconstruction results. However, since the positions and normals of the Gaussians are well optimized through our approach, screened Poisson reconstruction can achieve relatively good results.

    &  &  &  & \\  Methods & Pulled to centers & w/o Pull GS & w/o \(L_{Thin}\) & w/o \(L_{Tan}\) & w/o \(L_{Oth}\) & TSDF & Poisson & Full model \\  CD\(\) & 0.85 & 0.90 & 0.78 & 0.82 & 0.79 & 1.41 & 0.79 & 0.75 \\   

Table 4: Ablation studies on DTU dataset.

Gradient Constraint.We follow Neural-Pull  to use normalized SDF gradient for pulling operation. We report the result with an additional Eikonal term  to explicitly constrain the gradient length, as shown in Fig. 7. The result is significantly degenerated because that Neural-Pull depends on both predicted SDF values and gradient directions to optimize the SDF field. It makes the optimization even more complex when adding additional constraint on the gradient length.

## 5 Conclusion

We propose a method to learn neural SDFs for multi-view surface reconstruction with 3D Gaussian splatting. Our results show that we can more effectively leverage multi-view consistency to recover more accurate, smooth, and complete surfaces with geometry details by rendering 3D Gaussians pulled on the zero-level set. To this end, we dynamically align 3D Gaussians to the zero-level set and update neural SDFs through both differentiable pulling and splatting for both RGB and geometry constraints. Our methods successfully refine the signed distance field near the surface in a progressive manner, leading to plausible surface reconstruction. Our ablation studies justify the effectiveness of our novel modules, loss terms, and training strategies. Our evaluations show our superiority over the latest methods in terms of accuracy, completeness, and smoothness.

## 6 Acknowledgement

This work was supported by National Key R&D Program of China (2022YFC3800600), and the National Natural Science Foundation of China (62272263, 62072268), and in part by Tsinghua-Kuaiishou Institute of Future Media Data.