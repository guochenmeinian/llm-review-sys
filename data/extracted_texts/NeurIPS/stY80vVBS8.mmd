# Learning-Augmented Dynamic Submodular Maximization

Arpit Agarwal

Indian Institute of Technology Bombay

aarpit@iitb.ac.in

&Eric Balkanski

Columbia University

eb3224@columbia.edu

###### Abstract

In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms?

We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an \(O((, w, k))\) amortized update time over the sequence of updates that achieves a \(1/2-\) approximation for dynamic monotone submodular maximization under a cardinality constraint \(k\), where the prediction error \(\) is the number of elements that are not inserted and deleted within \(w\) time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error.

## 1 Introduction

Submodular functions are a well-studied family of functions that satisfy a natural diminishing returns property. Since many fundamental objectives are submodular, including coverage, diversity, and entropy, submodular optimization algorithms play an important role in machine learning [18; 8], network analysis , and mechanism design . For the canonical problem of maximizing a monotone submodular function under a cardinality constraint, the celebrated greedy algorithm achieves a \(1-1/e\) approximation guarantee , which is the best approximation guarantee achievable by any polynomial-time algorithm . Motivated by the highly dynamic nature of applications such as influence maximization in social networks and recommender systems on streaming platforms, a recent line of work has studied the problem of dynamic submodular maximization [20; 25; 28; 10; 11; 6; 7]. In the dynamic setting, the input consists of a stream of elements that are inserted or deleted from the set of active elements, and the goal is to maintain, throughout the stream, a subset of the active elements that maximizes a submodular function.

The standard worst-case approach to analyzing the update time of a dynamic algorithm is to measure its update time over the worst sequence of updates possible. However, in many application domains, dynamic data is not arbitrary and often exhibits patterns that can be learned from historical data. Very recent work has studied dynamic problems in settings where the algorithm is given as input predictions regarding the stream of updates [14; 22; 9]. This recent work is part of a broader research area called learning-augmented algorithms (or algorithms with predictions). In learning-augmented algorithms, the goal is to design algorithms that achieve an improved performance guarantee when the error of the prediction is small and a bounded guarantee even when the prediction error is arbitrarily large. A lot of the effort in this area has been focused on using predictions to improve the competitive ratio of online algorithms (see, e.g., [23; 30; 32; 24; 12; 3; 4; 19; 15; 5]), and more generally to improve the solution quality of algorithms.

For dynamic submodular maximization with predictions, Liu and Srinivas  considered a predicted-deletion model and achieved, under some mild assumption, a \(0.3178\) approximation and an \(((k, n))\)1 update time for dynamic monotone submodular maximization under a matroid constraint of rank \(k\) and over a stream of length \(n\). This approximation is an improvement over the best-known \(1/4\) approximation for dynamic monotone submodular maximization (without predictions) under a matroid constraint with an update time that is sublinear in \(n\). Since the update time of dynamic algorithms is often the main bottleneck in large-scale problems, another promising direction is to leverage predictions to improve the update time of dynamic algorithms.

#### Can predictions help to accelerate the update time of dynamic submodular maximization algorithms?

We note that the three very recent papers on dynamic algorithms with predictions have achieved improved update times for several dynamic graph problems . However, to the best of our knowledge, there is no previous result that achieves an improved update time for dynamic submodular maximization by using predictions.

**Our contributions.** In dynamic submodular maximization, the input is a submodular function \(f\,:\,2^{V}_{ 0}\) and a sequence of \(n\) element insertions and deletions. The active elements \(V_{t} V\) at time \(t\) are the elements that have been inserted and have not yet been deleted during the first \(t\) updates. The goal is to maintain, at every time step \(t\), a solution \(S_{t} V_{t}\) that is approximately optimal with respect to \(V_{t}\) while minimizing the number of queries to \(f\) at each time step, which is referred to as the update time. As in , we consider a prediction model where, at time \(t=0\), the algorithm is given a prediction regarding the sequence of updates, which can be used for preprocessing. More precisely, at time \(t=0\), the algorithm is given predictions \((_{a}^{+},_{a}^{-})\) about the insertion and deletion time of each element \(a\). A dynamic algorithm with predictions consists of two phases. During the precomputation phase at \(t=0\), the algorithm uses the predictions to perform queries before the start of the stream. During the streaming phase at time steps \(t>0\), the algorithm performs queries, and uses the precomputations, to maintain a good solution with respect to the true stream.

In this model, there is a trivial algorithm that achieves a constant update time when the predictions are exactly correct and an \(O(u)\) update time when the predictions are arbitrarily wrong. Here, \(u\) is the update time of an arbitrary algorithm \(\) for the problem without predictions. This algorithm precomputes, for each future time step \(t\), a solution for the elements that are predicted to be active at time \(t\) and then, during the streaming phase, returns the precomputed solutions while the prediction is correct and switches to running algorithm \(\) at the first error in the prediction. Thus, the interesting question is whether it is possible to obtain an improved update time not only when the predictions are exactly correct, but more generally when the error in the predictions is small. An important component of our model is the measure for the prediction error. Given a time window tolerance \(w\), an element \(a\) is considered to be correctly predicted if the predicted insertion and deletion times of \(a\) are both within \(w\) times steps of its true insertion and deletion times. The prediction error \(\) is then the number of elements that are not correctly predicted. Thus, \(=0\) if the predictions are exactly correct and \(=(n)\) if the predictions are completely wrong.

For dynamic monotone submodular maximization (without predictions) under a cardinality constraint \(k\), Lattanzi et al.  and Monemizadeh  concurrently obtained dynamic algorithms with \(O(( n, k))\) and \(O((n) k^{2})\) amortized update time, respectively, that achieve a \(1/2-\) approximation. More recently, Banihashem et al.  achieved a \(1/2-\) approximation with a \(O(k(k))\) amortized update time. Our main result is the following.

**Theorem**.: _For monotone submodular maximization under a cardinality constraint \(k\), there is a dynamic algorithm with predictions that, for any tolerance \(w\) and constant \(>0\), achieves an amortized expected query complexity per update of \(O((, w, k))\), an approximation of \(1/2-\) in expectation, and a query complexity of \((n)\) during the precomputation phase._

We note that, when the prediction error \(\) is arbitrarily large, our algorithm matches the \(O(( n, k))\) amortized expected query complexity per update in . It also achieves an approximation that matches the optimal approximation for dynamic algorithms (without predictions) with update time that is sublinear in \(n\). An intriguing open question is whether an improvement in update time can be obtained in the predicted-deletion model of  with no preprocessing and instead a predicted deletion time for each element \(a\) is given at the time when \(a\) is inserted.

**Related work.** For monotone submodular maximization under a cardinality constraint, dynamic algorithms with \(O(( n, k))\) and \(O((n) k^{2}))\) amortized update time that achieve a \(1/2\) approximation were concurrently obtained in . Recently, Banihashem et al.  gave a \(1/2\) approximation algorithm with a \(O(k(k))\) amortized update time. Chen and Peng  showed that any dynamic algorithm with an approximation better than \(1/2\) must have \((n)\) amortized query complexity per update. For matroid constraints, Chen and Peng  obtained an insertion-only algorithm. As mentioned in , the streaming algorithm in  can be adapted to also give an insertion-only algorithm. Two \(1/4\)-approximation dynamic algorithms with \((k)\) and \(((n) k^{2})\) amortized update time were concurrently obtained in  and .

Algorithms with predictions have been studied in a wide range of areas, including online algorithms , mechanism design , and differential privacy . Improved update times for several dynamic graph problems were very recently obtained by leveraging predictions . In particular, Liu and Srinivas  obtained, under some mild assumption on the prediction error, a \(0.3178\) approximation and a \(((k, n))\) update time for dynamic monotone submodular maximization under a matroid constraint of rank \(k\) in the more challenging predicted-deletion model. Thus, by using predictions, this result improves the \(1/4\) approximation achieved, without predictions, in  and  (but does not improve the \(1/2\) approximation for cardinality constraints). The results in  use a framework that takes as input an insertion-only dynamic algorithm. In contrast, we develop a framework that uses a fully dynamic algorithm and a deletion-robust algorithm.

## 2 Preliminaries

A function \(f:2^{V}\) defined over a ground set \(V\) is submodular if for all \(S T V\) and \(a V T\), we have that \(f_{S}(a) f_{T}(a)\), where \(f_{S}(a)=f(S\{a\})-f(S)\) is the marginal contribution of \(a\) to \(S\). It is monotone if \(f(S) f(T)\) for all \(S T V\). We consider the canonical problem of maximizing a monotone submodular function \(f\) under a cardinality constraint \(k\).

In **dynamic submodular maximization**, there is a stream \(\{(a_{t},o_{t})\}_{t=1}^{n}\) of \(n\) element insertions and deletions where \(o_{t}\{,\}\) and \(a_{t}\) is an element in \(V\). The active elements \(V_{t}\) are the elements that have been inserted and have not been deleted by time \(t\). We assume that \((a_{t},)\) and \((a_{t},)\) can occur in the stream only if \(a_{t} V_{t}\) and \(a_{t} V_{t}\), respectively, and that each element is inserted at most once.2 The goal is to maintain a solution \(S_{t} V_{t}\) that approximates the optimal solution over \(V_{t}\), which we denote by \(O_{t}\). Since our algorithmic framework takes as input a dynamic algorithm, we formally define dynamic algorithms in terms of black-box subroutines that are used in our algorithms.

**Definition 1**.: _A dynamic algorithm \((f,k)\) consists of the following four subroutines to process a stream \(\{(a_{t},o_{t})\}_{t=1}^{n}\). \((f,k)\) initializes a data structure \(A\) at \(t=0\). If \(o_{t}=o_{t}=\), \((A,a_{t})\) or \((A,a_{t})\) insert in \(A\) or delete from \(A\) element \(a_{t}\) at time \(t\). At time \(t\), \((A)\) returns \(S_{t} V(A)\) s.t. \(|S| k\), where \(V(A)=V_{t}\) is the set of elements that have been inserted in and not been deleted from \(A\)._

A dynamic algorithm achieves an \(\)-approximation in expectation if, for all time steps \(t\), \([f(S_{t})]_{S V_{t}:|S| k}f(S)\) and has a \(u(n,k)\) amortized expected query complexity per update if its expected total number of queries is \(n u(n,k)\).

In **dynamic submodular maximization with predictions**, the algorithm is given at time \(t=0\) predictions \(\{(_{a}^{+},_{a}^{-})\}_{a}\) about the insertion and deletion time of elements \(a\). The prediction error \(\) is the number of elements that are incorrectly predicted, where an element \(a\) is correctly predicted if it is inserted and deleted within a time window, of size parameterized by a time window tolerance \(w\), that is centered at the time at which \(a\) is predicted to be inserted and deleted.

**Definition 2**.: _Given a tolerance \(w_{+}\), predictions \(\{(_{a}^{+},_{a}^{-})\}_{a V}\), and true insertion and deletions times \(\{(t_{a}^{+},t_{a}^{-})\}_{a V}\), the prediction error is \(=|\{a V:|_{a}^{+}-t_{a}^{+}|>w|_{a}^{-}-t_{a}^{-}|>w\}|\)._

We note that \(=w=0\) corresponds to the predictions being exactly correct and \(=O(n)\) and \(w=O(n)\) corresponds to thems being arbitrarily wrong. We also emphasize that our model does not require knowing the entire ground set \(V\) at \(t=0\). Elements that are not known at \(t=0\) are assumed to have predicted arrival and departure times equal to infinity, and contribute to the prediction error \(\)

**Deletion-robust submodular maximization.** Our framework also takes as input a _deletion-robust algorithm_, which we formally define in terms of black-box subroutines. A deletion-robust algorithm finds a solution \(S V\) that is robust to the deletion of at most \(d\) elements.

**Definition 3**.: _Given a function \(f:2^{V}\), a cardinality constraint \(k\), and a maximum number of deletions parameter \(d\), a deletion-robust algorithm \((f,V,k,d)\) consists of a first subroutine \((f,V,k,d)\) that returns a robust set \(R V\) and a second subroutine \((f,R,D,k)\) that returns a set \(S R D\) such that \(|S| k\)._

A deletion-robust algorithm achieves an \(\) approximation if, for any \(f\), \(V\), \(k\), and \(d\), the subroutine \((f,V,k,d)\) returns \(R\) such that, for any \(D V\) such that \(|D| d\), \((f,R,D,k)\) returns \(S\) such that \([f(S)]_{T V D:|T| k}f(T)\). Kazemi et al.  show that there is a deletion-robust algorithm for monotone submodular maximization under a cardinality constraint such that Robust1 returns a set \(R\) of size \(|R|=O(^{-2}d k+k)\). It achieves a \(1/2-\) approximation in expectation, Robust1 has \(O(|V|(k+^{-1} k))\) query complexity, and Robust2 has \(O((^{-2}d k+k)^{-1} k)\) query complexity.

**The algorithmic framework.** We present an algorithmic framework that decomposes dynamic algorithms with predictions into two subroutines, Precomputations and UpdateSol. The remainder of the paper then consists of designing and analyzing these subroutines. We first introduce some terminology. An element \(a\) is said to be correctly predicted if \(|_{a}^{+}-t_{a}^{+}| w\) and \(|_{a}^{-}-t_{a}^{-}| w\). The _predicted elements_\(_{t}\) consist of all elements that could potentially be active at time \(t\) if correctly predicted, i.e., the elements \(a\) such that \(_{a}^{+} t+w\) and \(_{a}^{-} t-w\). During the precomputation phase, the first subroutine, Precomputations, takes as input the predicted elements \(_{t}\) and outputs, for each time step \(t\), a data structure \(P_{t}\) that will then be used at time \(t\) of the streaming phase to compute a solution efficiently. During the streaming phase, the active elements \(V_{t}\) are partitioned into the _predicted active elements_\(V_{t}^{1}=V_{t}_{t}\) and the _unpredicted active elements_\(V_{t}^{2}=V_{t}_{t}\). The second subroutine, UpdateSol, is given \(V_{t}^{1}\) and \(V_{t}^{2}\) as input and computes a solution \(S V_{t}^{1} V_{t}^{2}\) at each time step. UpdateSol is also given as input precomputations \(P_{t}\) and the current prediction error \(_{t}\). It also stores useful information for future time steps in a data structure \(A\).

```
1:Input:function \(f:2^{V}\), constraint \(k\), predictions \(\{(_{a}^{+},_{a}^{-})\}_{a V}\), tolerance \(w\)
2:\(_{t}\{a V:_{a}^{+} t+w_{a}^{-} t-w\}\) for \(t[n]\)
3:\(\{P_{t}\}_{t=1}^{n}(f,\{_{t}\}_{t=1}^{n},k)\)
4:\(V_{0},A\)
5:for\(t=1\) to \(n\)do
6: Update active elements \(V_{t}\) according to operation at time \(t\)
7:\(V_{t}^{1} V_{t}_{t}\), \(V_{t}^{2} V_{t}_{t}\)
8:\(_{t}\) current prediction error
9:\(A,S(f,k,A,t,P_{t},V_{t}^{1},V_{t}^{2},_{t}, _{t})\)
10:return\(S\) ```

**Algorithm 1** The Algorithmic Framework

## 3 The warm-up algorithm

In this section, we present subroutines that achieve an \((+w+k)\) amortized update time and a \(1/4-\) approximation in expectation. These warm-up subroutines assume that the error \(\) is known. They take as input a dynamic algorithm (without predictions) Dynamic and a deletion-robust algorithm Robust algorithm. The proofs are all deferred to the appendix.

The precomputations subroutineA main observation is that the problem of finding a solution \(S^{1} V_{t}^{1}\) among the predicted active elements corresponds to a deletion-robust problem over \(_{t}\) where the deleted elements \(D\) are the predicted elements \(_{t} V_{t}\) that are not active at time \(t\). WarmUp-Precomputations thus calls, for each time \(t\), the first stage Robust1 of Robust,

\[(f,\{_{t}\}_{t=1}^{n},k)=\{(f,_{t},k,d=+2w)\}_{t=1}^{n}.\]The algorithm sets the maximum number of deletions parameter \(d\) for Robust1 to \(+2w\) because the number of predicted elements \(_{t} V_{t}\) that are not active at time \(t\) is at most \(+2w\) (Lemma 8).

The updatesol subroutineWarmUp-UpdateSol finds a solution \(S^{1} V_{t}^{1}\) by calling Robust2 over the precomputed \(P_{t}\) and deleted elements \(D=_{t} V_{t}\). To find a solution \(S^{2} V_{t}^{2}\) among the unpredicted active elements, we use Dynamic over the stream of element insertions and deletions that result in unpredicted active elements \(V_{1}^{2},,V_{n}^{2}\), which is the stream that inserts elements \(V_{t}^{2} V_{t-1}^{2}\) and deletes elements \(V_{t-1}^{2} V_{t}^{2}\) at time \(t\). The solution \(S^{2}\) is then the solution produced by DynamicSol over this stream. The solution \(S\) returned by UpdateSol is the best solution between \(S^{1}\) and \(S^{2}\).

```
0:function \(f\), constraint \(k\), data structure \(A\), time \(t\), precomputations \(P_{t}\), predicted active elements \(V_{t}^{1}\), unpredicted active elements \(V_{t}^{2}\), predicted elements \(_{t}\)
1:\(S^{1}(f,P_{t},_{t} V_{t}^{1},k)\)
2:if\(t=1\)then\(A(f,k)\)
3:for\(a V_{t}^{2} V(A)\)do\((A,a)\)
4:for\(a V(A) V_{t}^{2}\)do\((A,a)\)
5:\(S^{2}(A)\)
6:return\(A,\{f(S^{1}),f(S^{2})\}\) ```

**Algorithm 2**WarmUp-UpdateSol

The analysis of the warm-up algorithmWe first analyze the approximation. We let \(_{1}\) and \(_{2}\) denote the approximations achieved by Robust and Dynamic. The first lemma shows that solution \(S^{1}\) is an \(_{1}\) approximation to the optimal solution over the predicted active elements \(V_{t}^{1}\) and that solution \(S^{2}\) is an \(_{2}\) approximation to the optimal solution over the unpredicted active elements \(V_{t}^{2}\).

**Lemma 1**.: _At every time step \(t\), \([f(S^{1})]_{1}(V_{t}^{1})\) and \([f(S^{2})]_{2}(V_{t}^{2})\)._

The main lemma for the amortized query complexity bounds the number of calls to DynamicIns.

**Lemma 2**.: WarmUp-UpdateSol _makes at most \(2\) calls to DynamicIns on \(A\) over the stream._

The main result for the warm-up algorithm is the following.

**Theorem 1**.: _For monotone submodular maximization under a cardinality constraint \(k\), Algorithm 1 with the WarmUp-Precomputations and WarmUp-UpdateSol subroutines achieves, for any tolerance \(w\) and constant \(>0\), an amortized expected query complexity per update during the streaming phase of \((+w+k)\), an approximation of \(1/4-\) in expectation, and a query complexity of \((n^{2}k)\) during the precomputation phase._

In the next sections, we improve the dependencies on \(,w,\) and \(k\) for the query complexity per update from linear to logarithmic, the approximation from \(1/4\) to \(1/2\), and the precomputations query complexity from \(O(n^{2}k)\) to \((n)\). We also remove the assumption that the prediction error is known.

## 4 The UpdateSol subroutine

In this section, we improve the dependencies in \(,w,\) and \(k\) for the amortized query complexity from linear to logarithmic, which is the main technical challenge. For finding a solution over the predicted active elements \(V_{t}^{1}\), the main idea is to not only use precomputations \(P_{t},\) but also to exploit computations from previous time steps \(t^{}<t\) over the previous predicted active elements \(V_{t^{}}^{1}\). As in the warm-up subroutine, the new UpdateSolMain subroutine also uses a precomputed deletion-robust solution \(P_{t}\), but it requires \(P_{t}\) to satisfy a property termed the _strongly robust property_ (Definition 4 below), which is stronger than the deletion-robust property of Definition 3. A strongly robust solution comprises two components \(Q\) and \(R\), where \(R\) is a small set of elements that have a high marginal contribution to \(Q\). The set \(Q\) is such that, for any deleted set \(D\), \(f(Q D)\) is guaranteed to, in expectation over the randomization of \(Q\), retain a large amount of \(f(Q)\).

**Definition 4**.: _A pair of sets \((Q,R)\) is \((d,,)\)-strongly robust, where \(d,k, 0\) and \(\), if__._
* _Size._ \(|Q| k\) _and_ \(|R|=O(^{-2}(d+k) k)\) _with probability_ \(1\)_,_
* _Value._ \(f(Q)|Q|/(2k)\) _with probability_ \(1\)_. In addition, if_ \(|Q|<k\)_, then for any set_ \(S V R\) _we have_ \(f_{Q}(S)<|S|/(2k)+\)_._
* _Robustness._ _For any_ \(D V\) _s.t._ \(|D| d\)_,_ \(_{Q}[f(Q D)](1-)f(Q)\)_._

The set \(P\) returned by the first stage Robust1\((f,V,k,d)\) of the deletion-robust algorithm of Kazemi et al.  can be decomposed into two sets \(Q\) and \(R\) that are, for any \(d,>0\) and with \(=(V)\), where \((V):=_{S V:|S| k}f(S)\), \((d,,)\)-strongly robust.3 Thus, with the Robust algorithm of , the set \(P_{t}\) returned by WarmUp-Precomputations can be decomposed into \(P_{t}\) and \(Q_{t}\) that are, for any \(>0\), \((2(+2w),,(_{t}))\)-strongly robust. We omit the proof of the \((d,,)\)-strongly robust property of Robust1 from  and, in the next section, we instead prove strong-robustness for our PrecomputationsMain subroutine which has better overall query complexity than .

The UpdateSolMain subroutine proceeds in phases. During each phase, UpdateSolMain maintains a data structure \((B,A,_{})\). The set \(B=Q_{t^{}}\) is a fixed base set chosen during the first time step \(t^{}\) of the current phase. \(A\) is a dynamic data structure used by a dynamic submodular maximization algorithm Dynamic that initializes \(A\) over function \(f_{B}\), cardinality constraint \(k-|B|\), and a parameter \(\) to be later discussed. If a new phase starts at time \(t\), note that if \((Q_{t},R_{t})\) are strongly robust, then the only predicted active elements \(V_{t}^{1}\) that are needed to find a good solution at time \(t\) are, in addition to \(B=Q_{t}\), the small set of elements \(R_{t}\) that are also in \(V_{t}^{1}\). Thus to find a good solution for the function \(f_{B}\), \((R_{t} V_{t}^{1}) V_{t}^{2}\) are inserted into \(A\). The solution that UpdateSolMain outputs at time step \(t\) are the active elements \(B V_{t}\) that are in the base for the current phase, together with the solution DynamicSol\((A)\) maintained by \(A\).

```
0: function \(f\), data structure \((B,A,_{})\), constraint \(k\), precomputations \(P_{t}=(Q_{t},R_{t})\), \(t\), upper bound \(_{t}^{}\) on prediction error, \(V_{t}^{1}\), \(V_{t}^{2}\), \(V_{t-1}\), parameter \(_{t}\)
1:if\(t=1\) or \(|^{}(A)|>}}{2}+w\)then\(\) Start a new phase
2:\(B Q_{t}\)
3:\(A(f_{B},k-|B|,=_{t}(k-|B|)/k)\)
4:for\(a(R_{t} V_{t}^{1}) V_{t}^{2}\)doDynamicIns\((A,a)\)
5:\(_{}_{t}^{}\)
6:else\(\) Continue the current phase
7:for\(a V_{t} V_{t-1}\)do\((A,a)\)
8:for\(a((A V_{t-1}) V_{t}\)do\((A,a)\)
9:\(S(B(A)) V_{t}\)
10:return\((B,A,_{}),S\) ```

**Algorithm 3**UpdateSolMain

During the next time steps \(t\) of the current phase, if an element \(a\) is inserted into the stream then \(a\) is inserted in \(A\) (independently of the predictions). If an element is deleted from the stream, then if it was in \(A\), it is deleted from \(A\). We define \(^{}(A)\) to be the collection of all insertion and deletion operations to \(A\), excluding the insertions of elements in \((R_{t} V_{t}^{1}) V_{t}^{2}\) at the time \(t\) where \(A\) was initialized. The current phase ends when \(^{}(A)\) exceeds \(_{}/2+w\). Since the update time of the dynamic algorithm in  depends on length of the stream, we upper bound the length of the stream handled by \(A\) during a phase.

The approximationThe parameter \(_{t}\) corresponds to a guess for \(_{t}:=(V_{t})\). In Section 6, we present the UpdateSolFull subroutine which calls UpdateSolMain with different guesses \(_{t}\). This parameter \(_{t}\) is needed when initializing Dynamic because our analysis requires that Dynamic satisfies a property that we call threshold-based, which we formally define next.

**Definition 5**.: _A dynamic algorithm Dynamic is threshold-based if, when initialized with threshold parameter \(\) such that \(_{t}(1+)\), a cardinality constraint \(k\), and \(>0\), it maintains a data structure \(A_{t}\) and solution \(_{t}=(A_{t})\) that satisfy, for all \(t\), \(f(_{t})|_{t}|\) and, if \(|_{t}|<(1-)k\), then for any set \(S V(A_{t})\), we have \(f_{_{t}}(S)<+\)._

**Lemma 3**.: _The Dynamic algorithm of Lattanzi et al. 4 is threshold-based._

The main lemma for the approximation guarantee is the following.

**Lemma 4**.: _Consider the data structure \((B,A,_{})\) returned by UpdateSolMain at time \(t\). Let \(t^{}\) be the time at which \(A\) was initialized, \((Q_{t^{}},R_{t^{}})\) and \(_{t^{}}\) be the precomputations and guess for \(_{t^{}}\) inputs to UpdateSolMain at time \(t^{}\). If \((Q_{t^{}},R_{t^{}})\) are \((d=2(_{}+2w),,_{t^{}})\)-strongly robust, \(_{t^{}}\) is such that \(_{t^{}}_{t}(1+)_{t^{ }}\), and Dynamic is a threshold-based dynamic algorithm, then the set \(S\) returned by UpdateSolMain is such that \([f(S)]_{t^{}}\)._

The update timeWe next analyze the query complexity of UpdateSolMain. Recall that \(u(n,k)\) denotes the amortized query complexity per update of Dynamic.

**Lemma 5**.: _Consider the data structure \((B,A,_{})\) returned by UpdateSolMain at time \(t\). Let \(t^{}\) be the time at which \(A\) was initialized and \((Q_{t^{}},R_{t^{}})\) be the precomputations input to UpdateSolMain at time \(t^{}\). If precomputations \((Q_{t^{}},R_{t^{}})\) are \((d=2(_{}+2w),,)\) strongly-robust, then the total number of queries performed by UpdateSol during the \(t-t^{}\) time steps between time \(t^{}\) and time \(t\) is \(O(u((_{}+w+k) k,k)(_{}+w+k) k)\). Additionally, the number of queries between time \(1\) and \(t\) is upper bounded by \(O(u(t,k) t)\)._

## 5 The Precomputations subroutine

In this section, we provide a Precomputations subroutine that has an improved query complexity compared to the warm-up precomputations subroutine. Recall that the warm-up subroutine computes a robust solution over predicted elements \(_{t}\), independently for all times \(t\). The improved Precomputations does not do this independently for each time step. Instead, it relies on the following lemma that shows that the data structure maintained by the dynamic algorithm of  can be used to find a strongly robust solution without any additional query.

**Lemma 6**.: _Let \((,)\) be the dynamic submodular maximization algorithm of  and \(A\) be the data structure it maintains. There is a Robust1FromDynamic algorithm such that, given as input a deletion size parameter \(d\), and the data structure \(A\) at time \(t\) with \(_{t}(1+)\), it outputs sets \((Q,R)\) that are \((d,,)\)-strongly robust with respect to the ground set \(V_{t}\). Moreover, this algorithm does not perform any oracle queries._

The improved PrecomputationsMain subroutine runs the dynamic algorithm of  and then, using the RobustFromDynamic algorithm of Lemma 6, computes a strongly-robust set from the data structure maintained by the dynamic algorithm.

```
1:function \(f:2^{V}\), constraint \(k\), predicted elements \(_{1},,_{n} V\), time error tolerance \(w\), parameter \(\), parameter \(h\)
2:\((f,k,)\)
3:for\(t=1\) to \(n\)do
4:for\(a_{t}_{t-1}\)do\((,a)\)
5:for\(a V()_{t}\)do\((,a)\)
6:if\(|V()|>0\)then\(Q_{t},R_{t}(f,,k,2(h+2w))\)
7:return\(\{(Q_{t},R_{t})\}_{t=1}^{n}\) ```

**Algorithm 4**PrecomputationsMain

The parameters \(\) and \(h\) correspond to guesses for \(\) and \(\) respectively.

**Lemma 7**.: _The total query complexity of the PrecomputationsMain algorithm is \(n u(n,k)\), where \(u(n,k)\) is the amortized query complexity of calls to Dynamic._The full algorithm

The UpdateSolMain and PrecomputationsMain subroutines use guesses \(_{t}\) and \(h\) for the optimal value \(_{t}\) at time \(t\) and the total prediction error \(\). In Appendix D, we describe the full UpdateSolFull and PrecomputationsFull subroutines that call the previously defined subroutines over different guesses \(_{t}\) and \(h\). The parameters of these calls must be carefully designed to bound the streaming amortized query complexity per update and precomputations query complexity. By combining the algorithmic framework (Algorithm 1) together with subroutines UpdateSolFull and PrecomputationsFull, we obtain our main result.

**Theorem 2**.: _Algorithm 1 with subroutines UpdateSolFull and PrecomputationsFull is a dynamic algorithm that, for any tolerance \(w\) and constant \(>0\), achieves an amortized expected query complexity per update during the streaming phase of \(O((, w, k))\), an approximation of \(1/2-\) in expectation, and a query complexity5 of \((n)\) during the precomputation phase._

Note that the query complexity per update during the streaming phase and the query complexity during the precomputation phase both have a polynomial dependence on \(\). Additionally, note that our update bound is not constant even when the prediction error is \(0\). However, with the following simple change, our algorithm achieves a constant update time when the predictions are exactly correct, while also maintaining its current guarantees: (1) as additional precomputations, also compute a predicted solution \(S_{t}\) for each time \(t\) assuming the predictions are exactly correct, (2) during the streaming phase, as long as the predictions are exactly correct, return the precomputed predicted solution \(S_{t}\). At the first time step where the predictions are no longer exactly correct, switch to our main algorithm in the paper.

## 7 Experiments

### Experimental Setup

Benchmarks.We compare our **DynamicWPred** algorithm to two benchmarks. The first is the **Dynamic** submodular maximization algorithm of , which does not use predictions. The second is the **OfflineGreedy** algorithm that computes an offline greedy solution \(_{t}\) at each time step \(t\) based on the set \(_{t}\) of available items in the predicted stream. In the streaming phase, it outputs the solution \(_{t} V_{t}\) at time \(t\) based on the available items \(V_{t}\). Note that this algorithm does not make any queries other than the queries used for pre-computation. These benchmarks are at two extremes in terms of their reliance on the predictions. DynamicWPred uses the Dynamic algorithm of  and the Robust algorithm of  as subroutines. We implemented our algorithm and OfflineGreedy in C++, and used the C++ implementation of Dynamic that is provided by . We set \(=0.2\) for all algorithms.

Metrics.We report the total number of oracle calls made by each algorithm when processing the actual stream \((,)\). This does not include any oracle calls during the pre-computation phase. We also report the average function value \(_{t[n]}f(S_{t})\) of the output sets over time steps \(t[n]\). Each experiment is repeated \(5\) times and the average values are reported.

Datasets and submodular function.We perform experiments on a subset of the Enron dataset from the SNAP Large Networks Data Collection . We select a set \(V\) of \(200\) nodes from the graph, and consider the subgraph induced by \(V N(V)\) where \(N(V)\) is the set of neighboring nodes of \(V\). This resulted in a subgraph with \(7845\) vertices and \(20033\) edges. The submodular function is the dominating set objective function over the ground set \(V\) with \(|V|=200\). Specifically, for a subset of nodes \(S V\), we define \(f(S)=|N(S) S|\), where \(N(S)\) is the set of neighboring nodes of \(S\). This function is monotone and submodular.

Generation of true stream.We consider the sliding window protocol from  in order to generate the dynamic stream of insertions and deletions. Specifically, we process the nodes \(V\) in an arbitrary order and consider a sliding window of size \(l\). When the window reaches a node \(a\), we add the operation \((a,)\) to the dynamic stream. Similarly, after \(l\) steps when the node \(a\) leaves the window, we add the operation \((a,)\) to the dynamic stream. Since, \(|V|=200\), we have \(n=400\) for all experiments. We report results with \(l=50\) (observations were similar for other values of \(l\)).

Generation of predicted stream.Given a target prediction error \(\) and window size \(w\), we generate the predicted stream for our experiments by adding perturbations to the actual stream such that Definition 2 is satisfied. Note that we add these perturbations while maintaining the consistency of the stream, i.e. the insertion of an element always happens before its deletion. In particular, we first select a set \(E\) of \(/2\) elements uniformly at random and let \(\) denote the set of all insertion and deletion times of these elements in the actual stream. For each \(e E\), we assign new insertion and deletion times in the predicted stream by randomly drawing from \(\). We make sure that these new insertion and deletion times are consistent and are at least a distance of \(w\) from the corresponding old times. The insertion and deletion times for \(e E\) remain the same so far. Now, for each \(e,e^{} E\), we randomly swap an their operations \((e,o)\) and \((e^{},o^{})\) that are within a distance of \(w\) while maintaining consistency.

### Experiment Results

**Experiment Set 1.** We first consider the effect of the prediction error \(\) on the function value. For ease of exposition, we overload the notation and report the fractional prediction error, i.e. prediction error divided by the length of the stream \(n\). From the first column of Figure 1, we observe that our algorithm outperforms OfflineGreedy in terms of function value when the prediction error is reasonably large, and always achieves a similar function value as Dynamic. Since Dynamic does not use the predictions, its performance remains constant as a function of the prediction errors. The performance of OfflineGreedy deteriorates quickly as a function of \(\) as it completely relies on the predictions. Note that the function value achieved by OfflineGreedy is not zero even in the case of large prediction error. This is because the error is not adversarial and some elements from its offline solution remain active during the streaming phase.

We also consider the effect of \(\) on the number of oracle calls. Figure 1 shows that the number of oracle calls of our algorithm is much better than Dynamic in the case of low prediction error, and is also not much worse in the case of large prediction error. This shows that our algorithm has consistency in the case of low \(\), but also robustness in the case of large \(\). The number of oracle calls of OfflineGreedy is very small as it completely relies on the prediction.

Figure 1: The number of queries and function value of our algorithm, DynamicWPred, and the two benchmarks for the Enron data and sliding window stream with \(l=50\) as a function of the prediction error \(\) with \(k=10\) and \(w=0\) (column 1), as a function of the cardinality parameter \(k\) with \(=0.25\) and \(w=0\) (column 2), and as a function of the window size parameter \(w\) with \(k=20\) and \(=0.125\) (column 3).

**Experiment Set 2.** We also consider the effect of cardinality parameter \(k\) on the function value and number of oracle calls. It can be observed from the second column of Figure 1 that the function value increases for all algorithms as a function of \(k\). Moreover, the rate of increase for the number of oracle calls made by our algorithm is similar to the rate of increase of Dynamic. **Experiment Set 3.** We consider the effect of the window size parameter \(w\) on the function value and number of oracle calls. Figure 1 shows that the window size has almost no impact on the function value of our algorithm. The number of oracle calls for our algorithm grows as a function of the window size but this growth is very small.

## 8 Limitations

To obtain an asymptotic improvement over the best-known amortized query complexity per update, the prediction error \(\) needs to be subpolynomial in the length of the stream \(n\), which is relatively small. However, our experimental results show that in practice the number of queries performed by our algorithm outperforms the number of queries of existing algorithm without predictions even when the prediction error is relatively large. Another limitation is the assumption that the algorithm is given all predictions at time \(t=0\). An intriguing open question is whether an improvement in update time can also be obtained in the predicted-deletion model where there is no preprocessing and instead a predicted deletion time for each element is given at the time when it is inserted.