# Robust Preference Optimization

through Reward Model Distillation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the _preferred_ generations to go to zero. In this work, we analyze this phenomenon and propose _distillation_ to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a _family of reward models_ that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.

## 1 Introduction

Language model (LM) post-training (or alignment) aims to steer language model policies towards responses that agree with human preferences. Early state-of-the-art approaches have focused on reward learning from human feedback. In this paradigm, preference annotations are used to train reward models, which then guide the optimization of the language model policy through online reinforcement learning (an approach broadly referred to as RLHF). Recent research on offline "Direct Preference Optimization" [DPO; 23] and extensions thereof [3; 31], however, has demonstrated that it is also possible to directly optimize policies on the preference data, which bypasses the need for a separate reward model--and its offline nature also leads to faster, and simpler, training frameworks.

While this direct approach to preference optimization is attractive in terms of its simplicity and efficiency, it also raises important questions about the effectiveness and robustness of the resulting policies--as well as the broader utility of using an explicit reward model. In this paper, we argue that explicit reward modeling can, in fact, offer substantial practical advantages that are not captured by DPO's formulation. In particular, we theoretically show that relying solely on the preference data can be a precarious strategy, with few natural brakes in place to prevent policies trained under the DPO objective from careening off towards degenerate policies when the preference data exhibits certain idiosyncratic properties. On the other hand, explicit reward models can easily be regularized and understood--regardless of whether they are Bradley-Terry models , margin-based ranking models , or simply any other kind of function that correlates well with human preferences [31; 17].

Taking a step back from pure direct preference optimization, we propose a method that merges the best of both worlds: an efficient reward model distillation algorithm that (i) operates effectively in the offline setting, (ii) makes minimal assumptions about the true, optimal reward we aim to maximize, and (iii) demonstrates greater robustness to the specific distribution of prompt/response data used for policy alignment. Drawing inspiration from prior knowledge distillation techniques [14; 26; 35; 10], we leverage the same change of variables trick employed in DPO to express the language model policy in terms of its implicit reward model . We then train the policy to match our desired, explicit reward via an \(L_{2}\) loss that directly regresses the pairwise differences in target rewards for any two generation pairs \((x,y_{1})\) and \((x,y_{2})\). We theoretically establish the equivalence between optimizing this distillation loss over a sufficiently diverse offline dataset of unlabeled examples and optimizing the traditional online RLHF objective.

Our reward model distillation approach, however, is not immune to some of the same challenges facing DPO-style learning of policies. In particular, reward model distillation requires having a reliable reward model--but having a reliable reward requires having a reliable method for extracting a reward model from a potentially noisy preference dataset. To address the uncertainty surrounding the "right" reward model, we introduce a pessimistic extension to our approach. This extension aims to maximize the worst-case improvement of our model across a plausible family of reward models (e.g., those sufficiently consistent with annotated preference data). This strategy aligns with that of existing work in conservative offline reinforcement learning [5; 16]. Interestingly, we derive that this pessimistic objective can be equivalently expressed and optimized by adding a simple additional KL-divergence regularization to the original distillation objective.

Empirically, we find that reward model distillation, particularly pessimistic reward model distillation, leads to similar performance to prior direct preference optimization methods in settings where the preference datasets used are unbiased, but significantly better performance in settings where the preference datasets are biased, when compared to DPO and the Identity Preference Optimization (IPO) framework of , which was introduced as a more robust alternative to DPO. To further support these empirical observations, we provide an extensive theoretical analysis that both (i) sheds more light on the degenerative tendencies of DPO and issues inherent to its objective, and (ii) highlights relative advantages of our explicitly regularized approaches.

## 2 Preliminaries

We begin with a brief review of Direct Preference Optimization (DPO)  and its analysis. Proofs of all theoretical results provided here, and in the rest of the paper, are deferred to Appendix A.

### The preference alignment problem

Let \(x\) be an input prompt, and let \(y_{}( x)\) be the language model policy \(_{}\)'s response to \(x\). Given some reward function \(r^{*}(x,y)\) and another reference policy \(_{}(y x)\), the goal of alignment is to solve for the "aligned" policy \(_{^{*}}(y x)\) that maximizes the following RLHF objective, i.e.,

\[_{^{*}}(y x)=*{argmax}_{_{}}_{ (x)}[_{_{}(y x)}[r^{*}(x,y)]-_{ }[_{}( x)\|_{}( x)]],\] (1)

where \((x)\) is a fixed distribution over prompts, and the KL-divergence term prevents the aligned policy from being dramatically different from the anchoring reference policy, \(_{}(y x)\). Here, the reward function \(r^{*}\) is typically not known in advance, but rather inferred from collected human preference data in the form of \((x,y^{w},y^{})\), where \(x\) is the prompt, \(y^{w}\) is the "winning", or preferred, response, and \(y^{}\) is the "losing", or dispreferred, response. A common approach is to assume that pairs \((y_{1},y_{2})\) follow a Bradley-Terry model , under which the probability that \(y_{1}\) is preferred to \(y_{2}\) given the reward function \(r^{*}\) and prompt \(x\) is \(p^{*}(y_{1} y_{2} x)=(r^{*}(x,y_{1})-r^{*}(x,y_{2}))\), where \(()\) is the sigmoid function and \(\) denotes preference. Under this model, we can use the preference data \((x,y^{w},y^{})_{}\) to estimate \(r^{*}\) via maximum likelihood estimation, i.e.,

\[*{argmin}_{r}_{(y^{w},y^{},x) _{}}[-(r(x,y^{w})-r_{}(x,y^{})) ].\] (2)

With \(\) in hand, Eq. (1) can be optimized using standard reinforcement learning algorithms [27; 29; 6].

### Direct preference optimization

DOP is a simple approach for offline policy optimization that uses preferences to directly align the language model policy, without training an intermediate reward model. Specifically, DPO leverages the fact that the optimal solution to the KL-constrained objective in (1) takes the form 

\[_{^{*}}(y x)=_{}(y x)( r^{*}(x,y)),\] (3)

where \(Z(x)=_{y}_{}(y x)(r^{*}(x,y))\) is the partition function. DPO reparameterizes the true reward function \(r^{*}\) in terms of the optimal policy \(_{^{*}}\) that it induces, i.e.,

\[r^{*}(x,y)=(}(y x)}{_{}( y x)})+ Z(x).\] (4)

Under the Bradley-Terry model, the likelihood that \(y_{1} y_{2}\) can then be written as

\[p^{*}(y_{1} y_{2} x)=(}(y_{ 1})_{}(y_{2})}{_{^{*}}(y_{2})_{}(y_{1} )}),\] (5)

where now \(_{^{*}}\) can be directly estimated on \(_{}\) following the objective in (2), in place of the intermediate reward model \(\), i.e., \(_{}(y x)_{_{}}_{ }(_{};_{})\) where

\[_{}(_{};_{})=_{(y^{w},y^{},x)_{}}[-( }(y^{w})_{}(y^{})}{_{ ^{*}}(y^{})_{}(y^{w})})].\] (6)

### Pitfalls of direct preference optimization

As argued in , the Bradley-Terry assumption that DPO strongly relies on for maximum likelihood estimation is sensitive to the underlying preference data. Specifically, if we have any two responses \(y_{1}\) and \(y_{2}\) where \(p^{*}(y_{1} y_{2} x)=1\), then the Bradley-Terry model dictates that \(r^{*}(y_{1})-r^{*}(y_{2})=+\), and therefore \(_{^{*}}(y_{2} x)=0\) for _any_ finite KL-regularization strength \(\).

We can illustrate this phenomenon on a broader level with the following example.

**Assumption 1**.: _Suppose we are given a preference dataset of (context-free) pairs \(_{}=\{(y_{i}^{w},y_{i}^{})\}_{i=1}^{n}\), the pairs \((y_{i}^{w},y_{i}^{})\) are mutually disjoint in both the elements. Further suppose that we optimize the DPO objective on \(_{}\) with a single parameter \(_{y}\) for each \(y\)._

**Proposition 1**.: _Under Assumption 1, for any \((y,y^{})\) such that \(y=y_{i}^{w}\) and \(y^{}=y_{i}^{}\) for some \(i\), we have \(}(y)_{}(y^{})}{_{^{*}}(y^{ })_{}(y)}\), for all global minimizers \(_{^{*}}\) of the DPO objective in (6), for any \(>0\)._

**Corollary 1**.: _Under Assumption 1, further assume that \(0<_{}(y)<1\) for all \(y\). Then \(_{^{*}}\) is a global minimizer of the DPO objective in (6) iff \(_{^{*}}((y^{})^{c}) 1\) with \(_{^{*}}(y_{i}^{w})>0\  i[n]\), where \((y^{})^{c}\) is the complement of the set of all responses \(y\) that appear as a disreferred \(y_{i}^{}\) for any \(i[n]\)._

Additional analysis of the training dynamics of DPO is also provided in SS5. A significant, and non-obvious, implication of Corollary 1 is that the set of global optima of the DPO loss also includes policies that can shift nearly all probability mass to responses that never even appear in the training set--and even assign near zero probability to all of the training data responses that do in fact correspond to winning generations, \(y^{w}\), a phenomenon that has been observed empirically [e.g., 20]. Stated differently, Corollary 1 implies that any \(^{*}\) merely satisfying \(_{^{*}}(y_{i}^{})=0\) with \(_{^{*}}(y_{i}^{w})>0\  i[n]\) is a global minimizer of the DPO objective in this setting. Though simplistic, the scenario in Assumption 1 is closer to reality than might first be appreciated: in many practical situations we can almost always expect the finite-sample preference data to contain one (or at most a few) preference annotations per example \((x,y_{1},y_{2})\), while the policies \(_{}\) can have billions of parameters (\( n\)). Of course, this issue can also be viewed as a classic instance of overfitting--with the additional caveat that as opposed to _overpredicting_ responses within the training set, we might overfit to _almost never_ producing anything like the "good" responses that do appear within the training set. Furthermore, without additional regularization (beyond \(\)), we can expect this degeneration to easily happen in typical preference datasets.

## 3 Uncertainty-aware reward model distillation

As discussed in the previous section, a core issue in preference optimization is that the true preference distribution \(p^{*}(y_{1} y_{2} x)\) is not known. Attempting to infer it from finite-sample preference data (that may further be biased or out-of-distribution with respect to the target domain) can then result in a failure to learn reasonable policies. In this section, we now propose an inherently regularized approach to direct preference optimization that uses uncertainty-aware reward model distillation.

### Reward model distillation

Suppose for the moment that the reward function \(r^{*}\) was in fact known, and did not have to be inferred from sampled preference data. Under this setting, we can then define an efficient offline optimization procedure that is similar in spirit to DPO, but no longer relies directly on a preference dataset. Concretely, given unlabeled samples \((x,y_{1},y_{2})\) (where the number of samples can be potentially unlimited), we can define a simple "distillation" loss, \(_{}(r^{*},_{})\), as follows:

\[_{}(r^{*},_{};)=_{(x,y_{ 1},y_{2})}[(r^{*}(x,y_{1})-r^{*}(x,y_{2})-(y_{1} x)_{}(y_{2} x)}{_{}(y_{2} x)_{ }(y_{1} x)})^{2}].\] (7)

Intuitively, the distillation loss seeks to exactly match _differences_ in reward model scores across all generation pairs \((x,y_{1},y_{2})\). It is then easy to see that under the Bradley-Terry model, this is equivalent to matching the strength of the preference relationship, \(y_{1} y_{2}\). Furthermore, by only matching differences, we can still conveniently ignore the log partition term, \( Z(x)\), in the implicit reward formulation for \(_{}\) as shown in (4), as it is constant across different \(y\) for any given \(x\). Finally, similar to the motivation in DPO, we can show that minimizing \(_{}(r^{*},_{};)\) indeed results in an optimally aligned policy \(_{^{*}}\), as long as the data distribution \(\) has sufficient support.

**Theorem 1**.: _Let \(\) denote the set of all possible responses for any model \(_{}\). Assume that \((_{}(y x))=\), i.e., the reference policy may generate any outcome with non-zero probability. Further, let \(((x,y_{1},y_{2}))=((x)) \). Let \(_{^{*}}(y x)_{_{}}\,_{ }(r^{*},_{};)\) be a minimizer over all possible policies, of the implicit reward distillation loss in (7), for which \(r^{*}(x,y)\) is assumed to be deterministic, and finite everywhere. Then for any \(>0\), \(_{^{*}}\) also maximizes the alignment objective in (1)._

The above result holds for a broad class of data distributions \((x,y_{1},y_{2})\), and makes no assumptions on \(r^{*}\) (e.g., it is no longer necessary for it to be defined using a Bradley-Terry model). In fact, this result can also be seen as strict generalization of the IPO framework of  when taking \(r^{*}(x,y)\{y=y_{w}\}\), if labeled pairs \((x,y_{w},y_{l})\) are provided instead of the unlabeled pairs \((x,y_{1},y_{2})\).

Of course, the true reward \(r^{*}\) is usually not known in practice. Still, as in standard RLHF, we can go about constructing good proxies by using the preference data to identify plausible target reward models \(r_{}\)--further guided by any amount of regularization and inductive bias that we desire. A natural choice is to first learn \(r_{}\) on the preference data \(_{}\) using standard methods, and then reuse \(_{}\) to distill \(_{}\), which is similar to classical settings in teacher-based model distillation [14; 26]. Furthermore, as \(r_{}\) is a real-valued model, at a bare minimum it is guaranteed to induce a regularized Bradley-Terry preference distribution \(p_{}(y_{1} y_{2} x)>0,\; x,y_{1},y_{2} \), and thereby avoid some of the degeneracies identified in SS2.3 for the maximum likelihood estimate under DPO.

### Pessimistic reward model distillation

Choosing a single reward model \(r_{}\) for anchoring the LM policy can naturally still lead to degenerate behavior if \(r_{}\) is a poor approximation of the true \(r^{*}\) that accurately reflects human preferences. However, we can easily extend our framework to handle uncertainty in the right target reward function by defining a confidence _set_ of \(k 1\) plausible target reward models, \(=\{r_{}^{1},,r_{}^{k}\},\) and training \(_{^{*}}(y x)\) to maximize the following "pessimistic" form of the objective in (1):

\[_{_{}}_{r_{}^{i}}_{(x) }_{_{}(y x)}[r_{}^{i}(x, y)]-_{_{}(y x)}[r_{}^{i}(x,y)]}_{ }-_{}(_{}(  x)\|_{}( x)).\] (8)In this pessimistic objective we are no longer optimizing \(_{}\) for a single reward, but optimizing \(_{}\) to produce generations that are scored favorably on average, even by the worst-case reward model in the set \(\), relative to the generations of the baseline policy \(_{}\).When the set \(=\{r^{*}\}\) consists of only the ground-truth reward, the objective (8) is equivalent to standard RLHF (1), up to a constant offset independent of \(\). More generally, whenever \(\) includes a good proxy \(\) for \(r^{*}\), the pessimistic advantage evaluation ensures that the the policy \(_{}^{*}\) that maximizes eq.8 still has a large advantage over \(_{}\) under all \(r\), including \(\). This use of pessimism to handle uncertainty in the knowledge of the true reward is related to similar techniques in the offline RL literature [16; 5].

For the objective to be meaningful, the set \(\) has to be chosen carefully. When \(\) is small, it might not include any good proxy for \(r^{*}\). Conversely, if \(\) is too rich, it forces \(_{^{*}}\) to be nearly identical to \(_{}\), since any deviations from \(_{}\) might be penalized by some reward model in \(\). Consequently, we want to design \(\) to be the smallest possible set which contains a reasonable approximation to \(r^{*}\).

To optimize (8), it turns out that we can formulate it as an equivalent constrained offline optimization problem, that we will show to conveniently admit a similar loss form as (7).

**Theorem 2** (Pessimistic distillation).: _Define the constrained minimizer_

\[_{^{*}}(y x)*{argmin}_{_{}_{}()}_{(x)}_{}(_{ }( x)\|_{}( x)),\] (9)

_where \(_{}()\) is the set of all possible policies with implicit reward models that are consistent with any target reward model \(r^{i}_{}\), i.e., \(_{}()\{_{_{i}}\}_{i=1}^{| |}\) where \(_{_{i}}_{}(y x)r^{i}_{ }(x,y)\). Then for any \(>0\), \(_{^{*}}\) also maximizes the pessimistic alignment objective in (8)._

To unpack this result, Theorem 2 stipulates that the \(_{}\) that maximizes the pessimistic objective in (8) is the policy in \(_{}()\) that is closest in _forward_ KL-divergence to \(_{}\) (see Figure 1).1 In addition, this policy also maximizes the expected reward of one of the \(r^{i}_{}\) (minus the additional weighted reverse KL-divergence penalty term). Intuitively, the forward KL-divergence term serves the role of biasing the model towards optimizing for reward models that are similar to the implicit reward that \(_{}\) already maximizes. Otherwise, there might exist a target reward model \(r^{i}_{}\) for which the advantage of \(_{}\) relative to \(_{}\) will be low, or even negative (a solution that we would like to avoid).

#### 3.2.1 Optimization

The constraint in (9) can then be relaxed and approximately optimized by introducing an objective with a Lagrangian-style penalty with strength \(>0\) on a form of distillation loss as (7), i.e.,

\[_{_{}}_{(x)}_{}(_{ }(y x)\|_{}(y x))+_{r^{i}_{}}_{}(r^{i}_{},_{ };),\] (10)

where in practice we divide by \(\) and instead optimize2

\[_{}(,_{};)=_{r^{i}_{ }}_{}(r^{i}_{},_{};)+_{(x)}_{}(_{ }( x)\|_{}( x)),\] (11)

where \(=^{-1}\). In reality, minimizing (11) for \(>0\) is equivalent to solving the constrained optimization problem in (9) with an implicitly larger set of possible reward models \(_{}\) indexed by \(\). More specifically, \(_{}\) also contains all reward models \(\) that are approximately consistent with the anchoring reward models \(r^{i}_{}\) contained in \(\), as the following result states.

Figure 1: A toy illustration of Theorem 2, which states that the optimal \(_{^{*}}\) for (8) is the policy in \(_{}()\) with the lowest forward-KL from \(_{}\). The set \(_{}()\) contains a (potentially infinite) set of policies \(_{1},_{2},\) corresponding to target reward models. Here, \(_{}\) assigns equal mass to \(y^{w}\) and \(y^{}\), \(_{}\) is the MLE solution for the DPO objective, which puts all probability mass on \(y^{w}\), and \(_{3}\) is the policy in \(_{}()\) with lowest forward-KL.

**Proposition 2** (Soft pessimistic distillation).: _Assume the same conditions as Theorem 1. Then for any \(0<<\), there exists a \( 0\) such that \(_{^{*}}(y x)*{argmin}_{_{}}_{ *{pdistill}}(,_{};)\), where \(_{^{*}}\) is a minimizer over all possible policies, is a solution to (9) for the effective reward model set_

\[_{}=_{r_{}^{i}} _{(x,y_{1},y_{2})}[r_{}^{i }(x,y_{1})-r_{}^{i}(x,y_{2})-(x,y_{1})+(x,y_{2} ))^{2}]}.\] (12)

As a result, optimizing (11) even when using the singleton \(=\{r_{}\}\) yields an implicitly pessimistic objective, in which the pessimism is over all reward models \(\) that are consistent up to \(\) with \(r_{}\).

### Pessimistic DPO

We can also observe that Proposition 2 can be leveraged to obtain an alternative, implicitly pessimistic, objective that uses DPO directly instead of distillation. Consider the following regularized DPO loss:

\[_{}(_{};_{})=_{}(_{};_{})+_{ (x)}_{}(_{}(y x)\|_{}(y  x)).\] (13)

Following a similar analysis as in Proposition 2, we can derive that this implicitly corresponds to maximizing the pessimistic objective in (8) for the reward model set

\[_{}=r_{_{}}_{} (_{};_{})_{_{}^{}} _{}(_{}^{};_{})+ },\] (14)

where \(r_{_{}}(x,y)_{}(y x)/_{}(y x)+ Z(x)\) is the implicit reward model defined by \(_{}\). \(_{}\) then corresponds to the set of reward models \(r_{_{}}\) that are all approximate minimizers of the DPO loss. This not only includes the MLE, but also all other estimators that obtain nearly the same loss. In principle, this can be expected to help ameliorate some of the issues of SS2.3: since driving the reward to \(\) only marginally decreases the \(_{}\) loss past a certain point, the set \(\) will also include finite reward functions \(|r_{_{}}(x,y)|<\) for any \(>0\). These rewards would then be preferred if they induce a policy with a smaller (forward) KL-divergence to \(_{}\) than the degenerate, infinite rewards.

## 4 Experimental results

The main motivation for reward distillation and pessimism is to increase alignment robustness in challenging settings where it is difficult to learn good policies directly from the preference data. To demonstrate the effectiveness of our approach, we run experiments on the popular TL;DR summarization task [29; 32], in which we simulate a scenario where the preference data has a spurious correlation between the _length_ of a summary and whether or not it is preferred.3

### Experimental setup

We first train an "oracle" reward model on the TL;DR preference data training set  and relabel all preference pairs with this oracle. This enables us to use the oracle reward model for evaluation, without worrying about the gap to true human preferences. After relabeling, longer responses (where longer is defined as \(y_{1}\) having at least \(10\%\) more tokens than \(y_{2}\)) are preferred in \(61\%\) of the examples.

To test the effect of a spurious correlation on preference-based policy optimization, we select as a training set 30K examples from the relabeled data such that the longer output is preferred in \(\) fraction of examples, with \(\{0.2,0.3,0.4,0.5,0.6,0.7,0.8\}\). Each such training set is denoted \(_{}\). At each \(_{}\), we compare our approach to DPO  and IPO , which are currently the most commonly used offline alignment methods. We test the following variants of distillation and pessimism:

* **Distilled DPO** (d-DPO): Trains a reward model \(r_{}\) on \(_{}\), and then optimizes \(_{}(r_{},_{};)\).
* **Pessimistic DPO** (p-DPO): A pessimistic version of DPO as described in SS3.3, trained on \(_{}\).
* **Pessimistic Distilled DPO** (pd-DPO): Combines the above two by training a reward model \(r_{}\) on \(_{}\) and optimizing the pessimistic distillation objective (Eq. (11)) with confidence set \(=\{r_{}\}\).
* **Pessimistic Ensemble DPO** (e-DPO): To create ensembles of reward models, we subsample from each \(_{}\) five preference datasets, \(_{,b}\), at \(b=\{0.2,0.4,0.5,0.6,0.8\}\), such that the fractionof pairs where the longer response is preferred is \(b\), and train reward models \(r_{,b}\) on those subsets. Consequently, sensitivity to length should vary across ensemble members. We then apply the same procedure as pd-DPO above, with a confidence set \(_{}=\{r_{,b}\}_{b=1}^{}\).

All reward models and policies are initialized from Palm-2-XS . Policies also go through a supervised finetuning step on human-written summaries from the original TL;DR training set  prior to alignment, and we term this policy \(_{}\). We evaluate performance by sampling summaries for test set prompts, evaluating the average reward according to the oracle reward model, and computing the advantage in average reward compared to \(_{}\) (before alignment). We train policies for \(10^{4}\) steps with batch size \(16\) and learning rate \(10^{-6}\), and reward models for \(3k\) steps with batch size \(64\) and learning rate \(4 10^{-6}\). We use the validation set for model selection during policy training and to choose the following hyperparameters. For all DPO variants, we sweep over \(\{.01,.1,1,3,10,30,100\}\). For IPO, we sweep over \(\{0.01,0.1,1,3,5,10,25\}\). For all pessimistic methods we anneal \(=/\) from \(10^{-4}\) to \(10^{-2}\) linearly during the \(10k\) training steps.

### Results

We present the results of our experiment in Figure 2. As can be seen in the plot, the more challenging setting is when \(<0.5\), which corresponds to a sample of preference annotations in which shorter outputs are generally preferred. This distribution shift is more difficult because as mentioned the oracle reward model (trained on human annotations) has a bias in favor of longer outputs . Nevertheless we get sizable improvements compared to the reference policy \(_{}\) for all length bias values.

All approaches that invoke distillation (d-DPO, e-DPO, dp-DPO) outperform IPO and DPO (\(p<.01\) by a Wald test) for \( 0.5\), where shorter responses are preferred. Pessimistic ensemble DPO (e-DPO) performs particularly well in these settings, generally outperforming all methods that use a single reward model. When longer responses are preferred (\(>0.6\)), single reward distillation (d-DPO) leads to the highest performance, significantly outperforming both DPO and IPO (\(p<.01\) by a Wald test). Interestingly, p-DPO does not provide empirical benefits relative to the distillation based methods, indicating that the distillation loss itself is quite important. For the effect of hyper-parameter selection, see Figure D.1. In DPO-based methods, the optimal value of \(\) is inversely correlated with the bias; in IPO the same holds for the \(\) hyperparameter.

To better understand the utility of reward ensembles in e-DPO, in particular when \(<0.5\), we examine the role of each reward model in the ensemble across different biases. Specifically, given the final e-DPO policy per length bias, for each example we identify the reward model \(r_{,b}\) that best matches the implicit reward of this policy, i.e., for which reward model is \(_{}\) minimized on that example (see Eq. (7) and (11)). We find that when the policy is trained on data where shorter preference are preferred (\(<.5\)), the reward model that best matches the policy often has the opposite bias (\(b\) is high), and vice versa. Thus, the success of e-DPO may be explained by its ability to distill from reward models that do not suffer from the bias in the policy training data, which is particularly

Figure 2: **Main results, showing the advantage in oracle reward compared to the initial finetuned policy. Errorbars correspond to bootstrap 95% confidence intervals for finite sample variance. Ensemble DPO (e-DPO) is significantly better than DPO and IPO in the challenging setup where shorter responses are preferred (\( 0.5\)), and is generally the best-performing method overall in this regime. Distilled DPO (d-DPO) performs best when longer responses are preferred (\(>0.6\)).**

helpful when \(.5\) as this bias is also not shared by the oracle RM. We provide the full distribution over reward models for all \(\) and \(\) in App. C. Overall, these results demonstrate the efficacy of training a policy by distilling from a reward model in the presence of distribution shifts, and that a careful design of an ensemble to mitigate spurious correlations can lead to further performance gains.4

## 5 Theoretical analysis

This section characterizes problems with the DPO objective and solutions offered by pessimistic DPO and distillation, focusing on the simplified scenario in which we optimize with respect to a single preference pairs \((y^{w},y^{})\). Once again, all proofs are deferred to Appendix A.

In its Lagrangian formulation, pessimistic DPO adds a forward KL term to the DPO objective (SS3.3). For the sake of analysis, we assume that the preference annotations are sampled from the reference distribution, \((x)_{}(y x)_{}(y x)\). Then a finite-sample approximation of the forward KL term is \(():=_{(y^{w},y^{})_{}}-( _{}(y^{})+_{}(y^{w})).\) By applying this finite-sample approximation, _p-DPO has a finite optimum, unlike DPO_, as shown in Proposition 1. Note that this analysis is limited in two ways: (1) as mentioned, we compute the KL term over the completions in the preference data; (2) we directly optimize the probability ratios \(_{w}=_{}(y^{w})/_{}(y^{w})\) and \(_{}=_{}(y^{})/_{}(y^{})\), rather than optimizing them jointly through the parameters. For sufficiently expressive \(_{}\), however, this approximation captures the behavior of the two algorithms reasonably well.

**Proposition 3**.: _Let \(}_{}\) represent a finite-sample approximation to \(_{}\) with the empirical forward KL term \(()\). For a fixed \(_{}(y^{w}_{i})\) and \(>1\), the \(*{argmin}_{_{}(y^{})}}_{}\) is \((1-_{}(y^{w}_{i}),_{}(y^{}_{i}))\), with \(_{}(y^{}_{i})=-(-1) +_{}(y^{w}_{i})+}(y^{}_{i}) }{_{}(y^{}_{i})}.\)_

The optimum in Proposition 3 corresponds to \(_{w}/_{}=^{-1}(-1)\). Recall that IPO seeks to assign a constant value to this ratio by minimizing \((}{_{}}-^{-1})^{2}\); the (unconstrained) optima are identical for \(^{-1}:=^{-1}(-1)\), but the loss surfaces are different (see Appendix B). DPO sets \(_{}(y^{}_{i}) 0\), as shown in Corollary 1; this is due not only to competition from \(_{}(y^{w}_{i})\) but from DPO penalizing positive probability on \(y^{}_{i}\). Analysis of the distilled loss gives a similar result:

**Proposition 4**.: _For any fixed \(_{}(y^{w}_{i})\) and \(>0\), the \(*{argmin}\) of the distilled DPO objective (eq. (7)) is \((1-_{}(y^{w}_{i}),_{}(y^{}_{i})\), with \(_{}(y^{}_{i})=(r_{t}(x,y^{}_{i})-r_{ t}(x,y^{w}_{i}))+_{}(y^{w}_{i})+}(y^{}_{i})}{_{}(y^{}_{i})}.\)_

While the setting is simplistic, the results are comforting: here the additional regularization effects of both distillation and pessimism (in the case of p-DPO) clearly help to avoid degenerate optima.

Why DPO can drive \((y^{w})\) to zero.In SS2.3 we pointed out a peculiarity of the DPO global optima: in certain cases, it can include policies where \((y^{w})\) may be nearly \(0\) for all \(y^{w}\) in the training set. This undesirable behavior has also been observed in practice [20; 22; 30]. For intuition on why this may happen, consider the simplified case where the policy is a bag-of-words model, \(_{}(y)(c(y))\) for \(c(y)\) representing a vector of counts in \(y\) and \(_{i}\) representing the unnormalized log-probability of token \(i\). Then we can formally show that DPO optimization monotonically decreases an upper bound on the probability of the _preferred_ completion, \(_{^{(t-1)}}(y^{w})_{^{(t)}}(y^{w}) _{^{(t)}}(y^{w})\).

**Proposition 5**.: _Let \(y^{w},y^{}^{n}\) be preferred vs. dispreferred outputs of length \(n\), with \(_{}(y^{w}),_{}(y^{})>0\) and corresponding count vectors \(c(y^{w}),c(y^{})\). Let \(_{}(y)=c(y)-nZ()\) for \(Z()=_{i}^{}e^{_{i}}\), with upper bound \(_{}(y)=c(y)-n_{j}_{j}\). Let \(^{(t)}\) represent the parameters of \(\) after \(t\) steps of gradient descent on \(_{}(\{y^{},y^{w},x\})\), with \(^{(0)}=0\). Then \(_{^{(t)}}(y^{w})_{^{(t)}}(y^{w}) _{^{(t-1)}}(y^{w})\) for all \(t\)._

Where does the probability mass go?If \(_{^{(t)}}(y^{w})\) decreases in \(t\), what other strings become more probable? In the following proposition, we show that under the bag-of-words model, DPO optimization moves probability mass away from \(y^{w}\) to sequences that contain only the tokens that maximize the difference between \(y^{w}\) and \(y^{}\). This is a concrete example of the type of undesirable optima described in SS2.3, now shown here to be realizable.

**Proposition 6**.: _Let \(y^{w}\) and \(y^{}\) be preferred / dispreferred outputs of length \(n\). Let \(=c(y^{w})-c(y^{})\) be the difference in unigram counts. Let \(=[i,i,,i]\), for \(i\), with \(||c()||_{1}=n\). Then \(_{^{(i)}}(y^{w})-_{^{(i)}}()=(t)k\) for some \(k 0\) and some non-decreasing \(:_{+}_{+}\)._

We have \(k=0\) when \(c(y^{w})=c()\), and \(k 0\) when \(||c(y^{w})||_{2}||c()||_{2}=n\) (dense \(c(y^{w})\)) and \(||||_{2}=||||_{}\) (sparse \(\)). This implies that when \(y^{w}\) and \(y^{}\) are similar, \(_{}(y^{w})\) will degrade more rapidly. Early stopping will therefore tradeoff between reaching the degenerate solution on such cases, and underfitting other cases in which \(y^{w}\) and \(y^{}\) are more distinct.

## 6 Related work

Recent work in offline alignment has focused on DPO  as a simpler alternative for aligning language models from preference data. Subsequent work has identified issues with DPO, including weak regularization  and a tendency to decrease the probability of winning generations during training . Other methods have explored various avenues for improvement. These include analyzing the impact of noise on DPO alignment , proposing to update the reference policy during training , and suggesting a variant of IPO with a per-context margin . Additional research has focused on token-level alignment methods [38; 22] and on developing a unified view of various offline alignment methods . This work builds upon several these findings, and provides further analysis, as well as a solution based on pessimism and reward distillation.

While offline alignment methods are popular, recent evidence suggests that online alignment methods such as RLHF [6; 29], may lead to more favorable outcomes [13; 30; 8; 34]. Notably, Zhu et al.  proposed iterative data smoothing, which uses a trained model to softly label data during RLHF. Whether online or offline, however, policies are still susceptible to overfitting to certain degenerate phenomena. To this end, reward ensembles have been widely investigated recently as a mechanism for tackling reward hacking in RLHF [9; 7; 39; 25], and in the context of multi-objective optimization [19; 24]. We use an ensemble of rewards to represent the uncertainty with respect to reward models that are suitable given preference data. Moskovitz et al.  focus on "composite" rewards, with the goal of achieving high task reward while ensuring that every individual component is above some threshold--also by applying a Lagrangian relaxation. In this work, we also consider multiple reward models, but we only focus on cases where there is no known, obvious reward decomposition.

Finally, the question of using a small amount of offline data to learn high-quality policies, instead of online access to reward feedback, has been widely studied in the offline reinforcement learning (RL) literature. The predominant approach here is to use pessimism, that is, to learn a policy with the highest reward under all plausible environment models consistent with the data, with an extensive theoretical [18; 37; 33] and empirical [16; 5; 36] body of supporting work. The key insight in this literature is that without pessimism, the RL algorithm learns undesirable behaviors which are not explicitly ruled out in the training data, and pessimism provides a robust way of preventing such undesirable extrapolations, while still preserving generalization within the support of the data.

## 7 Conclusion

LM alignment is crucial for deploying safe and helpful assistants, but is difficult due to lack of access to perfect preference oracles. We presented a thorough theoretical analysis of some of the degeneracies that DPO is susceptible to when learning from sampled human preference data. Furthermore, our findings suggest that explicit reward modeling remains a powerful vehicle for introducing regularization into post-training. By distilling the reward assigned by a single, explicit reward model--or a family of explicit reward models--directly into the implicit reward maximized by our policies using offline data, we demonstrated that we can achieve improved robustness to variations in preference dataset quality, while maintaining the simplicity of the DPO framework.

**Limitations.** The empirical results in the paper are based on one dataset and form of distribution shift. For deeper understanding of pessimism and ensembling, additional settings should be explored. The theoretical aspects of the paper are sometimes based on restrictive assumptions and simplifications. Nonetheless, they provide potential explanations for phenomena observed in real-world settings.

**Broader impact.** We introduce new ideas to the active field of research on preference-based post-training, which we hope will help facilitate the alignment of large models, and improve understanding of current approaches--ultimately supporting the development of capable and reliable AI systems.