# FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models

Tong Wu\({}^{1,2}\), Yinghao Xu\({}^{1}\), Ryan Po\({}^{1}\), Mengchen Zhang\({}^{3,5}\), Guandao Yang\({}^{1}\),

Jiaqi Wang\({}^{5}\), Ziwei Liu\({}^{4}\), Dahua Lin\({}^{2,5,6}\), Gordon Wetzstein\({}^{1}\)

\({}^{1}\) Stanford University \({}^{2}\) The Chinese University of Hong Kong \({}^{3}\) Zhejiang University

\({}^{4}\) S-Lab, NTU \({}^{5}\) Shanghai Artificial Intelligence Laboratory \({}^{6}\) CPII under InnoHK

###### Abstract

Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from source images. Current methods attempt to distill identity and style from source images. However, "style" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes like lighting and dynamics. Additionally, a simplified "style" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, letting users apply characteristics like lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes 1 M high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attributes adaptation framework (FiVA-Adapter), which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements. The data and models will be released at https://huggingface.co/datasets/FiVA/FiVA.

## 1 Introduction

Imagine an artist drawing a picture; he or she not only exhibits unique styles, identities, and spatial structures but also frequently integrates personal elements such as brushstrokes, composition, and lighting effects into their creations. These detailed visual features capture profound personal emotions and artistic expressions. However, despite the capability of current text-to-image models to generate high-quality images from textual or visual prompts, they encounter substantial challenges in effectively controlling these fine-grained visual concepts, which vary widely across different artistic domains. This limits the practical applications of text-to-image models in various fields.

Recently, significant research efforts have been made to advance controllable image generation. In particular, numerous studies have explored the use of _personalization_ techniques to preserve the identity of an object or person across different scenarios . On the other hand, there have been attempts to control the generation process by conditioning on the _style_ and _spatial structure_ of reference images , which involves mimicking abstract styles or leveraging spatial cues such as edge maps, semantic masks, or depth. Yet, these methods are limited to specific aspects and fall short in terms of generalizability. These methods use complex images as unified referenceswithout adequately disentangling visual attributes, resulting in a lack of control over generation based on specific attributes of the conditional images. This underscores the importance of extracting fine-grained visual concepts from images to achieve controllable generation. However, achieving this requires a well-annotated image dataset that displays various fine-grained attributes within the images and a unified framework to adapt these different attributes to facilitate generation.

In this paper, we present a comprehensive fine-grained visual attributes dataset (FiVA), featuring image pairs that meticulously delineate a variety of visual attributes, as shown in Figure 1. Instead of annotating the real-world images, we propose to leverage advanced 2D generative models within an automated data generation pipeline for data collection. We develop a systematic approach that includes attribute and subject definition, prompt creation, LLM-based filtering, and human validation to construct a dataset annotated with diverse visual attributes across 1 M image pairs. Building on this dataset, we introduce a fine-grained visual attributes adaptation framework (FiVA-Adapter) designed to control the fine-grained visual attributes during the generation process. Specifically, we propose to integrate a multimodal encoder, Q-former, into the image feature encoder before its insertion into the cross-attention modules. This integration aids in understanding tags or brief instructions for extracting image information. During inference, our approach allows for the isolation of specific visual attributes from the reference image before applying them to a target subject, and even the combination of different attribute types. This capability enables free and diverse user choices, significantly enhancing the adaptability and applicability of our method.

We conduct extensive experiments across a variety of attribute types on both synthetic and real-world test sets. Our results demonstrate that our method outperforms baseline methods in terms of precise controllability in attribute extraction, high textual alignment regarding the target prompt, and the flexibility to combine different attributes. This work aims to cater to an increasingly diverse array of user needs, recognizing the rich informational content that visual media embodies. Our hope is that it will pave the way for innovative applications that harness the full potential of visual attributes in myriad contexts.

Figure 1: **Overview. We propose the FiVA dataset and adapter to learn fine-grained visual attributes for better controllable image generation.**

Related Words

**Image Generation with Diffusion Models** Diffusion models have seen significant advancements in the field of image generation, driven by their impressive generative capabilities. DDPM  employs an iterative denoising process to transform Gaussian noise into images. The Latent Diffusion Model  enhances traditional DDPMs by employing score-matching in the image's latent space and introducing cross-attention-based controls. Rectified flow  improves training stability and enhances image synthesis by introducing flow matching. Another line of research focuses on architectural variants of diffusion models. The Diffusion Transformer  and its variants [10; 2; 3; 4] replace the U-Net backbone with transformers to increase scalability on large datasets.

**Controllable Image Generation** The ambiguity of relying solely on text conditioning often results in weak control for image diffusion models. To enhance guidance, some works adopt the scene graph as an abstract condition signal to control the visual content. To allow for more spatial control, several attempts such as ControlNet  and IP-Adapter  propose to build adaptors for visual generation by incorporating additional encoding layers, thus facilitating controlled generation under various conditions such as pose, edge, and depth. Further research investigates image generation under specified image conditions. Techniques like ObjectStitch , Paint-by-Example , and AnyDoor  leverage the CLIP  model and propose diffusion-based image editing methods conditioned on images. AnimateAnyone  suggests using a reference network to incorporate skeletal structures for image animation. However, these methods primarily focus on the visual attributes like identity and spatial structure of reference images. Other works such as DreamBooth , CustomDiffusion , StyleDrop , and StyleAligned  approach controlled generation by enforcing consistent style between reference and generated images. Yet even among these methods, the definition of style is inconsistent. We propose a fine-grained visual attribute adapter that decouples and adapts visual attributes from one or more source images into generated images, enhancing the specificity and applicability of the generated content.

**Datasets for Image Generation** Datasets provide a foundation for the rapid progress of diffusion models in image generation. MS-COCO  and Visual Genome , built with human annotation, are limited in scale to 0.3 and 5 million samples, respectively. The YFCC-100M  dataset scales up further, containing 99 million images sourced from the web with user-generated metadata. However, the noisy labels from the web often result in text annotations that are unrelated to the actual image content. The CC3M  and its variant CC12M  utilize web-collected images and alt-text, undergoing additional cleaning to enhance data quality. LAION-5B  further scales up the dataset to 5 billion samples for the research community, aiding in the development of foundational models. DiffusionDB  leverages large-scale pre-trained image generation models to create a synthetic prompt-image dataset, containing imaged generated by Stable Diffusion using prompts and hyperparameters specified by real users. However, these datasets only provide a coarse description of the images. DreamBooth  and CustomDiffusion , techniques focused on text-to-image customization, also provide their own dataset, containing sets of images capturing the same concept. However, such datasets focus only on individual concepts, and are limited in sample volume. Thus, we propose the fine-grained visual attributes dataset (FiVA) with high-quality attribute annotations to facilitate precise control of generative models with specific attribute instructions from users.

## 3 Dataset Overview

Visual attributes encompass the distinctive features present in photography or art. However, there exists no dataset annotated with fine-grained visual attributes, making the fine-grained control in generative models inapplicable. In the following, we present the details for how to construct the FiVA dataset with fine-grained visual attributes annotation. We first introduce our taxonomy for the fine-grained visual attributes. We then illustrate how to create large-scale text prompts with diverse attributes, which are further used for generating image data pairs using state-of-the-art text-to-image models. Considering the imperfect text-image alignment of current generative models, a data filtering stage is implemented to further refine the precision. Additionally, we conduct thorough human validation to ensure the quality of the dataset. We perform a comprehensive statistical analysis of the compiled dataset in the supplementary material.

### Data Construction Pipeline

**Taxonomy of Visual Attributes.** Visual attributes are a broad concept, varying significantly in different use cases. Therefore, we choose some of the most general types to cover a wide range of application scenarios. Specifically, we categorize these attributes into the following groups: color, lighting, focus and depth of field, artistic stroke, dynamics, rhythm, and design. Within each group, we further refine them into comprehensive subcategories. Initially, we reference examples from professional texts and then use GPT-4 to generate additional entries. We manually filter out any redundant or unreasonable suggestions. (Please refer to the supplementary material for more details). This comprehensive taxonomy enables us to encompass a wide range of applications in photography, art, and design, ensuring that our dataset is versatile and applicable across various visual domains.

**Prompts and Paired Images Generation.** We aim to construct image-image pairs with the same specific visual attribute. However, achieving this by filtering existing text-image datasets, LAION5B , etc. is very challenging due to the ambiguity in textual descriptions, which makes it difficult to accurately select the image pairs from the datasets. Therefore, we opt to construct the dataset using generative models: first, we generate prompts containing various visual attributes in bulk, and then we use state-of-the-art text-to-image models to generate images with these prompts.

For prompt generation, we augment the attribute names and descriptions with GPT-4 based on our taxonomy. Then, we combine each of the augment names or their combinations with specific objects to generate the final prompts. However, this process is non-trivial, as a specific branch of visual attributes may not be suitable for all kinds of random subjects. For example, "motion blur" can only be applied to dynamic subjects, and "candle light" cannot be applied to landscapes. We employ GPT-4 to construct a hierarchical subject tree, with parent nodes representing the primary categories of subjects and child nodes denoting specific objects within these main categories. We also utilize GPT-4 to associate visual attributes with the parent nodes in the subject tree to make the combination of them into a reasonable case. When creating prompts, we select \(n\) (e.g., 1, 2, 3) attributes along with one associated subject according to the subject tree. We construct over 1.5 million initial prompts in total and used playground-v2.5  to generate four images for each prompt. The generated images with the same attribute are considered to be positive pairs. However, we observe some disparity caused by the imperfect alignment with text as well as the different manifestations of the same

   Attributes & Color & Stroke & Lighting & Dynamic & Focus\_and\_DoF & Design & Rhythm & Average \\  Number & 429K & 301K & 370K & 89K & 107K & 66K & 52K & 202K \\ Accuracy & 0.96 & 0.92 & 0.76 & 0.81 & 0.85 & 0.89 & 0.73 & 0.84 \\ Standard Deviation & 0.12 & 0.15 & 0.19 & 0.14 & 0.12 & 0.14 & 0.18 & 0.15 \\   

Table 1: **Statistics and human validation results.** We report the number of individual images containing each specific visual attribute (some images could contain multiple attributes), along with human validation accuracy and cross-agreement measured by standard deviation.

Figure 2: **Examples of visual consistency application range.** Some visual attributes, such as ‘color’ and ‘stroke,’ are easily transferable across different subjects (**left**). However, other attributes, like ‘lighting’ and ‘dynamics,’ are range-sensitive, meaning they produce varying visual effects depending on the domain (**right**), resulting in more fine-grained, subject-specific definitions of sub-attributes.

attribute on different subjects. A further cleaning and filtering process is therefore needed to enhance quality and precision.

**Range-sensitive Data Filtering.** Not all generated images with text prompts containing the same attribute exhibit similar visual effects. Figure 2 illustrates examples of varying application ranges for different attributes where visual consistency is present. To achieve attribute-consistent image pairs, we define specific ranges for each attribute within which two images are highly likely to maintain visual consistency. Utilizing our subject tree, we apply a _range-sensitive data filtering_ approach hierarchically: if an attribute shows high visual consistency across subjects at the parent node, we record it as the range; if results are inconsistent, we proceed to its child nodes for further validation. The validation process involves constructing a \(3 3\) image grid generated with prompts containing the attribute and subjects sampled from a specific node. Using GPT-4, we assess image consistency for a given visual attribute by prompting: "I have a set of images and need to determine if they exhibit consistent <major attribute> traits of <specific attribute>." We ask GPT-4 to identify any inconsistent image IDs. This sampling is repeated multiple times, and if the proportion of inconsistent images remains below a predefined threshold, we consider the images consistent for the specified visual attribute within the given range.For attributes that cannot maintain consistency even at the leaf nodes, we simply remove them.

After filtering, we further compile statistics on the number of individual images for each major visual attribute, as shown in Table 1. Note that these numbers do not add up to the total image count, as each image can possess one or multiple different attributes.

### Human Validation

After filtering the data, we randomly selected 1,400 image pairs (200 per attribute) for human validation. Each pair was evaluated by three of eight trained annotators for attribute similarity, with the final result determined by majority vote. These pairs were randomly matched based on shared attribute descriptions, and annotators focused solely on judging the visual similarity of each pair's attributes--alignment with the original text prompt was not required.

Table 1 displays the accuracy for each attribute type, along with the overall average, underscoring the dataset's high precision and robustness for this study. To assess annotator agreement, all eight annotators also evaluated a subset of 350 image pairs (50 per attribute). The table further includes the standard deviation of these assessments, demonstrating strong consistency among annotators.

## 4 Method

In the following, we will show how we leverage FiVA dataset to build a fine-grained visual attribute adapter (FiVA-Adapter) that enables controllable image generation using images as prompts for specific visual attributes.

### Preliminary

**Image Prompt Adapter** (IP-Adapter)  is crafted to enable a pre-trained text-to-image diffusion model to generate images using image prompts. It comprises two main components: an image encoder for extracting image features, and adapted modules with decoupled cross-attention to embed these features into the pre-trained text-to-image diffusion model. The image encoder employs a pre-trained CLIP  image encoder followed by a compact trainable projection network, which projects the feature embedding into a sequence of features of length \(N\). The projected image features are then injected into the pre-trained U-Net model  via decoupled cross-attention, where different cross-attention layers handle text and image features separately. The formulation can be defined as:

\[Z^{new}=(}{})V+ ()^{T}}{})V^{},\] (1)

where \(Q=ZW_{q},K=F_{t}W_{k},V=F_{t}W_{v},K^{}=F_{v}W^{}_{k},V^{}=F _{v}W^{}_{v}\). \(F_{t}\) and \(F_{v}\) indicate the text and image condition features, respectively. \(W^{}_{k}\) and \(W^{}_{v}\) represent the weight matrices in the new cross-attention layer dedicated to the visual features. These matrices are the sole new parameters to be trained within this module and are initialized from \(W_{k}\) and \(W_{v}\) to facilitate faster convergence. In this paper, we follow the decoupled cross-attention mechanism for adapting image prompts.

**Q-Former** was initially introduced as a trainable module designed to bridge the representation gap between a image encoder and LLM. It comprises two transformer submodules: one functions as an image transformer to extract visual features, and the other serves as a text encoder and decoder. It takes learnable query tokens as input and associate with text via self-attention layers, and with frozen image features through cross-attention layers, ultimately producing text-aligned image features as output. Blip-Diffusion  utilizes Q-Former to derive text-aligned visual representations, allowing a diffusion model to generate new subject renditions from these representations. DEADiff  employs two Q-Formers, each focusing on "style" and "content" to extract distinct features for separate cross-attention layers, thereby enhancing the disentanglement of semantics and styles. It makes the Stylization Diffusion Model follow the'style' of reference images better. In this work, we propose the FiVA-Adapter that focuses on fine-grained control of visual attributes. Thus, disentangling the 'content' and'style' is not our goal.

### Fine-grained Visual Attribute Adapter

Our goal is to generate the target image \(\) following the attribute instruction of visual prompts and the text condition. Specifically, the generation can be formulated as:

\[=(,,y),\] (2)

where \(()\) is the generator, \(=\{_{k}|k[1,N]\}\) represents the visual prompts, \(=\{a_{k}|k[1,N]\}\) denotes the corresponding attribute instructions, and \(y\) is the text prompts.

To achieve our goal, our framework is composed of two key components: 1) _Attribute-specific Visual Prompt Extractor_: A feature extractor that can extract the attribute-specific image condition feature \(_{k}\) from image \(_{k}\) with respect to \(a_{k}\). 2) _Multi-image Dual Cross-Attention Module_: A module can embed both text prompt conditions and multiple image conditions into U-Net with two dedicated Cross-Attention Modules for multi-conditional image generation.

The overall pipeline of our method is illustrated in Figure 3.

**Attribute-specific Visual Prompt Extractor.** Inspired by BLIP-Diffusion , we employ the Q-former module to extract the attribute-specific image condition features. Specifically, the Q-former takes both image \(_{k}\) and attribute instruction \(a_{k}\) as inputs. It models the semantic relationship between the image and the attribute instruction, expected to extract condition feature that aligned with the given attribute. The extracted feature is then further projected by a projector comprises a Linear Layer and LayerNorm into the image condition \(_{k}\) to meet the input channel of the following cross-attention module.

**Multi-image Dual Cross-Attention Module.** IP-Adapter  introduces a Dual Cross-Attention Module where one cross-attention for text prompt condition and the other one for a single-image condition. In our work, we extend this module to adapt to multi-image conditions, facilitating multi-image controls. Specifically, we set a fixed number of attributes \(N\) to control the generated

Figure 3: **FiVA-Adapter architecture and training pipeline.** FiVA-Adapter has two key designs: 1) Attribute-specific Visual Prompt Extractor, 2) Multi-image Dual Cross-Attention Module.

image simultaneously. This allows us to use a fixed number of tokens as input for the cross-attention. The image condition features \(_{k}\) prepared by the Q-Former and channel projector are concatenated into \(=[_{0},_{1},,_{N}]\). Notably, since not all images in our dataset have \(N\) attributes, for a target image I with fewer than \(N\) attributes, we use unconditional image features \(^{zero}\) to pad the sequence. The unconditional feature \(^{zero}\) is generated by feeding a zero-image and an empty text into the Q-Former and followed channel projector. To ensure the condition signals are invariant to the order of prompts, we randomly shuffle the multi-image conditions during training. Finally, the multi-image conditions are passed to the cross-attention module as described in Equation 1.

**Image Prompts and Attribute Sampling.** For a target image characterized by a series of visual attributes and a specific subject, we randomly select reference images from the training set that share the same attribute. We incorporate LLM-based data filtering to enhance data sampling. It ensures that the subjects of sampled image prompts can share the attribute with the target image's subject.

Figure 4: **Qualitative comparisons on single attribute transferring.**

Additionally, we enhance the flexibility of attribute instructions by implementing tag augmentation on the attribute text, broadening the range of instructional keywords to accommodate various user inputs. Specifically, we prepare a list of augmented tags for each attribute using GPT-4. The augmented attribute text for prompt images is then randomly sampled from this candidate list of tags.

**Training and Inference** The training and inference setting of our framework are similar with the IP-Adapter . The learning rate is set to 2e-5 and weight decay is set to 1e-3 for stabling the training. The Q-former, channel projector, and multi-image cross-attention are trained and other parameters are frozen.

## 5 Experiment

### Experimental Settings

**Baselines** We compare our method with the state-of-the-art methods for customized image generation, including Dreambooth-Lora , IP-Adapter , DEADiff , and StyleAligned . Please find the implementation details of both our method and the baselines in the supplementary material.

**Evaluation Metrics** In order to evaluate the results systematically, we propose a small validation set with 100 reference images covering the seven attributes, together with 4 target subjects for each of them. The target subjects include 2 in-domain ones, which are randomly selected from the same major category with the subject from the reference image, and 2 out-domain ones, which are randomly selected from the other major categories. For the evaluation, we focus on two aspects: _attribute

    & DB-Lora & IP-Adapter & DEADiff & StyleAligned & **Ours** \\   & Sub-Acc & 0.393 & 0.163 & 0.605 & 0.520 & **0.817** \\   & Attr\&Sub-Acc & 0.240 & 0.150 & 0.260 & 0.298 & **0.348** \\   & in-domain & 0.180 & 0.161 & 0.211 & 0.196 & **0.228** \\   & out-domain & 0.177 & 0.135 & 0.205 & 0.189 & **0.229** \\   

Table 2: **User Study and CLIP Scores. Both quantitative results demonstrate our superior performance over the baseline in terms of both subject and joint accuracy.**

Figure 5: **The combination of multiple visual attributes enables the integration of specific characteristics from different reference images into the target subject.**

accuracy_ and _subject-accuracy_. Given the subjectivity of the issue and the lack of quantitative metrics, we incorporate both user studies and GPT scoring on our proposed validation set. Specifically, both users and GPT are instructed as follows: subject-accuracy is determined by whether the subject in the generated image is correct, while attribute-accuracy is evaluated in conjunction with the subject, referred to as attr&sub-accuracy, to eliminate the "image variance" solution that might artificially inflate accuracy. Please refer to the supplementary materials for more details about the validation set, metric implementation, and GPT scoring results.

### Quantitative Evaluation

We show quantitative evaluation results in Table 2, where both the CLIP-Score and user study results indicate that our method benefits from a higher subject accuracy. The user study further demonstrates that our method also excels in the joint accuracy of both subject and attribute. However, it is important to note that the joint accuracy is not high overall due to the challenging nature of this new task.

### Qualitative Evaluation

**Comparisons with previous methods.** Figure 4 demonstrates that our method effectively transfers the specific attribute to the target. In contrast, other methods exhibit various issues. The DB-Lora and IP-Adapter usually tend to create image variations without accurately following the target subject. While DEADiff follows the target subject better, its attribute accuracy is poor, and often resulting in round images. Additionally, the StyleAligned model produces unstable image quality, with a high risk of generating anomalous images.

**Visual attributes decomposition from the reference image.** Based on our method, the visual information extracted from the same reference image can be different, depending on the tag input. Figure 6 shows an example of this, demonstrating the effectiveness of fine-grained attribute decomposition.

**Combination of multiple visual attributes into the target subject.** We demonstrate how multiple reference images can be used to combine specific attributes from each into a new image with a target subject. As shown in Figure 5, our method accurately extracts the specified attribute from two or even three reference images and combines them with the target subject. To further validate the method's sensitivity to the input tags, we conducted experiments by exchanging the tags for the same reference images. The results confirm that the visual attribute extraction is both valid and distinct for different tag inputs, highlighting the method's flexibility.

Figure 6: **Attribute decomposition.** One reference image can be decomposed into different attributes via different tags.

Figure 7: **Ablation on range-sensitive data filter.** It helps improve the attribute accuracy and protect the original generation capacity.

**Effect of range-sensitive data filter.** The filter introduced in Section 3.1 is aimed for regularizing the similarity application range of some attributes like lighting and dynamic modelling. In Figure 7, we show some qualitative results of its effects. First, it slightly enhances the attribute accuracy rate regarding the difficult visual attributes like lighting. What's more important, it also helps protect the generation capacity of the original pre-trained model from being harmed by the heavy noise in the paired data without filtering, leading to highly image quality and lower rate in producing failure cases like distorted human face and body.

## 6 Conclusion

In conclusion, our work addresses the limitations of current text-to-image models in controlling fine-grained visual concepts by introducing a comprehensive dataset with fine-grained visual attributes, FiVA dataset, and a novel visual prompt adapter along with it. Our approach enables precise manipulation of specific visual attributes, offering greater flexibility and applicability across various photography, artistic, and other practical domains. We believe that our contributions will pave the way for more sophisticated and user-driven image generation technologies. We will discuss the limitations and future works in the supplementary material.

**Limitations and Future Works.** The main limitation of the dataset is its heavy reliance on the capacity of the generative model, which might constrain the realism, range of available visual attributes, and attribute accuracy between paired data. For example, specific attributes like photographic composition techniques or creative photography can hardly be created in this way. This might also introduce some bias in appearance distribution introduced by the generative model. In the future, we will consider collecting some high-quality data from platforms with professional photographers and designers, and involve human annotation to create paired data, which can further enhance the dataset with a more realistic data distribution and more complex visual attributes.

## 7 Acknowledgment

This study is supported by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of CPII under the InnoHK. It is also supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).