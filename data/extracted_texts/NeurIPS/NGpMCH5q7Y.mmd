# Integrating Suboptimal Human Knowledge with Hierarchical Reinforcement Learning for Large-Scale Multiagent Systems

Integrating Suboptimal Human Knowledge with Hierarchical Reinforcement Learning for Large-Scale Multiagent Systems

 Dingbang Liu1,2, Shohei Kato2, Wen Gu3, Fenghui Ren1, Jun Yan1, Guoxin Su1

University of Wollongong1, Nagoya Institute of Technology2,

Japan Advanced Institute of Science and Technology3

dl956@uowmail.edu.au, shohey@nitech.ac.jp, wgu@jaist.ac.jp

fren@uow.edu.au, jyan@uow.edu.au, guoxin@uow.edu.au

###### Abstract

Due to the exponential growth of agent interactions and the curse of dimensionality, learning efficient coordination from scratch is inherently challenging in large-scale multi-agent systems. While agents' learning is data-driven, sampling from millions of steps, human learning processes are quite different. Inspired by the concept of Human-on-the-Loop and the daily human hierarchical control, we propose a novel knowledge-guided multi-agent reinforcement learning framework (hhk-MARL), which combines human abstract knowledge with hierarchical reinforcement learning to address the learning difficulties among a large number of agents. In this work, fuzzy logic is applied to represent human suboptimal knowledge, and agents are allowed to freely decide how to leverage the proposed prior knowledge. Additionally, a graph-based group controller is built to enhance agent coordination. The proposed framework is end-to-end and compatible with various existing algorithms. We conduct experiments in challenging domains of the StarCraft Multi-agent Challenge combined with three famous algorithms: IQL, QMIX, and Qatten. The results show that our approach can greatly accelerate the training process and improve the final performance, even based on low-performance human prior knowledge.

## 1 Introduction

As an essential attribute of multi-agent reinforcement learning (MARL), scalability deserves more attention since it contributes to autonomous collective learning behaviors among agents. In many engineering and scientific disciplines, algorithms must possess sufficient scalability to cooperate properly [1; 2; 3]. Unfortunately, training a large number of agents presents inherent challenges. The joint action-state space increases exponentially with the number of agents, resulting in the curse of dimensionality [4; 5]. Agents, suffering from sparse rewards and sample inefficiency [6; 7], encounter'start-up' problems  and often get trapped in local optima, to the extent that learning becomes impossible in the worst cases [9; 10].

To alleviate the exploration burden and overcome the curse of dimensionality, knowledge transfer methods have attracted significant research interest in dealing with large-scale multi-agent systems (MAS) [11; 12; 13]. As the most intuitive and common source, transferring prior knowledge from humans has received considerable attention [14; 15]. An important line of research involves extracting prior knowledge from expert trajectories through imitation learning to address sequential decision-making problems [16; 17]. However, despite the emphasis on Human-on-the-Loop , most research still focuses on step-by-step action demonstrations, which require high-quality and comprehensive prior knowledge [11; 14; 17]. The deployment of this type of human knowledge becomes increasingly difficult in complex tasks, necessitating overwhelming human effort. Additionally, creating human-guided state-action pairs for each scenario is so time-consuming that even human experts struggle to anticipate actions in these highly challenging tasks.

Fortunately, human guidance is not limited to step-by-step action demonstrations, and other high-level knowledge exists to reduce the human effort involved . Human prior knowledge can be abstracted and transferred to agents through abstract rules or heuristics [19; 20; 21]. Although the provided knowledge is suboptimal, it can greatly reduce the computational burden and improve algorithm performance. However, current discussions are still limited to single- or two-player scenarios. Unfortunately, utilizing human prior knowledge properly is significantly more challenging in MAS. Due to non-stationarity and multiple Nash equilibria, most single-agent methods are unsuitable for multi-agent scenarios. Additionally, human knowledge is usually applied indiscriminately, and mapping knowledge from humans to agents is still in its early stages .

To better transfer knowledge from humans, it's essential to understand the nature of human knowledge. Despite accomplishing many complex tasks in daily life, human activities do not require much active attention (Figure 1). When deciding to get up and walk, the brain uses the prefrontal cortex to generate commands, but the intricate coordination of sensory inputs and muscles that follows does not require any conscious attention. Instead, it's mostly executed by a lower nerve network in the spinal cord, sometimes called central pattern generators (CPG) . Indeed, humans are naturally adept at abstracting and providing high-level knowledge, and asking them to provide detailed step-by-step action guides is overly demanding. It may be preferable to have humans deliver abstract knowledge at the top level, while agents spontaneously decide on the utilization of the proposed knowledge based on their 'CPG'.

Inspired by the hierarchical control of human daily activity, we propose a novel knowledge-guided MARL framework to integrate abstract human knowledge into MARL algorithms in an end-to-end manner. As several works have demonstrated, a hierarchical structure  and graphical models  can greatly benefit large-scale MAS. We believe our approach, the hierarchical human knowledge multi-agent reinforcement learning framework (hhk-MARL), can alleviate learning difficulties among a large number of agents. In our framework, trainable fuzzy logic rules are applied to represent human knowledge. Since the prior knowledge is suboptimal, our framework allows agents to adjust the utilization of proposed knowledge through hyper-network based knowledge integration. Additionally, to enhance agent coordination, we construct a graph to determine the importance of agents and simplify their relationships. By applying human knowledge at the top level while maintaining agents' ability to develop self-policies at the bottom level, our hierarchical method helps 'warm-start' the training process and overcomes the learning difficulties of large-scale MAS. Experimental results from challenging tasks in the StarCraft Multi-agent Challenge (SMAC)  demonstrate that our approach can be easily combined with various MARL algorithms, significantly improving their learning efficiency.

## 2 Preliminaries

### Partially observable Markov game

We view our problem as a cooperative multi-agent task. This task can be modeled as a decentralized partially observable Markov decision process (Dec-POMDP) , given by a tuple \(G\):

\[G= S,U,P,r,Z,O,n,\] (1)

where \(s S\) defines the global environment state. The observations of each agent \(i N\{1,,n\}\) are partially observable \(o^{i} O^{i}\), which are determined by the observation function \(Z(s,i):S N p(O)\). Based on its local observation \(o_{i}\), each agent \(i\) selects its action \(u_{i} U_{i}\) at each time step according to its stochastic policy \(_{i}:O_{i} U_{i}\). Then, based on the state transition function \(P:S U_{1} U_{n} S\), the joint action of all agents \(\) changes the environment into a new state. All agents share the same reward function \(r(s,):S R\), and aim to learn poli

Figure 1: Human daily hierarchical control.

cies to maximize action-value functions \(Q(s_{t},_{t})=_{_{t+1},s_{t+1} P}[_{l=0} ^{Th_{t}}^{l}r_{t+l}(s_{t},_{t})|s_{t},_{t}]\) where \(\) is the discount factor and \(Th\) is the time horizon.

### Fuzzy logic

Since human knowledge is highly abstract and uncertain, it is inappropriate to represent knowledge with hard rules . On the contrary, fuzzy logic can tackle issues of uncertainty and lexical vagueness, pacing its way to depict human imprecise knowledge . A fuzzy logic rule is usually in the form of 'IF \(X\) is \(A\) and \(Y\) is \(B\) THEN \(Z\) is \(C\)' with a membership function \(\) for each fuzzy set used to calculate the truth value \(T\) of each precondition:

\[T_{A}=_{A}(x_{0}):X,T_{B}=_{B}(y_{0}):Y\] (2)

where \(x_{0}\) and \(y_{0}\) are the observation values for \(X\) and \(Y\), respectively. To derive the conclusion of this fuzzy rule, both preconditions must be satisfied:

\[_{A B}(x_{0},y_{0})=min(_{A}(x_{0}),_{B}(y_{0}))\] (3)

Finally, the conclusion's strength, \(\), is obtained as follows:

\[=min(T_{A},T_{B})=min(_{A}(x_{0}),_{B}(y_{0}))\] (4)

Therefore, a fuzzy logic rule takes the observation values as inputs and outputs the value of the conclusion to illustrate how likely it is to operate the designed actions under the current observations.

### Knowledge representation and integration

Although human knowledge is highly instructive, such prior knowledge is suboptimal and covers only a small part of the state space. Applying it indiscriminately can hinder the training process. To avoid negative knowledge transfer, agents should have the freedom to decide the utilization of transferred knowledge. For better intuition, a teacher (human) and student (agent) model could be considered. To guide student research, the mentor might give the doctoral student some comments. For example, a comment for literature review could be: _'Read paper with high citation score'_, which can be expressed in the fuzzy logic rule as 'IF \(O\) is \(high\), THEN \(action\) is \(read\)'. Here, \(O\) is the observation of the citation score, and \(high\) is a fuzzy set \(M\) whose membership function could simply be, \(_{high}(o):clip[0.05o,0,1]\). Therefore, the higher the score, the stronger the action \(read\) will be. Apparently, such knowledge is abstract and proposed for a specific state in the research process. Indeed, it is unrealistic to require the mentor to design a comprehensive checklist, and uncertainty and lexical vagueness are inevitable. The student is expected to autonomously decide when to follow the mentor's advice and when to trust their own judgment. Inspired by the CPG of human hierarchical control (Figure 1), to integrate abstract human knowledge into the agent learning process, we design a knowledge 'CPG' for the agent to freely decide how to leverage the suboptimal knowledge.

## 3 Human knowledge guided hierarchical framework

In this section, we will illustrate a novel human knowledge-guided MARL hierarchical framework (hhk-MARL) that integrates abstract human knowledge into MARL in an end-to-end manner to enhance agents' learning efficiency. The overall architecture is shown in Figure 2, which is divided into three levels, mimicking the human hierarchical control (Figure 1). Inspired by the CPG of humans, a hyper-network based knowledge integration is built to allow agents elegantly leverage the proposed human prior knowledge. The details of each module will be elaborated in the following sections, and the meanings of symbols can be found in Appendix A.3.

### Knowledge controller

Since requiring humans to provide comprehensive step-by-step demonstrations for large-scale MAS is unrealistic, we focus on abstract and suboptimal human knowledge to reduce the human effort involved. As introduced in Section 2.2, fuzzy logic is applied to capture human imprecise knowledge. Compared to other knowledge representation methods, fuzzy logic is closer to the structure of human knowledge, making it more interpretable. Furthermore, it has been proven that fuzzy logic is more suitable for training large-scale multi-agent systems with the advantage of generalization . Inspired by previous works on knowledge representation with fuzzy logic [20; 27], we leverage fuzzy logic to abstract human prior knowledge in this work. The general form of each rule can be represented as:

* \(Rule\ L\): IF \(O_{1}\) is \(M_{L_{1}}\) AND \(O_{2}\) is \(M_{L_{2}}\) AND \(\) AND \(O_{z}\) is \(M_{L_{z}}\), THEN \(Action\) is \(u_{kk}\)

Here, \(O_{z}^{i}\) are variables about the system extracted from agent \(i\)'s observations, and \(M_{L_{z}}^{i}\) are fuzzy sets for the corresponding variables \(O_{z}^{i}\). The strength of \(Rule\ L\) on corresponding actions is obtained:

\[_{L}=min[_{L_{1}}(o_{1}),_{L_{2}}(o_{2}),,_{L_{z}}(o_{z})]\] (5)

where \(o_{z}^{i}\) are observation values for \(O_{z}^{i}\) and \(_{L}^{i}\) are membership functions for the fuzzy sets \(M_{L}^{i}\). An example of fuzzy logic rule design is illustrated in Section 2.3 for clarity. For fairness, humans are only allowed to propose guidance based on agent's local observations.

As the transferred human knowledge is very rough, inspired by , we add trainable weights \(\) to adapt proposed knowledge to current tasks. With these trainable weights, the knowledge can be optimized similarly to a neural network. For each rule, there are \(z+1\) corresponding weights: the first \(z\) weights adjust the causal degree of each precondition, and the last weight \(_{z+1}\) indicates the confidence in the proposed knowledge. Therefore, the human preference vector \(Q_{F_{L}}\) for fuzzy rule \(L\) on each action can be represented as (an example of human preference vector is in Appendix A.6):

\[Q_{F_{L}}(u_{1},,u_{k})=_{L_{z+1}}min[_{L_{1}}_{L_{1}} (o_{1}),_{L_{2}}_{L_{2}}(o_{2}),,_{L_{z}}_{L_{ z}}(o_{z})]\] (6)

These trainable weights are initialized at 1 to avoid disturbing the prior knowledge, and then adjusted through the reinforcement learning based on the reward signal. However, it is worth mentioning that a fuzzy rule can be initialized with a higher weight when there is high confidence in it.

We refer to this knowledge representation module as the knowledge controller, shown in Figure 2 bottom right. To maintain the merit of scalability, this knowledge controller is shared among all agents. Specifically, for agent \(i\), the knowledge controller takes the agent \(i\)'s observation \(o^{i}\) as input

Figure 2: The overall framework of the human knowledge guided hierarchical MARL. The general architecture is proposed in the middle which is separated into three levels. Agents can develop their own policies with traditional MARL algorithm shown at bottom left. The graph-based group controller is depicted at top right to enhance agents’ coordination. The knowledge controller is comprised with fuzzy logic rules to represent human knowledge which is demonstrated at bottom right. The hyper-networks of knowledge integration is illustrated at top left to allow agents freely decide the use of proposed human knowledge.

and outputs \(L\) number of human preference vectors based on built-in fuzzy logic rules:

\[Q_{F}^{i}=\{Q_{F_{1}}^{i},Q_{F_{2}}^{i},,Q_{F_{L}}^{i}\}\] (7)

### Knowledge integration

Although trainable weights are added to the knowledge controller to mitigate the knowledge mismatch problem, the proposed knowledge is still quite rough, covering only a small portion of the state space. Since humans and agents have different perceptions and knowledge structures, it is more appropriate to allow agents to determine the utilization of prior knowledge. To not distort human knowledge and avoid negative knowledge transfer, we propose a hyper-networks based knowledge integration to integrate human prior knowledge into agents' policies. Similar to human CPG (Figure 1), this approach allows humans to design abstract knowledge from a high-level, while agents can autonomously decide whether to accept the proposed knowledge and how to leverage it.

Although applying a concatenated neural network as the knowledge integration is straightforward, it is difficult to capture the dynamic knowledge requirements in different states. To allow agents to automatically adapt to human guidance, motivated by previous research , we propose a hyper-networks based knowledge integration that allows agents to refine the proposed prior knowledge based on the local observation. As shown in Figure 2 (top left), the knowledge integration consists of two networks that take the human preference vectors \(Q_{F}^{i}\) and the agent preference vector \(Q_{LOC}^{i}\) as input, and output the knowledge-guided action preference vector \(Q_{i}\) based on the agent \(i\)'s local observation. Formally, the first network takes the observation \(o^{i}\) of agent \(i\) as input and generates weights for the second network, which combines the agent's policy with human knowledge:

\[Q_{i}=k_{}(Q_{F}^{i},Q_{LOC}^{i})\] (8)

where:

\[=h_{}(o^{i})\] (9)

Here, \(h_{}()\) is the hyper-network that generates the weights \(\) for the integration \(k_{}()\). To encourage agents to frequently explore human knowledge at the start, a hyperparameter \(\) is considered:

\[=max(h_{},)\] (10)

This hyperparameter is initialized to 1 and quickly decreases to zero to not impair the agents' autonomy in knowledge adjustment. Similar to the knowledge controller, the knowledge integration is also trained based on the reinforcement learning process and this module is also shared among all agents.

### Group controller

Besides the'start-up' problem from the curse of dimensionality, another difficulty in large-scale MAS is the intricate interactions among agents, which intensify as the number of agents increases. To enhance agent coordination and further improve scalability, motivated by the graph's ability to simplify relationships [1; 28], a simple neural network is applied to generate a cooperation graph among agents (Figure 2 top right). This group controller uses local observation to output cooperation tendency among agents. Specifically, based on \(o^{i}\), agent \(i\) can propose the allies it wants to cooperate with, following the tendency strength \(_{i,j}\):

\[_{i}=\{_{i,1},,_{i,N}\}\] (11)

where \(_{i,j}\) is the cooperation tendency of agent \(i\) toward agent \(j\). After agents deliver their tendencies, this will form a relationship graph (an example is shown in Figure 9):

\[\{_{1},,_{N}\}\] (12)

Here, an agent's importance is considered based on other allies' cooperation tendencies toward it:

\[\{^{1},,^{N}\}=softmax(sum(\{_{1}, ,_{N}\},dim=-2))\] (13)

where \(^{i}\) is the importance weight of agent \(i\) and will be used to weight the agent's selected action. The group controller is also shared among all agents. Additionally, to not violate the principle of centralized training with decentralized execution (CTDE), this module will only be applied during the training process.

### Knowledge guided hierarchical MARL

To transfer knowledge from humans, we integrate abstract human knowledge into the agent learning process by mimicking the hierarchical control of human daily activities. As demonstrated in Figure 2, human can easily propose abstract knowledge at a high-level, while agents autonomously form specific action-state strategies based on the MARL algorithm. This learning framework is end-to-end and can be combined with various MARL algorithms. To clarify this process, QMIX  algorithm is applied as an example here. Initially, the agent \(i\)'s preference vector \(Q_{LOC}^{i}(^{i},)\) is calculated based on its local Q network. Then, the human preference vectors \(Q_{F}^{i}(o^{i})\) are generated from the knowledge controller, mentioned in Section 3.1. Next, following Equation 8, the human knowledge is integrated into agent \(i\)'s policy, forming the knowledge-guided action preference vector \(Q_{i}\). This vector \(Q_{i}\) is used to sample the action \(u^{i}\) of agent \(i\) based on the \(\)-policy with the Q value \(Q_{i}(o^{i},u^{i})\). To imply the importance of agents in the group, the importance weight \(^{i}\) is obtained from Equation 13 to weight \(Q_{i}(o^{i},u^{i})\) and form the weighted Q value \(Q^{i}(o^{i},u^{i})\). Finally, the weighted Q values from all agents are aggregated to derive the global value \(Q_{tot}\) using the QMIX mixing network:

\[Q_{tot}(o,u)=MixingNetwork(^{1} Q_{1}(o^{1},u^{1}),, ^{N} Q_{N}(o^{N},u^{N}))\] (14)

Subsequently, \(Q_{tot}\) is trained to minimize:

\[_{tot}=_{\{o_{t}^{i},o_{t+1}^{i},u_{t}^{i},u_{t+1}^ {i}\}_{i=1}^{N}}[Q_{tot}(\{o_{t}^{i},u_{t}^{i}\}_{i=1}^{N})-y _{t}]^{2}\] (15)

where \(y_{t}\) is calculated as follows, and \(r_{t}\) is the reward at time step \(t\):

\[y_{t}=r_{t}+[MixingNetwork(^{1}(o_{t+1}^{1},u_{t+1}^{1}), ,^{N}(o_{t+1}^{N},u_{t+1}^{N}))]\] (16)

To comply with the CTDE, the group controller and mixing network are only applied during the training process, while the local Q network, knowledge controller and knowledge integration are applied at both stages. The pseudo-code of our method is provided in Algorithm 1.

## 4 Experiments and analysis

In our experiments, we aim to answer the following questions: (1) Can the proposed framework improve the scalability of MARL algorithms? (2) What's the function of each module in our framework? (3) Is using suboptimal human knowledge justified, and how does its quality influence the learning process? To answer these questions, we conduct our experiments on challenging scenarios of the StarCraft Multi-Agent Challenge (SMAC)  with an increasing number of agents involved.
Focusing on micromanagement control, SMAC has been proposed as a common benchmark for MARL methods. In the following experiments, we test our framework on challenging scenarios in SMAC from the 'Hard' and 'Super Hard' categories, setting the game AI difficulty to 'Very Hard'. To demonstrate the impact of the agent number on algorithm performance, we add two self-designed scenarios with more agents involved. To verify the effect of our end-to-end framework, we combine our method with three famous MARL algorithms: IQL , QMIX , and Qatten , naming the corresponding approaches hhkIQL, hhkQMIX, and hhkQatten, respectively. In this study, each algorithm and its corresponding approach are configured with the same hyperparameters and neural network structures. Furthermore, all experimental results are derived across three separate trials with different random seeds, with 32 test episodes in each trial. The shaded region in Figure 4 and 7, and the error bar in Figure 5 denote standard deviation in this work. Details of the hyperparameter settings are in Appendix A.5.

Instead of requiring high-quality expert demonstrations, this work aims to transfer suboptimal knowledge from humans, considering eight pieces of knowledge for challenging tasks in SMAC. Due to space limitations, we use the human prior knowledge: _'Attack the closest enemy'_ as an example. As illustrated in Section 3.1, this abstract knowledge can be represented with a fuzzy logic rule:

* IF \(e\_d\) is \(small\), THEN \(action\) is \(attackEnemyId\).

Here, \(e\_d\) represents the agent's observation of the distance between itself and enemies. The corresponding membership function for the fuzzy set \(small\) is elaborated in Figure 3 and is defined as a simple linear function. More details about the applied human suboptimal knowledge can be found in Appendix A.6. As demonstrated, these fuzzy logic rules are abstract, suboptimal, and designed for specific states (e.g., attack actions are not always available), resulting in a win rate of \(0\%\) when

Figure 4: Experimental results for our approaches and their corresponding baselines in five scenarios. The shaded region denotes standard deviation of average evaluation over 3 trials.

Figure 3: Membership function: ’\(e\_d\) is \(small\)’. X-axis denotes the observation value for variable \(e\_d\) and Y-axis denotes the truth value.

agents are solely manipulated by the proposed knowledge. Nonetheless, these rules are interpretable and easy to design and understand from the human aspect. All rules are designed based on the agent's local observation. Furthermore, it is worth mentioning that the proposed human prior knowledge is not well-crafted and is highly subjective, with no strict requirement for its appropriateness. Anyone can develop their fuzzy logic rules based on their own experience.

### Result and evaluation

To answer the first question, we deploy the approaches in scenarios with an increasing number of agents. The learning curves for all approaches across all tasks are illustrated in Figure 4, and the performance comparison based on the number of agents is elaborated in Figure 5. In conclusion, the experimental results indicate that even based on highly abstract and low-performance human prior knowledge, our knowledge-guided approaches not only significantly accelerate the training process but also enhance the final performance across scenarios with various numbers of agents.

As the number of agents increases, the joint action-state space expands exponentially, making it exceedingly challenging for agents to learn from scratch. Despite these challenges, our approaches still manage to overcome the'start-up' issue (Figure 4). By alleviating learning difficulties, while baseline algorithms fail in tasks with many agents (IQL in Figure 4(d), IQL and QMIX in Figure 4(e)), our approaches help agents overcome the initial learning challenges, improving scalability. Furthermore, as demonstrated by the win rate curves in Figure 4(a), our approaches greatly accelerate the training process, benefiting the initial stages of agent learning. Notably, our method (hhKIQL), even combined with the least effective baseline algorithm (IQL), achieves performance comparable to the best baseline algorithm (QMIX).

To better visualize our method in scenarios with different numbers of agents, we summarize the algorithms' performance in Figure 5. By leveraging human prior knowledge, our end-to-end method consistently helps MARL algorithms improve their performance across various numbers of agents. Additionally, it is worth mentioning the improvement our method brings to the IQL algorithm, shown in the first image of Figure 5. As one of the oldest fully decentralized MARL algorithms, IQL inherently struggles to handle coordination in large-scale MAS, displaying low performance in all scenarios. By contrast, benefiting from the integration of human knowledge and the relationship graph to mitigate'start-up' issues and enhance cooperation, our approach significantly improves the scalability of the IQL algorithm.

### Ablation study

To answer the following two questions, we design two ablation experiments: one to identify the function of each module and another to assess the influence of human suboptimal knowledge. Considering our improvement on the IQL algorithm, we use it as the basis in the '10m vs 11m' scenario to provide a clear comparison. The ablation results are detailed in Figure 7.

#### 4.3.1 Module function

The experiment results on module functions are depicted in Figure 7(a), where 'hhKIQL-graphOnly' represents our method with only the group controller, 'hhKIQL-humanOnly' indicates our method integrating solely human knowledge, and 'hhKIQL-fixedKnow' denotes our method without trainable parameters in fuzzy logic rules.

Figure 5: Performance comparison between baselines and our methods under the number of agents increase. The error bar is based on standard deviation

**Effect of group controller:** To elaborate on the role of the group controller, we first present agents' cooperation graph in Figure 6. The relationship graph reveals that the cooperation demand changes over time. With the group controller, agents can independently choose allies to collaborate with, and important agents are emphasized. As depicted in Figure 7(a), the group controller enhances cooperation among agents and accelerates the training process, evidenced by a higher convergence rate compared to that of the IQL algorithm.

**Effect of human knowledge:** Although the transferred knowledge is suboptimal, human prior knowledge can significantly improve performance. The knowledge integration, functioning as a 'CPG', allows human to effortlessly provide abstract knowledge from a high-level perspective. As a result in Figure 7(a), the knowledge-guided algorithm achieves overall better performance than the baseline algorithm. However, it is worth noting that this ablation algorithm can be further improved with the group controller installed, as evidenced by comparing it to the hhkIQL approach.

**Trainable knowledge:** The benefits of trainable knowledge are demonstrated in Figure 7(a). Although both approaches achieve similar performance, the approach with trainable fuzzy logic rules realizes a faster convergence rate than its ablation counterpart. With the trainable knowledge controller, the prior knowledge is constantly optimized during training, enhancing the adaptation of the provided knowledge to the current task. As a result, agents can make better use of proposed knowledge and learn faster.

#### 4.3.2 Human suboptimal knowledge influence

In this section, we explore the impact of suboptimal human knowledge and how our approach addresses inappropriate knowledge. The ablation results are described in Figure 7(b). To assess the effect of knowledge quality, we consider human knowledge with more fuzzy logic rules to be more comprehensive. For fairness, these rules are inherited among ablation approaches. For example, 'hhkIQL (8 rules)' will include the 5 rules used in 'hhkIQL (5 rules)', and so forth. Since knowledge from humans is highly subjective, it is hard to judge whether the transferred knowledge is inappropriate. To identify how our approach deals with inappropriate knowledge, we substitute the values in human preference vectors \(Q_{F}^{i}\) with random value to represent inappropriate knowledge, which is denoted as 'hhkIQL (random)' in Figure 7(b).

Figure 6: After the training of hhkIQL in ’10m vs 11m’ scenario, the change of each agent’s \(\) during a battle episode. The nodes are agents and the edges are agents’ tendency to cooperate with others. \(t\) is time step. The died agents are not shown in the graph (full details are in Appendix A.7).

Figure 7: Ablation studies under ’10m vs 11m’ scenario. (a) ablation study on the function of each module in our method; (b) ablation study on the influence of various suboptimal human knowledge.

As demonstrated in Figure 7(b), more comprehensive human knowledge can help agents achieve better performance. Although 'hhkIQL (1 rules)' can achieve faster learning speed than 'hhkIQL (3 rules)' and 'hhkIQL (5 rules)', it results in lower final performance. We guess that with fewer rules applied, there is a reduced learning burden on knowledge utilization, but this also leads to lower final outcomes. Notably, even though more comprehensive human knowledge is beneficial, the performance of these approaches remains similar. Furthermore, as the learning curve of 'hhkIQL (random)' indicates, the knowledge integration module can efficiently filter out negative knowledge. While it takes agents more time to learn how to utilize the proposed knowledge, 'hhkIQL (random)' can still outperform the baseline algorithm, highlighting the importance of the knowledge integration module. In conclusion, our approach does not rely on high-quality human knowledge, and the knowledge integration module can successfully mitigate the negative knowledge transfer problem, allowing humans to propose any knowledge they consider useful for agents.

## 5 Conclusion and future work

In this study, we introduce a novel hierarchical learning framework for enhancing coordination in large-scale MAS by leveraging suboptimal human knowledge. This framework consists of the group controller, the knowledge controller, and the knowledge integration, allowing humans to provide knowledge at the top level while agents develop their own policies at the bottom. Evaluated in SMAC with three famous algorithms, our end-to-end methods surpass corresponding baselines in learning speed and final performance, even with low-performance human knowledge integrated. Furthermore, this framework successfully improves the scalability of algorithm, handling scenarios with numerous agents where standard MARL algorithms fail. In the future, we will apply our approach in domains with even more agents involved, and explore its application to heterogeneous agents.