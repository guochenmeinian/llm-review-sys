# Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification

Jan Schuchardt1, Mihail Stoian2, Arthur Kosmala1, Stephan Gunnemann1

{j.schuchardt, a.kosmala, s.guennemann}@tum.de, mihail.stoian@utn.de

1Dept. of Computer Science & Munich Data Science Institute, Technical University of Munich

2Dept. of Engineering, University of Technology Nuremberg

###### Abstract

Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via _mechanism-agnostic_ subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving _mechanism-specific_ guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for _privacy accounting_, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with Renyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.

## 1 Introduction

Composability and amplification by subsampling are two key properties of Differential Privacy (DP)  that make it possible to provide provable privacy guarantees for machine learning algorithms that iteratively learn from batches of data.

Composability means that privacy gracefully deteriorates when iteratively applying a differentially private mechanism to a dataset . Kairouz et al. , Murtagh and Vadhan  ultimately derived tight composition theorems that optimally characterize the privacy parameters \((^{},^{})\) of a composed mechanism as a function of the component mechanisms' parameters \((,)\). However, later work demonstrated that these guarantees are only tight in a _mechanism-agnostic_ sense . Stronger _mechanism-specific_ composition guarantees can often be obtained by using additional information beyond the fact that the mechanisms satisfies some notion of DP. Methods for tracking privacy over multiple iterations (e.g. ) are referererd to as _privacy accountants_.

Amplification by subsampling means that privacy can be strenghtened by applying a differentially private mechanism to randomly sampled batches of a dataset . Similar to Kairouz et al. , Balle et al.  ultimately proposed a framework for deriving tight subsampling theorems that optimally characterize the privacy parameters \((^{},^{})\) of a subsampled mechanism as a function of the underlying _base mechanism_'s parameters \((,)\).

However, their framework for deriving subsampling theorems has two key limitations. Firstly, as we shall demonstrate, the resultant guarantees are generally only tight in a mechanism-agnosticsense: Given \((,)\), one can construct _some_ worst-case \((,)\)-DP mechanism that is \((^{},^{})\)-DP under subsampling. The specific mechanism at hand may however be significantly more private. Secondly, their framework does not explain how to derive subsampling guarantees for privacy accountants. In fact, there is so far no unified framework for deriving subsampling guarantees for privacy accountants.

To address these two limitations, we propose a novel framework for _unified mechanism-specific amplification by subsampling_ analysis, using conditional optimal transport. Our proposed framework (1) lets us derive mechanism-specific subsampling guarantees, which can be stronger than mechanism-agnostic guarantees, (2) lets us recover mechanism-agnostic guarantees via a pessimistic upper bound - essentially subsuming the approach from  - and (3) lets us derive guarantees for approximate differential privacy , moments accounting  (i.e., Renyi differential privacy ), and accounting with dominating pairs  (e.g., numerical accounting [16; 8; 17; 18; 19; 20; 21]) in a unified, principled manner.

As a practical application, we consider the problem of tightly analyzing group privacy under subsampling (see Fig. 1). Assume a scenario where \(K\) individuals may independently decide to contribute to a dataset. Further assume that this dataset is processed by randomly sampling a batch and applying an \((,)\)-DP _base mechanism_. A special case of this setting is discussed in a recent technical note , which assumes that the \(K\) individuals collaboratively agree to either contribute or not contribute all their data and that the base mechanism is Gaussian. In the general setting we consider, the best known method for providing privacy guarantees for the entire group is to (1) use existing bounds to show that the subsampled mechanism guarantees \((^{},^{})\)-DP for individuals and (2) use the group privacy property to show that this implies \((K^{},K e^{K^{}} ^{})\)-DP for the group.

Our proposed framework lets us derive stronger, tight _group privacy amplification_ guarantees. By analyzing group privacy and subsampling jointly, these guarantees accurately capture that it is unlikely for a large fraction of the group's data to simultaneously appear in a batch. Our framework further lets us derive dominating pairs , which enables tight tracking of group privacy under composition. Overall, our main contributions are that we

* demonstrate for the first time that there is a qualitative difference between mechanism-specific and mechanism-agnostic tightness in privacy amplification,
* propose a general framework for deriving mechanism-specific amplification by subsampling guarantees, subsuming prior work on mechanism-agnostic amplification,
* develop the first unified method for deriving subsampling guarantees for privacy accounting,
* and derive the first tight subsampling guarantees for general group privacy.

As part of our group privacy analysis, we also significantly generalize recent results derived for subsampled matrix mechanisms , which is of independent interest. Experimental evaluation demonstrates that our tight mechanism-specific guarantees outperform both tight mechanism-agnostic bounds and post-hoc use of the group privacy property.

## 2 Background and preliminaries

**Problem setting.** We consider the same setting as Balle et al. , but with an additional privacy accounting and group privacy perspective. We assume a space of datasets \(\), a space of batches \(\) that can be constructed from these datasets, and an output space \(\). For the sake of exposition, we assume that \(\) is the powerset \(()\) of some finite discrete set \(\), that \(=\{y x x\}\), and that \(=^{D}\). We further consider a random _subsampling scheme_\(S:\) that generates batches from datasets, and a random _base mechanism_\(B:^{D}\) that maps batches to outputs. We write \(s_{x}(y)\) for the pmf of \(S(X)\) and \(b_{y}(z)\) for the pdf of \(B(y)\). In Appendix D, we introduce a more general setting that admits arbitrary spaces, for which we derive all our results. Our goal is to

Figure 1: Group members \(x_{1}^{},x_{2}^{}\) contribute to a dataset, while group member \(x_{3}^{}\) does not. For small subsampling rates \(r\), it is unlikely to access a single (\(=2r(1-r)\)) or even both (\(=r^{2}\)) inserted elements when applying a base mechanism \(B\) to a subsampled batch (e.g., the yellow one). This further obfuscates which data was contributed by members of group \(\{x_{1}^{},x_{2}^{},x_{3}^{}\}\).

provide formal privacy guarantees for the _subsampled_ mechanism \(M=B S\), which takes a dataset, subsamples a batch from it, and then applies the random base mechanism to this batch.

**Dataset and batch relations.** Specifically, we want to prove privacy under _neighboring relations_\(_{}^{2}\) between datasets. For example, the _insertion/removal_ relation \(x_{,}x^{}\) implies that \(x^{}=x\{a\}\) or \(x^{}=x\{a\}\) for some \(a\). The _substitution_ relation \(x_{,}x^{}\) implies that \(x^{}=x\{a\}\{a^{}\}\) for some \(a,a^{}\). In addition, we assume that \(\) is equipped with a batch neighboring relation \(_{}^{2}\). The batch relation \(_{}\) can be distinct from the dataset relation \(_{}\).

**Subsampling schemes.** While we consider arbitrary subsampling schemes \(S:\), we shall later apply our framework to three particularly common ones: Subsampling _without replacement_ and _with replacement_ sample sets and multisets of fixed size \(|y|=q\) uniformly at random. _Poisson_ subsampling includes each element of \(x\) with independent probability ("rate") \(r\).

**Approximate differential privacy.** A mechanism \(M:^{D}\) is privacy-preserving under symmetric neighboring relation \(_{}\) when the distributions of \(M(x),M(x^{})\) with densities \(m_{x},m^{}_{x}\) are almost indistinguishable for all \(x_{}x^{}\). Approximate differential privacy (ADP)  quantifies this via hockey stick divergences :

**Definition 2.1**.: For \( 0\), a mechanism \(M:^{D}\) is \((,)\)-DP under symmetric relation \(_{}\) iff \( x_{}x^{}:H_{^{c}}(m_{x}||m_{x^{ }})\) with \(H_{}(m_{x}||m_{x^{}})=_{^{D}}\{m_{x}(z)/m_{x^{ }}(z)-,0\} m_{x^{}}(z)\,z\).

Note that this divergence-based definition of approximate differential privacy is equivalent to requiring for all datasets \(x_{}x^{}\) and all events \(S^{D}\) that \([M(x) S] e^{}[M(x^{}) S]+\).

**Renyi differential privacy.** Privacy accounting was first popularized by methods that used moment bounds instead of \((,)\) pairs to better track privacy under composition . These methods were later developed into a moments-based notion of privacy - Renyi differential privacy (RDP) :

**Definition 2.2**.: For \( 1\), a mechanism \(M:^{D}\) is \((,)\)-RDP under symmetric relation \(_{}\) iff \( x_{}x^{}:(_{}(m_{x}||m_{x^{ }}))/(-1)\) with \(_{}(m_{x}||m_{x^{}})=_{^{D}}m_{x}(z)^{}  m_{x^{}}(z)^{1-}z\).

Note that \(_{}\)_is not the Renyi divergence, but its \(\)th moment, i.e., a scaled and exponentiated Renyi divergence._ We use this notation to eliminate exponential terms that arise in amplification for RDP (see ). If a mechanism is \((,)\)-RDP, its \(T\)-fold self-composition is \((,T)\)-RDP. These RDP parameters can then be converted to ADP parameters (\(,\)), albeit in a lossy manner .

**Dominating pairs.** Later work proposed other numerical  or analytical accounting techniques . Zhu et al.  developed a unified view on these works by introducing the notion of _dominating pairs_:

**Definition 2.3**.: A pair of distributions \((P,Q)\) with densities \((p,q)\) is a dominating pair for mechanism \(M\) under neighboring relation \(_{}\), if \(H_{}(m_{x}||m_{x^{}}) H_{}(p||q)\) for all \(x_{}x^{}\) and all \( 0\).

Given a dominating pair \((P,Q)\), one can use various numerical or analytic techniques, such as convolution of privacy loss distributions  or central limit theorems of tradeoff functions , to track the privacy of mechanism \(M\) under composition (see Fig. 2 in ).

**Group privacy.** The _group privacy property_ is the graceful decay of privacy when considering multiple user's data. This is normally formalized via the notion of induced distance (see ):

**Definition 2.4**.: The distance \(d_{}(x,x^{})\) induced by relation \(_{}\) is the length of the shortest sequence \((x_{1},,x_{K-1})^{K-1}\) such that \(x_{}x_{1}\), \( k:x_{k}_{}x_{k+1}\), and \(x_{K-1}_{}x^{}\).

_Example 2.5_.: Let \(_{}\) be the insertion/removal relation \(_{}\). Then the induced distance \(d_{}\) between \(x=\{1,2\}\) and \(x^{}=\{2,3\}\) is \(2\), because \(\{1,2\}_{}\{1,2,3\}_{}\{2,3\}\).

**Proposition 2.6** (Vadhan ).: _If mechanism \(M:^{D}\) is \((,)\)-DP under relation \(_{}\), then it is \((K,K e^{K})\)-DP under group relation \(\{(x,x^{})^{2} d_{}(x,x^{})=K\}\)._

That is, for sufficiently small \(\) the ADP parameters deteriorate approximately linearly with induced distance. As baselines for our experiments, we use even tighter bounds (see Appendix C.1.4).

## 3 Unified mechanism-specific amplification by subsampling

Our goal is to develop a general procedure for (tightly) deriving ADP parameters \((^{},^{})\), RDP parameters \((^{},^{})\), or dominating pairs \((P^{},Q^{})\) for the subsampled mechanism \(M=B S\) with subsampling scheme \(S\) and base mechanism \(B\). Based on Definitions 2.1 to 2.3, this requires that we evaluate or bound the hockey stick divergence \(H_{}\) or the scaled and exponentiated Renyi divergence \(_{}\) between the distributions of \(M(x)\) and \(M(x^{})\) with \(x_{}x^{}\). To discuss both simultaneously, let us write \(_{}\) for either \(H_{}\) or \(_{}\) and assume that \( 0\) or \( 1\), respectively.

There are two challenges: Firstly, the distribution of \(M(x)\) is high-dimensional mixture distribution with one component per possible batch \(y\) and weights given by batch probabilitites \(s_{x}(y)\), i.e., \(m_{x}(z)=_{y}b_{y}(z) s_{x}(y)\). Secondly, there may be no simple analytic formula for the component densities. For instance, evaluating the density of perturbed model gradients in noisy SGD  requires backpropagation. Both challenges make it hard to evaluate or bound \(_{}(m_{x}||m_{x^{}})\).

A useful property that will help us in addressing the first problem of having a large number of mixture components is the joint convexity of \(_{}\) in the space of probability density functions :

**Lemma 3.1**.: _Consider arbitrary densities \(f_{1}^{(1)},f_{2}^{(1)},f_{1}^{(2)},f_{2}^{(2)}:^{D}_{+}\) and weight \(w\). Then, \(_{}(wf_{1}^{(1)}+(1-w)f_{2}^{(1)}||wf_{1}^{(2)}+(1-w)f_{2}^{(2)}) w _{}(f_{1}^{(1)}||f_{1}^{(2)})+(1-w)_{}(f_{2}^{(1)}||f_{2} ^{(2)})\)._

In our case, the \(f\)s can be base mechanism densities \(b_{y}\) with different \(y\), and the weights can be given by the subsampling distribution. We could thus upper-bound the mixture divergence \(_{}(m_{x}||m_{x^{}})\) in terms of component divergences - if the mixtures had identical weights. This is generally not the case, since subsampling mass functions \(s_{x}()\) and \(s_{x^{}}()\) depend on datasets \(x x^{}\).

To still leverage the joint convexity of \(_{}(m_{x}||m_{x^{}})\), we want to rewrite \(m_{x}\) and \(m_{x^{}}\) as mixtures with identical weights. This is exactly what is offered by _couplings_ between probability mass functions. Intuitively, a coupling between two pmfs \(p_{1},p_{2}:\) specifies for every \(y_{1},y_{2}\) how much probability should be transported from \(y_{1}\) to \(y_{2}\) to transform \(p_{1}\) into \(p_{2}\). Couplings can be formalized and generalized to multiple pmfs as follows (for a more thorough introduction, see ):

**Definition 3.2**.: A coupling between \(N\) pmfs \(p_{1},,p_{N}:\) on batch space \(\) is a joint pmf \(:^{N}\) where the \(n\)th marginal is \(p_{n}\), i.e., \(p_{n}(y_{n}^{*})=_{^{N}}[y_{n}=y_{n}^{*}] ()\).

Given a valid coupling \(\) between subsampling mass functions \(s_{x}\) and \(s_{x^{}}\), we can use marginalization to rewrite \(m_{x}(z)\) as \(_{^{2}}b_{y_{1}}(z)()\). Similarly, we can rewrite \(m_{x^{}}(z)\) as \(_{^{2}}b_{y_{2}}(z)()\). Now, both \(m_{x}\) and \(m_{x^{}}\) are mixtures with identical weights and one component per pair of batches in \(^{2}\). We can thus recursively apply Lemma 3.1 to show (full proof in Appendix E):

**Theorem 3.3**.: _Consider a subsampled mechanism \(M=B S\), and an arbitrary coupling \(\) between subsampling mass functions \(s_{x}()\) and \(s_{x^{}}()\). Then,_

\[_{}(m_{x}||m_{x^{}})_{^{2}}c_{ }(y^{(1)},y^{(2)})(y^{(1)},y^{(2)})\] (1)

_with cost function \(c_{}:_{+}\) defined by \(c_{}(y^{(1)},y^{(2)})=_{}(b_{y^{(1)}}||b_{y^{(2)}})\)._

We write \(y^{(1)}\) instead of \(y_{1}\) to simplify later notations. While every coupling \(\) yields a valid upper bound, the guarantees can be tightened by finding a coupling \(^{*}\) that minimizes the r.h.s. of Eq. (1). We thus have an _optimal transport problem_, where the cost \(c_{}\) of transporting probability from batch \(y^{(1)}\) to batch \(y^{(2)}\) depends on the divergence \(_{}\) of base mechanism densities \(b_{y^{(1)}}\) and \(b_{y^{(2)}}\).

Unfortunately, experimental evaluation in Appendix B.1.4 shows that the resultant guarantees can be much weaker than those from prior work - even with an optimal coupling \(^{*}\). This is because Theorem 3.3 results from recursively applying the joint convexity property (Lemma 3.1) to \(_{}(m_{x}||m_{x^{}})\). Each recursive step splits each mixture into two smaller mixtures and further upper-bounds the divergence that is achieved by _our specific subsampled mechanism \(M\) on our specific pair of datasets \(x,x^{}\)_. Upon fully decomposing the overall divergence into divergences between single-mixture components, this sequence of bounds is in fact larger than even the divergence \(_{}(_{}||_{^{}})\) achieved by _a worst-case subsampled mechanism \(\) on a worst-case pair of datasets \(,^{}\)_. To overcome this limitation, we propose to limit the recursion depth in order to obtain a tighter upper bound that matches _our specific subsampled mechanism \(M\) on a worst-case pair of datasets \(,^{}\)_.

Limiting the recursion depth to which Lemma 3.1 is applied means upper-bounding \(_{}(m_{x}||m_{x^{}})\) in terms of mixture divergences that have not been fully decomposed into their individual components. Specifically, we propose to do so by defining an optimal transport problem between _multiple_ subsampling mass functions conditioned on different events (proof in Appendix E):

**Theorem 3.4**.: _Consider a subsampled mechanism \(M=B S\). Further consider two disjoint partitionings \(_{i=1}^{I}A_{i}=\) and \(_{j=1}^{J}E_{j}=\) of batch space \(\) such that all \(A_{i}\) and \(E_{j}\) have non-zero probability under the distribution of \(S_{x}\) and \(S_{x^{}}\), respectively. Let \(\) be an arbitrary coupling between conditional mass functions \(s_{x}( A_{1}),,s_{x}( A_{I}),s_{x^{}}( E _{1}),,s_{x^{}}( E_{J})\). Then,_

\[_{}(m_{x}||m_{x^{}})_{^{I+J}}c_{ }(^{(1)},^{(2)})((^{(1)},^{(2)})),\]

_with cost function \(c_{}:^{I}^{J}_{+}\) defined by_

\[c_{}(^{(1)},^{(2)})=_{}(_{i=1}^{I}b_{y_{ i}^{(1)}}[S(x) A_{i}]\|_{j=1}^{J}b_{y_{j}^{(2)}}[S(x^{ }) E_{j}]).\] (2)

In other words: We now have an optimal transport problem between \(I+J\) probability mass functions coupled by \(\). The transport cost \(c_{}\) is a divergence between two mixtures. The components of the first mixture are base mechanisms densities given batches \(y_{i}^{(1)}\) from batch tuple \(^{(1)}^{I}\). The weights of the first mixture are probabilities of events \(A_{i}\). The second mixture is defined analogously.

Note that we can recover Theorem 3.3 by conditioning on a single event, i.e., \(A_{1}=,E_{1}=\). As we shall demonstrate, a more fine-grained partitioning lets us obtain tighter bounds that match the divergence \(_{}(m_{}||m_{^{}})\) of our specific subsampled mechanism \(M\) on a worst-case pair of datasets \(,^{}\). In the extreme case of defining event per possible batch, Theorem 3.4 holds with equality.

Overall, Theorem 3.4 reduces the broad problem of bounding mixture divergences to the canonical problem of optimal transport between conditional distributions. But before we can apply it, we need to address two open problems: Evaluating the cost function and designing optimal couplings.

**Cost function bound.** Not having an analytic expression for every base mechanism density \(b_{y}(z)\) may make it intractable to evaluate cost function \(c_{}\). We thus propose to bound it via an approach that is inherent to differential privacy: Considering worst-case inputs (proof in Appendix E).

**Proposition 3.5**.: _Consider \(^{(1)}^{I},^{(2)}^{J}\), and cost function \(c\) defined in Eq.2. Let \(d_{}\) be the distance induced by \(_{}\) (see Definition 2.4). Then, \(c_{}(^{(1)},^{(2)})_{}(^{(1)},^ {(2)})\), with_

\[_{}(^{(1)},^{(2)})=_{}^{(1)},}^{(2)}}c_{}(}^{(1)},}^{(2)})\] (3)

_subject to \( k,l\{1,2\}, t,u:d_{}(_{t}^{(k)},_{ u}^{(l)}) d_{}(y_{t}^{(k)},y_{u}^{(l)})\) and \(}^{(1)}^{I},}^{(2)}^{J}\)._

Put differently: We construct two new mixtures with components that are adversarially chosen to maximize divergence while retaining the pairwise distances between batches in \(^{(1)}\) and \(^{(2)}\). Note that, for the special case of \(_{}=H_{}\) and \(I=J=1\), this bound corresponds to the "group privacy profile" in . As we shall demonstrate in the next sections, this bound \(\) can often be evaluated using high-level information about the base mechanism \(B:^{D}\), such as global sensitivity.

**Sufficient optimality condition.** While every coupling \(\) yields a valid upper bound in Theorem 3.4, this bound can be tightened by designing an optimal coupling \(^{*}\). To inform this design, we generalize the notion of distance-compatible couplings from  to an arbitrary number of distributions:

**Definition 3.6**.: A coupling \(\) between mass functions \(p_{1},,p_{N}\) on batch space \(\) is \(d_{}\)-compatible when \(()>0\) only if \( u>1:d_{}(y_{1},y_{u})=d_{}(\{y_{1}\},(p_{u}))\) and \( u>t>1:d_{}(y_{u},y_{t})=d_{}((p_{t}), (p_{u}))\), where \((p_{u})\) is the support of \(s_{u}\) and the distance between two sets is the minimum distance of their elements.

Essentially, a \(d_{}\)-compatible coupling only assigns probability to a tuple of batches \(\) when all pairs \(y_{i},y_{j}\) have the smallest possible distance to \(y_{1}\) and each other while still being in the support of their distributions. In Appendix F, we prove that \(d_{}\)-compatibility is sufficient for optimality. We further show that the optimal value has a canonical form whenever a \(d_{}\)-compatible couplings exists.

**Summary.** To summarize, we propose the following three-step procedure for deriving subsampling guarantees: (1) Define two partitions of the batch space into \(I\) and \(J\) events. (2) Define a simultaneous coupling between the corresponding \(I+J\) conditional distributions to obtain a bound in terms of divergences between small mixtures with \(I\) and \(J\) components. (3) Bound these mixture divergences by considering \(I+J\) worst-case mixture components under pairwise batch distance constraints.

### Tight mechanism-specific group privacy amplification

As a concrete example, and to illustrate the difference between mechanism-specific and -agnostic (see Section 3.2) bounds, let us consider the group privacy setting from Fig. 1. We have a group of \(K\) users that can independently decide to contribute or not contribute their data, and the resulting dataset is Poisson subsampled. To provide formal privacy guarantees to the group, we need to prove that the distributions of \(M(x),M(x^{})\) are almost indistinguishable for \(x,x^{}\) with induced distance \(d_{,}(x,x^{}) K\). Let \(x_{K_{+},K_{-},}x^{}\) be the relation that implies that \(K_{+}\) records are inserted and \(K_{-}\) records are removed to construct \(x^{}\) from \(x\). We can then show (full proof in Appendix M):

**Theorem 3.7**.: _Let \(M=B S\), where \(S\) is Poisson subsampling with rate \(r\). Let \(_{}\) be the insertion/removal batch relation \(_{,}\). Then, for all \(x_{K_{+},K_{-},}x^{}\), \(_{}(m_{x}||m_{x^{}})\) is l.e.q._

\[_{}_{}(_{i=1}^{K_{-}+1}b_{y_{i}^{(1)}} (i-1 K_{-},r)||_{j=1}^{K_{+}+1}b_{y_{j}^{(2)}} (j-1 K_{+},r)),\] (4)

_subject to constraints \(^{K_{-}+K_{+}+2}\), as well as \( l\{1,2\}, t,u:d_{}(y_{t}^{(l)},y_{u}^{(l)})|t-u|\), and \( t,u:d_{}(y_{t}^{(1)},y_{u}^{(2)})(t-1)+(u-1)\)._

Proof sketch.: We let \(A_{i}\) and \(E_{j}\) be the events that \(S(x)\) contains \(i-1\) deleted elements and \(S(x^{})\) contains \(j-1\) inserted elements, respectively. Our coupling defines the following generative process: We sample \(y_{1}^{(1)}\) from \(s_{x}( A_{1})\) and let \(y_{1}^{(2)} y_{1}^{(1)}\). We then iteratively generate the other \(y_{i}^{(1)}\) by sampling a permutation in which we add the \(K_{-}\) deleted elements to \(y_{1}^{(1)}\). We then generate the other \(y_{i}^{(2)}\) by sampling a permutation in which we add the \(K_{+}\) inserted elements to \(y_{1}^{(2)}\). The result then follows from the cost function bound in Proposition 3.5 and \(d_{}\)-compatibility of the coupling. Batches \(y_{u}^{(1)}\) and \(y_{t}^{(1)}\) and have a distance bounded by \(|u-t|\), because one can be obtained from the other by removing/inserting \(|u-t|\) elements. The constraints for \(y_{u}^{(2)}\) and \(y_{t}^{(2)}\) are analogous. Batches \(y_{u}^{(1)}\) and \(y_{t}^{(2)}\) have a distance bounded by \((t-i)+(u-1)\) because we need to remove \(t-1\) elements and insert \(u-1\) elements to construct one from the other. 

Next, we can solve the constrained optimization problem in Theorem 3.7 - i.e., determine worst-case components - to obtain mechanism-specific guarantees. For instance (proof in Appendix M.2.1):

**Theorem 3.8**.: _Let \(M=B S\), where \(S\) is Poisson subsampling with rate \(r\), and \(B\) is the Gaussian mechanism \(h+V\) with \(h:^{D}\) and \(V(,^{2}_{D})\). Define the \(_{2}\)-sensitivity \(L_{2}=_{y_{,}y^{}}||f(y)-f(y^{})||_{2}\). Then for all \(x_{K_{+},K_{-},}x^{}\), \(_{}(m_{x}||m_{x^{}})\) is l.e.q._

\[_{}(_{i=1}^{K_{-}+1}f_{i}^{(1)}(i-1 K _{-},r)||_{j=1}^{K_{+}+1}f_{j}^{(2)}(j-1 K_{+},r) ),\]

_with univariate normal densities \(f_{i}^{(1)}=((i-1),/\,L_{2})\), \(f_{j}^{(2)}=(-(j-1),/\,L_{2})\)._

Note that this bound is mechanism-specific in that it explicitly depends on \(B\) being a Gaussian mechanism with standard deviation \(\) and sensitivity \(L_{2}\). The bound can be numerically evaluated to arbitrary precision using standard techniques from privacy accounting literature (see Appendix M.4). In Appendix M.2 we derive similar guarantees for Laplace and randomized response mechanisms.

**Tightness.** These bounds are tight in a mechanism-specific sense: One cannot derive stronger ADP or RDP guarantees without additional information about the datasets \(x,x^{}\) or the underlying function \(h\) (proofs in Appendix M.3).

**Asymptotic bounds.** Our focus is on tight bounds that can be explicitly computed. However, some early works on RDP accounting (e.g.) also provided asymptotic versions of their bounds, e.g., as a function of divergence parameter \(\). For completeness, we use Theorem 3.8 to generalize the asymptotic bounds of Abadi et al.  to the group privacy setting in Appendix N.5.

**Other contributions.** Our solutions to the optimization problem in Theorem 3.7 are of independent interest: The special case of \(_{}=H_{}\), \(K_{-}=0\) or \(K_{+}=0\), and Gaussian mechanisms wasderived to analyze matrix mechanisms in . We significantly generalize it via an entirely different proof strategy that admits \(_{}\{H_{},_{}\}\), arbitrary \(K_{+},K_{-}_{0}\), and non-Gaussian mechanisms (see Appendix O).

### Tight mechanism-agnostic group privacy amplification

To further illustrate the difference between mechanism-specific and -agnostic bounds, let us apply the framework of Balle et al.  to the same setting: For \(K_{-}=0\) or \(K_{+}=0\) and \( 0\), their ansatz shows that the subsampled mechanism \(M=B S\) is \((^{},^{})\)-DP with \(^{}=(1+(1-(0 K,r))(e^{ }-1))\) and \(^{}=_{k=1}^{K}(k K,r)_{k}\), with group privacy parameters \(_{k}=_{y,y^{}}H_{()}(b_{y}\|b_{y^{}})\) s.t. \(d_{}(y,y^{}) k\) (see Appendix N).

**Tightness.** This bound is tight in a mechanism-agnostic sense, i.e., for every \( 0\), one can construct _some_ worst-case mechanism that exactly attains the bound (see Appendix N.2).

**Mechanism-specific vs mechanism-agnostic tightness.** An apparent advantage of this result is that it expresses \((^{},^{})\) as a simple analytic formula of base mechanism DP parameters \(,_{1},,_{K}\). However, this simplicity comes at a cost: As we show in Appendix N.3, this guarantee implicitly upper-bounds the tight mixture divergence bound from Theorem 3.7 in terms of its component divergences using joint convexity.2 As we experimentally demonstrate in Section 4, this relaxation leads to weaker privacy guarantees for group size \(K 2\). This gap has gone unnoticed in earlier work, because Theorem 3.8 happens to be identical to the bound of Balle et al.  for the special case of \(K=1\) (see proof of Proposition 30 in ). By studying the more complex group privacy setting, we demonstrate for the first time that _there is a qualitative difference between mechanism-agnostic and mechanism-specific tightness in privacy amplification_.

### From mechanism-specific to mechanism-agnostic guarantees

The observed relation between tight mechanism-specific and mechanism-agnostic bounds does in fact extend beyond group privacy (see Fig. 2): As we demonstrate in Appendices G and H, mechanism-agnostic subsampling guarantees from prior work can equivalently be derived by (1) conditioning on at most \(4\) events indicating the presence of inserted / deleted / substituted elements, (2) defining a simultaneous coupling, and (3) using joint convexity to upper-bound the resultant mechanism-specific guarantee by component divergences. This includes the ADP guarantees of Balle et al.  (and thus prior work [14; 43; 44]) and the RDP guarantees from [28; 29; 30; 40].

**Subsumption of .** Going even further, we show in Appendix G.2 that our approach subsumes the entire framework of Balle et al. : Any mechanism-agnostic guarantee derived via their ansatz can equivalently be derived by defining a (potentially suboptimal) simultaneous coupling between four subsampling distributions and upper-bounding the resultant guarantee via (advanced) joint convexity.

**Contribution.** Importantly, our contribution is not in the bounds themselves, but in the identification of this implicitly underlying pattern that unifies prior work. We further generalize this pattern through our proposed framework to enable tight analysis for more challenging scenarios like group privacy.

**Novel RDP accounting bounds.** In Appendix I, we use this observation to derive a variety of novel mechanism-agnostic RDP bounds for different combinations of \(_{}\), \(_{}\), and subsampling schemes, such as insertion/removal and subsampling without replacement. We further derive a simple but tight mechanism-specific guarantee for subsampled randomized response under substitution (see Theorem I.3) that outperforms the best known mechanism-agnostic bound (see Section 4).

Figure 2: Mechanism-agnostic guarantees for (a) graph modification [40; 41; 42] (b) insertion/removal [14; 15; 29; 30] (c) substitution [43; 44; 15; 28] can be derived from (d) our proposed framework. In (b-c), events \(A_{i}\) and \(E_{j}\) indicate the presence of inserted or substituted elements.

### From mechanism-specific guarantees to dominating pairs.

Because the mechanism-specific guarantees derived via our framework do not obfuscate the underlying distributions, they can be used to identify dominating pairs. These dominating pairs can then be combined with arbitrary (numerical) accountants to track privacy under composition. For example, we can immediately read off from Theorem 3.8 that the two Gaussian mixtures are a dominating pair for the "insert-\(K_{+}\)-remove-\(K_{-}\)" relation. In Appendix J, we describe a procedure for constructing dominating pairs when the bound is a weighted sum of multiple mixture divergences. We can thus determine dominating pairs for any bound derived via optimal transport. As we demonstrate in Appendix K, the dominating pairs for subsampled mechanisms derived by Zhu et al.  (special cases of which appear in ) can equivalently be proven via our framework.

**Subsampling with replacement.** As a novel contribution, we derive for the first time dominating pairs for subsampling _with_ replacement (see Theorem L.5), which were posited but not proven in  (see discussion in Appendix L). This is enabled by our solution to the problem in Theorem 3.7. As we experimentally show in Appendix B.1.5, these bounds can be much stronger than those derived via the framework of Balle et al. . These results thus demonstrate that _there is a qualitative difference between mechanism-specific and mechanism-agnostic tightness even for group size \(K=1\)._

### Limitations and future work

**Limitations.** While our proposed framework can be applied to arbitrary subsampling distributions, conditioning on a finite set of events may be too restrictive in certain settings (e.g., continuous batch spaces). Also, while we found conditioning on the number of modified elements to be sufficient for all considered scenarios, an automated procedure for selecting events would be desirable. Maximal couplings fulfill this purpose in , but only yield pairs of distributions.

**Future work.** A natural direction is applying our ansatz to novel settings other than group privacy. In particular, it could potentially be used to provide epoch-level guarantees for correlated subsampling (e.g., batching via shuffling), similar to . We present a preliminary result for \(2\)-fold non-adaptive composition in Appendix P. Future work could also use our solutions to the problem in Theorem 3.7 to generalize the matrix mechanism analysis from  to substitutions and non-Gaussian distributions.

## 4 Experimental evaluation

The purpose of the following experiments is to verify that there can be a benefit to using mechanism-specific over mechanism-agnostic subsampling bounds, and that a joint analysis of subsampling and group privacy can offer stronger guarantees than post-hoc application of the generic group privacy property. In all figures, "specific" refers to our proposed mechanism-specific bounds, "agnostic" refers to (tight) mechanism-agnostic bounds, and "post-hoc" refers to applying the generic group privacy property to tight mechanism-specific bounds derived for group size \(1\). For all experiments, we assume \(_{p}\) sensitivities of \(1\). Further details on the experimental setup are provided in Appendix C. An implementation will be made available at https://cs.cit.tum.de/daml/group-amplification.

Figure 3: Randomized response with WOR subsampling (\(q\)\(/\)\(N=0.001\)), group size \(1\), and varying true response probability \(\).

### Mechanism-agnostic and mechanism-specific guarantees

**Randomized response and RDP.** One potential source of looseness in mechanism-agnostic bounds is that they bound mixture divergences in terms of component divergences that can be summarized by a single privacy parameter. As an example, let us consider the best known guarantee for substitution, subsampling without replacement, and RDP from . As discussed by the authors, it is only tight up to factors that are constant in \(\). In Fig. 4 we compare it to our tight mechanism-specific bound for randomized response (see Theorem I.3) with batch-to-dataset ratio \(q\ /\ N=0.001\) and true response probability \(\{0.6,0.75,0.9\}\). In Appendix B.1.1 we consider other ratios. The tight guarantee eliminates the constant factors and thus achieves much smaller \(\) for a wide range of \((1,10^{4}]\).

**Group privacy and ADP.** Another potential source of looseness in mechanism-agnostic bounds is that they stem from a binary partitioning of the batch space (recall Fig. 2), which may not be sufficient when there are multiple possible levels of privacy leakage (e.g. number of sampled group elements). We demonstrate this in Fig. 4, by comparing the tight mechanism-agnostic group privacy bound derived via the framework from  to our tight mechanism-specific bound for Laplace mechanisms (see Theorem M.2), ADP, scale \(=1\), subsampling rate \(r=0.2\), and varying group size. In Appendix B.1.2 we repeat the experiment with other mechanisms and parameter values, and also consider RDP. As discussed in Section 3.2, the mechanism-agnostic bound is identical to the mechanisms-specific bound for group size \(K=1\). For group sizes \(K 2\), however, the fine-grained partitioning underlying the mechanism-specific bound yields stronger privacy guarantees. These results confirm that we need to distinguish between mechanism-agnostic and mechanism-specific tightness when analyzing complex subsampling settings.

### Post-hoc and mechanism-specific group privacy analysis

**Single-iteration group privacy.** Our next goal is to demonstrate the benefit of analyzing group privacy and subsampling jointly. In Fig. 6, we evaluate our tight guarantee for Gaussian mechanisms (see Theorem 3.8) with \(=2\), rate \(r=0.2\), and varying group size. As a baseline, we evaluate the same tight guarantee with \(K_{+}+K_{-}=1\) and apply the generic group privacy property (see Appendix C.1.4) in a post-hoc manner. As can be seen, our tight analysis can lead to much stronger privacy guarantees. However, we interestingly observe in Appendix B.2.1 that the generic group privacy properties of ADP and RDP can serve as increasingly tight upper bounds when considering more private base mechanisms (e.g., \(=5\)) and much smaller subsampling rates (e.g., \(r=10^{-3}\)).

**Composed group privacy.** Nevertheless, even the gaps in tightness for very private mechanisms may quickly accumulate when repeatedly applying these mechanisms to a dataset. In Fig. 6, we use the dominating pairs derived via our framework to conduct tight PLD accounting  with "connect the dots"  quantization using Google's dp_accounting library . For \(=5\), \(r=10^{-3}\), and fixed \(=2\) (for other paramaters and mechanisms, see Appendix B.2.3), the post-hoc analysis quickly diverges with increasing group size and number of iterations. For example, with group size \(16\) and privacy budget \(=10^{-6}\), the post-hoc analysis allows less than \(100\) iterations of DP-SGD training , whereas our tight analysis enables training for over \(1000\) iterations. In Appendix B.2.5 we train an image classification model on MNIST with PLD accounting to demonstrate that this increased number of training iterations can translate to much higher model utility.

## 5 Related Work

**Amplification by subsampling for ADP.** Using subsampling to strenghten \((,)\)-DP guarantees has a long history [13; 14] particularly in privacy-preserving machine learning [46; 47; 6]. For a thorough introduction to subsampling and its interaction with composition, we refer the reader to . Balle et al.  ultimately proposed a framework for deriving tight mechanism-agnostic subsampling guarantees. Our work subsumes theirs and enables the derivation of mechanism-specific guarantees.

**Amplification for privacy accountants.** Abadi et al. 's seminal work on moments accounting already considered subsampling. Similiar results were derived for the more general notion of RDP  in [28; 29; 30]. More recent works on accounting generally include a discussion of either Poisson subsampling or subsampling without replacement [17; 18; 19; 21; 11]. Subsampling with replacement is discussed in , albeit without complete proofs, which we provide in Appendix L. We demonstrate that amplification guarantees for the central notions of RDP  and dominating pairs  can - just like guarantees for ADP - be derived via optimal transport. Our novel approach not only unifies prior, but also enables a principled analysis of challenging scenarios like group privacy.

**Group privacy amplification.** A special case of group privacy amplification (which we use as an example to demonstrate the utility of our general framework) with hockey stick divergences, Gaussian mechanisms, and groups that collaboratively agree to contribute all their data was posited in  and proven in a recent follow-up note . However, their result (and proof strategy) does not cover Renyi divergences, other mechanisms (Laplace, randomized response), and the standard notion of group privacy under which group members are not forced to collaborate (see, e.g., [3; 7; 9; 15; 23]).

**Hierarchical randomized smoothing.** Randomized smoothing [49; 50; 51] is a technique for constructing provably robust models via DP achieved through input perturbations (e.g. [52; 53; 54; 55; 56; 57; 58]). Similar to subsampling, Scholten et al.  proposed to only apply perturbations to randomly sampled parts of an input. We repurpose their technique of treating subsampling indicators as observable random variables in order to construct dominating pairs from weighted sums of divergences (see Appendix J.1).

**Unified amplification for \(f\)-DP.** Wang et al.  proved a form of joint concavity for the tradeoff functions underlying \(f\)-DP accounting [9; 34]. This enables them to analyze mixtures induced by random initialization and shuffling in a unified manner. However, they do _not_ consider amplification by subsampling, and explicitly state that they need to address this problem in future work. Note that dominating pairs, which can be derived via our proposed framework, can be used in f-DP accounting due to duality of privacy profiles and tradeoff functions [9; 10].

## 6 Conclusion

The main purpose of this work is to provide a unified, principled framework for mechanism-specific subsampling analysis and subsampling analysis for privacy accountants. To this end, we proposed a three-step procedure based on optimal transport between conditional subsampling distributions. Beyond recovering known guarantees for Renyi DP and dominating pairs, this procedure lets us derive novel results that were previously only available for approximate DP, such as non-standard combinations of subsampling schemes and neighboring relations, or subsampling with replacement. We then applied this procedure to the problem of analyzing group privacy under subsampling. Our experimental evaluation demonstrates that our mechanism-specific group privacy amplification bounds are not only tight, but can also significantly outperform tight mechanism-agnostic bounds and traditional group privacy results - even under composition. On a higher level, these bounds represent a novel contribution to a larger body of work (e.g., [71; 38; 72]) that demonstrates the benefit of analyzing multiple properties of differential privacy jointly instead of independently.

## 7 Acknowledgements

We are grateful to Georgios Kaissis for valuable discussions and feedback on generalizing our work beyond Renyi differential privacy, Yan Scholten for pointing out connections between subsampled mechanisms and hierarchical randomized smoothing, as well as Leo Schwinn and Nicholas Gao for proofreading our manuscript. This research was funded by the German Research Foundation, grant GU 1409/4-1, and the Munich Data Science Institute (MDSI) at Technical University of Munich (TUM) via the Linde/MDSI Doctoral Fellowship program and the MDSI Seed Fund.