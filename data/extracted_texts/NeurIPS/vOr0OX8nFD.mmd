# M3CoL: Harnessing Shared Relations via

Multimodal Mixup Contrastive Learning

for Multimodal Classification

Raja Kumar\({}^{}\)

Indian Institute of Technology Bombay, Mumbai, India

Raghav Singhal\({}^{}\)

Indian Institute of Technology Bombay, Mumbai, India

Pranamya Kulkarni

Indian Institute of Technology Bombay, Mumbai, India

Deval Mehta

AIM for Health Lab, Department of Data Science & AI, Monash University, Australia

Kshitij Jadhav

Indian Institute of Technology Bombay, Mumbai, India

###### Abstract

Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose **M3CoL**, a **M**ulti**modal **M**ixup **C**ontrastive **L**earning approach to capture nuanced _shared relations_ inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with the corresponding samples from other modalities. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that **M3CoL** effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research.

## 1 Introduction

In the era of abundant multimodal data, it is crucial to equip artificial intelligence with multimodal capabilities . At the heart of advancements in multimodal learning is contrastive learning, which maximizes similarity for positive pairs and minimizes it for negative pairs, making it practical for multimodal representation learning. CLIP  is a prominent example that employs contrastive learning to understand the direct link between paired modalities and seamlessly maps images and text into a shared space for cross-modal understanding. However, traditional contrastive learning methods often overlook shared relationships between samples across different modalities, which can result in the learning of representations that are not fully optimized for capturing the underlying connections between diverse data types. These methods focus on distinguishing between positive and negative pairs of samples, typically treating each instance as an independent entity. They tend to disregard the rich, shared relational information that could exist between samples within and across modalities.

While traditional contrastive learning methods treat paired modalities as positive samples and non-corresponding ones as negative, they often overlook shared relations between different samples. As shown in the left panel of Figure 1 (Left panel), classical contrastive learning approach assumes perfect one-to-one relations between modalities, which is rare in real-world data. For example, sharedelements in images or text can relate even across separate samples, as illustrated by the elements like _"tomato sauce"_ and _"basil"_ in Figure 1. Our approach, illustrated in the right panel of Figure 1, goes beyond simple pairwise alignment by capturing shared relationships across mixed samples. By creating newer data points through convex combinations of data points our method effectively models complex relationships, such as imperfect bijections , enhancing multimodal performance.

Our approach builds upon the success of data augmentation techniques such as Mixup  and their variants [5; 6; 7], which have proven beneficial for enhancing learned feature spaces, improving both robustness and performance. Mixup trains models on synthetic data created through convex combinations of two datapoint-label pairs . These techniques are particularly valuable in low sample settings, as they help prevent overfitting and the learning of ineffective shortcuts [9; 10], common in contrastive learning. Building on the success of recent Mixup strategies [11; 12; 13] and MixCo , we introduce M3Co, a novel approach that adapts and enhances contrastive learning principles to complex multimodal settings. M3Co modifies the CLIP loss to handle multimodal scenarios, addressing the problem of instance discrimination, where models overly focus on distinguishing individual instances instead of capturing relationships between modalities. M3Co eliminates instance discrimination and enhances robust representation learning by capturing shared relations. Our results demonstrate improvements in performance and generalization across a range of multimodal tasks.

## 2 Methodology

**Pipeline Overview.** Figure 2 depicts our framework, which comprises of three components: unimodal prediction modules, a fusion module, and a Mixup-based contrastive loss. We obtain latent representations (using learnable modality specific encoders \(f^{(1)}\) and \(f^{(2)}\)) of individual modalities and fuse them (denoted by concatenation symbol '+') to generate a joint multimodal representation, which is optimized using a supervised objective (through classifier 3). The unimodal prediction modules provide additional supervision during training (via classifier 1 and 2). These strategies enable deeper integration of modalities and allow the models to compensate for the weaknesses of one modality with the strengths of another. The Mixup-based contrastive loss (denoted by \(_{}\)) updates the representations by capturing shared relations inherent in the multimodal data.

Figure 1: Comparison of traditional contrastive and our proposed M3Co loss. \(^{(1)}_{i}\) and \(^{(2)}_{i}\) denote representations of the \(i\)-th sample from modalities 1 and 2, respectively. Traditional contrastive loss (left panel) aligns corresponding sample representations across modalities. M3Co (right panel) mixes the \(i\)-th and \(j\)-th samples from modality 1 and enforces the representations of this mixture to align with the representations of the corresponding \(i\)-th and \(j\)-th samples from modality 2, and vice versa.

Figure 2: Architecture of our proposed M3CoL model.

**Multimodal Mixup Contrastive Learning.** Given a batch of \(N\) multimodal samples, let \(_{i}^{(1)}\) and \(_{i}^{(2)}\) denote the \(i\)-th samples for the first and second modalities, respectively. The modality encoders, \(f^{(1)}\) and \(f^{(2)}\), generate the corresponding embeddings \(_{i}^{(1)}\) and \(_{i}^{(2)}\):

\[_{i}^{(1)}=f^{(1)}(_{i}^{(1)}),_{i}^{(2)}= f^{(2)}(_{i}^{(2)})\] (1)

We generate a mixture, \(}_{i,j}^{(1)}\), of the samples \(_{i}^{(1)}\) and \(_{j}^{(1)}\) by taking their convex combination. Similarly, we generate a mixture, \(}_{i,k}^{(2)}\), using the convex combination of the samples \(_{i}^{(2)}\) and \(_{k}^{(2)}\) (Eq. 2). For the text modality, instead of directly mixing the raw inputs, we mix the text embeddings . The mixing indices \(j,k\) are drawn arbitrarily, without replacement, from \([1,N]\), for both the modalities. We mix both the modalities using a factor \((,)\). Based on the findings of , which demonstrated enhanced performance for \(\) values between 0.1 and 0.4, we chose \(=0.15\) after experimenting with several values in this range. The mixtures are fed through the respective encoders to obtain the embeddings: \(}_{i,j}^{(1)}\), and \(}_{i,k}^{(2)}\) (Eq. 3).

\[}_{i,j}^{(1)}=_{i}_{i}^{(1 )}+(1-_{i})_{j}^{(1)}, }_{i,k}^{(2)}=_{i}_{i}^{(2)}+(1- _{i})_{k}^{(2)}\] (2) \[}_{i}^{(1)}=}_{i,j}^{(1)}=f^{( 1)}(}_{i,j}^{(1)}), }_{i}^{(2)}=}_{i,k}^{(2)}=f^{(2)}( }_{i,k}^{(2)})\] (3)

We generate embeddings for the entire batch \(}^{(1)}\) and \(}^{(2)}\), where the \(i\)-th elements, \(}_{i}^{(1)}\) and \(}_{i}^{(2)}\), correspond to \(}_{i,m_{i}}^{(1)}\) and \(}_{i,m_{i}}^{(2)}\), respectively. The unidirectional contrastive loss  over \(^{(2)}\) is conventionally defined as:

\[_{}(^{(1)},^{(2)})=- _{i=1}^{N}_{i}^{(1)}_{i}^{(2)}/)}{_{j=1}^{N}( _{i}^{(1)}_{j}^{(2)}/)}\] (4)

where \(\) indicates dot product and \(\) is a temperature hyperparameter. While this formulation is suitable for computing similarity among aligned samples from different modalities, our method requires flexibility to handle both aligned and non-aligned samples. To achieve this, we define the unidirectional multimodal contrastive loss between \(_{i}^{(1)}\) and \(_{m}^{(2)}\) over \(^{(2)}\) as:

\[_{}(_{i}^{(1)},^{(2)};m)=-_{i}^{(1)}_{m}^{(2)}/ )}{_{j=1}^{N}(_{i}^{(1)}_{j}^{(2)}/)}\] (5)

where \(^{(1)}\) and \(^{(2)}\) are \(^{2}\) normalized, \(\) is a temperature hyperparameter, and \(m\) is a sample index in \([1,N]\). Although the multimodal contrastive loss (Eq. 5) can learn indirect relations, it is insufficient for learning shared semi-positive relations between modalities. Therefore, we introduce a Mixup-based contrastive loss to capture these relations that promotes generalized learning, as this process is more nuanced than simply discriminating positives from negatives. Now, following standard works , we make our loss bidirectional. We define this bidirectional Mixup contrastive loss M3Co for each modality (Eq. 6, 7) and the total M3Co loss as:

\[_{}^{(1)} =_{i=1}^{N}[_{i}_{}(}_{i,j}^{(1)},^{(2)};i)+(1-_{i}) _{}(}_{i,j}^{(1)},^{(2)}; j)]\] \[+_{i=1}^{N}\{_{i}_{}(_{i}^{(2)},}^{(1)};i)+(1-_{i}) _{}(_{j}^{(2)},}^{(1)}; i)\}\] (6)

\[_{}^{(2)} =_{i=1}^{N}[_{i}_{}(}_{i,k}^{(2)},^{(1)};i)+(1-_{i}) _{}(}_{i,k}^{(2)},^{(1)}; k)]\] \[+_{i=1}^{N}\{_{i}_{}(_{i}^{(1)},}^{(2)};i)+(1-_{i}) _{}(_{k}^{(1)},}^{(2)}; i)\}\] (7)\(_{}^{(1,2)}=(_{}^{(1)} +_{}^{(2)})\), where \(^{(1)}\), \(}^{(1)}\), \(^{(2)}\), and \(}^{(2)}\) are \(^{2}\) normalized. Note that the parts of the loss functions in Eq. (6, 7) inside curly parantheses make them bidirectional. Mixup-based methods enhance generalization by capturing clean patterns in the early training stages but can eventually overfit to noise if continued for too long [20; 21; 22]. To address this, we implement a schedule that transitions from the Mixup-based M3Co loss to a non-Mixup multimodal contrastive loss. We design this transition so that the non-Mixup loss retains the ability to learn shared or indirect relationships between modalities. By using a bidirectional SoftClip-based loss [9; 16; 23], we relax the rigid one-to-one correspondence, allowing the model to capture many-to-many relations [23; 24]. The bidirectional **MultiSoftClip** loss for each modality (Eq. 8, 9) and its combination is:

\[_{}^{(1)}= _{i=1}^{N}_{l=1}^{N}_{i}^{(1)}_{l}^{(1)}/)}{_{t=1}^ {N}(_{i}^{(1)}_{t}^{(1)}/)} (_{}(_{i}^{(2)},^{(1)};l)+ _{}(_{l}^{(1)},^{(2)};i)) \] (8) \[_{}^{(2)}= _{i=1}^{N}_{l=1}^{N}_{i}^{(2)}_{l}^{(2)}/)}{_{t=1}^ {N}(_{i}^{(2)}_{t}^{(2)}/)} (_{}(_{i}^{(1)},^{(2)};l)+ _{}(_{l}^{(2)},^{(1)};i)) \] (9) \[_{}^{(1,2)}=(_{}^{(1)}+_{}^{(2)})^{(1)}^{(2)} ^{2}M\] (10)

**Unimodal Predictions and Fusion.** The encoders produce latent representations for each of the \(M\) modalities, serving as inputs to individual classifiers that generate modality-specific predictions. These representations are used for modality-specific supervision only during training. The unimodal prediction task, \(_{}\), involves minimizing the cross-entropy loss \(_{}\) between these predictions and the corresponding ground truth labels, for each modality. We merge the unimodal latent representations by concatenating them and pass the combined representation to the output classifier. These predictions serve as the final outputs used during inference. The multimodal prediction process, \(_{}\), minimizes the cross-entropy loss between the predictions and the corresponding labels.

**Combined Learning Objective.** Our overall loss objective utilizes a schedule to combine our M3Co and MultiSClip loss functions weighted by a hyperparameter \(\), along with the unimodal and multimodal cross-entropy losses. We use M3Co for the first one-third  part of training, and then transition to MultiSClip. The end-to-end loss is defined as:

\[_{}=_{\,|\,}+_{}+_{}\] (11)

## 3 Experiments and Results

**Datasets and Implementation Details.** We evaluate on four diverse multimodal classification datasets: N24News , Food-101 , ROSMAP , and BRCA . N24News and Food-101 are image-text classification datasets. ROSMAP and BRCA are medical datasets, each containing three modalities: DNA methylation, miRNA expression, and mRNA expression. We use a ViT  as the image encoder for N24News and Food-101. For N24News, the text encoder is a pretrained BERT/RoBERTa [29; 30], while we use a pretrained BERT as the text encoder for Food-101. The classifiers for the above two datasets are three layer MLPs with ReLU activations. For ROSMAP and BRCA, which are small datasets, we use two layer MLPs as feature encoders for each modality, and two layer MLPs as classifiers. Details and related work are presented in Appendix A.1 and A.5.

**Results.** The experimental results from Table 1, 2, 5, reveal the following findings: **(i)** M3CoL consistently outperforms all SOTA methods across all text sources on N24News when using the same encoders, beats SOTA on all evaluation metrics on ROSMAP and BRCA, and also achieves competitive results on Food-101; **(ii)** contrastive-based methods with any form of alignment demonstrate superior performance compared to other multimodal methods; **(iii)** our proposed M3CoL method,which employs a contrastive-based approach with shared alignment, improves over the traditional contrastive-based models and the SOTA multimodal methods. We present a detailed analysis of the various components of our method in Table 6, and text-guided visualization in Appendix A.4.

**Discussion and Conlusions.** Aligning representations across modalities presents significant challenges due to the complex, often non-bijective relationships in real-world multimodal data . These relationships can involve many-to-many mappings or even lack clear associations, as exemplified by linguistic ambiguities and synonymy in vision-language tasks. We propose M3Co, a novel contrastive-based alignment method that captures shared relations beyond explicit pairwise associations by aligning mixed samples from one modality with corresponding samples from others. Our approach incorporates Mixup-based contrastive learning, introducing controlled noise that mirrors the inherent variability in multimodal data, thus enhancing robustness and generalizability. The M3Co loss, combined with an architecture leveraging unimodal and fusion modules, enables continuous updating of representations necessary for accurate predictions and deeper integration of modalities. This method generalizes across diverse domains, including image-text, high-dimensional multi-omics, and data with more than two modalities. Experiments on four public multimodal classification datasets demonstrate the effectiveness of our approach in learning robust representations that surpass traditional multimodal alignment techniques.

    &  &  &  \\   & **AGG** & **ALI** & **Image** & **Text** & **Headline** & **Caption** & **Abstract** \\  Image-only & - & - & ViT & - &  \\  Text-only & - & - & - & BERT & \(72.1\) & \(72.7\) & \(78.3\) \\ UniConcat & Early & ✗ & ViT & BERT & \(78.6\) & \(76.8\) & \(80.8\) \\ UniS-MMC & Early & ✓ & ViT & BERT & \(80.3\) & \(77.5\) & \(83.2\) \\ M3CoL (Ours) & Early & ✓ & ViT & BERT & \(_{_{0.05}}\) & \(_{_{0.03}}\) & \(_{_{0.06}}\) \\  Text-only & - & - & - & RoBERTa & \(71.8\) & \(72.9\) & \(79.7\) \\ UniConcat & Early & ✗ & ViT & RoBERTa & \(78.9\) & \(77.9\) & \(83.5\) \\ N24News & Early & ✗ & ViT & RoBERTa & \(79.41\) & \(77.45\) & \(83.33\) \\ UniS-MMC & Early & ✓ & ViT & RoBERTa & \(80.3\) & \(78.1\) & \(84.2\) \\ M3CoL (Ours) & Early & ✓ & ViT & RoBERTa & \(_{_{0.19}}\) & \(_{_{0.08}}\) & \(_{_{0.03}}\) \\   

Table 1: Classification Accuracy (ACC) on N24News on three different text sources. AGG denotes early/late modality fusion, ALI indicates presence/absence of alignment. Our method consistently outperforms the state-of-the-art across all text sources and backbone combinations.

    &  &  &  \\   & **AGG** & **ALI** & **ACC**\(\) & **F1**\(\) & **AUC**\(\) & **ACC**\(\) & **WF1**\(\) & **MF1**\(\) \\  GRidge & Early & ✗ & \(76.0\) & \(76.9\) & \(84.1\) & \(74.5\) & \(72.6\) & \(65.6\) \\ BPLSDA & Early & ✗ & \(74.2\) & \(75.5\) & \(83.0\) & \(64.2\) & \(53.4\) & \(36.9\) \\ BSPLSDA & Early & ✗ & \(75.3\) & \(76.4\) & \(83.8\) & \(63.9\) & \(52.2\) & \(35.1\) \\ MOGONET & Late & ✗ & \(81.5\) & \(82.1\) & \(87.4\) & \(82.9\) & \(82.5\) & \(77.4\) \\ TMC & Late & ✗ & \(82.5\) & \(82.3\) & \(88.5\) & \(84.2\) & \(84.4\) & \(80.6\) \\ CF & Early & ✗ & \(78.4\) & \(78.8\) & \(88.0\) & \(81.5\) & \(81.5\) & \(77.1\) \\ GMU & Early & ✗ & \(77.6\) & \(78.4\) & \(86.9\) & \(80.0\) & \(79.8\) & \(74.6\) \\ MOSEGCN & Early & ✗ & \(83.0\) & \(82.7\) & \(83.2\) & \(86.7\) & \(86.8\) & \(81.1\) \\ DYNAMICS & Early & ✗ & \(85.7\) & \(86.3\) & \(91.1\) & \(87.7\) & \(88.0\) & \(84.5\) \\  M3CoL (Ours) & Early & ✓ & \(_{_{0.94}}\) & \(_{_{0.94}}\) & \(_{_{0.59}}\) & \(_{_{0.57}}\) & \(_{_{0.42}}\) & \(_{_{0.54}}\) \\   

Table 2: Comparison of Classification Accuracy (ACC), Area Under the Curve (AUC), F1 score (F1) on ROSMAP, and Classification Accuracy (ACC), Weighted F1 score (WF1), and Micro F1 score (MF1) on BRCA datasets. AGG denotes early/late modality fusion, ALI indicates presence/absence of alignment. Our method significantly outperforms the state-of-the-art across all metrics.