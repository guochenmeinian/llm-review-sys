# DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation

Yuang Ai\({}^{,}\)\({}^{}\) Xiaoqiang Zhou\({}^{,}\)\({}^{}\)\({}^{}\) Huailo Huang\({}^{,,,}\) Xiaotian Han\({}^{}\) Zhengyu Chen\({}^{}\) Quanzeng You\({}^{}\) Hongxia Yang\({}^{}\)

\({}^{}\)MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences

\({}^{}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{}\)ByteDance, Inc \({}^{}\)University of Science and Technology of China

shallowrean555@gmail.com, huaibo.huang@cripac.ia.ac.cn

Code and models: https://github.com/shallowream204/DreamClear

###### Abstract

Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. **GenIR**, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a

Figure 1: We present **DreamClear**, a high-capacity image restoration model that delivers photorealistic restoration of real-world LQ images, outperforming SOTA diffusion-based models in handling diverse degradations.

large-scale dataset of one million high-quality images. Our second contribution, **DreamClear**, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration.

## 1 Introduction

Image restoration (IR), a vital field in computer vision, targets transforming degraded low-quality (LQ) images into high-quality (HQ) counterparts. While IR has achieved significant advancements under predefined conditions, such as super-resolution [92; 10] and denoising [12; 40] tasks, real-world IR remains a formidable challenge due to the diversity and complexity of degradation types. The disconnect between training data and real-world scenarios is substantial, as existing datasets inadequately encapsulate the intricacies of real-world degradations. Efforts to bridge this gap include domain adaptation [5; 74; 23; 81], dataset collection [68; 8; 11; 91], and degradation simulation [64; 85; 49; 71]. However, in contrast to the leaps in Natural Language Processing (NLP)  and AI-Generated Content (AIGC)  enabled by large-scale models and extensive data, IR's progress is not as pronounced. Real-world challenges persist, and the potential of large-scale data and high-capacity models remains largely untapped. This leads us to two critical questions: _how can we obtain a large-scale dataset that accurately represents real-world IR, and based on this, how can we construct powerful models tailored for real-world IR scenarios?_

Addressing the first question, considerable efforts have been made to curate IR datasets. Given the challenge of collecting real-world paired IR data, these datasets are typically constructed by acquiring HQ images and then simulating degradations to generate corresponding LQ images. While many works [64; 85; 49; 71] have refined the degradation simulation process, this paper focuses on the acquisition of HQ images and the associated challenges of copyright and privacy protection. The predominant method for obtaining HQ images is web scraping. Current open-source IR datasets, such as DIV2K  and Flickr2K , contain only a few thousand images, insufficient for covering a broad spectrum of real-world scenarios. Larger collections like SUPIR , with 20 million images, highlight the labor-intensive nature of large-scale dataset curation. Moreover, images sourced from the internet often involve copyright issues and privacy concerns, particularly with identifiable human faces. To advance the IR field effectively, there is an urgent need for a dataset curation method that is privacy-safe and cost-effective.

In response, we present an under-explored approach in the image restoration (IR) field: creating high-quality, non-existent images to enhance dataset curation efficiency, while evading copyright and privacy issues. We unveil **GenIR**, a privacy-conscious, automated data curation pipeline that repurposes the generative prior in pretrained text-to-image (T2I) models for IR tasks, and uses multimodal large language models (MLLMs) to generate text prompts, thereby improving data synthesis quality. GenIR operates in three stages: (1) image-text pairs construction, (2) dual-prompt fine-tuning, and (3) data generation & filtering. Initially, GenIR utilizes existing IR datasets and the advanced MLLM, Gemini-1.5-Pro , to create image-text pairs, while generating negative samples via an image-to-image pipeline . Subsequently, we apply a dual-prompt learning strategy to adapt pretrained T2I models to the IR task, generating suitable prompts for data synthesis. In the final stage, MLLMs create various scene descriptions and synthesize images using the adapted image prior, with a focus on ensuring no identifiable individuals are included. MLLMs also assess and filter the synthesized data based on quality, producing high-quality images that are privacy-safe and copyright-free. Through GenIR, we generate a dataset of one million high-quality images, proving its efficacy in training a robust real-world IR model.

Armed with a large-scale, high-quality image dataset, our focus shifts to the construction of a high-capacity IR model that can robustly generalize to real-world scenarios. Recent state-of-the-art approaches [77; 70; 80] employ the generative priors in pretrained Stable Diffusion  (SD) for realistic image restoration, underlining the power of rich generative prior in SD. As Fig.1illustrates, SD-based methods outperform GAN-based ones. However, these strategies often neglect the degradation priors in input low-quality images, a critical element in blind IR . This insight leads us to investigate the integration of degradation prior into diffusion-based IR models, and how to optimize its synergy with large models.

In this paper, we introduce **DreamClear**, a high-capacity real-world image restoration model, grounded on a large dataset. DreamClear is based on Diffusion Transformer (DiT) , the cornerstone of modern diffusion-based systems (_e.g._, Sora , Stable Diffusion 3 ). Our model employs a dual-branch framework with textual guidance from multi-modal large language models (MLLMs) for photorealistic restoration. DreamClear first processes the low-quality image through a lightweight network to produce a reference image. We propose ControlFormer to enhance the control over DiT-based T2I models, thereby better utilizing the low-quality and reference images to guide the content of the generated image. To further improve the model's generalization across diverse and complex degradations, we incorporate implicit prior degradation information to refine the solution space. Specifically, we suggest a Mixture of Adaptive Modulator (MoAM), which extracts token-wise degradation representations and dynamically integrates various restoration experts for each token based on the Mixture-of-Experts (MoE)  structure, thereby enhancing the model's adaptability to different degradation severities (See Fig. 1).

The main contributions of this work can be summarized as follows:

* We propose GenIR, a pioneering automated data curation pipeline for image restoration. It addresses the urgent need for privacy-safe and cost-effective methods in image restoration, yielding a dataset of one million high-quality images.
* We present DreamClear, a robust, high-capacity IR model that incorporates degradation priors into diffusion-based frameworks. This model improves control over content generation, adapts to various degradations, and generalizes well across diverse real-world scenarios.
* Extensive experiments across both low-level (synthetic & real-world) and high-level (detection & segmentation) benchmarks have demonstrated DreamClear's state-of-the-art performance in handling intricate real-world scenarios.

## 2 Related Work

Image Restoration.Image Restoration aims at restoring a high-quality image from the low-quality input image. Over the past decade, different approaches have been proposed for image super-resolution [95; 31; 65; 96], denoising [87; 84; 67], deblurring [57; 69; 56], deraining [14; 34; 32; 33], inpainting [97; 3], etc. Recently, researchers have increasingly focused on enhancing the generalization ability to diverse degradations in real-world applications [74; 68; 63]. The degradation simulation is improved from simple degradations to complex degradation processes, such as BSRGAN , Real-ESRGAN  and AnimeSR . With the improved degradation simulation process, many recent methods can deal with diverse degradation types and achieve promising performance in real-world scenarios [43; 8]. With the paired data, training a randomly initialized restoration model from scratch is one way to improve generalization ability . The other way is to exploit the generative prior in the pre-trained generative model, such as GAN or diffusion models [52; 63; 4; 37; 66; 9]. In this work, we propose a data synthesis pipeline and introduce a real-world image restoration model with high generalizability.

Generative Prior.Generative models learn the image synthesis process and embed the image prior in the model weights. The image prior in a high-quality image generator, such as StyleGAN  and Stable Diffusion , can be adapted to other visual restoration tasks [43; 8; 38; 20]. To use the image prior in GANs, an additional encoder is often applied to convert the input image to the latent space [76; 48]. For the diffusion models, the forward process adds noise to the image gradually and finally converts the image to the latent noise space [15; 50]. By manipulating in the latent feature space, the input image is integrated into the generation process as a conditional input, and the synthesis process exploits the image prior in the pre-trained models. The generative prior in the pre-trained models can also serve as a good initialization for downstream synthesis tasks [63; 78; 35]. We exploit the generative prior in the pre-trained diffusion models to synthesize datasets for image restoration tasks and train a restoration model for real-world applications.

Synthetic Dataset.Data size and data quality are widely recognized as essential for many vision tasks. A large-scale high-quality dataset can facilitate the large model training and improve the model ability greatly [83; 22; 21; 24; 26]. Existing large-scale datasets are often manually collected with laborious human efforts [16; 39]. More importantly, the data crawled from the internet may leak privacy information [58; 24], raising concerns related to AI security. The synthesized datasets can not only reduce the laborious human efforts, but also avoid the privacy information leakage. High-quality synthesized datasets are verified to be effective in many vision tasks [28; 25; 6]. Our work is the first to explore the dataset synthesis in the image restoration field.

## 3 Privacy-Safe Dataset Curation Pipeline

Traditionally, IR datasets are created by scraping web images and simulating degradations to generate low-quality (LQ) counterparts. This process is labor-intensive and rife with copyright and privacy issues, especially with identifiable human faces. To advance the IR field, a privacy-safe and cost-effective dataset curation method is needed. Drawing inspiration from the success of text-to-image (T2I) models in synthesizing high-quality images, we introduce the GenIR pipeline. This novel approach leverages the generative priors of pre-trained T2I models to construct extensive, privacy-safe datasets. However, the efficacy of T2I models is contingent upon carefully crafted prompts for generating high-quality images fitting for IR tasks.

To tackle this, GenIR, as illustrated in Fig. 2, employs a streamlined three-stage process. Initially, we construct positive and negative samples, each paired with corresponding text prompts. Subsequently, a dual-prompt based finetuning strategy concurrently learns both positive and negative prompts. Finally, we utilize LLMs to generate a diverse array of text prompts, leading to the creation and filtering of data. Throughout this process, we maintain stringent privacy standards, ensuring no specific personal information is embedded in the text prompts or the generated images.

Image-Text Pairs Construction.We use high-resolution, texture-rich images in existing IR datasets [44; 2; 22; 39] as positive samples. Given the unavailability of corresponding text prompts, we employ the sophisticated MLLM, Gemini-1.5-Pro , to generate necessary prompts via language instructions. Moreover, to identify and eliminate unwanted content during the T2I model's fine-tuning and enhance image quality, we generate negative samples representing undesirable outcomes using the T2I model. As depicted in Fig. 2 (a), we adopt the image-to-image pipeline proposed in , using the T2I model and manually designed prompts such as "_cartoon, painting,..., over-smooth, dirty_", to directly generate negative samples.

Dual-Prompt based Fine-Tuning.Rather than relying on complex, labor-intensive prompts with limited applicability, we propose an innovative dual-prompt based fine-tuning approach to refine the T2I model for our data needs. As illustrated in Fig. 2 (b), we employ positive and negative samples to learn their corresponding prompts. Specifically, we use \(M\) positive tokens \(\{ p_{1}\,,\,, p_{M}\}\) and \(N\)

Figure 2: An overview of the three-stage **GenIR** pipeline, which includes (a) Image-Text Pairs Construction, (b) Dual-Prompt Based Fine-Tuning, and (c) Data Generation & Filtering.

negative tokens \(\{ n_{1},, n_{N}\}\) to represent desired and undesired attributes, respectively, and subsequently learn their embeddings. We initialize these new positive and negative tokens using frequently used positive (e.g., "_4k, highly detailed, professional..._") and negative text prompts (e.g., "_deformation, low quality, over-smooth..._"). As the text condition is integrated into the diffusion model via cross-attention, we also refine the attention block to better comprehend these new tokens. After fine-tuning the T2I model with our curated image-text pairs, we can efficiently employ the learned prompts and refined diffusion model to readily generate the needed data.

Data Generation & Filtering.In addition to the quality of images, the diversity of scenes within the IR dataset is of paramount importance. To address this, we leverage Gemini to generate one million text prompts, describing varied scenes under carefully curated language instructions. These instructions explicitly proscribe the inclusion of personal or sensitive information, thereby ensuring privacy. As depicted in Fig. 2 (c), we employ the fine-tuned T2I model in conjunction with the learned positive and negative prompts to generate HQ images.

Classifier-free guidance (CFG)  provides a mechanism to effectively utilize negative prompts, thereby mitigating the generation of undesired content. During the sampling phase, the denoising model \(_{}\) anticipates two outcomes: one associated with the positive prompt \(pos\) and the other with the negative prompt \(neg\). The final CFG prediction is formulated as

\[_{}(z_{t},t,pos,neg)=_{}(z_{t},t,pos) +(1-)_{}(z_{t},t,neg),\] (1)

where \(\) denotes the CFG guidance scale. Post-sampling, the generated images are evaluated by a quality classifier, which decides whether to retain the images based on the predicted probabilities. This binary classifier is trained on positive and negative samples. Gemini is subsequently used to ascertain whether the images exhibit blatant semantic errors or inappropriate content.

Contrasted with direct web crawling, our GenIR provides a more cost-effective and privacy-preserving approach to data acquisition. It circumvents the potential infringement of personal privacy information prevalent on the web, thereby ensuring our research is both ethical and secure - a crucial aspect in the current artificial intelligence landscape characterized by extensive data usage. Ultimately, we gather one million high-resolution (\(2040 1356\)) images, each of superior quality.

## 4 High-Capacity Image Restoration Model

The complex and varied degradation of real-world images presents a major challenge to the practical applicability of restoration models. We introduce DreamClear, a high-capacity image restoration model that dynamically integrates various restoration experts, guided by prior degradation information. DreamClear is built upon on PixArt-\(\), a pre-trained T2I diffusion model based on the Diffusion Transformer (DiT)  architecture, which has proven its powerful generative capabilities .

Architecture Overview.Fig. 3 illustrates that DreamClear features a dual-branch architecture, encompassing an LQ Branch and a Reference Branch. LQ images \(I_{lq}\) are processed by SwinIR , a lightweight degradation remover, resulting in smoother, albeit less detailed, reference images \(I_{ref}\). Considering potential detail loss in \(I_{ref}\), we employ both \(I_{lq}\) and \(I_{ref}\) to direct the diffusion model. Moreover, we utilize the open-source MLLM, LLaVA , to generate detailed captions for training images using the prompt "_Describe this image and its style in a very detailed manner_", supporting the T2I diffusion model in attaining more realistic restoration.

ControlFormer.ControlNet , a prevalent structure for managing diffusion models, is tailored for the U-Net  structure in SD. It is unsuitable for DiT, stemming from the architecture difference. To address this, we present ControlFormer, which inherits ControlNet's core features (trainable copy blocks and zero-initialized layers) but is adapted for the DiT-based T2I model, as shown in Fig. 3. ControlFormer, duplicating all DiT Blocks from PixArt-\(\), employs the MoAM block to combine LQ features \(x_{lq}\) and reference features \(x_{ref}\). This DiT-optimized ControlFormer maintains ControlNet's essential components, providing effective spatial control within DiT.

Mixture-of-Adaptive-Modulator.To enhance our model's robustness to real-world degradations, we propose a degradation-aware Mixture-of-Adaptive-Modulator (MoAM) for effective LQ and reference feature fusion. As shown in Fig. 3, MoAM consists of adaptive modulators (AM), a cross-attention layer, and a router block. AM employs AdaLN  to learn dimension-wise scale \(\) and shift \(\) parameters, embedding conditional information into input features.

MoAM operates in three steps: 1) For DiT features \(x_{in}\), we calculate the cross-attention output \(x_{attn}^{N C}\) between \(x_{lq}^{N C}\) and \(x_{ref}^{N C}\), where \(N\) and \(C\) denote the number of visual tokens and hidden size. \(x_{in}\) is then modulated using \(x_{attn}\) followed by a zero linear layer. A token-wise degradation map \(D^{N C}\) is generated through the linear mapping of \(x_{attn}\). 2) Features are further modulated using AM, with \(x_{ref}\) as the AM condition to extract clean features. 3) We adopt a mixture of degradation-aware experts to adapt to diverse degradations, detailed below.

Given varying degradations in real-world images, our method dynamically processes tokens using degradation priors. Each MoAM block consists of \(K\) restoration experts (_i.e._, AM) \(\{E_{1},,E_{K}\}\), each specialized for specific degradation scenarios. A routing network \(R()\) dynamically merges expert guidance for tokens, based on \(D\). The routing network, a two-layer MLP followed by softmax, yields token-wise expert weights \(w=R(D)^{N K}\). The dynamic mixture of restoration experts is formulated as

\[(i)=_{k=1}^{K}w(i,k) Net_{k}^{}[x_{lq}(i)],(i) =_{k=1}^{K}w(i,k) Net_{k}^{}[x_{lq}(i)],\] (2)

\[x_{out}=(1+) x_{in}+,\] (3)

where \(i\) and \(k\) index tokens and experts respectively, \(Net^{}\) and \(Net^{}\) map within an expert to \(\) and \(\), and \(\) denotes element-wise multiplication. MoAM dynamically fuses expert knowledge, leveraging degradation priors to tackle complex degradations.

## 5 Experiments

### Experimental Setup

Datasets.We adopt a combination of DIV2K , Flickr2K , LSDIR , DIV8K , and our generated dataset to train DreamClear. We employ the Real-ESRGAN  degradation pipeline to generate LQ images, using the same degradation settings as SeeSR  to ensure a fair comparison. All experiments are conducted on scaling factor \( 4\).

For testing datasets, following previous works [63; 80; 70], we evaluate our method on both synthetic and real-world benchmarks. For synthetic benchmarks, we randomly crop 3,000 patches from the

Figure 3: Architecture of the proposed **DreamClear**. DreamClear adopts a dual-branch structure, using Mixture of Adaptive Modulator to merge LQ features and Reference features. We utilize MLLM to generate detailed text prompt as the guidance for T2I model.

validation sets of DIV2K and LSDIR, and degrade them using the same settings as training. We name these two benchmarks as _DIV2K-Val_ and _LSDIR-Val_ respectively. For real-world benchmarks, we conduct experiments on commonly used _RealSR_ and _DRealSR_ datasets. Besides, we establish another real-world benchmark, called _RealLQ250_, which includes a total of 250 LQ images of size \(256 256\) used in previous works [70; 42; 64; 86; 80] or sourced from the Internet, without corresponding GT images. For all testing datasets with GT images, the resolution of the HQ-LQ image pairs is \(1024 1024\) and \(256 256\), respectively.

Metrics.Following SeeSR , we adopt PSNR and SSIM (calculated on the Y channel of transformed YCbCr space) as reference-based distortion metrics, LPIPS  and DISTS  as reference-based perceptual metrics, NIQE , MANIQA , MUSIQ  and CLIPIQA  as no-reference metrics. FID  is used to evaluate the image quality. These metrics can achieve a comprehensive evaluation of the restoration effects.

Implementation DetailsFor training GenIR and DreamClear, we both use the original latent diffusion loss . The proposed GenIR framework is built on SDXL  and trained over 5 days using 16 NVIDIA A100 GPUs. The training is conducted on \(1024 1024\) resolution images with

   Datasets & Metrics &  BSRGAN  \\ ENSRGAN \\  &  Real- \\ GAN \\  &  Real- \\ GAN \\  &  Real- \\ GAN \\  & 
 Real- \\ GAN \\  &a batch-size of 256. For data generation, we use 256 NVIDIA V100 GPUs and spend 5 days to generate the large-scale dataset. Our DreamClear is built upon PixArt-\(\) and LLaVA . The SwinIR model in DiffBIR  is used as the lightweight degradation remover. We use the AdamW optimizer with a learning rate of \(5e^{-5}\) to train our model. The training is conducted on \(1024 1024\) resolution images, running for 7 days on 32 NVIDIA A100 GPUs with a batch-size of 128. The number of experts \(K\) in Eq. (2) is set to \(3\). For inference of DreamClear, we adopt iDDPM  with 50 sampling steps, CFG guidance scale \(=4.5\).

### Comparison with State-of-the-Art Methods

We compare our method with state-of-the-art GAN-based methods (BSRGAN , Real-ESRGAN , SwinIR-GAN , and DASR ) and recent diffusion-based methods (StableSR , DiffBIR , ResShift , SinSR , SeeSR , and SUPIR ).

Quantitative Comparisons.Tab. 1 presents quantitative results on various benchmarks. Our method consistently excels in perceptual metrics (LPIPS, DISTS, FID) on synthetic datasets, signifying high perceptual quality. On real-world benchmarks, our method performs strongly across most no-reference metrics (NIQE, MANIQA, MUSIQ, CLIPIQA), attesting to the high quality of our restorations. Our diffusion-based method prioritizes photorealistic restoration. Despite lower PSNR/SSIM scores, recent works [79; 80] argue these metrics inadequately represent visual quality, and it is necessary to reconsider the reference values of existing metrics and propose more effective methods to evaluate modern image restoration methods. We believe that as the field of image quality assessment (IQA) evolves, more suitable metrics will emerge to adequately measure the performance of advanced IR models.

Figure 4: Qualitative comparisons on both synthetic (the first row) and real-world (the last two rows) benchmarks. Please zoom in for a better view.

Figure 5: User study. Vote percentage denotes average user preference per model. The Top-K ratio indicates the proportion of images preferred by top K users. Our model is highly preferred, with most images being rated as top quality by the majority.

Qualitative Comparisons.We provide qualitative comparisons in Fig. 4. When handling severe degradations (the first row), only our DreamClear can not only reason the correct structure but also generate clear details, while other methods may generate deformed structure and blurry results. When it comes to real-world images, our method can achieve results that are rich in detail and more natural (the third row). More real-world visual comparisons are in Appendix A.4.

User Study.We conducted a user study to evaluate our model's restoration quality using 100 low-quality images, restored by our method and five others. Users were asked to rank the restorations considering visual quality, naturalness, and accuracy, among others. The study, involving 256 evaluators, was designed for fairness and wide participation. Two metrics, vote percentage and Top-K ratio, were used to analyze the results. As shown in Fig. 5, our model led on both metrics, receiving over 45% of total votes and being the top choice for 80% of the images, demonstrating its consistent superiority in producing high-quality images. Refer to Appendix A.2 for more details.

### Evaluation on Downstream Benchmarks

We assess the benefits of image restoration for downstream high-level vision tasks by conducting detection and segmentation experiments on the COCO 2017  and ADE20K  datasets using various restoration models. Low-quality images are generated and restored under the same conditions as in training. We use the robust visual backbone RMT  (with Mask R-CNN  1\(\) for COCO, with UperNet  for ADE20K) for these tasks. Tab. 2 shows that our model obtains the best performance, implying its superiority in benefiting downstream tasks. Despite its superior performance in semantic restoration and fine-grained image recognition tasks, there's still substantial room for improvement.

### Ablation Study

Analysis of Generated Dataset for Real-World IR.Due to the extensive time required to train diffusion-based models, we use SwinIR-GAN to investigate the impact of generated datasets on

    & LPIPS \(\) & DISTS \(\) & FID \(\) & MANIQA \(\) & MUSIQ \(\) & CLIPIQA \(\) & \(AP^{b}\) & \(AP^{m}\) & mIoU \\  Mixture of AM & **0.3657** & **0.1637** & **20.61** & **0.4320** & **68.44** & **0.6963** & **19.3** & **16.7** & **31.9** \\ AM & 0.3981 & 0.1843 & 25.75 & 0.4067 & 66.18 & 0.6646 & 18.0 & 15.6 & 28.6 \\ Cross-Attention & 0.4177 & 0.2016 & 29.74 & 0.3785 & 63.21 & 0.6497 & 17.2 & 15.1 & 26.3 \\ Zero-Linear & 0.4082 & 0.1976 & 29.89 & 0.4122 & 66.11 & 0.6673 & 17.6 & 15.3 & 27.2 \\  Dual-Branch & **0.3657** & **0.1637** & **20.61** & **0.4320** & **68.44** & **0.6963** & **19.3** & **16.7** & **31.9** \\ w/o Reference Branch & 0.4207 & 0.2033 & 30.91 & 0.3985 & 64.04 & 0.6582 & 15.9 & 14.0 & 24.7 \\  Detailed Text Prompt & 0.3657 & 0.1637 & 20.61 & **0.4320** & **68.44** & **0.6963** & **19.3** & **16.7** & **31.9** \\ Null Prompt & **0.3521** & **0.1607** & **20.47** & 0.4230 & 67.26 & 0.6812 & 18.8 & 16.2 & 29.8 \\   

Table 3: Ablation results on _DIV2K-Val_, COCO val2017 and ADE20K for DreamClear.

Figure 6: Impact of synthetic training data. As data size increases, performance improves on _LSDIR-Val_.

real-world image restoration (IR). SwinIR is trained on varying quantities of generated data for comparison with the DF2K-trained model. Fig. 6 shows that the model trained on an equivalent number of generated images exhibits marginally lower perceptual but higher no-reference metrics than the DF2K model. As the dataset size increases, all metrics improve, reinforcing our belief that larger datasets enhance model generalizability and restoration performance. Notably, the model trained with 100,000 generated images outperforms the DF2K model, underscoring the advantages of utilizing large-scale synthetic datasets for real-world IR. More ablations of GenIR are provided in Appendix A.3.

Ablations of DreamClear.We conduct ablation studies to scrutinize the contribution of each component within DreamClear. Evaluating perceptual fidelity via LPIPS, DISTS, and FID metrics, and assessing the image quality of restoration results with MANIQ, MUSIQ, and CLIPIQA, we find that DreamClear outperforms its ablated versions across most metrics, substantiating the importance of these components (Tab. 3). Notably, null prompts slightly outperform detailed prompts on perceptual metrics. However, superior results on no-reference and high-level vision metrics suggest that the MLLM-provided detailed text prompts better preserve semantic information. Visual comparisons detailed in Appendix A.3 further reinforce the benefits of semantic guidance in image restoration provided by text prompts.

## 6 Limitations and Broader Impact

Our model leverages the generative prior of pre-trained diffusion models for image restoration, with diverse synthesized datasets used during training to enhance model performance. In situations of severe image degradation, while our method could predict reasonable and realistic results, the synthesized texture details may not exist in the ground-truth image. A high-quality reference image or explicit human instruction may compensate such a limitation in some degree.

Another limitation lies in the deployment in practical applications. Our model is a diffusion-based model and it needs multiple inference steps to restore the input low-quality image. While our model can predict more plausible results than existing methods, it can not meet the requirement of real-time inference speed in many practical applications. Model distillation and model quantization may compensate the limitation of inference speed.

This paper is a purely academic study of real-world image restoration (IR). However, considering image restoration's vital role in many practical applications, this work may not only bring some positive societal influence, _e.g._, improving the quality of images captured by smartphones, but also lead to some potential risks like privacy information leakage from photos on social media. However, the positive societal effects of image restoration far exceed the potential negative impacts, and people may make use of some other techniques, such as inpainting and watermarking, to remove the private information in images.

## 7 Conclusion

To address the challenges in real-world image restoration (IR), we develop **GenIR**, a privacy-safe automated pipeline that generates a large-scale dataset of one million high-quality images, serving as a robust training resource for IR models. Additionally, we introduce **DreamClear**, a potent IR model that seamlessly integrates degradation priors into diffusion-based IR models. It introduces the novel Mixture of Adaptive Modulator (MoAM) to adapt to diverse real-world degradations. Our comprehensive experiments underline its outstanding performance in managing complex real-world situations, marking a substantial progression in IR.