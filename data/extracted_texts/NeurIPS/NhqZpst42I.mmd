# Understanding Visual Feature Reliance through the Lens of Complexity

Thomas Fel

Google DeepMind

Brown University

&Louis Bethune

&Andrew Kyle Lampinen

Universite de Toulouse

&Google DeepMind

&Thomas Serre

Brown University

&Katherine Hermann

Google DeepMind

###### Abstract

Recent studies suggest that deep learning models' inductive bias towards favoring simpler features may be one of the sources of shortcut learning. Yet, there has been limited focus on understanding the complexity of the myriad features that models learn. In this work, we introduce a new metric for quantifying feature complexity, based on \(\)-information and capturing whether a feature requires complex computational transformations to be extracted. Using this \(\)-information metric, we analyze the complexities of 10,000 features--represented as directions in the penultimate layer--that were extracted from a standard ImageNet-trained vision model. Our study addresses four key questions: First, we ask _what_ features look like as a function of complexity and find a spectrum of simple-to-complex features present within the model. Second, we ask _when_ features are learned during training. We find that simpler features dominate early in training, and more complex features emerge gradually. Third, we investigate _where_ within the network simple and complex features "flow", and find that simpler features tend to bypass the visual hierarchy via residual connections. Fourth, we explore the connection between features' complexity and their importance in driving the network's decision. We find that complex features tend to be less important. Surprisingly, important features become accessible at earlier layers during training, like a "sedimentation process," allowing the model to build upon these foundational elements.

_"It is necessary to have on hand a method of measuring the complexity of calculating devices which in turn can be done if one has a theory of the complexity of functions, some partial results on this problem have been obtained by Shannon."_

Darmouth Workshop proposal 

Measuring complexity is one of the core problems described by Shannon & McCarty in the famous 1956 proposal of the Dartmouth workshop. This problem--and the question, _"How can a set of (hypothetical) neurons be arranged to form concepts?"_--encapsulate what we investigate: how do neural networks form features and concepts , and how can their complexity be quantified?

Recent studies  reveal that models often favor simpler features, which may contribute to shortcut learning . For example, CNNs privilege texture over object shape  and single diagnostic pixels over semantic content . Moreover, models tend to prefer input features that are linearly rather than nonlinearly related to task labels . However, therehas not been a comprehensive quantitative framework for assessing the complexities of features learned by large-scale, natural image-trained vision models used in practice. Leveraging recent observations and advances in feature (also called "concept") extraction from the area of Explainable AI [83; 8; 25; 29; 32; 77; 19; 32; 42], we extract a large set of features from an ImageNet-trained model , and analyze their complexity.

Our contributions are as follows:

* We build upon \(\)-information --which measures the mutual information between two variables, considering computational constraints--to introduce a measure of feature complexity. We use this measure to quantify the complexity of over 10,000 features in an ImageNet model  at each epoch of training.
* We visualize the differences between simple and complex features on a spectrum to understand which features are readily available to our model and which ones require more computation and transformation to retrieve.
* We investigate _where_ sensitivity to simple versus complex features emerges during a forward pass through the model. Our findings suggest that residual connections "teleport" simple features, computed in early layers, to the final layer. The main branch naturally facilitates the layer-wise construction of more complex features.
* We examine feature learning dynamics, revealing _when_ different concepts emerge over the course of training, and find that complex concepts tend to appear later than simpler ones.
* We explore the link between complexity and importance in driving the model's decisions. We find a preference for simpler features over more complex ones. This simplicity bias emerges during training, and, surprisingly, the model simplifies its most important features over time.

Figure 1: **A) Simple vs. Complex Features.** Shown is an example of three features extracted using an overcomplete dictionary on the penultimate layer of a ResNet50 trained on ImageNet. Although all three features can be extracted from the final layer of a ResNet50, some features, such as \(z_{1}\), seem to respond to color, which can be linearly extractable directly from the input. In contrast, \(z_{2},z_{3}\) visualization appear more “Complex”, responding to more diverse stimuli. In this work, we seek to study the complexity of features. We start by introducing a computationally inspired complexity metric. Using this metric, we inspect both simple and complex features of a ResNet50. **B) Feature Evolution Across Layers.** Each row illustrates how a feature from the penultimate layer (\(z_{1},z_{2},z_{3}\)) evolves as we decode it using linear probing at the outputs of blocks 1, 5, and 10 of the ResNet50. Simpler features, like color, are decodable throughout the network. The feature in the middle shows similar visualization at block 10 and the penultimate layer, whereas the most complex feature is only decodable at the end. Our complexity metric, based on \(\)-information , measures how easily a model extracts a feature across its layers.

That is, during training, important features become accessible at an earlier layer (via a shorter computational graph).

## 1 Related Work

**Feature analysis.** Large vision models learn a diversity of features  to support performance on the training task and can exhibit preferences for certain features over others, for example textures over shapes . These preferences can be related to their use of shortcuts  which compromise generalization capabilities . Hermann et al.  suggest that a full account of a model's feature preferences should consider both the predictivity and _availability_ of features, and identify image properties that induce a shortcut bias. Relatedly, work shows that models often prefer features that are computationally simpler to extract--a "simplicity bias" .

**Explainability.** Attribution methods  seek to attribute model predictions to specific input parts and to visualize the most important area of an image for a given prediction. In response to the many limitations of these methods , Feature Visualization  methods have sought to allow for the generation of images that maximize certain structures in the model - e.g., a single neuron, entire channel, or direction, providing a clearer view features learned early , as well as circuits present in the models . Recently, work has scaled these methods to deeper models . Another approach, complementary to feature visualization, is automated concept extraction , which identifies a wide range of concepts - directions in activations space - learned by models, inspired by recent works that suggest that the number of learned features often exceeds the neuron count . This move towards over-complete dictionary learning for more comprehensive feature analysis represents a critical advancement.

**Complexity.** On a theoretical level, the complexity of functions in deep learning has long been a subject of interest, with traditional frameworks like VC-dimension falling short of adequacy with current results. In particular, deep learning models often have the capacity to memorize the entire dataset, yet still generalize ; the reason is often suggested to be a positive benefit of simplicity bias . Measures of the complexity of neural network functions are hard to make tractable . Recent work has proposed various methods to evaluate this complexity. For instance,  proposed a score of non-linearity propagation, while  introduced a measure of local complexity based on spline partitioning. Additionally,  demonstrated that models tend to learn functions with low sensitivity to random changes in the input. The role of optimizers in complexity has also been explored. It has been shown that different optimizers impact the features learned by models; for example,  found that sharpness-aware minimization (SAM)  learns more diverse features, both simple and hard, whereas stochastic gradient descent (SGD) models tend to rely on simpler features. Furthermore,  utilized category theory to propose a metric based on redundancy, which consist in merging neurons until a distance gap is too large, with this distance gap acting as a hyperparameter. Concurrent work by Lampinen et al.  studies representations induced by input features of different complexities when datasets are carefully controlled and manipulated. Finally, Okawa et al. , Park et al.  investigated the development of concepts during the training process on toy datasets and revealed that the sequence in which they appear, related to their complexity, can be attributed to the multiplicative emergence of compositional skills.

Concerning algorithmic complexity, Kolmogorov complexity , later expanded by Levin  to include a computational time component, offers a measure for evaluating the shortest programs capable of generating specific outputs on a Turing machine . This notion of complexity is at the roots of Solomonoff induction , which is often understood as the formal expression of Occam's razor and has received some attention in deep learning community . Further developing these concepts, \(\)-information  introduces computational constraints on mutual information measures, extending Shannon's legacy. This methodology enables the assessment of a feature's availability or the simplicity with which it can be decoded from a data source. We will formally introduce this concept in Section 2.

## 2 Method

Before we measure feature complexity, we define what is meant by features, explain how they are extracted, and then introduce the complexity metric.

**Model Setup.** We study feature complexity within an ImageNet-trained ResNet50 . We train the model for 90 epochs with an initial learning rate of 0.7, adjusted down by a factor of 10 at epochs 30, 60, and 80, achieving a 78.9% accuracy on the ImageNet validation set, which is on par with reported accuracy in similar studies [45; 114]. Focusing on one model reduces architectural variables, creating a controlled environment to analyze feature complexities and provide insights for broader model hypotheses.

**Feature Extraction.** We operate within a classical supervised machine learning setting on \((,,)\) - the underlying probability space - where \(\) is the sample space, \(\) is a \(\)-algebra on \(\), and \(\) is a probability measure on \(\). The input space is denoted \(^{d}\). Let the input data \(:\) be random variables with distributions \(P_{}\). We will explore how, from \(\) and using a neural network, we extract a series of \(k\) features. We will assume a classical vision neural network that admits a series of \(n\) intermediate spaces, such that:

\[_{t}:_{t}\;\;\;\;\{1,,n\}.\]

Initially, one might suggest that a feature is a dimension of the model, meaning, for example, that a feature could be a neuron in the last layer of the model \(=_{n}()_{i},i\{1,,|_{n}|\}\), thus each of the neurons would be a feature. However, several recent studies [83; 8; 25; 29; 32] have shown that our models actually learn a multitude of features, far more than the number of neurons, which explains, for example, why they are not mono-semantic [77; 19], which could also hinder our study of features. Therefore, we use a recent explainability method, Craft , to extract more features

Figure 2: **Qualitative Analysis of “Meta-feature” (cluster of features) Complexity. (Left)** A 2D UMAP projection displaying the 10,000 extracted features. The features are organized into 150 clusters using K-means clustering applied to the feature dictionary \(^{*}\). 30 clusters were selected for analysis of features at different complexity levels. (Right) For each Meta-feature cluster, we compute the average complexity score. This allows us to classify the features based on their complexity according to the model. Notably, simple features are often akin to color detectors (e.g., _grass_, _sky_) and detectors for _low-frequency patterns_ (e.g., _bokeh_ detector) or _lines_. In contrast, complex features encompass parts or structured objects, as well as features resembling shapes (such as _ears_ or _curve detectors_). Visualizations of individual Meta-features are presented in Appendix B.

than neurons and avoid this problem of superposition - or feature collapse. With \(_{n}()\) being the penultimate layer, we extract a large number of features, five times more than the number of neurons, using an over-complete dictionary of concepts \(^{}^{k|_{n}|}\), with \(k|_{n}|\). This dictionary is obtained by optimization over the entire training set and contains a total of \(k=10,000\) features. Thus, for a new point \(\), we obtain the value of the \(k\) features - we recall that the number of features is greater than the number of neurons, \(k|_{n}|\) - by solving the following optimization problem:

\[=*{arg\,min}_{ 0}||_{n}()- ^{}||_{F}\]

With \(^{k}\) being the value for each feature of the image \(\), in particular, and from now on for ease of notation, we consider \(_{i},i\{1,,k\}\) that we will simply denote \(z\) for the rest of the paper, as a _specific feature_ for which we want to compute a complexity score. Thus, in our work, a feature refers to a random scalar value extracted by a dictionary learning method on the activations. More details and full derivation regarding the training of \(^{}\) are available in the Appendix A.

**Complexity through the Lens of Computation.** To formalize this, still on \((,,)\), we denote the output space \(\), and \(:\) are random variables of a feature of interest with distributions \(P_{}\). The joint random vector \((,)\) representing an image \(\) and the value of its feature \(\) on \((,)\) has a joint distribution \(P\) defined over the product space \(\). Furthermore, \(()\) denotes the set of all probability measures on \(\). We can now associate, for an \(\) which we recall is a real-valued random variable, a corresponding feature \(\), another real-valued random variable, and we seek to correctly evaluate the complexity of the mapping from \(\) to \(\). For this, we turn to the \(\)-Information  that generalizes and extends the classical mutual information \((,)\) from Shannon's theory by overcoming its inability to take into account the computational capabilities of the decoder. Indeed, for two (not necessarily independent) random variables \(\) and \(\), and for any bijective mapping \(:\), Shannon's mutual information remains unchanged: \((,)=((),)\).

Consider, for instance, \(\) as a cryptographic function that encrypts an image \(\) using a bijective key-based algorithm (e.g., the AES encryption algorithm). If \(\) represents the original image, and \(()\) represents the cipherimage, the mutual information between \(\) and \(\) remains unchanged. This is because the encryption is a bijective process, and the information content is preserved. However, in practice, the encrypted images would be much harder to decode and use for training a model compared to the original one, without access to the decryption key. Another example we may think of is \(\) as a pixel shuffling operation. The information carried by \(\) does not disappear after processing by \(\). However, it may be harder to extract in practice.

This demonstrates the practical importance of \(\)-Information, as it considers the computational effort required to decode the information, highlighting the difference between _theoretical_ and _practical_ accessibility of information. Specifically, the \(\)-information proposes taking into account the computational constraint of the decoder by assuming it can only extract information using a _predictive family_\(=\{:\{\} ()\}\). The authors  then define the \(\)-entropy and the \(\)-conditional entropy as follows:

\[H_{}()=_{}_{P_{}}(- (;)),\ \ \ \ \ H_{}(|)=_{}_{P}(- (;)).\] (1)

Where \((;)\) is a function from \(\{\}()\) that returns a probability density \((;)\) on \(\) using side information \(\), or without side information \(\). The predictive family \(\) summarizes the computational capabilities of the decoder. When \(\) contains all possible functions, \(=\), it recovers Shannon's entropy as a special case. Intuitively, we seek the best possible prediction for \(\) knowing \(\) by maximizing the log-likelihood. Continuing, we naturally introduce the \(\)-information:

\[_{}()=H_{}()-H_{ }(|).\]

The complexity of the mapping from \(\) to \(\) can now be assessed by examining a hierarchy of predictive families \(_{1}_{n}\) of increasing expressiveness, like explored in . Each predictive family \(_{}\) corresponds to a partial forward up to depth \(\), followed by a decoding step. This involves determining at which point we can decode or make the information from \(\) to \(\)_available_. Formally, we define the complexity of the feature as dependent of the cumulative \(\)-information across layers:\[K(,)=1-_{}^{n}_{}( _{}()).\] (2)

Here, we define the predictive family \(\) as a class of linear probes with Gaussian prior. Under this hypothesis, the associated \(\)-information of this class possesses a closed-form solution (see Appendix C), which serves as the basis for our evaluation. A higher score implies that the feature \(\) is readily accessible and persists throughout the model's layers. Conversely, a lower score suggests that the feature \(\) is unveiled only at the very end of the model, if at all.

**Assumption.** Crucially, the correctness of the computation of \(_{}(_{}())\) relies on the hypothesis that each layer \(f_{}\) provides the optimal representation for the downstream linear probe \(\). In other words, we assume that \(_{}(_{}())= _{_{}}()\), or again that \(_{}^{*}=^{*} f_{}\). This hypothesis is reasonable, since a neural network is essentially "linearizing" the training set--projecting the training set into a space in which it is linearly separable. Thus, it makes sense to assume that each layer attempts to make the feature linearly decodable as efficiently as possible. If this condition is violated, the complexity measure may overestimate the true complexity of a feature (since we can only underestimate the \(\)-information). For example, this may happen if the optimal path to calculate a feature requires deviating from the linear decoding to make it easier to decode later. While some recent works have motivated a slightly different complexity metric based on redundancy , we show in Appendix E that our complexity measure is inherently linked to redundancy.

## 3 _What_ Do Complex Features Look Like? A Qualitative Analysis

This section presents a qualitative investigation of relatively simple versus more complex features. Drawing from critical insights of recent studies, which indicate a tendency of neural networks to prefer input features that are both predictive and not overly complex , this analysis aims to better understand the nature of features that are easily processed by models versus those that pose more significant challenges. Indeed, understanding the types of features that are too complex for our model can help us anticipate the types of shortcuts the model might rely on and, on the other hand, design methods to simplify the learning of complex features. This section of the manuscript is intentionally qualitative and aims to be exploratory. We applied our complexity metric to 10,000 features extracted from a fully trained ResNet50. For each feature, we computed the complexity score \(K(,)\) using a subset of 20,000 images from the validation set. Recognizing the impracticality of manually examining each of the 10,000 features, we employed a strategy to aggregate these features into a more manageable number of groups that we called Meta-features.

Figure 3: **Visualization of Meta-features, sorted by Complexity.** We use Feature visualization [31; 84] to visualize the Meta-features found after concept extraction. The entire visualization for each Meta-feature can be found in Appendix B.

**Method for Aggregating Features into Meta-features.** To condense the vast array of features into a reduced number of similar features, we applied K-means clustering to the feature dictionary \(^{}\), resulting in 150 distinct clusters. These clusters represent collections of features, referred to as Meta-features \(=\{_{1},,_{||}\}\); we then computed an average complexity score for each group. By selecting a diverse range of 30 clusters, chosen to cover a spectrum of complexity levels from the simplest to the most complex features, we aimed to provide a comprehensive overview of the diversity of feature complexity within the model. We propose to visualize the distance matrix in \(^{}\), showing feature complexity in Figure 2. This approach offers preliminary insights into features seen as simple or complex by the model.

**Simple Features.** Among the simpler features, we find elements primarily based on color, such as _sky_ and _sea_, as well as simple pattern detectors like _line_ detectors and low-frequency detectors exemplified by _bokeh_. Interestingly, features geared towards text detection, such as _watermark_, are also included in this group. These findings align with previous studies [117; 96; 12; 82], which have shown that neural networks tend to identify color and simple geometric patterns in the early layers as well as low-frequency detectors. This suggests that these features are relatively easy for neural networks to process and recognize. Furthermore, our findings detailed in Appendix 11 corroborate the theoretical work posited in [11; 72]: robust learning possibly induces the learning of shortcuts or reliance on "easy" features within the model.

**Medium Complexity Features.** Features with medium complexity reveal more nuanced and sometimes unexpected characteristics. We find, for example, _low-quality_ detectors sensitive to low-resolution images. Additionally, a significant number of concepts related to _human elements_ were observed despite the absence of a dedicated _human_ class in ImageNet. _Trademark-related_ features, distinct from simpler _watermark_ detectors, also reside within this intermediate complexity bracket.

**Complex Features.** Among the most complex features, we find several Meta-features that exhibit a notable degree of structural coherence, including categories such as _insect legs_, _curves_, and _ears_. These patterns represent structured configurations that are ostensibly more challenging for models to process than more localized features, echoing the ongoing discussion about texture bias in current models [10; 37; 46]. Intriguingly, the most complex Meta-features identified, namely _whiskers_ and _insect legs_, embody types of filament-like structures. Interestingly, we note that those types of features are known to be challenging for current models to identify accurately , aligning with documented difficulties in path-tracking tasks . Such tasks have revealed current models' limitations in tracing paths, which parallels challenges in connectomics , particularly in filament segmentation--a domain recognized for its complexity within deep learning research.

Now that we've browsed simple and complex features, another question arises: how does the model build these features during the forward pass? For instance, _where_ within the model does the formation of a watermark detector feature occur? And for more complex features that require greater structure, in which block of computation are these features formed within the model?

## 4 _Where_ do Complex Features Emerge

As suggested by previous work, simple features, like color detectors and low-frequency detectors, may already exist within the early layers of the model. An intriguing question arises: how does the model ensure the propagation of these features to the final latent space \(_{n}\), where features are extracted? A key component to consider in addressing this question is the role of residual connections within the ResNet  architecture. The formulation of a residual connection in ResNet blocks is mathematically represented as:

\[_{+1}()=_{}()}_{}+_{}_{})()}_{}\]

This equation highlights two distinct paths: the "Residual" branch, which facilitates the direct

Figure 4: **Simple Features Teleported by Residuals. (Left) CKA between residual branch activations \(_{}\) and final concept value \(z\). For simple concepts, beyond a certain layer (block 3), the residual already carries nearly all the information, effectively teleporting it to the last layer. (Right) Conversely, for complex features, both the main and residual branches gradually construct the features during the forward pass.**transfer of features from \(_{}\) to the subsequent layer \(+1\), and the "Main" branch, which introduces additional transformations to \(_{}\) through additional computation \(_{}\) to enhance its representational capacity. We aim to investigate the _flow_ of simple and complex features through these branches. In our analysis, we examine two subsets of features: 100 features of the highest complexity (top-1 percentile) and 100 features of the lowest complexity (bottom-1 percentile). We measure the Centered Kernel Alignment (CKA)  between the final concept values \(z\) and the activations from (A) the "Residual" branch \(_{}\), and (B) the "Main" branch \((_{}_{})\), at each residual block, as a proxy for concept information contained in each branch. The findings, illustrated in Figure 4, reveal that simple features are efficiently "teleported" to later layers through the residual branches - in other words, once computed, they are passed forward with little subsequent modification. In contrast, complex concepts are incrementally built up through an interactive process involving the "main" and "residual" branches. This understanding of feature evolution within network architectures emphasizes the importance of residual connections. This insight, though expected, clarifies a common conception by showing that simple features utilize the residual branch. The next step is to examine the temporal dynamics of feature development, specifically investigating when complex and simple concepts emerge during model training.

## 5 _When_ do Complex Features Arise

Figure 1 raises an important question: Does the complexity of a feature influence the time it takes to develop during training? To explore this, we refer to the 10,000 features extracted at the final epoch of our model as \(_{n}^{(e)}\), and we use \(_{n}^{(i)}\) to represent the penultimate layer of the model at any given epoch \(i\), where \(i\{1,,e\}\) and \(e\) represents the total number of epochs. We aim to determine how early each feature can be detected in previous epochs \(_{n}^{(i)}\) for \(i<e\). This involves calculating a specific decoding score; in our scenario, we define this score as \(_{}\)--the measure of \(\)-information between the model's penultimate activations across epochs and an ultimate feature values, where \(\) is the set of linear models. This metric helps us assess whether a feature was "readily available" at a certain epoch \(i\). The cumulative score \(\) is calculated by averaging this measure across all epochs, leading to our score:

\[(,)=1-_{i}^{e}_{}(_{n}^{(i)}()).\]

Figure 5: **A) Complex features emerge later in training.** There is a strong correlation between the complexity of a feature and the requisite temporal span for its decoding. The temporal decoding score, \(\), is derived as the mean \(\)-information across epochs, with \(\) representing the class encompassing linear models. A low score indicates a feature is accessible earlier during the training continuum, whereas a high score implies its tardy availability. The correlation between these scores suggests that complex features tend to emerge later in training. **B) Important features are being compressed by the neural network:**_Levin Machine_ **hypothesis.** The average complexity of 10,000 features extracted independently at each epoch increases rapidly before stabilizing (the black curve shows the average). However, among the top-1% of features in terms of importance, complexity decreases over time, as if the model is self-compressing or simplifying, akin to a sedimentation process.

The results, as illustrated in Figure 5A, showcase the complexity of a feature (\(K\)) with it's Time to Decode (\(\)) score. An observed correlation coefficient nearing \(0.5\) intimates that features of heightened complexity are generally decoded later during the training epoch. This finding suggests a nuanced interrelation between the layer for which a feature is available and the epoch of discovery: a feature decoded later in the forward pass trajectory also came online later in training. This naturally leads us to the question of the dynamics of model training. Can we get a deeper understanding of how precisely complex concepts are formed within the model? Does the model develop complex features solely upon necessity, thereby suggesting a correlation between the complexity of a feature and its importance?

## 6 Complexity and Importance: A Subtle Tango

Numerous studies have proposed hypotheses regarding the relationship between the importance and complexity of features within neural networks. A particularly notable hypothesis is the simplicity bias [3; 51; 110], which suggests that models leverage simpler features more frequently. This section aims to quantitatively validate these claims using our complexity metric paired with the importance of each feature. Because features are extracted from the penultimate layer, a closed-form relationship between features and logits can be derived due to the linear nature of this relationship. By analyzing this relationship over training for features of different complexity, we identify a surprising novel perspective: models appear to _reduce_ the complexity of their important features. This process is analogous to sedimentation and mirrors the operation of a _Levin_ Universal Search . The model incrementally shifts significant features to earlier layers, taking time to identify simpler algorithms in the process.

**Importance Measure.** The feature extraction framework outlined in Section 2 offers a structured approach to estimating the importance of a feature within the network. Specifically, the feature vector \(^{k}\) is linearly related to the model's decision-making process, exemplified by a logit calculation \(=^{}\), where \(^{|_{n}|}\) represents the weights of the penultimate layer for the class-specific logit. The contribution of the \(i\)-th feature, \(_{i}\), to the logit \(\) can be precisely measured by leveraging the gradient-input formulation, which is optimal for fidelity metrics within a linear context [5; 32]. This optimality and the closed-form expression are feasible primarily because the analysis is confined to the penultimate layer of the network. Formally, the importance of a feature \(_{i}\) is defined as: \((_{i})=_{_{x}}\|_{i}}{_{i}}_{i}\|\). In essence, the importance measure \((_{i})\) quantifies the average contribution of the \(i\)-th feature to the class-specific logit - essentially, the average score that each feature brings to the decision logit. More details on importance measures and the effect of inhibition features are available in Appendix G.

**Models Prefer Simple Features.** The analysis, supported by Figure 6 (right), demonstrates a clear trend indicating the model's simplicity bias. Among the 10,000 features extracted in the final epoch,

Figure 6: **Simplicity bias appears during training.** Complexity vs. Importance of 10,000 features extracted from a ResNet50 at Epochs 1 and 90 of training. In Epoch 1, important features are not necessarily simple and seem uniformly distributed. In contrast, by the end of training, there is a clear simplicity bias, consistent with numerous studies: the model prefers to rely on simpler features.

more complex features--characterized by higher \(K(,)\) values--are generally assigned lower importance (\(()\)). In contrast, simpler features predominantly influence the model's decisions. The plot on the left showcases the complexity and importance of 10,000 concepts extracted at the end of the first epoch; we observe that the model does not exhibit this simplicity bias at the end of the first epoch. More detail and study on the role of complex concept is proposed in Appendix D. This observation raises the question of the dynamic interplay between feature complexity and importance. To further investigate, we did a detailed analysis of the evolution of feature complexity and importance throughout the training process.

**Model as _Levin's Machine_: Simplifying the Complexity of Important Features.** A closer examination of the evolution of feature importance over time reveals an interesting phenomenon in Figure 5B: the emergence of two distinct phases during training. Initially, there is a global increase in feature complexity, with the model beginning its training with relatively simple features. Surprisingly, this is followed by a phase where the model actively reduces its overall complexity, specifically targeting and simplifying its most important features. The model appears to be "shortening" the computational "programs" responsible for generating these significant features. This observation suggests that the ResNet50 under study, like a Levin Machine, develops simpler computational paths for crucial features. Put simply, our complexity metric shows that important features are extracted at earlier layers, resembling sedimentation with foundational elements near the network's input.

This behavior presents a novel perspective on how neural networks might be intrinsically driven to generalize by simplifying the computation graph of their important features. However, at least at the early stages of learning, it also challenges our assumption that each layer is optimized to provide a linearly-separable representation for the downstream linear probe - early in learning, this assumption is clearly violated since some complex features could be represented more simply than they are initially. Thus, future work will be needed to fully disentangle the interaction of complexity and importance over training.

## 7 Conclusion

We introduced a complexity metric for neural network features, identifying both simple and complex types. We have shown where simple features flow - through residual connections - as opposed to complex ones that develop via collaboration with main branches. Our study further revealed that complex features are learned later in training than simple ones. We have concluded by exploring the relationship between feature complexity and importance, and discovered that the simplicity bias found in neural networks becomes more pronounced as training progresses. Surprisingly, we found that important features simplify over time, suggesting a _sedimentation process_ within neural networks that compresses important features to be accessible earlier in the network.