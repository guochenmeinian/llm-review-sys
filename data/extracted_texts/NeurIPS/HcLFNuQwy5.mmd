# SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation

Jonathan Roberts

University of Cambridge

jdr53@cam.ac.uk

&Kai Han

The University of Hong Kong

kaihanx@hku.hk

&Neil Houlsby

Google DeepMind

neilhoulsby@google.com

&Samuel Albanie

samuel.albanie.academic@gmail.com

###### Abstract

Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present **SciFIBench**, a scientific figure interpretation benchmark consisting of 2000 questions split between two tasks across 8 categories. The questions are curated from arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for quality control. We evaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain: [https://SciFIBench.github.io/](https://SciFIBench.github.io/).

## 1 Introduction

Lately, the rate of progress in the development of artificial intelligence (AI) has significantly increased. The emergence of foundation models , trained on large-scale broad data using extensive computational resources enabling generalisation across many downstream applications, has greatly expanded the range of possible domains and tasks in which machine intelligence can operate. Notable large language models (LLMs), such as GPT-4 , LLaMA , and PaLM , and subsequent large multimodal models (LMMs), for example, GPT-4V , Qwen , and Gemini , have proven to be flexible and generalisable across many tasks. In particular, their capabilities have been demonstrated in fields such as mathematics , medicine , and finance , as well as writing code  and the geographic and geospatial domains .

One area that is beginning to receive more attention is the _scientific domain_, which has the potential to greatly benefit from AI tooling. Although the current generation of frontier models is arguably unable to perform independent, end-to-end scientific research, there is an emerging body of evidence  suggesting they can be used as a tool to assist different stages of the scientific process. A key aspect of scientific research is the ability to _understand figures_, which serve as a rich, compressed source of complex information. As noted in , unique challenges arise from the complex and dense semantics of scientific images and the sophisticated language preferences of researchers. While the abilities of LMMs across some domains are relatively well-understood thanks to established benchmarks , their capacity to understand scientific figures is not well known. However, reliably characterising the ability of a model to interpret scientific figures is challenging without an obvious objective evaluation metric. Another consideration is the source of accurate ground truth; manually annotating a sufficiently large evaluation set of figures with accurate descriptions is unfeasible, and challenging without appropriate domain knowledge.

We circumvent these issues by reframing the evaluation to a multiple-choice setting, using the figure captions as ground truth descriptions - see Fig. 1. Concretely, using figure-caption pairs from arXiv papers, we construct a pool of multiple-choice questions for the two tasks shown in Fig. 1. Following other popular works , we adopt adversarial filtering when curating the negatives for each question to increase the difficulty. To further improve the quality, we utilise human verification on _every_ question to ensure they are maximally answerable. We create **SciFIBench** (Scientific Figure Interpretation Benchmark) by sampling from this question pool with the following three objectives in mind: (1) **Quality** - we perform human verification on every question to ensure high-quality questions that are answerable. (2) **Efficiency** - we choose a small-scale set of questions, enabling streamlined evaluation and ensuring the benchmark can maximally be used by the community. (3) **Robustness** - we conduct careful analysis to verify SciFIBench offers a suitably robust evaluation. Our benchmark consists of 2000 high-quality, challenging questions.

We evaluate a suite of 28 open- and closed-source LMM baselines on SciFIBench and compare the performance to human and vision-language model (VLM) baselines. To overcome the challenges associated with post-processing the output of LMMs to extract a specific answer at scale, we leverage Gemini-Pro  to parse the output of all evaluated LMMs and extract the relevant multiple-choice letter answers, enabling automatic evaluation. Finally, we carry out preliminary experiments probing the alignment and faithfulness of the LMMs when answering questions in our benchmark. We hope our insights will encourage further research in this direction.

To conclude, our main contributions are as follows: (i) We curate **SciFIBench** to evaluate scientific figure interpretation. (ii) We benchmark 28 LMMs on SciFIBench and compare the performance to human and VLM baselines. (iii) We introduce an experimental setting probing the instruction-following abilities and faithfulness of reasoning of the LMMs. (iv) We release SciFIBench to drive progress in LMM scientific figure interpretation and understanding research. We derive these key insights from our work:

* SciFIBench proves to be a challenging benchmark for current LMMs.
* GPT-40  and Gemini 1.5  are the best-performing models, outperforming all the VLM baselines but are beaten by the human baseline.
* Adversarial filtering significantly increases multiple-choice question difficulty but human filtering is crucial to ensure high-quality, answerable questions.
* Leveraging a strong LLM to evaluate the noisy output of the evaluated LMMs proves accurate and viable for automatic evaluation.
* The evaluated LMMs show varying levels of faithfulness in their answers.

## 2 Related Work

**Scientific Figure Interpretation.** Several approaches have been proposed to investigate the capacity of multimodal models to interpret scientific figures. These include **question answering** benchmarks such as ChartQA , PlotQA , and FigureQA , which ask complex reasoning questions about scientific figures. ACL-Fig  introduces the task of **type classification** for scientific figures. A

Figure 1: **Overview of SciFIBench. _Left_: our benchmark consists of 2000 multiple-choice scientific figure interpretation questions curated from arXiv papers using adversarial filtering and human verification to maximise difficulty and quality, respectively. _Right_: we evaluate a suite of LMMs on the two core SciFIBench tasks, leveraging an LLM for automatic evaluation.**

large body of literature exists that evaluates the quality of **generated captions** for scientific figures. The progenitor for many subsequent works is SciCap , in which an image-captioning model is trained to generate high-quality captions. SciCap+  builds this idea further and includes figure mention-paragraphs in addition to input figures. SciCap-Eval  investigates the usage of LLMs for ranking scientific figure captions. VisText  fine-tunes language models to generate captions for scientific charts, and FigCaps-HF  introduces a framework that initially learns a human feedback prediction model and incorporates this to optimise caption generation based on reader preference. The SciMMIR benchmark  characterises the abilities of vision-language models to understand scientific figures through **retrieval** experiments. More recently, a few works  have conducted a qualitative analysis of LMM (specifically, GPT-4V ) performance on a small handful of scientific figures. We draw inspiration from these works, incorporating some of the methodological ideas. However, our work focuses on a quantitative evaluation of LMMs for the task of understanding scientific figures, which has yet to be reported. We also re-frame the task to a multiple-choice setting as this is more suitable for robust evaluation of LMMs.

**LMM Benchmarks.** A number of benchmarks aimed at multimodal model evaluation have been developed in recent years. Prominent natural image benchmarks include LVLM-eHub , MMBench , MME , MM-Vet , and SEEDBench  and SEEDBench-2 , which both consist of multiple-choice questions across different domains and evaluation dimensions. A small-scale geographic and geospatial benchmark is introduced in . LAMM  evaluates a variety of computer vision tasks on 2D natural images as well as 3D point clouds. Other benchmarks, such as HallusionBench , focus on the failure modes and hallucinations of the models. MathVista  introduces a mathematical reasoning in visual contexts metadataset, which includes scientific figures and charts. This benchmark contains similar image types to our work but has a different focus and uses different question types. The MMMU benchmark  includes multi-discipline college-level image-based problems and questions. Although limited to text, we take inspiration by the adversarial filtering approach taken in , in the curation of the multiple-choice questions in our work. Our work incorporates stylistic and methodological inspiration from these works but tackles a different image type with a different overall focus of scientific figure interpretation.

## 3 SciFIBench

SciFIBench is comprised of 2000 questions, derived from figures and captions extracted from arXiv papers, curated into two tasks and split into 2 subsets based on the data source (Tab. 1).

### Tasks

SciFIBench consists of the following two tasks related to scientific figure interpretation (Fig. 1):

**Figure\(\)Caption**: Given an input figure, along with a set of 5 captions labeled A-E,

select the correct caption for the figure.

**Caption\(\)Figure**: Given an input caption, along with a set of 5 figures labeled A-E,

select the correct figure for the caption.

### Curation methodology

We use the SciCap dataset  as our initial source of scientific figure-caption pairs. SciCap is a large-scale dataset consisting of figures and corresponding captions extracted from arXiv computer science (CS) papers between the years 2010-2020. From SciCap, we select the _Single-Sentence_ subset (train, val, test), containing \(\)94k figure-caption pairs, and only includes captions that are one sentence in length. The figures are filtered to remove any containing subfigures, and the captions are normalised to remove figure numbers. We then perform the following preprocessing and curation steps:

1. **Deduplication**: We initially drop any captions (and corresponding figures) if they are duplicates.

**Subset** &  &  &  &  &  \\  
**CS** & 500 & 500 & 94k & 1 &  & ✓ \\
**General** & 500 & 500 & 102k & 7 &  & ✓ \\  

Table 1: **SciFIBench** is composed of two subsets of questions based on category and source.

2. **Compute embeddings**: We then use a variant of CLIP 1. to compute embeddings for each figure-caption pair. After normalising, we concatenate the text and image embeddings to form joint embeddings, represented as vectors \(x^{d}\), where \(d\) is equal to 2048.

3. **Construct vector database**: Using Faiss , we create a vector database of the joint embeddings.

4. **Find nearest neighbours**: For each embedding, we search for the \(k\) nearest neighbours based on Euclidean distance. Concretely, given the set of database embeddings \(\{x_{i},i=1..N\}^{d}\) and a query embedding \(q^{d}\), we compute the \(k\) nearest neighbours of \(q\) as:

\[(n_{1},,n_{k})=*{argmin}_{n=1..N}^{k}||q-x_{n}||. \]

5. **Similarity filtering**: To increase the likelihood the multiple-choice questions are answerable we remove very similar figure-caption pairs from our dataset (_e.g._, with minor formatting differences but no semantic difference) by dropping a sample (\(x_{s}\)) if its distance to the query embedding (_i.e._, \(||q-x_{s}||\)) falls below a threshold.

6. **Question construction**: For each selected figure-caption pair, we create multiple-choice questions using the \(k\) nearest neighbours. For the **Figure\(\)Caption** task, we create target captions by randomly shuffling the true caption with the corresponding \(k\) nearest neighbour captions. Similarly, for the **Caption\(\)Figure** task, we create the target figures by randomly shuffling the true figure with the corresponding \(k\) nearest neighbour figures.

7. **Categorisation**: We categorise questions based on the arXiv category of the true figure-caption pair. Questions in the 10 most common categories are grouped individually while those in less common categories are labelled 'other cs'; questions from cross-listed papers are labelled 'cross-list'.

8. **Difficulty filtering**: We adopt the average distance of the joint embeddings of the negatives to the true answer as a measure of question difficulty. We sort the questions based on this difficulty.

9. **Human verification**: We sample the most difficult questions per category and perform human verification to select 'answerable' questions. We classify a question as answerable if it contains sufficient information for a domain expert to determine the single correct answer (_i.e._, questions with ambiguous choices; or references to context-dependent details, such as 'Exp. 1', 'Config. 1', etc. are disregarded). Minor text edits were made for a small subset of the questions to reduce ambiguity.

Following these steps, we obtain a pool of high-quality questions. We evaluate GPT-4V  and Gemini-Pro Vision  on the pool and select questions that either model answers incorrectly and sample the remaining questions per category to create our curated 'CS' question set. As some categories had few answerable questions in the pool, category balance was approached, but not achieved in all cases - Fig. 3 illustrates the category representation. For example, the pool of possible 'cs.AI' category questions was dominated by figures/captions from a single paper; to avoid introducing bias, we only included 10 such questions per task. For analysis, a noisier subset was then constructed by taking the next 5000 most difficult questions per task, sampled across categories, without human checking. Example questions from the curated set are shown in Fig. 5. To expand the diversity of the questions in our benchmark, we utilised the ArXivCap dataset , which contains figure-caption pairs from papers covering 32 arXiv domains up to 2023. Due to its larger scale, we initially randomly downselected 25% of the data and removed all pairs from CS papers. We then repeated the curation steps outlined above, to create a pool of answerable questions for each task. We then carried out a category-balanced downsampling of questions Gemini 1.5 Pro  and GPT-4o  answered incorrectly to reach a final curated 'General' set of 500 questions per task. In Fig.2 we show figure and caption statistics for questions across both subsets of SciFIBench.

**Quality control.** We focus our evaluation on a small set of questions to ensure high quality. Having

Figure 2: **SciFIBench question figure and caption statistics**.

manually checked each question, we conservatively estimate the noise level in the curated set - _i.e._ included in SciFIBench - to be at most a few percent. In these minority, questionable cases, we estimate there is a reasonable chance the questions can be answered with appropriate domain expertise. Based on spot checks, we estimate the noise level on the noisy, uncurated questions to be \(\)20-25%. Minor cosmetic errors, such as typos in captions or obscured axis labels, originating from the original data were

Figure 4: **Difficulty distribution comparison of the _Noisy_ uncurated questions (5000) and the _Curated_ questions (1000) included in the CS set of SciFIBench**. We gauge difficulty using the mean L\({}_{2}\) distance between the joint embeddings (**x**) of the positive and negative answers for each question. A higher distance indicates an easier question.

Figure 5: **Example SciFIBench questions for each task with the challenging adversarially-selected and easier randomly-selected negatives. SciFIBench covers a broad range of figure types including line/pie/bar/flow charts, scatter/box/3D/contour plots, multiplots, maps, heatmaps, and decision trees.**

Figure 3: **SciFIBench category representation.**

deliberately left unchanged when included in SciFIBench to increase realism and difficulty. **Question difficulty.** Preliminary ablation studies on a random set of questions showed that, for nearly all the LMMs evaluated, selecting hard negatives using nearest neighbours determined by joint-embedding similarity yields the most challenging questions, with lower accuracy scores than the single-modality neighbours. Fig. 4 outlines a comparison of the difficulty distribution of the included curated questions and uncurated noisy questions, based on \(L_{2}\) distance. The effect of adversarial compared to random negatives selection can be seen in the disparity of the orange and blue distributions, with the adversarial negatives having a much lower mean \(L_{2}\) distance and therefore higher difficulty. As expected from the curation process, the curated adversarial distribution is more challenging than the noisy distribution.

## 4 Experiments

Through a variety of experiments, we evaluate the scientific figure interpretation abilities of a selection of LMMs on our SciFIBench benchmark and conduct a detailed analysis of their performance.

### Baselines

**LMMs.** We evaluate the following **closed-source** models: GPT-4 {V, Turbo, o}  Gemini-Pro Vision , Gemini 1.5 {Flash, Pro} , and Claude 3 {Opus, Sonnet and Haiku} . We also evaluate the following **open-source** models: IDEFICS , Qwen-VL , Emu2 , TransCore-M , InternLM-Composer 1,2 , CogVLM , OmniLMM , Yi , InstructBLIP , Monkey , and LLaVA-1.5 . We use chat / instruction-tuned variants of each model (rather than base models) and compare the performance of multiple model sizes where available. Roughly half of these baselines can take interleaved text and images as input, and therefore be evaluated on the Caption\(\)Figure task. We also consider a text-only baseline in which we provide the LMM with the output from an OCR model  rather than images. **VLM.** As a point of comparison, we evaluate strong VLM models on SciFIBench. Specifically, we evaluate a MetaCLIP  variant, the Google Multimodal Embedding Model , and the CLIP model  used to determine the nearest neighbour multiple-choice options. **Humans.** Additionally, we evaluate a human baseline to gauge the relative performance difference between humans and LMMs. The humans (undergraduate and postgraduate students) were presented with the same prompt as the models.

While it is difficult to say with certainty if arXiv data was included in the training sets of these models, there might be some leakage, as expected when using web images. However, given the scale of the training data, we do not expect this to impact our evaluation.

### Experimental Settings

**Inference.** For the closed-source models, inference was carried out via the OpenAI API  or Vertex AI API . We use Transformers  and OpenCompass toolkit  to access the open-source models and conduct inference using NVIDIA A100 GPUs. With current pricing, evaluating GPT-4V on SciFIBench costs \(\)$30. For the open-source models, the typical inference runtime using an A100 is \(\)1 hour (_e.g._, using Qwen-VL).

**Hyperparameters.** We select model hyperparameters that produce deterministic output. For the open-source models, we utilise the greedy search decoding strategy, in which the most probable token is selected from the model vocabulary \(V\) at each step, conditional on the preceding tokens _i.e._, \(w_{n+1}=*{arg\,max}_{w V}P(w|w_{1},w_{2},...,w_{n})\). For the Gemini and Claude models, we set the \(temperature\) to 0 and \(topk\) to 1; for the GPT-4 models, we also set the \(temperature\) to 0.

**Prompting.** We adopt a generic 0-shot chain-of-thought  style prompt for each task, details of which can be found in the Appendix. Where relevant, we follow model-specific prompting suggestions and modify the prompt template accordingly. We found that shuffling the order of the multiple-choice answers causes performance to vary within a range of 5%.

**Automatic Evaluation.** Despite instruction to constrain the format of the model answers to each question to just the target choice letter, _e.g._, 'A', most of the evaluated models did not consistently follow this, posing a challenge to automatic evaluation. To overcome this, we used Gemini-Pro to initially parse the output and extract the answer letter or flag if no single answer was given.

### Main Results

To gauge the abilities of frontier LMMs to interpret scientific figures, we evaluate a diverse set of LMMs and other baselines on SciFlBench, the results for which are displayed in Tab. 2. Note, our core analysis is in reference to results obtained on the adversarially generated question negatives. We present our key findings as follows:

**SciFlBench represents a difficult benchmark.** The best-performing models, GPT-4o and Gemini 1.5 Flash, attain scores of 73.8% and 70.1% for the Figure\(\)Caption task, and 65.4% and 66.1% for the Caption\(\)Figure tasks, respectively. This shows that even at the frontier there is room for improvement. Among the weaker models, there is much more headroom, with the weakest models only just equalling or surpassing the chance score. Overall, there is a large spread of performance scores across the models, suggesting the benchmark has a good range of question difficulties.

**Closed-source models are noticeably better than open-source models.** Considering the Figure\(\) Caption task, there is a difference of 34.6% between the scores of the best closed and open-sourced models. Moreover, the best-performing open-source model, TransCore-M underperforms the worst closed-source model. This difference is more pronounced for the Caption\(\)Figure task.

**Adversarially selected negatives are more challenging.** As an ablation, we compare model performance when answering questions with adversarially selected multiple-choice negatives and randomly selected negatives (see Tab. 2 coloured text). As expected, in the vast majority of cases, accuracy scores are higher on the random negatives - for some open-source models, the accuracy score more than doubles, and for the closed-source models, the maximum accuracy score is almost met. However, for the open-source models evaluated on the Caption\(\)Figure task, there is almost no change in performance between the adversarial and random negative settings. Given that the scores are close to the chance score, it is likely this task is too challenging for these models.

  &  &  &  &  \\  &  **Advers.** \\ **negatives** \\  } &  **Random** \\ **negatives** \\  } &  **Advers.** \\ **negatives** \\  } &  **Random** \\ **negatives** \\  } &  **Agres.** \\ **negatives** \\  } &  **Agres.** \\ **negatives** \\  } &  **Agres.** \\ **negatives** \\  } &  **Agres.** \\ **negatives** \\  } \\  &  **Classifier** \\  } & 

**Caption\(\)Figure is more difficult than Figure\(\)Caption.** Multi-image tasks are known to be challenging to LMMs , and slightly higher overall scores are attained on the Figure\(\)Caption task, especially in the random negatives setting. Considering the human baseline, a noticeably lower score is attained on the Caption\(\)Figure task, suggesting it is easier for humans to distinguish fine-grained details in the text domain. The VLM baselines show no discernible difference in performance across the tasks, a possible reflection of their pretraining strategy of jointly aligning language and vision.

**Performance does not necessarily scale with model size.** Considering the models that we evaluate multiple checkpoint sizes (_e.g._, IDEFICS, OmniLMM, Yi, etc.), we find that more often than not, the _smaller_ model outperforms the larger checkpoint on the adversarially selected negatives, however, the opposite is true for the randomly selected negatives. Additionally, the difference in performance is more pronounced on the randomly selected negatives.

**CLIP remains a strong baseline.** Across both tasks, on questions with adversarial negatives, the CLIP baseline performs comparably or superior to the leading open-source models, though is beaten by the closed-source models. When negatives are randomly selected, CLIP far surpasses the open-source models, almost equalling GPT-4V and the Gemini-Pro models.

**Humans are a stronger baseline.** The mean human baseline outperforms all the models, though does not achieve a perfect score, reflecting the challenging nature of SciFIBench and the fact that the participants were not necessarily domain experts. As indicated by the standard deviation, a range of accuracy scores were recorded for each task, with some participants scoring equal or lower than the best LMMs. It is worth noting a caveat to the human performance is that the human verification part of the curation process could have introduced bias toward questions that are 'easier' for humans to answer.

**OCR.** As detection of fine-grained textual detail is a key component of scientific figure interpretation, it is not unexpected that an above-chance score can be attained in a text-only setting. However, given the significant difference in performance between the text-only and image settings, it is clear that interpretation of visual details is required to answer the majority of questions in our benchmark.

In the Appendix, we include qualitative results for each task and examples of model output.

### Curated vs. noisy data

Here, we provide evidence that although our benchmark is relatively small, it is sufficiently robust. We evaluate a subset of our models on both the curated and noisy CS question sets and find that in almost every case the ranking is preserved across the datasets - and in the case where the rankings switch, the performance differential between the two models is small - suggesting there is little information to be gained by evaluating on an arbitrarily larger dataset. Additionally, we conduct bootstrapping to estimate the variance of model performance on the curated dataset. Concretely, for each task, we sample with replacement 500 times from the relevant question set and evaluate the performance of Gemini-Pro Vision (middle-performing model capable of both tasks) on the sample. Repeating this process 100k times yields a mean accuracy and variance of (\(56.00,0.05\)) and (\(52.40,0.05\)) for the two tasks. This low variance provides further evidence that our curated dataset is sufficiently representative.

Figure 8: **Error analysis. Left: performance with publication year for both tasks, centre: performance with caption length for the Figure\(\)Caption task, and right: performance with mean figure size for the Caption\(\)Figure task. Shaded regions show 95% Wilson confidence intervals.**

Figure 6: Gemini-1.5 Flash **k-shot results.**_Right_: answer refusal rate (0 indicates a valid answer for all questions).

We also analyse the degree to which the noisy data can be used to provide few-shot examples. We experimented with both 'random' and'similar' (selected based on similarity) examples. The results for Gemini 1.5 Flash are shown in Tab.6. We find that model performance is sensitive to the prompt and with some prompt structures, the presence of examples decreases performance. However, with certain prompts as the results show, incorporating potentially noisy examples can quantitatively improve performance compared to the 0-shot setting. Additionally, a qualitative review of the outputs suggests that the model's responses more closely follow the instructed format when examples are included, which reduces the need for an LLM to parse the correct answer from the LMM output. The finding that performance can decline with increased shots in certain scenarios is consistent with other works, _e.g._.

### Error analysis

Fig.7 displays a decomposition of performance by arXiv category for the Figure\(\)Caption task for a subset of the evaluated models. We find that the relative rankings of models remain broadly consistent across categories. However, there are clear differences in performance across the categories with most models scoring highly on'stat' questions and much lower on 'q-bio'. Refusal rates to provide a multiple-choice answers for every question was low among the evaluated LMMs, though models with a higher proclivity for verbose outputs tended to be less decisive. We include an analysis of performance across different properties of the figure-caption pairs making up each question, including the publication year of the source arXiv paper, caption length and figure size, in Fig.8. In general, there is no clear evidence of publication year and figure size having any impact on model performance, though a slight macro-trend of decreasing performance with increasing caption length is observed.

### Caption generation

We extend the breadth of our evaluation by assessing the capabilities of the LMMs to generate suitable captions for scientific figures. Initially, we construct a test set by randomly sampling 100 figure-caption pairs from the Figure\(\)Caption task. We then prompt a set of test LMM baselines to generate captions for each figure and select the GPT-4o, Gemini-1.5 Pro, Claude 3.5 Sonnet  models to evaluate the generated captions. These specific models were chosen as evaluators because they are'stronger' than the test models on most benchmarks and are from different model families, reducing potential bias. The generated captions were shuffled with the true caption and passed to the evaluator models to rank. We report the rankings across all samples in Tab.3. The results clearly delineate preference among the test models with the closed-source models outperforming the open-source models (as they do on SciFlBench). **Captions generated by all closed-source models are preferred over the true caption.** Conversely, the true captions are preferred over all the open-source model captions. **Strong agreement is shown between the evaluator models**. Further details can be found in the Appendix.

this extra information. For the remaining two augmentations, we repeat this process, however, we mark a randomly chosen _incorrect_ caption as <Correct>. For a selection of models, we evaluate the performance accuracy on each augmented question set and display the results in Fig. 10.

**Annotating an answer as correct significantly changes performance.** We find that for all models, marking the correct answer has a noticeable increase in performance relative to the baseline. Similarly, marking the incorrect answer as correct consistently decreases the performance relative to the baseline. There are also clear differences in sensitivity to this new information. For example, the performance relative to the baseline for Qwen-VL and Gemini-Pro Vision varies at most 30%, whereas for models like LLaVA-1.5 and OmniLMM, the difference exceeds 50%.

**Some models are better at following instructions.** We can obtain a gauge of the alignment of the models by analysing the degree to which instruction to ignore the <Correct> annotation is followed. In almost every case, we find that the instruction does cause the performance to change in the desired direction (_i.e._, towards the baseline score), though the amount of change varies depending on the model. For example, the performance of OmniLMM and TransCore-M shows almost no difference when instructed to ignore the annotation, suggesting weaker instruction-following. Whereas, the performance of CogVLM in particular changes drastically with the additional instruction.

## 5 Conclusions

We introduce the Scientific Figure Interpretation Benchmark (SciFlBench) to evaluate the capabilities of LMMs to interpret and understand scientific figures. We curate the multiple-choice questions in our benchmark using arXiv paper figure-captions pairs from the SciCap  and ArXiVCap  datasets and employ adversarial filtering to select hard negatives, increasing the difficulty of our benchmark. We use human verification when selecting questions to construct a robust, high-quality dataset that can be used to efficiently evaluate future models without the need for extensive compute or API credits. We benchmark the performance of 32 LMM, VLM and human baselines on SciFlBench, finding it to be challenging, with room for improvement. Finally, we analyse the alignment and instruction following abilities of the LMMs when answering questions in our benchmark. We release our dataset for the community to use and hope our work encourages further research in this important domain.

Figure 10: **Performance comparison on the augmented question sets**. Note, the labelled percentage changes reflect the change in accuracy relative to the baseline.

Figure 9: **Alignment experiment overview.** We create 4 augmentations of the baseline Figure\(\)Caption questions with different information and instructions.