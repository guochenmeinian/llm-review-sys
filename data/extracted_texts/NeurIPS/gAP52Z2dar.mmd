# Inverse Preference Learning:

Preference-based RL without a Reward Function

 Joey Hejna

Stanford University

jhejna@cs.stanford.edu &Dorsa Sadigh

Stanford University

dorsa@cs.stanford.edu

###### Abstract

Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods naively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the \(Q\)-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released1.

## 1 Introduction

Reinforcement Learning (RL) has shown marked success in fixed and narrow domains such as simulated control  and game-playing . When deploying RL in more complex settings, like in robotics or interaction with humans, one often runs into a critical bottleneck: the reward function. Obtaining reward labels in the real world can be complex, requiring difficult instrumentation  and painstaking tuning  to achieve reasonable levels of sample efficiency. Moreover, despite extensive engineering, reward functions can still be exploited by algorithms in ways that do not align with human values and intents , which can be detrimental in safety-critical applications .

Instead of hand-designing reward functions, contemporary works have attempted to learn them through expert demonstrations , natural language , or human feedback . Recently, reward functions learned through pairwise comparison queries--where a user is asked which of two demonstrated behaviors they prefer--have been shown to be effective in both control  and natural language domains . This is often referred to as _Reinforcement Learning with Human Feedback (RLHF)_. Reward functions learned via RLHF can directly capture human intent, while avoiding alternative and more expensive forms of human feedback such as expert demonstrations. Preference-based RL algorithms for RLHF often interleave reward-learning from comparisons with off-the-shelf RL algorithms.

While preference-based RL methods discover reward functions that are aligned with human preferences, they are not without flaws. Learned reward functions must have adequate coverage ofboth the state and action space to attain good downstream performance. Consequently, learning the reward function can be expensive, usually requiring thousands of labeled preference queries. To mitigate these challenges, recent works have proposed improving learned reward functions by adding inductive biases before optimization with RL. Hejna and Sadigh  pretrain reward functions with meta-learning. Park et al.  use data augmentation. Early et al.  and Kim et al.  make the reward function non-Markovian using recurrent or large transformer sequence model architectures respectively. Such approaches increase the upfront cost of preference-based RL by using additional data or compute. Moreover, these techniques still combine reward optimization with vanilla RL algorithms. Ultimately, this just adds an extra learned component to already notoriously delicate RL algorithms, further increasing hyper-parameter tuning overhead. Preference-based RL approaches often end up training up to four distinct neural networks independently: a critic (with up to two networks), an actor, and a reward function. This can be problematic as prediction errors cascade from the reward function, to the critic, and ultimately the actor causing high variance in downstream performance. To address these issues, we propose a parameter-efficient algorithm specifically designed for preference-based RL that completely eliminates the need to explicitly learn a reward function. In doing so, we reduce both complexity and compute cost.

The key insight of our work is that, under a fixed policy, the \(Q\)-function learned by off-policy RL algorithms captures the same information as the learned reward function. For example, both the \(Q\)-function and reward function encode information about how desirable a state-action pair is. This begs the question: why do we need to learn a reward function in the first place? Our proposed solution, Inverse Preference Learning or IPL, is an offline RL algorithm that is specifically designed for learning from preference data. Instead of relying on an explicit reward function, IPL directly optimizes the implicit rewards induced by the learned \(Q\)-function to be consistent with expert preferences. At the same time, IPL regularizes these implicit rewards to ensure high-quality behavior. As a result, IPL removes the need for a learned reward function and its associated computational and tuning expense.

Experimentally, we find that even though IPL does not explicitly learn a reward function, it achieves competitive performance with complicated Transformer-based reward learning techniques on offline Preference-based RL benchmarks with real-human feedback. At the same time, IPL consistently exhibits lower variance across runs as it does not suffer from the errors associated with querying a learned reward model. Finally, under a minimal parameter budget, IPL is able to outperform standard preference-based RL approaches that learn an explicit reward model.

## 2 Related Work

Our work builds upon literature in reward learning, preference-based RL, and imitation learning.

**Reward Learning.** Due to the challenges associated with designing and shaping effective reward signals, several works have investigated various approaches for learning reward functions. A large body of work uses inverse RL to learn a reward function from expert demonstrations , which are unfortunately difficult to collect  or often misaligned with true human preferences . Subsequently, reward learning techniques using other simpler forms of feedback such as scalar scores  and partial  or complete rankings  have been developed. One of the simplest forms of human feedback is pairwise comparisons, where the user chooses between two options. Often, pairwise comparison queries are sampled using techniques from active learning . However, to evaluate learned reward functions, these methods rely on either RL or traditional planning algorithms which are complex and computationally expensive. Our approach takes a simpler perspective that is parameter-efficient by combining reward and policy learning. Though it is not the focus of our work, IPL could additionally leverage active learning techniques for selecting preference data online.

**Preference-based Deep Reinforcement Learning.** Current approaches to preference based deep RL train a reward function, and then use that reward function in conjunction with a standard reinforcement learning algorithm . Several techniques have been developed to improve the learned reward function, such as pre-training , meta-learning , data augmentation , and non-Markovian modeling. Within the family of non-Markovian reward modeling , recent approaches have leveraged both LSTM networks  and transformers  for reward learning. But, these methods still rely on Markovian offline RL algorithms such as Implicit Q-Learning (IQL)  for optimization. Ultimately, this makes such approaches theoretically inconsistent as the policy learning component assumes the reward to be only a function of the current state and action. All techniques for learning the reward function in combination with standard RL methods [20; 49] end up adding additional hyper-parameter tuning and compute cost. IPL on the other hand, is directly designed for RL from preference data and eliminates the reward network entirely. Other recent works also consider contrastive objectives instead of RL [26; 23].

Recently, works in natural language processing have applied ideas from preference-based RL to tasks such as summarization [51; 54], instruction following , and question-answering . The RLHF paradigm has proven to be powerful even at the massive scale of aligning large language models. In this regime, learned reward models are massive, making an implicit reward method like IPL more attractive. In fact, IPL in a contextual bandits setting recovers concurrent work by Rafailov et al.  on implicit reward modeling in LLMs (see Appendix A). While we focus on control in our experiments, we hope our work can inform future explorations in language domains.

**Imitation Learning**. Our work builds on foundational knowledge in maximum entropy (MaxEnt) RL  and inverse RL . Recent works in MaxEnt inverse RL have used the mapping between \(Q\)-functions and reward functions under a fixed policy. Specifically, Garg et al.  show that the regularized MaxEnt inverse RL objective from Ho and Ermon  can be re-written using the \(Q\)-function instead of a reward function and Al-Hafez et al.  stabilize their approach. While the relationship between \(Q\)-functions and rewards has been used for MaxEnt inverse RL, we study this relationship when learning from preference data. While both problems seek to learn models of expert reward, the data differs significantly -- preference-based RL uses comparisons instead of optimal demonstrations. This necessitates a greatly different approach.

## 3 Inverse Preference Learning

In this section, we first describe the preference-based RL problem. Then, we describe how, leveraging techniques from imitation learning, we can remove the independently learned reward network from prior methods. This results in a simpler algorithm with lower computational cost and variance in performance.

### Preference-Based RL

We consider the reinforcement leraning (RL) paradigm where an agent seeks to maximize its expected cumulative discounted sum of rewards in a Markov Decision Process (MDP). Standard off-policy RL algorithms, do so using state, action, reward, and next state tuples \((s,a,r,s^{})\). In preference-based RL, however, the reward function \(r\) is unknown, and must be learned from human feedback. Thus, Traditional preference-based RL methods are thus usually separated into two stages: first, reward learning, where \(r_{E}\) is estimated by a learned reward function \(r_{}\), and second, reinforcement learning, where a policy \((a|s)\) is learned to maximize \(_{}[_{r=0}^{}^{r}r_{}(s,a)]\) with \(\) as the discount factor. Though our method combines these two phases, we use the building blocks of each and consequently review them here.

**Preference Learning.** First, similar to prior works [13; 32], we assume access to preference data in the form of binary comparisons. Each comparison is comprised of two behavior segments, \(^{(1)}\) and \(^{(2)}\), and a binary label \(y\) indicating which of the two was preferred by an expert. As in Wilson et al. , each behavior segment is simply a snippet of a trajectory of length \(k\), or \(=(s_{t},a_{t},s_{t+1},a_{t+1},,a_{t+k-1},s_{t+k})\). Increasing \(k\) can provide more information per label at the cost of potentially noisier labels. The label \(y\) for each comparison is assumed to be generated by an expert according to a Bradley-Terry Preference model :

\[P_{r_{E}}[^{(1)}>^{(2)}]=r_{E}(s_{t}^{(1)},a_{t} ^{(1)})}{_{t}r_{E}(s_{t}^{(1)},a_{t}^{(1)})+_{t}r_{E}(s_{t}^{(2 )},a_{t}^{(2)})},\] (1)

where \(r_{E}(s_{t},a_{t})\) is again the expert's unknown underlying reward model. We use the subscript \(r_{E}\) on probability \(P\) to indicate that the preference distribution above results from the expert's reward function. Let the dataset of these preferences be \(_{p}=\{(^{(1)},^{(2)},y)\}\). To learn \(r_{E}\), prior works in preference-based RL estimate a parametric reward function \(r_{}\) by minimizing the binary-cross-entropy over \(_{p}\):

\[_{p}()=-_{^{(1)},^{(2)},y ^{-}_{p}}[y P_{r_{}}[^{(1)} ^{(2)}]+(1-y)(1-P_{r_{}}[^{(1)}^{ (2)}])].\] (2)This objective results from simply minimizing \(_{_{p}}[D_{}(P_{r_{E}}||P_{})]\), the KL-divergence between the expert preference model and the one induced by \(r_{}\), effectively aligning it with the expert's preferences. We note that some other works in preference-based RL focus on learning an improved model \(r_{}\) to address the reward learning part of the problem . However, these methods still use off-the-shelf RL algorithms for the policy learning part of the problem.

**Reinforcement Learning.** Common off-policy RL methods learn a policy \(\) by alternating between policy evaluation (using the contrastive Bellam Operator \(_{r}^{}\)) to estimate \(Q^{}\) and policy improvement, where the policy \(\) is improved . Concretely, after repeated application of \(_{r}^{}\) as

\[(_{r}^{}Q)(s,a)=r(s,a)+_{s^{}-P(-|s,a)} [V^{}(s^{})],\] (3)

the policy can be improved by maximizing \(Q\). In some settings, the Bellman operator \(_{r}^{*}\) corresponding to the optimal policy \(^{*}\) can be used directly, removing the need for the policy improvement step. In these cases, we can simply extract \(^{*}\) from the resulting \(Q^{*}\).

To learn the optimal policy, two-phase preference based RL methods rely on recovering the optimal \(r_{E}\) in the reward learning phase before running RL. This potentially propagates errors from the estimated \(r_{}\) to the learned \(Q\)-function and ultimately learned policy \(\). In practice, it would be more efficient to eliminate the need for two separate stages. In the next section, we show how this can be done by establishing a bijection between reward functions \(r\) and \(Q\)-functions.

### Removing The Reward Function

In this section, we formally describe how the reward function can be removed from offline preference-based RL algorithms. Our key insight is that the \(Q\)-function learned by off-policy RL algorithms in fact encodes the same information as the reward function \(r(s,a)\). Consequently, it is unnecessary to learn both. First, we show how the reward function can be re-written in terms of the \(Q\) function allowing us to compute the preference model \(P_{Q}\) induced by the \(Q\)-function. Then, we derive an objective that simultaneously pushes \(Q\) to fit the expert's preferences while also remaining optimal.

Consider fitting a \(Q\) function via the Bellman operator \(_{r}^{}\) for a fixed policy \(\) until convergence where \(_{r}^{}Q=Q\). Here, to encode the cumulative discounted rewards when acting according to the policy, the \(Q\)-function depends on both \(r\) and \(\). This dependence, however, is directly disentangled by the Bellman equation. By rearranging it (Eq. (3)), we can solve for the reward function in terms of \(Q\) and \(\). This yields the so-called inverse soft-Bellman operator:

\[(^{}Q)(s,a)=Q(s,a)-_{s^{}}[V^{}(s ^{})].\] (4)

In fact, for a fixed policy \(\) the inverse-Bellman operator is bijective, implying a one-to-one correspondence between the \(Q\) function and the reward function. Though this was previously shown in maximum entropy RL , we prove the general case Lemma 1 in Appendix A.

Figure 1: A depiction of the difference between standard preference-based RL methods and Inverse Preference Learning. Standard preference-based RL first learns a reward function, then optimizes it with a blockbox RL algorithm. IPL trains a \(Q\) function to directly fit the expert’s preferences. This is done by aligning the implied reward model with the expert’s preference distribution and applying regularization.

Intuitively, this makes sense: when holding the policy constant, only the reward function affects \(Q\). We abbreviate the evaluation of \((^{}Q)(s,a)\) as \(r_{Q^{}}(s,a)\) to indicate that \(r_{Q^{}}\) is the unique implicit reward function induced by \(Q^{}\). Prior works in imitation learning leverage the inverse soft-Bellman operator to measure how closely the implicit reward model \(r_{Q^{}}\) aligns with expert demonstrations . Our key insight is that this equivalence can also be used to directly measure how closely our \(Q\) function aligns with the expert preference model _without ever directly learning_\(r\).

Consider the Bradley-Terry preference model in Equation1. For a fixed policy \(\) and its corresponding \(Q^{}\), we can obtain the preference model of the implicit reward function \(P_{Q^{}}[^{(1)}>^{(2)}]\) by substituting the inverse Bellman operator into Equation1 as follows:

\[P_{Q^{}}[^{(1)}>^{(2)}]=(^{}Q) (s_{t}^{(1)},a_{t}^{(1)})}{_{t}(^{}Q)(s_{t}^{(1)},a_{t} ^{(1)})+_{t}(^{}Q)(s_{t}^{(2)},a_{t}^{(2)})}.\] (5)

This substitution will allow us to measure the difference between the preferences implied by \(Q^{}\) and those of the expert. To minimize the difference, we can propagate gradients through the preference modeling loss (Equation2) and the implicit preference model \(P_{Q^{}}\) (Equation5) to \(Q\)--just as we would for a parameterized reward estimate \(r_{}\). Unfortunately, naively performing this substitution is insufficient to solve the RL objective for two reasons.

**The Optimal Inverse Bellman Operator.** First, we have used an arbitrary policy \(\), not the optimal one, for converting from \(Q\)-values to rewards. Though the \(Q\)-function may imply the expert's preferences, the corresponding policy could be extremely sub-optimal. To fix this problem, we need to use the optimal inverse bellman operator \(^{*}\) to ensure the extract \(Q\)-function corresponds to that of \(^{*}\). For this step, we can use any off-policy RL-algorithm that converges to the optimal policy! If the algorithm directly estimates the \(_{r}^{*}\), the corresponding \(^{*}\) can be estimated using the target from \(_{r}^{*}\), or

\[(^{*}Q)(s,a)=Q(s,a)-_{s^{}}[V^{} (s^{})]V^{}(s)_{r}^{*}.\]

In many cases, however, computing the optimal bellman operator \(_{r}^{*}\) is infeasible. Instead, many modern off-policy RL algorithms use policy improvement to converge to the optimal policy. These methods, like Haarnoja et al.  use \(Q^{}\) to estimate a new policy \(^{}\) such that \(Q^{^{}} Q^{}\). By repeatedly improving the policy, they eventually converge to \(Q^{*}\). Thus, by repeatedly improving the policy according to these algorithms, we can eventually converge to the optimal policy and can thus estimate corresponding optimal inverse bellman operator by using \(V^{}(s)=_{a^{*}(|s)}[Q(s,a)]\) in the above equation.

**Regularization.** Given we can estimate \(^{*}\) using targets from \(_{r}^{*}\) or policy improvement, we can fit the optimal \(Q\)-function by minimizing the following loss function

\[_{P}(Q)=-_{^{(1)},^{(2)},y_{ P}}[y P_{Q^{*}}[^{(1)}>^{(2)}]+(1-y)(1-P_{Q^{*}}[ ^{(1)}>^{(2)}]].\]

where \(P_{Q^{*}}\) is given by substituting \(^{*}Q\) into Eq.5. Unfortunately, optimizing this objective alone leads to poor results and may not converge when using RL algorithms that depend on policy improvement. This is because the above objective is under constrained due to the invariance of the Bradley-Terry preference model to shifts. By examining Eq.1, it can be seen that adding a constant value to all rewards does not change the probability of preferring a segment. However, shifting the reward function by a constant _does_ change the \(Q\)-function. RL algorithms using policy improvement monotonically increase the \(Q\)-function until reaching the maximum at \(Q^{*}\). Thus as the implicit reward continues to increase, \(Q^{*}\) will continue to increase and may never be reached. To resolve this issue, we insure that the optima of the preference loss is unique by introducing a convex regularizer \(()\) on the implicit rewards \(r_{Q^{}}=^{}Q\), giving us the regularized preference loss:

\[_{P}(Q)=-_{^{(1)},^{(2)},y_{ P}}[y P_{Q^{*}}[^{(1)}>^{(2)}]+(1-y)(1-P_{Q^{*}}[ ^{(1)}>^{(2)}]]+(^{*}Q)\] (6)

In practice we choose \(\) to be a form of L2 regularization as is commonly done in imitation learning  to prevent unbounded reward values. \(>0\) is a hyperparameter that controls the strength of regularization. Besides allowing us to guarantee convergence, regularization has a number of benefits. It can help center the implicit reward near zero, which has been shown to beneficial for RL . Moreover, it encourages more realistic implicit rewards. For example, a reward function might change rapidly by large values when only small perturbations are applied to the state or action. Though such reward functions might be unrealistic, they are completely valid solutions of the inverse-Bellmanoperator. Adding regularization can help penalize large deviations in reward unless they drastically reduce the preference loss. Thus, the first term of Eq. (6) encourages the \(Q\)-function to match the expert's preferences, while the second term smooths the implied reward function and makes it unique.

Our final algorithm, which we call Inverse Preference Learning (IPL) fits the optimal policy corresponding to the regularized expert reward function by repeatedly minimizing \(_{p}(Q)\) (Eq. (6)) and improving the value target used \(V^{}\) with the update step from any off-policy RL algorithm. In this manner, IPL performs dynamic programming through the inverse bellman operator until convergence. In Appendix A, we prove the following Theorem.

**Theorem 1**: _Given an off-policy RL algorithm that convergences to the optimal policy \(_{r}^{*}\) for some reward function \(r\) and regularizer \(\) such that Eq. (2) is strictly convex, IPL converges to \(_{r}^{*}\), corresponding to reward function \(r^{*}=_{r}_{_{p}}[D_{KL}(P_{r_{E}}||P_{ })]+(r)\)._

The proof of the theorem essentially relies on the fact that for a fixed policy \(\), we can optimize \(_{p}(Q)\) (Eq. (6)) to fit \(r^{*}\). Then, we can update the policy (or target values \(V^{}\)) and optimize \(_{p}(Q)\) again. Because \(r^{*}\) is unique, we fit \(r^{*}\) again the second time, but the \(Q\)-function has improved. There are many choices of regularizers where this holds. In tabular settings if \((r)=r^{2}\), \(_{p}(Q)\) reduces to L2 regularized logistic regression, which is strictly convex, guaranteeing convergence (Appendix A).

Effectively, IPL removes the need to learn a reward network, while still converging to similar solution as other preference-based RL algorithms. Learning a reward network requires more parameters and a completely separate optimization loop, increasing compute requirements. Moreover, an explicit reward model introduces a whole new suite of hyper-parameters that need to be tuned including the model architecture, capacity, learning rate, batch size, and stopping criterion. In fact, because human preference data is so difficult to collect, many approaches opt to use simple accuracy thresholds instead of validation criteria to decide when to stop training \(r_{}\). All of these components make preference-based RL unreliable and high-variance. On the other hand, our method completely removes all of these parameters in exchange for a single \(\) hyper-parameter that controls the regularization strength. Though we have theoretically derived IPL, in the next section we provide practical recipes for applying it to offline preference-based RL.

### IPL for Offline Preference-based RL

In offline preference-based RL, we assume access to a fixed offline dataset \(_{o}=\{(s,a,s^{})\}\) of interactions without reward labels generated by a reference policy \((a|s)\) in addition to the preference dataset \(_{p}\). Common approaches to offline RL seek to learn _conservative_ policies that do not stray too far away from the distribution of data generated by \((a|s)\). This is critical to prevent the policy \(\) from reaching out-of-distribution states during deployment which can be detrimental to performance. In this section, we detail a practical version of IPL that uses the \(\)QL offline RL algorithm . \(\)QL fits the KL-constrained RL objective

\[_{}_{}[_{t=r}^{}^{t}(r(s_{t},a_{ t})-|s_{t})}{(a_{t}|s_{t})})]\]

where \(\) controls the magnitude of the KL-divergence penalty. The \(\)QL algorithm directly fits the optimal \(Q\)-function using the optimal soft-Bellman operator 

\[(_{r}^{*}Q)(s,a)=r(s,a)+_{s^{}}[V^{}(s^{})],V^{}(s)=_{ (|s)}[e^{Q(s,a)/}].\]

In practice, \(V^{}\) is estimated using the linear loss function over the current \(Q\)-function. Thus, to fit the optimal \(Q\)-function, IPL with \(\)QL alternates between minimizing the preference loss \(_{p}(Q)\) (Eq. (6)), and updating a learned value function \(V\) until they converge to \(Q^{*}\) and \(V^{*}\). Note that we are not limited to using just \(_{p}\). Though the preference modeling part of \(_{p}(Q)\) can only be optimized with preference data \(_{p}\), the value function can be updated with offline data as well. In the presence of additional offline data, we find that updating the value function using \(_{p}_{o}\) leads to better performance. We approximate L2 regularization with the regularizer \((r)=_{_{p}_{o}}[r(s,a)^{2}]\), which imposes an L2 penalty across the support of the data. While one might try to use weight decay to emulate L2-regularization, doing so is difficult in practice as \(^{*}Q\) depends on both the \(Q\) network and the target network. We find that weighting the regularization equally between \(_{p}\) and \(_{o}\) performs well. After \(Q\) and \(V\) have converged, we can extract the policy using the closed form relationship \(^{*}(a|s)(a|s)((Q^{*}(s,a)-V^{*}(s))/)\) for KL-constrained RL as in Garg et al. (2019); Peng et al. (2019). The full algorithm for IPL with \(\)QL can be found in 1.

Though we have shown how IPL can be instantiated with \(\)QL, it is fully with other offline RL algorithms. In fact, IPL can also be used with online RL algorithms like SAC (Kumar et al., 2019). Critically, this makes the IPL framework general, as it can remove the need for reward modeling in nearly any preference-based RL setting. This makes IPL simpler and more efficient. In the next section, we show that IPL can attain the same performance as strong offline preference-based RL baselines, without learning a reward network.

## 4 Experiments

In this section, we aim to answer the following questions: First, how does IPL compare to prior preference-based RL algorithms on standard benchmarks? Second, how does IPL perform in extremely data-limited settings? And finally, how efficient is IPL in comparison to two-phase preference-based RL methods?

### Setup

As discussed in the previous section, though we use a KL-constrained objective for our theoretical derivation, in practice we can construct versions of IPL based on any offline RL algorithm. In our experiments we evaluate IPL with Implicit Q-Learning (IQL) (Kumar et al., 2019), since it has been used in prior offline preference-based RL works. This allows us to directly compare IPL by isolating its implicit reward component and using the same exact hyper-parameters as prior works. Using IPL with IQL amounts to updating the value function according to the asymmetric expectile loss function instead of the linear loss function. Concretely, this can be done by replacing the value update in Algorithm 1 with \(_{V}_{_{p}_{o}}[|-1 (Q(s,a)-V(s)<0)|(Q(s,a)-V(s))^{2}]\) where \(\) is the expectile.

Inspired by Park et al. (2019), we introduce data augmentations that sample sub-sections of behavior segments \(\) during training. While such augmentations are inapplicable to non-Markovian reward models, we find that they boost performance for Markovian reward models while also reducing the total number of state-action pairs per batch of preference data. This is important as IPL needs data from both \(_{p}\) and \(_{o}\) to regularize the implicit reward function. Additional experiment details and hyper-parameters can be found in the Appendix.

### How does IPL perform on preference-based RL benchmarks?

We compare IPL to other offline preference-based RL approaches on D4RL Gym Locomotion (2017) and Robosuite robotics (Rasmari et al., 2019) datasets with real-human preference data from Kim et al. (2019). We compare IQL-based IPL, with the same hyper-parameters, to various baselines that learn a reward model \(r_{}\) before optimization with IQL. Markovian Reward or MR denotes using a standard Markovian MLP reward model, like those used in Christiano et al. (2019) and Lee et al. (2019). Note that this is also equivalent to T-REX (Kumar et al., 2019) for offline RL. Non-Markovian Reward or NMR denotes using the non-Markovian LSTM based reward model from Early et al. (2019). Preference Transformer (PT) is a state-of-the-art approach that leverages a large transformer architecture to learn a non-Markovian reward and preference weighting function. B-REX uses bayesian optimization to fit a linear reward function from predefined features (Kumar et al., 2019), which in our case are random Gaussian projections of the states and actions. For fairness, we also compare against our own implementation of IQL with a Markovian Reward function that uses the same data augmentation as IPL.

Our results are summarized in Table 1. Starting with the first column, we see that preference-based RL methods are able to match IQL with the ground truth reward function in many cases. On, several tasks, however, the MR implementation from Kim et al. (2019) fairs rather poorly. The non-Markovian methods, (NMR and PT) improve performance. It is worth noting that on many tasks our implementation of a MR (sixth column) performs far better than reported in Kim et al. , likely due to our careful tuning of \(r_{}\) and use of data-augmentations. Our method, IPL, achieves competitive performance across the board.

In general, IPL with IQL performs on-par or better than both our implementation of MR and PT in most datasets despite not learning a separate reward network. Specifically, IPL has the same performance or better performance than our MR implementation on six of eight tasks. More importantly, IPL does extremely well in comparison to Preference Transformer's reported results. On five of eight tasks IPL performs better than PT while having over 10 times fewer parameters, making IPL far more efficient. To be consistent with Kim et al. , we report results after a million training steps but performance for IPL often peaks earlier (see learning curves in the Appendix). For example, with early stopping IPL also outperforms PT on "hop-m-r". We posit that this is because the \(Q\)-function in IPL is tasked with both fitting the expert's preference model and optimal policy simultaneously, making both the policy and reward function non-stationary during training. In some datasets, this was more unstable.

IPL also has the lowest average standard-deviation across seeds, meaning it yields more consistent results than explicit reward methods. For standard two-phase preference-based RL algorithms, errors in the reward model are propagated to and exacerbated by the \(Q\) function. IPL circumvents this problem by not explicitly learning the reward.

Finally, in Table 2, we consider various design decisions of IPL. Augmentations provide a strong boost in the robotics environment, but offer only minor improvements in locomotion. Removing regularization, however, is detrimental to performance. This is likely because without regularization, the implicit reward values can continue to increase, leading to exploding estimates of \(Q\). Finally, we show that IPL is compatible with other offline RL algorithms by combining it with \(\)QL . We find that with \(\)QL, IPL performs even better on some tasks, but worse on others. Finally, in Appendix B, we also show that IPL can be combined with online preference-based RL algorithms like PEBBLE .

### How does IPL scale with Data?

Collecting preference comparisons is often viewed as the most expensive part of preference-based RL. To investigate how well IPL performs in data limited settings, we construct scripted preference datasets

 Dataset &  **IQL** \\ (Oracle) \\  &  **MR** \\ (from ) \\  &  **LSTM** \\ (from ) \\  &  **PT** \\ (from ) \\  &  **BREX** \\ (rempl.) \\  &  **MR** \\ (rempl.) \\  & 
 **IPL** \\ (Ours) \\  \\  hop-m-r & 83.06\(\)15.8 & 11.56\(\)30.3 & 57.88 \(\)40.6 & **84.54\(\)4.1** & 62.0\(\)20.3 & 70.20\(\)35.0 & 73.57 \(\)6.7 \\ hop-m-e & 73.55\(\)41.5 & 57.75\(\)23.7 & 38.63\(\)35.6 & 68.96\(\)33.9 & 85.1 \(\)8.0 & **103.0\(\)5.6** & 74.52 \(\)10.1 \\ walk-m-r & 73.11\(\)8.1 & 72.07\(\)2.0 & **77.00\(\)3.0** & 71.27 \(\)10.3 & 10.3 \(\)5.4 & 68.79\(\)5.6 & 59.92 \(\)5.1 \\ walk-m-e & 107.8\(\)2.2 & **108.3\(\)3.9** & **110.4\(\)0.9** & **110.1\(\)0.2** & 99.62\(\)3.0 & **109.1\(\)1.3** & **108.51\(\)0.6** \\ lift-ph & 96.75\(\)1.8 & 84.75\(\)6.2 & 91.50\(\)5.4 & 91.75\(\)5.9 & 96.6 \(\)3.0 & **98.84\(\)2.3** & **97.60\(\)2.9** \\ lift-mh & 86.75\(\)2.8 & **91.00\(\)2.8** & **90.8\(\)5.8** & 86.75\(\)6.0 & 60.4 \(\)25.1 & **90.04\(\)4.5** & **87.20\(\)5.3** \\ can-ph & 74.50\(\)6.8 & 68.00\(\)9.1 & 62.00\(\)10.9 & 69.67\(\)5.9 & 63.0 \(\)20.3 & **76.40\(\)3.7** & **74.8\(\)2.4** \\ can-mh & 56.25\(\)8.8 & 47.50\(\)3.5 & 30.50\(\)8.7 & 50.50\(\)6.5 & 30.4 \(\)23.0 & 53.6 \(\)7.9 & **57.6\(\)5.0** \\  Avg Std & 10.95 & 10.2 & 13.87 & 9.08 & 13.77 & 8.23 & **4.8** \\ 

Table 1: Average normalized scores of all baselines on human-preference benchmarks from Kim et al. . For the D4RL locomotion tasks “hop” corresponds to hopper, “m” to medium (training the data generating agent to 1/3 expert performance), “r” to replay buffer data, and “e” to data from the end of training. For the Robomimic tasks lift and can, “ph” corresponds to proficient human data and “mh” to multi-human data of differing optimality. The first four columns are taken from Kim et al. . “reimpl.” is our reimplementation of Markovian Reward with IQL. The “Avg Std” row shows the average standard deviation across all eight environments. We run five seeds and report the final performance at the end of training like Kostrikov et al. . Bolded values are within 95% of the top performing method. Note that standard deviation values in the table were rounded for space. On some tasks IPL achieves higher performance earlier in training, which is not reflected above (See Appendix B). We find that IPL outperforms PT on many environments, and also performs similarly to our implementation of MR despite not training a reward function.

of four different sizes for five tasks from the MetaWorld benchmark  used in prior preference-based RL works [32; 22]. We then train on the preference data \(_{p}\) by setting \(_{o}=\{(s,a,s^{})_{p}\}\) and use the same hyper-parameters for all environments and methods where applicable. Our results are summarized in Table 3. Again, IPL is a strong reward-free baseline. We find that at all data scales, IPL performs competitively to our implementation of MR (IQL with a learned Markovian reward) and consistently outperforms it in Button Press and Assembly. Increasing the amount of preference data generally improves performance across the board. However, as we generate queries uniformly at random some preference datasets may be easier to learn from than others, leading to deviations from this trend in some cases. As in the benchmark results in Table 1, IPL exhibits lower variance across seeds and tasks, in this case at three of four data scales.

### How efficient is IPL?

One benefit of IPL over other preference-based RL methods is its parameter efficiency. By removing the reward network, IPL uses fewer parameters than other methods while achieving the same performance. In Table 4, we show the number of parameters for each method used in the last two sections. Preference Transformer uses over ten times more parameters than IPL, and the LSTM-based NMR model from Early et al.  uses nearly twice as many. When dealing with a limited compute or memory budget, this can be important. To exacerbate this effect, we consider an extremely parameter efficient version of IPL, denoted "IPL (64)" in Table 4, based on Advantage Weighted Actor Critic (AWAC)  which eliminates the second critic and value networks used in IQL  and uses a two-layer 64-dimensional MLP. We then compare this parameter-efficient IPL to MR with the same parameter budget which results in "MR (35)", a 35-dimensional MLP. Results are depicted on the left of Fig. 2. MR trained with a smaller network is unable to adequately fit the data, resulting in lower performance. Only after increasing the network size past that of IPL can MR begin to match performance.

Aside from parameter efficiency, IPL is also "hyper-parameter efficient". By removing the reward network, IPL removes a whole set of hyper-parameters associated with two phase preference based RL methods, like reward network architecture, learning rate, stopping criterion, and more. In the

 
**Dataset** & **No Aug** & \(=0\) & **IPL-XQL** & **IPL** \\  hop-m-r & \(70.46 6.7\) & \(10.41 2.26\) & \(80.4 2.13\) & \(73.57 6.67\) \\ walk-m-r & \(58.50 5.3\) & \(4.85 1.52\) & \(57.82 5.24\) & \(59.92 5.11\) \\ lift-mh & \(84.8 4.1\) & \(52.60 10.1\) & \(89.00 4.4\) & \(87.20 5.3\) \\ can-mh & \(53.2 5.8\) & \(13.8 5.7\) & \(59.0 5.0\) & \(57.6 5.00\) \\ 

Table 2: Ablations for IPL on the offline human-preference benchmark. We consider removing data augmentation, removing regularization \(=0\), and other offline RL algorithms (\(\)QL). Full results can be found in Appendix B.

 Preference Queries & 500 & 1000 & 2000 & 4000 \\  Button Press & MR & \(\) & \(49.3 12.1\) & \(54.7 26.8\) & \(78.3 9.2\) \\  & IPL & \(53.3 8.5\) & \(\) & \(\) & \(\) \\  & MR & \(\) & \(\) & \(\) & \(\) \\  & IPL & \(\) & \(78.7 12.4\) & \(\) & \(\) \\  & MR & \(\) & \(\) & \(\) & \(\) \\  & IPL & \(\) & \(\) & \(58.8 7.4\) & \(65.9 6.7\) \\  & MR & \(\) & \(\) & \(23.9 18.8\) & \(\) \\  & IPL & \(\) & \(\) & \(\) & \(\) \\  & MR & \(0.6 0.7\) & \(0.7 1.0\) & \(0.0 0.0\) & \(2.6 2.8\) \\  & IPL & \(\) & \(\) & \(\) & \(\) \\  Avg Std & MR & 5.9 & \(\) & \(13.14\) & \(5.36\) \\  & IPL & \(\) & \(7.22\) & \(\) & \(\) \\ 

Table 3: Results on five MetaWorld tasks at four different preference data scales. We run five seeds for each method, and take the highest average performance across seeds from the learning curves. More details can be found in Appendix B. IPL performs the same or better than IQL with a Markovian reward model on the majority of tasks and preference data scales without training a reward model.

middle of Fig. 2 we show how the performance of MR is affected when the reward function is over or under fit. Choosing the correct number of steps to train the reward model usually requires collecting a validation set of preference data, which is costly to obtain. Instead of this, IPL only has a single regularization parameter, \(\). The right side of Fig. 2 shows the sensitivity of IPL to \(\). We find that in many cases, varying \(\) has little effect on performance unless it is perturbed by a large amount.

## 5 Conclusion

**Summary.** We introduce Inverse Preference Learning, a novel algorithm for offline preference-based RL that avoids learning a reward function. Our key insight is to leverage the inverse soft-Bellman operator, which computes the mapping from \(Q\)-functions to rewards under a fixed policy. The IPL algorithm trains a \(Q\)-function to regress towards the optimal \(Q^{*}\) while at the same time admitting implicit reward values that are consistent with an expert's preferences. Even though IPL does not require learning a separate reward network, on robotics benchmarks it attains competitive performance with preference-based RL baselines that use twice to ten-times the number of model parameters.

**Limitations and Future Work.** A number of future directions remain. Specifically, the implicit reward function and policy learned by IPL are both non-stationary during training, which sometimes causes learning to be more unstable than with a fixed reward function. This is a core limitation future work could address by better mixing policy improvement and preference-matching steps to improve stability. More broadly, implicit reward preference-based RL methods are not limited to continuous control or binary feedback. Applying implicit reward techniques to other forms of feedback or extending IPL to language-based RLHF tasks remain exciting future directions.

**Method** & **Params** \\  PT & 2942218 \\ NMR & 508746 \\ MR & 348426 \\ IPL & 278537 \\ MR (64) & 34892 \\ IPL (64) & 14025 \\ MR (35) & 14012 \\ 

Table 4: Parameter counts for different methods. The bottom three rows are for the limited parameter budget experiments in Section 4.4.

Figure 2: **Left:** Performance comparison with different parameter numbers. MR (35) has the same parameter budget as IPL (64). MR (64) has over twice as many. We see that with the same number of parameters as IPL, MR is unable to adequetly fit the data and performs poorly. **Middle:** MR when the reward function is trained for a varying number of steps – with too few the reward model under-fits, and with too many it over-fits, both leading to worse performance. **Right:** IPL with different regularization strengths. On the drawer open task, performance is largely unaffected. For more ablations, see the Appendix.