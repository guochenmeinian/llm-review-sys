# Reciprocal Learning

Julian Rodemann

Department of Statistics

LMU Munich

j.rodemann@lmu.de

&Christoph Jansen

Computing & Communications

Lancaster University Leipzig

c.jansen@lancaster.ac.uk

&Georg Schollmeyer

Department of Statistics

LMU Munich

g.schollmeyer@lmu.de

###### Abstract

We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: _reciprocal learning_. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits.

## 1 Introduction

The era of data abundance is drawing to a close. While GPT-3  still had to make do with 300 billion tokens, Llama 3  was trained on 15 trillion. With the stock of high-quality data growing at a much smaller rate , adequate training data might run out within this decade . Generally and beyond language models, machine learning is threatened by degrading data quality and quantity . Apparently, learning ever more parameters from ever more data is not the exclusive route to success. Models also have to learn from which data to learn. This has sparked a lot of interest in sample efficiency , subsampling , coresets , data subset selection , and data pruning  in recent years.

Instead of proposing yet another method along these lines, we demonstrate that a broad spectrum of well-established machine learning algorithms already exhibits a _reciprocal_ relationship between data and parameters. That is, parameters are not only learned from data, but data is also iteratively chosen based on currently optimal parameters with the aim of increasing sample efficiency. For instance, consider self-training algorithms in semi-supervised learning , see section 2.1. They iteratively add pseudo-labeled variants of unlabeled data to the labeled training data. The pseudo-labels are predicted by the current model, and thus depend on the parameters learned by the model from the labeled data in the first place. Other examples comprise active learning , Bayesian optimization , superset learning , and multi-armed bandits , see Appendix A for details.

In this paper, we develop a unifying framework, called reciprocal learning, that allows for a principled analysis of all these methods. After an initial model fit to the training data, reciprocal learning algorithms alter the latter in a way _that depends_ on the fit. This dependence can have various facets, ranging from predicting labels (self-training) over taking actions (bandits) to querying an oracle (active learning), all based on the current model fit. It can entail both adding and removing data. Figure 7 illustrates this oscillating procedure and compares it to a well-known illustration of classicalmachine learning. A pressing question naturally arises: Given the additional degrees of freedom these algorithms enjoy through data selection, can they at all reach a stable point; that is, can they converge? Convergence is well-understood for classical empirical risk minimization (ERM), where it refers to the sequence of model parameters. In reciprocal learning, however, we need to extend the notion of convergence to the sequence of both parameters _and_ data. It is self-evident that convergence is a desirable property of any learning algorithm. Only if an algorithm converges to a unique solution, we can identify a unique model and use it for deployment or assessment on test data. Practically speaking, convergence of a training procedure means that subsequent iterations will no longer change the model, giving rise to stopping criteria. Generally speaking, convergence is a prerequisite for any further theoretical or empirical assessment of such methods. In the literature on reciprocal learning algorithms like active learning, self-training, or multi-armed bandits, there is no consensus on when to stop them. And while a myriad of stopping criteria exist [109; 121; 48; 110; 28; 22; 11; 79; 96], only some come with generalization error bounds [41; 40; 88], but none - to the best of our knowledge - comes with rigorous guarantees of convergence. We address this research gap by proving convergence of all these methods under a set of sufficient conditions.

Our strategy will be to take a decision-theoretic perspective on both the parameter and the data selection problem. While the former is well studied, in particular its ubiquitous solution strategy ERM, little attention is commonly paid to a formal treatment of the other side of the coin: data selection. We particularly study the hidden interaction between parameter and data selection. We identify a _sample adaption_ function \(f\) which maps from the sample and the empirical risk minimizer in iteration \(t\) to the sample in \(t+1\). Bounding the change of the sample in \(t+1\) by the change in the sample and the change in the model in \(t\) (i.e., \(f\) being Lipschitz-continuous with a sufficiently small constant) will turn out to guarantee convergence of reciprocal learning algorithms. In response to this key finding, we study which algorithms fulfill this restriction on \(f\). We prove that the sample adaption is sufficiently Lipschitz for reciprocal learning to converge, if (1) it is non-greedy, i.e., it adds _and_ removes data, (2) predictions are probabilistic and (3) the selection of data is either randomized or regularized. Conclusively, we transfer these results to common practices in active learning, self-training, and bandits, showing which algorithms converge and which do not.

## 2 Reciprocal learning

Machine learning deals with two pivotal objects: data and parameters. Typically, parameters are learned from data through ERM. In various branches of machine learning, however, the relationship between data and parameters is in fact _reciprocal_, as argued above. In what follows, we show that this reciprocity corresponds to two interdependent decision problems and explicitly study how learned parameters affect the subsequent training data. We emphasize that our analysis focuses on reciprocity

Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from “The Elements of Statistical Learning” [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in _response to_ the model fit. Grey ellipse indicates restriction of sample space in \(t=2\) through realization in \(t=1\). Sample in \(t\) thus depends on model in \(t-1\)_and_ sample in \(t-1\).

between parameters and _training_ data only. The population and test data thereof are assumed to be fixed, i.e., our inference goal is static. Specifically, we call a machine learning algorithm _reciprocal_ if it performs iterative ERM on training data that depends on the previous ERM, see definition 1. This dependence can be induced by any kind of data collection, removal, or generation that is affected by the model fit. In particular, it can be stochastic (think of Thompson-sampling in multi-armed bandits) as well as deterministic in nature (think of maximizing a confidence score in self-training).

**Definition 1** (Reciprocal Learning, informal).: _An algorithm that iteratively outputs \(_{t}=*{arg\,min}_{}_{(Y,X)_{ t}}(Y,X,)\) shall be called reciprocal learning algorithm if_

\[_{t}=f(_{t-1},_{t-1},n_{t-1}),\]

_where \(_{t}\) are empirical distributions - from a space of probability distributions \(\) - of \(Y,X\) of size \(n_{t}\) in iteration \(t\{1,,T\}\). Let \((Y,X,)=(Y,p(X,))\) denote a loss function with \(p(X,)\) a prediction function that maps to the image of \(Y\). Further denote by \(Y,X\) random variables describing the training data, and \(_{t}\) a parameter vector of the model in \(t\)._

In principle, the above definition needs no restriction on the nestedness between data in \(t\) and \(t-1\). In practice, however, most algorithms iteratively either only add training data or both add and remove instances, see extensive list of examples in appendix A. That is, data in \(t\) is either a superset of data in \(t-1\) or a distinct set. We will address these two cases in the remainder of the paper, referring to the former as greedy (only adding data) and to the latter as non-greedy (adding and removing data). For classification problems, i.e., discrete image of \(Y\), we typically have \(p(X,)=(g(X,))\) with \(:\) a sigmoid function and \(g:\). For regression problems, we simply have \(p:\). The notation \(_{t}=f(_{t-1},_{t-1},n_{t-1})\) shall be understood as a mere indication of the distribution's dependence on ERM in the previous iteration. We will be more specific soon.

### An illustrating running example: self-training

In appendix A, we demonstrate at length that several well-established machine learning procedures turn out to be special cases of reciprocal learning as specified in definition 1 and more formally in definitions 6 and 7 below. Here, we seek to illustrate the principles of reciprocal learning by the simple running example of self-training in a semi-supervised learning (SSL) setup. The aim of SSL is to learn a predictive classification function \((x,)\) parameterized by \(\) utilizing both labeled and unlabeled data. Self-training is a popular algorithm class within SSL. Algorithms of that class start by fitting a model on labeled data by ERM and then exploit this model to predict labels for the unlabeled data. In a second step, some instances of the unlabeled data are selected (according to a "confidence score", a measure of predictive uncertainty, see [2; 49; 80; 83; 53; 84; 18] for examples) to be added to the training data together with the predicted labels. In other words, self-training algorithms label unlabeled data themselves and ultimately learn from these "pseudo-labels" by iteratively adding pseudo-labeled variants of unlabeled data to the labeled training data. The pseudo-labels are predicted by the current model, and thus depend on the parameters learned by the model from the labeled data in the first place. This latter dependence constitutes the sample adaption function in definition 1. The sample of labeled and pseudo-labeled data in \(t\) depends on the sample and the model (through its predicted pseudo-labels) in \(t-1\). For a more comprehensive and formal introduction of self-training, we refer the curious reader to appendix A.1.

### A decision-theoretic perspective

On a high level, reciprocal learning can be viewed as sequential decision-making. First, a parameter \(_{t}\) is fitted through ERM, which corresponds to solving a decision problem characterized by the triple \((,,)\) with \(\) the unknown set of states of nature, the action space \(=\) of potential parameter fits (estimates), and a loss function \(:\), analogous to classical statistical decision theory . Secondly, features \(x_{t}\) are chosen and data points \((x_{t},y_{t})\) are added to or removed from the training data inducing a new empirical distribution \(_{t+1}\), where \(y_{t}\) is predicted (self-training), queried (active learning) or observed (bandits). These features \(x_{t}\) are found by solving another decision problem \((,,_{_{t}})\), where - crucially - the loss function \(_{_{t}}\) depends on the previous decision problem's solution \(_{t}\). This time, the action space corresponds to the feature space \(=\).

**Illustration 1**.: _Think of reciprocal learning as a sequential decision-making problem:_

\[t=1\]

_\[_{1}\] solves decision problem \[(,,)\] \[a_{1}\] solves decision problem \[(,,_{_{1}})\] \[a_{2}\] solves decision problem \[(,,_{_{2}})\]_Loosely speaking, the data is judged in light of the parameters here. Excitingly, such an approach is symmetrical to any type of classical machine learning, where parameters are judged in light of the data. This twist in perspective will later pave the way for another type of regularization - that of data, not of parameters. As the decision problem \((,,)\) is well-known through its solution strategy ERM, see definition 1, we want to be more specific about \((,,_{_{t}})\) with \(=\). In particular, we need a solution strategy for all of the loss functions in the family \(_{}:;(x,) _{_{t}}(,x)\) in iteration \(t\). Definition 2 does the job. The family \(_{}\) describes all potential loss functions in the data selection problem arising from respective solutions of the parameter selection problem.1 Redefining this family of functions as a single one \(}:\) makes it clear that we can retrieve the decision criterion \(c:\) from it, which is a generalization of classical decision criteria \(c:\) retrieved from classical losses \(:\), see  for instance.

**Definition 2** (Data Selection).: _Let \(c:\) be a criterion for the decision problem \((,,_{_{t}})\) with bounded \(=\) of selecting features to be added to the sample in iteration \(t\). Define \(:;\ (x,_{t})))}{_{x^{}}(c(x^{},_{t}))d(x)}\) as standardized version thereof with \(\) the Lebesgue measure on \(\). For a model \(_{t}\) in iteration \(t\), it assigns to each feature vector \(x\) a value between \(0\) and \(1\) that can be used as drawing probabilities. Drawing \(x\) according to \((x,_{t})\) shall be called stochastic data selection \(x_{s}(_{t})\).2 The function \(x_{d}:;_{t}*{arg\,max}_{x }(x,_{t})\) shall be called deterministic data selection function._

The data selection function can be understood as the workhorse of reciprocal learning: It describes the non-trivial part of the sample adaption function \(f\), see definition 1. For any model \(_{t}\) in \(t\), a data selection function chooses a feature vector to be added to the training data in \(t+1\), based on a criterion \(c\). This happens either stochastically through \(x_{s}\) by drawing from \(\) according to \(\) or deterministically through \(x_{d}\). Examples for \(c\) comprise confidence measures in self-training, acquisition functions in active learning, or policies in multi-armed bandits. For an example of stochastic data selection, consider \(c\) to be the classical Bayes criterion  in \((,,_{_{t}})\). In this case, drawing from \(\) as prescribed by \(x_{s}\) corresponds to well-known Thompson sampling . As already hinted at, we will need some regularization (definition 3) of the data selection. Intuitively, the regularization term smoothes out the criterion \(c(,)\). In other words, the higher the constant \(}\), the less the selection of data is affected by small changes of \(\) for given \(()\). This is completely symmetrical to classical statistical regularization in ERM, where the regularization terms smoothes out the effect of the data on parameter selection, see also figure 2.

**Definition 3** (Data Regularization).: _Consider \(c:\) a criterion for the decision problem \((,,_{_{t}})\) with \(\) as in definition 2. Define the following regularized (deterministic) data selection function:_

\[x_{d,}:;\ *{ arg\,max}_{x}\{c(,)+} ()\},\]

_where \(()\) is a \(\)-strongly convex regularizer. In complete analogy to definition 2, we can define a stochastic regularized data selection function as \(x_{s,}()\) by drawing \(x\) according to a normalized version of \(c(,)+}()\)._

Figure 2: Data regularization is symmetrical to classical regularization, see illustration in ”The Elements of Statistical Learning” [31, Figure 7.2].

We will denote a generic data selection function as \(}\{x_{d},x_{s},x_{d,},x_{s,}\}\) in what follows. For the non-greedy variant of reciprocal learning, where data is both added and removed, we need to define data removal as well. A straightforward strategy is to randomly remove data points with uniform removal probabilities. The following function \(}^{-}\) describes the effect of this procedure in expectation.

**Definition 4** (Data Removal Function).: _Given an empirical distribution \((Y,X)\) of a sample, the function \(}^{-}:;\ (Y,X)  Xd(X)\) shall be called data removal function._

### Formal definition and desirable properties

In order to study reciprocal learning in a meaningful way, we need to be a bit more specific about how \(_{t}\) depends on empirical risk minimization in \(t-1\), and specifically on \(_{t-1}\). The following definition 5 of the _sample adaption function_ allows for this. It will be the pivotal object in this work. The function describes in a general way and for any \(t\) how empirical distributions of training data in \(t\) are affected by the model, the empirical distribution of training data, and its size in \(t-1\), respectively.

**Definition 5** (Sample Adaption).: _Denote by \(\) a parameter space, by \(\) a space of probability distributions of \(X\) and \(Y\), and \(\) the natural numbers. The function \(f:\) shall be called the greedy and the function \(f_{n}:\) the non-greedy sample adaption function._

A greedy sample adaption function outputs a distribution \(^{}(Y,X)\) in the iteration after \(\) solved ERM on a sample of size \(n\) described by \((Y,X)\), which led to an enhancement of the training data that changed \((Y,X)\) to \(^{}(Y,X)\). It will come in different flavors for different types of algorithms, see examples in section A. Generally, we have \(f(,(Y,X),n)=^{}(Y,X)\), with \(^{}(Y,X)\) being induced by

\[^{}(Y=1,X=x)=}()) (}(),)\,+\,n\,(Y=1,\, X=x)}{n+1}\ _{}}\,dy\ _{}}\,dx,\] (1)

in case of \(=\{0,1\}\), where \(:\{0,1\}\) is any function that assigns a label \(y\), potentially based on the model \(\), to selected \(x\), and \(}\) any function that selects features \(x\) given a model \(\), for example, \(x_{d}\), \(x_{s}\), \(x_{d,}\), or \(x_{s,}\) as defined above. They give rise to \(_{}}\) and \(_{}}\), respectively. We can be so specific about the sample adaption function due to \(P(Y=1,\,X=x)=P(X=x)-P(Y=0,\,X=x)\) in binary classification problems. We can analogously define the non-greedy variant \(f_{n}(,(Y,X))\), where one instance is removed by \(}^{-}\) and one instance is added by \(}\) per iteration. To this end, define \(^{}(Y=1,\,X=x)\) by replacing the integrand in equation (1) by

\[}())(}( ),)\,+\,n_{0}\,(Y=1,\,X=x)-1(x=}^{-}( (Y,X)))(}^{-}((Y,X)), )}{n_{0}},\] (2)

where \(n_{0}\) is the size of the initial training data set. Notably, we observe that both sample adaption functions entail a reflexive effect of the model on subsequent data akin to performative prediction , see section 5 for a discussion.

We can now define reciprocal learning (definition 1) more formally given the sample adaption function as follows, both in greedy and non-greedy flavors.

**Definition 6** (Greedy Reciprocal Learning).: _With \(,\,,\,X,\,Y\), and \(\) as above, we define_

\[R:& ;\\ (,(Y,X),n)&\,(^{},^{}(Y,X),n^{})\]

_as reciprocal learning, where \(^{}=*{arg\,min}_{}_{(Y,X) ^{}(Y,X)}(Y,X,)\) and \(^{}(Y,X)=f(,(Y,X),n)\) as well as \(n^{}=n+1\), with \(f\) a sample adaption function, see definition 5. Note the equivalence to the informal recursive definition 1 with \(f(_{t-1},(Y,X)_{t-1},n_{t-1})=(Y,X)_{t}\)._

**Definition 7** (Non-Greedy Reciprocal Learning).: _With \(,\,,\,X,\,Y\) and \(Y\) as above, we define_

\[R_{n}:&;\\ (,(Y,X))&\,(^{},^{}(Y,X)) \]

_as reciprocal learning, where \(^{}(Y,X)=f_{n}(,(Y,X))\) and \(^{}=*{arg\,min}_{}_{(Y,X) ^{}(Y,X)}\ (Y,X,)\) with \(f_{n}\) a non-greedy sample adaption function, see definition 5._We introduce two desirable properties of reciprocal learning. First, we define convergence as a state in which the model stops changing in response to newly added data. This kind of stability allows to stop the process in good faith: Hypothetical subsequent iterations would not have changed the model. Definition 8 offers a straightforward way of formalizing this, implying standard Cauchy convergence.

**Definition 8** (Convergence of Reciprocal Learning).: _Let \(g:\) be a strictly monotone decreasing function and \(R\) (\(R_{n}\)) any (non-greedy) reciprocal learning algorithm (definitions 6 and 7) outputting \(R_{t}\) (\(R_{n,t}\)) in iteration \(t\). Then \(\{R,R_{n}\}\) is said to **converge** if \(||_{k},_{j}|| g(t)\) for all \(k,j t,\) and \(_{t}g(t)=0\), where \(||||\) is a norm on the codomains of \(R\) and \(R_{n}\), respectively. In this case, define \(_{c}\{R_{c},R_{n,c}\}\) as the limit of this convergent sequence \(\)._

Contrary to classical ERM, convergence of reciprocal learning implies stability of both data _and_ parameters. Technically, it refers to all components of the functions \(R\) and \(R_{n}\), respectively, see definition 8. It guarantees that \(_{t-1}\) solves ERM on the sample induced by it in \(t\). However, this does not say much about its optimality in general. What if the algorithm had outputted a different \(_{t-1}\) in the first place? The empirical risk could have been lower on the sample in \(t\) induced by it. The following definition describes such a look-ahead optimality. It can be interpreted as the optimal data-parameter combination.

**Definition 9** (Optimal Data-Parameter Combination).: _Consider (non-greedy) reciprocal learning \(R\) (\(R_{n}\)), see definitions 6 and 7. Define \(R^{*}\) and \(R^{*}_{n}\) as optimal data-parameter combination in reciprocal learning if \(R^{*}_{n}=(^{*}_{n^{*}},^{*}_{n})=_{, }_{(Y,X) f_{n}(,)}\ (Y,X,),\) and \(R^{*}=(^{*},^{*},n^{*})=_{,,n}_{(Y,X) f(,,n)}\ (Y,X,),\) respectively._

An optimal \(^{*}\) (or \(^{*}_{n}\), analogously) not only solves ERM on the sample it induces, but is also the best ERM-solution among all possible \(\) (\(_{n}\)) that _could have led_ to optimality on the respectively induced sample. In other words, \(^{*}\) (\(^{*}_{n}\)) is found by minimizing the empirical risk with respect to _whole_\(R\) (\(R_{n}\)). That is, it is found by minimizing the empirical risk with respect to \(\) given a sample (characterized by \(\) and \(n\)) and steering this very sample through \(\) simultaneously given only the initial sample. Technically, optimality (definition 9) is a bivariate \(\)-condition on \(_{(Y,X) f_{n}(,)}\ (Y,X,)\) and \(_{(Y,X) f(,,n)}\ (Y,X,),\) respectively. In contrast, convergence (definition 8) translates to a fixed-point condition on the \(\) viewed as a function \( \) in case of \(R\) and as \(\) in case of \(R_{n}\), see section 3.

### Self-training is an instance of reciprocal learning

Let us get back to our running example of self-training. It is easy to see that self-training is a special case of reciprocal learning with the sample adaption function \(f_{SSL}:;\ ( ,(Y,X),n)^{}(Y,X)\) defined through \(^{}(Y,X)\) being induced by

\[^{}(Y=1,X=x)=\ (x_{d}( ),)\ +\ n\,(Y=1,\ X=x)}{n+1}\ _{Y|X}\ dy\ _{X}\,dx\] (3)

where \(x_{d}()\) (definition 2) selects data with highest "confidence score" , see section 2.1, according to the model \(\), and gives rise to \(_{X}\). The prediction function \(:\{0,1\}\) returns the predicted "pseudo-label" of the selected \(x_{d}()\) based on the learned model \(\) and gives rise to \(_{Y|X}\). Moreover, we still assume binary target variables, i.e., the image of \(Y\) is \(\{0,1\}\), real-valued features \(X\), and only consider cases where the sample changes through the addition of one instance per iteration.3 The averaging with respect to \(_{X}\) and \(_{Y|X}\) accounts for the fact that we allow stochastic inclusion of \(X\) in the sample through randomized actions and for probabilistic predictions of \(Y X\), respectively. For now, however, it suffices to think of the special case of degenerate distributions \(_{X}\) and \(_{Y|X}\) putting point mass \(1\) on data with hard labels in the sample and \(0\) elsewhere.4 Through averaging with respect to \(_{Y|X}\) we can describe the joint distribution of hard labels \((y_{1},x_{1}),,(y_{n},x_{n})\) and predicted soft labels \(=(Y=1 x,)\) of \((_{n+1},x_{n+1}),,(_{n+t},x_{n+t})\). Summing up, both deterministic data selection and non-probabilistic (i.e., hard labels) predictions are well-defined special cases of the above with \(_{Y|X}\) and \(_{X}\) collapsing to trivial Dirac measures, respectively.

Convergence of reciprocal learning: Lipschitz is all you need

After having generalized several widely adopted machine learning algorithms to reciprocal learning, we will study their convergence (definition 8) and optimality (definition 9). Our general aim is to identify sufficient conditions for any reciprocal learning algorithm to converge and then show that such a convergent solution is sufficiently close to the optimal one. This will not only allow to assess convergence and optimality of examples 1 through 3 (self-training, active learning, multi-armed bandits, see appendix A) but of any other reciprocal learning algorithm. Besides further existing examples not detailed in this paper like superset learning  or Bayesian optimization , we are especially aiming at potential future - yet to be proposed - algorithms. On this background, our conditions for convergence and optimality can be understood as design principles. Before turning to these concrete conditions on reciprocal learning algorithms, we need some general assumptions on the loss function for the remainder of the paper. Assumptions 1 and 2 can be considered quite mild and are fulfilled by a broad class of loss functions, see [95, Chapter 12] or . For instance, the L2-regularized (ridge) logistic loss has Lipschitz-continuous gradients both with respect to features and parameters. For a discussion of assumption 3, we refer to appendix E.2.

**Assumption 1** (Continuous Differentiability in Features).: _A loss function \((Y,X,)\) is said to be continuously differentiable with respect to features if the gradient \(_{X}(Y,X,)\) exists and is \(\)-Lipschitz continuous in \(\), \(x\), and \(y\) with respect to the L2-norm on domain and codomain._

**Assumption 2** (Continuous Differentiability in Parameters).: _A loss function \((Y,X,)\) is continuously differentiable with respect to parameters if the gradient \(_{}(Y,X,)\) exists and is \(\)-Lipschitz continuous in \(\), \(x\), and \(y\) with respect to the L2-norm on domain and codomain._

**Assumption 3** (Strong Convexity).: _Loss \((Y,X,)\) is said to be \(\)-strongly convex if \((y,x,)(y,x,^{})+_{} (y,x,^{})^{}(-^{})+ \|-^{}\|_{2}^{2},\) for all \(,^{},y,x\). Observe convexity for \(=0\)._

Let us now turn to specific and more constructive conditions on reciprocal learning's workhorse, the data selection problem \((,,_{})\). At the heart of these conditions lies a common goal: We want to establish some continuity in how the data changes from \(t-1\) to \(t\) in response to \(_{t-1}\) and \(_{t-1}\). It is self-evident that without any such continuity, convergence seems out of reach. As it will turn out, bounding the change of the data in \(t\) by the change of what happens in \(t-1\) will be sufficient for convergence, see figure 3. We thus need the sample adaption function (definition 5) to be Lipschitz-continuous. Theorem 1 will deliver this for subsets of conditions 1 through 5 in case of binary classification problems. The reason for the latter restriction is that we need an explicit definition of \(f\) to constructively prove its Lipschitz-continuity.

**Condition 1** (Data Regularization).: _Data selection is regularized as per definition 3._

**Condition 2** (Soft Labels Prediction).: _The prediction function \(:\{0,1\}\) on bounded \(\) gives rise to a non-degenerate distribution of \(Y X\) for any \(\) such that we can consider soft label predictions \(p:\) with \(p(x,)=(g(X,))\) with \(:\) a sigmoid function. Further, assume that the loss is jointly smooth in these predictions. That is, \(_{p}(y,p(x,))\) exists and is Lipschitz-continuous in \(x\) and \(\)._

**Condition 3** (Stochastic Data Selection).: _Data is selected stochastically according to \(x_{s}\) by drawing from a normalized criterion \())}{_{s^{}}(c(x^{},_{t}))d (x)}\), see definition 2._

**Condition 4** (Continuous Selection Criterion).: _It holds for the decision criterion \(c:\) in the decision problem \((,,_{_{t}})\) of selecting features to be added to the sample that \(_{x}c(x,)\) and \(_{}c(x,)\) are bounded from above._

**Condition 5** (Linear Selection Criterion).: _The decision criterion \(c:\) in \((,,_{_{t}})\) is linear in \(x\) and Lipschitz-continuous in \(\) with a Lipschitz constant \(L_{c}\) that is independent from \(x\)._

We can interpret \(p\) as \(P_{}(Y X=x)\) in condition 2, see also definition 1. In other words, soft labels in the form of probability distributions are available. Adding observations with soft labels to the data can be implemented either through randomization, i.e., by adding \(x\) with label 1 with probability \(p\) and vice versa, or through weighted retraining. Note that condition 4 implies condition 5 through characterization of Lipschitz-continuity by bounded gradients. We need two implications of these conditions to establish Lipschitz-continuity of the sample adaption in reciprocal learning. First, it can be shown that regularized data selection (condition 1) is Lipschitz-continuous in the model, see lemma 1. Second, the soft label prediction function (condition 2) is Lipschitz in both data and model, if the data selection, in turn, is Lipschitz-continuous in the model, see lemma 2.

**Lemma 1** (Regularized Data Selection is Lipschitz).: _Regularized Data Selection_

\[x_{d,}:;\ *{argmax}_{ }\{c(,)+} ()\}\]

_with \(\)-strongly convex regularizer, see definition 3 and condition 1, is \( L_{c}}{}\)-Lipschitz continuous, if \(c\) is linear in \(x\) (condition 5) and Lipschitz-continuous in \(\) with a Lipschitz constant \(L_{c}\) that is independent of \(x\)._

**Lemma 2** (Soft Label Prediction is Lipschitz).: _The soft label prediction function (condition 2)_

\[p:;p(}(),) =(}(),)\ dy\ _{}}d}\]

_is Lipschitz-continuous in both \(x\) and \(\) and \((x,)\) if \(}()_{}}d }\) is Lipschitz-continuous._

Proofs of all results in this paper can be found in appendix F. With the help of lemma 1 and 2, we are now able to state two key results. They tell us under which conditions the sample adaption functions in both greedy and non-greedy reciprocal learning are Lipschitz-continuous, which will turn out to be sufficient for convergence.

**Theorem 1** (Regularization Makes Sample Adaption Lipschitz-Continuous).: _If predictions are soft (condition 2) and the data selection is **regularized** (conditions 1 and 5), both greedy and non-greedy sample adaption functions \(f\) and \(f_{n}\) (see definition 5) in reciprocal learning with \(=\{0,1\}\) are Lipschitz-continuous with respect to the L2-norm on \(\) and \(\), and the Wasserstein-1-distance on \(\)._

**Theorem 2** (Randomization Makes Sample Adaption Lipschitz-Continuous).: _If predictions are soft (condition 2) and the data selection is **randomized** (conditions 3 and 4), greedy and non-greedy sample adaption functions are Lipschitz-continuous in the sense of theorem 1._

The general idea for both proofs is to show Lipschitz-continuity component-wise and then infer that \(f\) and \(f_{n}\) are Lipschitz with the supremum of all component-wise Lipschitz-constants. We can now leverage these theorems to state our main result. It tells us (via theorems 1 and 2 and conditions 1 - 5) which types of reciprocal learning algorithms converge. Recall that convergence (definition 8) in reciprocal learning implies a convergent model _and_ a convergent data set.

**Theorem 3** (Convergence of Non-Greedy Reciprocal Learning).: _If the non-greedy sample adaption \(f_{n}\) is Lipschitz-continuous with \(L(1+)^{-1}\), the iterates \(R_{n,t}=(_{t},_{t})\) of non-greedy reciprocal learning \(R_{n}\) (definition 7) converge to \(R_{n,c}=(_{c},_{c})\) at a linear rate._

The proof idea is as follows. We relate the Lipschitz-continuity of \(f_{n}\) to the Lipschitz-continuity of \(R_{n}\) via the dual characterization of the Wasserstein metric . If \(f_{n}\) is Lipschitz with \(L(1+)^{-1}\), we further show that \(R_{n}\) is a bivariate contraction. The Banach fixed-point theorem  then directly delivers uniqueness and existence of \((_{c},_{c})\) as convergent fixed point, which means that it holds \(_{c}=*{arg\,min}_{}_{(Y,X) f(_ {c},_{c})}\ (Y,X,)\). A complete proof can be found in appendix F.5. Building on earlier work on performatively optimal predictions , we can further relate this convergent training solution to the global solution of reciprocal learning, i.e., the optimal data-parameter fit, see definition 9. The following theorem 4 states that our convergent solution is close to the optimal one. It tells us that we did not enforce a trivial or even degenerate form of convergence (e.g., constant \(_{t}\)) by regularization and randomization. Theorem 4 only refers to the convergent parameter solution, not to the data. Note that the parameter solution is the crucial part of reciprocal learning for later deployment and assessment on test data.

Figure 3: Reciprocal learning converges if the change in sample (purple) is bounded by the change in model (yellow) and previous sample.

**Theorem 4** (Optimality of Convergent Solution).: _If non-greedy reciprocal learning converges in the sense of theorem 3, it holds \(||_{c}-^{*}||_{2}\) for \(_{c}\) and \(^{*}\) from the convergent data-parameter tuple \(R_{n,c}=(_{c},_{c})\) and the optimal one \(R_{n}^{*}=(^{*},^{*})\) if the loss is \(L_{}\)-Lipschitz in \(X\) and \(Y\)._

While theorem 1 and 2 guarantee that _both_ greedy \(f\) and non-greedy \(f_{n}\) are Lipschitz, theorem 3 and thus also theorem 4 only hold for non-greedy reciprocal learning. The question immediately comes to mind whether we can say anything about the asymptotic behavior of the greedy variant, too. The following theorem 5 gives an affirmative answer. Intuitively, there is no fixed point in \(\) if data is constantly being added and not removed such that \(n\).

**Theorem 5**.: _Greedy Reciprocal Learning does not converge in the sense of definition 8._

We conclude this section with another negative result. It states that theorem 3 is tight in theorem 1 and 2. Summing things up, Lipschitz-continuity is all you need for non-greedy reciprocal learning to converge.

**Theorem 6**.: _If the sample adaption \(f_{n}\) is not Lipschitz, non-greedy reciprocal learning can diverge._

## 4 Which reciprocal learning algorithms converge?

We briefly relate the above results to specific algorithms in active learning, bandits, and our running example of self-training. Assume a binary target variable and \(L(1+)^{-1}\) with \(\) and \(\) in the sense of assumption 1-3 throughout. First observe that any greedy (definition 6) algorithm that only adds data without removal does not converge in the sense of defintion 8 with respect to \(,\), and \(n\), see theorem 5. This provides a strong case for non-greedy self-training algorithms, often referred to as amending strategies  or self-training with editing  and noise filters , that add and remove data, as opposed to greedy ones like incremental or batch-wise self-training , see example 1, that only add pseudo-labeled data without removing any data. For detailed explanation and comparison of the two, please refer to Appendix A.1.1.

**Corollary 1** (Self-Training).: _Amending self-training algorithms converge in the sense of definition 8, if predicted pseudo-labels are soft (condition 2) and data selection is regularized (condition 1) or randomized (condition 3)._

Furthermore, we shed some light on the debate  in the literature on multi-armed bandits about whether to use deterministic strategies like upper confidence bound  or stochastic ones like epsilon-greedy  search or Thompson sampling , see example 3. Note, however, that this insight relates to _in-sample_ convergence only, see definition 8.

**Corollary 2** (Bandits).: _Non-greedy multi-armed bandits with Thompson sampling and epsilon-greedy strategies converge in the sense of definition 8 under additional condition 2, while bandits with upper confidence bound (UCB) are not guaranteed to converge._

What is more, condition 2 allows us to distinguish between active learning from weak and strong oracles, see  for literature surveys. The former posits the availability of probabilistic or noisy oracle feedback through soft labels ; the latter assumes the oracle to have access to an undeniable ground truth via hard labels .

**Corollary 3** (Active Learning).: _Active Learning from a strong oracle (i.e., providing hard labels) is not guaranteed to converge, while active learning from a weak oracle (soft labels) converges in the sense of definition 8 under additional condition 1 **or 3**._

## 5 Related work

Convergence of active learning, self-training, and other special cases of reciprocal learning has been touched upon in the context of stopping criteria . We refer to section 1 for a discussion and relate reciprocal learning to other fields in what follows.

**Continual Learning:** While reciprocity through, e.g., gradient-based data selection is a known phenomenon in continual learning , the inference goal is not static as in reciprocal learning. Continual learning rather aims at excelling at new tasks (that is, new populations), while reciprocal learning can simply be seen as a greedy approximation of extended ERM, see section 2.

**Online Learning:** In online learning and online convex optimization, the task is to predict \(y\) by \(\) from iteratively receiving \(x\). After each prediction, the true \(y\) and corresponding loss \((y,)\) is observed, see  for an introduction and appendix B.2 for an illustration. Reciprocal learning can thus be considered a special online learning framework. Typically, online learning assumes incoming data to be randomly drawn or even selected by an adversarial player, while being selected by the algorithm itself in reciprocal learning. The majority of the online learning literature is concerned with how to update a model in light of new data, while we focus on how data is selected based on the current model fit. Loosely speaking, online learning deals with only one side of the coin explicitly, while we take a _reciprocal_ point of view: We study both how to learn parameters from data and how to select data in light of fitted parameters.

**Coresets:** The aim of coreset construction is to find subsamples that lead to parameter fits close to the originally learned parameters . It can be seen as a post hoc approach, while reciprocal learning algorithms directly learn a "parameter-efficient" sample on the go.

**Performance Prediction:** The sample adaption functions in reciprocal learning are reminiscent of performative prediction, where today's predictions change tomorrow's population , and, more generally, of the "reflexivity problem" in social sciences . We identify analogous reflexive effects on the sample level in reciprocal learning via the sample adaption function \(f\) (or \(f_{n}\)), see section 2. Contrary to performative prediction, however, \(f\) (\(f_{n}\)) describes an _in-sample_ (performative prediction: population) reflexive effect of _both_ data and parameters (performative prediction: only parameters). Moreover, reciprocal learning describes specific and implementable algorithms, which allows for an explicit study of these reflexive effects. While we rely on similar techniques as in , namely Lipschitz-continuity and Wasserstein-distance, our work is thus conceptually different. For an illustration of these differences, see appendix B.1.

**Safe Active Learning:** Safe active learning explores the feature space by optimizing an acquisition criterion under a safety constraint . While this can be viewed as regularization akin to the one we propose in definition 3, both aim and structure are different: We want to enforce Lipschitz-continuity explicitly via a penalty term in data selection; safe active learning optimizes a selection criterion without penalty terms under constraints that are motivated by domain knowledge.

## 6 Discussion

**Summary:** We have embedded a wide range of established machine learning algorithms into a unifying framework, called _reciprocal learning_. This gave rise to a rigorous analysis of (1) _under which conditions_ and (2) _how fast_ these algorithms converge to an approximately optimal model. We further applied these results to common practices in self-training, active learning, and bandits.

**Limitations:** While our results guarantee the convergence of reciprocal learning algorithms, the opposite does generally not hold. That is, if our conditions are violated, we cannot rule out the possibility of (potentially weaker notions) of convergence. Furthermore, our analysis requires assumptions on the loss functions, as detailed in section 3 and appendix E. In particular, it needs to be \(\)-strongly convex and have \(\)-Lipschitz gradients, such that \(L(1+)^{-1}\) with \(L\) the Lipschitz-constant of the sample adaption. This limits our results' applicability. From another perspective, however, this is a feature rather than a bug, since the described restrictions can serve as design principles for self-training, active learning, or bandit algorithms that shall converge, see below.

**Future Work:** This article identifies sufficient conditions for convergence of reciprocal learning. These restrictions pave the way for a theory-informed design of novel algorithms. In particular, our results emphasize the importance of regularization of _both_ parameters and data for convergence. While the former is needed to control \(\) and \(\), see appendix E.2 for the example of Tikhonov-regularization, the latter guarantees Lipschitz-continuity of the sample adaption through theorem 1. Parameter regularization is well-studied and has been heavily applied. We conjecture that the concept of data regularization might bear similar practical potential. Another line of future research would be to address the question whether reciprocal learning algorithms are stable with respect to slight changes in the initial training data. In this sense,  might serve as a bridge to future research.