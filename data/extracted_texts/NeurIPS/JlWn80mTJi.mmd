# The Implicit Bias of Gradient Descent on Separable Multiclass Data

Hrithik Ravi\({}^{1}\)  Clayton Scott\({}^{1}\)  Daniel Soudry\({}^{2}\)  Yutong Wang\({}^{3}\)

\({}^{1}\)University of Michigan \({}^{2}\)Technion - Israel Institute of Technology

\({}^{3}\)Illinois Institute of Technology

{hrithikr, clayscot}@umich.edu

daniel.soudry@gmail.com

ywang562@iit.edu

###### Abstract

Implicit bias describes the phenomenon where optimization-based training algorithms, without explicit regularization, show a preference for simple estimators even when more complex estimators have equal objective values. Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an _exponential tail property_. However, there is a noticeable gap in analysis for multiclass classification, with only a handful of results which themselves are restricted to the cross-entropy loss. In this work, we employ the framework of Permutation Equivariant and Relative Margin-based (PERM) losses (Wang and Scott, 2024) to introduce a multiclass extension of the exponential tail property. This class of losses includes not only cross-entropy but also other losses. Using this framework, we extend the implicit bias result of Soudry et al. (2018) to multiclass classification. Furthermore, our proof techniques closely mirror those of the binary case, thus illustrating the power of the PERM framework for bridging the binary-multiclass gap.

## 1 Introduction

Overparameterized models such as neural networks have shown state-of-the-art performance in many applications, despite having the potential to overfit. Zhang et al. (2021) demonstrate that this potential is indeed realizable by training real-world models to fit random noise. In recent years, there have been several research efforts that aim to understand the impressive performance of overparametrized models despite this ability to overfit. Both the model architecture and the training algorithms for selecting the weights have been investigated in this regard.

Work on _implicit bias_(Soudry et al., 2018; Ji et al., 2020; Vardi, 2022) has focused on the latter factor. Implicit bias is the hypothesis that gradient-based methods have a built-in preference for models with low-complexity. This hypothesis is perhaps best understood in the setting of (unregularized) empirical risk minimization for learning a linear model under the assumption of linearly separable data. Soudry et al. (2018) showed that in binary classification, implicit bias holds when the loss has the exponential tail property (Soudry et al., 2018, Theorem 3). The same work also demonstrated implicit bias in the multiclass setting for the cross-entropy loss, but implicit bias for a more broadly defined class of losses in the multiclass case is left open. In this work, we extend the notion of the exponential tail property to multiclass losses and prove that the property is sufficient for implicit bias to occur in the multiclass setting. Toward this end, we employ the framework of permutation equivariant and relative margin-based (PERM) losses (Wang and Scott, 2024).

### Contributions

Multiclass extension of the exponential tail property (Definition 2.2)It is unclear how the exponential tail property for binary margin losses should be extended to the multiclass setting. By using the PERM framework, we provide a multiclass extension that generalizes the exponential tail property to multiclass (Definition 2.2 in Section 2.3). We further verify that this property holds for some common losses.

Sufficiency of the exponential tail property for implicit bias (Theorem 3.4)We prove that the proposed multiclass exponential tail property is sufficient for implicit bias. More precisely, we show in Theorem 3.4 that for almost all linearly separable multiclass datasets, given a convex, (\(\)-smooth, strictly decreasing) PERM loss satisfying the exponential tail property in Definition 2.2, gradient descent exhibits directional convergence to the hard-margin multiclass SVM.

### Related Work

Soudry et al. (2018) show that gradient descent, applied to _unregularized_ empirical risk minimization, converges to the hard-margin SVM solution at a slow logarithmic rate, provided the loss satisfies the exponential tail property (defined below). Nacson et al. (2019) improve the convergence rate using a specific step-size schedule. Ji and Telgarsky (2019) extend implicit bias to the setting of _quasi-complete separation_(Candes and Sur, 2020), where the two classes are linearly separated but with a margin of zero. Many works have also considered gradient-based methods beyond gradient descent. For example, Gunasekar et al. (2018) examine the implicit bias effects of mirror descent (Beck and Teboulle, 2003), steepest descent (Boyd and Vandenberghe, 2004), and _adaptive_ gradient descent (Duchi et al., 2011; Kingma and Ba, 2015). Cotter et al. (2012); Clarkson et al. (2012); Ji et al. (2021) study first order methods that are designed specifically to approach the hard-margin SVM as quickly as possible.

Results for the multiclass setting are more scarce, and are _always_ specific to cross-entropy. Soudry et al. (2018) establish implicit bias for cross-entropy loss. Lyu and Li (2019) focus on homogeneous predictors and prove convergence of GD on cross-entropy loss to a KKT point of the margin-maximization problem. Lyu and Li (2019) proves convergence of gradient flow to a generalized max-margin classifier for multiclass classification with cross-entropy loss using homogeneous models.1 In the special case when the model are linear classifiers, the generalized max-margin classifier reduces to the classical hard-margin SVM. Lyu et al. (2021) consider two-layer neural networks and prove convergence of GD on cross-entropy loss to the max-margin solution under an additional assumption on the data, that both \(\) and its negative counterpart \(-\) must belong to the dataset. Wang et al. (2023) prove that in certain overparameterized regimes, gradient descent on squared loss leads to an equivalent solution to gradient descent on cross-entropy loss.

Beyond work establishing (rate of) convergence to the max-margin classifier, there is also a separate line of work (Shamir, 2021; Schliserman and Koren, 2022, 2023) focusing on the _generalization_ aspect of implicit bias. These works examine the binary classification setting, with the exception of Schliserman and Koren (2022) who consider cross-entropy.

### Notations

Let \(K 2\) and \(d 1\) denote the number of classes and feature space dimension, respectively. Let \([K]:=\{1,2,,K\}\). Vectors are denoted by boldface lowercase letters, e.g., \(^{K}\) whose entries are denoted by \(v_{j}\) for \(j[K]\). Likewise, matrices are denoted by boldface uppercase letters, e.g., \(^{d K}\). The columns of \(\) are denoted \(_{1},,_{K}\). By \(_{n}\) and \(_{n}\) we denote the \(n\)-dimensional vectors of all 0's and all 1's respectively. The \(n n\) identity matrix is denoted by \(_{n}\).

By \(\|\|\) we denote the Euclidean norm of vector \(\). \(\|\|_{2}\) is the spectral norm of matrix \(\). Given two vectors \(,^{k}\), we write \(\) (resp. \(\)) if \(w_{j} v_{j}\) (resp. \(w_{j}>v_{j}\)) for all \(j[k]\); similarly we write \(\) (resp. \(\)) if \(w_{j} v_{j}\) (resp. \(w_{j}<v_{j}\)) for all \(j[k]\). On the otherhand, if \(\) and \(\) are equally-sized _symmetric matrices_, then by \(\) (resp. \(\)) we mean that \(-\) (resp. \(-\)) is positive semi-definite, i.e. \(- 0\) (resp. \(- 0\)).

A bijection from \([k]\) to itself is called a permutation on \([k]\). Denote by \((k)\) the set of all permutations on \([k]\). For each \((k)\), let \(_{}\) denote the permutation matrix corresponding to \(\). In other words, if \(^{k}\) is a vector, then \([_{}]_{j}=_{(j)}\).

## 2 Multiclass Loss Functions

In multiclass classification, a classifier is typically represented in terms of a _class-score function_\(f=(f_{1},,f_{K}):^{d}^{K}\), which maps an input \(^{d}\) to a vector \(:=f()\) of class scores. For instance, \(f\) may be a feed-forward neural network and \(\) in this context is sometimes referred to as the logits. The label set is \([K]\), and a label is predicted as \(*{argmax}_{j}f_{j}()\). A \(K\)-ary multiclass loss function is a vector-valued function \(=(_{1},,_{K}):^{K} ^{K}\) where \(_{y}(f())\) is the loss incurred for outputting \(f()\) when the ground truth label is \(y\).

In binary classification, a classifier is typically represented using a function \(g:^{d}\). The label set is \(\{-1,1\}\), and labels are predicted as \(*{sign}(g())\). A _binary margin loss_ is a function of the form \(:\) where \((yg())\) is the loss incurred for outputting \(g()\) when the ground truth label is \(y\). Margin losses have been central to the development of the theory of binary classification, and the lack of a multiclass counterpart to binary margin losses may have impaired the development of corresponding theory for multiclass classification. To address this issue, Wang and Scott (2024) introduce PERM losses as a bridge between binary and multiclass classification.

### Permutation equivariant and relative margin-based (PERM) losses

Assume the label set is \([K]\). Define 2 the matrix \(:=[-_{K-1}_{K-1}]^{(K-1)  K}\). Observe that \(=(v_{K}-v_{1},v_{K}-v_{2},,v_{K}-v_{K-1})^{}\) for all \(^{K}\).

**Definition 2.1** (PERM loss (Wang and Scott, 2024)).: _Let \(K 2\) be an integer, and \(\) be a \(K\)-ary multiclass loss function. We say that \(\) is_

1. permutation equivariant _if_ \((_{})=_{}( )\) _for all_ \(^{K}\) _and_ \((K)\)_,_
2. relative margin-based _if for each_ \(y[K]\) _there exists a function_ \(_{y}:^{K-1}\) _so that_ \(_{y}()=_{y}()=_{y}(v_{K}-v_{ 1},v_{K}-v_{2},,v_{K}-v_{K-1})\)_, for all_ \(^{K}\)_. We refer to the vector-valued function_ \(:=(_{1},,_{K})\) _as the_ reduced form _of_ \(\)_._
3. PERM _if_ \(\) _is both permutation equivariant and relative margin-based. In this case, the function_ \(:=_{K}\) _is referred to as the_ template _of_ \(\)_._

Wang and Scott (2024) show that PERM losses are characterized by their template \(\). To show this, they introduce the _matrix label code_, an encoding of labels as matrices. Thus, for each \(y[K-1]\), let \(_{y}\) be the \((K-1)(K-1)\) identity matrix, but with the \(y\)-th column replaced by all \(-1\)'s. For \(y=K\), let \(_{y}\) be the identity matrix. Note that when \(K=2\), this definition reduces to \(_{y}=(-1)^{y}\), the standard encoding of labels in the binary setting. Observe that (after permutation) \(_{y}=(v_{y}-v_{1},v_{y}-v_{2},,v_ {y}-v_{K})^{}^{K-1}\), where the \(v_{y}-v_{y}=0\) entry is omitted. Please see Wang and Scott (2024, Lemma B.2) for a simple proof.

**Theorem 2.1** (Wang and Scott (2024)).: _Let \(:^{K}^{K}\) be a PERM loss with template \(\), and let \(v^{K}\) and \(y[K]\) be arbitrary. Then \(\) is a symmetric function. Moreover,_

\[_{y}()=(_{y} ).\] (1)

_Conversely, let \(:^{K-1}\) be a symmetric function. Define a multiclass loss function \(=(_{1},,_{k}):^{K} ^{K}\) according to Eqn. (1). Then \(\) is a PERM loss with template \(\)._

Theorem 2.1 shows that a PERM loss is characterized by its template \(\). The right hand side of Eqn. (1) is referred to as the _relative margin form_ of the loss, which extends binary margin losses to multiclass. As noted by Wang and Scott (2024), an advantage of the relative margin form is that it decouples the labels from the predicted scores, which facilitates analysis. Our results below support this understanding.

Many losses in the literature are PERM losses, including the cross-entropy loss whose template is \(()=(1+_{i=1}^{K-1}(-u_{i}))\), the multiclass exponential loss (Mukherjee and Schapire, 2013) whose template is \(()=_{i=1}^{K-1}(-u_{i})\), and the PairLogLoss (Wang et al., 2022) whose template is = \(()=_{i=1}^{K-1}(1+(-u_{i}))\). See Wang and Scott (2024) for other examples.

### Regularity assumptions on loss functions

Let \(\) be a PERM loss with differentiable template \(\). If

\[}()<0,i\{1,2,K-1\}^{K-1}\]

i.e., the gradient of the template is entrywise strictly negative, then we say that the PERM loss \(\) is _strictly decreasing_. In this case, we write \(\), where \(\) is the 0-vector. If the template is differentiable, then it is convex if:

\[(})(})+(})^{ }(}-}),},} ^{K-1}\]

If \(\) is twice-differentiable, this is equivalent to saying that the Hessian is positive-semidefinite:

\[^{2}() 0^{K-1}\]

Finally, the template is said to be \(\)-smooth if its gradient is \(\)-Lipschitz:

\[\|(})-(}) \|\|}-}\|,},} ^{K-1}\]

If \(\) is twice-differentiable, this is equivalent to saying that the maximum eigenvalue of its Hessian is bounded by \(\):

\[\|^{2}()\|_{2}^{K-1}\]

where \(\|\|_{2}\) is the spectral norm of matrix \(\).

### Multiclass analogue of exponential tail property

In the binary setting, the exponential tail property defined in prior work (Soudry et al. (2018); Nacson et al. (2019); Ji et al. (2020)) is assumed to hold for the negative _derivative_ of the loss. Similarly, in the multiclass setting we are interested in bounding the negative _gradient_ of the PERM loss template.

**Definition 2.2** (Multiclass exponential tail property).: _A multiclass PERM loss with template \(:^{K-1}\) has the exponential tail (ET) property if there exist \(u_{+},u_{-}\) and positive \(c>0\) such

Figure 1: An illustration of the exponential tail property for the cross entropy/multinomial logistic loss when \(K=3\). _Panel a._ Plot of \(()=(1+(-u_{1})+(-u_{2}))\), the template for the multinomial logistic loss. Note that the complement of the positive orthant in the domain \(^{2}\) is shown in gray. _Panel b. and c._ Plot of the upper bound (shown in black) and lower bounds (red) of \(-}\) (blue) respectively. These bounds are from Appendix C.1.3 where \(u_{}=0\) and \(c=1\). Note that the lower bound is valid in the positive orthant, i.e., the red surface is below the blue one there.

_that for all \(i[K-1]\) the following holds:_

\[_{j[K-1]}u_{j}>u_{+},-}() c(-u_{i}), \] \[_{j[K-1]}u_{j}>u_{-},-}() c(1-_{j[K-1]}( -u_{j}))(-u_{i}).\]

**Remark 2.2**.: _We show in Appendix C that cross-entropy (CE), multiclass exponential loss, and PairLogLoss all have this property._

## 3 Main Result

Consider a dataset \(\{(_{n},y_{n})\}_{n=1}^{N}\), with \(_{n}^{d}\) and class labels \(y_{n}[K]:=\{1,,K\}\). The class score function for class \(k\) is \(f_{k}()=_{k}^{T}\). Define \(^{d N}\) to be the matrix whose \(n\)th column is \(_{n}\). Define \(^{d K}\) to be the matrix whose \(k\)th column is \(_{k}\). The learning objective is

\[()=_{n=1}^{N}_{y_{n}}( ^{}_{n})\,.\] (2)

From Eqn. 1, if \(\) is a PERM loss, then \(_{y}()=(_{y})\), and the learning objective becomes

\[()=_{i=1}^{N}(_{y_{i}} ^{}_{i})\,.\] (3)

Up to permuting the entries, \(_{y_{i}}^{}_{i}\) is equal to the \((K-1)\)-dimensional vector of relative-margins \([(_{y_{i}}-_{1})^{}_{i}, (_{y_{i}}-_{2})^{}_{i},\,, \,(_{y_{i}}-_{K})^{}_{i}]^ {}\), where the \(0\)-valued entry \((_{y_{i}}-_{y_{i}})^{}_{i}\) is omitted. This follows from Wang and Scott (2024, Lemma B.2).

We are now ready to state our assumptions on the loss:

**Assumption 3.1**.: _The PERM loss's template \(\) is convex, \(\)-smooth, strictly decreasing and non-negative. 34_

**Assumption 3.2**.: _The PERM loss has exponential tail as defined in Definition 2.2._

To optimize Eqn. (2) we employ gradient descent with fixed learning rate \(\). Define \(:=()\) where \(\) denotes vectorization by column-stacking (See Definition B.1), and let the gradient descent iterate at time \(t\) be \((t)\). Then:

\[(t+1)=(t)-( (t)).\]

Define the "matrix-version" of the trajectory \((t)^{d K}\) such that \((t)=((t))\). Throughout this work, we frequently work with the risk as a _matrix_-input scalar-output function \(()\), and as a _vector_-input scalar-output function \(()\).

These two formulations will each be useful in different situations. For instances, adopting the matrix perspective can facilitate calculation of bounds, e.g., in Section 4.2. On the other hand, the vectorized formulation is easier for defining the Hessian of the risk \(^{2}()\). See Appendix B for detail.

We focus on linearly separable datasets:

**Assumption 3.3**.: _The dataset is linearly separable, i.e. there exists \(^{dK}\) such that \( n[N], k[K]\{y_{n}\}:_{y_{n}}^{} _{n}_{k}^{}_{n}+1\). Equivalently, there exists \(^{d K}\) such that \( n[N],\ _{y_{n}}^{}_{n} \)._Finally, let \(}\) be the multiclass hard-margin SVM solution for the linearly separable dataset:

\[}=*{argmin}_{}\,\| \|^{2}\;\;\;\, n, k y_{n}:_{y_{n}}^{ }_{n}_{k}^{}_{n}+1.\] (4)

Now we state the main result of the paper:

**Theorem 3.4**.: _For any PERM loss satisfying Assumptions 3.1 and 3.2, for all linearly separable datasets such that Assumption 4.1 holds, any sufficiently small learning rate \(0<<2^{-1}_{}^{-2}()\), and any initialization \((0)\), the iterates of gradient descent will behave as_

\[(t)=}(t)+(t)\]

_where the norm of the residual, \(\|(t)\|\), is bounded. This implies a directional convergence behavior:_

\[_{t}(t)}{\|(t )\|}=}}{\|}\|}.\]

In Appendix I, we show experimental results demonstrating implicit bias towards the hard margin SVM when using the PairLogLoss, in line with Theorem 3.4.

## 4 Proof Sketch

In this section we will overview the proof of the result. Along the way, we prove lemmas that extend to the multiclass setting results from Soudry et al. (2018). The extensions are facilitated by the PERM framework, in particular the relative margin from of the loss.

We adopt the notation of Soudry et al. (2018) where possible throughout this proof. Recalling the notation and definitions from the paper: let us define the standard basis \(_{k}^{K}\) such that \((_{k})_{i}=_{ki}\) (where \(\) is the Kronecker-delta function), and the \(d\)-dimension identity matrix \(_{d}\). Define \(_{k}^{dk d}\) as the Kronecker product between \(_{k}\) and \(_{d}\), i.e. \(_{k}=_{k}_{d}\). We can then relate the original \(k^{th}\)-class predictor \(_{k}\) to the long column-vector \(\) as follows: \(_{k}^{}=_{k}\). Next define \(}_{n,k}:=(_{y_{n}}-_{k}) _{n}\). Using this notation, the multiclass SVM becomes

\[*{argmin}_{}\,\|\|^{2}\;\;\;\; \;\;\;\; n, k y_{n}:^{}}_{n,k} 1\] (5)

For each \(k[K]\), define \(_{k}=*{arg\,min}_{n}(}_{y_{n}}- }_{k})^{}_{n}=\{n:(}_{y_{n}}- }_{k})^{}_{n}=1\}\), i.e., the \(k^{th}\) class support vectors. From the KKT optimality conditions for Eqn. (5), we have for some dual variables \(_{n,k}>0\) that

\[}=_{n=1}^{N}_{k=1}^{K}_{n,k}}_{ n,k}_{n_{k}}.\] (6)

Finally, define

\[(t)=(t)-(t)}-}\] (7)

where \(}\) is a solution to

\[ k[K], n_{k}:\,(-_{n}^ {}(}_{y_{n}}-}_{k}))= _{n,k}.\] (8)

In Soudry et al. (2018), the existence of \(}\) is proven for the binary case for almost all datasets, and assumed in the multiclass case. Here, we also state the existence of \(}\) as an additional assumption:

**Assumption 4.1**.: _Eqn. 8 has a solution, denoted \(}\)._

We pose the problem of proving Assumption 4.1 for almost all datasets as a conjecture in Appendix H, where we also show experimentally that on a large number (100 instances for each choice of \(d\{2,3,4,5,6\}\) and \(K\{3,4,5,6\}\)) of synthetically generated linearly separable datasets, Assumption 4.1 indeed holds.

Note that \((t)=(t)-}\), and \(}\) is independent of \(t\), so bounding \((t)\) is equivalent to bounding \((t)\). Following the same steps as Soudry et al. (2018, Appendix E.3):

\[\|(t+1)\|^{2}-\|(t)\|^{2}= {\|(t+1)-(t)\|^{2}}_{}+2(t+1 )-(t))^{}(t)}_{}\] (9)

The high-level approach is to bound the two terms of the above expansion for \((t)\) and then use a telescoping argument to bound \((t)\) for all \(t>0\). Below we provide the main arguments; for a complete proof of the second term's bound, please refer to Appendix F.

### Bounding the First Term

Using \((1+x) x\) for all \(x>0\), we expand the first term as follows:

\[\|(t+1)-(t)\|^ {2} ^{2}\|((t) )\|^{2}+\|}\|^{2}t^{-2}+2}^{}((t)) (1+t^{-1})\] \[^{2}\|((t ))\|^{2}+\|}\|^{2}t^{-2}\]

Obtaining the second inequality requires proving that

\[2}^{}((t) )(1+t^{-1}) 0,}^{} ((t))<0\] (10)

We will spend the rest of this subsection going over the complete proof of this inequality.

First we state the following lemma (derived in Appendix B.2) that gives us a useful expression for the gradient of the risk w.r.t. \(\):

**Lemma 4.2**.: _For any \(^{d K}\), we have that \(()=_{i=1}^{N}_{i}( _{y_{i}}^{}_{i})^{ }_{y_{i}}\)._

This expression involves weight matrix \(\). However the inequality we set out to prove (Eqn. (10)) is in terms of \(=()\). Throughout our main result proof, these two different forms - weight matrix versus vectorization of that matrix - will each be useful in different situations. Thus, to shuttle back and forth between these forms, the following well-known identity is useful:

**Lemma 4.3**.: _For equally sized matrices \(\) and \(\), we have \(()^{}()=(^{})\)._

Now we can prove our inequality of interest, i.e., Eqn. (10).

**Lemma 4.4**.: _(Multiclass generalization of Soudry et al. (2018, Lemma 1)) For any PERM loss that is \(\)-smooth, strictly decreasing, and non-negative, (Assumption 3.1) and Assumption 3.2, and for almost all linearly separable datasets (Assumption 3.3), we have \(}^{}((t))<0\)._

Proof.: Define matrix \(}\) such that \(}=(})\). Since \((t)=((t))\), Lemma 4.3 implies

\[}^{}((t))=(}^{}((t)))\] (11)

To see how the PERM framework allows for a simple generalization of binary results, we will compare our multiclass proof side-by-side with the binary proof discussed in Soudry et al. (2018, Lemma 1). In the binary case, we have \(()=_{i=1}^{N}(y_{i}^{ }_{i})()= _{i=1}^{N}^{}(y_{i}^{}_{i}) y_{i}_{i}\). Thus \(}^{}()=_{i=1}^{N} ^{}(y_{i}^{}_{i})y_{i}}^{}_{i}\). In the multiclass case, the analogous quantity is \((}^{}((t)))\) which can be computed as

\[_{i=1}^{N}(}^{}_{i} (_{y_{i}}(t)^{}_{i} )^{}_{y_{i}})=_{i=1}^{N} (_{y_{i}}(t)^{}_{i} )^{}_{y_{i}}}^{} _{i}.\]

In the multiclass proof we used the risk gradient from Lemma 4.2 as well as the cyclic property of the trace operator. Then we dropped the trace because \((_{y_{i}}(t)^{}_{i})^{}_{y_{i}}}^{} _{i}\) is a scalar (since \(()^{K-1},_{y_{i}} (t)^{}_{i}^{K-1}\)). For illustrative purpose, we place the rest of the proof, in both the binary and multiclass setting, side-by-side:

**Binary**: \(}^{}((t))\).

Focusing on just the \(i\)-th term of this sum:

\[^{}(y_{i}(t)^{}_{i})y_{i}}^{}_{i}\]

\(\) is assumed to be strictly decreasing, i.e. \(^{}(y_{i}(t)^{}_{i})<0\). The dataset is linearly separable, so \(y_{i}}^{}_{i} 1\). Thus we obtain a sum (from \(i=1\) to \(N\)) of negative terms.

Thus we see how the PERM framework allows us to essentially mirror the binary proof. In Remark 4.5, we elaborate more on the necessity of the relative margin form here.

Lemma 4.4 directly implies the auxiliary inequality we set out to prove (see Eqn. (10)). Thus we obtain:

\[\|(t+1)-(t)\|^{2}^{ 2}\|((t))\|^{2}+ \|}\|^{2}t^{-2}\] (12)

**Remark 4.5**.: _Let us see what happens to our proof if we just used the general risk form in Eqn. (2) without the PERM framework. First, we need an expression for the gradient of the risk: \(()=_{i=1}^{N}_{i} _{y_{i}}(^{}})^{}\). Proceeding similarly to the binary case, we focus on just the \(i\)-th term of \((}^{}( ))\):_

\[(}^{}_{i}_{y_{ i}}(^{}})^{})=( _{y_{i}}(^{}})^{} }^{}})=_{y_{i}}( ^{}})^{}}^{}}\]

_From here it is not clear how to proceed. The linear separability condition (Assumption 3.3) is not useful anymore- it does not make a statement about the scores in the vector \(}^{}_{i}\), but rather their relative margins (produced by the multiplication \(_{y_{i}}}^{}_{i}\))._

### Bounding the Second Term

In the previous subsection we established a bound on the first term of Eqn. (9). Here we sketch the main arguments required to bound the second term, i.e. \(((t+1)-(t))^{} (t)\). For more details please refer to Appendix F. We state our final bound below as a lemma:

**Lemma 4.6**.: _(Generalization of Soudry et al. (2018, Lemma 2018)) Define \(\) to be the minimum SVM margin across all datapoints and classes, i.e. \(=_{k}[_{_{k}}}_{n, k}^{}}]>1\). Then_

\[ C_{1},C_{2},t_{1}:\, t>t_{1}:\,((t+1) -(t))^{}(t) C_{1}t^{- }+C_{2}t^{-2}\,.\] (13)

A remark is in order on the difference of the above result to Soudry et al. (2018, Lemma 20): on a high-level, we are able to generalize the argument of Soudry et al. (2018, Lemma 20) to account for _both binary and multiclass classification_, as well as general PERM ET losses beyond just CE.

We now proceed with the proof sketch. The first step is to rewrite \(((t+1)-(t))^{} (t)\) as

\[(-((t))-}[(t+1)-(t)])^{} (t)(t))}\]

\[=}^{}(t)(t^{-1}-(1+t^{-1}) )+((-_{i=1}^{N}_{i} (_{y_{i}}(t)^{}_{i} )^{}_{y_{i}})^{}(t))\]

\[-t^{-1}}^{}(t)((t))}\] (14)

We defer the bound on the first term \(}^{}(t)(t^{-1}-(1+t^{-1}))\) of Equation (14) to the appendix, and instead focus on the second two terms. Using the cyclic property of the trace, the term in the above final line involving the trace can be further simplified as:

\[_{i=1}^{N}-(_{y_{i}}(t )^{}_{i})^{}_{y_{i}} (t)^{}_{i}\] (15)

Note that for each \(i[N]\), a summand in Equation (15) is an inner product between two \((K-1)\)-dimensional vectors, i.e., \(-(_{y_{i}}(t)^{} _{i})\) and \(_{y_{i}}(t)^{}_{i}\). To proceed, to expand this inner product out as

\[_{i=1}^{N}_{k[K]\{y_{i}\}}[-(_{y_{i}}(t)^{}_{i})]_{k}[ _{y_{i}}(t)^{}_{i}]_{k}\] (16)

**Remark 4.7**.: _Here, \([]_{k}:^{K-1}\) is defined as the coordinate projection such that \([_{y_{i}}^{}_{i}]_{k}= }_{i,k}^{}\). Note that \([]_{k}\) implicitly depends on \(i\) (the \(}_{i,y_{i}}\), 0-entry is omitted). But we abuse notation for brevity. Please see Appendix D for a more precise definition._Using Equation (6) and Equation (8) we express the last two terms in Equation (14) as

\[_{i=1}^{N}_{k[K]\{y_{i}\}}- (_{y_{i}}(t)^{}_{i}) _{k}_{y_{i}}(t)^{}_{i }_{k}-t^{-1}}^{}(t)\] \[=_{i=1}^{N}_{k[K]\{y_{i}\}}(- (_{y_{i}}(t)^{}_{i}) _{k}-t^{-1}(-}^{}}_{i,k}) _{\{i S_{k}\}})}_{i,k}^{}(t)\] (17)

Finally, to upper bound the above expression, we consider a single tuple \((i,k)\) case-by-case, depending on the sign of \(}_{i,k}^{}(t)\). This is the step where the upper and lower bounds in Definition 2.2 come in. Lemma D.2 in the appendix essentially applies Definition 2.2 to the relative margins to yield

\[-(_{y_{i}}(t)^{} _{i})_{k}(-}_{i,k}^{} (t)),\] (18) \[-(_{y_{i}}(t)^{} _{i})_{k}(1-_{r[K]\{y_{i}\}}(- }_{i,r}^{}(t)))(-}_{i,k}^{ }(t))\] (19)

for all \(k K\{y_{i}\}\). We use Definition 2.2's exponential tail bounds by proving that the relative margins \(}_{i,k}^{}(t)\) that appear in Lemma D.2 eventually become positive. This is true due to the following lemma (see Appendix E for the proof, which again mirrors the binary case):

**Lemma 4.8**.: _(Multiclass generalization of Soudry et al. (2018, Lemma 1)) Consider any linearly separable dataset, and any PERM loss with template \(\) that is convex, \(\)-smooth, strictly decreasing, and non-negative. For all \(k\{1,...,K\}\), let \(_{k}(t)\) be the gradient descent iterates at iteration \(t\) for the \(k^{th}\) class. Then \( i\{1,...,N\}, j\{1,...,K\}\{y_{i}\}:_{t }(_{y_{i}}(t)-_{j}(t))^{}_{ i}\)._

This lemma lets us use the exponential tail bounds with any finite \(u_{}\). To conclude, we apply the upper (18) and lower bounds (19) to the summation in Equation (17), and reduce the problem to that of Soudry et al. (2018, Appendix E), thereby proving Lemma 4.6. See our Appendix F for details.

### Tying It All Together

We use the logic of Soudry et al. (2018, Appendix A.2) to conclude the analysis. Define

\[C=_{t=0}^{}\|(t+1)-(t )\|^{2}_{t=0}^{}^{2}\| ((t))\|^{2}+\|} \|^{2}t^{-2}.\]

In the latter inequality we used Eqn. (12). Thus, \(C\) is bounded because from Soudry et al. (2018, Lemma 10), we know that \(_{t=0}^{}\|((t) )\|^{2}<\). Here we note that Soudry et al. (2018, Lemma 10) requires the ERM objective \(()\) to be \(^{}\)-smooth for some positive \(^{}\). It is easy to show that if the loss is \(\)-smooth, then \(()\) is \(_{}^{2}()\)-smooth. This explains the learning rate condition \(<2/(_{}^{2}())\) in our theorem. Also, a \(t^{-p}\) power series converges for any \(p>1\).

Recalling the initial expansion of \(\|(t+1)\|\) from Eqn. (9):

\[\|(t+1)\|^{2}=\|(t+1 )-(t)\|^{2}+2((t+1) -(t))^{}(t)+\| (t)\|^{2}.\] (20)

Combining the bounds in Eqn. (12) and Lemma 4.6 into Eqn. (9), we find

\[\|(t)\|^{2}-\|(t_{1}) \|^{2}=_{u=t_{1}}^{t-1}[\|(u+1)\| ^{2}-\|(u)\|^{2}] C+2_{u=t_{1}}^ {t-1}[C_{1}u^{-}+C_{2}u^{-2}].\]

Therefore, \(\|(t)\|\) is bounded, which proves our main theorem.

## 5 Limitations

Here we describe some of our work's limitations/possible future research directions. We note that these questions have been analyzed for the binary classification setting, but not for multiclass.

Non-ET lossesIn our paper we only analyze multiclass implicit bias for losses with the ET property. Another possible line of future work is to analyze the gradient descent dynamics for non-ET losses. Nacson et al. (2019) and Ji et al. (2020) prove that in the binary setting, ET and well-behaved super-polynomial tailed losses ensure convergence to the maximum-margin direction, while other losses may converge to a different direction with poor margin. Is such a characterization possible in the multiclass setting?

Other gradient-based methodsThis paper only analyzes vanilla gradient descent. Another line of work involves exploring implicit bias effects of other gradient-based methods, such as those characterized in Gunasekar et al. (2018). Nacson et al. (2022) uses similar proof techniques to prove results for SGD, which is prevalent in practice and often generalizes better than vanilla GD ((Amir et al., 2021)).

Non-asymptotic analysisOur result proves that the gradient descent predictors _asymptotically_ do not overfit. However, in the binary classification case, Shamir (2021) goes one step further and proves that for gradient-based methods, throughout the entire training process (not just asymptotically), both the empirical risk and the generalization error decrease at an essentially optimal rate (or remain optimally constant). Does the same phenomenon occur in the multiclass setting?

## 6 Conclusion

We use the permutation equivariant and relative margin-based (PERM) loss framework to provide an multiclass extension of the binary ET property. On a high level, while the binary ET bounds the negative derivative of the loss, our multiclass ET bounds each negative partial derivative of the PERM template \(\). We demonstrate our definition's validity for multinomial logistic loss, multiclass exponential loss, and PairLogLoss. We develop new techniques for analyzing multiclass gradient descent, and apply these to generalize binary implicit bias results to the multiclass setting. Our main result is that for almost all linearly separable multiclass datasets and a suitable ET PERM loss, the gradient descent iterates directionally converge towards the hard-margin multiclass SVM solution.

Our proof techniques in this paper demonstrate the power of the PERM framework to facilitate extensions of known binary results to multiclass settings and provide a unified treatment of both binary and multiclass classification. Thus it is possible that the binary results discussed in the Limitations section can also be extended using the PERM loss framework. In the future we would like to consider more complex settings that have been analyzed primarily for the binary case, such as non-separable data (Ji and Telgarsky (2019)) and two-layer neural nets (Lyu et al. (2021)).