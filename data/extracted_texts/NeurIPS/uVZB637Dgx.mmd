# What Mechanisms Does Knowledge Distillation Distill?

Cindy Wu

University of Cambridge

&Ekdeep Singh Lubana

University of Michigan

CBS, Harvard University

&Bruno Kacper Mlodozeniec

University of Cambridge

Robert Kirk

University College London

&David Krueger

University of Cambridge

Correspondence to wu.cindyx@gmail.com.

###### Abstract

Knowledge distillation is a commonly-used compression method in ML due to the popularity of increasingly large-scale models, but it is unclear if all the information a teacher model contains is distilled into the smaller student model. We aim to formalize the concept of 'knowledge' to investigate how knowledge is transferred during distillation, focusing on shared invariant outputs to counterfactual changes of dataset latent variables (we call these latents mechanisms). We define a student model to be a good stand-in model for a teacher if it shares the teacher's learned mechanisms, and find that Jacobian matching and contrastive representation learning are viable methods by which to train such models. While these methods do not result in perfect transfer of mechanisms, we show they often improve student fidelity or mitigate simplicity bias (as measured by the teacher-to-student KL divergence and accuracy on various out-of-distribution test datasets), especially on datasets with spurious statistical correlations.

## 1 Introduction

Increasingly large deep neural networks (DNNs) trained on huge, web-crawled datasets have shown unprecedented performance on a multitude of tasks , including emergent capabilities that help with more general-purpose and flexible behaviour with SOTA on tasks they were not finetuned on . However, resource constraints faced in realistic scenarios, e.g., latency or energy budgets, impact the feasibility of practically deploying such large models. _Knowledge distillation_ was motivated as a framework to address this challenge, wherein a smaller "student" model is trained to mimic the outputs produced by the pre-trained "teacher" network on some available dataset. The underlying hypothesis is that enforcing consistency between the outputs produced by two models will yield a "transfer of knowledge" , resulting in the less performant model (student) inheriting the _mechanisms_ used by the more performant model (teacher) to make its predictions.

The immense success of distillation in several diverse domains  does make the argument above sound intuitively correct. However, follow-up work focused on developing a better understanding of knowledge distillation has raised doubt on this viewpoint . These works demonstrate that distilled student models infrequently make the same errors as the teacher models. This is an unlikely result if the models were sharing knowledge (and hence relying on the same mechanisms for making their predictions). These papers make the success of knowledge distillation surprising, and it remains unclear what precise prediction mechanisms, if any, the student inherits from the teacher. Since the student is often observed to outperform the teacher , it may be learning entirely novel mechanisms that the teacher does noteven possess. This is possible because in practical settings, the distillation dataset is likely of direct relevance to the application of interest. It is also likely smaller than (or minimally overlaps with) the teacher's pretraining data, and may not be available in offline distillation [27; 48; 49; 50; 51]. Since these smaller distillation datasets are often underspecified (i.e. they contain several predictive attributes that can be used to produce the correct output [52; 53; 54; 55; 56]), a student can in principle learn to match outputs produced by the teacher through a different mechanism to that used by the teacher. Previous explorations into why knowledge distillation works suggests it is unclear what would motivate the student to learn similar prediction mechanisms as the teacher. For example, Cheng et al.  suggest that knowledge distillation enforces learning various concepts simultaneously. In addition to providing additional information, they find that teacher outputs guide the optimisation process by preventing excessive exploration of the loss landscape. Phuong et al.  find data geometry (e.g. class separation) and optimiser bias to also be contributing factors. This leads to another question: what design decisions in distillation pipelines incentivize the student to learn the same prediction mechanisms as the teacher model? Beyond developing a better understanding of distillation, answering these questions clarify when distillation can be used for producing a student model that serves as a faithful replacement of its teacher counterpart. We make the following contributions:

* **Formalizing knowledge transfer.** We define successful knowledge transfer as when the student and teacher produce the same outputs under systematically generated counterfactuals of a dataset. This definition abstracts away the precise implementation of a prediction mechanism and only emphasizes the behavioral equivalence of two models to define a notion of'shared knowledge'.
* **Characterizing knowledge transfer in distillation techniques** Motivated by our definition, we develop synthetic datasets spanning different modalities to allow counterfactual generation and hence enable precise characterization of which prediction mechanisms a model relies on for producing its outputs. We demonstrate that the standard distillation pipeline of matching teacher logits suffers from a _simplicity bias_[57; 58; 59; 60], resulting in the student learning primarily the simplest mechanisms in the distillation dataset. If the distillation dataset and the teacher's pretraining dataset have different distributions, the student and teacher may learn entirely distinct prediction mechanisms.
* **Methods for reducing simplicity bias and improving student-teacher matching.** We investigate two distillation methods aiming to more closely match model representations. We find evidence for decreased teacher-to-student KL divergence and less simplicity bias towards certain _spurious features_.

## 2 Preliminaries: Knowledge Distillation

**Notation.** Consider a neural network \(f:^{n}^{d}^{K}\) that takes \(n\)-dimensional inputs \(x^{n}\), has parameters \(^{d}\), and produces an output \(f(x;)^{K}\) (interpreted as the logits in a classification setting). The neural network predictions are the composition \(f^{_{}}=_{T} f\), where \(_{T}(z)_{i}=/T}}{_{j}/T}}\) is the temperature-weighted softmax function for some temperature parameter \(T>0\). Cross-entropy loss on a dataset \([K]\) (where \([K]\) denotes the set \(\{1,2,,K\}\)) for a model with parameters \(\) is written \((f(;))\).

Let \(_{}\) be the dataset used to train the teacher model \(f_{}:^{n}^{d_{}} ^{k}\) (whose parameters are \(_{}^{d_{}}\)). The goal in knowledge distillation is to use this teacher model to train a'student' model \(f_{}:^{n}^{d_{}} ^{k}\) by finding a set of parameters \(_{}^{d_{}}\) such that the outputs of the student model \(f_{}(;_{})\) match, in some specific sense, outputs from the teacher model \(f_{}(;_{})\) on a 'distillation dataset' \(_{}\). We distinguish between the dataset used for training the teacher versus the one used for distilling the teacher into the student to a) emphasize the fact that a practitioner who acquires an off-the-shelf, pretrained teacher model (offline training) is unlikely to have access to the data used for training it, and b) explore situations where the student dataset is markedly different - perhaps more diverse and likely containing _spurious mechanisms_. There exists a diverse range of possible distillation methods . Of these, we explore three: Jacobian matching, contrastive representation distillation, and soft targets only from standard distillation. Beyond their widespread use, we choose these methods because they focus only on input/output information - i.e. no intermediate representations are used.

**(A) Standard distillation.** First proposed in the context of neural networks by Hinton et al. , the standard distillation pipeline involves optimizing the agreement between teacher and student model predictions by minimizing the \(\)-divergence between them:

\[_{x_{}}[D_{}(f_{ }^{_{}}(x;_{})\|f_{}^{_{}}(x;_{}))]\] (1)

The objective above is equivalent to minimising cross-entropy loss with \(f_{}^{_{}}(x;_{})\) as the'soft' targets for a student model. As shown by Hinton et al. , assuming the logits \(f_{}(x),f_{}(x)\) are zero-centered (mean zero), in the high temperature limit \(T\) this is equivalent to minimising the average logit squared difference: \(\|f_{}(x)-f_{}(x)\|^{2}\).

While a standard cross-entropy loss promoting correct classification is often added to the distillation objective, we follow recent works on understanding knowledge distillation  and focus on the distillation objective only. No auxiliary classification loss is added while training the student, isolating the distillation objective's effect in inducing knowledge transfer between teacher and student.

**(B) Jacobian Matching.** A Jacobian matching distillation loss matches norm of the gradient of logits with respect to the input between teacher and student: \(f_{}^{_{}},f_{}^{_{ }}\) and the input-output Jacobians \(_{f_{}^{_{}}(,_{})}\), \(_{f_{}^{_{}}(,_{})}^ {_{}(,_{})}^{K n}\) match on examples in the distillation dataset \(_{}\). This method is equivalent to classical distillation with analytical addition of perturbation noise to inputs. We can decompose the loss function into one representing usual squared error loss and a regularisation term (Tikhonov regulariser). This does not just match on datapoints, but infinitely many points in their neighbourhood. This can be achieved by adding a the following penalty to the standard distillation loss of Eq. 1:

\[\|_{f_{}^{_{}}(,_{ })}()-_{f_{}^{_{}}(,_{})}()\|_{2}^{2}\] (2)

Beyond distillation , the objective above or variants of it have been introduced in several contexts, such as improving out-of-distribution generalization , improving adversarial robustness , and for learning disentangled representations .

**(C) Contrastive Distillation.** In contrastive distillation, the goal is to train the student to maximise the _mutual information_ between the representations (typically, the features in the penultimate layer) of the teacher network and the student network on the transfer dataset \(_{}\). In , the authors propose to do this by maximising a lower-bound on the mutual information objective given below. Denote by \(g_{}:^{n}^{h_{}}\), \(g_{}:^{n}^{h_{}}\) the functions producing the penultimate layer features in the student and teacher models respectively.

**Teacher Representation:** & \(Z_{}=g_{}(X)\) \\
**Student Representation:** & \(Z_{}=g_{}(X)\) \\ 

\[(Z_{t},Z_{s})_{p(Z_{},Z_{})}[ h(Z_{},Z_{})]+N_{p(Z_{ })p(Z_{})}[(1-h(Z_{},Z_{}))]\] (3)

where \(h:^{h_{}} R^{h_{}}\) is a learnable function optimized jointly with the parameters of the student; it can be interpreted as an auxiliary "critic" predicting whether the representations were sampled jointly (from \(p(Z_{},Z_{})\)) or independently (from \(p(Z_{})p(Z_{})\)), assuming that they are sampled jointly \({}^{1}\!/\!(N+1)\) of the time. \(h\) typically takes the parametric form:

\[h(_{},_{})=_{ }^{}_{}/}{_{}^{ }_{}/+_{}|1}}, _{}=W_{}_{}/\|W_{}_ {}\|,\] (4)

where \(W_{}^{h_{} h_{}}\), \(W_{}^{h_{} h_{}}\) are learnable parameters, and \(,h_{}\) are hyperparameters.

## 3 Defining Knowledge Transfer

**Motivation.** As discussed in Sec. 1, despite distillation's immense success in various fields [28; 29; 36; 30; 31; 37], a formal notion of precisely what knowledge, if any, is transferred from the teacher to student has yet to be defined. For instance, consider a visual object recognition task. In such scenarios, backgrounds are often correlated with the object category due to sampling bias [52; 53; 56]. Here, a model can rely on either the background or more intrinsically meaningful attributes of the object, such as its shape, to solve the recognition task. To understand which, we can evaluate how the model's prediction changes when image backgrounds are altered. If predictions change, the model relies on information in the (spurious) attribute of image background; if the predictions do not change, the model is invariant to background. Formalizing this intuition, prior work calls use of a predictive attribute to produce outputs a "mechanism" , and defines two models that rely on the same mechanisms as _mechanistically similar_. This framework is relevant to the problem of knowledge transfer in distillation as well. Specifically, if a student model that is perhaps more resource-efficient behaves the same way as a teacher model (i.e. is mechanistically similar), it can serve as a _faithful_ replacement of the teacher. We generalize their formalization to our distillation setup next.

Let \(I=(i_{1},,i_{k})\) denote a non-empty subsequence of indices \((1,2,,d)\). Consider a set of latents \(\), that instantiates a data-generating process (DGP) \(g:\) from the latents \(\) to observations \(\) and a labeling function \(h:\) from latents \(\) to labels \(\). We assume _observational sufficiency_ of the DGP: observations are sufficient for determining the label.

**Definition 3.1**.: **(Mechanism.)** For a particular latent configuration \(\), we say that \(f(.;):\) uses mechanism \(I\) on that example (where \(I[d_{z}]\) is the subset of indices of the latents) whenever \(f(g(^{});)=f(g();)\) for all \(^{}|^{}_{I}=_{I}\).

Based on previous research (Appendix A), simplicity bias is where a model tends to rely on latent features which produce a'simpler' decision boundary or solution. We call such latents corresponding to spurious correlations in the learned model _spurious cues_.

Figure 1: **Synthetic Datasets.** Following the protocol above, we embed synthetic cues in two existing datasets: (1) Dominoes , where CIFAR-10 images are concatenated with F-MNIST images with the same class number. A third spurious mechanism selects a location of the CIFAR-10 image to set as randomised pixels (we refer to this as ‘box’). If the box mechanism is correlated with CIFAR-10, then the location is determined by the CIFAR-10 label. (2) Spurious parity, where the simple task acts as a spurious mechanism. The label is the parity of the specified hard task subsequence (which is selected for by the control bit). All the hard tasks together act as a single complex mechanism. Shown here is a setup with 3 hard tasks, 8 total hard task bits, 3 hard task bits per task and 2 simple task bits.

Figure 2: **Knowledge Transfer:** We define successful knowledge transfer of a teacher and student model based on how they respond to unit interventions on the data-generating process, i.e., interventions on specific dimensions of the latent vector \(z\); e.g., \(_{1}\) (shape) and \(_{2}\) (background) in the figure. Here, yellow circles represent the prediction of a given model (column) on a counterfactual image (row). Models whose predictions are invariant to the same set of interventions (denoted \(_{1}_{2}\)) are termed mechanistically similar.

While a few recent analyses have addressed similar questions, they focus on in-distribution test datasets and coarse-grained measures like loss values [39; 31; 44]. In contrast, we emphasize out-of-distribution, counterfactual datasets. This is motivated from a notion of behavioral equivalence as the ideal goal of knowledge distillation. Our experiments are focused on the following questions for analysis:

1. For soft-target-matching-only distillation, only the simplest mechanisms will match.
2. Training a student model by minimizing the standard distillation objective is insufficient to guarantee knowledge transfer from teacher to student. Meanwhile, there exist distillation methods more likely to transfer all mechanisms.
3. Even if a teacher and student have similar fidelity (accuracy on the base task on the distillation set or even the teacher's training set), they do not necessarily behave the same out of distribution .

The second point above is a consequence of the recent advances made in the in the field of Nonlinear Independent Component Analysis [68; 69; 70; 71; 72; 66] and disentangled representation learning [73; 74]. These demonstrate that producing the same outputs on a given dataset is insufficient to guarantee two models rely on the same underlying mechanisms for making their predictions. These suggest distillation is limited in what knowledge it can transfer--this depends on what data is shown to the models during distillation. We probe this further via empirical investigations.

## 4 Mechanistic Evaluation of Distillation

In this section we highlight the experimental setup and high-level results.

### Training and Evaluation

**Dataset Generation.** We follow prior work on understanding distillation, which primarily uses synthetic datasets to evaluate distillation protocols in a controlled manner [39; 41; 40; 45; 43; 42]. Having control over the data-generating process allows us to be precise about the distribution shift that occurs in the distillation dataset with respect to the teacher's pretraining data, in order to evaluate a student model's reliance on different mechanisms by altering the underlying latents. We assume a set of 'natural' latents underlie the labeling function \(h\) of the data-generating process. All other latents are either uncorrelated with the label or model'spurious' cues in the data. If using information from spurious latents leads to simpler functions, neural network simplicity bias [59; 57; 58; 35; 75; 76; 77; 78] suggests that a network will rely on them rather than the natural attributes for reducing the task loss. We denote the \(n\) mechanisms defined by these spurious latents as \(\{I_{s_{1}},I_{s_{2}},,I_{s_{n}}\}\). We design two datasets across both image and text data, called _dominoes_ (images) and _parity_ (language), shown in Fig. 1. These datasets have been used by prior works for modeling neural networks' behavior regarding simplicity bias [57; 35], transfer learning [79; 80; 81; 82], disentangled representation learning , and scaling laws .

**Training Protocol.** We use teacher and distillation datasets with different distributions over the latents, modeling the fact that a practitioner training a student model is unlikely to have access to the same dataset as the teacher model's training data. To understand the effect of distribution shifts, we test our dominoes dataset (with an image mechanism and two spurious mechanisms) under all possible combinations of distillation and teacher dataset mechanisms (Figure 3).

**Evaluation Protocol.** To evaluate whether a model uses any given mechanism, we randomise or remove latents corresponding to the content of the original image on the dominoes dataset, and report the expected divergence (as in Definition 3.1). For the image dominoes datasets, we remove the latents entirely.

### Distillation Loss and Distribution Shift on Dominoes Dataset

This section explores the effect of distribution shifts on a 3-mechanism image dataset for ResNet-18 self-distillation. We use each of 7 possible datasets where at least 1 of 3 mechanisms exists for teacher training, distillation and test evaluation. This gives 49 student/teacher mechanism combinations and 343 categorical final test values per loss function, each run with 3 seeds. Notation: \(S\) is distillation dataset (student) mechanism and \(T\) is teacher dataset mechanism. 'Base distillation' means softmax logit KL matching. All mechanisms are denoted by a single letter (see Figure 1) - \(I\): image (CIFAR10), _A_: spurious mechanism A (box), _B_: spurious mechanism B (F-MNIST). In Figures 3, 4, each strip corresponds to a different test mechanism, and each group to the relationship between \(S\) and \(T\). This grouping does not use the relation between the test mechanism and \(S\), \(T\). However, they are used in Appendix D Tables 1, 2. Finally, Figure 9 shows the mean and variance of final accuracy values for separated test mechanisms. Refer to Appendix B, D, E for the rest of the results.

Simplicity bias is observed in Figure 9 column 1 with base distillation. When the box mechanism (mechanism A) is present in the student and teacher datasets, it is learned while CIFAR-10 and F-MNIST are ignored. This is expected behaviour, as the teacher also shows simplicity bias (Section C). Interestingly, the student can learn a new F-MNIST mechanism (column 1, row 4 under test mechanism B) if it is present in the distillation dataset and correlated with the box mechanism--this is an example of a type of'secondary transfer' which we discuss in further detail below.

For Jacobian loss, data points in Figure 9 with a change greater than 2 standard deviations are often cases of distribution shift. In particular, there is decreased learning if the test mechanism was only in the distillation dataset. Jacobian loss is more likely to match \(S\) to \(T\) than base distillation if the

Figure 4: **Final test KL divergence**. Notation is as in Figure 3. Middle: Jacobian loss leads to especially high range of final KL divergence when the teacher and student do not share mechanisms. Right: contrastive loss further bounds teacher-student KL divergence and results in most effective matching of teacher and student.

Figure 3: **Final test accuracy. Notation**: _Similarity_: test mechanism overlaps completely with both student and teacher mechanisms. _Student_: test mechanism in \(S\) and not in \(T\). There must be a shared mechanism between \(S\) and \(T\), excluding the test mechanism. _Teacher_: test mechanism in \(T\) but not in \(S\), with same criteria for shared subtask as in Student group. _Student TP_: test mechanism is covered by \(S\) and shares a subset with the \(T\). _Overlap_: \(S\), \(T\) share a subset and this subset is not in test mechanism. _Neither_: teacher shares no mechanisms with student. Any scenarios not fitting these categories are classified under _Other_, a broad class where the student and teacher each contain some subset of the test mechanism. Left: for the ‘Similarity’ group, a lower bound is observed on performance across all combinations. This bound is highest when all three mechanisms are present (test mechanism IAB). Middle: Jacobian has slightly lower performance when teacher and student do not share mechanisms (‘Neither’ group), improved performance for certain test mechanisms when only the teacher contains it (‘Teacher’, ‘Teacher SP’ groups) and reduced performance for certain test mechanisms when only the student contains it (‘Student’, ‘Student TP’ groups). Right: contrastive loss strongly upper bounds test accuracy for spurious mechanisms in ‘Similarity’ group. Even when the student, teacher and test datasets share the spurious mechanism (‘Similarity’ group), learning is impeded. The only exception to this is the box mechanism in the test dataset, where simplicity bias is still observed.

test mechanism is in \(T\) but not in/partially in \(S\) (Table 1 column 'in \(T\)'). Also, it is less likely to transfer all of \(S\) (Table 1 column 'In \(S\)') if it is not in/partially in the teacher dataset. KL divergence on test datasets decreases if the teacher and student datasets are identical (Figure 13. In contrast, KL divergence increases when \(S T\) (e.g. mechanism I and AB) or one of \(T\) or \(S\) contains extra mechanisms that the other dataset did not (Appendix E). In this sense, Jacobian loss may not improve accuracy, but leads to better matching overall.

Contrastive loss shows strong suppression of box mechanism transfer where it is not present in both teacher and student datasets (Figure 3, Figure 9). This transfer suppression is greatest when the spurious test mechanism is present in only one of the student or teacher datasets. Relative to performance in base distillation, this effect is surprisingly strong for the image corrupted by the box mechanism (test mechanism IA, 'Student'/'Teacher' groups, Figure 3). This may model well behaviour on a type of spurious feature often present in realistic vision datasets. However, for all mechanisms, training with contrastive loss takes longer to achieve the same accuracy, resulting in a significant performance-robustness trade-off (Appendix D, Table 1, column '\(EQ\)'). It can also lead to transfer of simple mechanisms if they are present in both datasets. Contrastive loss produces the largest decrease in teacher-student output KL divergence compared to base and Jacobian distillation (Figure 4). The greatest KL divergence values for this loss function are for the 'Neither' group, where the teacher and student match but do not match the test mechanism. Contrastive loss seems to trade off matching on the support set of the teacher and student's intersection for poorer performance entirely out of distribution of both.

Base distillation often leads to a type of'secondary transfer': if the teacher mechanism contains mechanisms P and Q, the student mechanism contains mechanisms Q and R, and we test on just mechanism P, the student may have high performance. This seems obvious, as the shared mechanism Q allows the teacher to produce correctly labelled examples. However, Jacobian and contrastive losses are less likely to produce this effect (Figure 9).

### Fraction of Spurious Mechanism on Parity Dataset

Figure 6 shows the training steps required to achieve a given accuracy threshold as a function of the distillation dataset probability of simple task parity correlating with hard task parity. This shows a test dataset where only the hard task corresponds to the label. The case where both simple and hard tasks correspond to the label is in Appendix F. The teacher dataset contains only clean data--i.e. hard task bit parity is the label, while simple task bits are randomised with respect to the label. More hard task substrings to pick from increases the relative difficulty of learning the hard task mechanism, compared to the simple task. For full results, including accuracy and entropy over training time on datasets with counterfactually randomised latents, see Appendix F.

Except for the case where all distillation training samples have the simple task on, the model can always learn the hard task. This is true for both MLPs and transformers (Figure 6). The rate at which this task is learned is strongly affected by the fraction of samples with only the hard task present. The more hard tasks, the more training time is required to learn to a specific accuracy threshold for a fixed fraction of spurious mechanism. Adding a Jacobian loss term speeds up the learning of harder mechanisms present in both teacher and student datasets. This can be seen by comparing Figure 6(a) to (b): the student reaches higher evaluation accuracy within a fixed number of steps on the dataset with only hard task corresponding to the label.

## 5 Discussion

For all results in this section, the effect of changes such as adding loss terms will differ depending on the modality and dataset, hence the results here should not be considered general.

We observe simplicity bias in the base distillation accuracy plot in 3, where all mechanisms containing spurious latents have higher maximum accuracy scores. Full results (Appendix D, E) show that the presence of the box mechanism in both teacher and student datasets will transfer the box mechanism to the student to near 100% accuracy, to the detriment of learning the image.

Jacobian matching loss on vision datasets has an effect of improved matching of the teacher mechanism, and reduced learning of newer mechanisms only present in the student. In general the results for this loss function are subtle, though the most statistically significant (2 standard deviations minimum)differences can be found for student and teacher datasets with little overlap (Figure 9 and Appendices D, E). This could be explained by the theory presented in Appendix A. We postulate that due to the complexity and subtlety of this effect, other methods such as matching input-activation Jacobians or using different datasets may produce more pronounced or qualitatively different results.

Across all teacher-student-test dataset triplets we tested, contrastive loss has the lowest teacher-student KL divergence, despite being the slowest to train. This effect also holds on the patterned box dataset, where not only the location but also the pixel values correspond to the label (Appendix E). However, there is an accuracy penalty of around 40% (Appendix D Table 1). This trade-off may be worthwhile in cases where it is important that the nature of the representation the student learns is similar to that of the teacher and large quantities of compute are available. Since prior work  shows that with a long enough distillation training, distillation effectiveness increases, there is no reason to believe that increasing epochs will not eliminate this accuracy penalty. We leave exploration of this phenomenon to further work.

Figure 5: **Final accuracy/accuracy change and standard deviation on various test mechanisms, with dominoes dataset.** Row labels within the heatmaps indicate student mechanism. The base distillation column gives raw values. The Jacobian and contrastive loss columns are differences, given by by new loss function minus base distillation. Column 2: the effect of Jacobian loss is subtle. It typically results in the greatest reduction in performance when the student dataset alone contains the test mechanism. Column 3: contrastive loss leads to reduction in transfer of the spurious mechanisms A and B (rows 2, 3) when both are present in the student and teacher datasets.

## 6 Conclusion

In the datasets we examined, we found that Jacobian matching is useful when the teacher dataset is cleaner than the student dataset. Furthermore, we found that contrastive distillation results in a noticeable mitigation of simplicity bias. For both Jacobian and contrastive representation distillation, when the test mechanism either subsets, is a subset of, or only partially overlaps with the teacher and student mechanisms, transfer is reduced when compared to base distillation. In both cases, we also observe slower training. Distillation results are always stopped at a fixed number of epochs, so final accuracy may continue improving in these examples if the student is trained for longer.

Results on the parity dataset also agree with our simplicity bias hypothesis. On distillation datasets where the simple task always corresponds to the hard task's label, the hard task will never be learned by the student. Jacobian matching has the strongest effect on this dataset. It speeds up transfer of the hard task from a clean teacher distilled on a student dataset with the simple task, as long as the dataset has some examples for which the hard task only is predictive of the label.

### Further Work

We suggest that further work investigates how exactly the model uses each of the mechanisms, potentially locating 'circuits' corresponding to localized computation of concepts in the network, as per recent interpretability literature [85; 86; 87; 88; 89]. In particular, the mechanism definition may be most useful when each latent dimension corresponds to 'features'--for example, using the Fourier spectrum of image data  or gradient spectral clustering . Such human-imperceptible statistical correlations often form the backbone of how models learn algorithms to compute tasks [87; 90], leading to models vulnerable to adversarial attacks. A more modular representation should also allow our data-generating process to align better to how models process information.

Figure 6: **Steps to reach particular accuracy threshold on a test dataset vs distillation simple task fraction, parity dataset. Test dataset: hard task always on, simple task randomised. Distillation dataset: hard task always on, and simple task probability on is given by \(x\)-axis. Teacher dataset: hard task only (for difference with test dataset, see Appendix B—in this case, teacher and test datasets are identical). When a given accuracy value is never obtained, an ‘x’ is plotted and the datapoint is omitted from quadratic interpolation. Each data point is a separately trained student. Error bars show steps required in accuracy for \(\) 1 standard deviation. (a, c) The model always learns the hard task, except when all distillation examples contain the spurious mechanism. Steps to reach a given accuracy for the hard task increases as fraction of simple mechanism in distillation dataset increases. This is expected for per-datapoint simplicity bias. (b) Compare to (a): for a given simple mechanism fraction, fewer training steps are required for reaching a given accuracy threshold.**