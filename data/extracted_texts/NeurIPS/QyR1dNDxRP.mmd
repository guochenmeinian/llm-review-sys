# Provable Tempered Overfitting of

Minimal Nets and Typical Nets

 Itamar Harel

Technion

itamarharel01@gmail.com &William M. Hoza

The University of Chicago

&Gal Vardi

Weizmann Institute of Science

&Itay Evron

Technion

&Nathan Srebro

Toyota Technological Institute at Chicago &Daniel Soudry

Technion

###### Abstract

We study the overfitting behavior of fully connected deep Neural Networks (NNs) with binary weights fitted to perfectly classify a noisy training set. We consider interpolation using both the smallest NN (having the minimal number of weights) and a random interpolating NN. For both learning rules, we prove overfitting is tempered. Our analysis rests on a new bound on the size of a threshold circuit consistent with a partial function. To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.

## 1 Introduction

Neural networks (NNs) famously exhibit strong generalization capabilities, seemingly in defiance of traditional generalization theory. Specifically, NNs often generalize well empirically even when trained to interpolate the training data perfectly . This motivated an extensive line of work attempting to explain the overfitting behavior of NNs, and particularly their generalization capabilities when trained to perfectly fit a training set with corrupted labels (_e.g.,_).

In an attempt to better understand the aforementioned generalization capabilities of NNs, Mallinar et al.  proposed a taxonomy of benign, tempered, and catastrophic overfitting. An algorithm that perfectly interpolates a training set with corrupted labels, _i.e.,_ an interpolator, is said to have tempered overfitting if its generalization error is neither benign nor catastrophic -- not optimal but much better than trivial. However, the characterization of overfitting in NNs is still incomplete, especially in _deep_ NNs when the input dimension is neither very high nor very low. In this paper, we aim to understand the overfitting behavior of deep NNs in this regime.

We start by analyzing tempered overfitting in "min-size" NN interpolators, _i.e.,_ whose neural layer widths are selected to minimize the total number of weights. The number of parameters in a model is a natural complexity measure in learning theory and practice. For instance, it is theoretically well understood that \(L_{1}\) regularization in a sparse linear regression setting yields a sparse regressor. Practically, finding small-sized deep models is a common objective used in pruning (_e.g.,_) and neural architecture search (_e.g.,_). Recently, Manoj and Srebro  proved that the _shortest program_ (Turing machine) that perfectly interpolates noisy datasets exhibits tempered overfitting, illustrating how a powerful model can avoid catastrophic overfitting by returning a min-size interpolator.

Furthermore, we study tempered overfitting in random ("typical") interpolators -- NNs sampled uniformly from the set of parameters that perfectly fit the training set. Given a narrow teacher model and no label noise, Buzaglo et al.  recently proved that such typical interpolators, which may be _highly overparameterized_, generalize well. This is remarkable since these interpolators do not rely onexplicit regularization or the implicit bias of any gradient algorithm. An immediate question arises -- what kind of generalization behavior do typical interpolators exhibit in the presence of label noise? This is especially interesting in light of theoretical and empirical findings that typical NNs implement low-frequency functions , while interpolating noisy training sets may require high frequencies.

For both the min-size and typical NN interpolators, we study the generalization behavior under an underlying _noisy_ teacher model. We focus on deep NNs with binary weights and activations (similar NNs are used in resource-constrained environments; _e.g.,_). Our analysis reveals that these models exhibit a tempered overfitting behavior that depends on the statistical properties of the label noise. For independent noise, in addition to an upper bound we also find a lower bound on the expected generalization error. Our results are illustrated in Figure 1 below, in which the yellow line in the right panel is similar to empirically observed linear behavior (e.g., 57, Figures 2, 3, and 6).

The contributions of this paper are:

* Returning a min-size NN interpolator is a natural learning rule that follows the Occam's-razor principle. We show that this learning rule exhibits tempered overfitting (Section 4.1).
* We prove that overparameterized random NN interpolators typically exhibit tempered overfitting with generalization close to a min-size NN interpolator (Section 4.2).
* To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.
* The above results rely on a key technical result -- datasets generated by a constant-size teacher model with label noise can be interpolated1 using a NN of constant depth with threshold activations, binary weights, a width sublinear in \(N\), and roughly \(H(^{}) N\) weights, where \(H(^{})\) is the binary entropy function of the fraction of corrupted labels (Section 3).

Figure 1: **Types of overfitting behaviors.** Consider a binary classification problem of learning a realizable distribution \(_{0}\). Let \(\) be the distribution induced by adding an \(^{}\)-probability for a data point’s label to be flipped relative to \(_{0}\). Suppose a model is trained with data from \(\). Then, assuming the classes are balanced, the trivial generalization performance is \(0.5\) (in gray; _e.g.,_ with a constant predictor). **Left.** Evaluating the model on \(\), a Bayes-optimal hypothesis (in red) obtains a generalization error of \(^{}\). For large enough training sets, our results (Section 4) dictate a tempered overfitting behavior illustrated above. For arbitrary noise, the error is approximately bounded by \(1-^{^{}}(1-^{})^{1 -^{}}\) (blue). For independent noise, the error is concentrated around the tighter \(2^{}(1-^{})\) (yellow). A similar figure was previously shown in Manoj and Srebro  for shortest-program interpolators. **Right.** Assuming independent noise, the left figure can be transformed into the error of the model on \(_{0}\) (see Lemma A.9). The linear behavior in the independent setting (yellow) is similar to the behavior observed empirically in Mallinar et al. [57, Figures 2, 3, and 6].

Setting

Notation.We reserve bold lowercase characters for vectors, bold uppercase characters for matrices, and regular uppercase characters for random elements. We use \(\) to denote the base 2 logarithm, and \(\) to denote the natural logarithm. For a pair of vectors \(=(d_{1},,d_{L}),^{}=(d_{ 1}^{},,d_{L}^{})^{L}\) we denote \(^{}\) if for all \(l[L]\), \(d_{l} d_{l}^{}\). We use \(\) to denote the XOR between two binary \(\{0,1\}\) values, and \(\) to denote the Hadamard (elementwise) product between two vectors. We use \(H()\) to denote the entropy of some distribution \(\). Finally, we use \(()\) for the Bernoulli distribution with parameter \(\), and \(H()\) for its entropy, which is the binary entropy function.

### Model: Fully connected threshold NNs with binary weights

Similarly to Buzaglo et al. , we define the following model.

**Definition 2.1** (Binary threshold networks).: For a depth \(L\), widths \(=(d_{1},,d_{L})\), input dimension \(d_{0}\), a scaled-neuron fully connected binary threshold NN, or binary threshold network, is a mapping \( h_{}\) such that \(h_{}:\{0,1\}^{d_{0}}\{0,1\}^{d_{L}}\), parameterized by

\[=\{^{(l)},^{(l )},^{(l)}\}_{l=1}^{L}\,,\]

where for every layer \(l[L]\),

\[^{(l)}\!_{l}^{W}\!=\!\{0,1\}^{ d_{l} d_{l-1}},\;^{(l)}\!_{l}^{ }\!=\!\{-1,0,1\}^{d_{l}},\;^{(l)}\! _{l}^{b}\!=\!\{-d_{l-1}+1,,d_{l-1}\}^{d_{l}}\;.\]

This mapping is defined recursively as \(h_{}()=h^{(L)}( )\) where

\[h^{(0)}() =\,,\] \[ l[L] h^{(l)}() =\{(^{(l)} (^{(l)}h^{(l-1)}( ))+^{(l)})>\}\,.\]

We denote the number of weights by \(w()=_{l=1}^{L}d_{l}d_{l-1}\), and the total number of neurons by \(n()=_{l=1}^{L}d_{l}\). The total number of parameters in such a NN is \(M()=w()+2n()\). We denote the set of functions representable as binary networks of widths \(\) by \(_{}^{}\) and their corresponding parameter space by \(^{}()\).

_Remark 2.2_.: Our generalization results are for the above formulation of neuron scalars \(\), _i.e._, ternary scaling _before_ the activation. However, we could have derived similar results if, instead, we changed the scale \(\) to appear _after_ the activation and also adjusted the range of the biases (see Appendix G). Although we chose the former for simplicity, the latter is similar to the ubiquitous phenomenon in neuroscience known as "Dale's Law" . This law, in a simplified form, means that all outgoing synapses of a neuron have the same effect, _e.g._, are all excitatory (positive) or all inhibitory (negative).

_Remark 2.3_ (Simple counting argument).: Let \(_{}\{d_{1},,d_{L-1}\}\) be the maximal hidden-layer width. Then, combinatorially, it holds that

\[_{}^{}|}_{ }} ()|}_{ \\ } )}_{}+ )}_{\\ }((3)+ )}_{}).\]

This implies, using classical PAC bounds , that the sample complexity of learning with the _finite_ hypothesis class \(_{}^{}\) is \(O(w()+n() {d}_{})\) (a more refined bound on \(|_{}^{}|\) is given in Lemma F.1). In Section 4 we show how this generalization bound can be improved in our setting.

### Data model: A teacher network and label-flip noise

Data distribution.Let \(=\{0,1\}^{d_{0}}\) and let \(\) be some joint distribution over a finite sample space \(\{0,1\}\) of features and labels.

**Assumption 2.4** (Teacher assumption).: We assume a "teacher NN" \(h^{}\) generating the labels. A label flipping noise is then added with a noise level of \(^{}=_{(X,Y)}(Y h^{ }(X))\), or equivalently

\[Y h^{}(X)(^{})\,.\]

The label noise is _independent_ when \(Y h^{}(X)\) is independent of the features \(X\) (in Section 4 it leads to stronger generalization results compared to ones for arbitrary noise).

### Learning problem: Classification with interpolators

We consider the problem of binary classification over a training set \(S=\{(_{i},y_{i})\}_{i=1}^{N}\) with \(N\) data points, sampled from the noisy joint distribution \(\) described above. We always assume that \(S\) is sampled i.i.d., and therefore, with some abuse of notation, we use \((S)=^{N}(S)=_{i=1}^{N} (_{i},y_{i})\). For a hypothesis \(h:\{0,1\}\), we define the risk, _i.e.,_ the generalization error w.r.t. \(\), as

\[_{}(h)_{(X,Y) }(h(X) Y)\,.\]

We also define the empirical risk, _i.e.,_ the training error,

\[_{S}(h)_{n=1}^{N} \{h(_{n}) y_{n}\}\,.\]

We say a hypothesis is an _interpolator_ if \(_{S}(h)=0\).

In this paper, we are specifically interested in _consistent_ datasets that can be perfectly fit. This is formalized in the following definition.

**Definition 2.5** (Consistent datasets).: A dataset \(S=\{(_{i},y_{i})\}_{i=1}^{N}\) is consistent if

\[ i,j[N]\;_{i}=_{j} y _{i}=y_{j}\,.\]

Motivated by modern NNs which are often extremely overparameterized, we are interested in the generalization behavior of interpolators, _i.e.,_ models that fit a consistent training set perfectly. Specifically, we consider Framework 1. While this framework is general enough to fit any minimal training error models, we shall be interested in the generalization of \(A(S)\) in cases where the training set is most likely consistent (Def. 2.5).

``` Input: A training set \(S\). Algorithm: if\(S\) is consistent: return an interpolator \(A(S)=h\) (such that \(_{S}(h)=0\)) else: return an arbitrary hypothesis \(A(S)=h\) (_e.g.,_\(h()=0,\)) ```

**Framework 1** Learning interpolators

In Section 4, we analyze the generalization of two learning rules that fall under this framework: (1) learning min-size NN interpolators and (2) sampling random NN interpolators. Our analysis reveals a tempered overfitting behavior in both cases.

## 3 Interpolating a noisy training set

Our main generalization results rely on a key technical result, which shows how to memorize any consistent training set generated according to our noisy teacher model. We prove that the memorizing "student" NN can be small enough to yield meaningful generalization bounds in the next sections.

We begin by noticing that under a teacher model \(h^{}\) (Assumption 2.4), the labels of a consistent dataset \(S\) (Def. 2.5) can be decomposed as

\[ i[N]\;y_{i}=h^{}(_{i}) f (_{i})\,,\] (1)

where \(f:\{0,1\}^{d_{0}}\{0,1\}\) indicates a label flip in the \(i^{}\) example, and can be defined arbitrarily for \( S\). Motivated by this observation, we now show an upper bound for the dimensions of a network interpolating \(S\), by bounding the dimensions of an NN implementing an arbitrary "partial" function \(f\) defined on \(N\) points.

**Theorem 3.1** (Memorizing the label flips).: _Let \(f\{0,1\}^{d_{0}}\{0,1,\}\) be any function.2 Let \(N=|f^{-1}(\{0,1\})|\) and \(N_{1}=|f^{-1}(1)|\). There exists a depth-\(14\) binary threshold network \(\{0,1\}^{d_{0}}\{0,1\}\), with widths \(\), satisfying the following._

1. \(\) _is consistent with_ \(f\)_, i.e., for every_ \(\{0,1\}^{d_{0}}\)_, if_ \(f()\{0,1\}\)_, then_ \(()=f()\)_._
2. _The total number of weights in_ \(\) _is at most_ \((1+o(1))}+(d_{0})\)_. More precisely,_ \[w()=}+(} )^{3/4}\,N+O(d_{0}^{2} N)\,.\]
3. _Every layer of_ \(\) _has width at most_ \((})^{3/4}(d_{0})\)_. More precisely,_ \[_{}=(})^{3/4}\,N+O(d_{0} N)\,.\]

The main takeaway from Theorem 3.1 is that label flips can be memorized with networks with a number of parameters that is optimal in the leading order \(N_{S}(h^{})\), i.e., not far from the minimal information-theoretical value. The proofs for this section are given in Appendix D.

Proof idea.Denote \(S=f^{-1}(\{0,1\})\). We employ established techniques from the pseudorandomness literature to construct an efficient _hitting set generator_ (HSG)3 for the class of all conjunctions of literals. The HSG definition implies that there exists a seed on which the generator outputs a truth table that agrees with \(f\) on \(S\). The network \(\) computes any requested bit of that truth table.

_Remark 3.2_ (Dependence on \(d_{0}\)).: In Appendix E we show that the \(O(d_{0}^{2} N)\) term is nearly tight, yet it can be relaxed when using some closely related NN architectures. For example, with a single additional layer of width \((} N)\) with ternary weights in the first layer, _i.e.,_\(_{1}^{W}\!=\!\{-1,0,1\}\) instead of \(\{0,1\}\), the \(O(d_{0}^{2} N)\) term of Theorem 3.1 can be improved to \(O(d_{0}^{3/2} N+d_{0}^{3}N)\).

Next, with the bound on the dimensions of a NN implementing \(f\), we can bound the dimensions of a min-size interpolating NN by bounding the dimensions of a NN implementing the XOR of \(\) and \(h^{}\).

**Lemma 3.3** (XOR of two NNs).: _Let \(h_{1},h_{2}\) be two binary NNs with depths \(L_{1} L_{2}\) and widths \(^{(1)},^{(2)}\), respectively. Then, there exists a NN \(h\) with depth \(L_{} L_{2}+2\) and widths_

\[_{}(d_{1}^{(1)}+d_{1}^{(2)},\,, \,d_{L_{1}}^{(1)}+d_{L_{1}}^{(2)},\,d_{L_{1}+1}^{(2)}+1,\,,\,d_{L_{2}}^{ (2)}+1,\,2,\,1)\,,\]

_such that for all inputs \(\{0,1\}^{d_{0}}\), \(h()=h_{1}() h_{2}( )\)._

Combining Theorem 3.1 and Lemma 3.3 results in the following corollary.

**Corollary 3.4** (Memorizing a consistent dataset).: _For any teacher \(h^{}\) of depth \(L^{}\) and dimensions \(^{}\) and any consistent training set \(S\) generated from it, there exists an interpolating NN \(h\) (i.e., \(_{S}(h)=0\)) of depth \(L=\{L^{},14\}+2\) and dimensions \(\), such that the number of weights is_

\[w()  w(^{})+N H(_{S} (h^{}))+2n(^{})N^{3/4}H (_{S}(h^{}))^{3/4}N\] \[+O(d_{0}(d_{0}+n(^{} )) N)\]

_and the maximal width is_

\[_{}_{}^{}+N^{3/4} H( _{S}(h^{}))^{3/4}( N)+O(d_{0}(N))\,.\]

Proof idea.We explicitly construct a NN with the desired properties. We can choose a subset of neurons to implement the teacher NN and another subset to implement the NN memorizing the label flips. Furthermore, we zero the weights between the two subsets. Two additional layers compute the XOR of the outputs, thus yielding the labels as in (1). This is illustrated in Figure 2.

## 4 Tempered overfitting of min-size and random interpolators

In this section, we provide our main results on the overfitting behavior of interpolating NNs. We consider min-size NN interpolators and random NN interpolators. For both learning rules, we prove tempered overfitting. Namely, we show that the test performance of the learned interpolators is not much worse than the Bayes optimal error.

First, for the sake of readability, let us define the marginal peak probability of the distribution.

**Definition 4.1** (Peak marginal probability).: \(_{}_{}_{(X,Y) }(X=)\)_._

Our results in this section focus on cases where the number of training samples is \(N=(d_{0}^{2} d_{0})\) and \(N=o(1/_{}})\). In such regimes, the data consistency probability is high4 and our bounds are meaningful. Note that given the binarization of the data, \(N=o(1/_{}})\) implies an exponential upper bound of \(N=o(2^{d_{0}/2})\), achieved by the uniform distribution, _i.e.,_ when \(_{}=2^{-d_{0}}\). Due to the exponential growth of the sample space w.r.t. the input dimension, we find this assumption to be reasonable. Also, \(N=(d_{0}^{2} d_{0})\) implies that the input dimension cannot be arbitrarily large, but may still be non trivially small (see comparison to previous work in Section 5).

### Min-size interpolators

We consider min-size NN interpolators of a fixed depth, _i.e.,_ networks with the smallest number of weights for a certain depth that interpolate a given training set. In realizable settings, achieving good generalization performance by restricting the number of parameters in the learned interpolating model is a natural and well-understood approach. Indeed, in such cases, generalization follows directly from standard VC-dimension bounds . However, when interpolating _noisy_ data, the size of the returned model increases with the number of samples (in order to memorize the noise; see _e.g.,_ Vardi et al. ), making it challenging to guarantee generalization. In what follows, we prove that even when interpolating noisy data, min-size NNs exhibit good generalization performance.

Learning rule: Min-size NN interpolator.Given a consistent dataset \(S\) and a fixed depth \(L\), a min-size NN interpolator, or min-#weights interpolator, is a binary threshold network \(h\) (see Def. 2.1) that achieves \(_{S}(h)=0\) using a minimal number of weights. Recall that \(w()=_{l=1}^{L}d_{l}d_{l-1}\) and define the _minimal_ number of weights required to implement a given hypothesis \(h\),

\[w_{L}(h)_{^{L}}w( )\;\;h_{}^{}\,.\]

The learning rule is then defined as

\[A_{L}(S)*{argmin}_{h}w_{L}(h)\;\;_{S}(h)=0\,.\]

Figure 2: **Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label flips). Then, we “merge” these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher’s; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set.**

**Theorem 4.2** (Tempered overfitting of min-size NN interpolators).: _Let \(\) be a distribution induced by a noisy teacher of depth \(L^{}\), widths \(d^{}\), \(n(d^{})\) neurons, and a noise level of \(^{}<}{{2}}\) (Assumption 2.4). There exists \(c>0\) such that the following holds. Let \(S^{N}\) be a training set such that \(N=n(d^{})^{4}H(^{})^{ 3}(n(d^{}))^{c}+d_{0}^{2} d_{0}\) and \(N=o(_{}})\). Then, for any fixed depth \(L\{L^{},14\}+2\), the generalization error of the min-size depth-\(L\) NN interpolator satisfies the following._

* _Under arbitrary label noise,_ \[_{S}[_{}(A_{L}(S ))] 1-2^{-H(^{})}+o(1).\]
* _Under independent label noise,_ \[|_{S}[_{}(A_{L} (S))]-2^{}(1-^{} )|=o(1).\]

Here, \(o(1)\) indicates terms that become insignificant when the number of samples \(N\) is large. We illustrate these behaviors in Figure 1. Moreover, we discuss these results and the proof idea in Section 4.3 after presenting the corresponding results for posterior sampling. The complete proof with detailed characterization of the \(o(1)\) terms is given in Appendix F.1.

### Random NN interpolators (posterior sampling)

Recent empirical  and theoretical  works have shown that, somewhat surprisingly, randomly sampled deep NNs that interpolate a training set often generalize well. We now turn to analyzing such random interpolators under our teacher assumption and noisy labels (Assumption 2.4). As with min-size NN interpolators, our analysis here reveals a tempered overfitting behavior.

Prior distribution.A distribution over parameters induces a prior distribution over hypotheses by

\[(h)=_{}(h_{ }=h)\,.\]

We focus on the prior induced by the _uniform prior_ over the parameters of binary threshold networks. Specifically, for a fixed depth \(L\) and dimensions \(\), we consider \((^{}( ))\). In other words, to generate \(h\), each weight, bias, and neuron scalar in the NN is sampled independently and uniformly from its respective domain.

Learning rule: Posterior sampling.For any training set \(S\), denote the probability to sample an interpolating NN by \(p_{S}\ (_{S}(h)=0)\). When \(p_{S}>0\), define the posterior distribution \(_{S}\) as

\[_{S}(h)(h_{S} (h)=0)=(h)}{p_{S}}\{_{S}(h)=0\}\,.\] (2)

When \(p_{S}=0\), use an arbitrary \(_{S}\). Finally, the posterior sampling rule is \(A_{}(S)_{S}\).

_Remark 4.3_ (Hypothesis expressivity).: The following result requires that the student NN is large enough to interpolate _any_ consistent \(S\) (see Corollary 3.4), thus, \(p_{S}>0\) and \(_{S}\) is defined as in (2).

**Theorem 4.4** (Tempered overfitting of random NN interpolators).: _Let \(\) be a distribution induced by a noisy teacher of depth \(L^{}\), widths \(^{}\), \(n(^{})\) neurons, and a noise level of \(^{}<}{{2}}\) (Assumption 2.4). There exists a constant \(c>0\) such that the following holds. Let \(S^{N}\) be a training set such that \(N=n(d^{})^{4}(n(d^{}) )^{c}+d_{0}^{2} d_{0}\) and \(N=o(_{}})\). Then, for any student network of depth \(L\{L^{},14\}+2\) and widths \(^{L}\) holding_

\[ l=1,,L^{}-1 d_{l} d_{l}^{}+N^{3/4}(  N)^{c}+c d_{0}(N)\,,\] (3)

_the generalization error of posterior sampling satisfies the following._

* _Under arbitrary label noise,_ \[_{S,A_{}(S)}[_{ }(A_{}(S))] 1-2^{-H( ^{})}+O() (d_{}+d_{0})}{N})\,.\]
* _Under independent label noise,_ \[|_{S,A_{}(S)}[_{ }(A_{}(S))]-2^{ }(1-^{})| O()(d_{}+d_{0})}{N}})\,.\]The proof and a detailed description of the error terms are given in Appendix F.2.

Remarkably, note that the interpolating NN in the theorem might be highly overparameterized, and that for such NNs good generalization is not guaranteed by standard generalization bounds . This theorem complements a similar result by Buzaglo et al.  for the realizable setting.

### Discussion

The overfitting behaviors described in this section are illustrated in Figure 1.

Proof idea.We extend the information-theoretical generalization bounds from  to this paper's setting in which label collisions in the datasets have a non-zero probability. In particular, we bound the interpolator's complexity from below by the mutual information between the model and the training set. Since the model is interpolating, we can further bound the mutual information by a quantity dependent on the population error. From the other direction, we bound the model's complexity from above by (1) its size in the min-size setting of Section 4.1, and (2) by the negative log interpolation probability for the posterior sampling of Section 4.2. Together with Corollary 3.4 we obtain the bounds above on the expected generalization error.

In Figure 2 we illustrated the construction of a memorizing network used to bound the complexity of the min-size interpolator. In the following Figure 3 we illustrate how the interpolation probability \(p_{S}\) can be bounded to induce a meaningful generalization bound.

Following Remark 3.2, the assumption \(N=(d_{0}^{2} d_{0})\) can be relaxed in some related architectures. For example, with a single additional layer of width \(O(} N)\) and ternary weights in the first layer \(_{1}^{W}=\{-1,0,1\}\), the requirement can be relaxed to \(N=d_{0}^{3/2} d_{0}\).

_Remark 4.5_ (Higher weight quantization).: The bounds in the arbitrary noise setting can easily be extended to NNs with higher quantization levels. For example, letting \(_{l}^{W}\) such that \(_{l}^{W}=Q\) and \(\{0,1\}_{l}^{W}\), under the appropriate assumptions, we get that

\[_{(S,A(S))}[_{} (A(S))] 1-Q^{-H(^{})}\,,\]

which is a meaningful bound for noise levels \(^{}(Q)\) for some \((Q)<}{{2}}\).5 Tighter results would require utilizing the additional quantization levels to achieve smaller dimensions of the interpolating network, and are left to future work.

## 5 Related work

Benign and tempered overfitting.The benign overfitting phenomenon has been extensively studied in recent years. Previous works analyzed the conditions in which benign overfitting occurs in linear regression , kernel regression , and linear classification . Moreover, several works proved benign overfitting in classification using nonlinear NNs . All the aforementioned benign overfitting results require high-dimensional settings, namely, the input dimension is larger than the number of training samples.

Figure 3: **Interpolating a dataset with an overparameterized student.** We build on the construction from Figure 2 that memorizes a dataset using a subset of the parameters (blue, yellow, and red edges). Then, redundant neurons (gray) can be effectively ignored by setting their neuron scaling parameters (\(\)) to 0, leaving the redundant weights (gray edges) unconstrained. Thus, the interpolation probability \(p_{S}\) can be bounded by a quantity exponentially decaying in the number of neurons \(n()\) rather than in the number of weights \(w()=(N)\).

Mallinar et al.  suggested the taxonomy of benign, tempered, and catastrophic overfitting, which we use in this work. They demonstrated empirically that nonlinear NNs in classification tasks exhibit tempered overfitting. As mentioned in the introduction, our theoretical results for the independent noise case closely resemble these empirical findings (see Figure 1). Tempered overfitting in kernel ridge regression was theoretically studied in Mallinar et al. , Zhou et al. , Barzilai and Shamir . In univariate ReLU NNs (namely, for input dimension \(1\)), tempered overfitting was obtained for both classification  and regression . Manoj and Srebro  proved tempered overfitting for a learning rule returning short programs in some programming language. Finally, tempered overfitting is well understood for the \(1\)-nearest-neighbor learning rule, where the asymptotic risk is roughly twice the Bayes risk .

Circuit complexity.Theorem 3.1 (our NN for memorizing label flips) is in a similar spirit as several prior theorems in the area of _circuit complexity_. For example, Lupanov famously proved that every function \(f\{0,1\}^{d_{0}}\{0,1\}\) can be computed by a circuit consisting of \((1+o(1)) 2^{d_{0}}/d_{0}\) many AND/OR/NOT gates, where the AND/OR gates have fan-in two . Lupanov's bound, which is tight , is analogous to Theorem 3.1, because a NN can be considered a type of circuit.

Even more relevant is a line of work that analyzes the circuit complexity of an arbitrary partial function \(f\{0,1\}^{d_{0}}\{0,1,\}\) with a given domain size \(N\) and a given number of \(1\)-inputs \(N_{1}\), similar to the setup of Theorem 3.1. See Jukna's textbook for an overview [45, Section 1.4.2]. We highlight the work of Chashkin, who showed that every such function can be computed by a circuit (of unbounded depth and bounded fan-in) with \((1+o(1))}}{}}+O(d_{0})\) gates .

To the best of our knowledge, prior to our work, nothing analogous to Chashkin's theorem  was known regarding constant-depth threshold networks. It is conceivable that one could adapt Chashkin's construction  to the binary threshold network setting as a method of proving Theorem 3.1, but our proof of Theorem 3.1 uses a different approach. Our proof relies on shallow threshold networks computing _\(k\)-wise independent generators_ and an _error-reduction_ technique that was developed in the context of space-bounded derandomization , among other ingredients.

Memorization.Our construction shows how noisy data can be interpolated using a small threshold NN with binary weights. It essentially requires memorizing the noisy examples. The task of memorization, namely, finding a smallest NN that allows for interpolation of arbitrary data points, has been extensively studied in recent decades. Memorization of \(N\) arbitrary points in general position in \(^{d}\) with a two-layer NN can be achieved using \(O()\) hidden neurons . Memorizing arbitrary \(N\) points, even if they are not in general position, can be done using two-layer networks with \(O(N)\) neurons . With three-layer networks, \(O()\) neurons suffice, but the number of parameters is still linear in \(N\). Using deeper networks allows for memorization with a sublinear number of parameters . For example, memorization with networks of depth \(\) requires only \(()\) parameters . However, we note that in the aforementioned results, the number of quantization levels is not constant, namely, the number of bits in the representation of each weight depends on \(N\).6 Moreover, even in the sublinear constructions of , the number of bits required to represent the network is \((N)\). As a result, in this work we cannot rely on these constructions to obtain meaningful bounds.

Posterior sampling and guess and check.The generalization of random interpolating neural networks has previously been studied, both empirically and theoretically . Theisen et al.  studied the generalization of interpolating random linear and random features classifiers. Valle-Perez et al. , Mingard et al.  considered the Gaussian process approximation to random NNs which typically requires networks with infinite width. Buzaglo et al.  provided a method to obtain generalization results for quantized random NNs of general architectures -- possibly deep and with finite width, under the assumption of a narrow teacher model. A variant of this approach was used to prove our generalization results of posterior sampling, with the XOR network (Lemma 3.3) used in the role of the teacher.

Extensions, limitations, and future work

In this work, we focused on binary (fully connected) threshold networks of depth \(L 16\) (Section 2.1) with binary input features (Section 2.2), for which we were able to derive nontrivial generalization bounds.

Our results can be extended with simple modifications to derive bounds in other settings. For instance, to NNs with higher weight quantization (see Remark 4.5), or to ReLU networks (since any threshold network with binary weights can be computed by a not-much-larger ReLU network with a constant quantization level). Unfortunately, without more sophisticated arguments these extensions result in looser generalization bounds. The "bottleneck" of our approach is the reliance on (nearly) tight bounds on the widths of interpolating NNs.

Extending the results to other architectures (_e.g.,_ CNNs or fully connected without neuron scaling) and other quantization schemes (_e.g.,_ floating point representations) will mainly require utilizing their specific structure to derive tighter bounds on the complexity (_e.g.,_ number of weights or number of bits) needed to interpolate consistent datasets. Furthermore, our bounds require the depth of the networks to be at least \(16\), and the width to be \((N^{3/4})\), which might be deemed impractical for real datasets.7 The key to alleviating these requirements is, again, obtaining tighter complexity results.

Our paper focused on consistent training sets (Def. 2.5), in order to allow perfect interpolation. Realistically, models do not always perfectly interpolate the training set, and therefore it is interesting to find generalization bounds for non-interpolating models, depending on the training error. In addition, it is interesting to relate the generalization to the training _loss_, and not just to the training accuracy. Such extensions will require either broadening our generalization results or deriving new ones.