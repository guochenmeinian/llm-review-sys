# Coherent Soft Imitation Learning

Joe Watson\({}^{*}\)\({}^{}\)\({}^{}\)

&Sandy H. Huang\({}^{@sectionsign}\)

&Nicolas Heess\({}^{@sectionsign}\)

\({}^{@sectionsign}\) **Google DeepMind**

London, United Kingdom

{shhuang,heess}@google.com

Corresponding author. Work done during an internship at Google DeepMind.

\({}^{}\)**TU Darmstadt**

Darmstadt, Germany

joe@robot-learning.de

\({}^{}\)**Systems AI for Robot Learning**

German Research Center for AI

dfki.de

###### Abstract

Imitation learning methods seek to learn from an expert either through behavioral cloning (BC) for the policy or inverse reinforcement learning (IRL) for the reward. Such methods enable agents to learn complex tasks from humans that are difficult to capture with hand-designed reward functions. Choosing between BC or IRL for imitation depends on the quality and state-action coverage of the demonstrations, as well as additional access to the Markov decision process. Hybrid strategies that combine BC and IRL are rare, as initial policy optimization against inaccurate rewards diminishes the benefit of pretraining the policy with BC. This work derives an imitation method that captures the strengths of both BC and IRL. In the entropy-regularized ('soft') reinforcement learning setting, we show that the behavioral-cloned policy can be used as both a shaped reward and a critic hypothesis space by inverting the regularized policy update. This _coherency_ facilitates fine-tuning cloned policies using the reward estimate and additional interactions with the environment. Our approach conveniently achieves imitation learning through initial behavioral cloning and subsequent refinement via RL with online or offline data sources. The simplicity of the approach enables graceful scaling to high-dimensional and vision-based tasks, with stable learning and minimal hyperparameter tuning, in contrast to adversarial approaches. For the open-source implementation and simulation results, see joemwatson.github.io/csil.

## 1 Introduction

Imitation learning (IL) methods  provide algorithms for learning skills from expert demonstrations in domains such as robotics , either through direct mimicry (behavioral cloning, bc) or inferring the latent reward (inverse reinforcement learning, irl) assuming a Markov decision process (mdp) . While bc is straightforward to implement, it is susceptible to covariate shift during evaluation . irl overcomes this problem but tackles the more complex task of jointly learning

Figure 1: By using regression rather than classification, our approach infers a shaped reward from expert data for this contextual bandit problem, whereas classifiers require additional non-expert data and may struggle to resolve the difference between expert (\(\)) and non-expert (\(\)) samples.

a reward and policy from interactions. So far, these two approaches to imitation have been largely separated , as optimizing the policy with an evolving reward estimate counteracts the benefit of pre-training the policy . Combining the objectives would require careful weighting during learning and lacks convergence guarantees . The benefit of combining bc and irl is sample-efficient learning that enables any bc policy to be further improved using additional experience.

**Contribution.** Our il method naturally combines bc and irl by using the perspectives of entropy-regularized reinforcement learning (rl) [13; 14; 15; 16; 17; 18; 19] and gradient-based irl [20; 21]. Our approach uses a bc policy to define an estimate of a shaped reward for which it is optimal, which can then be used to finetune the policy with additional knowledge, such as online interactions, offline data, or a dynamics model. Using the cloned policy to specify the reward avoids the need for careful regularization and hyperparameter tuning associated with adversarial imitation learning [11; 22], and we estimate a shaped variant of the _true_ reward, rather than use a heuristic proxy, e.g., [23; 24]. In summary,

1. We use entropy-regularized rl to obtain a reward for which a behavioral-cloned policy is optimal, which we use to improve the policy and overcome the covariate shift problem.
2. We introduce approximate stationary stochastic process policies to implement this approach for continuous control by constructing specialized neural network architectures.
3. We show strong performance on online and offline imitation for high-dimensional and image-based continuous control tasks compared to current state-of-the-art methods.

## 2 Background & related work

A Markov decision process is a tuple \(,,,r,,_{0}\), where \(\) is the state space, \(\) is the action space, \(:^{+}\) is the transition model, \(r:\) is the reward function, \(\) is the discount factor, and \(_{0}\) is the initial state distribution \(_{0}_{0}()\). When evaluating a policy \(\), we use the occupancy measure \(_{}(,)()\,_{}()\), \(_{}()_{t=0}^{}\,^{t}_{t}^{}()\) where \(_{t+1}^{}(^{})=(^{}, {a})\,()\,_{t}^{}()\,\,\). This measure is used to compute infinite-horizon discounted expectations, \(_{,_{}}[f(,)]=_{ _{t+1}(,_{t}),\,_{t}( _{t})}[_{t=0}^{}^{t}f(_{t},_{t})]\). Densities \(d_{}(,)=(1-)\,_{}(,)\) and \(_{}()=(1-)\,_{}()\) are normalized measures. When the reward is unknown, imitation can be performed from a dataset \(\) of transitions \((,,^{})\) as demonstrations, obtained from the discounted occupancy measure of the expert policy \(_{*}=_{}_{,_{}}[r(,)]\). The policy could be inferred directly or by inferring a reward and policy jointly, referred to as behavioral cloning and inverse reinforcement learning respectively.

Behavioral cloning.The simplest approach to imitation is to directly mimic the behavior using regression , \(_{}l(,)+\,(),\) with some loss function \(l\), hypothesis space \(\) and regularizer \(\). This method is effective with sufficient data coverage and an appropriate hypothesis space but suffers from compounding errors and cannot improve unless querying the expert . The mimic-exp algorithm of Rajaraman et al.  shows that the optimal no-interaction policy matches bc.

Inverse reinforcement learning.Rather than just inferring the policy, irl infers the reward and policy jointly and relies on access to the underlying mdp[26; 25]. irl iteratively finds a reward for which the expert policy is optimal compared to the set of possible policies while also finding the policy that maximizes this reward function. The learned policy seeks to match or even improve upon the expert using additional data beyond the demonstrations, such as environment interactions . To avoid repeatedly solving the inner rl problem, one can consider the game-theoretic approach , where the optimization problem is a two-player zero-sum game where the policy and reward converge to the saddle point minimizing the 'apprenticeship error' \(_{}_{r}\,_{}[r]-_ {_{}}[r]\). This approach is attractive as it learns the policy and reward concurrently . However, in practice, saddle-point optimization can be challenging when scaling to high dimensions and can exhibit instabilities and hyperparameter sensitivity . Compared to bc, irl methods in theory scale to longer task horizons by overcoming compounding errors through environment interactions .

Maximum entropy inverse reinforcement learning & soft policy iteration.irl is an under-specified problem , in particular in regions outside of the demonstration distribution. An effective way to address this is _causal entropy_ regularization , by applying the principle of maximum entropy  to irl (me-irl). Moreover, the entropy-regularized formulation has an elegant closed-form solution . The approach can be expressed as an equivalent constrained minimum relative entropy problem for policy \(q()\) against a uniform prior \(p()=_{}()\) using the KullbackLeibler (kl) divergence and a constraint matching expected features \(\), with Lagrangian

\[_{q,}_{}_{q}()\,_{}[q( {a}) p()]\,+^{}( _{,}[(,)]-_{,_{q}}[(,)]).\] (1)

The constraint term can be interpreted as the apprenticeship error, where \(^{}(,)=r_{}(,)\). Solving Equation 1 using dynamic programming yields the'soft' Bellman equation ,

\[(,)\!=\!r_{}(,)+_{ ^{}(,)}[_{} (^{})],\,_{}()\!=\!\!\!_{ }\!\!((,))p ()\,,\] (2)

for temperature \(=1\). Using Jensen's inequality and importance sampling, this target is typically replaced with a lower-bound , which has the same optimum and samples from the optimized policy rather than the initial policy like many practical deep rl algorithms :

\[(,)\!=\!r_{}(,)+\,_{ ^{} q(^{}),\,^{}(,)}[(^{},^{})- ( q(^{}\!^{})- p(^{ }\!^{}))].\] (3)

The policy update blends the exponentiated advantage function 'pseudo-likelihood' with the prior, as a form of regularized Boltzmann policy  that resembles a Bayes posterior ,

\[q_{}()(^{-1}((, )-_{}())\,p().\] (4)

These regularized updates can also be used for rl, where it is the solution to a kl-regularized rl objective, \(_{q}_{,_{q}}[(,)]- \,_{}[q(\!) p(\! )]\), where the temperature \(\) now controls the strength of the regularization. This regularized policy update is known as soft-  or posterior policy iteration  (spi, ppi), as it resembles a Bayesian update. In the function approximation setting, the update is performed in a variational fashion by minimizing the reverse kl divergence between the parametric policy \(q_{}\) and the critic-derived update at sampled states \(\),

\[_{} =_{}_{}[_{}[q_{}() q_{}( )]]=_{}_{}(),\] \[_{}() =_{ q_{}(),\,}[(,)-( q_{}(\!)- p())].\] (5)

The above objective \(_{}\) can be maximized using reparameterized gradients and minibatches from replay buffer \(\). A complete derivation of spi and me-irl is provided in Appendix K.1.

Gradient-based inverse reinforcement learning.An alternative irl strategy is a gradient-based approach (girl) that avoids saddle-point optimization by learning a reward function such that the bc policy's policy gradient is zero , which satisfies first-order optimality. However, this approach does not remedy the ill-posed nature of irl. Moreover, the Hessian is required for the sufficient condition of optimality , which is undesirable for policies with many parameters.

Related work.Prior state-of-the-art methods have combined the game-theoretic irl objective  with entropy-regularized rl. These methods can be viewed as minimizing a divergence between the expert and policy, and include gail, airl, \(f\)-max, dac, valuedice, iqlearn and proximal point imitation learning (ppi, ). These methods differ through their choice of on-policy policy gradient (gail) or off-policy actor-critic (dac, iqLearn, ppi), and also how the minimax optimization is implemented, e.g., using a classifier (gail, dac, airl), implicit reward functions (valuedice, iqLearn) or Lagrangian dual objective (ppi). Alternative approaches to entropy-regularized il use the Wasserstein metric (pwil) , labelling of sparse proxy rewards (squl) , feature matching , maximum likelihood  and matching state marginals  to specify the reward. Prior works at the intersection of irl and bc include policy matching , policy-based gail classifiers , annealing between a bc and gail policy  and discriminator-weighted bc. Our approach is inspired by gradient-based irl, which avoids the game-theoretic objective and instead estimates the reward by analyzing the policy update steps with respect to the bc policy. An entropy-regularized girl setting was investigated in the context of learning from policy updates . For further discussion on related work, see Appendix A.

## 3 Coherent imitation learning

For efficient and effective imitation learning, we would like to combine the simplicity of behavioral cloning with the structure of the mdp, as this would provide a means to initialize and refine the imitating policy through interactions with the environment. In this section, we propose such a method using a girl-inspired connection between bc and irl in the entropy-regularized rl setting. By inverting the entropy-regularized policy update in Equation 4, we derive a reward for which the behavioral-cloning policy is optimal, a quality we refer to as _coherence_.

Inverting soft policy iteration and reward shaping.The \(\)-regularized closed-form policy based on Equation 4 can be inverted to provide expressions for the critic and reward (Theorem 1).

**Theorem 1**.: _(KL-regularized policy improvement inversion). Let \(p\) and \(q_{}\) be the prior and pseudo-posterior policy given by posterior policy iteration (Equation 4). The critic can be expressed as_

\[(,)=()}{p( )}+_{}(),_{}( {s})=_{}((, ))p(|)\,.\] (6)

_Substituting into the KL-regularized Bellman equation lower-bound from Equation 2,_

\[r(,)=()}{p( {s})}+_{}()-\,_{^{} (,)}[_{}(^{ })].\] (7)

_The \(_{}()\) term is the'soft' value function. We assume \(q_{}()=0\) whenever \(p()=0\)._

For the proof, see Appendix K.2. Jacq et al.  and Cao et al.  derived a similar inversion but for a maximum entropy policy iteration between two policies, rather than ppi between the prior and posterior. These expressions for the reward and critic are useful as they can be viewed as a valid form of shaping (Lemma 1) , where \(( q()- p())\) is a shaped 'coherent' reward.

**Lemma 1**.: _(Reward shaping, Ng et al. ). For a reward function \(r:\) with optimal policy \(\), for a bounded state-based potential function \(:\), a reward function \(\) shaped by the potential function satisfying \(r(,)=(,)+()-\,_{ {s}^{}(,)}[(^{})]\) has a shaped critic \(}\) satisfying \((,)=}(,)+()\) and the same optimal policy as the original reward._

**Definition 1**.: _The shaped 'coherent' reward and critic are derived from the log policy ratio by combining Lemma 1 and Theorem 1, with value function \(_{}()\) as the potential \(()\). When policy \(q_{}(|)\) models the data \(\) while matching its prior \(p\) otherwise, the density ratio should exhibit the following shaping_

\[(,)=()}{p()}\,\{ 0,,\\ <0,,\\ =0,..\]

_In a continuous setting, the learned policy should capture this shaping approximately (Figure 2)._

Coherent soft imitation learning (csl, Algorithm 1) uses the bc policy to initialize the coherent reward and uses this reward to improve the policy further with additional interactions outside of \(\). However, for the coherent reward to match Definition 1, the policy \(q()\) needs to match the policy prior \(p()\) outside of the data distribution. To achieve this, it is useful to recognize the policy \(q()\) as a 'pseudo-posterior' of prior \(p()\) and incorporate this Bayesian view into the bc step.

**Imitation learning with pseudo-posteriors.** The 'pseudo-posteriors' formalism  is a generalized way of viewing the policy derived for me-irl and kl-regularized rl in Equation 4.

**Definition 2**.: _Pseudo-posteriors are solutions to kl-constrained or minimum divergence problems with an additional scalar objective or vector-valued constraint term and Lagrange multipliers \(\),_

\[_{q} _{ q()}[f()]-(_{}[q() p()]-)  q_{}()(^{-1}\,f())\,p().\] \[_{q} _{}[q() p()]-^{}(_{ q()}[f()]-^{*})  q_{}()(^{}())\,p( ).\]

_The \((^{}())\) term is an unnormalized 'pseudo-likelihood', as it facilities Bayesian inference to solve a regularized optimization problem specified by \(\)._

Optimizing the policy distribution, the top objective captures kl-regularized rl, where \(f\) is the return, and the regularization is implemented as a constraint with bound \(\) or soft penalty with constant \(\). The bottom objective is the form seen in me-irl (Equation 1), where \(\) is the feature space that defines the reward model. Contrasting these pseudo-posterior policies against bc with Gaussian processes, e.g., , we can compare the Bayesian likelihood used for regression with the critic-based pseudo-likelihood obtained from imitation learning. The pseudo-likelihoods in entropy-regularized il methods produce an effective imitating policy by incorporating the mdp and trajectory distribution because \(f\) captures the cumulative reward, compared to just the action prediction error typically captured by bc regression likelihoods. This point is expanded in Appendix B.

Figure 2: The coherent reward from Figure 1, made using a stationary Gaussian process policy, extended out-of-distribution. The reward approximates the shaping described in Definition 1.

Behavior cloning with pseudo-posteriors.Viewing the policy \(q()\) as a pseudo-posterior has three main implications when performing behavior cloning and designing the policy and prior:

1. We perform conditional density estimation to maximize the posterior predictive likelihood, rather than using a data likelihood, which would require assuming additive noise .
2. We require a hypothesis space \(p(,)\), e.g., a tabular policy or function approximator, that is used to define both the prior and posterior through weights \(\).
3. We need a fixed weight prior \(p()\) that results in a predictive distribution that matches the desired policy prior \(p()= p(,)p()\, \ \).

These desiderata are straightforward to achieve for maximum entropy priors in the tabular setting, where count-based conditional density estimation is combined with the prior, e.g., \(p()=_{}()\). Unfortunately, point 3 is challenging in the continuous setting when using function approximation, as shown in Figure 3. However, we can adopt ideas from Gaussian process theory to approximate such policies, which is discussed further in Section 4. The learning objective combines maximizing the likelihood of the demonstrations w.r.t. the predictive distribution, as well as kl regularization against the prior weight distribution, performing regularized heteroscedastic regression ,

\[_{}_{,}[ q_{}()]-\,_{}[q_{} () p()],\ q_{}()= p( ,)\,q_{}()\,.\] (8)

This objective has been used to fit parametric Gaussian processes with a similar motivation .

We now describe how this bc approach and the coherent reward are used for imitation learning (Algorithm 1) when combined with soft policy iteration algorithms, such as sac.

On a high level, Algorithm 1 can be summarized by the following steps,

1. Perform regularized bc on the demonstrations with a parametric stochastic policy \(q_{}()\).
2. Define the coherent reward, \(r_{}(,)=( q_{}() - p())\), with temperature \(\).
3. With temperature \(<\), perform spi to improve on the bc policy with the coherent reward.

The reduced temperature \(\) is required to improve the policy after inverting it with temperature \(\).

This coherent approach contrasts combining bc and prior actor-critic irl methods, where learning the reward and critic from scratch can lead to 'unlearning' the initial bc policy . Moreover, while our method does not seek to learn the true underlying reward, but instead opts for one shaped by the environment used to generate demonstrations, we believe this is a reasonable compromise in practice. Firstly, csil uses irl to tackle the covariate shift problem in bc, as the reward shaping (Definition 1) encourages returning to the demonstration distribution when outside it, which is also the goal of prior imitation methods, e.g., . Secondly, in the entropy-regularized setting, an irl method has the drawback of requiring demonstrations generated from two different environments or discount factors to accurately infer the true reward , which is often not readily available in practice, e.g., when teaching a single robot a task. Finally, additional mlps could be used to estimate the true (unshaped) reward from data  if desired. We also provide a divergence minimization perspective of csil in Section J of the Appendix. Another quality of csil is that it is conceptually simpler than alternative irl approaches such as airl. As we simply use a shaped estimate of the true reward with entropy-regularized rl, csil inherits the theoretical properties of these regularized algorithms  regarding performance. As a consequence, this means that analysis of the imitation quality requires analyzing the initial behavioral cloning procedure.

Partial expert coverage, minimax optimization, and regularized regression.The reward shaping theory from Lemma 1 shows that an advantage function can be used as a shaped reward function when the state potential is a value function. By inverting the soft policy update, the log policy ratio acts as a form of its advantage function. However, in practice, we do not have access to the complete advantage function but rather an estimate due to finite samples and partial state coverage. Suppose the expert demonstrations provide only partial coverage of the state-action space. In this case, the role of an inverse reinforcement learning algorithm is to use additional knowledge of the mdp, e.g., online samples, a dynamics model, or an offline dataset, to improve the reward estimate. As csil uses the log policy ratio learned only from demonstration data, how can it be a useful shaped reward estimate? We show in Theorem 2 that using entropy regularization in the initial behavioral cloning step plays a similar role to the saddle-point optimization in game-theoretic irl.

**Theorem 2**.: _(Coherent inverse reinforcement learning as kl-regularized behavioral cloning). A kl-regularized game-theoretic irl objective, with policy \(q_{}()\) and coherent reward parameterization \(_{}(,)=( q_{ }()- p())\) where \( 0\), is lower bounded by a scaled kl-regularized behavioral cloning objective and a constant term when the optimal policy, posterior, and prior share a hypothesis space \(p(,)\) and finite parameters \(\),_

\[_{,}[ r_{}(,)]-_{  q_{}(),\, {s}_{q_{}}()}[r_{}( ,)-( q_{}()- p())]\] \[(_{, }[ q_{}()]-\,_{}[q_{}( ) p()]+_{, }[ p() ]).\]

_where \(=(-)/\) and \(_{,}[ p( )]\) is constant. The regression objective bounds the irl objective for a worst-case on-policy state distribution \(_{q}()\), which motivates its scaling through \(\). If \(\) has sufficient coverage, no rl finetuning or kl regularization is required, so \(=\) and \(=0\). If \(\) does not have sufficient coverage, then let \(<\) so \(>0\) to regularize the BC fit and finetune the policy with rl accordingly with additional soft policy iteration steps._

The proof is provided in Appendix K.3. In the tabular setting, it is straightforward for the cloned policy to reflect the prior distribution outside of the expert's data distribution. However, this behavior is harder to capture in the continuous setting where function approximation is typically adopted.

## 4 Stationary processes for continuous control policies

Maximum entropy methods used in reinforcement learning can be recast as a minimum relative entropy problem against a regularizing prior policy with a uniform action distribution, i.e., a prior \(p()=_{}()\ \ \), where \(_{}\) is the uniform distribution over \(\). Achieving such a policy in the tabular setting is straightforward, as the policy can be updated independently for each state. However, such a policy construction is far more difficult for continuous states due to function approximation capturing correlations between states. To achieve the desired behavior, we can use stationary process theory (Definition 3) to construct an appropriate function space (Figure 3).

**Definition 3**.: _(Stationary process, Cox and Miller ). A process \(f:\) is stationary if its joint distribution in \(\), \(p(_{1},,_{n})\), is constant w.r.t. \(_{1},,_{n}\) and all \(n_{>0}\)._

To approximate a stationary policy using function approximation, Gaussian process (gp) theory provides a means using features in a Gaussian linear model that defines a stationary process . To approximate such feature spaces using neural networks, this can be achieved through a relatively wide final layer with a periodic activation function (\(f_{}\)), which can be shown to satisfy the stationarity property . Refer to Appendix C for technical details. To reconcile this approach with prior work such as sac, we use the predictive distribution of our Gaussian process in lieu of a network directly

Figure 3: For the coherent reward to be effective, the stochastic policy should return to the prior distribution outside of the data distribution (left). The typical heteroscedastic parametric policies have undefined out-of-distribution behavior and typically collapse to the action limits due to the network extrapolation and tanh transformation (middle). By approximating stationary Gaussian processes, we can design policies that exhibit the desired behavior with minimal network modifications (right).

predicting Gaussian moments. The policy is defined as \(=(()),\) where \(()=().\) The weights are factorized row-wise \(=[_{1},,_{d_{a}}]^{},\ _{i}=(_{i},_{i})\) to define a gp with independent actions. Using change-of-variables like sac, the policy is expressed per-action as

\[q(a_{i})=(z_{i};_{i}^{}(),\ ()^{}_{i}\ ())|(a_{i}}{z_{ i}})|^{-1},()=f_{}(}_{}()).\]

\(}\) are weights drawn from a distribution, e.g., a Gaussian, that also characterizes the stochastic process and \(_{}\) is an arbitrary mlp. We refer to this heteroscedastic stationary model as hetstat.

Function approximation necessitates several additional practical implementation details of csil.

Approximating and regularizing the critic.Theorem 1 and Lemma 1 show that the log policy ratio is also a shaped critic. However, we found this model was not expressive enough for further policy evaluation. Instead, the critic is approximated using as a feedforward network and pre-trained after the policy using SARSA  and the squared Bellman error. For coherency, a useful inductive bias is to minimize the critic Jacobian w.r.t. the expert actions to approximate first-order optimality, i.e., \(_{}_{,}[_{} _{}(,)],\) as an auxiliary critic loss. For further discussion, see Section H in the Appendix. Pre-training and regularization are ablated in Figures 3(c) and 3(d) in the Appendix.

Using the cloned policy as prior.While a uniform prior is used in the csil reward throughout, in practice, we found that finetuning the cloned policy with this prior in the soft Bellman equation (Equation 3) leads to divergent policies. Replacing the maximum entropy regularization with KL regularization against the cloned policy, i.e., \(_{}[q_{}() q_{_{1}}()],\) mitigated divergent behavior. This regularization still retains a maximum entropy effect if the bc policy behaves like a stationary process. This approach matches prior work on kl-regularized lrl from demonstrations, e.g., .

'Faithful' heteroscedastic regression loss.To fit a parametric pseudo-posterior to the expert dataset, we use the predictive distribution for conditional density estimation  using heteroscedastic regression . A practical issue with heteroscedastic regression with function approximators is the incorrect modeling of data as noise . This can be overcome with a 'faithful' loss function and modelling construction , which achieves the desired minimization of the squared error in the mean and fits the predictive variance to model the residual errors. For more details, see Appendix D.

Refining the coherent reward.The hetstat network we use still approximates a stationary process, as shown in Figure 3. To ensure spurious reward values from approximation errors are not exploited during learning, it can be beneficial to refine, in a minimax fashion, the coherent reward with the additional data seen during learning. This minimax refinement both reduces the stationary approximation errors of the policy, while also improving the reward from the game-theoretic irl perspective. For further details, intuition and ablations, see Appendix E, G and N.6 respectively.

Algorithm 1 with these additions can be found summarized in Algorithm 2 in Appendix I. An extensive ablation study of these adjustments can be found in Appendix N.

## 5 Experimental results

We evaluate csil against baseline methods on tabular and continuous state-action environments. The baselines are popular entropy-regularized imitation learning methods discussed in Section 2. Moreover, ablation studies are provided in Appendix N for the experiments in Section 5.2 and 5.3

 mdp & Variant & Expert & bc & Classifier & me-irl & gail & iqlearn & pil & csil \\  Dense & Nominal & 0.266 & 0.200 & 0.249 & 0.251 & 0.253 & 0.244 & 0.229 & **0.257** \\  & Windy & 0.123 & 0.086 & 0.103 & **0.111** & 0.105 & 0.104 & 0.104 & 0.107 \\ Sparse & Nominal & 1.237 & 1.237 & **1.237** & 1.131 & **1.237** & **1.237** & **1.237** & **1.237** \\  & Windy & 0.052 & 0.002 & **0.044** & 0.036 & 0.043 & 0.002 & 0.002 & **0.044** \\  

Table 1: Inverse optimal control, combining spi and known dynamics, in tabular mdps. The ‘dense’ mdp has a uniform initial state distribution and four goal states. The ‘sparse’ mdp has one initial state, one goal state and one forbidden state. The agents are trained on the nominal mdp. The ‘windy’ mdp has a random disturbance across all states to evaluate the robustness of the policy. csil performs well across all settings despite being the simpler algorithm relative to the irl baselines.

### Tabular inverse optimal control

We consider inverse optimal control in two tabular environments to examine the performance of csil without approximations. One ('dense') has a uniform initial distribution across the state space and several goal states, while the other ('sparse') has one initial state, one goal state, and one forbidden state with a large negative reward. Table 1 and Appendix M.1 shows the results for a nominal and a 'window' environment, which has a fixed dynamics disturbance to test the policy robustness. The results show that csil is effective across all settings, especially the sparse environment where many baselines produce policies that are brittle to disturbances and, therefore, covariate shift. Moreover, csil produces value functions similar to gail (Figures 23 and 27) while being a simpler algorithm.

### Continuous control from agent demonstrations

A standard benchmark of deep imitation learning is learning MuJoCo  Gym  and Adroit  tasks from agent demonstrations. We evaluate online and offline learning, where a fixed dataset is used in lieu of environment interactions . Acme was used to implement csil and baselines, and expert data was obtained using rlds from existing sources . Returns are normalized with respect to the reported expert and random performance  (Table 2).

Online imitation learning.In this setting, we used dac (actor-critic gail), iQLearn, ppil, sqil, and pwl as entropy-regularized imitation learning baselines. We evaluate on the standard benchmark of locomotion-based Gym tasks, using the sac expert data generated by Orsini et al.. In this setting, Figures 4 and 28 show csil closely matches the best baseline performance across environments and dataset sizes. We also evaluate on the Adroit environments, which involve manipulation tasks with a complex 27-dimensional robot hand . In this setting, Figures 5 and 29 show that saddle-point methods struggle due to the instability of the optimization in high dimensions without careful regularization and hyperparameter selection . In contrast, csil is very effective, matching or surpassing bc, highlighting the benefit of coherency for both policy initialization and improvement. In the Appendix, Figure 29 includes sac from demonstrations (sacfd)  as an oracle baseline with access to the true reward. csil exhibits greater sample efficiency than sacfd, presumably due to the bc initialization and the shaped reward, and often matches final performance. Figures 38, 39 and 40 in the Appendix show an ablation of iqlearn and ppl with bc pre-training. We observe a fast unlearning of the bc policy due to the randomly initialized rewards, which was also observed by Orsini et al., so any initial performance improvement is negligible or temporary.

Figure 4: Normalized performance of csil against baselines for online imitation learning for MuJoCo Gym tasks. Uncertainty intervals depict quartiles over ten seeds. To assess convergence across seeds, performance is chosen using the highest 25th percentile of the episode return during learning.

Figure 5: Normalized performance of csil against baselines for online imitation learning for Adroit tasks. Uncertainty intervals depict quartiles over ten seeds. To assess convergence across seeds, performance is chosen using the highest filtered 25th percentile of the return during learning.

Offline imitation learning.Another challenging setting for deep rl is _offline_ learning, where the online interactions are replaced with a static'supplementary' dataset. Note that prior works have used 'offline' learning to describe the setting where only the demonstrations are used for learning , like in bc, such that the ll method becomes a form of regularized bc. Standard offline learning is challenging primarily because the critic approximation can favor unseen actions, motivating appropriate regularization (e.g., ). We use the full-replay datasets from the d4rl benchmark  of the Gym locomotion tasks for the offline dataset, so the offline and demonstration data is not the same. We also evaluate smodice and demodice, two offline imitation learning methods which use state-based value functions, weighted bc policy updates, and a discriminator-based reward. We opt not to reproduce them in acme and use the original implementations. As a consequence, the demonstrations are from a different dataset, but are normalized appropriately. For further details on smodice and demodice, see Appendix A and L.

Figures 6 and 30 demonstrate that offline learning is significantly harder than online learning, with no method solving the tasks with few demonstrations. This can be attributed to the lack of overlap between the offline and demonstration data manifesting as a sparse reward signal. However, with more demonstrations, csl can achieve reasonable performance. This performance is partly due to the strength of the initial bc policy, but csl can still demonstrate policy improvement in some cases (Figure 40). Figure 30 includes cql as an oracle baseline, which has access to the true reward function. For some environments, csl outperforms cql with enough demonstrations. The demodice and smodice baselines are both strong, especially for few demonstrations. However, performance does not increase so much with more demonstrations. Since csl could also be implemented with a state-based value function and a weighed bc policy update (e.g., ), further work is needed to determine which components matter most for effective offline imitation learning.

### Continuous control from human demonstrations from states and images

As a more realistic evaluation, we consider learning robot manipulation tasks such as picking (Lift), pick-and-place (PickPlaceCan), and insertion (NutAssemblySquare) from random initializations and mixed-quality human demonstrations using the robomimic datasets , which also include image observations. The investigation of Mandlekar et al.  considered offline rl and various forms of bc with extensive model selection. Instead, we investigate the applicability of imitation learning, using online learning with csl as an alternative to bc model selection. One aspect of these tasks is that they have a sparse reward based on success, which can be used to define an absorbing

Figure 6: Normalized performance of csl against baselines for offline imitation learning for Gym tasks. Uncertainty intervals depict quartiles over ten seeds. To assess convergence across seeds, performance is chosen using the highest filtered 25th percentile of the episode return during learning.

Figure 7: Average success rate over 50 evaluations for online imitation learning for robomimic tasks. Uncertainty intervals depict quartiles over ten seeds. To assess convergence across seeds, performance is chosen using the highest 25th percentile of the averaged success during learning.

state. As observed in prior work , in practice there is a synergy between absorbing states and the sign of the reward, where positive rewards encourage survival and negative rewards encourage minimum-time strategies. We found that csil was initially ill-suited to these goal-oriented tasks as the rewards are typically positive. In the same way that the airl reward can be designed for a given sign , we can design negative csil rewards by deriving an upper-bound of the log policy ratio and subtracting it in the reward. For further details, see Appendix F. An ablation study in Appendix N.9 shows that the negative csil reward matches or outperforms a constant negative reward, especially when given fewer demonstrations. These tasks are also more difficult due to the variance in the initial state, requiring much larger function approximators than in Section 5.2.

Figures 7 and 36 show the performance of csil and baselines, where up to 200 demonstrations are required to sufficiently solve the tasks. csil achieves effective performance and reasonable demonstration sample efficiency across all environments, while baselines such as dac struggle to solve the harder tasks. Moreover, Figure 36 shows csil is again more sample efficient than sacfd.

Figure 8 shows the performance on csil when using image-based observations. A shared cnn torso is used between the policy, reward and critic, and the convolutional layers are frozen after the bc stage. csil was able to solve the simpler tasks in this setting, matching the model selection strategy of Mandlekar et al. , demonstrating its scalability. In the offline setting, Figure 37 shows state-based results with sub-optimal ('mixed human') demonstrations as supplementary dataset. While some improvement could be made on simpler tasks, on the whole it appears much harder to learn from suboptimal human demonstrations offline. This supports previous observations that human behaviour can be significantly different to that of rl agents, such that imitation performance is affected [11; 84].

## 6 Discussion

We have shown that 'coherency' is an effective approach to il, combining bc with irl-based finetuning by using a shaped learned reward for which the bc policy is optimal. We have demonstrated the effectiveness of csil empirically across a range of settings, particularly for high-dimensional tasks and offline learning, due to csil leveraging bc for initializing the policy and reward.

In Figure 35 of the Appendix, we investigate why baselines iqLearn and ppiL, both similar to csil, struggle in the high-dimensional and offline settings. Firstly, the sensitivity of the saddle-point optimization can be observed in the stability of the expert reward and critic values during learning. Secondly, we observe that the policy optimization does not effectively minimize the bc objective by fitting the expert actions. This suggests that these methods do not necessarily converge to a bc solution and lack the 'coherence' quality that csil has that allows it to refine bc policies.

A current practical limitation of csil is understanding when the initial bc policy is viable as a coherent reward. Our empirical results show that csil can solve complex control tasks using only one demonstration, where its bc policy is ineffective. However, if a demonstration-rich bc policy cannot solve the task (e.g., image-based NutAssemblySquare), csil also appears to be unable to solve the task. This suggests there is scope to improve the bc models and learning to make performance more consistent across more complex environments. An avenue for future work is to investigate the performance of csil with richer policy classes beyond mlps, such as recurrent architectures and multi-modal action distributions, to assess the implications of the richer coherent reward and better model sub-optimal human demonstrations.

Figure 8: Average success rate over 50 evaluations for image-based online imitation learning for robomimic tasks. Uncertainty intervals depict quartiles over five seeds. Baseline results obtained from Mandlekar et al. . bc (csil) denotes the performance of csil’s initial policy for comparison.