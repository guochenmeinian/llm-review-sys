# Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators

Benedikt Alkin \({}^{1,2}\) Andreas Furst \({}^{1}\) Simon Schmid \({}^{3}\) Lukas Gruber \({}^{1}\)

Markus Holzleitner \({}^{4}\) Johannes Brandstetter \({}^{1,2}\)

\({}^{1}\) ELLIS Unit Linz, Institute for Machine Learning, JKU Linz, Austria

\({}^{2}\) NXAI GmbH, Linz, Austria

\({}^{3}\) Software Competence Center Hagenberg GmbH, Hagenberg, Austria

\({}^{4}\) MaLGa Center, Department of Mathematics, University of Genoa, Italy

{alkin, fuerst, brandstetter}@ml.jku.at

###### Abstract

Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics. Project page: https://ml-jku.github.io/UPT

## 1 Introduction

In scientific pursuits, extensive efforts have produced highly intricate mathematical models of physical phenomena, many of which are naturally expressed as partial differential equations (PDEs) . Solving most PDEs is analytically intractable and necessitates falling back on compute-expensive numerical approximation schemes. In recent years, deep neural network based surrogates, most importantly neural operators [51; 61; 47], have emerged as a computationally efficient alternative [99; 119], and impact e.g., weather forecasting [48; 8; 1], molecular modeling [7; 4], or computational fluid dynamics [107; 30; 51; 43; 31; 19]. Additional to computational efficiency, neural surrogates offer potential to introduce generalization capabilities across phenomena, as well as generalization across characteristics such as boundary conditions or PDE coefficients [66; 14]. Consequently, the nature of neural operators inherently complements handcrafted numerical solvers which are characterized by a substantial set of solver requirements, and mostly due to these requirements tend to differ among sub-problems .

However, similar to their numerical counterparts, different neural network techniques are prevalent across applications. For example, when contrasting particle- and grid-based dynamics in computational fluid dynamics (CFD), i.e., Lagrangian and Eulerian discretization schemes. This is in contrast to other areas of deep learning where the flexibility of transformers  has enabled unified architectures across domains, allowing advancements in one domain to also benefit all others. This has lead to an efficient scaling of architectures, paving the way for large "foundation" models  that are pretrained on huge passive datasets [25; 36].

We introduce Universal Physics Transformers (UPTs), an efficient and unified neural operator learning paradigm with strong focus on scalibility over a wide range of spatio-temporal problems. UPTs flexibly encode different grids, and/or different number of particles into a compressed latent space representation which facilitates scaling to large-scale simulations. Latent space rollouts are enforced by inverse encoding and decoding surrogates, leading to fast simulated trajectories which is particularly important for large systems. For decoding, the latent representation can be evaluated at any point in space-time. UPTs operate without grid- or particle-based latent structures, and demonstrate the beneficial scaling-behavior of transformer backbone architectures. Figure 1 sketches the UPT modeling paradigm.

We summarize our contributions as follows: (i) we introduce the UPT framework for efficiently scaling neural operators; (ii) we formulate encoding and decoding schemes such that dynamics can be propagated efficiently in a compressed and fixed-size latent space; (iii) we demonstrate applicability on diverse applications, putting a strong research focus on the scalability of UPTs.

## 2 Background

Partial differential equations.We focus on experiments on (systems of) PDEs, that evolve a signal \((t,)=^{t}()^{d}\) in a single temporal dimension \(t[0,T]\) and \(m\) spatial dimensions \( U^{m}\), for an open set \(U\). With \(1 l\), systems of PDEs of order \(l\) can be written as

\[(D^{l}^{t}(),,D^{1}^{t}(), }{ t^{l}}^{t}(),,^{t}(),^{t}(),,t)=, U,\,t[0,T]\;,\] (1)

where \(\) is a mapping to \(^{d}\), and for \(i=1,...,l\), \(D^{i}\) denotes the differential operator mapping to all \(i\)-th order partial derivatives of \(\) with respect to the spacial variable \(\), whereas \(}{ t^{l}}\) outputs the corresponding time derivative of order \(i\). Any \(l\)-times continuously differentiable \(:[0,T] U^{d}\) fulfilling the relation Eq. (1) is called a _classical solution_ of Eq. (1). Also other notions of solvability (e.g. in the sense of weak derivatives/distributions) are possible, for sake of simplicity we do not go into details here. Additionally, initial conditions specify \(^{t}()\) at time \(t=0\) and boundary conditions \(B[^{t}]()\) at the boundary of the spatial domain.

Figure 1: Schematic sketch of the UPT learning paradigm. UPTs flexible encode different grids, and/or different number of particles into a unified latent space representation, and subsequently unroll dynamics in the latent space. The latent space is kept at a fixed size to ensure scalability to larger systems. UPTs decode the latent representation at any query point.

We work mostly with the incompressible Navier-Stokes equations , which e.g., in two spatial dimensions conserve the velocity flow field \((t,x,y):[0,T]^{2}^{2}\) via:

\[}{ t}=-+^{2}-  p+\,,=0\,\] (2)

where \(\) is the convection, i.e., the rate of change of \(\) along \(\), \(\) is the viscosity parameter, \(^{2}\) the viscosity, i.e., the diffusion or net movement of \(\), \( p\) the internal pressure gradient, and \(\) an external force. The constraint \(=0\) yields mass conservation of the Navier-Stokes equations. A detailed depiction of the involved differential operators is given in Appendix B.1.

Operator learning.Operator learning  learns a mapping between function spaces - a concept which is often used to approximate solutions of PDEs. Similar to Kovachki et al. , we assume \(,\) to be Banach spaces of functions on compact domains \(^{d_{x}}\) or \(^{d_{y}}\), mapping into \(^{d_{u}}\) or \(^{d_{x}}\), respectively. The goal of operator learning is to learn a ground truth operator \(:\) via an approximation \(}:\). This is usually done in the vein of supervised learning by i.i.d. sampling input-output pairs, with the notable difference, that in operator learning the spaces sampled from are not finite dimensional. More precisely, with a given data set consisting of \(N\) function pairs \((_{i},_{i})=(_{i},(_{i}))\), \(i=1,...N\), we aim to learn \(}:\), so that \(\) can be approximated in a suitably chosen norm.

In the context of PDEs, \(\) can e.g. be the mapping from an initial condition \((0,)=^{0}()\) to the solutions \((t,)=^{t}()\) of Eq. (1) at all times. In the case of classical solutions, if \(U\) is bounded, \(\) can then be chosen as a subspace of \(C(,^{d})\), the set of continuous functions from domain \(\) (the closure of \(U\)) mapping to \(^{d}\), whereas \( C([0,T],^{d})\), so that \(\) or \(\) consist of all \(l\)-times continuously differentiable functions on the respective spaces. In case of weak solutions, the associated spaces \(\) and \(\) can be chosen as Sobolev spaces.

We follow the popular approach to approximate \(\) via three maps : \(} \ \). The encoder \(:^{h_{1}}\) takes an input function and maps it to a finite dimensional latent feature representation. For example, \(\) could embed a continuous function to a chosen hidden dimension \(^{h_{1}}\) for a collection of grid points. Next, \(:^{h_{1}}^{h_{2}}\) approximates the action of the operator \(\), and \(\) decodes the hidden representation, and thus creates the output functions via \(:^{h_{2}}\), which in many cases is point-wise evaluated at the output grid or output mesh.

Particle vs. grid-based methods.Often, numerical simulation methods can be classified into two distinct families: particle and grid-based methods. This specification is notably prevalent, for instance, in the field of computational fluid dynamics (CFD), where Lagrangian and Eulerian discretization schemes offer different characteristics dependent on the PDEs. In simpler terms, Eulerian schemes essentially monitor velocities at specific fixed grid points. These points, represented by a spatially limited number of nodes, control volumes, or cells, serve to discretize the continuous space. This process leads to grid-based or mesh-based representations. In contrast to such grid- and mesh-based representations, in Lagrangian schemes, the discretization is carried out using finitely many material points, often referred to as particles, which move with the local deformation of the continuum. Roughly speaking, there are three families of Lagrangian schemes: discrete element methods , material point methods , and smoothed particle hydrodynamics (SPH) . In this work, we focus on SPH methods, which approximate the field properties using radial kernel interpolations over adjacent particles at the location of each particle. The strength of SPH lies in its ability to operate without being constrained by connectivity issues, such as meshes. This characteristic proves especially beneficial when simulating systems that undergo significant deformations.

Latent space representation of neural operators.For larger meshes or larger number of particles, memory consumption and inference speed become more and more important. Fourier Neural Operator (FNO) based methods work on regular grids, or learn a mapping to a regular latent grid, e.g., geometry-informed neural operators (GINO) . In three dimensions, the stored Fourier modes have the shape \(h n_{x} n_{y} n_{z}\), where \(h\) is the hidden size and \(n_{x}\), \(n_{y}\), \(n_{z}\) are the respective Fourier modes. Similarly, the latent space of CNN-based methods, e.g., Raonic et al. , Gupta & Brandstetter , is of shape \(h w_{x} w_{y} w_{z}\), where \(w_{x}\), \(w_{y}\), \(w_{z}\) are the respective grid points. In three dimension, the memory requirement in each layer increases cubically with increasing number of modes or grid points. In contrast, transformer based neural operators, e.g., Hao et al. , Cao , Li et al. , operate on a token-based latent space of dimension \(n_{} h\), where usually \(n_{} n_{}\), and GNN based neural operators, e.g., Li et al. , operate on a node based latent space of dimension \(n_{} h\), where usually \(n_{}=n_{}\). For large number of inputs, this becomes infeasible as every layer has to process a large number of tokens. Contrary, UPTs compress the inputs into a low-dimensional latent space, which drastically decreases computational requirements. Different architectures and their scaling limits are compared in Fig. 2.

## 3 Universal Physics Transformers

**Problem formulation**. Our goal is to learn a mapping between the solutions \(^{t}\) and \(^{t^{}}\) of Eq. (1) at timesteps \(t\) and \(t^{}\), respectively. Our dataset should consist of \(N\) function pairs \((^{t}_{i},^{t^{}}_{i})\), \(i=1,..,N\), where each \(^{t}_{i}\) is sampled at \(k\) spatial locations \(\{^{1}_{i},,^{k}_{i}\} U\). Similarly, we query each output signal \(}^{t^{}}_{i}\) at \(k^{}\) spatial locations \(\{^{1}_{i},,^{k^{}}_{i}\} U\). Then each input signal can be represented by \(^{t}_{i,k}=(^{t}_{i}(^{1}_{i}),,^{t}_{i}(^ {k}_{i}))^{T}^{k d}\) as a tensor of shape \(k d\), similar for the output. For particle- or mesh-based inputs, it is often simpler to represent the input as graph \(=(V,E)\) with \(k\) nodes \(\{^{1}_{i},^{k}_{i}\} V\), edges \(E\) and node features \(\{^{t}_{i}(^{1}_{i}),,^{t}_{i}(^{k}_{i})\}\).

**Architecture desiderata**. We want Universal Physics Transformers (UPTs) to fulfill the following set of desiderata: (i) an encoder \(\), which flexibly encodes different grids, and/or different number of particles into a unified latent representation of shape \(n_{} h\), where \(n_{}\) is the chosen number of tokens in the latent space and \(h\) is the hidden dimension; (ii) an approximator \(\) and a training procedure, which allows us to forward propagate dynamics purely within the latent space without mapping back to the spatial domain at each operator step; and iii) a decoder \(\) that queries the latent representation at different locations. The UPT architecture is schematically sketched in Fig. 3.

Figure 3: Left: UPT compresses information from various grids or differing particles with an encoder, propagates this information forward in time through the approximator and decodes at arbitrary query positions. Right: Training procedure to enable latent rollouts via inverse encoding/decoding losses.

Figure 2: Qualitative exploration of scaling limits. Starting from 32K input points (scale 1), we train a 68M parameter model for a few steps with batchsize \(1\) and measure the required GPU memory. Models without a compressed latent space (GNN, Transformer) quickly reach their limits while models with a compressed latent space (GINO, UPT) scale much better with the number of inputs. However, as GINO compresses the latent space onto a regular grid, the scaling benefits are largely voided on 3D problems. The efficient latent space compression of UPTs can fit up to 4.2M points (scale 128). We use a linear attention transformer  for this study. “Disc. Conv.” denotes “Discretization Convergent”. Appendix D.7 outlines implementation details and complexities.

**Encoder**. The goal of the encoder \(\) is to compress the input signal \(_{i}^{t}\), which is represented by a point cloud \(_{i,k}^{t}\). Importantly, the encoder should learn to selectively focus on important parts of the input. This is a desirable property as, for example, in many computational fluid dynamics simulations large areas are characterized by laminar flows, whereas turbulent flows tend to occur especially around obstacles. If \(k\) is large, we employ a hierarchical encoder.

The encoder \(\) first embeds \(k\) points into hidden dimension \(h\), adding position encoding  to the different nodes, i.e., \(_{i,k}^{t}^{k d}^{k h}\). In the first hierarchy, information is exchanged between local points and a selected set of \(n_{}\) supernode points. For Eulerian discretization schemes those supernodes can either be uniformly sampled on a regular grid as in , or selected based on the given mesh. The latter has the advantage that mesh characteristics are automatically taken into account, e.g., dense or sparse mesh regions are represented by different numbers of nodes. Furthermore, adaptation to new meshes is straightforward. We implement the first hierarchy by randomly selecting \(n_{}\) supernodes on the mesh, choosing \(n_{}\) such that the mesh characteristic is preserved. Similarly, in the Lagrangian discretization scheme, choosing supernodes based on particle positions provides the same advantages as selecting them based on the mesh.

Information is aggregated at the selected \(n_{}\) supernodes via a message passing layer  using a radius graph between points. Importantly, messages only flow towards the \(n_{}\) supernodes, and thus the compute complexity of the first hierarchy scales linearly with \(n_{}\). The second hierarchy consists of transformer blocks  followed by a perceiver block  with \(n_{}\) learned queries of dimension \(h\). To summarize, the encoder \(\) maps \(_{i}^{t}\) to a latent space via

\[:_{i}^{t} }_{i,k}^{t}^{k  d}}^{k h}}^{n_{s} h}\] \[}^{n_{s} h} }_{i}^{t}^{n_{} h}\;,\]

where typically \(n_{} n_{}<k\). If the number of points is small, the first hierarchy can be omitted.

Note that randomly sampling mesh cells or particles implicitly encodes the underlying mesh or particle density and allocates more supernodes to highly resolved areas in the mesh or densely populated particle regions. Therefore, this can be seen as an implicit "importance sampling" of the underlying simulation. Additional implementation details are provided in Appendix F.1.

This encoder design projects into a fixed size latent space as it is an efficient way to compress the input into a fixed size representation to enable scaling to large-scale systems while remaining compute efficient. However, if an application requires a variable sized latent space, one could also remove the perceiver pooling layer. With this change the number of supernodes is equal to the number of latent tokens and complex problems could be tackled by a larger supernode count.

**Approximator**. The approximator propagates the compressed representation forward in time. As \(n_{}\) is small, forward propagation in time is fast. We employ a transformer as approximator.

\[:_{i}^{t}^{n_{} h} _{i}^{t^{}}^{n_{} h }\;.\]

Notably, the approximator can be applied multiple times, propagating the signal forward in time by \( t\) each time. If \( t\) is small enough, the input signal can be approximated at arbitrary future times \(t^{}\).

**Decoder**. The task of decoder \(\) is to query the latent representation at \(k^{}\) arbitrary locations to construct the prediction of the output signal \(_{i}^{t^{}}\) at time \(t^{}\). More formally, given the output positions \(\{_{i}^{1},,_{i}^{k^{}}\} U\) at \(k^{}\) spatial locations and the latent representation \(_{i}^{t^{}}\), the decoder predicts the output signal \(_{i,k^{}}^{t^{}}=(_{i}^{t^{}}(_{i}^{1}), ,_{i}^{t^{}}(_{i}^{k^{}}))^{T}\) at these spatial locations at timestep \(t^{}\),

\[:(_{i}^{t^{}},\{_{i}^{1},,_{i}^{k^{ }}\})}_{i,k^{}}^{t^{}}^{k^ {} d}\;.\]

The decoder is implemented via a perceiver-like cross attention layer using a positional embedding of the output positions as query and the latent representation \(_{i}^{t^{}}\) as keys and values. Since there is no interaction between queries, the latent representation can be queried at arbitrarily many positions without large computational overhead. This decoding mechanism establishes a connection of conditioned neural fields to operator learning .

**Model Conditioning**. To condition the model to the current timestep \(t\) and to boundary conditions such as the inflow velocity, we add feature modulation to all transformer and perceiver blocks. We use DiT modulation , which consists of a dimension-wise scale, shift and gate operation that areapplied to the attention and MLP module of the transformer. Scale, shift and gate are dependent on an embedding of the timestep and boundary conditions (e.g. velocity).

**Training procedure**. UPTs model the dynamics fully within a latent representation, such that during inference only the initial state of the system \((0,)=^{0}()\) is encoded into a latent representation \(^{0}\). From there on, instead of autoregressively feeding the decoder's prediction into the encoder, UPTs propagate \(^{0}\) forward in time to \(^{t^{}}\) through iteratively applying the approximator \(\) in the latent space. We call this procedure _latent rollout_. Especially for large meshes or many particles, the benefits of latent space rollouts, i.e. fast inference, pays off.

To enable latent rollouts, the responsibilities of encoder \(\), approximator \(\) and decoder \(\) need to be isolated. Therefore, we invert the encoding and decoding by means of two reconstruction losses during training as visualized in Fig. 3. First, an inverse encoding is performed, wherein the input \(_{i}^{t}\) is reconstructed from the encoded latent state \(_{i}^{t}\) by querying it with the decoder at \(k\) input locations \(\{_{i}^{1},,_{i}^{k}\}\). Second, we invert the decoding by reconstructing the latent state \(_{i}^{t^{}}\) from the output signal \(}_{i}^{t^{}}\) at \(k^{}\) spatial locations \(\{_{1}^{1},,_{i}^{k^{}}\}\). Using two reconstruction losses, the encoder is forced to focus on encoding a state \(_{i}^{t}\) into a latent representation \(^{t}\), and similarly the decoder is forced to focus on making predictions out of a latent representation \(^{t^{}}\).

**Related methods**. The closest work to ours are transformer neural operators of Cao , Li et al. , Hao et al.  which encode different query points into a tokenized latent space representation of dimension \(n_{} h\), where \(n_{}\) varies based on the number of input points, i.e., \(n_{} n_{}\). Wu et al.  adds a learnable mapping into a fixed latent space of dimension \(n_{} h\) to each transformer layer, and projects back to dimension \(n_{} h\) after self-attention. In contrast, UPTs use fixed \(n_{}\) for the unified latent space representation \(n_{} h\).

For the modeling of temporal PDEs, a common scheme is to map the input solution at time \(t\) to the solution at next time step \(t^{}\)[51; 13; 95]. Especially for systems that are modeled by graph-based representations, predicted accelerations at nodes are numerically integrated to model the time evolution of the system [87; 81]. Recently, equivariant graph neural operators  were introduced which model time evolution via temporal convolutions in Fourier space. More related to our work are methods that propagate dynamics in the latent space [50; 111]. Once the system is encoded, time evolution is modeled via LSTMs , or even linear propagators [63; 73]. In Li et al. , attention-based layers are used for encoding the spatial information of the input and query points, while time updates in the latent space are performed using recurrent MLPs. Similarly, Bryutkin et al.  use recurrent MLPs for temporal updates within the latent space, while utilizing a graph transformer for encoding the input observations.

Building universal models aligns with the contemporary trend of foundation models for science. Recent works comprise pretraining over multiple heterogeneous physical systems, mostly in the form of PDEs , foundation models for weather and climate , or material modeling [67; 118; 4].

Methods similar to our latent space modeling have been proposed in the context of diffusion models  where a pre-trained compression model is used to compress the input into a latent space from which a diffusion model can be trained at much lower costs. Similarly, our approach also compresses the high-dimensional input into a low-dimensional latent space, but without a two stage approach. Instead, we learn the compression end-to-end via inverse encoding and decoding techniques.

## 4 Experiments

We ran experiments across different settings, assessing three key aspects of UPTs: (i) **Effectiveness of the latent space representation**. We test on steady state flow simulations in 3D, comparing against methods that use regular grid representations, and thus considerably larger latent space representations. (ii) **Scalability**. We test on transient flow simulations on large meshes. Specifically, we test the effectiveness of latent space rollouts, and assess how well UPTs generalize across different flow regime, and different domains, i.e., different number of mesh points and obstacles. (iii) **Lagrangian dynamics modeling**. Finally, we assess how well UPTs model the underlying field characteristics when applied to particle-based simulations. We outline the most important results in the following sections and provide implementation details and additional results in Appendix D. Most notably, UPT also compares favorably against baseline regular grid methods on regular grid datasets D.3.

### Steady state flows

For steady state prediction, we consider the ShapeNet-Car dataset generated by . It consists of 889 car shapes from ShapeNet , where each car surface is represented by 3.6K mesh points in 3D space. We randomly create a train/test split containing 700/189 samples. We regress the pressure at each surface point with a mean-squared error (MSE) loss and sweep hyperparameters per model. Due to the small scale of this dataset, we train the largest possible model that is able to generalize the best. Training even larger models resulted in a performance decrease due to overfitting. We optimize the model size for all methods where the best mesh based models (GINO, UPT) contain around 300M parameters. The best regular grid based models (U-Net [86; 31], FNO ) are significantly smaller and range from 15M to 100M. Additional details are listed in Appendix D.4.

ShapeNet-Car is a small-scale dataset. Consequently, methods that map the mesh onto a regular grid can employ grids of _extremely_ high resolution, such that the number of grid points is orders of magnitude higher than the number of mesh points. For example, a grid resolution of 64 points per spatial dimension results in 262K grid points, which is 73x the number of mesh points. As UPT is designed to operate directly on the mesh, we compare at different grid resolutions.

UPT is able to outperform models on smaller resolutions, for example UPT with only 64 latent tokens achieves a test MSE of 2.31 whereas GINO with resolution \(48^{3}\) (110K tokens) achieves only 2.58 while taking significantly more runtime and memory. When additionally using feature engineering in the form of a signed distance function and \(64^{3}\) grid resolution UPT achieves a competitive performance of 2.24 compared to 2.14 of GINO while remaining efficient. All test errors are multiplied by 100. UPT achieves a favorable cost-vs-performance tradeoff where the best GINO models requires 900 seconds per epoch whereas a only slightly worse (-0.17) UPT model takes only 4 seconds. We show a comprehensive table with all results in Appendix Tab. 4.

### Transient flows

We test the scalability of UPTs on large-scale transient flow simulations. For this purpose, we self-generate 10K Navier-Stokes simulations within a pipe flow using the pisoFoam solver from OpenFOAM , which we split into 8K training, 1K validation and 1K test trajectories. For each simulation, between one and four objects (circles of variable size) are placed randomly within the pipe flow, and the uni-directional inflow velocity varies between 0.01 to 0.06 m/s. The simulation is carried out for 100s resulting in 100 timesteps that are used for training neural surrogates. Note that pisoFoam requires much smaller \( t\) to remain stable (between 2K and 200K timesteps for 100s of simulation time). We use an adaptive meshing algorithm which results in 29K to 59K mesh points. Further dataset details are outlined in Appendix D.5.

Figure 4: Example rollout trajectories of the UPT-68M model, visually demonstrating the efficacy of UPT physics modeling. The UPT model is trained across different obstacles, different flow regimes, and different mesh discretizations. Interestingly, the absolute error might suggest that UPT trajectories diverge, although physics are still simulated faithfully. This stems from subtle shifts in predictions throughout the rollout duration, likely attributed to the point-wise decoding of the latent field.

Model-wise, UPT uses the hierarchical encoder setup with all optional components depicted in Fig. 3. A message passing layer aggregates local information into \(n_{}=2048\) randomly selected supernodes, a transformer processes the supernodes and a perceiver pools the supernodes into \(n_{}=512\) latent tokens. Approximator and decoder are unchanged. We compare UPT against GINO, U-Net and FNO. For U-Net and FNO, we interpolate the mesh onto a regular grid. We condition the models onto the current timestep and inflow velocity by modulating features within the model. We employ FiLM conditioning for U-Net , the "Spatial-Spectral" conditioning method introduced in  for FNO and GINO, and DiT for UPT . Implementation details are provided in Appendix D.5.

We train all models for 100 epochs and evaluate test MSE as well as rollout performance for which we use the number of timesteps until the Pearson correlation of the rollout drops below 0.8 as evaluation metric . We do not employ any techniques to stabilize rollouts . The left side of Fig. 5, shows that UPTs outperform compared methods by a large margin. Training even larger models becomes increasingly expensive and is infeasible for our current computational budget. UPT-68M and GINO-68M training takes roughly 450 A100 hours. UPT-8M and UPT-17M take roughly 150 and 200 A100 hours, respectively. Figure 4 shows a rollout and Appendix D.5.1 presents additional ones. We also study out-of-distribution generalization (e.g. more obstacles) in Appendix D.5.6.

While one would ideally use lots of supernodes and query the latent space with all positions during training, increasing those quantities increases training costs and the performance gains saturate. Therefore, we only use 2048 supernodes and 16K randomly selected query positions during training. We investigate discretization convergence in the right part of Fig. 5 where we vary the number of input/output points and the number of supernodes. We use the 68M models _without_ any retraining, i.e., we test models on "discretization convergence" as, during training, the mesh was discretized into 2048 supernodes and 16K query positions. UPT generalizes across a wide range of different number of input or output positions, with even slight performance increases when using more input points. Similarly, using more supernodes increases performance slightly. Additionally, we investigate training with more supernodes and/or more latent tokens in a reduced setting in Appendix D.5.3.

Finally, we evaluate training with inverse encoding and decoding techniques, see Fig. 3. We investigate the impact of the latent rollout by training our largest model - a 68M UPT. The latent rollout achieves on par results to autoregressively unrolling via the physics domain, but speeds up the inference significantly as shown in Tab. 1. However, in its current implementation the latent rollout requires a non-negligible overhead during training. We discuss this limitation in Appendix A.

### Lagrangian fluid dynamics

Scaling particle-based methods such as discrete element methods or smoothed particle hydrodynamics to 10 million or more particles presents a significant challenge [117; 9], yet it also opens a distinctive opportunity for neural surrogates. Such systems are far beyond the scope of this work. We however present a framing of how to model such systems via UPTs such that the studied scaling properties

Figure 5: Left and middle: MSE and correlation time on the testset. UPTs outperform compared methods on all model scales by a large margin. Right: We study discretization convergence by varying the number of input/output points or the number of gridpoints/supernodes of models that were trained on inputs between 8K and 24K points, 8K target points. UPT demonstrates a stable performance across different number of input/outputs even though it has never seen that number of input/output points during training. We study discretization convergence of supernodes in App. D.5.2, where UPT also shows a steady improvement when more supernodes are used during inference. Additionally, we study smaller UPT models and training on less data in Appendix D.5.4 and D.5.5.

of UPTs could be exploited. In order to do so, we demonstrate how UPTs capture inherent field characteristics when applied to Lagrangian SPH simulations, as provided in LagrangeBench . Here, GNNs, such as Graph Network-based Simulators (GNS)  and Steerable E(3) Equivariant Graph Neural Networks (SEGNNs)  are strong baselines, where predicted accelerations at the nodes are numerically integrated to model the time evolution of the particles. In contrast, UPTs learn underlying dynamics without dedicated particle-structures, and propagate dynamics forward without the guidance of numerical time integration schemes. An overview of the conceptual differences between GNS/SEGNN and UPTs is shown in Fig. 6.

We use the Taylor-Green vortex dataset in three dimensions (TGV3D). The TGV system was introduced as a test scenario for turbulence modeling . It is an unsteady flow of a decaying vortex, displaying an exact closed form solution of the incompressible Navier-Stokes equations in Cartesian coordinates. We note that the TGV3D dataset models the same trajectories but does so by tracking particles using SPH to solve the equations. Formulating the TGV system as a UPT learning problem allows the same trajectories to be queried at different positions, enabling the recovery of the complete velocity field, whereas GNNs can only evaluate the velocities of the particles. Consequently, the evaluation against GNNs should be viewed as an illustration of the efficacy of UPTs in learning field characteristics, rather than a comprehensive GNN versus UPT comparison. More details and experiments on the 2D version of the Taylor-Green vortex dataset (TGV2D) are in Appendix D.6.

For UPT training, we input two consecutive velocities of the particles in the dataset at timesteps \(t\) and \(t-1\), and the respective particle positions. We regress two consecutive velocities at a later timesteps \(\{t^{}-1,t^{}\}=\{t+ T-1,t+ T\}\) with mean-squared error (MSE) objective. For all experiments we use \( T=10 t\). We query the decoder to output velocities at target positions. UPTs encode the first two velocities of a trajectory, and autoregressively propagate dynamics forward in the latent space. We report the Euclidean norm of velocity differences across all \(k\) particles. Figure 7 compares the rollout performance of GNS, SEGNN and UPT and shows the speedup of both methods compared to the SPH solver. The results demonstrate that UPTs effectively learn the underlying field dynamics.

## 5 Discussion

**Potential for extreme scale**. UPTs consist of mainly transformer  layers which allows application of the same scaling and parallelism principles as are commonly used for training extreme-scale language models. For example, the recent work Llama 3  trained on up to 16K H100 GPUs.

 Model & Time on 16 CPUs & Time on 1 GPU & Speedup \\   pisoFoam & 120s & - & 1x \\ GINO-68M (autoreg.) & 48s & 1.2s & 100x \\ UPT-68M (autoreg.) & 46s & 2.0s & 60x \\ UPT-68M (latent) & 3s & **0.3s** & **400x** \\ 

Table 1: Required time to simulate a full trajectory rollout. UPT and GINO are orders of magnitude faster than traditional finite volume solvers. The latent rollout is additionally more than 5x faster than an autoregressive rollout via the physics domain. Neural surrogate models are also faster on CPUs as traditional solvers require extremely small timescales to remain stable (\( t 0.05\) vs. \( t=1\)).

Figure 6: Conceptual difference between GNS/SEGNN on the left and UPT on the right side. GNS/SEGNN predicts the acceleration of a particle which is then integrated to calculate the next position. UPTs directly model the velocity field and allow for large timestep predictions.

While we do not envision that we train UPT on such a massive scale in the foreseeable future, the used techniques can be readily applied to UPTs.

**Benefits of the latent rollout.** While the latent rollout does not provide a significant performance improvement, it is almost an order of magnitude faster. The UPT framework allows to trade-off training compute vs inference compute. If inference time is crucial for a given application, one can train UPT with the inverse encoding and decoding objectives, requiring more training compute but greatly speeding up inference. If inference time is not important, one can simply train UPT without the reconstruction objectives to reduce training costs.

Additionally, the latent rollout enables applicability to Lagrangian simulations. As UPT models the underlying field instead of tracking individual particle positions it does not have access to particle locations at inference time. Therefore, autoregressive rollouts are impossible since the encoder requires particle positions as input. When using the latent rollout, it is sufficient to know the initial particle positions as dynamics are propagated without any spatial positions. After propagating the latent space forward in time, the latent space can be queried at arbitrary positions to evaluate the underlying field at given positions. We show this by querying with regular grid positions in Figure 7.

## 6 Conclusion

We have introduced Universal Physics Transformers (UPTs) framework for efficiently scaling neural operators, demonstrating its applicability to a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and number of particles. The UPT training procedure separates responsibilities between components, allowing a forward propagation in time purely within the latent space. Finally, UPTs allow for queries of the latent space representation at any point in space-time.