# Coherence-free Entrywise Estimation of Eigenvectors

in Low-rank Signal-plus-noise Matrix Models

 Hao Yan

Department of Statistics

University of Wisconsin-Madison

Madison, WI 53706

United States of America

hyan84@wisc.edu

&Keith Levin

Department of Statistics

University of Wisconsin-Madison

Madison, WI 53706

United States of America

kdlevin@wisc.edu

###### Abstract

Spectral methods are widely used to estimate eigenvectors of a low-rank signal matrix subject to noise. These methods use the leading eigenspace of an observed matrix to estimate this low-rank signal. Typically, the entrywise estimation error of these methods depends on the coherence of the low-rank signal matrix with respect to the standard basis. In this work, we present a novel method for eigenvector estimation that avoids this dependence on coherence. Assuming a rank-one signal matrix, under mild technical conditions, the entrywise estimation error of our method provably has no dependence on the coherence under Gaussian noise (i.e., in the spiked Wigner model), and achieves the optimal estimation rate up to logarithmic factors. Simulations demonstrate that our method performs well under non-Gaussian noise and that an extension of our method to the case of a rank-\(r\) signal matrix has little to no dependence on the coherence. In addition, we derive new metric entropy bounds for rank-\(r\) singular subspaces under \(_{2,}\) distance, which may be of independent interest. We use these new bounds to improve the best known lower bound for rank-\(r\) eigenspace estimation under \(_{2,}\) distance.

## 1 Introduction

Spectral methods are extensively used in contemporary data science and engineering . The fundamental idea underlying these methods is that the eigenspace or singular subspace of an observed matrix reflects important structure present in the data from which it is derived. Spectral methods have been deployed successfully in a variety of tasks, including low-rank matrix denoising , factor analysis , community detection , pairwise ranking  and matrix completion . The widespread use of spectral methods has driven extensive research into the theoretical properties of eigenspaces and singular subspaces, yielding normal approximation results  as well as perturbation bounds . For a more comprehensive recent review of spectral methods, see .

### Eigenspace estimation in low-rank matrix models

Consider an unknown symmetric matrix \(^{}=^{}^{}^{}^{n n}\), where \(^{}^{n r}\) has orthonormal columns and \(^{}^{r r}\) is diagonal, containing the nonzero eigenvalues of \(^{}\) ordered so that \(|_{1}^{}||_{2}^{}||_{r}^{ }|>0\). Our goal is to estimate \(^{}\) from a noisy observation

\[=^{}+^{n n},\] (1)where \(=[W_{ij}]_{1 i,j n}\) is a symmetric random noise matrix with mean zero. We restrict our attention here to the symmetric case for the sake of simplicity, but we expect that our results can be extended to the asymmetric case using standard dilation arguments .

Throughout this paper, we assume that the entries of \(\) are subgaussian.

**Assumption 1**.: _The entries of \(\) on and above the diagonal are independent and symmetric about zero with common variance \(^{2}\) and common subgaussian parameter \(_{W}\)._

We remind the reader that the subgaussian parameter \(_{W}\) serves as a "proxy" for the variance. Indeed, in the Gaussian case, we have \(^{2}=_{W}\), while \(^{2}_{W}\) more generally .

Spectral methods often estimate \(^{}\) directly using the \(r\) leading eigenvectors \(\) of \(\). As a result, the entrywise and row-wise behavior of \(\) has attracted considerable attention . Given an estimator \(}^{n r}\), the estimation error is measured in terms of the \(_{2,}\) distance

\[d_{2,}(},^{})=_{_ {r}}\|^{}-}\|_{2,},\] (2)

where the presence of \(\) is to resolve rotational non-identifiability. In the rank-one case, this reduces to the \(_{}\) distance,

\[d_{}(},^{})=\{\|^{}- }\|_{},\|^{}+}\|_{}\}\] (3)

for \(^{}^{n}\) and a given estimator \(}^{n}\). Estimation error bounds in \(_{}\) or \(_{2,}\) distance typically rely on the incoherence parameter \(\) of \(^{}\), defined as

\[=\|^{}\|_{2,}^{2}[1,n/r].\]

In the rank-one case with Gaussian noise, if the leading eigenvalue \(^{}\) of \(^{}\) satisfies \(|^{}|=()\), Theorem 4.1 of  shows that with probability at least \(1-O(n^{-8})\), the leading eigenvector \(\) of \(\) satisfies

\[d_{}(,^{})+ \|^{}\|_{})}{|^{}|}=+}{|^{}|}.\] (4)

When \(|^{}|\), a regime of most interest (no polynomial-time algorithm is known when \(|^{}|\), and estimation is easy when \(|^{}|\)), we show in Lemma 1 that Equation (4) is not improvable up to log-factors, as a result of the Baik-Ben Arous-Peche (BBP) phase transition  (see  for BBP-style phase transitions in the setting of this paper).

**Lemma 1**.: _Under Equation (1) with Gaussian noise, let \(^{}=^{}^{}^{}\). If both limits \(_{n}^{}/()>\!1\) and \(_{n}/n\) exist, then for any \(\!\![1,n]\), there exists \(^{}^{n-1}\) such that almost surely,_

\[_{n}d_{}(,^{})_{n} }{2|^{}|^{2}}.\] (5)

Equation (5) shows that the spectral estimate \(\) has an intrinsic dependence on \(\), with especially bad performance when \(\) is large and \(|^{}|\!=\!()\). In this large-\(\) regime, beyond low-rankedness, \(^{}\) exhibits additional structure (e.g., sparsity) that is not fully utilized by the spectral estimator. This suggests that the dependence on \(\) in Equations (4) and (5) is a shortcoming of the spectral estimator. In Algorithm 1, we present a new estimator designed to remove this dependence on \(\). Theorem 1 shows that up to log-factors, it matches the minimax lower bound discussed below (see Equation (9) in Section 1.2). Experiments in Section 5 further support our theoretical results.

In the rank-\(r\) case with Gaussian noise, Theorem 4.2 in  shows that when \(|^{}_{r}|\),

\[d_{2,}(,^{})+)}{|^{}_{r}|}\] (6)

with probability at least \(1-O(n^{-8})\), where \(=|^{}_{1}|/|^{}_{r}|\) is the condition number of \(^{}\). Here again, the estimation error depends on \(\), and we conjecture that this dependence is also sub-optimal. Algorithm 2 extends our rank-\(1\) estimation algorithm to this more general rank-\(r\) case. Experiments in Section 5 show that Algorithm 2 outperforms the naive spectral method in the general rank-\(r\) case, with little to no sensitivity to the coherence \(\). We note in passing that the dependence on \(\) in Equation (6) can likely be removed , though we do not pursue this here.

### Minimax lower bounds for subspace estimation

Minimax lower bounds have been established for a variety of subspace estimation problems, including sparse PCA [21; 55], matrix denoising , structural matrix estimation , network estimation [35; 63] and estimating linear functions of eigenvectors [42; 28]. Most of these studies focus on minimax lower bounds under the Frobenius or operator norm, derived using the packing numbers of Grassmann manifolds [46; 13]. In the matrix denoising literature, it is well-known that for \(r 1\),

\[_{_{r}}\|}- ^{}\|_{}\{}{ |_{r}^{}|},\}\] (7)

holds in a minimax sense (see Theorem 3 in  or Theorem 4 in ). Far fewer papers have considered lower bounds under \(d_{2,}\)[18; 4], and these results are derived via the trivial lower bound

\[d_{2,}(},^{})}_{ _{r}}\|}-^{ }\|_{},\] (8)

which holds for any \(},^{}^{n r}\). Applying the lower bounds in Equations (7) and (8), we have

\[_{^{}^{n r}:^{}^{}= _{r}}d_{2,}(},^{})\{ }{|_{r}^{}|},}\}\] (9)

holds for any estimator \(}^{n r}\). When \(^{}\) is incoherent, meaning that \(=O(1)\), this lower bound is achieved by the spectral estimation rate in Equation (6) when \(|_{r}^{}|=()\), and is achieved trivially by \(}=_{n,r}\) when \(|_{r}^{}|=o()\). On the other hand, when \(^{}\) is coherent, in the sense that \(=(1)\), and \(|_{r}^{}|=()\), our discussion above in Section (1.1) (including our new results in Theorem 1) suggests that the rate in Equation (9) can be achieved up to log-factors.

This leaves open the question of the minimax rate when \(=(1)\) and \(|_{r}^{}|=o()\). In this case, the lower bound in Equation (9) cannot exceed \(\). This seems suboptimal, as we expect some dependence on \(\|^{}\|_{2,}\) (for example, consider the extreme case when \(^{}\) is very near zero). This suboptimality arises from the naive lower bound in Equation (8). In Theorem 2, we improve this lower bound, removing the \(\) dependence in Equation (9). This improved lower bound makes use of novel metric entropy bounds for singular subspaces, which may be of independent interest.

### Notation and roadmap

We use \(C\) to denote a constant whose precise values may change from line to line. For a positive integer \(n\), we write \([n]=\{1,2,,n\}\). \(||\) denotes the cardinality of a set \(\). For real numbers \(a\) and \(b\), we write \(a b=\{a,b\}\) and \(a b=\{a,b\}\). For a vector \(=(v_{1},v_{2},,v_{n})^{}^{n}\), we use the norms \(\|\|_{2}=^{n}v_{i}^{2}}\) and \(\|\|_{}=_{i}|v_{i}|\). We let \(_{i}^{n},i[n]\) denote the standard basis vectors of \(^{n}\). \(^{n-1}=\{^{n}:\|\|_{2}=1\}\) denotes the unit sphere. For a matrix \(^{n n}\), \(_{i,}\) denotes its \(i\)-th row as a row vector, \(\|\|\) denotes its operator norm and \(\|\|_{2,}=_{i[n]}\|_{i,}\|_{2}\) indicates the maximum row-wise \(_{2}\) norm. \(_{n}^{n}\) denotes the \(n\)-by-\(n\) identity matrix. \(_{r}\) denotes the \(r\)-dimensional orthogonal group. We use both standard Landau notation and asymptotic notation: for positive functions \(f(n)\) and \(g(n)\), we write \(f(n) g(n)\), \(f(n)=(g(n))\) or \(g(n)=o(f(n))\) if \(f(n)/g(n)\) as \(n\). We write \(f(n) g(n)\), \(f(n)=(g(n))\) or \(g(n)=O(f(n))\) if for some constant \(C>0\), we have \(f(n)/g(n) C\) for all sufficiently large \(n\). We write \(f(n)=(g(n))\) if both \(f(n)=O(g(n))\) and \(g(n)=O(f(n))\).

The remainder of the paper is organized as follows. In Section 2, we study the eigenspace estimation problem for rank-one matrices and propose a new algorithm that achieves the minimax optimal error rate up to logarithmic factors in the growth regime where \(|^{}|=()\) (Theorem 1). In Section 3, we extend this algorithm to rank-\(r\) eigenspace estimation. In Section 4, we present theoretical results for the metric entropy of subspaces under \(d_{}\) and \(d_{2,}\) and improve Equation (9) under the growth regime where \(|^{}|=O()\) (Theorem 2). Numerical results are provided in Section 5. We conclude in Section 6 with a discussion of the limitations of our study and directions for future work. Detailed proofs of all lemmas and theorems can be found in the appendix.

## 2 Rank-one matrix eigenspace estimation

In this section, we study the model in Equation (1) when \(^{}\) is rank-one with eigendecomposition \(^{}=^{}^{}^{}\), where \(^{}\) and \(^{}^{n-1}\). As discussed in Section 1, spectral methods may have sub-optimal dependence on \(\) compared to the lower bound in Equation (9). We show that a better estimator is possible by working with a carefully selected subset of entries of \(\). We start with a key observation in Lemma 2, which states that any unit vector contains a subset of large entries, and this subset has a sufficiently large cardinality. This subset is the key to our new estimator.

**Lemma 2**.: _Let \(\) be the set_

\[=\{^{-}n,,^{-}n,^{-}n\},\] (10)

_where \(L\) is given by_

\[L=.\] (11)

_For \(n\) sufficiently large, for every \(^{n-1}\), there exists \(_{0}\) such that_

\[^{2}}|\{i:|v_{i}|_{0}\}|> ^{2}^{2}n}.\] (12)

To motivate Algorithm 1, suppose \(^{}\) is entrywise positive. For \(_{0}\), denote the set in Equation (12) by \(I_{_{0}}\). By Equation (12), the sum of entries \(M_{ij}^{}\) with \(i,j I_{_{0}}\) grows as \((|^{}||I_{_{0}}|^{-2}n)\), while the sum of the corresponding entries of \(\) grows as \(O(|I_{_{0}}|)\). That is, when \(|^{}|\) is sufficiently large, the signal contained in the entries \(i,j I_{_{0}}\) dominates the noise. If we knew \(I_{_{0}}\), utilizing the entries in \(I_{_{0}}\) would reduce the estimation error incurred by small entries of \(^{}\). In practice, we do not know \(I_{_{0}}\) and must estimate such a subset. To ensure that this is possible, we impose a technical assumption on \(^{}\). We discuss this assumption below in Remark 2.

**Assumption 2**.: _There exists an \(_{0}\) satisfying Equation (12) and a constant \(0<_{0} 1\) such that for all sufficiently large \(n\),_

\[\{i:|u_{i}^{}|[(1-_{0})_{0},(1+_{0})_{0}] \}=.\]

We pause to give a few examples to illustrate Assumption 2. First, consider \(^{}=c_{1}_{1}+c_{2}n^{-1/2}_{n}\), with \(c_{1},c_{2}=(1)\) chosen so that \(\|^{}\|_{2}=1\). We note that \(c_{1},c_{2}\) both depend on \(n\), but are bounded away from zero as \(n\) grows, and one can verify that Assumption 2 holds with \(_{0}=^{-1/2}n\) and \(_{0}=1/2\). As another example, consider \(^{}=n^{-1/2}_{n}^{n}\). One may verify that taking \(_{0}=( n)^{-L/2}=1/\) and \(_{0}=0.4\) satisfies the conditions in Assumption 2.

As an example of a setting that violates Assumption 2, consider \(^{}\) obtained by renormalizing a vector of i.i.d. Gaussians. This results in \(^{}\) being Haar-distributed on \(^{n-1}\) and Assumption 2 is violated with high probability. To see this, note that \(^{}/\) where \( N(0,_{n})\) (see Theorem 3.4.6 in ). Since with high probability \(\|\|_{}=O()\), Equation (12) holds only when \(_{0} 1/\). For Assumption 2 to hold, there must be a gap of \((1)\) between the entries of \(\), which fails with high probability.

It is tempting to conclude from the counter-example just given that renormalizing a vector of i.i.d. entries must necessarily result in a \(^{}\) that violates Assumption 2, but this is not always the case. If the entrywise distribution has suitable structure, \(^{}\) may still obey Assumption 2. As an illustration, suppose that \(^{}\) is obtained by renormalizing a vector \(=(g_{1},g_{2},,g_{n})^{}^{n}\) with i.i.d. entries from a distribution with variance \(1\), so that \(^{}/\). If the \(g_{i}\) are drawn by taking \(g_{i}=a\) with probability \(p\) and \(g_{i}=b\) with probability \(1-p\), then each entry of \(^{}\) is either approximately \(ap/\) or approximately \(b(1-p)/\). Choosing \(a,b\) and \(p\) appropriately, we can ensure a gap between the entries of \(^{}\) of size \(O(n^{-1/2})\), and we can take \(_{0}=( n)^{-L} n^{-1/2}\).

Our new estimator of \(^{}\) is a refinement based on the leading eigenvector and eigenvalue of \(\). For the spectral estimator to provide a useful initialization, we make Assumption 3 on \(^{}\).

**Assumption 3**.: _There exists a constant \(C_{1}>2400/_{0}\), where \(_{0}\) is as in Assumption 2, such that the leading eigenvalue \(^{}\) of \(^{}\) satisfies_

\[|^{}| C_{1}n n}.\] (13)

**Remark 1**.: _The dependence on \(_{0}\) in Assumption 3 is for technical reasons discussed in Remark 2. In our proofs, we do not optimize the dependence on \(C_{1}\) and assume that \(C_{1}>2400/_{0}\). As demonstrated in our experiments in Section 5, \(|^{}|n n}\) appears sufficient in practice. When \(|^{}|n}\), the spectral estimator fails to provide any useful initial estimate and it is believed that no polynomial-time algorithm can succeed. We provide more discussion on this matter in Remark 6._

The final ingredient required for Algorithm 1 is a leading eigenvalue estimate \(\) that recovers the true signal eigenvalue \(^{}\) suitably well.

**Assumption 4**.: _Under Assumption 3, \(\) is such that with probability at least \(1-O(n^{-8} n)\),_

\[|-^{}| C_{2}}^{5/ 2}n.\]

Assumption 4 seems stringent at first. The top eigenvalue \(\) of \(\) achieves only a \(O(n})\) error rate (see Lemma 2.2 and Equation (3.12) in ). This is because \(=^{}+n^{2}\!/\!^{}+O( n})\) (see  or Theorem 2.3 in ; see also ). Luckily, in our setting, the bias-corrected estimate

\[_{c}=(+-4n^{2} }),\] (14)

_does_ satisfy Assumption 4. Another estimator, which falls naturally out of Algorithm 1, also satisfies Assumption 4. We find that it performs similarly to the debiased estimator \(_{c}\) empirically, and so we do not explore it here. Theoretical results for this estimator are in the appendix.

With the above assumptions in hand, we propose a new method given in Algorithm 1. We note that the main computational bottleneck of Algorithm 1 is to find the leading eigenvector \(\) of \(\), and thus the runtime is essentially the same as for standard spectral methods (see, e.g., ).

```
0: Observed matrix \(^{n n}\); leading eigenvalue estimate \(\); parameter \(>0\).
0:\(}^{n}\)
1: If \(<0\), set \(=-\). Obtain the top eigenvector \(^{n-1}\) of \(\).
2: Pick any \(\) such that the set \(=\{i:|u_{i}|\}\) satisfies \[||^{2}^{2}n},\] (15) \[\{i:(1-)<|u_{i}|<(1+)\}=,\] (16)
3: Let \(^{n n}\) be diagonal with \(Q_{kk}=(u_{k})&k\\ 1&k^{c}\) and let \(}=\).
4: Set \(=}_{jk}}\) and let \(_{j}=(_{k}_{jk}) (})\) for \(j[n]\).
5: For each \(j[n]\), set \(_{j}=u_{j}\) if \(|u_{j}|(/) n\), and \(_{j}=Q_{jj}_{j}\) otherwise. ```

**Algorithm 1** Coherence-free eigenvector estimation algorithm

**Remark 2**.: _In our proofs, we set \(=_{0}/2\). Equation (16), Assumption 2 and the \(_{0}\)-dependence in Assumption 3 are technical requirements to ensure that with high probability, \(\) is one of a few deterministic sets, avoiding the complicated dependence between \(\) and \(\). Empirically, Algorithm 1 works well even without these technical conditions. We conjecture that Assumption 2 as well as the \(_{0}\)-dependence in Assumption 3 can be removed. See Section 5 for further discussion._

As alluded to above, the intuition behind Algorithm 1 is that we aim to concentrate our efforts on estimating the large entries of \(^{}\). Consider an entry of \(\) given by \(^{}u_{i}^{}u_{j}^{}+W_{ij}\). Intuitively, locations corresponding to small entries of \(^{}\) produce small \(u_{i}^{}u_{j}^{}\). These entries of \(\) have a small signal to noise ratio compared to those arising from products of large entries of \(^{}\). If we knew the locations of the large entries of \(^{}\), we could use them to obtain more accurate estimates of \(^{}\). Essentially, both Algorithm 1 above and Algorithm 2 presented below consist of two parts: finding the large locations, and using those locations to improve our initial spectral estimate of \(^{}\).

Algorithm 1 assumes that the entrywise variance \(^{2}\) of \(\) is known. Of course, in practice, this is not the case, and we must estimate \(^{2}\). There are several well-established methods for this estimation task. For example, when \(\) is asymmetric,  introduces an estimator based on the median singular value of \(\). In our case, it suffices to estimate \(^{2}\) using a simple plug-in estimator

\[^{2}=_{1 i j n}(Y_{ij}- _{ij})^{2},\] (17)

where \(}=^{}^{n n}\). In general, if \(^{}\) has rank \(r\), then we set \(}=^{}\), where \(\) is the leading \(r\) eigenvalues of \(\) (sorted by non-increasing magnitude) and \(^{n r}\) contains the corresponding \(r\) leading orthonormal eigenvectors as its columns. Lemma 3 controls the estimation error of the plug-in estimator \(^{2}\) for a general rank-\(r\) signal matrix.

**Lemma 3**.: _Under the model given in Equation (1), let \(^{}=^{}^{}{^{}}^{}\) be a rank-\(r\) matrix with \(r 1\), where \(^{}=(_{1}^{},_{2} ^{},,_{r}^{})\) such that \(|_{1}^{}||_{r}^{}|\). Suppose that Assumption 1 holds and that \(|_{r}^{}| 20n}\), then the estimator \(^{2}\) given in Equation (17) is such that with probability at least \(1-O(n^{-8})\),_

\[|^{2}-^{2}|r}{n}+}{cn}+r}{n^{3/2}}\] (18)

_where \(c>0\) is a universal constant._

**Remark 3**.: _We use the plug-in estimator \(\) in the debiased estimator \(_{c}\) in Equation (14) and to construct \(}\) in Step 5 of Algorithm 1. Lemma 3 shows that this only introduces an extra log-factor to the error bound, which does not affect the estimation error rate of either \(^{}\) or \(^{}\)._

Our main result, Theorem 1, controls the estimation error of Algorithm 1, as measured under \(_{}\).

**Theorem 1**.: _Under the model in Equation (1), suppose that Assumptions 1, 2, 3 and 4 hold. Then for \(n\) sufficiently large, the estimate \(}^{n}\) produced by Algorithm 1 satisfies_

\[d_{}(},^{}) }( n)^{5/2}}{|^{}|}\]

_with probability at least \(1-O(n^{-8} n)\), where \(C>0\) is a universal constant._

**Remark 4**.: _Under Gaussian noise, in the regime \(|^{}|=()\), our upper bound is minimax rate-optimal up to log-factors compared to the lower bound in Equation (9). In particular, the rate obtained in Theorem 1 does not depend on the coherence parameter \(\). A more careful analysis might be able to remove some of the log-factors in Theorem 1, but we leave this matter for future work._

## 3 Rank-\(r\) matrix eigenspace estimation

To handle the more general case in which the signal matrix \(^{}\) is rank \(r\), we propose Algorithm 2, which yields an estimate of \(^{}\). This is achieved by estimating the \(r\) leading eigenvectors separately, then combining them into an estimate \(}^{n r}\). We explore the empirical performance of Algorithm 2 via simulation in Section 5.2 and leave its theoretical analysis to future work. Algorithm 2 requires the observed matrix \(\) and an estimate of the \(k\)-th leading eigenvalue \(_{k}^{}\) of \(^{}\) as input. Similar to the rank-one case, the top-\(r\) leading eigenvalues \(_{1},_{2},,_{r}\) of \(\) are biased . We again use a debiased estimator \(_{k,c}\) for \(k[r]\) as input to Algorithm 2, given by

\[_{k,c}=(_{k}+^{2}-4n ^{2}}).\] (19)

Algorithm 2 is a natural extension of Algorithm 1. Essentially, it converts the problem of estimating \(_{k}^{}\) into an eigenvector estimation problem under a rank-one signal-plus-noise model given by

\[=_{k}^{}_{k}^{}{_{k}^{}}^{}+( ^{}_{-k}+)=_{k}^{}_{k}^{}_{k}^{ }+(^{}-_{k}^{}_{k}^{}_{k}^{ }+),\]

where \(^{}_{-k}=^{}-_{k}^{}_{k}^{} _{k}^{}\). There are two differences between Algorithms 1 and 2. First, we remove Equation (16) from Algorithm 1, as this is mainly a technical requirement (see Remark 2).

More importantly, in Algorithm 2, we conjugate \(\) by a random orthogonal matrix \(_{n}\). The \((r-1)\) leading eigenvectors of \(_{-k}^{}^{}\) form a random subspace of \(^{n(r-1)}\), which allows us to treat \(_{-k}^{}^{}\) as a noise matrix. By way of illustration, consider the rank-\(2\) case with \(_{-1}^{}=_{2}^{}_{2}^{}_{2}^{}\). \(_{2}^{}\) behaves similarly to a random vector drawn uniformly from \(^{n-1}\). Therefore, one would expect \(_{2}^{}_{2}^{}\) to behave similarly to a noise matrix \(n^{-1}^{}\), where \( N(0,_{n})\) (see Chapter 3 of ). As in the discussion after Lemma 2, we can find a set of indices \(I_{_{0}}\) such that the signal in the corresponding entries of \(_{1}^{}\) dominates the noise.

```
0: Observed matrix \(^{n n}\); \(k\)-th leading eigenvalue estimate \(_{k}\). If \(_{k}{<0}\), set \(-\).
0:\(}_{k}^{n}\)
1: Obtain the top-\(r\) eigenvectors \(}\) of \(^{}\), where \(_{n}\) is Haar-distributed.
2: Set \(=((}_{ ,k}))\) and set \(}=^{}\).
3: Pick an \(_{0}\) such that for \(:=\{i:|_{i,k}|_{0}\}\), \(|| 1/_{0}^{2}^{2}n\).
4: Set \(=(_{j,}_{j})^{1/2}\) and set \(_{j}=_{}_{j}/( _{k}})\) for \(j[n]\).
5: Let \(=^{}}\). For \(j[n]\), set \(_{k,j}=U_{k,j}&|U_{k,j}|(/| _{k}|) n\\ (^{}})_{j}&\) ```

**Algorithm 2** Coherent optimal eigenvector estimation algorithm

As mentioned above, our experiments in Section 5.2 indicate that Algorithm 2 performs well. A proof of its performance, however, is more complicated than Theorem 1. The main difficulty arises from the fact that in Algorithm 2, we conjugate by a random orthogonal transformation. This ensures that when considering the large entries of one signal eigenvector, the other signal eigenvectors are ignorable. Unfortunately, this random orthogonal transformation breaks Assumption 2 and introduces complicated dependency structure, requiring a more careful analysis that we leave for future work.

## 4 Estimation lower bounds under \(_{2,}\) distance

Theorem 1 demonstrates that for a rank-one signal, dependence of the estimation rate on \(\) can be removed when \(|^{}|=()\). In this regime, our result matches the lower bound in Equation (9) up to log-factors, making this the minimax lower bound under Assumption 2. As discussed in Remark 2, we expect Assumption 2 can be removed via a more careful analysis, rendering the lower bound in Equation (9) optimal under \(|^{}|=()\). On the other hand, as discussed in Section 1.2, the lower bound in Equation (9) is sub-optimal in the regime where \(|^{}|=O()\). In what follows, we aim to improve upon Equation (9) by deriving metric entropy bounds  for rank-\(r\) singular subspaces under the \(_{2,}\) distance when \(|^{}|=O()\).

Recall that for a semi-metric \(\) defined on a set \(\), we may define the \(\)-packing number \((,,)\) of \(\) under \(\) (see Chapter 15 in ). The packing \(\)-entropy, \((,,)\), captures the complexity of the space \(\), and a lower bound on the \(\)-entropy can be translated into a lower bound on the minimax estimation error rate. For a given \(r[n]\) and \(1 n/r\), we consider the parameter set

\[(n,r,)=\{^{n r}:^{ }=_{r},\ \|\|_{2,}}\}.\] (20)

Below, we write \(_{r,}\) for \((n,r,)\). Lemma 4 lower bounds \((_{r,},d_{2,},)\). We focus on the range \(r/n^{2} r/n\), as the lower bound in Equation (9) shows that \(^{2} r/n\) is not achievable when \(|^{}|=O()\) and \(^{2} r/n\) is achieved by the trivial all-zeros estimate.

**Lemma 4**.: _Suppose that \(n/\{4,r\}\) and \( 12(12n)\), and let \(>0\) be such that_

\[^{2}r}{8e^{2}n}^{2}^{2} r}{96e^{2}n (12n/)},\] (21)

_where \(c_{0}>0\) is a universal constant. Then when \(n\) is sufficiently large,_

\[(_{r,},d_{2,},)}{^{2}}\] (22)

**Remark 5**.: _Lemma 4 applies when \( n n/r\). A more careful analysis might relax the lower bound, but when \( n\), any \(_{r,}\) is nearly incoherent and Equation (9) is nearly optimal. An upper bound matching Lemma 4 up to log-factors can be found in the appendix._

Lemma 4 implies an improved lower bound compared to Equation (9). Consider the parameter space

\[(^{},,r)=\{(^{},^{}): {}^{}=^{}_{r},^{}_{r, }\}.\]

Using the Yang-Barron method , we obtain a lower bound for eigenspace estimation in the rank-\(r\) signal-plus-noise model for \(|^{}|=O()\). A detailed proof can be found in the appendix.

**Theorem 2**.: _Under Assumption 1 and the conditions of Lemma 4, for any \(0<^{}(6})^{-1}\), where \(C_{0}>0\) is a universal constant related to covering numbers of Grassmann manifolds, there is a universal constant \(c>0\) such that for all sufficiently large \(n\),_

\[_{^{n r}}_{(^{}, ^{})(^{},,r)}_{^{}, {U}^{}}d_{2,}(},^{}) c (}{^{}}}).\]

**Remark 6**.: _Theorem 2 removes the upper limit \(\) from Equation (9), suggesting that \(\) only comes to bear when \(^{}\). This regime is not well studied, as the BBP transition  implies that spectral methods fail, but other algorithms might achieve our lower bound. For example, signal detection is possible if structure is present . Unfortunately, any such algorithm is likely to be computationally expensive, given the general belief that no polynomial-time algorithm can succeed when \(^{}\)[39; 9]. We leave further exploration of this small-\(^{}\) regime to future work._

**Remark 7**.: _The parameter space \((^{},,r)\) considered in Theorem 2 contains only signal matrices with condition number \(=1\). In recent work, the authors have established lower bounds akin to Theorem 2 that show the role of condition number. A full accounting of the interplay between condition number and coherence is a promising area for future work._

## 5 Numerical experiments

We turn to a brief experimental exploration of our theoretical results. All experiments were run in a distributed environment on commodity hardware without GPUs. In total, the experiments reported below used 3425 compute-hours. Mean memory usage was 3.5 GB, with a maximum of 11 GB.

### Simulations for rank-one eigenspace estimation

We begin with the rank-one setting considered in Algorithm 1, in which we observe

\[=^{}+=^{}^{}^{}{}^{ }+,\]

and wish to recover \(^{}^{n-1}\). We take \(^{}=\) in all experiments, matching the rate in Remark 1. We consider three distributions for the entries of \(\): Gaussian, Laplacian and Rademacher, all scaled to have variance \(^{2}=1\). We consider two approaches to generating \(^{}\). In either case, we set a random entry of \(^{}\) to be \(a\{0.3,0.55,0.8\}\), then generate the remaining entries by either

1. drawing uniformly from \(}^{n-2}\), or
2. drawing uniformly from \(\{ 1\}^{n-1}\) then normalizing these to have \(_{2}\) norm \(}\).

In both cases, \(\|^{}\|_{}\!\!=\!\!a\) and \(\!=\!a^{2}n\) with high probability. We take \(a\!\!=\!\!(1)\), since in finite samples, \(Cn^{-1/2} n\) (which is nearly incoherent) is hard to discern from a constant (e.g., when \(n\!=\!20000\), \(4n^{-1/2}\! n 0.28\), nearly matching \(a\!=\!0.3\)). Having generated \(=^{}+\), we estimate \(^{}\) using both the spectral estimate \(\) and Algorithm 1 and measure their estimation error under \(d_{}\). We report the mean of 20 independent trials for each combination of problem size \(n\), magnitude \(a\) and methods for generating \(^{}\) and \(\). We vary \(n\) from \(100\) to \(15100\) in increments of \(1000\).

When running Algorithm 1, we use the debiased estimate \(_{}\) from Equation (14). This requires an estimate of \(\), for which we use the plug-in estimator in Equation (17). We set \(=0\), eliminating Equation (16). Similar to Algorithm 2, we conjugate \(\) by a random orthogonal matrix \(_{n}\). We expect the top eigenvector \(}\) of \(^{}\) to be approximately uniformly distributed on \(^{n-1}\), as it is close to \(^{}\) (see Theorem 2.1 in ). Thus, the median of the absolute values of \(}\) should be \((n^{-1/2})\). Instead of selecting \(_{0}\) according to Equation (15), we set \(_{0}\) to be this median. Therequirement in Equation (15) is then fulfilled, since there are roughly \(n/2\) entries larger than the median. After obtaining \(}\) from Algorithm 1, we return \(^{}}\) as our estimate of \(^{}\). We note that this random rotation further serves to illustrate that Assumption 2 is merely a technical requirement: after a random rotation, with high probability, \(^{}\) does not satisfy Assumption 2.

Figure 1 compares the accuracy in estimating \(^{}\) using the leading eigenvector of \(\) (blue) and Algorithm 1 (orange) under the three noise settings and two generating procedures for \(^{}\). Shaded bands indicate \(95\%\) bootstrap confidence intervals (CIs). Across settings, Algorithm 1 recovers \(^{}\) with a much smaller estimation error under \(d_{}\) compared to the naive spectral estimate, especially when \(\|^{}\|_{}\) (i.e., the coherence \(\)) is large. The spectral method degrades noticeably as coherence increases, while Algorithm 1 has far less dependence on \(\). Indeed, under Gaussian noise (the first column of Figure 1), it has no visible dependence on \(\). Under Rademacher noise (middle column of Figure 1), the dependence of Algorithm 1 on \(\) appears slightly reversed from that of the spectral estimator. Under Laplacian noise (right column of Figure 1), there seems to be a slight dependence on \(\). Further examination in the appendix suggests that this is due to estimating the entries _other_ than the largest element of \(^{}\), and is likely asymptotically smaller than the rate in Theorem 1.

### Simulations for rank-\(r\) eigenvector estimation

For the rank-\(r\) setting, we have a signal matrix with eigenvalues \(|_{1}^{}||_{r}^{}|\). We observe

\[=^{}+=^{}^{}^{}+ ,\]

where \(^{}=(_{1}^{},,_{ r}^{})\), and we wish to recover \(^{}^{n r}\). We take \(=1\) and \(_{r}^{}=\). Recovering each column of \(^{}\) separately requires an eigengap, defined for \(k[r]\) as \(_{k}:=|_{k}^{}-_{k+1}^{}|\) and \(_{0}=\). Typically, the estimation error of the \(k\)-th eigenvector has a \(O(\{_{k},_{k-1}\}^{-1})\) dependence on the eigengaps . Here, we set \(_{k}=0.5\), so \(_{k}^{}=0.5(r-k+2)\) for all \(k[r]\). As in the rank-one case, we generate entries of \(\) from Gaussian, Laplacian and Rademacher distributions, all scaled to have unit variance. The true eigenvectors \(^{}\) are generated by repeating the following procedure for \(k[r]\):

1. Randomly select an element of \(^{n}\) and set it to be \(a\{0.3,0.55,0.8\}\).
2. Generate the rest of \(\) by drawing uniformly from \(}^{n-2}\).
3. Set \(_{k}^{}=(_{n}-^{},_{1:(k-1)}^{},_ {1:(k-1)})\), normalize \(_{k}^{}\) to have unit \(_{2}\) norm, and set \(^{},_{k}=_{k}^{}\). If \(k=1\), then we take \(^{},_{1:(k-1)}^{},_{1:(k-1)}\) to be the zero matrix.

Under this procedure, each \(_{k}^{}\) has coherence approximately \(a^{2}n\). Since the large entries of the \(_{k}^{}\) are unlikely to appear in the same rows, \(^{}\) also has coherence \(a^{2}n\). We take \(r=2\) here.

Figure 1: Estimation error measured in \(d_{}\) as a function of dimension \(n\), by the leading eigenvector (blue) and Algorithm 1 (orange), for \(\|^{}\|_{}\) equal to \(0.8,0.55\) and \(0.3\) (dotted, dashed and solid lines, respectively). We consider \(^{}\) generated from the Bernoulli (top row) and Haar (bottom row) schemes, and we consider Gaussian (left), Rademacher (middle) and Laplacian (right) noise.

Having generated \(\!=\!}\!+\!\), we obtain estimates via the naive spectral method and Algorithm 2. and measure their estimation error under \(d_{}\). We vary \(n\) from \(100\) to \(15100\) in increments of \(1000\) and report the mean of \(20\) independent trials for each combination of \(n\), \(a\) and noise distribution. The results are summarized in Figure 2, showing the error for \(^{}}\), \(k\{1,2\}\) using the spectral estimate (blue/purple) and Algorithm 2 (orange/red), under Gaussian (left), Rademacher (middle) and Laplacian (right) noise. Shaded bands indicate \(95\%\) bootstrap CIs. In all settings, Algorithm 2 improves markedly on the spectral estimator. Algorithm 2 shows no visible dependence on \(\) under Gaussian noise. Under Rademacher noise, its \(\)-dependence is the reverse of the spectral estimator. Under Laplacian noise, it shows slight dependence on \(\), which we again expect to be asymptotically smaller than the rate in Theorem 1. In the appendix, we consider \(r\!=\!3\) and measure error under \(d_{2,}\), and we again find that Algorithm 2 outperforms spectral estimators and is far less sensitive to \(\).

### Comparison with other methods

To the best of our knowledge, we are the first paper to consider the task of non-spectral entrywise eigenvector estimation. The nearest obvious competing method might be based on approximate message passing (AMP; see  for an overview). Such a comparison is included in the appendix, where our experiments indicate that while AMP performs well in recovering the signal eigenvectors as measured by \(_{2}\) error, our method as specified in Algorithms 1 and 2 perform better under entrywise and \(_{2,}\) error. Experimental details and further discussion can be found in the appendix.

## 6 Discussion, limitations and conclusion

We have presented new methods for eigenvector estimation in signal-plus-noise matrix models and new lower bounds for estimation rates in these models. The entrywise estimation error of our method has no dependence on the coherence \(\) for rank-one signal matrices, and achieves the optimal estimation rate up to log-factors. Simulations show that our method tolerates non-Gaussian noise and its extension to rank-\(r\) signal matrices has little dependence on \(\). One limitation of our method is that it assumes homoscedastic noise. Future work will aim to relax this assumption and the technical condition in Assumption 2. In the rank-\(r\) case, Algorithm 2 estimates each eigenvector separately, requiring an eigengap. Future work will avoid this by simultaneously or iteratively estimating multiple eigenvectors. We note in closing that inequitable social impacts from abuse or misuse of models and methods are common, but we see no particular such impacts in the present work.