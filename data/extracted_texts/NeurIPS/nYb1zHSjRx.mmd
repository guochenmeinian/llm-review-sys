# Do LLMs estimate uncertainty well in instruction-following?

Juyeon Heo

University of Cambridge

jh2324@cam.ac.uk

&Miao Xiong

National University of Singapore

miao.xiong@u.nus.edu

Christina Heinze-Deml

Apple

c_heinzedeml@apple.com &Jaya Narain

Apple

jnarain@apple.com

This work was done during an Apple internship.

###### Abstract

Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stemming from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.

## 1 Introduction

Large language models (LLMs) have garnered interest for their potential as personal AI agents across various domains, such as healthcare, fitness, nutrition, and psychological counseling (Li et al., 2024; Wang et al., 2023a; Tu et al., 2024). A key to building safe and useful personal AI agents with LLMs lies in their ability to follow instructions precisely. Deployed models must adhere to the constraints and guidelines provided by users to ensure that the outputs are both aligned with user intentions and safe. Yet recent research has exposed significant limitations in LLMs' ability to follow instructions (Zhou et al., 2023; Zeng et al., 2023; Qin et al., 2024; Xia et al., 2024; Kim et al., 2024; Yan et al., 2024). For example, even large models like GPT-4 achieve only around 80% instruction-following accuracy on simple and non-ambiguous instructions from benchmark datasets, and smaller models perform even worse, with accuracy less than 50% (Sun et al., 2024).

Since LLMs are prone to errors, their ability to accurately assess and communicate their own uncertainty is essential. This becomes particularly important in high-stakes applications, where mistakes can have serious consequences. For instance, an LLM developed for personal psychological counseling must strictly adhere to guidelines that avoid topics that might potentially cause trauma. If the LLM misinterprets or deviates from these instructions but accurately recognizes and signals high uncertainty, it could prompt further review or intervention, thereby preventing the delivery of potentially harmful advice.

However, uncertainty estimation in instruction-following tasks has received limited attention, with most research focusing on fact-based tasks like question answering and summarization (Fadeeva et al., 2023; Kuhn et al., 2023; Xiong et al., 2023; Ye et al., 2024), where factual correctness is the primary concern. In contrast, as shown in Figure 1, instruction-following tasks focus on whether a model's response adheres to a set of given instructions, rather than estimating the factual accuracy. Given these different source of uncertainty, it is unclear whether existing methods, which are typically designed for estimating factual uncertainty, can accurately capture uncertainty in instruction following. For example, while semantic entropy (Farquhar et al., 2024) is considered the gold standard for fact-based tasks, it may not be suitable for instruction-following. Both responses, 'Regular exercise strengthens muscles' and 'Regular exercise reduces stress', follow the given instruction in Figure 1 but convey different semantic meanings, leading to a high semantic uncertainty score, which inaccurately reflects instruction-following performance. This example highlights the need for frameworks tailored to evaluating methods and models for uncertainty estimation in instruction-following tasks.

To evaluate how well existing uncertainty estimation methods and models perform on instruction-following tasks, we evaluate six uncertainty estimation methods across four LLMs on the IFEval benchmark dataset (Zhou et al., 2023). However, we find that multiple factors are entangled in existing benchmarks. For instance, uncertainty can stem from both task execution quality and instruction following, making it difficult to isolate and directly compare methods and models based solely on their ability to estimate instruction-following uncertainty. To address these issues, we design a new benchmark dataset with two versions to enable a more controllable and fine-grained evaluation. The **Controlled** version provides a structured assessment by removing confounding factors and offering tasks with varying difficulty levels, split into Controlled-Easy, where correct and incorrect responses are easy to distinguish, and Controlled-Hard, which focuses on more subtle errors. In contrast, the **Realistic** version uses naturally generated LLM responses, retaining real-world signals. Together, these datasets provide a comprehensive framework for evaluating uncertainty estimation methods and models under both controlled and real-world conditions.

Our analysis revealed several key findings from the controlled evaluation: 1) Verbalized method consistently outperforms dscogit-based methods like perplexity in the Controlled-Easy setting, where correct and incorrect responses are relatively easier to distinguish. Specifically, normalized p(true) (Kadavath et al., 2022) proves to be a reliable uncertainty method across both Controlled-Easy and Realistic settings. 2) Smaller models often outperform larger ones in verbalized confidence, suggesting that factors beyond model size, such as tuning or architecture, may contribute to better uncertainty estimation in certain tasks. 3) Probes relying on the internal states of LLMs outperform logit-based and verbalized confidence, highlighting promising directions for future work. 4) In more challenging tasks like Controlled-Hard, which involve subtle off-target responses, all approaches including internal representations struggle to estimate uncertainty accurately, pointing to inherent limitations in LLMs' ability to handle complex uncertainty. These findings from our controlled evaluation setups provide crucial insights into the limitations and potential of LLMs for uncertainty estimation in instruction-following tasks, towards more trustworthy AI agents.

### Contributions

* [leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,leftmargin=*]
* **Systematic Evaluation:** We present the first systematic evaluation of uncertainty estimation methods in instruction-following tasks, addressing a gap in existing research.
* **Benchmark Dataset:** We identify key challenges in existing datasets and introduce a new benchmark dataset specifically tailored for direct comparison and fine-grained analysis of uncertainty estimation methods and models in both controlled and real-world conditions.
* **Findings:** Our evaluation results highlight the potential of self-evaluation and probing methods and point out limitations in handling more complex tasks, underscoring the need for further research to advance uncertainty estimation in instruction-following tasks.

## 2 Uncertainty estimation ability in instruction-following on IFEval

In this section, we evaluate LLMs' uncertainty estimation abilities using the IFEval dataset (Zhou et al., 2023), applying six baseline methods across four LLMs. We selected IFEval because it is designed so that a simple and deterministic program can verify whether a response follows the instructions. This enables a fully automatic and accurate assessment of a model's instruction-following capability, thereby minimizing uncertainties from ambiguous evaluation criteria.

### Methods

**Data** We evaluate uncertainty estimation with the IFEval dataset (Zhou et al., 2023), which is designed to evaluate the instruction-following ability of LLMs on 25 verifiable instruction types under 9 categories across 541 total prompts. Each prompt consists of two components: a task and an instruction, where the _instruction_ specifies the action (e.g., "please do not use keywords", "please start/finish your response with exact sentence") and the _task_ provides the context for executing the instruction (e.g., "please write a resume", "please give a summary about solar system").

**Models and Metrics** We evaluate four LLMs of varying sizes: LLaMA2-chat-7B (Touvron et al., 2023), LLaMA2-chat-13B (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Phi-3-mini-128k-instruct (Abdin et al., 2024). To avoid randomness in decoding, we employ greedy decoding without sampling. Area Under the Receiver Operating Characteristic curve (AUROC) (Pedregosa et al., 2011) is used to measure if the models' uncertainty estimation matches the ground truth labels on correctness in instruction following, generated using the automated evaluation functions from IFEval.

**Baseline uncertainty estimation methods** To evaluate uncertainty in instruction-following, we employ several baseline methods, including self-evaluation of their own uncertainty (verbalized confidence, normalized p(true) and p(true)), logits-based method (perplexity, sequence probability, and mean token entropy), and internal states of model (probe):

* **Verbalized confidence**(Lin et al., 2022): The model's self-reported confidence, scored from 0 to 9, indicating its perceived likelihood that the response correctly follows instructions. Detailed prompts are in the Appendix.
* **Normalized p(true) and p(true)**(Kadavath et al., 2022): These methods assess the probability of the 'true' token, calculated from a binary choice prompt. We modified the calculation to determine the probability of token 'A' given the prompt: "_Does the response: (A) Follow instructions (B) Not follow instructions. The answer is: "_. Normalized p(true) adjusts for biases by considering both tokens of 'true' and 'false' probabilities: \(p()/(p()+p())\), here is \(p()/(p()+p())\)

Figure 1: **Why evaluating uncertainty estimation ability in instruction-following matters**. Uncertainty in instruction-following distinct from factual correctness, as illustrated by this example. While both responses shown are factually correct, the first fails to follow the instruction, resulting in high uncertainty from instruction-following despite low uncertainty from factuality. The second response adheres to the instruction, with low uncertainty in both areas. Prior work on uncertainty has focused primarily on factual correctness, underscoring the need for an evaluation framework for instruction-following tasks.

* [leftmargin=*]
* **Perplexity and Sequence probability**(Jelinek et al., 1977; Fomicheva et al., 2020): Perplexity measures the likelihood of generating a given sequence: \(\{-_{i=1}^{t} p_{}(x_{i} x_{<i})\}\) where \(t\) is the sequence length. Sequence probability, an unnormalized version, is calculated as: \(\{-_{i=1}^{t} p_{}(x_{i} x_{<i})\}\), making it more sensitive to the length of the response, with longer sequences generally having lower probabilities.
* **Mean token entropy for LLMs**(Fomicheva et al., 2020): Entropy measures uncertainty based on token prediction variability: \(H=-_{i=1}^{t}p_{}(x_{i} x_{<i}) p_{}(x_{i}  x_{<i})\)
* **Probe** Drawing inspiration from Liu et al. (2024), we trained a linear model as an uncertainty estimation function that maps the internal representations of LLMs to instruction-following success labels, where the probability predicted by the linear model used as uncertainty scores.

### Findings

Table 1 summarizes uncertainty evaluation results using IFEval.

**LLMs struggle to estimate uncertainty in instruction-following.** Average AUROC values across models and uncertainty estimation methods hover around chance levels (between 0.43 and 0.53), indicating that the models consistently fail to reliably assess their own uncertainty in instruction-following. This underscores the challenge LLMs face in detecting when their responses deviate from the instructions.

**Sequence probability outperforms perplexity, revealing a length signal in uncertainty estimation.** Sequence probability consistently achieves higher AUROC scores than perplexity across most models. Sequence probability is tied to by sequence length, whereas perplexity is not. For example, in LLaMA2-chat-13B, the AUROC for sequence probability averages 0.61 (above chance) across instruction types, whereas perplexity lags at 0.44. This finding implies that response length may inadvertently provide a signal in some uncertainty estimation metrics, even though it may not correlate directly with the correctness of the response in instruction-following.

**No model or method consistently excels across instruction types.** As shown in Table 3 in Appendix, there is no consistent pattern of performance of uncertainty estimation method or model across different instruction types. This lack of consistency indicates that none of the uncertainty estimation methods evaluated reliably capture uncertainty across all instruction types and models.

### Challenges in evaluating uncertainty estimation using existing datasets

An instruction-following dataset with clear evaluation criteria, like IFEval, is important for evaluating instruction-following. However, we identified the importance of several additional factors to aid in comparatively evaluating instruction-following _uncertainty_ of LLMs.

**1) Uncertainty estimation methods and models are only evaluated in length-biased settings in existing instruction-following datasets, missing comparisons on controlled, length-neutral conditions.** We observe that token length significantly impacts uncertainty estimation in instruction-following tasks in existing datasets, where responses are not controlled but are generated as part of

    &  &  \\
**Model** & **Verbal** & **Perplexity** & **Sequence** & **Nor-p(true)** & **p(true)** & **Entropy** \\  LLaMA2-chat-13B & 0.53 & 0.44 & 0.61 & 0.47 & 0.51 & 0.48 & 0.57 \\ LLaMA2-chat-7B & 0.53 & 0.43 & 0.54 & 0.52 & 0.51 & 0.44 & 0.59 \\ Mistral-7B-Instruct-v0.3 & 0.50 & 0.48 & 0.56 & 0.57 & 0.47 & 0.50 & 0.64 \\ Phi-3mini-128k-instruct & 0.53 & 0.43 & 0.48 & 0.55 & 0.45 & 0.45 & 0.54 \\   

Table 1: **AUROC for baseline uncertainty estimation methods applied to the IFEval dataset**(Zhou et al., 2023). AUROC measures how well each method’s uncertainty estimates align with the ground truth regarding correct or incorrect instruction-following across four LLMs. Success Rate (SR) represents the model’s instruction-following accuracy, calculated using the IFEval evaluation function. Notably, LLMs struggle to estimate uncertainty in instruction-following tasks hover around chance levels (between 0.43 and 0.53).**the evaluation. With datasets like IFEval, naturally generated _incorrect responses tend to be longer than correct ones across models_ in Figure 1(a) and Appendix Figure 10.

If this pattern holds across instruction types, length could arguably be a reliable signal in evaluating instruction-following uncertainty. However, we found that the relationship between response length and correctness is not uniform (Figure 1(b)). These varying patterns are not surprising, as response length is related to what the instruction requires. For instance, an instruction like "please elaborate" would naturally result in a longer response, whereas "make it concise" would lead to a shorter one.

These inconsistencies suggest that length is not a reliable or generalizable signal for uncertainty estimation across all instruction types and models. This highlights the need for a controlled evaluation framework that includes a set of length-neutralized responses. By comparing LLMs and uncertainty estimation methods in both length-biased and length-neutral settings, we can more accurately assess their true performance, independent of confounding factors like token length.

**2) Uncertainty sourced from task execution quality is entangled with uncertainty stemming from instruction-following, complicating accurate evaluation.** In the IFEval data, each prompt consists of two parts: the _task_ context (e.g., "Write a brief summary about the solar system") and the specific _instruction_ (e.g., "Please do not mention any planet names"). When measuring uncertainty with baseline methods, uncertainty can stem from both task execution quality (i.e., how well the task itself is accomplished - clear, detailed, and informative) and instruction-following accuracy (i.e., whether the instruction is followed). This creates an entanglement that complicates evaluation. For example, consider the response "Objects in space around a star", which is vague and unclear with low task quality but adheres to the instruction of avoiding mentioning planet names. Alternatively, "The solar system consists of a central star surrounded by various celestial bodies, including Earth and Mars" is informative with high task quality but fails to follow the instruction. We observe that task execution quality also influences the models' uncertainty scores. For example, in Table 5 in the Appendix, the LLaMA-2-chat-7B model assigns an average verbalized confidence score of 7.0 to the first response grouping (low task quality but correct instruction-following) and 7.4 to the second (high task quality but incorrect instruction-following).

This shows that task quality can confound uncertainty estimation in instruction-following. When task execution quality is not controlled, the uncertainty arising from task completion can overshadow the uncertainty associated with following instructions, leading to inaccurate evaluations of the model's true instruction-following uncertainty.

**3) Differences in the severity of instruction-following mistakes across models create inconsistent difficulty levels, complicating model comparisons.** Our primary objective is to assess LLMs' uncertainty estimation capabilities, independent of their instruction-following accuracy in generating responses. However, when using the IFEval dataset, these two factors are entangled, making it difficult to isolate uncertainty estimation from the model's overall instruction-following performance. For example, LLaMA-2-chat-13B, which generally has a higher instruction-following

Figure 2: **Existing instruction-following datasets only evaluate uncertainty estimation methods in length-biased settings, missing comparisons on controlled, length-neutral conditions.** (a) Token lengths distribution for LLaMA-2-chat-7B shows that incorrect responses tend to be longer than correct ones. (b) Token length differences broken down by instruction type and model, with positive values showing that incorrect responses tend to be longer. (c) Token length distribution in the Controlled version of our benchmark, where token length is balanced across all responses.

accuracy, tends to make more obvious errors when it fails to follow instructions. On the other hand, LLaMA-2-chat-7B not only makes these obvious mistakes but also exhibits more subtle instruction-following errors, where responses partially follow the instructions but miss specific details. As a result, uncertainty estimation becomes more challenging for LLaMA-2-chat-7B, where it has to measure uncertainty in more nuanced instruction violations, compared to LLaMA-2-chat-13B.

To quantify the difference in task difficulty, we use GPT-4 to score the responses on a scale from 0 to 9 based on their adherence to instructions. Table 5 shows that the score gap between correct and incorrect responses is smaller for LLaMA-2-chat-7B compared to LLaMA-2-chat-13B, highlighting the more subtle nature of 7B's errors. In contrast, 13B's errors are more drastic, making them easier to identify and be recognized as having high uncertainty. This variation in task difficulty across models complicates direct comparisons of their uncertainty estimation abilities. To ensure fair comparisons across models, it is necessary to evaluate models under controlled difficulty levels.

## 3 Uncertainty estimation ability in instruction-following on our benchmark data

The challenges identified in the previous section--length bias, the entanglement of task execution quality with instruction-following, and varying difficulty levels across models--underscore the need for a controlled and robust framework for evaluating uncertainty estimation. To address these issues, we develop a new benchmark dataset comprising two versions: a Controlled version and a Realistic version. These versions allow for the evaluation of uncertainty estimation under controlled conditions (Controlled) and real-world scenarios (Realistic).

### Benchmark dataset for controlled evaluation setups

To disentangle the complexities that can obscure uncertainty estimation, we design two distinct versions of the dataset: Controlled and Realistic. The Controlled version neutralizes the influence of token length. Meanwhile, the Realistic version leverages actual LLM-generated responses that naturally incorporate real-world signals, including length signal, without manual intervention.

In both versions, we use GPT-4 to filter out low-quality responses, ensuring that the uncertainty being measured comes from instruction-following, not poor task execution. We apply a filtering process using GPT-4 evaluations of task quality of each response on a scale 0-9. Only responses that received a high task quality score (\(>\)8) are included. These datasets enable using the same responses with all models in the uncertainty evaluation task, controlling the difficulty of uncertainty evaluation across models. This allowing for direct comparisons in uncertainty estimation.

#### 3.1.1 Controlled version

In this version, we eliminate the length effect, along with ensuring consistent levels of difficulty across responses to ensure that the evaluation focuses purely on uncertainty estimation. To neutralize the impact of token length, we use GPT-4 to generate both correct and incorrect responses with similar token length (see Appendix for the prompt used to generate responses). Figure 1(c) shows the absence of length bias in the token length distribution. We introduce two levels of controlled difficulty--**Controlled-Easy** and **Controlled-Hard**-by generating three categories of responses: _completely incorrect, correct, and subtly off-target_. In the Controlled-Easy, we calculate AUROC based on distinguishing between correct and completely incorrect responses, while in the Controlled-Hard, we calculate AUROC based on distinguishing between correct and subtly off-target responses. These are more challenging cases where the responses only slightly deviate from the instructions, testing the model's ability to recognize subtle mistakes. While these mistakes are subtle, they could still be important in real-world deployments. Table 4 in the Appendix shows an example from this version. Additional statistics are provided in Table 6 in the Appendix.

#### 3.1.2 Realistic version

In the Realistic version, we retain the natural length and signals inherent in responses generated by multiple LLMs (LLaMA2-chat-7B, LLaMA2-chat-13B, Mistral-7B-Instruct-v0.3, Phi-3-mini-128k, and LLaMA2-chat-70B). Here, the goal is to evaluate uncertainty estimation methods in a scenario 

[MISSING_PAGE_FAIL:7]

**Probe still struggles with uncertainty estimation in more challenging scenarios.** In Controlled-Hard, Probe AUROC scores drop below 0.60 across all models, revealing that even the internal representations struggle to estimate uncertainty accurately when the task complexity increases. This suggests a limitation in LLMs' ability to handle nuanced or complex uncertainty, highlighting the need for further model fine-tuning or the development of more sophisticated uncertainty estimation methods to improve uncertainty estimation in these difficult tasks.

#### 3.2.2 Comparison of models

**Mistral-7B-Instruct consistently demonstrates the strongest performance in verbalized confidence across all tasks** (Controlled-Easy, Controlled-Hard, and Realistic), outperforming even the larger LLaMA2-13B model, highlighting its effective internal calibration for self-assessment. On the other hand, **Phi-3-mini-128k leads in normalized p(true)**, consistently achieving the best AUROC across both easy and hard tasks in Controlled, as well as in Realistic versions, showcasing its strength in binary choice settings. In contrast, **LLaMA-2-13B excels in Perplexity**, indicating its proficiency in logit-based uncertainty estimation. These findings are particularly interesting because **smaller models, such as Mistral-7B-Instruct and Phi-3-mini-128k outperform the larger LLaMA-2-that-13B in self-evaluation methods**. This suggests that factors beyond model size, such as tuning or architecture, may contribute to better uncertainty quantification in certain tasks. However, LLaMA2-13B still shines in logit-based approaches like perplexity, indicating that different methods may favor different models.

## 4 Related work

**Uncertainty estimation in LLMs.** Existing uncertainty estimation methods can be broadly categorized into four types based on the source of information: _verbalized, logit-based, multi-sample_, and _probing-based_ methods. Among them, verbalized methods (Lin et al., 2022; Xiong et al., 2023; Tian et al., 2023) rely on model's self-evaluation by prompting LLMs to explicitly express their uncertainty in theirouptut. Logit-based methods, such as perplexity (Jelinek et al., 1977), sequence probability (Fomicheva et al., 2020), and mean token entropy (Fomicheva et al., 2020) mainly utilize information from the next token prediction distribution. Multi-sample methods (Aichberger et al., 2024; Kuhn et al., 2023; Farquhar et al., 2024) generate multiple responses for the same question, estimating uncertainty through the semantic diversity among the responses. However, these multi-sample methods are less applicable to instruction-following tasks, which only focus on strict adherence to instructions rather than variations in semantic meaning. Lastly, probing-based methods (Liu et al., 2024; Andritz et al., 2024) train external supervised model on model representations to infer uncertainty. In addition, it is worth noting that most existing works focus on factuality-related tasks such as question answering (Xiong et al., 2023; Tian et al., 2023) and summarization tasks (Kuhn et al., 2023), with little attention on instruction-tuning tasks. Our work seek to bridge this gap by evaluating how well current uncertainty metrics capture uncertainties specific to instruction-following scenarios.

Further related work on instruction-following in LLMs and benchmark datasets for evaluating LLMs as evaluators is available in Appendix A.6.

## 5 Conclusion

In this paper, we conduct the first comprehensive evaluation of uncertainty estimation in LLMs specifically in the context of instruction-following tasks, addressing a gap in existing research that primarily focuses on fact-based tasks. We identify limitations associated with existing benchmark datasets and introduce a new benchmark with two versions--Controlled and Realistic--designed to provide a comprehensive framework for evaluating uncertainty estimation methods and models under both controlled and real-world conditions. Our analysis revealed that verbalized self-evaluation methods outperform logit-based approaches in Controlled-Easy tasks, while internal model states provide more reliable uncertainty signals in both Controlled-Easy and Realistic settings. However, all methods struggle with more complex tasks in Controlled-Hard, highlighting the limitations of LLMs and future direction for uncertainty estimation in instruction-following.

**Limitations and Future Work** One limitation is the narrow scope of instruction types and domains included in the benchmark, which may not fully capture the diversity of real-world tasks. Furthermore, similar to other research evaluating LLMs, there is a potential risk of leakage, where the models may have been exposed to similar tasks during pre-training, potentially affecting the results. In future work, expanding the benchmark to include a broader range of domains and evaluating more LLMs would further deepen understanding of uncertainty estimation in instruction-following. Additional analysis could also investigate _why_ LLMs tend to fail to provide accurate uncertainty estimates in instruction-following, which could lead to the development of trustworthy AI agents.

#### Acknowledgments

We thank our team members for their invaluable guidance and resources. We also express our gratitude to Sinead Williamson, Oussama Elachbar, Udhyakumar Nallasamy, Shirley Ren for their helpful feedback on the paper, and to Guillermo Sapiro for his ongoing support of this work.