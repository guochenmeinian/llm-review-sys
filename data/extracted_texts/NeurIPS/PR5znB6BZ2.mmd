# Efficient Beam Tree Recursion

Jishnu Ray Chowdhury  Cornelia Caragea

Computer Science

University of Illinois Chicago

jraych2@uic.edu  cornelia@uic.edu

###### Abstract

Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as an extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although better than previous approaches in terms of memory usage, BT-RvNN can be still exorbitantly expensive. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by \(10-16\) times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form \(f:I\!\!R^{n d} I\!\!R^{d}\) into a token contextualizer of the form \(f:I\!\!R^{n d} I\!\!R^{n d}\). Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models. Our code is available at the link: https://github.com/JRC1995/BeamRecursionFamily.

## 1 Introduction

Recursive Neural Networks (RvNNs)  in their most general form can be thought of as a repeated application of some arbitrary neural function (the recursive cell) combined with some arbitrary halting criterion. The halting criterion can be dynamic (dependent on input) or static (independent of the input). From this viewpoint, nearly any neural network encoder in the deep learning family can be seen as a special instance of an RvNN. For example, Universal Transformers  repeat a Transformer  layer block as a recursive cell and adaptively halt by tracking the halting probability in each layer using some neural function . Deep Equilibrium Models (DEQ)  implicitly "repeat" a recursive cell function using some root-finding method which is equivalent to using the convergence of hidden states dynamics as the halting criterion. As Bai et al.  also showed, any traditional Transformer - i.e. stacked layer blocks of Transformers with non-shared weights can be also equivalently reformulated as a recursive repetition of a big sparse Transformer block with the halting criterion being some preset static upperbound (some layer depth set as a hyperparameter).

A broad class of RvNNs can also be viewed as a repeated application of a Graph Neural Network (GNN)  - allowing iterative message passing for some arbitrary depth (determined by the halting criterion). Transformer layers can be seen as implementing a fully connected (all-to-all) graph with sequence tokens as nodes and attention-based edge weights. In natural language processing, we often encounter the use of stacks of GNNs (weight shared or not) to encourage message-passing through underlying linguistic structures such as dependency parses, constituency parses, discourse structures, abstract meaning representations, and the like .

[MISSING_PAGE_FAIL:2]

2. Often Tree-RvNNs are the core modules behind models that have been shown to productively length-generalize or compositionally generalize [71; 10; 46; 32; 5; 45; 52] in settings where other family of models struggle (at least without extensive pre-training).
3. The recursive cell function of Tree-RvNNs can still allow them to go outside their explicit structural constraints by effectively organizing information in their hidden states if necessary. The projective tree-structure provides only a rough contour for information flow through Tree-RvNNs which Tree-RvNNs can learn to go around just as an RNN can . So in practice some of these constraints can be less concerning than one may think.

**Issues:** Unfortunately, the flexibility of Tree-RvNNs over RNNs comes at a cost. While in RNNs we could just follow on the same chain-structure tree (left-to-right or right-to-left) for any input, in Tree-RvNNs we have to also consider some way to dynamically induce a tree-structure to follow. This can be done externally--that is, we can rely on human inputs or some external parser. However, neither of them is always ideal. Here, we are focusing on _complete Tree-RvNN_ setups that do their own parsing without any ground-truth tree information anywhere in the pipeline. Going this direction makes the implementation of Tree-RvNNs more challenging because it is hard to induce discrete tree structures through backpropagation. Several methods have been proposed to either induce discrete structures [31; 9; 66; 61], or approximate them through some continuous mechanism [10; 95; 72; 71]. Nevertheless, all these methods have their trade offs - some do not actually work in structured-sensitive tasks [57; 9; 72], others need to resort to reinforcement learning and an array of optimizing techniques  or instead use highly sophisticated architectures that can become practically too expensive in space or time or both [51; 71; 10]. Moreover, many of the above models [10; 31; 9] have been framed and used mainly as a sentence encoder of the form \(f:^{n d}^{d}\) (\(n\) being the sequence size). This formulation as sentence encoder limits their applicability as a deep learning building block - for example, we cannot use them as an intermediate block and send their output to another module of their kind or some other kind like Transformers because the output of a sentence encoder will just be a single vector.

**Our Contributions:** Believing that simplicity is a virtue, we direct our attention to Gumbel-Tree (GT) RvNN  which uses a simple easy-first parsing technique  to automatically greedily parse tree structures and compose sentence representations according to them (we provide a more extended discussion on related works in Appendix). Despite its simplicity, GT-RvNN relies on Straight-Through Estimation (STE)  which induces biased gradient, and has been shown empirically to fail in structure-sensitive tasks . Yet, a recent approach - Beam Tree RvNN (BT-RvNN)  - promisingly shows that simply using beam search instead of the greedy approach succeeds quite well in structure-sensitive tasks like ListOps  and Logical Inference  without needing STE. Nevertheless, BT-RvNN can still have exhorbitant memory usage. Furthermore, so far it has been only tested as a sentence encoder. We take a step towards addressing these issues in this paper:

1. We identify a critical memory bottleneck in both Gumbel-Tree RvNN and Beam-Tree RvNN and propose strategies to fix this bottleneck (see SS3). Our strategies reduce the peak memory usage of Beam-Tree RvNNs by \(10\)-\(16\) times in certain stress tests (see Table 4).
2. We propose a strategy to utilize the intermediate tree nodes (the span representations) to provide top-down signals to the original terminal representations using a parent attention mechanism. This allows a way to contextualize token representations using RvNNs enabling us to go beyond sentence-encoding (see SS4).
3. We show that the proposed efficient variant of BT-RvNN incurs marginal accuracy loss if at all compared to the original--and in some cases even outperforms the original by a large margin (in ListOps).

## 2 Existing Framework

Here we first describe the existing framework used in Choi et al.  and Ray Chowdhury and Caragea . In the next section, we identify its weakness in terms of memory consumption and resolve it.

**Task Structure:** As in related prior works [9; 10; 71], we start with our focus (although we will expand - see SS4) on exploring the use of RvNNs as sentence encoders of the form: \(f:^{n d}^{d}\). Given a sequence of \(n\) vectors of size \(d\) as input (of the form \(^{n d}\)), \(f\) compresses it into a single vector (of the form \(^{d}\)). Sentence encoders can be used for sentence-pair comparison or classification.

**Components:** The core components of this framework are: a scoring function \(score:I\!\!R^{d}\!\!R\) and a recursive cell \(rec:I\!\!R^{d} I\!\!R^{d}\!\!R^{d}\). The \(score\) is implemented as \(score(v)=W_{v}v\) where \(W_{v} I\!\!R^{1 d}\). The \(rec(child_{l},child_{r})\) function is implemented as below:

\[l,\\ r,\\ g,\\ h=(child_{l};\\ child_{r}W_{1}^{rec}+b_{1})W_{2}^{rec}+b_{2}\] (1) \[p=LN((l) child_{l}+(r) child_{r}+(g)  h)\] (2)

Here - \(\) is \(sigmoid\); \([;]\) represents concatenation; \(p\) is the parent node representation built from the children \(child_{l}\) and \(child_{r}\), \(W_{1}^{rec} I\!\!R^{2 d d_{cell}}\); \(b_{1} I\!\!R^{d_{cell}}\); \(W_{2}^{rec} I\!\!R^{d_{cell} 4 d}\); \(b_{2} I\!\!R^{d}\), and \(l,r,g,h I\!\!R^{d}\). \(LN\) is layer normalization. \(d_{cell}\) is generally set as \(4 d\). Overall, this is the Gated Recursive Cell (GRC) that was originally introduced by Shen et al.  and has been consistently shown to be superior [71; 10; 66] to earlier RNN cells like Long Short Term Memory Networks (LSTM) [33; 80].

Note that these models generally also apply an initial transformation layer to the terminal nodes before starting up the RvNN. Similar to [71; 10; 66], we apply a single linear transform followed by a layer normalization as the initial transformation.

**Greedy Search Tree Recursive Neural Networks:** Here we describe the implementation of Gumbel-Tree RvNN . Assume that we have a sequence of hidden states of the form \((h_{1}^{t},h_{2}^{t},...h_{n}^{t})\) in some intermediate recursion layer \(t\). For the recursive step in that layer, first all possible parent node candidates are computed as:

\[p_{1}^{t}=rec(h_{1}^{t},h_{2}^{t}),p_{2}^{t}=rec(h_{2}^{t},h_{3}^{t}),,p _{n-1}^{t}=rec(h_{n-1}^{t},h_{n}^{t})\] (3)

Second, each parent candidate node \(p_{i}^{t}\) is scored as \(e_{i}^{t}=score(p_{i}^{t})\). Next, the index of the top score is selected as \(j=argmax(e_{1:n-1}^{t})\). Finally, now the update rule for the next recursion can be expressed as:

\[h_{i}^{t+1}=h_{i}^{t}&i<j\\ rec(h_{i}^{t},h_{i+1}^{t})&i=j\\ h_{i+1}^{t}&i>j\] (4)

Note that this is essentially a greedy tree search process where in each turn all the locally available choices (parent candidates) are scored and the maximum scoring choice is selected. In each iteration, the sequence size is decreased by 1. In the final step only one representation will be remaining (the root node). At this point, however, the tree parsing procedure is not differentiable because it purely relies on an argmax. In practice, reinforcement learning , STE with gumbel softmax , or techniques like SPIGOT  have been used to replace argmax. Below we discuss another alternative and our main focus.

**Beam Search Tree Recursive Neural Networks (BT-RvNN):** Seeing the above algorithm as a greedy process provides a natural extension through beam search as done in Ray Chowdhury and Caragea . BT-RvNN replaces the argmax in Gumbel-Tree RvNN with a stochastic Top-\(K\) that stochastically extracts both the \(K\) highest scoring parent candidates and the \(K\) corresponding log-softmaxed scores. The process collects all parent node compositions and also accumulates (by addition) log-softmaxed scores for each selected choices in corresponding beams. With this, the end result is \(K\) beams of accumulated scores and \(K\) beams of root node representations. The final representation is a softmax-based marginalization of the \(K\) root nodes: \(_{i})}{_{j}(s_{j})} b_{i}\) where \(b_{i}\) is the root node representation of beam \(i\) and \(s_{i}\) is the accumulated (added) log-softmaxed scores at every iteration for beam \(i\). Doing this enabled BT-RvNN to improve greatly  over greedy Gumbel-Tree recursion . However, BT-RvNN can also substantially increase the memory usage, which makes it inconvenient to use.

## 3 Bottleneck and Solution

In this section, we identify a major bottleneck in the memory usage that exists in the above framework (both for greedy-search and beam-search) that can be adjusted for.

**Bottleneck:** The main bottleneck in the above existing framework is Eqn. 3. That is, the framework runs a rather heavy recursive cell (concretely, the GRC function in Eqn. 2) **in parallel** for every item in the sequence and **for every iteration**. In contrast, RNNs could use the same GRC function but for **one position at a time** sequentially - taking very little memory. While Transformers also use similarly big feedforward networks like GRC in parallel for all hidden states - they have fixed a number of layers - whereas BT-RvNNs may have to recurse for hundreds of layers depending on the input making this bottleneck more worrisome. However, we think this bottleneck can be highly mitigated. Below, we present our proposals to fix this bottleneck.

### Efficient Beam Tree Recursive Neural Network (EBT-RvNN)

Here, we describe our new model EBT-RvNN. It extends BT-RvNN by incorporating the fixes below. We present the contrast between the previous method and our current method visually in Figure 1.

**Fix 1 (new scorer):** At any iteration \(t\), we start only with some sequence \((h_{1}^{t},,h_{n}^{t})\). In the existing framework, starting from this the function to compute the score for any child-pair will look like:

\[e_{i}=score rec(h_{i},h_{i+1})\] (5)

This is, in fact, the only reason for which we need to apply \(rec\) to all positions (in Eqn. 3) at this iteration because that is the only way to get the corresponding score; that is, currently the score computation entangles the recursive cell \(rec\) and the \(score\). However, there is no clear reason to do this. Instead we can just replace \(score rec\) (in Eqn. 5) with a single new scorer function (\(score_{new}:I\!\!R^{2 d} I\!\!R\)) that directly interacts with the concatenation of \((h_{i},h_{i+1})\) without the \(rec\) as an intermediate step - and thus disentangling it from the scorer. We use a parameter-light 2-layered MLP to replace \(score rec\):

\[e_{i}=score_{new}(h_{i},h_{i+1})=([h_{i};h_{i+1}]W_{1}^{s}+b_{1} ^{s})W_{2}^{s}+b_{2}^{s}\] (6)

Here, \(W_{1}^{s} I\!\!R^{2 d d_{s}},W_{1}^{s}2 I\!\!R^{d_{s} 1 },b_{1}^{s} I\!\!R^{d_{s}},b_{2}^{s} I\!\!R\). Since lightness of the \(score_{new}\) function is critical for lower memory usage (this has to be run in parallel for all contiguous pairs) we set \(d_{s}\) to be small (\(d_{s}=64\)). In this formulation, the \(rec\) function will be called only for the selected contiguous pairs (siblings) in Eqn. 4.

**Fix 2 (slicing):** While we already took a major step above in making BT-RvNN more efficient, we can still go further. It is unclear whether the full hidden state vector size \(d\) is necessary for parsing decisions. Parsing typically hangs on more coarse-grained abstract information - for example, when doing arithmetic while precise numerical information needs to be stored in the hidden states for future

Figure 1: Visualization of the contrast between the existing framework (left) and the proposed one (right). \(H_{1},H_{2},H_{3}\) are the input representations in the iteration. The possible contiguous pairs of them are candidate child pairs for nodes to be built in this iteration. On the left side, we see each pair is in parallel fed to the recursive cells to create their corresponding candidate parent representations. Then they are scored and one parent (\(P_{1}\)) is selected. On the right side (our approach), each child pair candidate is directly scored. The faded colored bars in \(H_{1},H_{2},H_{3}\) represent sliced away vector values. The scoring function then selects one child pair. Then only that specific selected child pair is composed using the recursive cell to create the parent representation (\(P_{1}\)) not wasting unnecessary compute by applying the recursive cell for other non-selected child pairs.

computation, the exact numerical information is not relevant for parsing - only the class of being a number should suffice. Thus, we assume that we can project the inputs into a low-dimensional space for scoring. One way to do that is to use a linear layer. However, parallel matrix multiplications on the full hidden state can be still costly when done for each hidden state in the sequence in every recursion. So, instead, we can allow the initial transformation or the RvNN itself to implicitly learn to organize parsing-related information in some sub-region of the vector. We can treat only the first \(min(d_{s},d)\) (out of \(d\)) as relevant for parsing decisions. Then we can simply slice the first \(min(d_{s},d)\) out before sending the candidate child pairs to the scoring function. Thus, the score computation can be presented as below:

\[e_{i}=score_{new}(h_{i}[0:min(d_{s},d)],h_{i+1}[0:min(d_{s},d)])\] (7)

So, now \(W^{s}_{1},d)}$} d_{s}\). As before we keep \(d_{s}\) small (\(d_{s}=64\)). Now, the total hidden state size (\(d\)) can be kept as high as needed to preserve overall representation capacity without making the computation scale as much with increasing \(d\). The model can now just learn through gradient descent to organize parsing relevant features in the first \(min(d_{s},d)\) values of the hidden states because only through them will gradient signals related to parsing scores propagate. Note that unlike , we are not running a different \(rec\) function for the parsing decisions. The parsing decision in our case still depends on the output of the same single recursive cell but from the previous iterations (if any).

**No OneSoft:** Ray Chowdhury and Caragea  also introduced a form of soft top-\(k\) function (OneSoft) for better gradient propagation in BT-RvNN. While we still use that as a baseline model, we do not include OneSoft in EBT-RvNN. This is because OneSoft generally doubles the memory usage and EBT-RvNN already runs well without it. The combination of EBT-RvNN with OneSoft can be studied more in the future, but it is a variable that we do not focus on in this study.

None of the fixes here makes any strict asymptotic difference in terms of sequence length but it does lift a large overhead that can be empirically demonstrated (see Table 1).

## 4 Beyond Sentence Encoding

As discussed before many of the previous models [9; 10; 31; 66] in this sphere that focus on competency on structure-sensitive tasks have been framed to work only as a sentence encoder of the form \(f:$}$}\). Taking a step further, we also explore a way to use bottom-up Tree-RvNNs for token-level contextualization, i.e., to make it serve as a function of the form \(f:$}$}\). This allows Tree-RvNNs to be stackable with other deep learning modules like Transformers.

Here, we consider whether we can re-formalize EBT-RvNNs for token contextualization. In EBT-RvNN5, strictly speaking, the output is not just the final sentence encoding (the root encoding), but also the intermediate non-terminal tree nodes. Previously, we ignored them after we got the root encoding. However, using them can be the key to creating a token contextualization out of EBT-RvNNs. Essentially, what EBT-RvNN will build is a tree structure with node representations - the terminal nodes being the initial token vectors, the root node being the overall sentence encoding vector, and the non-terminal nodes representing different scales of hierarchies as previously discussed.

Under this light, one way to create a token contextualization is to contextualize the terminal representations based on higher-level composite representations at different scales or hierarchies of which the terminal representation is a part of. In other words, while we use a bottom-up process of building wholes from parts during sentence encoding, for token contextualization, we can implement a top-down process of contextualizing parts from the wholes that it compose.

A similar idea is used by Teng et al.  to recursively contextualize child node representations based on their immediate parent node using another recursive cell starting from the root and ending up at the terminal node representations. The contextualized terminal node representations can then become the contextualized token representations. But this idea requires costly sequential operations.

An alternative - that we propose - is to allow the terminal nodes to attend  to the non-terminal nodes to retrieve relevant information to different scales of hierarchies. More precisely, if we want the terminal nodes to be contextualized by the wholes that they compose then we want to restrict the attention to only the parents (direct or indirect). This can be done by creating an attention mask based on the induced tree structures. In practice, we allow every terminal node as queries to attend to every node as keys and values but use a mask to allow attention only if the key represents the same node as that represented by the query or the key represents some parent (direct or indirect) of the node represented by the query. We also implement a relative positional encoding to bias attention  - using the difference in heights of the nodes as the relative distance. In essence, we are proposing the use of a form of graph attention network . This attention mechanism can be repeated for iterative refinement of the token representations through multiple layers of message-passing.

In the case of EBT-RvNNs, we can create separate token representations for each beam and then marginalize them based on beam scores. We describe our concrete setup briefly below but more details are presented in Appendix F.

**Step 1:** First, we begin with beams of representations before they were marginalized. This allows us to access discrete edges connecting parents for every beam. As a setup, we have some \(b\) beams of tree node representations and their structural information (edge connections).

**Step 2:** We use Gated Attention Unit (GAU) , a modern Transformer variant, as the attention mechanism block. We use the terminal node representations as queries (\(Q\)) and all the non-terminal nodes as keys (\(K\)) and values (\(V\)). We use GAU, like a graph attention network , by using an adjacency matrix \(A\) as an attention mask. \(A_{ij}\) is set as \(1\) if and only if \(Q_{i}\) is a child of \(K_{j}\) based on our tree extraction. Otherwise \(A_{ij}\) is set as \(0\). Thus attention is only allowed from parents to their terminal children (direct or indirect).

**Step 3:** We implement a basic relative positional encoding - similar to that of Raffel et al. . The only difference is that for us, the relative distances are the relative height distances.

**Step 4:** The GAU layer is repeated for iterative refinement of terminal node representations. We repeat for two iterations since this is an expensive step.

**Step 5:** As before, we marginalize the beams based on the accumulated log-softmax scores after the terminal node representations are contextualized.

**Use Case:** In theory, the contextualized terminal node representations that are built up in Step 5 can be used for any task like sequence labelling or masked language modeling. At this point, we explore one specific use case - sentence-pair matching tasks (natural language inference and paraphrase detection). For these tasks we have two input sequences that we need to compare. Previously we only created sentence encoding for each sequences and made the vectors interact, but now we can make the whole of two sequences of contextualized terminal-node embeddings interact with each other through a stack of GAU-based self-attention. This is an approach that we use for some of the sentence-matching tasks in Natural Language Processing (Table 3). The models are trained end to end. We discuss the technical details about these architectural setups more explicitly in Appendix F.

## 5 Experiments and Results

### Model Nomenclature

**Sentence Encoder models: Transformer** refers to Transformers ; **UT** refers to Universal Transformers ; **CRvNN** refers to Continuous Recursive Neural Network ; **OM** refers to Ordered Memory ; **BT-GRC** refers to **BT-RvNN** implemented with GRC ; **BT-GRC OS** refers to BT-GRC combined with OneSoft (OS) Top-\(K\) function ; **EBT-GRC** refers to our proposed EBT-RvNN model with GRC; **GT-GRC** refers to Gumbel-Tree RvNN  but with GRC as the recursive cell; **EGT-GRC** refers to GT-GRC plus the fixes that we propose.

**Sentence Interaction Models:** Sequence interaction models refer to the models in the style described in Section 4. These models use some deeper interaction between contextualized token representations from both sequences without bottlenecking the interactions through a pair of vectors. We use **EBT-GAU** to refer to the approach described in Section 4. **EGT-GAU** refers to a new baseline which uses the same framework as EBT-GAU except it replaces the Beam-Tree-Search with greedy STE gumbel-softmax based selection as in . **GAU** refers to a plain stack of Gated Attention Units  (made to approximate the parameters of EBT-GAU) that do not use any Tree-RvNNs and is trained directly on <SEP> concatenated sequence pairs.

### Efficiency Analysis

In Table 1, we compare the empirical time-memory trade offs of the most relevant Tree-RvNN models (particularly those that are competent in ListOps and logical inference). We use CRvNN in the no halting mode as  because otherwise it can start to halt trivially because of limited training data. For the splits of lengths \(200-1000\) we use the data shared by Havrylov et al. ; for the \(1500-2000\) split we sample from the training set of LRA listops .

We can observe from the table that EBT-GRC achieves better memory efficiency among all the strong RvNN contenders (GT-GRC and EGT-GRC fail on ListOps/Logical inference) except for OM. However, OM's memory efficiency comes with a massive cost in time, being nearly \(8\) times slower than EBT-GRC. Compared to BT-GRC OS's \(~{}43\)GB peak memory consumption in \(900\)-\(1000\) sequence length from , the memory consumption of EBT-GRC is reduced to only \(~{}2.8\)GB. Even compared to BT-GRC, the reduction is near ten times. EBT-GRC even outperforms the original greedy GT-GRC used in Choi et al. . Removing the slicing from the full model EBT-GRC (i.e., \(-\)slice) can substantially increase the memory cost. This becomes most apparent when training with higher hidden state size (compare (\(512\)) vs. (\(512\),\(-\)slice)). This shows the effectiveness of slicing.

### Results

Hyperparameters are presented in Appendix G, architecture details are presented in Appendix F, task details are provided in Appendix B and additional results (besides what is presented below) in logical inference and text classification are provided in Appendix C.

**List Operations (ListOps):** The task of ListOps consist of hierarchical nested operations that neural networks generally struggle to solve particularly in length-generalizable settings. There are only a few known contenders that achieve decent performance in the task [31; 10; 71; 66]. For this task we use the original training set  with the length generalization splits from Havrylov et al. , the argument generalization splits from Ray Chowdhury and Caragea , and the LRA test set from Tay et al. . The different splits test the model in different out-of-distribution settings (one in unseen lengths, another in an unseen number of arguments, and another in both unseen lengths and arguments). Remarkably, as can be seen from Table 2, EBT-GRC outperforms most of the previous models in accuracy - only being slightly behind OM for some argument generalization splits. EBT-GRC \(-\)Slice represents the performance of EBT-GRC without slicing. It shows that slicing in fact improves the accuracy as well in this context but even without slicing the model is better than BT-GRC or BT-GRC OS.

   &  \\ 
**Model** & \)} & \)} & \)} & \)} \\  & Time & Memory & Time & Memory & Time & Memory & Time & Memory \\  & (min) & (GB) & (min) & (GB) & (min) & (GB) & (min) & (GB) \\  OM & \(8.0\) & \(0.09\) & \(20.6\) & \(0.21\) & \(38.2\) & \(0.35\) & \(76.6\) & \(0.68\) \\ CRvNN & \(1.5\) & \(1.57\) & \(4.3\) & \(12.2\) & \(8.0\) & \(42.79\) & OOM & OOM \\ GT-GRC & \(0.5\) & \(0.35\) & \(2.1\) & \(1.95\) & \(3.5\) & \(5.45\) & \(7.1\) & \(21.76\) \\ EGT-GRC & \(1\) & \(0.07\) & \(2.5\) & \(0.3\) & \(4.3\) & \(0.81\) & \(8.5\) & \(3.15\) \\ BT-GRC & \(1.1\) & \(1.71\) & \(2.6\) & \(9.82\) & \(5.1\) & \(27.27\) & OOM & OOM \\ BT-GRC OS & \(1.4\) & \(2.74\) & \(4.0\) & \(15.5\) & \(7.1\) & \(42.95\) & OOM & OOM \\  EBT-GRC & \(1.2\) & \(0.19\) & \(3.2\) & \(1.01\) & \(5.5\) & \(2.78\) & \(10.5\) & \(10.97\) \\ \(-\)slice & \(1.2\) & \(0.35\) & \(3.2\) & \(1.95\) & \(5.4\) & \(5.4\) & \(10.3\) & \(21.12\) \\ (512) & \(1.2\) & \(0.41\) & \(3.3\) & \(1.29\) & \(5.6\) & \(3.13\) & \(12.1\) & \(11.41\) \\ (512,\(-\) slice) & \(1.2\) & \(1.55\) & \(3.3\) & \(7.77\) & \(5.5\) & \(21.02\) & OOM & OOM \\  

Table 1: Empirical time and (peak) memory consumption for various models on an RTX A6000. Ran on \(100\) ListOps data with batch size \(1\) and the same hyperparameters as used on ListOps on various sequence lengths. (-slice) indicates EBT-GRC without slicing from Fix 2, (512) indicates EBT-GRC with the hidden state dimension (\(d\)) set as \(512\) (instead of \(128\)).(512,-slice) represents EBT-GRC with \(512\) dimensional hidden state size and without slicing.

**Logical Inference and Sentiment Classification:** We show the results of our models in formal logical inference (another dataset where only RvNN-like models have shown some success) and sentiment classification in Appendix C. There we show that our EBT-GRC can easily keep up on these tasks with BT-GRC despite being much more efficient.

**NLI and Paraphrase Detection:** As we can see from Table 3, although EBT-GRC does not strictly outperform BT-GRC or BT-GRC OS, it remains in the same ballpark performance. The sentence interaction models, unsurprisingly, tend to have higher scores that sentence encoder modes because of their more parameters and more interaction space. We do not treat them commensurate here. Among the sequence interaction models, our EBT-GAU generally outperforms both baseline models in its vicinity - GAU and EGT-GAU. Even when used in conjunction with a Transformer, beam search still maintains some usefulness over simpler STE-based greedy search (EGT-GAU) and it shows some potential against pure Transformer stacks as well (GAU).

## 6 Conclusion

We identify a memory bottleneck in a popular RvNN framework  which has caused BT-RvNN  to require more memory than it needs to. Mitigating this bottleneck allows us to reduce the memory consumption of EBT-RvNN (an efficient variant of BT-RvNN) by \(10\)-\(16\) times without much other cost and while preserving similar task performances (and sometimes even beating the original BT-RvNN). The fixes also equally apply to any model using the framework including the original Gumbel Tree model . We believe our work can serve as a basic baseline and a bridge for the development of more scalable models in the RvNN family and beyond.

## 7 Limitations

Although our proposal improves upon the computational trade-offs over some of the prior works [10; 9; 66; 71], it can be still more expensive than standard RNNs although we address this limitation, to an extent, in our concurrent work . Moreover, our investigation of utilizing bottom-up Tree-RvNNs for top-down signal (without using expensive CYK models [15; 16]) is rather preliminary (efficiency being our main focus). This area of investigation needs to be focused more in the future. Moreover, although our proposal reduces memory usage it does not help much on accuracy scores compared with other competitive RvNNs.

 
**Model** & **near-IID** &  &  & **LRA** \\ (Lengths) & \(\) 1000 & 200-300 & 500-600 & 900-1000 & 100-1000 & 100-1000 & \(2000\) \\ (Arguments) & \(\) 5 & \(\) 5 & \(\) 5 & \(\) 5 & 10 & 15 & 10 \\   & & & & & & \\  GoldTreeGRC & \(99.9_{ 2}\) & \(99.9_{ 9}\) & \(99.8_{1}\) & \(100_{ 5}\) & \(81.2_{28}\) & \(79.5_{14}\) & \(78.5_{29}\) \\   & & & & & & \\  Transformer * & \(57.4_{4}\) & — & — & — & — & — & — \\ UT * & \(71.5_{78}\) & — & — & — & — & — & — \\ GT-GRC & \(75_{4.6}\) & \(47.7_{8.4}\) & \(42.7_{2.8}\) & \(37.53_{37}\) & \(50.9_{15}\) & \(51.4_{16}\) & \(45.3_{12}\) \\ EGT-GRC & \(84.2_{19}\) & \(51.3_{37}\) & \(42.9_{35}\) & \(34.4_{35}\) & \(44.7_{17}\) & \(40.8_{16}\) & \(34.4_{14}\) \\ OM & \(}\) & \(99.6_{ 7}\) & \(92.4_{13}\) & \(76.3_{13}\) & \(}\) & \(76.3_{38}\) & \(}\) \\ CRvNN & \(99.7_{ 2.8}\) & \(98.8_{11}\) & \(97.2_{23}\) & \(94.9_{49}\) & \(66.6_{40}\) & \(43.7_{38}\) & \(55.38_{44}\) \\ BT-GRC & \(99.4_{2.7}\) & \(96.8_{10}\) & \(93.6_{22}\) & \(88.4_{27}\) & \(75.2_{28}\) & \(59.1_{79}\) & \(63.4_{57}\) \\ BT-GRC OS & \(99.6_{5.4}\) & \(97.2_{35}\) & \(94.8_{65}\) & \(92.2_{86}\) & \(73.3_{64}\) & \(63.1_{92}\) & \(66.1_{101}\) \\  EBT-GRC & \(}\) & \(}\) & \(}\) & \(}\) & \(82.5_{13}\) & \(}\) & \(}\) \\ EBT-GRC \(-\) Slice & \(99.7_{3}\) & \(98.6_{12}\) & \(98.4_{17}\) & \(98.6_{14}\) & \(79.3_{20}\) & \(74.4_{37}\) & \(75.5_{25}\) \\  

Table 2: Accuracy on ListOps. For our models, we report the median of \(3\) runs. Our models were trained on lengths \(\) 100, depth \(\) 20, and arguments \(\) 5. * represents results copied from . We bold the best results that do not use gold trees. Subscript represents standard deviation. As an example, \(90_{1}=90 0.1\)

## 8 Acknowledgments

This research is supported in part by NSF CAREER award #1802358, NSF IIS award #2107518, and UIC Discovery Partners Institute (DPI) award. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF or DPI. We thank our anonymous reviewers for their constructive feedback.