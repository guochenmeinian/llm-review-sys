# end for

[MISSING_PAGE_EMPTY:1]

[Busoniu et al., 2008]. Inspired by this, we reconsider the cooperative game from an asynchronous perspective. In other words, each agent is assigned a priority (_i.e._ order) of decision-making at each step, thus the Stackelberg equilibrium (SE) [Von Stackelberg, 2010] is naturally set up as the learning objective. Specifically, the upper-level agents make decisions before the lower-level agents (Each agent represents a unique level, with upper and lower levels being relative.) Therefore, the lower-level agents can acquire the actual actions of the upper-level agents by communication and make their decisions conditioned on what the upper-level agents would do. Importantly, **we never break the fundamental dynamic, \(p(s_{t+1}|s_{t},^{1:k-1})\), in the multi-agent system.** The agents make decisions asynchronously but perform actions simultaneously as the default environment setting.

Under this setting, the SE is likely to be Pareto superior to the average Nash equilibrium (NE) in games that require a high cooperation level [Zhang et al., 2020]. However, _is it necessary to decide a specific priority of decision-making for each agent?_ Ideally, the optimal joint policy can be decomposed by any orders [Wen et al., 2019], _e.g._, \(^{*}(a_{1},a_{2}|s)=^{*}(a_{1}|s)^{*}(a_{2}|s,a_{1})=^{*}(a_{2}|s) ^{*}(a_{1}|s,a_{2})\). But during the learning process, agents are unlikely to use other agents' optimal actions for gradient calculation, making it still vulnerable to the relative overgeneralization problem [Wei et al., 2018]. This means there is no guarantee that different orders will converge to the same suboptimal. We also claim that the different priorities of decision-making may affect the optimality of the convergence of the learning algorithm in Section 3. Note that relative overgeneralization occurs when a suboptimal NE in the joint space of actions is preferred over an optimal NE because each agent's action in the suboptimal equilibrium is a better choice when matched with arbitrary actions from the cooperative agents.

This work proposes a novel multi-level communication scheme for cooperative MARL, _Sequential Communication_ (SeqComm), to enable agents to coordinate with each other explicitly. Specifically, SeqComm has two-phase communication, negotiation phase and launching phase. In the negotiation phase, agents communicate their hidden states of observations with others simultaneously. Then, they can generate multiple predicted trajectories, called _intention_, by modeling the environmental dynamics and other agents' actions. In addition, the priority of decision-making is determined by communicating and comparing the agents' intentions, which are evaluated by their state-value functions. **The value of each intention represents the predicted rewards obtained by treating that agent as the first mover of the order sequence.** The sequence of others follows the same procedure as aforementioned with the upper-level agents fixed. In the launching phase, the upper-level agents take the lead in decision-making and communicate their actual actions with the lower-level agents. The actual actions will be executed simultaneously in the environment without changes.

SeqComm is currently built on MAPPO [Yu et al., 2021]. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we evaluate SeqComm on StarCraft multi-agent challenge v2 (SMACv2) [Samvelyan et al., 2019]. We demonstrate that SeqComm outperforms existing communication-free and communication-based methods in various maps in SMACv2. By ablation studies, we confirm that treating agents asynchronously is a more effective way to promote coordination, and SeqComm can provide the proper priority of decision-making for agents to develop better coordination.

## 2 Related Work

**Communication.** Existing work [Jiang and Lu, 2018, Kim et al., 2019, Singh et al., 2019, Das et al., 2019, Zhang et al., 2019, Jiang et al., 2020, Ding et al., 2020, Konan et al., 2022] in this realm mainly focus on how to extract valuable messages. ATOC [Jiang and Lu, 2018] and IC3Net [Singh et al., 2019] utilize gate mechanisms to decide when to communicate with other agents. Several studies [Das et al., 2019, Konan et al., 2022] employ multi-round communication to fully reason the intentions of others and establish complex collaboration strategies. Social influence [Jaques et al., 2019] uses communication to influence the behaviors of others. I2C [Ding et al., 2020] only communicates with agents that are relevant and influential which are determined by causal inference. However, all these methods focus on how to exploit valuable information from current or past partial observations effectively and properly. More recently, some studies [Kim et al., 2021, Du et al., 2021, Pretorius et al., 2021] begin to answer the question: can we favor cooperation beyond sharing partial observation? They allow agents to imagine their future states with a world model and communicate those with others. IS [Pretorius et al., 2021], as the representation of this line of research, enables each agent to share its intention with other agents in the form of the encoded imagined trajectory and use the attention module to figure out the importance of the received intention. However, two concerns arise. On one hand, circular dependencies can lead to inaccurate predicted future trajectories as long as the multi-agent system treats agents synchronously. On the other hand, MARL struggles in extracting useful information from numerous messages, not to mention more complex and dubious messages, _i.e._ predicted future trajectories. Unlike these studies, we treat the agents from an asynchronous perspective, therefore, circular dependencies can be naturally resolved. Moreover, agents send actions to lower-level agents, making the messages compact and informative.

**Coordination.** The agents are essentially independent decision-makers in execution and may break ties between equally good actions randomly. Thus, in the absence of additional mechanisms, different agents may break ties in different ways, and the resulting joint actions may be suboptimal. Coordination graphs (Guestrin et al., 2002; Bohmer et al., 2020; Wang et al., 2021) simplify the coordination when the global Q-function can be additively decomposed into local Q-functions that only depend on the actions of a subset of agents. Typically, a coordination graph expresses a higher-order value decomposition among agents. This improves the representational capacity to distinguish other agents' effects on local utility functions, which addresses the miscoordination problems caused by partial observability.

Another general approach to solving the coordination problem is to make sure that ties are broken by all agents in the same way, requiring that random action choices are somehow coordinated or negotiated. Social conventions (Boutilier, 1996) or role assignments (Prasad et al., 1998) encode prior preferences towards certain joint actions and help break ties during action selection. Communication (Fischer et al., 2004; Vlassis, 2007) can be used to negotiate action choices, either alone or in combination with the aforementioned techniques. Our method follows this line of research by utilizing the ordering of agents and actions to break the ties, other than the enhanced representational capacity of the local value function.

More discussions of related work are in Appendix C.

## 3 Problem Formulation

**Cost-Free Communication.** The decentralized partially observable Markov decision process (DecPOMDP) can be extended to multi-agent POMDP (Oliehoek et al., 2016) by sharing observations among agents via communication. The joint observations are not necessarily equivalent to the state. However, joint observations can be used to represent the state better than single observations.

Previous work (Pynadath and Tambe, 2002) shows that under cost-free communication, agents would share optimal messages for mutual interest. If the communication cost is high, there is a balance between delivering all the useful messages for greater benefits and keeping the amount of communication as low as possible. In addition, analyzing this extreme case gives us some understanding of the benefit of communication, even if the results do not apply across all domains. However, even under multi-agent POMDP where agents can get joint observations, coordination problems can still arise (Busoniu et al., 2008). Suppose the centralized critic has learnt actions pairs \([a_{1},a_{2}]\) and \([b_{1},b_{2}]\) that are equally optimal. Without any prior information, the individual policies \(_{1}\) and \(_{2}\) learned from the centralized critic can break the ties randomly and may choose \(a_{1}\) and \(b_{2}\), respectively.

**Multi-Agent Sequential Decision-Making.** We consider fully cooperative multi-agent tasks that are modeled as multi-agent POMDP, where \(n\) agents interact with the environment according to the following procedure, which we refer to as _multi-agent sequential decision-making_.

At each timestep \(t\), assume the priority (_i.e._ order) of decision-making for all agents is given and each priority level has only one agent (_i.e._, agents make decisions one by one). Note that the smaller the level index, the higher priority of decision-making is. The agent at each level \(k\) gets its own observation \(o_{t}^{k}\) drawn from the state \(s_{t}\), and receives messages \(_{t}^{-k}\) from all other agents, where \(_{t}^{-k}\{\{o_{t}^{1},a_{t}^{1}\},,\{o_{t}^{k-1},a_{t}^ {k-1}\},o_{t}^{k+1},,o_{t}^{n}\}\). Equivalently, \(_{t}^{-k}\) can be written as \(\{^{-k}},_{t}^{1:k-1}\}\), where \(^{-k}}\) denotes the joint observations of all agents except \(k\) (in practice, agents communicate the hidden states/encodings of observations), and \(_{t}^{1:k-1}\) denotes the joint actions of agents \(1\) to \(k-1\). For the agent at the first level (_i.e._, \(k=1\)), \(_{t}^{1:k-1}=\). Then, the agent determines its action \(a_{t}^{k}\) sampled from its policy \(_{k}(|o_{t}^{k},_{t}^{-k})\) or equivalently \(_{k}(|o_{t},_{t}^{1:k-1})\) and sends it to the lower-level agents. After all, agents have determined their actions, they perform the joint actions\(_{t}\), which can be seen as sampled from the joint policy \((|s_{t})\)_factorized_ as \(_{k=1}^{n}_{k}(|_{t},_{t}^{1:k-1})\), in the environment and get a shared reward \(r(s_{t},_{t})\) and the state transitions to next state \(s^{}\) according to the transition probability \(p(s^{}|s_{t},_{t})\). All agents aim to maximize the expected return \(_{t=0}^{}^{t}r_{t}\), where \(\) is the discount factor. The state-value function and action-value function of the level-\(k\) agent are defined as follows:

\[V_{_{k}}(s,^{1:k-1})}_{ _{0}^{k,n}_{k:n}\\ _{1:0}}[_{t=0}^{}^{t}r_{t}|s_ {0}=s,_{0}^{1:k-1}=^{1:k-1}]\] \[Q_{_{k}}(s,^{1:k})}_{ _{0}^{k+1}_{k+1:n}\\ _{1:0}}[_{t=0}^{}^{t}r_{t}|s_ {0}=s,_{0}^{1:k}=^{1:k}].\]

For the setting of multi-agent sequential decision-making discussed above, we have the following proposition.

**Proposition 1**.: _If all the agents update their policy with individual TRPO (Schulman et al., 2015) sequentially in multi-agent sequential decision-making, then the joint policy of all agents are guaranteed to improve monotonically and converge._

Proof.: The proof is given in Appendix A. 

Proposition 1 indicates that SeqComm has the performance guarantee regardless of the priority of decision-making in multi-agent sequential decision-making. However, the priority of decision-making indeed affects the optimality of the converged joint policy, and we have the following claim.

**Claim 1**.: _The different priorities of decision-making affect the optimality of the convergence of the learning algorithm due to the relative overgeneralization problem._

We use a one-step matrix game as an example, as illustrated in Figure 1(a), to demonstrate the influence of the priority of decision-making on the learning process. Due to relative overgeneralization (Wei et al., 2018), agent \(B\) tends to choose \(b_{2}\) or \(b_{3}\). Specifically, \(b_{2}\) or \(b_{3}\) in the suboptimal equilibrium is a better choice than \(b_{1}\) in the optimal equilibrium when matched with arbitrary actions from agent \(A\). Therefore, as shown in Figure 1(b), \(B A\) (_i.e._, agent \(B\) makes decisions before \(A\), and \(A\)'s policy conditions on the action of \(B\)) and _Simultaneous_ (_i.e._, two agents make decisions simultaneously and independently) are easily trapped into local optima. However, if agent \(A\) goes first, things can be different, as \(A B\) achieves the optimum. As long as agent \(A\) does not suffer from relative overgeneralization, it can help agent \(B\) get rid of local optima by narrowing down the search space of \(B\). Besides, a policy that determines the priority of decision-making can be learned under the

Figure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. \(A B\), \(B A\), _Simultaneous_, and _Learned_ represent that agent \(A\) makes decisions first, agent \(B\) makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO (Yu et al., 2021) is used as the backbone.

guidance of the state-value function, denoted as _Learned_. It obtains better performance than \(B A\) and _Simultaneous_, which indicates that dynamically determining the order during policy learning can be beneficial as we do not know the optimal priority in advance.

**Remark 1**.: The priority (_i.e._ order) of decision-making affects the optimality of the converged joint policy in multi-agent sequential decision-making, thus it is critical to determine the order. However, learning the order directly requires an additional centralized policy in execution, which is not generalizable in a scenario where the number of agents varies. Moreover, its learning complexity exponentially increases with the number of agents, making it infeasible in many cases.

## 4 Sequential Communication

In this paper, we cast our eyes in another direction and resort to the world model, which is the dynamic model of the environment. Ideally, we can randomly sample candidate order sequences, evaluate them under the world model (see Section 4.1), and choose the order sequence that is deemed the most promising under the true dynamic. SeqComm is designed based on this principle to determine the priority of decision-making via communication.

In SeqComm, communication is separated into phases serving different purposes and multi-round communication in one phase is possible. One is the _negotiation_ phase for agents to determine the priority of decision-making. Another is the _launching_ phase for agents to act conditioning on actual actions upper-level agents will take to implement _explicit coordination via communication_. The overview of SeqComm is illustrated in Figure 2. Each SeqComm agent consists of a policy, a critic, and a world model, as illustrated in Figure 3, and the parameters of all networks are shared across agents (Gupta et al., 2017).

**World Model.** The world model is needed to predict and evaluate future trajectories. SeqComm, unlike previous works (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021), can utilize received hidden states of other agents in the first round of communication to model more precise environment dynamics for the explicit coordination in the next round of communication. Once an agent can access other agents' hidden states, it shall have adequate information to estimate their actions since all agents are homogeneous and parameter-sharing. Therefore, the world model \(()\) takes as input the joint hidden states \(_{t}=\{h_{t}^{1},,h_{t}^{n}\}\) and actions \(_{t}\), and predicts the next joint observations and reward. In practice, before the inputs pass into the world model, the attention module \(}\) is utilized to process the input.

\[}_{t+1},_{t+1}=_{i}(}(_{t}, _{t})).\]

The reason that we adopt the attention module is to entitle the world model to be generalizable in the scenarios where additional agents are introduced or existing agents are removed.

Figure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (_left_) and the launching phase (_right_). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions.

### Negotiation Phase

In the negotiation phase, the observation encoder first takes \(o_{t}\) as input and outputs a hidden state \(h_{t}\) to compress the information, which is used to communicate with others. Note that many studies (Ding et al., 2020; Jiang and Lu, 2018) found that redundant messages may impair the learning process empirically. In more detail, the model can converge slowly or sometimes lead to a worse sub-optimal. Agents then determine the priority of decision-making by _intentions_ which is established and evaluated based on the world model.

**Priority of Decision-Making.** Intention is the key element in determining the priority of decision-making. The notion of intention is described as an agent's future behavior in previous works (Rabinowitz et al., 2018; Raileanu et al., 2018; Kim et al., 2021). However, we define the _intention_ as an agent's future behavior _without considering others_.

As mentioned before, an agent's intention considering others can lead to circular dependencies and cause miscoordination. By our definition, the intention of an agent should be depicted as all future trajectories considering that agent as the first mover and ignoring the others. However, there are many possible future trajectories as the priority of the rest of the agents is _unfixed_. In practice, we use the Monte Carlo method to estimate the intention value based on all future trajectories. Note that it is uniform across priorities for unfixed agents. Each order should be treated equally since we do not have any prior for the distribution.

Taking agent \(i\) at timestep \(t\) to illustrate, it firstly considers itself as the first-mover and produces its action only based on the joint hidden states, \(^{i}_{t}_{i}(|_{}(_{t},)\), where we again use an attention module \(_{}\) to handle the input. For the order sequence of lower-level agents, we randomly sample a set of order sequences from unfixed agents. Assume agent \(j\) is the second-mover, agent \(i\) models \(j\)'s action by considering the upper-level action following its own policy \(^{j}_{t}_{i}(|_{}(_{t}, ^{i}_{t}))\). The same procedure is applied to predict the actions of all other agents following the sampled order sequence. Based on the joint hidden states and predicted actions, the next joint observations \(}_{t+1}\) can be predicted by the world model \(\). The length of the predicted future trajectory is \(H\) and it can then be written as \(^{t}=\{}_{t+1},}_{t+1},,}_{t+H}, }_{t+H}\}\) by repeating the procedure aforementioned.

Then, the agent uses its critic (state-value function) to evaluate the future trajectory and output value \(v_{^{t}}\). The intention value is defined as the average value of \(F\) future trajectories with different sampled order sequences. _Through the critic, we have linked the order and agent performance together._

After all the agents have computed their intentions and the corresponding value, they again communicate their intention values to others. Then, agents would compare and choose the agent with the highest intention value to be the first mover. The priority of lower-level decision-making follows the same procedure with the upper-level agents fixed. Note that some agents may communicate intention values with others multiple times until the priority of decision-making is finally determined.

### Negotiation Phase for Local Communication

The full communication version of SeqComm is constructed based on theoretical derivation. It has a theoretical guarantee to some extent, but some assumptions, e.g., broadcast communication, can be unrealistic and incur lots of communication overhead. Therefore, we provide another version of SeqComm in scenarios where agents can only communicate with nearby agents (agents within a limited communication range).

In more detail, the agent first calculates its intention value based only on the hidden states of nearby agents. After comparing with the intention values of nearby agents (intention values are communicated with the nearby agents), the agent can determine the upper-level and lower-level nearby agents. Unlike the previous version of SeqComm, agents cannot distinguish the detailed order sequence of the upper-level nearby agents since their communication ranges may not overlap. Therefore, the intention values are calculated and communicated among agents for _only one time_. The local communication version greatly reduces communication overhead, making it more suitable for many real applications.

For more details of the algorithms, please refer to the Appendix D for the pseudo-code.
As for the launching phase, agents communicate to obtain additional information to make decisions. Apart from the received hidden states from the last phase, we allow agents to get what _actual_ actions the upper-level agents will take in execution, while other studies can only infer others' actions by opponent modeling (Rabinowitz et al., 2018; Raileanu et al., 2018) or communicating intentions (Kim et al., 2021). Therefore, miscoordination can be naturally avoided, and a better cooperation strategy is possible since lower-level agents can adjust their behaviors accordingly.

A lower-level agent \(i\) make a decision following the policy \(_{i}(|_{a}(_{t},_{t}^{upper}))\), where \(_{t}^{upper}\) means received actual actions from all upper-level agents. As long as the agent has decided on an action, it will send the action to all other lower-level agents through the communication channel. _Note that the actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially._

### Theoretical Analysis

As intention values determine the priority of decision-making, SeqComm is likely to choose different orders _at different timesteps_ during training. However, we have the following proposition that theoretically guarantees the performance of the learned joint policy under SeqComm.

**Proposition 2**.: _The monotonic improvement and convergence of the joint policy in SeqComm are independent of the priority of decision-making of agents at each timestep._

Proof.: The proof is given in Appendix A. 

The priority of decision-making is chosen under the world model, thus the compounding errors in the world model can result in discrepancies between the predicted returns of the same order under the world model and the true dynamics. We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).

**Theorem 1**.: _Let the expected total variation between two transition distributions be bounded at each timestep as \(_{t}_{s_{s,t}}[D_{TV}(p(s^{}|s,)||(s^ {}|s,))]_{m}\), and the policy divergences at level \(k\) be bounded as \(_{s,^{1:k-1}}D_{TV}(_{,k}(a^{k}|s,^{1:k-1})||_{k}(a^ {k}|s,^{1:k-1}))_{_{k}}\), where \(_{}\) is the data collecting policy for the model and \((s^{}|s,)\) is the transition distribution under the model. Then the model return \(\) and true return \(\) of the policy \(\) are bounded as:_

\[[] []-\] \[(_{m}+2_{k=1}^{n }_{_{k}})}{(1-)^{2}}+_{k=1}^{n}_{ _{k}}}{(1-)}]}_{C(_{m},_{_{1:n}})}.\]

Proof.: The proof is given in Appendix B. 

**Remark 2**.: Theorem 1 provides a useful relationship between the compounding errors and the policy update. As long as we improve the return under the true dynamic by more than the gap, \(C(_{m},_{_{1:n}})\), we can guarantee the policy improvement under the world model. If no such policy exists to overcome the gap, it implies the model error is too high, that is, there is a large discrepancy between the world model and true dynamics. Thus the order sequence obtained under the world model is not reliable. Such an order sequence is almost the same as a random one. Though a random order sequence also has the theoretical guarantee of Proposition 2, we will show in Section 5.2 that a random order sequence leads to a poor local optimum empirically.

Figure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions.

## 5 Experiments

SeqComm is currently instantiated based on MAPPO (Yu et al., 2021). We evaluate SeqComm nine maps in StarCraft multi-agent challenge v2 (SMACv2) (Ellis et al., 2024).

In the experiments, SeqComm and baselines are parameter-sharing for fast convergence (Gupta et al., 2017; Terry et al., 2020). We have fine-tuned the baselines for a fair comparison. _The world model in the SMACv2 environment is trained from scratch and kept fine-tuned in the learning process._ Therefore, no extra prior knowledge is provided. Please refer to the Appendix for the hyperparameter settings. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.

### Results

**SMACv2.** We have evaluated our method on the _most representative and challenging_ multi-agent environment currently available. Compared with SMAC (Samvelyan et al., 2019), SMACv2 has some better properties, _i.e._ stochasticity and partial observability. In other words, agents need to cooperate more in the new environment to complete tasks, whereas they could achieve a certain success rate without cooperation in the original environment.

We have chosen nine maps for extensive evaluation and made some minor changes to the observation part of agents to make it more difficult. Specifically, the sight range of agents is reduced from \(9\) to \(3\), and agents cannot perceive any information about their allies even if they are within the sight range. NDQ (Wang et al., 2020) adopts a similar change to increase the difficulty of action coordination. The rest of the settings remain the same as the default. In summary, _we require the environment to be one where a high success rate cannot be achieved solely based on individual observations._

We also evaluate the local communication version of SeqComm. Agents can only communicate with nearby agents (agents within their communication range). Note that the map size and the total number

Figure 4: Learning curves of SeqComm and baselines in nine SMACv2 maps.

of agents restrict the number of nearby agents. As the task progresses, the number of nearby agents is from 2 to 4.

**Analysis.** The learning curves of SeqComm and the baselines in terms of the win rate are illustrated in Figure 4. All communication-based methods perform better than communicaion-free method (MAPPO). In easy scenarios, communication may not be very useful, but experiments have shown that in cases with significant partial observability and stochasticity, communication can greatly enhance agent ability.

We compare our method with TarMAC , which holds a similar position in communication settings to that of MAPPO in communication-free settings. SeqComm outperforms TarMAC in all maps, which verifies the gain of explicit action coordination. Moreover, the full-communication version performs better than the local-communication version because the former can access more information. However, it also costs more communication overhead.

### Ablation Studies

**Priority of Decision-Making.** We primarily want to contribute a practical version to the community. Moreover, _the fewer communicative agents there are, the fewer possible orders there are, thus increasing the probability of randomly obtaining a good order_. It would be more meaningful to demonstrate that devoting effort to finding a good order is still important in such a scenario. Therefore, we do the ablation study for the local version of the SeqComm. In more detail, we compare SeqComm with two ablation baselines: the priority of decision-making is determined randomly at each timestep, denoted as _Random_, and agents only access the observations of others during training and execution, denoted as _No action_.

As depicted in Figure 7, SeqComm achieves a higher win rate than _Random_ and _No action_ in all the maps. These results verify the importance of the priority of decision-making and the necessity to adjust it continuously during one episode. It is also demonstrated that SeqComm can provide a proper priority of decision-making. As discussed in Section 4.4, although _Random_ also has the theoretical guarantee, they converge to poor local optima in practice. Surprisingly, in most tasks, _Random_ performs worse than _No action_. It again verifies that a bad order may fail to improve coordination or even impair it.

**Communication Range.** We also conduct experiments to demonstrate the impact of different communication ranges. We set communication ranges to {1, 3, 9}, in addition to the default range of 6. We notice a steady improvement in performance as the communication range increases. Therefore, the choice of communication range is a trade-off between communication overhead and agent performance. In our previous experiments, we choose a compromise value of 3 for the local version to validate the effectiveness of our method. Results refer to Figure 5.

Figure 5: Ablation studies of the communication ranges.

Figure 6: Ablation studies on the network mechanisms.

**Network Mechanism.** We replaced the attention mechanism for local communication with an aggregation method. In more detail, messages are concatenated and passd into a five-layer linear neural networks. The curve is based on 3 random seeds and tested on the terran 10v10 map. The results refer to Figure 6.

## 6 Conclusions

We have proposed SeqComm, which enables agents to coordinate well and explicitly with each other, and it, from an asynchronous perspective, allows agents to make decisions sequentially. A two-phase communication scheme has been adopted to determine the priority of decision-making and transfer messages accordingly. Empirically, it is demonstrated that SeqComm outperforms baselines in a variety of cooperative multi-agent scenarios.

**Limitations.** The assumption of accessing the local observation of any other agent could be strong since it is unsuitable for all applications. Thus, we provide a local communication version of SeqComm for assumption relaxation in the experiment.