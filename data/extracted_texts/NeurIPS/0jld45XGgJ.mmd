# Neural Collapse versus Low-rank Bias:

Is Deep Neural Collapse Really Optimal?

 Peter Sukenik

Institute of Science and Technology Austria

3400 Klosterneuburg, Austria

peter.sukenik@ista.ac.at

Christoph Lampert

Institute of Science and Technology Austria

3400 Klosterneuburg, Austria

chl@ista.ac.at

&Marco Mondelli

Institute of Science and Technology Austria

3400 Klosterneuburg, Austria

marco.mondelli@ista.ac.at

Equal contribution

###### Abstract

Deep neural networks (DNNs) exhibit a surprising structure in their final layer known as neural collapse (NC), and a growing body of works has currently investigated the propagation of neural collapse to earlier layers of DNNs - a phenomenon called deep neural collapse (DNC). However, existing theoretical results are restricted to special cases: linear models, only two layers or binary classification. In contrast, we focus on non-linear models of arbitrary depth in multi-class classification and reveal a surprising qualitative shift. As soon as we go beyond two layers or two classes, DNC stops being optimal for the deep unconstrained features model (DUFM) - the standard theoretical framework for the analysis of collapse. The main culprit is a low-rank bias of multi-layer regularization schemes: this bias leads to optimal solutions of even lower rank than the neural collapse. We support our theoretical findings with experiments on both DUFM and real data, which show the emergence of the low-rank structure in the solution found by gradient descent.

## 1 Introduction

What is the geometric structure of layers and learned representations in deep neural networks (DNNs)? To address this question, Papyan et al.  focused on the very last layer of DNNs at convergence and experimentally measured what is now widely known as Neural Collapse (NC). This phenomenon refers to four properties that simultaneously emerge during the terminal phase of training: feature vectors of training samples from the same class collapse to the common class-mean (NC1); the class means form a simplex equiangular tight frame or an orthogonal frame (NC2); the class means are aligned with the rows of the last layer's weight matrix (NC3); and, finally, the classifier in the last layer is a nearest class center classifier (NC4). Since the influential paper , a line of research has aimed at explaining the emergence of NC theoretically, mostly focusing on the unconstrained features model (UFM) . In this model, motivated by the network's perfect expressivity, one treats the last layer's feature vectors as a free variable and explicitly optimizes them together with the last layer's weight matrix, "peeling off" the rest of the network [9; 23]. With UFM, the NC was demonstrated in a variety of settings, both as the global optimum and as the convergence point of gradient flow.

The emergence of the NC in the last layer led to a natural research question - does some form of collapse propagate beyond the last layer to earlier layers of DNNs? A number of empirical works[20; 17; 43; 40; 35] gave evidence that this is indeed the case, and we will refer to this phenomenon as Deep Neural Collapse (DNC). On the theoretical side, the optimality of the DNC was obtained _(i)_ for the UFM with two layers connected by a non-linearity in , _(ii)_ for the UFM with several linear layers in , and _(iii)_ for the deep UFM (DUFM) with non-linear activations in the context of binary classification . No existing work handles the general case in which there are _multiple classes_ and the UFM is _deep_ and _non-linear_.

In this work, _we close the gap and reveal a surprising behavior_ not occurring in the simpler settings above: for multiple classes and layers, the DNC as formulated in previous works is _not an optimal solution_ of DUFM. In particular, the class means at the optimum do not form an orthogonal frame (nor an equiangular tight frame), thus violating the second property of DNC.

Let \(L\) and \(K\) denote the number of layers and classes, respectively. Then, if either \(L 3\) and \(K 10\) or \(L 4\) and \(K 6\), we provide an explicit combinatorial construction of a class of solutions that outperforms DNC. Specifically, the loss achieved by our construction is a factor \(K^{(L-3)/(2L+2)}\) lower than the loss of the DNC solution. Our result holds as long as all matrices are regularized.

We also identify the reason behind the sub-optimality of DNC: a _low-rank bias_. Intuitively, this bias arises from the representation cost of a DNN with \(l_{2}\) regularization, which equals the Schatten-\(p\) quasi norm  in the deep linear case. The quasi norm is well approximated by the rank, and this intuition carries over to the non-linear case as well. In fact, the rank of our construction is \(()\), while the rank of the DNC solution is \(K\). We note that after the application of the ReLU, the rank of the final layer is again equal to \(K\), in order to fit the training data. We also show that the first property of neural collapse (convergence to class means) continues to be strictly optimal even in this general setting and its deep counterpart is approximately optimal with smoothed ReLU activations.

We support our theoretical results with empirical findings in three regimes: _(i)_ DUFM training, _(ii)_ training on standard datasets (MNIST , CIFAR-10 ) with DUFM-like regularization, and _(iii)_ training on standard datasets with standard regularization. In all cases, gradient descent retrieves solutions with very low rank, which can exhibit symmetric structures in agreement with our combinatorial construction, see e.g. the lower-right plot of Figure 4. We also investigate the effect of three common hyperparameters - weight decay, learning rate and width - on the rank of the solution at convergence. On the one hand, high weight decay, high learning rate and small width lead to a strong low-rank bias. On the other hand, small (yet still non-zero!) weight decay, small learning rate or large width (and more complex datasets as well) lead to a higher-rank solution, even if that is not the global optimum, and this solution often coincides with DNC, which is in agreement with earlier experimental evidence. Altogether, our findings show that if a DNC solution is found, it is not because of its global optimality, but just because of an implicit bias of the optimization procedure.

The implications of our results go beyond _deep_ neural collapse. In fact, our theory suggests that even the NC2 in the last layer is not optimal, and this is corroborated by our experiments, where the singular value structure of the last layer's class-mean matrices is imbalanced, ruling out orthogonality. This means that standard single-layer UFM, as well as its deep-linear or two-layer extensions, are not sufficient to describe the full picture, as they display a qualitatively different phenomenology.

## 2 Related work

Neural Collapse.Several papers (a non-exhaustive list includes [12; 15; 5; 32; 33; 58]) use neural collapse as a practical tool in applications, among which OOD detection and transfer learning are the most prevalent. On the theoretical side, the emergence of NC has been investigated, with the majority of works considering some form of UFM [36; 9]. [55; 34] show global optimality of NC under the cross-entropy (CE) loss, and  under the MSE loss. Similar results are obtained by [9; 49; 18; 8] for the class-imbalanced setting. [61; 23; 59] refine the analysis by showing that the loss landscape of the UFM model is benign - all stationary points are either local minima or strict saddle points which can be escaped by conventional optimizers. A more loss-agnostic approach connecting CE and MSE loss is considered in . NC has also been analyzed for a large number of classes , in an NTK regime , or in graph neural networks . We refer the reader to  for a survey.

The emergence of NC has also been studied through the lens of the gradient flow dynamics.  considers MSE loss and small initialization, and  a renormalized gradient flow of the last layer's features after fixing the last layer's weights to be conditionally optimal.  studies the CE loss dynamics and shows convergence in direction of the gradient flow to a KKT point of the max-marginproblem of the UFM, extending a similar analysis for the last layer's weights in . The convergence speed under both losses is described in . Going beyond UFM,  study the emergence of NC in homogeneous networks under gradient flow;  provides sufficient conditions for neural collapse; and  perturbs the unconstrained features to account for the limitations of the model.

More recently,  mentions a possible propagation of the NC to earlier layers of DNNs, giving preliminary measurements. These are then significantly extended in , which measure the emergence of some form of DNC in DNNs. On the theoretical front, an extension to a two-layer non-linear model is provided in , to a deep linear model in  and to a deep non-linear model for binary classification in . Alternatively to DUPM,  studies DNC in an end-to-end setting with a special layer-wise training procedure.

Low-rank bias.The low-rank bias is a well-known phenomenon, especially in the context of matrix/tensor factorization and deep linear networks (see e.g. ). For non-linear DNNs,  studies the gradient flow optimization of ReLU networks, giving lower and upper bounds on the average soft rank.  studies SGD training on deep ReLU networks, showing upper bounds on the rank of the weight matrices as a function of batch size, weight decay and learning rate.  proves several training invariances that may lead to low-rank, but the results require the norm of at least one weight matrix to diverge and the architecture to end with a couple of linear layers.  presents bounds on the singular values of non-linear layers in a rather generic setting, not necessarily at convergence. More closely related to our work is , which considers a deep linear network followed by a single non-linearity and then by a single layer. Their arguments to study the low-rank bias are similar to the intuitive explanation of Section 4.  shows that increasing the depth results in lower effective rank of the penultimate layer's Gram matrix both at initialization and at convergence. The true rank is also measured, but on rather shallow networks and it is far above the DNC rank.  shows a strong low-rank bias of sharpness-aware minimization, although only in layers where DNC does not yet occur and the rank is high.  study special functional ranks (Jacobi and bottleneck) of DNNs, providing asymptotic results and empirical measurements. These results are refined in , which show a bottleneck structure of the rank both experimentally and theoretically. The measurements of the singular values at convergence in  are in agreement with those of Section 6.3. We highlight that _none_ of the results above allows to reason about DNC optimality, as they focus on infinite width/depth, effective or functional ranks, orthogonal settings, or are not quantitative enough.

## 3 Preliminaries

We study the class balanced setting with \(N=Kn\) samples from \(K\) classes, \(n\) per class. Let \(f(x)=W_{L}(W_{L-1}( W_{1}(x)))\) be a DNN with backbone \(()\). The backbone represents the majority of the deep network _before_ the last \(L\) layers, e.g. the convolutional part of a ResNet20. Let \(X^{d N}\) be the training data, and \(H_{1}=(X)^{d_{1} N},H_{2}=(W_{1}H_{1}) ^{d_{2} N},,H_{L}=(W_{L-1}H_{L-1})^{ d_{L} N}\) its feature vector representations in the last \(L\) layers, with \(_{l}\) denoting their counterparts before applying the ReLU \(\). We refer to \(h_{ci}^{l}\) and \(_{ci}^{l}\) as to the \(i\)-th sample of \(c\)-th class of \(H_{l}\) and \(_{l}\), respectively. Let \(_{c}^{l}=_{i=1}^{n}h_{ci}^{l}\) and \(_{c}^{l}=_{i=1}^{n}_{ci}^{l}\) be the class means at layer \(l\) after and before applying \(\), and \(M_{l},_{l}\) the matrices of the respective class means stacked into columns. We organize the training samples so that the labels \(Y^{K N}\) equal \(I_{K}_{n}^{T}\), where \(I_{K}\) is a \(K K\) identity matrix, \(\) is the Kronecker product and \(_{n}\) the all-one vector of size \(n\).

Deep neural collapse (DNC).As there are no biases in our network model, the second property of DNC requires the class mean matrices to be orthogonal (instead of forming an ETF) .

**Definition 1**.: _We say that layer \(l\) exhibits DNC 1, 2 or 3 if the corresponding conditions are satisfied (the properties can be stated for both after and before the application of ReLU):_

* _The within-class variability of either_ \(H_{l}\) _or_ \(_{l}\) _is_ \(0\)_. Formally,_ \(h_{ci}^{l}=h_{cj}^{l},_{ci}^{l}=_{cj}^{l}\) _for all_ \(i,j[n]\) _or, in matrix notation,_ \(H_{l}=M_{l}_{n}^{T},_{l}=_{l} _{n}^{T}\)_._
* _The class-mean matrices_ \(M_{l},_{l}\) _are orthogonal, i.e.,_ \(M_{l}^{T}M_{l} I_{K},_{l}^{T}_{l} I_{K}\)_._
* _The rows of the weight matrix_ \(W_{l}\) _are either 0 or collinear with one of the columns of the class-means matrix_ \(M_{l}\)_._

Deep unconstrained features model.To define DUPM, we generalize the model in  to an arbitrary number of classes \(K\).

**Definition 2**.: _The \(L\)-layer deep unconstrained features model (\(L\)-DUFM) denotes the following optimization problem:_

\[_{H_{1},W_{1},,W_{L}}\|W_{L}(W_{L-1}(  W_{2}(W_{1}H_{1})))-Y\|_{F}^{2}+_{l=1}^{L}}}{2}\|W_{l}\|_{F}^{2}+}}{2} \|H_{1}\|_{F}^{2},\] (1)

_where \(\|\|_{F}\) denotes the Frobenius norm and \(_{H_{1}},_{W_{1}},,_{W_{L}}>0\) are regularization parameters._

## 4 Low-rank solutions outperform deep neural collapse

Intuitive explanation of the low-rank bias.Consider a simplified version of \(L\)-DUFM:

\[_{H_{1},W_{1},,W_{L}}\|W_{L}(W_{L-1} W_ {2}W_{1}H_{1})-Y\|_{F}^{2}+_{l=1}^{L}}}{2} \|W_{l}\|_{F}^{2}+}}{2}\|H_{1}\|_{F }^{2}.\] (2)

Compared to (1), (2) removes all non-linearities except in the last layer, making the remaining part of the network a deep linear model, a construction similar to the one in . Now, we leverage the variational form of the Schatten-\(p\) quasi-norm , which gives

\[c\|_{L}\|_{S_{2/L}}^{2/L}=_{H_{1},W_{1},,W_{L-1} :H_{1}W_{1} W_{L-1}=_{L}}_{l=1}^{L-1}}} {2}\|W_{l}\|_{F}^{2}+}}{2}\|H_{1}\| _{F}^{2},\]

where \(c\) can be computed explicitly. Thus, after solving for \(H_{1},W_{1},,W_{L-1}\), the simplified \(L\)-DUFM problem (2) can be reduced to

\[_{_{L}W_{L}}\|W_{L}(_{L})-Y \|_{F}^{2}+}}{2}\|W_{L}\|_{F}^{2}+_{L}}}{2}\|_{L}\|_{S_{2/L}}^{2/L}.\]

For large values of \(L\), \(\|_{L}\|_{S_{2/L}}^{2/L}\) is well approximated by the rank of \(_{L}\). Hence, the objective value is low when the output \(W_{L}H_{L}\) fits \(Y\) closely, while keeping \(_{L}\) low-rank, which justifies the low-rank bias. Crucially, the presence of additional non-linearities in the \(L\)-DUFM model (1) does not change this effect much, as long as one is able to define solutions for which most of the intermediate feature matrices \(_{l}\) are non-negative (so that ReLU does not have an effect).

Low-rank solution outperforming DNC.We define the combinatorial solution that outperforms DNC, starting from the graph structure on which the construction is based.

**Definition 3**.: _A triangular graph \(_{n}\) of order \(n\) is a line graph of a complete graph \(_{n}\) of order \(n\). \(_{n}\) has \(\) vertices, each representing an edge of the complete graph, and there is an edge between a pair of vertices if and only if the corresponding edges in the complete graph share a vertex. Moreover, let \(T_{n}\) be the normalized incidence matrix of \(_{n},\) i.e., \((T_{n})_{i,j}=}\) if vertex \(i\) belongs to edge \(j\) and 0 otherwise. Let \(G_{n}\) denote the adjacency matrix of \(_{n}.\)_

We recall that \(_{n}\) is a strongly regular graph with parameters \((n(n-1)/2,2(n-2),n-2,4)\) and spectrum \(2(n-2)\) with multiplicity 1, \(n-4\) with multiplicity \(n-1\) and \(-2\) with multiplicity \(n(n-3)/2.\) Next, we construct an explicit solution \((H_{1},W_{1},,W_{L})\) based on the triangular graph. For ease of exposition, we focus on the case where the number of classes \(K\) equals \(\) for some \(r 4,\) deferring the general definition to Appendix A.1.

**Definition 4**.: _Let \(K=\) for \(r 4\). Then, a strongly regular graph (SRG) solution of the \(L\)-DUFM problem (1) is obtained by setting the matrices \((H_{1},W_{1},,W_{L})\) as follows:_

* _For all_ \(l,\) _the feature matrices_ \(H_{l},_{l}\) _are DNC1 collapsed, i.e.,_ \(H_{l}=M_{l}_{n}^{T},_{l}=_{l} _{n}^{T}.\)__
* _For_ \(2 l L-1\)_,_ \(M_{l}=_{l}\)_, each row of_ \(_{l}\) _is a non-negative multiple of a row of_ \(T_{r}\) _(as in Definition_ 3_), and the sum of squared norms of the rows of_ \(_{l}\) _corresponding to a row of_ \(T_{r}\) _is the same for each row of_ \(T_{r}\)_. Since_ \(_{l}\) _is entry-wise non-negative,_ \(M_{l}=_{l}.\)__
* _For_ \(l=1,\)__\(W_{1},M_{1}\) _are any pair of matrices minimizing the objective conditionally on_ \(M_{2}\) _defined above._* _For_ \(l 2\)_,_ \(W_{l}\) _minimizes the objective conditional to input and output to that layer._
* _As for the last layer_ \(L\)_, let_ \(A_{L}\) _be a_ \(K r\) _matrix where the set of rows equals the set of vectors with two_ \((-1)\) _entries and_ \(r-2\)__\((+1)\) _entries. Then,_ \(M_{L}=(_{L})\)_, the rows of_ \(_{L}\) _are a non-negative multiple of_ \(A_{L}T_{r}\)_, and the sum of their squared norms corresponding to either row of_ \(A_{L}T_{r}\) _is equal._
* _Finally, the Frobenius norms (i.e. scales) of_ \(M_{1},W_{1},,W_{L}\) _are chosen so as to minimize (_1_) while satisfying the construction above._

In this construction, columns and rows of class-mean matrices are associated to edges and vertices of the complete graph \(_{r}\). Each row (corresponding to a vertex) has non-zero entries at columns that correspond to edges containing the vertex. In the final layer, each row of \(_{L}\) corresponds to a weighting of vertices in \(_{r}\) s.t. exactly two vertices get \(-1\) weight and the rest \(+1\), and the value at a column is the sum of the values of the vertices of the edge. The class-mean matrices of the SRG solution are illustrated in Figure 1 for \(L=4\) and \(K=10\) (which gives \(r=5\)): we display \(M_{3},_{4},_{4}^{T}_{4}\) and, for comparison, also \(_{4}^{T}_{4}\) of a DNC solution. Very similar solutions to SRG are shown for \(K=6\) and \(K=15\) in Figures 7 and 8 of Appendix B.1.

Let us highlight the properties of the SRG solution, which are crucial to outperform DNC. First, the rank of the intermediate feature and weight matrices is very low, only of order \((K^{1/2})\), since by construction there are only \(r=(K^{1/2})\) linearly independent rows. This is contrasted with the DNC solution that has rank \(K\) in all intermediate feature and weight matrices. The low rank of the SRG solution is due to the specific structure of the triangular graph, which has many eigenvalues equal to \(-2\) that become 0 after adding twice a diagonal matrix. Second, the definition of \(_{L}\) ensures that \(M_{L}=(_{L})\) has full rank \(K\). This allows the output \(W_{L}M_{L}\) to also have full rank and, therefore, fit the identity matrix \(I_{K}\), thus reducing the first term in the loss (1). Finally, the highly symmetric nature of the SRG solution balances the feature and weight matrices so as to minimize large entries and, therefore, the Frobenius norms, thus reducing the other terms in the loss (1).

Main result.For any \(L\)-DUFM problem (specified by \(K,n\) and all the regularization parameters), let \(_{SRG},_{DNC}\) be the losses incurred by the SRG and DNC solutions, see Definitions 4 and 1, respectively. At this point we are ready to state our key result.

**Theorem 5**.: _If \(K 6,L 4\) or \(K 10,L=3\) and \(d_{l} K\) for all \(l\), then \(_{SRG}<_{DNC}\). Moreover, consider any sequence of \(L\)-DUFM problems for which \(K\) so that \(0.499>_{DNC}\) for each problem. In that case,_

\[_{SRG}}{_{DNC}}=(K^{}).\] (3)

In words, as long as the number of classes and layers is not too small, the SRG solution always outperforms the collapsed one and the gap grows with the number of classes \(K\).

Figure 1: Strongly regular graph (SRG) solution with \(L=4\), \(K=10\) and \(r=5\). **Left:** Class-mean matrix of the third layer \(M_{3}\). The non-zero entries of each row have the same value and their number is \(r-1\), which corresponds to the degree of the complete graph \(_{r}\). **Middle:** Class-mean matrix of the fourth layer before ReLU \(_{4}\) (**middle left**), and its Gram matrix \(_{4}^{T}_{4}\) (**middle right**). The SRG construction has very low rank before ReLU: \((_{4})=r\) and \(((_{4}))=K\). **Right:**\(_{4}^{T}_{4}\) for DNC. The DNC solution has rank \(K\) in all layers before and after ReLU.

The proof first computes the conditionally optimal values of \(\|W_{l}\|_{F}^{2}\) for both the SRG and DNC solutions. The specific structure of these solutions enables to calculate pseudoinverses of the intermediate features, thus enabling the explicit computation of the weight norms. All these values depend only on the singular values of the feature matrices, which are explicitly given by their scale. As a result, both \(_{SRG}\) and \(_{DNC}\) are expressed via an optimization problem in a single scalar variable and, by comparing these problems, the statement follows. The details are deferred to Appendix A.1.

Although the argument requires \(L=3,K 10\) or \(L 4,K 6\), the experiments in Appendix B.1 show that the DNC solution is not optimal when \(L 4,K 3\) or \(L=3,K 7\). Furthermore, for \(L=3\) and large \(K\), there is a large gap between \(_{SRG}\) and \(_{DNC}\) (even if (3) trivializes). For either \(K=2\) or \(L=2\), the DNC is optimal, as shown in .

## 5 Within-class variability collapse is still optimal

While the DNC2 property conflicts with the low-rank bias, the same is not true for DNC1, as the within-class variability collapse supports a low rank. We show below that the last-layer NC1 property remains optimal for any \(L\)-DUPM problem. A proof sketch follows, with the complete argument deferred to Appendix A.2.

**Theorem 6**.: _The optimal solutions of the \(L\)-DUPM (1) exhibit DNC1 at layer \(L\), i.e.,_

\[H_{L}^{*}=M_{L}^{*}_{n}^{T}\]

_holds for any optimal solution \((H_{1}^{*},W_{1}^{*},,W_{L}^{*})\) of the \(L\)-DUPM problem._

Proof sketch:.: Assume by contradiction that there exists an optimal solution of (1) with regularization parameters \((_{H_{1}},_{W_{1}},,_{W_{L}})\), denoted as \((H_{1}^{*},W_{1}^{*},,W_{L}^{*})\), which does not exhibit neural collapse at layer \(L\). Then, we can construct two _different_ optimal solutions of the \(L\)-DUPM problem with \(n=1\) and regularization parameters \((n_{H_{1}},_{W_{1}},,_{W_{L}})\) of the form \((H_{1}^{(1)},W_{1}^{*},,W_{L}^{*})\) and \((H_{1}^{(2)},W_{1}^{*},,W_{L}^{*})\). These two solutions share the weight matrices, and \(H_{1}\) (and, therefore, \(H_{L}\)) only differs in a single column (w.l.o.g., the first column). The optimality of these solutions can be proved using separability and symmetry of the loss function w.r.t. the columns of \(H_{1}\).

Denote the first (differing) columns of \(H_{L}^{(1)}\) and \(H_{L}^{(2)}\) as \(x\) and \(y\), respectively. By exploiting the linearity of the loss function on a ray \(\{th_{11}^{1},t 0\}\) for any \(h_{11}^{1}\), a direct computation gives that \(x\) and \(y\) are not aligned. Let \(\) be the loss in (1). By optimality of both solutions, we get

\[.}{ W_{L}}|_{(H_{1},W_{1},,W_ {L})=(H_{1}^{(1)},W_{1}^{*},,W_{L}^{*})}=0=.}{ W_{L}}|_{(H_{1},W_{1},,W_{L})=(H_{1}^{(2)},W_{1}^{*}, ,W_{L}^{*})}.\] (4)

An application of the chain rule gives

\[}{ W_{L}}=_{F}}{ _{L+1}}_{L+1}}{ W_{L}}+ _{W_{L}}W_{L}=_{F}}{_{L+1}}H_{L}^{T }+_{W_{L}}W_{L},\]

where \(_{L+1}\) is the model output and \(_{F}\) the first term of \(\), corresponding to the label fit. Plugging this back into (4) and using that \(W_{L}^{*}\) is the same in both expressions, we get \(A(H_{L}^{(1)})^{T}=B(H_{L}^{(2)})^{T}\), where we have denoted by \(A\) and \(B\) the partial derivatives \(_{F}}{_{L+1}}\) evaluated at \((H_{1}^{(1)},W_{1}^{*},,W_{L}^{*})\) and \((H_{1}^{(2)},W_{1}^{*},,W_{L}^{*})\), respectively. As \(_{F}\) is separable with respect to the columns of \(H_{l},_{l}\) for all \(l\), the matrices \(A,B\) can only differ in their first columns (denoted by \(a,b\)), and they are identical otherwise. This implies that \(ax^{T}=by^{T}\). After some simple considerations and using that \(x\) and \(y\) are not aligned, we reach a contradiction, as we conclude that \(x y\) is impossible. 

The difficulty in extending Theorem 6 to a result on the unique optimality of DNC1 for all layers stems from the special role of \(W_{L}\) as the loss is differentiable w.r.t. it. By considering a differentiable relaxation of ReLU, we show below an approximate result for a _relaxed_\(L\)-DUPM model.

**Definition 7**.: _We denote by \(_{}\) (or \(_{}\)) a function satisfying the following conditions: (i) \(_{}(x)=(x)\), for \(x(-,0][,)\), (ii) \(0<_{}(x)<(x)\) for \(x(0,)\), and (iii) \(_{}\) is continuously differentiable with derivative bounded by a universal constant and strictly positive on \((0,)\)._

**Theorem 8**.: _Denote by \(L\)-DUFM\({}_{}\) the equivalent of (1), with \(\) replaced by \(_{}\). Let \(D=\{d_{2},d_{3},,d_{L}\}\) and \(=_{H_{1}}_{W_{1}}_{W_{L}}\), with the regularization parameters upper bounded by \(1/(L+1)\). Then, for any globally optimal solution of the \(L\)-DUFM\({}_{}\) problem, the distance between any two feature vectors of the same class in any layer is at most_

\[}{(L+1)^{L+1}}.\] (5)

In words, as the activation function approaches ReLU (i.e., \( 0\)), the within-class variability tends to \(0\). The proof starts with a similar strategy as the argument of Theorem 6 and then explicitly tracks the error due to replacing \(\) with \(_{}\) through the layers. The full argument is in Appendix A.2.

## 6 Numerical results

We employ the standard DNC1 metric \((_{W})/(_{B})\), where \(_{W},_{B}\) are the within and between class variabilities. This is widely used in the literature [52; 43; 4] and considered more stable than other metrics . We measure the DNC2 metric as the condition number of \(M_{l}\) for \(l 1\). We do not measure DNC3 here, as it is not well-defined for solutions that do not satisfy DNC2. For end-to-end DNN experiments, we employ a model from  where an MLP with a few layers is attached to a ResNet20 backbone. The output of the backbone is then treated as unconstrained features, and DNC metrics are measured for the MLP layers.

### DUFM training

We start with the \(L\)-DUFM model (1), training both features and weights. In the top row of Figure 2, we consider a \(4\)-DUFM, with \(K=10\) and \(n=50\), presenting the training progression of the losses (left plot), the DNC1 metrics (center plot) and the singular values at convergence (right plot).

The results are in excellent agreement with our theory. First, the training loss outperforms that of the DNC solution, and it is rather close to that of the SRG solution. Second, DNC1 holds in a clear way in all layers, especially in the last ones. Third, the solution at convergence exhibits a strong low rank bias: the ranks of intermediate layers range from 5 to 8, and they are always the same in all intermediate layers within one run. For comparison, we recall that the intermediate layers of the DNC solution have full rank \(K=10\). Third, for a few runs, the Gram matrices of the intermediate class means resulting from gradient descent training coincide with those of an SRG solution. Finally we highlight that, similarly to our theory, the solutions found in all our experiments in the entire Section 6 have non-negative pre-activations in all intermediate layers of the MLP head except the last one.

Impact of number of classes and depth.For \(K=2\) or \(L=2,\) we recover the results of [48; 51] irrespective of other hyperparameters. The higher the number of classes, the more prevalent are low-rank solutions, while finding DNC solutions becomes challenging. The same holds for increasing the number of layers. For \(L=3\) and low number of classes (\(K 6\)), we weren't able to experimentally find solutions that would outperform DNC, which aligns nicely with the fact that SRG outperforms DNC only from \(K=10\) for \(L=3\). For large number of classes, the difference between the loss of low-rank solutions and the DNC loss is considerable already for \(L=3\) and becomes even larger for higher \(L\). This is illustrated in the left plot of Figure 3.

For \(L 5\) and moderate number of classes (\(K 30\)), gradient descent solutions are as follows: until layer \(L-1\), feature matrices share the same rank and have similar Gram matrices; intermediate activations are typically non-negative, and the ReLU has no effect; then, the rank jumps to \(K\) after the final ReLU, as pre-activations are also negative. For large \(L\) or large \(K,\) the rank of the first few layers is low, growing gradually in the last couple of layers (see Figure 6 in Appendix B.1); the ReLU is active only in the final layers. This means that not only very low-rank solutions outperform DNC (as shown by our theory), but such solutions are routinely reached by gradient descent.

Impact of weight decay and width.While neither weight decay nor width influence Theorem 5 - which shows that DNC is not optimal - both quantities influence the nature of the solutions found by gradient descent. In particular, the stronger the weight decay, the lower the rank, see the middle plot in Figure 3. For very small weight decay, DNC is sometimes recovered; for very high weight decay,it is never recovered. The width has an opposite effect, see the right plot of Figure 3. For small width, low-rank solutions are much more likely to be found; large width has a strong implicit bias towards DNC and, thus, rank \(K\) solutions. This means that, surprisingly, a larger width leads to a larger loss, since low-rank solutions exhibit a smaller loss than DNC. Thus, at least in DUPM, the infinite-width limit prevents gradient descent from finding a globally optimal solution, and sub-optimal solutions are reached with increasingly high probability.

### End-to-end experiments with DUPM-like regularization

Next, we train a DNN backbone with an MLP head, regularizing _only_ the output of the backbone and the layers of the MLP head (and not the layers of the backbone). This regularization is closer to our theory than the standard one, since we explicitly regularize the Frobenius norm of the unconstrained features. We also note that training with such a regularization scheme is easier than training with the standard regularization scheme. In the bottom row of Figure 2, we consider a ResNet20 backbone with a 4-layer MLP head trained on CIFAR10.

Figure 3: All experiments refer to the training of an \(L\)-DUFM model. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation. **Left:** Ratio between SRG and DNC loss (\(_{SRG}/_{DNC}\)), as a function of \(r\), where the number of classes is \(K=\). Different curves correspond to different values of \(L\{3,4,5\}\). **Middle:** Average rank at convergence, as a function of the weight decay in \(_{2}\)-scale, when \(L=4\) and \(K=15\). **Right:** Empirical probability of finding a DNC solution as a function of the width, when \(L=4\) and \(K=10\).

Figure 2: Training loss compared against DNC and SRG losses **(left)**, DNC1 metric training progression **(middle)** and singular value distribution at convergence **(right)**. **Top row:** 4-DUFM training with \(K=10\), \(=0.004\) for all regularization parameters, learning rate of \(0.5\) and width \(30\). Results are averaged over 10 runs, and we show the confidence intervals at \(1\) standard deviation. **Bottom row:** Training of a ResNet20 with a 4-layer MLP head on CIFAR10, using a DUPM-like regularization. We use weight decay \(0.005\) except \(_{H_{1}}=0.000005\) (to compensate for \(n=5000\), which significantly influences the total regularization strength), learning rate \(0.05\) and width \(64\) for all the MLP layers. Results are averaged over 5 runs, and we show the confidence intervals at \(1\) standard deviation.

The results agree well with our theory, and they are qualitatively similar to those of Section 6.1 for DUFM training. The DNNs consistently outperform the DNC loss, but still achieve DNC1. The ranks of class-mean matrices range from 5 to 6, and they are always the same in all intermediate layers within one run. Remarkably, the SRG solution was found by gradient descent also in this setting.

Both weight decay and learning rate affect the average rank of the solutions found by gradient descent. Varying the width can lead to unexpected results, as it changes the ratio between the number of parameters in the MLP and that in the backbone, so the effect of the width is harder to interpret. Similar results can be seen on MNIST.

### End-to-end experiments

Finally, we perform experiments with standard regularization and the same architecture (i.e., DNN backbone plus MLP head) as in Section 6.2. In particular, in Figure 4 we consider a ResNet20 backbone with a 5-layer MLP head trained on CIFAR10 and MNIST with standard weight regularization.

Overall, the results remain qualitatively similar to those discussed above. This demonstrates that, in spite of a different loss landscape compared to previous settings, the low-rank bias is still responsible for DNC2 not being attained. Specifically, for CIFAR10, the rank in the third layer ranges between 8 and 9, and for MNIST ranges between 5 and 7; in contrast, the DNC solution has rank \(K=10\). All DNNs display DNC1 across all layers. Remarkably, for the MNIST experiment the solution displayed in Figure 4 found by gradient descent is the SRG solution (compare the gram matrices in bottom right plot of Figure 4 with the right-most plot of Figure 1).

The difficulty of the learning task plays a significant role in this setting: when training on MNIST, it is rather easy to reach low-rank solutions and rather difficult to reach DNC solutions and the rank depends heavily on the regularization strength as shown in Figure 10 of Appendix B.3; when training on CIFAR-10, the weight decay needs to be high for the class mean matrices to be rank deficient. Moreover, the learning rate no longer exhibits a clear relation with the rank, since gradient descent diverges when the learning is too large. We also observe that the rank deficiency is the strongest in the mid-layer of the MLP head, creating a "rank bottleneck". This can be seen by a closer look at the tails of the singular values, which better match zero at intermediate layers (the green and red curves corresponding to layers 3 and 4 have tails slightly lower than the other curves). In a more precise manner, we further measured effective ranks of all the layers in Figure 4. For instance, the effective

Figure 4: Training of a ResNet20 with a 5-layer MLP head on CIFAR-10 (**top row**) and MNIST (**bottom row**), using the standard regularization. We pick a large weight decay (\(0.08\) for CIFAR-10 and \(0.04\) for MNIST) and a large learning rate (\(0.005\) for CIFAR-10 and \(0.01\) for MNIST). Results are averaged over 5 runs, and we show the confidence intervals at \(1\) standard deviation. **Left:** DNC1 metric training progression. **Middle:** Singular value distributions at convergence for all the layers. **Right:** Gram matrices of \(M_{3}\) (CIFAR-10) and \(_{5}\) (MNIST).

ranks of CIFAR10 experiment layers are \(8.96,7.46,6.88,7.04,7.73),\) which shows the middle layer is closest to a low hard rank matrix. The rank bottleneck is also mentioned in [22; 21; 56]. In fact, these works also measure extremely low ranks, but [22; 21] do it on synthetic data with very low inner dimension, while  focuses on fully convolutional architectures trained with CE loss and including biases.

In summary, Figure 4 shows that both the low-rank bias and the optimality of DNC1 carry over to the standard training regime. This means that there are hyperparameter settings for which deep neural collapse, _including in the very last layer_, is not reached (and likely not even optimal). Although the sub-optimality of DNC in the last layer is not proved formally, this phenomenon is supported by evidence across all experimental settings and further corroborated by our theory where our SRG construction is far from being DNC2-collapsed in the last layer.

## 7 Conclusion

In this work, we reveal that the deep neural collapse is _not_ an optimal solution of the deep unconstrained features model - the extension of the widely used unconstrained features model. This finding considerably changes our overall understanding of DNC, as all the previous models in simplified settings showed the global optimality of neural collapse and of its deep counterpart. The main culprit - the low-rank bias - makes the orthogonal frame property of DNC, and thus DNC as a whole, too high rank to be optimal. We demonstrate this low-rank bias across a variety of experimental settings, from DUFM training to end-to-end training with the standard weight regularization. While the structure of the Gram matrices of class means is not captured by orthogonal matrices (or by the ETF), the within-class variability collapse remains optimal. Our theoretical analysis proves this for the DUFM problem, and our numerical results showcase the phenomenon across various settings.

Our analysis focuses on the MSE loss, but we expect similar results to hold for the cross-entropy loss and, in particular, that the same SRG construction proposed here would still refute the optimality of DNC. We leave as an open question whether DNC1 is strictly optimal across _all_ layers. While proving this would likely require new ideas, we note that _none_ of our experiments converged to a solution that would not be DNC1-collapsed.