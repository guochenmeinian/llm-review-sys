# Why Do We Need Weight Decay

in Modern Deep Learning?

Francesco D'Angelo, Maksym Andriushchenko, Aditya Varre, Nicolas Flammarion

Theory of Machine Learning Lab

EPFL, Lausanne, Switzerland

{francesco.dangelo,maksym.andriushchenko,aditya.varre,nicolas.flammarion}@epfl.ch

Equal contribution

###### Abstract

Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the _loss stabilization mechanism_. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the _bias-variance tradeoff_ in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/tml-epfl/why-weight-decay

## 1 Introduction

The training of modern neural networks broadly falls into two regimes: _over-training_, which involves multiple passes through a dataset and necessitates effective regularization strategies to avoid overfitting; and _under-training_, characterized by fewer passes due to large amounts of training data and computational constraints (Hoffmann et al., 2022). Modern deep learning unequivocally embodies both training regimes: ResNet architectures on computer vision tasks (He et al., 2016) serve as quintessential examples of the over-training regime, while the training of large language models (Brown et al., 2020) stands as a hallmark of the under-training regime. Despite their differences, both regimes extensively adopt weight decay as a regularization technique, though its effectiveness and role remain subjects of ongoing debate. For the first regime, Zhang et al. (2016) showed that even when using weight decay, neural networks can still fully memorize the data, thus questioning its regularization properties. For the second, regularization is inherently unnecessary as the limited number of passes already prevents overfitting. These considerations raise important questions about the necessity and purpose of weight decay, introducing uncertainty about its widespread usage. To illustrate the effect of weight decay in the two regimes,

Figure 1: Test error vs. dataset size on CIFAR-10-5m for a _fixed_ number of training iteration. Weight decay is helpful in both: the over-training and the under-training, one-pass regime.

we conduct a simple experiment. We train a ResNet18 on subsets of the CIFAR-5m dataset (Nakkiran et al., 2020) with sizes from \(10\,000\) to \(5\) mln. The computational budget of each training session is fixed to \(5\) mln iterations, which amounts to a range of passes between \(500\) and one. In the over-training regime (left in Fig. 1), weight decay does not prevent the models from achieving zero training error, but its presence still improves the test error. Attempting to explain this generalization benefit, recent works (Li and Arora, 2019; Li et al., 2020) bring forth the hypothesis that it is inadequate to think about weight decay as a capacity constraint since it still bears an effect on the training of scale-invariant models. As a result, understanding the effect of weight decay on the optimization dynamics becomes crucial to understanding generalization. Nevertheless, this line of work heavily relies on an _effective learning rate_ (ELR) which only emerges as a consequence of scale-invariance and therefore does not apply to general architectures. In the under-training regime (right in Fig. 1), where the generalization gap vanishes, weight decay seem to facilitate faster training for slightly better accuracy. However, a characterization of the mechanisms through which weight decay impacts the training speed in this regime remains underexplored.

Our work delves into the mechanisms underlying the benefits of weight decay by training established machine learning models in both regimes: ResNet on popular vision tasks (over-training) and Transformer on text data (under-training). Towards this goal, we make the following contributions:

* In the over-training regime, we unveil the mechanism by which weight decay effectively reduces the generalization gap. We demonstrate that combining weight decay with large learning rates enables non-vanishing SGD noise, which through its implicit regularization controls the norm of the Jacobian leading to improved performance. Moreover, our investigation offers a thorough explanation for the effectiveness of employing exponential moving average and learning rate decay in combination with weight decay.
* In the under-training regime, particularly for LLMs trained with one-pass Adam, we confirm experimentally that weight decay does not bring any regularization effect and is simply equivalent to a modified ELR. We explain the training curves commonly observed with weight decay: through this ELR, weight decay better modulates the bias-variance trade-off, resulting in lower loss. Additionally, we show that weight decay has another important practical benefit: enabling stable training with the bfloat16 precision.

### Related work

The concept of employing \(_{2}\) weight penalty traces back to studies on the stability of solutions for ill-posed problems (Tikhonov, 1943). It has since been extensively explored in statistics (Foster, 1961; Hoerl, 1962; Hoerl and Kennard, 1970). Krogh and Hertz (1991) present one of the earliest systematic studies on weight decay tailored for _neural networks_. Generalization bounds, such as those by Shalev-Shwartz and Ben-David (2014), suggest that weight decay can be _sufficient_ for generalization, although not necessary, e.g., due to the implicit regularization of gradient methods (Soudry et al., 2018). Zhang et al. (2016) argue that while weight decay improves test accuracy, the improvement is not substantial (\( 1\)-\(2\%\) on ImageNet), indicating the key role of implicit regularization. Loshchilov and Hutter (2019) highlight the distinct effects of weight decay and \(_{2}\) regularization, particularly for Adam, suggesting that Adam combined with weight decay (AdamW) leads to superior regularization and simpler hyperparameter tuning. For GPT-3 training, Brown et al. (2020) suggest that they include weight decay to provide _a small amount of regularization_, although we believe it is not the primary reason as we discuss in Sec. 3.

Multiple works have focused on weight decay as a tool influencing optimization dynamics. Van Laarhoven (2017) emphasizes that weight decay's impact on scale-invariant networks is primarily seen in terms of an effective learning rate. Zhang et al. (2018) propose three mechanisms of weight decay regularization: (1) increasing the effective learning rate for scale-invariant networks, although as we discuss, the same holds for networks beyond scale invariance (2) approximating the regularization of the input Jacobian for an optimizer inspired by second-order methods, (3) inducing a specific dampening effect in this optimizer. Li and Arora (2019); Li et al. (2020) explore the optimization properties of scale-invariant deep networks for which the effective learning rate can be derived. Lewkowycz and Gur-Ari (2020) suggest that the best generalization is achieved with the smallest \(\) although it necessitates longer training. Additionally, Lewkowycz (2021) propose a criterion for detecting when to decay the learning rate based on the evolution of the weight norm. Bjorck et al. (2021) explore the effect of decoupling weight decay, especially in the early stage of training. Li et al. (2022) make BERT architecture scale-invariant to enhance training stability and make it more compatible with standard SGD. Recently, Kosson et al. (2023) show a mechanism through which weight decay balances rotational updates across different layers that motivates a new optimizer.

The seminal paper of Krizhevsky et al. (2012) that introduced AlexNet suggest that weight decay serves not only as a regularizer but also reduces the model's training error, functioning as an _optimization tool_. In recent work, Hoffmann et al. (2022) briefly observe that weight decay enhances the training performance of Adam for training LLMs, but only after \( 80\%\) of the total iterations. However, they do not provide an explanation for this behavior, a point we delve into in Sec. 3.

## 2 Weight decay in the over-training regime

In this section, we delve into the influence of weight decay in the over-training regime, with a specific focus on image classification tasks. We focus on the training of ResNet models (He et al., 2016) using SGD on Tiny-ImageNet (Wu et al., 2017) and report additional experiments in appendix C for VGG, Resnet32 and scale-invariant Resnet architectures on CIFAR10 and CIFAR100 (Krizhevsky and Hinton, 2009).

**Notations and setup.** Let \((x_{i},y_{i})_{i=1}^{n}\) be the training inputs and labels where \(x_{i}^{d},\ y_{i}^{c}\), and \(c\) is number of classes. Let \(h:^{p}^{d}^{c}\) be the hypothesis class of neural network and for any parameter \(^{p}\) where the function \(h_{}():^{d}^{c}\) represents the network predictions. The training loss \(\) and the \(_{2}\)-regularized loss \(_{}\), for \( 0\), are given by:

\[()=_{i=1}^{N}(y_{i},h_{}(x_{i}))_{}()=( )+^{2}.\]

where \((,):^{c}^{c}\) denotes the cross-entropy (CE) loss function. With \(i_{t}([N])\), the SGD algorithm on \(_{}\) (here with batch size 1 and with replacement) with a learning rate (LR) \(\) is

\[_{t+1}=_{t}-_{}(y_{i_{t}},h (_{t},x_{i_{t}}))-_{t}.\] (1)

Along the training we track three different iterates: (1) **large-LR** denoted by \(_{t}\) which use a large constant LR to exploit the SGD noise, (2) **fine-tuning \(}_{t}\)** which starting from \(_{t}\) use a small LR and (3) the **exponential moving average** (EMA) \(}_{t}\) along the large-LR iterates.

### Loss stabilization and weight decay

To understand whether minimizing the regularized objective \(_{}\) alone ensures optimal generalization, we compare test errors in Fig. 1(a) across various settings. Although both large and small LRs minimize the regularized objective, the evidence that optimal performance is achieved exclusively with large LRs indicates that _the objective alone is insufficient to explain the benefits of WD or ensure generalization.2_ This experiment reaffirms the widely acknowledged consensus that _implicit regularization induced by the LR is crucial_(Keskar et al., 2016; Li et al., 2019; Andriushchenko et al.,

Figure 2: **Training with and w/o weight decay. We report the test error for Resnet18 on CIFAR-10 (1(a)) and Tiny-ImageNet (2(d)) trained with and without weight decay and with small and large learning rates. We also include the correspondent EMA, represented by dashed lines. After the first 250 epochs the learning rate is decayed to \(=10^{-3}\) for all the curves. We report also the L2 norm of the parameters (1(c)) and Train CE (1(b)) which after the decay converges to the same value for all the runs with the same \(\).**

2023). Despite revealing an interplay between weight decay and large initial LR, the understanding of the corresponding dynamics remains limited. In this section, our goal is to comprehensively understand these dynamics, particularly to elucidate the difference in generalization between training with and without weight decay and using different learning rates, as observed in Fig. 2a. Given the regularization of the \(_{2}\) norm of parameters, it is natural to wonder whether weight decay's improvement primarily stems from its ability to control the norm of the trained model. The experiment in Fig. 2c clearly illustrates that distinct training trajectories, while resulting in the same final \(_{2}\) norm for parameters, can yield different levels of generalization stating that _the \(_{2}\)-norm of the learned model's parameters is inconsequential_. This observation suggests that once the norm is constrained by weight decay, the critical factor influencing the model's generalization is the subsequent choice of LR. We should note that neural networks can be explicitly made scale-invariant by means of normalization layers and small architectural changes. Li and Arora (2019) have used this setting to reveal that the training dynamics has an effective learning rate which, depending on the L2-norm of the parameters, reduces the effect of WD to merely a scheduler for the learning rate. Our analysis presents a broader perspective that does not depend on scale invariance. At the core of our examination are the unique properties of exponentially tailed loss functions, such as CE: when the data is separable and WD is not applied, the infimum of the loss is at infinity, leading to the unbounded growth of the weight norm (Ji and Telgarsky, 2019; Soudry et al., 2018). The application of WD, by inhibiting this growth, prevents the decrease of CE loss, which in turn, significantly alters the optimization dynamics.

Indeed, examining the parameter norm evolution in Fig. 2c, we notice how it rapidly decreases to stabilize within a small, approximately constant interval. Similarly, the Train CE in Fig. 2b displays a stabilization effect beyond which it cannot decrease without a reduction in the learning rate. We hypothesize that WD enables an optimization dynamic akin to that on the surface of a sphere of certain radius thus triggering the following essential mechanism:

_Constraining the parameter norm hinders the decrease of the CE, thereby enabling non-vanishing noise in SGD. This allows SGD implicit regularization to unfold and steer the optimization trajectory._

Next, we empirically characterize this implicit regularization mechanism.

### The noise driven process

The long-held belief that the implicit regularization property of SGD is pivotal to the generalization capabilities of Neural Networks has been a cornerstone in the field of deep learning (Keskar et al., 2016). Many theoretical studies (Blanc et al., 2020; Li et al., 2021; Damian et al., 2021), attempting to understand this phenomenon, draw upon the essential finding that, in the case of regression and when Gaussian noise is added to the labels, the shape of the covariance of the stochastic gradients matches the shape of the Hessian. This allows Damian et al. (2021) and Pillaud-Vivien et al. (2022) to show that the trajectory of the SGD iterates closely tracks the solution of a regularized problem. In our analysis, we conjecture a similar result; the dynamics of SGD with CE, closely track a regularized process. The important difference in our statement is that unlike Blanc et al. (2020) and Damian et al. (2021), we do not need to add noise to the labels at each iteration. Instead, weight decay, in combination with large-LR induces a label noise-like behavior via loss stabilization (Andriushchenko et al., 2023). To better understand the interplay between weight decay, loss stabilization and the noise of SGD, it is convenient to consider the binary classification case where \(y_{i}\{0,1\}\). We also define the Jacobian of the network as \(J(x_{i},) h_{}(x_{i})^{p}\) and its norm averaged across the dataset: \( J()_{F}^{2}=_{i=1}^{N}( h_{}(x_{i}) h_{}(x_{i})^{})\). Denoting the noise of the gradient by \(g_{t}=_{}(_{t})-_{} (y_{i_{t}},h(_{t},x_{i_{t}}))\), the SGD update in equation 1 becomes:

\[_{t+1}=(1-)_{t}-( _{t})+ g_{t}\,.\] (2)

Furthermore, we consider a Gaussian approximation of the SGD noise, matching the first and second moment of \(g_{t}\). A substantial body of research has built upon this approximation and verified its validity (Li et al., 2020, 2021; Smith et al., 2020; Xie et al., 2020; Li et al., 2021). In particular Li et al. (2021) demonstrated how modelling the SGD noise by a Gaussian is sufficient to understand its generalization. The mean is zero due to the unbiased mini-batch gradients: \((_{t})[g_{t}]=0\)whereas for the second moment:

\[_{_{t}} :=_{i=1}^{N}_{i}(_{t}) _{i}^{}(_{t})-(_{t}) ^{}(_{t})_{i=1}^{N}_{ i}(_{t})_{i}^{}(_{t})\] \[_{i=1}^{N}(_{i}^{}(_{t }))^{2} h_{_{t}}(x_{i}) h_{_{t}}(x_{i})^{} _{,}^{2}(_{t})_{i=1}^{N}  h_{_{t}}(x_{i}) h_{_{t}}(x_{i})^{}\,.\] (3)

In equation 3, we consider \((_{t})^{}(_{t})\) negligible compared to \(_{i}(_{t})_{i}^{}(_{t})\) as the gradient noise variance dominates the square of the gradient noise mean. This fact has been used in previous works (Mori et al., 2022; Zhu et al., 2018; Jastrzebski et al., 2017) and in particular Jastrzebski et al. (2017) and Saxe et al. (2019) empirically verify it. Finally, we assume that the first derivative is approximately constant across all datapoints: \(_{i}^{}(_{t})_{j}^{}(_{t})\;  i,j\) and denote this common quantity as \(_{,}(_{t})\). This last approximation is referred to as "decoupling approximation" and has been empirically verified for classification (Mori et al., 2022). Furthermore, in App. C.6 we performed additional experiments to verify the decoupling approximation by comparing the spectrum of the SGD covariance with and without this approximation during the large LR phase.

Altogether, these considerations lead to the following formulation of the SGD update:

\[_{t+1}(1-)_{t}-( _{t})-_{,}(_{t})_{i=1} ^{N} h_{_{t}}(x_{i})\,_{i}^{t},^{t} (0,)$.}\] (4)

This series of approximations, allows us to define the quantity \(_{,}(_{t})\), which has a fundamental role in the characterization of the training dynamics. We refer to it as _the scale of the noise_ because it regulates its intensity. Although \(\) and \(\) influence the noise scale indirectly through the trajectory of \(_{t}\), we explicitly highlight this dependence to emphasize our objective: to characterize the influence of WD and LR on the stochastic dynamics of SGD. We develop this characterization building upon the connection between the Jacobian of the network and the covariance of the SGD noise, which motivates the introduction of the following implicit regularization mechanism:

**Conjecture 1**.: _Consider the algorithm in Eq. 1 with \(_{0}\) initialized from a distribution \(_{0}(^{(p)})\). For any input \(x\), let \(_{t},h(_{t},x)\) be the random variables that denote the iterate at time \(t\) and its functional value. The stochastic process \((h(_{t},x))_{t}\) converges to the stationary distribution \(_{,}^{}(x)\) with mean \(_{,}(x)=h(_{,}^{*},x)\) for which \(_{,}^{*}\) is a first-order stationary point of the following regularized loss:_

\[}_{}()_{}( )+_{,}^{2}J()_{F}^{2}.\] (5)

The conjecture illustrates how varying noise levels correspond to distinct processes, wherein the mean of each solves a unique regularized problem. Moreover, it describes how each noise level determines the strength of the regularization. Using the mean to formulate the conjecture is a natural choice; even at stationarity,3 the values of the loss \((_{t})\) and the regularizer \(J()_{F}^{2}\) would be dominated

Figure 3: **Resnet18 on Tiny-ImageNet.** Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \(\) and \(\).

by the noise. To unveil the existence of an implicit regularization and to analyze the evolution of the distribution, we need to look at its summary statistics, in this case, the mean. While insights from Langevin dynamics suggest employing learning rate annealing to converge towards the mean of the stationary distribution, this approach introduces additional complexities which we discuss in Section 2.3. We instead consider an exponential moving average (EMA) \((}_{t})_{t 0}\) of the SGD iterates with parameter \(=0.999\).

The most important implication of the conjecture is that the strength of the regularization \(_{,}\) depends on both the LR \(\) and the WD parameter \(\). Our experiments in Fig. 3, provide empirical validation for this conjecture. When trained with different combinations of \(\) and \(\), the EMA converges to models with different test performances. When fixing \(\) there exists an optimal value of learning rate \(\) which gives the best test performance while the Jacobian norm monotonically increases. A similar picture can be drawn when fixing the learning rate.

Therefore, given two solutions \(_{_{l},_{l}}\) and \(_{_{s},_{s}}\) for which Test error\((_{_{l},_{l}})<\) Test error\((_{_{s},_{s}})\); the difference in their performances can be explained with the difference in their regularization strengths \(_{,}\). The solution \(_{_{l},_{l}}\) benefits from better regularization and therefore endows better generalization properties. Furthermore, the heatmap for test error in Fig. 3 indicates that the minimum error is not achieved by a single combination of \(\) and \(\), but rather along a contour where their product \(\) appears to be constant. This observation suggests an optimal trade-off between the learning rate and weight decay parameter, characterized by a curve in the parameter space where their product remains constant. Characterizing this relationship might reveal a useful tool which practitioners can adopt to optimally select values of weight decay and learning rate. Fig. 3(c), 3(d) confirm that the product \(\) is the quantity controlling the regularization; combinations of \(\) and \(\) with the same product show similar test performances and Jacobian norm. For Tiny-Imagenet, the test error exhibits an optimal value for \( 0.005\) where increases beyond this point lead to _over-regularization_ and decreases result in _under-regularization_. Simultaneously, the Jacobian norm \(J_{F}\) consistently exhibits a monotonically decreasing trend.

To better understand the properties of the noise scale during training, we can observe in Fig. 3(a), 3(b) how higher values of the training loss given by larger \(\) correspond to higher values of the noise scale \(_{,}\). The latter is measured by computing the Frobenius norm of the first derivative of the loss with respect to the predictions4 averaged over all training datapoints \(_{i=1}^{N}_{i}^{}()_{F}\). The scale of the noise and therefore the strength of the regularization vanish when \(() 0\) which happens for small values of \(\). Therefore, WD combined with a large LR stabilizes the CE to a larger value, preventing the noise from vanishing and regulating the implicit regularization. This fact further confirms Conjecture 1, demonstrating that with smaller noise scales, the mean (EMA) tends towards a point where the Jacobian's norm is higher, compared to trajectories with larger noise scales. To further validate our conjecture, we created snapshot ensembles in the ResNet18 on CIFAR-10 setting, by averaging in function space along the SGD trajectory every 10 epochs for the combinations of learning rate (LR) and weight decay (WD) considered. To assess whether the mean of the stationary distribution in function space aligns closely with the EMA, where the Jacobian norm is regularized, we compared the performance of snapshot ensembles with that of the EMA. Additionally, we computed the Total Variation Distance between the softmax outputs of the ensemble and the EMA. The results are reported in App. C.7 and show a strong alignment.

Figure 4: **Resnet18 on Tiny-ImageNet.** Training for \(200\) epochs with different \(\) and \(\); the scale of the noise monotonically increases with the train loss and \(\) Fig. 3(a), 3(b). The test error instead, presents an optimal value of \(\) Fig. 3(c) while the Jacobian norm decreases monotonically Fig. 3(d).

**The role of the dynamics of the norm.** As discussed at the end of previous sub-section, after a rapid initial decrease of norm, the optimization resembles the dynamics of SGD projected onto a sphere. We stress that this is the crucial phase in training and the implicit regularization induced by SGD during this spherical optimization leads to better generalization. To validate this observation and isolate it from the initial norm drop, we investigate the behavior of SGD on a sphere with scale-invariant networks (Li and Arora, 2019). Scale invariance is chosen for its ease of LR tuning and for comparison with existing works (Kodryan et al., 2022; Li et al., 2020). Fig. 15 depicts a similar phenomenon as Fig. 3, where the test error vs. LR exhibits a U-shaped curve. While Kodryan et al. (2022) makes a similar observation, they do not provide an explanation. We demonstrate that the implicit regularization of the Jacobian norm is the key factor, elucidating its dependence on LR.

**Effective learning rate vs. high training loss.**Zhang et al. (2018); Van Laarhoven (2017) explored the relationship between LR and WD, introducing the concept of effective LR \(_{e}=/_{2}^{2}\). These studies highlight that WD, preventing unbounded growth of the norm, enables the training process to evolve with a higher effective LR. This hypothesis is justified only with scale-invariance (which does not hold for general architecture). Furthermore, the underlying mechanism by which a higher LR enhances generalization is understood only in limited settings (Li et al., 2019). We propose that a high LR, combined with WD, leads to an increase in \(_{,}\). This hypothesis allows us to fully characterize and understand the mechanism through which generalization is enhanced.

**Mixing in the function space.** A simpler conjecture could have been stated in terms of the mixing of the iterates \((_{t})_{t 0}\) towards a solution of the regularized objective \(_{}^{*}\). However, Li et al. (2020) shows empirical evidence against mixing in the parameter space, emphasizing the necessity of considering the function space. Hence, our conjecture is formulated to capture stationarity in function space.

**On the benefit of normalization.** Our conjecture characterizes the mixing distribution but does not delve into the speed of the mixing process. In our experiments, we observe that normalization layers enable faster mixing. Li et al. (2020) observes a similar phenomenon in the case of scale-invariant networks, specifically the fast equilibrium conjecture, which is addressed by Li et al. (2022). We note that this phenomenon persists even when the models are not exactly scale-invariant.

### EMA and Fine-tuning

The large-LR phase sets the stage for SGD's inherent biases to emerge but to actually exploit such bias, reducing the stochastic noise is necessary. This can be attained in two different ways: averaging (EMA) or decaying the learning rate (fine-tuning), both strategies are widely adopted in practice. This section illustrates the relation between the two and highlights the benefits of using one over the other and the implications for our analysis. From a practical standpoint, implementing EMA is more efficient than LR-decay because it does not require additional gradient iterations or hyperparameter tuning. While both methods enhance performance, their effectiveness is contingent on being combined with loss stabilization, supporting the hypothesis that the noisy dynamics is the underlying factor for their success. Although EMA shows only a slight advantage, our experiments in Fig. 4(b), 3(d) demonstrate that it consistently outperforms learning rate decay in various settings.

When empirically validating Conjecture 1, the EMA is useful to characterize the limit point (i.e., \(t\)) but cannot adequately capture the dynamics throughout the entire trajectory. This limitation arises because different points along the trajectory are at different loss values, making the comparison of any relevant regularized quantities inconsistent. An approach to overcome this inconsistency is to project the iterate \(_{t}\) onto a manifold of constant loss. This can be achieved via early-stopped gradient flow (Li et al., 2021) (SGD with small LR) on the CE loss with \(=0\) where \(_{t}\) is projected to a nearby point \(}_{t}\), such that \((}_{t}), t\). In practice, this corresponds to fine-tuning via LR-decay. Since after fine-tuning \((}_{t})(}_{t^{ }}),\  t,t^{}\) see Fig. 4(a), we can compare \(J(}_{t})_{F}\) and \(J(}_{t^{}})_{F}\) and understand its evolution. In the experiments detailed in Fig. 4(c), we report \(J_{F}\) along the fine-tuned iterates \(}_{t}\) and observe a decreasing trend, i.e., the sequence \(J(}_{t})_{t 0}\) is decreasing. This fact empirically validates that the entire trajectory of the iterates \((_{t})_{t 0}\), closely following the trajectory of the fine-tuned iterates \((}_{t})_{t 0}\), bias the model towards a regularized solution that might enhances generalization. This also explains why learning rate schedules, such as step-decay, which starts with a large value and then decrease it, can enhance generalization.

Despite providing a straightforward methodology to analyze the trajectory, LR-decay introduces additional complexities that cause deviations from the conjecture. Indeed, in Fig. 4(c) we observe that the final points of the fine-tuned iterates report the opposite trend compared to the EMA (smaller \(\) lead to larger \( J_{F}\)). This discrepancy is potentially due to the state-dependent nature of the SGD noise covariance in equation 3; decreasing the LR and removing WD can alter the stationary distribution and the regularized objective, leading to a different solution than anticipated by Conjecture 1.

## 3 Weight decay in the under-training regime

In this section, we investigate how WD enhances optimization in the under-training regime. Although the phenomenon is more general, we focus on LLMs for which one-epoch training is typically used.

**Two key effects of weight decay in the under-training regime.** WD is widely used in training state-of-the-art LLMs like GPT-3, Chinchilla, and Llama (Brown et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023). While Brown et al. (2020) suggest that WD offers _"a small amount of regularization,"_ its necessity remains unclear in the context of _one-pass_ training where the population loss is directly minimized. As a sanity check, in Fig. 19 in Appendix, we verify that the generalization gap is close to zero even for models trained without WD. Instead of the regularization effect, we suggest that the two most crucial effects of WD in the under-training regime are (1) better optimization of the training loss as briefly observed by Hoffmann et al. (2022), (2) prevention of loss divergences under the bfloat16 weight precision. We reproduce this phenomenon at a smaller scale with 124M parameters in Fig. 6: the final training loss is smaller for \(\) equal to \(0.1\) and \(0.3\) compared to \(0\). We study both mechanisms which stand in contrast to the over-training regime of Sec. 2, where the primary concerns are not optimization and stability, but rather generalization.

**Experimental setup.** We use the NanoGPT repository (Karpathy, 2023) for training GPT-2 models (Radford et al., 2019) on OpenWebText. We train a 124M parameter model (known as GPT-2-Small) for \(50\,000\) iterations with a batch size of \(256\). For most experiments, we reduce the default context length from \(1024\) to \(256\) to ensure practicality within an academic budget. Alternatively, we could have reduced the number of training iterations or batch size, but this would lead to insufficiently trained models. Unless mentioned otherwise, we train with AdamW using the default LR \(0.0006\), a short 400-iteration LR warmup, gradient clipping with the \(_{2}\)-threshold \(1.0\), and \(10\) cosine LR decay. We keep all other hyperparameters at their default values (see App. B).

**Better optimization with WD is reproducible at a smaller scale.** The findings from Hoffmann et al. (2022) (Fig. A7 therein) indicate that WD in AdamW leads to lower training loss (\( 0.02\) lower), primarily towards the end of training. The reduction of training loss directly translates to a better downstream performance and makes this observation practically relevant. Additionally, performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better starting point

Figure 5: **EMA vs Fine-tuning.** Training of standard Resnet18 on CIFAR-10 for 100 epochs fixing \(=0.0125\) and varying the learning rate. In Fig. 4(a) we report different levels of loss stabilization, in Fig. 4(b) we report the test errors and in Fig. 4(c) and Fig. 4(d) the norm of the Jacobian and of the weights respectively. The quantities are measured for the SGD iterates, the EMA and the fine-tuning. The latter is performed for 100 epochs every 3 with \(=10^{-3}\).

Figure 6: **GPT-2-124M on OpenWebText.** We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.

in terms of the final loss after fine-tuning. Moreover, in Fig. 21, we show that decoupling WD, as advocated by Loshchilov and Hutter (2019), is not necessary to achieve this effect: a simple \(_{2}\) penalty added to the loss suffices. Lastly, in Fig. 22, we show that a similar improvement in training loss is observed for _SGD with momentum_ suggesting that adaptive LRs are not key for this phenomenon.

**Effective LR induced by weight decay in AdamW.** We hypothesize that the use of WD for LLM training results in an increased effective LR, even in the absence of scale invariance of the training loss for modern transformer architectures. Here we show that WD in combination with sign SGD--utilized as a surrogate for Adam (Balles and Hennig, 2018)--is equivalent to projected SGD on the sphere, with an effective LR \(_{}}}{{\|_{t}\|_{2}}}\), similarly to Van Laarhoven (2017). Consider the update rule of sign SGD on loss \(\) with WD:

\[_{t+1}=(1-_{t}_{t})_{t}-_{t} (_{t}(_{t}))=(1-_{t}_ {t})\|_{t}\|_{2}[_{t}}{\| _{t}\|_{2}}-( _{t}(_{t}))}{(1-_{t}_{t})\|_{t}\|_{2}}].\]

Considering the evolution of the direction \(}:=}}{{\|\|_{2}}}\),

\[}_{t+1}[}_{t}-}{ (1-_{t}_{t})\|_{t}\|_{2}} (_{t}(_{t}))].\]

When \((_{t}(_{t}))\) is determined solely by the direction \(}_{t}\), the evolution of the direction of weights becomes the primary matter. This scenario occurs when the function \(\) is scale-invariant or homogeneous. Observing the trend of the gradient norm and the parameter norm \(\|_{t}\|_{2}\) from Fig. 26, we note an inverse relationship, i.e., the gradient norm is higher when the parameter norm is lower. This behavior is reminiscent of scale-invariant networks for which \(()=()\), for any \( 0\). Thus, controlling parameter norms with WD allows implicit changes to the LR schedule which we verify experimentally next.

**Matching the effective LR _without_ weight decay.** To verify the key role of the ELR \(}}{{\|_{t}\|_{2}}}\), we train a model without WD but with ELR corresponding to models trained with \(\{0.1,0.3\}\) and report results in Fig. 7. We observe that the _whole_ training dynamics is matched which confirms our hypothesis. This fully explains the observation from Hoffmann et al. (2022) about the advantage of AdamW over Adam: there exists an LR schedule (albeit a non-standard one) shown in Fig. 7 (_middle_) that leads to the same loss profile as the original AdamW run. However, we note that this holds only for full float32 precision, and models trained with bfloat16 precision diverge in the middle of training. This experience suggests that WD is still necessary in practice to prevent loss divergence. We also note that matching the ELR \(}}{{\|_{t}\|_{2}}}\) derived above for sign SGD instead of \(}}{{\|_{t}\|_{2}^{2}}}\) for plain SGD (Zhang et al., 2018; Hoffer et al., 2018) is critical for AdamW. Otherwise, the runs diverge very early in training, even with float32 parameter precision.

**Explaining the training dynamics of AdamW.** Classical optimization theory suggests that convergence of SGD-based algorithms primarily depends on two factors: the _bias_ term that influences the rate at which initial conditions are forgotten and the _variance_ term that results from noise in the gradient estimates (Moulines and Bach, 2011). We argue that these two factors, together with the observation about higher ELR for WD, can explain the loss profiles from Fig. 6. If we consider the simple case of SGD with a constant LR \(\) applied to a linear least-squares problem, the expected excess risk after \(t\) iterations can be bounded as a sum of a bias and variance terms:

\[(1-)^{t}_{0}-_{ *}^{2}+^{2},\]

Figure 7: **GPT-2-124M on OpenWebText.**_Left_: The effective LR \(_{t}/\|_{t}\|_{2}\) for the models reported in Fig. 6. _Middle_: The LR schedule that matches the effective LR \(_{t}/\|_{t}\|_{2}\) of the runs with weight decay \(0.1\) and \(0.3\). _Right_: Matching the effective LR is sufficient to match the whole training dynamics of the loss if we avoid the loss spikes by using full precision (float32 instead of bfloat16).

where \(\) is a uniform bound on the variance of the noise of gradient estimates, \(\) a lower bound on the objective function's Hessian, \(_{0}\) the initial point and \(_{*}\) the optimum. For linear models, it is well-established that a larger LR accelerates the contraction of the bias term but has a detrimental impact on the variance term, ultimately leading the variance term to dominate. Coming back to the dynamics in Fig. 6, with a large ELR at the start, the convergence becomes primarily bottlenecked by the high variance term proportional to the learning rate, leading to higher loss values in the presence of WD. Conversely, towards the end of training, when ELR and the variance term are reduced, we see that WD catches up and performs better at the end, thanks to its relatively higher ELR _throughout the training_ and thus better bias contraction. This perspective sheds light on the observation that EMA for LLMs is most advantageous when employed with large LRs (Sanyal et al., 2023) as we also illustrate in Fig. 23. As the variance dominates in this case, variance reduction of the averaging helps.

**Experiments with bfloat16.** Training in reduced precision is essential for speeding up training and reducing GPU memory requirements (Kalamkar et al., 2019). We further elaborate on the fact that WD is not fully equivalent to higher ELR and remains necessary for stable bfloat16 training. While Scao et al. (2022) observe that usage of float16 can cause loss divergences, bfloat16 is considered much more stable and is de-facto standard in LLM training. Although bfloat16 shares the same floating-point exponent size as float32 (thus, the _range_ of possible values is the same), it offers lower precision, with only \(7\) bits for the fraction instead of \(23\).

We observe that even more stable bfloat16 data type can still exhibit late-training spikes that irreparably harm model performance _in standard practical settings_, such as with a larger context length (e.g., \(1024\) instead of \(256\) as in the previous experiments). Therefore, we focus on this configuration for the experiments shown in Fig. 8. Runs with a moderate LR \(0.001\) (the default LR of Adam) without WD exhibit late-training divergence for _all_ random seeds when using bfloat16. By comparison, training with float32 remains entirely stable. Importantly, we observe that the model _does not recover_ after the loss spikes which contrasts with the loss spikes described in the Edge of Stability phenomenon (Cohen et al., 2021, 2022). We emphasize that all these runs use gradient clipping with the standard \(_{2}\)-threshold. Finally, we observe that divergences can be prevented by reducing the LR, e.g., from \(0.001\) to \(0.0006\). However, this adjustment leads to slower training, as illustrated in Fig. 24 in the Appendix. Instead, the most effective approach is to use a higher LR of \(0.001\)_with WD_, which enables stable bfloat16 training and yields a better final training loss.

## 4 Conclusions

In this paper, we demonstrate how weight decay, a single hyperparameter, can manifest three distinct effects across different training regimes: it offers regularization when combined with stochastic noise, improves optimization of the training loss, and guarantees stability in low-precision training environments. In the over-training regime, the scale of the noise \(_{,}\) is the fundamental quantity governing the implicit regularization strength of SGD. Weight decay combined with large LR enables the noisy dynamics to evolve by maintaining the scale at a constant level. Techniques such as EMA or fine-tuning work by reducing noise, thereby allowing for the effective exploitation of the accumulated hidden regularization. Coming to the under-training regime, AdamW (Loshchilov and Hutter, 2019) was introduced as a _regularization_ method. Instead, we argue that it is effective as it modulates the ELR to attune the bias-variance tradeoff. In addition, it also improves the stability of training. In summary, weight decay is seldom valuable as an explicit regularizer; instead, its widespread adoption can be attributed to its ability to induce desirable changes in optimization dynamics. We also acknowledge limitations of our work: given our limited computational resources, we do not conduct truly large-scale experiments. Moreover, we do not prove new theoretical results. Instead, we strive to provide a clear experimental picture and formulate general explanations for the effectiveness of weight decay in different training regimes.

Figure 8: **GPT-2-124M on OpenWebText with context length 1024.** Weight decay prevents divergence for LR \(0.001\) and enables stable bfloat16 training. The three random seeds are denoted with \(,,\) lines.