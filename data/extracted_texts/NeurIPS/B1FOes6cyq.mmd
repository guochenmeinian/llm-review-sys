# Learning from Teaching Regularization:

Generalizable Correlations Should be Easy to Imitate

 Can Jin\({}^{1}\)\({}^{*}\)

&Tong Che\({}^{2*}\)

&Hongwu Peng\({}^{3}\)\({}^{}\)

&Yiyuan Li\({}^{4}\)\({}^{}\)

&Dimitris N. Metaxas\({}^{1}\)\({}^{}\)

&Marco Pavone\({}^{5}\)\({}^{}\)

\({}^{1}\)Rutgers University \({}^{2}\)Nvidia Research \({}^{3}\)University of Connecticut

\({}^{4}\)University of North Carolina at Chapel Hill \({}^{5}\)Stanford University

can.jin@rutgers.edu, tongc@nvidia.com

###### Abstract

Generalization remains a central challenge in machine learning. In this work, we propose _Learning from Teaching_ (**LoT** ), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT.

## 1 Introduction

Improving the generalization performance of models on unseen data is a major challenge in machine learning [6; 7; 73; 79; 107]. Despite its significant advances, identifying the most generalizable model within the vast space of potential models remains challenging. Existing deep learning approaches focus on crafting the hypothesis spaces where prediction errors are optimized using training data [33; 69; 71]. These spaces are shaped by inductive biases [33; 70] embedded in the neural architectures which include implicit assumptions about the data [1; 25; 95], objective functions (notably regularizers) [20; 68; 103], and learning methodologies [14; 72; 87].

In this paper, to enhance generalization, we use the methodology of regularization [37; 51; 88], which prioritizes specific regions in the hypothesis spaces. Regularization techniques often involve employing auxiliary losses or regularizers [20; 38; 103] alongside the primary task losses. For instance, L1 regularization [41; 92; 93] encourages sparsity within models [16; 40; 54; 57]. Other regularization techniques include model averaging [44; 102], dropout techniques [37; 67; 100], and additionaloptimization components [43; 63; 104]. Due to its effectiveness and simplicity, regularization is critical in modern machine learning techniques for achieving better generalization [37; 109].

We aim to answer the research question: _Among all possible models fitting the training data, which ones are inherently generalizable?_ A common belief in cognitive science is that human intelligence development involves distilling information and filtering out extraneous details to discern'simple' correlations among a few selected relevant abstract variables [18; 94]. This approach leads to the formation of correlations through simple patterns [2; 56] at the right scales. However, identifying simple correlations in deep learning remains challenging, mostly due to not being easy to identify the right scale of the problem. Studies in emergent languages suggest that the more structured a language is, the more efficiently it can be transmitted to message receivers [11; 56]. Inspired by this finding, we propose defining simple and generalizable correlations at the right scales, as those that can be readily imitated by other learners, provided they possess suitable inductive biases.

Based on this definition, we propose a novel regularization approach, _Learning from Teaching_ (**LoT** ). The core of LoT is to compute a measure of 'initiability' for the main model to learn data correlations at the correct scales. By adding this measure to the objective function and optimizing it during training, we encourage the teacher model to refine its learned multiscale correlations, making them more accessible through teaching, which in turn leads to better generalization. LoT computes this measure by jointly training the main model as the 'teacher' with one or more auxiliary'student' models. The student models strive to distill and assimilate the correlations acquired by the teacher model. Thus, the learning performance of the student defines the measure of imitability of the teacher, which is then used as the LoT regularizer.

We conduct comprehensive experiments using LoT to improve the Reinforcement Learning (RL) formulation, as well as in Natural Language Processing (NLP) and Computer Vision (CV) applications. In RL, the experimental results demonstrate that LoT attains an average normalized reward enhancement of \(44\%\) on four Atari games. In language modeling tasks, LoT achieves significant perplexity reductions on the Penn Tree Bank  and WikiText-103 . Notably, LoT enhances the supervised fine-tuning performance of LLaMA [96; 97] models on GSM8K  and MATH . In image classification tasks, LoT achieves accuracy gains of \(1.99\%\) and \(0.83\%\) on CIFAR-100  and ImageNet-1K , respectively.

## 2 Methodology

### Generalizable and Spurious Correlations

Given a dataset \(=\{(_{1},y_{1}),,(_{n},y_{n})\}\) generated from a data-generating distribution \(\), there are infinitely many continuous functions \(f\) such that \(f()=y\) for all \((,y)\). Therefore, finding the \(f\) that precisely models the true generalizable correlation between \(\) and \(y\) is challenging, especially with real-world data like natural images, which are complex and multiscale. In such scenarios, a neural network may compute incorrect (according to the ground-truth relationship between variables) yet perfect (in the empirical data distribution) correlations that explain the relationship between \(\) and \(y\)[32; 73]. This phenomenon is particularly evident when \(y\) is entirely noise-based and independent of \(\), but the neural network still fits \(y\) to \(\) perfectly [73; 107]. This process, often called brute-force memorization [2; 13], involves the network creating intricate computational strategies to encode all \((,y)\) pairs in the samples. Consequently, correlations established in this way are spurious, originating from sampling noise in the data rather than ground-truth relationships.

But how do humans distinguish generalizable correlations from spurious ones? Instead of relying on brute-force memorization to establish input-output correspondences, humans naturally focus on understanding high-level concepts within the input data, selectively ignoring irrelevant details [18; 94]. This approach leads to the formation of correlations through simple, comprehensible patterns [2; 11]. Empirical evidence in emergent languages also suggests that the more compositional a language is, the more learners will use it [11; 56].

We can, therefore, define the distinctions between generalizable and spurious correlations. First, generalizable correlations are simple and comprehensible, exhibiting lower Kolmogorov Complexity [31; 58; 90]. Second, while there is only one ground-truth correlation for a dataset, the number of spurious correlations can be massive. These two major distinctions lead to the following hypothesis.

Hypothesis:Generalizable correlations should be more easily imitable by learners compared to spurious correlations. Specifically, assume \(T_{G}\) and \(T_{S}\) are two teacher models that capture the generalizable correlation and spurious correlation from a dataset, respectively. We have student learners \(S_{G}\) and \(S_{S}\) that separately imitate \(T_{G}\) and \(T_{S}\):

* From an effectiveness perspective, the final training and test losses of learner \(S_{G}\) after training are typically lower than those of learner \(S_{S}\).
* From an efficiency perspective, during training, the test losses of learner \(S_{G}\) decrease more rapidly than those of \(S_{S}\).

This hypothesis emphasizes that generalizable correlations inherent in data are not only more interpretable but also more readily imitable. It suggests that the inherent simplicity and uniqueness of generalizable correlations make them more attainable and recognizable for learning algorithms, in contrast to the complex and abundant nature of spurious correlations derived from noise. In the following we present our novel approach.

### Learning from Teaching Regularization

Building upon the Hypothesis, we propose that the ease of imitation of the teacher model by student models can serve as a proxy for the generalizability of learned representations. By measuring the 'imitability' of the teacher model in the learning process, we can infer the generalizability of it. A teacher that is easier to imitate implies higher generalization. We then design a novel regularization approach that involves training a teacher model \(T\) alongside student models \(S\) to imitate \(T\), subsequently measuring the imitability of the teacher during training. We maximize imitability by incorporating it as an additional loss during the training of the teacher \(T\). This imitability loss is termed the Learning from Teaching regularizer (LoT regularizer). By doing so, \(T\) is optimized to be a teacher that is easier to imitate and, thus, possesses superior generalization compared to models without the LoT regularizer. We refer to this class of regularization methods as 'Learning from Teaching Regularization' (LoT). LoT aligns with the broader concept of regularization in machine learning, where the goal is to promote generalizable representations and prevent overfitting.

Although LoT can be applied to supervised, unsupervised, and reinforcement learning, we begin our discussion with supervised learning. We train a network \(T_{}\), parameterized by \(\), as the main model, which also serves as the teacher model. Additionally, we train a set of \(K\) networks \(S_{i},i=1,2,,K\), as the student models1. The total set of parameters of the \(K\) networks is denoted by \(\). Given a training dataset \(_{t}=\{(_{1},y_{1}),,(_{n},y_{n})\}\), we train \(T\) and \(S\) to model \(p(y|)\), denoted as \(p_{t}(y|)\) and \(p_{s}(y|)\), respectively. Additionally, LoT includes a predefined imitability metric \(_{s,t}()=(S(),T())\). Intuitively, \(_{s,t}\) measures the difference between \(S\) and \(T\)'s predictions on the same input (occasionally denoted as \(\) henceforth for convenience). There are many possible choices for the metric \(\), such as the \(L^{2}\) loss between the hidden representations of a specific layer. In our experiments, we choose \(()=_{}(p_{s}(y|)\|p_{t}(y|))\), which is the KL-divergence , to quantify the distribution similarity between \(S\) and \(T\).

We first train the teacher model. The objective function of the teacher combines the regular task loss with the additional LoT regularizer \(R()\) (defined in Equation 3). For example, in supervised learning, we can use the negative log-likelihood loss for the regular task loss, and the objective function can be written as:

\[L_{t}()=-_{t}|}_{(_{i},y_{i}) _{t}} p_{t}(y_{i}|_{i})+R(),\] (1)

where \(|_{t}|\) is the number of samples in the dataset \(_{t}\).

To train the student networks and enhance information diversity, we require an independent unlabelled dataset, denoted as \(_{s}=\{_{1},,_{m}\}\). This dataset can be identical to \(_{t}\), or generated either by a generative model trained on \(_{t}\) or through alternative augmentation methods (e.g. synthetic data generation). This unlabelled dataset constitutes the environment for the student networks to follow the prediction of the teacher and, therefore, explores and generalizes beyond the original training data.

Specifically, the student networks' goal is to imitate the correlations acquired by the teacher network during the training process. The training loss for students can be written as:

\[L_{s}()=_{s}|}_{_{s}} _{i=1}^{K}_{s_{i},t}(),\] (2)

where \(|_{s}|\) is the number of samples in the unlabelled dataset \(_{s}\). The loss function \(L_{s}\) encourages the student networks to learn from the teacher network by minimizing the difference between their predictions, as measured by the metric \(_{s,t}()\).

The feedback from all students \(S_{i}\) constitutes the LoT regularizer:

\[R()=_{s}|}_{ _{s}}_{i=1}^{K}_{i}_{t,s_{i}}(),\] (3)

where \(_{i} 0\) represents the coefficient weight of the \(i\)-th student \(S_{i}\), with \(_{i=1}^{K}_{i}=1\). The \(_{i}\) can be either a learnable parameter or fixed, such as \(\). Essentially, the LoT regularizer measures the imitability of the teacher. The regularization coefficient \(\) controls the trade-off between the original task learning objective of \(T\) and the feedback from the students.

The detailed procedure of LoT for supervised and unsupervised learning is outlined in Algorithm 1, and LoT regularization for RL (using PPO as an example) is outlined in Algorithm 2. The teacher \(T\) and student \(S_{i}\) networks are initialized differently to ensure they learn diverse features and representations. In both algorithms, the teacher and student networks iteratively learn from each other, with the students imitating the teacher's correlations and the teacher incorporating the students' feedback into the learning process.

```
1:Input: Dataset \(_{s},_{t}\), Regularization Coefficient \(>0\), Student Steps Ratio \(N>0\)
2: Initialize teacher network \(T\) parameterized by \(\) and student networks \(S_{i},i=1,2, K\), parameterized by \(\).
3:repeat
4: Sample a batch of data \(_{t}_{t},_{s}_{s}\)
5: Compute \(()=_{s}|}_{ _{s}}_{i=1}^{K}_{i}_{t,s_{i}}()\)
6: Compute \(_{t}()=-_{t}|}_{(,y) _{t}} p_{t}(y|)\) + \(()\)
7: Update \(\) using gradient \(_{}_{t}()\)
8:for\(i=1\)to\(N\)do
9: Sample \(_{s}_{s}\)
10: Compute \(_{s}()=_{s}|}_{ _{s}}_{i=1}^{K}_{s_{i},t}()\)
11: Update student networks' parameters \(\) using loss gradient \(_{}_{s}()\)
12:endfor
13:until\(T\) converges ```

**Algorithm 1** Learning from Teaching Regularization

### Discussion

The works most related to LoT are knowledge distillation (KD) [29; 36] and ease-of-teaching [11; 56] in emergent languages. However, LoT differs significantly from these approaches. In KD, a teacher model containing task-specific knowledge transmits this knowledge to a student model (often smaller than the teacher), with the primary focus on the student's performance post-distillation. Conversely, in LoT, both the teacher and student models may lack or possess different task-specific knowledge. Generalization is improved through joint training, incorporating additional signals from student feedback. In emergent languages, Li and Bowling  propose that structured language is easier to teach to other agents than less structured ones, achieving higher task success rates with less training. Additionally, Chaabouni et al.  identify a strong positive correlation between language transmission efficiency to new message receivers and the degree of compositionality (structuredness) of the language. In LoT, we focus on tasks distinct from emergent languages, finding that generalizable correlations are easier to imitate. Under our Hypothesis, we design a novel LoT regularizer and algorithm to enhance the generalization of deep neural networks, extending the ease-of-teaching concept to supervised, unsupervised, and reinforcement learning. In parallel work, Ning et al.  proposes Learning by Teaching (LbT), which utilizes teacher and student models to generate answers as training samples for the teacher model. However, the regularization method in LoT is fundamentally distinct from that in Ning et al. .

## 3 Experiments

We first validate our Hypothesis in Section 3.1. Subsequently, we assess the performance of LoT across several tasks: Atari games (Section 3.2), language modeling (Section 3.3), and image classification (Section 3.4). We compare LoT to a Teacher-only baseline, wherein the regularization coefficient \(\) in \(R()\) is set to 0, thereby blocking the student feedback. Unless specified otherwise, we employ only one student model. Except for the Atari games where the student can learn from the offline samples of the teacher, we set \(N=1\) to manage computation (we study the impact of \(N\) in Section 3.6). Moreover, we study the computational efficiency and effects of hyperparameters of LoT in Sections 3.5 and 3.6.

### Generalizable Correlations are Easier to Imitate than Spurious Correlations.

In our Hypothesis, learners are presumed to more readily imitate generalizable correlations than spurious ones. To investigate this, we design experiments involving two distinct teacher models: a sophisticated teacher and a deceptive teacher. The sophisticated teacher effectively captures generalizable correlations, while the deceptive teacher primarily learns spurious correlations. We use an identical student model to learn from both teachers separately, monitoring the student-teacher KL divergence during training and testing. The student that learns easier-to-imitate correlations is expected to exhibit lower training and test KL losses with fewer training steps.

We employ the ViT-B/16 and ViT-L/16 architectures  for both the teachers and students. The sophisticated teachers are trained on the full CIFAR-100  training set for \(10{,}000\) steps to achieve optimal convergence. The deceptive teachers, using the same hyperparameters and training steps as the sophisticated teachers, are trained on a random subset of \(2,560\) images from the CIFAR-100 training set, leading to over-fitting. Consequently, the sophisticated teachers are expected to exhibit better generalization ability (their test accuracy surpasses that of the deceptive teachers by 14%).

The two student models referred to as the sophisticated student and the deceptive student, share identical hyperparameters and initializations. They are trained to imitate the correlations from their respective teachers on the full CIFAR-100 training set. The teacher models are kept frozen during the training of the students, with the objective \(L_{s}()\) defined as follows:

\[L_{s}()=_{s}|}_{_{s }}_{}(p_{s}(y|)||p_{t}(y|)),\] (4)

where \(_{s}\) represents the full training set of CIFAR-100.

We present the training and test losses in Figure 1 and make the following observations:

Figure 1: Training and test KL-divergence losses of student models in LoT using ViT-B/16 and ViT-L/16 on CIFAR-100 with different teacher models. The sophisticated students achieve lower losses than the deceptive students given the same computational budget.

* Given the same computational budget, the sophisticated students achieve lower final KL losses on both the training and test sets compared to the deceptive students. This suggests that the student can more effectively imitate the prediction distribution of a teacher that captures generalizable correlations.
* The deceptive students require more training steps to achieve the same training and test student-teacher KL losses as the sophisticated students. This indicates that learners tend to grasp spurious correlations much more slowly than generalizable correlations.

These results suggest that generalizable correlations are easier to imitate than spurious ones. In LoT, we expect the teacher model to master generalizable correlations by incorporating feedback from students via the LoT regularizer.

### Atari Games

We conduct experiments on four Atari games, namely BeamRider, Breakout, UpNDown, and Gravitar, following the implementation in Huang et al. . Both the LoT and Teacher-only agents have identical hyperparameters. All agents are trained using Proximal Policy Optimization (PPO) . While the teacher agents interact with the game environment, the student agents are trained on the **most recent**\(10{,}240\)**samples** generated by the teacher agents, ensuring that LoT and Teacher-only experience the same environmental interactions. We use different \(\) values for various games and set \(N=5\) to efficiently imitate the teacher. More details are provided in Appendix D.

The empirical results are presented in Figure 2, and we make the following observations:

* LoT improves the agent return compared to the Teacher-only version with \(20\) million teacher training steps. Specifically, LoT achieves {\(63.14\%\), \(9.79\%\), \(66.48\%\), \(35.70\%\)} normalized return enhancements on {BeamRider, Gravitar, UpNDown, Breakout}.
* The performance gain of LoT becomes more prominent as the training progresses (from \(15\) million to \(20\) million steps).

These results suggest that LoT is an effective approach for enhancing the generalization of RL agents, as it requires no additional environmental interactions while delivering significant performance gains.

### Language Modeling

Language modeling is a widely acknowledged NLP task, and regularization techniques have been demonstrated to significantly enhance performance in this domain . To examine the impact of LoT on language modeling, we conduct experiments in two scenarios: unsupervised language pretraining and supervised fine-tuning.

#### 3.3.1 Unsupervised Language Pretraining

We conduct experiments of LoT and Teacher-only using LSTM , AWD-LSTM , and Transformer-XL  for teacher and student on Penn Tree Bank (PTB)  and WikiText-103 . We follow the implementations outlined in Dai et al. , Merity et al. , Zaremba et al. . In LoT, we utilize different coefficients \(\) for various architectures and benchmarks to control the LoT regularizer. To ensure a fair comparison, we maintain the same total number of training steps

Figure 2: The episodic return of the teacher agent in LoT and the Teacher-only on four Atari games (averaged over ten runs). LoT demonstrates return gains over Teacher-only on all games.

(with teacher and student training steps accumulated) for LoT and the Teacher-only setup. Please refer to Appendix D for more implementation details.

From the empirical results presented in Table 1, we observe that LoT achieves notable perplexity (PPL) gains across various architectures and benchmarks under the same number of learning steps as Teacher-only. Specifically, LoT achieves at least 2 points PPL gains across all settings, and a \(11.03\) gain for LSTM on PTB. It indicates that LoT can be effectively applied to both LSTM and Transformer architectures in language pretraining.

#### 3.3.2 Supervised Fine-tuning

Furthermore, to evaluate the effectiveness of LoT in fine-tuning pretrained large language models (LLMs), we conduct supervised fine-tuning (SFT) experiments using LLaMA-1  and LLaMA-2  on two mathematical reasoning benchmarks: GSM8K  and MATH .

We compare LoT to in-context learning (ICL)  and SFT. Following Touvron et al. , the number of in-context examples is 8 for GSM8K and 4 for MATH. The SFT configuration follows Yue et al. , and we fine-tune the LLaMA models for four epochs. In LoT, the teacher and student models share the same architecture for simplicity. The models are trained for two epochs in LoT to match the total training steps in SFT for fair comparison. All other configurations are consistent with those used in SFT. More implementation details are described in Appendix D.

We measure the accuracy of greedy decoding results in Table 2, and we observe that LoT enhances reasoning abilities on all architecture and dataset choices. This indicates the competence of LoT in improving the fine-tuning performance with a computational cost comparable to SFT.

### Image Classification

To investigate the effects of LoT on computer vision tasks, we apply LoT to image classification by conducting experiments using ResNets , MobileNetV2 , ViT , and Swin  architectures pretrained on ImageNet-1K and ImageNet-21K  as teacher and student models. We choose CIFAR-100  and ImageNet-1K as the downstream datasets. The total training steps for LoT and the Teacher-only approach are the same for a fair comparison. Further implementation details are provided in Appendix D. We conclude the following observations from results in Table 3:

* LoT achieves accuracy gains across various architectures and datasets without additional computational costs. For example, LoT improves test accuracy by almost 2 points using a ResNet-18 teacher and a ResNet-50 student on CIFAR-100 after pretrained on ImageNet-1K. Similarly, on the larger-scaled ImageNet dataset ImageNet-21K, LoT still obtains nearly 1 point improvement using ViT-B/16 as the teacher and ViT-L/16 as the student.
* The generalization of teacher models can be effectively enhanced by students of larger sizes. For instance, ResNet-50, ViT-L/16, and Swin-L students can enhance the performance of ResNet-18, ViT-B/16, and Swin-B teachers, respectively. Similarly, small student models

  
**Dataset** & **Teacher** & **Student** & **Teacher \#Param.** & **Teacher-only** & **LoT** \\   & LSTM & LSTM & 20M & \(82.75 0.36\) & \( 0.54\) \\  & AWD-LSTM & AWD-LSTM & 24M & \(58.69 0.37\) & \( 0.56\) \\   & Transformer-XL-B & Transformer-XL-B & 151M & \(23.72 0.41\) & \( 0.38\) \\  & Transformer-XL-L & Transformer-XL-L & 257M & \(18.50 0.25\) & \( 0.23\) \\   

Table 1: The test perplexity of the teacher model in LoT and the baseline on PTB and WikiText-103. Results are averaged over three runs. LoT achieves consistent perplexity reduction over different choices of architectures and benchmarks.

  
**Setting** & **GSM8K** & **MATH** \\  LLaMA-1 \(_{+}\) & \(10.69 0.87\) & \(2.84 0.25\) \\ LLaMA-1 \(_{+}\) & \(34.39 1.28\) & \(4.78 0.23\) \\ LLaMA-1 \(_{+}\) & \( 1.46\) & \( 0.28\) \\  LLaMA-2 \(_{+}\) & \(14.62 0.96\) & \(2.46 0.25\) \\ LLaMA-2 \(_{+}\) & \(39.81 1.34\) & \(5.79 0.31\) \\ LLaMA-2 \(_{+}\) & \( 1.62\) & \( 0.22\) \\   

Table 2: The accuracy of the teacher model in LoT and the baseline on GSM8K and MATH. Results are averaged over three runs.

can also enhance the generalization performance of larger teacher models using LoT. For example, a MobileNetV2 student improves the performance of RestNet-18 and ResNet-50 by more than 1 point on CIFAR-100 with a much smaller model size. Similar results appear on the ViT-L/16 teacher and ViT-B/16 student combination in the ImageNet-1K task.
* For transformer-based models, employing different architectures for teachers and students achieves better performance than sharing the same architecture. For example, when applying a ViT-B/16 student, a ViT-L/16 teacher achieves \(0.27\%\) more accuracy than using a ViT-L/16 student. This suggests that using different architectures for teacher and student increases information diversity, which contributes to enhanced generalization for teacher models .

These experimental results demonstrate the effectiveness of LoT in enhancing the generalization of pretrained CNN-based and Transformer-based vision models in image classification.

### Analysis of Computational Cost and Efficiency

For supervised and unsupervised tasks, LoT involves training teacher models alongside student models as outlined in Algorithm 1. Compared to Teacher-only, the potential limitation of LoT is that it requires additional computation and memory for the student models. Therefore, in our results in Section 3, we maintain the same total training steps between LoT (accumulated for the teacher and student) and Teacher-only and demonstrate that LoT achieves better generalization performance under the same number of updates. In this regard, we show the test accuracy of image classification between LoT and Teacher-only using ViT models with respect to the total training steps in Figure 3. We note that LoT achieves better test accuracy than Teacher-only in both ViT-B/16 and ViT-L/16 with fewer total training steps. Moreover, we demonstrate that LoT remains effective even when the student model is smaller than the teacher model in Table 3, which further reduces the computation cost compared to Teacher-only in the same total training steps and accommodates different student model choices with resource constraints. We provide more results regards efficiency of LoT in Appendix H.

In RL tasks, only the teacher model interacts with the environment to collect samples, and the student can learn from the teacher samples exclusively (please refer to Appendix G for the algorithm of

 
**Pretrained** & **Downstream** & **Teacher** & **Student** & **Image Size** & **Teacher/Student \#Param.** & **Teacher-only** & **LoT** \\   & & ResNet-18 & MobileNetV2 & \(224^{2}\) & 12M / 4M & \(81.14 0.58\) & \( 0.36\) \\  & & ResNet-18 & ResNet-18 & \(224^{2}\) & 12M / 12M & \(81.14 0.58\) & \( 0.25\) \\  & & ResNet-18 & ResNet-50 & \(224^{2}\) & 12M / 26M & \(81.14 0.58\) & \( 0.26\) \\  & & ResNet-50 & MobileNetV2 & \(224^{2}\) & 26M / 4M & \(84.09 0.32\) & \( 0.44\) \\  & & ResNet-50 & ResNet-18 & \(224^{2}\) & 26M / 12M & \(84.09 0.32\) & \( 0.19\) \\  & & ResNet-50 & ResNet-50 & \(224^{2}\) & 26M / 26M & \(84.09 0.32\) & \( 0.38\) \\   & & ViT-B/16 & ViT-B/16 & \(384^{2}\) & 86M / 86M & \(91.57 0.31\) & \( 0.35\) \\  & & ViT-B/16 & ViT-L/16 & \(384^{2}\) & 86M / 307M & \(91.57 0.31\) & \( 0.44\) \\  & & ViT-L/16 & ViT-B/16 & \(384^{2}\) & 307M / 86M & \(93.44 0.28\) & \( 0.33\) \\  & & ViT-L/16 & ViT-L/16 & \(384^{2}\) & 307M / 307M & \(93.44 0.28\) & \( 0.26\) \\   & & ViT-B/16 & ViT-B/16 & \(384^{2}\) & 86M / 86M & \(83.97 0.11\) & \( 0.15\) \\  & & ViT-B/16 & ViT-L/16 & \(384^{2}\) & 86M / 307M & \(83.97 0.11\) & \( 0.08\) \\  & & ViT-L/16 & ViT-B/16 & \(384^{2}\) & 307M / 86M & \(85.15 0.17\) & \( 0.09\) \\   & & ViT-L/16 & ViT-L/16 & \(384^{2}\) & 307M / 307M & \(85.15 0.17\) & \( 0.11\) \\   & & Swin-B & Swin-B & \(384^{2}\) & 88M / 88M & \(86.37 0.06\) & \( 0.15\) \\   & & Swin-B & Swin-L & \(384^{2}\) & 88M / 197M & \(86.37 0.06\) & \( 0.14\) \\   & & Swin-L & Swin-B & \(384^{2}\) & 197M / 88M & \(87.27 0.11\) & \( 0.12\) \\    & & Swin-L & Swin-L & \(384^{2}\) & 197M / 197M & \(87.27 0.11\) & \( 0.09\) \\  

Table 3: The test accuracy of the teacher model for various teacher-student model combinations in LoT and the baseline. Results are averaged over three runs. LoT consistently enhances test performance in all model choices and datasets.

Figure 3: Test accuracy of teacher models in LoT and Teacher-only using ViT-B/16 and ViT-L/16 on CIFAR-100. LoT achieves higher test accuracy with fewer training steps.

PPO-version LoT ). Therefore, LoT introduces negligible computation costs since sample collections are more resource-intensive than fitting the agent network to the samples in RL. For instance, in our Atari games experiments, the training time of LoT (606 minutes) is comparable to the Teacher-only setting (597 minutes) on a single NVIDIA A6000 GPU.

### Additional Investigation

Comparison to KD.To investigate the effect of LoT compared to other student-teacher learning paradigms, we compare LoT to the born-again networks (BAN) baseline . In BAN, we select the checkpoint with the best performance of the Teacher-only model as the (frozen) teacher and distill its knowledge into a student model with an identical architecture. Equal weights are assigned to the hard loss (from the dataset) and soft loss (from the teacher) to train the student model . All other configurations remain consistent with LoT. The results in Table 4 indicate that LoT achieves superior performance than BAN with a strong feedback model, further indicating the significance of the interactive learning process in LoT.

Effect of regularization coefficient \(\).The strength of regularization plays a crucial role in the overall training effect . To investigate the effects of LoT on the generalization of the teacher model, we perform experiments on PTB using the LSTM architecture for both teacher and student models. The configuration follows Section 3.3, except that we gradually increase the value of \(\) in LoT from \(0\) to \(1.7\) and examine the test PPL of the teacher model. The results are presented in Figure 4 (left). We observe that the performance of the teacher model improves rapidly as \(\) increases from \(0\) to \(1\), and when the value exceeds this point, the performance of the teacher begins to decline. This observation suggests that moderate feedback from the student is most beneficial for the teacher, but an excessively strong signal can hinder the teacher's learning process. Similar effects of large \(\) values have been noted in joint teacher-student training in knowledge distillation .

Effect of student steps ratio \(N\).To demonstrate the importance of the student steps ratio \(N\) in LoT, we conduct additional experiments by training LSTM teacher and student models on PTB using various values of \(N\). The empirical results presented in Figure 4 (right) indicate that the teacher benefits most from a moderate \(N\) value, such as \(4\) or \(5\). This finding suggests that achieving a balanced ratio between teacher and student model updates is crucial for optimal performance. When \(N\) is too low, the student may not sufficiently learn from the teacher, thereby reducing the quality of the feedback it provides. Conversely, if \(N\) is too high, the student may overfit the teacher's errors, resulting in less effective imitability measurement.

  
**Dataset** & **Teacher** & **Student** & **Teacher-only** & **BAN (Student)** & **LoT (Teacher)** \\  CIFAR-100 & ResNet-18 & ResNet-18 & 81.14 & 82.08 & **82.89** \\ CIFAR-100 & ResNet-50 & ResNet-50 & 84.09 & 84.73 & **86.04** \\ CIFAR-100 & ViT-B/16 & ViT-B/16 & 91.57 & 92.44 & **93.17** \\ CIFAR-100 & ViT-L/16 & ViT-L/16 & 93.44 & 93.82 & **94.18** \\   

Table 4: Performance comparison of Teacher-only, BAN and LoT on CIFAR-100. LoT achieves superior performance to Teacher-only and BAN.

Figure 4: Effects of regularization coefficient \(\) (left) and student steps ratio \(N\) (right). \(=1\) is the best \(\) value to achieve the lowest test perplexity of the teacher model, and moderate student steps ratio \(N\) such as 4 and 5 benefit the teacher model the most.

Conclusion

Identifying generalizable multiscale correlations from the vast space of possible correlations remains a significant challenge in machine learning. Inspired by cognitive science beliefs about human intelligence, we have shown experimentally that generalizable correlations are more imitable by other learners. In particular, we introduced a novel regularization method, LoT, which identifies generalizable correlations by teaching student models and exploiting their feedback. We conducted comprehensive experiments across various learning tasks and neural architectures. The results demonstrate that our proposed regularizer enhances model performance effectively and efficiently. In conclusion, our proposed LoT regularization offers a promising new approach to improve the generalization of neural networks by leveraging the learning process of student models and incorporating their feedback to refine the teacher model.

## 5 Acknowledgments

Metaxas is partially supported by research grants from NSF: 2310966, 2235405, 2212301, 2003874, 1951890, AFOSR 23RT0630, and NIH 2R01HL127661.