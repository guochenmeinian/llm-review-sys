# GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces

Josephine Lamp\({}^{1,2}\) Mark Derdzinski\({}^{2}\) Christopher Hannemann\({}^{2}\)

Joost van der Linden\({}^{2}\) Lu Feng\({}^{1}\) Tianhao Wang\({}^{1}\) David Evans\({}^{1}\)

\({}^{1}\)University of Virginia, Charlottesville, VA, USA; \({}^{2}\)Dexcom, USA

jl4rj@virginia.edu; {mark.derdzinski; christopher.hannemann;

joost.vanderlinden}@dexcom.com; {lu.feng; tianhao; evans}@virginia.edu

###### Abstract

We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synthetic glucose traces with strong privacy guarantees.

## 1 Introduction

The sharing of medical time series data can facilitate therapy development. As a motivating example, sharing glucose traces can contribute to the understanding of diabetes disease mechanisms and the development of artificial insulin delivery systems that improve people with diabetes' quality of life. Unsurprisingly, there are serious legal and privacy concerns (e.g., HIPAA, GDPR) with the sharing of such granular, longitudinal time series data in a medical context . One solution is to generate a set of synthetic traces from the original traces. In this way, the synthetic data may be shared publicly in place of the real ones with significantly reduced privacy and legal concerns.

This paper focuses on the problem of generating high-quality, privacy-preserving synthetic glucose traces, a task which generalizes to other time series sources and application domains, including activity sequences, inpatient events, hormone traces and cyber-physical systems. Specifically, we focus on long (over 200 timesteps), bounded, univariate time series glucose traces. We assume that available data does not have any labels or extra information including features or metadata, which is quite common, especially in diabetes. Continuous Glucose Monitors (CGMs) easily and automatically send glucose measurements taken subcutaneously at fixed intervals (e.g., every 5 minutes) to data storage facilities, but tracking other sources of diabetes-related data is challenging . We characterize the quality of the generated traces based on three criteria-- synthetic traces should (1) conserve characteristics of the real data, i.e., glucose dynamics and control-related metrics (_fidelity_); (2) contain representation of diverse types of realistic traces, without the introduction of anomalous patterns that do not occur in real traces (_breadth_); and (3) be usable in place of the original data for real-world use cases (_utility_).

Generative Adversarial Networks (GANs)  have shown promise in the generation of time series data. However, previous methods for time series synthesis, e.g., [4; 5; 6], suffer from one or more of the following issues when applied to glucose traces: 1) surprisingly, they do not generate realistic synthetic glucose traces - in particular, they produce human physiologically impossible phenomenon in the traces; 2) they require additional information (features, metadata or labels) to guide the model learning which are not available for our traces; 3) they do not include any privacy guarantees, or, in order to uphold a strong formal privacy guarantee, severely degrade the utility of the synthetic data.

Generating high-quality synthetic glucose traces is a difficult task due to the innate characteristics of glucose data. Glucose traces can be best understood as sequences of events, which we call _motifs_, shown in Figure 1, and they are more event-driven than many other types of time series. As such, a current glucose value may be more influenced by an event that occurred in the far past compared to values from immediate previous timesteps. For example, a large meal eaten earlier in the day (30-90 minutes ago) may influence a patient's glucose more than the glucose values from the past 15 minutes. As a result, although there is some degree of temporal dependence within the traces, _only_ conserving the immediate temporal relationships amongst values at previous timesteps does not adequately capture the dynamics of this type of data. In particular, we find that the main reason previous methods fail is because they may not sufficiently learn event-related characteristics of glucose traces.

**Contributions.** We present _GlucoSynth_, a privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (events) within the traces, in addition to the typical temporal dynamics contained within time series. We formalize the concept of motifs and define a notion of _motif causality_, inspired from Granger causality , which characterizes relationships amongst sequences of motifs within time series traces (Section 4). We define a local motif loss to first train a motif causality block that learns the motif causal relationships amongst the sequences of motifs in the real traces. The block outputs a motif causality matrix, that quantifies the causal value of seeing one particular motif after some other motif. Unrealistic motif sequences (such as a peak to an immediate drop in glucose values) will have causal relationships close to 0 in the causality matrix. We build a novel GAN framework that is trained to optimize motif causality within the traces in addition to temporal dynamics and distributional characteristics of the data (Section 5). Explicitly, the generator computes a motif causality matrix from each batch of synthetic data it generates, and compares it with the real causality matrix. As such, as the generator learns to generate synthetic data that yields a realistic causal matrix (thereby identifying appropriate causal relationships from the motifs), it implicitly learns not to generate unrealistic motif sequences. We also integrate differential privacy (DP)  into the framework (Section 6), which provides an intuitive bound on how much information may be disclosed about any individual in the dataset, allowing the GlucoSynth model to be trained with privacy guarantees. Finally, in Section 7, we present a comprehensive evaluation using 1.2 million glucose traces from individuals with diabetes collected across 2022, showcasing the suitability of our model to outperform all previous models and generate high-quality synthetic glucose traces with strong privacy guarantees.

## 2 Related Work

We focus the scope of our comparison on current state-of-the-art methods for synthetic time series which all build upon Generative Adversarial Networks (GANs)  and transformation-based approaches . An extended related work is in Appendix A.

Figure 1: Example Real Glucose Traces and Glucose Motifs from our Dataset.

**Time Series.** Brophy et al.  provides a survey of GANs for time series synthesis. TimeGan  is a popular benchmark that jointly learns an embedding space using supervised and adversarial objectives in order to capture the temporal dynamics amongst traces. Esteban et al.  develops two time series GAN models (RGAN/RCGAN) with RNN architectures, conditioned on auxiliary information provided at each timestep during training. TTS-GAN  trains a GAN model that uses a transformer encoding architecture in order to best preserve temporal dynamics. Transformation-based approaches such as real-valued non-volume preserving transformations (NVP)  and Fourier Flows (FF) , have also had success for time series data. These methods model the underlying distribution of the real data to transform the input traces into a synthetic data set. Methods that only focus on learning the temporal or distributional dynamics in time series are not sufficient for generating realistic synthetic glucose traces due to the lack of temporal dependence within sequences of glucose motifs.

**Differentially-Private GANs.** To protect sensitive data, several GAN architectures have been designed to incorporate privacy-preserving noise needed to satisfy differential privacy guarantees . Frigerio et al.  extends a simple differentially-private architecture (dpGAN) to time-series data and RDP-CGAN  develops a convolutional GAN architecture specifically for medical data. These methods find large gaps in performance between the non-private and private models. Providing strong theoretical DP guarantees using these methods often results in synthetic data with too little fidelity for use in real-world scenarios. Our framework carefully integrates DP into the motif causality block and each network of the GAN, resulting in a better utility-privacy tradeoff than previous methods.

## 3 Preliminaries

### Motifs

Glucose (and many other) traces can be best understood as sequences of events or _motifs_. Motifs characterize phenomenon in the traces, such as peaks or troughs. We define a _motif_, \(\), as a short, ordered sequence of values (\(v\)) of specified length \(\), \(=[v_{i},v_{i+1},,v_{i+}]\) and \(\) is a tolerance value to allow approximate matching (within \(\) for each value). Some examples of glucose traces and motifs are shown in Figure 1. We denote a set of \(n\) time series traces as \(X=[x_{1},...,x_{n}]\). Each time series may be represented as a sequence of motifs: \(x_{i}=[_{i_{1}},_{i_{2}}...]\) where each \(i_{j}\) gives the index of the motif in the set that matches \(x_{i_{j^{}},},...x_{i_{(j+1)},-i}\). Given the motif length \(\), the motif set is the union of all size-\(\) chunks in the traces. This definition is chosen for a straightforward implementation but motifs can be generated in other ways, such as through the use of rolling windows or signal processing techniques [16; 17]. Motifs are pulled from the data such that there is always a match from a trace motif to a motif from the set (if multiple matches, the closest one is chosen).

### Glucose Dynamics (Why Standard Approaches Fail)

We first present a study of the characteristics of glucose data in order to motivate the development of our framework. Although there are general patterns in sequences of glucose motifs (e.g., motif patterns corresponding to patients that eat 2x vs. 3x a day), individual glucose motifs are typically not time-dependent, as illustrated in Figure 2. The radial graphs display the temporal distribution of

Figure 2: Temporal Distributions of Sample Motifs. Each radial graph displays the temporal distribution of a motif; there are 24 radial bars from 00:00 to 23:00, and each segment displays the % of motif occurrences by each hour. Glucose motifs 1 and 2 are from Fig. 1; they are not temporally-dependent and show up across the day. Temporal motifs 1 and 2 are from a cardiology dataset .

the first two glucose motifs from Figure 1 and two temporally-dependent motifs from a cardiology dataset . There are 24 radial bars from 00:00 to 23:00 for each hour of the day, and the bar value is the percentage of total motif occurrences at that hour across the entire dataset (i.e., value of 10 would indicate that 10% of the time that motif occurs during that hour). Note that the glucose motifs show up fairly evenly _across_ all hours of the day whereas the motifs from the cardiology dataset have shifts in their distribution and show up frequently at _specific_ hours of the day. The lack of temporal dependence in glucose motifs is likely due to the diverse patient behaviors within a patient population. Glucose in particular is highly variable and influenced by many factors including eating, exercise, stress levels, and sleep patterns. Moreover, due to innate variability within human physiology, motif occurrences can differ even for the _same_ patient across weeks or months. These findings indicate that only conserving the temporal relationships within glucose traces (as many previous methods do) may not be sufficient to properly learn glucose dynamics and output realistic synthetic traces.

### Granger Causality

Granger causality  is commonly used to quantify relationships amongst time series without limiting the degree to which temporal relationships may be understood as done in other time series models, e.g., pure autoregressive ones. In this framework, an entire system (set of traces) is studied _together_, allowing for a broader characterization of their relationships, which may be advantageous, especially for long time series. We define \(x_{t}^{n}\) as an \(n\)-dimensional vector of time series observed across \(n\) traces and \(T\) timesteps. To study causality, a vector autoregressive model (VAR)  may be used. A set of traces at time \(t\) is represented as a linear combination of the previous \(K\) lags in the series: \(x_{t}=_{k=1}^{K}A^{(k)}x_{t-k}+e_{t}\) where each \(A^{(k)}\) is a \(n n\) dimensional matrix that describes how lag \(k\) affects the future timepoints in the series' and \(e_{t}\) is a zero mean noise. Given this framework, we state that time series \(q\) does not _Granger-cause_ time series \(p\), if and only if for all \(k\), \(A^{(k)}_{p,q}=0\). To better represent nonlinear dynamics amongst traces, a nonlinear autoregressive model (NAR) , \(g\), may be defined, in which \(x_{t}=g(x_{1_{<t}},...,x_{n_{<t}})+e_{t}\) where \(x_{p_{<t}}=(x_{p_{1}}...,x_{p_{t-1}},x_{p_{t}})\) describes the past of series \(p\). The NAR nonlinear functions are commonly modeled jointly using neural networks.

## 4 Motif Causality

Using Granger causality as defined would overwhelm the generator with too much information, resulting in convergence issues for the GAN. Instead of looking at traces comprehensively, we need a way to _scope_ how the generator understands relationships between time series. To this end, we aim to use the same intuition developed from Granger causality, namely developing an understanding of relationships comprehensively using less stringent temporal constraints, but scope these relationships specifically in terms of _motifs_. Therefore, we develop a concept of _motif causality_ which, by learning causal relationships amongst sequences of motifs, allows the generator to learn realistic motif sequences and produce high quality synthetic traces as a result.

### Extending Granger Causality to Motifs

In order to quantify the relationships amongst sequences of motifs to best capture glucose dynamics, we extend the idea of Granger causality to work with motifs. Given a motif set with \(m\) motifs, we build a separate (component) model, called a _motif network_ in our method, for each motif, resulting in \(m\) motif networks. For a single motif \(_{i}\) at time \(t\), \(_{i_{t}}\), we define a function \(g_{i}\) specifying how motifs in previous timesteps are mapped to that motif: \(_{i_{t}}=g_{i}(_{1_{<t}},...,_{m_{<t}})+e_{i_{t}}\) where \(_{j_{<t}}=(_{j_{1}}...,_{j_{t-1}},_{j_{t}})\) describes the past of motif \(_{j}\). The output of \(g_{i}\) is a vector, which is added to the noise vector \(e_{i_{t}}\). Essentially, we define motif \(_{i}\) in terms of its relationship to past motifs. The \(g_{i}\) function takes in some _mapping_ that describes how motifs in previous timesteps are mapped to the current motif \(_{i_{t}}\). The mapping is not specified in this notation, and could be defined in many different ways. In our case, we instantiate \(g_{i}\) using a single-layer LSTM, described next.

A \(g_{i}\) function for each motif \(_{i}\) in the motif set is modeled using a motif network with a single-layer RNN architecture. For a RNN predicting a single component motif, let \(h_{t}^{m}\) represent the \(m\)-dimensional hidden state at time \(t\). This represents the historical context of the motifs in the series for predicting a component motif at time \(t\), \(_{i_{t}}\). At time \(t\), the hidden state is updated: \(h_{t}=g_{i}(h_{t-1})+e_{i_{t}}\). \(g_{i}\) here is the function describing how motifs in previous timesteps are mappedto the current motif, and is modeled (instantiated) as a single-layer LSTM as they are good at modeling long, nonlinear dependencies amongst traces . The output for a motif \(_{i}\) at time \(t\), \(_{i_{t}}\) can be obtained by a linear decoding of the hidden state, \(_{i_{t}}=W^{o}h_{t}+e_{i_{t}}\), where \(W^{o}\) is a matrix of the output weights. These weights control the update of the hidden state and thereby control the influence of past motifs on this component motif. Essentially, this function learns a weighting that quantifies how helpful motifs in previous timesteps are for predicting the specified motif \(_{i}\) at time \(t\). We note that we define causality in this way based on how Granger causality models such relationships, which is different from traditional causality models.

If all elements in the \(j\)th column of \(W^{o}\) are zero (\(W^{o}_{:j}=0\)), this is a sufficient condition for an input motif \(_{j}\) being motif non-causal on an output \(_{i}\). Therefore, we can find the motifs that are motif-causal for motif \(_{i}\) using a group lasso penalty optimization across the columns of \(W^{o}\):

\[_{W}_{t=2}^{T}(_{i_{t}}-g_{i}(_{0_{<t}},...,_{m_{<t}}))^{2}+ _{j=1}^{m}||W^{o}_{:j}||_{2}\]

We define this as the _local motif loss_, \(_{ml}\), which is optimized in each motif network using proximal gradient descent.

### Training the Motif Causality Block

We next describe how the motif causality block is trained to learn motif causal relationships amongst traces, displayed in Figure 3. The block is structured in this way to accommodate the privacy integration (Section 6.2); here, we present its implementation without any privacy noise.

**Partition data.** First, the data is partitioned into \(r\) partitions (Step 1, Figure 3) such that no models are trained on overlapping data. The number of partitions, \(r\), is a user-specified hyperparameter.

**Build motif network for each motif.** Next, within each data partition a set of motif networks is trained. As a pre-processing step, we assume each trace has been chunked into a sequence of motifs of size \(\) (Section 3.1). \(\) is a hyperparameter, which we suggest chosen based on the longest effect time of a trace event. We use \(=48\), corresponding to 4 hours of time, because large glucose events (from behaviors like eating) are encompassed within that time frame; see Appendix B for more details. We assume a tolerance of \(=2\) mg/dL, chosen to allow for reasonable variations in glucose. To model motif causality for an entire set of data, a \(g_{i}\) function is implemented for each motif via a separate RNN motif net following the description provided previously, resulting in \(m\) total networks (Step 2a, Figure 3). If all the motifs were trained together using a single motif network, it would not be possible to quantify the exact causal effects between each individual motif as we would not know which exact motifs contributed to a prediction (only that there is some combination of unknown motifs that contribute to an accurate prediction for a particular motif). By training each motif network separately, we are able to quantify the exact effect each motif has on each other, without any confounding effects from other motifs.

**Combine outputs of individual motif networks.** Each motif network outputs a vector of weights \(W^{o}\) of dimensionality \(1 m\), corresponding to the learned causal relationships (Step 2b, Figure 3). Values in the vector are between 0 (no causal relationship) and 1 (strongest causal relationship) and

Figure 3: Motif Causality Block.

give the degree to which every other motif is motif causal of the particular motif \(_{i}\) the RNN was specialized for. To return a complete matrix that summarizes causal relationships amongst _all_ motifs, we stack the weights (Step 2c). The output of each data partition is a complete motif causality matrix, resulting in \(r\) total matrices, each of dimensionality \(m m\).

**Aggregate matrices and integrate with GAN.** After motif causality matrices have been outputted from each data partition, the weights in the matrices are aggregated (Step 3, Figure 3) to return the final aggregate causality matrix, \(M\) (Step 4). In the nonprivate version, the weights are averaged. Finally, \(M\) is sent to the generator to help it learn how to conserve motif relationships within sequences of motifs in the synthetically generated data. Details are described next in the subsequent section.

## 5 GlucoSynth

The complete GlucoSynth framework, shown in Figure 4, comprises four key blocks: the motif causality block (explained previously in Section 4), an autoencoder, a generator and a discriminator. We walk through the remaining components of the framework surrounding the GAN next.

### GAN Architecture Components

**Autencoder.** We use an autoencoder (AE) with an RNN architecture to learn a lower dimensional representation of the traces, allowing the generator to better preserve underlying temporal dynamics of the traces. The autoencoder consists of two networks: an _embedder_ and a _recovery network_. The embedder uses an encoding function to map the real data into a lower dimensional space: \(Enc(x):x^{n} x_{e}^{e}\) while the recovery network reverses this process, mapping the embedded data back to the original dimensional space: \(Dec(x_{e}):x_{e}^{e}^{n}\). A toolboxforted autoencoder perfectly reconstructs the original input data, such that \(x= Dec(Enc(x))\). This process yields the Reconstruction Loss, \(_{R}\), the Mean Square Error (MSE) between the original data \(x\) and the recovered data, \(\): \((x,)\).

**Generator.** We implement the generator via an RNN or LSTM. Importantly, the generator works in the embedded space, by receiving the input traces passed through the embedder (\(x_{e}\)). To generate synthetic data, a random vector of noise, \(z\) is passed through the generator and then the recovery network to return the synthetic traces in the original dimensional space. To learn how to produce high-quality synthetic data, the generator receives three key pieces of information:

_1 - Stepwise._ The generator receives batches of real data to guide the generation of realistic next step vectors. To do this, a Stepwise Loss, \(_{S}\), is computed at time \(t\) using the MSE between the batch of embedded real data, \(x_{ct}\), and the batch of embedded synthetic data, \(_{et}\): \((x_{ct},_{et})\). This allows the generator to compare (and learn to correct) the discrepancies in stepwise data distributions.

_2 - Motif Causality._ The generator needs to preserve sequences of motifs in addition to temporal dynamics. Using the aggregate causality matrix \(M\) returned from the Motif Causality Block, the generator computes a motif causality matrix, \(M_{}\), on the set of synthetic data \(\). Because the original

Figure 4: Overview of GlucoSynth Architecture.

causality matrix was not trained on data in the embedded space, we first run the set of embedded synthetic data through the recovery network \(_{e}\). From there, the Motif Causality Loss, \(_{M}\), is computed as the MSE error between the two matrices: \((M,M_{})\). These matrices give a causal value of seeing a motif \(_{i}\) in the future after some motif \(_{j}\)-- unrealistic motif sequences will have causal values close to 0. As the generator learns to generate synthetic data that yields a realistic causal matrix (thereby identifying appropriate causal relationships from the motifs), it implicitly learns to not generate unrealistic motif sequences.

_3 - Distributional._ To guide the generator to produce a diverse set of traces, the generator computes a Distributional Loss, \(_{D}\), the moments loss (MML), between the overall distribution of the real data \(x_{e}\) and the distribution of the synthetic data \(_{e}\): \((x_{e},_{e})\). The MML is the difference in the mean and variance of two matrices.

**Discriminator.** The discriminator is a traditional discriminator model using an RNN, the only change being it also works in the embedded space. The discriminator yields the Adversarial Loss Real, \(_{Ar}\), the Binary Cross Entropy (BCE) between the discriminator guesses on the real data \(y_{x_{e}}\) and the ground truth \(y\), a vector of 0's, \((y_{x_{e}},y)\) and the Adversarial Loss Fake, \(_{Af}\), the BCE between the discriminator guesses on the fake data \(y_{_{e}}\) and the ground truth \(y\), a vector of 1's, \((y_{_{e}},y)\).

### Training Procedure

First, the motif causality block is trained following the procedure described in Section 4.2, and then the rest of the GAN is trained. The autoencoder is optimized to minimize \(_{R}+_{S}\), where \(\) is a hyperparameter that balances the two loss functions. If the AE only receives \(_{R}\) (as is typically done), it becomes overspecialized, i.e., it becomes too good at learning the best lower dimensional representation of the data such that the embedded data are no longer helpful to the generator. For this reason, the AE also receives \(_{S}\), enabling the dual training of the generator and embedder. The generator is optimized using \((1-_{Af})+(_{S}+_{D})+_{M}\), where \(\) is a hyperparameter that balances the effect of the stepwise and distributional loss. Finally the discriminator is optimized using the traditional adversarial feedback \(_{Af}+_{Ar}\). The networks are trained in sequence (within each epoch) in the following order: autoencoder, generator, then discriminator. In our experiments we set \(=0.1\) and \(=10\) as they enable GlucoSynth to converge fastest, i.e., in the fewest epochs.

## 6 Providing Differential Privacy

There are two components to our privacy architecture, described in the following two subsections: (1) each network in the GAN (Embedder, Recovery, Generator and Discriminator networks) is trained in a differentially private manner using the Differentially-Private Stochastic Gradient Descent (DP-SGD) algorithm from Abadi et al. ; and (2) the motif causality block is trained using the PATE framework from Papernot et al. . Importantly, two completely separate datasets are used for the training of the motif causality block (dataset B in Figure 4) and the GAN (dataset A in Figure 4). We structure the privacy integration in this way to allow for better privacy-utility trade-offs. Our design satisfies the formal differential privacy notion introduced by Dwork et al. . Differential Privacy (DP) provides an intuitive bound on the amount of information that can be learned about any individual in a dataset. A randomized algorithm \(\) satisfies \((,)\)-differential privacy if, for all datasets \(D_{1}\) and \(D_{2}\) differing by at most a single unit, and all \(S()\), \(Pr[(D_{1}) S] e^{}Pr[(D_{2}) S]+\). The parameters \(\) and \(\) determine the _privacy loss budget_, which provide a way to tradeoff privacy and utility; smaller values have stronger privacy. Importantly, privacy is provisioned at the _trace_ level, and we assume each individual has only one trace in the dataset.

### Training the GAN Networks with DP

To add privacy to the GAN components, each of the networks (Embedder, Recovery, Generator and Discriminator) is trained in a differentially private manner using DP-SGD . Although the overall GAN framework is complicated, the individual networks all use simple RNN or LSTM architectures with Adam optimizers. As such, adding DP noise to their network weights is straightforward. We employ the following procedure using Tensorflow Privacy functions . Since there are four networks being trained with DP, we divide the privacy loss budget evenly to get the budget per network, \(_{net}=/4\). Then, we use Tensorflow's built-in DP accountant to determine how much noise must be added to the weights of each network based on the number of epochs, batch size, number of traces and \(_{net}\). This function returns a noise multiplier, which we use when we instantiate a Tensorflow DP Keras Adam Optimizer for each network. Finally, we train each of the networks using their respective DP Keras Adam Optimizer, which automatically trains the network using DP-SGD.

### Training the Motif Causality Block with DP

We train the motif causality block using the PATE framework . PATE provides a way to return aggregated votes about the class a data point belongs to. First, the data is partitioned into \(r\) partitions, where \(r\) is determined based on the size of the dataset and the privacy loss budget. Then, a class membership model is trained independently for each partition. The class membership votes from each partition are aggregated by adding noise to the vote matrix and the noisiest votes are returned using the max-of-Laplacian mechanism (LNMax), tuned based on the privacy budget and \(r\).

We use PATE to train the motif causality block: instead of predicting the degree of class membership we predict _causal_ membership, e.g., does motif \(_{i}\) have a causal relationship to \(_{j}\). The motif causality block is trained in the same procedure described in Section 4.2 with two changes: (1) the number of data partitions, \(r\), is determined based on the privacy budget, instead of a user-specified value; (2) the final causality matrix \(M\) is aggregated using DP across the partitions. In normal PATE, carefully calibrated noise is added to a matrix of votes for each class, such that the classes with the noisiest votes are outputted. In our use, each value in a motif causality matrix may be likened to a class (i.e., causal "class" prediction between motif \(_{i}\) and \(_{j}\)). Thus, we use the LNMax mechanism (from predefined Tensorflow Privacy functions ) to aggregate the matrices weights and return \(M\).

We use PATE instead of training each motif network using DP-SGD for better privacy-utility trade-offs. With DP-SGD, we would need to add noise to _every_ motif net, eating up our privacy budget quickly and severely impacting the quality of the returned casuality matrices. PATE allows us to train each of the motif networks without any noise on the gradients, but then aggregates their returned causality matrices in a privacy-preserving manner, resulting in a better privacy-utility trade-off.

## 7 Evaluation

Evaluating synthetic data is notoriously difficult , so we provide an extensive evaluation across three criteria. Synthetic data should: 1) conserve characteristics of the real data (_fidelity_, Section 7.1); 2) contain diverse patterns from the real data without the introduction of anomalous patterns (_breadth_, Section 7.2); and 3) be usable in place of the original for real-world use cases (_utility_, Section 7.3).

**Data and Benchmarks.** We use 100,000 single-day glucose traces randomly sampled across each month from January to December 2022, for a total of 1.2 million traces, collected from Dexcom's G6 Continuous Glucose Monitors (CGMs) . Data was recorded every 5 minutes (\(T=288\)) and each trace was aligned temporally from 00:00 to 23:59. We restrict our comparison to the five most closely related state-of-the-art models for generating synthetic univariate time series with no labels or auxiliary data: Three nonprivate--TimeGAN , Fourier Flows (FF) , non-volume preserving transformations (NVP) ; and two private--RGAN  and dpGAN . We refer the reader to Appendix B for additional experimental details and all hyperparameter settings, including reasoning behind the choice of motif size \(=48\).

### Fidelity

**Visualization.** We provide visualizations of sample real and synthetic glucose traces from all models. Although this is not a comprehensive way to evaluate trace quality, it does give a snapshot view about what synthetic traces may look like. We provide heatmap visualizations, where each heatmap contains 100 randomly sampled glucose traces. Each row is a single trace from timestep 0 to 288. The values in each row indicate the glucose value (between 40 mg/dL and 400 mg/dL). Figure 5 shows the nonprivate models, and Figures 8, 9, 10 in Appendix C.1 show the private models with different privacy budgets. Upon examining the heatmaps, we notice that GlucoSynth consistently generates realistic looking glucose traces, even at very small privacy budgets.

**Population Statistics.** To evaluate fidelity on a population scale, we compute a common set of glucose metrics and test if the difference between the synthetic and real data is statistically significant. Table 1 provides an abbreviated summary of the results; Appendix C.2 has complete results. GlucoSynth performs the best, with few statistical differences between the real and synthetic data for \( 0.1\).

**Distributional Comparisons.** We visualize differences in distributions between the real and synthetic data by plotting the distribution of variances and using PCA . Figure 6 shows the variance distribution for the nonprivate models. Additional comparisons across privacy budgets are available in Appendix C.3. In both nonprivate and private settings, GlucoSynth produces synthetic distributions closest to the real ones, better than all other models.

### Breadth

We quantify breadth in terms of glucose motifs. For each model's synthetic traces, we build a motif set (see Section 3.1). Given a real motif set from the validation traces \(S_{x}\), for each synthetic motif set \(S_{}\), we compute "Validation Motifs", (VM), the fraction of motifs found in the validation motif set that are present in the synthetic motif set, \(/|S_{}|\). This metric quantifies how good our synthetic motif set is (e.g., are its motifs mostly similar to motifs found in real traces). We also compute metrics related to _coverage_, the fraction of motifs in the validation motif set that are found in our synthetic data, defined as \(/|S_{x}|\). This gives a sense of the breadth in a more traditional manner. To compare actual _distributions_ of motifs (not just counts), we compute the MSE between the distribution of real motifs \(S_{x}\) and the distribution of synthetic motifs \(S_{}\). This gives a measure about how close the synthetic motif distribution is to the real one. We want high VM and coverage, and low MSE. Results are in Table 1 with additional analysis in Appendix D; overall our model provides the best breadth.

### Utility

We evaluate our synthetic glucose traces for use in a glucose forecasting task using the common paradigm TSTR (Train on Synthetic, Test on Real), in which the synthetic data is used to train the model and then tested on the real validation data. We train an LSTM network optimized for glucose forecasting tasks  and report the Root Mean Square Error (RMSE) in Table 1. We run the experiment 10 times and train the LSTM for 10,000 epochs. We have also tested with other models including RNNs, attention-based models and other LSTM architectures (such as bidirectional LSTMs) but show the results for the best performing model, the LSTM optimized for glucose forecasting. Since RMSE provides a limited view about the model's predictions, we also plot the Clarke Error Grid , which visualizes the differences between a predictive and reference measurement, and is a basis for evaluating the safety of diabetes-related medical devices. More details are in Appendix E. GlucoSynth provides the best forecasting results compared to all other models across all privacy budgets.

Figure 5: Heatmaps for Nonprivate Models

[MISSING_PAGE_FAIL:10]