# _Banana_: _Bana_ch Fixed-Point _N_etwork for Pointcloud Segmentation with Inter-Part Equiv_a_riance

Congyue Deng\({}^{1*}\) Jiahui Lei\({}^{2*}\) Bokui Shen\({}^{1}\) Kostas Daniilidis\({}^{2,3}\) Leonidas Guibas\({}^{1}\)

\({}^{1}\) Stanford University \({}^{2}\) University of Pennsylvania \({}^{3}\) Archimedes, Athena RC

{congyue, willshen, guibas}@cs.stanford.edu, {leijh, kostas}@cis.upenn.edu

###### Abstract

Equivariance has gained strong interest as a desirable network property that inherently ensures robust generalization. However, when dealing with complex systems such as articulated objects or multi-object scenes, effectively capturing inter-part transformations poses a challenge, as it becomes entangled with the overall structure and local transformations. The interdependence of part assignment and per-part group action necessitates a novel equivariance formulation that allows for their co-evolution. In this paper, we present _Banana_, a Banach fixed-point network for pointcloud segmentation with inter-part equivariance **by construction**. Our key insight is to iteratively solve a fixed-point problem, where point-part assignment labels and per-part \((3)\)-equivariance co-evolve simultaneously. We provide theoretical derivations of both per-step equivariance and global convergence, which induces an equivariant final convergent state. Our formulation naturally provides a strict definition of inter-part equivariance that generalizes to unseen inter-part configurations. Through experiments conducted on both articulated objects and multi-object scans, we demonstrate the efficacy of our approach in achieving strong generalization under inter-part transformations, even when confronted with substantial changes in pointcloud geometry and topology.

## 1 Introduction

From articulated objects to multi-object scenes, multi-body systems, i.e. shapes composed of multiple parts where each part can be moved separately, are prevalent in various daily-life scenarios. However, modeling such systems presents considerable challenges compared to rigid objects, primarily due to the infinite shape variations resulting from inter-part pose transformations with exponentially growing complexities. Successfully modeling these shapes demands generalization across potential inter-part configurations. While standard data augmentation techniques can potentially alleviate such problems, exhaustive augmentation can be highly expensive especially given the complexity of such systems compared to rigid objects. Meanwhile, recent rigid-shape analysis techniques have made significant progress in building generalization through the important concept of equivariance . Equivariance dictates that when a transformation is applied to the input data, the network's output should undergo a corresponding transformation. For example, if a is rotated by 90 degrees, the segmentation masks should also rotate accordingly. However, to the best of our knowledge, no existing work has managed to extend the formulation of equivariance to inter-part configurations.

In this paper, we address the challenge of achieving inter-part equivariance in the context of part segmentation, a fundamental task in analysis, which is also the key to generalizing equivaraince from single object to multi-body system. Extending equivariance to multi-body systems presents notable challenges in both formulation and realization. When dealing with a shape that allows for inter-part motions, the model must exhibit equivariance to both global and local state changes, which can only be defined by combining part assignment and per-part transformations. For example, to model anoven's inter-part states, we first need to differentiate between the parts corresponding to the door and the body and then define the movement of each individual part. An intriguing "chicken or the egg" problem arises when attempting to build inter-part equivariance without access to a provided segmentation, where segmentation is necessary to define equivariance, but segmentation itself is the desired output that we aim to produce.

Our key insight to tackle this seeming dilemma is to model inter-part equivariance as a sequential fixed-point problem by co-evolving part segmentation labels and per-part \((3)\)-equivariance. We provide theoretical derivations for both the per-step behavior and global convergence behavior, which are crucial aspects of a fixed-point problem. We show that our formulation establishes per-step equivariance through network construction, which then induces an overall inter-part equivariance upon convergence. Thus, by having equivariant per-step progression and global convergence, our formulation naturally gives rise to a strict definition of inter-part equivariance through iterative inference that generalizes to unseen part configurations. We further bring our formulation to concrete model designs by proposing a novel part-aware equivariant network with a weighted message-passing paradigm. With localized network operators and per-part \((3)\)-equivvarant features, the network is able to guarantee per-step inter-part equivariance as well as facilitate stable convergence during the iterative inference. We test our framework on articulated objects to generalize from static rest states to novel articulates and multi-object scenes to generalize from clean synthetic scenes to cluttered real scans. Our model shows strong generalization in both scenarios even under significant changes in pointcloud geometry or topology.

To summarize, our key contributions are

* To the best of our knowledge, we are the first to provide a strict definition of inter-part equivariance for pointcloud segmentation and introduce a learning framework with such equivariance **by construction**.
* We propose a fixed-point framework with one-step training and iterative inference and show that the per-step equivariance induces an overall equivariance upon convergence.
* We design a part-aware equivariant message-passing network with stable convergence.
* Experiments show our strong generalization under inter-part configuration changes even when they cause subsequent changes in pointcloud geometry or topology.

## 2 Related Work

**Equivariant pointcloud networks**. Existing works in equivariant 3D learning mainly focus on rigid \((3)\) transformations. As a well-studied problem, it has developed comprehensive theories [39; 14; 79; 2; 81] and abundant network designs [71; 20; 11; 15; 3; 36; 42; 55], which benefit a variety of 3D vision and robotics tasks ranging from pose estimation [44; 46; 54; 61; 89], shape reconstruction [11; 10], to object interaction and manipulation [19; 27; 60; 63; 80; 82]. A few recent works have extended \((3)\) equivariance to part level [86; 41; 50].  employs a local equivariant feature extractor for object bounding box prediction, showing robustness under object-level and scene-level pose changes.  learns an \((3)\)-equivvarant object prior and applies it to object

Figure 1: **”Chicken or the egg” problem with inter-part equivariance. Left:** Rigid \((3)\)-equivariance. If a rigid transformation is applied to the input, the output segmentation transforms accordingly. **Right:** Inter-part equivariance. Analogously, we want the input and output to be coherent under _per-part_ transformations. But such a definition of equivariance requires part segmentations, which is also exactly the desired network output, resulting in a “chicken or the egg” problem.

detection and segmentation in scenes with robustness to scene configuration changes.  learns part-level equivariance and pose canonicalization from a collection of articulated objects with a two-stage coarse-to-fine network. However, all these works are purely heuristics-based, and none has provided a strict definition of inter-part equivariance.

**Multi-body systems**. From small-scale articulated objects to large-scale multi-object scenes, there is a wide range of 3D observations that involve multiple movable parts or objects, exhibiting various configurations and temporal variations. This has sparked a rich variety of works dedicated to addressing the challenges in multi-body systems, studying their part correspondences [56; 49; 8; 48; 43], segmentations [85; 76; 67; 31; 30; 70; 12], reconstructions [34; 37; 32; 53; 40], or rearrangements [78; 68], to name just a few. However, as we know the system is acted via products of group actions; a more structured network that exploits such motion prior by construction is desired. Only a few works have studied this problem.  exploits global and local gauge equivariance in 3D object detection via bottom-up point grouping but without a systematical study of inter-object transformations.  further exploits equivariance of object compositions in scene segmentation, but as an EM-based approach, it requires exhaustive and inefficient enumerations and no global convergence is guaranteed. In contrast, our approach offers a theoretically sound framework that achieves multi-body equivariance by construction, which enables robust analysis for various tasks and provides a more structured and reliable solution compared to existing methods.

**Iterative inference**. From recurrent neural networks [23; 29] to flow [16; 59; 17; 38; 83] or diffusion-based generative models [65; 66; 28; 64; 51; 88], iterative inference has played many diverse yet essential roles in the development of computer vision. Though naturally adapted to sequential or temporal data, iterative inference can also be applied to static learning problems such as image analysis [22; 21; 73; 26] and pointcloud segmentation [84; 47]. Recent studies also show that iterative inference presents stronger generalizability than one-step forward predictions [62; 6], with explanations of their relations to the "working memory" of human minds [4; 5] or human visual systems [45; 35]. In our work, we employ iterative inference on static pointclouds to co-evolve two inter-twined attributes: part segmentation and inter-part equivaraince, in order to address a seemingly contradictory situation.

## 3 Inter-Part Equivariance

We start by defining inter-part equivariance on pointclouds with given part segmentations (Sec. 3.1). Then, we will introduce how to extend this definition to unsegmented pointclouds using a fixed-point framework (Sec. 3.2) with one-step training and iterative inference. The iterations have per-step equivariance by network construction, which is shown to induce an overall equivariance upon convergence (Sec 3.3). Finally, we will also explain how to eliminate part orders for instance segmentation (Sec 3.4).

### Multi-Body Systems

Let \(^{N 3}\) be a pointcloud with \(P\) parts. Segmentations on \(\) are represented as point-part assignment matrices \(^{N P}\) which sum up to 1 along the \(P\) dimension. As the action of \((3)^{P}\) on \(\) and its resulting equivariance is subject to the part decompositions, we define the action of \((3)^{P}\) on (pointcloud, segmentation) pairs \((,)\). When \(\) is a binary mask, the

Figure 2: **Single-state training and novel-state iterative inference. Left:** Training with ground-truth segmentation as both network input and output. **Right:** Equivariant Banach iterations. At each step \(k\), the network is equivariant to its current segmentation input \(^{(k)}\) by construction. At the end of the iterations, it converges to its fixed point with an induced overall equivariance.

parts of \(\) are \(P\) disjoint sub-pointclouds and each \((3)\) component of \((3)^{P}\) acts on one part separately. More concretely, for any transformation \(=(_{1},,_{P})(3)^{P}\), we define \((,):=(^{},)\) where the \(n\)-th point \(_{n}\) is transformed to \(^{}_{n}\) by

\[^{}_{n}:=_{p=1}^{P}_{np}(_{n}_{p}+_{p}).\] (1)

where \(_{p}\) and \(_{p}\) are the rotation and translation components of \(_{p}(3)\). For soft segmentation masks, we view each point as probabilistically assigned to one of the \(P\) parts.

**Equivariance**. Similar to \((3)\)-equivariance on rigid shapes \(f()=f(),(3)\), we define the inter-part equivariance on \((,)\) pairs by saying that a function \(f\) is \((3)^{P}\)-equivariant if \((3)^{P}\),

\[f((,))=f(,).\] (2)

Note that if the function outputs \(f(,)\) are no longer (pointcloud, segmentation) pairs, the \((3)^{P}\)-action on its outputs needs to be specified. A special case is when \((3)^{P}\) acts trivially on \(f(,)\) by \(f((,))=f(,)\), which defines the common-sense "invariance". For a pointcloud segmentation network with per-point part-label outputs \(^{N P}\), we desire it to be \((3)^{P}\)-invariant under the above definitions, that is \(f((,))\), \((3)^{P}\).

### Banach Fixed-Point Iterations

**"Chicken or the egg"**. The "chicken or the egg" nature of segmentation with inter-part equivariance immediately emerges from the above definition: to enforce \((3)^{P}\)-equivariance for a neural network \(f(;)\), the part segmentation \(\) is required as input, which is also exactly the desired output of the network (Fig. 1). That this,

\[f(,;)=.\] (3)

To resolve this dilemmatic problem, we approach Eq. 3 as a fixed-point equation on function \(f(,;)\)_w.r.t._\(\). And instead of using single-step forward predictions, we solve it with Banach fixed-point iterations (Fig. 2).

**Training**. At training time, we aim to optimize the network weights \(\) such that the labeled s \((_{i},_{i}),i\) from the dataset become fixed points of the function \(f(,)\):

\[_{*}=*{arg\,min}_{}|}_{i }\|f(_{i},_{i};)-_{i}\|.\] (4)

However, there exists a trivial solution which is the identity function \(f(,;)\), meaning that any arbitrary segmentation can satisfy the objective for any pointclouds. Thus, to avoid such degenerated cases, we have to limit the network expressivity. More specifically, we limit the Lipschitz constant \(L\) of the network _w.r.t._\(\) which is defined by

\[\|f(,_{1};)-f(,_{2};)\|  L\|_{1}-_{2}\|,,\, _{1},_{2}.\] (5)

If \(f\) degenerates to the identical mapping, its Lipschitz \(L=1\), which violates the above constraint. When \(L<1\), \(f\) is a contractive function and has a unique fixed point based on the Banach fixed-point theorem. Our training objective is to align this fixed point with the ground-truth segmentation.

**Iterative inference**. At inference time, given an input pointcloud \(_{0}\), the learnable parameters \(_{*}\) fixed in \(f\) and we look for a segmentation \(\) that satisfies

\[f_{_{0},_{*}}():=f(_{0},;_ {*})=.\] (6)

We solve this equation with Banach fixed-point iterations 

\[^{*}=_{k}^{(k)}=_{k}f_{_{0},_{*}}^{(k)}(^{(0)})\] (7)

where \(^{(0)}\) is a random initialization of the segmentation on \(_{0}\). For a contractive function \(f\), the uniqueness of \(^{*}\) induces a well-defined mapping from pointclouds to segmentations \(f_{}^{*}:_{0}^{*}\).

### Equivariance

For an input pairs \((,)\) with known segmentation \(\), we employ an \((3)\)-equivariant backbone  on each individual part and construct an \((3)^{P}\)-equivariant network, which will be further explained in Section 4. This guarantees the inter-part equivariance at training time and at each timestep during inference, but most importantly we desire the overall \((3)^{P}\)-equivariance at the convergence point of the Banach iteration. Here we prove two properties:

**Self-coherence of convergent state**. At each inference timestep \(k\), we have \((3)^{P}\)-equivariance subject to \(^{(k)}\), that is, \((3)^{P}\),

\[f((_{0},^{(k)});)=f(_{0}, ^{(k)};), k,\] (8)

Because of the continuity of \(f\) and the compactness of \(^{N P}\), we can take limits _w.r.t._\(k\) on both sides, which gives

\[f((_{0},^{*});)=f(_{0}, ^{*};)=f^{*}_{}(_{0}).\] (9)

This shows the self-coherence of the convergent point between \(f^{*}_{}\) and \(^{*}\).

**Generalization to novel inter-part states**. Suppose the network has seen \((,_{})\) in the training set and learned \(f_{}(,_{};_{*})=_{}\) with the loss function in Eq. 4 minimized to zero. Now apply an inter-part transformation \((3)^{P}\) and test the network on \((^{},_{}):=(, _{})\). We would like to show \(f^{*}_{}(^{})=f^{*}_{}()=_{ }\). First of all, we know that \(_{}\) itself is a fixed point of \(f(^{},;)\), which is due to the equivariance _w.r.t._\(_{}\) at training time:

\[f(^{},_{};)=f((, _{});)=f(,_{};).\] (10)

Now if \(f\) is a contractive function on \(\), we have the **uniqueness** of Banach fixed-points and thus \(_{}\)**is **the only** fixed point for \(f(^{},;)\), implying that the iterations in Eq. 7 must converge to \(_{}\). Equivariance of the per-step iteration and the final convergent state are illustrated in Fig. 2 right.

In less ideal cases when the training loss is not zero but a small error \(\), we view it as the distance \(\|^{(1)}-^{(0)}\|\) between the ground-truth \(^{(0)}=_{}\) and the one-step iteration output \(^{(1)}\). The distance between the actual fixed-point \(^{*}\) and the ground-truth \(_{}\) is then bounded by

\[\|^{*}-_{}\|=_{k=0}^{}\|^{(k +1)}-^{(k)}\|_{k=0}^{}L^{k}\|^{(1)}-^{(0)}\|=\] (11)

where \(L\) is the Lipschitz constant of the network.

### Part Permutations

A pointcloud segmentation represented by \(^{N P}\) inherently carries an order between the \(P\) parts, which is the assumption in semantic segmentation problems. But in various real-world scenarios, it is common to encounter situations where multiple disjoint parts are associated with the same semantic label, without any clear or coherent part orderings. In order to tackle this challenge, we also extend our method to encompass the instance segmentation problem without part orderings. For simplicity, here we assume that all parts can be permuted together by \(_{P}\). In practice, permutations only occur among the parts within each semantic label, for which we can easily substitute the \(_{P}\) below with its subgroup and the conclusion still holds. We define an equivalence relation on \(^{N P}\) by

\[_{1}_{2}_{P}, }_{1}=_{2},\] (12)

which gives us a quotient space \(^{N P}/_{P}\) and each equivalent class \(}\) in this quotient space represents an instance segmentation without part ordering. We can define a metric on \(}\) by

\[d(}_{1},}_{2}):=_{ _{P}}\|_{1}-_{2}\|,\] (13)

which makes \((^{N P}/_{P},d)\) a complete metric space, on which the Banach fixed-point theorem holds and so are the iterations and convergence properties. Proofs for the well-definedness of \(d\) and the completeness of the quotient space are shown in the supplementary material.

## 4 Part-Aware Equivariant Network

In this section, we present our part-aware equivariant network for segmentation label updates (Fig.3). We utilize a \((3)\)-equivariant framework called Vector Neurons (VN)  as the backbone to encode per-point features \(^{N C 3}\), ensuring equivariance within each part. Subsequently, these features are transformed into invariant representations, eliminating relative poses to facilitate inter-part information propagation and global aggregations.

**Segmentation-weighted message passing**. Key to our part-aware equivariant network is a message-passing module weighted by the input segmentation \(\) (Fig. 3 left). Intuitively, given a point \(_{n}\) with latent feature \(_{n}\), for its neighborhood points \(_{m}\), we only allow information propagation between \(_{n}\) and \(_{m}\) if they belong to the same part. When the part segmentation \(\) is a soft mask, we compute the probability \(p_{nm}\) of \(_{n}\) and \(_{m}\) belonging to the same part by \(p_{nm}=_{n}_{m}^{t}\), \(_{n},_{m}^{1 P}\) and weight the local message passing with

\[_{n}^{}=_{m}p_{nm}\;(_{m}- _{n},_{n})_{m}p_{nm}.\] (14)

Proof of inter-part equivariance for this module can be found in the supplementary material.

**Network architecture**. Fig. 3 right shows the overall network architecture for segmentation label updates. Given an input pair \((,)\), we first extract a per-point \((3)\)-equivariant local feature \(^{N C 3}\) within each part masked by \(\). \(\) is then passed through a sequence of weighted convolutions with the message passing defined in Eq. 14. To enable efficient inter-part information exchange, after each message-passing layer, we compute a global invariant feature through a per-point VN-invariant layer, followed by a global pooling and a concatenation between per-point and global features. After the point convolutions, the per-point equivariant features \(_{n}\) are converted to invariant features \(_{n}\) with inter-part poses canceled. We then apply a global pooling on \(_{n}\) to obtain a global shape code, which is decomposed into \(P\) part codes \(_{1},,_{P}\) as in . Finally, we compute a point-part assignment score for all \((_{n},_{p})\) pairs (resulting in an \(N P\) tensor) and apply a softmax activation along its \(P\) dimension to obtain the updated segmentation \(^{}\). For instance segmentation without part permutations (Sec. 3.4), we modify the computation of the part feature \(\) by replacing the global feature decomposition with point feature grouping based on \(\).

For pointcloud networks with set-invariance across all points, directly restricting the upper bound of the network Lipschitz using weight truncations [72; 24; 87] will greatly harm network expressivity as it limits the output range on each point. On the other hand, the space of all possible segmentations \(^{N P}\) has extremely high dimensionality, making the computation of Lipschitz regularization losses  rather inefficient. Therefore, we do not explicitly constrain the Lipschitz constant of our network. Nevertheless, we confine all operations involving \(\) to local neighborhoods per point, as shown in Eq. 14. This practical approach generally ensures a small Lipschitz constant for the network in most scenarios. The stability of fixed-point convergence will be studied in Sec. 5.3.

## 5 Experiments

### Articulated-object part segmentation

We begin by evaluating our method on articulated-object part segmentation using on four categories of the Shape2Motion dataset : washing machine, oven, eyeglasses, and refrigerator. To demon

Figure 3: **Part-aware equivariant network. Left: Segmentation-weighted message passing. Right: Overall architecture for segmentation label update. Color gradients on the tensors indicate the usage of segmentation labels,**strate the generalizability of our approach, we train our model on objects in a single rest state, such as ovens with closed doors (as depicted in Fig. 5, left). This training setup aligns with many synthetic datasets featuring static shapes [9; 52]. Subsequently, we assess the performance of our model on articulated states, where global and inter-part pose transformations are applied, replicating real-world scenarios. We evaluate our model on both unseen articulation states of the training instances (Tab. 1, left) and on unseen instances (Tab. 1, right). In both cases, the joint angles are uniformly sampled from the motion range for each category. We compare our network to the most widely adopted object-level part segmentation networks PointNet  and DGCNN  without equivariance, plus VNN  with global \((3)\)-equivariance. We also compare to a VNN variant, named VNN-Inv, which first computes per-point local \((3)\)-equivariant features and then converts them to invariant features and applies invariant message passing. This is a strategy adopted by  for human segmentation. All IoUs are computed for semantic parts. In the case of eyeglasses and refrigerators, which have two parts sharing the same semantic label, we train our network with three motion part inputs but output two semantic part labels. This is feasible because our weighted message passing (Eq. 14) is agnostic to the number of parts.

Fig. 4 demonstrates our iterative inference process on novel states after training on the rest states. Fig. 5 shows our segmentation predictions on the four categories. The inter-part pose transformations can cause significant changes in pointcloud geometry and topology, yet our method can robustly generalize to the unseen articulations with inter-part equivariance.

### Multi-Object Scans

We also test our instance segmentation framework (without part orders) on multi-object scans.

**Segmentation transfer on DynLab**. DynLab  is a collection of scanned laboratory scenes, each with 2-3 rigidly moving solid objects captured under 8 different configurations with object positions randomly changed. We overfit our model to the first configuration of each scene and apply it to the 7 novel configurations, transfering the segmentation from the first scan to the others via inter-object equivariance.

We compare our method to a variety of multi-body segmentation methods, including motion-based co-segmentation methods (DeepPart , NPP , MultiBodySync ), direct segmentation prediction models (PointNet++ , MeteorNet ), geometry-based point grouping (Ward-linkage ), and pre-trained indoor instance segmentation modules (PointGroup ). The baseline setups follow . Tab. 2 shows the segmentation IoUs. One thing to note is that motion-based co-segmentation can also be viewed as learning inter-object equivariance via Siamese training, yet they cannot reach the performance of equivariance by construction.

   Setting &  &  \\  Category &  Washing \\ machine \\  &  Oven Eye- \\ glasses \\  &  Refrige- \\ glasses \\  &  Washing \\ rator \\  &  Oven Eye- \\ machine \\  &  Refrige- \\ glasses \\  & 
 Refrige- \\ rator \\  \\  PointNet  & 46.18 \(\) 3.84 & 44.08 \(\) 8.97 & 38.96 \(\)18.41 & 38.37 \(\)12.32 & 46.15 \(\) 3.18 & 45.29 \(\) 9.54 & 39.21 \(\) 17.34 & 39.00 \(\)12.13 \\ DGCNN  & 46.78 \(\) 4.37 & 44.30 \(\)0.84 & 33.35 \(\)29.36 & 39.70 \(\)14.05 & 46.60 \(\) 4.03 & 46.69 \(\)12.76 & 34.96 \(\) 22.14 & 40.13 \(\) 13.81 \\ VNN  & 47.35 \(\) 1.93 & 53.64 \(\)13.27 & 57.08 \(\)15.52 & 52.36 \(\)11.93 & 47.09 \(\) 1.84 & 51.01 \(\)11.09 & 62.98 \(\)11.09 & 48.49 \(\) 11.58 \\ VNN-Inv & 46.58 \(\) 1.61 & 62.32 \(\)14.49 & 37.24 \(\) 7.76 & 60.78 \(\)15.57 & 46.66 \(\) 1.15 & 67.35 \(\)16.50 & 38.19 \(\) 10.26 & 63.18 \(\) 14.98 \\
**Ours** & **82.32**\(\)15.08 & **81.91**\(\)0.81 & **77.78**\(\)14.45 & **77.26**\(\)7.79 & **84.99**\(\)11.76 & **82.84**\(\)8.13 & **78.65**\(\)10.36 & **73.93**\(\)14.08 \\   

Table 1: **Shape2Motion results. Numbers are segmentation IoU multiplied by 100.**

Figure 4: **Banach iterations on Shape2Motion. The network is trained on rest-state objects (left) and tested on novel articulation states.**

**Synthetic to real chair scans**. We train our model using a synthetic dataset constructed from clean ShapeNet chair models  with all instances lined up and facing the same direction (Fig. 6 left). We then test our model on the real chair scans from  with diverse scene configurations (Fig. 6 right). The configurations range from easy to hard, including: \(\) (all chairs standing on the floor), **SO(3)** (chairs laid down), and **Pile** (chairs piled into clutters). Remarkably, our model with inter-object equivariance demonstrates successful generalization across all these scenarios, even in the most challenging Pile setting with cluttered objects.

### Ablation Studies

**Network operators and convergence**. We show the importance of our locally confined message passing and \((3)\)-features (Eq. 14) to the convergence of test-time iterations. As opposed to extracting part-level equivariant features, one can also apply pose canonicalizations to each part individually and employ non-equivariant learning methods in the canonical space. For \((3)\)-transformations with a translation component in \(^{3}\) and a rotation component in \((3)\), the former can be canonicalized by subtracting the part centers and the latter by per-part PCA. Thus we compare our full model with two ablated versions with _part-level_ equivariance replaced by canonicalization: **PCA**, where we canonicalize both the part translations and rotations and simply employ a non-equivariant DGCNN  for the weighted message passing; **VNN**, where we only canonicalize the part translations but preserve the \((3)\)-equivariant structures in the network.

Tab. 3 shows the segmentation IoUs of the full model and the ablated versions on the Shape2Motion oven category. espite canonicalizations being agnostic to per-part \((3)\) transformations, they rely on absolute poses and positions, which breaks the locality of network operations, resulting in a significant increase in the Lipschitz constant of the network. Consequently, when any local equivariant operator is replaced by canonicalization, the network performance experiences a drastic drop.

To further examine the convergence ranges, we test the three models under different \(^{(0)}\) initializations by adding random noises \((0,1)\) to \(_{}\) according to \(^{(0)}=(1-)_{}+\), \(\). Fig. 7 shows the performance changes of the three models with gradually increased noise levels

   Method & Seg. IoU \\  PointNet++  & \(39.4 7.1\) \\ MeteorNet  & \(71.8 9.7\) \\ DeepPart  & \(66.3 17.2\) \\ NPP  & \(71.6 7.7\) \\ Ward-linkage  & \(88.6 5.8\) \\ PointGroup  & \(72.4 12.5\) \\ MultiBodySync  & \(94.0 3.1\) \\
**Ours** & \(\) \\   

Table 2: **DynLab segmentation.** Figure 6: **Chair scan segmentation.**on \(^{(0)}\). The PCA rotation canonicalization has a theoretically unbounded Lipschitz constant, making the network unable to converge stably even within a small neighborhood of \(_{}\). The \((3)\)-equivariant VNN with translation canonicalization perfectly overfits to the ground-truth fixed point but exhibits a rapid decline in performance as the initial \(^{(0)}\) deviates from \(_{}\). In contrast, our \((3)\)-equivariant network demonstrates stable performance across different noise levels.

**Lipschitz constraints.**. We also provide a study of different Lipschitz constraining methods under different norms in Tab. 4. For weight truncation , we use the \(l_{}\)-Lipschitz as it is the loosest constraint and set the per-layer Lipschitz upper bound to be 0.99999. For regularization losses, we set the overall network Lipschitz threshold to 0.99. The adversarial sampling  loss only works with \(l_{2}\)-norm. We reduce the total number of points from 2048 to 1024 and the GNN neighbors from 40 to 20 due to the large memory consumption for the loss computations. Based on our observations, not having an explicit Lipschitz constraint and using the SE(3)-equivariant message passing work best for the current situation, yet none of these existing Lipschitz constraining methods are helpful to the network performance. We also plot the \(l_{2}\)-norm regularization losses in the supplementary material, showing that they are zero almost everywhere.

## 6 Conclusions

In this work, we propose _Banana_, which provides both theoretical insights and experimental analysis of inter-part equivariance. While equivariance is typically examined from an algebraic perspective, we approach it from an analysis standpoint and obtain a strict formulation of inter-part equivariance at the convergent point of an iteration sequence. Based on our theoretical formulation, we propose a novel framework that co-evolves segmentation labels and per-part \((3)\)-equivariance, showing strong generalization under both global and per-part transformations even when these transformations result in subsequent changes to the pointcloud geometry or topology. Experimentally, we show that our model can generalize from rest-state articulated objects to unseen articulations, and from synthetic toy multi-object scenes to real scans with diverse object configurations.

**Limitations and future work.** While our local segmentation-weighted message passing reduces the Lipschitz constant of the network, practically providing stability during test-time iterations, it is important to note that it is not explicitly bounded or regularized. Lipschitz bounds and regularizations for set-invariant networks without compromising network expressivity would be an interesting and important problem for future study. In addition, we study the full \((3)^{P}\) action on multi-body systems, but in many real-world scenarios, different parts or objects may not move independently due to physical constraints. Limiting the \((3)^{P}\)-action to its subset of physically plausible motions can potentially increase our feature-embedding conciseness and learning efficiency.

**Broader Impacts**. Our study focuses on the general 3D geometry and its equivariance properties. Specifically, we investigate everyday appliances in common scenes, for which we do not anticipate any direct negative social impact. However, we acknowledge that our work could potentially be misused for harmful purposes. On the other hand, our research also has potential applications in areas that can benefit society, such as parsing proteins in medical science and building home robots for senior care.